{"7ffad2c63f3f2a6497437e841d3f0ce5755788c7":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/linear_model\/tests\/test_bayes.py":"MODIFY","sklearn\/linear_model\/bayes.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["24","- :class:`linear_model.ARDRegression` (bug fix)","143","- Fixed a bug in :class:`linear_model.ARDRegression` which caused incorrectly","144","  updated estimates for the standard deviation and the coefficients.","145","  :issue:`10153` by :user:`J?rg D?pfert <jdoepfert>`.","146",""],"delete":[]}],"sklearn\/linear_model\/tests\/test_bayes.py":[{"add":["11","from sklearn.utils.testing import assert_equal","113","def test_update_of_sigma_in_ard():","114","    # Checks that `sigma_` is updated correctly after the last iteration","115","    # of the ARDRegression algorithm. See issue #10128.","116","    X = np.array([[1, 0],","117","                  [0, 0]])","118","    y = np.array([0, 0])","119","    clf = ARDRegression(n_iter=1)","120","    clf.fit(X, y)","121","    # With the inputs above, ARDRegression prunes one of the two coefficients","122","    # in the first iteration. Hence, the expected shape of `sigma_` is (1, 1).","123","    assert_equal(clf.sigma_.shape, (1, 1))","124","    # Ensure that no error is thrown at prediction stage","125","    clf.predict(X, return_std=True)","126","","127",""],"delete":[]}],"sklearn\/linear_model\/bayes.py":[{"add":["471","        # Compute sigma and mu (using Woodbury matrix identity)","472","        def update_sigma(X, alpha_, lambda_, keep_lambda, n_samples):","482","            return sigma_","483","","484","        def update_coeff(X, y, coef_, alpha_, keep_lambda, sigma_):","487","            return coef_","488","","489","        # Iterative procedure of ARDRegression","490","        for iter_ in range(self.n_iter):","491","            sigma_ = update_sigma(X, alpha_, lambda_, keep_lambda, n_samples)","492","            coef_ = update_coeff(X, y, coef_, alpha_, keep_lambda, sigma_)","523","        # update sigma and mu using updated parameters from the last iteration","524","        sigma_ = update_sigma(X, alpha_, lambda_, keep_lambda, n_samples)","525","        coef_ = update_coeff(X, y, coef_, alpha_, keep_lambda, sigma_)","526",""],"delete":["471","        # Iterative procedure of ARDRegression","472","        for iter_ in range(self.n_iter):","473","            # Compute mu and sigma (using Woodbury matrix identity)"]}]}},"0caa548d7552ac78f86a24be74f4d6d49e502ace":{"changes":{"sklearn\/cluster\/hierarchical.py":"MODIFY","sklearn\/cluster\/tests\/test_feature_agglomeration.py":"MODIFY"},"diff":{"sklearn\/cluster\/hierarchical.py":[{"add":["778","        if (self.pooling_func != 'deprecated' and","779","                not isinstance(self, AgglomerationTransform)):"],"delete":["778","        if self.pooling_func != 'deprecated':"]}],"sklearn\/cluster\/tests\/test_feature_agglomeration.py":[{"add":["6","from sklearn.utils.testing import assert_true, assert_no_warnings","18","    assert_no_warnings(agglo_mean.fit, X)","19","    assert_no_warnings(agglo_median.fit, X)"],"delete":["6","from sklearn.utils.testing import assert_true","18","    agglo_mean.fit(X)","19","    agglo_median.fit(X)"]}]}},"8585275120d713cdf98466592383621dce40da92":{"changes":{"sklearn\/datasets\/base.py":"MODIFY","doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/datasets\/data\/boston_house_prices.csv":"MODIFY"},"diff":{"sklearn\/datasets\/base.py":[{"add":["704","        .. versionchanged:: 0.20","705","            Fixed a wrong data point at [445, 0].","706",""],"delete":[]}],"doc\/whats_new\/v0.20.rst":[{"add":["387","Datasets","388","","389","- Fixed a bug in :func:`dataset.load_boston` which had a wrong data point.","390","  :issue:`10801` by :user:`Takeshi Yoshizawa <tarcusx>`.","391","","487","  :issue:`10420` by :user:`Jonathan Ohayon <Johayon>`"],"delete":["482","  :issue:`10420` by :user:`Jonathan Ohayon <Johayon>`"]}],"sklearn\/datasets\/data\/boston_house_prices.csv":[{"add":["447","10.6718,0,18.1,0,0.74,6.459,94.8,1.9879,24,666,20.2,43.06,23.98,11.8"],"delete":["447","0.6718,0,18.1,0,0.74,6.459,94.8,1.9879,24,666,20.2,43.06,23.98,11.8"]}]}},"2aba6e29458d9ab1dc609db8fde6569c314313bf":{"changes":{"sklearn\/linear_model\/tests\/test_huber.py":"MODIFY","doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/linear_model\/logistic.py":"MODIFY","sklearn\/linear_model\/huber.py":"MODIFY"},"diff":{"sklearn\/linear_model\/tests\/test_huber.py":[{"add":["44","def test_huber_max_iter():","45","    X, y = make_regression_with_outliers()","46","    huber = HuberRegressor(max_iter=1)","47","    huber.fit(X, y)","48","    assert huber.n_iter_ == huber.max_iter","49","","50",""],"delete":[]}],"doc\/whats_new\/v0.20.rst":[{"add":["394","- ``n_iter_`` may vary from previous releases in","395","  :class:`linear_model.LogisticRegression` with ``solver='lbfgs'`` and","396","  :class:`linear_model.HuberRegressor`.  For Scipy <= 1.0.0, the optimizer could","397","  perform more than the requested maximum number of iterations. Now both","398","  estimators will report at most ``max_iter`` iterations even if more were","399","  performed. :issue:`10723` by `Joel Nothman`_.","400",""],"delete":[]}],"sklearn\/linear_model\/logistic.py":[{"add":["712","            # In scipy <= 1.0.0, nit may exceed maxiter.","713","            # See https:\/\/github.com\/scipy\/scipy\/issues\/7854.","714","            n_iter_i = min(info['nit'], max_iter)","1113","        .. versionchanged:: 0.20","1114","","1115","            In SciPy <= 1.0.0 the number of lbfgs iterations may exceed","1116","            ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.","1117",""],"delete":["712","            n_iter_i = info['nit'] - 1"]}],"sklearn\/linear_model\/huber.py":[{"add":["184","","185","        .. versionchanged:: 0.20","186","","187","            In SciPy <= 1.0.0 the number of lbfgs iterations may exceed","188","            ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.","270","        # In scipy <= 1.0.0, nit may exceed maxiter.","271","        # See https:\/\/github.com\/scipy\/scipy\/issues\/7854.","272","        self.n_iter_ = min(dict_['nit'], self.max_iter)"],"delete":["184","        Not available if SciPy version is 0.9 and below.","266","        self.n_iter_ = dict_['nit']"]}]}},"b25e2223288776e6cefd3be467df285c52ed8249":{"changes":{"sklearn\/preprocessing\/_discretization.py":"MODIFY","doc\/datasets\/index.rst":"MODIFY","doc\/whats_new\/v0.20.rst":"MODIFY","doc\/modules\/compose.rst":"MODIFY","sklearn\/tests\/test_discriminant_analysis.py":"MODIFY","sklearn\/discriminant_analysis.py":"MODIFY","doc\/modules\/preprocessing.rst":"MODIFY","sklearn\/preprocessing\/_encoders.py":"MODIFY","doc\/faq.rst":"MODIFY","sklearn\/datasets\/descr\/wine_data.rst":"MODIFY","sklearn\/decomposition\/pca.py":"MODIFY","doc\/developers\/contributing.rst":"MODIFY"},"diff":{"sklearn\/preprocessing\/_discretization.py":[{"add":["69","        The edges of each bin. Contain arrays of varying shapes ``(n_bins_, )``"],"delete":["69","        The edges of each bin. Contain arrays of varying shapes (n_bins_, )."]}],"doc\/datasets\/index.rst":[{"add":["156",".. _sample_generators:"],"delete":["156",".. _generated_datasets:"]}],"doc\/whats_new\/v0.20.rst":[{"add":["139","  and :issue:`11315` by :user:`Thomas Fan <thomasjpfan>`.","210","  :issue:`11166` by `Gael Varoquaux`_"],"delete":["139","  and :issue:`11315` by :user:`Thomas Fan <thomasjpfan>`_.","210","  :issue:`11166`by `Gael Varoquaux`_"]}],"doc\/modules\/compose.rst":[{"add":["502"," * :ref:`sphx_glr_auto_examples_compose_plot_column_transformer.py`","503"," * :ref:`sphx_glr_auto_examples_compose_plot_column_transformer_mixed_types.py`"],"delete":["502"," * :ref:`sphx_glr_auto_examples_compose_column_transformer.py`","503"," * :ref:`sphx_glr_auto_examples_compose_column_transformer_mixed_types.py`"]}],"sklearn\/tests\/test_discriminant_analysis.py":[{"add":["326","    assert_warns_message(DeprecationWarning, \"Attribute ``covariances_`` was \"","328","                         \"in 0.21. Use ``covariance_`` instead\", getattr, clf,"],"delete":["326","    assert_warns_message(DeprecationWarning, \"Attribute covariances_ was \"","328","                         \"in 0.21. Use covariance_ instead\", getattr, clf,"]}],"sklearn\/discriminant_analysis.py":[{"add":["624","    @deprecated(\"Attribute ``covariances_`` was deprecated in version\"","626","                \"``covariance_`` instead\")"],"delete":["624","    @deprecated(\"Attribute covariances_ was deprecated in version\"","626","                \"covariance_ instead\")"]}],"doc\/modules\/preprocessing.rst":[{"add":["584","  * :ref:`sphx_glr_auto_examples_preprocessing_plot_discretization.py`","585","  * :ref:`sphx_glr_auto_examples_preprocessing_plot_discretization_classification.py`","586","  * :ref:`sphx_glr_auto_examples_preprocessing_plot_discretization_strategies.py`"],"delete":["584","  * :ref:`sphx_glr_auto_examples_plot_discretization.py`","585","  * :ref:`sphx_glr_auto_examples_plot_discretization_classification.py`","586","  * :ref:`sphx_glr_auto_examples_plot_discretization_strategies.py`"]}],"sklearn\/preprocessing\/_encoders.py":[{"add":["200","            The ``active_features_`` attribute was deprecated in version","207","        (and then potentially masked by ``active_features_`` afterwards)","210","            The ``feature_indices_`` attribute was deprecated in version","217","            The ``n_values_`` attribute was deprecated in version","271","    @deprecated(\"The ``active_features_`` attribute was deprecated in version \"","278","    @deprecated(\"The ``feature_indices_`` attribute was deprecated in version \"","285","    @deprecated(\"The ``n_values_`` attribute was deprecated in version \""],"delete":["200","            The `active_features_` attribute was deprecated in version","207","        (and then potentially masked by `active_features_` afterwards)","210","            The `feature_indices_` attribute was deprecated in version","217","            The `n_values_` attribute was deprecated in version","271","    @deprecated(\"The 'active_features_' attribute was deprecated in version \"","278","    @deprecated(\"The 'feature_indices_' attribute was deprecated in version \"","285","    @deprecated(\"The 'n_values_' attribute was deprecated in version \""]}],"doc\/faq.rst":[{"add":["364","See also :ref:`sphx_glr_auto_examples_compose_plot_column_transformer_mixed_types.py` for an"],"delete":["364","See also :ref:`sphx_glr_auto_examples_compose_column_transformer_mixed_types.py` for an"]}],"sklearn\/datasets\/descr\/wine_data.rst":[{"add":["23","","94","  (Also submitted to Journal of Chemometrics)."],"delete":["93","  (Also submitted to Journal of Chemometrics)."]}],"sklearn\/decomposition\/pca.py":[{"add":["131","        If ``n_components == 'mle'`` and ``svd_solver == 'full'``, Minka's"],"delete":["131","        If ``n_components == 'mle'`` and ``svd_solver == 'full'``, Minka\\'s"]}],"doc\/developers\/contributing.rst":[{"add":["43","=================="],"delete":["43","======="]}]}},"b961ff4a79dc98f8886178a7e3b55b4c8c7f1991":{"changes":{"doc\/modules\/model_persistence.rst":"MODIFY"},"diff":{"doc\/modules\/model_persistence.rst":[{"add":["75","* The training data, e.g. a reference to an immutable snapshot"],"delete":["75","* The training data, e.g. a reference to a immutable snapshot"]}]}},"8424dd1e2a1e5624d0c5488022c34c57e2a313dd":{"changes":{".travis.yml":"MODIFY"},"diff":{".travis.yml":[{"add":["59","       if: type = cron OR commit_message =~ \/\\[scipy-dev\\]\/"],"delete":["59","       if: type = cron OR commit_message ~ \/\\[scipy-dev\\]\/"]}]}},"a38067093f08bf04483440984bcd5e8b6973f78c":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/mixture\/tests\/test_gaussian_mixture.py":"MODIFY","sklearn\/mixture\/tests\/test_mixture.py":"ADD","sklearn\/mixture\/base.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["366","- Fixed a bug in :class:`mixture.BaseMixture` where the reported `n_iter_` was","367","  missing an iteration. It affected :class:`mixture.GaussianMixture` and","368","  :class:`mixture.BayesianGaussianMixture`. :issue:`10740` by :user:`Erich","369","  Schubert <kno10>` and :user:`Guillaume Lemaitre <glemaitre>`.","370",""],"delete":[]}],"sklearn\/mixture\/tests\/test_gaussian_mixture.py":[{"add":["2","# License: BSD 3 clause"],"delete":["2","# License: BSD 3 clauseimport warnings"]}],"sklearn\/mixture\/tests\/test_mixture.py":[{"add":[],"delete":[]}],"sklearn\/mixture\/base.py":[{"add":["210","            for n_iter in range(1, self.max_iter + 1):"],"delete":["210","            for n_iter in range(self.max_iter):"]}]}},"03dd2870157490f1f8f484d9b81bfe0ac1df4917":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/linear_model\/tests\/test_ridge.py":"MODIFY","sklearn\/linear_model\/ridge.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["279","- Fixed a bug in :class:`linear_model.RidgeCV` where using integer ``alphas``","280","  raised an error. :issue:`10393` by :user:`Mabel Villalba-Jim¨¦nez <mabelvj>`.","281","","476","  :issue:`10420` by :user:`Jonathan Ohayon <Johayon>`"],"delete":["473","  :issue:`10420` by :user:`Jonathan Ohayon <Johayon>`"]}],"sklearn\/linear_model\/tests\/test_ridge.py":[{"add":["13","from sklearn.utils.testing import assert_raises_regex","54","","708","def test_ridgecv_int_alphas():","709","    X = np.array([[-1.0, -1.0], [-1.0, 0], [-.8, -1.0],","710","                  [1.0, 1.0], [1.0, 0.0]])","711","    y = [1, 1, 1, -1, -1]","712","","713","    # Integers","714","    ridge = RidgeCV(alphas=(1, 10, 100))","715","    ridge.fit(X, y)","716","","717","","718","def test_ridgecv_negative_alphas():","719","    X = np.array([[-1.0, -1.0], [-1.0, 0], [-.8, -1.0],","720","                  [1.0, 1.0], [1.0, 0.0]])","721","    y = [1, 1, 1, -1, -1]","722","","723","    # Negative integers","724","    ridge = RidgeCV(alphas=(-1, -10, -100))","725","    assert_raises_regex(ValueError,","726","                        \"alphas cannot be negative.\",","727","                        ridge.fit, X, y)","728","","729","    # Negative floats","730","    ridge = RidgeCV(alphas=(-0.1, -1.0, -10.0))","731","    assert_raises_regex(ValueError,","732","                        \"alphas cannot be negative.\",","733","                        ridge.fit, X, y)","734","","735",""],"delete":[]}],"sklearn\/linear_model\/ridge.py":[{"add":["780","","1044","        if np.any(self.alphas < 0):","1045","            raise ValueError(\"alphas cannot be negative. \"","1046","                             \"Got {} containing some \"","1047","                             \"negative value instead.\".format(self.alphas))","1048","","1051","                out, c = _errors(float(alpha), y, v, Q, QT_y)","1053","                out, c = _values(float(alpha), y, v, Q, QT_y)","1093","        self.alphas = np.asarray(alphas)","1336",""],"delete":["1045","                out, c = _errors(alpha, y, v, Q, QT_y)","1047","                out, c = _values(alpha, y, v, Q, QT_y)","1087","        self.alphas = alphas"]}]}},"5da8869d3f2036ac7f6cadf3e05c69e9f5140cc2":{"changes":{"sklearn\/src\/cblas\/atlas_misc.h":"MODIFY"},"diff":{"sklearn\/src\/cblas\/atlas_misc.h":[{"add":["186","   #define UPR q"],"delete":["186","   #deffine UPR q"]}]}},"84f5ee6529b2dd0d374b0ce0b172916d9b4e356b":{"changes":{"sklearn\/metrics\/classification.py":"MODIFY","doc\/modules\/calibration.rst":"MODIFY"},"diff":{"sklearn\/metrics\/classification.py":[{"add":["1930","    of only 0 and 1). The Brier loss is composed of refinement loss and","1931","    calibration loss."],"delete":["1921","","1923","","1932","    of only 0 and 1).","1933","","1940",""]}],"doc\/modules\/calibration.rst":[{"add":["111","both isotonic calibration and sigmoid calibration. ","112","The Brier score is a metric which is a combination of calibration loss and refinement loss,","113",":func:`brier_score_loss`, reported in the legend (the smaller the better).","114","Calibration loss is defined as the mean squared deviation from empirical probabilities","115","derived from the slope of ROC segments. Refinement loss can be defined as the expected","116","optimal loss as measured by the area under the optimal cost curve."],"delete":["111","both isotonic calibration and sigmoid calibration. The calibration performance","112","is evaluated with Brier score :func:`brier_score_loss`, reported in the legend","113","(the smaller the better)."]}]}},"4bead39f7899cbd08ab08959a0311843c84f69df":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/datasets\/samples_generator.py":"MODIFY","sklearn\/datasets\/tests\/test_samples_generator.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["158","- Fixed a bug in :func:`datasets.make_circles`, where no odd number of data ","159","  points could be generated. :issue:`10037` by :user:`Christian Braune ","160","  <christianbraune79>`_.","161","  "],"delete":[]}],"sklearn\/datasets\/samples_generator.py":[{"add":["587","        The total number of points generated. If odd, the inner circle will","588","        have one point more than the outer circle.","602","    factor : 0 < double < 1 (default=.8)","614","    if factor >= 1 or factor < 0:","617","    n_samples_out = n_samples \/\/ 2","618","    n_samples_in = n_samples - n_samples_out","619","","621","    # so as not to have the first point = last point, we set endpoint=False","622","    linspace_out = np.linspace(0, 2 * np.pi, n_samples_out, endpoint=False)","623","    linspace_in = np.linspace(0, 2 * np.pi, n_samples_in, endpoint=False)","624","    outer_circ_x = np.cos(linspace_out)","625","    outer_circ_y = np.sin(linspace_out)","626","    inner_circ_x = np.cos(linspace_in) * factor","627","    inner_circ_y = np.sin(linspace_in) * factor","631","    y = np.hstack([np.zeros(n_samples_out, dtype=np.intp),","632","                   np.ones(n_samples_in, dtype=np.intp)])"],"delete":["587","        The total number of points generated.","601","    factor : double < 1 (default=.8)","613","    if factor > 1 or factor < 0:","617","    # so as not to have the first point = last point, we add one and then","618","    # remove it.","619","    linspace = np.linspace(0, 2 * np.pi, n_samples \/\/ 2 + 1)[:-1]","620","    outer_circ_x = np.cos(linspace)","621","    outer_circ_y = np.sin(linspace)","622","    inner_circ_x = outer_circ_x * factor","623","    inner_circ_y = outer_circ_y * factor","627","    y = np.hstack([np.zeros(n_samples \/\/ 2, dtype=np.intp),","628","                   np.ones(n_samples \/\/ 2, dtype=np.intp)])"]}],"sklearn\/datasets\/tests\/test_samples_generator.py":[{"add":["27","from sklearn.datasets import make_circles","388","","389","","390","def test_make_circles():","391","    factor = 0.3","392","","393","    for (n_samples, n_outer, n_inner) in [(7, 3, 4), (8, 4, 4)]:","394","        # Testing odd and even case, because in the past make_circles always","395","        # created an even number of samples.","396","        X, y = make_circles(n_samples, shuffle=False, noise=None,","397","                            factor=factor)","398","        assert_equal(X.shape, (n_samples, 2), \"X shape mismatch\")","399","        assert_equal(y.shape, (n_samples,), \"y shape mismatch\")","400","        center = [0.0, 0.0]","401","        for x, label in zip(X, y):","402","            dist_sqr = ((x - center) ** 2).sum()","403","            dist_exp = 1.0 if label == 0 else factor**2","404","            assert_almost_equal(dist_sqr, dist_exp,","405","                                err_msg=\"Point is not on expected circle\")","406","","407","        assert_equal(X[y == 0].shape, (n_outer, 2),","408","                     \"Samples not correctly distributed across circles.\")","409","        assert_equal(X[y == 1].shape, (n_inner, 2),","410","                     \"Samples not correctly distributed across circles.\")","411","","412","    assert_raises(ValueError, make_circles, factor=-0.01)","413","    assert_raises(ValueError, make_circles, factor=1.)"],"delete":[]}]}},"da71b827b8b56bd8305b7fe6c13724c7b5355209":{"changes":{"sklearn\/linear_model\/tests\/test_coordinate_descent.py":"MODIFY","doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/linear_model\/coordinate_descent.py":"MODIFY"},"diff":{"sklearn\/linear_model\/tests\/test_coordinate_descent.py":[{"add":["805","","806","","807","def test_coef_shape_not_zero():","808","    est_no_intercept = Lasso(fit_intercept=False)","809","    est_no_intercept.fit(np.c_[np.ones(3)], np.ones(3))","810","    assert est_no_intercept.coef_.shape == (1,)"],"delete":[]}],"doc\/whats_new\/v0.20.rst":[{"add":["248","- Fixed a bug in :class:`sklearn.linear_model.Lasso`","249","  where the coefficient had wrong shape when ``fit_intercept=False``.","250","  :issue:`10687` by :user:`Martin Hahn <martin-hahn>`.","251",""],"delete":[]}],"sklearn\/linear_model\/coordinate_descent.py":[{"add":["765","            self.coef_ = coef_[0]","766","            self.dual_gap_ = dual_gaps_[0]","767","        else:","768","            self.coef_ = coef_","769","            self.dual_gap_ = dual_gaps_"],"delete":["766","        self.coef_, self.dual_gap_ = map(np.squeeze, [coef_, dual_gaps_])"]}]}},"002f95cd912783d2e86f36a5bb79aca79b6ab917":{"changes":{"sklearn\/model_selection\/_search.py":"MODIFY"},"diff":{"sklearn\/model_selection\/_search.py":[{"add":["1129","    Note that before SciPy 0.16, the ``scipy.stats.distributions`` do not","1130","    accept a custom RNG instance and always use the singleton RNG from","1131","    ``numpy.random``. Hence setting ``random_state`` will not guarantee a","1132","    deterministic iteration whenever ``scipy.stats`` distributions are used to","1133","    define the parameter search space.","1134",""],"delete":[]}]}},"b0e91e4110942e5b3c4333b1c6b6dfefbd1a6124":{"changes":{"examples\/gaussian_process\/plot_gpc_xor.py":"MODIFY","examples\/compose\/plot_column_transformer.py":"MODIFY"},"diff":{"examples\/gaussian_process\/plot_gpc_xor.py":[{"add":["44","    contours = plt.contour(xx, yy, Z, levels=[0.5], linewidths=2,","45","                           colors=['k'])"],"delete":["44","    contours = plt.contour(xx, yy, Z, levels=[0], linewidths=2,","45","                           linetypes='--')"]}],"examples\/compose\/plot_column_transformer.py":[{"add":["42","from sklearn.svm import LinearSVC","119","    ('svc', LinearSVC()),"],"delete":["42","from sklearn.svm import SVC","119","    ('svc', SVC(kernel='linear')),"]}]}},"5d9a686be14560eb49e5a8b9bb237b776c519373":{"changes":{"sklearn\/linear_model\/stochastic_gradient.py":"MODIFY","sklearn\/linear_model\/tests\/test_sgd.py":"MODIFY"},"diff":{"sklearn\/linear_model\/stochastic_gradient.py":[{"add":["848","        check_is_fitted(self, \"t_\")","849",""],"delete":["804","        check_is_fitted(self, \"t_\")","805",""]}],"sklearn\/linear_model\/tests\/test_sgd.py":[{"add":["2","import pytest","470","    def test_sgd_predict_proba_method_access(self):","471","        # Checks that SGDClassifier predict_proba and predict_log_proba methods","472","        # can either be accessed or raise an appropriate error message","473","        # otherwise. See","474","        # https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/10938 for more","475","        # details.","476","        for loss in SGDClassifier.loss_functions:","477","            clf = SGDClassifier(loss=loss)","478","            if loss in ('log', 'modified_huber'):","479","                assert hasattr(clf, 'predict_proba')","480","                assert hasattr(clf, 'predict_log_proba')","481","            else:","482","                message = (\"probability estimates are not \"","483","                           \"available for loss={!r}\".format(loss))","484","                assert not hasattr(clf, 'predict_proba')","485","                assert not hasattr(clf, 'predict_log_proba')","486","                with pytest.raises(AttributeError,","487","                                   message=message):","488","                    clf.predict_proba","489","                with pytest.raises(AttributeError,","490","                                   message=message):","491","                    clf.predict_log_proba","492",""],"delete":[]}]}},"653de6ca54becfc02ea9c90db567e8613843ccda":{"changes":{"doc\/modules\/model_evaluation.rst":"MODIFY"},"diff":{"doc\/modules\/model_evaluation.rst":[{"add":["466","    * Macro-average recall as described in [Mosley2013]_, [Kelleher2015]_ and [Guyon2015]_:","467","      the recall for each class is computed independently and the average is taken over all classes.","468","      In [Guyon2015]_, the macro-average recall is then adjusted to ensure that random predictions","469","      have a score of :math:`0` while perfect predictions have a score of :math:`1`.","470","      One can compute the macro-average recall using ``recall_score(average=\"macro\")`` in :func:`recall_score`.","471","    * Class balanced accuracy as described in [Mosley2013]_: the minimum between the precision","472","      and the recall for each class is computed. Those values are then averaged over the total","473","      number of classes to get the balanced accuracy.","474","    * Balanced Accuracy as described in [Urbanowicz2015]_: the average of sensitivity and selectivity","475","      is computed for each class and then averaged over total number of classes.","478","    the :func:`balanced_accuracy_score` function.","493","  .. [Urbanowicz2015] Urbanowicz R.J.,  Moore, J.H. `ExSTraCS 2.0: description and evaluation of a scalable learning","494","     classifier system < https:\/\/doi.org\/10.1007\/s12065-015-0128-8>`_, Evol. Intel. (2015) 8: 89."],"delete":["466","    * Normalized class-wise accuracy average as described in [Guyon2015]_: for multi-class","467","      classification problem, each sample is assigned the class with maximum prediction value.","468","      The predictions are then binarized to compute the accuracy of each class on a","469","      one-vs-rest fashion. The balanced accuracy is obtained by averaging the individual","470","      accuracies over all classes and then normalized by the expected value of balanced","471","      accuracy for random predictions (:math:`0.5` for binary classification, :math:`1\/C`","472","      for C-class classification problem).","473","    * Macro-average recall as described in [Mosley2013]_ and [Kelleher2015]_: the recall","474","      for each class is computed independently and the average is taken over all classes.","477","    the :func:`balanced_accuracy_score` function. However, the macro-averaged recall","478","    is implemented in :func:`sklearn.metrics.recall_score`: set ``average`` parameter","479","    to ``\"macro\"``."]}]}},"02d0a03ad1c1003ed57d54bf0d2bdedea7299c36":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/feature_extraction\/text.py":"MODIFY","sklearn\/feature_extraction\/tests\/test_text.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["488","- Fixed bug in :class:`feature_extraction.text.TFIDFVectorizer` which ","489","  was ignoring the parameter ``dtype``. In addition,","490","  :class:`feature_extraction.text.TFIDFTransformer` will preserve ``dtype``","491","  for floating and raise a warning if ``dtype`` requested is integer.","492","  :issue:`10441` by :user:`Mayur Kulkarni <maykulkarni>` and","493","  :user:`Guillaume Lemaitre <glemaitre>`.","494","  "],"delete":[]}],"sklearn\/feature_extraction\/text.py":[{"add":["13","from __future__ import unicode_literals, division","21","import warnings","32","from ..utils.validation import check_is_fitted, check_array, FLOAT_DTYPES","576","        return np.diff(X.indptr)","1120","        X = check_array(X, accept_sparse=('csr', 'csc'))","1122","            X = sp.csr_matrix(X)","1123","        dtype = X.dtype if X.dtype in FLOAT_DTYPES else np.float64","1124","","1127","            df = _document_frequency(X).astype(dtype)","1135","            idf = np.log(n_samples \/ df) + 1","1136","            self._idf_diag = sp.diags(idf, offsets=0,","1137","                                      shape=(n_features, n_features),","1138","                                      format='csr',","1139","                                      dtype=dtype)","1159","        X = check_array(X, accept_sparse='csr', dtype=FLOAT_DTYPES, copy=copy)","1160","        if not sp.issparse(X):","1161","            X = sp.csr_matrix(X, dtype=np.float64)","1372","                 dtype=np.float64, norm='l2', use_idf=True, smooth_idf=True,","1437","    def _check_params(self):","1438","        if self.dtype not in FLOAT_DTYPES:","1439","            warnings.warn(\"Only {} 'dtype' should be used. {} 'dtype' will \"","1440","                          \"be converted to np.float64.\"","1441","                          .format(FLOAT_DTYPES, self.dtype),","1442","                          UserWarning)","1443","","1456","        self._check_params()","1477","        self._check_params()"],"delete":["13","from __future__ import unicode_literals","31","from ..utils.validation import check_is_fitted","575","        return np.diff(sp.csc_matrix(X, copy=False).indptr)","1120","            X = sp.csc_matrix(X)","1123","            df = _document_frequency(X)","1131","            idf = np.log(float(n_samples) \/ df) + 1.0","1132","            self._idf_diag = sp.spdiags(idf, diags=0, m=n_features,","1133","                                        n=n_features, format='csr')","1153","        if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.floating):","1154","            # preserve float family dtype","1155","            X = sp.csr_matrix(X, copy=copy)","1156","        else:","1157","            # convert counts or binary occurrences to floats","1158","            X = sp.csr_matrix(X, dtype=np.float64, copy=copy)","1369","                 dtype=np.int64, norm='l2', use_idf=True, smooth_idf=True,"]}],"sklearn\/feature_extraction\/tests\/test_text.py":[{"add":["3","import pytest","4","from scipy import sparse","5","","33","                                   SkipTest, assert_raises,","34","                                   assert_allclose_dense_sparse)","1042","@pytest.mark.parametrize(\"X_dtype\", [np.float32, np.float64])","1043","def test_tfidf_transformer_type(X_dtype):","1044","    X = sparse.rand(10, 20000, dtype=X_dtype, random_state=42)","1045","    X_trans = TfidfTransformer().fit_transform(X)","1046","    assert X_trans.dtype == X.dtype","1047","","1048","","1049","def test_tfidf_transformer_sparse():","1050","    X = sparse.rand(10, 20000, dtype=np.float64, random_state=42)","1051","    X_csc = sparse.csc_matrix(X)","1052","    X_csr = sparse.csr_matrix(X)","1053","","1054","    X_trans_csc = TfidfTransformer().fit_transform(X_csc)","1055","    X_trans_csr = TfidfTransformer().fit_transform(X_csr)","1056","    assert_allclose_dense_sparse(X_trans_csc, X_trans_csr)","1057","    assert X_trans_csc.format == X_trans_csr.format","1058","","1059","","1060","@pytest.mark.parametrize(","1061","    \"vectorizer_dtype, output_dtype, expected_warning, msg_warning\",","1062","    [(np.int32, np.float64, UserWarning, \"'dtype' should be used.\"),","1063","     (np.int64, np.float64, UserWarning, \"'dtype' should be used.\"),","1064","     (np.float32, np.float32, None, None),","1065","     (np.float64, np.float64, None, None)]","1066",")","1067","def test_tfidf_vectorizer_type(vectorizer_dtype, output_dtype,","1068","                               expected_warning, msg_warning):","1069","    X = np.array([\"numpy\", \"scipy\", \"sklearn\"])","1070","    vectorizer = TfidfVectorizer(dtype=vectorizer_dtype)","1071","    with pytest.warns(expected_warning, match=msg_warning) as record:","1072","            X_idf = vectorizer.fit_transform(X)","1073","    if expected_warning is None:","1074","        assert len(record) == 0","1075","    assert X_idf.dtype == output_dtype","1076","","1077",""],"delete":["30","                                   SkipTest, assert_raises)","37","import pytest","38",""]}]}},"3e26fc63be99942ca32efd181f1ce04efeae418a":{"changes":{"doc\/tutorial\/machine_learning_map\/pyparsing.py":"MODIFY","sklearn\/tests\/test_multiclass.py":"MODIFY","doc\/whats_new\/v0.19.rst":"MODIFY","sklearn\/decomposition\/online_lda.py":"MODIFY","sklearn\/tree\/_splitter.pyx":"MODIFY","sklearn\/utils\/tests\/test_random.py":"MODIFY","sklearn\/neighbors\/graph.py":"MODIFY","sklearn\/metrics\/ranking.py":"MODIFY","doc\/glossary.rst":"MODIFY","sklearn\/externals\/joblib\/numpy_pickle_utils.py":"MODIFY","sklearn\/model_selection\/_search.py":"MODIFY","sklearn\/neighbors\/quad_tree.pyx":"MODIFY","sklearn\/ensemble\/tests\/test_forest.py":"MODIFY","benchmarks\/bench_plot_randomized_svd.py":"MODIFY","examples\/gaussian_process\/plot_gpr_noisy_targets.py":"MODIFY","sklearn\/externals\/joblib\/pool.py":"MODIFY","sklearn\/preprocessing\/data.py":"MODIFY","sklearn\/model_selection\/_split.py":"MODIFY","doc\/developers\/maintainer.rst":"MODIFY","sklearn\/ensemble\/iforest.py":"MODIFY","sklearn\/utils\/validation.py":"MODIFY","examples\/preprocessing\/plot_scaling_importance.py":"MODIFY","sklearn\/neighbors\/quad_tree.pxd":"MODIFY","examples\/gaussian_process\/plot_gpc_isoprobability.py":"MODIFY","examples\/svm\/plot_rbf_parameters.py":"MODIFY","doc\/tutorial\/machine_learning_map\/ML_MAPS_README.txt":"MODIFY","sklearn\/multiclass.py":"MODIFY","sklearn\/cluster\/_k_means_elkan.pyx":"MODIFY","doc\/themes\/scikit-learn\/static\/nature.css_t":"MODIFY","sklearn\/datasets\/tests\/test_base.py":"MODIFY","sklearn\/linear_model\/sgd_fast.pyx":"MODIFY"},"diff":{"doc\/tutorial\/machine_learning_map\/pyparsing.py":[{"add":["1606","         - explicitly expand the tabs in your input string before calling\r"],"delete":["1606","         - explictly expand the tabs in your input string before calling\r"]}],"sklearn\/tests\/test_multiclass.py":[{"add":["82","    # Test if partial_fit is working as intended"],"delete":["82","    # Test if partial_fit is working as intented"]}],"doc\/whats_new\/v0.19.rst":[{"add":["809","  the weighted impurity decrease from splitting is no longer atleast"],"delete":["809","  the weighted impurity decrease from splitting is no longer alteast"]}],"sklearn\/decomposition\/online_lda.py":[{"add":["44","        Exponential value of expectation of log topic word distribution."],"delete":["44","        Exponential value of expection of log topic word distribution."]}],"sklearn\/tree\/_splitter.pyx":[{"add":["1255","        # We assume implicitly that end_positive = end and","1491","        # We assume implicitly that end_positive = end and"],"delete":["1255","        # We assume implicitely that end_positive = end and","1491","        # We assume implicitely that end_positive = end and"]}],"sklearn\/utils\/tests\/test_random.py":[{"add":["108","    class_probabilities = [np.array([0.5, 0.5]), np.array([0.6, 0.1, 0.3])]","110","    got = random_choice_csc(n_samples, classes, class_probabilities,","116","        assert_array_almost_equal(class_probabilities[k], p, decimal=1)","120","    class_probabilities = [np.array([0.5, 0.5]), np.array([0, 1\/2, 1\/2])]","129","        assert_array_almost_equal(class_probabilities[k], p, decimal=1)","133","    class_probabilities = [np.array([1.0, 0.0]), np.array([0.0, 1.0, 0.0])]","135","    got = random_choice_csc(n_samples, classes, class_probabilities,","141","                        minlength=len(class_probabilities[k])) \/ n_samples","142","        assert_array_almost_equal(class_probabilities[k], p, decimal=1)","146","    class_probabilities = [np.array([0.0, 1.0]), np.array([1.0])]","155","        assert_array_almost_equal(class_probabilities[k], p, decimal=1)","159","    # the length of an array in classes and class_probabilities is mismatched","161","    class_probabilities = [np.array([0.5, 0.5]), np.array([0.6, 0.1, 0.3])]","163","                  class_probabilities, 1)","167","    class_probabilities = [np.array([0.5, 0.5]), np.array([0.6, 0.1, 0.3])]","169","                  class_probabilities, 1)","173","    class_probabilities = [np.array([0.5, 0.5]), np.array([0.6, 0.1, 0.3])]","175","                  class_probabilities, 1)","179","    class_probabilities = [np.array([0.5, 0.6]), np.array([0.6, 0.1, 0.3])]","181","                  class_probabilities, 1)"],"delete":["108","    class_probabilites = [np.array([0.5, 0.5]), np.array([0.6, 0.1, 0.3])]","110","    got = random_choice_csc(n_samples, classes, class_probabilites,","116","        assert_array_almost_equal(class_probabilites[k], p, decimal=1)","120","    class_probabilites = [np.array([0.5, 0.5]), np.array([0, 1\/2, 1\/2])]","129","        assert_array_almost_equal(class_probabilites[k], p, decimal=1)","133","    class_probabilites = [np.array([1.0, 0.0]), np.array([0.0, 1.0, 0.0])]","135","    got = random_choice_csc(n_samples, classes, class_probabilites,","141","                        minlength=len(class_probabilites[k])) \/ n_samples","142","        assert_array_almost_equal(class_probabilites[k], p, decimal=1)","146","    class_probabilites = [np.array([0.0, 1.0]), np.array([1.0])]","155","        assert_array_almost_equal(class_probabilites[k], p, decimal=1)","159","    # the length of an array in classes and class_probabilites is mismatched","161","    class_probabilites = [np.array([0.5, 0.5]), np.array([0.6, 0.1, 0.3])]","163","                  class_probabilites, 1)","167","    class_probabilites = [np.array([0.5, 0.5]), np.array([0.6, 0.1, 0.3])]","169","                  class_probabilites, 1)","173","    class_probabilites = [np.array([0.5, 0.5]), np.array([0.6, 0.1, 0.3])]","175","                  class_probabilites, 1)","179","    class_probabilites = [np.array([0.5, 0.6]), np.array([0.6, 0.1, 0.3])]","181","                  class_probabilites, 1)"]}],"sklearn\/neighbors\/graph.py":[{"add":["70","        for mode='distance' as this will preserve backwards compatibility.","145","        for mode='distance' as this will preserve backwards compatibility."],"delete":["70","        for mode='distance' as this will preserve backwards compatibilty.","145","        for mode='distance' as this will preserve backwards compatibilty."]}],"sklearn\/metrics\/ranking.py":[{"add":["420","        # the presence of floating point errors"],"delete":["420","        # the presense of floating point errors"]}],"doc\/glossary.rst":[{"add":["992","        Continuous multioutput targets are represented as multiple"],"delete":["992","        Continous multioutput targets are represented as multiple"]}],"sklearn\/externals\/joblib\/numpy_pickle_utils.py":[{"add":["103","        # file which."],"delete":["103","        # file whic."]}],"sklearn\/model_selection\/_search.py":[{"add":["699","                # `\"param_%s\" % name` at the first occurrence of `name`."],"delete":["699","                # `\"param_%s\" % name` at the first occurence of `name`."]}],"sklearn\/neighbors\/quad_tree.pyx":[{"add":["424","                This is useful in t-SNE to compute the negative forces."],"delete":["424","                This is usefull in t-SNE to compute the negative forces."]}],"sklearn\/ensemble\/tests\/test_forest.py":[{"add":["225","    importances_parallel = est.feature_importances_","226","    assert_array_almost_equal(importances, importances_parallel)"],"delete":["225","    importances_parrallel = est.feature_importances_","226","    assert_array_almost_equal(importances, importances_parrallel)"]}],"benchmarks\/bench_plot_randomized_svd.py":[{"add":["33","(c) plot: time vs norm, varying datasets"],"delete":["33","(c) plot: time vs norm, varing datasets"]}],"examples\/gaussian_process\/plot_gpr_noisy_targets.py":[{"add":["51","# Instantiate a Gaussian Process model"],"delete":["51","# Instanciate a Gaussian Process model"]}],"sklearn\/externals\/joblib\/pool.py":[{"add":["575","            # arrays and that is else able to dump to memmap large in-memory"],"delete":["575","            # arrays and that is alse able to dump to memmap large in-memory"]}],"sklearn\/preprocessing\/data.py":[{"add":["2793","            If True, check that all data is positive and non-zero."],"delete":["2793","            If True, check that all data is postive and non-zero."]}],"sklearn\/model_selection\/_split.py":[{"add":["1098","    RepeatedStratifiedKFold: Repeats Stratified K-Fold n times."],"delete":["1098","    RepeatedStratifiedKFold: Repeates Stratified K-Fold n times."]}],"doc\/developers\/maintainer.rst":[{"add":["86","The branch targeted by the Cron job and the frequency of the Cron job is set"],"delete":["86","The branch targetted by the Cron job and the frequency of the Cron job is set"]}],"sklearn\/ensemble\/iforest.py":[{"add":["285","        # We subtract self.offset_ to make 0 be the threshold value for being"],"delete":["285","        # We substract self.offset_ to make 0 be the threshold value for being"]}],"sklearn\/utils\/validation.py":[{"add":["499","        # when no dtype conversion happened, for example dtype = None. The"],"delete":["499","        # when no dtype conversion happend, for example dtype = None. The"]}],"examples\/preprocessing\/plot_scaling_importance.py":[{"add":["91","# Show first principal components"],"delete":["91","# Show first principal componenets"]}],"sklearn\/neighbors\/quad_tree.pxd":[{"add":["28","    # Base storage structure for cells in a QuadTree object"],"delete":["28","    # Base storage stucture for cells in a QuadTree object"]}],"examples\/gaussian_process\/plot_gpc_isoprobability.py":[{"add":["48","# Instantiate and fit Gaussian Process Model"],"delete":["48","# Instanciate and fit Gaussian Process Model"]}],"examples\/svm\/plot_rbf_parameters.py":[{"add":["33","them apart in the blink of an eye."],"delete":["33","them appart in the blink of an eye."]}],"doc\/tutorial\/machine_learning_map\/ML_MAPS_README.txt":[{"add":["21","you can move objects around, etc. as you need."],"delete":["21","you can move objects around, ect. as you need."]}],"sklearn\/multiclass.py":[{"add":["199","        # outperform or match a dense label binarizer in all cases and has also"],"delete":["199","        # outpreform or match a dense label binarizer in all cases and has also"]}],"sklearn\/cluster\/_k_means_elkan.pyx":[{"add":["152","    # Get the initial set of upper bounds and lower bounds for each sample."],"delete":["152","    # Get the inital set of upper bounds and lower bounds for each sample."]}],"doc\/themes\/scikit-learn\/static\/nature.css_t":[{"add":["788","\/*-----------------------The Code Sprint Sponsor Banner---------------------*\/"],"delete":["788","\/*-----------------------The Code Sprint Sponser Banner---------------------*\/"]}],"sklearn\/datasets\/tests\/test_base.py":[{"add":["250","    # is a surprising behaviour because reading bunch.key uses"],"delete":["250","    # is a suprising behaviour because reading bunch.key uses"]}],"sklearn\/linear_model\/sgd_fast.pyx":[{"add":["388","        (4) Passive Aggressive-I, eta = min(alpha, loss\/norm(x))","389","        (5) Passive Aggressive-II, eta = 1.0 \/ (norm(x) + 0.5*alpha)"],"delete":["388","        (4) Passive Agressive-I, eta = min(alpha, loss\/norm(x))","389","        (5) Passive Agressive-II, eta = 1.0 \/ (norm(x) + 0.5*alpha)"]}]}},"f84581bffd3ac48540de3af5850470b5869267de":{"changes":{"sklearn\/metrics\/regression.py":"MODIFY","sklearn\/metrics\/tests\/test_regression.py":"MODIFY"},"diff":{"sklearn\/metrics\/regression.py":[{"add":["312","    if (y_true < 0).any() or (y_pred < 0).any():"],"delete":["312","    if not (y_true >= 0).all() and not (y_pred >= 0).all():"]}],"sklearn\/metrics\/tests\/test_regression.py":[{"add":["66","    assert_raises_regex(ValueError, \"Mean Squared Logarithmic Error cannot be \"","67","                        \"used when targets contain negative values.\",","68","                        mean_squared_log_error, [1., 2., 3.], [1., -2., 3.])","69","    assert_raises_regex(ValueError, \"Mean Squared Logarithmic Error cannot be \"","70","                        \"used when targets contain negative values.\",","71","                        mean_squared_log_error, [1., -2., 3.], [1., 2., 3.])","72",""],"delete":[]}]}},"cd3e0dcf020159d855911a28f77c67462d4241c2":{"changes":{"sklearn\/tests\/test_impute.py":"MODIFY","doc\/modules\/impute.rst":"MODIFY","sklearn\/impute.py":"MODIFY"},"diff":{"sklearn\/tests\/test_impute.py":[{"add":["100","@pytest.mark.parametrize(\"strategy\", [\"mean\", \"median\",","101","                                      \"most_frequent\", \"constant\"])","102","def test_imputation_error_sparse_0(strategy):","103","    # check that error are raised when missing_values = 0 and input is sparse","104","    X = np.ones((3, 5))","105","    X[0] = 0","106","    X = sparse.csc_matrix(X)","107","","108","    imputer = SimpleImputer(strategy=strategy, missing_values=0)","109","    with pytest.raises(ValueError, match=\"Provide a dense array\"):","110","        imputer.fit(X)","111","","112","    imputer.fit(X.toarray())","113","    with pytest.raises(ValueError, match=\"Provide a dense array\"):","114","        imputer.transform(X)","115","","116","","144","              lambda z, v, p: safe_median(np.hstack((z, v))))]","445","    X = sparse_random_matrix(100, 100, density=0.10)","446","    missing_values = X.data[0]","447","","448","    pipeline = Pipeline([('imputer',","449","                          SimpleImputer(missing_values=missing_values)),","450","                         ('tree',","451","                          tree.DecisionTreeRegressor(random_state=0))])"],"delete":["126","             (\"mean\", 0, lambda z, v, p: np.mean(v)),","128","              lambda z, v, p: safe_median(np.hstack((z, v)))),","129","             (\"median\", 0, lambda z, v, p: np.median(v))]","430","    pipeline = Pipeline([('imputer', SimpleImputer(missing_values=0)),","431","                         ('tree', tree.DecisionTreeRegressor(random_state=0))])","437","    X = sparse_random_matrix(100, 100, density=0.10)"]}],"doc\/modules\/impute.rst":[{"add":["58","    >>> X = sp.csc_matrix([[1, 2], [0, -1], [8, 4]])","59","    >>> imp = SimpleImputer(missing_values=-1, strategy='mean')","61","    SimpleImputer(copy=True, fill_value=None, missing_values=-1, strategy='mean', verbose=0)","62","    >>> X_test = sp.csc_matrix([[-1, 2], [6, -1], [7, 6]])","63","    >>> print(imp.transform(X_test).toarray())      # doctest: +NORMALIZE_WHITESPACE","64","    [[3. 2.]","65","     [6. 3.]","66","     [7. 6.]]","68","Note that this format is not meant to be used to implicitly store missing values","69","in the matrix because it would densify it at transform time. Missing values encoded","70","by 0 must be used with dense input."],"delete":["58","    >>> X = sp.csc_matrix([[1, 2], [0, 3], [7, 6]])","59","    >>> imp = SimpleImputer(missing_values=0, strategy='mean')","61","    SimpleImputer(copy=True, fill_value=None, missing_values=0, strategy='mean', verbose=0)","62","    >>> X_test = sp.csc_matrix([[0, 2], [6, 0], [7, 6]])","63","    >>> print(imp.transform(X_test))      # doctest: +NORMALIZE_WHITESPACE  +ELLIPSIS","64","    [[4.          2.        ]","65","     [6.          3.666...]","66","     [7.          6.        ]]","68","Note that, here, missing values are encoded by 0 and are thus implicitly stored","69","in the matrix. This format is thus suitable when there are many more missing","70","values than observed values."]}],"sklearn\/impute.py":[{"add":["238","            # missing_values = 0 not allowed with sparse data as it would","239","            # force densification","240","            if self.missing_values == 0:","241","                raise ValueError(\"Imputation not possible when missing_values \"","242","                                 \"== 0 and input is sparse. Provide a dense \"","243","                                 \"array instead.\")","244","            else:","245","                self.statistics_ = self._sparse_fit(X,","246","                                                    self.strategy,","247","                                                    self.missing_values,","248","                                                    fill_value)","259","        mask_data = _get_mask(X.data, missing_values)","260","        n_implicit_zeros = X.shape[0] - np.diff(X.indptr)","261","","262","        statistics = np.empty(X.shape[1])","263","","264","        if strategy == \"constant\":","265","            # for constant strategy, self.statistcs_ is used to store","266","            # fill_value in each column","267","            statistics.fill(fill_value)","268","","270","            for i in range(X.shape[1]):","271","                column = X.data[X.indptr[i]:X.indptr[i+1]]","272","                mask_column = mask_data[X.indptr[i]:X.indptr[i+1]]","273","                column = column[~mask_column]","275","                # combine explicit and implicit zeros","276","                mask_zeros = _get_mask(column, 0)","277","                column = column[~mask_zeros]","278","                n_explicit_zeros = mask_zeros.sum()","279","                n_zeros = n_implicit_zeros[i] + n_explicit_zeros","281","                if strategy == \"mean\":","282","                    s = column.size + n_zeros","283","                    statistics[i] = np.nan if s == 0 else column.sum() \/ s","285","                elif strategy == \"median\":","286","                    statistics[i] = _get_median(column,","287","                                                n_zeros)","289","                elif strategy == \"most_frequent\":","290","                    statistics[i] = _most_frequent(column,","291","                                                   0,","292","                                                   n_zeros)","293","        return statistics","343","            # for constant strategy, self.statistcs_ is used to store","344","            # fill_value in each column","383","        if sparse.issparse(X):","384","            if self.missing_values == 0:","385","                raise ValueError(\"Imputation not possible when missing_values \"","386","                                 \"== 0 and input is sparse. Provide a dense \"","387","                                 \"array instead.\")","388","            else:","389","                mask = _get_mask(X.data, self.missing_values)","390","                indexes = np.repeat(np.arange(len(X.indptr) - 1, dtype=np.int),","391","                                    np.diff(X.indptr))[mask]","393","                X.data[mask] = valid_statistics[indexes].astype(X.dtype,","394","                                                                copy=False)"],"delete":["144","        - If X is sparse and `missing_values=0`;","239","            self.statistics_ = self._sparse_fit(X,","240","                                                self.strategy,","241","                                                self.missing_values,","242","                                                fill_value)","253","        # Count the zeros","254","        if missing_values == 0:","255","            n_zeros_axis = np.zeros(X.shape[1], dtype=int)","257","            n_zeros_axis = X.shape[0] - np.diff(X.indptr)","259","        # Mean","260","        if strategy == \"mean\":","261","            if missing_values != 0:","262","                n_non_missing = n_zeros_axis","264","                # Mask the missing elements","265","                mask_missing_values = _get_mask(X.data, missing_values)","266","                mask_valids = np.logical_not(mask_missing_values)","268","                # Sum only the valid elements","269","                new_data = X.data.copy()","270","                new_data[mask_missing_values] = 0","271","                X = sparse.csc_matrix((new_data, X.indices, X.indptr),","272","                                      copy=False)","273","                sums = X.sum(axis=0)","275","                # Count the elements != 0","276","                mask_non_zeros = sparse.csc_matrix(","277","                    (mask_valids.astype(np.float64),","278","                     X.indices,","279","                     X.indptr), copy=False)","280","                s = mask_non_zeros.sum(axis=0)","281","                n_non_missing = np.add(n_non_missing, s)","282","","283","            else:","284","                sums = X.sum(axis=0)","285","                n_non_missing = np.diff(X.indptr)","286","","287","            # Ignore the error, columns with a np.nan statistics_","288","            # are not an error at this point. These columns will","289","            # be removed in transform","290","            with np.errstate(all=\"ignore\"):","291","                return np.ravel(sums) \/ np.ravel(n_non_missing)","292","","293","        # Median + Most frequent + Constant","294","        else:","295","            # Remove the missing values, for each column","296","            columns_all = np.hsplit(X.data, X.indptr[1:-1])","297","            mask_missing_values = _get_mask(X.data, missing_values)","298","            mask_valids = np.hsplit(np.logical_not(mask_missing_values),","299","                                    X.indptr[1:-1])","300","","301","            # astype necessary for bug in numpy.hsplit before v1.9","302","            columns = [col[mask.astype(bool, copy=False)]","303","                       for col, mask in zip(columns_all, mask_valids)]","304","","305","            # Median","306","            if strategy == \"median\":","307","                median = np.empty(len(columns))","308","                for i, column in enumerate(columns):","309","                    median[i] = _get_median(column, n_zeros_axis[i])","310","","311","                return median","312","","313","            # Most frequent","314","            elif strategy == \"most_frequent\":","315","                most_frequent = np.empty(len(columns))","316","","317","                for i, column in enumerate(columns):","318","                    most_frequent[i] = _most_frequent(column,","319","                                                      0,","320","                                                      n_zeros_axis[i])","321","","322","                return most_frequent","323","","324","            # Constant","325","            elif strategy == \"constant\":","326","                return np.full(X.shape[1], fill_value)","414","        if sparse.issparse(X) and self.missing_values != 0:","415","            mask = _get_mask(X.data, self.missing_values)","416","            indexes = np.repeat(np.arange(len(X.indptr) - 1, dtype=np.int),","417","                                np.diff(X.indptr))[mask]","419","            X.data[mask] = valid_statistics[indexes].astype(X.dtype,","420","                                                            copy=False)","422","            if sparse.issparse(X):","423","                X = X.toarray()","424",""]}]}},"d9a0774afc5d30a8e98a25e1e8c3a0c428e88dfa":{"changes":{"sklearn\/gaussian_process\/kernels.py":"MODIFY","sklearn\/utils\/testing.py":"MODIFY","sklearn\/gaussian_process\/tests\/test_kernels.py":"MODIFY","examples\/model_selection\/plot_precision_recall.py":"MODIFY"},"diff":{"sklearn\/gaussian_process\/kernels.py":[{"add":["32","from ..utils.fixes import signature"],"delete":["32","from sklearn.externals.funcsigs import signature"]}],"sklearn\/utils\/testing.py":[{"add":["48","from sklearn.utils.fixes import signature"],"delete":["48","from sklearn.externals.funcsigs import signature"]}],"sklearn\/gaussian_process\/tests\/test_kernels.py":[{"add":["7","from sklearn.utils.fixes import signature"],"delete":["5","from sklearn.externals.funcsigs import signature","6",""]}],"examples\/model_selection\/plot_precision_recall.py":[{"add":["139","from sklearn.utils.fixes import signature"],"delete":["139","from sklearn.externals.funcsigs import signature"]}]}},"622f912095308733ddfe572a619b1574b9da335e":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/metrics\/cluster\/tests\/test_supervised.py":"MODIFY","sklearn\/metrics\/cluster\/supervised.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["423","- Fixed a bug in :func:`metrics.cluster.fowlkes_mallows_score` to avoid integer","424","  overflow. Casted return value of `contingency_matrix` to `int64` and computed","425","  product of square roots rather than square root of product.","426","  :issue:`9515` by :user:`Alan Liddell <aliddell>` and","427","  :user:`Manh Dao <manhdao>`.","428",""],"delete":[]}],"sklearn\/metrics\/cluster\/tests\/test_supervised.py":[{"add":["175","def test_int_overflow_mutual_info_fowlkes_mallows_score():","176","    # Test overflow in mutual_info_classif and fowlkes_mallows_score","183","    assert_all_finite(mutual_info_score(x, y))","184","    assert_all_finite(fowlkes_mallows_score(x, y))"],"delete":["175","def test_int_overflow_mutual_info_score():","176","    # Test overflow in mutual_info_classif","183","    assert_all_finite(mutual_info_score(x.ravel(), y.ravel()))"]}],"sklearn\/metrics\/cluster\/supervised.py":[{"add":["854","    c = contingency_matrix(labels_true, labels_pred,","855","                           sparse=True).astype(np.int64)","859","    return np.sqrt(tk \/ pk) * np.sqrt(tk \/ qk) if tk != 0. else 0."],"delete":["854","    c = contingency_matrix(labels_true, labels_pred, sparse=True)","858","    return tk \/ np.sqrt(pk * qk) if tk != 0. else 0."]}]}},"fe6e562c777179f60e43cca40d86203bf2276178":{"changes":{"sklearn\/neural_network\/multilayer_perceptron.py":"MODIFY"},"diff":{"sklearn\/neural_network\/multilayer_perceptron.py":[{"add":["930","            if len(np.setdiff1d(classes, self.classes_, assume_unique=True)):"],"delete":["930","            if np.setdiff1d(classes, self.classes_, assume_unique=True):"]}]}},"d045c1abc8f424b987f39ffc289e6cfa2e6d4e79":{"changes":{"doc\/modules\/multiclass.rst":"MODIFY","doc\/whats_new\/v0.21.rst":"MODIFY","doc\/modules\/svm.rst":"MODIFY","sklearn\/utils\/multiclass.py":"MODIFY","sklearn\/svm\/base.py":"MODIFY","sklearn\/utils\/estimator_checks.py":"MODIFY","sklearn\/utils\/tests\/test_multiclass.py":"MODIFY"},"diff":{"doc\/modules\/multiclass.rst":[{"add":["229","dataset is used ``n_classes`` times. The decision function is the result","230","of a monotonic transformation of the one-versus-one classification."],"delete":["229","dataset is used ``n_classes`` times."]}],"doc\/whats_new\/v0.21.rst":[{"add":["26","- :func:`svm.SVC.decision_function` and","27","  :func:`multiclass.OneVsOneClassifier.decision_function`. |Fix|","28","","380",":mod:`sklearn.svm`","381","..................","382","","383","- |Fix| Fixed an issue in :func:`svm.SVC.decision_function`","384","  when ``decision_function_shape='ovr'``. The decision_function value of a given","385","  sample was different depending on whether the decision_function was evaluated","386","  on the sample alone or on a batch containing this same sample due to the scaling","387","  used in decision_function. :pr:`10440` by :user:`Jonathan Ohayon <Johayon>`.","388","","389",":mod:`sklearn.multiclass`","390",".........................","391","","392","- |Fix| Fixed an issue in :func:`multiclass.OneVsOneClassifier.decision_function`","393","  where the decision_function value of a given sample was different depending on","394","  whether the decision_function was evaluated on the sample alone or on a batch","395","  containing this same sample due to the scaling used in decision_function.","396","  :pr:`10440` by :user:`Jonathan Ohayon <Johayon>`."],"delete":[]}],"doc\/modules\/svm.rst":[{"add":["115","``decision_function_shape`` option allows to monotically transform the results of the","117","n_classes)``."],"delete":["115","``decision_function_shape`` option allows to aggregate the results of the","117","n_classes)``::"]}],"sklearn\/utils\/multiclass.py":[{"add":["433","    # Monotonically transform the sum_of_confidences to (-1\/3, 1\/3)","434","    # and add it with votes. The monotonic transformation  is","435","    # f: x -> x \/ (3 * (|x| + 1)), it uses 1\/3 instead of 1\/2","436","    # to ensure that we won't reach the limits and change vote order.","440","    transformed_confidences = (sum_of_confidences \/","441","                               (3 * (np.abs(sum_of_confidences) + 1)))","442","    return votes + transformed_confidences"],"delete":["433","    max_confidences = sum_of_confidences.max()","434","    min_confidences = sum_of_confidences.min()","435","","436","    if max_confidences == min_confidences:","437","        return votes","438","","439","    # Scale the sum_of_confidences to (-0.5, 0.5) and add it with votes.","443","    eps = np.finfo(sum_of_confidences.dtype).eps","444","    max_abs_confidence = max(abs(max_confidences), abs(min_confidences))","445","    scale = (0.5 - eps) \/ max_abs_confidence","446","    return votes + sum_of_confidences * scale"]}],"sklearn\/svm\/base.py":[{"add":["549","        If decision_function_shape='ovr', the decision function is a monotonic","550","        transformation of ovo decision function."],"delete":[]}],"sklearn\/utils\/estimator_checks.py":[{"add":["838","        if (name, method) in [('NuSVC', 'decision_function'),"],"delete":["838","        if (name, method) in [('SVC', 'decision_function'),","839","                              ('NuSVC', 'decision_function'),"]}],"sklearn\/utils\/tests\/test_multiclass.py":[{"add":["18","from sklearn.utils.testing import assert_allclose","26","from sklearn.utils.multiclass import _ovr_decision_function","382","","383","","384","def test_ovr_decision_function():","385","    # test properties for ovr decision function","386","","387","    predictions = np.array([[0, 1, 1],","388","                            [0, 1, 0],","389","                            [0, 1, 1],","390","                            [0, 1, 1]])","391","","392","    confidences = np.array([[-1e16, 0, -1e16],","393","                            [1., 2., -3.],","394","                            [-5., 2., 5.],","395","                            [-0.5, 0.2, 0.5]])","396","","397","    n_classes = 3","398","","399","    dec_values = _ovr_decision_function(predictions, confidences, n_classes)","400","","401","    # check that the decision values are within 0.5 range of the votes","402","    votes = np.array([[1, 0, 2],","403","                      [1, 1, 1],","404","                      [1, 0, 2],","405","                      [1, 0, 2]])","406","","407","    assert_allclose(votes, dec_values, atol=0.5)","408","","409","    # check that the prediction are what we expect","410","    # highest vote or highest confidence if there is a tie.","411","    # for the second sample we have a tie (should be won by 1)","412","    expected_prediction = np.array([2, 1, 2, 2])","413","    assert_array_equal(np.argmax(dec_values, axis=1), expected_prediction)","414","","415","    # third and fourth sample have the same vote but third sample","416","    # has higher confidence, this should reflect on the decision values","417","    assert (dec_values[2, 2] > dec_values[3, 2])","418","","419","    # assert subset invariance.","420","    dec_values_one = [_ovr_decision_function(np.array([predictions[i]]),","421","                                             np.array([confidences[i]]),","422","                                             n_classes)[0] for i in range(4)]","423","","424","    assert_allclose(dec_values, dec_values_one, atol=1e-6)"],"delete":[]}]}}}