{"3d1b786546a033acbafe2ed32d7a008b8e659e5f":{"changes":{"sklearn\/calibration.py":"MODIFY","sklearn\/feature_selection\/rfe.py":"MODIFY","sklearn\/linear_model\/logistic.py":"MODIFY","sklearn\/linear_model\/coordinate_descent.py":"MODIFY","sklearn\/linear_model\/omp.py":"MODIFY","sklearn\/linear_model\/least_angle.py":"MODIFY","sklearn\/linear_model\/ridge.py":"MODIFY"},"diff":{"sklearn\/calibration.py":[{"add":["267","    See also","268","    --------","269","    CalibratedClassifierCV","270",""],"delete":[]}],"sklearn\/feature_selection\/rfe.py":[{"add":["103","    See also","104","    --------","105","    RFECV : Recursive feature elimination with built-in cross-validated","106","        selection of the best number of features","107","","372","    See also","373","    --------","374","    RFE : Recursive feature elimination","375",""],"delete":[]}],"sklearn\/linear_model\/logistic.py":[{"add":["1122","    LogisticRegressionCV : Logistic regression with built-in cross validation"],"delete":[]}],"sklearn\/linear_model\/coordinate_descent.py":[{"add":["642","    ElasticNetCV : Elastic net model with best model selection by","643","        cross-validation.","1692","    MultiTaskElasticNet : Multi-task L1\/L2 ElasticNet with built-in","1693","        cross-validation.","1694","    ElasticNet","1695","    MultiTaskLasso","1880","    MultiTaskLasso : Multi-task L1\/L2 Lasso with built-in cross-validation","1881","    Lasso","1882","    MultiTaskElasticNet"],"delete":["1690","    ElasticNet, MultiTaskLasso","1875","    Lasso, MultiTaskElasticNet"]}],"sklearn\/linear_model\/omp.py":[{"add":["600","    OrthogonalMatchingPursuitCV"],"delete":["600",""]}],"sklearn\/linear_model\/least_angle.py":[{"add":["826","    LassoLarsIC"],"delete":[]}],"sklearn\/linear_model\/ridge.py":[{"add":["626","    RidgeClassifier : Ridge classifier","627","    RidgeCV : Ridge regression with built-in cross validation","628","    :class:`sklearn.kernel_ridge.KernelRidge` : Kernel ridge regression","629","        combines ridge regression with the kernel trick","775","    Ridge : Ridge regression","776","    RidgeClassifierCV :  Ridge classifier with built-in cross validation","1239","    Ridge : Ridge regression","1240","    RidgeClassifier : Ridge classifier","1241","    RidgeClassifierCV : Ridge classifier with built-in cross validation","1324","    Ridge : Ridge regression","1325","    RidgeClassifier : Ridge classifier","1326","    RidgeCV : Ridge regression with built-in cross validation"],"delete":["626","    RidgeClassifier, RidgeCV, :class:`sklearn.kernel_ridge.KernelRidge`","772","    Ridge, RidgeClassifierCV","1235","    Ridge: Ridge regression","1236","    RidgeClassifier: Ridge classifier","1237","    RidgeClassifierCV: Ridge classifier with built-in cross validation","1320","    Ridge: Ridge regression","1321","    RidgeClassifier: Ridge classifier","1322","    RidgeCV: Ridge regression with built-in cross validation"]}]}},"6eb19831d1725027afa1f0d941af97aab50db9a1":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","examples\/decomposition\/plot_faces_decomposition.py":"MODIFY","sklearn\/decomposition\/tests\/test_sparse_pca.py":"MODIFY","sklearn\/decomposition\/sparse_pca.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["79","- :class:`decomposition.SparsePCA` (bug fix)","195","- :class:`decomposition.SparsePCA` now exposes ``normalize_components``. When","196","  set to True, the train and test data are centered with the train mean ","197","  repsectively during the fit phase and the transform phase. This fixes the","198","  behavior of SparsePCA. When set to False, which is the default, the previous","199","  abnormal behaviour still holds. The False value is for backward","200","  compatibility and should not be used.","201","  :issue:`11585` by :user:`Ivan Panico <FollowKenny>`.","202",""],"delete":[]}],"examples\/decomposition\/plot_faces_decomposition.py":[{"add":["83","                                      random_state=rng,","84","                                      normalize_components=True),"],"delete":["83","                                      random_state=rng),"]}],"sklearn\/decomposition\/tests\/test_sparse_pca.py":[{"add":["4","import pytest","10","from sklearn.utils.testing import assert_allclose","17","from sklearn.decomposition import SparsePCA, MiniBatchSparsePCA, PCA","46","@pytest.mark.filterwarnings(\"ignore:normalize_components\")","47","@pytest.mark.parametrize(\"norm_comp\", [False, True])","48","def test_correct_shapes(norm_comp):","51","    spca = SparsePCA(n_components=8, random_state=rng,","52","                     normalize_components=norm_comp)","57","    spca = SparsePCA(n_components=13, random_state=rng,","58","                     normalize_components=norm_comp)","64","@pytest.mark.filterwarnings(\"ignore:normalize_components\")","65","@pytest.mark.parametrize(\"norm_comp\", [False, True])","66","def test_fit_transform(norm_comp):","71","                          random_state=0, normalize_components=norm_comp)","76","                           alpha=alpha, normalize_components=norm_comp)","88","@pytest.mark.filterwarnings(\"ignore:normalize_components\")","89","@pytest.mark.parametrize(\"norm_comp\", [False, True])","91","def test_fit_transform_parallel(norm_comp):","96","                          random_state=0, normalize_components=norm_comp)","101","                     random_state=0, normalize_components=norm_comp).fit(Y)","107","@pytest.mark.filterwarnings(\"ignore:normalize_components\")","108","@pytest.mark.parametrize(\"norm_comp\", [False, True])","109","def test_transform_nan(norm_comp):","115","    estimator = SparsePCA(n_components=8, normalize_components=norm_comp)","119","@pytest.mark.filterwarnings(\"ignore:normalize_components\")","120","@pytest.mark.parametrize(\"norm_comp\", [False, True])","121","def test_fit_transform_tall(norm_comp):","125","                          random_state=rng, normalize_components=norm_comp)","127","    spca_lasso = SparsePCA(n_components=3, method='cd',","128","                           random_state=rng, normalize_components=norm_comp)","133","@pytest.mark.filterwarnings(\"ignore:normalize_components\")","134","@pytest.mark.parametrize(\"norm_comp\", [False, True])","135","def test_initialization(norm_comp):","140","                      random_state=rng, normalize_components=norm_comp)","142","    if norm_comp:","143","        assert_allclose(model.components_,","144","                        V_init \/ np.linalg.norm(V_init, axis=1)[:, None])","145","    else:","146","        assert_allclose(model.components_, V_init)","149","@pytest.mark.filterwarnings(\"ignore:normalize_components\")","150","@pytest.mark.parametrize(\"norm_comp\", [False, True])","151","def test_mini_batch_correct_shapes(norm_comp):","154","    pca = MiniBatchSparsePCA(n_components=8, random_state=rng,","155","                             normalize_components=norm_comp)","160","    pca = MiniBatchSparsePCA(n_components=13, random_state=rng,","161","                             normalize_components=norm_comp)","167","@pytest.mark.filterwarnings(\"ignore:normalize_components\")","168","@pytest.mark.parametrize(\"norm_comp\", [False, True])","169","def test_mini_batch_fit_transform(norm_comp):","175","                                   alpha=alpha,","176","                                   normalize_components=norm_comp).fit(Y)","184","            spca = MiniBatchSparsePCA(n_components=3, n_jobs=2, alpha=alpha,","185","                                      random_state=0,","186","                                      normalize_components=norm_comp)","187","            U2 = spca.fit(Y).transform(Y)","191","        spca = MiniBatchSparsePCA(n_components=3, n_jobs=2, alpha=alpha,","192","                                  random_state=0,","193","                                  normalize_components=norm_comp)","194","        U2 = spca.fit(Y).transform(Y)","199","                                    random_state=0,","200","                                    normalize_components=norm_comp).fit(Y)","202","","203","","204","def test_scaling_fit_transform():","205","    alpha = 1","206","    rng = np.random.RandomState(0)","207","    Y, _, _ = generate_toy_data(3, 1000, (8, 8), random_state=rng)","208","    spca_lars = SparsePCA(n_components=3, method='lars', alpha=alpha,","209","                          random_state=rng, normalize_components=True)","210","    results_train = spca_lars.fit_transform(Y)","211","    results_test = spca_lars.transform(Y[:10])","212","    assert_allclose(results_train[0], results_test[0])","213","","214","","215","def test_pca_vs_spca():","216","    rng = np.random.RandomState(0)","217","    Y, _, _ = generate_toy_data(3, 1000, (8, 8), random_state=rng)","218","    Z, _, _ = generate_toy_data(3, 10, (8, 8), random_state=rng)","219","    spca = SparsePCA(alpha=0, ridge_alpha=0, n_components=2,","220","                     normalize_components=True)","221","    pca = PCA(n_components=2)","222","    pca.fit(Y)","223","    spca.fit(Y)","224","    results_test_pca = pca.transform(Z)","225","    results_test_spca = spca.transform(Z)","226","    assert_allclose(np.abs(spca.components_.dot(pca.components_.T)),","227","                    np.eye(2), atol=1e-5)","228","    results_test_pca *= np.sign(results_test_pca[0, :])","229","    results_test_spca *= np.sign(results_test_spca[0, :])","230","    assert_allclose(results_test_pca, results_test_spca)","231","","232","","233","@pytest.mark.parametrize(\"spca\", [SparsePCA, MiniBatchSparsePCA])","234","def test_spca_deprecation_warning(spca):","235","    rng = np.random.RandomState(0)","236","    Y, _, _ = generate_toy_data(3, 10, (8, 8), random_state=rng)","237","    warn_message = \"normalize_components\"","238","    assert_warns_message(DeprecationWarning, warn_message,","239","                         spca(normalize_components=False).fit, Y)"],"delete":["9","from sklearn.utils.testing import assert_array_equal","16","from sklearn.decomposition import SparsePCA, MiniBatchSparsePCA","45","def test_correct_shapes():","48","    spca = SparsePCA(n_components=8, random_state=rng)","53","    spca = SparsePCA(n_components=13, random_state=rng)","59","def test_fit_transform():","64","                          random_state=0)","69","                           alpha=alpha)","82","def test_fit_transform_parallel():","87","                          random_state=0)","92","                     random_state=0).fit(Y)","98","def test_transform_nan():","104","    estimator = SparsePCA(n_components=8)","108","def test_fit_transform_tall():","112","                          random_state=rng)","114","    spca_lasso = SparsePCA(n_components=3, method='cd', random_state=rng)","119","def test_initialization():","124","                      random_state=rng)","126","    assert_array_equal(model.components_, V_init)","129","def test_mini_batch_correct_shapes():","132","    pca = MiniBatchSparsePCA(n_components=8, random_state=rng)","137","    pca = MiniBatchSparsePCA(n_components=13, random_state=rng)","143","def test_mini_batch_fit_transform():","149","                                   alpha=alpha).fit(Y)","157","            U2 = MiniBatchSparsePCA(n_components=3, n_jobs=2, alpha=alpha,","158","                                    random_state=0).fit(Y).transform(Y)","162","        U2 = MiniBatchSparsePCA(n_components=3, n_jobs=2, alpha=alpha,","163","                                random_state=0).fit(Y).transform(Y)","168","                                    random_state=0).fit(Y)"]}],"sklearn\/decomposition\/sparse_pca.py":[{"add":["68","    normalize_components : boolean, optional (default=False)","69","        - if False, use a version of Sparse PCA without components","70","          normalization and without data centering. This is likely a bug and","71","          even though it's the default for backward compatibility,","72","          this should not be used.","73","        - if True, use a version of Sparse PCA with components normalization","74","          and data centering.","75","","76","        .. versionadded:: 0.20","77","","78","        .. deprecated:: 0.22","79","           ``normalize_components`` was added and set to ``False`` for","80","           backward compatibility. It would be set to ``True`` from 0.22","81","           onwards.","82","","94","    mean_ : array, shape (n_features,)","95","        Per-feature empirical mean, estimated from the training set.","96","        Equal to ``X.mean(axis=0)``.","97","","106","                 V_init=None, verbose=False, random_state=None,","107","                 normalize_components=False):","119","        self.normalize_components = normalize_components","139","","140","        if self.normalize_components:","141","            self.mean_ = X.mean(axis=0)","142","            X = X - self.mean_","143","        else:","144","            warnings.warn(\"normalize_components=False is a \"","145","                          \"backward-compatible setting that implements a \"","146","                          \"non-standard definition of sparse PCA. This \"","147","                          \"compatibility mode will be removed in 0.22.\",","148","                          DeprecationWarning)","149","","168","","169","        if self.normalize_components:","170","            components_norm = \\","171","                    np.linalg.norm(self.components_, axis=1)[:, np.newaxis]","172","            components_norm[components_norm == 0] = 1","173","            self.components_ \/= components_norm","174","","219","","220","        if self.normalize_components:","221","            X = X - self.mean_","222","","225","","226","        if not self.normalize_components:","227","            s = np.sqrt((U ** 2).sum(axis=0))","228","            s[s == 0] = 1","229","            U \/= s","230","","287","    normalize_components : boolean, optional (default=False)","288","        - if False, use a version of Sparse PCA without components","289","          normalization and without data centering. This is likely a bug and","290","          even though it's the default for backward compatibility,","291","          this should not be used.","292","        - if True, use a version of Sparse PCA with components normalization","293","          and data centering.","294","","295","        .. versionadded:: 0.20","296","","297","        .. deprecated:: 0.22","298","           ``normalize_components`` was added and set to ``False`` for","299","           backward compatibility. It would be set to ``True`` from 0.22","300","           onwards.","301","","310","    mean_ : array, shape (n_features,)","311","        Per-feature empirical mean, estimated from the training set.","312","        Equal to ``X.mean(axis=0)``.","313","","322","                 shuffle=True, n_jobs=1, method='lars', random_state=None,","323","                 normalize_components=False):","327","            random_state=random_state,","328","            normalize_components=normalize_components)","352","","353","        if self.normalize_components:","354","            self.mean_ = X.mean(axis=0)","355","            X = X - self.mean_","356","        else:","357","            warnings.warn(\"normalize_components=False is a \"","358","                          \"backward-compatible setting that implements a \"","359","                          \"non-standard definition of sparse PCA. This \"","360","                          \"compatibility mode will be removed in 0.22.\",","361","                          DeprecationWarning)","362","","378","","379","        if self.normalize_components:","380","            components_norm = \\","381","                    np.linalg.norm(self.components_, axis=1)[:, np.newaxis]","382","            components_norm[components_norm == 0] = 1","383","            self.components_ \/= components_norm","384",""],"delete":["87","                 V_init=None, verbose=False, random_state=None):","182","        s = np.sqrt((U ** 2).sum(axis=0))","183","        s[s == 0] = 1","184","        U \/= s","257","                 shuffle=True, n_jobs=1, method='lars', random_state=None):","261","            random_state=random_state)"]}]}},"7108d17be443cf20588d59ebbd8f8e9da91bd2d0":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/utils\/validation.py":"MODIFY","sklearn\/utils\/tests\/test_validation.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["337","Utils","338","","339","- :func:`utils.validation.check_array` yield a ``FutureWarning`` indicating","340","  that arrays of bytes\/strings will be interpreted as decimal numbers","341","  beginning in version 0.22. :issue:`10229` by :user:`Ryan Lee <rtlee9>`","342",""],"delete":[]}],"sklearn\/utils\/validation.py":[{"add":["518","        # in the future np.flexible dtypes will be handled like object dtypes","519","        if dtype_numeric and np.issubdtype(array.dtype, np.flexible):","520","            warnings.warn(","521","                \"Beginning in version 0.22, arrays of strings will be \"","522","                \"interpreted as decimal numbers if parameter 'dtype' is \"","523","                \"'numeric'. It is recommended that you convert the array to \"","524","                \"type np.float64 before passing it to check_array.\",","525","                FutureWarning)","526",""],"delete":[]}],"sklearn\/utils\/tests\/test_validation.py":[{"add":["287","    # deprecation warning if string-like array with dtype=\"numeric\"","288","    X_str = [['a', 'b'], ['c', 'd']]","289","    assert_warns_message(","290","        FutureWarning,","291","        \"arrays of strings will be interpreted as decimal numbers if \"","292","        \"parameter 'dtype' is 'numeric'. It is recommended that you convert \"","293","        \"the array to type np.float64 before passing it to check_array.\",","294","        check_array, X_str, \"numeric\")","295","    assert_warns_message(","296","        FutureWarning,","297","        \"arrays of strings will be interpreted as decimal numbers if \"","298","        \"parameter 'dtype' is 'numeric'. It is recommended that you convert \"","299","        \"the array to type np.float64 before passing it to check_array.\",","300","        check_array, np.array(X_str, dtype='U'), \"numeric\")","301","    assert_warns_message(","302","        FutureWarning,","303","        \"arrays of strings will be interpreted as decimal numbers if \"","304","        \"parameter 'dtype' is 'numeric'. It is recommended that you convert \"","305","        \"the array to type np.float64 before passing it to check_array.\",","306","        check_array, np.array(X_str, dtype='S'), \"numeric\")","307","","308","    # deprecation warning if byte-like array with dtype=\"numeric\"","309","    X_bytes = [[b'a', b'b'], [b'c', b'd']]","310","    assert_warns_message(","311","        FutureWarning,","312","        \"arrays of strings will be interpreted as decimal numbers if \"","313","        \"parameter 'dtype' is 'numeric'. It is recommended that you convert \"","314","        \"the array to type np.float64 before passing it to check_array.\",","315","        check_array, X_bytes, \"numeric\")","316","    assert_warns_message(","317","        FutureWarning,","318","        \"arrays of strings will be interpreted as decimal numbers if \"","319","        \"parameter 'dtype' is 'numeric'. It is recommended that you convert \"","320","        \"the array to type np.float64 before passing it to check_array.\",","321","        check_array, np.array(X_bytes, dtype='V1'), \"numeric\")","322",""],"delete":[]}]}},"71402efc2ff2205ed15811c5b29787a413a4b588":{"changes":{"sklearn\/linear_model\/stochastic_gradient.py":"MODIFY"},"diff":{"sklearn\/linear_model\/stochastic_gradient.py":[{"add":["1258","        - 'optimal': eta = 1.0 \/ (alpha * (t + t0))","1259","        - 'invscaling': eta = eta0 \/ pow(t, power_t) [default]"],"delete":["1258","        - 'optimal': eta = 1.0 \/ (alpha * (t + t0)) [default]","1259","        - 'invscaling': eta = eta0 \/ pow(t, power_t)"]}]}},"60b0cf8a2b59d17835351573d20d39888b191b91":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/metrics\/classification.py":"MODIFY","sklearn\/metrics\/tests\/test_classification.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["258","- Fixed a bug in :func:`metrics.precision_precision_recall_fscore_support`","259","  when truncated `range(n_labels)` is passed as value for `labels`.","260","  :issue:`10377` by :user:`Gaurav Dhingra <gxyd>`.","261",""],"delete":[]}],"sklearn\/metrics\/classification.py":[{"add":["1074","        if n_labels is not None:"],"delete":[]}],"sklearn\/metrics\/tests\/test_classification.py":[{"add":["199","    # tests non-regression on issue #10307","200","    y_true = np.array([[0, 1, 1], [1, 0, 0]])","201","    y_pred = np.array([[1, 1, 1], [1, 0, 1]])","202","    p, r, f, _ = precision_recall_fscore_support(y_true, y_pred,","203","                                                 average='samples',","204","                                                 labels=[0, 1])","205","    assert_almost_equal(np.array([p, r, f]), np.array([3 \/ 4, 1, 5 \/ 6]))","206",""],"delete":[]}]}},"60cf1d62d2c0c1bf8ad321c7c098f6494abf4597":{"changes":{"sklearn\/datasets\/samples_generator.py":"MODIFY","sklearn\/datasets\/tests\/test_samples_generator.py":"MODIFY"},"diff":{"sklearn\/datasets\/samples_generator.py":[{"add":["162","    # Use log2 to avoid overflow errors","163","    if n_informative < np.log2(n_classes * n_clusters_per_class):"],"delete":["162","    if 2 ** n_informative < n_classes * n_clusters_per_class:"]}],"sklearn\/datasets\/tests\/test_samples_generator.py":[{"add":["86","                                                         (10, [1\/3] * 3, 10),","87","                                                         (np.int(64), [1], 1)","131","                    assert_array_almost_equal(np.abs(centroid) \/ class_sep,","132","                                              np.ones(n_informative),","133","                                              decimal=5,","140","                                  np.abs(centroid) \/ class_sep,","141","                                  np.ones(n_informative),","142","                                  decimal=5,","143","                                  err_msg=\"Clusters should not be centered \""],"delete":["86","                                                         (10, [1\/3] * 3, 10)","130","                    assert_array_almost_equal(np.abs(centroid),","131","                                              [class_sep] * n_informative,","132","                                              decimal=0,","139","                                  np.abs(centroid),","140","                                  [class_sep] * n_informative,","141","                                  decimal=0,","142","                                  err_msg=\"Clusters should not be cenetered \""]}]}},"2eb254c296c9d3d555ae9f4a44ef54ab63dd8564":{"changes":{"sklearn\/impute.py":"MODIFY","sklearn\/utils\/estimator_checks.py":"MODIFY"},"diff":{"sklearn\/impute.py":[{"add":["44","    if is_scalar_nan(value_to_mask):"],"delete":["44","    if value_to_mask is np.nan:"]}],"sklearn\/utils\/estimator_checks.py":[{"add":["1168","    # include NaN values when the estimator should deal with them","1169","    if name in ALLOW_NAN:","1170","        # set randomly 10 elements to np.nan","1171","        rng = np.random.RandomState(42)","1172","        mask = rng.choice(X.size, 10, replace=False)","1173","        X.reshape(-1)[mask] = np.nan","1174","","1189","    result = dict()","1190","    for method in check_methods:","1191","        if hasattr(estimator, method):","1192","            result[method] = getattr(estimator, method)(X)","1193",""],"delete":["53","from sklearn.exceptions import ConvergenceWarning","1177","    result = dict()","1178","    for method in check_methods:","1179","        if hasattr(estimator, method):","1180","            result[method] = getattr(estimator, method)(X)","1181",""]}]}},"1966f0ba7f9773da1fd8face7205e3976053db02":{"changes":{"sklearn\/datasets\/kddcup99.py":"MODIFY"},"diff":{"sklearn\/datasets\/kddcup99.py":[{"add":["236","                          download_if_missing=True, percent10=True):"],"delete":["236","                          download_if_missing=True, random_state=None,","237","                          percent10=True):","251","    random_state : int, RandomState instance or None, optional (default=None)","252","        Random state for shuffling the dataset.","253","        If int, random_state is the seed used by the random number generator;","254","        If RandomState instance, random_state is the random number generator;","255","        If None, the random number generator is the RandomState instance used","256","        by `np.random`.","257",""]}]}},"74b69df981878cef73099eb296ee9c2d88a986b6":{"changes":{"sklearn\/svm\/libsvm.pyx":"MODIFY","sklearn\/svm\/base.py":"MODIFY","sklearn\/svm\/tests\/test_svm.py":"MODIFY"},"diff":{"sklearn\/svm\/libsvm.pyx":[{"add":["56","    int svm_type=0, kernel='rbf', int degree=3,","344","    int svm_type=0, kernel='rbf', int degree=3,","464","    int n_fold, svm_type=0, kernel='rbf', int degree=3,"],"delete":["56","    int svm_type=0, str kernel='rbf', int degree=3,","344","    int svm_type=0, str kernel='rbf', int degree=3,","464","    int n_fold, svm_type=0, str kernel='rbf', int degree=3,"]}],"sklearn\/svm\/base.py":[{"add":[],"delete":["230","        if six.PY2:","231","            # In python2 ensure kernel is ascii bytes to prevent a TypeError","232","            if isinstance(kernel, six.types.UnicodeType):","233","                kernel = str(kernel)","234","        if six.PY3:","235","            # In python3 ensure kernel is utf8 unicode to prevent a TypeError","236","            if isinstance(kernel, bytes):","237","                kernel = str(kernel, 'utf8')","238",""]}],"sklearn\/svm\/tests\/test_svm.py":[{"add":["507","    # Test that a unicode kernel name does not cause a TypeError","510","        clf = svm.SVC(kernel=u'linear', probability=True)","512","        clf.predict_proba(T)","513","        svm.libsvm.cross_validation(iris.data,","514","                                    iris.target.astype(np.float64), 5,","515","                                    kernel=u'linear',","516","                                    random_seed=0)","519","    clf = svm.SVC(kernel='linear', probability=True)","521","    clf.predict_proba(T)","522","    svm.libsvm.cross_validation(iris.data,","523","                                iris.target.astype(np.float64), 5,","524","                                kernel='linear',","525","                                random_seed=0)"],"delete":["507","    # Test that a unicode kernel name does not cause a TypeError on clf.fit","510","        clf = svm.SVC(kernel=unicode('linear'))","512","","513","        # Test ascii bytes (str is bytes in python2)","514","        clf = svm.SVC(kernel=str('linear'))","515","        clf.fit(X, Y)","516","    else:","517","        # Test unicode (str is unicode in python3)","518","        clf = svm.SVC(kernel=str('linear'))","519","        clf.fit(X, Y)","520","","521","        # Test ascii bytes (same as str on python2)","522","        clf = svm.SVC(kernel=bytes('linear', 'ascii'))","523","        clf.fit(X, Y)","526","    clf = svm.SVC(kernel='linear')"]}]}},"92c9095591c5885838987686a0dc7f8cff8bcf5e":{"changes":{"sklearn\/model_selection\/_validation.py":"MODIFY","sklearn\/exceptions.py":"MODIFY","sklearn\/model_selection\/tests\/test_validation.py":"MODIFY"},"diff":{"sklearn\/model_selection\/_validation.py":[{"add":["17","from traceback import format_exception_only","477","            warnings.warn(\"Estimator fit failed. The score on this train-test\"","479","                          \"Details: \\n%s\" %","480","                          (error_score, format_exception_only(type(e), e)[0]),","481","                          FitFailedWarning)"],"delete":["476","            warnings.warn(\"Classifier fit failed. The score on this train-test\"","478","                          \"Details: \\n%r\" % (error_score, e), FitFailedWarning)"]}],"sklearn\/exceptions.py":[{"add":["121","    FitFailedWarning('Estimator fit failed. The score on this train-test","122","    partition for these parameters will be set to 0.000000.","123","    Details: \\\\nValueError: Penalty term must be positive; got (C=-2)\\\\n',)"],"delete":["121","    FitFailedWarning(\"Classifier fit failed. The score on this train-test","122","    partition for these parameters will be set to 0.000000. Details:","123","    \\\\nValueError('Penalty term must be positive; got (C=-2)',)\",)"]}],"sklearn\/model_selection\/tests\/test_validation.py":[{"add":["11","from sklearn.exceptions import FitFailedWarning","12","","13","from sklearn.tests.test_grid_search import FailingClassifier","45","from sklearn.model_selection._validation import _fit_and_score","1427","","1428","","1429","def test_fit_and_score():","1430","    # Create a failing classifier to deliberately fail","1431","    failing_clf = FailingClassifier(FailingClassifier.FAILING_PARAMETER)","1432","    # dummy X data","1433","    X = np.arange(1, 10)","1434","    fit_and_score_args = [failing_clf, X, None, dict(), None, None, 0,","1435","                          None, None]","1436","    # passing error score to trigger the warning message","1437","    fit_and_score_kwargs = {'error_score': 0}","1438","    # check if the warning message type is as expected","1439","    assert_warns(FitFailedWarning, _fit_and_score, *fit_and_score_args,","1440","                 **fit_and_score_kwargs)","1441","    # since we're using FailingClassfier, our error will be the following","1442","    error_message = \"ValueError: Failing classifier failed as required\"","1443","    # the warning message we're expecting to see","1444","    warning_message = (\"Estimator fit failed. The score on this train-test \"","1445","                       \"partition for these parameters will be set to %f. \"","1446","                       \"Details: \\n%s\" % (fit_and_score_kwargs['error_score'],","1447","                                          error_message))","1448","    # check if the same warning is triggered","1449","    assert_warns_message(FitFailedWarning, warning_message, _fit_and_score,","1450","                         *fit_and_score_args, **fit_and_score_kwargs)"],"delete":[]}]}},"47ce5e1c9d0a63ed3b56b4a9f44fb51a1b35b7cd":{"changes":{"sklearn\/feature_extraction\/text.py":"MODIFY","sklearn\/feature_extraction\/tests\/test_text.py":"MODIFY"},"diff":{"sklearn\/feature_extraction\/text.py":[{"add":["308","    def _validate_params(self):","309","        \"\"\"Check validity of ngram_range parameter\"\"\"","310","        min_n, max_m = self.ngram_range","311","        if min_n > max_m:","312","            raise ValueError(","313","                \"Invalid value for ngram_range=%s \"","314","                \"lower boundary larger than the upper boundary.\"","315","                % str(self.ngram_range))","316","","508","        self._validate_params()","509","","533","        self._validate_params()","534","","897","        self._validate_params()"],"delete":[]}],"sklearn\/feature_extraction\/tests\/test_text.py":[{"add":["37","import pytest","998","","999","","1000","@pytest.mark.parametrize(\"vec\", [","1001","        HashingVectorizer(ngram_range=(2, 1)),","1002","        CountVectorizer(ngram_range=(2, 1)),","1003","        TfidfVectorizer(ngram_range=(2, 1))","1004","    ])","1005","def test_vectorizers_invalid_ngram_range(vec):","1006","    # vectorizers could be initialized with invalid ngram range","1007","    # test for raising error message","1008","    invalid_range = vec.ngram_range","1009","    message = (\"Invalid value for ngram_range=%s \"","1010","               \"lower boundary larger than the upper boundary.\"","1011","               % str(invalid_range))","1012","","1013","    assert_raise_message(","1014","        ValueError, message, vec.fit, [\"good news everyone\"])","1015","    assert_raise_message(","1016","        ValueError, message, vec.fit_transform, [\"good news everyone\"])","1017","","1018","    if isinstance(vec, HashingVectorizer):","1019","        assert_raise_message(","1020","            ValueError, message, vec.transform, [\"good news everyone\"])"],"delete":[]}]}},"db59dd74df576345cc026cc0d6c99392d3649d3b":{"changes":{"doc\/modules\/clustering.rst":"MODIFY","sklearn\/cluster\/_hierarchical.py":"MODIFY","sklearn\/cluster\/_hierarchical_fast.pyx":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY","sklearn\/cluster\/tests\/test_hierarchical.py":"MODIFY","benchmarks\/bench_plot_hierarchical.py":"ADD"},"diff":{"doc\/modules\/clustering.rst":[{"add":["1689",""],"delete":[]}],"sklearn\/cluster\/_hierarchical.py":[{"add":["20","from ..neighbors import DistanceMetric","21","from ..neighbors._dist_metrics import METRIC_MAPPING","111","    mst_array = mst_array[np.argsort(mst_array.T[2], kind='mergesort'), :]","468","        if (linkage == 'single'","469","                and affinity != 'precomputed'","470","                and not callable(affinity)","471","                and affinity in METRIC_MAPPING):","472","","473","            # We need the fast cythonized metric from neighbors","474","            dist_metric = DistanceMetric.get_metric(affinity)","475","","476","            # The Cython routines used require contiguous arrays","477","            X = np.ascontiguousarray(X, dtype=np.double)","478","","479","            mst = _hierarchical.mst_linkage_core(X, dist_metric)","480","            # Sort edges of the min_spanning_tree by weight","481","            mst = mst[np.argsort(mst.T[2], kind='mergesort'), :]","482","","483","            # Convert edge list into standard hierarchical clustering format","484","            out = _hierarchical.single_linkage_label(mst)","485","        else:","486","            out = hierarchy.linkage(X, method=linkage, metric=affinity)"],"delete":["109","    mst_array = mst_array[np.argsort(mst_array.T[2]), :]","466","        out = hierarchy.linkage(X, method=linkage, metric=affinity)"]}],"sklearn\/cluster\/_hierarchical_fast.pyx":[{"add":["15","from ..neighbors._dist_metrics cimport DistanceMetric","29","from numpy.math cimport INFINITY","30","","451","","452","","453","# Implements MST-LINKAGE-CORE from https:\/\/arxiv.org\/abs\/1109.2378","454","@cython.boundscheck(False)","455","@cython.nonecheck(False)","456","def mst_linkage_core(","457","        DTYPE_t [:, ::1] raw_data,","458","        DistanceMetric dist_metric):","459","    \"\"\"","460","    Compute the necessary elements of a minimum spanning","461","    tree for computation of single linkage clustering. This","462","    represents the MST-LINKAGE-CORE algorithm (Figure 6) from","463","    *Modern hierarchical, agglomerative clustering algorithms*","464","    by Daniel Mullner (https:\/\/arxiv.org\/abs\/1109.2378).","465","","466","    In contrast to the scipy implementation is never computes","467","    a full distance matrix, generating distances only as they","468","    are needed and releasing them when no longer needed.","469","","470","    Parameters","471","    ----------","472","    raw_data: array of shape (n_samples, n_features)","473","        The array of feature data to be clustered. Must be C-aligned","474","","475","    dist_metric: DistanceMetric","476","        A DistanceMetric object conforming to the API from","477","        ``sklearn.neighbors._dist_metrics.pxd`` that will be","478","        used to compute distances.","479","","480","    Returns","481","    -------","482","    mst_core_data: array of shape (n_samples, 3)","483","        An array providing information from which one","484","        can either compute an MST, or the linkage hierarchy","485","        very efficiently. See https:\/\/arxiv.org\/abs\/1109.2378","486","        algorithm MST-LINKAGE-CORE for more details.","487","    \"\"\"","488","    cdef:","489","        ITYPE_t n_samples = raw_data.shape[0]","490","        np.int8_t[:] in_tree = np.zeros(n_samples, dtype=np.int8)","491","        DTYPE_t[:, ::1] result = np.zeros((n_samples - 1, 3))","492","","493","        np.ndarray label_filter","494","","495","        ITYPE_t current_node = 0","496","        ITYPE_t new_node","497","        ITYPE_t i","498","        ITYPE_t j","499","        ITYPE_t num_features = raw_data.shape[1]","500","","501","        DTYPE_t right_value","502","        DTYPE_t left_value","503","        DTYPE_t new_distance","504","","505","        DTYPE_t[:] current_distances = np.full(n_samples, INFINITY)","506","","507","    for i in range(n_samples - 1):","508","","509","        in_tree[current_node] = 1","510","","511","        new_distance = INFINITY","512","        new_node = 0","513","","514","        for j in range(n_samples):","515","            if in_tree[j]:","516","                continue","517","","518","            right_value = current_distances[j]","519","            left_value = dist_metric.dist(&raw_data[current_node, 0],","520","                                          &raw_data[j, 0],","521","                                          num_features)","522","","523","            if left_value < right_value:","524","                current_distances[j] = left_value","525","","526","            if current_distances[j] < new_distance:","527","                new_distance = current_distances[j]","528","                new_node = j","529","","530","        result[i, 0] = current_node","531","        result[i, 1] = new_node","532","        result[i, 2] = new_distance","533","        current_node = new_node","534","","535","    return np.array(result)","536",""],"delete":[]}],"doc\/whats_new\/v0.22.rst":[{"add":["170","- |Enhancement| :class:`cluster.AgglomerativeClustering` has a faster and more","171","  more memory efficient implementation of single linkage clustering.","172","  :pr:`11514` by :user:`Leland McInnes <lmcinnes>`.","173",""],"delete":[]}],"sklearn\/cluster\/tests\/test_hierarchical.py":[{"add":["282","def test_sparse_scikit_vs_scipy():","316","# Make sure our custom mst_linkage_core gives","317","# the same results as scipy's builtin","318","@pytest.mark.parametrize('seed', range(5))","319","def test_vector_scikit_single_vs_scipy_single(seed):","320","    n_samples, n_features, n_clusters = 10, 5, 3","321","    rng = np.random.RandomState(seed)","322","    X = .1 * rng.normal(size=(n_samples, n_features))","323","    X -= 4. * np.arange(n_samples)[:, np.newaxis]","324","    X -= X.mean(axis=1)[:, np.newaxis]","325","","326","    out = hierarchy.linkage(X, method='single')","327","    children_scipy = out[:, :2].astype(np.int)","328","","329","    children, _, n_leaves, _ = _TREE_BUILDERS['single'](X)","330","","331","    # Sort the order of child nodes per row for consistency","332","    children.sort(axis=1)","333","    assert_array_equal(children, children_scipy,","334","                       'linkage tree differs'","335","                       ' from scipy impl for'","336","                       ' single linkage.')","337","","338","    cut = _hc_cut(n_clusters, children, n_leaves)","339","    cut_scipy = _hc_cut(n_clusters, children_scipy, n_leaves)","340","    assess_same_labelling(cut, cut_scipy)","341","","342",""],"delete":["282","def test_scikit_vs_scipy():"]}],"benchmarks\/bench_plot_hierarchical.py":[{"add":[],"delete":[]}]}},"4a2b96f8e6c4a07cbc6803459ea8a727b2e54cbb":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/metrics\/cluster\/tests\/test_supervised.py":"MODIFY","sklearn\/metrics\/cluster\/supervised.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["281","- Fixed a bug to avoid integer overflow. Casted product to 64 bits integer in","282","  :func:`mutual_info_score`.","283","  :issue:`9772` by :user:`Kumar Ashutosh <thechargedneutron>`.","284",""],"delete":[]}],"sklearn\/metrics\/cluster\/tests\/test_supervised.py":[{"add":["15","from sklearn.utils import assert_all_finite","175","def test_int_overflow_mutual_info_score():","176","    # Test overflow in mutual_info_classif","177","    x = np.array([1] * (52632 + 2529) + [2] * (14660 + 793) + [3] * (3271 +","178","                 204) + [4] * (814 + 39) + [5] * (316 + 20))","179","    y = np.array([0] * 52632 + [1] * 2529 + [0] * 14660 + [1] * 793 +","180","                 [0] * 3271 + [1] * 204 + [0] * 814 + [1] * 39 + [0] * 316 +","181","                 [1] * 20)","182","","183","    assert_all_finite(mutual_info_score(x.ravel(), y.ravel()))","184","","185",""],"delete":[]}],"sklearn\/metrics\/cluster\/supervised.py":[{"add":["603","    outer = pi.take(nzx).astype(np.int64) * pj.take(nzy).astype(np.int64)"],"delete":["603","    outer = pi.take(nzx) * pj.take(nzy)"]}]}},"1f7fa760d4a32f06f195d74ee865c630c1d64633":{"changes":{"sklearn\/utils\/estimator_checks.py":"MODIFY"},"diff":{"sklearn\/utils\/estimator_checks.py":[{"add":["1053","    # Add noise to X to test the possible values of the labels","1054","    rng = np.random.RandomState(7)","1055","    X_noise = np.concatenate([X, rng.uniform(low=-3, high=3, size=(5, 2))])","1056","    labels = clusterer.fit_predict(X_noise)","1057","","1061","    labels_sorted = np.unique(labels)","1062","    assert_array_equal(labels_sorted, np.arange(labels_sorted[0],","1063","                                                labels_sorted[-1] + 1))","1065","    # Labels are expected to start at 0 (no noise) or -1 (if noise)","1066","    assert_true(labels_sorted[0] in [0, -1])","1067","    # Labels should be less than n_clusters - 1","1070","        assert_greater_equal(n_clusters - 1, labels_sorted[-1])","1071","    # else labels should be less than max(labels_) which is necessarily true"],"delete":["1056","    pred_sorted = np.unique(pred)","1057","    assert_array_equal(pred_sorted, np.arange(pred_sorted[0],","1058","                                              pred_sorted[-1] + 1))","1060","    # labels_ should be greater than -1","1061","    assert_greater_equal(pred_sorted[0], -1)","1062","    # labels_ should be less than n_clusters - 1","1065","        assert_greater_equal(n_clusters - 1, pred_sorted[-1])","1066","    # else labels_ should be less than max(labels_) which is necessarily true"]}]}},"c6005e1ebdf5605a21cb4d0c4a268bb8f0c8396b":{"changes":{"sklearn\/linear_model\/tests\/test_bayes.py":"MODIFY","sklearn\/linear_model\/bayes.py":"MODIFY"},"diff":{"sklearn\/linear_model\/tests\/test_bayes.py":[{"add":["10","from sklearn.utils.testing import assert_array_less","12","from sklearn.utils import check_random_state","64","def test_prediction_bayesian_ridge_ard_with_constant_input():","65","    # Test BayesianRidge and ARDRegression predictions for edge case of","66","    # constant target vectors","67","    n_samples = 4","68","    n_features = 5","69","    random_state = check_random_state(42)","70","    constant_value = random_state.rand()","71","    X = random_state.random_sample((n_samples, n_features))","72","    y = np.full(n_samples, constant_value)","73","    expected = np.full(n_samples, constant_value)","74","","75","    for clf in [BayesianRidge(), ARDRegression()]:","76","        y_pred = clf.fit(X, y).predict(X)","77","        assert_array_almost_equal(y_pred, expected)","78","","79","","80","def test_std_bayesian_ridge_ard_with_constant_input():","81","    # Test BayesianRidge and ARDRegression standard dev. for edge case of","82","    # constant target vector","83","    # The standard dev. should be relatively small (< 0.01 is tested here)","84","    n_samples = 4","85","    n_features = 5","86","    random_state = check_random_state(42)","87","    constant_value = random_state.rand()","88","    X = random_state.random_sample((n_samples, n_features))","89","    y = np.full(n_samples, constant_value)","90","    expected_upper_boundary = 0.01","91","","92","    for clf in [BayesianRidge(), ARDRegression()]:","93","        _, y_std = clf.fit(X, y).predict(X, return_std=True)","94","        assert_array_less(y_std, expected_upper_boundary)","95","","96",""],"delete":[]}],"sklearn\/linear_model\/bayes.py":[{"add":["164","        eps = np.finfo(np.float64).eps","165","        # Add `eps` in the denominator to omit division by zero if `np.var(y)`","166","        # is zero","167","        alpha_ = 1. \/ (np.var(y) + eps)","450","        eps = np.finfo(np.float64).eps","451","        # Add `eps` in the denominator to omit division by zero if `np.var(y)`","452","        # is zero","453","        alpha_ = 1. \/ (np.var(y) + eps)"],"delete":["164","        alpha_ = 1. \/ np.var(y)","447","        alpha_ = 1. \/ np.var(y)"]}]}},"4e56f82c30e836735404c496392a2afd665297fd":{"changes":{"sklearn\/metrics\/ranking.py":"MODIFY","sklearn\/feature_selection\/univariate_selection.py":"MODIFY"},"diff":{"sklearn\/metrics\/ranking.py":[{"add":["212","        precision, recall, _ = precision_recall_curve(","305","        fpr, tpr, _ = roc_curve(y_true, y_score,","306","                                sample_weight=sample_weight)"],"delete":["212","        precision, recall, thresholds = precision_recall_curve(","305","        fpr, tpr, tresholds = roc_curve(y_true, y_score,","306","                                        sample_weight=sample_weight)"]}],"sklearn\/feature_selection\/univariate_selection.py":[{"add":["428","        threshold = stats.scoreatpercentile(scores,","429","                                            100 - self.percentile)","430","        mask = scores > threshold","431","        ties = np.where(scores == threshold)[0]"],"delete":["428","        treshold = stats.scoreatpercentile(scores,","429","                                           100 - self.percentile)","430","        mask = scores > treshold","431","        ties = np.where(scores == treshold)[0]"]}]}},"14e7c328df9968ca403e47168314ae28390c07d5":{"changes":{"sklearn\/externals\/copy_joblib.sh":"MODIFY","sklearn\/externals\/joblib\/format_stack.py":"ADD","examples\/compose\/plot_compare_reduction.py":"MODIFY","sklearn\/tests\/test_site_joblib.py":"ADD","sklearn\/covariance\/graph_lasso_.py":"MODIFY","sklearn\/externals\/joblib\/numpy_pickle.py":"ADD","sklearn\/utils\/_joblib.py":"ADD","sklearn\/externals\/joblib\/_memory_helpers.py":"ADD","sklearn\/externals\/joblib\/backports.py":"ADD","sklearn\/multioutput.py":"MODIFY","benchmarks\/bench_saga.py":"MODIFY","sklearn\/externals\/joblib\/disk.py":"ADD","sklearn\/externals\/joblib\/_multiprocessing_helpers.py":"ADD","sklearn\/externals\/joblib\/parallel.py":"ADD","sklearn\/externals\/joblib\/numpy_pickle_utils.py":"ADD","sklearn\/model_selection\/_search.py":"MODIFY","sklearn\/linear_model\/logistic.py":"MODIFY","sklearn\/linear_model\/omp.py":"MODIFY","sklearn\/externals\/joblib\/pool.py":"ADD","sklearn\/utils\/__init__.py":"MODIFY","sklearn\/cluster\/mean_shift_.py":"MODIFY","sklearn\/ensemble\/forest.py":"MODIFY","sklearn\/externals\/joblib\/memory.py":"ADD","sklearn\/tests\/test_pipeline.py":"MODIFY","sklearn\/externals\/joblib\/_compat.py":"ADD","sklearn\/externals\/joblib\/hashing.py":"ADD","examples\/applications\/wikipedia_principal_eigenvector.py":"MODIFY","sklearn\/externals\/joblib\/_parallel_backends.py":"ADD","sklearn\/metrics\/pairwise.py":"MODIFY","sklearn\/utils\/validation.py":"MODIFY","benchmarks\/bench_mnist.py":"MODIFY","sklearn\/neighbors\/base.py":"MODIFY","sklearn\/pipeline.py":"MODIFY","benchmarks\/bench_tsne_mnist.py":"MODIFY","sklearn\/datasets\/svmlight_format.py":"MODIFY","sklearn\/multiclass.py":"MODIFY","sklearn\/linear_model\/randomized_l1.py":"MODIFY","sklearn\/compose\/_column_transformer.py":"MODIFY","sklearn\/externals\/setup.py":"MODIFY","sklearn\/utils\/tests\/test_validation.py":"MODIFY","sklearn\/datasets\/lfw.py":"MODIFY","sklearn\/linear_model\/coordinate_descent.py":"MODIFY","sklearn\/decomposition\/online_lda.py":"MODIFY","sklearn\/externals\/joblib\/__init__.py":"ADD","sklearn\/manifold\/mds.py":"MODIFY","benchmarks\/bench_plot_nmf.py":"MODIFY","examples\/cluster\/plot_feature_agglomeration_vs_univariate_selection.py":"MODIFY","sklearn\/externals\/joblib\/numpy_pickle_compat.py":"ADD","doc\/developers\/utilities.rst":"MODIFY",".travis.yml":"MODIFY","benchmarks\/bench_covertype.py":"MODIFY","sklearn\/ensemble\/partial_dependence.py":"MODIFY","doc\/glossary.rst":"MODIFY","sklearn\/externals\/joblib\/logger.py":"ADD","sklearn\/externals\/joblib\/my_exceptions.py":"ADD","doc\/whats_new\/v0.20.rst":"MODIFY","doc\/modules\/classes.rst":"MODIFY","sklearn\/feature_selection\/rfe.py":"MODIFY","sklearn\/linear_model\/theil_sen.py":"MODIFY","build_tools\/travis\/install.sh":"MODIFY","benchmarks\/bench_rcv1_logreg_convergence.py":"MODIFY","sklearn\/utils\/estimator_checks.py":"MODIFY","sklearn\/decomposition\/tests\/test_sparse_pca.py":"MODIFY","sklearn\/tests\/test_multioutput.py":"MODIFY","sklearn\/linear_model\/tests\/test_randomized_l1.py":"MODIFY","sklearn\/linear_model\/base.py":"MODIFY","sklearn\/ensemble\/bagging.py":"MODIFY","\/dev\/null":"DELETE","sklearn\/linear_model\/stochastic_gradient.py":"MODIFY","sklearn\/model_selection\/_validation.py":"MODIFY","sklearn\/decomposition\/dict_learning.py":"MODIFY","sklearn\/ensemble\/voting_classifier.py":"MODIFY","sklearn\/externals\/joblib\/func_inspect.py":"ADD","sklearn\/cluster\/k_means_.py":"MODIFY","sklearn\/linear_model\/least_angle.py":"MODIFY"},"diff":{"sklearn\/externals\/copy_joblib.sh":[{"add":["14","cp -r $INSTALL_FOLDER\/joblib joblib","20","find joblib -name \"*.py\" | xargs sed -i.bak \"s\/from joblib\/from sklearn.externals.joblib\/\"","21","find joblib -name \"*.bak\" | xargs rm"],"delete":["14","cp -r $INSTALL_FOLDER\/joblib _joblib","20","find _joblib -name \"*.py\" | xargs sed -i.bak \"s\/from joblib\/from sklearn.externals.joblib\/\"","21","find _joblib -name \"*.bak\" | xargs rm"]}],"sklearn\/externals\/joblib\/format_stack.py":[{"add":[],"delete":[]}],"examples\/compose\/plot_compare_reduction.py":[{"add":["106","from sklearn.utils import Memory"],"delete":["106","from sklearn.externals.joblib import Memory"]}],"sklearn\/tests\/test_site_joblib.py":[{"add":[],"delete":[]}],"sklearn\/covariance\/graph_lasso_.py":[{"add":["25","from ..utils import Parallel, delayed"],"delete":["25","from ..externals.joblib import Parallel, delayed"]}],"sklearn\/externals\/joblib\/numpy_pickle.py":[{"add":[],"delete":[]}],"sklearn\/utils\/_joblib.py":[{"add":[],"delete":[]}],"sklearn\/externals\/joblib\/_memory_helpers.py":[{"add":[],"delete":[]}],"sklearn\/externals\/joblib\/backports.py":[{"add":[],"delete":[]}],"sklearn\/multioutput.py":[{"add":["27","from .utils import Parallel, delayed"],"delete":["27","from .externals.joblib import Parallel, delayed"]}],"benchmarks\/bench_saga.py":[{"add":["14","from sklearn.utils import delayed, Parallel, Memory"],"delete":["14","from sklearn.externals.joblib import delayed, Parallel, Memory"]}],"sklearn\/externals\/joblib\/disk.py":[{"add":[],"delete":[]}],"sklearn\/externals\/joblib\/_multiprocessing_helpers.py":[{"add":[],"delete":[]}],"sklearn\/externals\/joblib\/parallel.py":[{"add":[],"delete":[]}],"sklearn\/externals\/joblib\/numpy_pickle_utils.py":[{"add":[],"delete":[]}],"sklearn\/model_selection\/_search.py":[{"add":["31","from ..utils import Parallel, delayed"],"delete":["31","from ..externals.joblib import Parallel, delayed"]}],"sklearn\/linear_model\/logistic.py":[{"add":["34","from ..utils import Parallel, delayed"],"delete":["34","from ..externals.joblib import Parallel, delayed"]}],"sklearn\/linear_model\/omp.py":[{"add":["18","from ..utils import Parallel, delayed"],"delete":["18","from ..externals.joblib import Parallel, delayed"]}],"sklearn\/externals\/joblib\/pool.py":[{"add":[],"delete":[]}],"sklearn\/utils\/__init__.py":[{"add":["17","from ._joblib import cpu_count, Parallel, Memory, delayed","18","from ._joblib import parallel_backend","30","           \"check_symmetric\", \"indices_to_mask\", \"deprecated\",","31","           \"cpu_count\", \"Parallel\", \"Memory\", \"delayed\", \"parallel_backend\"]"],"delete":["17","from ..externals.joblib import cpu_count","29","           \"check_symmetric\", \"indices_to_mask\", \"deprecated\"]"]}],"sklearn\/cluster\/mean_shift_.py":[{"add":["26","from ..utils import Parallel","27","from ..utils import delayed"],"delete":["26","from ..externals.joblib import Parallel","27","from ..externals.joblib import delayed"]}],"sklearn\/ensemble\/forest.py":[{"add":["54","from ..utils import Parallel, delayed"],"delete":["54","from ..externals.joblib import Parallel, delayed"]}],"sklearn\/externals\/joblib\/memory.py":[{"add":[],"delete":[]}],"sklearn\/tests\/test_pipeline.py":[{"add":["35","from sklearn.utils import Memory","910","                        \"sklearn.utils.Memory.\"","933","                        \"sklearn.utils.Memory.\""],"delete":["35","from sklearn.externals.joblib import Memory","910","                        \"sklearn.externals.joblib.Memory.\"","933","                        \"sklearn.externals.joblib.Memory.\""]}],"sklearn\/externals\/joblib\/_compat.py":[{"add":[],"delete":[]}],"sklearn\/externals\/joblib\/hashing.py":[{"add":[],"delete":[]}],"examples\/applications\/wikipedia_principal_eigenvector.py":[{"add":["47","from sklearn.utils import Memory"],"delete":["47","from sklearn.externals.joblib import Memory"]}],"sklearn\/externals\/joblib\/_parallel_backends.py":[{"add":[],"delete":[]}],"sklearn\/metrics\/pairwise.py":[{"add":["26","from ..utils import Parallel","27","from ..utils import delayed","28","from ..utils import cpu_count"],"delete":["26","from ..externals.joblib import Parallel","27","from ..externals.joblib import delayed","28","from ..externals.joblib import cpu_count"]}],"sklearn\/utils\/validation.py":[{"add":["26","from ..utils._joblib import Memory","185","    sklearn.utils.Memory instance (typically a str denoting the","206","                         \" interface as sklearn.utils.Memory.\""],"delete":["26","from ..externals.joblib import Memory","185","    sklearn.externals.joblib.Memory instance (typically a str denoting the","206","                         \" interface as sklearn.externals.joblib.Memory.\""]}],"benchmarks\/bench_mnist.py":[{"add":["43","from sklearn.utils import Memory"],"delete":["43","from sklearn.externals.joblib import Memory"]}],"sklearn\/neighbors\/base.py":[{"add":["25","from ..utils import Parallel, delayed"],"delete":["25","from ..externals.joblib import Parallel, delayed"]}],"sklearn\/pipeline.py":[{"add":["17","from .utils import Parallel, delayed"],"delete":["17","from .externals.joblib import Parallel, delayed"]}],"benchmarks\/bench_tsne_mnist.py":[{"add":["17","from sklearn.utils import Memory"],"delete":["17","from sklearn.externals.joblib import Memory"]}],"sklearn\/datasets\/svmlight_format.py":[{"add":["134","        from sklearn.utils import Memory"],"delete":["134","        from sklearn.externals.joblib import Memory"]}],"sklearn\/multiclass.py":[{"add":["54","from .utils import Parallel","55","from .utils import delayed"],"delete":["54","from .externals.joblib import Parallel","55","from .externals.joblib import delayed"]}],"sklearn\/linear_model\/randomized_l1.py":[{"add":["21","from ..utils import Memory, Parallel, delayed","111","                             \" a sklearn.utils.Memory\""],"delete":["21","from ..externals.joblib import Memory, Parallel, delayed","111","                             \" a sklearn.externals.joblib.Memory\""]}],"sklearn\/compose\/_column_transformer.py":[{"add":["14","from ..utils import Parallel, delayed"],"delete":["14","from ..externals.joblib import Parallel, delayed"]}],"sklearn\/externals\/setup.py":[{"add":["6","    config.add_subpackage('joblib')"],"delete":["6","    config.add_subpackage('_joblib')"]}],"sklearn\/utils\/tests\/test_validation.py":[{"add":["744","                        \"sklearn.utils.Memory.\"","749","                        \"sklearn.utils.Memory. Got memory='{}' \""],"delete":["744","                        \"sklearn.externals.joblib.Memory.\"","749","                        \"sklearn.externals.joblib.Memory. Got memory='{}' \""]}],"sklearn\/datasets\/lfw.py":[{"add":["33","from ..utils import Memory"],"delete":["33","from ..externals.joblib import Memory"]}],"sklearn\/linear_model\/coordinate_descent.py":[{"add":["20","from ..utils import Parallel, delayed"],"delete":["20","from ..externals.joblib import Parallel, delayed"]}],"sklearn\/decomposition\/online_lda.py":[{"add":["23","from ..utils import Parallel, delayed"],"delete":["23","from ..externals.joblib import Parallel, delayed"]}],"sklearn\/externals\/joblib\/__init__.py":[{"add":[],"delete":[]}],"sklearn\/manifold\/mds.py":[{"add":["14","from ..utils import Parallel","15","from ..utils import delayed"],"delete":["14","from ..externals.joblib import Parallel","15","from ..externals.joblib import delayed"]}],"benchmarks\/bench_plot_nmf.py":[{"add":["24","from sklearn.utils import Memory"],"delete":["24","from sklearn.externals.joblib import Memory"]}],"examples\/cluster\/plot_feature_agglomeration_vs_univariate_selection.py":[{"add":["32","from sklearn.utils import Memory"],"delete":["32","from sklearn.externals.joblib import Memory"]}],"sklearn\/externals\/joblib\/numpy_pickle_compat.py":[{"add":[],"delete":[]}],"doc\/developers\/utilities.rst":[{"add":["47","  ``sklearn.utils.Memory`` instance (typically a str denoting"],"delete":["47","  ``sklearn.externals.joblib.Memory`` instance (typically a str denoting"]}],".travis.yml":[{"add":["40","    # This environment tests the newest supported Anaconda release.","41","    # It runs tests requiring pandas and PyAMG.","42","    # It also runs with the site joblib instead of the vendored copy of joblib.","46","           JOBLIB_VERSION=\"0.12.0\" COVERAGE=true","48","           SKLEARN_SITE_JOBLIB=1"],"delete":["40","    # This environment tests the newest supported Anaconda release (5.0.0)","41","    # It also runs tests requiring Pandas and PyAMG","45","           COVERAGE=true"]}],"benchmarks\/bench_covertype.py":[{"add":["61","from sklearn.utils import Memory"],"delete":["61","from sklearn.externals.joblib import Memory"]}],"sklearn\/ensemble\/partial_dependence.py":[{"add":["12","from ..utils import Parallel, delayed"],"delete":["12","from ..externals.joblib import Parallel, delayed"]}],"doc\/glossary.rst":[{"add":["1487","          backend by using :func:`sklearn.utils.parallel_backend`."],"delete":["1487","          backend by using :func:`sklearn.externals.joblib.parallel.parallel_backend`."]}],"sklearn\/externals\/joblib\/logger.py":[{"add":[],"delete":[]}],"sklearn\/externals\/joblib\/my_exceptions.py":[{"add":[],"delete":[]}],"doc\/whats_new\/v0.20.rst":[{"add":["220","  one was added (:ref:`environment_variable`). The main API of joblib is now","221","  exposed in :mod:`sklearn.utils`.","222","  :issue:`11166`by `Gael Varoquaux`_"],"delete":["85","**Other backward incompatible change** The vendored version of the joblib","86","module is now found at `sklearn.externals._joblib` (:issue:`11166`). The","87","main API of joblib is still exposed in `sklearn.externals.joblib`, but","88","code doing imports of subpackages of `sklearn.externals.joblib` will","89","break.","90","","226","  one was added (:ref:`environment_variable`).","227","  :issue:`11166` by `Gael Varoquaux`_"]}],"doc\/modules\/classes.rst":[{"add":["1477","Utilities from joblib:","1478","","1479",".. autosummary::","1480","   :toctree: generated\/","1481","   :template: class.rst","1482","","1483","   utils.Memory","1484","   utils.Parallel","1485","","1486",".. autosummary::","1487","   :toctree: generated\/","1488","   :template: function.rst","1489","","1490","   utils.cpu_count","1491","   utils.delayed","1492","   utils.parallel_backend"],"delete":[]}],"sklearn\/feature_selection\/rfe.py":[{"add":["17","from ..utils import Parallel, delayed"],"delete":["17","from ..externals.joblib import Parallel, delayed"]}],"sklearn\/linear_model\/theil_sen.py":[{"add":["23","from ..utils import Parallel, delayed"],"delete":["23","from ..externals.joblib import Parallel, delayed"]}],"build_tools\/travis\/install.sh":[{"add":["61","    if [[ -n \"$JOBLIB_VERSION\" ]]; then","62","        TO_INSTALL=\"$TO_INSTALL joblib=$JOBLIB_VERSION\"","63","    fi","64",""],"delete":[]}],"benchmarks\/bench_rcv1_logreg_convergence.py":[{"add":["10","from sklearn.utils import Memory"],"delete":["10","from sklearn.externals.joblib import Memory"]}],"sklearn\/utils\/estimator_checks.py":[{"add":["16","from sklearn.utils._joblib import hash, Memory"],"delete":["16","from sklearn.externals.joblib import hash, Memory"]}],"sklearn\/decomposition\/tests\/test_sparse_pca.py":[{"add":["153","        import sklearn.utils._joblib.parallel as joblib_par"],"delete":["153","        import sklearn.externals.joblib.parallel as joblib_par"]}],"sklearn\/tests\/test_multioutput.py":[{"add":["20","from sklearn.utils import cpu_count"],"delete":["20","from sklearn.externals.joblib import cpu_count"]}],"sklearn\/linear_model\/tests\/test_randomized_l1.py":[{"add":["58","                        \" a sklearn.utils.Memory instance\","],"delete":["58","                        \" a sklearn.externals.joblib.Memory instance\","]}],"sklearn\/linear_model\/base.py":[{"add":["26","from ..utils import Parallel, delayed"],"delete":["26","from ..externals.joblib import Parallel, delayed"]}],"sklearn\/ensemble\/bagging.py":[{"add":["15","from ..utils import Parallel, delayed"],"delete":["15","from ..externals.joblib import Parallel, delayed"]}],"\/dev\/null":[{"add":[],"delete":[]}],"sklearn\/linear_model\/stochastic_gradient.py":[{"add":["11","from ..utils import Parallel, delayed"],"delete":["11","from ..externals.joblib import Parallel, delayed"]}],"sklearn\/model_selection\/_validation.py":[{"add":["27","from ..utils import Parallel, delayed","28","from ..utils._joblib import logger"],"delete":["27","from ..externals.joblib import Parallel, delayed, logger"]}],"sklearn\/decomposition\/dict_learning.py":[{"add":["17","from ..utils import Parallel, delayed, cpu_count"],"delete":["17","from ..externals.joblib import Parallel, delayed, cpu_count"]}],"sklearn\/ensemble\/voting_classifier.py":[{"add":["20","from ..utils import Parallel, delayed"],"delete":["20","from ..externals.joblib import Parallel, delayed"]}],"sklearn\/externals\/joblib\/func_inspect.py":[{"add":[],"delete":[]}],"sklearn\/cluster\/k_means_.py":[{"add":["31","from ..utils import Parallel","32","from ..utils import delayed"],"delete":["31","from ..externals.joblib import Parallel","32","from ..externals.joblib import delayed"]}],"sklearn\/linear_model\/least_angle.py":[{"add":["25","from ..utils import Parallel, delayed"],"delete":["25","from ..externals.joblib import Parallel, delayed"]}]}},"3f96eeedd122d6cb83536e0650dc5735546f3692":{"changes":{"conftest.py":"MODIFY"},"diff":{"conftest.py":[{"add":["11","    np.set_printoptions(legacy=True)"],"delete":["11","    np.set_printoptions(sign='legacy')"]}]}},"2160ea0a74bf259012c5b5d47502e3a3267b2844":{"changes":{"sklearn\/decomposition\/tests\/test_nmf.py":"MODIFY","sklearn\/decomposition\/nmf.py":"MODIFY"},"diff":{"sklearn\/decomposition\/tests\/test_nmf.py":[{"add":["481","","482","","483","def test_nmf_underflow():","484","    # Regression test for an underflow issue in _beta_divergence","485","    rng = np.random.RandomState(0)","486","    n_samples, n_features, n_components = 10, 2, 2","487","    X = np.abs(rng.randn(n_samples, n_features)) * 10","488","    W = np.abs(rng.randn(n_samples, n_components)) * 10","489","    H = np.abs(rng.randn(n_components, n_features))","490","","491","    X[0, 0] = 0","492","    ref = nmf._beta_divergence(X, W, H, beta=1.0)","493","    X[0, 0] = 1e-323","494","    res = nmf._beta_divergence(X, W, H, beta=1.0)","495","    assert_almost_equal(res, ref)"],"delete":[]}],"sklearn\/decomposition\/nmf.py":[{"add":["116","    indices = X_data > EPSILON","117","    WH_data = WH_data[indices]","118","    X_data = X_data[indices]"],"delete":["116","    WH_data = WH_data[X_data != 0]","117","    X_data = X_data[X_data != 0]"]}]}},"28a2ad265f4063db0aaa2ec463da73a32042ae05":{"changes":{"doc\/modules\/model_evaluation.rst":"MODIFY"},"diff":{"doc\/modules\/model_evaluation.rst":[{"add":["176","    >>> def my_custom_loss_func(y_true, y_pred):","177","    ...     diff = np.abs(y_true - y_pred).max()","180","    >>> # score will negate the return value of my_custom_loss_func,","181","    >>> # which will be np.log(2), 0.693, given the values for X","182","    >>> # and y defined below.","183","    >>> score = make_scorer(my_custom_loss_func, greater_is_better=False)","184","    >>> X = [[1], [1]]","185","    >>> y  = [0, 1]","188","    >>> clf = clf.fit(X, y)","189","    >>> my_custom_loss_func(clf.predict(X), y) # doctest: +ELLIPSIS","191","    >>> score(clf, X, y) # doctest: +ELLIPSIS","192","    -0.69..."],"delete":["176","    >>> def my_custom_loss_func(ground_truth, predictions):","177","    ...     diff = np.abs(ground_truth - predictions).max()","180","    >>> # loss_func will negate the return value of my_custom_loss_func,","181","    >>> #  which will be np.log(2), 0.693, given the values for ground_truth","182","    >>> #  and predictions defined below.","183","    >>> loss  = make_scorer(my_custom_loss_func, greater_is_better=False)","184","    >>> score = make_scorer(my_custom_loss_func, greater_is_better=True)","185","    >>> ground_truth = [[1], [1]]","186","    >>> predictions  = [0, 1]","189","    >>> clf = clf.fit(ground_truth, predictions)","190","    >>> loss(clf,ground_truth, predictions) # doctest: +ELLIPSIS","191","    -0.69...","192","    >>> score(clf,ground_truth, predictions) # doctest: +ELLIPSIS"]}]}},"ff5aac0c9e5555f52165ccfbf489e6592a58cf9c":{"changes":{"CONTRIBUTING.md":"MODIFY","doc\/developers\/contributing.rst":"MODIFY"},"diff":{"CONTRIBUTING.md":[{"add":["145","-  No flake8 warnings, check with:","148","  $ pip install flake8","149","  $ flake8 path\/to\/module.py"],"delete":["145","-  No pyflakes warnings, check with:","148","  $ pip install pyflakes","149","  $ pyflakes path\/to\/module.py","150","  ```","151","","152","-  No PEP8 warnings, check with:","153","","154","  ```bash","155","  $ pip install pep8","156","  $ pep8 path\/to\/module.py","157","  ```","158","","159","-  AutoPEP8 can help you fix some of the easy redundant errors:","160","","161","  ```bash","162","  $ pip install autopep8","163","  $ autopep8 path\/to\/pep8.py"]}],"doc\/developers\/contributing.rst":[{"add":["287","* No flake8 warnings, check with::","289","    $ pip install flake8","290","    $ flake8 path\/to\/module.py"],"delete":["287","* No pyflakes warnings, check with::","289","    $ pip install pyflakes","290","    $ pyflakes path\/to\/module.py","291","","292","* No PEP8 warnings, check with::","293","","294","    $ pip install pep8","295","    $ pep8 path\/to\/module.py","296","","297","* AutoPEP8 can help you fix some of the easy redundant errors::","298","","299","    $ pip install autopep8","300","    $ autopep8 path\/to\/pep8.py"]}]}},"6ade12a55b27f66e41754898998c9ce12c5271c2":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/tree\/_criterion.pyx":"MODIFY","sklearn\/tree\/tests\/test_tree.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["549","  ","550","- Fixed a bug in :class:`tree.MAE` to ensure sample weights are being used ","551","  during the calculation of tree MAE impurity. Previous behaviour could ","552","  cause suboptimal splits to be chosen since the impurity calculation ","553","  considered all samples to be of equal weight importance.","554","  :issue:`11464` by :user:`John Stott <JohnStott>`.  "],"delete":[]}],"sklearn\/tree\/_criterion.pyx":[{"add":["1240","        cdef DOUBLE_t w = 1.0","1241","        cdef DOUBLE_t impurity = 0.0","1249","                if sample_weight != NULL:","1250","                    w = sample_weight[i]","1251","","1252","                impurity += fabs(y_ik - self.node_medians[k]) * w","1253","","1256","    cdef void children_impurity(self, double* p_impurity_left,","1257","                                double* p_impurity_right) nogil:","1274","        cdef DOUBLE_t w = 1.0","1275","        cdef DOUBLE_t impurity_left = 0.0","1276","        cdef DOUBLE_t impurity_right = 0.0","1288","                if sample_weight != NULL:","1289","                    w = sample_weight[i]","1290","","1291","                impurity_left += fabs(y_ik - median) * w","1292","        p_impurity_left[0] = impurity_left \/ (self.weighted_n_left * ","1293","                                              self.n_outputs)","1302","                if sample_weight != NULL:","1303","                    w = sample_weight[i]","1304","","1305","                impurity_right += fabs(y_ik - median) * w","1306","        p_impurity_right[0] = impurity_right \/ (self.weighted_n_right * ","1307","                                                self.n_outputs)"],"delete":["1240","        cdef DOUBLE_t w_y_ik","1241","","1242","        cdef double impurity = 0.0","1250","                impurity += <double> fabs((<double> y_ik) - <double> self.node_medians[k])","1253","    cdef void children_impurity(self, double* impurity_left,","1254","                                double* impurity_right) nogil:","1275","        impurity_left[0] = 0.0","1276","        impurity_right[0] = 0.0","1277","","1285","                impurity_left[0] += <double>fabs((<double> y_ik) -","1286","                                                 <double> median)","1287","        impurity_left[0] \/= <double>((self.weighted_n_left) * self.n_outputs)","1296","                impurity_right[0] += <double>fabs((<double> y_ik) -","1297","                                                  <double> median)","1298","        impurity_right[0] \/= <double>((self.weighted_n_right) *","1299","                                      self.n_outputs)"]}],"sklearn\/tree\/tests\/test_tree.py":[{"add":["20","from sklearn.utils.testing import assert_allclose","1696","    \"\"\"Check MAE criterion produces correct results on small toy dataset:","1697","","1698","    ------------------","1699","    | X | y | weight |","1700","    ------------------","1701","    | 3 | 3 |  0.1   |","1702","    | 5 | 3 |  0.3   |","1703","    | 8 | 4 |  1.0   |","1704","    | 3 | 6 |  0.6   |","1705","    | 5 | 7 |  0.3   |","1706","    ------------------","1707","    |sum wt:|  2.3   |","1708","    ------------------","1709","","1710","    Because we are dealing with sample weights, we cannot find the median by","1711","    simply choosing\/averaging the centre value(s), instead we consider the","1712","    median where 50% of the cumulative weight is found (in a y sorted data set)","1713","    . Therefore with regards to this test data, the cumulative weight is >= 50%","1714","    when y = 4.  Therefore:","1715","    Median = 4","1716","","1717","    For all the samples, we can get the total error by summing:","1718","    Absolute(Median - y) * weight","1719","","1720","    I.e., total error = (Absolute(4 - 3) * 0.1)","1721","                      + (Absolute(4 - 3) * 0.3)","1722","                      + (Absolute(4 - 4) * 1.0)","1723","                      + (Absolute(4 - 6) * 0.6)","1724","                      + (Absolute(4 - 7) * 0.3)","1725","                      = 2.5","1726","","1727","    Impurity = Total error \/ total weight","1728","             = 2.5 \/ 2.3","1729","             = 1.08695652173913","1730","             ------------------","1731","","1732","    From this root node, the next best split is between X values of 3 and 5.","1733","    Thus, we have left and right child nodes:","1734","","1735","    LEFT                    RIGHT","1736","    ------------------      ------------------","1737","    | X | y | weight |      | X | y | weight |","1738","    ------------------      ------------------","1739","    | 3 | 3 |  0.1   |      | 5 | 3 |  0.3   |","1740","    | 3 | 6 |  0.6   |      | 8 | 4 |  1.0   |","1741","    ------------------      | 5 | 7 |  0.3   |","1742","    |sum wt:|  0.7   |      ------------------","1743","    ------------------      |sum wt:|  1.6   |","1744","                            ------------------","1745","","1746","    Impurity is found in the same way:","1747","    Left node Median = 6","1748","    Total error = (Absolute(6 - 3) * 0.1)","1749","                + (Absolute(6 - 6) * 0.6)","1750","                = 0.3","1751","","1752","    Left Impurity = Total error \/ total weight","1753","            = 0.3 \/ 0.7","1754","            = 0.428571428571429","1755","            -------------------","1756","","1757","    Likewise for Right node:","1758","    Right node Median = 4","1759","    Total error = (Absolute(4 - 3) * 0.3)","1760","                + (Absolute(4 - 4) * 1.0)","1761","                + (Absolute(4 - 7) * 0.3)","1762","                = 1.2","1763","","1764","    Right Impurity = Total error \/ total weight","1765","            = 1.2 \/ 1.6","1766","            = 0.75","1767","            ------","1768","    \"\"\"","1771","","1772","    # Test MAE where sample weights are non-uniform (as illustrated above):","1773","    dt_mae.fit(X=[[3], [5], [3], [8], [5]], y=[6, 7, 3, 4, 3],","1774","               sample_weight=[0.6, 0.3, 0.1, 1.0, 0.3])","1775","    assert_allclose(dt_mae.tree_.impurity, [2.5 \/ 2.3, 0.3 \/ 0.7, 1.2 \/ 1.6])","1776","    assert_array_equal(dt_mae.tree_.value.flat, [4.0, 6.0, 4.0])","1777","","1778","    # Test MAE where all sample weights are uniform:","1779","    dt_mae.fit(X=[[3], [5], [3], [8], [5]], y=[6, 7, 3, 4, 3],","1780","               sample_weight=np.ones(5))","1781","    assert_array_equal(dt_mae.tree_.impurity, [1.4, 1.5, 4.0 \/ 3.0])","1784","    # Test MAE where a `sample_weight` is not explicitly provided.","1785","    # This is equivalent to providing uniform sample weights, though","1786","    # the internal logic is different:","1787","    dt_mae.fit(X=[[3], [5], [3], [8], [5]], y=[6, 7, 3, 4, 3])","1788","    assert_array_equal(dt_mae.tree_.impurity, [1.4, 1.5, 4.0 \/ 3.0])","1789","    assert_array_equal(dt_mae.tree_.value.flat, [4, 4.5, 4.0])"],"delete":["1695","    # check MAE criterion produces correct results","1696","    # on small toy dataset","1699","    dt_mae.fit([[3], [5], [3], [8], [5]], [6, 7, 3, 4, 3])","1700","    assert_array_equal(dt_mae.tree_.impurity, [1.4, 1.5, 4.0\/3.0])","1703","    dt_mae.fit([[3], [5], [3], [8], [5]], [6, 7, 3, 4, 3],","1704","               [0.6, 0.3, 0.1, 1.0, 0.3])","1705","    assert_array_equal(dt_mae.tree_.impurity, [7.0\/2.3, 3.0\/0.7, 4.0\/1.6])","1706","    assert_array_equal(dt_mae.tree_.value.flat, [4.0, 6.0, 4.0])"]}]}},"dd69361a0d9c6ccde0d2353b00b86e0e7541a3e3":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/metrics\/tests\/test_common.py":"MODIFY","sklearn\/metrics\/ranking.py":"MODIFY","sklearn\/metrics\/tests\/test_ranking.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["394","- :func:`metrics.average_precision_score` now supports binary ``y_true``","395","  other than ``{0, 1}`` or ``{-1, 1}`` through ``pos_label`` parameter.","396","  :issue:`9980` by :user:`Hanmin Qin <qinhanmin2014>`.","397","","634","- Fixed a bug where :func:`metrics.average_precision_score` will sometimes return","635","  ``nan`` when ``sample_weight`` contains 0.","636","  :issue:`9980` by :user:`Hanmin Qin <qinhanmin2014>`.","637",""],"delete":[]}],"sklearn\/metrics\/tests\/test_common.py":[{"add":["259","    \"average_precision_score\",","260","    \"weighted_average_precision_score\",","261","    \"micro_average_precision_score\",","262","    \"macro_average_precision_score\",","263","    \"samples_average_precision_score\",","264","","300","    \"average_precision_score\",","301","    \"weighted_average_precision_score\",","302","    \"micro_average_precision_score\",","303","    \"macro_average_precision_score\",","304","    \"samples_average_precision_score\",","305","","674","            measure_with_strobj = metric_str(y1_str.astype('O'), y2)"],"delete":["243","","244","    \"average_precision_score\",","245","    \"weighted_average_precision_score\",","246","    \"micro_average_precision_score\",","247","    \"macro_average_precision_score\",","248","    \"samples_average_precision_score\",","249","","669","            measure_with_strobj = metric(y1_str.astype('O'), y2)"]}],"sklearn\/metrics\/ranking.py":[{"add":["22","from functools import partial","23","","129","def average_precision_score(y_true, y_score, average=\"macro\", pos_label=1,","154","        True binary labels or binary label indicators.","177","    pos_label : int or str (default=1)","178","        The label of the positive class. Only applied to binary ``y_true``.","179","        For multilabel-indicator ``y_true``, ``pos_label`` is fixed to 1.","180","","217","            y_true, y_score, pos_label=1, sample_weight=None):","219","            y_true, y_score, pos_label=pos_label, sample_weight=sample_weight)","225","    y_type = type_of_target(y_true)","226","    if y_type == \"multilabel-indicator\" and pos_label != 1:","227","        raise ValueError(\"Parameter pos_label is fixed to 1 for \"","228","                         \"multilabel-indicator y_true. Do not set \"","229","                         \"pos_label or set pos_label to 1.\")","230","    average_precision = partial(_binary_uninterpolated_average_precision,","231","                                pos_label=pos_label)","232","    return _average_binary_score(average_precision, y_true, y_score,","233","                                 average, sample_weight=sample_weight)","515","    precision[np.isnan(precision)] = 0"],"delete":["127","def average_precision_score(y_true, y_score, average=\"macro\",","152","        True binary labels (either {0, 1} or {-1, 1}).","211","            y_true, y_score, sample_weight=None):","213","            y_true, y_score, sample_weight=sample_weight)","219","    return _average_binary_score(_binary_uninterpolated_average_precision,","220","                                 y_true, y_score, average,","221","                                 sample_weight=sample_weight)"]}],"sklearn\/metrics\/tests\/test_ranking.py":[{"add":["683","def test_average_precision_score_pos_label_multilabel_indicator():","684","    # Raise an error for multilabel-indicator y_true with","685","    # pos_label other than 1","686","    y_true = np.array([[1, 0], [0, 1], [0, 1], [1, 0]])","687","    y_pred = np.array([[0.9, 0.1], [0.1, 0.9], [0.8, 0.2], [0.2, 0.8]])","688","    erorr_message = (\"Parameter pos_label is fixed to 1 for multilabel\"","689","                     \"-indicator y_true. Do not set pos_label or set \"","690","                     \"pos_label to 1.\")","691","    assert_raise_message(ValueError, erorr_message, average_precision_score,","692","                         y_true, y_pred, pos_label=0)","693","","694",""],"delete":[]}]}}}