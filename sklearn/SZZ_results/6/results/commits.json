{"60cf1d62d2c0c1bf8ad321c7c098f6494abf4597":{"changes":{"sklearn\/datasets\/samples_generator.py":"MODIFY","sklearn\/datasets\/tests\/test_samples_generator.py":"MODIFY"},"diff":{"sklearn\/datasets\/samples_generator.py":[{"add":["162","    # Use log2 to avoid overflow errors","163","    if n_informative < np.log2(n_classes * n_clusters_per_class):"],"delete":["162","    if 2 ** n_informative < n_classes * n_clusters_per_class:"]}],"sklearn\/datasets\/tests\/test_samples_generator.py":[{"add":["86","                                                         (10, [1\/3] * 3, 10),","87","                                                         (np.int(64), [1], 1)","131","                    assert_array_almost_equal(np.abs(centroid) \/ class_sep,","132","                                              np.ones(n_informative),","133","                                              decimal=5,","140","                                  np.abs(centroid) \/ class_sep,","141","                                  np.ones(n_informative),","142","                                  decimal=5,","143","                                  err_msg=\"Clusters should not be centered \""],"delete":["86","                                                         (10, [1\/3] * 3, 10)","130","                    assert_array_almost_equal(np.abs(centroid),","131","                                              [class_sep] * n_informative,","132","                                              decimal=0,","139","                                  np.abs(centroid),","140","                                  [class_sep] * n_informative,","141","                                  decimal=0,","142","                                  err_msg=\"Clusters should not be cenetered \""]}]}},"0caa548d7552ac78f86a24be74f4d6d49e502ace":{"changes":{"sklearn\/cluster\/hierarchical.py":"MODIFY","sklearn\/cluster\/tests\/test_feature_agglomeration.py":"MODIFY"},"diff":{"sklearn\/cluster\/hierarchical.py":[{"add":["778","        if (self.pooling_func != 'deprecated' and","779","                not isinstance(self, AgglomerationTransform)):"],"delete":["778","        if self.pooling_func != 'deprecated':"]}],"sklearn\/cluster\/tests\/test_feature_agglomeration.py":[{"add":["6","from sklearn.utils.testing import assert_true, assert_no_warnings","18","    assert_no_warnings(agglo_mean.fit, X)","19","    assert_no_warnings(agglo_median.fit, X)"],"delete":["6","from sklearn.utils.testing import assert_true","18","    agglo_mean.fit(X)","19","    agglo_median.fit(X)"]}]}},"8585275120d713cdf98466592383621dce40da92":{"changes":{"sklearn\/datasets\/base.py":"MODIFY","doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/datasets\/data\/boston_house_prices.csv":"MODIFY"},"diff":{"sklearn\/datasets\/base.py":[{"add":["704","        .. versionchanged:: 0.20","705","            Fixed a wrong data point at [445, 0].","706",""],"delete":[]}],"doc\/whats_new\/v0.20.rst":[{"add":["387","Datasets","388","","389","- Fixed a bug in :func:`dataset.load_boston` which had a wrong data point.","390","  :issue:`10801` by :user:`Takeshi Yoshizawa <tarcusx>`.","391","","487","  :issue:`10420` by :user:`Jonathan Ohayon <Johayon>`"],"delete":["482","  :issue:`10420` by :user:`Jonathan Ohayon <Johayon>`"]}],"sklearn\/datasets\/data\/boston_house_prices.csv":[{"add":["447","10.6718,0,18.1,0,0.74,6.459,94.8,1.9879,24,666,20.2,43.06,23.98,11.8"],"delete":["447","0.6718,0,18.1,0,0.74,6.459,94.8,1.9879,24,666,20.2,43.06,23.98,11.8"]}]}},"db59dd74df576345cc026cc0d6c99392d3649d3b":{"changes":{"doc\/modules\/clustering.rst":"MODIFY","sklearn\/cluster\/_hierarchical.py":"MODIFY","sklearn\/cluster\/_hierarchical_fast.pyx":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY","sklearn\/cluster\/tests\/test_hierarchical.py":"MODIFY","benchmarks\/bench_plot_hierarchical.py":"ADD"},"diff":{"doc\/modules\/clustering.rst":[{"add":["1689",""],"delete":[]}],"sklearn\/cluster\/_hierarchical.py":[{"add":["20","from ..neighbors import DistanceMetric","21","from ..neighbors._dist_metrics import METRIC_MAPPING","111","    mst_array = mst_array[np.argsort(mst_array.T[2], kind='mergesort'), :]","468","        if (linkage == 'single'","469","                and affinity != 'precomputed'","470","                and not callable(affinity)","471","                and affinity in METRIC_MAPPING):","472","","473","            # We need the fast cythonized metric from neighbors","474","            dist_metric = DistanceMetric.get_metric(affinity)","475","","476","            # The Cython routines used require contiguous arrays","477","            X = np.ascontiguousarray(X, dtype=np.double)","478","","479","            mst = _hierarchical.mst_linkage_core(X, dist_metric)","480","            # Sort edges of the min_spanning_tree by weight","481","            mst = mst[np.argsort(mst.T[2], kind='mergesort'), :]","482","","483","            # Convert edge list into standard hierarchical clustering format","484","            out = _hierarchical.single_linkage_label(mst)","485","        else:","486","            out = hierarchy.linkage(X, method=linkage, metric=affinity)"],"delete":["109","    mst_array = mst_array[np.argsort(mst_array.T[2]), :]","466","        out = hierarchy.linkage(X, method=linkage, metric=affinity)"]}],"sklearn\/cluster\/_hierarchical_fast.pyx":[{"add":["15","from ..neighbors._dist_metrics cimport DistanceMetric","29","from numpy.math cimport INFINITY","30","","451","","452","","453","# Implements MST-LINKAGE-CORE from https:\/\/arxiv.org\/abs\/1109.2378","454","@cython.boundscheck(False)","455","@cython.nonecheck(False)","456","def mst_linkage_core(","457","        DTYPE_t [:, ::1] raw_data,","458","        DistanceMetric dist_metric):","459","    \"\"\"","460","    Compute the necessary elements of a minimum spanning","461","    tree for computation of single linkage clustering. This","462","    represents the MST-LINKAGE-CORE algorithm (Figure 6) from","463","    *Modern hierarchical, agglomerative clustering algorithms*","464","    by Daniel Mullner (https:\/\/arxiv.org\/abs\/1109.2378).","465","","466","    In contrast to the scipy implementation is never computes","467","    a full distance matrix, generating distances only as they","468","    are needed and releasing them when no longer needed.","469","","470","    Parameters","471","    ----------","472","    raw_data: array of shape (n_samples, n_features)","473","        The array of feature data to be clustered. Must be C-aligned","474","","475","    dist_metric: DistanceMetric","476","        A DistanceMetric object conforming to the API from","477","        ``sklearn.neighbors._dist_metrics.pxd`` that will be","478","        used to compute distances.","479","","480","    Returns","481","    -------","482","    mst_core_data: array of shape (n_samples, 3)","483","        An array providing information from which one","484","        can either compute an MST, or the linkage hierarchy","485","        very efficiently. See https:\/\/arxiv.org\/abs\/1109.2378","486","        algorithm MST-LINKAGE-CORE for more details.","487","    \"\"\"","488","    cdef:","489","        ITYPE_t n_samples = raw_data.shape[0]","490","        np.int8_t[:] in_tree = np.zeros(n_samples, dtype=np.int8)","491","        DTYPE_t[:, ::1] result = np.zeros((n_samples - 1, 3))","492","","493","        np.ndarray label_filter","494","","495","        ITYPE_t current_node = 0","496","        ITYPE_t new_node","497","        ITYPE_t i","498","        ITYPE_t j","499","        ITYPE_t num_features = raw_data.shape[1]","500","","501","        DTYPE_t right_value","502","        DTYPE_t left_value","503","        DTYPE_t new_distance","504","","505","        DTYPE_t[:] current_distances = np.full(n_samples, INFINITY)","506","","507","    for i in range(n_samples - 1):","508","","509","        in_tree[current_node] = 1","510","","511","        new_distance = INFINITY","512","        new_node = 0","513","","514","        for j in range(n_samples):","515","            if in_tree[j]:","516","                continue","517","","518","            right_value = current_distances[j]","519","            left_value = dist_metric.dist(&raw_data[current_node, 0],","520","                                          &raw_data[j, 0],","521","                                          num_features)","522","","523","            if left_value < right_value:","524","                current_distances[j] = left_value","525","","526","            if current_distances[j] < new_distance:","527","                new_distance = current_distances[j]","528","                new_node = j","529","","530","        result[i, 0] = current_node","531","        result[i, 1] = new_node","532","        result[i, 2] = new_distance","533","        current_node = new_node","534","","535","    return np.array(result)","536",""],"delete":[]}],"doc\/whats_new\/v0.22.rst":[{"add":["170","- |Enhancement| :class:`cluster.AgglomerativeClustering` has a faster and more","171","  more memory efficient implementation of single linkage clustering.","172","  :pr:`11514` by :user:`Leland McInnes <lmcinnes>`.","173",""],"delete":[]}],"sklearn\/cluster\/tests\/test_hierarchical.py":[{"add":["282","def test_sparse_scikit_vs_scipy():","316","# Make sure our custom mst_linkage_core gives","317","# the same results as scipy's builtin","318","@pytest.mark.parametrize('seed', range(5))","319","def test_vector_scikit_single_vs_scipy_single(seed):","320","    n_samples, n_features, n_clusters = 10, 5, 3","321","    rng = np.random.RandomState(seed)","322","    X = .1 * rng.normal(size=(n_samples, n_features))","323","    X -= 4. * np.arange(n_samples)[:, np.newaxis]","324","    X -= X.mean(axis=1)[:, np.newaxis]","325","","326","    out = hierarchy.linkage(X, method='single')","327","    children_scipy = out[:, :2].astype(np.int)","328","","329","    children, _, n_leaves, _ = _TREE_BUILDERS['single'](X)","330","","331","    # Sort the order of child nodes per row for consistency","332","    children.sort(axis=1)","333","    assert_array_equal(children, children_scipy,","334","                       'linkage tree differs'","335","                       ' from scipy impl for'","336","                       ' single linkage.')","337","","338","    cut = _hc_cut(n_clusters, children, n_leaves)","339","    cut_scipy = _hc_cut(n_clusters, children_scipy, n_leaves)","340","    assess_same_labelling(cut, cut_scipy)","341","","342",""],"delete":["282","def test_scikit_vs_scipy():"]}],"benchmarks\/bench_plot_hierarchical.py":[{"add":[],"delete":[]}]}},"2aba6e29458d9ab1dc609db8fde6569c314313bf":{"changes":{"sklearn\/linear_model\/tests\/test_huber.py":"MODIFY","doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/linear_model\/logistic.py":"MODIFY","sklearn\/linear_model\/huber.py":"MODIFY"},"diff":{"sklearn\/linear_model\/tests\/test_huber.py":[{"add":["44","def test_huber_max_iter():","45","    X, y = make_regression_with_outliers()","46","    huber = HuberRegressor(max_iter=1)","47","    huber.fit(X, y)","48","    assert huber.n_iter_ == huber.max_iter","49","","50",""],"delete":[]}],"doc\/whats_new\/v0.20.rst":[{"add":["394","- ``n_iter_`` may vary from previous releases in","395","  :class:`linear_model.LogisticRegression` with ``solver='lbfgs'`` and","396","  :class:`linear_model.HuberRegressor`.  For Scipy <= 1.0.0, the optimizer could","397","  perform more than the requested maximum number of iterations. Now both","398","  estimators will report at most ``max_iter`` iterations even if more were","399","  performed. :issue:`10723` by `Joel Nothman`_.","400",""],"delete":[]}],"sklearn\/linear_model\/logistic.py":[{"add":["712","            # In scipy <= 1.0.0, nit may exceed maxiter.","713","            # See https:\/\/github.com\/scipy\/scipy\/issues\/7854.","714","            n_iter_i = min(info['nit'], max_iter)","1113","        .. versionchanged:: 0.20","1114","","1115","            In SciPy <= 1.0.0 the number of lbfgs iterations may exceed","1116","            ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.","1117",""],"delete":["712","            n_iter_i = info['nit'] - 1"]}],"sklearn\/linear_model\/huber.py":[{"add":["184","","185","        .. versionchanged:: 0.20","186","","187","            In SciPy <= 1.0.0 the number of lbfgs iterations may exceed","188","            ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.","270","        # In scipy <= 1.0.0, nit may exceed maxiter.","271","        # See https:\/\/github.com\/scipy\/scipy\/issues\/7854.","272","        self.n_iter_ = min(dict_['nit'], self.max_iter)"],"delete":["184","        Not available if SciPy version is 0.9 and below.","266","        self.n_iter_ = dict_['nit']"]}]}},"61fa3155da5706a35a8fb28c76ccbc8aee41cb5b":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/preprocessing\/_encoders.py":"MODIFY","sklearn\/preprocessing\/tests\/test_encoders.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["304","- :class:`preprocessing.OneHotEncoder` now supports the","305","  :meth:`get_feature_names` method to obtain the transformed feature names.","306","  :issue:`10181` by  :user:`Nirvan Anjirbag <Nirvan101>` and","307","  `Joris Van den Bossche`_.","308",""],"delete":[]}],"sklearn\/preprocessing\/_encoders.py":[{"add":["242","    >>> enc.get_feature_names()","243","    array(['x0_Female', 'x0_Male', 'x1_1', 'x1_2', 'x1_3'], dtype=object)","643","    def get_feature_names(self, input_features=None):","644","        \"\"\"Return feature names for output features.","645","","646","        Parameters","647","        ----------","648","        input_features : list of string, length n_features, optional","649","            String names for input features if available. By default,","650","            \"x0\", \"x1\", ... \"xn_features\" is used.","651","","652","        Returns","653","        -------","654","        output_feature_names : array of string, length n_output_features","655","","656","        \"\"\"","657","        check_is_fitted(self, 'categories_')","658","        cats = self.categories_","659","        if input_features is None:","660","            input_features = ['x%d' % i for i in range(len(cats))]","661","        elif(len(input_features) != len(self.categories_)):","662","            raise ValueError(","663","                \"input_features should have length equal to number of \"","664","                \"features ({}), got {}\".format(len(self.categories_),","665","                                               len(input_features)))","666","","667","        feature_names = []","668","        for i in range(len(cats)):","669","            names = [","670","                input_features[i] + '_' + six.text_type(t) for t in cats[i]]","671","            feature_names.extend(names)","672","","673","        return np.array(feature_names, dtype=object)","674",""],"delete":[]}],"sklearn\/preprocessing\/tests\/test_encoders.py":[{"add":["0","# -*- coding: utf-8 -*-","458","def test_one_hot_encoder_feature_names():","459","    enc = OneHotEncoder()","460","    X = [['Male', 1, 'girl', 2, 3],","461","         ['Female', 41, 'girl', 1, 10],","462","         ['Male', 51, 'boy', 12, 3],","463","         ['Male', 91, 'girl', 21, 30]]","464","","465","    enc.fit(X)","466","    feature_names = enc.get_feature_names()","467","    assert isinstance(feature_names, np.ndarray)","468","","469","    assert_array_equal(['x0_Female', 'x0_Male',","470","                        'x1_1', 'x1_41', 'x1_51', 'x1_91',","471","                        'x2_boy', 'x2_girl',","472","                        'x3_1', 'x3_2', 'x3_12', 'x3_21',","473","                        'x4_3',","474","                        'x4_10', 'x4_30'], feature_names)","475","","476","    feature_names2 = enc.get_feature_names(['one', 'two',","477","                                            'three', 'four', 'five'])","478","","479","    assert_array_equal(['one_Female', 'one_Male',","480","                        'two_1', 'two_41', 'two_51', 'two_91',","481","                        'three_boy', 'three_girl',","482","                        'four_1', 'four_2', 'four_12', 'four_21',","483","                        'five_3', 'five_10', 'five_30'], feature_names2)","484","","485","    with pytest.raises(ValueError, match=\"input_features should have length\"):","486","        enc.get_feature_names(['one', 'two'])","487","","488","","489","def test_one_hot_encoder_feature_names_unicode():","490","    enc = OneHotEncoder()","491","    X = np.array([[u'c?t1', u'dat2']], dtype=object).T","492","    enc.fit(X)","493","    feature_names = enc.get_feature_names()","494","    assert_array_equal([u'x0_c?t1', u'x0_dat2'], feature_names)","495","    feature_names = enc.get_feature_names(input_features=[u'n?me'])","496","    assert_array_equal([u'n?me_c?t1', u'n?me_dat2'], feature_names)","497","","498",""],"delete":[]}]}},"6b1d8e5e1404853eece7bf56bde00d3683840a29":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/tests\/test_discriminant_analysis.py":"MODIFY","sklearn\/discriminant_analysis.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["56","  ","57",":mod:`sklearn.discriminant_analysis`","58","....................................","60","- |Fix| A ``ChangedBehaviourWarning`` is now raised when","61","  :class:`discriminant_analysis.LinearDiscriminantAnalysis` is given as","62","  parameter ``n_components > min(n_features, n_classes - 1)``, and","63","  ``n_components`` is changed to ``min(n_features, n_classes - 1)`` if so.","64","  Previously the change was made, but silently. :issue:`11526` by","65","  :user:`William de Vazelhes<wdevazelhes>`.","76",":mod:`sklearn.linear_model`","77","...........................","78","","79","- |Feature| :class:`linear_model.LogisticRegression` and","80","  :class:`linear_model.LogisticRegressionCV` now support Elastic-Net penalty,","81","  with the 'saga' solver. :issue:`11646` by :user:`Nicolas Hug <NicolasHug>`.","82","","83","- |Fix| Fixed a bug in the 'saga' solver where the weights would not be","84","  correctly updated in some cases. :issue:`11646` by `Tom Dupre la Tour`_.","85","","86","- |Fix| Fixed a bug in :class:`linear_model.MultiTaskElasticNet` and","87","  :class:`linear_model.MultiTaskLasso` which were breaking when","88","  ``warm_start = True``. :issue:`12360` by :user:`Aakanksha Joshi <joaak>`.","89",""],"delete":["57",":mod:`sklearn.linear_model`","58","...........................","59","","60","- |Fix| Fixed a bug in :class:`linear_model.MultiTaskElasticNet` and","61","  :class:`linear_model.MultiTaskLasso` which were breaking when","62","  ``warm_start = True``. :issue:`12360` by :user:`Aakanksha Joshi <joaak>`.","154",":mod:`sklearn.linear_model`","155","...........................","156","","157","- |Feature| :class:`linear_model.LogisticRegression` and","158","  :class:`linear_model.LogisticRegressionCV` now support Elastic-Net penalty,","159","  with the 'saga' solver. :issue:`11646` by :user:`Nicolas Hug <NicolasHug>`.","160","","161","- |Fix| Fixed a bug in the 'saga' solver where the weights would not be","162","  correctly updated in some cases. :issue:`11646` by `Tom Dupre la Tour`_."]}],"sklearn\/tests\/test_discriminant_analysis.py":[{"add":["4","from sklearn.exceptions import ChangedBehaviorWarning","5","from sklearn.utils import check_random_state","6","from sklearn.utils.testing import (assert_array_equal, assert_no_warnings,","7","                                   assert_warns_message)","262","@pytest.mark.parametrize('n_features', [3, 5])","263","@pytest.mark.parametrize('n_classes', [5, 3])","264","def test_lda_dimension_warning(n_classes, n_features):","265","    # FIXME: Future warning to be removed in 0.23","266","    rng = check_random_state(0)","267","    n_samples = 10","268","    X = rng.randn(n_samples, n_features)","269","    # we create n_classes labels by repeating and truncating a","270","    # range(n_classes) until n_samples","271","    y = np.tile(range(n_classes), n_samples \/\/ n_classes + 1)[:n_samples]","272","    max_components = min(n_features, n_classes - 1)","273","","274","    for n_components in [max_components - 1, None, max_components]:","275","        # if n_components <= min(n_classes - 1, n_features), no warning","276","        lda = LinearDiscriminantAnalysis(n_components=n_components)","277","        assert_no_warnings(lda.fit, X, y)","278","","279","    for n_components in [max_components + 1,","280","                         max(n_features, n_classes - 1) + 1]:","281","        # if n_components > min(n_classes - 1, n_features), raise warning","282","        # We test one unit higher than max_components, and then something","283","        # larger than both n_features and n_classes - 1 to ensure the test","284","        # works for any value of n_component","285","        lda = LinearDiscriminantAnalysis(n_components=n_components)","286","        msg = (\"n_components cannot be larger than min(n_features, \"","287","               \"n_classes - 1). Using min(n_features, \"","288","               \"n_classes - 1) = min(%d, %d - 1) = %d components.\" %","289","               (n_features, n_classes, max_components))","290","        assert_warns_message(ChangedBehaviorWarning, msg, lda.fit, X, y)","291","        future_msg = (\"In version 0.23, setting n_components > min(\"","292","                      \"n_features, n_classes - 1) will raise a \"","293","                      \"ValueError. You should set n_components to None\"","294","                      \" (default), or a value smaller or equal to \"","295","                      \"min(n_features, n_classes - 1).\")","296","        assert_warns_message(FutureWarning, future_msg, lda.fit, X, y)","297","","298",""],"delete":[]}],"sklearn\/discriminant_analysis.py":[{"add":["14","from .exceptions import ChangedBehaviorWarning","168","    n_components : int, optional (default=None)","169","        Number of components (<= min(n_classes - 1, n_features)) for","170","        dimensionality reduction. If None, will be set to","171","        min(n_classes - 1, n_features).","430","        # FIXME: Future warning to be removed in 0.23","453","        # Maximum number of components no matter what n_components is","454","        # specified:","455","        max_components = min(len(self.classes_) - 1, X.shape[1])","456","","458","            self._max_components = max_components","460","            if self.n_components > max_components:","461","                warnings.warn(","462","                    \"n_components cannot be larger than min(n_features, \"","463","                    \"n_classes - 1). Using min(n_features, \"","464","                    \"n_classes - 1) = min(%d, %d - 1) = %d components.\"","465","                    % (X.shape[1], len(self.classes_), max_components),","466","                    ChangedBehaviorWarning)","467","                future_msg = (\"In version 0.23, setting n_components > min(\"","468","                              \"n_features, n_classes - 1) will raise a \"","469","                              \"ValueError. You should set n_components to None\"","470","                              \" (default), or a value smaller or equal to \"","471","                              \"min(n_features, n_classes - 1).\")","472","                warnings.warn(future_msg, FutureWarning)","473","                self._max_components = max_components","474","            else:","475","                self._max_components = self.n_components"],"delete":["167","    n_components : int, optional","168","        Number of components (< n_classes - 1) for dimensionality reduction.","449","        # Get the maximum number of components","451","            self._max_components = len(self.classes_) - 1","453","            self._max_components = min(len(self.classes_) - 1,","454","                                       self.n_components)"]}]}},"28a2ad265f4063db0aaa2ec463da73a32042ae05":{"changes":{"doc\/modules\/model_evaluation.rst":"MODIFY"},"diff":{"doc\/modules\/model_evaluation.rst":[{"add":["176","    >>> def my_custom_loss_func(y_true, y_pred):","177","    ...     diff = np.abs(y_true - y_pred).max()","180","    >>> # score will negate the return value of my_custom_loss_func,","181","    >>> # which will be np.log(2), 0.693, given the values for X","182","    >>> # and y defined below.","183","    >>> score = make_scorer(my_custom_loss_func, greater_is_better=False)","184","    >>> X = [[1], [1]]","185","    >>> y  = [0, 1]","188","    >>> clf = clf.fit(X, y)","189","    >>> my_custom_loss_func(clf.predict(X), y) # doctest: +ELLIPSIS","191","    >>> score(clf, X, y) # doctest: +ELLIPSIS","192","    -0.69..."],"delete":["176","    >>> def my_custom_loss_func(ground_truth, predictions):","177","    ...     diff = np.abs(ground_truth - predictions).max()","180","    >>> # loss_func will negate the return value of my_custom_loss_func,","181","    >>> #  which will be np.log(2), 0.693, given the values for ground_truth","182","    >>> #  and predictions defined below.","183","    >>> loss  = make_scorer(my_custom_loss_func, greater_is_better=False)","184","    >>> score = make_scorer(my_custom_loss_func, greater_is_better=True)","185","    >>> ground_truth = [[1], [1]]","186","    >>> predictions  = [0, 1]","189","    >>> clf = clf.fit(ground_truth, predictions)","190","    >>> loss(clf,ground_truth, predictions) # doctest: +ELLIPSIS","191","    -0.69...","192","    >>> score(clf,ground_truth, predictions) # doctest: +ELLIPSIS"]}]}},"6ade12a55b27f66e41754898998c9ce12c5271c2":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/tree\/_criterion.pyx":"MODIFY","sklearn\/tree\/tests\/test_tree.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["549","  ","550","- Fixed a bug in :class:`tree.MAE` to ensure sample weights are being used ","551","  during the calculation of tree MAE impurity. Previous behaviour could ","552","  cause suboptimal splits to be chosen since the impurity calculation ","553","  considered all samples to be of equal weight importance.","554","  :issue:`11464` by :user:`John Stott <JohnStott>`.  "],"delete":[]}],"sklearn\/tree\/_criterion.pyx":[{"add":["1240","        cdef DOUBLE_t w = 1.0","1241","        cdef DOUBLE_t impurity = 0.0","1249","                if sample_weight != NULL:","1250","                    w = sample_weight[i]","1251","","1252","                impurity += fabs(y_ik - self.node_medians[k]) * w","1253","","1256","    cdef void children_impurity(self, double* p_impurity_left,","1257","                                double* p_impurity_right) nogil:","1274","        cdef DOUBLE_t w = 1.0","1275","        cdef DOUBLE_t impurity_left = 0.0","1276","        cdef DOUBLE_t impurity_right = 0.0","1288","                if sample_weight != NULL:","1289","                    w = sample_weight[i]","1290","","1291","                impurity_left += fabs(y_ik - median) * w","1292","        p_impurity_left[0] = impurity_left \/ (self.weighted_n_left * ","1293","                                              self.n_outputs)","1302","                if sample_weight != NULL:","1303","                    w = sample_weight[i]","1304","","1305","                impurity_right += fabs(y_ik - median) * w","1306","        p_impurity_right[0] = impurity_right \/ (self.weighted_n_right * ","1307","                                                self.n_outputs)"],"delete":["1240","        cdef DOUBLE_t w_y_ik","1241","","1242","        cdef double impurity = 0.0","1250","                impurity += <double> fabs((<double> y_ik) - <double> self.node_medians[k])","1253","    cdef void children_impurity(self, double* impurity_left,","1254","                                double* impurity_right) nogil:","1275","        impurity_left[0] = 0.0","1276","        impurity_right[0] = 0.0","1277","","1285","                impurity_left[0] += <double>fabs((<double> y_ik) -","1286","                                                 <double> median)","1287","        impurity_left[0] \/= <double>((self.weighted_n_left) * self.n_outputs)","1296","                impurity_right[0] += <double>fabs((<double> y_ik) -","1297","                                                  <double> median)","1298","        impurity_right[0] \/= <double>((self.weighted_n_right) *","1299","                                      self.n_outputs)"]}],"sklearn\/tree\/tests\/test_tree.py":[{"add":["20","from sklearn.utils.testing import assert_allclose","1696","    \"\"\"Check MAE criterion produces correct results on small toy dataset:","1697","","1698","    ------------------","1699","    | X | y | weight |","1700","    ------------------","1701","    | 3 | 3 |  0.1   |","1702","    | 5 | 3 |  0.3   |","1703","    | 8 | 4 |  1.0   |","1704","    | 3 | 6 |  0.6   |","1705","    | 5 | 7 |  0.3   |","1706","    ------------------","1707","    |sum wt:|  2.3   |","1708","    ------------------","1709","","1710","    Because we are dealing with sample weights, we cannot find the median by","1711","    simply choosing\/averaging the centre value(s), instead we consider the","1712","    median where 50% of the cumulative weight is found (in a y sorted data set)","1713","    . Therefore with regards to this test data, the cumulative weight is >= 50%","1714","    when y = 4.  Therefore:","1715","    Median = 4","1716","","1717","    For all the samples, we can get the total error by summing:","1718","    Absolute(Median - y) * weight","1719","","1720","    I.e., total error = (Absolute(4 - 3) * 0.1)","1721","                      + (Absolute(4 - 3) * 0.3)","1722","                      + (Absolute(4 - 4) * 1.0)","1723","                      + (Absolute(4 - 6) * 0.6)","1724","                      + (Absolute(4 - 7) * 0.3)","1725","                      = 2.5","1726","","1727","    Impurity = Total error \/ total weight","1728","             = 2.5 \/ 2.3","1729","             = 1.08695652173913","1730","             ------------------","1731","","1732","    From this root node, the next best split is between X values of 3 and 5.","1733","    Thus, we have left and right child nodes:","1734","","1735","    LEFT                    RIGHT","1736","    ------------------      ------------------","1737","    | X | y | weight |      | X | y | weight |","1738","    ------------------      ------------------","1739","    | 3 | 3 |  0.1   |      | 5 | 3 |  0.3   |","1740","    | 3 | 6 |  0.6   |      | 8 | 4 |  1.0   |","1741","    ------------------      | 5 | 7 |  0.3   |","1742","    |sum wt:|  0.7   |      ------------------","1743","    ------------------      |sum wt:|  1.6   |","1744","                            ------------------","1745","","1746","    Impurity is found in the same way:","1747","    Left node Median = 6","1748","    Total error = (Absolute(6 - 3) * 0.1)","1749","                + (Absolute(6 - 6) * 0.6)","1750","                = 0.3","1751","","1752","    Left Impurity = Total error \/ total weight","1753","            = 0.3 \/ 0.7","1754","            = 0.428571428571429","1755","            -------------------","1756","","1757","    Likewise for Right node:","1758","    Right node Median = 4","1759","    Total error = (Absolute(4 - 3) * 0.3)","1760","                + (Absolute(4 - 4) * 1.0)","1761","                + (Absolute(4 - 7) * 0.3)","1762","                = 1.2","1763","","1764","    Right Impurity = Total error \/ total weight","1765","            = 1.2 \/ 1.6","1766","            = 0.75","1767","            ------","1768","    \"\"\"","1771","","1772","    # Test MAE where sample weights are non-uniform (as illustrated above):","1773","    dt_mae.fit(X=[[3], [5], [3], [8], [5]], y=[6, 7, 3, 4, 3],","1774","               sample_weight=[0.6, 0.3, 0.1, 1.0, 0.3])","1775","    assert_allclose(dt_mae.tree_.impurity, [2.5 \/ 2.3, 0.3 \/ 0.7, 1.2 \/ 1.6])","1776","    assert_array_equal(dt_mae.tree_.value.flat, [4.0, 6.0, 4.0])","1777","","1778","    # Test MAE where all sample weights are uniform:","1779","    dt_mae.fit(X=[[3], [5], [3], [8], [5]], y=[6, 7, 3, 4, 3],","1780","               sample_weight=np.ones(5))","1781","    assert_array_equal(dt_mae.tree_.impurity, [1.4, 1.5, 4.0 \/ 3.0])","1784","    # Test MAE where a `sample_weight` is not explicitly provided.","1785","    # This is equivalent to providing uniform sample weights, though","1786","    # the internal logic is different:","1787","    dt_mae.fit(X=[[3], [5], [3], [8], [5]], y=[6, 7, 3, 4, 3])","1788","    assert_array_equal(dt_mae.tree_.impurity, [1.4, 1.5, 4.0 \/ 3.0])","1789","    assert_array_equal(dt_mae.tree_.value.flat, [4, 4.5, 4.0])"],"delete":["1695","    # check MAE criterion produces correct results","1696","    # on small toy dataset","1699","    dt_mae.fit([[3], [5], [3], [8], [5]], [6, 7, 3, 4, 3])","1700","    assert_array_equal(dt_mae.tree_.impurity, [1.4, 1.5, 4.0\/3.0])","1703","    dt_mae.fit([[3], [5], [3], [8], [5]], [6, 7, 3, 4, 3],","1704","               [0.6, 0.3, 0.1, 1.0, 0.3])","1705","    assert_array_equal(dt_mae.tree_.impurity, [7.0\/2.3, 3.0\/0.7, 4.0\/1.6])","1706","    assert_array_equal(dt_mae.tree_.value.flat, [4.0, 6.0, 4.0])"]}]}},"03dd2870157490f1f8f484d9b81bfe0ac1df4917":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/linear_model\/tests\/test_ridge.py":"MODIFY","sklearn\/linear_model\/ridge.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["279","- Fixed a bug in :class:`linear_model.RidgeCV` where using integer ``alphas``","280","  raised an error. :issue:`10393` by :user:`Mabel Villalba-Jim¨¦nez <mabelvj>`.","281","","476","  :issue:`10420` by :user:`Jonathan Ohayon <Johayon>`"],"delete":["473","  :issue:`10420` by :user:`Jonathan Ohayon <Johayon>`"]}],"sklearn\/linear_model\/tests\/test_ridge.py":[{"add":["13","from sklearn.utils.testing import assert_raises_regex","54","","708","def test_ridgecv_int_alphas():","709","    X = np.array([[-1.0, -1.0], [-1.0, 0], [-.8, -1.0],","710","                  [1.0, 1.0], [1.0, 0.0]])","711","    y = [1, 1, 1, -1, -1]","712","","713","    # Integers","714","    ridge = RidgeCV(alphas=(1, 10, 100))","715","    ridge.fit(X, y)","716","","717","","718","def test_ridgecv_negative_alphas():","719","    X = np.array([[-1.0, -1.0], [-1.0, 0], [-.8, -1.0],","720","                  [1.0, 1.0], [1.0, 0.0]])","721","    y = [1, 1, 1, -1, -1]","722","","723","    # Negative integers","724","    ridge = RidgeCV(alphas=(-1, -10, -100))","725","    assert_raises_regex(ValueError,","726","                        \"alphas cannot be negative.\",","727","                        ridge.fit, X, y)","728","","729","    # Negative floats","730","    ridge = RidgeCV(alphas=(-0.1, -1.0, -10.0))","731","    assert_raises_regex(ValueError,","732","                        \"alphas cannot be negative.\",","733","                        ridge.fit, X, y)","734","","735",""],"delete":[]}],"sklearn\/linear_model\/ridge.py":[{"add":["780","","1044","        if np.any(self.alphas < 0):","1045","            raise ValueError(\"alphas cannot be negative. \"","1046","                             \"Got {} containing some \"","1047","                             \"negative value instead.\".format(self.alphas))","1048","","1051","                out, c = _errors(float(alpha), y, v, Q, QT_y)","1053","                out, c = _values(float(alpha), y, v, Q, QT_y)","1093","        self.alphas = np.asarray(alphas)","1336",""],"delete":["1045","                out, c = _errors(alpha, y, v, Q, QT_y)","1047","                out, c = _values(alpha, y, v, Q, QT_y)","1087","        self.alphas = alphas"]}]}},"60b0cf8a2b59d17835351573d20d39888b191b91":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/metrics\/classification.py":"MODIFY","sklearn\/metrics\/tests\/test_classification.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["258","- Fixed a bug in :func:`metrics.precision_precision_recall_fscore_support`","259","  when truncated `range(n_labels)` is passed as value for `labels`.","260","  :issue:`10377` by :user:`Gaurav Dhingra <gxyd>`.","261",""],"delete":[]}],"sklearn\/metrics\/classification.py":[{"add":["1074","        if n_labels is not None:"],"delete":[]}],"sklearn\/metrics\/tests\/test_classification.py":[{"add":["199","    # tests non-regression on issue #10307","200","    y_true = np.array([[0, 1, 1], [1, 0, 0]])","201","    y_pred = np.array([[1, 1, 1], [1, 0, 1]])","202","    p, r, f, _ = precision_recall_fscore_support(y_true, y_pred,","203","                                                 average='samples',","204","                                                 labels=[0, 1])","205","    assert_almost_equal(np.array([p, r, f]), np.array([3 \/ 4, 1, 5 \/ 6]))","206",""],"delete":[]}]}},"9cc59e70907e3646ae09ecefefb71a7352674e80":{"changes":{"sklearn\/utils\/metaestimators.py":"MODIFY"},"diff":{"sklearn\/utils\/metaestimators.py":[{"add":["47","        # 3. Step parameters and other initialisation arguments"],"delete":["47","        # 3. Step parameters and other initilisation arguments"]}]}},"5da8869d3f2036ac7f6cadf3e05c69e9f5140cc2":{"changes":{"sklearn\/src\/cblas\/atlas_misc.h":"MODIFY"},"diff":{"sklearn\/src\/cblas\/atlas_misc.h":[{"add":["186","   #define UPR q"],"delete":["186","   #deffine UPR q"]}]}},"4bead39f7899cbd08ab08959a0311843c84f69df":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/datasets\/samples_generator.py":"MODIFY","sklearn\/datasets\/tests\/test_samples_generator.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["158","- Fixed a bug in :func:`datasets.make_circles`, where no odd number of data ","159","  points could be generated. :issue:`10037` by :user:`Christian Braune ","160","  <christianbraune79>`_.","161","  "],"delete":[]}],"sklearn\/datasets\/samples_generator.py":[{"add":["587","        The total number of points generated. If odd, the inner circle will","588","        have one point more than the outer circle.","602","    factor : 0 < double < 1 (default=.8)","614","    if factor >= 1 or factor < 0:","617","    n_samples_out = n_samples \/\/ 2","618","    n_samples_in = n_samples - n_samples_out","619","","621","    # so as not to have the first point = last point, we set endpoint=False","622","    linspace_out = np.linspace(0, 2 * np.pi, n_samples_out, endpoint=False)","623","    linspace_in = np.linspace(0, 2 * np.pi, n_samples_in, endpoint=False)","624","    outer_circ_x = np.cos(linspace_out)","625","    outer_circ_y = np.sin(linspace_out)","626","    inner_circ_x = np.cos(linspace_in) * factor","627","    inner_circ_y = np.sin(linspace_in) * factor","631","    y = np.hstack([np.zeros(n_samples_out, dtype=np.intp),","632","                   np.ones(n_samples_in, dtype=np.intp)])"],"delete":["587","        The total number of points generated.","601","    factor : double < 1 (default=.8)","613","    if factor > 1 or factor < 0:","617","    # so as not to have the first point = last point, we add one and then","618","    # remove it.","619","    linspace = np.linspace(0, 2 * np.pi, n_samples \/\/ 2 + 1)[:-1]","620","    outer_circ_x = np.cos(linspace)","621","    outer_circ_y = np.sin(linspace)","622","    inner_circ_x = outer_circ_x * factor","623","    inner_circ_y = outer_circ_y * factor","627","    y = np.hstack([np.zeros(n_samples \/\/ 2, dtype=np.intp),","628","                   np.ones(n_samples \/\/ 2, dtype=np.intp)])"]}],"sklearn\/datasets\/tests\/test_samples_generator.py":[{"add":["27","from sklearn.datasets import make_circles","388","","389","","390","def test_make_circles():","391","    factor = 0.3","392","","393","    for (n_samples, n_outer, n_inner) in [(7, 3, 4), (8, 4, 4)]:","394","        # Testing odd and even case, because in the past make_circles always","395","        # created an even number of samples.","396","        X, y = make_circles(n_samples, shuffle=False, noise=None,","397","                            factor=factor)","398","        assert_equal(X.shape, (n_samples, 2), \"X shape mismatch\")","399","        assert_equal(y.shape, (n_samples,), \"y shape mismatch\")","400","        center = [0.0, 0.0]","401","        for x, label in zip(X, y):","402","            dist_sqr = ((x - center) ** 2).sum()","403","            dist_exp = 1.0 if label == 0 else factor**2","404","            assert_almost_equal(dist_sqr, dist_exp,","405","                                err_msg=\"Point is not on expected circle\")","406","","407","        assert_equal(X[y == 0].shape, (n_outer, 2),","408","                     \"Samples not correctly distributed across circles.\")","409","        assert_equal(X[y == 1].shape, (n_inner, 2),","410","                     \"Samples not correctly distributed across circles.\")","411","","412","    assert_raises(ValueError, make_circles, factor=-0.01)","413","    assert_raises(ValueError, make_circles, factor=1.)"],"delete":[]}]}},"12c5ce25e885cded5d0d2beaae7589b878ed8672":{"changes":{"sklearn\/tree\/_splitter.pyx":"MODIFY"},"diff":{"sklearn\/tree\/_splitter.pyx":[{"add":["537","    if n == 0:","538","      return"],"delete":[]}]}},"f158e2dfe2af1b23ae3f9d86c598013b2c155c3f":{"changes":{"sklearn\/linear_model\/tests\/test_coordinate_descent.py":"MODIFY","sklearn\/tests\/test_impute.py":"MODIFY","sklearn\/linear_model\/tests\/test_logistic.py":"MODIFY","sklearn\/neighbors\/tests\/test_neighbors.py":"MODIFY","examples\/compose\/plot_compare_reduction.py":"MODIFY","sklearn\/model_selection\/tests\/test_search.py":"MODIFY","sklearn\/tests\/test_multiclass.py":"MODIFY","sklearn\/covariance\/graph_lasso_.py":"MODIFY","sklearn\/linear_model\/coordinate_descent.py":"MODIFY","sklearn\/model_selection\/tests\/test_split.py":"MODIFY","doc\/modules\/cross_validation.rst":"MODIFY","sklearn\/linear_model\/tests\/test_ridge.py":"MODIFY","doc\/modules\/ensemble.rst":"MODIFY","sklearn\/tests\/test_calibration.py":"MODIFY","sklearn\/feature_selection\/tests\/test_rfe.py":"MODIFY","sklearn\/feature_extraction\/tests\/test_text.py":"MODIFY","sklearn\/tests\/test_metaestimators.py":"MODIFY","sklearn\/calibration.py":"MODIFY","examples\/ensemble\/plot_gradient_boosting_oob.py":"MODIFY","doc\/modules\/learning_curve.rst":"MODIFY","doc\/modules\/model_evaluation.rst":"MODIFY","doc\/glossary.rst":"MODIFY","sklearn\/model_selection\/_search.py":"MODIFY","doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/feature_selection\/rfe.py":"MODIFY","sklearn\/linear_model\/logistic.py":"MODIFY","sklearn\/ensemble\/tests\/test_forest.py":"MODIFY","doc\/tutorial\/statistical_inference\/model_selection.rst":"MODIFY","sklearn\/covariance\/tests\/test_graph_lasso.py":"MODIFY","sklearn\/covariance\/tests\/test_graphical_lasso.py":"MODIFY","sklearn\/linear_model\/omp.py":"MODIFY","sklearn\/ensemble\/tests\/test_weight_boosting.py":"MODIFY","doc\/modules\/linear_model.rst":"MODIFY","sklearn\/metrics\/tests\/test_score_objects.py":"MODIFY","sklearn\/model_selection\/_split.py":"MODIFY","sklearn\/ensemble\/tests\/test_bagging.py":"MODIFY","sklearn\/model_selection\/_validation.py":"MODIFY","sklearn\/linear_model\/tests\/test_least_angle.py":"MODIFY","sklearn\/model_selection\/tests\/test_validation.py":"MODIFY","sklearn\/linear_model\/least_angle.py":"MODIFY"},"diff":{"sklearn\/linear_model\/tests\/test_coordinate_descent.py":[{"add":["148","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","235","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","293","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","365","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","460","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","487","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","501","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","514","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","532","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22"],"delete":[]}],"sklearn\/tests\/test_impute.py":[{"add":["436","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22"],"delete":[]}],"sklearn\/linear_model\/tests\/test_logistic.py":[{"add":["469","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","496","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","530","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","570","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","734","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","774","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","890","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","984","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","993","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","1086","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22"],"delete":[]}],"sklearn\/neighbors\/tests\/test_neighbors.py":[{"add":["183","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22"],"delete":[]}],"examples\/compose\/plot_compare_reduction.py":[{"add":["65","grid = GridSearchCV(pipe, cv=5, n_jobs=1, param_grid=param_grid)","116","grid = GridSearchCV(cached_pipe, cv=5, n_jobs=1, param_grid=param_grid)"],"delete":["65","grid = GridSearchCV(pipe, cv=3, n_jobs=1, param_grid=param_grid)","116","grid = GridSearchCV(cached_pipe, cv=3, n_jobs=1, param_grid=param_grid)"]}],"sklearn\/model_selection\/tests\/test_search.py":[{"add":["181","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","182","","253","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","316","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","347","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","382","    estimators = [GridSearchCV(LinearSVC(random_state=0), grid,","383","                               iid=False, cv=3),","385","                                     n_iter=2, iid=False, cv=3)]","414","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","443","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","457","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","489","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","500","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","516","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","530","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","561","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","584","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","621","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","651","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","690","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","761","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","789","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","1143","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","1176","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","1254","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","1284","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","1303","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","1437","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","1490","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","1522","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","1616","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22"],"delete":["377","    estimators = [GridSearchCV(LinearSVC(random_state=0), grid, iid=False),","379","                                     n_iter=2, iid=False)]"]}],"sklearn\/tests\/test_multiclass.py":[{"add":["334","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","428","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","606","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","702","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","753","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22"],"delete":[]}],"sklearn\/covariance\/graph_lasso_.py":[{"add":["499","        .. versionchanged:: 0.20","500","            ``cv`` default value if None will change from 3-fold to 5-fold","501","            in v0.22.","502","","572","    def __init__(self, alphas=4, n_refinements=4, cv='warn', tol=1e-4,","906","        .. versionchanged:: 0.20","907","            ``cv`` default value if None will change from 3-fold to 5-fold","908","            in v0.22.","909",""],"delete":["568","    def __init__(self, alphas=4, n_refinements=4, cv=None, tol=1e-4,"]}],"sklearn\/linear_model\/coordinate_descent.py":[{"add":["1054","                 copy_X=True, cv='warn', verbose=False, n_jobs=1,","1313","        .. versionchanged:: 0.20","1314","            ``cv`` default value if None will change from 3-fold to 5-fold","1315","            in v0.22.","1316","","1387","                 copy_X=True, cv='warn', verbose=False, n_jobs=1,","1470","        .. versionchanged:: 0.20","1471","            ``cv`` default value if None will change from 3-fold to 5-fold","1472","            in v0.22.","1473","","1583","                 max_iter=1000, tol=1e-4, cv='warn', copy_X=True,","2004","        .. versionchanged:: 0.20","2005","            ``cv`` default value if None will change from 3-fold to 5-fold","2006","            in v0.22.","2007","","2062","    >>> clf = linear_model.MultiTaskElasticNetCV(cv=3)","2066","    MultiTaskElasticNetCV(alphas=None, copy_X=True, cv=3, eps=0.001,","2093","                 max_iter=1000, tol=1e-4, cv='warn', copy_X=True,","2178","        .. versionchanged:: 0.20","2179","            ``cv`` default value if None will change from 3-fold to 5-fold","2180","            in v0.22.","2181","","2243","                 cv='warn', verbose=False, n_jobs=1, random_state=None,"],"delete":["1054","                 copy_X=True, cv=None, verbose=False, n_jobs=1,","1383","                 copy_X=True, cv=None, verbose=False, n_jobs=1,","1575","                 max_iter=1000, tol=1e-4, cv=None, copy_X=True,","2050","    >>> clf = linear_model.MultiTaskElasticNetCV()","2054","    MultiTaskElasticNetCV(alphas=None, copy_X=True, cv=None, eps=0.001,","2081","                 max_iter=1000, tol=1e-4, cv=None, copy_X=True,","2227","                 cv=None, verbose=False, n_jobs=1, random_state=None,"]}],"sklearn\/model_selection\/tests\/test_split.py":[{"add":["3","import pytest","25","from sklearn.utils.testing import assert_no_warnings","53","from sklearn.model_selection._split import CV_WARNING","54","from sklearn.model_selection._split import NSPLIT_WARNING","204","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","1404","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","1430","def test_nsplit_default_warn():","1431","    # Test that warnings are raised. Will be removed in 0.22","1432","    assert_warns_message(FutureWarning, NSPLIT_WARNING, KFold)","1433","    assert_warns_message(FutureWarning, NSPLIT_WARNING, GroupKFold)","1434","    assert_warns_message(FutureWarning, NSPLIT_WARNING, StratifiedKFold)","1435","    assert_warns_message(FutureWarning, NSPLIT_WARNING, TimeSeriesSplit)","1436","","1437","    assert_no_warnings(KFold, n_splits=5)","1438","    assert_no_warnings(GroupKFold, n_splits=5)","1439","    assert_no_warnings(StratifiedKFold, n_splits=5)","1440","    assert_no_warnings(TimeSeriesSplit, n_splits=5)","1441","","1442","","1443","def test_check_cv_default_warn():","1444","    # Test that warnings are raised. Will be removed in 0.22","1445","    assert_warns_message(FutureWarning, CV_WARNING, check_cv)","1446","","1447","    assert_no_warnings(check_cv, cv=5)","1448","","1449",""],"delete":["3",""]}],"doc\/modules\/cross_validation.rst":[{"add":["140","  >>> cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)","141","  >>> cross_val_score(clf, iris.data, iris.target, cv=cv)  # doctest: +ELLIPSIS","142","  array([0.977..., 0.977..., 1.  ..., 0.955..., 1.        ])","168","      array([0.977..., 0.933..., 0.955..., 0.933..., 0.977...])","230","    ...                         scoring='precision_macro', cv=5,","462","  >>> X = np.arange(10)","463","  >>> ss = ShuffleSplit(n_splits=5, test_size=0.25,","467","  [9 1 6 7 3 0 5] [2 8 4]","468","  [2 9 8 0 6 7 4] [3 5 1]","469","  [4 5 1 0 6 9 7] [2 3 8]","470","  [2 7 5 8 0 3 4] [6 1 9]","471","  [4 1 0 6 8 9 3] [5 2 7]"],"delete":["140","  >>> cv = ShuffleSplit(n_splits=3, test_size=0.3, random_state=0)","141","  >>> cross_val_score(clf, iris.data, iris.target, cv=cv)","142","  ...                                                     # doctest: +ELLIPSIS","143","  array([0.97..., 0.97..., 1.        ])","144","","170","      array([0.97..., 0.93..., 0.95...])","232","    ...                         scoring='precision_macro',","464","  >>> X = np.arange(5)","465","  >>> ss = ShuffleSplit(n_splits=3, test_size=0.25,","469","  ...","470","  [1 3 4] [2 0]","471","  [1 4 3] [0 2]","472","  [4 0 2] [1 3]"]}],"sklearn\/linear_model\/tests\/test_ridge.py":[{"add":["492","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","551","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","581","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","598","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","608","    r = RidgeCV(alphas=alphas, cv=None, store_cv_values=True)","622","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","632","    r = RidgeClassifierCV(alphas=alphas, cv=None, store_cv_values=True)","743","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","754","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22"],"delete":["604","    r = RidgeCV(alphas=alphas, store_cv_values=True)","627","    r = RidgeClassifierCV(alphas=alphas, store_cv_values=True)"]}],"doc\/modules\/ensemble.rst":[{"add":["169","    >>> scores = cross_val_score(clf, X, y, cv=5)","170","    >>> scores.mean()                               # doctest: +ELLIPSIS","171","    0.98...","175","    >>> scores = cross_val_score(clf, X, y, cv=5)","176","    >>> scores.mean()                               # doctest: +ELLIPSIS","181","    >>> scores = cross_val_score(clf, X, y, cv=5)","259","the top of the tree contribute to the final prediction decision of a","260","larger fraction of the input samples. The **expected fraction of the","375","    >>> scores = cross_val_score(clf, iris.data, iris.target, cv=5)"],"delete":["169","    >>> scores = cross_val_score(clf, X, y)","170","    >>> scores.mean()                             # doctest: +ELLIPSIS","171","    0.97...","175","    >>> scores = cross_val_score(clf, X, y)","176","    >>> scores.mean()                             # doctest: +ELLIPSIS","181","    >>> scores = cross_val_score(clf, X, y)","259","the top of the tree contribute to the final prediction decision of a ","260","larger fraction of the input samples. The **expected fraction of the ","375","    >>> scores = cross_val_score(clf, iris.data, iris.target)"]}],"sklearn\/tests\/test_calibration.py":[{"add":["28","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","104","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22"],"delete":[]}],"sklearn\/feature_selection\/tests\/test_rfe.py":[{"add":["231","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","323","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22"],"delete":[]}],"sklearn\/feature_extraction\/tests\/test_text.py":[{"add":["734","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","772","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22"],"delete":[]}],"sklearn\/tests\/test_metaestimators.py":[{"add":["1","import pytest","49","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22"],"delete":["1",""]}],"sklearn\/calibration.py":[{"add":["77","        .. versionchanged:: 0.20","78","            ``cv`` default value if None will change from 3-fold to 5-fold","79","            in v0.22.","80","","105","    def __init__(self, base_estimator=None, method='sigmoid', cv='warn'):"],"delete":["101","    def __init__(self, base_estimator=None, method='sigmoid', cv=3):"]}],"examples\/ensemble\/plot_gradient_boosting_oob.py":[{"add":["76","def cv_estimate(n_splits=None):"],"delete":["76","def cv_estimate(n_splits=3):"]}],"doc\/modules\/learning_curve.rst":[{"add":["83","  ...                                               np.logspace(-7, 3, 3),","84","  ...                                               cv=5)","85","  >>> train_scores            # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE","86","  array([[0.93..., 0.94..., 0.92..., 0.91..., 0.92...],","87","         [0.93..., 0.94..., 0.92..., 0.91..., 0.92...],","88","         [0.51..., 0.52..., 0.49..., 0.47..., 0.49...]])","90","  array([[0.90..., 0.84..., 0.94..., 0.96..., 0.93...],","91","         [0.90..., 0.84..., 0.94..., 0.96..., 0.93...],","92","         [0.46..., 0.25..., 0.50..., 0.49..., 0.52...]])"],"delete":["83","  ...                                               np.logspace(-7, 3, 3))","84","  >>> train_scores           # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE","85","  array([[0.94..., 0.92..., 0.92...],","86","         [0.94..., 0.92..., 0.92...],","87","         [0.47..., 0.45..., 0.42...]])","89","  array([[0.90..., 0.92..., 0.94...],","90","         [0.90..., 0.92..., 0.94...],","91","         [0.44..., 0.39..., 0.45...]])"]}],"doc\/modules\/model_evaluation.rst":[{"add":["101","    >>> cross_val_score(clf, X, y, scoring='recall_macro',","102","    ...                 cv=5)  # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE","103","    array([0.96..., 1.  ..., 0.96..., 0.96..., 1.        ])","105","    >>> cross_val_score(model, X, y, cv=5, scoring='wrong_choice')","153","    >>> grid = GridSearchCV(LinearSVC(), param_grid={'C': [1, 10]},","154","    ...                     scoring=ftwo_scorer, cv=5)","254","    >>> cv_results = cross_validate(svm.fit(X, y), X, y,","255","    ...                             scoring=scoring, cv=5)","257","    >>> print(cv_results['test_tp'])  # doctest: +NORMALIZE_WHITESPACE","258","    [10  9  8  7  8]","260","    >>> print(cv_results['test_fn'])  # doctest: +NORMALIZE_WHITESPACE","261","    [0 1 2 3 2]"],"delete":["101","    >>> cross_val_score(clf, X, y, scoring='recall_macro') # doctest: +ELLIPSIS","102","    array([0.980..., 0.960..., 0.979...])","104","    >>> cross_val_score(model, X, y, scoring='wrong_choice')","152","    >>> grid = GridSearchCV(LinearSVC(), param_grid={'C': [1, 10]}, scoring=ftwo_scorer)","252","    >>> cv_results = cross_validate(svm.fit(X, y), X, y, scoring=scoring)","254","    >>> print(cv_results['test_tp'])          # doctest: +NORMALIZE_WHITESPACE","255","    [16 14  9]","257","    >>> print(cv_results['test_fn'])          # doctest: +NORMALIZE_WHITESPACE","258","    [1 3 7]"]}],"doc\/glossary.rst":[{"add":["1416","        all is an option), the default is 3-fold and will change to 5-fold","1417","        in version 0.22."],"delete":["1416","        all is an option), the default is 3-fold."]}],"sklearn\/model_selection\/_search.py":[{"add":["415","                 refit=True, cv='warn', verbose=0, pre_dispatch='2*n_jobs',","889","        .. versionchanged:: 0.20","890","            ``cv`` default value if None will change from 3-fold to 5-fold","891","            in v0.22.","892","","944","    >>> clf = GridSearchCV(svc, parameters, cv=5)","945","    >>> clf.fit(iris.data, iris.target)","947","    GridSearchCV(cv=5, error_score=...,","956","    >>> sorted(clf.cv_results_.keys())","1097","                 n_jobs=1, iid='warn', refit=True, cv='warn', verbose=0,","1233","        .. versionchanged:: 0.20","1234","            ``cv`` default value if None will change from 3-fold to 5-fold","1235","            in v0.22.","1236","","1414","                 fit_params=None, n_jobs=1, iid='warn', refit=True, cv='warn',"],"delete":["415","                 refit=True, cv=None, verbose=0, pre_dispatch='2*n_jobs',","940","    >>> clf = GridSearchCV(svc, parameters) # doctest: +SKIP","941","    >>> clf.fit(iris.data, iris.target) # doctest: +SKIP","943","    GridSearchCV(cv=None, error_score=...,","952","    >>> sorted(clf.cv_results_.keys()) # doctest: +SKIP","1093","                 n_jobs=1, iid='warn', refit=True, cv=None, verbose=0,","1406","                 fit_params=None, n_jobs=1, iid='warn', refit=True, cv=None,"]}],"doc\/whats_new\/v0.20.rst":[{"add":["197","- Added control over the normalization in","336","","550","","551","- Fixed a bug in :class:`tree.MAE` to ensure sample weights are being used","552","  during the calculation of tree MAE impurity. Previous behaviour could","553","  cause suboptimal splits to be chosen since the impurity calculation","555","  :issue:`11464` by :user:`John Stott <JohnStott>`.","741","- The default value of the ``n_estimators`` parameter of","742","  :class:`ensemble.RandomForestClassifier`, :class:`ensemble.RandomForestRegressor`,","743","  :class:`ensemble.ExtraTreesClassifier`, :class:`ensemble.ExtraTreesRegressor`,","744","  and :class:`ensemble.RandomTreesEmbedding` will change from 10 in version 0.20","808","  :func:`metrics.adjusted_mutual_information_score`,","810","  will have a new default value. In version 0.22, the default normalizer for each","912","Model selection","913","","914","- The default number of cross-validation folds ``cv`` and the default number of","915","  splits ``n_splits`` in the :class:`model_selection.KFold`-like splitters will change","916","  from 3 to 5 in 0.22 as 3-fold has a lot of variance.","917","  :issue:`11557` by :user:`Alexandre Boucaud <aboucaud>`.","918","","936",""],"delete":["197","- Added control over the normalization in ","336","  ","550","  ","551","- Fixed a bug in :class:`tree.MAE` to ensure sample weights are being used ","552","  during the calculation of tree MAE impurity. Previous behaviour could ","553","  cause suboptimal splits to be chosen since the impurity calculation ","555","  :issue:`11464` by :user:`John Stott <JohnStott>`.  ","741","- The default value of the ``n_estimators`` parameter of ","742","  :class:`ensemble.RandomForestClassifier`, :class:`ensemble.RandomForestRegressor`, ","743","  :class:`ensemble.ExtraTreesClassifier`, :class:`ensemble.ExtraTreesRegressor`, ","744","  and :class:`ensemble.RandomTreesEmbedding` will change from 10 in version 0.20 ","808","  :func:`metrics.adjusted_mutual_information_score`, ","810","  will have a new default value. In version 0.22, the default normalizer for each ","929","  "]}],"sklearn\/feature_selection\/rfe.py":[{"add":["314","        .. versionchanged:: 0.20","315","            ``cv`` default value if None will change from 3-fold to 5-fold","316","            in v0.22.","317","","388","    def __init__(self, estimator, step=1, cv='warn', scoring=None, verbose=0,"],"delete":["384","    def __init__(self, estimator, step=1, cv=None, scoring=None, verbose=0,"]}],"sklearn\/linear_model\/logistic.py":[{"add":["1410","    cv : integer or cross-validation generator, default: None","1416","        .. versionchanged:: 0.20","1417","            ``cv`` default value if None will change from 3-fold to 5-fold","1418","            in v0.22.","1419","","1576","    def __init__(self, Cs=10, fit_intercept=True, cv='warn', dual=False,"],"delete":["1410","    cv : integer or cross-validation generator","1572","    def __init__(self, Cs=10, fit_intercept=True, cv=None, dual=False,"]}],"sklearn\/ensemble\/tests\/test_forest.py":[{"add":["448","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22"],"delete":[]}],"doc\/tutorial\/statistical_inference\/model_selection.rst":[{"add":["62","    >>> X = [\"a\", \"a\", \"a\", \"b\", \"b\", \"c\", \"c\", \"c\", \"c\", \"c\"]","63","    >>> k_fold = KFold(n_splits=5)","66","    Train: [2 3 4 5 6 7 8 9] | test: [0 1]","67","    Train: [0 1 4 5 6 7 8 9] | test: [2 3]","68","    Train: [0 1 2 3 6 7 8 9] | test: [4 5]","69","    Train: [0 1 2 3 4 5 8 9] | test: [6 7]","70","    Train: [0 1 2 3 4 5 6 7] | test: [8 9]","76","    [0.963..., 0.922..., 0.963..., 0.963..., 0.930...]","90","    array([0.96388889, 0.92222222, 0.9637883 , 0.9637883 , 0.93036212])","100","    array([0.96578289, 0.92708922, 0.96681476, 0.96362897, 0.93192644])","233","a stratified 3-fold. The default will change to a 5-fold cross-validation in","234","version 0.22.","265","    >>> lasso = linear_model.LassoCV(cv=3)","270","    LassoCV(alphas=None, copy_X=True, cv=3, eps=0.001, fit_intercept=True,"],"delete":["62","    >>> X = [\"a\", \"a\", \"b\", \"c\", \"c\", \"c\"]","63","    >>> k_fold = KFold(n_splits=3)","66","    Train: [2 3 4 5] | test: [0 1]","67","    Train: [0 1 4 5] | test: [2 3]","68","    Train: [0 1 2 3] | test: [4 5]","74","    [0.934..., 0.956..., 0.939...]","88","    array([0.93489149, 0.95659432, 0.93989983])","98","    array([0.93969761, 0.95911415, 0.94041254])","231","a stratified 3-fold.","262","    >>> lasso = linear_model.LassoCV()","267","    LassoCV(alphas=None, copy_X=True, cv=None, eps=0.001, fit_intercept=True,"]}],"sklearn\/covariance\/tests\/test_graph_lasso.py":[{"add":["4","import pytest","5","","122","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","146","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22"],"delete":[]}],"sklearn\/covariance\/tests\/test_graphical_lasso.py":[{"add":["118","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","141","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22"],"delete":[]}],"sklearn\/linear_model\/omp.py":[{"add":["787","        .. versionchanged:: 0.20","788","            ``cv`` default value if None will change from 3-fold to 5-fold","789","            in v0.22.","790","","828","                 max_iter=None, cv='warn', n_jobs=1, verbose=False):"],"delete":["824","                 max_iter=None, cv=None, n_jobs=1, verbose=False):"]}],"sklearn\/ensemble\/tests\/test_weight_boosting.py":[{"add":["199","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22"],"delete":[]}],"doc\/modules\/linear_model.rst":[{"add":["140","    >>> reg = linear_model.RidgeCV(alphas=[0.1, 1.0, 10.0], cv=3)","142","    RidgeCV(alphas=[0.1, 1.0, 10.0], cv=3, fit_intercept=True, scoring=None,"],"delete":["140","    >>> reg = linear_model.RidgeCV(alphas=[0.1, 1.0, 10.0])","142","    RidgeCV(alphas=[0.1, 1.0, 10.0], cv=None, fit_intercept=True, scoring=None,"]}],"sklearn\/metrics\/tests\/test_score_objects.py":[{"add":["252","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22"],"delete":[]}],"sklearn\/model_selection\/_split.py":[{"add":["51","NSPLIT_WARNING = (","52","    \"You should specify a value for 'n_splits' instead of relying on the \"","53","    \"default value. The default value will change from 3 to 5 \"","54","    \"in version 0.22.\")","55","","56","CV_WARNING = (","57","    \"You should specify a value for 'cv' instead of relying on the \"","58","    \"default value. The default value will change from 3 to 5 \"","59","    \"in version 0.22.\")","60","","61","","371","        .. versionchanged:: 0.20","372","            ``n_splits`` default value will change from 3 to 5 in v0.22.","373","","422","    def __init__(self, n_splits='warn', shuffle=False,","424","        if n_splits is 'warn':","425","            warnings.warn(NSPLIT_WARNING, FutureWarning)","426","            n_splits = 3","459","        .. versionchanged:: 0.20","460","            ``n_splits`` default value will change from 3 to 5 in v0.22.","461","","494","    def __init__(self, n_splits='warn'):","495","        if n_splits is 'warn':","496","            warnings.warn(NSPLIT_WARNING, FutureWarning)","497","            n_splits = 3","555","        .. versionchanged:: 0.20","556","            ``n_splits`` default value will change from 3 to 5 in v0.22.","557","","595","    def __init__(self, n_splits='warn', shuffle=False, random_state=None):","596","        if n_splits is 'warn':","597","            warnings.warn(NSPLIT_WARNING, FutureWarning)","598","            n_splits = 3","718","        .. versionchanged:: 0.20","719","            ``n_splits`` default value will change from 3 to 5 in v0.22.","720","","727","    >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])","728","    >>> y = np.array([1, 2, 3, 4, 5, 6])","729","    >>> tscv = TimeSeriesSplit(n_splits=5)","731","    TimeSeriesSplit(max_train_size=None, n_splits=5)","739","    TRAIN: [0 1 2 3] TEST: [4]","740","    TRAIN: [0 1 2 3 4] TEST: [5]","749","    def __init__(self, n_splits='warn', max_train_size=None):","750","        if n_splits is 'warn':","751","            warnings.warn(NSPLIT_WARNING, FutureWarning)","752","            n_splits = 3","1309","    >>> X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [3, 4], [5, 6]])","1310","    >>> y = np.array([1, 2, 1, 2, 1, 2])","1311","    >>> rs = ShuffleSplit(n_splits=5, test_size=.25, random_state=0)","1313","    5","1315","    ShuffleSplit(n_splits=5, random_state=0, test_size=0.25, train_size=None)","1319","    TRAIN: [1 3 0 4] TEST: [5 2]","1320","    TRAIN: [4 0 2 5] TEST: [1 3]","1321","    TRAIN: [1 2 4 0] TEST: [3 5]","1322","    TRAIN: [3 4 1 0] TEST: [5 2]","1323","    TRAIN: [3 5 1 0] TEST: [2 4]","1324","    >>> rs = ShuffleSplit(n_splits=5, train_size=0.5, test_size=.25,","1329","    TRAIN: [1 3 0] TEST: [5 2]","1330","    TRAIN: [4 0 2] TEST: [1 3]","1331","    TRAIN: [1 2 4] TEST: [3 5]","1332","    TRAIN: [3 4 1] TEST: [5 2]","1333","    TRAIN: [3 5 1] TEST: [2 4]","1545","    >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])","1546","    >>> y = np.array([0, 0, 0, 1, 1, 1])","1547","    >>> sss = StratifiedShuffleSplit(n_splits=5, test_size=0.5, random_state=0)","1549","    5","1551","    StratifiedShuffleSplit(n_splits=5, random_state=0, ...)","1556","    TRAIN: [5 2 3] TEST: [4 1 0]","1557","    TRAIN: [5 1 4] TEST: [0 2 3]","1558","    TRAIN: [5 0 2] TEST: [4 3 1]","1559","    TRAIN: [4 1 0] TEST: [2 3 5]","1560","    TRAIN: [0 5 1] TEST: [3 4 2]","1904","def check_cv(cv='warn', y=None, classifier=False):","1925","        .. versionchanged:: 0.20","1926","            ``cv`` default value will change from 3-fold to 5-fold in v0.22.","1927","","1941","    if cv is 'warn':","1942","        warnings.warn(CV_WARNING, FutureWarning)"],"delete":["408","    def __init__(self, n_splits=3, shuffle=False,","474","    def __init__(self, n_splits=3):","569","    def __init__(self, n_splits=3, shuffle=False, random_state=None):","695","    >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])","696","    >>> y = np.array([1, 2, 3, 4])","697","    >>> tscv = TimeSeriesSplit(n_splits=3)","699","    TimeSeriesSplit(max_train_size=None, n_splits=3)","715","    def __init__(self, n_splits=3, max_train_size=None):","1272","    >>> X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])","1273","    >>> y = np.array([1, 2, 1, 2])","1274","    >>> rs = ShuffleSplit(n_splits=3, test_size=.25, random_state=0)","1276","    3","1278","    ShuffleSplit(n_splits=3, random_state=0, test_size=0.25, train_size=None)","1282","    TRAIN: [3 1 0] TEST: [2]","1283","    TRAIN: [2 1 3] TEST: [0]","1284","    TRAIN: [0 2 1] TEST: [3]","1285","    >>> rs = ShuffleSplit(n_splits=3, train_size=0.5, test_size=.25,","1290","    TRAIN: [3 1] TEST: [2]","1291","    TRAIN: [2 1] TEST: [0]","1292","    TRAIN: [0 2] TEST: [3]","1504","    >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])","1505","    >>> y = np.array([0, 0, 1, 1])","1506","    >>> sss = StratifiedShuffleSplit(n_splits=3, test_size=0.5, random_state=0)","1508","    3","1510","    StratifiedShuffleSplit(n_splits=3, random_state=0, ...)","1515","    TRAIN: [1 2] TEST: [3 0]","1516","    TRAIN: [0 2] TEST: [1 3]","1517","    TRAIN: [0 2] TEST: [3 1]","1861","def check_cv(cv=3, y=None, classifier=False):","1895","    if cv is None:"]}],"sklearn\/ensemble\/tests\/test_bagging.py":[{"add":["500","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22"],"delete":[]}],"sklearn\/model_selection\/_validation.py":[{"add":["40","def cross_validate(estimator, X, y=None, groups=None, scoring=None, cv='warn',","95","        .. versionchanged:: 0.20","96","            ``cv`` default value if None will change from 3-fold to 5-fold","97","            in v0.22.","98","","181","    >>> cv_results = cross_validate(lasso, X, y, cv=3,","182","    ...                             return_train_score=False)","191","    >>> scores = cross_validate(lasso, X, y, cv=3,","262","def cross_val_score(estimator, X, y=None, groups=None, scoring=None, cv='warn',","306","        .. versionchanged:: 0.20","307","            ``cv`` default value if None will change from 3-fold to 5-fold","308","            in v0.22.","309","","350","    >>> print(cross_val_score(lasso, X, y, cv=3))  # doctest: +ELLIPSIS","623","def cross_val_predict(estimator, X, y=None, groups=None, cv='warn', n_jobs=1,","665","        .. versionchanged:: 0.20","666","            ``cv`` default value if None will change from 3-fold to 5-fold","667","            in v0.22.","668","","729","    >>> y_pred = cross_val_predict(lasso, X, y, cv=3)","903","def permutation_test_score(estimator, X, y, groups=None, cv='warn',","954","        .. versionchanged:: 0.20","955","            ``cv`` default value if None will change from 3-fold to 5-fold","956","            in v0.22.","957","","1044","                   train_sizes=np.linspace(0.1, 1.0, 5), cv='warn',","1045","                   scoring=None, exploit_incremental_learning=False, n_jobs=1,","1104","        .. versionchanged:: 0.20","1105","            ``cv`` default value if None will change from 3-fold to 5-fold","1106","            in v0.22.","1107","","1289","                     cv='warn', scoring=None, n_jobs=1, pre_dispatch=\"all\",","1341","        .. versionchanged:: 0.20","1342","            ``cv`` default value if None will change from 3-fold to 5-fold","1343","            in v0.22.","1344",""],"delete":["40","def cross_validate(estimator, X, y=None, groups=None, scoring=None, cv=None,","177","    >>> cv_results = cross_validate(lasso, X, y, return_train_score=False)","186","    >>> scores = cross_validate(lasso, X, y,","257","def cross_val_score(estimator, X, y=None, groups=None, scoring=None, cv=None,","341","    >>> print(cross_val_score(lasso, X, y))  # doctest: +ELLIPSIS","614","def cross_val_predict(estimator, X, y=None, groups=None, cv=None, n_jobs=1,","716","    >>> y_pred = cross_val_predict(lasso, X, y)","890","def permutation_test_score(estimator, X, y, groups=None, cv=None,","1027","                   train_sizes=np.linspace(0.1, 1.0, 5), cv=None, scoring=None,","1028","                   exploit_incremental_learning=False, n_jobs=1,","1268","                     cv=None, scoring=None, n_jobs=1, pre_dispatch=\"all\","]}],"sklearn\/linear_model\/tests\/test_least_angle.py":[{"add":["184","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","421","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","438","@pytest.mark.filterwarnings('ignore::FutureWarning')","529","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22"],"delete":[]}],"sklearn\/model_selection\/tests\/test_validation.py":[{"add":["242","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","284","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","406","                                         return_train_score=val, cv=5)","513","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","534","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","572","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","599","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","793","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","843","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","890","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","905","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","920","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","967","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","986","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","1340","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","1346","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","1355","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","1383","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","1425","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22","1454","@pytest.mark.filterwarnings('ignore: You should specify a value')  # 0.22"],"delete":["404","                                         return_train_score=val)"]}],"sklearn\/linear_model\/least_angle.py":[{"add":["1015","        .. versionchanged:: 0.20","1016","            ``cv`` default value if None will change from 3-fold to 5-fold","1017","            in v0.22.","1018","","1077","                 normalize=True, precompute='auto', cv='warn',","1228","        .. versionchanged:: 0.20","1229","            ``cv`` default value if None will change from 3-fold to 5-fold","1230","            in v0.22.","1231","","1307","                 normalize=True, precompute='auto', cv='warn',"],"delete":["1073","                 normalize=True, precompute='auto', cv=None,","1299","                 normalize=True, precompute='auto', cv=None,"]}]}},"87d96a2c2af9e02f3ff34221e7b98904e4216c60":{"changes":{"sklearn\/preprocessing\/tests\/test_label.py":"MODIFY","doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/preprocessing\/label.py":"MODIFY"},"diff":{"sklearn\/preprocessing\/tests\/test_label.py":[{"add":["210","    # Fail on inverse_transform(\"\")","211","    msg = \"bad input shape ()\"","212","    assert_raise_message(ValueError, msg, le.inverse_transform, \"\")","213","","214","","215","def test_label_encoder_empty_array():","216","    le = LabelEncoder()","217","    le.fit(np.array([\"1\", \"2\", \"1\", \"2\", \"2\"]))","218","    # test empty transform","219","    transformed = le.transform([])","220","    assert_array_equal(np.array([]), transformed)","221","    # test empty inverse transform","222","    inverse_transformed = le.inverse_transform([])","223","    assert_array_equal(np.array([]), inverse_transformed)","224",""],"delete":[]}],"doc\/whats_new\/v0.20.rst":[{"add":["307","Preprocessing","308","","309","- Fixed bugs in :class:`preprocessing.LabelEncoder` which would sometimes throw","310","  errors when ``transform`` or ``inverse_transform`` was called with empty arrays.","311","  :issue:`10458` by :user:`Mayur Kulkarni <maykulkarni>`.","312",""],"delete":[]}],"sklearn\/preprocessing\/label.py":[{"add":["128","        # transform of empty array is empty array","129","        if _num_samples(y) == 0:","130","            return np.array([])","152","        y = column_or_1d(y, warn=True)","153","        # inverse transform of empty array is empty array","154","        if _num_samples(y) == 0:","155","            return np.array([])"],"delete":[]}]}},"4a2b96f8e6c4a07cbc6803459ea8a727b2e54cbb":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/metrics\/cluster\/tests\/test_supervised.py":"MODIFY","sklearn\/metrics\/cluster\/supervised.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["281","- Fixed a bug to avoid integer overflow. Casted product to 64 bits integer in","282","  :func:`mutual_info_score`.","283","  :issue:`9772` by :user:`Kumar Ashutosh <thechargedneutron>`.","284",""],"delete":[]}],"sklearn\/metrics\/cluster\/tests\/test_supervised.py":[{"add":["15","from sklearn.utils import assert_all_finite","175","def test_int_overflow_mutual_info_score():","176","    # Test overflow in mutual_info_classif","177","    x = np.array([1] * (52632 + 2529) + [2] * (14660 + 793) + [3] * (3271 +","178","                 204) + [4] * (814 + 39) + [5] * (316 + 20))","179","    y = np.array([0] * 52632 + [1] * 2529 + [0] * 14660 + [1] * 793 +","180","                 [0] * 3271 + [1] * 204 + [0] * 814 + [1] * 39 + [0] * 316 +","181","                 [1] * 20)","182","","183","    assert_all_finite(mutual_info_score(x.ravel(), y.ravel()))","184","","185",""],"delete":[]}],"sklearn\/metrics\/cluster\/supervised.py":[{"add":["603","    outer = pi.take(nzx).astype(np.int64) * pj.take(nzy).astype(np.int64)"],"delete":["603","    outer = pi.take(nzx) * pj.take(nzy)"]}]}},"aba87b7eb2ea2a4fa0ed9cfddae03e3b0c8d0162":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["98","- Fixed a bug in :class:`naive_bayes.GaussianNB` which incorrectly raised","99","  error for prior list which summed to 1.","100","  :issue:`10005` by :user:`Gaurav Dhingra <gxyd>`.","101",""],"delete":[]}]}},"653de6ca54becfc02ea9c90db567e8613843ccda":{"changes":{"doc\/modules\/model_evaluation.rst":"MODIFY"},"diff":{"doc\/modules\/model_evaluation.rst":[{"add":["466","    * Macro-average recall as described in [Mosley2013]_, [Kelleher2015]_ and [Guyon2015]_:","467","      the recall for each class is computed independently and the average is taken over all classes.","468","      In [Guyon2015]_, the macro-average recall is then adjusted to ensure that random predictions","469","      have a score of :math:`0` while perfect predictions have a score of :math:`1`.","470","      One can compute the macro-average recall using ``recall_score(average=\"macro\")`` in :func:`recall_score`.","471","    * Class balanced accuracy as described in [Mosley2013]_: the minimum between the precision","472","      and the recall for each class is computed. Those values are then averaged over the total","473","      number of classes to get the balanced accuracy.","474","    * Balanced Accuracy as described in [Urbanowicz2015]_: the average of sensitivity and selectivity","475","      is computed for each class and then averaged over total number of classes.","478","    the :func:`balanced_accuracy_score` function.","493","  .. [Urbanowicz2015] Urbanowicz R.J.,  Moore, J.H. `ExSTraCS 2.0: description and evaluation of a scalable learning","494","     classifier system < https:\/\/doi.org\/10.1007\/s12065-015-0128-8>`_, Evol. Intel. (2015) 8: 89."],"delete":["466","    * Normalized class-wise accuracy average as described in [Guyon2015]_: for multi-class","467","      classification problem, each sample is assigned the class with maximum prediction value.","468","      The predictions are then binarized to compute the accuracy of each class on a","469","      one-vs-rest fashion. The balanced accuracy is obtained by averaging the individual","470","      accuracies over all classes and then normalized by the expected value of balanced","471","      accuracy for random predictions (:math:`0.5` for binary classification, :math:`1\/C`","472","      for C-class classification problem).","473","    * Macro-average recall as described in [Mosley2013]_ and [Kelleher2015]_: the recall","474","      for each class is computed independently and the average is taken over all classes.","477","    the :func:`balanced_accuracy_score` function. However, the macro-averaged recall","478","    is implemented in :func:`sklearn.metrics.recall_score`: set ``average`` parameter","479","    to ``\"macro\"``."]}]}},"4e56f82c30e836735404c496392a2afd665297fd":{"changes":{"sklearn\/metrics\/ranking.py":"MODIFY","sklearn\/feature_selection\/univariate_selection.py":"MODIFY"},"diff":{"sklearn\/metrics\/ranking.py":[{"add":["212","        precision, recall, _ = precision_recall_curve(","305","        fpr, tpr, _ = roc_curve(y_true, y_score,","306","                                sample_weight=sample_weight)"],"delete":["212","        precision, recall, thresholds = precision_recall_curve(","305","        fpr, tpr, tresholds = roc_curve(y_true, y_score,","306","                                        sample_weight=sample_weight)"]}],"sklearn\/feature_selection\/univariate_selection.py":[{"add":["428","        threshold = stats.scoreatpercentile(scores,","429","                                            100 - self.percentile)","430","        mask = scores > threshold","431","        ties = np.where(scores == threshold)[0]"],"delete":["428","        treshold = stats.scoreatpercentile(scores,","429","                                           100 - self.percentile)","430","        mask = scores > treshold","431","        ties = np.where(scores == treshold)[0]"]}]}},"f84581bffd3ac48540de3af5850470b5869267de":{"changes":{"sklearn\/metrics\/regression.py":"MODIFY","sklearn\/metrics\/tests\/test_regression.py":"MODIFY"},"diff":{"sklearn\/metrics\/regression.py":[{"add":["312","    if (y_true < 0).any() or (y_pred < 0).any():"],"delete":["312","    if not (y_true >= 0).all() and not (y_pred >= 0).all():"]}],"sklearn\/metrics\/tests\/test_regression.py":[{"add":["66","    assert_raises_regex(ValueError, \"Mean Squared Logarithmic Error cannot be \"","67","                        \"used when targets contain negative values.\",","68","                        mean_squared_log_error, [1., 2., 3.], [1., -2., 3.])","69","    assert_raises_regex(ValueError, \"Mean Squared Logarithmic Error cannot be \"","70","                        \"used when targets contain negative values.\",","71","                        mean_squared_log_error, [1., -2., 3.], [1., 2., 3.])","72",""],"delete":[]}]}},"622f912095308733ddfe572a619b1574b9da335e":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/metrics\/cluster\/tests\/test_supervised.py":"MODIFY","sklearn\/metrics\/cluster\/supervised.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["423","- Fixed a bug in :func:`metrics.cluster.fowlkes_mallows_score` to avoid integer","424","  overflow. Casted return value of `contingency_matrix` to `int64` and computed","425","  product of square roots rather than square root of product.","426","  :issue:`9515` by :user:`Alan Liddell <aliddell>` and","427","  :user:`Manh Dao <manhdao>`.","428",""],"delete":[]}],"sklearn\/metrics\/cluster\/tests\/test_supervised.py":[{"add":["175","def test_int_overflow_mutual_info_fowlkes_mallows_score():","176","    # Test overflow in mutual_info_classif and fowlkes_mallows_score","183","    assert_all_finite(mutual_info_score(x, y))","184","    assert_all_finite(fowlkes_mallows_score(x, y))"],"delete":["175","def test_int_overflow_mutual_info_score():","176","    # Test overflow in mutual_info_classif","183","    assert_all_finite(mutual_info_score(x.ravel(), y.ravel()))"]}],"sklearn\/metrics\/cluster\/supervised.py":[{"add":["854","    c = contingency_matrix(labels_true, labels_pred,","855","                           sparse=True).astype(np.int64)","859","    return np.sqrt(tk \/ pk) * np.sqrt(tk \/ qk) if tk != 0. else 0."],"delete":["854","    c = contingency_matrix(labels_true, labels_pred, sparse=True)","858","    return tk \/ np.sqrt(pk * qk) if tk != 0. else 0."]}]}},"e070611bdb4884c222b40e4f59b65e8c6ca35380":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/model_selection\/tests\/test_search.py":"MODIFY","sklearn\/model_selection\/_search.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["55","- :class:`dummy.DummyRegressor` now has a ``return_std`` option in its","149","- :class:`neighbors.RadiusNeighborsRegressor` and","278","  of cross-validation values for different alphas. :issue:`10297` by","280","","491","- The :func:`covariance.graph_lasso`, :class:`covariance.GraphLasso` and","492","  :class:`covariance.GraphLassoCV` have been renamed to","508","- Changed ValueError exception raised in :class:`model_selection.ParameterSampler`","509","  to a UserWarning for case where the class is instantiated with a greater value of","510","  ``n_iter`` than the total space of parameters in the parameter grid. ``n_iter`` now","511","  acts as an upper bound on iterations.","512","  :issue:`#10982` by :user:`Juliet Lawton <julietcl>`","513",""],"delete":["55","- :class:`dummy.DummyRegressor` now has a ``return_std`` option in its ","149","- :class:`neighbors.RadiusNeighborsRegressor` and ","278","  of cross-validation values for different alphas. :issue:`10297` by ","280","  ","491","- The :func:`covariance.graph_lasso`, :class:`covariance.GraphLasso` and ","492","  :class:`covariance.GraphLassoCV` have been renamed to "]}],"sklearn\/model_selection\/tests\/test_search.py":[{"add":["1387","    # raise warning if n_iter is bigger than total parameter space","1390","    n_iter = 7","1391","    grid_size = 6","1392","    expected_warning = ('The total space of parameters %d is smaller '","1393","                        'than n_iter=%d. Running %d iterations. For '","1394","                        'exhaustive searches, use GridSearchCV.'","1395","                        % (grid_size, n_iter, grid_size))","1396","    assert_warns_message(UserWarning, expected_warning,","1397","                         list, sampler)","1398",""],"delete":["1387","    # raise error if n_iter too large","1390","    assert_raises(ValueError, list, sampler)"]}],"sklearn\/model_selection\/_search.py":[{"add":["244","            n_iter = self.n_iter","246","            if grid_size < n_iter:","247","                warnings.warn(","248","                    'The total space of parameters %d is smaller '","249","                    'than n_iter=%d. Running %d iterations. For exhaustive '","250","                    'searches, use GridSearchCV.'","251","                    % (grid_size, self.n_iter, grid_size), UserWarning)","252","                n_iter = grid_size","253","            for i in sample_without_replacement(grid_size, n_iter,"],"delete":["245","            if grid_size < self.n_iter:","246","                raise ValueError(","247","                    \"The total space of parameters %d is smaller \"","248","                    \"than n_iter=%d. For exhaustive searches, use \"","249","                    \"GridSearchCV.\" % (grid_size, self.n_iter))","250","            for i in sample_without_replacement(grid_size, self.n_iter,"]}]}},"2eb254c296c9d3d555ae9f4a44ef54ab63dd8564":{"changes":{"sklearn\/impute.py":"MODIFY","sklearn\/utils\/estimator_checks.py":"MODIFY"},"diff":{"sklearn\/impute.py":[{"add":["44","    if is_scalar_nan(value_to_mask):"],"delete":["44","    if value_to_mask is np.nan:"]}],"sklearn\/utils\/estimator_checks.py":[{"add":["1168","    # include NaN values when the estimator should deal with them","1169","    if name in ALLOW_NAN:","1170","        # set randomly 10 elements to np.nan","1171","        rng = np.random.RandomState(42)","1172","        mask = rng.choice(X.size, 10, replace=False)","1173","        X.reshape(-1)[mask] = np.nan","1174","","1189","    result = dict()","1190","    for method in check_methods:","1191","        if hasattr(estimator, method):","1192","            result[method] = getattr(estimator, method)(X)","1193",""],"delete":["53","from sklearn.exceptions import ConvergenceWarning","1177","    result = dict()","1178","    for method in check_methods:","1179","        if hasattr(estimator, method):","1180","            result[method] = getattr(estimator, method)(X)","1181",""]}]}},"2e0b27483a842e115e6f8e388fca98accef00735":{"changes":{"doc\/install.rst":"MODIFY","doc\/whats_new\/v0.20.rst":"MODIFY","README.rst":"MODIFY","doc\/index.rst":"MODIFY"},"diff":{"doc\/install.rst":[{"add":["23","","24",".. warning::","25","","26","    Scikit-learn 0.20 is the last version to support Python 2.7 and Python 3.4.","27","    Scikit-learn 0.21 will require Python 3.5 or newer.","28",""],"delete":[]}],"doc\/whats_new\/v0.20.rst":[{"add":["13",".. warning::","14","","15","    Version 0.20 is the last version of scikit-learn to support Python 2.7 and Python 3.4.","16","    Scikit-learn 0.21 will require Python 3.5 or higher.","17",""],"delete":[]}],"README.rst":[{"add":["55","**Scikit-learn 0.20 is the last version to support Python2.7.**","56","Scikit-learn 0.21 and later will require Python 3.5 or newer.","57",""],"delete":[]}],"doc\/index.rst":[{"add":["209","                    <li><strong>Scikit-learn 0.21 will drop support for Python 2.7 and Python 3.4.<\/strong>","210","                    <\/li>","211","                    <li><em>July 2018.<\/em> scikit-learn 0.20 is available for download (<a href=\"whats_new.html#version-0-20\">Changelog<\/a>).","212","                    <\/li>"],"delete":["217","                    <li><em>September 2016.<\/em> scikit-learn 0.18.0 is available for download (<a href=\"whats_new\/v0.18.html#version-0-18\">Changelog<\/a>).","218","                    <\/li>","219","                    <li><em>November 2015.<\/em> scikit-learn 0.17.0 is available for download (<a href=\"whats_new\/v0.17.html\">Changelog<\/a>).","220","                    <\/li>","221","                    <li><em>March 2015.<\/em> scikit-learn 0.16.0 is available for download (<a href=\"whats_new\/v0.16.html\">Changelog<\/a>).","222","                    <\/li>"]}]}},"3b037b0e281cb42e5bc58e29813f4ee54d39899e":{"changes":{"sklearn\/datasets\/base.py":"MODIFY","sklearn\/datasets\/covtype.py":"MODIFY","sklearn\/datasets\/olivetti_faces.py":"MODIFY","sklearn\/datasets\/twenty_newsgroups.py":"MODIFY","sklearn\/datasets\/kddcup99.py":"MODIFY","sklearn\/datasets\/samples_generator.py":"MODIFY","sklearn\/datasets\/rcv1.py":"MODIFY"},"diff":{"sklearn\/datasets\/base.py":[{"add":["146","    random_state : int, RandomState instance or None (default=0)","147","        Determines random number generation for dataset shuffling. Pass an int","148","        for reproducible output across multiple function calls.","149","        See :term:`Glossary <random_state>`."],"delete":["146","    random_state : int, RandomState instance or None, optional (default=0)","147","        If int, random_state is the seed used by the random number generator;","148","        If RandomState instance, random_state is the random number generator;","149","        If None, the random number generator is the RandomState instance used","150","        by `np.random`."]}],"sklearn\/datasets\/covtype.py":[{"add":["59","    random_state : int, RandomState instance or None (default)","60","        Determines random number generation for dataset shuffling. Pass an int","61","        for reproducible output across multiple function calls.","62","        See :term:`Glossary <random_state>`."],"delete":["59","    random_state : int, RandomState instance or None, optional (default=None)","60","        Random state for shuffling the dataset.","61","        If int, random_state is the seed used by the random number generator;","62","        If RandomState instance, random_state is the random number generator;","63","        If None, the random number generator is the RandomState instance used","64","        by `np.random`."]}],"sklearn\/datasets\/olivetti_faces.py":[{"add":["66","    random_state : int, RandomState instance or None (default=0)","67","        Determines random number generation for dataset shuffling. Pass an int","68","        for reproducible output across multiple function calls.","69","        See :term:`Glossary <random_state>`."],"delete":["66","    random_state : int, RandomState instance or None, optional (default=0)","67","        If int, random_state is the seed used by the random number generator;","68","        If RandomState instance, random_state is the random number generator;","69","        If None, the random number generator is the RandomState instance used","70","        by `np.random`."]}],"sklearn\/datasets\/twenty_newsgroups.py":[{"add":["171","    random_state : int, RandomState instance or None (default)","172","        Determines random number generation for dataset shuffling. Pass an int","173","        for reproducible output across multiple function calls.","174","        See :term:`Glossary <random_state>`."],"delete":["171","    random_state : numpy random number generator or seed integer","172","        Used to shuffle the dataset."]}],"sklearn\/datasets\/kddcup99.py":[{"add":["141","    random_state : int, RandomState instance or None (default)","142","        Determines random number generation for dataset shuffling and for","143","        selection of abnormal samples if `subset='SA'`. Pass an int for","144","        reproducible output across multiple function calls.","145","        See :term:`Glossary <random_state>`."],"delete":["141","    random_state : int, RandomState instance or None, optional (default=None)","142","        Random state for shuffling the dataset. If subset='SA', this random","143","        state is also used to randomly select the small proportion of abnormal","144","        samples.","145","        If int, random_state is the seed used by the random number generator;","146","        If RandomState instance, random_state is the random number generator;","147","        If None, the random number generator is the RandomState instance used","148","        by `np.random`."]}],"sklearn\/datasets\/samples_generator.py":[{"add":["127","    random_state : int, RandomState instance or None (default)","128","        Determines random number generation for dataset creation. Pass an int","129","        for reproducible output across multiple function calls.","130","        See :term:`Glossary <random_state>`.","317","    random_state : int, RandomState instance or None (default)","318","        Determines random number generation for dataset creation. Pass an int","319","        for reproducible output across multiple function calls.","320","        See :term:`Glossary <random_state>`.","423","    random_state : int, RandomState instance or None (default)","424","        Determines random number generation for dataset creation. Pass an int","425","        for reproducible output across multiple function calls.","426","        See :term:`Glossary <random_state>`.","514","    random_state : int, RandomState instance or None (default)","515","        Determines random number generation for dataset creation. Pass an int","516","        for reproducible output across multiple function calls.","517","        See :term:`Glossary <random_state>`.","598","    random_state : int, RandomState instance or None (default)","599","        Determines random number generation for dataset shuffling and noise.","600","        Pass an int for reproducible output across multiple function calls.","601","        See :term:`Glossary <random_state>`.","660","    random_state : int, RandomState instance or None (default)","661","        Determines random number generation for dataset shuffling and noise.","662","        Pass an int for reproducible output across multiple function calls.","663","        See :term:`Glossary <random_state>`.","732","    random_state : int, RandomState instance or None (default)","733","        Determines random number generation for dataset creation. Pass an int","734","        for reproducible output across multiple function calls.","735","        See :term:`Glossary <random_state>`.","869","    random_state : int, RandomState instance or None (default)","870","        Determines random number generation for dataset noise. Pass an int","871","        for reproducible output across multiple function calls.","872","        See :term:`Glossary <random_state>`.","930","    random_state : int, RandomState instance or None (default)","931","        Determines random number generation for dataset noise. Pass an int","932","        for reproducible output across multiple function calls.","933","        See :term:`Glossary <random_state>`.","995","    random_state : int, RandomState instance or None (default)","996","        Determines random number generation for dataset noise. Pass an int","997","        for reproducible output across multiple function calls.","998","        See :term:`Glossary <random_state>`.","1071","    random_state : int, RandomState instance or None (default)","1072","        Determines random number generation for dataset creation. Pass an int","1073","        for reproducible output across multiple function calls.","1074","        See :term:`Glossary <random_state>`.","1124","    random_state : int, RandomState instance or None (default)","1125","        Determines random number generation for dataset creation. Pass an int","1126","        for reproducible output across multiple function calls.","1127","        See :term:`Glossary <random_state>`.","1183","    random_state : int, RandomState instance or None (default)","1184","        Determines random number generation for dataset creation. Pass an int","1185","        for reproducible output across multiple function calls.","1186","        See :term:`Glossary <random_state>`.","1223","    random_state : int, RandomState instance or None (default)","1224","        Determines random number generation for dataset creation. Pass an int","1225","        for reproducible output across multiple function calls.","1226","        See :term:`Glossary <random_state>`.","1272","    random_state : int, RandomState instance or None (default)","1273","        Determines random number generation for dataset creation. Pass an int","1274","        for reproducible output across multiple function calls.","1275","        See :term:`Glossary <random_state>`.","1333","    random_state : int, RandomState instance or None (default)","1334","        Determines random number generation for dataset creation. Pass an int","1335","        for reproducible output across multiple function calls.","1336","        See :term:`Glossary <random_state>`.","1385","    random_state : int, RandomState instance or None (default)","1386","        Determines random number generation for dataset creation. Pass an int","1387","        for reproducible output across multiple function calls.","1388","        See :term:`Glossary <random_state>`.","1448","    random_state : int, RandomState instance or None (default)","1449","        Determines random number generation for dataset creation. Pass an int","1450","        for reproducible output across multiple function calls.","1451","        See :term:`Glossary <random_state>`.","1536","    random_state : int, RandomState instance or None (default)","1537","        Determines random number generation for dataset creation. Pass an int","1538","        for reproducible output across multiple function calls.","1539","        See :term:`Glossary <random_state>`.","1627","    random_state : int, RandomState instance or None (default)","1628","        Determines random number generation for dataset creation. Pass an int","1629","        for reproducible output across multiple function calls.","1630","        See :term:`Glossary <random_state>`."],"delete":["127","    random_state : int, RandomState instance or None, optional (default=None)","128","        If int, random_state is the seed used by the random number generator;","129","        If RandomState instance, random_state is the random number generator;","130","        If None, the random number generator is the RandomState instance used","131","        by ``np.random``.","318","    random_state : int, RandomState instance or None, optional (default=None)","319","        If int, random_state is the seed used by the random number generator;","320","        If RandomState instance, random_state is the random number generator;","321","        If None, the random number generator is the RandomState instance used","322","        by `np.random`.","425","    random_state : int, RandomState instance or None, optional (default=None)","426","        If int, random_state is the seed used by the random number generator;","427","        If RandomState instance, random_state is the random number generator;","428","        If None, the random number generator is the RandomState instance used","429","        by `np.random`.","517","    random_state : int, RandomState instance or None, optional (default=None)","518","        If int, random_state is the seed used by the random number generator;","519","        If RandomState instance, random_state is the random number generator;","520","        If None, the random number generator is the RandomState instance used","521","        by `np.random`.","602","    random_state : int, RandomState instance or None, optional (default=None)","603","        If int, random_state is the seed used by the random number generator;","604","        If RandomState instance, random_state is the random number generator;","605","        If None, the random number generator is the RandomState instance used","606","        by `np.random`.","665","    random_state : int, RandomState instance or None, optional (default=None)","666","        If int, random_state is the seed used by the random number generator;","667","        If RandomState instance, random_state is the random number generator;","668","        If None, the random number generator is the RandomState instance used","669","        by `np.random`.","738","    random_state : int, RandomState instance or None, optional (default=None)","739","        If int, random_state is the seed used by the random number generator;","740","        If RandomState instance, random_state is the random number generator;","741","        If None, the random number generator is the RandomState instance used","742","        by `np.random`.","876","    random_state : int, RandomState instance or None, optional (default=None)","877","        If int, random_state is the seed used by the random number generator;","878","        If RandomState instance, random_state is the random number generator;","879","        If None, the random number generator is the RandomState instance used","880","        by `np.random`.","938","    random_state : int, RandomState instance or None, optional (default=None)","939","        If int, random_state is the seed used by the random number generator;","940","        If RandomState instance, random_state is the random number generator;","941","        If None, the random number generator is the RandomState instance used","942","        by `np.random`.","1004","    random_state : int, RandomState instance or None, optional (default=None)","1005","        If int, random_state is the seed used by the random number generator;","1006","        If RandomState instance, random_state is the random number generator;","1007","        If None, the random number generator is the RandomState instance used","1008","        by `np.random`.","1081","    random_state : int, RandomState instance or None, optional (default=None)","1082","        If int, random_state is the seed used by the random number generator;","1083","        If RandomState instance, random_state is the random number generator;","1084","        If None, the random number generator is the RandomState instance used","1085","        by `np.random`.","1135","    random_state : int, RandomState instance or None, optional (default=None)","1136","        If int, random_state is the seed used by the random number generator;","1137","        If RandomState instance, random_state is the random number generator;","1138","        If None, the random number generator is the RandomState instance used","1139","        by `np.random`.","1195","    random_state : int, RandomState instance or None, optional (default=None)","1196","        If int, random_state is the seed used by the random number generator;","1197","        If RandomState instance, random_state is the random number generator;","1198","        If None, the random number generator is the RandomState instance used","1199","        by `np.random`.","1236","    random_state : int, RandomState instance or None, optional (default=None)","1237","        If int, random_state is the seed used by the random number generator;","1238","        If RandomState instance, random_state is the random number generator;","1239","        If None, the random number generator is the RandomState instance used","1240","        by `np.random`.","1286","    random_state : int, RandomState instance or None, optional (default=None)","1287","        If int, random_state is the seed used by the random number generator;","1288","        If RandomState instance, random_state is the random number generator;","1289","        If None, the random number generator is the RandomState instance used","1290","        by `np.random`.","1348","    random_state : int, RandomState instance or None, optional (default=None)","1349","        If int, random_state is the seed used by the random number generator;","1350","        If RandomState instance, random_state is the random number generator;","1351","        If None, the random number generator is the RandomState instance used","1352","        by `np.random`.","1401","    random_state : int, RandomState instance or None, optional (default=None)","1402","        If int, random_state is the seed used by the random number generator;","1403","        If RandomState instance, random_state is the random number generator;","1404","        If None, the random number generator is the RandomState instance used","1405","        by `np.random`.","1465","    random_state : int, RandomState instance or None, optional (default=None)","1466","        If int, random_state is the seed used by the random number generator;","1467","        If RandomState instance, random_state is the random number generator;","1468","        If None, the random number generator is the RandomState instance used","1469","        by `np.random`.","1554","    random_state : int, RandomState instance or None, optional (default=None)","1555","        If int, random_state is the seed used by the random number generator;","1556","        If RandomState instance, random_state is the random number generator;","1557","        If None, the random number generator is the RandomState instance used","1558","        by `np.random`.","1621","","1647","    random_state : int, RandomState instance or None, optional (default=None)","1648","        If int, random_state is the seed used by the random number generator;","1649","        If RandomState instance, random_state is the random number generator;","1650","        If None, the random number generator is the RandomState instance used","1651","        by `np.random`."]}],"sklearn\/datasets\/rcv1.py":[{"add":["104","    random_state : int, RandomState instance or None (default)","105","        Determines random number generation for dataset shuffling. Pass an int","106","        for reproducible output across multiple function calls.","107","        See :term:`Glossary <random_state>`."],"delete":["104","    random_state : int, RandomState instance or None, optional (default=None)","105","        Random state for shuffling the dataset.","106","        If int, random_state is the seed used by the random number generator;","107","        If RandomState instance, random_state is the random number generator;","108","        If None, the random number generator is the RandomState instance used","109","        by `np.random`.","184",""]}]}},"b25e2223288776e6cefd3be467df285c52ed8249":{"changes":{"sklearn\/preprocessing\/_discretization.py":"MODIFY","doc\/datasets\/index.rst":"MODIFY","doc\/whats_new\/v0.20.rst":"MODIFY","doc\/modules\/compose.rst":"MODIFY","sklearn\/tests\/test_discriminant_analysis.py":"MODIFY","sklearn\/discriminant_analysis.py":"MODIFY","doc\/modules\/preprocessing.rst":"MODIFY","sklearn\/preprocessing\/_encoders.py":"MODIFY","doc\/faq.rst":"MODIFY","sklearn\/datasets\/descr\/wine_data.rst":"MODIFY","sklearn\/decomposition\/pca.py":"MODIFY","doc\/developers\/contributing.rst":"MODIFY"},"diff":{"sklearn\/preprocessing\/_discretization.py":[{"add":["69","        The edges of each bin. Contain arrays of varying shapes ``(n_bins_, )``"],"delete":["69","        The edges of each bin. Contain arrays of varying shapes (n_bins_, )."]}],"doc\/datasets\/index.rst":[{"add":["156",".. _sample_generators:"],"delete":["156",".. _generated_datasets:"]}],"doc\/whats_new\/v0.20.rst":[{"add":["139","  and :issue:`11315` by :user:`Thomas Fan <thomasjpfan>`.","210","  :issue:`11166` by `Gael Varoquaux`_"],"delete":["139","  and :issue:`11315` by :user:`Thomas Fan <thomasjpfan>`_.","210","  :issue:`11166`by `Gael Varoquaux`_"]}],"doc\/modules\/compose.rst":[{"add":["502"," * :ref:`sphx_glr_auto_examples_compose_plot_column_transformer.py`","503"," * :ref:`sphx_glr_auto_examples_compose_plot_column_transformer_mixed_types.py`"],"delete":["502"," * :ref:`sphx_glr_auto_examples_compose_column_transformer.py`","503"," * :ref:`sphx_glr_auto_examples_compose_column_transformer_mixed_types.py`"]}],"sklearn\/tests\/test_discriminant_analysis.py":[{"add":["326","    assert_warns_message(DeprecationWarning, \"Attribute ``covariances_`` was \"","328","                         \"in 0.21. Use ``covariance_`` instead\", getattr, clf,"],"delete":["326","    assert_warns_message(DeprecationWarning, \"Attribute covariances_ was \"","328","                         \"in 0.21. Use covariance_ instead\", getattr, clf,"]}],"sklearn\/discriminant_analysis.py":[{"add":["624","    @deprecated(\"Attribute ``covariances_`` was deprecated in version\"","626","                \"``covariance_`` instead\")"],"delete":["624","    @deprecated(\"Attribute covariances_ was deprecated in version\"","626","                \"covariance_ instead\")"]}],"doc\/modules\/preprocessing.rst":[{"add":["584","  * :ref:`sphx_glr_auto_examples_preprocessing_plot_discretization.py`","585","  * :ref:`sphx_glr_auto_examples_preprocessing_plot_discretization_classification.py`","586","  * :ref:`sphx_glr_auto_examples_preprocessing_plot_discretization_strategies.py`"],"delete":["584","  * :ref:`sphx_glr_auto_examples_plot_discretization.py`","585","  * :ref:`sphx_glr_auto_examples_plot_discretization_classification.py`","586","  * :ref:`sphx_glr_auto_examples_plot_discretization_strategies.py`"]}],"sklearn\/preprocessing\/_encoders.py":[{"add":["200","            The ``active_features_`` attribute was deprecated in version","207","        (and then potentially masked by ``active_features_`` afterwards)","210","            The ``feature_indices_`` attribute was deprecated in version","217","            The ``n_values_`` attribute was deprecated in version","271","    @deprecated(\"The ``active_features_`` attribute was deprecated in version \"","278","    @deprecated(\"The ``feature_indices_`` attribute was deprecated in version \"","285","    @deprecated(\"The ``n_values_`` attribute was deprecated in version \""],"delete":["200","            The `active_features_` attribute was deprecated in version","207","        (and then potentially masked by `active_features_` afterwards)","210","            The `feature_indices_` attribute was deprecated in version","217","            The `n_values_` attribute was deprecated in version","271","    @deprecated(\"The 'active_features_' attribute was deprecated in version \"","278","    @deprecated(\"The 'feature_indices_' attribute was deprecated in version \"","285","    @deprecated(\"The 'n_values_' attribute was deprecated in version \""]}],"doc\/faq.rst":[{"add":["364","See also :ref:`sphx_glr_auto_examples_compose_plot_column_transformer_mixed_types.py` for an"],"delete":["364","See also :ref:`sphx_glr_auto_examples_compose_column_transformer_mixed_types.py` for an"]}],"sklearn\/datasets\/descr\/wine_data.rst":[{"add":["23","","94","  (Also submitted to Journal of Chemometrics)."],"delete":["93","  (Also submitted to Journal of Chemometrics)."]}],"sklearn\/decomposition\/pca.py":[{"add":["131","        If ``n_components == 'mle'`` and ``svd_solver == 'full'``, Minka's"],"delete":["131","        If ``n_components == 'mle'`` and ``svd_solver == 'full'``, Minka\\'s"]}],"doc\/developers\/contributing.rst":[{"add":["43","=================="],"delete":["43","======="]}]}},"f89131bc37c172aafb54db461089b0cbc5bcaf62":{"changes":{"sklearn\/neighbors\/binary_tree.pxi":"MODIFY","doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/neighbors\/kde.py":"MODIFY","sklearn\/neighbors\/ball_tree.pyx":"MODIFY","sklearn\/neighbors\/tests\/test_kde.py":"MODIFY"},"diff":{"sklearn\/neighbors\/binary_tree.pxi":[{"add":["1010","    cdef np.ndarray sample_weight_arr","1016","    cdef readonly DTYPE_t[::1] sample_weight","1017","    cdef public DTYPE_t sum_weight","1041","        self.sample_weight_arr = np.empty(1, dtype=DTYPE, order='C')","1047","        self.sample_weight = get_memview_DTYPE_1D(self.sample_weight_arr)","1064","                 leaf_size=40, metric='minkowski', sample_weight=None, **kwargs):","1068","","1090","","1091","        if sample_weight is not None:","1092","            self.sample_weight_arr = np.asarray(sample_weight, dtype=DTYPE, order='C')","1093","            self.sample_weight = get_memview_DTYPE_1D(self.sample_weight_arr)","1094","            self.sum_weight = np.sum(self.sample_weight)","1095","        else:","1096","            self.sample_weight = None","1097","            self.sum_weight = <DTYPE_t> n_samples","1098","","1099","","1672","                log_min_bound = (log(self.sum_weight) +","1675","                log_max_bound = (log(self.sum_weight) +","2142","        cdef ITYPE_t i, i1, i2, i_node","2143","        cdef DTYPE_t N1, N2","2148","        cdef bint with_sample_weight = self.sample_weight is not None","2149","        cdef DTYPE_t* sample_weight","2150","        if with_sample_weight:","2151","            sample_weight = &self.sample_weight[0]","2154","        cdef DTYPE_t N","2155","        cdef DTYPE_t log_weight","2156","        if with_sample_weight:","2157","            N = self.sum_weight","2158","        else:","2159","            N = <DTYPE_t> self.data.shape[0]","2191","            if with_sample_weight:","2192","                N1 = _total_node_weight(node_data, sample_weight,","2193","                                        idx_array, i_node)","2194","            else:","2195","                N1 = node_info.idx_end - node_info.idx_start","2224","                    if with_sample_weight:","2225","                        log_weight = np.log(sample_weight[idx_array[i]])","2226","                    else:","2227","                        log_weight = 0.","2229","                                                     log_density + log_weight)","2237","                if with_sample_weight:","2238","                    N1 = _total_node_weight(node_data, sample_weight,","2239","                                            idx_array, i1)","2240","                    N2 = _total_node_weight(node_data, sample_weight,","2241","                                            idx_array, i2)","2242","                else:","2243","                    N1 = node_data[i1].idx_end - node_data[i1].idx_start","2244","                    N2 = node_data[i2].idx_end - node_data[i2].idx_start","2306","        cdef ITYPE_t i, i1, i2, iw, start, end","2307","        cdef DTYPE_t N1, N2","2310","        cdef NodeData_t* node_data = &self.node_data[0]","2311","        cdef bint with_sample_weight = self.sample_weight is not None","2312","        cdef DTYPE_t* sample_weight","2313","        cdef DTYPE_t log_weight","2314","        if with_sample_weight:","2315","            sample_weight = &self.sample_weight[0]","2326","        if with_sample_weight:","2327","            N1  = _total_node_weight(node_data, sample_weight,","2328","                                     idx_array, i_node)","2329","            N2 = self.sum_weight","2330","        else:","2331","            N1 = <DTYPE_t>(node_info.idx_end - node_info.idx_start)","2332","            N2 = <DTYPE_t>self.data.shape[0]","2359","                if with_sample_weight:","2360","                    log_weight = np.log(sample_weight[idx_array[i]])","2361","                else:","2362","                    log_weight = 0.","2364","                                                    (log_dens_contribution +","2365","                                                     log_weight))","2373","            if with_sample_weight:","2374","                N1 = _total_node_weight(node_data, sample_weight,","2375","                                        idx_array, i1)","2376","                N2 = _total_node_weight(node_data, sample_weight,","2377","                                        idx_array, i2)","2378","            else:","2379","                N1 = <DTYPE_t>(self.node_data[i1].idx_end - self.node_data[i1].idx_start)","2380","                N2 = <DTYPE_t>(self.node_data[i2].idx_end - self.node_data[i2].idx_start)","2605","","2606","cdef inline DTYPE_t _total_node_weight(NodeData_t* node_data,","2607","                                       DTYPE_t* sample_weight,","2608","                                       ITYPE_t* idx_array,","2609","                                       ITYPE_t i_node):","2610","    cdef ITYPE_t i","2611","    cdef DTYPE_t N = 0.0","2612","    for i in range(node_data[i_node].idx_start, node_data[i_node].idx_end):","2613","        N += sample_weight[idx_array[i]]","2614","    return N"],"delete":["1059","                 leaf_size=40, metric='minkowski', **kwargs):","1656","                log_min_bound = (log(n_samples) +","1659","                log_max_bound = (log(n_samples) +","2126","        cdef ITYPE_t i, i1, i2, N1, N2, i_node","2133","        cdef ITYPE_t N = self.data.shape[0]","2165","            N1 = node_info.idx_end - node_info.idx_start","2195","                                                     log_density)","2203","                N1 = node_data[i1].idx_end - node_data[i1].idx_start","2204","                N2 = node_data[i2].idx_end - node_data[i2].idx_start","2266","        cdef ITYPE_t i, i1, i2, N1, N2","2279","        N1 = node_info.idx_end - node_info.idx_start","2280","        N2 = self.data.shape[0]","2308","                                                    log_dens_contribution)","2316","            N1 = self.node_data[i1].idx_end - self.node_data[i1].idx_start","2317","            N2 = self.node_data[i2].idx_end - self.node_data[i2].idx_start"]}],"doc\/whats_new\/v0.20.rst":[{"add":["208","- Add `sample_weight` parameter to the fit method of","209","  :class:`neighbors.KernelDensity` to enables weighting in kernel density","210","  estimation.","211","  :issue:`4394` by :user:`Samuel O. Ronsin <samronsin>`.","212",""],"delete":[]}],"sklearn\/neighbors\/kde.py":[{"add":["9","from ..utils import check_array, check_random_state, check_consistent_length","10","","115","    def fit(self, X, y=None, sample_weight=None):","123","        sample_weight: array_like, shape (n_samples,), optional","124","            List of sample weights attached to the data X.","129","        if sample_weight is not None:","130","            sample_weight = check_array(sample_weight, order='C', dtype=DTYPE,","131","                                        ensure_2d=False)","132","            if sample_weight.ndim != 1:","133","                raise ValueError(\"the shape of sample_weight must be ({0},),\"","134","                                 \" but was {1}\".format(X.shape[0],","135","                                                       sample_weight.shape))","136","            check_consistent_length(X, sample_weight)","137","            if sample_weight.min() <= 0:","138","                raise ValueError(\"sample_weight must have positive values\")","139","","145","                                          sample_weight=sample_weight,","167","        if self.tree_.sample_weight is None:","168","            N = self.tree_.data.shape[0]","169","        else:","170","            N = self.tree_.sum_weight","222","        u = rng.uniform(0, 1, size=n_samples)","223","        if self.tree_.sample_weight is None:","224","            i = (u * data.shape[0]).astype(np.int64)","225","        else:","226","            cumsum_weight = np.cumsum(np.asarray(self.tree_.sample_weight))","227","            sum_weight = cumsum_weight[-1]","228","            i = np.searchsorted(cumsum_weight, u * sum_weight)"],"delete":["9","from ..utils import check_array, check_random_state","114","    def fit(self, X, y=None):","152","        N = self.tree_.data.shape[0]","204","        i = rng.randint(data.shape[0], size=n_samples)","205",""]}],"sklearn\/neighbors\/ball_tree.pyx":[{"add":["64","    cdef bint with_sample_weight = tree.sample_weight is not None","65","    cdef DTYPE_t* sample_weight","66","    cdef DTYPE_t sum_weight_node","67","    if with_sample_weight:","68","        sample_weight = &tree.sample_weight[0]","69","","74","    if with_sample_weight:","75","        sum_weight_node = 0","76","        for i in range(idx_start, idx_end):","77","            sum_weight_node += sample_weight[idx_array[i]]","78","            this_pt = data + n_features * idx_array[i]","79","            for j from 0 <= j < n_features:","80","                centroid[j] += this_pt[j] * sample_weight[idx_array[i]]","82","        for j in range(n_features):","83","            centroid[j] \/= sum_weight_node","84","    else:","85","        for i in range(idx_start, idx_end):","86","            this_pt = data + n_features * idx_array[i]","87","            for j from 0 <= j < n_features:","88","                centroid[j] += this_pt[j]","89","","90","        for j in range(n_features):","91","            centroid[j] \/= n_points"],"delete":["68","    for i in range(idx_start, idx_end):","69","        this_pt = data + n_features * idx_array[i]","70","        for j from 0 <= j < n_features:","71","            centroid[j] += this_pt[j]","73","    for j in range(n_features):","74","        centroid[j] \/= n_points"]}],"sklearn\/neighbors\/tests\/test_kde.py":[{"add":["139","    kde = KernelDensity()","140","    assert_raises(ValueError, kde.fit, np.random.random((200, 10)),","141","                  sample_weight=np.random.random((200, 10)))","142","    assert_raises(ValueError, kde.fit, np.random.random((200, 10)),","143","                  sample_weight=-np.random.random(200))","156","","157","","158","def test_kde_sample_weights():","159","    n_samples = 400","160","    size_test = 20","161","    weights_neutral = 3 * np.ones(n_samples)","162","    for d in [1, 2, 10]:","163","        rng = np.random.RandomState(0)","164","        X = rng.rand(n_samples, d)","165","        weights = 1 + (10 * X.sum(axis=1)).astype(np.int8)","166","        X_repetitions = np.repeat(X, weights, axis=0)","167","        n_samples_test = size_test \/\/ d","168","        test_points = rng.rand(n_samples_test, d)","169","        for algorithm in ['auto', 'ball_tree', 'kd_tree']:","170","            for metric in ['euclidean', 'minkowski', 'manhattan',","171","                           'chebyshev']:","172","                if algorithm != 'kd_tree' or metric in KDTree.valid_metrics:","173","                    kde = KernelDensity(algorithm=algorithm, metric=metric)","174","","175","                    # Test that adding a constant sample weight has no effect","176","                    kde.fit(X, sample_weight=weights_neutral)","177","                    scores_const_weight = kde.score_samples(test_points)","178","                    sample_const_weight = kde.sample(random_state=1234)","179","                    kde.fit(X)","180","                    scores_no_weight = kde.score_samples(test_points)","181","                    sample_no_weight = kde.sample(random_state=1234)","182","                    assert_allclose(scores_const_weight, scores_no_weight)","183","                    assert_allclose(sample_const_weight, sample_no_weight)","184","","185","                    # Test equivalence between sampling and (integer) weights","186","                    kde.fit(X, sample_weight=weights)","187","                    scores_weight = kde.score_samples(test_points)","188","                    sample_weight = kde.sample(random_state=1234)","189","                    kde.fit(X_repetitions)","190","                    scores_ref_sampling = kde.score_samples(test_points)","191","                    sample_ref_sampling = kde.sample(random_state=1234)","192","                    assert_allclose(scores_weight, scores_ref_sampling)","193","                    assert_allclose(sample_weight, sample_ref_sampling)","194","","195","                    # Test that sample weights has a non-trivial effect","196","                    diff = np.max(np.abs(scores_no_weight - scores_weight))","197","                    assert diff > 0.001","198","","199","                    # Test invariance with respect to arbitrary scaling","200","                    scale_factor = rng.rand()","201","                    kde.fit(X, sample_weight=(scale_factor * weights))","202","                    scores_scaled_weight = kde.score_samples(test_points)","203","                    assert_allclose(scores_scaled_weight, scores_weight)"],"delete":[]}]}},"3a704b1ad3e73a492c46271b98464a71ba46c190":{"changes":{"doc\/modules\/clustering.rst":"MODIFY","doc\/modules\/classes.rst":"MODIFY"},"diff":{"doc\/modules\/clustering.rst":[{"add":["1584","","1585",".. _contingency_matrix:","1586","","1587","Contingency Matrix","1588","------------------","1589","","1590","Contingency matrix (:func:`sklearn.metrics.cluster.contingency_matrix`)","1591","reports the intersection cardinality for every true\/predicted cluster pair.","1592","The contingency matrix provides sufficient statistics for all clustering","1593","metrics where the samples are independent and identically distributed and","1594","one doesn't need to account for some instances not being clustered.","1595","","1596","Here is an example::","1597","","1598","   >>> from sklearn.metrics.cluster import contingency_matrix","1599","   >>> x = [\"a\", \"a\", \"a\", \"b\", \"b\", \"b\"]","1600","   >>> y = [0, 0, 1, 1, 2, 2]","1601","   >>> contingency_matrix(x, y)","1602","   array([[2, 1, 0],","1603","          [0, 1, 2]])","1604","","1605","The first row of output array indicates that there are three samples whose","1606","true cluster is \"a\". Of them, two are in predicted cluster 0, one is in 1,","1607","and none is in 2. And the second row indicates that there are three samples","1608","whose true cluster is \"b\". Of them, none is in predicted cluster 0, one is in","1609","1 and two are in 2.","1610","","1611","A :ref:`confusion matrix <confusion_matrix>` for classification is a square","1612","contingency matrix where the order of rows and columns correspond to a list","1613","of classes.","1614","","1615","","1616","Advantages","1617","~~~~~~~~~~","1618","","1619","- Allows to examine the spread of each true cluster across predicted","1620","  clusters and vice versa.","1621","","1622","- The contingency table calculated is typically utilized in the calculation","1623","  of a similarity statistic (like the others listed in this document) between","1624","  the two clusterings.","1625","","1626","Drawbacks","1627","~~~~~~~~~","1628","","1629","- Contingency matrix is easy to interpret for a small number of clusters, but","1630","  becomes very hard to interpret for a large number of clusters.","1631","","1632","- It doesn't give a single metric to use as an objective for clustering","1633","  optimisation.","1634","","1635","","1636",".. topic:: References","1637","","1638"," * `Wikipedia entry for contingency matrix","1639","   <https:\/\/en.wikipedia.org\/wiki\/Contingency_table>`_"],"delete":[]}],"doc\/modules\/classes.rst":[{"add":["856","   metrics.cluster.contingency_matrix"],"delete":[]}]}},"b961ff4a79dc98f8886178a7e3b55b4c8c7f1991":{"changes":{"doc\/modules\/model_persistence.rst":"MODIFY"},"diff":{"doc\/modules\/model_persistence.rst":[{"add":["75","* The training data, e.g. a reference to an immutable snapshot"],"delete":["75","* The training data, e.g. a reference to a immutable snapshot"]}]}},"3f96eeedd122d6cb83536e0650dc5735546f3692":{"changes":{"conftest.py":"MODIFY"},"diff":{"conftest.py":[{"add":["11","    np.set_printoptions(legacy=True)"],"delete":["11","    np.set_printoptions(sign='legacy')"]}]}},"2160ea0a74bf259012c5b5d47502e3a3267b2844":{"changes":{"sklearn\/decomposition\/tests\/test_nmf.py":"MODIFY","sklearn\/decomposition\/nmf.py":"MODIFY"},"diff":{"sklearn\/decomposition\/tests\/test_nmf.py":[{"add":["481","","482","","483","def test_nmf_underflow():","484","    # Regression test for an underflow issue in _beta_divergence","485","    rng = np.random.RandomState(0)","486","    n_samples, n_features, n_components = 10, 2, 2","487","    X = np.abs(rng.randn(n_samples, n_features)) * 10","488","    W = np.abs(rng.randn(n_samples, n_components)) * 10","489","    H = np.abs(rng.randn(n_components, n_features))","490","","491","    X[0, 0] = 0","492","    ref = nmf._beta_divergence(X, W, H, beta=1.0)","493","    X[0, 0] = 1e-323","494","    res = nmf._beta_divergence(X, W, H, beta=1.0)","495","    assert_almost_equal(res, ref)"],"delete":[]}],"sklearn\/decomposition\/nmf.py":[{"add":["116","    indices = X_data > EPSILON","117","    WH_data = WH_data[indices]","118","    X_data = X_data[indices]"],"delete":["116","    WH_data = WH_data[X_data != 0]","117","    X_data = X_data[X_data != 0]"]}]}},"d9863547875abfe7ab690b5b70058d4f4daa65c8":{"changes":{"sklearn\/tests\/test_impute.py":"MODIFY","sklearn\/impute.py":"MODIFY"},"diff":{"sklearn\/tests\/test_impute.py":[{"add":["707","","708","","709","@pytest.mark.parametrize(\"imputer_constructor\",","710","                         [SimpleImputer, ChainedImputer])","711","@pytest.mark.parametrize(","712","    \"imputer_missing_values, missing_value, err_msg\",","713","    [(\"NaN\", np.nan, \"Input contains NaN\"),","714","     (\"-1\", -1, \"types are expected to be both numerical.\")])","715","def test_inconsistent_dtype_X_missing_values(imputer_constructor,","716","                                             imputer_missing_values,","717","                                             missing_value,","718","                                             err_msg):","719","    # regression test for issue #11390. Comparison between incoherent dtype","720","    # for X and missing_values was not raising a proper error.","721","    rng = np.random.RandomState(42)","722","    X = rng.randn(10, 10)","723","    X[0, 0] = missing_value","724","","725","    imputer = imputer_constructor(missing_values=imputer_missing_values)","726","","727","    with pytest.raises(ValueError, match=err_msg):","728","        imputer.fit_transform(X)"],"delete":[]}],"sklearn\/impute.py":[{"add":["42","def _check_inputs_dtype(X, missing_values):","43","    if (X.dtype.kind in (\"f\", \"i\", \"u\") and","44","            not isinstance(missing_values, numbers.Real)):","45","        raise ValueError(\"'X' and 'missing_values' types are expected to be\"","46","                         \" both numerical. Got X.dtype={} and \"","47","                         \" type(missing_values)={}.\"","48","                         .format(X.dtype, type(missing_values)))","49","","50","","193","        _check_inputs_dtype(X, self.missing_values)","799","        _check_inputs_dtype(X, self.missing_values)"],"delete":["53",""]}]}},"c09352c2419217588c5f49c9258fd9722722f12b":{"changes":{"sklearn\/metrics\/scorer.py":"MODIFY","sklearn\/model_selection\/_split.py":"MODIFY","sklearn\/utils\/fixes.py":"MODIFY","sklearn\/model_selection\/tests\/test_search.py":"MODIFY","sklearn\/covariance\/graph_lasso_.py":"MODIFY","sklearn\/utils\/multiclass.py":"MODIFY","sklearn\/datasets\/samples_generator.py":"MODIFY","sklearn\/tests\/test_init.py":"MODIFY","sklearn\/utils\/__init__.py":"MODIFY","sklearn\/feature_extraction\/dict_vectorizer.py":"MODIFY","sklearn\/feature_extraction\/tests\/test_text.py":"MODIFY","sklearn\/model_selection\/_search.py":"MODIFY"},"diff":{"sklearn\/metrics\/scorer.py":[{"add":["41","from ..utils.fixes import _Iterable as Iterable"],"delete":["21","from collections import Iterable"]}],"sklearn\/model_selection\/_split.py":[{"add":["30","from ..utils.fixes import _Iterable as Iterable"],"delete":["17","from collections import Iterable"]}],"sklearn\/utils\/fixes.py":[{"add":["312","","313","","314","# To be removed once this fix is included in six","315","try:","316","    from collections.abc import Sequence as _Sequence  # noqa","317","    from collections.abc import Iterable as _Iterable  # noqa","318","    from collections.abc import Mapping as _Mapping  # noqa","319","    from collections.abc import Sized as _Sized  # noqa","320","except ImportError:  # python <3.3","321","    from collections import Sequence as _Sequence  # noqa","322","    from collections import Iterable as _Iterable  # noqa","323","    from collections import Mapping as _Mapping  # noqa","324","    from collections import Sized as _Sized  # noqa"],"delete":[]}],"sklearn\/model_selection\/tests\/test_search.py":[{"add":["16","from sklearn.utils.fixes import _Iterable as Iterable, _Sized as Sized"],"delete":["2","from collections import Iterable, Sized"]}],"sklearn\/covariance\/graph_lasso_.py":[{"add":["21","from ..utils.fixes import _Sequence as Sequence","610","        if isinstance(n_alphas, Sequence):","686","            if not isinstance(n_alphas, Sequence):"],"delete":["25","import collections","610","        if isinstance(n_alphas, collections.Sequence):","686","            if not isinstance(n_alphas, collections.Sequence):"]}],"sklearn\/utils\/multiclass.py":[{"add":["19","from ..utils.fixes import _Sequence as Sequence"],"delete":["9","from collections import Sequence","23",""]}],"sklearn\/datasets\/samples_generator.py":[{"add":["17","from ..utils.fixes import _Iterable as Iterable"],"delete":["13","from collections import Iterable"]}],"sklearn\/tests\/test_init.py":[{"add":["2","import subprocess","3","","4","import pkgutil","5","","6","import pytest","7","","8","import sklearn","9","from sklearn.utils.testing import assert_equal","10","","27","","28","","29","def test_import_sklearn_no_warnings():","30","    # Test that importing scikit-learn main modules doesn't raise any warnings.","31","","32","    try:","33","        pkgs = pkgutil.iter_modules(path=sklearn.__path__, prefix='sklearn.')","34","        import_modules = '; '.join(['import ' + modname","35","                                    for _, modname, _ in pkgs","36","                                    if (not modname.startswith('_') and","37","                                        # add deprecated top level modules","38","                                        # below to ignore them","39","                                        modname not in [])])","40","","41","        message = subprocess.check_output(['python', '-Wdefault',","42","                                           '-c', import_modules],","43","                                          stderr=subprocess.STDOUT)","44","        message = message.decode(\"utf-8\")","45","        message = '\\n'.join([line for line in message.splitlines()","46","                             if not (","47","                                     # ignore ImportWarning due to Cython","48","                                     \"ImportWarning\" in line or","49","                                     # ignore DeprecationWarning due to pytest","50","                                     \"pytest\" in line or","51","                                     # ignore DeprecationWarnings due to","52","                                     # numpy.oldnumeric","53","                                     \"oldnumeric\" in line","54","                                     )])","55","        assert 'Warning' not in message","56","        assert 'Error' not in message","57","","58","    except Exception as e:","59","        pytest.skip('soft-failed test_import_sklearn_no_warnings.\\n'","60","                    ' %s, \\n %s' % (e, message))"],"delete":["6","from sklearn.utils.testing import assert_equal","7",""]}],"sklearn\/utils\/__init__.py":[{"add":["3","","19","from ..utils.fixes import _Sequence as Sequence"],"delete":["3","from collections import Sequence"]}],"sklearn\/feature_extraction\/dict_vectorizer.py":[{"add":["14","from ..utils.fixes import _Mapping as Mapping"],"delete":["5","from collections import Mapping"]}],"sklearn\/feature_extraction\/tests\/test_text.py":[{"add":["35","from sklearn.utils.fixes import _Mapping as Mapping","36","from collections import defaultdict"],"delete":["35","","36","from collections import defaultdict, Mapping"]}],"sklearn\/model_selection\/_search.py":[{"add":["15","from collections import namedtuple, defaultdict","36","from ..utils.fixes import _Mapping as Mapping, _Sequence as Sequence","37","from ..utils.fixes import _Iterable as Iterable"],"delete":["15","from collections import Mapping, namedtuple, defaultdict, Sequence, Iterable"]}]}},"727314b2feda7e9c4c7de460eff8a360def0e440":{"changes":{"sklearn\/preprocessing\/tests\/test_data.py":"MODIFY","sklearn\/preprocessing\/data.py":"MODIFY"},"diff":{"sklearn\/preprocessing\/tests\/test_data.py":[{"add":["2113","    assert np.issubdtype(enc.categories_[0].dtype, np.str_)","2127","    assert np.issubdtype(enc.categories_[0].dtype, np.str_)","2229","def test_categorical_encoder_warning():","2230","    enc = CategoricalEncoder()","2231","    X = [['Male', 1], ['Female', 3]]","2232","    np.testing.assert_no_warnings(enc.fit_transform, X)","2233","","2234",""],"delete":["2113","    assert np.issubdtype(enc.categories_[0].dtype, str)","2127","    assert np.issubdtype(enc.categories_[0].dtype, str)"]}],"sklearn\/preprocessing\/data.py":[{"add":["2998","        if not hasattr(X, 'dtype') and np.issubdtype(X_temp.dtype, np.str_):","3041","        if not hasattr(X, 'dtype') and np.issubdtype(X_temp.dtype, np.str_):"],"delete":["2998","        if not hasattr(X, 'dtype') and np.issubdtype(X_temp.dtype, str):","3041","        if not hasattr(X, 'dtype') and np.issubdtype(X_temp.dtype, str):"]}]}},"6eb19831d1725027afa1f0d941af97aab50db9a1":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","examples\/decomposition\/plot_faces_decomposition.py":"MODIFY","sklearn\/decomposition\/tests\/test_sparse_pca.py":"MODIFY","sklearn\/decomposition\/sparse_pca.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["79","- :class:`decomposition.SparsePCA` (bug fix)","195","- :class:`decomposition.SparsePCA` now exposes ``normalize_components``. When","196","  set to True, the train and test data are centered with the train mean ","197","  repsectively during the fit phase and the transform phase. This fixes the","198","  behavior of SparsePCA. When set to False, which is the default, the previous","199","  abnormal behaviour still holds. The False value is for backward","200","  compatibility and should not be used.","201","  :issue:`11585` by :user:`Ivan Panico <FollowKenny>`.","202",""],"delete":[]}],"examples\/decomposition\/plot_faces_decomposition.py":[{"add":["83","                                      random_state=rng,","84","                                      normalize_components=True),"],"delete":["83","                                      random_state=rng),"]}],"sklearn\/decomposition\/tests\/test_sparse_pca.py":[{"add":["4","import pytest","10","from sklearn.utils.testing import assert_allclose","17","from sklearn.decomposition import SparsePCA, MiniBatchSparsePCA, PCA","46","@pytest.mark.filterwarnings(\"ignore:normalize_components\")","47","@pytest.mark.parametrize(\"norm_comp\", [False, True])","48","def test_correct_shapes(norm_comp):","51","    spca = SparsePCA(n_components=8, random_state=rng,","52","                     normalize_components=norm_comp)","57","    spca = SparsePCA(n_components=13, random_state=rng,","58","                     normalize_components=norm_comp)","64","@pytest.mark.filterwarnings(\"ignore:normalize_components\")","65","@pytest.mark.parametrize(\"norm_comp\", [False, True])","66","def test_fit_transform(norm_comp):","71","                          random_state=0, normalize_components=norm_comp)","76","                           alpha=alpha, normalize_components=norm_comp)","88","@pytest.mark.filterwarnings(\"ignore:normalize_components\")","89","@pytest.mark.parametrize(\"norm_comp\", [False, True])","91","def test_fit_transform_parallel(norm_comp):","96","                          random_state=0, normalize_components=norm_comp)","101","                     random_state=0, normalize_components=norm_comp).fit(Y)","107","@pytest.mark.filterwarnings(\"ignore:normalize_components\")","108","@pytest.mark.parametrize(\"norm_comp\", [False, True])","109","def test_transform_nan(norm_comp):","115","    estimator = SparsePCA(n_components=8, normalize_components=norm_comp)","119","@pytest.mark.filterwarnings(\"ignore:normalize_components\")","120","@pytest.mark.parametrize(\"norm_comp\", [False, True])","121","def test_fit_transform_tall(norm_comp):","125","                          random_state=rng, normalize_components=norm_comp)","127","    spca_lasso = SparsePCA(n_components=3, method='cd',","128","                           random_state=rng, normalize_components=norm_comp)","133","@pytest.mark.filterwarnings(\"ignore:normalize_components\")","134","@pytest.mark.parametrize(\"norm_comp\", [False, True])","135","def test_initialization(norm_comp):","140","                      random_state=rng, normalize_components=norm_comp)","142","    if norm_comp:","143","        assert_allclose(model.components_,","144","                        V_init \/ np.linalg.norm(V_init, axis=1)[:, None])","145","    else:","146","        assert_allclose(model.components_, V_init)","149","@pytest.mark.filterwarnings(\"ignore:normalize_components\")","150","@pytest.mark.parametrize(\"norm_comp\", [False, True])","151","def test_mini_batch_correct_shapes(norm_comp):","154","    pca = MiniBatchSparsePCA(n_components=8, random_state=rng,","155","                             normalize_components=norm_comp)","160","    pca = MiniBatchSparsePCA(n_components=13, random_state=rng,","161","                             normalize_components=norm_comp)","167","@pytest.mark.filterwarnings(\"ignore:normalize_components\")","168","@pytest.mark.parametrize(\"norm_comp\", [False, True])","169","def test_mini_batch_fit_transform(norm_comp):","175","                                   alpha=alpha,","176","                                   normalize_components=norm_comp).fit(Y)","184","            spca = MiniBatchSparsePCA(n_components=3, n_jobs=2, alpha=alpha,","185","                                      random_state=0,","186","                                      normalize_components=norm_comp)","187","            U2 = spca.fit(Y).transform(Y)","191","        spca = MiniBatchSparsePCA(n_components=3, n_jobs=2, alpha=alpha,","192","                                  random_state=0,","193","                                  normalize_components=norm_comp)","194","        U2 = spca.fit(Y).transform(Y)","199","                                    random_state=0,","200","                                    normalize_components=norm_comp).fit(Y)","202","","203","","204","def test_scaling_fit_transform():","205","    alpha = 1","206","    rng = np.random.RandomState(0)","207","    Y, _, _ = generate_toy_data(3, 1000, (8, 8), random_state=rng)","208","    spca_lars = SparsePCA(n_components=3, method='lars', alpha=alpha,","209","                          random_state=rng, normalize_components=True)","210","    results_train = spca_lars.fit_transform(Y)","211","    results_test = spca_lars.transform(Y[:10])","212","    assert_allclose(results_train[0], results_test[0])","213","","214","","215","def test_pca_vs_spca():","216","    rng = np.random.RandomState(0)","217","    Y, _, _ = generate_toy_data(3, 1000, (8, 8), random_state=rng)","218","    Z, _, _ = generate_toy_data(3, 10, (8, 8), random_state=rng)","219","    spca = SparsePCA(alpha=0, ridge_alpha=0, n_components=2,","220","                     normalize_components=True)","221","    pca = PCA(n_components=2)","222","    pca.fit(Y)","223","    spca.fit(Y)","224","    results_test_pca = pca.transform(Z)","225","    results_test_spca = spca.transform(Z)","226","    assert_allclose(np.abs(spca.components_.dot(pca.components_.T)),","227","                    np.eye(2), atol=1e-5)","228","    results_test_pca *= np.sign(results_test_pca[0, :])","229","    results_test_spca *= np.sign(results_test_spca[0, :])","230","    assert_allclose(results_test_pca, results_test_spca)","231","","232","","233","@pytest.mark.parametrize(\"spca\", [SparsePCA, MiniBatchSparsePCA])","234","def test_spca_deprecation_warning(spca):","235","    rng = np.random.RandomState(0)","236","    Y, _, _ = generate_toy_data(3, 10, (8, 8), random_state=rng)","237","    warn_message = \"normalize_components\"","238","    assert_warns_message(DeprecationWarning, warn_message,","239","                         spca(normalize_components=False).fit, Y)"],"delete":["9","from sklearn.utils.testing import assert_array_equal","16","from sklearn.decomposition import SparsePCA, MiniBatchSparsePCA","45","def test_correct_shapes():","48","    spca = SparsePCA(n_components=8, random_state=rng)","53","    spca = SparsePCA(n_components=13, random_state=rng)","59","def test_fit_transform():","64","                          random_state=0)","69","                           alpha=alpha)","82","def test_fit_transform_parallel():","87","                          random_state=0)","92","                     random_state=0).fit(Y)","98","def test_transform_nan():","104","    estimator = SparsePCA(n_components=8)","108","def test_fit_transform_tall():","112","                          random_state=rng)","114","    spca_lasso = SparsePCA(n_components=3, method='cd', random_state=rng)","119","def test_initialization():","124","                      random_state=rng)","126","    assert_array_equal(model.components_, V_init)","129","def test_mini_batch_correct_shapes():","132","    pca = MiniBatchSparsePCA(n_components=8, random_state=rng)","137","    pca = MiniBatchSparsePCA(n_components=13, random_state=rng)","143","def test_mini_batch_fit_transform():","149","                                   alpha=alpha).fit(Y)","157","            U2 = MiniBatchSparsePCA(n_components=3, n_jobs=2, alpha=alpha,","158","                                    random_state=0).fit(Y).transform(Y)","162","        U2 = MiniBatchSparsePCA(n_components=3, n_jobs=2, alpha=alpha,","163","                                random_state=0).fit(Y).transform(Y)","168","                                    random_state=0).fit(Y)"]}],"sklearn\/decomposition\/sparse_pca.py":[{"add":["68","    normalize_components : boolean, optional (default=False)","69","        - if False, use a version of Sparse PCA without components","70","          normalization and without data centering. This is likely a bug and","71","          even though it's the default for backward compatibility,","72","          this should not be used.","73","        - if True, use a version of Sparse PCA with components normalization","74","          and data centering.","75","","76","        .. versionadded:: 0.20","77","","78","        .. deprecated:: 0.22","79","           ``normalize_components`` was added and set to ``False`` for","80","           backward compatibility. It would be set to ``True`` from 0.22","81","           onwards.","82","","94","    mean_ : array, shape (n_features,)","95","        Per-feature empirical mean, estimated from the training set.","96","        Equal to ``X.mean(axis=0)``.","97","","106","                 V_init=None, verbose=False, random_state=None,","107","                 normalize_components=False):","119","        self.normalize_components = normalize_components","139","","140","        if self.normalize_components:","141","            self.mean_ = X.mean(axis=0)","142","            X = X - self.mean_","143","        else:","144","            warnings.warn(\"normalize_components=False is a \"","145","                          \"backward-compatible setting that implements a \"","146","                          \"non-standard definition of sparse PCA. This \"","147","                          \"compatibility mode will be removed in 0.22.\",","148","                          DeprecationWarning)","149","","168","","169","        if self.normalize_components:","170","            components_norm = \\","171","                    np.linalg.norm(self.components_, axis=1)[:, np.newaxis]","172","            components_norm[components_norm == 0] = 1","173","            self.components_ \/= components_norm","174","","219","","220","        if self.normalize_components:","221","            X = X - self.mean_","222","","225","","226","        if not self.normalize_components:","227","            s = np.sqrt((U ** 2).sum(axis=0))","228","            s[s == 0] = 1","229","            U \/= s","230","","287","    normalize_components : boolean, optional (default=False)","288","        - if False, use a version of Sparse PCA without components","289","          normalization and without data centering. This is likely a bug and","290","          even though it's the default for backward compatibility,","291","          this should not be used.","292","        - if True, use a version of Sparse PCA with components normalization","293","          and data centering.","294","","295","        .. versionadded:: 0.20","296","","297","        .. deprecated:: 0.22","298","           ``normalize_components`` was added and set to ``False`` for","299","           backward compatibility. It would be set to ``True`` from 0.22","300","           onwards.","301","","310","    mean_ : array, shape (n_features,)","311","        Per-feature empirical mean, estimated from the training set.","312","        Equal to ``X.mean(axis=0)``.","313","","322","                 shuffle=True, n_jobs=1, method='lars', random_state=None,","323","                 normalize_components=False):","327","            random_state=random_state,","328","            normalize_components=normalize_components)","352","","353","        if self.normalize_components:","354","            self.mean_ = X.mean(axis=0)","355","            X = X - self.mean_","356","        else:","357","            warnings.warn(\"normalize_components=False is a \"","358","                          \"backward-compatible setting that implements a \"","359","                          \"non-standard definition of sparse PCA. This \"","360","                          \"compatibility mode will be removed in 0.22.\",","361","                          DeprecationWarning)","362","","378","","379","        if self.normalize_components:","380","            components_norm = \\","381","                    np.linalg.norm(self.components_, axis=1)[:, np.newaxis]","382","            components_norm[components_norm == 0] = 1","383","            self.components_ \/= components_norm","384",""],"delete":["87","                 V_init=None, verbose=False, random_state=None):","182","        s = np.sqrt((U ** 2).sum(axis=0))","183","        s[s == 0] = 1","184","        U \/= s","257","                 shuffle=True, n_jobs=1, method='lars', random_state=None):","261","            random_state=random_state)"]}]}},"84f5ee6529b2dd0d374b0ce0b172916d9b4e356b":{"changes":{"sklearn\/metrics\/classification.py":"MODIFY","doc\/modules\/calibration.rst":"MODIFY"},"diff":{"sklearn\/metrics\/classification.py":[{"add":["1930","    of only 0 and 1). The Brier loss is composed of refinement loss and","1931","    calibration loss."],"delete":["1921","","1923","","1932","    of only 0 and 1).","1933","","1940",""]}],"doc\/modules\/calibration.rst":[{"add":["111","both isotonic calibration and sigmoid calibration. ","112","The Brier score is a metric which is a combination of calibration loss and refinement loss,","113",":func:`brier_score_loss`, reported in the legend (the smaller the better).","114","Calibration loss is defined as the mean squared deviation from empirical probabilities","115","derived from the slope of ROC segments. Refinement loss can be defined as the expected","116","optimal loss as measured by the area under the optimal cost curve."],"delete":["111","both isotonic calibration and sigmoid calibration. The calibration performance","112","is evaluated with Brier score :func:`brier_score_loss`, reported in the legend","113","(the smaller the better)."]}]}},"beb2aa0395cdbc65843db9acf1a9fdfd25e18e7e":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/mixture\/tests\/test_gaussian_mixture.py":"MODIFY","sklearn\/mixture\/base.py":"MODIFY","sklearn\/mixture\/gaussian_mixture.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["62","- :class:`neural_network.BaseMultilayerPerceptron` (bug fix)","579","- Fixed a bug in :class:`mixture.BaseMixture` and its subclasses","580","  :class:`mixture.GaussianMixture` and :class:`mixture.BayesianGaussianMixture`","581","  where the ``lower_bound_`` was not the max lower bound across all","582","  initializations (when ``n_init > 1``), but just the lower bound of the last","583","  initialization. :issue:`10869` by :user:`Aur¨¦lien G¨¦ron <ageron>`.","584",""],"delete":["64","- :class:`neural_network.BaseMultilayerPerceptron` (bug fix)"]}],"sklearn\/mixture\/tests\/test_gaussian_mixture.py":[{"add":["807","@ignore_warnings(category=ConvergenceWarning)","808","def test_convergence_detected_with_warm_start():","809","    # We check that convergence is detected when warm_start=True","810","    rng = np.random.RandomState(0)","811","    rand_data = RandomData(rng)","812","    n_components = rand_data.n_components","813","    X = rand_data.X['full']","814","","815","    for max_iter in (1, 2, 50):","816","        gmm = GaussianMixture(n_components=n_components, warm_start=True,","817","                              max_iter=max_iter, random_state=rng)","818","        for _ in range(100):","819","            gmm.fit(X)","820","            if gmm.converged_:","821","                break","822","        assert gmm.converged_","823","        assert max_iter >= gmm.n_iter_","824","","825","","1011","    for random_state in range(25):","1012","        rand_data = RandomData(np.random.RandomState(random_state), scale=1)","1013","        n_components = rand_data.n_components","1014","        X = rand_data.X['full']","1016","        gmm1 = GaussianMixture(n_components=n_components, n_init=1,","1017","                               max_iter=1, random_state=random_state).fit(X)","1018","        gmm2 = GaussianMixture(n_components=n_components, n_init=10,","1019","                               max_iter=1, random_state=random_state).fit(X)","1021","        assert gmm2.lower_bound_ >= gmm1.lower_bound_"],"delete":["766","","993","    random_state = 0","994","    rand_data = RandomData(np.random.RandomState(random_state), scale=1)","995","    n_components = rand_data.n_components","996","    X = rand_data.X['full']","998","    gmm1 = GaussianMixture(n_components=n_components, n_init=1,","999","                           max_iter=1, random_state=random_state).fit(X)","1000","    gmm2 = GaussianMixture(n_components=n_components, n_init=100,","1001","                           max_iter=1, random_state=random_state).fit(X)","1003","    assert_greater(gmm2.lower_bound_, gmm1.lower_bound_)"]}],"sklearn\/mixture\/base.py":[{"add":["174","        The method fits the model ``n_init`` times and sets the parameters with","176","        trial, the method iterates between E-step and M-step for ``max_iter``","178","        ``tol``, otherwise, a ``ConvergenceWarning`` is raised.","179","        If ``warm_start`` is ``True``, then ``n_init`` is ignored and a single","180","        initialization is performed upon the first call. Upon consecutive","181","        calls, training starts where it left off.","237","","238","            lower_bound = (-np.infty if do_init else self.lower_bound_)","241","                prev_lower_bound = lower_bound","245","                lower_bound = self._compute_lower_bound(","248","                change = lower_bound - prev_lower_bound","255","            self._print_verbose_msg_init_end(lower_bound)","257","            if lower_bound > max_lower_bound:","258","                max_lower_bound = lower_bound","271","        self.lower_bound_ = max_lower_bound"],"delete":["174","        The method fits the model `n_init` times and set the parameters with","176","        trial, the method iterates between E-step and M-step for `max_iter`","178","        `tol`, otherwise, a `ConvergenceWarning` is raised.","234","                self.lower_bound_ = -np.infty","237","                prev_lower_bound = self.lower_bound_","241","                self.lower_bound_ = self._compute_lower_bound(","244","                change = self.lower_bound_ - prev_lower_bound","251","            self._print_verbose_msg_init_end(self.lower_bound_)","253","            if self.lower_bound_ > max_lower_bound:","254","                max_lower_bound = self.lower_bound_"]}],"sklearn\/mixture\/gaussian_mixture.py":[{"add":["514","        In that case, 'n_init' is ignored and only a single initialization","515","        occurs upon the first call.","579","        Lower bound value on the log-likelihood (of the training data with","580","        respect to the model) of the best fit of EM."],"delete":["577","        Log-likelihood of the best fit of EM."]}]}},"bcd6ff387e3b273fb5edf2fb0a12497655eb2df9":{"changes":{"sklearn\/utils\/estimator_checks.py":"MODIFY"},"diff":{"sklearn\/utils\/estimator_checks.py":[{"add":["90","    yield check_sample_weights_invariance","557","@ignore_warnings(category=(DeprecationWarning, FutureWarning))","558","def check_sample_weights_invariance(name, estimator_orig):","559","    # check that the estimators yield same results for","560","    # unit weights and no weights","561","    if (has_fit_parameter(estimator_orig, \"sample_weight\") and","562","            not (hasattr(estimator_orig, \"_pairwise\")","563","                 and estimator_orig._pairwise)):","564","        # We skip pairwise because the data is not pairwise","565","","566","        estimator1 = clone(estimator_orig)","567","        estimator2 = clone(estimator_orig)","568","        set_random_state(estimator1, random_state=0)","569","        set_random_state(estimator2, random_state=0)","570","","571","        X = np.array([[1, 3], [1, 3], [1, 3], [1, 3],","572","                      [2, 1], [2, 1], [2, 1], [2, 1],","573","                      [3, 3], [3, 3], [3, 3], [3, 3],","574","                      [4, 1], [4, 1], [4, 1], [4, 1]], dtype=np.dtype('float'))","575","        y = np.array([1, 1, 1, 1, 2, 2, 2, 2,","576","                      1, 1, 1, 1, 2, 2, 2, 2], dtype=np.dtype('int'))","577","","578","        estimator1.fit(X, y=y, sample_weight=np.ones(shape=len(y)))","579","        estimator2.fit(X, y=y, sample_weight=None)","580","","581","        for method in [\"predict\", \"transform\"]:","582","            if hasattr(estimator_orig, method):","583","                X_pred1 = getattr(estimator1, method)(X)","584","                X_pred2 = getattr(estimator2, method)(X)","585","                assert_allclose(X_pred1, X_pred2, rtol=0.5,","586","                                err_msg=\"For %s sample_weight=None is not\"","587","                                        \" equivalent to sample_weight=ones\"","588","                                        % name)","589","","590",""],"delete":[]}]}},"47ce5e1c9d0a63ed3b56b4a9f44fb51a1b35b7cd":{"changes":{"sklearn\/feature_extraction\/text.py":"MODIFY","sklearn\/feature_extraction\/tests\/test_text.py":"MODIFY"},"diff":{"sklearn\/feature_extraction\/text.py":[{"add":["308","    def _validate_params(self):","309","        \"\"\"Check validity of ngram_range parameter\"\"\"","310","        min_n, max_m = self.ngram_range","311","        if min_n > max_m:","312","            raise ValueError(","313","                \"Invalid value for ngram_range=%s \"","314","                \"lower boundary larger than the upper boundary.\"","315","                % str(self.ngram_range))","316","","508","        self._validate_params()","509","","533","        self._validate_params()","534","","897","        self._validate_params()"],"delete":[]}],"sklearn\/feature_extraction\/tests\/test_text.py":[{"add":["37","import pytest","998","","999","","1000","@pytest.mark.parametrize(\"vec\", [","1001","        HashingVectorizer(ngram_range=(2, 1)),","1002","        CountVectorizer(ngram_range=(2, 1)),","1003","        TfidfVectorizer(ngram_range=(2, 1))","1004","    ])","1005","def test_vectorizers_invalid_ngram_range(vec):","1006","    # vectorizers could be initialized with invalid ngram range","1007","    # test for raising error message","1008","    invalid_range = vec.ngram_range","1009","    message = (\"Invalid value for ngram_range=%s \"","1010","               \"lower boundary larger than the upper boundary.\"","1011","               % str(invalid_range))","1012","","1013","    assert_raise_message(","1014","        ValueError, message, vec.fit, [\"good news everyone\"])","1015","    assert_raise_message(","1016","        ValueError, message, vec.fit_transform, [\"good news everyone\"])","1017","","1018","    if isinstance(vec, HashingVectorizer):","1019","        assert_raise_message(","1020","            ValueError, message, vec.transform, [\"good news everyone\"])"],"delete":[]}]}},"b0e91e4110942e5b3c4333b1c6b6dfefbd1a6124":{"changes":{"examples\/gaussian_process\/plot_gpc_xor.py":"MODIFY","examples\/compose\/plot_column_transformer.py":"MODIFY"},"diff":{"examples\/gaussian_process\/plot_gpc_xor.py":[{"add":["44","    contours = plt.contour(xx, yy, Z, levels=[0.5], linewidths=2,","45","                           colors=['k'])"],"delete":["44","    contours = plt.contour(xx, yy, Z, levels=[0], linewidths=2,","45","                           linetypes='--')"]}],"examples\/compose\/plot_column_transformer.py":[{"add":["42","from sklearn.svm import LinearSVC","119","    ('svc', LinearSVC()),"],"delete":["42","from sklearn.svm import SVC","119","    ('svc', SVC(kernel='linear')),"]}]}},"9e1d9efad16378142b0c4ddd19969fca198a0f7c":{"changes":{"sklearn\/kernel_approximation.py":"MODIFY"},"diff":{"sklearn\/kernel_approximation.py":[{"add":["46","    Examples","47","    --------","48","    >>> from sklearn.kernel_approximation import RBFSampler","49","    >>> from sklearn.linear_model import SGDClassifier","50","    >>> X = [[0, 0], [1, 1], [1, 0], [0, 1]]","51","    >>> y = [0, 0, 1, 1]","52","    >>> rbf_feature = RBFSampler(gamma=1, random_state=1)","53","    >>> X_features = rbf_feature.fit_transform(X)","54","    >>> clf = SGDClassifier(max_iter=5)","55","    >>> clf.fit(X_features, y)","56","    ... # doctest: +NORMALIZE_WHITESPACE","57","    SGDClassifier(alpha=0.0001, average=False, class_weight=None,","58","           early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,","59","           l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=5,","60","           n_iter=None, n_iter_no_change=5, n_jobs=1, penalty='l2',","61","           power_t=0.5, random_state=None, shuffle=True, tol=None,","62","           validation_fraction=0.1, verbose=0, warm_start=False)","63","    >>> clf.score(X_features, y)","64","    1.0","65",""],"delete":[]}]}},"f8b09ce4bfa52af648a8239f4b206d237f6d6cfb":{"changes":{"sklearn\/linear_model\/tests\/test_omp.py":"MODIFY","doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/linear_model\/omp.py":"MODIFY"},"diff":{"sklearn\/linear_model\/tests\/test_omp.py":[{"add":["24","# Make X not of norm 1 for testing","25","X *= 10","26","y *= 10","118","    coef_normalized = omp.coef_[0].copy()","119","    omp.set_params(fit_intercept=True, normalize=False)","121","    assert_array_almost_equal(coef_normalized, omp.coef_)","122","","123","    omp.set_params(fit_intercept=False, normalize=False)","124","    omp.fit(X, y[:, 0])","125","    assert_true(np.count_nonzero(omp.coef_) <= n_nonzero_coefs)"],"delete":["115","    omp.set_params(fit_intercept=False, normalize=False)","116","","120","    assert_true(np.count_nonzero(omp.coef_) <= n_nonzero_coefs)"]}],"doc\/whats_new\/v0.20.rst":[{"add":["127","- Fixed a bug in :class:`linear_model.OrthogonalMatchingPursuit` that was","128","  broken when setting ``normalize=False``.","129","  by `Alexandre Gramfort`_.","130",""],"delete":[]}],"sklearn\/linear_model\/omp.py":[{"add":["8","from math import sqrt","103","","113","            Lkk = linalg.norm(X[:, lam]) ** 2 - v","114","            if Lkk <= min_float:  # selected atoms are dependent","117","            L[n_active, n_active] = sqrt(Lkk)","118","        else:","119","            L[0, 0] = linalg.norm(X[:, lam])","120","","125","","126","        # solves LL'x = X'y as a composition of two triangular systems","129","","238","            Lkk = Gram[lam, lam] - v","239","            if Lkk <= min_float:  # selected atoms are dependent","242","            L[n_active, n_active] = sqrt(Lkk)","243","        else:","244","            L[0, 0] = sqrt(Gram[lam, lam])","245","","251","        # solves LL'x = X'y as a composition of two triangular systems"],"delete":["93","    L[0, 0] = 1.","112","            if 1 - v <= min_float:  # selected atoms are dependent","115","            L[n_active, n_active] = np.sqrt(1 - v)","120","        # solves LL'x = y as a composition of two triangular systems","231","            if 1 - v <= min_float:  # selected atoms are dependent","234","            L[n_active, n_active] = np.sqrt(1 - v)","240","        # solves LL'x = y as a composition of two triangular systems"]}]}},"02d0a03ad1c1003ed57d54bf0d2bdedea7299c36":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/feature_extraction\/text.py":"MODIFY","sklearn\/feature_extraction\/tests\/test_text.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["488","- Fixed bug in :class:`feature_extraction.text.TFIDFVectorizer` which ","489","  was ignoring the parameter ``dtype``. In addition,","490","  :class:`feature_extraction.text.TFIDFTransformer` will preserve ``dtype``","491","  for floating and raise a warning if ``dtype`` requested is integer.","492","  :issue:`10441` by :user:`Mayur Kulkarni <maykulkarni>` and","493","  :user:`Guillaume Lemaitre <glemaitre>`.","494","  "],"delete":[]}],"sklearn\/feature_extraction\/text.py":[{"add":["13","from __future__ import unicode_literals, division","21","import warnings","32","from ..utils.validation import check_is_fitted, check_array, FLOAT_DTYPES","576","        return np.diff(X.indptr)","1120","        X = check_array(X, accept_sparse=('csr', 'csc'))","1122","            X = sp.csr_matrix(X)","1123","        dtype = X.dtype if X.dtype in FLOAT_DTYPES else np.float64","1124","","1127","            df = _document_frequency(X).astype(dtype)","1135","            idf = np.log(n_samples \/ df) + 1","1136","            self._idf_diag = sp.diags(idf, offsets=0,","1137","                                      shape=(n_features, n_features),","1138","                                      format='csr',","1139","                                      dtype=dtype)","1159","        X = check_array(X, accept_sparse='csr', dtype=FLOAT_DTYPES, copy=copy)","1160","        if not sp.issparse(X):","1161","            X = sp.csr_matrix(X, dtype=np.float64)","1372","                 dtype=np.float64, norm='l2', use_idf=True, smooth_idf=True,","1437","    def _check_params(self):","1438","        if self.dtype not in FLOAT_DTYPES:","1439","            warnings.warn(\"Only {} 'dtype' should be used. {} 'dtype' will \"","1440","                          \"be converted to np.float64.\"","1441","                          .format(FLOAT_DTYPES, self.dtype),","1442","                          UserWarning)","1443","","1456","        self._check_params()","1477","        self._check_params()"],"delete":["13","from __future__ import unicode_literals","31","from ..utils.validation import check_is_fitted","575","        return np.diff(sp.csc_matrix(X, copy=False).indptr)","1120","            X = sp.csc_matrix(X)","1123","            df = _document_frequency(X)","1131","            idf = np.log(float(n_samples) \/ df) + 1.0","1132","            self._idf_diag = sp.spdiags(idf, diags=0, m=n_features,","1133","                                        n=n_features, format='csr')","1153","        if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.floating):","1154","            # preserve float family dtype","1155","            X = sp.csr_matrix(X, copy=copy)","1156","        else:","1157","            # convert counts or binary occurrences to floats","1158","            X = sp.csr_matrix(X, dtype=np.float64, copy=copy)","1369","                 dtype=np.int64, norm='l2', use_idf=True, smooth_idf=True,"]}],"sklearn\/feature_extraction\/tests\/test_text.py":[{"add":["3","import pytest","4","from scipy import sparse","5","","33","                                   SkipTest, assert_raises,","34","                                   assert_allclose_dense_sparse)","1042","@pytest.mark.parametrize(\"X_dtype\", [np.float32, np.float64])","1043","def test_tfidf_transformer_type(X_dtype):","1044","    X = sparse.rand(10, 20000, dtype=X_dtype, random_state=42)","1045","    X_trans = TfidfTransformer().fit_transform(X)","1046","    assert X_trans.dtype == X.dtype","1047","","1048","","1049","def test_tfidf_transformer_sparse():","1050","    X = sparse.rand(10, 20000, dtype=np.float64, random_state=42)","1051","    X_csc = sparse.csc_matrix(X)","1052","    X_csr = sparse.csr_matrix(X)","1053","","1054","    X_trans_csc = TfidfTransformer().fit_transform(X_csc)","1055","    X_trans_csr = TfidfTransformer().fit_transform(X_csr)","1056","    assert_allclose_dense_sparse(X_trans_csc, X_trans_csr)","1057","    assert X_trans_csc.format == X_trans_csr.format","1058","","1059","","1060","@pytest.mark.parametrize(","1061","    \"vectorizer_dtype, output_dtype, expected_warning, msg_warning\",","1062","    [(np.int32, np.float64, UserWarning, \"'dtype' should be used.\"),","1063","     (np.int64, np.float64, UserWarning, \"'dtype' should be used.\"),","1064","     (np.float32, np.float32, None, None),","1065","     (np.float64, np.float64, None, None)]","1066",")","1067","def test_tfidf_vectorizer_type(vectorizer_dtype, output_dtype,","1068","                               expected_warning, msg_warning):","1069","    X = np.array([\"numpy\", \"scipy\", \"sklearn\"])","1070","    vectorizer = TfidfVectorizer(dtype=vectorizer_dtype)","1071","    with pytest.warns(expected_warning, match=msg_warning) as record:","1072","            X_idf = vectorizer.fit_transform(X)","1073","    if expected_warning is None:","1074","        assert len(record) == 0","1075","    assert X_idf.dtype == output_dtype","1076","","1077",""],"delete":["30","                                   SkipTest, assert_raises)","37","import pytest","38",""]}]}},"ff5aac0c9e5555f52165ccfbf489e6592a58cf9c":{"changes":{"CONTRIBUTING.md":"MODIFY","doc\/developers\/contributing.rst":"MODIFY"},"diff":{"CONTRIBUTING.md":[{"add":["145","-  No flake8 warnings, check with:","148","  $ pip install flake8","149","  $ flake8 path\/to\/module.py"],"delete":["145","-  No pyflakes warnings, check with:","148","  $ pip install pyflakes","149","  $ pyflakes path\/to\/module.py","150","  ```","151","","152","-  No PEP8 warnings, check with:","153","","154","  ```bash","155","  $ pip install pep8","156","  $ pep8 path\/to\/module.py","157","  ```","158","","159","-  AutoPEP8 can help you fix some of the easy redundant errors:","160","","161","  ```bash","162","  $ pip install autopep8","163","  $ autopep8 path\/to\/pep8.py"]}],"doc\/developers\/contributing.rst":[{"add":["287","* No flake8 warnings, check with::","289","    $ pip install flake8","290","    $ flake8 path\/to\/module.py"],"delete":["287","* No pyflakes warnings, check with::","289","    $ pip install pyflakes","290","    $ pyflakes path\/to\/module.py","291","","292","* No PEP8 warnings, check with::","293","","294","    $ pip install pep8","295","    $ pep8 path\/to\/module.py","296","","297","* AutoPEP8 can help you fix some of the easy redundant errors::","298","","299","    $ pip install autopep8","300","    $ autopep8 path\/to\/pep8.py"]}]}},"5763284ef9e04ffc52cf759a59bdc6bb81107616":{"changes":{"sklearn\/neighbors\/tests\/test_neighbors.py":"MODIFY","sklearn\/neighbors\/base.py":"MODIFY"},"diff":{"sklearn\/neighbors\/tests\/test_neighbors.py":[{"add":["20","from sklearn.utils.testing import assert_raises_regex","111","def test_n_neighbors_datatype():","112","    # Test to check whether n_neighbors is integer","113","    X = [[1, 1], [1, 1], [1, 1]]","114","    expected_msg = \"n_neighbors does not take .*float.* \" \\","115","                   \"value, enter integer value\"","116","    msg = \"Expected n_neighbors > 0. Got -3\"","117","","118","    neighbors_ = neighbors.NearestNeighbors(n_neighbors=3.)","119","    assert_raises_regex(TypeError, expected_msg, neighbors_.fit, X)","120","    assert_raises_regex(ValueError, msg,","121","                        neighbors_.kneighbors, X=X, n_neighbors=-3)","122","    assert_raises_regex(TypeError, expected_msg,","123","                        neighbors_.kneighbors, X=X, n_neighbors=3.)","124","","125",""],"delete":[]}],"sklearn\/neighbors\/base.py":[{"add":["260","            else:","261","                if not np.issubdtype(type(self.n_neighbors), np.integer):","262","                    raise TypeError(","263","                        \"n_neighbors does not take %s value, \"","264","                        \"enter integer value\" %","265","                        type(self.n_neighbors))","335","        elif n_neighbors <= 0:","336","            raise ValueError(","337","                \"Expected n_neighbors > 0. Got %d\" %","338","                n_neighbors","339","            )","340","        else:","341","            if not np.issubdtype(type(n_neighbors), np.integer):","342","                raise TypeError(","343","                    \"n_neighbors does not take %s value, \"","344","                    \"enter integer value\" %","345","                    type(n_neighbors))"],"delete":[]}]}},"02c7f986eb0fdca086f35343a922a239e4835017":{"changes":{"sklearn\/model_selection\/_split.py":"MODIFY","sklearn\/externals\/joblib\/parallel.py":"MODIFY"},"diff":{"sklearn\/model_selection\/_split.py":[{"add":["1255","        complement of the train size. By default (the parameter is"],"delete":["1255","        complement of the train size. By default (the is parameter"]}],"sklearn\/externals\/joblib\/parallel.py":[{"add":["247","    n_jobs is the number of workers requested by the callers."],"delete":["247","    n_jobs is the is the number of workers requested by the callers."]}]}},"8424dd1e2a1e5624d0c5488022c34c57e2a313dd":{"changes":{".travis.yml":"MODIFY"},"diff":{".travis.yml":[{"add":["59","       if: type = cron OR commit_message =~ \/\\[scipy-dev\\]\/"],"delete":["59","       if: type = cron OR commit_message ~ \/\\[scipy-dev\\]\/"]}]}},"dd69361a0d9c6ccde0d2353b00b86e0e7541a3e3":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/metrics\/tests\/test_common.py":"MODIFY","sklearn\/metrics\/ranking.py":"MODIFY","sklearn\/metrics\/tests\/test_ranking.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["394","- :func:`metrics.average_precision_score` now supports binary ``y_true``","395","  other than ``{0, 1}`` or ``{-1, 1}`` through ``pos_label`` parameter.","396","  :issue:`9980` by :user:`Hanmin Qin <qinhanmin2014>`.","397","","634","- Fixed a bug where :func:`metrics.average_precision_score` will sometimes return","635","  ``nan`` when ``sample_weight`` contains 0.","636","  :issue:`9980` by :user:`Hanmin Qin <qinhanmin2014>`.","637",""],"delete":[]}],"sklearn\/metrics\/tests\/test_common.py":[{"add":["259","    \"average_precision_score\",","260","    \"weighted_average_precision_score\",","261","    \"micro_average_precision_score\",","262","    \"macro_average_precision_score\",","263","    \"samples_average_precision_score\",","264","","300","    \"average_precision_score\",","301","    \"weighted_average_precision_score\",","302","    \"micro_average_precision_score\",","303","    \"macro_average_precision_score\",","304","    \"samples_average_precision_score\",","305","","674","            measure_with_strobj = metric_str(y1_str.astype('O'), y2)"],"delete":["243","","244","    \"average_precision_score\",","245","    \"weighted_average_precision_score\",","246","    \"micro_average_precision_score\",","247","    \"macro_average_precision_score\",","248","    \"samples_average_precision_score\",","249","","669","            measure_with_strobj = metric(y1_str.astype('O'), y2)"]}],"sklearn\/metrics\/ranking.py":[{"add":["22","from functools import partial","23","","129","def average_precision_score(y_true, y_score, average=\"macro\", pos_label=1,","154","        True binary labels or binary label indicators.","177","    pos_label : int or str (default=1)","178","        The label of the positive class. Only applied to binary ``y_true``.","179","        For multilabel-indicator ``y_true``, ``pos_label`` is fixed to 1.","180","","217","            y_true, y_score, pos_label=1, sample_weight=None):","219","            y_true, y_score, pos_label=pos_label, sample_weight=sample_weight)","225","    y_type = type_of_target(y_true)","226","    if y_type == \"multilabel-indicator\" and pos_label != 1:","227","        raise ValueError(\"Parameter pos_label is fixed to 1 for \"","228","                         \"multilabel-indicator y_true. Do not set \"","229","                         \"pos_label or set pos_label to 1.\")","230","    average_precision = partial(_binary_uninterpolated_average_precision,","231","                                pos_label=pos_label)","232","    return _average_binary_score(average_precision, y_true, y_score,","233","                                 average, sample_weight=sample_weight)","515","    precision[np.isnan(precision)] = 0"],"delete":["127","def average_precision_score(y_true, y_score, average=\"macro\",","152","        True binary labels (either {0, 1} or {-1, 1}).","211","            y_true, y_score, sample_weight=None):","213","            y_true, y_score, sample_weight=sample_weight)","219","    return _average_binary_score(_binary_uninterpolated_average_precision,","220","                                 y_true, y_score, average,","221","                                 sample_weight=sample_weight)"]}],"sklearn\/metrics\/tests\/test_ranking.py":[{"add":["683","def test_average_precision_score_pos_label_multilabel_indicator():","684","    # Raise an error for multilabel-indicator y_true with","685","    # pos_label other than 1","686","    y_true = np.array([[1, 0], [0, 1], [0, 1], [1, 0]])","687","    y_pred = np.array([[0.9, 0.1], [0.1, 0.9], [0.8, 0.2], [0.2, 0.8]])","688","    erorr_message = (\"Parameter pos_label is fixed to 1 for multilabel\"","689","                     \"-indicator y_true. Do not set pos_label or set \"","690","                     \"pos_label to 1.\")","691","    assert_raise_message(ValueError, erorr_message, average_precision_score,","692","                         y_true, y_pred, pos_label=0)","693","","694",""],"delete":[]}]}},"65489cacc4b3abb090a0fa9b22d8d4c7735539d8":{"changes":{"doc\/modules\/covariance.rst":"MODIFY"},"diff":{"doc\/modules\/covariance.rst":[{"add":["9","Many statistical problems require the estimation of a","13","have a large influence on the estimation's quality. The","14","`sklearn.covariance` package provides tools for accurately estimating","15","a population's covariance matrix under various settings.","25","by the classical *maximum likelihood estimator* (or \"empirical","29","sample is an unbiased estimator of the corresponding population's","35",":meth:`EmpiricalCovariance.fit` method. Be careful that results depend","36","on whether the data are centered, so one may want to use the","37","``assume_centered`` parameter accurately. More precisely, if","38","``assume_centered=False``, then the test set is supposed to have the","39","same mean vector as the training set. If not, both should be centered","40","by the user, and ``assume_centered=True`` should be used.","65","In scikit-learn, this transformation (with a user-defined shrinkage","69","and its :meth:`ShrunkCovariance.fit` method. Again, results depend on","70","whether the data are centered, so one may want to use the","71","``assume_centered`` parameter accurately.","75","smallest and the largest eigenvalues of the empirical covariance matrix.","96","In their 2004 paper [1]_, O. Ledoit and M. Wolf propose a formula","191","the precision matrix will be zero. This is why it makes sense to","192","estimate a sparse precision matrix: the estimation of the covariance","193","matrix is better conditioned by learning independence relations from","194","the data. This is known as *covariance selection*.","274","Real data sets are often subject to measurement or recording","276","of reasons. Observations which are very uncommon are called","277","outliers.","280","outliers in the data. Therefore, one should use robust"],"delete":["9","Many statistical problems require at some point the estimation of a","13","has a large influence on the estimation's quality. The","14","`sklearn.covariance` package aims at providing tools affording","15","an accurate estimation of a population's covariance matrix under","16","various settings.","26","with the classical *maximum likelihood estimator* (or \"empirical","30","sample is an unbiased estimator of the corresponding population","36",":meth:`EmpiricalCovariance.fit` method.  Be careful that depending","37","whether the data are centered or not, the result will be different, so","38","one may want to use the ``assume_centered`` parameter accurately. More precisely","39","if one uses ``assume_centered=False``, then the test set is supposed to have the","40","same mean vector as the training set. If not so, both should be centered by the","41","user, and ``assume_centered=True`` should be used.","66","In the scikit-learn, this transformation (with a user-defined shrinkage","70","and its :meth:`ShrunkCovariance.fit` method.  Again, depending whether","71","the data are centered or not, the result will be different, so one may","72","want to use the ``assume_centered`` parameter accurately.","76","smallest and the largest eigenvalue of the empirical covariance matrix.","97","In their 2004 paper [1]_, O. Ledoit and M. Wolf propose a formula so as","192","the precision matrix will be zero. This is why it makes sense to estimate","193","a sparse precision matrix: by learning independence relations from the","194","data, the estimation of the covariance matrix is better conditioned. This","195","is known as *covariance selection*.","275","Real data set are often subjects to measurement or recording","277","of reason. Every observation which is very uncommon is called an","278","outlier.","281","outlying observations in the data. Therefore, one should use robust"]}]}},"2afcc3692633dc80dc5454dd0eef3ae077b4c4cf":{"changes":{"examples\/gaussian_process\/plot_gpr_noisy_targets.py":"MODIFY"},"diff":{"examples\/gaussian_process\/plot_gpr_noisy_targets.py":[{"add":["87","# Instantiate a Gaussian Process model","88","gp = GaussianProcessRegressor(kernel=kernel, alpha=dy ** 2,"],"delete":["87","# Instanciate a Gaussian Process model","88","gp = GaussianProcessRegressor(kernel=kernel, alpha=(dy \/ y) ** 2,"]}]}},"34ad7d4d160af281827425fcd88fe744ffe850f1":{"changes":{"sklearn\/model_selection\/_search.py":"MODIFY"},"diff":{"sklearn\/model_selection\/_search.py":[{"add":["970","        |  'poly'    |     --    |      2     |       0.80      |...|    2    |","972","        |  'poly'    |     --    |      3     |       0.70      |...|    4    |","974","        |  'rbf'     |     0.1   |     --     |       0.80      |...|    3    |","976","        |  'rbf'     |     0.2   |     --     |       0.93      |...|    1    |","988","            'split0_test_score'  : [0.80, 0.70, 0.80, 0.93],","989","            'split1_test_score'  : [0.82, 0.50, 0.70, 0.78],","990","            'mean_test_score'    : [0.81, 0.60, 0.75, 0.85],","991","            'std_test_score'     : [0.01, 0.10, 0.05, 0.08],","993","            'split0_train_score' : [0.80, 0.92, 0.70, 0.93],","994","            'split1_train_score' : [0.82, 0.55, 0.70, 0.87],","995","            'mean_train_score'   : [0.81, 0.74, 0.70, 0.90],","996","            'std_train_score'    : [0.01, 0.19, 0.00, 0.03],","999","            'mean_score_time'    : [0.01, 0.06, 0.04, 0.04],","1000","            'std_score_time'     : [0.00, 0.00, 0.00, 0.01],","1276","        |    'rbf'     |     0.1     |       0.80        |...|       2       |","1278","        |    'rbf'     |     0.2     |       0.90        |...|       1       |","1280","        |    'rbf'     |     0.3     |       0.70        |...|       1       |","1289","            'split0_test_score'  : [0.80, 0.90, 0.70],","1290","            'split1_test_score'  : [0.82, 0.50, 0.70],","1291","            'mean_test_score'    : [0.81, 0.70, 0.70],","1292","            'std_test_score'     : [0.01, 0.20, 0.00],","1294","            'split0_train_score' : [0.80, 0.92, 0.70],","1295","            'split1_train_score' : [0.82, 0.55, 0.70],","1296","            'mean_train_score'   : [0.81, 0.74, 0.70],","1297","            'std_train_score'    : [0.01, 0.19, 0.00],","1298","            'mean_fit_time'      : [0.73, 0.63, 0.43],","1299","            'std_fit_time'       : [0.01, 0.02, 0.01],","1300","            'mean_score_time'    : [0.01, 0.06, 0.04],","1301","            'std_score_time'     : [0.00, 0.00, 0.00],"],"delete":["970","        |  'poly'    |     --    |      2     |        0.8      |...|    2    |","972","        |  'poly'    |     --    |      3     |        0.7      |...|    4    |","974","        |  'rbf'     |     0.1   |     --     |        0.8      |...|    3    |","976","        |  'rbf'     |     0.2   |     --     |        0.9      |...|    1    |","988","            'split0_test_score'  : [0.8, 0.7, 0.8, 0.9],","989","            'split1_test_score'  : [0.82, 0.5, 0.7, 0.78],","990","            'mean_test_score'    : [0.81, 0.60, 0.75, 0.82],","991","            'std_test_score'     : [0.02, 0.01, 0.03, 0.03],","993","            'split0_train_score' : [0.8, 0.9, 0.7],","994","            'split1_train_score' : [0.82, 0.5, 0.7],","995","            'mean_train_score'   : [0.81, 0.7, 0.7],","996","            'std_train_score'    : [0.03, 0.03, 0.04],","999","            'mean_score_time'    : [0.007, 0.06, 0.04, 0.04],","1000","            'std_score_time'     : [0.001, 0.002, 0.003, 0.005],","1276","        |    'rbf'     |     0.1     |        0.8        |...|       2       |","1278","        |    'rbf'     |     0.2     |        0.9        |...|       1       |","1280","        |    'rbf'     |     0.3     |        0.7        |...|       1       |","1289","            'split0_test_score'  : [0.8, 0.9, 0.7],","1290","            'split1_test_score'  : [0.82, 0.5, 0.7],","1291","            'mean_test_score'    : [0.81, 0.7, 0.7],","1292","            'std_test_score'     : [0.02, 0.2, 0.],","1294","            'split0_train_score' : [0.8, 0.9, 0.7],","1295","            'split1_train_score' : [0.82, 0.5, 0.7],","1296","            'mean_train_score'   : [0.81, 0.7, 0.7],","1297","            'std_train_score'    : [0.03, 0.03, 0.04],","1298","            'mean_fit_time'      : [0.73, 0.63, 0.43, 0.49],","1299","            'std_fit_time'       : [0.01, 0.02, 0.01, 0.01],","1300","            'mean_score_time'    : [0.007, 0.06, 0.04, 0.04],","1301","            'std_score_time'     : [0.001, 0.002, 0.003, 0.005],"]}]}},"2242c59fc890455bd121e4a03375c5632f31ef93":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/ensemble\/tests\/test_forest.py":"MODIFY","examples\/ensemble\/plot_voting_probas.py":"MODIFY","sklearn\/ensemble\/tests\/test_voting_classifier.py":"MODIFY","sklearn\/tests\/test_calibration.py":"MODIFY","sklearn\/utils\/estimator_checks.py":"MODIFY","examples\/ensemble\/plot_random_forest_regression_multioutput.py":"MODIFY","sklearn\/feature_selection\/tests\/test_rfe.py":"MODIFY","sklearn\/feature_selection\/tests\/test_from_model.py":"MODIFY","sklearn\/ensemble\/forest.py":"MODIFY","sklearn\/ensemble\/tests\/test_weight_boosting.py":"MODIFY","examples\/applications\/plot_prediction_latency.py":"MODIFY","examples\/ensemble\/plot_ensemble_oob.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["735","Classifiers and regressors","736","","737","- The default value of the ``n_estimators`` parameter of ","738","  :class:`ensemble.RandomForestClassifier`, :class:`ensemble.RandomForestRegressor`, ","739","  :class:`ensemble.ExtraTreesClassifier`, :class:`ensemble.ExtraTreesRegressor`, ","740","  and :class:`ensemble.RandomTreesEmbedding` will change from 10 in version 0.20 ","741","  to 100 in 0.22. A FutureWarning is raised when the default value is used.","742","  :issue:`11542` by :user:`Anna Ayzenshtat <annaayzenshtat>`.","743",""],"delete":["24",""]}],"sklearn\/ensemble\/tests\/test_forest.py":[{"add":["33","from sklearn.utils.testing import assert_no_warnings","189","@pytest.mark.filterwarnings('ignore:The default value of n_estimators')","436","@pytest.mark.filterwarnings('ignore:The default value of n_estimators')","494","@pytest.mark.filterwarnings('ignore:The default value of n_estimators')","532","@pytest.mark.filterwarnings('ignore:The default value of n_estimators')","556","@pytest.mark.filterwarnings('ignore:The default value of n_estimators')","746","@pytest.mark.filterwarnings('ignore:The default value of n_estimators')","784","@pytest.mark.filterwarnings('ignore:The default value of n_estimators')","852","@pytest.mark.filterwarnings('ignore:The default value of n_estimators')","910","@pytest.mark.filterwarnings('ignore:The default value of n_estimators')","989","@pytest.mark.filterwarnings('ignore:The default value of n_estimators')","1009","@pytest.mark.filterwarnings('ignore:The default value of n_estimators')","1040","@pytest.mark.filterwarnings('ignore:The default value of n_estimators')","1178","@pytest.mark.filterwarnings('ignore:The default value of n_estimators')","1217","@pytest.mark.filterwarnings('ignore:The default value of n_estimators')","1233","@pytest.mark.filterwarnings('ignore:The default value of n_estimators')","1246","","1247","","1248","@pytest.mark.parametrize('forest',","1249","                         [RandomForestClassifier, RandomForestRegressor,","1250","                          ExtraTreesClassifier, ExtraTreesRegressor,","1251","                          RandomTreesEmbedding])","1252","def test_nestimators_future_warning(forest):","1253","    # FIXME: to be removed 0.22","1254","","1255","    # When n_estimators default value is used","1256","    msg_future = (\"The default value of n_estimators will change from \"","1257","                  \"10 in version 0.20 to 100 in 0.22.\")","1258","    est = forest()","1259","    est = assert_warns_message(FutureWarning, msg_future, est.fit, X, y)","1260","","1261","    # When n_estimators is a valid value not equal to the default","1262","    est = forest(n_estimators=100)","1263","    est = assert_no_warnings(est.fit, X, y)"],"delete":[]}],"examples\/ensemble\/plot_voting_probas.py":[{"add":["32","clf2 = RandomForestClassifier(n_estimators=100, random_state=123)"],"delete":["32","clf2 = RandomForestClassifier(random_state=123)"]}],"sklearn\/ensemble\/tests\/test_voting_classifier.py":[{"add":["2","import pytest","4","","78","@pytest.mark.filterwarnings('ignore:The default value of n_estimators')","91","@pytest.mark.filterwarnings('ignore:The default value of n_estimators')","103","@pytest.mark.filterwarnings('ignore:The default value of n_estimators')","117","@pytest.mark.filterwarnings('ignore:The default value of n_estimators')","150","@pytest.mark.filterwarnings('ignore:The default value of n_estimators')","218","@pytest.mark.filterwarnings('ignore:The default value of n_estimators')","236","@pytest.mark.filterwarnings('ignore:The default value of n_estimators')","258","@pytest.mark.filterwarnings('ignore:The default value of n_estimators')","302","@pytest.mark.filterwarnings('ignore:The default value of n_estimators')","337","@pytest.mark.filterwarnings('ignore:The default value of n_estimators')","390","@pytest.mark.filterwarnings('ignore:The default value of n_estimators')","408","@pytest.mark.filterwarnings('ignore:The default value of n_estimators')"],"delete":[]}],"sklearn\/tests\/test_calibration.py":[{"add":["4","import pytest","27","@pytest.mark.filterwarnings('ignore:The default value of n_estimators')"],"delete":["26","@ignore_warnings"]}],"sklearn\/utils\/estimator_checks.py":[{"add":["344","        # FIXME: The default number of trees was changed and is set to 'warn'","345","        # for some of the ensemble methods. We need to catch this case to avoid","346","        # an error during the comparison. To be reverted in 0.22.","347","        if estimator.n_estimators == 'warn':","348","            estimator.set_params(n_estimators=5)","349","        else:","350","            estimator.set_params(n_estimators=min(5, estimator.n_estimators))"],"delete":["344","        estimator.set_params(n_estimators=min(5, estimator.n_estimators))"]}],"examples\/ensemble\/plot_random_forest_regression_multioutput.py":[{"add":["45","regr_multirf = MultiOutputRegressor(RandomForestRegressor(n_estimators=100,","46","                                                          max_depth=max_depth,","50","regr_rf = RandomForestRegressor(n_estimators=100, max_depth=max_depth,","51","                                random_state=2)"],"delete":["45","regr_multirf = MultiOutputRegressor(RandomForestRegressor(max_depth=max_depth,","49","regr_rf = RandomForestRegressor(max_depth=max_depth, random_state=2)"]}],"sklearn\/feature_selection\/tests\/test_rfe.py":[{"add":["3","import pytest","339","@pytest.mark.filterwarnings('ignore:The default value of n_estimators')"],"delete":[]}],"sklearn\/feature_selection\/tests\/test_from_model.py":[{"add":["37","@pytest.mark.filterwarnings('ignore:The default value of n_estimators')","241","@pytest.mark.filterwarnings('ignore:The default value of n_estimators')"],"delete":[]}],"sklearn\/ensemble\/forest.py":[{"add":["137","                 n_estimators=100,","244","","245","        if self.n_estimators == 'warn':","246","            warnings.warn(\"The default value of n_estimators will change from \"","247","                          \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)","248","            self.n_estimators = 10","249","","408","                 n_estimators=100,","647","                 n_estimators=100,","767","        .. versionchanged:: 0.20","768","           The default value of ``n_estimators`` will change from 10 in","769","           version 0.20 to 100 in version 0.22.","770","","984","                 n_estimators='warn',","1045","        .. versionchanged:: 0.20","1046","           The default value of ``n_estimators`` will change from 10 in","1047","           version 0.20 to 100 in version 0.22.","1048","","1228","                 n_estimators='warn',","1285","        .. versionchanged:: 0.20","1286","           The default value of ``n_estimators`` will change from 10 in","1287","           version 0.20 to 100 in version 0.22.","1288","","1475","                 n_estimators='warn',","1534","        .. versionchanged:: 0.20","1535","           The default value of ``n_estimators`` will change from 10 in","1536","           version 0.20 to 100 in version 0.22.","1537","","1691","                 n_estimators='warn',","1753","        .. versionchanged:: 0.20","1754","           The default value of ``n_estimators`` will change from 10 in","1755","           version 0.20 to 100 in version 0.22.","1756","","1859","                 n_estimators='warn',"],"delete":["137","                 n_estimators=10,","402","                 n_estimators=10,","411","","642","                 n_estimators=10,","975","                 n_estimators=10,","1215","                 n_estimators=10,","1458","                 n_estimators=10,","1670","                 n_estimators=10,","1834","                 n_estimators=10,"]}],"sklearn\/ensemble\/tests\/test_weight_boosting.py":[{"add":["2","import pytest","4","","281","@pytest.mark.filterwarnings('ignore:The default value of n_estimators')"],"delete":[]}],"examples\/applications\/plot_prediction_latency.py":[{"add":["287","         'instance': RandomForestRegressor(n_estimators=100),"],"delete":["287","         'instance': RandomForestRegressor(),"]}],"examples\/ensemble\/plot_ensemble_oob.py":[{"add":["47","        RandomForestClassifier(n_estimators=100,","48","                               warm_start=True, oob_score=True,","52","        RandomForestClassifier(n_estimators=100,","53","                               warm_start=True, max_features='log2',","57","        RandomForestClassifier(n_estimators=100,","58","                               warm_start=True, max_features=None,"],"delete":["47","        RandomForestClassifier(warm_start=True, oob_score=True,","51","        RandomForestClassifier(warm_start=True, max_features='log2',","55","        RandomForestClassifier(warm_start=True, max_features=None,"]}]}},"7108d17be443cf20588d59ebbd8f8e9da91bd2d0":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/utils\/validation.py":"MODIFY","sklearn\/utils\/tests\/test_validation.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["337","Utils","338","","339","- :func:`utils.validation.check_array` yield a ``FutureWarning`` indicating","340","  that arrays of bytes\/strings will be interpreted as decimal numbers","341","  beginning in version 0.22. :issue:`10229` by :user:`Ryan Lee <rtlee9>`","342",""],"delete":[]}],"sklearn\/utils\/validation.py":[{"add":["518","        # in the future np.flexible dtypes will be handled like object dtypes","519","        if dtype_numeric and np.issubdtype(array.dtype, np.flexible):","520","            warnings.warn(","521","                \"Beginning in version 0.22, arrays of strings will be \"","522","                \"interpreted as decimal numbers if parameter 'dtype' is \"","523","                \"'numeric'. It is recommended that you convert the array to \"","524","                \"type np.float64 before passing it to check_array.\",","525","                FutureWarning)","526",""],"delete":[]}],"sklearn\/utils\/tests\/test_validation.py":[{"add":["287","    # deprecation warning if string-like array with dtype=\"numeric\"","288","    X_str = [['a', 'b'], ['c', 'd']]","289","    assert_warns_message(","290","        FutureWarning,","291","        \"arrays of strings will be interpreted as decimal numbers if \"","292","        \"parameter 'dtype' is 'numeric'. It is recommended that you convert \"","293","        \"the array to type np.float64 before passing it to check_array.\",","294","        check_array, X_str, \"numeric\")","295","    assert_warns_message(","296","        FutureWarning,","297","        \"arrays of strings will be interpreted as decimal numbers if \"","298","        \"parameter 'dtype' is 'numeric'. It is recommended that you convert \"","299","        \"the array to type np.float64 before passing it to check_array.\",","300","        check_array, np.array(X_str, dtype='U'), \"numeric\")","301","    assert_warns_message(","302","        FutureWarning,","303","        \"arrays of strings will be interpreted as decimal numbers if \"","304","        \"parameter 'dtype' is 'numeric'. It is recommended that you convert \"","305","        \"the array to type np.float64 before passing it to check_array.\",","306","        check_array, np.array(X_str, dtype='S'), \"numeric\")","307","","308","    # deprecation warning if byte-like array with dtype=\"numeric\"","309","    X_bytes = [[b'a', b'b'], [b'c', b'd']]","310","    assert_warns_message(","311","        FutureWarning,","312","        \"arrays of strings will be interpreted as decimal numbers if \"","313","        \"parameter 'dtype' is 'numeric'. It is recommended that you convert \"","314","        \"the array to type np.float64 before passing it to check_array.\",","315","        check_array, X_bytes, \"numeric\")","316","    assert_warns_message(","317","        FutureWarning,","318","        \"arrays of strings will be interpreted as decimal numbers if \"","319","        \"parameter 'dtype' is 'numeric'. It is recommended that you convert \"","320","        \"the array to type np.float64 before passing it to check_array.\",","321","        check_array, np.array(X_bytes, dtype='V1'), \"numeric\")","322",""],"delete":[]}]}},"73fc2e4618dbdeb6e71f8a63de2a5bc673c80629":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/neighbors\/kd_tree.pyx":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["663","- Fixed a bug in ``KDTree`` construction that results in faster construction","664","  and querying times. :issue:`11556` by :user:`Jake VanderPlas <jakevdp>`","665",""],"delete":[]}],"sklearn\/neighbors\/kd_tree.pyx":[{"add":["71","","72","    for j in range(n_features):"],"delete":[]}]}},"a38067093f08bf04483440984bcd5e8b6973f78c":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/mixture\/tests\/test_gaussian_mixture.py":"MODIFY","sklearn\/mixture\/tests\/test_mixture.py":"ADD","sklearn\/mixture\/base.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["366","- Fixed a bug in :class:`mixture.BaseMixture` where the reported `n_iter_` was","367","  missing an iteration. It affected :class:`mixture.GaussianMixture` and","368","  :class:`mixture.BayesianGaussianMixture`. :issue:`10740` by :user:`Erich","369","  Schubert <kno10>` and :user:`Guillaume Lemaitre <glemaitre>`.","370",""],"delete":[]}],"sklearn\/mixture\/tests\/test_gaussian_mixture.py":[{"add":["2","# License: BSD 3 clause"],"delete":["2","# License: BSD 3 clauseimport warnings"]}],"sklearn\/mixture\/tests\/test_mixture.py":[{"add":[],"delete":[]}],"sklearn\/mixture\/base.py":[{"add":["210","            for n_iter in range(1, self.max_iter + 1):"],"delete":["210","            for n_iter in range(self.max_iter):"]}]}},"4989a9503753a92089f39e154a2bb5d160b5d276":{"changes":{"doc\/conf.py":"MODIFY","\/dev\/null":"DELETE","examples\/applications\/skip_stock_market.py":"ADD"},"diff":{"doc\/conf.py":[{"add":["245","        'sklearn': None}"],"delete":["245","        'sklearn': None},","246","    'expected_failing_examples':","247","        ['..\/examples\/applications\/plot_stock_market.py'],"]}],"\/dev\/null":[{"add":[],"delete":[]}],"examples\/applications\/skip_stock_market.py":[{"add":[],"delete":[]}]}},"eec069794b5c67ca793820b31df3c55a7c5b4ce6":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/linear_model\/cd_fast.pyx":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["341","- Fixed condition triggering gap computation in :class:`linear_model.Lasso`","342","  and :class:`linear_model.ElasticNet` when working with sparse matrices.","343","  :issue:`10992` by `Alexandre Gramfort`_.","344",""],"delete":[]}],"sklearn\/linear_model\/cd_fast.pyx":[{"add":["482","                if fabs(w[ii]) > w_max:","483","                    w_max = fabs(w[ii])","484",""],"delete":["482","                if w[ii] > w_max:","483","                    w_max = w[ii]"]}]}},"2dc226154769841d07601c19b089086f4aa31cf9":{"changes":{"\/dev\/null":"DELETE","build_tools\/circle\/build_doc.sh":"MODIFY","examples\/applications\/plot_stock_market.py":"ADD",".circleci\/config.yml":"MODIFY","README.rst":"MODIFY"},"diff":{"\/dev\/null":[{"add":[],"delete":[]}],"build_tools\/circle\/build_doc.sh":[{"add":["120","  scikit-image=\"${SCIKIT_IMAGE_VERSION:-*}\" pandas=\"${PANDAS_VERSION:-*}\""],"delete":["120","  scikit-image=\"${SCIKIT_IMAGE_VERSION:-*}\""]}],"examples\/applications\/plot_stock_market.py":[{"add":[],"delete":[]}],".circleci\/config.yml":[{"add":["49","      - PANDAS_VERSION: 0.13.1"],"delete":[]}],"README.rst":[{"add":["56","require scikit-image >= 0.9.3 and a few examples require pandas >= 0.13.1."],"delete":["56","require scikit-image >= 0.9.3 as well."]}]}},"1966f0ba7f9773da1fd8face7205e3976053db02":{"changes":{"sklearn\/datasets\/kddcup99.py":"MODIFY"},"diff":{"sklearn\/datasets\/kddcup99.py":[{"add":["236","                          download_if_missing=True, percent10=True):"],"delete":["236","                          download_if_missing=True, random_state=None,","237","                          percent10=True):","251","    random_state : int, RandomState instance or None, optional (default=None)","252","        Random state for shuffling the dataset.","253","        If int, random_state is the seed used by the random number generator;","254","        If RandomState instance, random_state is the random number generator;","255","        If None, the random number generator is the RandomState instance used","256","        by `np.random`.","257",""]}]}},"74b69df981878cef73099eb296ee9c2d88a986b6":{"changes":{"sklearn\/svm\/libsvm.pyx":"MODIFY","sklearn\/svm\/base.py":"MODIFY","sklearn\/svm\/tests\/test_svm.py":"MODIFY"},"diff":{"sklearn\/svm\/libsvm.pyx":[{"add":["56","    int svm_type=0, kernel='rbf', int degree=3,","344","    int svm_type=0, kernel='rbf', int degree=3,","464","    int n_fold, svm_type=0, kernel='rbf', int degree=3,"],"delete":["56","    int svm_type=0, str kernel='rbf', int degree=3,","344","    int svm_type=0, str kernel='rbf', int degree=3,","464","    int n_fold, svm_type=0, str kernel='rbf', int degree=3,"]}],"sklearn\/svm\/base.py":[{"add":[],"delete":["230","        if six.PY2:","231","            # In python2 ensure kernel is ascii bytes to prevent a TypeError","232","            if isinstance(kernel, six.types.UnicodeType):","233","                kernel = str(kernel)","234","        if six.PY3:","235","            # In python3 ensure kernel is utf8 unicode to prevent a TypeError","236","            if isinstance(kernel, bytes):","237","                kernel = str(kernel, 'utf8')","238",""]}],"sklearn\/svm\/tests\/test_svm.py":[{"add":["507","    # Test that a unicode kernel name does not cause a TypeError","510","        clf = svm.SVC(kernel=u'linear', probability=True)","512","        clf.predict_proba(T)","513","        svm.libsvm.cross_validation(iris.data,","514","                                    iris.target.astype(np.float64), 5,","515","                                    kernel=u'linear',","516","                                    random_seed=0)","519","    clf = svm.SVC(kernel='linear', probability=True)","521","    clf.predict_proba(T)","522","    svm.libsvm.cross_validation(iris.data,","523","                                iris.target.astype(np.float64), 5,","524","                                kernel='linear',","525","                                random_seed=0)"],"delete":["507","    # Test that a unicode kernel name does not cause a TypeError on clf.fit","510","        clf = svm.SVC(kernel=unicode('linear'))","512","","513","        # Test ascii bytes (str is bytes in python2)","514","        clf = svm.SVC(kernel=str('linear'))","515","        clf.fit(X, Y)","516","    else:","517","        # Test unicode (str is unicode in python3)","518","        clf = svm.SVC(kernel=str('linear'))","519","        clf.fit(X, Y)","520","","521","        # Test ascii bytes (same as str on python2)","522","        clf = svm.SVC(kernel=bytes('linear', 'ascii'))","523","        clf.fit(X, Y)","526","    clf = svm.SVC(kernel='linear')"]}]}},"da71b827b8b56bd8305b7fe6c13724c7b5355209":{"changes":{"sklearn\/linear_model\/tests\/test_coordinate_descent.py":"MODIFY","doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/linear_model\/coordinate_descent.py":"MODIFY"},"diff":{"sklearn\/linear_model\/tests\/test_coordinate_descent.py":[{"add":["805","","806","","807","def test_coef_shape_not_zero():","808","    est_no_intercept = Lasso(fit_intercept=False)","809","    est_no_intercept.fit(np.c_[np.ones(3)], np.ones(3))","810","    assert est_no_intercept.coef_.shape == (1,)"],"delete":[]}],"doc\/whats_new\/v0.20.rst":[{"add":["248","- Fixed a bug in :class:`sklearn.linear_model.Lasso`","249","  where the coefficient had wrong shape when ``fit_intercept=False``.","250","  :issue:`10687` by :user:`Martin Hahn <martin-hahn>`.","251",""],"delete":[]}],"sklearn\/linear_model\/coordinate_descent.py":[{"add":["765","            self.coef_ = coef_[0]","766","            self.dual_gap_ = dual_gaps_[0]","767","        else:","768","            self.coef_ = coef_","769","            self.dual_gap_ = dual_gaps_"],"delete":["766","        self.coef_, self.dual_gap_ = map(np.squeeze, [coef_, dual_gaps_])"]}]}},"6eb218e301430a97e167ad8da018062f317c4d7e":{"changes":{"sklearn\/feature_extraction\/tests\/test_image.py":"MODIFY","doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/feature_extraction\/image.py":"MODIFY"},"diff":{"sklearn\/feature_extraction\/tests\/test_image.py":[{"add":["4","from __future__ import division","173","def test_extract_patch_same_size_image():","174","    face = downsampled_face","175","    # Request patches of the same size as image","176","    # Should return just the single patch a.k.a. the image","177","    patches = extract_patches_2d(face, face.shape, max_patches=2)","178","    assert_equal(patches.shape[0], 1)","179","","180","","181","def test_extract_patches_less_than_max_patches():","182","    face = downsampled_face","183","    i_h, i_w = face.shape","184","    p_h, p_w = 3 * i_h \/\/ 4, 3 * i_w \/\/ 4","185","    # this is 3185","186","    expected_n_patches = (i_h - p_h + 1) * (i_w - p_w + 1)","187","","188","    patches = extract_patches_2d(face, (p_h, p_w), max_patches=4000)","189","    assert_equal(patches.shape, (expected_n_patches, p_h, p_w))","190","","191",""],"delete":[]}],"doc\/whats_new\/v0.20.rst":[{"add":["205","Feature Extraction","206","","207","- Fixed a bug in :func:`feature_extraction.image.extract_patches_2d` which would","208","  throw an exception if ``max_patches`` was greater than or equal to the number","209","  of all possible patches rather than simply returning the number of possible","210","  patches. :issue:`10100` by :user:`Varun Agrawal <varunagrawal>`","211","","212","\t"],"delete":[]}],"sklearn\/feature_extraction\/image.py":[{"add":["231","        elif (isinstance(max_patches, (numbers.Integral))","232","              and max_patches >= all_patches):","233","            return all_patches"],"delete":[]}]}},"5d9a686be14560eb49e5a8b9bb237b776c519373":{"changes":{"sklearn\/linear_model\/stochastic_gradient.py":"MODIFY","sklearn\/linear_model\/tests\/test_sgd.py":"MODIFY"},"diff":{"sklearn\/linear_model\/stochastic_gradient.py":[{"add":["848","        check_is_fitted(self, \"t_\")","849",""],"delete":["804","        check_is_fitted(self, \"t_\")","805",""]}],"sklearn\/linear_model\/tests\/test_sgd.py":[{"add":["2","import pytest","470","    def test_sgd_predict_proba_method_access(self):","471","        # Checks that SGDClassifier predict_proba and predict_log_proba methods","472","        # can either be accessed or raise an appropriate error message","473","        # otherwise. See","474","        # https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/10938 for more","475","        # details.","476","        for loss in SGDClassifier.loss_functions:","477","            clf = SGDClassifier(loss=loss)","478","            if loss in ('log', 'modified_huber'):","479","                assert hasattr(clf, 'predict_proba')","480","                assert hasattr(clf, 'predict_log_proba')","481","            else:","482","                message = (\"probability estimates are not \"","483","                           \"available for loss={!r}\".format(loss))","484","                assert not hasattr(clf, 'predict_proba')","485","                assert not hasattr(clf, 'predict_log_proba')","486","                with pytest.raises(AttributeError,","487","                                   message=message):","488","                    clf.predict_proba","489","                with pytest.raises(AttributeError,","490","                                   message=message):","491","                    clf.predict_log_proba","492",""],"delete":[]}]}},"c6005e1ebdf5605a21cb4d0c4a268bb8f0c8396b":{"changes":{"sklearn\/linear_model\/tests\/test_bayes.py":"MODIFY","sklearn\/linear_model\/bayes.py":"MODIFY"},"diff":{"sklearn\/linear_model\/tests\/test_bayes.py":[{"add":["10","from sklearn.utils.testing import assert_array_less","12","from sklearn.utils import check_random_state","64","def test_prediction_bayesian_ridge_ard_with_constant_input():","65","    # Test BayesianRidge and ARDRegression predictions for edge case of","66","    # constant target vectors","67","    n_samples = 4","68","    n_features = 5","69","    random_state = check_random_state(42)","70","    constant_value = random_state.rand()","71","    X = random_state.random_sample((n_samples, n_features))","72","    y = np.full(n_samples, constant_value)","73","    expected = np.full(n_samples, constant_value)","74","","75","    for clf in [BayesianRidge(), ARDRegression()]:","76","        y_pred = clf.fit(X, y).predict(X)","77","        assert_array_almost_equal(y_pred, expected)","78","","79","","80","def test_std_bayesian_ridge_ard_with_constant_input():","81","    # Test BayesianRidge and ARDRegression standard dev. for edge case of","82","    # constant target vector","83","    # The standard dev. should be relatively small (< 0.01 is tested here)","84","    n_samples = 4","85","    n_features = 5","86","    random_state = check_random_state(42)","87","    constant_value = random_state.rand()","88","    X = random_state.random_sample((n_samples, n_features))","89","    y = np.full(n_samples, constant_value)","90","    expected_upper_boundary = 0.01","91","","92","    for clf in [BayesianRidge(), ARDRegression()]:","93","        _, y_std = clf.fit(X, y).predict(X, return_std=True)","94","        assert_array_less(y_std, expected_upper_boundary)","95","","96",""],"delete":[]}],"sklearn\/linear_model\/bayes.py":[{"add":["164","        eps = np.finfo(np.float64).eps","165","        # Add `eps` in the denominator to omit division by zero if `np.var(y)`","166","        # is zero","167","        alpha_ = 1. \/ (np.var(y) + eps)","450","        eps = np.finfo(np.float64).eps","451","        # Add `eps` in the denominator to omit division by zero if `np.var(y)`","452","        # is zero","453","        alpha_ = 1. \/ (np.var(y) + eps)"],"delete":["164","        alpha_ = 1. \/ np.var(y)","447","        alpha_ = 1. \/ np.var(y)"]}]}},"96a2c10da1d5d793a57d8c1734f7a06c5ec88347":{"changes":{"sklearn\/linear_model\/tests\/test_coordinate_descent.py":"MODIFY",".gitignore":"MODIFY","doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/linear_model\/coordinate_descent.py":"MODIFY"},"diff":{"sklearn\/linear_model\/tests\/test_coordinate_descent.py":[{"add":["5","import pytest","672","@pytest.mark.parametrize(\"check_input\", [True, False])","673","def test_enet_copy_X_True(check_input):","674","    X, y, _, _ = build_dataset()","675","    X = X.copy(order='F')","676","","677","    original_X = X.copy()","678","    enet = ElasticNet(copy_X=True)","679","    enet.fit(X, y, check_input=check_input)","680","","681","    assert_array_equal(original_X, X)","682","","683","","684","def test_enet_copy_X_False_check_input_False():","685","    X, y, _, _ = build_dataset()","686","    X = X.copy(order='F')","687","","688","    original_X = X.copy()","689","    enet = ElasticNet(copy_X=False)","690","    enet.fit(X, y, check_input=False)","691","","692","    # No copying, X is overwritten","693","    assert_true(np.any(np.not_equal(original_X, X)))","694","","695",""],"delete":[]}],".gitignore":[{"add":["67",".pytest_cache\/","68","_configtest.o.d"],"delete":[]}],"doc\/whats_new\/v0.20.rst":[{"add":["237","- Fixed a bug in :class:`linear_model.ElasticNet` which caused the input to be","238","  overridden when using parameter ``copy_X=True`` and ``check_input=False``.","239","  :issue:`10581` by :user:`Yacine Mazari <ymazari>`.","240",""],"delete":[]}],"sklearn\/linear_model\/coordinate_descent.py":[{"add":["702","        # Remember if X is copied","703","        X_copied = False","707","            X_copied = self.copy_X and self.fit_intercept","710","                             copy=X_copied, multi_output=True, y_numeric=True)","714","        # Ensure copying happens only once, don't do it again if done above","715","        should_copy = self.copy_X and not X_copied","718","                     self.fit_intercept, copy=should_copy)"],"delete":["707","                             copy=self.copy_X and self.fit_intercept,","708","                             multi_output=True, y_numeric=True)","714","                     self.fit_intercept, copy=False)"]}]}},"9a301b45789b0c09336b48b4e1d6091d3c33d5c0":{"changes":{"doc\/developers\/tips.rst":"MODIFY"},"diff":{"doc\/developers\/tips.rst":[{"add":["81","When a unit test fails, the following tricks can make debugging easier:"],"delete":["81","When a unit tests fail, the following tricks can make debugging easier:"]}]}},"53622e856c2747c7b559d3d29410cd37ca0addd2":{"changes":{"examples\/ensemble\/plot_isolation_forest.py":"MODIFY","sklearn\/ensemble\/iforest.py":"MODIFY","doc\/whats_new\/v0.20.rst":"MODIFY","examples\/plot_anomaly_comparison.py":"MODIFY","sklearn\/utils\/estimator_checks.py":"MODIFY","sklearn\/ensemble\/tests\/test_iforest.py":"MODIFY","benchmarks\/bench_isolation_forest.py":"MODIFY"},"diff":{"examples\/ensemble\/plot_isolation_forest.py":[{"add":["42","clf = IsolationForest(behaviour='new', max_samples=100,","43","                      random_state=rng, contamination='auto')"],"delete":["42","clf = IsolationForest(max_samples=100, random_state=rng, contamination='auto')"]}],"sklearn\/ensemble\/iforest.py":[{"add":["91","    behaviour : str, default='old'","92","        Behaviour of the ``decision_function`` which can be either 'old' or","93","        'new'. Passing ``behaviour='new'`` makes the ``decision_function``","94","        change to match other anomaly detection algorithm API which will be","95","        the default behaviour in the future. As explained in details in the","96","        ``offset_`` attribute documentation, the ``decision_function`` becomes","97","        dependent on the contamination parameter, in such a way that 0 becomes","98","        its natural threshold to detect outliers.","99","","100","        .. versionadded:: 0.20","101","           ``behaviour`` is added in 0.20 for back-compatibility purpose.","102","","103","        .. deprecated:: 0.20","104","           ``behaviour='old'`` is deprecated in 0.20 and will not be possible","105","           in 0.22.","106","","107","        .. deprecated:: 0.22","108","           ``behaviour`` parameter will be deprecated in 0.22 and removed in","109","           0.24.","110","","136","        Assuming behaviour == 'new', offset_ is defined as follows.","143","        Assuming the behaviour parameter is set to 'old', we always have","144","        offset_ = -0.5, making the decision function independent from the","145","        contamination parameter.","164","                 behaviour='old',","181","","182","        self.behaviour = behaviour","214","        if self.behaviour == 'old':","215","            warnings.warn('behaviour=\"old\" is deprecated and will be removed '","216","                          'in version 0.22. Please use behaviour=\"new\", which '","217","                          'makes the decision_function change to match '","218","                          'other anomaly detection algorithm API.',","219","                          FutureWarning)","220","","262","        if self.behaviour == 'old':","263","            # in this case, decision_function = 0.5 + self.score_samples(X):","264","            if self._contamination == \"auto\":","265","                raise ValueError(\"contamination parameter cannot be set to \"","266","                                 \"'auto' when behaviour == 'old'.\")","267","","268","            self.offset_ = -0.5","269","            self._threshold_ = sp.stats.scoreatpercentile(","270","                self.decision_function(X), 100. * self._contamination)","271","","272","            return self","273","","274","        # else, self.behaviour == 'new':","279","            return self","280","","281","        # else, define offset_ wrt contamination parameter, so that the","282","        # threshold_ attribute is implicitly 0 and is not needed anymore:","283","        self.offset_ = sp.stats.scoreatpercentile(","284","            self.score_samples(X), 100. * self._contamination)","307","        threshold = self.threshold_ if self.behaviour == 'old' else 0","308","        is_inlier[self.decision_function(X) < threshold] = -1","409","        if self.behaviour != 'old':","410","            raise AttributeError(\"threshold_ attribute does not exist when \"","411","                                 \"behaviour != 'old'\")","414","        return self._threshold_"],"delete":["232","            # need to save (depreciated) threshold_ in this case:","233","            self._threshold_ = sp.stats.scoreatpercentile(","234","                self.score_samples(X), 100. * 0.1)","235","        else:","236","            self.offset_ = sp.stats.scoreatpercentile(","237","                self.score_samples(X), 100. * self._contamination)","260","        is_inlier[self.decision_function(X) < 0] = -1","363","        if self.contamination == 'auto':","364","            return self._threshold_","365","        return self.offset_"]}],"doc\/whats_new\/v0.20.rst":[{"add":["915"," - A ``behaviour`` parameter has been introduced in :class:`ensemble.IsolationForest`","916","  to ensure backward compatibility.","917","  In the old behaviour, the ``decision_function`` is independent of the ``contamination``","918","  parameter. A threshold attribute depending on the ``contamination`` parameter is thus","919","  used.","920","  In the new behaviour the ``decision_function`` is dependent on the ``contamination``","921","  parameter, in such a way that 0 becomes its natural threshold to detect outliers.","922","  Setting behaviour to \"old\" is deprecated and will not be possible in version 0.22.","923","  Beside, the behaviour parameter will be removed in 0.24.","924","  :issue:`11553` by `Nicolas Goix`_.","925",""],"delete":[]}],"examples\/plot_anomaly_comparison.py":[{"add":["82","    (\"Isolation Forest\", IsolationForest(behaviour='new',","83","                                         contamination=outliers_fraction,"],"delete":["82","    (\"Isolation Forest\", IsolationForest(contamination=outliers_fraction,"]}],"sklearn\/utils\/estimator_checks.py":[{"add":["370","    if estimator.__class__.__name__ == \"IsolationForest\":","371","        # XXX to be removed in 0.22.","372","        # this is used because the old IsolationForest does not","373","        # respect the outlier detection API and thus and does not","374","        # pass the outlier detection common tests.","375","        estimator.set_params(behaviour='new')","376",""],"delete":[]}],"sklearn\/ensemble\/tests\/test_iforest.py":[{"add":["17","from sklearn.utils.testing import assert_raises_regex","50","@pytest.mark.filterwarnings('ignore:threshold_ attribute')","67","@pytest.mark.filterwarnings('ignore:threshold_ attribute')","68","@pytest.mark.filterwarnings('ignore:behaviour=\"old\"')","97","@pytest.mark.filterwarnings('ignore:threshold_ attribute')","98","@pytest.mark.filterwarnings('ignore:behaviour=\"old\"')","136","    # test threshold_ attribute error when behaviour is not old:","137","    msg = \"threshold_ attribute does not exist when behaviour != 'old'\"","138","    assert_raises_regex(AttributeError, msg, getattr,","139","                        IsolationForest(behaviour='new'), 'threshold_')","140","","143","@pytest.mark.filterwarnings('ignore:behaviour=\"old\"')","153","@pytest.mark.filterwarnings('ignore:behaviour=\"old\"')","170","@pytest.mark.filterwarnings('ignore:threshold_ attribute')","171","@pytest.mark.filterwarnings('ignore:behaviour=\"old\"')","197","@pytest.mark.filterwarnings('ignore:behaviour=\"old\"')","222","@pytest.mark.filterwarnings('ignore:threshold_ attribute')","229","        clf = IsolationForest(behaviour='new', random_state=rng,","230","                              contamination=contamination)","240","@pytest.mark.filterwarnings('ignore:behaviour=\"old\"')","249","@pytest.mark.filterwarnings('ignore:threshold_ attribute')","250","@pytest.mark.filterwarnings('ignore:behaviour=\"old\"')","276","@pytest.mark.filterwarnings('ignore:behaviour=\"old\"')","289","@pytest.mark.filterwarnings('ignore:default contamination')","290","@pytest.mark.filterwarnings('ignore:behaviour=\"old\"')","300","    assert_warns_message(FutureWarning,","301","                         'behaviour=\"old\" is deprecated and will be removed '","302","                         'in version 0.22',","303","                         clf.fit, X)","304","","305","    clf = IsolationForest().fit(X)","310","","311","","312","@pytest.mark.filterwarnings('ignore:default contamination')","313","@pytest.mark.filterwarnings('ignore:behaviour=\"old\"')","314","def test_behaviour_param():","315","    X_train = [[1, 1], [1, 2], [2, 1]]","316","    clf1 = IsolationForest(behaviour='old').fit(X_train)","317","    clf2 = IsolationForest(behaviour='new', contamination='auto').fit(X_train)","318","    assert_array_equal(clf1.decision_function([[2., 2.]]),","319","                       clf2.decision_function([[2., 2.]]))"],"delete":["212","        clf = IsolationForest(random_state=rng, contamination=contamination)","276","    clf = IsolationForest(contamination='auto').fit(X)"]}],"benchmarks\/bench_isolation_forest.py":[{"add":["121","    model = IsolationForest(behaviour='new', n_jobs=-1,","122","                            random_state=random_state)"],"delete":["121","    model = IsolationForest(n_jobs=-1, random_state=random_state)"]}]}},"816e2eb5233d7926d4fe7c1892abfe329d0a8bdc":{"changes":{"\/dev\/null":"DELETE","doc\/conftest.py":"ADD"},"diff":{"\/dev\/null":[{"add":[],"delete":[]}],"doc\/conftest.py":[{"add":[],"delete":[]}]}},"5cf3fb70a1a476be79d262ce8bff4cee73b72401":{"changes":{"sklearn\/manifold\/locally_linear.py":"MODIFY"},"diff":{"sklearn\/manifold\/locally_linear.py":[{"add":["591","    embedding_ : array-like, shape [n_samples, n_components]","595","        Reconstruction error associated with `embedding_`"],"delete":["591","    embedding_vectors_ : array-like, shape [n_components, n_samples]","595","        Reconstruction error associated with `embedding_vectors_`"]}]}},"fe6e562c777179f60e43cca40d86203bf2276178":{"changes":{"sklearn\/neural_network\/multilayer_perceptron.py":"MODIFY"},"diff":{"sklearn\/neural_network\/multilayer_perceptron.py":[{"add":["930","            if len(np.setdiff1d(classes, self.classes_, assume_unique=True)):"],"delete":["930","            if np.setdiff1d(classes, self.classes_, assume_unique=True):"]}]}},"7d745eeed354c205bcccefb95f72cfbc283bd6e7":{"changes":{"doc\/modules\/clustering.rst":"MODIFY","doc\/whats_new\/v0.20.rst":"MODIFY","doc\/modules\/classes.rst":"MODIFY","sklearn\/cluster\/setup.py":"MODIFY","examples\/cluster\/plot_optics.py":"ADD","sklearn\/cluster\/__init__.py":"MODIFY","examples\/cluster\/plot_cluster_comparison.py":"MODIFY","sklearn\/cluster\/tests\/test_optics.py":"ADD","sklearn\/cluster\/_optics_inner.pyx":"ADD","sklearn\/cluster\/optics_.py":"ADD"},"diff":{"doc\/modules\/clustering.rst":[{"add":["93","   * - :ref:`OPTICS <optics>`","94","     - minimum cluster membership","95","     - Very large ``n_samples``, large ``n_clusters``","96","     - Non-flat geometry, uneven cluster sizes, variable cluster density ","97","     - Distances between points","98","   ","804","    - Use OPTICS clustering in conjunction with the `extract_dbscan` method. OPTICS","805","      clustering also calculates the full pairwise matrix, but only keeps one row in","806","      memory at a time (memory complexity n).","807","","826",".. _optics:","827","","828","OPTICS","829","======","830","","831","The :class:`OPTICS` algorithm shares many similarities with the","832",":class:`DBSCAN` algorithm, and can be considered a generalization of","833","DBSCAN that relaxes the ``eps`` requirement from a single value to a value","834","range. The key difference between DBSCAN and OPTICS is that the OPTICS","835","algorithm builds a *reachability* graph, which assigns each sample both a","836","``reachability_`` distance, and a spot within the cluster ``ordering_``","837","attribute; these two attributes are assigned when the model is fitted, and are","838","used to determine cluster membership. If OPTICS is run with the default value","839","of *inf* set for ``max_bound``, then DBSCAN style cluster extraction can be","840","performed in linear time for any given ``eps`` value using the","841","``extract_dbscan`` method. Setting ``max_bound`` to a lower value will result","842","in shorter run times, and can be thought of as the maximum cluster object size","843","(in diameter) that OPTICS will be able to extract.","844","","845",".. |optics_results| image:: ..\/auto_examples\/cluster\/images\/sphx_glr_plot_optics_001.png","846","        :target: ..\/auto_examples\/cluster\/plot_optics.html","847","        :scale: 50","848","","849",".. centered:: |optics_results|","850","","851","The *reachability* distances generated by OPTICS allow for variable density","852","extraction of clusters within a single data set. As shown in the above plot,","853","combining *reachability* distances and data set ``ordering_`` produces a","854","*reachability plot*, where point density is represented on the Y-axis, and","855","points are ordered such that nearby points are adjacent. 'Cutting' the","856","reachability plot at a single value produces DBSCAN like results; all points","857","above the 'cut' are classified as noise, and each time that there is a break","858","when reading from left to right signifies a new cluster. The default cluster","859","extraction with OPTICS looks at changes in slope within the graph to guess at","860","natural clusters. There are also other possibilities for analysis on the graph","861","itself, such as generating hierarchical representations of the data through","862","reachability-plot dendrograms. The plot above has been color-coded so that","863","cluster colors in planar space match the linear segment clusters of the","864","reachability plot-- note that the blue and red clusters are adjacent in the","865","reachability plot, and can be hierarchically represented as children of a","866","larger parent cluster.","867","","868",".. topic:: Examples:","869","","870","     * :ref:`sphx_glr_auto_examples_cluster_plot_optics.py`","871","","872","","873",".. topic:: Comparison with DBSCAN","874","    ","875","    The results from OPTICS ``extract_dbscan`` method and DBSCAN are not quite","876","    identical. Specifically, while *core_samples* returned from both OPTICS","877","    and DBSCAN are guaranteed to be identical, labeling of periphery and noise","878","    points is not. This is in part because the first sample processed by","879","    OPTICS will always have a reachability distance that is set to ``inf``,","880","    and will thus generally be marked as noise rather than periphery. This","881","    affects adjacent points when they are considered as candidates for being","882","    marked as either periphery or noise. While this effect is quite local to","883","    the starting point of the dataset and is unlikely to be noticed on even","884","    moderately large datasets, it is worth also noting that non-core boundry","885","    points may switch cluster labels on the rare occasion that they are","886","    equidistant to a competeing cluster due to how the graph is read from left","887","    to right when assigning labels. ","888","","889","    Note that for any single value of ``eps``, DBSCAN will tend to have a","890","    shorter run time than OPTICS; however, for repeated runs at varying ``eps``","891","    values, a single run of OPTICS may require less cumulative runtime than","892","    DBSCAN. It is also important to note that OPTICS output can be unstable at","893","    ``eps`` values very close to the initial ``max_bound`` value. OPTICS seems","894","    to produce near identical results to DBSCAN provided that ``eps`` passed to","895","    ``extract_dbscan`` is a half order of magnitude less than the inital","896","    ``max_bound`` that was used to fit; using a value close to ``max_bound``","897","    will throw a warning, and using a value larger will result in an exception. ","898","","899",".. topic:: Computational Complexity","900","","901","    Spatial indexing trees are used to avoid calculating the full distance","902","    matrix, and allow for efficient memory usage on large sets of samples.","903","    Different distance metrics can be supplied via the ``metric`` keyword.","904","    ","905","    For large datasets, similar (but not identical) results can be obtained via","906","    `HDBSCAN <https:\/\/hdbscan.readthedocs.io>`_. The HDBSCAN implementation is","907","    multithreaded, and has better algorithmic runtime complexity than OPTICS--","908","    at the cost of worse memory scaling. For extremely large datasets that","909","    exhaust system memory using HDBSCAN, OPTICS will maintain *n* (as opposed","910","    to *n^2* memory scaling); however, tuning of the ``max_bound`` parameter","911","    will likely need to be used to give a solution in a reasonable amount of","912","    wall time.","913","","914",".. topic:: References:","915","","916"," *  \"OPTICS: ordering points to identify the clustering structure.\"","917","    Ankerst, Mihael, Markus M. Breunig, Hans-Peter Kriegel, and J?rg Sander.","918","    In ACM Sigmod Record, vol. 28, no. 2, pp. 49-60. ACM, 1999.","919","","920"," *  \"Automatic extraction of clusters from hierarchical clustering","921","    representations.\"","922","    Sander, J?rg, Xuejie Qin, Zhiyong Lu, Nan Niu, and Alex Kovarsky.","923","    In Advances in knowledge discovery and data mining,","924","    pp. 75-87. Springer Berlin Heidelberg, 2003. ","925",""],"delete":[]}],"doc\/whats_new\/v0.20.rst":[{"add":["179","- A new clustering algorithm: :class:`cluster.OPTICS`: an algoritm","180","  related to :class:`cluster.DBSCAN`, that has hyperparameters easier to","181","  set and tat scales better, by :user:`Shane <espg>`.","182",""],"delete":[]}],"doc\/modules\/classes.rst":[{"add":["100","   cluster.OPTICS","115","   cluster.optics"],"delete":[]}],"sklearn\/cluster\/setup.py":[{"add":["25","    config.add_extension('_optics_inner',","26","                         sources=['_optics_inner.pyx'],","27","                         include_dirs=[numpy.get_include()],","28","                         libraries=libraries)"],"delete":[]}],"examples\/cluster\/plot_optics.py":[{"add":[],"delete":[]}],"sklearn\/cluster\/__init__.py":[{"add":["13","from .optics_ import OPTICS","21","           'OPTICS',"],"delete":[]}],"examples\/cluster\/plot_cluster_comparison.py":[{"add":["118","    optics = cluster.OPTICS(min_samples=30, maxima_ratio=.8,","119","                            rejection_ratio=.4)","137","        ('OPTICS', optics),"],"delete":[]}],"sklearn\/cluster\/tests\/test_optics.py":[{"add":[],"delete":[]}],"sklearn\/cluster\/_optics_inner.pyx":[{"add":[],"delete":[]}],"sklearn\/cluster\/optics_.py":[{"add":[],"delete":[]}]}},"6c4b87868470c933eaf28d2dc2f3e59ec5e78724":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/linear_model\/tests\/test_logistic.py":"MODIFY","sklearn\/linear_model\/logistic.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["75","- :class:`linear_model.LogisticRegressionCV` (bug fix)","467","- Fixed a bug in :class:`linear_model.LogisticRegressionCV` where the 'ovr'","468","  strategy was always used to compute cross-validation scores in the","469","  multiclass setting, even if 'multinomial' was set.","470","  :issue:`8720` by :user:`William de Vazelhes <wdevazelhes>`.","471",""],"delete":[]}],"sklearn\/linear_model\/tests\/test_logistic.py":[{"add":["8","from sklearn.metrics.scorer import get_scorer","32","    _log_reg_scoring_path)","495","@pytest.mark.parametrize('scoring, multiclass_agg_list',","496","                         [('accuracy', ['']),","497","                          ('precision', ['_macro', '_weighted']),","498","                          # no need to test for micro averaging because it","499","                          # is the same as accuracy for f1, precision,","500","                          # and recall (see https:\/\/github.com\/","501","                          # scikit-learn\/scikit-learn\/pull\/","502","                          # 11578#discussion_r203250062)","503","                          ('f1', ['_macro', '_weighted']),","504","                          ('neg_log_loss', ['']),","505","                          ('recall', ['_macro', '_weighted'])])","506","def test_logistic_cv_multinomial_score(scoring, multiclass_agg_list):","507","    # test that LogisticRegressionCV uses the right score to compute its","508","    # cross-validation scores when using a multinomial scoring","509","    # see https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/8720","510","    X, y = make_classification(n_samples=100, random_state=0, n_classes=3,","511","                               n_informative=6)","512","    train, test = np.arange(80), np.arange(80, 100)","513","    lr = LogisticRegression(C=1., solver='lbfgs', multi_class='multinomial')","514","    # we use lbfgs to support multinomial","515","    params = lr.get_params()","516","    # we store the params to set them further in _log_reg_scoring_path","517","    for key in ['C', 'n_jobs', 'warm_start']:","518","        del params[key]","519","    lr.fit(X[train], y[train])","520","    for averaging in multiclass_agg_list:","521","        scorer = get_scorer(scoring + averaging)","522","        assert_array_almost_equal(","523","            _log_reg_scoring_path(X, y, train, test, Cs=[1.],","524","                                  scoring=scorer, **params)[2][0],","525","            scorer(lr, X[test], y[test]))","526","","527",""],"delete":["31",")"]}],"sklearn\/linear_model\/logistic.py":[{"add":["925","    log_reg = LogisticRegression(multi_class=multi_class)"],"delete":["925","    log_reg = LogisticRegression(fit_intercept=fit_intercept)"]}]}},"71402efc2ff2205ed15811c5b29787a413a4b588":{"changes":{"sklearn\/linear_model\/stochastic_gradient.py":"MODIFY"},"diff":{"sklearn\/linear_model\/stochastic_gradient.py":[{"add":["1258","        - 'optimal': eta = 1.0 \/ (alpha * (t + t0))","1259","        - 'invscaling': eta = eta0 \/ pow(t, power_t) [default]"],"delete":["1258","        - 'optimal': eta = 1.0 \/ (alpha * (t + t0)) [default]","1259","        - 'invscaling': eta = eta0 \/ pow(t, power_t)"]}]}},"7ffad2c63f3f2a6497437e841d3f0ce5755788c7":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/linear_model\/tests\/test_bayes.py":"MODIFY","sklearn\/linear_model\/bayes.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["24","- :class:`linear_model.ARDRegression` (bug fix)","143","- Fixed a bug in :class:`linear_model.ARDRegression` which caused incorrectly","144","  updated estimates for the standard deviation and the coefficients.","145","  :issue:`10153` by :user:`J?rg D?pfert <jdoepfert>`.","146",""],"delete":[]}],"sklearn\/linear_model\/tests\/test_bayes.py":[{"add":["11","from sklearn.utils.testing import assert_equal","113","def test_update_of_sigma_in_ard():","114","    # Checks that `sigma_` is updated correctly after the last iteration","115","    # of the ARDRegression algorithm. See issue #10128.","116","    X = np.array([[1, 0],","117","                  [0, 0]])","118","    y = np.array([0, 0])","119","    clf = ARDRegression(n_iter=1)","120","    clf.fit(X, y)","121","    # With the inputs above, ARDRegression prunes one of the two coefficients","122","    # in the first iteration. Hence, the expected shape of `sigma_` is (1, 1).","123","    assert_equal(clf.sigma_.shape, (1, 1))","124","    # Ensure that no error is thrown at prediction stage","125","    clf.predict(X, return_std=True)","126","","127",""],"delete":[]}],"sklearn\/linear_model\/bayes.py":[{"add":["471","        # Compute sigma and mu (using Woodbury matrix identity)","472","        def update_sigma(X, alpha_, lambda_, keep_lambda, n_samples):","482","            return sigma_","483","","484","        def update_coeff(X, y, coef_, alpha_, keep_lambda, sigma_):","487","            return coef_","488","","489","        # Iterative procedure of ARDRegression","490","        for iter_ in range(self.n_iter):","491","            sigma_ = update_sigma(X, alpha_, lambda_, keep_lambda, n_samples)","492","            coef_ = update_coeff(X, y, coef_, alpha_, keep_lambda, sigma_)","523","        # update sigma and mu using updated parameters from the last iteration","524","        sigma_ = update_sigma(X, alpha_, lambda_, keep_lambda, n_samples)","525","        coef_ = update_coeff(X, y, coef_, alpha_, keep_lambda, sigma_)","526",""],"delete":["471","        # Iterative procedure of ARDRegression","472","        for iter_ in range(self.n_iter):","473","            # Compute mu and sigma (using Woodbury matrix identity)"]}]}},"97a15dbad3ef1ae8fd35a9ff4cb33b3a522b7341":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/linear_model\/tests\/test_logistic.py":"MODIFY","sklearn\/linear_model\/logistic.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["439","  ","440","- Fixed a bug in :func:`sklearn.linear_model.LogisticRegression` where the ","441","  multi_class='multinomial' with binary output with warm_start = True","442","  :issue:`10836` by :user:`Aishwarya Srinivasan <aishgrt1>`."],"delete":[]}],"sklearn\/linear_model\/tests\/test_logistic.py":[{"add":["12","from sklearn.utils.testing import assert_allclose","1241","","1242","","1243","def test_warm_start_converge_LR():","1244","    # Test to see that the logistic regression converges on warm start,","1245","    # with multi_class='multinomial'. Non-regressive test for #10836","1246","","1247","    rng = np.random.RandomState(0)","1248","    X = np.concatenate((rng.randn(100, 2) + [1, 1], rng.randn(100, 2)))","1249","    y = np.array([1] * 100 + [-1] * 100)","1250","    lr_no_ws = LogisticRegression(multi_class='multinomial',","1251","                                  solver='sag', warm_start=False)","1252","    lr_ws = LogisticRegression(multi_class='multinomial',","1253","                               solver='sag', warm_start=True)","1254","","1255","    lr_no_ws_loss = log_loss(y, lr_no_ws.fit(X, y).predict_proba(X))","1256","    lr_ws_loss = [log_loss(y, lr_ws.fit(X, y).predict_proba(X)) ","1257","                 for _ in range(5)]","1258","","1259","    for i in range(5):","1260","        assert_allclose(lr_no_ws_loss, lr_ws_loss[i], rtol=1e-5)"],"delete":[]}],"sklearn\/linear_model\/logistic.py":[{"add":["678","","679","            if n_classes == 1:","680","                w0[0, :coef.shape[1]] = -coef","681","                w0[1, :coef.shape[1]] = coef","682","            else:","683","                w0[:, :coef.shape[1]] = coef","684",""],"delete":["678","            w0[:, :coef.shape[1]] = coef"]}]}},"888bced086d76199a54f462eb14700df8bc387aa":{"changes":{"sklearn\/datasets\/rcv1.py":"MODIFY"},"diff":{"sklearn\/datasets\/rcv1.py":[{"add":["26","# The original vectorized data can be found at:","27","#    http:\/\/www.ai.mit.edu\/projects\/jmlr\/papers\/volume5\/lewis04a\/a13-vector-files\/lyrl2004_vectors_test_pt0.dat.gz","28","#    http:\/\/www.ai.mit.edu\/projects\/jmlr\/papers\/volume5\/lewis04a\/a13-vector-files\/lyrl2004_vectors_test_pt1.dat.gz","29","#    http:\/\/www.ai.mit.edu\/projects\/jmlr\/papers\/volume5\/lewis04a\/a13-vector-files\/lyrl2004_vectors_test_pt2.dat.gz","30","#    http:\/\/www.ai.mit.edu\/projects\/jmlr\/papers\/volume5\/lewis04a\/a13-vector-files\/lyrl2004_vectors_test_pt3.dat.gz","31","#    http:\/\/www.ai.mit.edu\/projects\/jmlr\/papers\/volume5\/lewis04a\/a13-vector-files\/lyrl2004_vectors_train.dat.gz","32","# while the original stemmed token files can be found","33","# in the README, section B.12.i.:","34","#    http:\/\/www.ai.mit.edu\/projects\/jmlr\/papers\/volume5\/lewis04a\/lyrl2004_rcv1v2_README.htm"],"delete":["26","# The original data can be found at:","27","# http:\/\/jmlr.csail.mit.edu\/papers\/volume5\/lewis04a\/a13-vector-files\/lyrl2004_vectors_test_pt0.dat.gz","28","# http:\/\/jmlr.csail.mit.edu\/papers\/volume5\/lewis04a\/a13-vector-files\/lyrl2004_vectors_test_pt1.dat.gz","29","# http:\/\/jmlr.csail.mit.edu\/papers\/volume5\/lewis04a\/a13-vector-files\/lyrl2004_vectors_test_pt2.dat.gz","30","# http:\/\/jmlr.csail.mit.edu\/papers\/volume5\/lewis04a\/a13-vector-files\/lyrl2004_vectors_test_pt3.dat.gz","31","# http:\/\/jmlr.csail.mit.edu\/papers\/volume5\/lewis04a\/a13-vector-files\/lyrl2004_vectors_train.dat.gz"]}]}},"8616a0863c4e27dc70164345e0de12a5fc649861":{"changes":{"sklearn\/ensemble\/_gradient_boosting.pyx":"MODIFY","doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/ensemble\/gradient_boosting.py":"MODIFY","sklearn\/ensemble\/tests\/test_gradient_boosting.py":"MODIFY"},"diff":{"sklearn\/ensemble\/_gradient_boosting.pyx":[{"add":["205","        if X.format != 'csr':","206","            raise ValueError(\"When X is a sparse matrix, a CSR format is\"","207","                             \" expected, got {!r}\".format(type(X)))","210","        if not isinstance(X, np.ndarray) or np.isfortran(X):","211","            raise ValueError(\"X should be C-ordered np.ndarray,\"","212","                             \" got {}\".format(type(X)))"],"delete":["207","        if not isinstance(X, np.ndarray):","208","            raise ValueError(\"X should be in np.ndarray or csr_matrix format,\"","209","                             \"got %s\" % type(X))"]}],"doc\/whats_new\/v0.20.rst":[{"add":["150","- Fixed a bug when fitting :class:`ensemble.GradientBoostingClassifier` or","151","  :class:`ensemble.GradientBoostingRegressor` with ``warm_start=True`` which","152","  previously raised a segmentation fault due to a non-conversion of CSC matrix","153","  into CSR format expected by ``decision_function``. Similarly, Fortran-ordered","154","  arrays are converted to C-ordered arrays in the dense case. :issue:`9991` by","155","  :user:`Guillaume Lemaitre <glemaitre>`.","156",""],"delete":[]}],"sklearn\/ensemble\/gradient_boosting.py":[{"add":["1041","            # The requirements of _decision_function (called in two lines","1042","            # below) are more constrained than fit. It accepts only CSR","1043","            # matrices.","1044","            X = check_array(X, dtype=DTYPE, order=\"C\", accept_sparse='csr')"],"delete":[]}],"sklearn\/ensemble\/tests\/test_gradient_boosting.py":[{"add":["18","from sklearn.ensemble._gradient_boosting import predict_stages","394","def test_check_inputs_predict_stages():","395","    # check that predict_stages through an error if the type of X is not","396","    # supported","397","    x, y = datasets.make_hastie_10_2(n_samples=100, random_state=1)","398","    x_sparse_csc = csc_matrix(x)","399","    clf = GradientBoostingClassifier(n_estimators=100, random_state=1)","400","    clf.fit(x, y)","401","    score = np.zeros((y.shape)).reshape(-1, 1)","402","    assert_raise_message(ValueError,","403","                         \"When X is a sparse matrix, a CSR format is expected\",","404","                         predict_stages, clf.estimators_, x_sparse_csc,","405","                         clf.learning_rate, score)","406","    x_fortran = np.asfortranarray(x)","407","    assert_raise_message(ValueError,","408","                         \"X should be C-ordered np.ndarray\",","409","                         predict_stages, clf.estimators_, x_fortran,","410","                         clf.learning_rate, score)","411","","412","","883","def test_warm_start_sparse():","884","    # Test that all sparse matrix types are supported","885","    X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1)","886","    sparse_matrix_type = [csr_matrix, csc_matrix, coo_matrix]","887","    for Cls in [GradientBoostingRegressor, GradientBoostingClassifier]:","888","        est_dense = Cls(n_estimators=100, max_depth=1, subsample=0.5,","889","                        random_state=1, warm_start=True)","890","        est_dense.fit(X, y)","891","        est_dense.predict(X)","892","        est_dense.set_params(n_estimators=200)","893","        est_dense.fit(X, y)","894","        y_pred_dense = est_dense.predict(X)","895","","896","        for sparse_constructor in sparse_matrix_type:","897","            X_sparse = sparse_constructor(X)","898","","899","            est_sparse = Cls(n_estimators=100, max_depth=1, subsample=0.5,","900","                             random_state=1, warm_start=True)","901","            est_sparse.fit(X_sparse, y)","902","            est_sparse.predict(X)","903","            est_sparse.set_params(n_estimators=200)","904","            est_sparse.fit(X_sparse, y)","905","            y_pred_sparse = est_sparse.predict(X)","906","","907","            assert_array_almost_equal(est_dense.oob_improvement_[:100],","908","                                      est_sparse.oob_improvement_[:100])","909","            assert_array_almost_equal(y_pred_dense, y_pred_sparse)","910","","911","","912","def test_warm_start_fortran():","913","    # Test that feeding a X in Fortran-ordered is giving the same results as","914","    # in C-ordered","915","    X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1)","916","    for Cls in [GradientBoostingRegressor, GradientBoostingClassifier]:","917","        est_c = Cls(n_estimators=1, random_state=1, warm_start=True)","918","        est_fortran = Cls(n_estimators=1, random_state=1, warm_start=True)","919","","920","        est_c.fit(X, y)","921","        est_c.set_params(n_estimators=11)","922","        est_c.fit(X, y)","923","","924","        X_fortran = np.asfortranarray(X)","925","        est_fortran.fit(X_fortran, y)","926","        est_fortran.set_params(n_estimators=11)","927","        est_fortran.fit(X_fortran, y)","928","","929","        assert_array_almost_equal(est_c.predict(X),","930","                                  est_fortran.predict(X))","931","","932",""],"delete":[]}]}},"3d1b786546a033acbafe2ed32d7a008b8e659e5f":{"changes":{"sklearn\/calibration.py":"MODIFY","sklearn\/feature_selection\/rfe.py":"MODIFY","sklearn\/linear_model\/logistic.py":"MODIFY","sklearn\/linear_model\/coordinate_descent.py":"MODIFY","sklearn\/linear_model\/omp.py":"MODIFY","sklearn\/linear_model\/least_angle.py":"MODIFY","sklearn\/linear_model\/ridge.py":"MODIFY"},"diff":{"sklearn\/calibration.py":[{"add":["267","    See also","268","    --------","269","    CalibratedClassifierCV","270",""],"delete":[]}],"sklearn\/feature_selection\/rfe.py":[{"add":["103","    See also","104","    --------","105","    RFECV : Recursive feature elimination with built-in cross-validated","106","        selection of the best number of features","107","","372","    See also","373","    --------","374","    RFE : Recursive feature elimination","375",""],"delete":[]}],"sklearn\/linear_model\/logistic.py":[{"add":["1122","    LogisticRegressionCV : Logistic regression with built-in cross validation"],"delete":[]}],"sklearn\/linear_model\/coordinate_descent.py":[{"add":["642","    ElasticNetCV : Elastic net model with best model selection by","643","        cross-validation.","1692","    MultiTaskElasticNet : Multi-task L1\/L2 ElasticNet with built-in","1693","        cross-validation.","1694","    ElasticNet","1695","    MultiTaskLasso","1880","    MultiTaskLasso : Multi-task L1\/L2 Lasso with built-in cross-validation","1881","    Lasso","1882","    MultiTaskElasticNet"],"delete":["1690","    ElasticNet, MultiTaskLasso","1875","    Lasso, MultiTaskElasticNet"]}],"sklearn\/linear_model\/omp.py":[{"add":["600","    OrthogonalMatchingPursuitCV"],"delete":["600",""]}],"sklearn\/linear_model\/least_angle.py":[{"add":["826","    LassoLarsIC"],"delete":[]}],"sklearn\/linear_model\/ridge.py":[{"add":["626","    RidgeClassifier : Ridge classifier","627","    RidgeCV : Ridge regression with built-in cross validation","628","    :class:`sklearn.kernel_ridge.KernelRidge` : Kernel ridge regression","629","        combines ridge regression with the kernel trick","775","    Ridge : Ridge regression","776","    RidgeClassifierCV :  Ridge classifier with built-in cross validation","1239","    Ridge : Ridge regression","1240","    RidgeClassifier : Ridge classifier","1241","    RidgeClassifierCV : Ridge classifier with built-in cross validation","1324","    Ridge : Ridge regression","1325","    RidgeClassifier : Ridge classifier","1326","    RidgeCV : Ridge regression with built-in cross validation"],"delete":["626","    RidgeClassifier, RidgeCV, :class:`sklearn.kernel_ridge.KernelRidge`","772","    Ridge, RidgeClassifierCV","1235","    Ridge: Ridge regression","1236","    RidgeClassifier: Ridge classifier","1237","    RidgeClassifierCV: Ridge classifier with built-in cross validation","1320","    Ridge: Ridge regression","1321","    RidgeClassifier: Ridge classifier","1322","    RidgeCV: Ridge regression with built-in cross validation"]}]}},"41d648ef8863e6cadcb6c0a13f66dda7d0e9ea89":{"changes":{"sklearn\/metrics\/_ranking.py":"MODIFY","sklearn\/metrics\/tests\/test_common.py":"MODIFY","doc\/modules\/classes.rst":"MODIFY","doc\/modules\/model_evaluation.rst":"MODIFY","sklearn\/metrics\/__init__.py":"MODIFY","examples\/model_selection\/plot_det.py":"ADD","doc\/whats_new\/v0.24.rst":"MODIFY","sklearn\/metrics\/tests\/test_ranking.py":"MODIFY"},"diff":{"sklearn\/metrics\/_ranking.py":[{"add":["220","def detection_error_tradeoff_curve(y_true, y_score, pos_label=None,","221","                                   sample_weight=None):","222","    \"\"\"Compute error rates for different probability thresholds.","223","","224","    Note: This metrics is used for ranking evaluation of a binary","225","    classification task.","226","","227","    Read more in the :ref:`User Guide <det_curve>`.","228","","229","    Parameters","230","    ----------","231","    y_true : array, shape = [n_samples]","232","        True targets of binary classification in range {-1, 1} or {0, 1}.","233","","234","    y_score : array, shape = [n_samples]","235","        Estimated probabilities or decision function.","236","","237","    pos_label : int, optional (default=None)","238","        The label of the positive class","239","","240","    sample_weight : array-like of shape = [n_samples], optional","241","        Sample weights.","242","","243","    Returns","244","    -------","245","    fpr : array, shape = [n_thresholds]","246","        False positive rate (FPR) such that element i is the false positive","247","        rate of predictions with score >= thresholds[i]. This is occasionally","248","        referred to as false acceptance propability or fall-out.","249","","250","    fnr : array, shape = [n_thresholds]","251","        False negative rate (FNR) such that element i is the false negative","252","        rate of predictions with score >= thresholds[i]. This is occasionally","253","        referred to as false rejection or miss rate.","254","","255","    thresholds : array, shape = [n_thresholds]","256","        Decreasing score values.","257","","258","    See also","259","    --------","260","    roc_curve : Compute Receiver operating characteristic (ROC) curve","261","    precision_recall_curve : Compute precision-recall curve","262","","263","    Examples","264","    --------","265","    >>> import numpy as np","266","    >>> from sklearn.metrics import detection_error_tradeoff_curve","267","    >>> y_true = np.array([0, 0, 1, 1])","268","    >>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])","269","    >>> fpr, fnr, thresholds = detection_error_tradeoff_curve(y_true, y_scores)","270","    >>> fpr","271","    array([0.5, 0.5, 0. ])","272","    >>> fnr","273","    array([0. , 0.5, 0.5])","274","    >>> thresholds","275","    array([0.35, 0.4 , 0.8 ])","276","","277","    \"\"\"","278","    if len(np.unique(y_true)) != 2:","279","        raise ValueError(\"Only one class present in y_true. Detection error \"","280","                         \"tradeoff curve is not defined in that case.\")","281","","282","    fps, tps, thresholds = _binary_clf_curve(y_true, y_score,","283","                                             pos_label=pos_label,","284","                                             sample_weight=sample_weight)","285","","286","    fns = tps[-1] - tps","287","    p_count = tps[-1]","288","    n_count = fps[-1]","289","","290","    # start with false positives zero","291","    first_ind = (","292","        fps.searchsorted(fps[0], side='right') - 1","293","        if fps.searchsorted(fps[0], side='right') > 0","294","        else None","295","    )","296","    # stop with false negatives zero","297","    last_ind = tps.searchsorted(tps[-1]) + 1","298","    sl = slice(first_ind, last_ind)","299","","300","    # reverse the output such that list of false positives is decreasing","301","    return (","302","        fps[sl][::-1] \/ n_count,","303","        fns[sl][::-1] \/ p_count,","304","        thresholds[sl][::-1]","305","    )","306","","307",""],"delete":[]}],"sklearn\/metrics\/tests\/test_common.py":[{"add":["31","from sklearn.metrics import detection_error_tradeoff_curve","208","    \"detection_error_tradeoff_curve\": detection_error_tradeoff_curve,","305","    \"detection_error_tradeoff_curve\",","327","    \"detection_error_tradeoff_curve\",","358","    \"detection_error_tradeoff_curve\",","471","    \"detection_error_tradeoff_curve\","],"delete":[]}],"doc\/modules\/classes.rst":[{"add":["948","   metrics.detection_error_tradeoff_curve"],"delete":[]}],"doc\/modules\/model_evaluation.rst":[{"add":["308","   detection_error_tradeoff_curve","1440",".. _det_curve:","1441","","1442","Detection error tradeoff (DET)","1443","------------------------------","1444","","1445","The function :func:`detection_error_tradeoff_curve` computes the","1446","detection error tradeoff curve (DET) curve [WikipediaDET2017]_.","1447","Quoting Wikipedia:","1448","","1449","  \"A detection error tradeoff (DET) graph is a graphical plot of error rates for","1450","  binary classification systems, plotting false reject rate vs. false accept","1451","  rate. The x- and y-axes are scaled non-linearly by their standard normal","1452","  deviates (or just by logarithmic transformation), yielding tradeoff curves","1453","  that are more linear than ROC curves, and use most of the image area to","1454","  highlight the differences of importance in the critical operating region.\"","1455","","1456","DET curves are a variation of receiver operating characteristic (ROC) curves","1457","where False Negative Rate is plotted on the ordinate instead of True Positive","1458","Rate.","1459","DET curves are commonly plotted in normal deviate scale by transformation with","1460",":math:`\\phi^{-1}` (with :math:`\\phi` being the cumulative distribution","1461","function).","1462","The resulting performance curves explicitly visualize the tradeoff of error","1463","types for given classification algorithms.","1464","See [Martin1997]_ for examples and further motivation.","1465","","1466","This figure compares the ROC and DET curves of two example classifiers on the","1467","same classification task:","1468","","1469",".. image:: ..\/auto_examples\/model_selection\/images\/sphx_glr_plot_det_001.png","1470","   :target: ..\/auto_examples\/model_selection\/plot_det.html","1471","   :scale: 75","1472","   :align: center","1473","","1474","**Properties:**","1475","","1476","* DET curves form a linear curve in normal deviate scale if the detection","1477","  scores are normally (or close-to normally) distributed.","1478","  It was shown by [Navratil2007]_ that the reverse it not necessarily true and even more","1479","  general distributions are able produce linear DET curves.","1480","","1481","* The normal deviate scale transformation spreads out the points such that a","1482","  comparatively larger space of plot is occupied.","1483","  Therefore curves with similar classification performance might be easier to","1484","  distinguish on a DET plot.","1485","","1486","* With False Negative Rate being \"inverse\" to True Positive Rate the point","1487","  of perfection for DET curves is the origin (in contrast to the top left corner","1488","  for ROC curves).","1489","","1490","**Applications and limitations:**","1491","","1492","DET curves are intuitive to read and hence allow quick visual assessment of a","1493","classifier's performance.","1494","Additionally DET curves can be consulted for threshold analysis and operating","1495","point selection.","1496","This is particularly helpful if a comparison of error types is required.","1497","","1498","One the other hand DET curves do not provide their metric as a single number.","1499","Therefore for either automated evaluation or comparison to other","1500","classification tasks metrics like the derived area under ROC curve might be","1501","better suited.","1502","","1503",".. topic:: Examples:","1504","","1505","  * See :ref:`sphx_glr_auto_examples_model_selection_plot_det.py`","1506","    for an example comparison between receiver operating characteristic (ROC)","1507","    curves and Detection error tradeoff (DET) curves.","1508","","1509",".. topic:: References:","1510","","1511","  .. [WikipediaDET2017] Wikipedia contributors. Detection error tradeoff.","1512","     Wikipedia, The Free Encyclopedia. September 4, 2017, 23:33 UTC.","1513","     Available at: https:\/\/en.wikipedia.org\/w\/index.php?title=Detection_error_tradeoff&oldid=798982054.","1514","     Accessed February 19, 2018.","1515","","1516","  .. [Martin1997] A. Martin, G. Doddington, T. Kamm, M. Ordowski, and M. Przybocki,","1517","     `The DET Curve in Assessment of Detection Task Performance","1518","     <http:\/\/www.dtic.mil\/docs\/citations\/ADA530509>`_,","1519","     NIST 1997.","1520","","1521","  .. [Navratil2007] J. Navractil and D. Klusacek,","1522","     \"`On Linear DETs,","1523","     <http:\/\/www.research.ibm.com\/CBG\/papers\/icassp07_navratil.pdf>`_\"","1524","     2007 IEEE International Conference on Acoustics,","1525","     Speech and Signal Processing - ICASSP '07, Honolulu,","1526","     HI, 2007, pp. IV-229-IV-232."],"delete":[]}],"sklearn\/metrics\/__init__.py":[{"add":["9","from ._ranking import detection_error_tradeoff_curve","107","    'detection_error_tradeoff_curve',"],"delete":[]}],"examples\/model_selection\/plot_det.py":[{"add":[],"delete":[]}],"doc\/whats_new\/v0.24.rst":[{"add":["272","- |Feature| Added :func:`metrics.detection_error_tradeoff_curve` to compute","273","  Detection Error Tradeoff curve classification metric.","274","  :pr:`10591` by :user:`Jeremy Karnowski <jkarnows>` and","275","  :user:`Daniel Mohns <dmohns>`.","276",""],"delete":[]}],"sklearn\/metrics\/tests\/test_ranking.py":[{"add":["18","from sklearn.utils._testing import assert_raises","24","from sklearn.metrics import detection_error_tradeoff_curve","929","@pytest.mark.parametrize(\"y_true,y_score,expected_fpr,expected_fnr\", [","930","    ([0, 0, 1], [0, 0.5, 1], [0], [0]),","931","    ([0, 0, 1], [0, 0.25, 0.5], [0], [0]),","932","    ([0, 0, 1], [0.5, 0.75, 1], [0], [0]),","933","    ([0, 0, 1], [0.25, 0.5, 0.75], [0], [0]),","934","    ([0, 1, 0], [0, 0.5, 1], [0.5], [0]),","935","    ([0, 1, 0], [0, 0.25, 0.5], [0.5], [0]),","936","    ([0, 1, 0], [0.5, 0.75, 1], [0.5], [0]),","937","    ([0, 1, 0], [0.25, 0.5, 0.75], [0.5], [0]),","938","    ([0, 1, 1], [0, 0.5, 1], [0.0], [0]),","939","    ([0, 1, 1], [0, 0.25, 0.5], [0], [0]),","940","    ([0, 1, 1], [0.5, 0.75, 1], [0], [0]),","941","    ([0, 1, 1], [0.25, 0.5, 0.75], [0], [0]),","942","    ([1, 0, 0], [0, 0.5, 1], [1, 1, 0.5], [0, 1, 1]),","943","    ([1, 0, 0], [0, 0.25, 0.5], [1, 1, 0.5], [0, 1, 1]),","944","    ([1, 0, 0], [0.5, 0.75, 1], [1, 1, 0.5], [0, 1, 1]),","945","    ([1, 0, 0], [0.25, 0.5, 0.75], [1, 1, 0.5], [0, 1, 1]),","946","    ([1, 0, 1], [0, 0.5, 1], [1, 1, 0], [0, 0.5, 0.5]),","947","    ([1, 0, 1], [0, 0.25, 0.5], [1, 1, 0], [0, 0.5, 0.5]),","948","    ([1, 0, 1], [0.5, 0.75, 1], [1, 1, 0], [0, 0.5, 0.5]),","949","    ([1, 0, 1], [0.25, 0.5, 0.75], [1, 1, 0], [0, 0.5, 0.5]),","950","])","951","def test_detection_error_tradeoff_curve_toydata(y_true, y_score,","952","                                                expected_fpr, expected_fnr):","953","    # Check on a batch of small examples.","954","    fpr, fnr, _ = detection_error_tradeoff_curve(y_true, y_score)","955","","956","    assert_array_almost_equal(fpr, expected_fpr)","957","    assert_array_almost_equal(fnr, expected_fnr)","958","","959","","960","@pytest.mark.parametrize(\"y_true,y_score,expected_fpr,expected_fnr\", [","961","    ([1, 0], [0.5, 0.5], [1], [0]),","962","    ([0, 1], [0.5, 0.5], [1], [0]),","963","    ([0, 0, 1], [0.25, 0.5, 0.5], [0.5], [0]),","964","    ([0, 1, 0], [0.25, 0.5, 0.5], [0.5], [0]),","965","    ([0, 1, 1], [0.25, 0.5, 0.5], [0], [0]),","966","    ([1, 0, 0], [0.25, 0.5, 0.5], [1], [0]),","967","    ([1, 0, 1], [0.25, 0.5, 0.5], [1], [0]),","968","    ([1, 1, 0], [0.25, 0.5, 0.5], [1], [0]),","969","])","970","def test_detection_error_tradeoff_curve_tie_handling(y_true, y_score,","971","                                                     expected_fpr,","972","                                                     expected_fnr):","973","    fpr, fnr, _ = detection_error_tradeoff_curve(y_true, y_score)","974","","975","    assert_array_almost_equal(fpr, expected_fpr)","976","    assert_array_almost_equal(fnr, expected_fnr)","977","","978","","979","def test_detection_error_tradeoff_curve_sanity_check():","980","    # Exactly duplicated inputs yield the same result.","981","    assert_array_almost_equal(","982","        detection_error_tradeoff_curve([0, 0, 1], [0, 0.5, 1]),","983","        detection_error_tradeoff_curve(","984","            [0, 0, 0, 0, 1, 1], [0, 0, 0.5, 0.5, 1, 1])","985","    )","986","","987","","988","@pytest.mark.parametrize(\"y_score\", [","989","    (0), (0.25), (0.5), (0.75), (1)","990","])","991","def test_detection_error_tradeoff_curve_constant_scores(y_score):","992","    fpr, fnr, threshold = detection_error_tradeoff_curve(","993","        y_true=[0, 1, 0, 1, 0, 1],","994","        y_score=np.full(6, y_score)","995","    )","996","","997","    assert_array_almost_equal(fpr, [1])","998","    assert_array_almost_equal(fnr, [0])","999","    assert_array_almost_equal(threshold, [y_score])","1000","","1001","","1002","@pytest.mark.parametrize(\"y_true\", [","1003","    ([0, 0, 0, 0, 0, 1]),","1004","    ([0, 0, 0, 0, 1, 1]),","1005","    ([0, 0, 0, 1, 1, 1]),","1006","    ([0, 0, 1, 1, 1, 1]),","1007","    ([0, 1, 1, 1, 1, 1]),","1008","])","1009","def test_detection_error_tradeoff_curve_perfect_scores(y_true):","1010","    fpr, fnr, _ = detection_error_tradeoff_curve(","1011","        y_true=y_true,","1012","        y_score=y_true","1013","    )","1014","","1015","    assert_array_almost_equal(fpr, [0])","1016","    assert_array_almost_equal(fnr, [0])","1017","","1018","","1019","def test_detection_error_tradeoff_curve_bad_input():","1020","    # input variables with inconsistent numbers of samples","1021","    assert_raises(ValueError, detection_error_tradeoff_curve,","1022","                  [0, 1], [0, 0.5, 1])","1023","    assert_raises(ValueError, detection_error_tradeoff_curve,","1024","                  [0, 1, 1], [0, 0.5])","1025","","1026","    # When the y_true values are all the same a detection error tradeoff cannot","1027","    # be computed.","1028","    assert_raises(ValueError, detection_error_tradeoff_curve,","1029","                  [0, 0, 0], [0, 0.5, 1])","1030","    assert_raises(ValueError, detection_error_tradeoff_curve,","1031","                  [1, 1, 1], [0, 0.5, 1])","1032","","1033",""],"delete":[]}]}},"f485a9edf92e009f3b898e35dfbf5daea433d74f":{"changes":{"sklearn\/linear_model\/stochastic_gradient.py":"MODIFY","sklearn\/linear_model\/tests\/test_sgd.py":"MODIFY"},"diff":{"sklearn\/linear_model\/stochastic_gradient.py":[{"add":["84","    def _validate_params(self, set_max_iter=True, for_partial_fit=False):","122","            if not for_partial_fit:","123","                warnings.warn(","124","                    \"max_iter and tol parameters have been \"","125","                    \"added in %s in 0.19. If both are left unset, \"","126","                    \"they default to max_iter=5 and tol=None. \"","127","                    \"If tol is not None, max_iter defaults to max_iter=1000. \"","128","                    \"From 0.21, default max_iter will be 1000, and\"","129","                    \" default tol will be 1e-3.\" % type(self), FutureWarning)","130","                # Before 0.19, default was n_iter=5","542","        self._validate_params(for_partial_fit=True)","987","        self._validate_params(for_partial_fit=True)"],"delete":["84","    def _validate_params(self, set_max_iter=True):","122","            warnings.warn(","123","                \"max_iter and tol parameters have been added in %s in 0.19. If\"","124","                \" both are left unset, they default to max_iter=5 and tol=None\"","125","                \". If tol is not None, max_iter defaults to max_iter=1000. \"","126","                \"From 0.21, default max_iter will be 1000, \"","127","                \"and default tol will be 1e-3.\" % type(self).__name__,","128","                FutureWarning)","129","            # Before 0.19, default was n_iter=5","541","        self._validate_params()","986","        self._validate_params()"]}],"sklearn\/linear_model\/tests\/test_sgd.py":[{"add":["1196","    def init(max_iter=None, tol=None, n_iter=None, for_partial_fit=False):","1198","        sgd._validate_params(for_partial_fit=for_partial_fit)","1213","    # Test that for_partial_fit will not throw warnings for max_iter or tol","1214","    assert_no_warnings(init, None, None, None, True)","1215",""],"delete":["1196","    def init(max_iter=None, tol=None, n_iter=None):","1198","        sgd._validate_params()"]}]}},"f793f55310055b056be0cfcbac9169860065941c":{"changes":{"examples\/cluster\/plot_cluster_comparison.py":"MODIFY"},"diff":{"examples\/cluster\/plot_cluster_comparison.py":[{"add":["171","        # add black color for outliers (if any)","172","        colors = np.append(colors, [\"#000000\"])"],"delete":[]}]}},"92c9095591c5885838987686a0dc7f8cff8bcf5e":{"changes":{"sklearn\/model_selection\/_validation.py":"MODIFY","sklearn\/exceptions.py":"MODIFY","sklearn\/model_selection\/tests\/test_validation.py":"MODIFY"},"diff":{"sklearn\/model_selection\/_validation.py":[{"add":["17","from traceback import format_exception_only","477","            warnings.warn(\"Estimator fit failed. The score on this train-test\"","479","                          \"Details: \\n%s\" %","480","                          (error_score, format_exception_only(type(e), e)[0]),","481","                          FitFailedWarning)"],"delete":["476","            warnings.warn(\"Classifier fit failed. The score on this train-test\"","478","                          \"Details: \\n%r\" % (error_score, e), FitFailedWarning)"]}],"sklearn\/exceptions.py":[{"add":["121","    FitFailedWarning('Estimator fit failed. The score on this train-test","122","    partition for these parameters will be set to 0.000000.","123","    Details: \\\\nValueError: Penalty term must be positive; got (C=-2)\\\\n',)"],"delete":["121","    FitFailedWarning(\"Classifier fit failed. The score on this train-test","122","    partition for these parameters will be set to 0.000000. Details:","123","    \\\\nValueError('Penalty term must be positive; got (C=-2)',)\",)"]}],"sklearn\/model_selection\/tests\/test_validation.py":[{"add":["11","from sklearn.exceptions import FitFailedWarning","12","","13","from sklearn.tests.test_grid_search import FailingClassifier","45","from sklearn.model_selection._validation import _fit_and_score","1427","","1428","","1429","def test_fit_and_score():","1430","    # Create a failing classifier to deliberately fail","1431","    failing_clf = FailingClassifier(FailingClassifier.FAILING_PARAMETER)","1432","    # dummy X data","1433","    X = np.arange(1, 10)","1434","    fit_and_score_args = [failing_clf, X, None, dict(), None, None, 0,","1435","                          None, None]","1436","    # passing error score to trigger the warning message","1437","    fit_and_score_kwargs = {'error_score': 0}","1438","    # check if the warning message type is as expected","1439","    assert_warns(FitFailedWarning, _fit_and_score, *fit_and_score_args,","1440","                 **fit_and_score_kwargs)","1441","    # since we're using FailingClassfier, our error will be the following","1442","    error_message = \"ValueError: Failing classifier failed as required\"","1443","    # the warning message we're expecting to see","1444","    warning_message = (\"Estimator fit failed. The score on this train-test \"","1445","                       \"partition for these parameters will be set to %f. \"","1446","                       \"Details: \\n%s\" % (fit_and_score_kwargs['error_score'],","1447","                                          error_message))","1448","    # check if the same warning is triggered","1449","    assert_warns_message(FitFailedWarning, warning_message, _fit_and_score,","1450","                         *fit_and_score_args, **fit_and_score_kwargs)"],"delete":[]}]}},"002f95cd912783d2e86f36a5bb79aca79b6ab917":{"changes":{"sklearn\/model_selection\/_search.py":"MODIFY"},"diff":{"sklearn\/model_selection\/_search.py":[{"add":["1129","    Note that before SciPy 0.16, the ``scipy.stats.distributions`` do not","1130","    accept a custom RNG instance and always use the singleton RNG from","1131","    ``numpy.random``. Hence setting ``random_state`` will not guarantee a","1132","    deterministic iteration whenever ``scipy.stats`` distributions are used to","1133","    define the parameter search space.","1134",""],"delete":[]}]}},"331fb7bc68306e71fb18a0e08ed3b70b3fe6e4a9":{"changes":{"doc\/modules\/lda_qda.rst":"MODIFY"},"diff":{"doc\/modules\/lda_qda.rst":[{"add":["44","classes, so this is, in general, a rather strong dimensionality reduction, and"],"delete":["44","classes, so this is a in general a rather strong dimensionality reduction, and"]}]}},"1f7fa760d4a32f06f195d74ee865c630c1d64633":{"changes":{"sklearn\/utils\/estimator_checks.py":"MODIFY"},"diff":{"sklearn\/utils\/estimator_checks.py":[{"add":["1053","    # Add noise to X to test the possible values of the labels","1054","    rng = np.random.RandomState(7)","1055","    X_noise = np.concatenate([X, rng.uniform(low=-3, high=3, size=(5, 2))])","1056","    labels = clusterer.fit_predict(X_noise)","1057","","1061","    labels_sorted = np.unique(labels)","1062","    assert_array_equal(labels_sorted, np.arange(labels_sorted[0],","1063","                                                labels_sorted[-1] + 1))","1065","    # Labels are expected to start at 0 (no noise) or -1 (if noise)","1066","    assert_true(labels_sorted[0] in [0, -1])","1067","    # Labels should be less than n_clusters - 1","1070","        assert_greater_equal(n_clusters - 1, labels_sorted[-1])","1071","    # else labels should be less than max(labels_) which is necessarily true"],"delete":["1056","    pred_sorted = np.unique(pred)","1057","    assert_array_equal(pred_sorted, np.arange(pred_sorted[0],","1058","                                              pred_sorted[-1] + 1))","1060","    # labels_ should be greater than -1","1061","    assert_greater_equal(pred_sorted[0], -1)","1062","    # labels_ should be less than n_clusters - 1","1065","        assert_greater_equal(n_clusters - 1, pred_sorted[-1])","1066","    # else labels_ should be less than max(labels_) which is necessarily true"]}]}},"14e7c328df9968ca403e47168314ae28390c07d5":{"changes":{"sklearn\/externals\/copy_joblib.sh":"MODIFY","sklearn\/externals\/joblib\/format_stack.py":"ADD","examples\/compose\/plot_compare_reduction.py":"MODIFY","sklearn\/tests\/test_site_joblib.py":"ADD","sklearn\/covariance\/graph_lasso_.py":"MODIFY","sklearn\/externals\/joblib\/numpy_pickle.py":"ADD","sklearn\/utils\/_joblib.py":"ADD","sklearn\/externals\/joblib\/_memory_helpers.py":"ADD","sklearn\/externals\/joblib\/backports.py":"ADD","sklearn\/multioutput.py":"MODIFY","benchmarks\/bench_saga.py":"MODIFY","sklearn\/externals\/joblib\/disk.py":"ADD","sklearn\/externals\/joblib\/_multiprocessing_helpers.py":"ADD","sklearn\/externals\/joblib\/parallel.py":"ADD","sklearn\/externals\/joblib\/numpy_pickle_utils.py":"ADD","sklearn\/model_selection\/_search.py":"MODIFY","sklearn\/linear_model\/logistic.py":"MODIFY","sklearn\/linear_model\/omp.py":"MODIFY","sklearn\/externals\/joblib\/pool.py":"ADD","sklearn\/utils\/__init__.py":"MODIFY","sklearn\/cluster\/mean_shift_.py":"MODIFY","sklearn\/ensemble\/forest.py":"MODIFY","sklearn\/externals\/joblib\/memory.py":"ADD","sklearn\/tests\/test_pipeline.py":"MODIFY","sklearn\/externals\/joblib\/_compat.py":"ADD","sklearn\/externals\/joblib\/hashing.py":"ADD","examples\/applications\/wikipedia_principal_eigenvector.py":"MODIFY","sklearn\/externals\/joblib\/_parallel_backends.py":"ADD","sklearn\/metrics\/pairwise.py":"MODIFY","sklearn\/utils\/validation.py":"MODIFY","benchmarks\/bench_mnist.py":"MODIFY","sklearn\/neighbors\/base.py":"MODIFY","sklearn\/pipeline.py":"MODIFY","benchmarks\/bench_tsne_mnist.py":"MODIFY","sklearn\/datasets\/svmlight_format.py":"MODIFY","sklearn\/multiclass.py":"MODIFY","sklearn\/linear_model\/randomized_l1.py":"MODIFY","sklearn\/compose\/_column_transformer.py":"MODIFY","sklearn\/externals\/setup.py":"MODIFY","sklearn\/utils\/tests\/test_validation.py":"MODIFY","sklearn\/datasets\/lfw.py":"MODIFY","sklearn\/linear_model\/coordinate_descent.py":"MODIFY","sklearn\/decomposition\/online_lda.py":"MODIFY","sklearn\/externals\/joblib\/__init__.py":"ADD","sklearn\/manifold\/mds.py":"MODIFY","benchmarks\/bench_plot_nmf.py":"MODIFY","examples\/cluster\/plot_feature_agglomeration_vs_univariate_selection.py":"MODIFY","sklearn\/externals\/joblib\/numpy_pickle_compat.py":"ADD","doc\/developers\/utilities.rst":"MODIFY",".travis.yml":"MODIFY","benchmarks\/bench_covertype.py":"MODIFY","sklearn\/ensemble\/partial_dependence.py":"MODIFY","doc\/glossary.rst":"MODIFY","sklearn\/externals\/joblib\/logger.py":"ADD","sklearn\/externals\/joblib\/my_exceptions.py":"ADD","doc\/whats_new\/v0.20.rst":"MODIFY","doc\/modules\/classes.rst":"MODIFY","sklearn\/feature_selection\/rfe.py":"MODIFY","sklearn\/linear_model\/theil_sen.py":"MODIFY","build_tools\/travis\/install.sh":"MODIFY","benchmarks\/bench_rcv1_logreg_convergence.py":"MODIFY","sklearn\/utils\/estimator_checks.py":"MODIFY","sklearn\/decomposition\/tests\/test_sparse_pca.py":"MODIFY","sklearn\/tests\/test_multioutput.py":"MODIFY","sklearn\/linear_model\/tests\/test_randomized_l1.py":"MODIFY","sklearn\/linear_model\/base.py":"MODIFY","sklearn\/ensemble\/bagging.py":"MODIFY","\/dev\/null":"DELETE","sklearn\/linear_model\/stochastic_gradient.py":"MODIFY","sklearn\/model_selection\/_validation.py":"MODIFY","sklearn\/decomposition\/dict_learning.py":"MODIFY","sklearn\/ensemble\/voting_classifier.py":"MODIFY","sklearn\/externals\/joblib\/func_inspect.py":"ADD","sklearn\/cluster\/k_means_.py":"MODIFY","sklearn\/linear_model\/least_angle.py":"MODIFY"},"diff":{"sklearn\/externals\/copy_joblib.sh":[{"add":["14","cp -r $INSTALL_FOLDER\/joblib joblib","20","find joblib -name \"*.py\" | xargs sed -i.bak \"s\/from joblib\/from sklearn.externals.joblib\/\"","21","find joblib -name \"*.bak\" | xargs rm"],"delete":["14","cp -r $INSTALL_FOLDER\/joblib _joblib","20","find _joblib -name \"*.py\" | xargs sed -i.bak \"s\/from joblib\/from sklearn.externals.joblib\/\"","21","find _joblib -name \"*.bak\" | xargs rm"]}],"sklearn\/externals\/joblib\/format_stack.py":[{"add":[],"delete":[]}],"examples\/compose\/plot_compare_reduction.py":[{"add":["106","from sklearn.utils import Memory"],"delete":["106","from sklearn.externals.joblib import Memory"]}],"sklearn\/tests\/test_site_joblib.py":[{"add":[],"delete":[]}],"sklearn\/covariance\/graph_lasso_.py":[{"add":["25","from ..utils import Parallel, delayed"],"delete":["25","from ..externals.joblib import Parallel, delayed"]}],"sklearn\/externals\/joblib\/numpy_pickle.py":[{"add":[],"delete":[]}],"sklearn\/utils\/_joblib.py":[{"add":[],"delete":[]}],"sklearn\/externals\/joblib\/_memory_helpers.py":[{"add":[],"delete":[]}],"sklearn\/externals\/joblib\/backports.py":[{"add":[],"delete":[]}],"sklearn\/multioutput.py":[{"add":["27","from .utils import Parallel, delayed"],"delete":["27","from .externals.joblib import Parallel, delayed"]}],"benchmarks\/bench_saga.py":[{"add":["14","from sklearn.utils import delayed, Parallel, Memory"],"delete":["14","from sklearn.externals.joblib import delayed, Parallel, Memory"]}],"sklearn\/externals\/joblib\/disk.py":[{"add":[],"delete":[]}],"sklearn\/externals\/joblib\/_multiprocessing_helpers.py":[{"add":[],"delete":[]}],"sklearn\/externals\/joblib\/parallel.py":[{"add":[],"delete":[]}],"sklearn\/externals\/joblib\/numpy_pickle_utils.py":[{"add":[],"delete":[]}],"sklearn\/model_selection\/_search.py":[{"add":["31","from ..utils import Parallel, delayed"],"delete":["31","from ..externals.joblib import Parallel, delayed"]}],"sklearn\/linear_model\/logistic.py":[{"add":["34","from ..utils import Parallel, delayed"],"delete":["34","from ..externals.joblib import Parallel, delayed"]}],"sklearn\/linear_model\/omp.py":[{"add":["18","from ..utils import Parallel, delayed"],"delete":["18","from ..externals.joblib import Parallel, delayed"]}],"sklearn\/externals\/joblib\/pool.py":[{"add":[],"delete":[]}],"sklearn\/utils\/__init__.py":[{"add":["17","from ._joblib import cpu_count, Parallel, Memory, delayed","18","from ._joblib import parallel_backend","30","           \"check_symmetric\", \"indices_to_mask\", \"deprecated\",","31","           \"cpu_count\", \"Parallel\", \"Memory\", \"delayed\", \"parallel_backend\"]"],"delete":["17","from ..externals.joblib import cpu_count","29","           \"check_symmetric\", \"indices_to_mask\", \"deprecated\"]"]}],"sklearn\/cluster\/mean_shift_.py":[{"add":["26","from ..utils import Parallel","27","from ..utils import delayed"],"delete":["26","from ..externals.joblib import Parallel","27","from ..externals.joblib import delayed"]}],"sklearn\/ensemble\/forest.py":[{"add":["54","from ..utils import Parallel, delayed"],"delete":["54","from ..externals.joblib import Parallel, delayed"]}],"sklearn\/externals\/joblib\/memory.py":[{"add":[],"delete":[]}],"sklearn\/tests\/test_pipeline.py":[{"add":["35","from sklearn.utils import Memory","910","                        \"sklearn.utils.Memory.\"","933","                        \"sklearn.utils.Memory.\""],"delete":["35","from sklearn.externals.joblib import Memory","910","                        \"sklearn.externals.joblib.Memory.\"","933","                        \"sklearn.externals.joblib.Memory.\""]}],"sklearn\/externals\/joblib\/_compat.py":[{"add":[],"delete":[]}],"sklearn\/externals\/joblib\/hashing.py":[{"add":[],"delete":[]}],"examples\/applications\/wikipedia_principal_eigenvector.py":[{"add":["47","from sklearn.utils import Memory"],"delete":["47","from sklearn.externals.joblib import Memory"]}],"sklearn\/externals\/joblib\/_parallel_backends.py":[{"add":[],"delete":[]}],"sklearn\/metrics\/pairwise.py":[{"add":["26","from ..utils import Parallel","27","from ..utils import delayed","28","from ..utils import cpu_count"],"delete":["26","from ..externals.joblib import Parallel","27","from ..externals.joblib import delayed","28","from ..externals.joblib import cpu_count"]}],"sklearn\/utils\/validation.py":[{"add":["26","from ..utils._joblib import Memory","185","    sklearn.utils.Memory instance (typically a str denoting the","206","                         \" interface as sklearn.utils.Memory.\""],"delete":["26","from ..externals.joblib import Memory","185","    sklearn.externals.joblib.Memory instance (typically a str denoting the","206","                         \" interface as sklearn.externals.joblib.Memory.\""]}],"benchmarks\/bench_mnist.py":[{"add":["43","from sklearn.utils import Memory"],"delete":["43","from sklearn.externals.joblib import Memory"]}],"sklearn\/neighbors\/base.py":[{"add":["25","from ..utils import Parallel, delayed"],"delete":["25","from ..externals.joblib import Parallel, delayed"]}],"sklearn\/pipeline.py":[{"add":["17","from .utils import Parallel, delayed"],"delete":["17","from .externals.joblib import Parallel, delayed"]}],"benchmarks\/bench_tsne_mnist.py":[{"add":["17","from sklearn.utils import Memory"],"delete":["17","from sklearn.externals.joblib import Memory"]}],"sklearn\/datasets\/svmlight_format.py":[{"add":["134","        from sklearn.utils import Memory"],"delete":["134","        from sklearn.externals.joblib import Memory"]}],"sklearn\/multiclass.py":[{"add":["54","from .utils import Parallel","55","from .utils import delayed"],"delete":["54","from .externals.joblib import Parallel","55","from .externals.joblib import delayed"]}],"sklearn\/linear_model\/randomized_l1.py":[{"add":["21","from ..utils import Memory, Parallel, delayed","111","                             \" a sklearn.utils.Memory\""],"delete":["21","from ..externals.joblib import Memory, Parallel, delayed","111","                             \" a sklearn.externals.joblib.Memory\""]}],"sklearn\/compose\/_column_transformer.py":[{"add":["14","from ..utils import Parallel, delayed"],"delete":["14","from ..externals.joblib import Parallel, delayed"]}],"sklearn\/externals\/setup.py":[{"add":["6","    config.add_subpackage('joblib')"],"delete":["6","    config.add_subpackage('_joblib')"]}],"sklearn\/utils\/tests\/test_validation.py":[{"add":["744","                        \"sklearn.utils.Memory.\"","749","                        \"sklearn.utils.Memory. Got memory='{}' \""],"delete":["744","                        \"sklearn.externals.joblib.Memory.\"","749","                        \"sklearn.externals.joblib.Memory. Got memory='{}' \""]}],"sklearn\/datasets\/lfw.py":[{"add":["33","from ..utils import Memory"],"delete":["33","from ..externals.joblib import Memory"]}],"sklearn\/linear_model\/coordinate_descent.py":[{"add":["20","from ..utils import Parallel, delayed"],"delete":["20","from ..externals.joblib import Parallel, delayed"]}],"sklearn\/decomposition\/online_lda.py":[{"add":["23","from ..utils import Parallel, delayed"],"delete":["23","from ..externals.joblib import Parallel, delayed"]}],"sklearn\/externals\/joblib\/__init__.py":[{"add":[],"delete":[]}],"sklearn\/manifold\/mds.py":[{"add":["14","from ..utils import Parallel","15","from ..utils import delayed"],"delete":["14","from ..externals.joblib import Parallel","15","from ..externals.joblib import delayed"]}],"benchmarks\/bench_plot_nmf.py":[{"add":["24","from sklearn.utils import Memory"],"delete":["24","from sklearn.externals.joblib import Memory"]}],"examples\/cluster\/plot_feature_agglomeration_vs_univariate_selection.py":[{"add":["32","from sklearn.utils import Memory"],"delete":["32","from sklearn.externals.joblib import Memory"]}],"sklearn\/externals\/joblib\/numpy_pickle_compat.py":[{"add":[],"delete":[]}],"doc\/developers\/utilities.rst":[{"add":["47","  ``sklearn.utils.Memory`` instance (typically a str denoting"],"delete":["47","  ``sklearn.externals.joblib.Memory`` instance (typically a str denoting"]}],".travis.yml":[{"add":["40","    # This environment tests the newest supported Anaconda release.","41","    # It runs tests requiring pandas and PyAMG.","42","    # It also runs with the site joblib instead of the vendored copy of joblib.","46","           JOBLIB_VERSION=\"0.12.0\" COVERAGE=true","48","           SKLEARN_SITE_JOBLIB=1"],"delete":["40","    # This environment tests the newest supported Anaconda release (5.0.0)","41","    # It also runs tests requiring Pandas and PyAMG","45","           COVERAGE=true"]}],"benchmarks\/bench_covertype.py":[{"add":["61","from sklearn.utils import Memory"],"delete":["61","from sklearn.externals.joblib import Memory"]}],"sklearn\/ensemble\/partial_dependence.py":[{"add":["12","from ..utils import Parallel, delayed"],"delete":["12","from ..externals.joblib import Parallel, delayed"]}],"doc\/glossary.rst":[{"add":["1487","          backend by using :func:`sklearn.utils.parallel_backend`."],"delete":["1487","          backend by using :func:`sklearn.externals.joblib.parallel.parallel_backend`."]}],"sklearn\/externals\/joblib\/logger.py":[{"add":[],"delete":[]}],"sklearn\/externals\/joblib\/my_exceptions.py":[{"add":[],"delete":[]}],"doc\/whats_new\/v0.20.rst":[{"add":["220","  one was added (:ref:`environment_variable`). The main API of joblib is now","221","  exposed in :mod:`sklearn.utils`.","222","  :issue:`11166`by `Gael Varoquaux`_"],"delete":["85","**Other backward incompatible change** The vendored version of the joblib","86","module is now found at `sklearn.externals._joblib` (:issue:`11166`). The","87","main API of joblib is still exposed in `sklearn.externals.joblib`, but","88","code doing imports of subpackages of `sklearn.externals.joblib` will","89","break.","90","","226","  one was added (:ref:`environment_variable`).","227","  :issue:`11166` by `Gael Varoquaux`_"]}],"doc\/modules\/classes.rst":[{"add":["1477","Utilities from joblib:","1478","","1479",".. autosummary::","1480","   :toctree: generated\/","1481","   :template: class.rst","1482","","1483","   utils.Memory","1484","   utils.Parallel","1485","","1486",".. autosummary::","1487","   :toctree: generated\/","1488","   :template: function.rst","1489","","1490","   utils.cpu_count","1491","   utils.delayed","1492","   utils.parallel_backend"],"delete":[]}],"sklearn\/feature_selection\/rfe.py":[{"add":["17","from ..utils import Parallel, delayed"],"delete":["17","from ..externals.joblib import Parallel, delayed"]}],"sklearn\/linear_model\/theil_sen.py":[{"add":["23","from ..utils import Parallel, delayed"],"delete":["23","from ..externals.joblib import Parallel, delayed"]}],"build_tools\/travis\/install.sh":[{"add":["61","    if [[ -n \"$JOBLIB_VERSION\" ]]; then","62","        TO_INSTALL=\"$TO_INSTALL joblib=$JOBLIB_VERSION\"","63","    fi","64",""],"delete":[]}],"benchmarks\/bench_rcv1_logreg_convergence.py":[{"add":["10","from sklearn.utils import Memory"],"delete":["10","from sklearn.externals.joblib import Memory"]}],"sklearn\/utils\/estimator_checks.py":[{"add":["16","from sklearn.utils._joblib import hash, Memory"],"delete":["16","from sklearn.externals.joblib import hash, Memory"]}],"sklearn\/decomposition\/tests\/test_sparse_pca.py":[{"add":["153","        import sklearn.utils._joblib.parallel as joblib_par"],"delete":["153","        import sklearn.externals.joblib.parallel as joblib_par"]}],"sklearn\/tests\/test_multioutput.py":[{"add":["20","from sklearn.utils import cpu_count"],"delete":["20","from sklearn.externals.joblib import cpu_count"]}],"sklearn\/linear_model\/tests\/test_randomized_l1.py":[{"add":["58","                        \" a sklearn.utils.Memory instance\","],"delete":["58","                        \" a sklearn.externals.joblib.Memory instance\","]}],"sklearn\/linear_model\/base.py":[{"add":["26","from ..utils import Parallel, delayed"],"delete":["26","from ..externals.joblib import Parallel, delayed"]}],"sklearn\/ensemble\/bagging.py":[{"add":["15","from ..utils import Parallel, delayed"],"delete":["15","from ..externals.joblib import Parallel, delayed"]}],"\/dev\/null":[{"add":[],"delete":[]}],"sklearn\/linear_model\/stochastic_gradient.py":[{"add":["11","from ..utils import Parallel, delayed"],"delete":["11","from ..externals.joblib import Parallel, delayed"]}],"sklearn\/model_selection\/_validation.py":[{"add":["27","from ..utils import Parallel, delayed","28","from ..utils._joblib import logger"],"delete":["27","from ..externals.joblib import Parallel, delayed, logger"]}],"sklearn\/decomposition\/dict_learning.py":[{"add":["17","from ..utils import Parallel, delayed, cpu_count"],"delete":["17","from ..externals.joblib import Parallel, delayed, cpu_count"]}],"sklearn\/ensemble\/voting_classifier.py":[{"add":["20","from ..utils import Parallel, delayed"],"delete":["20","from ..externals.joblib import Parallel, delayed"]}],"sklearn\/externals\/joblib\/func_inspect.py":[{"add":[],"delete":[]}],"sklearn\/cluster\/k_means_.py":[{"add":["31","from ..utils import Parallel","32","from ..utils import delayed"],"delete":["31","from ..externals.joblib import Parallel","32","from ..externals.joblib import delayed"]}],"sklearn\/linear_model\/least_angle.py":[{"add":["25","from ..utils import Parallel, delayed"],"delete":["25","from ..externals.joblib import Parallel, delayed"]}]}},"cba99e01330c5df6d7da33c5c94a687d364986e4":{"changes":{"sklearn\/ensemble\/iforest.py":"MODIFY","sklearn\/ensemble\/tests\/test_iforest.py":"MODIFY","sklearn\/linear_model\/tests\/test_sag.py":"MODIFY"},"diff":{"sklearn\/ensemble\/iforest.py":[{"add":["72","        .. versionchanged:: 0.20","73","           The default value of ``contamination`` will change from 0.1 in 0.20","74","           to ``'auto'`` in 0.22.","75","","178","        if self.contamination == \"legacy\":","179","            warnings.warn('default contamination parameter 0.1 will change '","180","                          'in version 0.22 to \"auto\". This will change the '","181","                          'predict method behavior.',","182","                          FutureWarning)","183","            self._contamination = 0.1","184","        else:","185","            self._contamination = self.contamination","186","","228","        if self._contamination == \"auto\":","237","                self.score_samples(X), 100. * self._contamination)"],"delete":["152","","153","        if contamination == \"legacy\":","154","            warnings.warn('default contamination parameter 0.1 will change '","155","                          'in version 0.22 to \"auto\". This will change the '","156","                          'predict method behavior.',","157","                          DeprecationWarning)","221","        if self.contamination == \"auto\":","228","        elif self.contamination == \"legacy\":  # to be rm in 0.22","229","            self.offset_ = sp.stats.scoreatpercentile(","230","                self.score_samples(X), 100. * 0.1)","233","                self.score_samples(X), 100. * self.contamination)"]}],"sklearn\/ensemble\/tests\/test_iforest.py":[{"add":["64","@pytest.mark.filterwarnings('ignore:default contamination')","92","@pytest.mark.filterwarnings('ignore:default contamination')","131","@pytest.mark.filterwarnings('ignore:default contamination')","140","@pytest.mark.filterwarnings('ignore:default contamination')","156","@pytest.mark.filterwarnings('ignore:default contamination')","181","@pytest.mark.filterwarnings('ignore:default contamination')","221","@pytest.mark.filterwarnings('ignore:default contamination')","229","@pytest.mark.filterwarnings('ignore:default contamination')","254","@pytest.mark.filterwarnings('ignore:default contamination')","268","    X = [[0.0], [1.0]]","269","    clf = IsolationForest()","270","","271","    assert_warns_message(FutureWarning,","274","                         clf.fit, X)","275","","276","    clf = IsolationForest(contamination='auto').fit(X)"],"delete":["259","    assert_warns_message(DeprecationWarning,","262","                         IsolationForest, )","263","    X = [[0.0], [1.0]]","264","    clf = IsolationForest().fit(X)"]}],"sklearn\/linear_model\/tests\/test_sag.py":[{"add":["19","from sklearn.utils.testing import assert_allclose","297","    assert_allclose(weights1, clf.coef_)","298","    assert_allclose(intercept1, clf.intercept_)","299","    assert_allclose(weights2, clf.coef_)","300","    assert_allclose(intercept2, clf.intercept_)"],"delete":["271","@ignore_warnings","297","    assert_array_almost_equal(weights1, clf.coef_, decimal=10)","298","    assert_array_almost_equal(intercept1, clf.intercept_, decimal=10)","299","    assert_array_almost_equal(weights2, clf.coef_, decimal=10)","300","    assert_array_almost_equal(intercept2, clf.intercept_, decimal=10)"]}]}},"26421be2dfe9d9e633a69abda4caed3ff2b669d1":{"changes":{"Makefile":"MODIFY"},"diff":{"Makefile":[{"add":["36","\t$(PYTEST) sklearn --showlocals -v --cov=sklearn --cov-report=html:coverage"],"delete":["36","\t$(PYTEST) sklearn --show-locals -v --with-cov sklearn"]}]}},"3e26fc63be99942ca32efd181f1ce04efeae418a":{"changes":{"doc\/tutorial\/machine_learning_map\/pyparsing.py":"MODIFY","sklearn\/tests\/test_multiclass.py":"MODIFY","doc\/whats_new\/v0.19.rst":"MODIFY","sklearn\/decomposition\/online_lda.py":"MODIFY","sklearn\/tree\/_splitter.pyx":"MODIFY","sklearn\/utils\/tests\/test_random.py":"MODIFY","sklearn\/neighbors\/graph.py":"MODIFY","sklearn\/metrics\/ranking.py":"MODIFY","doc\/glossary.rst":"MODIFY","sklearn\/externals\/joblib\/numpy_pickle_utils.py":"MODIFY","sklearn\/model_selection\/_search.py":"MODIFY","sklearn\/neighbors\/quad_tree.pyx":"MODIFY","sklearn\/ensemble\/tests\/test_forest.py":"MODIFY","benchmarks\/bench_plot_randomized_svd.py":"MODIFY","examples\/gaussian_process\/plot_gpr_noisy_targets.py":"MODIFY","sklearn\/externals\/joblib\/pool.py":"MODIFY","sklearn\/preprocessing\/data.py":"MODIFY","sklearn\/model_selection\/_split.py":"MODIFY","doc\/developers\/maintainer.rst":"MODIFY","sklearn\/ensemble\/iforest.py":"MODIFY","sklearn\/utils\/validation.py":"MODIFY","examples\/preprocessing\/plot_scaling_importance.py":"MODIFY","sklearn\/neighbors\/quad_tree.pxd":"MODIFY","examples\/gaussian_process\/plot_gpc_isoprobability.py":"MODIFY","examples\/svm\/plot_rbf_parameters.py":"MODIFY","doc\/tutorial\/machine_learning_map\/ML_MAPS_README.txt":"MODIFY","sklearn\/multiclass.py":"MODIFY","sklearn\/cluster\/_k_means_elkan.pyx":"MODIFY","doc\/themes\/scikit-learn\/static\/nature.css_t":"MODIFY","sklearn\/datasets\/tests\/test_base.py":"MODIFY","sklearn\/linear_model\/sgd_fast.pyx":"MODIFY"},"diff":{"doc\/tutorial\/machine_learning_map\/pyparsing.py":[{"add":["1606","         - explicitly expand the tabs in your input string before calling\r"],"delete":["1606","         - explictly expand the tabs in your input string before calling\r"]}],"sklearn\/tests\/test_multiclass.py":[{"add":["82","    # Test if partial_fit is working as intended"],"delete":["82","    # Test if partial_fit is working as intented"]}],"doc\/whats_new\/v0.19.rst":[{"add":["809","  the weighted impurity decrease from splitting is no longer atleast"],"delete":["809","  the weighted impurity decrease from splitting is no longer alteast"]}],"sklearn\/decomposition\/online_lda.py":[{"add":["44","        Exponential value of expectation of log topic word distribution."],"delete":["44","        Exponential value of expection of log topic word distribution."]}],"sklearn\/tree\/_splitter.pyx":[{"add":["1255","        # We assume implicitly that end_positive = end and","1491","        # We assume implicitly that end_positive = end and"],"delete":["1255","        # We assume implicitely that end_positive = end and","1491","        # We assume implicitely that end_positive = end and"]}],"sklearn\/utils\/tests\/test_random.py":[{"add":["108","    class_probabilities = [np.array([0.5, 0.5]), np.array([0.6, 0.1, 0.3])]","110","    got = random_choice_csc(n_samples, classes, class_probabilities,","116","        assert_array_almost_equal(class_probabilities[k], p, decimal=1)","120","    class_probabilities = [np.array([0.5, 0.5]), np.array([0, 1\/2, 1\/2])]","129","        assert_array_almost_equal(class_probabilities[k], p, decimal=1)","133","    class_probabilities = [np.array([1.0, 0.0]), np.array([0.0, 1.0, 0.0])]","135","    got = random_choice_csc(n_samples, classes, class_probabilities,","141","                        minlength=len(class_probabilities[k])) \/ n_samples","142","        assert_array_almost_equal(class_probabilities[k], p, decimal=1)","146","    class_probabilities = [np.array([0.0, 1.0]), np.array([1.0])]","155","        assert_array_almost_equal(class_probabilities[k], p, decimal=1)","159","    # the length of an array in classes and class_probabilities is mismatched","161","    class_probabilities = [np.array([0.5, 0.5]), np.array([0.6, 0.1, 0.3])]","163","                  class_probabilities, 1)","167","    class_probabilities = [np.array([0.5, 0.5]), np.array([0.6, 0.1, 0.3])]","169","                  class_probabilities, 1)","173","    class_probabilities = [np.array([0.5, 0.5]), np.array([0.6, 0.1, 0.3])]","175","                  class_probabilities, 1)","179","    class_probabilities = [np.array([0.5, 0.6]), np.array([0.6, 0.1, 0.3])]","181","                  class_probabilities, 1)"],"delete":["108","    class_probabilites = [np.array([0.5, 0.5]), np.array([0.6, 0.1, 0.3])]","110","    got = random_choice_csc(n_samples, classes, class_probabilites,","116","        assert_array_almost_equal(class_probabilites[k], p, decimal=1)","120","    class_probabilites = [np.array([0.5, 0.5]), np.array([0, 1\/2, 1\/2])]","129","        assert_array_almost_equal(class_probabilites[k], p, decimal=1)","133","    class_probabilites = [np.array([1.0, 0.0]), np.array([0.0, 1.0, 0.0])]","135","    got = random_choice_csc(n_samples, classes, class_probabilites,","141","                        minlength=len(class_probabilites[k])) \/ n_samples","142","        assert_array_almost_equal(class_probabilites[k], p, decimal=1)","146","    class_probabilites = [np.array([0.0, 1.0]), np.array([1.0])]","155","        assert_array_almost_equal(class_probabilites[k], p, decimal=1)","159","    # the length of an array in classes and class_probabilites is mismatched","161","    class_probabilites = [np.array([0.5, 0.5]), np.array([0.6, 0.1, 0.3])]","163","                  class_probabilites, 1)","167","    class_probabilites = [np.array([0.5, 0.5]), np.array([0.6, 0.1, 0.3])]","169","                  class_probabilites, 1)","173","    class_probabilites = [np.array([0.5, 0.5]), np.array([0.6, 0.1, 0.3])]","175","                  class_probabilites, 1)","179","    class_probabilites = [np.array([0.5, 0.6]), np.array([0.6, 0.1, 0.3])]","181","                  class_probabilites, 1)"]}],"sklearn\/neighbors\/graph.py":[{"add":["70","        for mode='distance' as this will preserve backwards compatibility.","145","        for mode='distance' as this will preserve backwards compatibility."],"delete":["70","        for mode='distance' as this will preserve backwards compatibilty.","145","        for mode='distance' as this will preserve backwards compatibilty."]}],"sklearn\/metrics\/ranking.py":[{"add":["420","        # the presence of floating point errors"],"delete":["420","        # the presense of floating point errors"]}],"doc\/glossary.rst":[{"add":["992","        Continuous multioutput targets are represented as multiple"],"delete":["992","        Continous multioutput targets are represented as multiple"]}],"sklearn\/externals\/joblib\/numpy_pickle_utils.py":[{"add":["103","        # file which."],"delete":["103","        # file whic."]}],"sklearn\/model_selection\/_search.py":[{"add":["699","                # `\"param_%s\" % name` at the first occurrence of `name`."],"delete":["699","                # `\"param_%s\" % name` at the first occurence of `name`."]}],"sklearn\/neighbors\/quad_tree.pyx":[{"add":["424","                This is useful in t-SNE to compute the negative forces."],"delete":["424","                This is usefull in t-SNE to compute the negative forces."]}],"sklearn\/ensemble\/tests\/test_forest.py":[{"add":["225","    importances_parallel = est.feature_importances_","226","    assert_array_almost_equal(importances, importances_parallel)"],"delete":["225","    importances_parrallel = est.feature_importances_","226","    assert_array_almost_equal(importances, importances_parrallel)"]}],"benchmarks\/bench_plot_randomized_svd.py":[{"add":["33","(c) plot: time vs norm, varying datasets"],"delete":["33","(c) plot: time vs norm, varing datasets"]}],"examples\/gaussian_process\/plot_gpr_noisy_targets.py":[{"add":["51","# Instantiate a Gaussian Process model"],"delete":["51","# Instanciate a Gaussian Process model"]}],"sklearn\/externals\/joblib\/pool.py":[{"add":["575","            # arrays and that is else able to dump to memmap large in-memory"],"delete":["575","            # arrays and that is alse able to dump to memmap large in-memory"]}],"sklearn\/preprocessing\/data.py":[{"add":["2793","            If True, check that all data is positive and non-zero."],"delete":["2793","            If True, check that all data is postive and non-zero."]}],"sklearn\/model_selection\/_split.py":[{"add":["1098","    RepeatedStratifiedKFold: Repeats Stratified K-Fold n times."],"delete":["1098","    RepeatedStratifiedKFold: Repeates Stratified K-Fold n times."]}],"doc\/developers\/maintainer.rst":[{"add":["86","The branch targeted by the Cron job and the frequency of the Cron job is set"],"delete":["86","The branch targetted by the Cron job and the frequency of the Cron job is set"]}],"sklearn\/ensemble\/iforest.py":[{"add":["285","        # We subtract self.offset_ to make 0 be the threshold value for being"],"delete":["285","        # We substract self.offset_ to make 0 be the threshold value for being"]}],"sklearn\/utils\/validation.py":[{"add":["499","        # when no dtype conversion happened, for example dtype = None. The"],"delete":["499","        # when no dtype conversion happend, for example dtype = None. The"]}],"examples\/preprocessing\/plot_scaling_importance.py":[{"add":["91","# Show first principal components"],"delete":["91","# Show first principal componenets"]}],"sklearn\/neighbors\/quad_tree.pxd":[{"add":["28","    # Base storage structure for cells in a QuadTree object"],"delete":["28","    # Base storage stucture for cells in a QuadTree object"]}],"examples\/gaussian_process\/plot_gpc_isoprobability.py":[{"add":["48","# Instantiate and fit Gaussian Process Model"],"delete":["48","# Instanciate and fit Gaussian Process Model"]}],"examples\/svm\/plot_rbf_parameters.py":[{"add":["33","them apart in the blink of an eye."],"delete":["33","them appart in the blink of an eye."]}],"doc\/tutorial\/machine_learning_map\/ML_MAPS_README.txt":[{"add":["21","you can move objects around, etc. as you need."],"delete":["21","you can move objects around, ect. as you need."]}],"sklearn\/multiclass.py":[{"add":["199","        # outperform or match a dense label binarizer in all cases and has also"],"delete":["199","        # outpreform or match a dense label binarizer in all cases and has also"]}],"sklearn\/cluster\/_k_means_elkan.pyx":[{"add":["152","    # Get the initial set of upper bounds and lower bounds for each sample."],"delete":["152","    # Get the inital set of upper bounds and lower bounds for each sample."]}],"doc\/themes\/scikit-learn\/static\/nature.css_t":[{"add":["788","\/*-----------------------The Code Sprint Sponsor Banner---------------------*\/"],"delete":["788","\/*-----------------------The Code Sprint Sponser Banner---------------------*\/"]}],"sklearn\/datasets\/tests\/test_base.py":[{"add":["250","    # is a surprising behaviour because reading bunch.key uses"],"delete":["250","    # is a suprising behaviour because reading bunch.key uses"]}],"sklearn\/linear_model\/sgd_fast.pyx":[{"add":["388","        (4) Passive Aggressive-I, eta = min(alpha, loss\/norm(x))","389","        (5) Passive Aggressive-II, eta = 1.0 \/ (norm(x) + 0.5*alpha)"],"delete":["388","        (4) Passive Agressive-I, eta = min(alpha, loss\/norm(x))","389","        (5) Passive Agressive-II, eta = 1.0 \/ (norm(x) + 0.5*alpha)"]}]}},"95993a4b2b7d067d8d7fff91ccb2463dbd427e7c":{"changes":{"sklearn\/tree\/tree.py":"MODIFY","sklearn\/ensemble\/tests\/test_forest.py":"MODIFY","sklearn\/tree\/tests\/test_tree.py":"MODIFY"},"diff":{"sklearn\/tree\/tree.py":[{"add":["438","                class_type = self.classes_[0].dtype","439","                predictions = np.zeros((n_samples, self.n_outputs_),","440","                                       dtype=class_type)"],"delete":["438","                predictions = np.zeros((n_samples, self.n_outputs_))","439",""]}],"sklearn\/ensemble\/tests\/test_forest.py":[{"add":["1339","","1340","","1341","@pytest.mark.filterwarnings('ignore:The default value of n_estimators')","1342","@pytest.mark.parametrize('name', FOREST_CLASSIFIERS)","1343","@pytest.mark.parametrize('oob_score', (True, False))","1344","def test_multi_target(name, oob_score):","1345","    ForestClassifier = FOREST_CLASSIFIERS[name]","1346","","1347","    clf = ForestClassifier(bootstrap=True, oob_score=oob_score)","1348","","1349","    X = iris.data","1350","","1351","    # Make multi column mixed type target.","1352","    y = np.vstack([","1353","        iris.target.astype(float),","1354","        iris.target.astype(int),","1355","        iris.target.astype(str),","1356","    ]).T","1357","","1358","    # Try to fit and predict.","1359","    clf.fit(X, y)","1360","    clf.predict(X)"],"delete":[]}],"sklearn\/tree\/tests\/test_tree.py":[{"add":["1830","","1831","","1832","@pytest.mark.parametrize('name', CLF_TREES)","1833","def test_multi_target(name):","1834","    Tree = CLF_TREES[name]","1835","","1836","    clf = Tree()","1837","","1838","    X = iris.data","1839","","1840","    # Make multi column mixed type target.","1841","    y = np.vstack([","1842","        iris.target.astype(float),","1843","        iris.target.astype(int),","1844","        iris.target.astype(str),","1845","    ]).T","1846","","1847","    # Try to fit and predict.","1848","    clf.fit(X, y)","1849","    clf.predict(X)"],"delete":[]}]}},"cd3e0dcf020159d855911a28f77c67462d4241c2":{"changes":{"sklearn\/tests\/test_impute.py":"MODIFY","doc\/modules\/impute.rst":"MODIFY","sklearn\/impute.py":"MODIFY"},"diff":{"sklearn\/tests\/test_impute.py":[{"add":["100","@pytest.mark.parametrize(\"strategy\", [\"mean\", \"median\",","101","                                      \"most_frequent\", \"constant\"])","102","def test_imputation_error_sparse_0(strategy):","103","    # check that error are raised when missing_values = 0 and input is sparse","104","    X = np.ones((3, 5))","105","    X[0] = 0","106","    X = sparse.csc_matrix(X)","107","","108","    imputer = SimpleImputer(strategy=strategy, missing_values=0)","109","    with pytest.raises(ValueError, match=\"Provide a dense array\"):","110","        imputer.fit(X)","111","","112","    imputer.fit(X.toarray())","113","    with pytest.raises(ValueError, match=\"Provide a dense array\"):","114","        imputer.transform(X)","115","","116","","144","              lambda z, v, p: safe_median(np.hstack((z, v))))]","445","    X = sparse_random_matrix(100, 100, density=0.10)","446","    missing_values = X.data[0]","447","","448","    pipeline = Pipeline([('imputer',","449","                          SimpleImputer(missing_values=missing_values)),","450","                         ('tree',","451","                          tree.DecisionTreeRegressor(random_state=0))])"],"delete":["126","             (\"mean\", 0, lambda z, v, p: np.mean(v)),","128","              lambda z, v, p: safe_median(np.hstack((z, v)))),","129","             (\"median\", 0, lambda z, v, p: np.median(v))]","430","    pipeline = Pipeline([('imputer', SimpleImputer(missing_values=0)),","431","                         ('tree', tree.DecisionTreeRegressor(random_state=0))])","437","    X = sparse_random_matrix(100, 100, density=0.10)"]}],"doc\/modules\/impute.rst":[{"add":["58","    >>> X = sp.csc_matrix([[1, 2], [0, -1], [8, 4]])","59","    >>> imp = SimpleImputer(missing_values=-1, strategy='mean')","61","    SimpleImputer(copy=True, fill_value=None, missing_values=-1, strategy='mean', verbose=0)","62","    >>> X_test = sp.csc_matrix([[-1, 2], [6, -1], [7, 6]])","63","    >>> print(imp.transform(X_test).toarray())      # doctest: +NORMALIZE_WHITESPACE","64","    [[3. 2.]","65","     [6. 3.]","66","     [7. 6.]]","68","Note that this format is not meant to be used to implicitly store missing values","69","in the matrix because it would densify it at transform time. Missing values encoded","70","by 0 must be used with dense input."],"delete":["58","    >>> X = sp.csc_matrix([[1, 2], [0, 3], [7, 6]])","59","    >>> imp = SimpleImputer(missing_values=0, strategy='mean')","61","    SimpleImputer(copy=True, fill_value=None, missing_values=0, strategy='mean', verbose=0)","62","    >>> X_test = sp.csc_matrix([[0, 2], [6, 0], [7, 6]])","63","    >>> print(imp.transform(X_test))      # doctest: +NORMALIZE_WHITESPACE  +ELLIPSIS","64","    [[4.          2.        ]","65","     [6.          3.666...]","66","     [7.          6.        ]]","68","Note that, here, missing values are encoded by 0 and are thus implicitly stored","69","in the matrix. This format is thus suitable when there are many more missing","70","values than observed values."]}],"sklearn\/impute.py":[{"add":["238","            # missing_values = 0 not allowed with sparse data as it would","239","            # force densification","240","            if self.missing_values == 0:","241","                raise ValueError(\"Imputation not possible when missing_values \"","242","                                 \"== 0 and input is sparse. Provide a dense \"","243","                                 \"array instead.\")","244","            else:","245","                self.statistics_ = self._sparse_fit(X,","246","                                                    self.strategy,","247","                                                    self.missing_values,","248","                                                    fill_value)","259","        mask_data = _get_mask(X.data, missing_values)","260","        n_implicit_zeros = X.shape[0] - np.diff(X.indptr)","261","","262","        statistics = np.empty(X.shape[1])","263","","264","        if strategy == \"constant\":","265","            # for constant strategy, self.statistcs_ is used to store","266","            # fill_value in each column","267","            statistics.fill(fill_value)","268","","270","            for i in range(X.shape[1]):","271","                column = X.data[X.indptr[i]:X.indptr[i+1]]","272","                mask_column = mask_data[X.indptr[i]:X.indptr[i+1]]","273","                column = column[~mask_column]","275","                # combine explicit and implicit zeros","276","                mask_zeros = _get_mask(column, 0)","277","                column = column[~mask_zeros]","278","                n_explicit_zeros = mask_zeros.sum()","279","                n_zeros = n_implicit_zeros[i] + n_explicit_zeros","281","                if strategy == \"mean\":","282","                    s = column.size + n_zeros","283","                    statistics[i] = np.nan if s == 0 else column.sum() \/ s","285","                elif strategy == \"median\":","286","                    statistics[i] = _get_median(column,","287","                                                n_zeros)","289","                elif strategy == \"most_frequent\":","290","                    statistics[i] = _most_frequent(column,","291","                                                   0,","292","                                                   n_zeros)","293","        return statistics","343","            # for constant strategy, self.statistcs_ is used to store","344","            # fill_value in each column","383","        if sparse.issparse(X):","384","            if self.missing_values == 0:","385","                raise ValueError(\"Imputation not possible when missing_values \"","386","                                 \"== 0 and input is sparse. Provide a dense \"","387","                                 \"array instead.\")","388","            else:","389","                mask = _get_mask(X.data, self.missing_values)","390","                indexes = np.repeat(np.arange(len(X.indptr) - 1, dtype=np.int),","391","                                    np.diff(X.indptr))[mask]","393","                X.data[mask] = valid_statistics[indexes].astype(X.dtype,","394","                                                                copy=False)"],"delete":["144","        - If X is sparse and `missing_values=0`;","239","            self.statistics_ = self._sparse_fit(X,","240","                                                self.strategy,","241","                                                self.missing_values,","242","                                                fill_value)","253","        # Count the zeros","254","        if missing_values == 0:","255","            n_zeros_axis = np.zeros(X.shape[1], dtype=int)","257","            n_zeros_axis = X.shape[0] - np.diff(X.indptr)","259","        # Mean","260","        if strategy == \"mean\":","261","            if missing_values != 0:","262","                n_non_missing = n_zeros_axis","264","                # Mask the missing elements","265","                mask_missing_values = _get_mask(X.data, missing_values)","266","                mask_valids = np.logical_not(mask_missing_values)","268","                # Sum only the valid elements","269","                new_data = X.data.copy()","270","                new_data[mask_missing_values] = 0","271","                X = sparse.csc_matrix((new_data, X.indices, X.indptr),","272","                                      copy=False)","273","                sums = X.sum(axis=0)","275","                # Count the elements != 0","276","                mask_non_zeros = sparse.csc_matrix(","277","                    (mask_valids.astype(np.float64),","278","                     X.indices,","279","                     X.indptr), copy=False)","280","                s = mask_non_zeros.sum(axis=0)","281","                n_non_missing = np.add(n_non_missing, s)","282","","283","            else:","284","                sums = X.sum(axis=0)","285","                n_non_missing = np.diff(X.indptr)","286","","287","            # Ignore the error, columns with a np.nan statistics_","288","            # are not an error at this point. These columns will","289","            # be removed in transform","290","            with np.errstate(all=\"ignore\"):","291","                return np.ravel(sums) \/ np.ravel(n_non_missing)","292","","293","        # Median + Most frequent + Constant","294","        else:","295","            # Remove the missing values, for each column","296","            columns_all = np.hsplit(X.data, X.indptr[1:-1])","297","            mask_missing_values = _get_mask(X.data, missing_values)","298","            mask_valids = np.hsplit(np.logical_not(mask_missing_values),","299","                                    X.indptr[1:-1])","300","","301","            # astype necessary for bug in numpy.hsplit before v1.9","302","            columns = [col[mask.astype(bool, copy=False)]","303","                       for col, mask in zip(columns_all, mask_valids)]","304","","305","            # Median","306","            if strategy == \"median\":","307","                median = np.empty(len(columns))","308","                for i, column in enumerate(columns):","309","                    median[i] = _get_median(column, n_zeros_axis[i])","310","","311","                return median","312","","313","            # Most frequent","314","            elif strategy == \"most_frequent\":","315","                most_frequent = np.empty(len(columns))","316","","317","                for i, column in enumerate(columns):","318","                    most_frequent[i] = _most_frequent(column,","319","                                                      0,","320","                                                      n_zeros_axis[i])","321","","322","                return most_frequent","323","","324","            # Constant","325","            elif strategy == \"constant\":","326","                return np.full(X.shape[1], fill_value)","414","        if sparse.issparse(X) and self.missing_values != 0:","415","            mask = _get_mask(X.data, self.missing_values)","416","            indexes = np.repeat(np.arange(len(X.indptr) - 1, dtype=np.int),","417","                                np.diff(X.indptr))[mask]","419","            X.data[mask] = valid_statistics[indexes].astype(X.dtype,","420","                                                            copy=False)","422","            if sparse.issparse(X):","423","                X = X.toarray()","424",""]}]}},"d9a0774afc5d30a8e98a25e1e8c3a0c428e88dfa":{"changes":{"sklearn\/gaussian_process\/kernels.py":"MODIFY","sklearn\/utils\/testing.py":"MODIFY","sklearn\/gaussian_process\/tests\/test_kernels.py":"MODIFY","examples\/model_selection\/plot_precision_recall.py":"MODIFY"},"diff":{"sklearn\/gaussian_process\/kernels.py":[{"add":["32","from ..utils.fixes import signature"],"delete":["32","from sklearn.externals.funcsigs import signature"]}],"sklearn\/utils\/testing.py":[{"add":["48","from sklearn.utils.fixes import signature"],"delete":["48","from sklearn.externals.funcsigs import signature"]}],"sklearn\/gaussian_process\/tests\/test_kernels.py":[{"add":["7","from sklearn.utils.fixes import signature"],"delete":["5","from sklearn.externals.funcsigs import signature","6",""]}],"examples\/model_selection\/plot_precision_recall.py":[{"add":["139","from sklearn.utils.fixes import signature"],"delete":["139","from sklearn.externals.funcsigs import signature"]}]}},"abb43c1aef10a3348216006f6be5e34dcd6ac157":{"changes":{"sklearn\/decomposition\/online_lda.py":"MODIFY"},"diff":{"sklearn\/decomposition\/online_lda.py":[{"add":["7","Link: https:\/\/github.com\/blei-lab\/onlineldavb","259","        https:\/\/github.com\/blei-lab\/onlineldavb"],"delete":["7","Link: http:\/\/matthewdhoffman.com\/code\/onlineldavb.tar","259","        http:\/\/matthewdhoffman.com\/\/code\/onlineldavb.tar"]}]}},"d045c1abc8f424b987f39ffc289e6cfa2e6d4e79":{"changes":{"doc\/modules\/multiclass.rst":"MODIFY","doc\/whats_new\/v0.21.rst":"MODIFY","doc\/modules\/svm.rst":"MODIFY","sklearn\/utils\/multiclass.py":"MODIFY","sklearn\/svm\/base.py":"MODIFY","sklearn\/utils\/estimator_checks.py":"MODIFY","sklearn\/utils\/tests\/test_multiclass.py":"MODIFY"},"diff":{"doc\/modules\/multiclass.rst":[{"add":["229","dataset is used ``n_classes`` times. The decision function is the result","230","of a monotonic transformation of the one-versus-one classification."],"delete":["229","dataset is used ``n_classes`` times."]}],"doc\/whats_new\/v0.21.rst":[{"add":["26","- :func:`svm.SVC.decision_function` and","27","  :func:`multiclass.OneVsOneClassifier.decision_function`. |Fix|","28","","380",":mod:`sklearn.svm`","381","..................","382","","383","- |Fix| Fixed an issue in :func:`svm.SVC.decision_function`","384","  when ``decision_function_shape='ovr'``. The decision_function value of a given","385","  sample was different depending on whether the decision_function was evaluated","386","  on the sample alone or on a batch containing this same sample due to the scaling","387","  used in decision_function. :pr:`10440` by :user:`Jonathan Ohayon <Johayon>`.","388","","389",":mod:`sklearn.multiclass`","390",".........................","391","","392","- |Fix| Fixed an issue in :func:`multiclass.OneVsOneClassifier.decision_function`","393","  where the decision_function value of a given sample was different depending on","394","  whether the decision_function was evaluated on the sample alone or on a batch","395","  containing this same sample due to the scaling used in decision_function.","396","  :pr:`10440` by :user:`Jonathan Ohayon <Johayon>`."],"delete":[]}],"doc\/modules\/svm.rst":[{"add":["115","``decision_function_shape`` option allows to monotically transform the results of the","117","n_classes)``."],"delete":["115","``decision_function_shape`` option allows to aggregate the results of the","117","n_classes)``::"]}],"sklearn\/utils\/multiclass.py":[{"add":["433","    # Monotonically transform the sum_of_confidences to (-1\/3, 1\/3)","434","    # and add it with votes. The monotonic transformation  is","435","    # f: x -> x \/ (3 * (|x| + 1)), it uses 1\/3 instead of 1\/2","436","    # to ensure that we won't reach the limits and change vote order.","440","    transformed_confidences = (sum_of_confidences \/","441","                               (3 * (np.abs(sum_of_confidences) + 1)))","442","    return votes + transformed_confidences"],"delete":["433","    max_confidences = sum_of_confidences.max()","434","    min_confidences = sum_of_confidences.min()","435","","436","    if max_confidences == min_confidences:","437","        return votes","438","","439","    # Scale the sum_of_confidences to (-0.5, 0.5) and add it with votes.","443","    eps = np.finfo(sum_of_confidences.dtype).eps","444","    max_abs_confidence = max(abs(max_confidences), abs(min_confidences))","445","    scale = (0.5 - eps) \/ max_abs_confidence","446","    return votes + sum_of_confidences * scale"]}],"sklearn\/svm\/base.py":[{"add":["549","        If decision_function_shape='ovr', the decision function is a monotonic","550","        transformation of ovo decision function."],"delete":[]}],"sklearn\/utils\/estimator_checks.py":[{"add":["838","        if (name, method) in [('NuSVC', 'decision_function'),"],"delete":["838","        if (name, method) in [('SVC', 'decision_function'),","839","                              ('NuSVC', 'decision_function'),"]}],"sklearn\/utils\/tests\/test_multiclass.py":[{"add":["18","from sklearn.utils.testing import assert_allclose","26","from sklearn.utils.multiclass import _ovr_decision_function","382","","383","","384","def test_ovr_decision_function():","385","    # test properties for ovr decision function","386","","387","    predictions = np.array([[0, 1, 1],","388","                            [0, 1, 0],","389","                            [0, 1, 1],","390","                            [0, 1, 1]])","391","","392","    confidences = np.array([[-1e16, 0, -1e16],","393","                            [1., 2., -3.],","394","                            [-5., 2., 5.],","395","                            [-0.5, 0.2, 0.5]])","396","","397","    n_classes = 3","398","","399","    dec_values = _ovr_decision_function(predictions, confidences, n_classes)","400","","401","    # check that the decision values are within 0.5 range of the votes","402","    votes = np.array([[1, 0, 2],","403","                      [1, 1, 1],","404","                      [1, 0, 2],","405","                      [1, 0, 2]])","406","","407","    assert_allclose(votes, dec_values, atol=0.5)","408","","409","    # check that the prediction are what we expect","410","    # highest vote or highest confidence if there is a tie.","411","    # for the second sample we have a tie (should be won by 1)","412","    expected_prediction = np.array([2, 1, 2, 2])","413","    assert_array_equal(np.argmax(dec_values, axis=1), expected_prediction)","414","","415","    # third and fourth sample have the same vote but third sample","416","    # has higher confidence, this should reflect on the decision values","417","    assert (dec_values[2, 2] > dec_values[3, 2])","418","","419","    # assert subset invariance.","420","    dec_values_one = [_ovr_decision_function(np.array([predictions[i]]),","421","                                             np.array([confidences[i]]),","422","                                             n_classes)[0] for i in range(4)]","423","","424","    assert_allclose(dec_values, dec_values_one, atol=1e-6)"],"delete":[]}]}},"102620f8d122d244dd08ccaa015d67a97d8a28ae":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/base.py":"MODIFY","sklearn\/tests\/test_base.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["124","- Fixed a bug when setting parameters on meta-estimator, involving both a","125","  wrapped estimator and its parameter. :issue:`9999` by :user:`Marcus Voss","126","  <marcus-voss>` and `Joel Nothman`_.","127",""],"delete":[]}],"sklearn\/base.py":[{"add":["265","                valid_params[key] = value"],"delete":[]}],"sklearn\/tests\/test_base.py":[{"add":["248","def test_set_params_updates_valid_params():","249","    # Check that set_params tries to set SVC().C, not","250","    # DecisionTreeClassifier().C","251","    gscv = GridSearchCV(DecisionTreeClassifier(), {})","252","    gscv.set_params(estimator=SVC(), estimator__C=42.0)","253","    assert gscv.estimator.C == 42.0","254","","255",""],"delete":[]}]}}}