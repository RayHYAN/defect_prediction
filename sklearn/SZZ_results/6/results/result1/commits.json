{"6c4b87868470c933eaf28d2dc2f3e59ec5e78724":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/linear_model\/tests\/test_logistic.py":"MODIFY","sklearn\/linear_model\/logistic.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["75","- :class:`linear_model.LogisticRegressionCV` (bug fix)","467","- Fixed a bug in :class:`linear_model.LogisticRegressionCV` where the 'ovr'","468","  strategy was always used to compute cross-validation scores in the","469","  multiclass setting, even if 'multinomial' was set.","470","  :issue:`8720` by :user:`William de Vazelhes <wdevazelhes>`.","471",""],"delete":[]}],"sklearn\/linear_model\/tests\/test_logistic.py":[{"add":["8","from sklearn.metrics.scorer import get_scorer","32","    _log_reg_scoring_path)","495","@pytest.mark.parametrize('scoring, multiclass_agg_list',","496","                         [('accuracy', ['']),","497","                          ('precision', ['_macro', '_weighted']),","498","                          # no need to test for micro averaging because it","499","                          # is the same as accuracy for f1, precision,","500","                          # and recall (see https:\/\/github.com\/","501","                          # scikit-learn\/scikit-learn\/pull\/","502","                          # 11578#discussion_r203250062)","503","                          ('f1', ['_macro', '_weighted']),","504","                          ('neg_log_loss', ['']),","505","                          ('recall', ['_macro', '_weighted'])])","506","def test_logistic_cv_multinomial_score(scoring, multiclass_agg_list):","507","    # test that LogisticRegressionCV uses the right score to compute its","508","    # cross-validation scores when using a multinomial scoring","509","    # see https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/8720","510","    X, y = make_classification(n_samples=100, random_state=0, n_classes=3,","511","                               n_informative=6)","512","    train, test = np.arange(80), np.arange(80, 100)","513","    lr = LogisticRegression(C=1., solver='lbfgs', multi_class='multinomial')","514","    # we use lbfgs to support multinomial","515","    params = lr.get_params()","516","    # we store the params to set them further in _log_reg_scoring_path","517","    for key in ['C', 'n_jobs', 'warm_start']:","518","        del params[key]","519","    lr.fit(X[train], y[train])","520","    for averaging in multiclass_agg_list:","521","        scorer = get_scorer(scoring + averaging)","522","        assert_array_almost_equal(","523","            _log_reg_scoring_path(X, y, train, test, Cs=[1.],","524","                                  scoring=scorer, **params)[2][0],","525","            scorer(lr, X[test], y[test]))","526","","527",""],"delete":["31",")"]}],"sklearn\/linear_model\/logistic.py":[{"add":["925","    log_reg = LogisticRegression(multi_class=multi_class)"],"delete":["925","    log_reg = LogisticRegression(fit_intercept=fit_intercept)"]}]}},"888bced086d76199a54f462eb14700df8bc387aa":{"changes":{"sklearn\/datasets\/rcv1.py":"MODIFY"},"diff":{"sklearn\/datasets\/rcv1.py":[{"add":["26","# The original vectorized data can be found at:","27","#    http:\/\/www.ai.mit.edu\/projects\/jmlr\/papers\/volume5\/lewis04a\/a13-vector-files\/lyrl2004_vectors_test_pt0.dat.gz","28","#    http:\/\/www.ai.mit.edu\/projects\/jmlr\/papers\/volume5\/lewis04a\/a13-vector-files\/lyrl2004_vectors_test_pt1.dat.gz","29","#    http:\/\/www.ai.mit.edu\/projects\/jmlr\/papers\/volume5\/lewis04a\/a13-vector-files\/lyrl2004_vectors_test_pt2.dat.gz","30","#    http:\/\/www.ai.mit.edu\/projects\/jmlr\/papers\/volume5\/lewis04a\/a13-vector-files\/lyrl2004_vectors_test_pt3.dat.gz","31","#    http:\/\/www.ai.mit.edu\/projects\/jmlr\/papers\/volume5\/lewis04a\/a13-vector-files\/lyrl2004_vectors_train.dat.gz","32","# while the original stemmed token files can be found","33","# in the README, section B.12.i.:","34","#    http:\/\/www.ai.mit.edu\/projects\/jmlr\/papers\/volume5\/lewis04a\/lyrl2004_rcv1v2_README.htm"],"delete":["26","# The original data can be found at:","27","# http:\/\/jmlr.csail.mit.edu\/papers\/volume5\/lewis04a\/a13-vector-files\/lyrl2004_vectors_test_pt0.dat.gz","28","# http:\/\/jmlr.csail.mit.edu\/papers\/volume5\/lewis04a\/a13-vector-files\/lyrl2004_vectors_test_pt1.dat.gz","29","# http:\/\/jmlr.csail.mit.edu\/papers\/volume5\/lewis04a\/a13-vector-files\/lyrl2004_vectors_test_pt2.dat.gz","30","# http:\/\/jmlr.csail.mit.edu\/papers\/volume5\/lewis04a\/a13-vector-files\/lyrl2004_vectors_test_pt3.dat.gz","31","# http:\/\/jmlr.csail.mit.edu\/papers\/volume5\/lewis04a\/a13-vector-files\/lyrl2004_vectors_train.dat.gz"]}]}},"61fa3155da5706a35a8fb28c76ccbc8aee41cb5b":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/preprocessing\/_encoders.py":"MODIFY","sklearn\/preprocessing\/tests\/test_encoders.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["304","- :class:`preprocessing.OneHotEncoder` now supports the","305","  :meth:`get_feature_names` method to obtain the transformed feature names.","306","  :issue:`10181` by  :user:`Nirvan Anjirbag <Nirvan101>` and","307","  `Joris Van den Bossche`_.","308",""],"delete":[]}],"sklearn\/preprocessing\/_encoders.py":[{"add":["242","    >>> enc.get_feature_names()","243","    array(['x0_Female', 'x0_Male', 'x1_1', 'x1_2', 'x1_3'], dtype=object)","643","    def get_feature_names(self, input_features=None):","644","        \"\"\"Return feature names for output features.","645","","646","        Parameters","647","        ----------","648","        input_features : list of string, length n_features, optional","649","            String names for input features if available. By default,","650","            \"x0\", \"x1\", ... \"xn_features\" is used.","651","","652","        Returns","653","        -------","654","        output_feature_names : array of string, length n_output_features","655","","656","        \"\"\"","657","        check_is_fitted(self, 'categories_')","658","        cats = self.categories_","659","        if input_features is None:","660","            input_features = ['x%d' % i for i in range(len(cats))]","661","        elif(len(input_features) != len(self.categories_)):","662","            raise ValueError(","663","                \"input_features should have length equal to number of \"","664","                \"features ({}), got {}\".format(len(self.categories_),","665","                                               len(input_features)))","666","","667","        feature_names = []","668","        for i in range(len(cats)):","669","            names = [","670","                input_features[i] + '_' + six.text_type(t) for t in cats[i]]","671","            feature_names.extend(names)","672","","673","        return np.array(feature_names, dtype=object)","674",""],"delete":[]}],"sklearn\/preprocessing\/tests\/test_encoders.py":[{"add":["0","# -*- coding: utf-8 -*-","458","def test_one_hot_encoder_feature_names():","459","    enc = OneHotEncoder()","460","    X = [['Male', 1, 'girl', 2, 3],","461","         ['Female', 41, 'girl', 1, 10],","462","         ['Male', 51, 'boy', 12, 3],","463","         ['Male', 91, 'girl', 21, 30]]","464","","465","    enc.fit(X)","466","    feature_names = enc.get_feature_names()","467","    assert isinstance(feature_names, np.ndarray)","468","","469","    assert_array_equal(['x0_Female', 'x0_Male',","470","                        'x1_1', 'x1_41', 'x1_51', 'x1_91',","471","                        'x2_boy', 'x2_girl',","472","                        'x3_1', 'x3_2', 'x3_12', 'x3_21',","473","                        'x4_3',","474","                        'x4_10', 'x4_30'], feature_names)","475","","476","    feature_names2 = enc.get_feature_names(['one', 'two',","477","                                            'three', 'four', 'five'])","478","","479","    assert_array_equal(['one_Female', 'one_Male',","480","                        'two_1', 'two_41', 'two_51', 'two_91',","481","                        'three_boy', 'three_girl',","482","                        'four_1', 'four_2', 'four_12', 'four_21',","483","                        'five_3', 'five_10', 'five_30'], feature_names2)","484","","485","    with pytest.raises(ValueError, match=\"input_features should have length\"):","486","        enc.get_feature_names(['one', 'two'])","487","","488","","489","def test_one_hot_encoder_feature_names_unicode():","490","    enc = OneHotEncoder()","491","    X = np.array([[u'c?t1', u'dat2']], dtype=object).T","492","    enc.fit(X)","493","    feature_names = enc.get_feature_names()","494","    assert_array_equal([u'x0_c?t1', u'x0_dat2'], feature_names)","495","    feature_names = enc.get_feature_names(input_features=[u'n?me'])","496","    assert_array_equal([u'n?me_c?t1', u'n?me_dat2'], feature_names)","497","","498",""],"delete":[]}]}},"f89131bc37c172aafb54db461089b0cbc5bcaf62":{"changes":{"sklearn\/neighbors\/binary_tree.pxi":"MODIFY","doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/neighbors\/kde.py":"MODIFY","sklearn\/neighbors\/ball_tree.pyx":"MODIFY","sklearn\/neighbors\/tests\/test_kde.py":"MODIFY"},"diff":{"sklearn\/neighbors\/binary_tree.pxi":[{"add":["1010","    cdef np.ndarray sample_weight_arr","1016","    cdef readonly DTYPE_t[::1] sample_weight","1017","    cdef public DTYPE_t sum_weight","1041","        self.sample_weight_arr = np.empty(1, dtype=DTYPE, order='C')","1047","        self.sample_weight = get_memview_DTYPE_1D(self.sample_weight_arr)","1064","                 leaf_size=40, metric='minkowski', sample_weight=None, **kwargs):","1068","","1090","","1091","        if sample_weight is not None:","1092","            self.sample_weight_arr = np.asarray(sample_weight, dtype=DTYPE, order='C')","1093","            self.sample_weight = get_memview_DTYPE_1D(self.sample_weight_arr)","1094","            self.sum_weight = np.sum(self.sample_weight)","1095","        else:","1096","            self.sample_weight = None","1097","            self.sum_weight = <DTYPE_t> n_samples","1098","","1099","","1672","                log_min_bound = (log(self.sum_weight) +","1675","                log_max_bound = (log(self.sum_weight) +","2142","        cdef ITYPE_t i, i1, i2, i_node","2143","        cdef DTYPE_t N1, N2","2148","        cdef bint with_sample_weight = self.sample_weight is not None","2149","        cdef DTYPE_t* sample_weight","2150","        if with_sample_weight:","2151","            sample_weight = &self.sample_weight[0]","2154","        cdef DTYPE_t N","2155","        cdef DTYPE_t log_weight","2156","        if with_sample_weight:","2157","            N = self.sum_weight","2158","        else:","2159","            N = <DTYPE_t> self.data.shape[0]","2191","            if with_sample_weight:","2192","                N1 = _total_node_weight(node_data, sample_weight,","2193","                                        idx_array, i_node)","2194","            else:","2195","                N1 = node_info.idx_end - node_info.idx_start","2224","                    if with_sample_weight:","2225","                        log_weight = np.log(sample_weight[idx_array[i]])","2226","                    else:","2227","                        log_weight = 0.","2229","                                                     log_density + log_weight)","2237","                if with_sample_weight:","2238","                    N1 = _total_node_weight(node_data, sample_weight,","2239","                                            idx_array, i1)","2240","                    N2 = _total_node_weight(node_data, sample_weight,","2241","                                            idx_array, i2)","2242","                else:","2243","                    N1 = node_data[i1].idx_end - node_data[i1].idx_start","2244","                    N2 = node_data[i2].idx_end - node_data[i2].idx_start","2306","        cdef ITYPE_t i, i1, i2, iw, start, end","2307","        cdef DTYPE_t N1, N2","2310","        cdef NodeData_t* node_data = &self.node_data[0]","2311","        cdef bint with_sample_weight = self.sample_weight is not None","2312","        cdef DTYPE_t* sample_weight","2313","        cdef DTYPE_t log_weight","2314","        if with_sample_weight:","2315","            sample_weight = &self.sample_weight[0]","2326","        if with_sample_weight:","2327","            N1  = _total_node_weight(node_data, sample_weight,","2328","                                     idx_array, i_node)","2329","            N2 = self.sum_weight","2330","        else:","2331","            N1 = <DTYPE_t>(node_info.idx_end - node_info.idx_start)","2332","            N2 = <DTYPE_t>self.data.shape[0]","2359","                if with_sample_weight:","2360","                    log_weight = np.log(sample_weight[idx_array[i]])","2361","                else:","2362","                    log_weight = 0.","2364","                                                    (log_dens_contribution +","2365","                                                     log_weight))","2373","            if with_sample_weight:","2374","                N1 = _total_node_weight(node_data, sample_weight,","2375","                                        idx_array, i1)","2376","                N2 = _total_node_weight(node_data, sample_weight,","2377","                                        idx_array, i2)","2378","            else:","2379","                N1 = <DTYPE_t>(self.node_data[i1].idx_end - self.node_data[i1].idx_start)","2380","                N2 = <DTYPE_t>(self.node_data[i2].idx_end - self.node_data[i2].idx_start)","2605","","2606","cdef inline DTYPE_t _total_node_weight(NodeData_t* node_data,","2607","                                       DTYPE_t* sample_weight,","2608","                                       ITYPE_t* idx_array,","2609","                                       ITYPE_t i_node):","2610","    cdef ITYPE_t i","2611","    cdef DTYPE_t N = 0.0","2612","    for i in range(node_data[i_node].idx_start, node_data[i_node].idx_end):","2613","        N += sample_weight[idx_array[i]]","2614","    return N"],"delete":["1059","                 leaf_size=40, metric='minkowski', **kwargs):","1656","                log_min_bound = (log(n_samples) +","1659","                log_max_bound = (log(n_samples) +","2126","        cdef ITYPE_t i, i1, i2, N1, N2, i_node","2133","        cdef ITYPE_t N = self.data.shape[0]","2165","            N1 = node_info.idx_end - node_info.idx_start","2195","                                                     log_density)","2203","                N1 = node_data[i1].idx_end - node_data[i1].idx_start","2204","                N2 = node_data[i2].idx_end - node_data[i2].idx_start","2266","        cdef ITYPE_t i, i1, i2, N1, N2","2279","        N1 = node_info.idx_end - node_info.idx_start","2280","        N2 = self.data.shape[0]","2308","                                                    log_dens_contribution)","2316","            N1 = self.node_data[i1].idx_end - self.node_data[i1].idx_start","2317","            N2 = self.node_data[i2].idx_end - self.node_data[i2].idx_start"]}],"doc\/whats_new\/v0.20.rst":[{"add":["208","- Add `sample_weight` parameter to the fit method of","209","  :class:`neighbors.KernelDensity` to enables weighting in kernel density","210","  estimation.","211","  :issue:`4394` by :user:`Samuel O. Ronsin <samronsin>`.","212",""],"delete":[]}],"sklearn\/neighbors\/kde.py":[{"add":["9","from ..utils import check_array, check_random_state, check_consistent_length","10","","115","    def fit(self, X, y=None, sample_weight=None):","123","        sample_weight: array_like, shape (n_samples,), optional","124","            List of sample weights attached to the data X.","129","        if sample_weight is not None:","130","            sample_weight = check_array(sample_weight, order='C', dtype=DTYPE,","131","                                        ensure_2d=False)","132","            if sample_weight.ndim != 1:","133","                raise ValueError(\"the shape of sample_weight must be ({0},),\"","134","                                 \" but was {1}\".format(X.shape[0],","135","                                                       sample_weight.shape))","136","            check_consistent_length(X, sample_weight)","137","            if sample_weight.min() <= 0:","138","                raise ValueError(\"sample_weight must have positive values\")","139","","145","                                          sample_weight=sample_weight,","167","        if self.tree_.sample_weight is None:","168","            N = self.tree_.data.shape[0]","169","        else:","170","            N = self.tree_.sum_weight","222","        u = rng.uniform(0, 1, size=n_samples)","223","        if self.tree_.sample_weight is None:","224","            i = (u * data.shape[0]).astype(np.int64)","225","        else:","226","            cumsum_weight = np.cumsum(np.asarray(self.tree_.sample_weight))","227","            sum_weight = cumsum_weight[-1]","228","            i = np.searchsorted(cumsum_weight, u * sum_weight)"],"delete":["9","from ..utils import check_array, check_random_state","114","    def fit(self, X, y=None):","152","        N = self.tree_.data.shape[0]","204","        i = rng.randint(data.shape[0], size=n_samples)","205",""]}],"sklearn\/neighbors\/ball_tree.pyx":[{"add":["64","    cdef bint with_sample_weight = tree.sample_weight is not None","65","    cdef DTYPE_t* sample_weight","66","    cdef DTYPE_t sum_weight_node","67","    if with_sample_weight:","68","        sample_weight = &tree.sample_weight[0]","69","","74","    if with_sample_weight:","75","        sum_weight_node = 0","76","        for i in range(idx_start, idx_end):","77","            sum_weight_node += sample_weight[idx_array[i]]","78","            this_pt = data + n_features * idx_array[i]","79","            for j from 0 <= j < n_features:","80","                centroid[j] += this_pt[j] * sample_weight[idx_array[i]]","82","        for j in range(n_features):","83","            centroid[j] \/= sum_weight_node","84","    else:","85","        for i in range(idx_start, idx_end):","86","            this_pt = data + n_features * idx_array[i]","87","            for j from 0 <= j < n_features:","88","                centroid[j] += this_pt[j]","89","","90","        for j in range(n_features):","91","            centroid[j] \/= n_points"],"delete":["68","    for i in range(idx_start, idx_end):","69","        this_pt = data + n_features * idx_array[i]","70","        for j from 0 <= j < n_features:","71","            centroid[j] += this_pt[j]","73","    for j in range(n_features):","74","        centroid[j] \/= n_points"]}],"sklearn\/neighbors\/tests\/test_kde.py":[{"add":["139","    kde = KernelDensity()","140","    assert_raises(ValueError, kde.fit, np.random.random((200, 10)),","141","                  sample_weight=np.random.random((200, 10)))","142","    assert_raises(ValueError, kde.fit, np.random.random((200, 10)),","143","                  sample_weight=-np.random.random(200))","156","","157","","158","def test_kde_sample_weights():","159","    n_samples = 400","160","    size_test = 20","161","    weights_neutral = 3 * np.ones(n_samples)","162","    for d in [1, 2, 10]:","163","        rng = np.random.RandomState(0)","164","        X = rng.rand(n_samples, d)","165","        weights = 1 + (10 * X.sum(axis=1)).astype(np.int8)","166","        X_repetitions = np.repeat(X, weights, axis=0)","167","        n_samples_test = size_test \/\/ d","168","        test_points = rng.rand(n_samples_test, d)","169","        for algorithm in ['auto', 'ball_tree', 'kd_tree']:","170","            for metric in ['euclidean', 'minkowski', 'manhattan',","171","                           'chebyshev']:","172","                if algorithm != 'kd_tree' or metric in KDTree.valid_metrics:","173","                    kde = KernelDensity(algorithm=algorithm, metric=metric)","174","","175","                    # Test that adding a constant sample weight has no effect","176","                    kde.fit(X, sample_weight=weights_neutral)","177","                    scores_const_weight = kde.score_samples(test_points)","178","                    sample_const_weight = kde.sample(random_state=1234)","179","                    kde.fit(X)","180","                    scores_no_weight = kde.score_samples(test_points)","181","                    sample_no_weight = kde.sample(random_state=1234)","182","                    assert_allclose(scores_const_weight, scores_no_weight)","183","                    assert_allclose(sample_const_weight, sample_no_weight)","184","","185","                    # Test equivalence between sampling and (integer) weights","186","                    kde.fit(X, sample_weight=weights)","187","                    scores_weight = kde.score_samples(test_points)","188","                    sample_weight = kde.sample(random_state=1234)","189","                    kde.fit(X_repetitions)","190","                    scores_ref_sampling = kde.score_samples(test_points)","191","                    sample_ref_sampling = kde.sample(random_state=1234)","192","                    assert_allclose(scores_weight, scores_ref_sampling)","193","                    assert_allclose(sample_weight, sample_ref_sampling)","194","","195","                    # Test that sample weights has a non-trivial effect","196","                    diff = np.max(np.abs(scores_no_weight - scores_weight))","197","                    assert diff > 0.001","198","","199","                    # Test invariance with respect to arbitrary scaling","200","                    scale_factor = rng.rand()","201","                    kde.fit(X, sample_weight=(scale_factor * weights))","202","                    scores_scaled_weight = kde.score_samples(test_points)","203","                    assert_allclose(scores_scaled_weight, scores_weight)"],"delete":[]}]}},"65489cacc4b3abb090a0fa9b22d8d4c7735539d8":{"changes":{"doc\/modules\/covariance.rst":"MODIFY"},"diff":{"doc\/modules\/covariance.rst":[{"add":["9","Many statistical problems require the estimation of a","13","have a large influence on the estimation's quality. The","14","`sklearn.covariance` package provides tools for accurately estimating","15","a population's covariance matrix under various settings.","25","by the classical *maximum likelihood estimator* (or \"empirical","29","sample is an unbiased estimator of the corresponding population's","35",":meth:`EmpiricalCovariance.fit` method. Be careful that results depend","36","on whether the data are centered, so one may want to use the","37","``assume_centered`` parameter accurately. More precisely, if","38","``assume_centered=False``, then the test set is supposed to have the","39","same mean vector as the training set. If not, both should be centered","40","by the user, and ``assume_centered=True`` should be used.","65","In scikit-learn, this transformation (with a user-defined shrinkage","69","and its :meth:`ShrunkCovariance.fit` method. Again, results depend on","70","whether the data are centered, so one may want to use the","71","``assume_centered`` parameter accurately.","75","smallest and the largest eigenvalues of the empirical covariance matrix.","96","In their 2004 paper [1]_, O. Ledoit and M. Wolf propose a formula","191","the precision matrix will be zero. This is why it makes sense to","192","estimate a sparse precision matrix: the estimation of the covariance","193","matrix is better conditioned by learning independence relations from","194","the data. This is known as *covariance selection*.","274","Real data sets are often subject to measurement or recording","276","of reasons. Observations which are very uncommon are called","277","outliers.","280","outliers in the data. Therefore, one should use robust"],"delete":["9","Many statistical problems require at some point the estimation of a","13","has a large influence on the estimation's quality. The","14","`sklearn.covariance` package aims at providing tools affording","15","an accurate estimation of a population's covariance matrix under","16","various settings.","26","with the classical *maximum likelihood estimator* (or \"empirical","30","sample is an unbiased estimator of the corresponding population","36",":meth:`EmpiricalCovariance.fit` method.  Be careful that depending","37","whether the data are centered or not, the result will be different, so","38","one may want to use the ``assume_centered`` parameter accurately. More precisely","39","if one uses ``assume_centered=False``, then the test set is supposed to have the","40","same mean vector as the training set. If not so, both should be centered by the","41","user, and ``assume_centered=True`` should be used.","66","In the scikit-learn, this transformation (with a user-defined shrinkage","70","and its :meth:`ShrunkCovariance.fit` method.  Again, depending whether","71","the data are centered or not, the result will be different, so one may","72","want to use the ``assume_centered`` parameter accurately.","76","smallest and the largest eigenvalue of the empirical covariance matrix.","97","In their 2004 paper [1]_, O. Ledoit and M. Wolf propose a formula so as","192","the precision matrix will be zero. This is why it makes sense to estimate","193","a sparse precision matrix: by learning independence relations from the","194","data, the estimation of the covariance matrix is better conditioned. This","195","is known as *covariance selection*.","275","Real data set are often subjects to measurement or recording","277","of reason. Every observation which is very uncommon is called an","278","outlier.","281","outlying observations in the data. Therefore, one should use robust"]}]}},"727314b2feda7e9c4c7de460eff8a360def0e440":{"changes":{"sklearn\/preprocessing\/tests\/test_data.py":"MODIFY","sklearn\/preprocessing\/data.py":"MODIFY"},"diff":{"sklearn\/preprocessing\/tests\/test_data.py":[{"add":["2113","    assert np.issubdtype(enc.categories_[0].dtype, np.str_)","2127","    assert np.issubdtype(enc.categories_[0].dtype, np.str_)","2229","def test_categorical_encoder_warning():","2230","    enc = CategoricalEncoder()","2231","    X = [['Male', 1], ['Female', 3]]","2232","    np.testing.assert_no_warnings(enc.fit_transform, X)","2233","","2234",""],"delete":["2113","    assert np.issubdtype(enc.categories_[0].dtype, str)","2127","    assert np.issubdtype(enc.categories_[0].dtype, str)"]}],"sklearn\/preprocessing\/data.py":[{"add":["2998","        if not hasattr(X, 'dtype') and np.issubdtype(X_temp.dtype, np.str_):","3041","        if not hasattr(X, 'dtype') and np.issubdtype(X_temp.dtype, np.str_):"],"delete":["2998","        if not hasattr(X, 'dtype') and np.issubdtype(X_temp.dtype, str):","3041","        if not hasattr(X, 'dtype') and np.issubdtype(X_temp.dtype, str):"]}]}},"2afcc3692633dc80dc5454dd0eef3ae077b4c4cf":{"changes":{"examples\/gaussian_process\/plot_gpr_noisy_targets.py":"MODIFY"},"diff":{"examples\/gaussian_process\/plot_gpr_noisy_targets.py":[{"add":["87","# Instantiate a Gaussian Process model","88","gp = GaussianProcessRegressor(kernel=kernel, alpha=dy ** 2,"],"delete":["87","# Instanciate a Gaussian Process model","88","gp = GaussianProcessRegressor(kernel=kernel, alpha=(dy \/ y) ** 2,"]}]}},"2242c59fc890455bd121e4a03375c5632f31ef93":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/ensemble\/tests\/test_forest.py":"MODIFY","examples\/ensemble\/plot_voting_probas.py":"MODIFY","sklearn\/ensemble\/tests\/test_voting_classifier.py":"MODIFY","sklearn\/tests\/test_calibration.py":"MODIFY","sklearn\/utils\/estimator_checks.py":"MODIFY","examples\/ensemble\/plot_random_forest_regression_multioutput.py":"MODIFY","sklearn\/feature_selection\/tests\/test_rfe.py":"MODIFY","sklearn\/feature_selection\/tests\/test_from_model.py":"MODIFY","sklearn\/ensemble\/forest.py":"MODIFY","sklearn\/ensemble\/tests\/test_weight_boosting.py":"MODIFY","examples\/applications\/plot_prediction_latency.py":"MODIFY","examples\/ensemble\/plot_ensemble_oob.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["735","Classifiers and regressors","736","","737","- The default value of the ``n_estimators`` parameter of ","738","  :class:`ensemble.RandomForestClassifier`, :class:`ensemble.RandomForestRegressor`, ","739","  :class:`ensemble.ExtraTreesClassifier`, :class:`ensemble.ExtraTreesRegressor`, ","740","  and :class:`ensemble.RandomTreesEmbedding` will change from 10 in version 0.20 ","741","  to 100 in 0.22. A FutureWarning is raised when the default value is used.","742","  :issue:`11542` by :user:`Anna Ayzenshtat <annaayzenshtat>`.","743",""],"delete":["24",""]}],"sklearn\/ensemble\/tests\/test_forest.py":[{"add":["33","from sklearn.utils.testing import assert_no_warnings","189","@pytest.mark.filterwarnings('ignore:The default value of n_estimators')","436","@pytest.mark.filterwarnings('ignore:The default value of n_estimators')","494","@pytest.mark.filterwarnings('ignore:The default value of n_estimators')","532","@pytest.mark.filterwarnings('ignore:The default value of n_estimators')","556","@pytest.mark.filterwarnings('ignore:The default value of n_estimators')","746","@pytest.mark.filterwarnings('ignore:The default value of n_estimators')","784","@pytest.mark.filterwarnings('ignore:The default value of n_estimators')","852","@pytest.mark.filterwarnings('ignore:The default value of n_estimators')","910","@pytest.mark.filterwarnings('ignore:The default value of n_estimators')","989","@pytest.mark.filterwarnings('ignore:The default value of n_estimators')","1009","@pytest.mark.filterwarnings('ignore:The default value of n_estimators')","1040","@pytest.mark.filterwarnings('ignore:The default value of n_estimators')","1178","@pytest.mark.filterwarnings('ignore:The default value of n_estimators')","1217","@pytest.mark.filterwarnings('ignore:The default value of n_estimators')","1233","@pytest.mark.filterwarnings('ignore:The default value of n_estimators')","1246","","1247","","1248","@pytest.mark.parametrize('forest',","1249","                         [RandomForestClassifier, RandomForestRegressor,","1250","                          ExtraTreesClassifier, ExtraTreesRegressor,","1251","                          RandomTreesEmbedding])","1252","def test_nestimators_future_warning(forest):","1253","    # FIXME: to be removed 0.22","1254","","1255","    # When n_estimators default value is used","1256","    msg_future = (\"The default value of n_estimators will change from \"","1257","                  \"10 in version 0.20 to 100 in 0.22.\")","1258","    est = forest()","1259","    est = assert_warns_message(FutureWarning, msg_future, est.fit, X, y)","1260","","1261","    # When n_estimators is a valid value not equal to the default","1262","    est = forest(n_estimators=100)","1263","    est = assert_no_warnings(est.fit, X, y)"],"delete":[]}],"examples\/ensemble\/plot_voting_probas.py":[{"add":["32","clf2 = RandomForestClassifier(n_estimators=100, random_state=123)"],"delete":["32","clf2 = RandomForestClassifier(random_state=123)"]}],"sklearn\/ensemble\/tests\/test_voting_classifier.py":[{"add":["2","import pytest","4","","78","@pytest.mark.filterwarnings('ignore:The default value of n_estimators')","91","@pytest.mark.filterwarnings('ignore:The default value of n_estimators')","103","@pytest.mark.filterwarnings('ignore:The default value of n_estimators')","117","@pytest.mark.filterwarnings('ignore:The default value of n_estimators')","150","@pytest.mark.filterwarnings('ignore:The default value of n_estimators')","218","@pytest.mark.filterwarnings('ignore:The default value of n_estimators')","236","@pytest.mark.filterwarnings('ignore:The default value of n_estimators')","258","@pytest.mark.filterwarnings('ignore:The default value of n_estimators')","302","@pytest.mark.filterwarnings('ignore:The default value of n_estimators')","337","@pytest.mark.filterwarnings('ignore:The default value of n_estimators')","390","@pytest.mark.filterwarnings('ignore:The default value of n_estimators')","408","@pytest.mark.filterwarnings('ignore:The default value of n_estimators')"],"delete":[]}],"sklearn\/tests\/test_calibration.py":[{"add":["4","import pytest","27","@pytest.mark.filterwarnings('ignore:The default value of n_estimators')"],"delete":["26","@ignore_warnings"]}],"sklearn\/utils\/estimator_checks.py":[{"add":["344","        # FIXME: The default number of trees was changed and is set to 'warn'","345","        # for some of the ensemble methods. We need to catch this case to avoid","346","        # an error during the comparison. To be reverted in 0.22.","347","        if estimator.n_estimators == 'warn':","348","            estimator.set_params(n_estimators=5)","349","        else:","350","            estimator.set_params(n_estimators=min(5, estimator.n_estimators))"],"delete":["344","        estimator.set_params(n_estimators=min(5, estimator.n_estimators))"]}],"examples\/ensemble\/plot_random_forest_regression_multioutput.py":[{"add":["45","regr_multirf = MultiOutputRegressor(RandomForestRegressor(n_estimators=100,","46","                                                          max_depth=max_depth,","50","regr_rf = RandomForestRegressor(n_estimators=100, max_depth=max_depth,","51","                                random_state=2)"],"delete":["45","regr_multirf = MultiOutputRegressor(RandomForestRegressor(max_depth=max_depth,","49","regr_rf = RandomForestRegressor(max_depth=max_depth, random_state=2)"]}],"sklearn\/feature_selection\/tests\/test_rfe.py":[{"add":["3","import pytest","339","@pytest.mark.filterwarnings('ignore:The default value of n_estimators')"],"delete":[]}],"sklearn\/feature_selection\/tests\/test_from_model.py":[{"add":["37","@pytest.mark.filterwarnings('ignore:The default value of n_estimators')","241","@pytest.mark.filterwarnings('ignore:The default value of n_estimators')"],"delete":[]}],"sklearn\/ensemble\/forest.py":[{"add":["137","                 n_estimators=100,","244","","245","        if self.n_estimators == 'warn':","246","            warnings.warn(\"The default value of n_estimators will change from \"","247","                          \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)","248","            self.n_estimators = 10","249","","408","                 n_estimators=100,","647","                 n_estimators=100,","767","        .. versionchanged:: 0.20","768","           The default value of ``n_estimators`` will change from 10 in","769","           version 0.20 to 100 in version 0.22.","770","","984","                 n_estimators='warn',","1045","        .. versionchanged:: 0.20","1046","           The default value of ``n_estimators`` will change from 10 in","1047","           version 0.20 to 100 in version 0.22.","1048","","1228","                 n_estimators='warn',","1285","        .. versionchanged:: 0.20","1286","           The default value of ``n_estimators`` will change from 10 in","1287","           version 0.20 to 100 in version 0.22.","1288","","1475","                 n_estimators='warn',","1534","        .. versionchanged:: 0.20","1535","           The default value of ``n_estimators`` will change from 10 in","1536","           version 0.20 to 100 in version 0.22.","1537","","1691","                 n_estimators='warn',","1753","        .. versionchanged:: 0.20","1754","           The default value of ``n_estimators`` will change from 10 in","1755","           version 0.20 to 100 in version 0.22.","1756","","1859","                 n_estimators='warn',"],"delete":["137","                 n_estimators=10,","402","                 n_estimators=10,","411","","642","                 n_estimators=10,","975","                 n_estimators=10,","1215","                 n_estimators=10,","1458","                 n_estimators=10,","1670","                 n_estimators=10,","1834","                 n_estimators=10,"]}],"sklearn\/ensemble\/tests\/test_weight_boosting.py":[{"add":["2","import pytest","4","","281","@pytest.mark.filterwarnings('ignore:The default value of n_estimators')"],"delete":[]}],"examples\/applications\/plot_prediction_latency.py":[{"add":["287","         'instance': RandomForestRegressor(n_estimators=100),"],"delete":["287","         'instance': RandomForestRegressor(),"]}],"examples\/ensemble\/plot_ensemble_oob.py":[{"add":["47","        RandomForestClassifier(n_estimators=100,","48","                               warm_start=True, oob_score=True,","52","        RandomForestClassifier(n_estimators=100,","53","                               warm_start=True, max_features='log2',","57","        RandomForestClassifier(n_estimators=100,","58","                               warm_start=True, max_features=None,"],"delete":["47","        RandomForestClassifier(warm_start=True, oob_score=True,","51","        RandomForestClassifier(warm_start=True, max_features='log2',","55","        RandomForestClassifier(warm_start=True, max_features=None,"]}]}},"41d648ef8863e6cadcb6c0a13f66dda7d0e9ea89":{"changes":{"sklearn\/metrics\/_ranking.py":"MODIFY","sklearn\/metrics\/tests\/test_common.py":"MODIFY","doc\/modules\/classes.rst":"MODIFY","doc\/modules\/model_evaluation.rst":"MODIFY","sklearn\/metrics\/__init__.py":"MODIFY","examples\/model_selection\/plot_det.py":"ADD","doc\/whats_new\/v0.24.rst":"MODIFY","sklearn\/metrics\/tests\/test_ranking.py":"MODIFY"},"diff":{"sklearn\/metrics\/_ranking.py":[{"add":["220","def detection_error_tradeoff_curve(y_true, y_score, pos_label=None,","221","                                   sample_weight=None):","222","    \"\"\"Compute error rates for different probability thresholds.","223","","224","    Note: This metrics is used for ranking evaluation of a binary","225","    classification task.","226","","227","    Read more in the :ref:`User Guide <det_curve>`.","228","","229","    Parameters","230","    ----------","231","    y_true : array, shape = [n_samples]","232","        True targets of binary classification in range {-1, 1} or {0, 1}.","233","","234","    y_score : array, shape = [n_samples]","235","        Estimated probabilities or decision function.","236","","237","    pos_label : int, optional (default=None)","238","        The label of the positive class","239","","240","    sample_weight : array-like of shape = [n_samples], optional","241","        Sample weights.","242","","243","    Returns","244","    -------","245","    fpr : array, shape = [n_thresholds]","246","        False positive rate (FPR) such that element i is the false positive","247","        rate of predictions with score >= thresholds[i]. This is occasionally","248","        referred to as false acceptance propability or fall-out.","249","","250","    fnr : array, shape = [n_thresholds]","251","        False negative rate (FNR) such that element i is the false negative","252","        rate of predictions with score >= thresholds[i]. This is occasionally","253","        referred to as false rejection or miss rate.","254","","255","    thresholds : array, shape = [n_thresholds]","256","        Decreasing score values.","257","","258","    See also","259","    --------","260","    roc_curve : Compute Receiver operating characteristic (ROC) curve","261","    precision_recall_curve : Compute precision-recall curve","262","","263","    Examples","264","    --------","265","    >>> import numpy as np","266","    >>> from sklearn.metrics import detection_error_tradeoff_curve","267","    >>> y_true = np.array([0, 0, 1, 1])","268","    >>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])","269","    >>> fpr, fnr, thresholds = detection_error_tradeoff_curve(y_true, y_scores)","270","    >>> fpr","271","    array([0.5, 0.5, 0. ])","272","    >>> fnr","273","    array([0. , 0.5, 0.5])","274","    >>> thresholds","275","    array([0.35, 0.4 , 0.8 ])","276","","277","    \"\"\"","278","    if len(np.unique(y_true)) != 2:","279","        raise ValueError(\"Only one class present in y_true. Detection error \"","280","                         \"tradeoff curve is not defined in that case.\")","281","","282","    fps, tps, thresholds = _binary_clf_curve(y_true, y_score,","283","                                             pos_label=pos_label,","284","                                             sample_weight=sample_weight)","285","","286","    fns = tps[-1] - tps","287","    p_count = tps[-1]","288","    n_count = fps[-1]","289","","290","    # start with false positives zero","291","    first_ind = (","292","        fps.searchsorted(fps[0], side='right') - 1","293","        if fps.searchsorted(fps[0], side='right') > 0","294","        else None","295","    )","296","    # stop with false negatives zero","297","    last_ind = tps.searchsorted(tps[-1]) + 1","298","    sl = slice(first_ind, last_ind)","299","","300","    # reverse the output such that list of false positives is decreasing","301","    return (","302","        fps[sl][::-1] \/ n_count,","303","        fns[sl][::-1] \/ p_count,","304","        thresholds[sl][::-1]","305","    )","306","","307",""],"delete":[]}],"sklearn\/metrics\/tests\/test_common.py":[{"add":["31","from sklearn.metrics import detection_error_tradeoff_curve","208","    \"detection_error_tradeoff_curve\": detection_error_tradeoff_curve,","305","    \"detection_error_tradeoff_curve\",","327","    \"detection_error_tradeoff_curve\",","358","    \"detection_error_tradeoff_curve\",","471","    \"detection_error_tradeoff_curve\","],"delete":[]}],"doc\/modules\/classes.rst":[{"add":["948","   metrics.detection_error_tradeoff_curve"],"delete":[]}],"doc\/modules\/model_evaluation.rst":[{"add":["308","   detection_error_tradeoff_curve","1440",".. _det_curve:","1441","","1442","Detection error tradeoff (DET)","1443","------------------------------","1444","","1445","The function :func:`detection_error_tradeoff_curve` computes the","1446","detection error tradeoff curve (DET) curve [WikipediaDET2017]_.","1447","Quoting Wikipedia:","1448","","1449","  \"A detection error tradeoff (DET) graph is a graphical plot of error rates for","1450","  binary classification systems, plotting false reject rate vs. false accept","1451","  rate. The x- and y-axes are scaled non-linearly by their standard normal","1452","  deviates (or just by logarithmic transformation), yielding tradeoff curves","1453","  that are more linear than ROC curves, and use most of the image area to","1454","  highlight the differences of importance in the critical operating region.\"","1455","","1456","DET curves are a variation of receiver operating characteristic (ROC) curves","1457","where False Negative Rate is plotted on the ordinate instead of True Positive","1458","Rate.","1459","DET curves are commonly plotted in normal deviate scale by transformation with","1460",":math:`\\phi^{-1}` (with :math:`\\phi` being the cumulative distribution","1461","function).","1462","The resulting performance curves explicitly visualize the tradeoff of error","1463","types for given classification algorithms.","1464","See [Martin1997]_ for examples and further motivation.","1465","","1466","This figure compares the ROC and DET curves of two example classifiers on the","1467","same classification task:","1468","","1469",".. image:: ..\/auto_examples\/model_selection\/images\/sphx_glr_plot_det_001.png","1470","   :target: ..\/auto_examples\/model_selection\/plot_det.html","1471","   :scale: 75","1472","   :align: center","1473","","1474","**Properties:**","1475","","1476","* DET curves form a linear curve in normal deviate scale if the detection","1477","  scores are normally (or close-to normally) distributed.","1478","  It was shown by [Navratil2007]_ that the reverse it not necessarily true and even more","1479","  general distributions are able produce linear DET curves.","1480","","1481","* The normal deviate scale transformation spreads out the points such that a","1482","  comparatively larger space of plot is occupied.","1483","  Therefore curves with similar classification performance might be easier to","1484","  distinguish on a DET plot.","1485","","1486","* With False Negative Rate being \"inverse\" to True Positive Rate the point","1487","  of perfection for DET curves is the origin (in contrast to the top left corner","1488","  for ROC curves).","1489","","1490","**Applications and limitations:**","1491","","1492","DET curves are intuitive to read and hence allow quick visual assessment of a","1493","classifier's performance.","1494","Additionally DET curves can be consulted for threshold analysis and operating","1495","point selection.","1496","This is particularly helpful if a comparison of error types is required.","1497","","1498","One the other hand DET curves do not provide their metric as a single number.","1499","Therefore for either automated evaluation or comparison to other","1500","classification tasks metrics like the derived area under ROC curve might be","1501","better suited.","1502","","1503",".. topic:: Examples:","1504","","1505","  * See :ref:`sphx_glr_auto_examples_model_selection_plot_det.py`","1506","    for an example comparison between receiver operating characteristic (ROC)","1507","    curves and Detection error tradeoff (DET) curves.","1508","","1509",".. topic:: References:","1510","","1511","  .. [WikipediaDET2017] Wikipedia contributors. Detection error tradeoff.","1512","     Wikipedia, The Free Encyclopedia. September 4, 2017, 23:33 UTC.","1513","     Available at: https:\/\/en.wikipedia.org\/w\/index.php?title=Detection_error_tradeoff&oldid=798982054.","1514","     Accessed February 19, 2018.","1515","","1516","  .. [Martin1997] A. Martin, G. Doddington, T. Kamm, M. Ordowski, and M. Przybocki,","1517","     `The DET Curve in Assessment of Detection Task Performance","1518","     <http:\/\/www.dtic.mil\/docs\/citations\/ADA530509>`_,","1519","     NIST 1997.","1520","","1521","  .. [Navratil2007] J. Navractil and D. Klusacek,","1522","     \"`On Linear DETs,","1523","     <http:\/\/www.research.ibm.com\/CBG\/papers\/icassp07_navratil.pdf>`_\"","1524","     2007 IEEE International Conference on Acoustics,","1525","     Speech and Signal Processing - ICASSP '07, Honolulu,","1526","     HI, 2007, pp. IV-229-IV-232."],"delete":[]}],"sklearn\/metrics\/__init__.py":[{"add":["9","from ._ranking import detection_error_tradeoff_curve","107","    'detection_error_tradeoff_curve',"],"delete":[]}],"examples\/model_selection\/plot_det.py":[{"add":[],"delete":[]}],"doc\/whats_new\/v0.24.rst":[{"add":["272","- |Feature| Added :func:`metrics.detection_error_tradeoff_curve` to compute","273","  Detection Error Tradeoff curve classification metric.","274","  :pr:`10591` by :user:`Jeremy Karnowski <jkarnows>` and","275","  :user:`Daniel Mohns <dmohns>`.","276",""],"delete":[]}],"sklearn\/metrics\/tests\/test_ranking.py":[{"add":["18","from sklearn.utils._testing import assert_raises","24","from sklearn.metrics import detection_error_tradeoff_curve","929","@pytest.mark.parametrize(\"y_true,y_score,expected_fpr,expected_fnr\", [","930","    ([0, 0, 1], [0, 0.5, 1], [0], [0]),","931","    ([0, 0, 1], [0, 0.25, 0.5], [0], [0]),","932","    ([0, 0, 1], [0.5, 0.75, 1], [0], [0]),","933","    ([0, 0, 1], [0.25, 0.5, 0.75], [0], [0]),","934","    ([0, 1, 0], [0, 0.5, 1], [0.5], [0]),","935","    ([0, 1, 0], [0, 0.25, 0.5], [0.5], [0]),","936","    ([0, 1, 0], [0.5, 0.75, 1], [0.5], [0]),","937","    ([0, 1, 0], [0.25, 0.5, 0.75], [0.5], [0]),","938","    ([0, 1, 1], [0, 0.5, 1], [0.0], [0]),","939","    ([0, 1, 1], [0, 0.25, 0.5], [0], [0]),","940","    ([0, 1, 1], [0.5, 0.75, 1], [0], [0]),","941","    ([0, 1, 1], [0.25, 0.5, 0.75], [0], [0]),","942","    ([1, 0, 0], [0, 0.5, 1], [1, 1, 0.5], [0, 1, 1]),","943","    ([1, 0, 0], [0, 0.25, 0.5], [1, 1, 0.5], [0, 1, 1]),","944","    ([1, 0, 0], [0.5, 0.75, 1], [1, 1, 0.5], [0, 1, 1]),","945","    ([1, 0, 0], [0.25, 0.5, 0.75], [1, 1, 0.5], [0, 1, 1]),","946","    ([1, 0, 1], [0, 0.5, 1], [1, 1, 0], [0, 0.5, 0.5]),","947","    ([1, 0, 1], [0, 0.25, 0.5], [1, 1, 0], [0, 0.5, 0.5]),","948","    ([1, 0, 1], [0.5, 0.75, 1], [1, 1, 0], [0, 0.5, 0.5]),","949","    ([1, 0, 1], [0.25, 0.5, 0.75], [1, 1, 0], [0, 0.5, 0.5]),","950","])","951","def test_detection_error_tradeoff_curve_toydata(y_true, y_score,","952","                                                expected_fpr, expected_fnr):","953","    # Check on a batch of small examples.","954","    fpr, fnr, _ = detection_error_tradeoff_curve(y_true, y_score)","955","","956","    assert_array_almost_equal(fpr, expected_fpr)","957","    assert_array_almost_equal(fnr, expected_fnr)","958","","959","","960","@pytest.mark.parametrize(\"y_true,y_score,expected_fpr,expected_fnr\", [","961","    ([1, 0], [0.5, 0.5], [1], [0]),","962","    ([0, 1], [0.5, 0.5], [1], [0]),","963","    ([0, 0, 1], [0.25, 0.5, 0.5], [0.5], [0]),","964","    ([0, 1, 0], [0.25, 0.5, 0.5], [0.5], [0]),","965","    ([0, 1, 1], [0.25, 0.5, 0.5], [0], [0]),","966","    ([1, 0, 0], [0.25, 0.5, 0.5], [1], [0]),","967","    ([1, 0, 1], [0.25, 0.5, 0.5], [1], [0]),","968","    ([1, 1, 0], [0.25, 0.5, 0.5], [1], [0]),","969","])","970","def test_detection_error_tradeoff_curve_tie_handling(y_true, y_score,","971","                                                     expected_fpr,","972","                                                     expected_fnr):","973","    fpr, fnr, _ = detection_error_tradeoff_curve(y_true, y_score)","974","","975","    assert_array_almost_equal(fpr, expected_fpr)","976","    assert_array_almost_equal(fnr, expected_fnr)","977","","978","","979","def test_detection_error_tradeoff_curve_sanity_check():","980","    # Exactly duplicated inputs yield the same result.","981","    assert_array_almost_equal(","982","        detection_error_tradeoff_curve([0, 0, 1], [0, 0.5, 1]),","983","        detection_error_tradeoff_curve(","984","            [0, 0, 0, 0, 1, 1], [0, 0, 0.5, 0.5, 1, 1])","985","    )","986","","987","","988","@pytest.mark.parametrize(\"y_score\", [","989","    (0), (0.25), (0.5), (0.75), (1)","990","])","991","def test_detection_error_tradeoff_curve_constant_scores(y_score):","992","    fpr, fnr, threshold = detection_error_tradeoff_curve(","993","        y_true=[0, 1, 0, 1, 0, 1],","994","        y_score=np.full(6, y_score)","995","    )","996","","997","    assert_array_almost_equal(fpr, [1])","998","    assert_array_almost_equal(fnr, [0])","999","    assert_array_almost_equal(threshold, [y_score])","1000","","1001","","1002","@pytest.mark.parametrize(\"y_true\", [","1003","    ([0, 0, 0, 0, 0, 1]),","1004","    ([0, 0, 0, 0, 1, 1]),","1005","    ([0, 0, 0, 1, 1, 1]),","1006","    ([0, 0, 1, 1, 1, 1]),","1007","    ([0, 1, 1, 1, 1, 1]),","1008","])","1009","def test_detection_error_tradeoff_curve_perfect_scores(y_true):","1010","    fpr, fnr, _ = detection_error_tradeoff_curve(","1011","        y_true=y_true,","1012","        y_score=y_true","1013","    )","1014","","1015","    assert_array_almost_equal(fpr, [0])","1016","    assert_array_almost_equal(fnr, [0])","1017","","1018","","1019","def test_detection_error_tradeoff_curve_bad_input():","1020","    # input variables with inconsistent numbers of samples","1021","    assert_raises(ValueError, detection_error_tradeoff_curve,","1022","                  [0, 1], [0, 0.5, 1])","1023","    assert_raises(ValueError, detection_error_tradeoff_curve,","1024","                  [0, 1, 1], [0, 0.5])","1025","","1026","    # When the y_true values are all the same a detection error tradeoff cannot","1027","    # be computed.","1028","    assert_raises(ValueError, detection_error_tradeoff_curve,","1029","                  [0, 0, 0], [0, 0.5, 1])","1030","    assert_raises(ValueError, detection_error_tradeoff_curve,","1031","                  [1, 1, 1], [0, 0.5, 1])","1032","","1033",""],"delete":[]}]}},"4989a9503753a92089f39e154a2bb5d160b5d276":{"changes":{"doc\/conf.py":"MODIFY","\/dev\/null":"DELETE","examples\/applications\/skip_stock_market.py":"ADD"},"diff":{"doc\/conf.py":[{"add":["245","        'sklearn': None}"],"delete":["245","        'sklearn': None},","246","    'expected_failing_examples':","247","        ['..\/examples\/applications\/plot_stock_market.py'],"]}],"\/dev\/null":[{"add":[],"delete":[]}],"examples\/applications\/skip_stock_market.py":[{"add":[],"delete":[]}]}},"eec069794b5c67ca793820b31df3c55a7c5b4ce6":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/linear_model\/cd_fast.pyx":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["341","- Fixed condition triggering gap computation in :class:`linear_model.Lasso`","342","  and :class:`linear_model.ElasticNet` when working with sparse matrices.","343","  :issue:`10992` by `Alexandre Gramfort`_.","344",""],"delete":[]}],"sklearn\/linear_model\/cd_fast.pyx":[{"add":["482","                if fabs(w[ii]) > w_max:","483","                    w_max = fabs(w[ii])","484",""],"delete":["482","                if w[ii] > w_max:","483","                    w_max = w[ii]"]}]}},"f485a9edf92e009f3b898e35dfbf5daea433d74f":{"changes":{"sklearn\/linear_model\/stochastic_gradient.py":"MODIFY","sklearn\/linear_model\/tests\/test_sgd.py":"MODIFY"},"diff":{"sklearn\/linear_model\/stochastic_gradient.py":[{"add":["84","    def _validate_params(self, set_max_iter=True, for_partial_fit=False):","122","            if not for_partial_fit:","123","                warnings.warn(","124","                    \"max_iter and tol parameters have been \"","125","                    \"added in %s in 0.19. If both are left unset, \"","126","                    \"they default to max_iter=5 and tol=None. \"","127","                    \"If tol is not None, max_iter defaults to max_iter=1000. \"","128","                    \"From 0.21, default max_iter will be 1000, and\"","129","                    \" default tol will be 1e-3.\" % type(self), FutureWarning)","130","                # Before 0.19, default was n_iter=5","542","        self._validate_params(for_partial_fit=True)","987","        self._validate_params(for_partial_fit=True)"],"delete":["84","    def _validate_params(self, set_max_iter=True):","122","            warnings.warn(","123","                \"max_iter and tol parameters have been added in %s in 0.19. If\"","124","                \" both are left unset, they default to max_iter=5 and tol=None\"","125","                \". If tol is not None, max_iter defaults to max_iter=1000. \"","126","                \"From 0.21, default max_iter will be 1000, \"","127","                \"and default tol will be 1e-3.\" % type(self).__name__,","128","                FutureWarning)","129","            # Before 0.19, default was n_iter=5","541","        self._validate_params()","986","        self._validate_params()"]}],"sklearn\/linear_model\/tests\/test_sgd.py":[{"add":["1196","    def init(max_iter=None, tol=None, n_iter=None, for_partial_fit=False):","1198","        sgd._validate_params(for_partial_fit=for_partial_fit)","1213","    # Test that for_partial_fit will not throw warnings for max_iter or tol","1214","    assert_no_warnings(init, None, None, None, True)","1215",""],"delete":["1196","    def init(max_iter=None, tol=None, n_iter=None):","1198","        sgd._validate_params()"]}]}},"f793f55310055b056be0cfcbac9169860065941c":{"changes":{"examples\/cluster\/plot_cluster_comparison.py":"MODIFY"},"diff":{"examples\/cluster\/plot_cluster_comparison.py":[{"add":["171","        # add black color for outliers (if any)","172","        colors = np.append(colors, [\"#000000\"])"],"delete":[]}]}},"beb2aa0395cdbc65843db9acf1a9fdfd25e18e7e":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/mixture\/tests\/test_gaussian_mixture.py":"MODIFY","sklearn\/mixture\/base.py":"MODIFY","sklearn\/mixture\/gaussian_mixture.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["62","- :class:`neural_network.BaseMultilayerPerceptron` (bug fix)","579","- Fixed a bug in :class:`mixture.BaseMixture` and its subclasses","580","  :class:`mixture.GaussianMixture` and :class:`mixture.BayesianGaussianMixture`","581","  where the ``lower_bound_`` was not the max lower bound across all","582","  initializations (when ``n_init > 1``), but just the lower bound of the last","583","  initialization. :issue:`10869` by :user:`Aurlien Gron <ageron>`.","584",""],"delete":["64","- :class:`neural_network.BaseMultilayerPerceptron` (bug fix)"]}],"sklearn\/mixture\/tests\/test_gaussian_mixture.py":[{"add":["807","@ignore_warnings(category=ConvergenceWarning)","808","def test_convergence_detected_with_warm_start():","809","    # We check that convergence is detected when warm_start=True","810","    rng = np.random.RandomState(0)","811","    rand_data = RandomData(rng)","812","    n_components = rand_data.n_components","813","    X = rand_data.X['full']","814","","815","    for max_iter in (1, 2, 50):","816","        gmm = GaussianMixture(n_components=n_components, warm_start=True,","817","                              max_iter=max_iter, random_state=rng)","818","        for _ in range(100):","819","            gmm.fit(X)","820","            if gmm.converged_:","821","                break","822","        assert gmm.converged_","823","        assert max_iter >= gmm.n_iter_","824","","825","","1011","    for random_state in range(25):","1012","        rand_data = RandomData(np.random.RandomState(random_state), scale=1)","1013","        n_components = rand_data.n_components","1014","        X = rand_data.X['full']","1016","        gmm1 = GaussianMixture(n_components=n_components, n_init=1,","1017","                               max_iter=1, random_state=random_state).fit(X)","1018","        gmm2 = GaussianMixture(n_components=n_components, n_init=10,","1019","                               max_iter=1, random_state=random_state).fit(X)","1021","        assert gmm2.lower_bound_ >= gmm1.lower_bound_"],"delete":["766","","993","    random_state = 0","994","    rand_data = RandomData(np.random.RandomState(random_state), scale=1)","995","    n_components = rand_data.n_components","996","    X = rand_data.X['full']","998","    gmm1 = GaussianMixture(n_components=n_components, n_init=1,","999","                           max_iter=1, random_state=random_state).fit(X)","1000","    gmm2 = GaussianMixture(n_components=n_components, n_init=100,","1001","                           max_iter=1, random_state=random_state).fit(X)","1003","    assert_greater(gmm2.lower_bound_, gmm1.lower_bound_)"]}],"sklearn\/mixture\/base.py":[{"add":["174","        The method fits the model ``n_init`` times and sets the parameters with","176","        trial, the method iterates between E-step and M-step for ``max_iter``","178","        ``tol``, otherwise, a ``ConvergenceWarning`` is raised.","179","        If ``warm_start`` is ``True``, then ``n_init`` is ignored and a single","180","        initialization is performed upon the first call. Upon consecutive","181","        calls, training starts where it left off.","237","","238","            lower_bound = (-np.infty if do_init else self.lower_bound_)","241","                prev_lower_bound = lower_bound","245","                lower_bound = self._compute_lower_bound(","248","                change = lower_bound - prev_lower_bound","255","            self._print_verbose_msg_init_end(lower_bound)","257","            if lower_bound > max_lower_bound:","258","                max_lower_bound = lower_bound","271","        self.lower_bound_ = max_lower_bound"],"delete":["174","        The method fits the model `n_init` times and set the parameters with","176","        trial, the method iterates between E-step and M-step for `max_iter`","178","        `tol`, otherwise, a `ConvergenceWarning` is raised.","234","                self.lower_bound_ = -np.infty","237","                prev_lower_bound = self.lower_bound_","241","                self.lower_bound_ = self._compute_lower_bound(","244","                change = self.lower_bound_ - prev_lower_bound","251","            self._print_verbose_msg_init_end(self.lower_bound_)","253","            if self.lower_bound_ > max_lower_bound:","254","                max_lower_bound = self.lower_bound_"]}],"sklearn\/mixture\/gaussian_mixture.py":[{"add":["514","        In that case, 'n_init' is ignored and only a single initialization","515","        occurs upon the first call.","579","        Lower bound value on the log-likelihood (of the training data with","580","        respect to the model) of the best fit of EM."],"delete":["577","        Log-likelihood of the best fit of EM."]}]}},"12c5ce25e885cded5d0d2beaae7589b878ed8672":{"changes":{"sklearn\/tree\/_splitter.pyx":"MODIFY"},"diff":{"sklearn\/tree\/_splitter.pyx":[{"add":["537","    if n == 0:","538","      return"],"delete":[]}]}},"aba87b7eb2ea2a4fa0ed9cfddae03e3b0c8d0162":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["98","- Fixed a bug in :class:`naive_bayes.GaussianNB` which incorrectly raised","99","  error for prior list which summed to 1.","100","  :issue:`10005` by :user:`Gaurav Dhingra <gxyd>`.","101",""],"delete":[]}]}},"331fb7bc68306e71fb18a0e08ed3b70b3fe6e4a9":{"changes":{"doc\/modules\/lda_qda.rst":"MODIFY"},"diff":{"doc\/modules\/lda_qda.rst":[{"add":["44","classes, so this is, in general, a rather strong dimensionality reduction, and"],"delete":["44","classes, so this is a in general a rather strong dimensionality reduction, and"]}]}},"9e1d9efad16378142b0c4ddd19969fca198a0f7c":{"changes":{"sklearn\/kernel_approximation.py":"MODIFY"},"diff":{"sklearn\/kernel_approximation.py":[{"add":["46","    Examples","47","    --------","48","    >>> from sklearn.kernel_approximation import RBFSampler","49","    >>> from sklearn.linear_model import SGDClassifier","50","    >>> X = [[0, 0], [1, 1], [1, 0], [0, 1]]","51","    >>> y = [0, 0, 1, 1]","52","    >>> rbf_feature = RBFSampler(gamma=1, random_state=1)","53","    >>> X_features = rbf_feature.fit_transform(X)","54","    >>> clf = SGDClassifier(max_iter=5)","55","    >>> clf.fit(X_features, y)","56","    ... # doctest: +NORMALIZE_WHITESPACE","57","    SGDClassifier(alpha=0.0001, average=False, class_weight=None,","58","           early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,","59","           l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=5,","60","           n_iter=None, n_iter_no_change=5, n_jobs=1, penalty='l2',","61","           power_t=0.5, random_state=None, shuffle=True, tol=None,","62","           validation_fraction=0.1, verbose=0, warm_start=False)","63","    >>> clf.score(X_features, y)","64","    1.0","65",""],"delete":[]}]}},"f8b09ce4bfa52af648a8239f4b206d237f6d6cfb":{"changes":{"sklearn\/linear_model\/tests\/test_omp.py":"MODIFY","doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/linear_model\/omp.py":"MODIFY"},"diff":{"sklearn\/linear_model\/tests\/test_omp.py":[{"add":["24","# Make X not of norm 1 for testing","25","X *= 10","26","y *= 10","118","    coef_normalized = omp.coef_[0].copy()","119","    omp.set_params(fit_intercept=True, normalize=False)","121","    assert_array_almost_equal(coef_normalized, omp.coef_)","122","","123","    omp.set_params(fit_intercept=False, normalize=False)","124","    omp.fit(X, y[:, 0])","125","    assert_true(np.count_nonzero(omp.coef_) <= n_nonzero_coefs)"],"delete":["115","    omp.set_params(fit_intercept=False, normalize=False)","116","","120","    assert_true(np.count_nonzero(omp.coef_) <= n_nonzero_coefs)"]}],"doc\/whats_new\/v0.20.rst":[{"add":["127","- Fixed a bug in :class:`linear_model.OrthogonalMatchingPursuit` that was","128","  broken when setting ``normalize=False``.","129","  by `Alexandre Gramfort`_.","130",""],"delete":[]}],"sklearn\/linear_model\/omp.py":[{"add":["8","from math import sqrt","103","","113","            Lkk = linalg.norm(X[:, lam]) ** 2 - v","114","            if Lkk <= min_float:  # selected atoms are dependent","117","            L[n_active, n_active] = sqrt(Lkk)","118","        else:","119","            L[0, 0] = linalg.norm(X[:, lam])","120","","125","","126","        # solves LL'x = X'y as a composition of two triangular systems","129","","238","            Lkk = Gram[lam, lam] - v","239","            if Lkk <= min_float:  # selected atoms are dependent","242","            L[n_active, n_active] = sqrt(Lkk)","243","        else:","244","            L[0, 0] = sqrt(Gram[lam, lam])","245","","251","        # solves LL'x = X'y as a composition of two triangular systems"],"delete":["93","    L[0, 0] = 1.","112","            if 1 - v <= min_float:  # selected atoms are dependent","115","            L[n_active, n_active] = np.sqrt(1 - v)","120","        # solves LL'x = y as a composition of two triangular systems","231","            if 1 - v <= min_float:  # selected atoms are dependent","234","            L[n_active, n_active] = np.sqrt(1 - v)","240","        # solves LL'x = y as a composition of two triangular systems"]}]}},"5763284ef9e04ffc52cf759a59bdc6bb81107616":{"changes":{"sklearn\/neighbors\/tests\/test_neighbors.py":"MODIFY","sklearn\/neighbors\/base.py":"MODIFY"},"diff":{"sklearn\/neighbors\/tests\/test_neighbors.py":[{"add":["20","from sklearn.utils.testing import assert_raises_regex","111","def test_n_neighbors_datatype():","112","    # Test to check whether n_neighbors is integer","113","    X = [[1, 1], [1, 1], [1, 1]]","114","    expected_msg = \"n_neighbors does not take .*float.* \" \\","115","                   \"value, enter integer value\"","116","    msg = \"Expected n_neighbors > 0. Got -3\"","117","","118","    neighbors_ = neighbors.NearestNeighbors(n_neighbors=3.)","119","    assert_raises_regex(TypeError, expected_msg, neighbors_.fit, X)","120","    assert_raises_regex(ValueError, msg,","121","                        neighbors_.kneighbors, X=X, n_neighbors=-3)","122","    assert_raises_regex(TypeError, expected_msg,","123","                        neighbors_.kneighbors, X=X, n_neighbors=3.)","124","","125",""],"delete":[]}],"sklearn\/neighbors\/base.py":[{"add":["260","            else:","261","                if not np.issubdtype(type(self.n_neighbors), np.integer):","262","                    raise TypeError(","263","                        \"n_neighbors does not take %s value, \"","264","                        \"enter integer value\" %","265","                        type(self.n_neighbors))","335","        elif n_neighbors <= 0:","336","            raise ValueError(","337","                \"Expected n_neighbors > 0. Got %d\" %","338","                n_neighbors","339","            )","340","        else:","341","            if not np.issubdtype(type(n_neighbors), np.integer):","342","                raise TypeError(","343","                    \"n_neighbors does not take %s value, \"","344","                    \"enter integer value\" %","345","                    type(n_neighbors))"],"delete":[]}]}},"cba99e01330c5df6d7da33c5c94a687d364986e4":{"changes":{"sklearn\/ensemble\/iforest.py":"MODIFY","sklearn\/ensemble\/tests\/test_iforest.py":"MODIFY","sklearn\/linear_model\/tests\/test_sag.py":"MODIFY"},"diff":{"sklearn\/ensemble\/iforest.py":[{"add":["72","        .. versionchanged:: 0.20","73","           The default value of ``contamination`` will change from 0.1 in 0.20","74","           to ``'auto'`` in 0.22.","75","","178","        if self.contamination == \"legacy\":","179","            warnings.warn('default contamination parameter 0.1 will change '","180","                          'in version 0.22 to \"auto\". This will change the '","181","                          'predict method behavior.',","182","                          FutureWarning)","183","            self._contamination = 0.1","184","        else:","185","            self._contamination = self.contamination","186","","228","        if self._contamination == \"auto\":","237","                self.score_samples(X), 100. * self._contamination)"],"delete":["152","","153","        if contamination == \"legacy\":","154","            warnings.warn('default contamination parameter 0.1 will change '","155","                          'in version 0.22 to \"auto\". This will change the '","156","                          'predict method behavior.',","157","                          DeprecationWarning)","221","        if self.contamination == \"auto\":","228","        elif self.contamination == \"legacy\":  # to be rm in 0.22","229","            self.offset_ = sp.stats.scoreatpercentile(","230","                self.score_samples(X), 100. * 0.1)","233","                self.score_samples(X), 100. * self.contamination)"]}],"sklearn\/ensemble\/tests\/test_iforest.py":[{"add":["64","@pytest.mark.filterwarnings('ignore:default contamination')","92","@pytest.mark.filterwarnings('ignore:default contamination')","131","@pytest.mark.filterwarnings('ignore:default contamination')","140","@pytest.mark.filterwarnings('ignore:default contamination')","156","@pytest.mark.filterwarnings('ignore:default contamination')","181","@pytest.mark.filterwarnings('ignore:default contamination')","221","@pytest.mark.filterwarnings('ignore:default contamination')","229","@pytest.mark.filterwarnings('ignore:default contamination')","254","@pytest.mark.filterwarnings('ignore:default contamination')","268","    X = [[0.0], [1.0]]","269","    clf = IsolationForest()","270","","271","    assert_warns_message(FutureWarning,","274","                         clf.fit, X)","275","","276","    clf = IsolationForest(contamination='auto').fit(X)"],"delete":["259","    assert_warns_message(DeprecationWarning,","262","                         IsolationForest, )","263","    X = [[0.0], [1.0]]","264","    clf = IsolationForest().fit(X)"]}],"sklearn\/linear_model\/tests\/test_sag.py":[{"add":["19","from sklearn.utils.testing import assert_allclose","297","    assert_allclose(weights1, clf.coef_)","298","    assert_allclose(intercept1, clf.intercept_)","299","    assert_allclose(weights2, clf.coef_)","300","    assert_allclose(intercept2, clf.intercept_)"],"delete":["271","@ignore_warnings","297","    assert_array_almost_equal(weights1, clf.coef_, decimal=10)","298","    assert_array_almost_equal(intercept1, clf.intercept_, decimal=10)","299","    assert_array_almost_equal(weights2, clf.coef_, decimal=10)","300","    assert_array_almost_equal(intercept2, clf.intercept_, decimal=10)"]}]}},"95993a4b2b7d067d8d7fff91ccb2463dbd427e7c":{"changes":{"sklearn\/tree\/tree.py":"MODIFY","sklearn\/ensemble\/tests\/test_forest.py":"MODIFY","sklearn\/tree\/tests\/test_tree.py":"MODIFY"},"diff":{"sklearn\/tree\/tree.py":[{"add":["438","                class_type = self.classes_[0].dtype","439","                predictions = np.zeros((n_samples, self.n_outputs_),","440","                                       dtype=class_type)"],"delete":["438","                predictions = np.zeros((n_samples, self.n_outputs_))","439",""]}],"sklearn\/ensemble\/tests\/test_forest.py":[{"add":["1339","","1340","","1341","@pytest.mark.filterwarnings('ignore:The default value of n_estimators')","1342","@pytest.mark.parametrize('name', FOREST_CLASSIFIERS)","1343","@pytest.mark.parametrize('oob_score', (True, False))","1344","def test_multi_target(name, oob_score):","1345","    ForestClassifier = FOREST_CLASSIFIERS[name]","1346","","1347","    clf = ForestClassifier(bootstrap=True, oob_score=oob_score)","1348","","1349","    X = iris.data","1350","","1351","    # Make multi column mixed type target.","1352","    y = np.vstack([","1353","        iris.target.astype(float),","1354","        iris.target.astype(int),","1355","        iris.target.astype(str),","1356","    ]).T","1357","","1358","    # Try to fit and predict.","1359","    clf.fit(X, y)","1360","    clf.predict(X)"],"delete":[]}],"sklearn\/tree\/tests\/test_tree.py":[{"add":["1830","","1831","","1832","@pytest.mark.parametrize('name', CLF_TREES)","1833","def test_multi_target(name):","1834","    Tree = CLF_TREES[name]","1835","","1836","    clf = Tree()","1837","","1838","    X = iris.data","1839","","1840","    # Make multi column mixed type target.","1841","    y = np.vstack([","1842","        iris.target.astype(float),","1843","        iris.target.astype(int),","1844","        iris.target.astype(str),","1845","    ]).T","1846","","1847","    # Try to fit and predict.","1848","    clf.fit(X, y)","1849","    clf.predict(X)"],"delete":[]}]}},"abb43c1aef10a3348216006f6be5e34dcd6ac157":{"changes":{"sklearn\/decomposition\/online_lda.py":"MODIFY"},"diff":{"sklearn\/decomposition\/online_lda.py":[{"add":["7","Link: https:\/\/github.com\/blei-lab\/onlineldavb","259","        https:\/\/github.com\/blei-lab\/onlineldavb"],"delete":["7","Link: http:\/\/matthewdhoffman.com\/code\/onlineldavb.tar","259","        http:\/\/matthewdhoffman.com\/\/code\/onlineldavb.tar"]}]}},"7d745eeed354c205bcccefb95f72cfbc283bd6e7":{"changes":{"doc\/modules\/clustering.rst":"MODIFY","doc\/whats_new\/v0.20.rst":"MODIFY","doc\/modules\/classes.rst":"MODIFY","sklearn\/cluster\/setup.py":"MODIFY","examples\/cluster\/plot_optics.py":"ADD","sklearn\/cluster\/__init__.py":"MODIFY","examples\/cluster\/plot_cluster_comparison.py":"MODIFY","sklearn\/cluster\/tests\/test_optics.py":"ADD","sklearn\/cluster\/_optics_inner.pyx":"ADD","sklearn\/cluster\/optics_.py":"ADD"},"diff":{"doc\/modules\/clustering.rst":[{"add":["93","   * - :ref:`OPTICS <optics>`","94","     - minimum cluster membership","95","     - Very large ``n_samples``, large ``n_clusters``","96","     - Non-flat geometry, uneven cluster sizes, variable cluster density ","97","     - Distances between points","98","   ","804","    - Use OPTICS clustering in conjunction with the `extract_dbscan` method. OPTICS","805","      clustering also calculates the full pairwise matrix, but only keeps one row in","806","      memory at a time (memory complexity n).","807","","826",".. _optics:","827","","828","OPTICS","829","======","830","","831","The :class:`OPTICS` algorithm shares many similarities with the","832",":class:`DBSCAN` algorithm, and can be considered a generalization of","833","DBSCAN that relaxes the ``eps`` requirement from a single value to a value","834","range. The key difference between DBSCAN and OPTICS is that the OPTICS","835","algorithm builds a *reachability* graph, which assigns each sample both a","836","``reachability_`` distance, and a spot within the cluster ``ordering_``","837","attribute; these two attributes are assigned when the model is fitted, and are","838","used to determine cluster membership. If OPTICS is run with the default value","839","of *inf* set for ``max_bound``, then DBSCAN style cluster extraction can be","840","performed in linear time for any given ``eps`` value using the","841","``extract_dbscan`` method. Setting ``max_bound`` to a lower value will result","842","in shorter run times, and can be thought of as the maximum cluster object size","843","(in diameter) that OPTICS will be able to extract.","844","","845",".. |optics_results| image:: ..\/auto_examples\/cluster\/images\/sphx_glr_plot_optics_001.png","846","        :target: ..\/auto_examples\/cluster\/plot_optics.html","847","        :scale: 50","848","","849",".. centered:: |optics_results|","850","","851","The *reachability* distances generated by OPTICS allow for variable density","852","extraction of clusters within a single data set. As shown in the above plot,","853","combining *reachability* distances and data set ``ordering_`` produces a","854","*reachability plot*, where point density is represented on the Y-axis, and","855","points are ordered such that nearby points are adjacent. 'Cutting' the","856","reachability plot at a single value produces DBSCAN like results; all points","857","above the 'cut' are classified as noise, and each time that there is a break","858","when reading from left to right signifies a new cluster. The default cluster","859","extraction with OPTICS looks at changes in slope within the graph to guess at","860","natural clusters. There are also other possibilities for analysis on the graph","861","itself, such as generating hierarchical representations of the data through","862","reachability-plot dendrograms. The plot above has been color-coded so that","863","cluster colors in planar space match the linear segment clusters of the","864","reachability plot-- note that the blue and red clusters are adjacent in the","865","reachability plot, and can be hierarchically represented as children of a","866","larger parent cluster.","867","","868",".. topic:: Examples:","869","","870","     * :ref:`sphx_glr_auto_examples_cluster_plot_optics.py`","871","","872","","873",".. topic:: Comparison with DBSCAN","874","    ","875","    The results from OPTICS ``extract_dbscan`` method and DBSCAN are not quite","876","    identical. Specifically, while *core_samples* returned from both OPTICS","877","    and DBSCAN are guaranteed to be identical, labeling of periphery and noise","878","    points is not. This is in part because the first sample processed by","879","    OPTICS will always have a reachability distance that is set to ``inf``,","880","    and will thus generally be marked as noise rather than periphery. This","881","    affects adjacent points when they are considered as candidates for being","882","    marked as either periphery or noise. While this effect is quite local to","883","    the starting point of the dataset and is unlikely to be noticed on even","884","    moderately large datasets, it is worth also noting that non-core boundry","885","    points may switch cluster labels on the rare occasion that they are","886","    equidistant to a competeing cluster due to how the graph is read from left","887","    to right when assigning labels. ","888","","889","    Note that for any single value of ``eps``, DBSCAN will tend to have a","890","    shorter run time than OPTICS; however, for repeated runs at varying ``eps``","891","    values, a single run of OPTICS may require less cumulative runtime than","892","    DBSCAN. It is also important to note that OPTICS output can be unstable at","893","    ``eps`` values very close to the initial ``max_bound`` value. OPTICS seems","894","    to produce near identical results to DBSCAN provided that ``eps`` passed to","895","    ``extract_dbscan`` is a half order of magnitude less than the inital","896","    ``max_bound`` that was used to fit; using a value close to ``max_bound``","897","    will throw a warning, and using a value larger will result in an exception. ","898","","899",".. topic:: Computational Complexity","900","","901","    Spatial indexing trees are used to avoid calculating the full distance","902","    matrix, and allow for efficient memory usage on large sets of samples.","903","    Different distance metrics can be supplied via the ``metric`` keyword.","904","    ","905","    For large datasets, similar (but not identical) results can be obtained via","906","    `HDBSCAN <https:\/\/hdbscan.readthedocs.io>`_. The HDBSCAN implementation is","907","    multithreaded, and has better algorithmic runtime complexity than OPTICS--","908","    at the cost of worse memory scaling. For extremely large datasets that","909","    exhaust system memory using HDBSCAN, OPTICS will maintain *n* (as opposed","910","    to *n^2* memory scaling); however, tuning of the ``max_bound`` parameter","911","    will likely need to be used to give a solution in a reasonable amount of","912","    wall time.","913","","914",".. topic:: References:","915","","916"," *  \"OPTICS: ordering points to identify the clustering structure.\"","917","    Ankerst, Mihael, Markus M. Breunig, Hans-Peter Kriegel, and J?rg Sander.","918","    In ACM Sigmod Record, vol. 28, no. 2, pp. 49-60. ACM, 1999.","919","","920"," *  \"Automatic extraction of clusters from hierarchical clustering","921","    representations.\"","922","    Sander, J?rg, Xuejie Qin, Zhiyong Lu, Nan Niu, and Alex Kovarsky.","923","    In Advances in knowledge discovery and data mining,","924","    pp. 75-87. Springer Berlin Heidelberg, 2003. ","925",""],"delete":[]}],"doc\/whats_new\/v0.20.rst":[{"add":["179","- A new clustering algorithm: :class:`cluster.OPTICS`: an algoritm","180","  related to :class:`cluster.DBSCAN`, that has hyperparameters easier to","181","  set and tat scales better, by :user:`Shane <espg>`.","182",""],"delete":[]}],"doc\/modules\/classes.rst":[{"add":["100","   cluster.OPTICS","115","   cluster.optics"],"delete":[]}],"sklearn\/cluster\/setup.py":[{"add":["25","    config.add_extension('_optics_inner',","26","                         sources=['_optics_inner.pyx'],","27","                         include_dirs=[numpy.get_include()],","28","                         libraries=libraries)"],"delete":[]}],"examples\/cluster\/plot_optics.py":[{"add":[],"delete":[]}],"sklearn\/cluster\/__init__.py":[{"add":["13","from .optics_ import OPTICS","21","           'OPTICS',"],"delete":[]}],"examples\/cluster\/plot_cluster_comparison.py":[{"add":["118","    optics = cluster.OPTICS(min_samples=30, maxima_ratio=.8,","119","                            rejection_ratio=.4)","137","        ('OPTICS', optics),"],"delete":[]}],"sklearn\/cluster\/tests\/test_optics.py":[{"add":[],"delete":[]}],"sklearn\/cluster\/_optics_inner.pyx":[{"add":[],"delete":[]}],"sklearn\/cluster\/optics_.py":[{"add":[],"delete":[]}]}},"102620f8d122d244dd08ccaa015d67a97d8a28ae":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/base.py":"MODIFY","sklearn\/tests\/test_base.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["124","- Fixed a bug when setting parameters on meta-estimator, involving both a","125","  wrapped estimator and its parameter. :issue:`9999` by :user:`Marcus Voss","126","  <marcus-voss>` and `Joel Nothman`_.","127",""],"delete":[]}],"sklearn\/base.py":[{"add":["265","                valid_params[key] = value"],"delete":[]}],"sklearn\/tests\/test_base.py":[{"add":["248","def test_set_params_updates_valid_params():","249","    # Check that set_params tries to set SVC().C, not","250","    # DecisionTreeClassifier().C","251","    gscv = GridSearchCV(DecisionTreeClassifier(), {})","252","    gscv.set_params(estimator=SVC(), estimator__C=42.0)","253","    assert gscv.estimator.C == 42.0","254","","255",""],"delete":[]}]}}}