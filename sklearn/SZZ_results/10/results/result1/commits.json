{"725ff4a2eeef171cb6b01b568faf23d170566f94":{"changes":{"sklearn\/cluster\/hierarchical.py":"MODIFY"},"diff":{"sklearn\/cluster\/hierarchical.py":[{"add":["743","            # Reassign cluster numbers"],"delete":["743","            # Reasign cluster numbers"]}]}},"5c6409c9e94cf9a1d9f78a53d613fb5904a42cd0":{"changes":{"sklearn\/manifold\/tests\/test_locally_linear.py":"MODIFY","sklearn\/manifold\/tests\/test_t_sne.py":"MODIFY"},"diff":{"sklearn\/manifold\/tests\/test_locally_linear.py":[{"add":["16","# ----------------------------------------------------------------------","35","# ----------------------------------------------------------------------"],"delete":["16","#----------------------------------------------------------------------","35","#----------------------------------------------------------------------"]}],"sklearn\/manifold\/tests\/test_t_sne.py":[{"add":["198","","199","    def fun(params):","200","        return _kl_divergence(params, P, alpha, n_samples, n_components)[0]","201","","202","    def grad(params):","203","        return _kl_divergence(params, P, alpha, n_samples, n_components)[1]","204",""],"delete":["198","    fun = lambda params: _kl_divergence(params, P, alpha, n_samples,","199","                                        n_components)[0]","200","    grad = lambda params: _kl_divergence(params, P, alpha, n_samples,","201","                                         n_components)[1]"]}]}},"54b0e4bf62152ac9abb5b93780a0af3e6f5d1c44":{"changes":{"sklearn\/model_selection\/_split.py":"MODIFY","sklearn\/utils\/metaestimators.py":"MODIFY","sklearn\/model_selection\/_validation.py":"MODIFY","sklearn\/tests\/test_multiclass.py":"MODIFY","sklearn\/model_selection\/tests\/test_split.py":"MODIFY","sklearn\/utils\/multiclass.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/svm\/base.py":"MODIFY","sklearn\/multiclass.py":"MODIFY","sklearn\/utils\/tests\/test_multiclass.py":"MODIFY"},"diff":{"sklearn\/model_selection\/_split.py":[{"add":[],"delete":["1679","","1680","def _safe_split(estimator, X, y, indices, train_indices=None):","1681","    \"\"\"Create subset of dataset and properly handle kernels.\"\"\"","1682","    if (hasattr(estimator, 'kernel') and callable(estimator.kernel) and","1683","            not isinstance(estimator.kernel, GPKernel)):","1684","        # cannot compute the kernel values with custom function","1685","        raise ValueError(\"Cannot use a custom kernel function. \"","1686","                         \"Precompute the kernel matrix instead.\")","1687","","1688","    if not hasattr(X, \"shape\"):","1689","        if getattr(estimator, \"_pairwise\", False):","1690","            raise ValueError(\"Precomputed kernels or affinity matrices have \"","1691","                             \"to be passed as arrays or sparse matrices.\")","1692","        X_subset = [X[index] for index in indices]","1693","    else:","1694","        if getattr(estimator, \"_pairwise\", False):","1695","            # X is a precomputed square kernel matrix","1696","            if X.shape[0] != X.shape[1]:","1697","                raise ValueError(\"X should be a square kernel matrix\")","1698","            if train_indices is None:","1699","                X_subset = X[np.ix_(indices, indices)]","1700","            else:","1701","                X_subset = X[np.ix_(indices, train_indices)]","1702","        else:","1703","            X_subset = safe_indexing(X, indices)","1704","","1705","    if y is not None:","1706","        y_subset = safe_indexing(y, indices)","1707","    else:","1708","        y_subset = None","1709","","1710","    return X_subset, y_subset","1711","","1712",""]}],"sklearn\/utils\/metaestimators.py":[{"add":["7","import numpy as np","8","from ..utils import safe_indexing","80","","81","","82","def _safe_split(estimator, X, y, indices, train_indices=None):","83","    \"\"\"Create subset of dataset and properly handle kernels.\"\"\"","84","    from ..gaussian_process.kernels import Kernel as GPKernel","85","","86","    if (hasattr(estimator, 'kernel') and callable(estimator.kernel) and","87","            not isinstance(estimator.kernel, GPKernel)):","88","        # cannot compute the kernel values with custom function","89","        raise ValueError(\"Cannot use a custom kernel function. \"","90","                         \"Precompute the kernel matrix instead.\")","91","","92","    if not hasattr(X, \"shape\"):","93","        if getattr(estimator, \"_pairwise\", False):","94","            raise ValueError(\"Precomputed kernels or affinity matrices have \"","95","                             \"to be passed as arrays or sparse matrices.\")","96","        X_subset = [X[index] for index in indices]","97","    else:","98","        if getattr(estimator, \"_pairwise\", False):","99","            # X is a precomputed square kernel matrix","100","            if X.shape[0] != X.shape[1]:","101","                raise ValueError(\"X should be a square kernel matrix\")","102","            if train_indices is None:","103","                X_subset = X[np.ix_(indices, indices)]","104","            else:","105","                X_subset = X[np.ix_(indices, train_indices)]","106","        else:","107","            X_subset = safe_indexing(X, indices)","108","","109","    if y is not None:","110","        y_subset = safe_indexing(y, indices)","111","    else:","112","        y_subset = None","113","","114","    return X_subset, y_subset"],"delete":["7",""]}],"sklearn\/model_selection\/_validation.py":[{"add":["25","from ..utils.metaestimators import _safe_split","29","from ._split import check_cv"],"delete":["28","","29","from ._split import check_cv, _safe_split"]}],"sklearn\/tests\/test_multiclass.py":[{"add":["26","from sklearn.model_selection import GridSearchCV, cross_val_score","607","","608","","609","def test_pairwise_indices():","610","    clf_precomputed = svm.SVC(kernel='precomputed')","611","    X, y = iris.data, iris.target","612","","613","    ovr_false = OneVsOneClassifier(clf_precomputed)","614","    linear_kernel = np.dot(X, X.T)","615","    ovr_false.fit(linear_kernel, y)","616","","617","    n_estimators = len(ovr_false.estimators_)","618","    precomputed_indices = ovr_false.pairwise_indices_","619","","620","    for idx in precomputed_indices:","621","        assert_equal(idx.shape[0] * n_estimators \/ (n_estimators - 1),","622","                     linear_kernel.shape[0])","623","","624","","625","def test_pairwise_attribute():","626","    clf_precomputed = svm.SVC(kernel='precomputed')","627","    clf_notprecomputed = svm.SVC()","628","","629","    for MultiClassClassifier in [OneVsRestClassifier, OneVsOneClassifier]:","630","        ovr_false = MultiClassClassifier(clf_notprecomputed)","631","        assert_false(ovr_false._pairwise)","632","","633","        ovr_true = MultiClassClassifier(clf_precomputed)","634","        assert_true(ovr_true._pairwise)","635","","636","","637","def test_pairwise_cross_val_score():","638","    clf_precomputed = svm.SVC(kernel='precomputed')","639","    clf_notprecomputed = svm.SVC(kernel='linear')","640","","641","    X, y = iris.data, iris.target","642","","643","    for MultiClassClassifier in [OneVsRestClassifier, OneVsOneClassifier]:","644","        ovr_false = MultiClassClassifier(clf_notprecomputed)","645","        ovr_true = MultiClassClassifier(clf_precomputed)","646","","647","        linear_kernel = np.dot(X, X.T)","648","        score_precomputed = cross_val_score(ovr_true, linear_kernel, y)","649","        score_linear = cross_val_score(ovr_false, X, y)","650","        assert_array_equal(score_precomputed, score_linear)"],"delete":["26","from sklearn.model_selection import GridSearchCV"]}],"sklearn\/model_selection\/tests\/test_split.py":[{"add":[],"delete":["47","from sklearn.model_selection._split import _safe_split","53","from sklearn.datasets import load_iris","64","iris = load_iris()","848","def test_safe_split_with_precomputed_kernel():","849","    clf = SVC()","850","    clfp = SVC(kernel=\"precomputed\")","851","","852","    X, y = iris.data, iris.target","853","    K = np.dot(X, X.T)","854","","855","    cv = ShuffleSplit(test_size=0.25, random_state=0)","856","    tr, te = list(cv.split(X))[0]","857","","858","    X_tr, y_tr = _safe_split(clf, X, y, tr)","859","    K_tr, y_tr2 = _safe_split(clfp, K, y, tr)","860","    assert_array_almost_equal(K_tr, np.dot(X_tr, X_tr.T))","861","","862","    X_te, y_te = _safe_split(clf, X, y, te, tr)","863","    K_te, y_te2 = _safe_split(clfp, K, y, te, tr)","864","    assert_array_almost_equal(K_te, np.dot(X_te, X_tr.T))","865","","866",""]}],"sklearn\/utils\/multiclass.py":[{"add":["161","        'binary', 'multiclass', 'multiclass-multioutput',","169","    if y_type not in ['binary', 'multiclass', 'multiclass-multioutput',","387","","388","","389","def _ovr_decision_function(predictions, confidences, n_classes):","390","    \"\"\"Compute a continuous, tie-breaking ovr decision function.","391","","392","    It is important to include a continuous value, not only votes,","393","    to make computing AUC or calibration meaningful.","394","","395","    Parameters","396","    ----------","397","    predictions : array-like, shape (n_samples, n_classifiers)","398","        Predicted classes for each binary classifier.","399","","400","    confidences : array-like, shape (n_samples, n_classifiers)","401","        Decision functions or predicted probabilities for positive class","402","        for each binary classifier.","403","","404","    n_classes : int","405","        Number of classes. n_classifiers must be","406","        ``n_classes * (n_classes - 1 ) \/ 2``","407","    \"\"\"","408","    n_samples = predictions.shape[0]","409","    votes = np.zeros((n_samples, n_classes))","410","    sum_of_confidences = np.zeros((n_samples, n_classes))","411","","412","    k = 0","413","    for i in range(n_classes):","414","        for j in range(i + 1, n_classes):","415","            sum_of_confidences[:, i] -= confidences[:, k]","416","            sum_of_confidences[:, j] += confidences[:, k]","417","            votes[predictions[:, k] == 0, i] += 1","418","            votes[predictions[:, k] == 1, j] += 1","419","            k += 1","420","","421","    max_confidences = sum_of_confidences.max()","422","    min_confidences = sum_of_confidences.min()","423","","424","    if max_confidences == min_confidences:","425","        return votes","426","","427","    # Scale the sum_of_confidences to (-0.5, 0.5) and add it with votes.","428","    # The motivation is to use confidence levels as a way to break ties in","429","    # the votes without switching any decision made based on a difference","430","    # of 1 vote.","431","    eps = np.finfo(sum_of_confidences.dtype).eps","432","    max_abs_confidence = max(abs(max_confidences), abs(min_confidences))","433","    scale = (0.5 - eps) \/ max_abs_confidence","434","    return votes + sum_of_confidences * scale"],"delete":["25","","162","        'binary', 'multiclass', 'multiclass-multioutput', ","170","    if y_type not in ['binary', 'multiclass', 'multiclass-multioutput', "]}],"doc\/whats_new.rst":[{"add":["464","    - Cross-validation of :class:`OneVsOneClassifier` and","465","      :class:`OneVsRestClassifier` now works with precomputed kernels.","466","      (`#7350 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/7350\/>`_)","467","      By `Russell Smith`_.","468","","4645","","4646",".. _Russell Smith: https:\/\/github.com\/rsmith54"],"delete":[]}],"sklearn\/svm\/base.py":[{"add":["11","from ..utils.multiclass import _ovr_decision_function"],"delete":["11","from ..multiclass import _ovr_decision_function"]}],"sklearn\/multiclass.py":[{"add":["39","import itertools","50","                               check_classification_targets,","51","                               _ovr_decision_function)","52","from .utils.metaestimators import _safe_split","53","","263","                                 X, next(columns) if self.classes_[i] in","264","                                 self.label_binarizer_.classes_ else","265","                                 np.zeros((1, len(y))))","397","    @property","398","    def _pairwise(self):","399","        \"\"\"Indicate if wrapped estimator is using a precomputed Gram matrix\"\"\"","400","        return getattr(self.estimator, \"_pairwise\", False)","401","","410","    indcond = np.arange(X.shape[0])[cond]","411","    return _fit_binary(estimator,","412","                       _safe_split(estimator, X, None, indices=indcond)[0],","413","                       y_binary, classes=[i, j]), indcond","485","        estimators_indices = list(zip(*(Parallel(n_jobs=self.n_jobs)(","486","            delayed(_fit_ovo_binary)","487","            (self.estimator, X, y, self.classes_[i], self.classes_[j])","488","            for i in range(n_classes) for j in range(i + 1, n_classes)))))","489","","490","        self.estimators_ = estimators_indices[0]","491","        try:","492","            self.pairwise_indices_ = estimators_indices[1] \\","493","                                     if self._pairwise else None","494","        except AttributeError:","495","            self.pairwise_indices_ = None","529","                                      (self.n_classes_ - 1) \/\/ 2)]","533","        combinations = itertools.combinations(range(self.n_classes_), 2)","534","        self.estimators_ = Parallel(","535","            n_jobs=self.n_jobs)(","536","                delayed(_partial_fit_ovo_binary)(","537","                    estimator, X, y, self.classes_[i], self.classes_[j])","538","                for estimator, (i, j) in izip(","539","                        self.estimators_, (combinations)))","540","","541","        self.pairwise_indices_ = None","542","","583","        indices = self.pairwise_indices_","584","        if indices is None:","585","            Xs = [X] * len(self.estimators_)","586","        else:","587","            Xs = [X[:, idx] for idx in indices]","588","","589","        predictions = np.vstack([est.predict(Xi)","590","                                 for est, Xi in zip(self.estimators_, Xs)]).T","591","        confidences = np.vstack([_predict_binary(est, Xi)","592","                                 for est, Xi in zip(self.estimators_, Xs)]).T","593","        Y = _ovr_decision_function(predictions,","594","                                   confidences, len(self.classes_))","595","","596","        return Y","602","    @property","603","    def _pairwise(self):","604","        \"\"\"Indicate if wrapped estimator is using a precomputed Gram matrix\"\"\"","605","        return getattr(self.estimator, \"_pairwise\", False)"],"delete":["49","                               check_classification_targets)","259","            X, next(columns) if self.classes_[i] in","260","            self.label_binarizer_.classes_ else","261","            np.zeros((1, len(y))))","401","    ind = np.arange(X.shape[0])","402","    return _fit_binary(estimator, X[ind[cond]], y_binary, classes=[i, j])","474","        self.estimators_ = Parallel(n_jobs=self.n_jobs)(","475","            delayed(_fit_ovo_binary)(","476","                self.estimator, X, y, self.classes_[i], self.classes_[j])","477","            for i in range(n_classes) for j in range(i + 1, n_classes))","511","                                (self.n_classes_-1) \/\/ 2)]","515","        self.estimators_ = Parallel(n_jobs=self.n_jobs)(","516","            delayed(_partial_fit_ovo_binary)(","517","                estimator, X, y, self.classes_[i], self.classes_[j])","518","            for estimator, (i, j) in izip(self.estimators_, ((i, j) for i","519","                                in range(self.n_classes_) for j in range","520","                                            (i + 1, self.n_classes_))))","561","        predictions = np.vstack([est.predict(X) for est in self.estimators_]).T","562","        confidences = np.vstack([_predict_binary(est, X) for est in self.estimators_]).T","563","        return _ovr_decision_function(predictions, confidences,","564","                                      len(self.classes_))","570","","571","def _ovr_decision_function(predictions, confidences, n_classes):","572","    \"\"\"Compute a continuous, tie-breaking ovr decision function.","573","","574","    It is important to include a continuous value, not only votes,","575","    to make computing AUC or calibration meaningful.","576","","577","    Parameters","578","    ----------","579","    predictions : array-like, shape (n_samples, n_classifiers)","580","        Predicted classes for each binary classifier.","581","","582","    confidences : array-like, shape (n_samples, n_classifiers)","583","        Decision functions or predicted probabilities for positive class","584","        for each binary classifier.","585","","586","    n_classes : int","587","        Number of classes. n_classifiers must be","588","        ``n_classes * (n_classes - 1 ) \/ 2``","589","    \"\"\"","590","    n_samples = predictions.shape[0]","591","    votes = np.zeros((n_samples, n_classes))","592","    sum_of_confidences = np.zeros((n_samples, n_classes))","593","","594","    k = 0","595","    for i in range(n_classes):","596","        for j in range(i + 1, n_classes):","597","            sum_of_confidences[:, i] -= confidences[:, k]","598","            sum_of_confidences[:, j] += confidences[:, k]","599","            votes[predictions[:, k] == 0, i] += 1","600","            votes[predictions[:, k] == 1, j] += 1","601","            k += 1","602","","603","    max_confidences = sum_of_confidences.max()","604","    min_confidences = sum_of_confidences.min()","605","","606","    if max_confidences == min_confidences:","607","        return votes","608","","609","    # Scale the sum_of_confidences to (-0.5, 0.5) and add it with votes.","610","    # The motivation is to use confidence levels as a way to break ties in","611","    # the votes without switching any decision made based on a difference","612","    # of 1 vote.","613","    eps = np.finfo(sum_of_confidences.dtype).eps","614","    max_abs_confidence = max(abs(max_confidences), abs(min_confidences))","615","    scale = (0.5 - eps) \/ max_abs_confidence","616","    return votes + sum_of_confidences * scale"]}],"sklearn\/utils\/tests\/test_multiclass.py":[{"add":["30","from sklearn.utils.metaestimators import _safe_split","31","from sklearn.model_selection import ShuffleSplit","32","from sklearn.svm import SVC","33","from sklearn import datasets","34","","273","                assert_raises_regex(ValueError, msg,","352","","353","","354","def test_safe_split_with_precomputed_kernel():","355","    clf = SVC()","356","    clfp = SVC(kernel=\"precomputed\")","357","","358","    iris = datasets.load_iris()","359","    X, y = iris.data, iris.target","360","    K = np.dot(X, X.T)","361","","362","    cv = ShuffleSplit(test_size=0.25, random_state=0)","363","    train, test = list(cv.split(X))[0]","364","","365","    X_train, y_train = _safe_split(clf, X, y, train)","366","    K_train, y_train2 = _safe_split(clfp, K, y, train)","367","    assert_array_almost_equal(K_train, np.dot(X_train, X_train.T))","368","    assert_array_almost_equal(y_train, y_train2)","369","","370","    X_test, y_test = _safe_split(clf, X, y, test, train)","371","    K_test, y_test2 = _safe_split(clfp, K, y, test, train)","372","    assert_array_almost_equal(K_test, np.dot(X_test, X_train.T))","373","    assert_array_almost_equal(y_test, y_test2)"],"delete":["268","                assert_raises_regex(ValueError, msg, "]}]}},"22cf46edbc33a6bae158bb6a5bd7b779285fcfa0":{"changes":{".travis.yml":"MODIFY","sklearn\/cross_decomposition\/pls_.py":"MODIFY"},"diff":{".travis.yml":[{"add":["41","    - DISTRIB=\"conda\" PYTHON_VERSION=\"3.5\" INSTALL_MKL=\"true\""],"delete":["41","    - DISTRIB=\"conda\" PYTHON_VERSION=\"3.5\" INSTALL_MKL=\"false\""]}],"sklearn\/cross_decomposition\/pls_.py":[{"add":["55","        # If y_score only has zeros x_weights will only have zeros. In","56","        # this case add an epsilon to converge to a more acceptable","57","        # solution","58","        if np.dot(x_weights.T, x_weights) < eps:","59","            x_weights += eps"],"delete":[]}]}},"cee48cdf8f67bca9408855d2cd5a20fabec60155":{"changes":{"sklearn\/linear_model\/tests\/test_coordinate_descent.py":"MODIFY","examples\/linear_model\/plot_lasso_model_selection.py":"MODIFY","sklearn\/linear_model\/least_angle.py":"MODIFY"},"diff":{"sklearn\/linear_model\/tests\/test_coordinate_descent.py":[{"add":["173","    mse_lars = interpolate.interp1d(lars.cv_alphas_, lars.mse_path_.T)"],"delete":["173","    mse_lars = interpolate.interp1d(lars.cv_alphas_, lars.cv_mse_path_.T)"]}],"examples\/linear_model\/plot_lasso_model_selection.py":[{"add":["140","plt.plot(m_log_alphas, model.mse_path_, ':')","141","plt.plot(m_log_alphas, model.mse_path_.mean(axis=-1), 'k',"],"delete":["140","plt.plot(m_log_alphas, model.cv_mse_path_, ':')","141","plt.plot(m_log_alphas, model.cv_mse_path_.mean(axis=-1), 'k',"]}],"sklearn\/linear_model\/least_angle.py":[{"add":["23","from ..utils import arrayfuncs, as_float_array, check_X_y, deprecated","1058","    mse_path_ : array, shape (n_folds, n_cv_alphas)","1154","        self.mse_path_ = mse_path","1167","    @property","1168","    @deprecated(\"Attribute mse_path_ is deprecated in 0.18 and \"","1169","                \"will be removed in 0.20. Use 'cv_mse_path_' instead\")","1170","    def cv_mse_path_(self):","1171","        return self.mse_path_","1172","","1273","    mse_path_ : array, shape (n_folds, n_cv_alphas)"],"delete":["23","from ..utils import arrayfuncs, as_float_array, check_X_y","1058","    cv_mse_path_ : array, shape (n_folds, n_cv_alphas)","1154","        self.cv_mse_path_ = mse_path","1267","    cv_mse_path_ : array, shape (n_folds, n_cv_alphas)"]}]}},"485445401622f2cae0921c6678bbfb47c0deca27":{"changes":{"sklearn\/decomposition\/tests\/test_nmf.py":"MODIFY","sklearn\/ensemble\/iforest.py":"MODIFY","sklearn\/decomposition\/nmf.py":"MODIFY","sklearn\/ensemble\/tests\/test_iforest.py":"MODIFY"},"diff":{"sklearn\/decomposition\/tests\/test_nmf.py":[{"add":["9","from sklearn.utils.testing import assert_raise_message, assert_no_warnings","236","    assert_no_warnings(nnmf, A, A, A, np.int64(1))","237","    msg = \"Number of components must be a positive integer; got (n_components=1.5)\"","238","    assert_raise_message(ValueError, msg, nnmf, A, A, A, 1.5)","239","    msg = \"Number of components must be a positive integer; got (n_components='2')\""],"delete":["9","from sklearn.utils.testing import assert_raise_message","135","","237","    msg = \"Number of components must be positive; got (n_components='2')\""]}],"sklearn\/ensemble\/iforest.py":[{"add":["12","import numbers","170","        elif isinstance(self.max_samples, numbers.Integral):","280","    if isinstance(n_samples_leaf, numbers.Integral):"],"delete":["169","        elif isinstance(self.max_samples, six.integer_types):","279","    if isinstance(n_samples_leaf, six.integer_types):"]}],"sklearn\/decomposition\/nmf.py":[{"add":["748","    if not isinstance(n_components, numbers.Integral) or n_components <= 0:","749","        raise ValueError(\"Number of components must be a positive integer;\"","751","    if not isinstance(max_iter, numbers.Integral) or max_iter < 0:","752","        raise ValueError(\"Maximum number of iterations must be a positive integer;\""],"delete":["21","from ..externals import six","749","    if not isinstance(n_components, six.integer_types) or n_components <= 0:","750","        raise ValueError(\"Number of components must be positive;\"","752","    if not isinstance(max_iter, numbers.Number) or max_iter < 0:","753","        raise ValueError(\"Maximum number of iteration must be positive;\""]}],"sklearn\/ensemble\/tests\/test_iforest.py":[{"add":["106","    assert_no_warnings(IsolationForest(max_samples=np.int64(2)).fit, X)","107","    assert_raises(ValueError, IsolationForest(max_samples='foobar').fit, X)","108","    assert_raises(ValueError, IsolationForest(max_samples=1.5).fit, X)"],"delete":["106","    assert_raises(ValueError,","107","                  IsolationForest(max_samples='foobar').fit, X)"]}]}},"59bf211789d29d70642fd64d6d4bef95ff518e8d":{"changes":{"sklearn\/metrics\/pairwise.py":"MODIFY","sklearn\/metrics\/pairwise_fast.pyx":"MODIFY"},"diff":{"sklearn\/metrics\/pairwise.py":[{"add":["895","           parameter ``dense_output`` for dense output."],"delete":["895","           parameter *dense_output* for sparse output."]}],"sklearn\/metrics\/pairwise_fast.pyx":[{"add":["16","ctypedef float [:, :] float_array_2d_t","17","ctypedef double [:, :] double_array_2d_t"],"delete":["16","ctypedef float [:, :] float_array_2d_t ","17","ctypedef double [:, :] double_array_2d_t "]}]}},"71408e09e4bf01a72567f9167a1e33fb9620d786":{"changes":{"doc\/conf.py":"MODIFY","\/dev\/null":"DELETE","doc\/templates\/numpydoc_docstring.rst":"ADD","sklearn\/datasets\/species_distributions.py":"MODIFY","build_tools\/circle\/build_doc.sh":"MODIFY","doc\/sphinxext\/sphinx_gallery\/gen_gallery.py":"MODIFY"},"diff":{"doc\/conf.py":[{"add":["34","    'numpydoc',","40","# this is needed for some reason...","41","# see https:\/\/github.com\/numpy\/numpydoc\/issues\/69","42","numpydoc_class_members_toctree = False","43","","44",""],"delete":["34","    'numpy_ext.numpydoc',"]}],"\/dev\/null":[{"add":[],"delete":[]}],"doc\/templates\/numpydoc_docstring.rst":[{"add":[],"delete":[]}],"sklearn\/datasets\/species_distributions.py":[{"add":["203","    - For an example of using this dataset with scikit-learn, see"],"delete":["203","","204","    * For an example of using this dataset with scikit-learn, see"]}],"build_tools\/circle\/build_doc.sh":[{"add":["111","pip install numpydoc"],"delete":[]}],"doc\/sphinxext\/sphinx_gallery\/gen_gallery.py":[{"add":[],"delete":["295",""]}]}},"fe03879cd32207c5ed671a71a82df64802e06d0e":{"changes":{"sklearn\/model_selection\/_split.py":"MODIFY","sklearn\/model_selection\/_validation.py":"MODIFY","sklearn\/calibration.py":"MODIFY","sklearn\/cross_validation.py":"MODIFY","sklearn\/feature_selection\/rfe.py":"MODIFY","sklearn\/learning_curve.py":"MODIFY","sklearn\/grid_search.py":"MODIFY","sklearn\/linear_model\/ridge.py":"MODIFY","sklearn\/model_selection\/_search.py":"MODIFY"},"diff":{"sklearn\/model_selection\/_split.py":[{"add":["1387","        binary or multiclass, :class:`StratifiedKFold` is used. In all other"],"delete":["1387","        binary or multiclass, :class:`StratifiedKFold` used. In all other"]}],"sklearn\/model_selection\/_validation.py":[{"add":["100","        either binary or multiclass, :class:`StratifiedKFold` is used. In all","348","        either binary or multiclass, :class:`StratifiedKFold` is used. In all","564","        either binary or multiclass, :class:`StratifiedKFold` is used. In all","704","        either binary or multiclass, :class:`StratifiedKFold` is used. In all","917","        either binary or multiclass, :class:`StratifiedKFold` is used. In all"],"delete":["100","        either binary or multiclass, :class:`StratifiedKFold` used. In all","348","        either binary or multiclass, :class:`StratifiedKFold` used. In all","564","        either binary or multiclass, :class:`StratifiedKFold` used. In all","704","        either binary or multiclass, :class:`StratifiedKFold` used. In all","917","        either binary or multiclass, :class:`StratifiedKFold` used. In all"]}],"sklearn\/calibration.py":[{"add":["65","        :class:`sklearn.model_selection.StratifiedKFold` is used. If ``y`` ","66","        is neither binary nor multiclass, :class:`sklearn.model_selection.KFold` ","67","        is used."],"delete":["65","        :class:`StratifiedKFold` used. If ``y`` is neither binary nor","66","        multiclass, :class:`KFold` is used."]}],"sklearn\/cross_validation.py":[{"add":["1228","        either binary or multiclass, :class:`StratifiedKFold` is used. In all","1413","        either binary or multiclass, :class:`StratifiedKFold` is used. In all","1699","        multiclass, :class:`StratifiedKFold` is used. In all other cases,","1773","        either binary or multiclass, :class:`StratifiedKFold` is used. In all"],"delete":["1228","        either binary or multiclass, :class:`StratifiedKFold` used. In all","1413","        either binary or multiclass, :class:`StratifiedKFold` used. In all","1699","        multiclass, :class:`StratifiedKFold` used. In all other cases,","1773","        either binary or multiclass, :class:`StratifiedKFold` used. In all"]}],"sklearn\/feature_selection\/rfe.py":[{"add":["295","        :class:`sklearn.model_selection.StratifiedKFold` is used. If the ","296","        estimator is a classifier or if ``y`` is neither binary nor multiclass, ","297","        :class:`sklearn.model_selection.KFold` is used."],"delete":["295","        :class:`StratifiedKFold` used. If the estimator is a classifier","296","        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used."]}],"sklearn\/learning_curve.py":[{"add":["77","        either binary or multiclass, ","78","        :class:`sklearn.model_selection.StratifiedKFold` is used. In all","79","        other cases, :class:`sklearn.model_selection.KFold` is used.","291","        either binary or multiclass, ","292","        :class:`sklearn.model_selection.StratifiedKFold` is used. In all","293","        other cases, :class:`sklearn.model_selection.KFold` is used."],"delete":["77","        either binary or multiclass, :class:`StratifiedKFold` used. In all","78","        other cases, :class:`KFold` is used.","290","        either binary or multiclass, :class:`StratifiedKFold` used. In all","291","        other cases, :class:`KFold` is used."]}],"sklearn\/grid_search.py":[{"add":["689","        either binary or multiclass, ","690","        :class:`sklearn.model_selection.StratifiedKFold` is used. In all","691","        other cases, :class:`sklearn.model_selection.KFold` is used.","901","        either binary or multiclass, ","902","        :class:`sklearn.model_selection.StratifiedKFold` is used. In all","903","        other cases, :class:`sklearn.model_selection.KFold` is used."],"delete":["689","        either binary or multiclass, :class:`StratifiedKFold` used. In all","690","        other cases, :class:`KFold` is used.","900","        either binary or multiclass, :class:`StratifiedKFold` used. In all","901","        other cases, :class:`KFold` is used."]}],"sklearn\/linear_model\/ridge.py":[{"add":["1133","        :class:`sklearn.model_selection.StratifiedKFold` is used, else, ","1134","        :class:`sklearn.model_selection.KFold` is used."],"delete":["1133","        :class:`StratifiedKFold` used, else, :class:`KFold` is used."]}],"sklearn\/model_selection\/_search.py":[{"add":["708","        either binary or multiclass, :class:`StratifiedKFold` is used. In all","964","        either binary or multiclass, :class:`StratifiedKFold` is used. In all"],"delete":["708","        either binary or multiclass, :class:`StratifiedKFold` used. In all","964","        either binary or multiclass, :class:`StratifiedKFold` used. In all"]}]}},"268af6918ec9c1c88242efa75eadacb7eba0870b":{"changes":{"sklearn\/decomposition\/online_lda.py":"MODIFY","sklearn\/decomposition\/tests\/test_online_lda.py":"MODIFY"},"diff":{"sklearn\/decomposition\/online_lda.py":[{"add":["522","                                                       random_init=False,","523","                                                       parallel=parallel)"],"delete":["522","                                                       random_init=False)"]}],"sklearn\/decomposition\/tests\/test_online_lda.py":[{"add":["201","                                        evaluate_every=1,"],"delete":[]}]}},"9cef05faf5cf7e776b4fc050bfc266d1542c1188":{"changes":{"sklearn\/base.py":"MODIFY"},"diff":{"sklearn\/base.py":[{"add":["255","        (such as pipelines). The latter have parameters of the form"],"delete":["255","        (such as pipelines). The former have parameters of the form"]}]}},"668b329d421e797d0b5dbea9035c5de986da60a5":{"changes":{"sklearn\/linear_model\/tests\/test_huber.py":"MODIFY","sklearn\/linear_model\/huber.py":"MODIFY"},"diff":{"sklearn\/linear_model\/tests\/test_huber.py":[{"add":["11","from sklearn.utils.testing import assert_false","67","    huber = HuberRegressor(fit_intercept=True)","72","    # Rescale coefs before comparing with assert_array_almost_equal to make sure","73","    # that the number of decimal places used is somewhat insensitive to the","74","    # amplitude of the coefficients and therefore to the scale of the data","75","    # and the regularization parameter","76","    scale = max(np.mean(np.abs(huber.coef_)),","77","                np.mean(np.abs(huber.intercept_)))","78","","80","    assert_array_almost_equal(huber.coef_ \/ scale, huber_coef \/ scale)","81","    assert_array_almost_equal(huber.intercept_ \/ scale,","82","                              huber_intercept \/ scale)","90","    sample_weight = np.ones(X.shape[0])","91","    sample_weight[1] = 3","92","    sample_weight[3] = 2","93","    huber.fit(X, y, sample_weight=sample_weight)","94","","95","    assert_array_almost_equal(huber.coef_ \/ scale, huber_coef \/ scale)","96","    assert_array_almost_equal(huber.intercept_ \/ scale,","97","                              huber_intercept \/ scale)","101","    huber_sparse = HuberRegressor(fit_intercept=True)","102","    huber_sparse.fit(X_csr, y, sample_weight=sample_weight)","103","    assert_array_almost_equal(huber_sparse.coef_ \/ scale,","104","                              huber_coef \/ scale)","116","    assert_array_equal(huber.outliers_, huber_sparse.outliers_)","123","    huber = HuberRegressor(fit_intercept=False, alpha=0.0, max_iter=100)","126","    assert_false(np.all(n_outliers_mask_1))","130","    assert_array_equal(n_outliers_mask_2, n_outliers_mask_1)","140","    X, y = make_regression_with_outliers(n_samples=10, n_features=2)","153","        alpha=0.0, loss=\"huber\", shuffle=True, random_state=0, n_iter=10000,"],"delete":["66","    huber = HuberRegressor(fit_intercept=True, alpha=0.1)","72","    assert_array_almost_equal(huber.coef_, huber_coef)","73","    assert_array_almost_equal(huber.intercept_, huber_intercept)","81","    huber.fit(X, y, sample_weight=[1, 3, 1, 2, 1])","82","    assert_array_almost_equal(huber.coef_, huber_coef, 3)","83","    assert_array_almost_equal(huber.intercept_, huber_intercept, 3)","87","    huber_sparse = HuberRegressor(fit_intercept=True, alpha=0.1)","88","    huber_sparse.fit(X_csr, y, sample_weight=[1, 3, 1, 2, 1])","89","    assert_array_almost_equal(huber_sparse.coef_, huber_coef, 3)","107","    huber = HuberRegressor(fit_intercept=False, alpha=0.0, max_iter=100,","108","                           epsilon=1.35)","117","","118","    assert_array_equal(n_outliers_mask_2, n_outliers_mask_1)","125","    X, y = make_regression_with_outliers(n_samples=5, n_features=2)","138","        alpha=0.0, loss=\"huber\", shuffle=True, random_state=0, n_iter=1000000,"]}],"sklearn\/linear_model\/huber.py":[{"add":["247","            # Make sure to initialize the scale parameter to a strictly","248","            # positive value:","249","            parameters[-1] = 1","255","        bounds[-1][0] = np.finfo(np.float64).eps * 10","270","        if dict_['warnflag'] == 2:","271","            raise ValueError(\"HuberRegressor convergence failed:\"","272","                             \" l-BFGS-b solver terminated with %s\"","273","                             % dict_['task'].decode('ascii'))"],"delete":["252","        bounds[-1][0] = 1e-12","267",""]}]}},"7bde7e7fa7ff6e0fefd7fbb9ffa98cf407dd181b":{"changes":{"doc\/whats_new.rst":"MODIFY"},"diff":{"doc\/whats_new.rst":[{"add":["127","   - Added weighted impurity-based early stopping criterion for decision tree","128","     growth. (`#6954","129","     <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/6954>`_) by `Nelson","130","     Liu`_"],"delete":["127","     - Added weighted impurity-based early stopping criterion for decision tree","128","       growth. (`#6954","129","       <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/6954>`_) by `Nelson","130","       Liu`_"]}]}},"1220ca3048a0cc184f5d1cd9ee9c03b18e55cb3d":{"changes":{"doc\/modules\/pipeline.rst":"MODIFY"},"diff":{"doc\/modules\/pipeline.rst":[{"add":["32","The :class:`Pipeline` is built using a list of ``(key, value)`` pairs, where","33","the ``key`` is a string containing the name you want to give this step and ``value``"],"delete":["32","The :class:`Pipeline` is build using a list of ``(key, value)`` pairs, where","33","the ``key`` a string containing the name you want to give this step and ``value``"]}]}},"42120e50bb1ff4e3d443cabdf9cd680b876aeec8":{"changes":{"examples\/model_selection\/plot_learning_curve.py":"MODIFY","sklearn\/model_selection\/tests\/test_search.py":"MODIFY","sklearn\/model_selection\/tests\/test_split.py":"MODIFY","doc\/modules\/cross_validation.rst":"MODIFY","doc\/tutorial\/statistical_inference\/model_selection.rst":"MODIFY","examples\/model_selection\/plot_roc_crossval.py":"MODIFY","examples\/mixture\/plot_gmm_covariances.py":"MODIFY","sklearn\/model_selection\/_split.py":"MODIFY","examples\/ensemble\/plot_gradient_boosting_oob.py":"MODIFY","sklearn\/model_selection\/tests\/test_validation.py":"MODIFY","doc\/whats_new.rst":"MODIFY","examples\/svm\/plot_rbf_parameters.py":"MODIFY","examples\/svm\/plot_svm_scale_c.py":"MODIFY"},"diff":{"examples\/model_selection\/plot_learning_curve.py":[{"add":["103","cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)","110","cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)"],"delete":["103","cv = ShuffleSplit(n_iter=100, test_size=0.2, random_state=0)","110","cv = ShuffleSplit(n_iter=10, test_size=0.2, random_state=0)"]}],"sklearn\/model_selection\/tests\/test_search.py":[{"add":["456","    cv = KFold(n_splits=3)","468","    cv = KFold(n_splits=3)","599","    n_splits = 3","603","    grid_search = GridSearchCV(SVC(), cv=n_splits, iid=False,","606","    grid_search_iid = GridSearchCV(SVC(), cv=n_splits, iid=True,","647","    n_splits = 3","650","    random_search = RandomizedSearchCV(SVC(), n_iter=n_search_iter,","651","                                       cv=n_splits,","655","                                           cv=n_splits, iid=True,","782","    n_splits = 3","787","        grid_search = GridSearchCV(clf, {'C': Cs}, scoring=score, cv=n_splits)","794","                               for cv_i in range(n_splits)))","797","        cv = StratifiedKFold(n_splits=n_splits)"],"delete":["456","    cv = KFold(n_folds=3)","468","    cv = KFold(n_folds=3)","599","    n_folds = 3","603","    grid_search = GridSearchCV(SVC(), cv=n_folds, iid=False,","606","    grid_search_iid = GridSearchCV(SVC(), cv=n_folds, iid=True,","647","    n_folds = 3","650","    random_search = RandomizedSearchCV(SVC(), n_iter=n_search_iter, cv=n_folds,","654","                                           cv=n_folds, iid=True,","781","    n_folds = 3","786","        grid_search = GridSearchCV(clf, {'C': Cs}, scoring=score, cv=n_folds)","793","                               for cv_i in range(n_folds)))","796","        cv = StratifiedKFold(n_folds=n_folds)"]}],"sklearn\/model_selection\/tests\/test_split.py":[{"add":["134","    n_splits = 2","136","    n_shuffle_splits = 10  # (the default value)","144","    kf = KFold(n_splits)","145","    skf = StratifiedKFold(n_splits)","153","    kf_repr = \"KFold(n_splits=2, random_state=None, shuffle=False)\"","154","    skf_repr = \"StratifiedKFold(n_splits=2, random_state=None, shuffle=False)\"","157","    ss_repr = (\"ShuffleSplit(n_splits=10, random_state=0, test_size=0.1, \"","161","    n_splits_expected = [n_samples, comb(n_samples, p), n_splits, n_splits,","162","                         n_unique_labels, comb(n_unique_labels, p),","163","                         n_shuffle_splits, 2]","170","        assert_equal(n_splits_expected[i], cv.get_n_splits(X, y, labels))","197","def check_cv_coverage(cv, X, y, labels, expected_n_splits=None):","200","    if expected_n_splits is not None:","201","        assert_equal(cv.get_n_splits(X, y, labels), expected_n_splits)","203","        expected_n_splits = cv.get_n_splits(X, y, labels)","213","    assert_equal(iterations, expected_n_splits)","237","        check_cv_coverage(skf_3, X2, y, labels=None, expected_n_splits=3)","240","    # classes are less than n_splits.","255","    # When n_splits is not integer:","262","    assert_raises(TypeError, KFold, n_splits=4, shuffle=None)","269","    check_cv_coverage(kf, X1, y=None, labels=None, expected_n_splits=3)","275","    check_cv_coverage(kf, X2, y=None, labels=None, expected_n_splits=3)","444","    check_cv_coverage(kf0, X_40, y, labels=None, expected_n_splits=5)","459","    n_splits = 3","461","    cv = KFold(n_splits=n_splits, shuffle=False)","470","    cv = KFold(n_splits, shuffle=True, random_state=0)","474","    cv = KFold(n_splits, shuffle=True, random_state=1)","485","    cv = StratifiedKFold(n_splits)","565","    n_splits = 1000","580","        splits = StratifiedShuffleSplit(n_splits=n_splits,","586","        n_splits_actual = 0","588","            n_splits_actual += 1","592","        assert_equal(n_splits_actual, n_splits)","619","    sss = StratifiedShuffleSplit(n_splits=1,","620","                                 test_size=0.5, random_state=0)","622","    train, test = next(iter(sss.split(X=X, y=y)))","656","        n_splits = 6","658","        slo = LabelShuffleSplit(n_splits, test_size=test_size, random_state=0)","664","        assert_equal(slo.get_n_splits(X, y, labels=l), n_splits)","909","    n_splits = 5","917","    ideal_n_labels_per_fold = n_samples \/\/ n_splits","922","    lkf = LabelKFold(n_splits=n_splits)","952","    n_splits = 5","954","    ideal_n_labels_per_fold = n_samples \/\/ n_splits","983","    assert_raises_regexp(ValueError, \"Cannot have number of splits.*greater\",","984","                         next, LabelKFold(n_splits=3).split(X, y, labels))","995","           StratifiedShuffleSplit(n_splits=3, random_state=0)]"],"delete":["134","    n_folds = 2","136","    n_iter = 10  # (the default value)","144","    kf = KFold(n_folds)","145","    skf = StratifiedKFold(n_folds)","153","    kf_repr = \"KFold(n_folds=2, random_state=None, shuffle=False)\"","154","    skf_repr = \"StratifiedKFold(n_folds=2, random_state=None, shuffle=False)\"","157","    ss_repr = (\"ShuffleSplit(n_iter=10, random_state=0, test_size=0.1, \"","161","    n_splits = [n_samples, comb(n_samples, p), n_folds, n_folds,","162","                n_unique_labels, comb(n_unique_labels, p), n_iter, 2]","169","        assert_equal(n_splits[i], cv.get_n_splits(X, y, labels))","196","def check_cv_coverage(cv, X, y, labels, expected_n_iter=None):","199","    if expected_n_iter is not None:","200","        assert_equal(cv.get_n_splits(X, y, labels), expected_n_iter)","202","        expected_n_iter = cv.get_n_splits(X, y, labels)","212","    assert_equal(iterations, expected_n_iter)","236","        check_cv_coverage(skf_3, X2, y, labels=None, expected_n_iter=3)","239","    # classes are less than n_folds.","254","    # When n_folds is not integer:","261","    assert_raises(TypeError, KFold, n_folds=4, shuffle=None)","268","    check_cv_coverage(kf, X1, y=None, labels=None, expected_n_iter=3)","274","    check_cv_coverage(kf, X2, y=None, labels=None, expected_n_iter=3)","443","    check_cv_coverage(kf0, X_40, y, labels=None, expected_n_iter=5)","458","    n_folds = 3","460","    cv = KFold(n_folds=n_folds, shuffle=False)","469","    cv = KFold(n_folds, shuffle=True, random_state=0)","473","    cv = KFold(n_folds, shuffle=True, random_state=1)","484","    cv = StratifiedKFold(n_folds)","564","    n_iter = 1000","579","        splits = StratifiedShuffleSplit(n_iter=n_iter,","585","        n_splits = 0","587","            n_splits += 1","591","        assert_equal(n_splits, n_iter)","618","    splits = StratifiedShuffleSplit(n_iter=1,","619","                                    test_size=0.5, random_state=0)","621","    train, test = next(iter(splits.split(X=X, y=y)))","655","        n_iter = 6","657","        slo = LabelShuffleSplit(n_iter, test_size=test_size, random_state=0)","663","        assert_equal(slo.get_n_splits(X, y, labels=l), n_iter)","908","    n_folds = 5","916","    ideal_n_labels_per_fold = n_samples \/\/ n_folds","921","    lkf = LabelKFold(n_folds=n_folds)","951","    n_folds = 5","953","    ideal_n_labels_per_fold = n_samples \/\/ n_folds","982","    assert_raises_regexp(ValueError, \"Cannot have number of folds.*greater\",","983","                         next, LabelKFold(n_folds=3).split(X, y, labels))","994","           StratifiedShuffleSplit(n_iter=3, random_state=0)]"]}],"doc\/modules\/cross_validation.rst":[{"add":["139","  >>> cv = ShuffleSplit(n_splits=3, test_size=0.3, random_state=0)","226","  >>> kf = KFold(n_splits=2)","255","  >>> skf = StratifiedKFold(n_splits=3)","280","  >>> lkf = LabelKFold(n_splits=3)","456","  >>> ss = ShuffleSplit(n_splits=3, test_size=0.25,","487","  >>> lss = LabelShuffleSplit(n_splits=4, test_size=0.5, random_state=0)"],"delete":["139","  >>> cv = ShuffleSplit(n_iter=3, test_size=0.3, random_state=0)","226","  >>> kf = KFold(n_folds=2)","255","  >>> skf = StratifiedKFold(n_folds=3)","280","  >>> lkf = LabelKFold(n_folds=3)","456","  >>> ss = ShuffleSplit(n_iter=3, test_size=0.25,","487","  >>> lss = LabelShuffleSplit(n_iter=4, test_size=0.5, random_state=0)"]}],"doc\/tutorial\/statistical_inference\/model_selection.rst":[{"add":["63","    >>> k_fold = KFold(n_splits=3)","72","    >>> kfold = KFold(n_splits=3)","108","    - :class:`KFold` **(n_splits, shuffle, random_state)**","112","    - :class:`LabelKFold` **(n_splits, shuffle, random_state)**"],"delete":["63","    >>> k_fold = KFold(n_folds=3)","72","    >>> kfold = KFold(n_folds=3)","108","    - :class:`KFold` **(n_folds, shuffle, random_state)**","112","    - :class:`LabelKFold` **(n_folds, shuffle, random_state)**"]}],"examples\/model_selection\/plot_roc_crossval.py":[{"add":["60","cv = StratifiedKFold(n_splits=6)"],"delete":["60","cv = StratifiedKFold(n_folds=6)"]}],"examples\/mixture\/plot_gmm_covariances.py":[{"add":["71","skf = StratifiedKFold(n_splits=4)"],"delete":["71","skf = StratifiedKFold(n_folds=4)"]}],"sklearn\/model_selection\/_split.py":[{"add":["124","    Note: ``LeaveOneOut()`` is equivalent to ``KFold(n_splits=n)`` and","199","    ``KFold(n_splits=n_samples \/\/ p)`` which creates non-overlapping test sets.","266","    def __init__(self, n_splits, shuffle, random_state):","267","        if not isinstance(n_splits, numbers.Integral):","270","                             % (n_splits, type(n_splits)))","271","        n_splits = int(n_splits)","273","        if n_splits <= 1:","276","                \" train\/test split by setting n_splits=2 or more,\"","277","                \" got n_splits={0}.\".format(n_splits))","283","        self.n_splits = n_splits","313","        if self.n_splits > n_samples:","315","                (\"Cannot have number of splits n_splits={0} greater\"","316","                 \" than the number of samples: {1}.\").format(self.n_splits,","341","        return self.n_splits","357","    n_splits : int, default=3","372","    >>> kf = KFold(n_splits=2)","376","    KFold(n_splits=2, random_state=None, shuffle=False)","386","    The first ``n_samples % n_splits`` folds have size","387","    ``n_samples \/\/ n_splits + 1``, other folds have size","388","    ``n_samples \/\/ n_splits``, where ``n_samples`` is the number of samples.","400","    def __init__(self, n_splits=3, shuffle=False,","402","        super(KFold, self).__init__(n_splits, shuffle, random_state)","410","        n_splits = self.n_splits","411","        fold_sizes = (n_samples \/\/ n_splits) * np.ones(n_splits, dtype=np.int)","412","        fold_sizes[:n_samples % n_splits] += 1","431","    n_splits : int, default=3","440","    >>> label_kfold = LabelKFold(n_splits=2)","444","    LabelKFold(n_splits=2)","466","    def __init__(self, n_splits=3):","467","        super(LabelKFold, self).__init__(n_splits, shuffle=False,","477","        if self.n_splits > n_labels:","478","            raise ValueError(\"Cannot have number of splits n_splits=%d greater\"","480","                             % (self.n_splits, n_labels))","490","        n_samples_per_fold = np.zeros(self.n_splits)","503","        for f in range(self.n_splits):","520","    n_splits : int, default=3","536","    >>> skf = StratifiedKFold(n_splits=2)","540","    StratifiedKFold(n_splits=2, random_state=None, shuffle=False)","550","    All the folds have size ``trunc(n_samples \/ n_splits)``, the last one has","555","    def __init__(self, n_splits=3, shuffle=False, random_state=None):","556","        super(StratifiedKFold, self).__init__(n_splits, shuffle, random_state)","568","        if np.all(self.n_splits > y_counts):","570","                             \" are less than n_splits=%d.\"","571","                             % (self.n_splits))","572","        if self.n_splits > min_labels:","576","                           \" be less than n_splits=%d.\"","577","                           % (min_labels, self.n_splits)), Warning)","584","        # So we pass np.zeroes(max(c, n_splits)) as data to the KFold","586","            KFold(self.n_splits, shuffle=self.shuffle,","587","                  random_state=rng).split(np.zeros(max(count, self.n_splits)))","595","                # KFold(...).split(X[:max(c, n_splits)]) when data is not 100%","607","        for i in range(self.n_splits):","636","","806","    def __init__(self, n_splits=10, test_size=0.1, train_size=None,","809","        self.n_splits = n_splits","865","        return self.n_splits","884","    n_splits : int (default 10)","907","    >>> rs = ShuffleSplit(n_splits=3, test_size=.25, random_state=0)","911","    ShuffleSplit(n_splits=3, random_state=0, test_size=0.25, train_size=None)","918","    >>> rs = ShuffleSplit(n_splits=3, train_size=0.5, test_size=.25,","933","        for i in range(self.n_splits):","958","    ``LabelShuffleSplit(test_size=10, n_splits=100)``.","966","    n_splits : int (default 5)","985","    def __init__(self, n_splits=5, test_size=0.2, train_size=None,","988","            n_splits=n_splits,","1025","    n_splits : int (default 10)","1048","    >>> sss = StratifiedShuffleSplit(n_splits=3, test_size=0.5, random_state=0)","1052","    StratifiedShuffleSplit(n_splits=3, random_state=0, ...)","1062","    def __init__(self, n_splits=10, test_size=0.1, train_size=None,","1065","            n_splits, test_size, train_size, random_state)","1096","        for _ in range(self.n_splits):"],"delete":["124","    Note: ``LeaveOneOut()`` is equivalent to ``KFold(n_folds=n)`` and","199","    ``KFold(n_folds=n_samples \/\/ p)`` which creates non-overlapping test sets.","266","    def __init__(self, n_folds, shuffle, random_state):","267","        if not isinstance(n_folds, numbers.Integral):","270","                             % (n_folds, type(n_folds)))","271","        n_folds = int(n_folds)","273","        if n_folds <= 1:","276","                \" train\/test split by setting n_folds=2 or more,\"","277","                \" got n_folds={0}.\".format(n_folds))","283","        self.n_folds = n_folds","313","        if self.n_folds > n_samples:","315","                (\"Cannot have number of folds n_folds={0} greater\"","316","                 \" than the number of samples: {1}.\").format(self.n_folds,","341","        return self.n_folds","357","    n_folds : int, default=3","372","    >>> kf = KFold(n_folds=2)","376","    KFold(n_folds=2, random_state=None, shuffle=False)","386","    The first ``n_samples % n_folds`` folds have size","387","    ``n_samples \/\/ n_folds + 1``, other folds have size","388","    ``n_samples \/\/ n_folds``, where ``n_samples`` is the number of samples.","400","    def __init__(self, n_folds=3, shuffle=False,","402","        super(KFold, self).__init__(n_folds, shuffle, random_state)","410","        n_folds = self.n_folds","411","        fold_sizes = (n_samples \/\/ n_folds) * np.ones(n_folds, dtype=np.int)","412","        fold_sizes[:n_samples % n_folds] += 1","431","    n_folds : int, default=3","440","    >>> label_kfold = LabelKFold(n_folds=2)","444","    LabelKFold(n_folds=2)","466","    def __init__(self, n_folds=3):","467","        super(LabelKFold, self).__init__(n_folds, shuffle=False,","477","        if self.n_folds > n_labels:","478","            raise ValueError(\"Cannot have number of folds n_folds=%d greater\"","480","                             % (self.n_folds, n_labels))","490","        n_samples_per_fold = np.zeros(self.n_folds)","503","        for f in range(self.n_folds):","520","    n_folds : int, default=3","536","    >>> skf = StratifiedKFold(n_folds=2)","540","    StratifiedKFold(n_folds=2, random_state=None, shuffle=False)","550","    All the folds have size ``trunc(n_samples \/ n_folds)``, the last one has","555","    def __init__(self, n_folds=3, shuffle=False, random_state=None):","556","        super(StratifiedKFold, self).__init__(n_folds, shuffle, random_state)","568","        if np.all(self.n_folds > y_counts):","570","                             \" are less than %d folds.\"","571","                             % (self.n_folds))","572","        if self.n_folds > min_labels:","576","                           \" be less than n_folds=%d.\"","577","                           % (min_labels, self.n_folds)), Warning)","584","        # So we pass np.zeroes(max(c, n_folds)) as data to the KFold","586","            KFold(self.n_folds, shuffle=self.shuffle,","587","                  random_state=rng).split(np.zeros(max(count, self.n_folds)))","595","                # KFold(...).split(X[:max(c, n_folds)]) when data is not 100%","607","        for i in range(self.n_folds):","805","    def __init__(self, n_iter=10, test_size=0.1, train_size=None,","808","        self.n_iter = n_iter","864","        return self.n_iter","883","    n_iter : int (default 10)","906","    >>> rs = ShuffleSplit(n_iter=3, test_size=.25, random_state=0)","910","    ShuffleSplit(n_iter=3, random_state=0, test_size=0.25, train_size=None)","917","    >>> rs = ShuffleSplit(n_iter=3, train_size=0.5, test_size=.25,","932","        for i in range(self.n_iter):","957","    ``LabelShuffleSplit(test_size=10, n_iter=100)``.","965","    n_iter : int (default 5)","984","    def __init__(self, n_iter=5, test_size=0.2, train_size=None,","987","            n_iter=n_iter,","1024","    n_iter : int (default 10)","1047","    >>> sss = StratifiedShuffleSplit(n_iter=3, test_size=0.5, random_state=0)","1051","    StratifiedShuffleSplit(n_iter=3, random_state=0, ...)","1061","    def __init__(self, n_iter=10, test_size=0.1, train_size=None,","1064","            n_iter, test_size, train_size, random_state)","1095","        for _ in range(self.n_iter):"]}],"examples\/ensemble\/plot_gradient_boosting_oob.py":[{"add":["76","def cv_estimate(n_splits=3):","77","    cv = KFold(n_splits=n_splits)","83","    val_scores \/= n_splits"],"delete":["76","def cv_estimate(n_folds=3):","77","    cv = KFold(n_folds=n_folds)","83","    val_scores \/= n_folds"]}],"sklearn\/model_selection\/tests\/test_validation.py":[{"add":["138","# The number of samples per class needs to be > n_splits,","139","# for StratifiedKFold(n_splits=3)","704","    cv = KFold(n_splits=3)"],"delete":["138","# The number of samples per class needs to be > n_folds, for StratifiedKFold(3)","703","    cv = KFold(n_folds=3)"]}],"doc\/whats_new.rst":[{"add":["64","  - **Parameters ``n_folds`` and ``n_iter`` renamed to ``n_splits``**","65","","66","    Some parameter names have changed: ","67","    The ``n_folds`` parameter in :class:`model_selection.KFold`, ","68","    :class:`model_selection.LabelKFold`, and ","69","    :class:`model_selection.StratifiedKFold` is now renamed to ``n_splits``.","70","    The ``n_iter`` parameter in :class:`model_selection.ShuffleSplit`,","71","    :class:`model_selection.LabelShuffleSplit`, ","72","    and :class:`model_selection.StratifiedShuffleSplit` is now renamed ","73","    to ``n_splits``.","74","","373","   - The parameters ``n_iter`` or ``n_folds`` in old CV splitters are replaced","374","     by the new parameter ``n_splits`` since it can provide a consistent ","375","     and unambiguous interface to represent the number of train-test splits.","376","     (`#7187 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/7187>`_)","377","     by `YenChen Lin`_.","378",""],"delete":[]}],"examples\/svm\/plot_rbf_parameters.py":[{"add":["61","smoothed out by increasing the number of CV iterations ``n_splits`` at the","130","cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)"],"delete":["61","smoothed out by increasing the number of CV iterations ``n_iter`` at the","130","cv = StratifiedShuffleSplit(n_iter=5, test_size=0.2, random_state=42)"]}],"examples\/svm\/plot_svm_scale_c.py":[{"add":["130","                            cv=ShuffleSplit(train_size=train_size,","131","                                            n_splits=250, random_state=1))"],"delete":["130","                            cv=ShuffleSplit(train_size=train_size, n_iter=250,","131","                                            random_state=1))"]}]}},"12d5f078313f280eddfc590296d51473257ea8d2":{"changes":{"sklearn\/utils\/_random.pyx":"MODIFY","sklearn\/linear_model\/tests\/test_logistic.py":"MODIFY"},"diff":{"sklearn\/utils\/_random.pyx":[{"add":[],"delete":["0","# cython: cdivision=True"]}],"sklearn\/linear_model\/tests\/test_logistic.py":[{"add":["632","        penalty=\"l1\", tol=1e-5)","635","        solver=\"liblinear\", fit_intercept=False, penalty=\"l1\", tol=1e-5)"],"delete":["632","        penalty=\"l1\")","635","        solver=\"liblinear\", fit_intercept=False, penalty=\"l1\")"]}]}},"7b3c467205206fa031de2ae9ddf6f1cddaf1bdd8":{"changes":{"doc\/whats_new.rst":"MODIFY"},"diff":{"doc\/whats_new.rst":[{"add":["331","    - :func:`silhouette_score` now again supports sparse input, and this also fixes","332","      examples\/text\/document_clustering.py. ","333","      By `YenChen Lin`_.","334",""],"delete":[]}]}},"928f72447eedbc16b927f40a87a67cac03fd4974":{"changes":{"sklearn\/datasets\/_svmlight_format.pyx":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/datasets\/svmlight_format.py":"MODIFY","sklearn\/datasets\/tests\/test_svmlight_format.py":"MODIFY"},"diff":{"sklearn\/datasets\/_svmlight_format.pyx":[{"add":["29","    cdef array.array data, indices, indptr","47","    query = np.arange(0, dtype=np.int64)","82","                query.resize(len(query) + 1)","83","                query[len(query) - 1] = np.int64(value)"],"delete":["29","    cdef array.array data, indices, indptr, query","47","    query = array.array(\"i\")","82","                array.resize_smart(query, len(query) + 1)","83","                query[len(query) - 1] = int(value)"]}],"doc\/whats_new.rst":[{"add":["314","    - :func:`datasets.load_svmlight_file` now is able to read long int QID values.","315","      (`#7101 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/7101>`_)","316","      By `Ibraim Ganiev`_.","317","","4318","","4319",".. _Ibraim Ganiev: https:\/\/github.com\/olologin"],"delete":[]}],"sklearn\/datasets\/svmlight_format.py":[{"add":["168","    query = frombuffer_empty(query, np.int64)"],"delete":["168","    query = frombuffer_empty(query, np.intc)"]}],"sklearn\/datasets\/tests\/test_svmlight_format.py":[{"add":["370","","371","","372","def test_load_with_long_qid():","373","    # load svmfile with longint qid attribute","374","    data = b(\"\"\"","375","    1 qid:0 0:1 1:2 2:3","376","    0 qid:72048431380967004 0:1440446648 1:72048431380967004 2:236784985","377","    0 qid:-9223372036854775807 0:1440446648 1:72048431380967004 2:236784985","378","    3 qid:9223372036854775807  0:1440446648 1:72048431380967004 2:236784985\"\"\")","379","    X, y, qid = load_svmlight_file(BytesIO(data), query_id=True)","380","","381","    true_X = [[1,          2,                 3],","382","             [1440446648, 72048431380967004, 236784985],","383","             [1440446648, 72048431380967004, 236784985],","384","             [1440446648, 72048431380967004, 236784985]]","385","","386","    true_y = [1, 0, 0, 3]","387","    trueQID = [0, 72048431380967004, -9223372036854775807, 9223372036854775807]","388","    assert_array_equal(y, true_y)","389","    assert_array_equal(X.toarray(), true_X)","390","    assert_array_equal(qid, trueQID)","391","","392","    f = BytesIO()","393","    dump_svmlight_file(X, y, f, query_id=qid, zero_based=True)","394","    f.seek(0)","395","    X, y, qid = load_svmlight_file(f, query_id=True, zero_based=True)","396","    assert_array_equal(y, true_y)","397","    assert_array_equal(X.toarray(), true_X)","398","    assert_array_equal(qid, trueQID)","399","","400","    f.seek(0)","401","    X, y = load_svmlight_file(f, query_id=False, zero_based=True)","402","    assert_array_equal(y, true_y)","403","    assert_array_equal(X.toarray(), true_X)"],"delete":[]}]}},"89247fcd7f02bbd92c9da141250ec8b82caf8dd0":{"changes":{"sklearn\/linear_model\/sgd_fast.pyx":"MODIFY"},"diff":{"sklearn\/linear_model\/sgd_fast.pyx":[{"add":["169","cdef class SquaredHinge(Classification):"],"delete":["169","cdef class SquaredHinge(LossFunction):"]}]}},"a627a079c6dc924ba93904079e773c95bbc4482d":{"changes":{"sklearn\/neighbors\/tests\/test_neighbors.py":"MODIFY"},"diff":{"sklearn\/neighbors\/tests\/test_neighbors.py":[{"add":["944","        results = {}","960","            results[algorithm] = neigh.kneighbors(test, return_distance=True)","961","        assert_array_almost_equal(results['brute'][0], results['ball_tree'][0])","962","        assert_array_almost_equal(results['brute'][1], results['ball_tree'][1])","963","        if 'kd_tree' in results:","964","            assert_array_almost_equal(results['brute'][0],","965","                                      results['kd_tree'][0])","966","            assert_array_almost_equal(results['brute'][1],","967","                                      results['kd_tree'][1])"],"delete":["944","        results = []","955","","961","            results.append(neigh.kneighbors(test, return_distance=True))","962","","963","        assert_array_almost_equal(results[0][0], results[1][0])","964","        assert_array_almost_equal(results[0][1], results[1][1])"]}]}},"f0862f7af379609c61789a8afa68eaa08b765a3c":{"changes":{"sklearn\/mixture\/tests\/test_dpgmm.py":"MODIFY","sklearn\/mixture\/tests\/test_gaussian_mixture.py":"MODIFY","doc\/modules\/classes.rst":"MODIFY","sklearn\/mixture\/__init__.py":"MODIFY","sklearn\/mixture\/bayesian_mixture.py":"ADD","sklearn\/mixture\/base.py":"MODIFY","sklearn\/mixture\/dpgmm.py":"MODIFY","doc\/whats_new.rst":"MODIFY","examples\/mixture\/plot_bayesian_gaussian_mixture.py":"ADD","sklearn\/mixture\/tests\/test_bayesian_mixture.py":"ADD","doc\/modules\/mixture.rst":"MODIFY","sklearn\/mixture\/gaussian_mixture.py":"MODIFY"},"diff":{"sklearn\/mixture\/tests\/test_dpgmm.py":[{"add":["185","    assert_warns_message(DeprecationWarning, \"The VBGMM class is not working \"","186","                         \"correctly and it's better to use \"","187","                         \"sklearn.mixture.BayesianGaussianMixture class \"","188","                         \"instead. VBGMM is deprecated in 0.18 and will be \"","189","                         \"removed in 0.20.\", VBGMM)"],"delete":["185","    assert_warns_message(","186","        DeprecationWarning,","187","        \"The VBGMM class is not working correctly and it's better to not use \"","188","        \"it. VBGMM is deprecated in 0.18 and will be removed in 0.20.\", VBGMM)"]}],"sklearn\/mixture\/tests\/test_gaussian_mixture.py":[{"add":["885","                                 \"Fitting the mixture model failed because \"","886","                                 \"some components have ill-defined empirical \"","887","                                 \"covariance (for instance caused by \"","888","                                 \"singleton or collapsed samples). Try to \"","889","                                 \"decrease the number of components, or \"","890","                                 \"increase reg_covar.\", gmm.fit, X)"],"delete":["885","                                 \"The algorithm has diverged because of too \"","886","                                 \"few samples per components. \"","887","                                 \"Try to decrease the number of components, \"","888","                                 \"or increase reg_covar.\", gmm.fit, X)"]}],"doc\/modules\/classes.rst":[{"add":["956","   mixture.BayesianGaussianMixture"],"delete":["957","   mixture.VBGMM"]}],"sklearn\/mixture\/__init__.py":[{"add":["10","from .bayesian_mixture import BayesianGaussianMixture","20","           'GaussianMixture',","21","           'BayesianGaussianMixture']"],"delete":["19","           'GaussianMixture']"]}],"sklearn\/mixture\/bayesian_mixture.py":[{"add":[],"delete":[]}],"sklearn\/mixture\/base.py":[{"add":["249","            Logarithm of the probability of each sample in X.","252","            Logarithm of the posterior probabilities (or responsibilities) of","253","            the point of each sample in X.","255","        log_prob_norm, log_resp = self._estimate_log_prob_resp(X)","256","        return np.mean(log_prob_norm), log_resp","267","            Logarithm of the posterior probabilities (or responsibilities) of","268","            the point of each sample in X."],"delete":["239","    @abstractmethod","250","            log p(X)","253","            logarithm of the responsibilities","255","        pass"]}],"sklearn\/mixture\/dpgmm.py":[{"add":["633","            \"to use sklearn.mixture.BayesianGaussianMixture class instead. \"","634","            \"VBGMM is deprecated in 0.18 and will be removed in 0.20.\")"],"delete":["633","            \"to not use it. VBGMM is deprecated in 0.18 and \"","634","            \"will be removed in 0.20.\")"]}],"doc\/whats_new.rst":[{"add":["66","    Some parameter names have changed:","67","    The ``n_folds`` parameter in :class:`model_selection.KFold`,","68","    :class:`model_selection.LabelKFold`, and","71","    :class:`model_selection.LabelShuffleSplit`,","72","    and :class:`model_selection.StratifiedShuffleSplit` is now renamed","143","   - Added new cross-validation splitter","144","     :class:`model_selection.TimeSeriesSplit` to handle time series data.","404","   - The old :class:`VBGMM` is deprecated in favor of the new","405","     :class:`BayesianGaussianMixture`. The new class solves the computational","406","     problems of the old class and computes the Variational Bayesian Gaussian","407","     mixture faster than before.","408","     Ref :ref:`b` for more information.","409","     (`#6651 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/6651>`_) by","410","     `Wei Xue`_ and `Thierry Guillemot`_.","411","","415","     (`#6666 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/6666>`_) by","416","     `Wei Xue`_ and `Thierry Guillemot`_.","426","     by the new parameter ``n_splits`` since it can provide a consistent"],"delete":["66","    Some parameter names have changed: ","67","    The ``n_folds`` parameter in :class:`model_selection.KFold`, ","68","    :class:`model_selection.LabelKFold`, and ","71","    :class:`model_selection.LabelShuffleSplit`, ","72","    and :class:`model_selection.StratifiedShuffleSplit` is now renamed ","143","   - Added new cross-validation splitter ","144","     :class:`model_selection.TimeSeriesSplit` to handle time series data. ","407","     By `Wei Xue`_ and `Thierry Guillemot`_.","417","     by the new parameter ``n_splits`` since it can provide a consistent "]}],"examples\/mixture\/plot_bayesian_gaussian_mixture.py":[{"add":[],"delete":[]}],"sklearn\/mixture\/tests\/test_bayesian_mixture.py":[{"add":[],"delete":[]}],"doc\/modules\/mixture.rst":[{"add":["135",".. _bgmm:","137","Bayesian Gaussian Mixture","138","=========================","140","The :class:`BayesianGaussianMixture` object implements a variant of the Gaussian","141","mixture model with variational inference algorithms.","143",".. _variational_inference:","144","","145","Estimation algorithm: variational inference","146","---------------------------------------------","147","","148","Variational inference is an extension of expectation-maximization that","149","maximizes a lower bound on model evidence (including","150","priors) instead of data likelihood. The principle behind","151","variational methods is the same as expectation-maximization (that is","152","both are iterative algorithms that alternate between finding the","153","probabilities for each point to be generated by each mixture and","154","fitting the mixtures to these assigned points), but variational","155","methods add regularization by integrating information from prior","156","distributions. This avoids the singularities often found in","157","expectation-maximization solutions but introduces some subtle biases","158","to the model. Inference is often notably slower, but not usually as","159","much so as to render usage unpractical.","160","","161","Due to its Bayesian nature, the variational algorithm needs more","162","hyper-parameters than expectation-maximization, the most","163","important of these being the concentration parameter ``dirichlet_concentration_prior``. Specifying","164","a high value of prior of the dirichlet concentration leads more often to uniformly-sized mixture","165","components, while specifying small (between 0 and 1) values will lead","166","to some mixture components getting almost all the points while most","167","mixture components will be centered on just a few of the remaining","168","points.","169","","170",".. figure:: ..\/auto_examples\/mixture\/images\/sphx_glr_plot_bayesian_gaussian_mixture_001.png","171","   :target: ..\/auto_examples\/mixture\/plot_bayesian_gaussian_mixture.html","172","   :align: center","173","   :scale: 50%","174","","175",".. topic:: Examples:","176","","177","    * See :ref:`plot_bayesian_gaussian_mixture.py` for a comparaison of","178","      the results of the ``BayesianGaussianMixture`` for different values","179","      of the parameter ``dirichlet_concentration_prior``.","180","","181","Pros and cons of variational inference with :class:BayesianGaussianMixture","182","--------------------------------------------------------------------------","189","   expectation-maximization solutions.","190","","191",":Automatic selection: when `dirichlet_concentration_prior` is small enough and","192","`n_components` is larger than what is found necessary by the model, the","193","Variational Bayesian mixture model has a natural tendency to set some mixture","194","weights values close to zero. This makes it possible to let the model choose a","195","suitable number of effective components automatically."],"delete":["135",".. _vbgmm:","137","VBGMM: variational Gaussian mixtures","138","====================================","140","The :class:`VBGMM` object implements a variant of the Gaussian mixture","141","model with :ref:`variational inference <variational_inference>` algorithms.","143","Pros and cons of class :class:`VBGMM`: variational inference","144","------------------------------------------------------------","151","   expectation-maximization solutions. One can then use full","152","   covariance matrices in high dimensions or in cases where some","153","   components might be centered around a single point without","154","   risking divergence.","170",".. _variational_inference:","171","","172","Estimation algorithm: variational inference","173","---------------------------------------------","174","","175","Variational inference is an extension of expectation-maximization that","176","maximizes a lower bound on model evidence (including","177","priors) instead of data likelihood.  The principle behind","178","variational methods is the same as expectation-maximization (that is","179","both are iterative algorithms that alternate between finding the","180","probabilities for each point to be generated by each mixture and","181","fitting the mixtures to these assigned points), but variational","182","methods add regularization by integrating information from prior","183","distributions. This avoids the singularities often found in","184","expectation-maximization solutions but introduces some subtle biases","185","to the model. Inference is often notably slower, but not usually as","186","much so as to render usage unpractical.","187","","188","Due to its Bayesian nature, the variational algorithm needs more","189","hyper-parameters than expectation-maximization, the most","190","important of these being the concentration parameter ``alpha``. Specifying","191","a high value of alpha leads more often to uniformly-sized mixture","192","components, while specifying small (between 0 and 1) values will lead","193","to some mixture components getting almost all the points while most","194","mixture components will be centered on just a few of the remaining","195","points.","196",""]}],"sklearn\/mixture\/gaussian_mixture.py":[{"add":["194","    covariance \/= nk.sum()","307","        \"Fitting the mixture model failed because some components have \"","308","        \"ill-defined empirical covariance (for instance caused by singleton \"","309","        \"or collapsed samples). Try to decrease the number of components, \"","360","        The determinant of the precision matrix for each component.","457","        lower bound average gain is below this threshold.","558","","559","    See Also","560","    --------","561","    BayesianGaussianMixture : Finite gaussian mixture model fit with a","562","        variational algorithm.","637","    def _m_step(self, X, log_resp):","638","        \"\"\"M step.","640","        Parameters","641","        ----------","642","        X : array-like, shape (n_samples, n_features)","643","","644","        log_resp : array-like, shape (n_samples, n_components)","645","            Logarithm of the posterior probabilities (or responsibilities) of","646","            the point of each sample in X.","647","        \"\"\"","650","            _estimate_gaussian_parameters(X, np.exp(log_resp), self.reg_covar,"],"delete":["191","    n_samples, _ = X.shape","195","    covariance \/= n_samples","308","        \"The algorithm has diverged because of too few samples per \"","309","        \"components. Try to decrease the number of components, \"","360","        The determinant of the cholesky decomposition.","361","        matrix.","458","        log_likelihood average gain is below this threshold.","633","    def _e_step(self, X):","634","        log_prob_norm, log_resp = self._estimate_log_prob_resp(X)","635","        return np.mean(log_prob_norm), np.exp(log_resp)","637","    def _m_step(self, X, resp):","640","            _estimate_gaussian_parameters(X, resp, self.reg_covar,"]}]}},"71617558dea900489926d88d80a55f952e6a9e07":{"changes":{"doc\/testimonials\/images\/ottogroup_logo.png":"ADD","doc\/testimonials\/testimonials.rst":"MODIFY"},"diff":{"doc\/testimonials\/images\/ottogroup_logo.png":[{"add":[],"delete":[]}],"doc\/testimonials\/testimonials.rst":[{"add":["800","`Otto Group <https:\/\/ottogroup.com\/>`_","801","-----------------------------------------","802","","803",".. raw:: html","804","","805","   <div class=\"logo\">","806","","807",".. image:: images\/ottogroup_logo.png","808","    :width: 120pt","809","    :target: https:\/\/ottogroup.com","810","","811",".. raw:: html","812","","813","   <\/div>","814","","815","Here at Otto Group, one of global Big Five B2C online retailers, we are using","816","scikit-learn in all aspects of our daily work from data exploration to development","817","of machine learning application to the productive deployment of those services.","818","It helps us to tackle machine learning problems ranging from e-commerce to logistics.","819","It consistent APIs enabled us to build the `Palladium REST-API framework","820","<https:\/\/github.com\/ottogroup\/palladium\/>`_ around it and continuously deliver","821","scikit-learn based services.","822","","823","","824",".. raw:: html","825","","826","  <span class=\"testimonial-author\">","827","","828","Christian Rammig, Head of Data Science, Otto Group","829","","830",".. raw:: html","831","","832","  <\/span>","833",""],"delete":[]}]}},"1f182cc0b47e55ba2e5df410a14582deeeed4610":{"changes":{"sklearn\/mixture\/base.py":"MODIFY"},"diff":{"sklearn\/mixture\/base.py":[{"add":["203","                self.lower_bound_ = -np.infty"],"delete":["203","                self.lower_bound_ = np.infty"]}]}},"0cffdfa616af487a255a6adedd516d3880d03462":{"changes":{"sklearn\/mixture\/gmm.py":"MODIFY"},"diff":{"sklearn\/mixture\/gmm.py":[{"add":["684","    if cv.shape[1] == 1:"],"delete":["684","    if covars.shape[1] == 1:"]}]}},"873f3eb6b9c6a9f10645cb76745ef19555a2f483":{"changes":{"sklearn\/neural_network\/multilayer_perceptron.py":"MODIFY","sklearn\/neural_network\/tests\/test_mlp.py":"MODIFY"},"diff":{"sklearn\/neural_network\/multilayer_perceptron.py":[{"add":["26","from ..utils.multiclass import _check_partial_fit_first_call, unique_labels","27","from ..utils.multiclass import type_of_target","271","        elif self._label_binarizer.y_type_ == 'multiclass':","492","                y_val = self._label_binarizer.inverse_transform(y_val)","900","        if not incremental:","901","            self._label_binarizer = LabelBinarizer()","902","            self._label_binarizer.fit(y)","903","            self.classes_ = self._label_binarizer.classes_","905","            classes = unique_labels(y)","906","            if np.setdiff1d(classes, self.classes_, assume_unique=True):","911","        y = self._label_binarizer.transform(y)","933","        return self._label_binarizer.inverse_transform(y_pred)","966","        if _check_partial_fit_first_call(self, classes):","967","            self._label_binarizer = LabelBinarizer()","968","            if type_of_target(y).startswith('multilabel'):","969","                self._label_binarizer.fit(y)","970","            else:","971","                self._label_binarizer.fit(classes)"],"delete":["26","from ..utils.multiclass import _check_partial_fit_first_call","270","        elif self.label_binarizer_.y_type_ == 'multiclass':","491","                y_val = self.label_binarizer_.inverse_transform(y_val)","821","    `label_binarizer_` : LabelBinarizer","822","        A LabelBinarizer object trained on the training set.","823","","896","        self.label_binarizer_ = LabelBinarizer()","897","","903","        self.label_binarizer_.fit(y)","905","        if not hasattr(self, 'classes_') or not incremental:","906","            self.classes_ = self.label_binarizer_.classes_","908","            classes = self.label_binarizer_.classes_","909","            if not np.all(np.in1d(classes, self.classes_)):","914","        y = self.label_binarizer_.transform(y)","936","        return self.label_binarizer_.inverse_transform(y_pred)","969","        _check_partial_fit_first_call(self, classes)"]}],"sklearn\/neural_network\/tests\/test_mlp.py":[{"add":["344","def test_partial_fit_unseen_classes():","345","    # Non regression test for bug 6994","346","    # Tests for labeling errors in partial fit","347","","348","    clf = MLPClassifier(random_state=0)","349","    clf.partial_fit([[1], [2], [3]], [\"a\", \"b\", \"c\"],","350","                    classes=[\"a\", \"b\", \"c\", \"d\"])","351","    clf.partial_fit([[4]], [\"d\"])","352","    assert_greater(clf.score([[1], [2], [3], [4]], [\"a\", \"b\", \"c\", \"d\"]), 0)","353","","354",""],"delete":["82","    mlp.classes_ = [0, 1]","91","    mlp.label_binarizer_.y_type_ = 'binary'","92",""]}]}}}