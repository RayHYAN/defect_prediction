{"c0a04a30e59e046f639d316e6204742923910b8d":{"changes":{"doc\/modules\/ensemble.rst":"MODIFY"},"diff":{"doc\/modules\/ensemble.rst":[{"add":["206","might result in models that consume a lot of RAM. The best parameter values"],"delete":["206","might result in models that consume a lot of ram. The best parameter values"]}]}},"89e07351509856e4fcb11fdd752dfcd34044cf57":{"changes":{"sklearn\/mixture\/gmm.py":"MODIFY","sklearn\/mixture\/tests\/test_gmm.py":"MODIFY"},"diff":{"sklearn\/mixture\/gmm.py":[{"add":["26","@deprecated(\"The function log_multivariate_normal_density is deprecated in 0.18\"","27","            \" and will be removed in 0.20.\")","652","@deprecated(\"The class GMM is deprecated in 0.18 and will be \"","653","            \" removed in 0.20. Use class GaussianMixture instead.\")","760","@deprecated(\"The functon distribute_covar_matrix_to_match_covariance_type\"","761","            \"is deprecated in 0.18 and will be removed in 0.20.\")"],"delete":["26","","651","@deprecated(\"The class GMM is deprecated and \"","652","            \"will be removed in 0.20. Use class GaussianMixture instead.\")"]}],"sklearn\/mixture\/tests\/test_gmm.py":[{"add":["14","from sklearn.utils.testing import (assert_greater, assert_raise_message,","15","                                   assert_warns_message, ignore_warnings)","82","    lpr = assert_warns_message(DeprecationWarning, \"The function\"","83","                             \" log_multivariate_normal_density is \"","84","                             \"deprecated in 0.18 and will be removed in 0.20.\",","85","                             mixture.log_multivariate_normal_density,","86","                             X, mu, cv, 'diag')","99","    lpr = assert_warns_message(DeprecationWarning, \"The function\"","100","                             \" log_multivariate_normal_density is \"","101","                             \"deprecated in 0.18 and will be removed in 0.20.\",","102","                             mixture.log_multivariate_normal_density,","103","                             X, mu, spherecv, 'spherical')","116","    lpr = assert_warns_message(DeprecationWarning, \"The function\"","117","                             \" log_multivariate_normal_density is \"","118","                             \"deprecated in 0.18 and will be removed in 0.20.\",","119","                             mixture.log_multivariate_normal_density,","120","                             X, mu, fullcv, 'full')"],"delete":["14","from sklearn.utils.testing import assert_greater","15","from sklearn.utils.testing import assert_raise_message","18","from sklearn.utils.testing import ignore_warnings","83","    lpr = mixture.log_multivariate_normal_density(X, mu, cv, 'diag')","96","    lpr = mixture.log_multivariate_normal_density(X, mu, spherecv,","97","                                                  'spherical')","100","","111","    lpr = mixture.log_multivariate_normal_density(X, mu, fullcv, 'full')"]}]}},"49fb295561948d63199da8a03ba3ca1535fb7608":{"changes":{"sklearn\/gaussian_process\/kernels.py":"MODIFY","sklearn\/gaussian_process\/tests\/test_kernels.py":"MODIFY"},"diff":{"sklearn\/gaussian_process\/kernels.py":[{"add":["1206","                self.__class__.__name__, np.ravel(self.length_scale)[0])","1350","        else:","1352","                self.__class__.__name__, np.ravel(self.length_scale)[0],","1353","                self.nu)"],"delete":["1206","                self.__class__.__name__, self.length_scale)","1350","        else:  # isotropic","1352","                self.__class__.__name__, self.length_scale, self.nu)"]}],"sklearn\/gaussian_process\/tests\/test_kernels.py":[{"add":["43","           DotProduct(sigma_0=2.0), DotProduct(sigma_0=2.0) ** 2,","44","           RBF(length_scale=[2.0]), Matern(length_scale=[2.0])]","307","","308","","309","def test_repr_kernels():","310","    \"\"\"Smoke-test for repr in kernels.\"\"\"","311","","312","    for kernel in kernels:","313","        repr(kernel)"],"delete":["43","           DotProduct(sigma_0=2.0), DotProduct(sigma_0=2.0) ** 2]"]}]}},"52bfd2585e5dec3b2ee2d1d9e2036ba637314cf4":{"changes":{"sklearn\/naive_bayes.py":"MODIFY"},"diff":{"sklearn\/naive_bayes.py":[{"add":["390","                             (unique_y[~unique_y_in_classes], classes))"],"delete":["390","                             (y[~unique_y_in_classes], classes))"]}]}},"9b25f5d37a78e9a28134b917c2768849f529cd6e":{"changes":{"doc\/developers\/advanced_installation.rst":"MODIFY","doc\/modules\/manifold.rst":"MODIFY","sklearn\/decomposition\/online_lda.py":"MODIFY","doc\/modules\/cross_validation.rst":"MODIFY","sklearn\/random_projection.py":"MODIFY","doc\/developers\/contributing.rst":"MODIFY","doc\/datasets\/olivetti_faces.rst":"MODIFY","AUTHORS.rst":"MODIFY","doc\/modules\/svm.rst":"MODIFY","sklearn\/kernel_approximation.py":"MODIFY","examples\/neighbors\/plot_species_kde.py":"MODIFY","sklearn\/datasets\/species_distributions.py":"MODIFY","doc\/modules\/clustering.rst":"MODIFY","doc\/support.rst":"MODIFY","CONTRIBUTING.md":"MODIFY","sklearn\/datasets\/olivetti_faces.py":"MODIFY","doc\/modules\/feature_selection.rst":"MODIFY","doc\/faq.rst":"MODIFY","doc\/related_projects.rst":"MODIFY","sklearn\/gaussian_process\/gaussian_process.py":"MODIFY","doc\/about.rst":"MODIFY","doc\/modules\/model_persistence.rst":"MODIFY","doc\/modules\/naive_bayes.rst":"MODIFY","doc\/modules\/feature_extraction.rst":"MODIFY","doc\/testimonials\/testimonials.rst":"MODIFY","doc\/tutorial\/statistical_inference\/finding_help.rst":"MODIFY","sklearn\/feature_selection\/mutual_info_.py":"MODIFY","doc\/presentations.rst":"MODIFY","doc\/modules\/tree.rst":"MODIFY","sklearn\/cluster\/birch.py":"MODIFY","sklearn\/linear_model\/least_angle.py":"MODIFY"},"diff":{"doc\/developers\/advanced_installation.rst":[{"add":["170","<https:\/\/pip.readthedocs.io\/en\/stable\/installing\/>`_ utility.","281","`microsoft visual c++ express 2008 <http:\/\/download.microsoft.com\/download\/A\/5\/4\/A54BADB6-9C3F-478D-8657-93B3FC9FE62D\/vcsetup.exe>`_","303","  <https:\/\/www.microsoft.com\/en-us\/download\/details.aspx?id=18950>`_","379","<https:\/\/nose.readthedocs.io\/en\/latest\/>`_ library. after"],"delete":["170","<https:\/\/pip.readthedocs.org\/en\/stable\/installing\/>`_ utility.","281","`microsoft visual c++ express 2008 <http:\/\/go.microsoft.com\/?linkid=7729279>`_","303","  <http:\/\/www.microsoft.com\/en-us\/download\/details.aspx?id=18950>`_","379","<https:\/\/nose.readthedocs.org\/en\/latest\/>`_ library. after"]}],"doc\/modules\/manifold.rst":[{"add":["148","     <http:\/\/science.sciencemag.org\/content\/290\/5500\/2319.full>`_"],"delete":["148","     <http:\/\/www.sciencemag.org\/content\/290\/5500\/2319.full>`_"]}],"sklearn\/decomposition\/online_lda.py":[{"add":["7","Link: http:\/\/matthewdhoffman.com\/code\/onlineldavb.tar","243","        http:\/\/matthewdhoffman.com\/\/code\/onlineldavb.tar"],"delete":["7","Link: http:\/\/www.cs.princeton.edu\/~mdhoffma\/code\/onlineldavb.tar","243","        http:\/\/www.cs.princeton.edu\/~mdhoffma\/code\/onlineldavb.tar"]}],"doc\/modules\/cross_validation.rst":[{"add":["345","   <http:\/\/people.csail.mit.edu\/romer\/papers\/CrossVal_SDM08.pdf>`_, SIAM 2008;"],"delete":["345","   <http:\/\/www.siam.org\/proceedings\/datamining\/2008\/dm08_54_Rao.pdf>`_, SIAM 2008;"]}],"sklearn\/random_projection.py":[{"add":["248","           http:\/\/web.stanford.edu\/~hastie\/Papers\/Ping\/KDD06_rp.pdf","583","           http:\/\/web.stanford.edu\/~hastie\/Papers\/Ping\/KDD06_rp.pdf"],"delete":["248","           http:\/\/www.stanford.edu\/~hastie\/Papers\/Ping\/KDD06_rp.pdf","583","           http:\/\/www.stanford.edu\/~hastie\/Papers\/Ping\/KDD06_rp.pdf"]}],"doc\/developers\/contributing.rst":[{"add":["30","We use `Git <https:\/\/git-scm.com\/>`_ for version control and","52","`the setuptool docs <http:\/\/setuptools.readthedocs.io\/en\/latest\/setuptools.html#development-mode>`_)::","131","`Git documentation <https:\/\/git-scm.com\/documentation>`_ on the web.)","227","   <http:\/\/astropy.readthedocs.io\/en\/latest\/development\/workflow\/development_workflow.html>`_","301","<http:\/\/www.sphinx-doc.org\/en\/stable\/>`_,","303","`pillow <http:\/\/pillow.readthedocs.io\/en\/latest\/>`_.","363","purpose, we use the `nose <http:\/\/nose.readthedocs.io\/en\/latest\/>`_","482","      <https:\/\/divmod.readthedocs.io\/en\/latest\/products\/pyflakes.html>`_ to automatically","491","<https:\/\/gist.github.com\/nateGeorge\/5455d2c57fb33c1ae04706f2dc4fee01>`_."],"delete":["30","We use `Git <http:\/\/git-scm.com\/>`_ for version control and","52","`the setuptool docs <https:\/\/pythonhosted.org\/setuptools\/setuptools.html#development-mode>`_)::","131","`Git documentation <http:\/\/git-scm.com\/documentation>`_ on the web.)","227","   <http:\/\/astropy.readthedocs.org\/en\/latest\/development\/workflow\/development_workflow.html>`_","301","<http:\/\/sphinx-doc.org\/>`_,","303","`pillow <http:\/\/pillow.readthedocs.org\/en\/latest\/>`_.","363","purpose, we use the `nose <http:\/\/nose.readthedocs.org\/en\/latest\/>`_","482","      <http:\/\/www.divmod.org\/trac\/wiki\/DivmodPyflakes>`_ to automatically","491","<https:\/\/svn.enthought.com\/enthought\/browser\/sandbox\/docs\/coding_standard.py>`_."]}],"doc\/datasets\/olivetti_faces.rst":[{"add":["7","`This dataset contains a set of face images`_ taken between April 1992 and April","8","1994 at AT&T Laboratories Cambridge. The","13",".. _This dataset contains a set of face images: http:\/\/www.cl.cam.ac.uk\/research\/dtg\/attarchive\/facedatabase.html"],"delete":["7","This dataset contains a set of face images taken between April 1992 and April","8","1994 at AT&T Laboratories Cambridge. The website describing the original","9","dataset is now defunct, but archived copies can be accessed through","10","`the Internet Archive's Wayback Machine`_. The","15",".. _the Internet Archive's Wayback Machine: http:\/\/wayback.archive.org\/web\/*\/http:\/\/www.uk.research.att.com\/facedatabase.html"]}],"AUTHORS.rst":[{"add":["35","  * `Virgile Fritsch <https:\/\/team.inria.fr\/parietal\/vfritsch\/>`_","40","  * `Olivier Grisel <https:\/\/twitter.com\/ogrisel>`_","43","  * `Brian Holt <http:\/\/personal.ee.surrey.ac.uk\/Personal\/B.Holt\/>`_","49","  * `Wei Li <http:\/\/kuantkid.github.io\/>`_","51","  * `Gilles Louppe <http:\/\/glouppe.github.io\/>`_","59","  * `Fabian Pedregosa <http:\/\/fa.bianp.net\/blog\/>`_","60","  * `Peter Prettenhofer <https:\/\/sites.google.com\/site\/peterprettenhofer\/>`_","62","  * `Jake VanderPlas <http:\/\/staff.washington.edu\/jakevdp\/>`_","64","  * `Gael Varoquaux <http:\/\/gael-varoquaux.info\/>`_"],"delete":["35","  * `Virgile Fritsch <http:\/\/parietal.saclay.inria.fr\/Members\/virgile-fritsch>`_","40","  * `Olivier Grisel <http:\/\/twitter.com\/ogrisel>`_","43","  * `Brian Holt <http:\/\/info.ee.surrey.ac.uk\/Personal\/B.Holt\/>`_","49","  * `Wei Li <http:\/\/kuantkid.github.com>`_","51","  * `Gilles Louppe <http:\/\/www.montefiore.ulg.ac.be\/~glouppe>`_","59","  * `Fabian Pedregosa <http:\/\/fseoane.net\/blog\/>`_","60","  * `Peter Prettenhofer <http:\/\/sites.google.com\/site\/peterprettenhofer\/>`_","62","  * `Jake VanderPlas <http:\/\/www.astro.washington.edu\/users\/vanderplas\/>`_","64","  * `Gael Varoquaux <http:\/\/gael-varoquaux.info\/blog\/>`_"]}],"doc\/modules\/svm.rst":[{"add":["621","   <http:\/\/link.springer.com\/article\/10.1007%2FBF00994018>`_,"],"delete":["621","   <http:\/\/www.springerlink.com\/content\/k238jx04hm87j80g\/>`_,"]}],"sklearn\/kernel_approximation.py":[{"add":["52","    (http:\/\/people.eecs.berkeley.edu\/~brecht\/papers\/08.rah.rec.nips.pdf)"],"delete":["52","    (http:\/\/www.eecs.berkeley.edu\/~brecht\/papers\/08.rah.rec.nips.pdf)"]}],"examples\/neighbors\/plot_species_kde.py":[{"add":["25","   <http:\/\/www.iucnredlist.org\/details\/13408\/0>`_ ,"],"delete":["25","   <http:\/\/www.iucnredlist.org\/apps\/redlist\/details\/13408\/0>`_ ,"]}],"sklearn\/datasets\/species_distributions.py":[{"add":["11","   <http:\/\/www.iucnredlist.org\/details\/3038\/0>`_ ,","15","   <http:\/\/www.iucnredlist.org\/details\/13408\/0>`_ ,","184","      <http:\/\/www.iucnredlist.org\/details\/3038\/0>`_ ,","188","      <http:\/\/www.iucnredlist.org\/details\/13408\/0>`_ ,"],"delete":["11","   <http:\/\/www.iucnredlist.org\/apps\/redlist\/details\/3038\/0>`_ ,","15","   <http:\/\/www.iucnredlist.org\/apps\/redlist\/details\/13408\/0>`_ ,","184","      <http:\/\/www.iucnredlist.org\/apps\/redlist\/details\/3038\/0>`_ ,","188","      <http:\/\/www.iucnredlist.org\/apps\/redlist\/details\/13408\/0>`_ ,"]}],"doc\/modules\/clustering.rst":[{"add":["426","cuts <http:\/\/people.eecs.berkeley.edu\/~malik\/papers\/SM-ncut.pdf>`_ problem on","878","   https:\/\/code.google.com\/archive\/p\/jbirch"],"delete":["426","cuts <http:\/\/www.cs.berkeley.edu\/~malik\/papers\/SM-ncut.pdf>`_ problem on","878","   https:\/\/code.google.com\/p\/jbirch\/"]}],"doc\/support.rst":[{"add":["30","  `stack exchange <http:\/\/stats.stackexchange.com\/>`_ is probably a more","100","<https:\/\/sourceforge.net\/projects\/scikit-learn\/files\/documentation\/>`_."],"delete":["30","  `metaoptimize.com\/qa <http:\/\/metaoptimize.com\/qa>`_ is probably a more","100","<http:\/\/sourceforge.net\/projects\/scikit-learn\/files\/documentation\/>`_."]}],"CONTRIBUTING.md":[{"add":["55","[Git documentation](https:\/\/git-scm.com\/documentation) on the web, or ask a friend or another contributor for help.)","217","[pillow](http:\/\/pillow.readthedocs.io\/en\/latest\/)."],"delete":["55","[Git documentation](http:\/\/git-scm.com\/documentation) on the web, or ask a friend or another contributor for help.)","217","[pillow](http:\/\/pillow.readthedocs.org\/en\/latest\/)."]}],"sklearn\/datasets\/olivetti_faces.py":[{"add":["2","The original database was available from","4","    http:\/\/www.cl.cam.ac.uk\/research\/dtg\/attarchive\/facedatabase.html","100","        http:\/\/www.cl.cam.ac.uk\/research\/dtg\/attarchive\/facedatabase.html"],"delete":["2","The original database was available from (now defunct)","4","    http:\/\/www.uk.research.att.com\/facedatabase.html","100","        http:\/\/www.uk.research.att.com\/facedatabase.html"]}],"doc\/modules\/feature_selection.rst":[{"add":["285","        Bootstrap.\" https:\/\/hal.inria.fr\/hal-00354771\/","289","       http:\/\/arxiv.org\/pdf\/0809.2932.pdf"],"delete":["285","        Bootstrap.\" http:\/\/hal.inria.fr\/hal-00354771\/","289","       http:\/\/arxiv.org\/pdf\/0809.2932"]}],"doc\/faq.rst":[{"add":["104","<https:\/\/sourceforge.net\/p\/scikit-learn\/mailman\/scikit-learn-general\/thread\/CAAkaFLWcBG+gtsFQzpTLfZoCsHMDv9UG5WaqT0LwUApte0TVzg@mail.gmail.com\/#msg33104380>`_."],"delete":["104","<http:\/\/sourceforge.net\/p\/scikit-learn\/mailman\/scikit-learn-general\/thread\/CAAkaFLWcBG%2BgtsFQzpTLfZoCsHMDv9UG5WaqT0LwUApte0TVzg%40mail.gmail.com\/#msg33104380>`_."]}],"doc\/related_projects.rst":[{"add":["24","  <https:\/\/skll.readthedocs.io\/en\/latest\/index.html>`_  A command-line","28","- `auto-sklearn <https:\/\/github.com\/automl\/auto-sklearn\/>`_","60","- `lightning <https:\/\/github.com\/scikit-learn-contrib\/lightning>`_ Fast state-of-the-art","75","- `py-earth <https:\/\/github.com\/scikit-learn-contrib\/py-earth>`_ Multivariate adaptive"],"delete":["24","  <https:\/\/skll.readthedocs.org\/en\/latest\/index.html>`_  A command-line","28","- `auto-sklearn <https:\/\/github.com\/automl\/auto-sklearn\/blob\/master\/source\/index.rst>`_","60","- `lightning <http:\/\/www.mblondel.org\/lightning\/>`_ Fast state-of-the-art","75","- `py-earth <https:\/\/github.com\/jcrudy\/py-earth>`_ Multivariate adaptive"]}],"sklearn\/gaussian_process\/gaussian_process.py":[{"add":["210","        http:\/\/www.jstor.org\/stable\/1269548"],"delete":["210","        http:\/\/www.jstor.org\/pss\/1269548"]}],"doc\/about.rst":[{"add":["115","`tinyclues <https:\/\/www.tinyclues.com\/>`_ funded the 2011 international Granada","128","<http:\/\/www.numfocus.org\/board.html>`_. NumFOCUS's mission is to foster","178",".. |tinyclues| image:: https:\/\/www.tinyclues.com\/web\/wp-content\/uploads\/2016\/06\/Tinyclues-PNG-logo.png","180","   :target: https:\/\/www.tinyclues.com\/","183",".. |afpy| image:: https:\/\/www.afpy.org\/logo.png","185","   :target: https:\/\/www.afpy.org","192",".. |FNRS| image:: http:\/\/www.fnrs.be\/en\/images\/FRS-FNRS_rose_transp.png","213","- We would like to thank `Rackspace <https:\/\/www.rackspace.com>`_ for providing","214","  us with a free `Rackspace Cloud <https:\/\/www.rackspace.com\/cloud\/>`_ account to","220","  <http:\/\/shiningpanda.com\/>`_ for free CPU time on their Continuous"],"delete":["115","`tinyclues <http:\/\/www.tinyclues.com\/>`_ funded the 2011 international Granada","128","<http:\/\/www.numfocus.org\/board>`_. NumFOCUS's mission is to foster","178",".. |tinyclues| image:: http:\/\/www.tinyclues.com\/static\/img\/logo.png","180","   :target: http:\/\/www.tinyclues.com\/","183",".. |afpy| image:: http:\/\/www.afpy.org\/logo.png","185","   :target: http:\/\/www.afpy.org","192",".. |FNRS| image:: http:\/\/www.fnrs.be\/uploaddocs\/images\/COMMUNIQUER\/FRS-FNRS_rose_transp.png","213","- We would like to thank `Rackspace <http:\/\/www.rackspace.com>`_ for providing","214","  us with a free `Rackspace Cloud <http:\/\/www.rackspace.com\/cloud\/>`_ account to","220","  <https:\/\/www.shiningpanda-ci.com\/>`_ for free CPU time on their Continuous"]}],"doc\/modules\/model_persistence.rst":[{"add":["16","persistence model, namely `pickle <https:\/\/docs.python.org\/2\/library\/pickle.html>`_::"],"delete":["16","persistence model, namely `pickle <http:\/\/docs.python.org\/2\/library\/pickle.html>`_::"]}],"doc\/modules\/naive_bayes.rst":[{"add":["73","   <http:\/\/www.cs.unb.ca\/~hzhang\/publications\/FLAIRS04ZhangH.pdf>`_"],"delete":["73","   <http:\/\/www.cs.unb.ca\/profs\/hzhang\/publications\/FLAIRS04ZhangH.pdf>`_"]}],"doc\/modules\/feature_extraction.rst":[{"add":["212"," * `MurmurHash3 <https:\/\/github.com\/aappleby\/smhasher>`_."],"delete":["212"," * `MurmurHash3 <http:\/\/code.google.com\/p\/smhasher\/wiki\/MurmurHash3>`_."]}],"doc\/testimonials\/testimonials.rst":[{"add":["66","<https:\/\/team.inria.fr\/visages\/>`_ for medical image analysis, `Privatics","504","    :target: https:\/\/www.brandwatch.com\/peerindex-and-brandwatch","531","`DataRobot <https:\/\/www.datarobot.com>`_","540","    :target: https:\/\/www.datarobot.com"],"delete":["66","<https:\/\/www.irisa.fr\/visages\/index>`_ for medical image analysis, `Privatics","294","","295","`Rangespan <https:\/\/www.rangespan.com>`_","506","    :target: http:\/\/www.peerindex.com\/","533","`DataRobot <http:\/\/www.datarobot.com>`_","542","    :target: http:\/\/www.datarobot.com"]}],"doc\/tutorial\/statistical_inference\/finding_help.rst":[{"add":["23","\t","24","  :Stack Exchange:","26","    The Stack Exchange family of sites hosts `multiple subdomains for Machine Learning questions`_.","30",".. _`multiple subdomains for Machine Learning questions`: http:\/\/meta.stackexchange.com\/questions\/130524\/which-stack-exchange-website-for-machine-learning-and-computational-algorithms","31",""],"delete":["15","  :Metaoptimize\/QA:","16","","17","    A forum for Machine Learning, Natural Language Processing and","18","    other Data Analytics discussions (similar to what Stackoverflow","19","    is for developers): http:\/\/metaoptimize.com\/qa","20","","21","    A good starting point is the discussion on `good freely available","22","    textbooks on machine learning`_","23","","33","","34","","35",".. _`good freely available textbooks on machine learning`: http:\/\/metaoptimize.com\/qa\/questions\/186\/good-freely-available-textbooks-on-machine-learning"]}],"sklearn\/feature_selection\/mutual_info_.py":[{"add":["351","    .. [1] `Mutual Information <https:\/\/en.wikipedia.org\/wiki\/Mutual_information>`_","426","    .. [1] `Mutual Information <https:\/\/en.wikipedia.org\/wiki\/Mutual_information>`_"],"delete":["351","    .. [1] `Mutual Information <http:\/\/en.wikipedia.org\/wiki\/Mutual_information>`_","426","    .. [1] `Mutual Information <http:\/\/en.wikipedia.org\/wiki\/Mutual_information>`_"]}],"doc\/presentations.rst":[{"add":["11","<http:\/\/www.scipy-lectures.org\/>`_. This will help you find your footing a"],"delete":["11","<http:\/\/scipy-lectures.org>`_. This will help you find your footing a"]}],"doc\/modules\/tree.rst":[{"add":["412",".. _CART: https:\/\/en.wikipedia.org\/wiki\/Predictive_analytics#Classification_and_regression_trees_.28CART.29"],"delete":["412",".. _CART: https:\/\/en.wikipedia.org\/wiki\/Predictive_analytics#Classification_and_regression_trees"]}],"sklearn\/cluster\/birch.py":[{"add":["403","      https:\/\/code.google.com\/archive\/p\/jbirch"],"delete":["403","      https:\/\/code.google.com\/p\/jbirch\/"]}],"sklearn\/linear_model\/least_angle.py":[{"add":["143","           <https:\/\/en.wikipedia.org\/wiki\/Lasso_(statistics)>`_"],"delete":["143","           <https:\/\/en.wikipedia.org\/wiki\/Lasso_(statistics)#Lasso_method>`_"]}]}},"814223cbfd9552f1ca5925b291616006f1d269a4":{"changes":{"sklearn\/model_selection\/_validation.py":"MODIFY","sklearn\/cross_validation.py":"MODIFY","sklearn\/model_selection\/tests\/test_validation.py":"MODIFY"},"diff":{"sklearn\/model_selection\/_validation.py":[{"add":["303","    if hasattr(score, 'item'):","304","        try:","305","            # e.g. unwrap memmapped scalars","306","            score = score.item()","307","        except ValueError:","308","            # non-scalar?","309","            pass"],"delete":[]}],"sklearn\/cross_validation.py":[{"add":["1650","    if hasattr(score, 'item'):","1651","        try:","1652","            # e.g. unwrap memmapped scalars","1653","            score = score.item()","1654","        except ValueError:","1655","            # non-scalar?","1656","            pass"],"delete":[]}],"sklearn\/model_selection\/tests\/test_validation.py":[{"add":["5","import tempfile","6","import os","773","","774","","775","def test_score_memmap():","776","    # Ensure a scalar score of memmap type is accepted","777","    iris = load_iris()","778","    X, y = iris.data, iris.target","779","    clf = MockClassifier()","780","    tf = tempfile.NamedTemporaryFile(mode='wb', delete=False)","781","    tf.write(b'Hello world!!!!!')","782","    tf.close()","783","    scores = np.memmap(tf.name, dtype=float)","784","    score = np.memmap(tf.name, shape=(), mode='w+', dtype=float)","785","    try:","786","        cross_val_score(clf, X, y, scoring=lambda est, X, y: score)","787","        # non-scalar should still fail","788","        assert_raises(ValueError, cross_val_score, clf, X, y,","789","                      scoring=lambda est, X, y: scores)","790","    finally:","791","        os.unlink(tf.name)"],"delete":[]}]}},"277b058713f64f55e787aac55fe0e1bbbd47576f":{"changes":{"sklearn\/ensemble\/bagging.py":"MODIFY","sklearn\/ensemble\/tests\/test_bagging.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/utils\/__init__.py":"MODIFY","sklearn\/ensemble\/tests\/test_iforest.py":"MODIFY"},"diff":{"sklearn\/ensemble\/bagging.py":[{"add":["22","from ..utils import indices_to_mask","36","def _generate_indices(random_state, bootstrap, n_population, n_samples):","37","    \"\"\"Draw randomly sampled indices.\"\"\"","38","    # Draw sample indices","39","    if bootstrap:","40","        indices = random_state.randint(0, n_population, n_samples)","41","    else:","42","        indices = sample_without_replacement(n_population, n_samples,","43","                                             random_state=random_state)","44","","45","    return indices","46","","47","","48","def _generate_bagging_indices(random_state, bootstrap_features,","49","                              bootstrap_samples, n_features, n_samples,","50","                              max_features, max_samples):","51","    \"\"\"Randomly draw feature and sample indices.\"\"\"","52","    # Get valid random state","53","    random_state = check_random_state(random_state)","54","","55","    # Draw indices","56","    feature_indices = _generate_indices(random_state, bootstrap_features,","57","                                        n_features, max_features)","58","    sample_indices = _generate_indices(random_state, bootstrap_samples,","59","                                       n_samples, max_samples)","60","","61","    return feature_indices, sample_indices","62","","63","","65","                               seeds, total_n_estimators, verbose):","69","    max_features = ensemble._max_features","70","    max_samples = ensemble._max_samples","87","        random_state = np.random.RandomState(seeds[i])","90","        try:  # Not all estimators accept a random_state","91","            estimator.set_params(random_state=seeds[i])","95","        # Draw random feature, sample indices","96","        features, indices = _generate_bagging_indices(random_state,","97","                                                      bootstrap_features,","98","                                                      bootstrap, n_features,","99","                                                      n_samples, max_features,","100","                                                      max_samples)","113","                not_indices_mask = ~indices_to_mask(indices, n_samples)","114","                curr_sample_weight[not_indices_mask] = 0","125","    return estimators, estimators_features","253","    def _fit(self, X, y, max_samples=None, max_depth=None, sample_weight=None):","291","        self._n_samples = n_samples","300","        # Validate max_samples","301","        if max_samples is None:","302","            max_samples = self.max_samples","303","        elif not isinstance(max_samples, (numbers.Integral, np.integer)):","309","        # Store validated integer row sampling value","310","        self._max_samples = max_samples","311","","312","        # Validate max_features","321","        # Store validated integer feature sampling value","322","        self._max_features = max_features","323","","324","        # Other checks","364","        self._seeds = seeds","382","            t[1] for t in all_results))","397","    def _get_estimators_indices(self):","398","        # Get drawn indices along both sample and feature axes","399","        for seed in self._seeds:","400","            # Operations accessing random_state must be performed identically","401","            # to those in `_parallel_build_estimators()`","402","            random_state = np.random.RandomState(seed)","403","            feature_indices, sample_indices = _generate_bagging_indices(","404","                random_state, self.bootstrap_features, self.bootstrap,","405","                self.n_features_, self._n_samples, self._max_features,","406","                self._max_samples)","407","","408","            yield feature_indices, sample_indices","409","","410","    @property","411","    def estimators_samples_(self):","412","        \"\"\"The subset of drawn samples for each base estimator.","413","","414","        Returns a dynamically generated list of boolean masks identifying","415","        the samples used for for fitting each member of the ensemble, i.e.,","416","        the in-bag samples.","417","","418","        Note: the list is re-created at each call to the property in order","419","        to reduce the object memory footprint by not storing the sampling","420","        data. Thus fetching the property may be slower than expected.","421","        \"\"\"","422","        sample_masks = []","423","        for _, sample_indices in self._get_estimators_indices():","424","            mask = indices_to_mask(sample_indices, self._n_samples)","425","            sample_masks.append(mask)","426","","427","        return sample_masks","428","","512","        estimator. Each subset is defined by a boolean mask.","580","        n_samples = y.shape[0]","589","            # Create mask for OOB samples","590","            mask = ~samples","875","        estimator. Each subset is defined by a boolean mask.","982","            # Create mask for OOB samples","983","            mask = ~samples"],"delete":["36","                               max_samples, seeds, total_n_estimators, verbose):","40","    max_features = ensemble.max_features","41","","42","    if (not isinstance(max_samples, (numbers.Integral, np.integer)) and","43","            (0.0 < max_samples <= 1.0)):","44","        max_samples = int(max_samples * n_samples)","45","","46","    if (not isinstance(max_features, (numbers.Integral, np.integer)) and","47","            (0.0 < max_features <= 1.0)):","48","        max_features = int(max_features * n_features)","49","","59","    estimators_samples = []","67","        random_state = check_random_state(seeds[i])","68","        seed = random_state.randint(MAX_INT)","71","        try:  # Not all estimator accept a random_state","72","            estimator.set_params(random_state=seed)","76","        # Draw features","77","        if bootstrap_features:","78","            features = random_state.randint(0, n_features, max_features)","79","        else:","80","            features = sample_without_replacement(n_features,","81","                                                  max_features,","82","                                                  random_state=random_state)","92","                indices = random_state.randint(0, n_samples, max_samples)","95","","97","                not_indices = sample_without_replacement(","98","                    n_samples,","99","                    n_samples - max_samples,","100","                    random_state=random_state)","101","","102","                curr_sample_weight[not_indices] = 0","105","            samples = curr_sample_weight > 0.","109","            if bootstrap:","110","                indices = random_state.randint(0, n_samples, max_samples)","111","            else:","112","                indices = sample_without_replacement(n_samples,","113","                                                     max_samples,","114","                                                     random_state=random_state)","115","","116","            sample_counts = bincount(indices, minlength=n_samples)","117","","119","            samples = sample_counts > 0.","122","        estimators_samples.append(samples)","125","    return estimators, estimators_samples, estimators_features","253","    def _fit(self, X, y, max_samples, max_depth=None, sample_weight=None):","299","        # if max_samples is float:","300","        if not isinstance(max_samples, (numbers.Integral, np.integer)):","328","            self.estimators_samples_ = []","362","                max_samples,","371","        self.estimators_samples_ += list(itertools.chain.from_iterable(","372","            t[1] for t in all_results))","374","            t[2] for t in all_results))","472","        estimator.","542","        n_samples = y.shape[0]","549","            mask = np.ones(n_samples, dtype=np.bool)","550","            mask[samples] = False","835","        estimator.","942","            mask = np.ones(n_samples, dtype=np.bool)","943","            mask[samples] = False"]}],"sklearn\/ensemble\/tests\/test_bagging.py":[{"add":["665","","666","","667","def test_oob_score_consistency():","668","    # Make sure OOB scores are identical when random_state, estimator, and ","669","    # training data are fixed and fitting is done twice","670","    X, y = make_hastie_10_2(n_samples=200, random_state=1)","671","    bagging = BaggingClassifier(KNeighborsClassifier(), max_samples=0.5,","672","                                max_features=0.5, oob_score=True,","673","                                random_state=1)","674","    assert_equal(bagging.fit(X, y).oob_score_, bagging.fit(X, y).oob_score_)","675","","676","","677","def test_estimators_samples():","678","    # Check that format of estimators_samples_ is correct and that results","679","    # generated at fit time can be identically reproduced at a later time","680","    # using data saved in object attributes.","681","    X, y = make_hastie_10_2(n_samples=200, random_state=1)","682","    bagging = BaggingClassifier(LogisticRegression(), max_samples=0.5,","683","                                max_features=0.5, random_state=1,","684","                                bootstrap=False)","685","    bagging.fit(X, y)","686","","687","    # Get relevant attributes","688","    estimators_samples = bagging.estimators_samples_","689","    estimators_features = bagging.estimators_features_","690","    estimators = bagging.estimators_","691","","692","    # Test for correct formatting","693","    assert_equal(len(estimators_samples), len(estimators))","694","    assert_equal(len(estimators_samples[0]), len(X))","695","    assert_equal(estimators_samples[0].dtype.kind, 'b')","696","","697","    # Re-fit single estimator to test for consistent sampling","698","    estimator_index = 0","699","    estimator_samples = estimators_samples[estimator_index]","700","    estimator_features = estimators_features[estimator_index]","701","    estimator = estimators[estimator_index]","702","","703","    X_train = (X[estimator_samples])[:, estimator_features]","704","    y_train = y[estimator_samples]","705","","706","    orig_coefs = estimator.coef_","707","    estimator.fit(X_train, y_train)","708","    new_coefs = estimator.coef_","709","","710","    assert_array_almost_equal(orig_coefs, new_coefs)","711","","712","","713","def test_max_samples_consistency():","714","    # Make sure validated max_samples and original max_samples are identical","715","    # when valid integer max_samples supplied by user","716","    max_samples = 100","717","    X, y = make_hastie_10_2(n_samples=2*max_samples, random_state=1)","718","    bagging = BaggingClassifier(KNeighborsClassifier(),","719","                                max_samples=max_samples,","720","                                max_features=0.5, random_state=1)","721","    bagging.fit(X, y)","722","    assert_equal(bagging._max_samples, max_samples)"],"delete":[]}],"doc\/whats_new.rst":[{"add":["259","   - The memory footprint is reduced (sometimes greatly) for :class:`BaseBagging`","260","     and classes that inherit from it, i.e, :class:`BaggingClassifier`,","261","     :class:`BaggingRegressor`, and :class:`IsolationForest`, by dynamically","262","     generating attribute ``estimators_samples_`` only when it is needed.","263","     By `David Staub`_.","264","","265","","4370","","4371",".. _David Staub: https:\/\/github.com\/staubda"],"delete":[]}],"sklearn\/utils\/__init__.py":[{"add":["34","           \"check_symmetric\", \"indices_to_mask\"]","421","","422","","423","def indices_to_mask(indices, mask_length):","424","    \"\"\"Convert list of indices to boolean mask.","425","","426","    Parameters","427","    ----------","428","    indices : list-like","429","        List of integers treated as indices.","430","    mask_length : int","431","        Length of boolean mask to be generated.","432","","433","    Returns","434","    -------","435","    mask : 1d boolean nd-array","436","        Boolean array that is True where indices are present, else False.","437","    \"\"\"","438","    if mask_length <= np.max(indices):","439","        raise ValueError(\"mask_length must be greater than max(indices)\")","440","","441","    mask = np.zeros(mask_length, dtype=np.bool)","442","    mask[indices] = True","443","","444","    return mask"],"delete":["34","           \"check_symmetric\"]"]}],"sklearn\/ensemble\/tests\/test_iforest.py":[{"add":["195","","196","","197","def test_max_samples_consistency():","198","    # Make sure validated max_samples in iforest and BaseBagging are identical","199","    X = iris.data","200","    clf = IsolationForest().fit(X)","201","    assert_equal(clf.max_samples_, clf._max_samples)"],"delete":[]}]}},"32d1236f4a4d01a2e1544fe9929e4e78630fa9eb":{"changes":{"sklearn\/ensemble\/bagging.py":"MODIFY","sklearn\/ensemble\/tests\/test_bagging.py":"MODIFY","sklearn\/ensemble\/tests\/test_base.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/ensemble\/base.py":"MODIFY","sklearn\/ensemble\/weight_boosting.py":"MODIFY","sklearn\/ensemble\/forest.py":"MODIFY","sklearn\/ensemble\/tests\/test_weight_boosting.py":"MODIFY"},"diff":{"sklearn\/ensemble\/bagging.py":[{"add":["88","        estimator = ensemble._make_estimator(append=False,","89","                                             random_state=random_state)"],"delete":["88","        estimator = ensemble._make_estimator(append=False)","89","","90","        try:  # Not all estimators accept a random_state","91","            estimator.set_params(random_state=seeds[i])","92","        except ValueError:","93","            pass"]}],"sklearn\/ensemble\/tests\/test_bagging.py":[{"add":["555","    assert_true(isinstance(estimator[0].steps[-1][1].random_state,","556","                           int))"],"delete":[]}],"sklearn\/ensemble\/tests\/test_base.py":[{"add":["7","import numpy as np","12","from sklearn.utils.testing import assert_not_equal","15","from sklearn.ensemble.base import _set_random_states","17","from sklearn.externals.odict import OrderedDict","18","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis","19","from sklearn.pipeline import Pipeline","20","from sklearn.feature_selection import SelectFromModel","25","    ensemble = BaggingClassifier(base_estimator=Perceptron(random_state=None),","26","                                 n_estimators=3)","33","    random_state = np.random.RandomState(3)","34","    ensemble._make_estimator(random_state=random_state)","35","    ensemble._make_estimator(random_state=random_state)","42","    assert_equal(ensemble[0].random_state, None)","43","    assert_true(isinstance(ensemble[1].random_state, int))","44","    assert_true(isinstance(ensemble[2].random_state, int))","45","    assert_not_equal(ensemble[1].random_state, ensemble[2].random_state)","56","","57","","58","def test_set_random_states():","59","    # Linear Discriminant Analysis doesn't have random state: smoke test","60","    _set_random_states(LinearDiscriminantAnalysis(), random_state=17)","61","","62","    clf1 = Perceptron(random_state=None)","63","    assert_equal(clf1.random_state, None)","64","    # check random_state is None still sets","65","    _set_random_states(clf1, None)","66","    assert_true(isinstance(clf1.random_state, int))","67","","68","    # check random_state fixes results in consistent initialisation","69","    _set_random_states(clf1, 3)","70","    assert_true(isinstance(clf1.random_state, int))","71","    clf2 = Perceptron(random_state=None)","72","    _set_random_states(clf2, 3)","73","    assert_equal(clf1.random_state, clf2.random_state)","74","","75","    # nested random_state","76","","77","    def make_steps():","78","        return [('sel', SelectFromModel(Perceptron(random_state=None))),","79","                ('clf', Perceptron(random_state=None))]","80","","81","    est1 = Pipeline(make_steps())","82","    _set_random_states(est1, 3)","83","    assert_true(isinstance(est1.steps[0][1].estimator.random_state, int))","84","    assert_true(isinstance(est1.steps[1][1].random_state, int))","85","    assert_not_equal(est1.get_params()['sel__estimator__random_state'],","86","                     est1.get_params()['clf__random_state'])","87","","88","    # ensure multiple random_state paramaters are invariant to get_params()","89","    # iteration order","90","","91","    class AlphaParamPipeline(Pipeline):","92","        def get_params(self, *args, **kwargs):","93","            params = Pipeline.get_params(self, *args, **kwargs).items()","94","            return OrderedDict(sorted(params))","95","","96","    class RevParamPipeline(Pipeline):","97","        def get_params(self, *args, **kwargs):","98","            params = Pipeline.get_params(self, *args, **kwargs).items()","99","            return OrderedDict(sorted(params, reverse=True))","100","","101","    for cls in [AlphaParamPipeline, RevParamPipeline]:","102","        est2 = cls(make_steps())","103","        _set_random_states(est2, 3)","104","        assert_equal(est1.get_params()['sel__estimator__random_state'],","105","                     est2.get_params()['sel__estimator__random_state'])","106","        assert_equal(est1.get_params()['clf__random_state'],","107","                     est2.get_params()['clf__random_state'])"],"delete":["18","    ensemble = BaggingClassifier(base_estimator=Perceptron(), n_estimators=3)","25","    ensemble._make_estimator()","26","    ensemble._make_estimator()"]}],"doc\/whats_new.rst":[{"add":["492","","493","    - Fix bug where :class:`ensemble.AdaBoostClassifier` and","494","      :class:`ensemble.AdaBoostRegressor` would perform poorly if the","495","      ``random_state`` was fixed","496","      (`#7411 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/7411>`_).","499","    - Fix bug in ensembles with randomization where the ensemble would not","500","      set ``random_state`` on base estimators in a pipeline or similar nesting.","501","      (`#7411 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/7411>`_).","502","      Note, results for :class:`ensemble.BaggingClassifier`","503","      :class:`ensemble.BaggingRegressor`, :class:`ensemble.AdaBoostClassifier`","504","      and :class:`ensemble.AdaBoostRegressor` will now differ from previous","505","      versions. By `Joel Nothman`_.","506",""],"delete":[]}],"sklearn\/ensemble\/base.py":[{"add":["12","from ..utils import _get_n_jobs, check_random_state","13","","14","MAX_RAND_SEED = np.iinfo(np.int32).max","15","","16","","17","def _set_random_states(estimator, random_state=None):","18","    \"\"\"Sets fixed random_state parameters for an estimator","19","","20","    Finds all parameters ending ``random_state`` and sets them to integers","21","    derived from ``random_state``.","22","","23","    Parameters","24","    ----------","25","","26","    estimator : estimator supporting get\/set_params","27","        Estimator with potential randomness managed by random_state","28","        parameters.","29","","30","    random_state : numpy.RandomState or int, optional","31","        Random state used to generate integer values.","32","","33","    Notes","34","    -----","35","    This does not necessarily set *all* ``random_state`` attributes that","36","    control an estimator's randomness, only those accessible through","37","    ``estimator.get_params()``.  ``random_state``s not controlled include","38","    those belonging to:","39","","40","        * cross-validation splitters","41","        * ``scipy.stats`` rvs","42","    \"\"\"","43","    random_state = check_random_state(random_state)","44","    to_set = {}","45","    for key in sorted(estimator.get_params(deep=True)):","46","        if key == 'random_state' or key.endswith('__random_state'):","47","            to_set[key] = random_state.randint(MAX_RAND_SEED)","48","","49","    if to_set:","50","        estimator.set_params(**to_set)","107","    def _make_estimator(self, append=True, random_state=None):","117","        if random_state is not None:","118","            _set_random_states(estimator, random_state)","119",""],"delete":["12","from ..utils import _get_n_jobs","69","    def _make_estimator(self, append=True):"]}],"sklearn\/ensemble\/weight_boosting.py":[{"add":["134","        random_state = check_random_state(self.random_state)","135","","141","                sample_weight,","142","                random_state)","168","    def _boost(self, iboost, X, y, sample_weight, random_state):","188","        random_state : numpy.RandomState","189","            The current random number generator","190","","430","    def _boost(self, iboost, X, y, sample_weight, random_state):","452","        random_state : numpy.RandomState","453","            The current random number generator","454","","470","            return self._boost_real(iboost, X, y, sample_weight, random_state)","473","            return self._boost_discrete(iboost, X, y, sample_weight,","474","                                        random_state)","476","    def _boost_real(self, iboost, X, y, sample_weight, random_state):","478","        estimator = self._make_estimator(random_state=random_state)","534","    def _boost_discrete(self, iboost, X, y, sample_weight, random_state):","536","        estimator = self._make_estimator(random_state=random_state)","961","    def _boost(self, iboost, X, y, sample_weight, random_state):","983","        random_state : numpy.RandomState","984","            The current random number generator","985","","1000","        estimator = self._make_estimator(random_state=random_state)","1006","        uniform_samples = random_state.random_sample(X.shape[0])"],"delete":["139","                sample_weight)","165","    def _boost(self, iboost, X, y, sample_weight):","424","    def _boost(self, iboost, X, y, sample_weight):","461","            return self._boost_real(iboost, X, y, sample_weight)","464","            return self._boost_discrete(iboost, X, y, sample_weight)","466","    def _boost_real(self, iboost, X, y, sample_weight):","468","        estimator = self._make_estimator()","469","","470","        try:","471","            estimator.set_params(random_state=self.random_state)","472","        except ValueError:","473","            pass","529","    def _boost_discrete(self, iboost, X, y, sample_weight):","531","        estimator = self._make_estimator()","532","","533","        try:","534","            estimator.set_params(random_state=self.random_state)","535","        except ValueError:","536","            pass","961","    def _boost(self, iboost, X, y, sample_weight):","997","        estimator = self._make_estimator()","998","","999","        try:","1000","            estimator.set_params(random_state=self.random_state)","1001","        except ValueError:","1002","            pass","1003","","1004","        generator = check_random_state(self.random_state)","1010","        uniform_samples = generator.random_sample(X.shape[0])"]}],"sklearn\/ensemble\/forest.py":[{"add":["306","                tree = self._make_estimator(append=False,","307","                                            random_state=random_state)"],"delete":["306","                tree = self._make_estimator(append=False)","307","                tree.set_params(random_state=random_state.randint(MAX_INT))"]}],"sklearn\/ensemble\/tests\/test_weight_boosting.py":[{"add":["5","from sklearn.utils.testing import assert_equal, assert_true, assert_greater","115","        # Check we used multiple estimators","116","        assert_greater(len(clf.estimators_), 1)","117","        # Check for distinct random states (see issue #7408)","118","        assert_equal(len(set(est.random_state for est in clf.estimators_)),","119","                     len(clf.estimators_))","120","","131","    reg = AdaBoostRegressor(random_state=0)","132","    reg.fit(boston.data, boston.target)","133","    score = reg.score(boston.data, boston.target)","136","    # Check we used multiple estimators","137","    assert_true(len(reg.estimators_) > 1)","138","    # Check for distinct random states (see issue #7408)","139","    assert_equal(len(set(est.random_state for est in reg.estimators_)),","140","                 len(reg.estimators_))","141",""],"delete":["5","from sklearn.utils.testing import assert_equal, assert_true","125","    clf = AdaBoostRegressor(random_state=0)","126","    clf.fit(boston.data, boston.target)","127","    score = clf.score(boston.data, boston.target)"]}]}},"0493d714a0a7fde4c26e92f4f5b7d2ea968e7ea1":{"changes":{"doc\/modules\/preprocessing.rst":"MODIFY"},"diff":{"doc\/modules\/preprocessing.rst":[{"add":["32","than others, it might dominate the objective function and make the"],"delete":["32","that others, it might dominate the objective function and make the"]}]}},"c84ff5e3514d0e48daa9dcaeafb80629b8db3ced":{"changes":{"sklearn\/tree\/_utils.pyx":"MODIFY","sklearn\/tree\/tree.py":"MODIFY","sklearn\/ensemble\/gradient_boosting.py":"MODIFY","sklearn\/tree\/_criterion.pyx":"MODIFY","sklearn\/ensemble\/tests\/test_forest.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/tree\/_utils.pxd":"MODIFY","sklearn\/ensemble\/forest.py":"MODIFY","sklearn\/tree\/tests\/test_tree.py":"MODIFY"},"diff":{"sklearn\/tree\/_utils.pyx":[{"add":["8","#          Nelson Liu <nelson@nelsonliu.me>","15","from libc.stdlib cimport calloc","305","","306","# =============================================================================","307","# WeightedPQueue data structure","308","# =============================================================================","309","","310","cdef class WeightedPQueue:","311","    \"\"\"A priority queue class, always sorted in increasing order.","312","","313","    Attributes","314","    ----------","315","    capacity : SIZE_t","316","        The capacity of the priority queue.","317","","318","    array_ptr : SIZE_t","319","        The water mark of the priority queue; the priority queue grows from","320","        left to right in the array ``array_``. ``array_ptr`` is always","321","        less than ``capacity``.","322","","323","    array_ : WeightedPQueueRecord*","324","        The array of priority queue records. The minimum element is on the","325","        left at index 0, and the maximum element is on the right at index","326","        ``array_ptr-1``.","327","    \"\"\"","328","","329","    def __cinit__(self, SIZE_t capacity):","330","        self.capacity = capacity","331","        self.array_ptr = 0","332","        safe_realloc(&self.array_, capacity)","333","","334","        if self.array_ == NULL:","335","            raise MemoryError()","336","","337","    def __dealloc__(self):","338","        free(self.array_)","339","","340","    cdef void reset(self) nogil:","341","        \"\"\"Reset the WeightedPQueue to its state at construction\"\"\"","342","        self.array_ptr = 0","343","        self.array_ = <WeightedPQueueRecord*> calloc(self.capacity,","344","                                                     sizeof(WeightedPQueueRecord))","345","","346","    cdef bint is_empty(self) nogil:","347","        return self.array_ptr <= 0","348","","349","    cdef SIZE_t size(self) nogil:","350","        return self.array_ptr","351","","352","    cdef int push(self, DOUBLE_t data, DOUBLE_t weight) nogil:","353","        \"\"\"Push record on the array.","354","        Returns 0 if successful; -1 on out of memory error.","355","        \"\"\"","356","        cdef SIZE_t array_ptr = self.array_ptr","357","        cdef WeightedPQueueRecord* array = NULL","358","        cdef SIZE_t i","359","","360","        # Resize if capacity not sufficient","361","        if array_ptr >= self.capacity:","362","            self.capacity *= 2","363","            array = <WeightedPQueueRecord*> realloc(self.array_,","364","                                                    self.capacity *","365","                                                    sizeof(WeightedPQueueRecord))","366","","367","            if array == NULL:","368","                # no free; __dealloc__ handles that","369","                return -1","370","            self.array_ = array","371","","372","        # Put element as last element of array","373","        array = self.array_","374","        array[array_ptr].data = data","375","        array[array_ptr].weight = weight","376","","377","        # bubble last element up according until it is sorted","378","        # in ascending order","379","        i = array_ptr","380","        while(i != 0 and array[i].data < array[i-1].data):","381","            array[i], array[i-1] = array[i-1], array[i]","382","            i -= 1","383","","384","        # Increase element count","385","        self.array_ptr = array_ptr + 1","386","        return 0","387","","388","    cdef int remove(self, DOUBLE_t data, DOUBLE_t weight) nogil:","389","        \"\"\"Remove a specific value\/weight record from the array.","390","        Returns 0 if successful, -1 if record not found.\"\"\"","391","        cdef SIZE_t array_ptr = self.array_ptr","392","        cdef WeightedPQueueRecord* array = self.array_","393","        cdef SIZE_t idx_to_remove = -1","394","        cdef SIZE_t i","395","","396","        if array_ptr <= 0:","397","            return -1","398","","399","        # find element to remove","400","        for i in range(array_ptr):","401","            if array[i].data == data and array[i].weight == weight:","402","                idx_to_remove = i","403","                break","404","","405","        if idx_to_remove == -1:","406","            return -1","407","","408","        # shift the elements after the removed element","409","        # to the left.","410","        for i in range(idx_to_remove, array_ptr-1):","411","            array[i] = array[i+1]","412","","413","        self.array_ptr = array_ptr - 1","414","        return 0","415","","416","    cdef int pop(self, DOUBLE_t* data, DOUBLE_t* weight) nogil:","417","        \"\"\"Remove the top (minimum) element from array.","418","        Returns 0 if successful, -1 if nothing to remove.\"\"\"","419","        cdef SIZE_t array_ptr = self.array_ptr","420","        cdef WeightedPQueueRecord* array = self.array_","421","        cdef SIZE_t i","422","","423","        if array_ptr <= 0:","424","            return -1","425","","426","        data[0] = array[0].data","427","        weight[0] = array[0].weight","428","","429","        # shift the elements after the removed element","430","        # to the left.","431","        for i in range(0, array_ptr-1):","432","            array[i] = array[i+1]","433","","434","        self.array_ptr = array_ptr - 1","435","        return 0","436","","437","    cdef int peek(self, DOUBLE_t* data, DOUBLE_t* weight) nogil:","438","        \"\"\"Write the top element from array to a pointer.","439","        Returns 0 if successful, -1 if nothing to write.\"\"\"","440","        cdef WeightedPQueueRecord* array = self.array_","441","        if self.array_ptr <= 0:","442","            return -1","443","        # Take first value","444","        data[0] = array[0].data","445","        weight[0] = array[0].weight","446","        return 0","447","","448","    cdef DOUBLE_t get_weight_from_index(self, SIZE_t index) nogil:","449","        \"\"\"Given an index between [0,self.current_capacity], access","450","        the appropriate heap and return the requested weight\"\"\"","451","        cdef WeightedPQueueRecord* array = self.array_","452","","453","        # get weight at index","454","        return array[index].weight","455","","456","    cdef DOUBLE_t get_value_from_index(self, SIZE_t index) nogil:","457","        \"\"\"Given an index between [0,self.current_capacity], access","458","        the appropriate heap and return the requested value\"\"\"","459","        cdef WeightedPQueueRecord* array = self.array_","460","","461","        # get value at index","462","        return array[index].data","463","","464","# =============================================================================","465","# WeightedMedianCalculator data structure","466","# =============================================================================","467","","468","cdef class WeightedMedianCalculator:","469","    \"\"\"A class to handle calculation of the weighted median from streams of","470","    data. To do so, it maintains a parameter ``k`` such that the sum of the","471","    weights in the range [0,k) is greater than or equal to half of the total","472","    weight. By minimizing the value of ``k`` that fulfills this constraint,","473","    calculating the median is done by either taking the value of the sample","474","    at index ``k-1`` of ``samples`` (samples[k-1].data) or the average of","475","    the samples at index ``k-1`` and ``k`` of ``samples``","476","    ((samples[k-1] + samples[k]) \/ 2).","477","","478","    Attributes","479","    ----------","480","    initial_capacity : SIZE_t","481","        The initial capacity of the WeightedMedianCalculator.","482","","483","    samples : WeightedPQueue","484","        Holds the samples (consisting of values and their weights) used in the","485","        weighted median calculation.","486","","487","    total_weight : DOUBLE_t","488","        The sum of the weights of items in ``samples``. Represents the total","489","        weight of all samples used in the median calculation.","490","","491","    k : SIZE_t","492","        Index used to calculate the median.","493","","494","    sum_w_0_k : DOUBLE_t","495","        The sum of the weights from samples[0:k]. Used in the weighted","496","        median calculation; minimizing the value of ``k`` such that","497","        ``sum_w_0_k`` >= ``total_weight \/ 2`` provides a mechanism for","498","        calculating the median in constant time.","499","","500","    \"\"\"","501","","502","    def __cinit__(self, SIZE_t initial_capacity):","503","        self.initial_capacity = initial_capacity","504","        self.samples = WeightedPQueue(initial_capacity)","505","        self.total_weight = 0","506","        self.k = 0","507","        self.sum_w_0_k = 0","508","","509","    cdef SIZE_t size(self) nogil:","510","        \"\"\"Return the number of samples in the","511","        WeightedMedianCalculator\"\"\"","512","        return self.samples.size()","513","","514","    cdef void reset(self) nogil:","515","        \"\"\"Reset the WeightedMedianCalculator to its state at construction\"\"\"","516","        self.samples.reset()","517","        self.total_weight = 0","518","        self.k = 0","519","        self.sum_w_0_k = 0","520","","521","    cdef int push(self, DOUBLE_t data, DOUBLE_t weight) nogil:","522","        \"\"\"Push a value and its associated weight","523","        to the WeightedMedianCalculator to be considered","524","        in the median calculation.","525","        \"\"\"","526","        cdef int return_value","527","        cdef DOUBLE_t original_median","528","","529","        if self.size() != 0:","530","            original_median = self.get_median()","531","        return_value = self.samples.push(data, weight)","532","        self.update_median_parameters_post_push(data, weight,","533","                                                original_median)","534","        return return_value","535","","536","    cdef int update_median_parameters_post_push(self, DOUBLE_t data,","537","                                                DOUBLE_t weight,","538","                                                DOUBLE_t original_median) nogil:","539","        \"\"\"Update the parameters used in the median calculation,","540","        namely `k` and `sum_w_0_k` after an insertion\"\"\"","541","","542","        # trivial case of one element.","543","        if self.size() == 1:","544","            self.k = 1","545","            self.total_weight = weight","546","            self.sum_w_0_k = self.total_weight","547","            return 0","548","","549","        # get the original weighted median","550","        self.total_weight += weight","551","","552","        if data < original_median:","553","            # inserting below the median, so increment k and","554","            # then update self.sum_w_0_k accordingly by adding","555","            # the weight that was added.","556","            self.k += 1","557","            # update sum_w_0_k by adding the weight added","558","            self.sum_w_0_k += weight","559","","560","            # minimize k such that sum(W[0:k]) >= total_weight \/ 2","561","            # minimum value of k is 1","562","            while(self.k > 1 and ((self.sum_w_0_k -","563","                                   self.samples.get_weight_from_index(self.k-1))","564","                                  >= self.total_weight \/ 2.0)):","565","                self.k -= 1","566","                self.sum_w_0_k -= self.samples.get_weight_from_index(self.k)","567","            return 0","568","","569","        if data >= original_median:","570","            # inserting above or at the median","571","            # minimize k such that sum(W[0:k]) >= total_weight \/ 2","572","            while(self.k < self.samples.size() and","573","                  (self.sum_w_0_k < self.total_weight \/ 2.0)):","574","                self.k += 1","575","                self.sum_w_0_k += self.samples.get_weight_from_index(self.k-1)","576","            return 0","577","","578","    cdef int remove(self, DOUBLE_t data, DOUBLE_t weight) nogil:","579","        \"\"\"Remove a value from the MedianHeap, removing it","580","        from consideration in the median calculation","581","        \"\"\"","582","        cdef int return_value","583","        cdef DOUBLE_t original_median","584","","585","        if self.size() != 0:","586","            original_median = self.get_median()","587","","588","        return_value = self.samples.remove(data, weight)","589","        self.update_median_parameters_post_remove(data, weight,","590","                                                  original_median)","591","        return return_value","592","","593","    cdef int pop(self, DOUBLE_t* data, DOUBLE_t* weight) nogil:","594","        \"\"\"Pop a value from the MedianHeap, starting from the","595","        left and moving to the right.","596","        \"\"\"","597","        cdef int return_value","598","        cdef double original_median","599","","600","        if self.size() != 0:","601","            original_median = self.get_median()","602","","603","        # no elements to pop","604","        if self.samples.size() == 0:","605","            return -1","606","","607","        return_value = self.samples.pop(data, weight)","608","        self.update_median_parameters_post_remove(data[0],","609","                                                  weight[0],","610","                                                  original_median)","611","        return return_value","612","","613","    cdef int update_median_parameters_post_remove(self, DOUBLE_t data,","614","                                                  DOUBLE_t weight,","615","                                                  double original_median) nogil:","616","        \"\"\"Update the parameters used in the median calculation,","617","        namely `k` and `sum_w_0_k` after a removal\"\"\"","618","        # reset parameters because it there are no elements","619","        if self.samples.size() == 0:","620","            self.k = 0","621","            self.total_weight = 0","622","            self.sum_w_0_k = 0","623","            return 0","624","","625","        # trivial case of one element.","626","        if self.samples.size() == 1:","627","            self.k = 1","628","            self.total_weight -= weight","629","            self.sum_w_0_k = self.total_weight","630","            return 0","631","","632","        # get the current weighted median","633","        self.total_weight -= weight","634","","635","        if data < original_median:","636","            # removing below the median, so decrement k and","637","            # then update self.sum_w_0_k accordingly by subtracting","638","            # the removed weight","639","","640","            self.k -= 1","641","            # update sum_w_0_k by removing the weight at index k","642","            self.sum_w_0_k -= weight","643","","644","            # minimize k such that sum(W[0:k]) >= total_weight \/ 2","645","            # by incrementing k and updating sum_w_0_k accordingly","646","            # until the condition is met.","647","            while(self.k < self.samples.size() and","648","                  (self.sum_w_0_k < self.total_weight \/ 2.0)):","649","                self.k += 1","650","                self.sum_w_0_k += self.samples.get_weight_from_index(self.k-1)","651","            return 0","652","","653","        if data >= original_median:","654","            # removing above the median","655","            # minimize k such that sum(W[0:k]) >= total_weight \/ 2","656","            while(self.k > 1 and ((self.sum_w_0_k -","657","                                   self.samples.get_weight_from_index(self.k-1))","658","                                  >= self.total_weight \/ 2.0)):","659","                self.k -= 1","660","                self.sum_w_0_k -= self.samples.get_weight_from_index(self.k)","661","            return 0","662","","663","    cdef DOUBLE_t get_median(self) nogil:","664","        \"\"\"Write the median to a pointer, taking into account","665","        sample weights.\"\"\"","666","        if self.sum_w_0_k == (self.total_weight \/ 2.0):","667","            # split median","668","            return (self.samples.get_value_from_index(self.k) +","669","                    self.samples.get_value_from_index(self.k-1)) \/ 2.0","670","        if self.sum_w_0_k > (self.total_weight \/ 2.0):","671","            # whole median","672","            return self.samples.get_value_from_index(self.k-1)"],"delete":[]}],"sklearn\/tree\/tree.py":[{"add":["58","CRITERIA_REG = {\"mse\": _criterion.MSE, \"friedman_mse\": _criterion.FriedmanMSE,","59","                \"mae\": _criterion.MAE}","340","                criterion = CRITERIA_REG[self.criterion](self.n_outputs_,","341","                                                         n_samples)","786","        The function to measure the quality of a split. Supported criteria","787","        are \"mse\" for the mean squared error, which is equal to variance","788","        reduction as feature selection criterion, and \"mae\" for the mean","789","        absolute error."],"delete":["58","CRITERIA_REG = {\"mse\": _criterion.MSE, \"friedman_mse\": _criterion.FriedmanMSE}","339","                criterion = CRITERIA_REG[self.criterion](self.n_outputs_)","784","        The function to measure the quality of a split. The only supported","785","        criterion is \"mse\" for the mean squared error, which is equal to","786","        variance reduction as feature selection criterion."]}],"sklearn\/ensemble\/gradient_boosting.py":[{"add":["722","    def __init__(self, loss, learning_rate, n_estimators, criterion,","723","                 min_samples_split, min_samples_leaf, min_weight_fraction_leaf,","731","        self.criterion = criterion","765","                criterion=self.criterion,","1299","    criterion : string, optional (default=\"friedman_mse\")","1300","        The function to measure the quality of a split. Supported criteria","1301","        are \"friedman_mse\" for the mean squared error with improvement","1302","        score by Friedman, \"mse\" for mean squared error, and \"mae\" for","1303","        the mean absolute error. The default value of \"friedman_mse\" is","1304","        generally the best as it can provide a better approximation in","1305","        some cases.","1306","","1437","                 subsample=1.0, criterion='friedman_mse', min_samples_split=2,","1446","            criterion=criterion, min_samples_split=min_samples_split,","1654","    criterion : string, optional (default=\"friedman_mse\")","1655","        The function to measure the quality of a split. Supported criteria","1656","        are \"friedman_mse\" for the mean squared error with improvement","1657","        score by Friedman, \"mse\" for mean squared error, and \"mae\" for","1658","        the mean absolute error. The default value of \"friedman_mse\" is","1659","        generally the best as it can provide a better approximation in","1660","        some cases.","1661","","1791","                 subsample=1.0, criterion='friedman_mse', min_samples_split=2,","1799","            criterion=criterion, min_samples_split=min_samples_split,"],"delete":["722","    def __init__(self, loss, learning_rate, n_estimators, min_samples_split,","723","                 min_samples_leaf, min_weight_fraction_leaf,","764","                criterion='friedman_mse',","1428","                 subsample=1.0, min_samples_split=2,","1437","            min_samples_split=min_samples_split,","1774","                 subsample=1.0, min_samples_split=2,","1782","            min_samples_split=min_samples_split,"]}],"sklearn\/tree\/_criterion.pyx":[{"add":["14","#          Nelson Liu <nelson@nelsonliu.me>","22","from libc.math cimport fabs","31","from ._utils cimport WeightedMedianCalculator","689","    def __cinit__(self, SIZE_t n_outputs, SIZE_t n_samples):","696","","697","        n_samples: SIZE_t","698","            The total number of samples to fit on","970","cdef class MAE(RegressionCriterion):","971","    \"\"\"Mean absolute error impurity criterion","972","","973","       MAE = (1 \/ n)*(\\sum_i |y_i - f_i|), where y_i is the true","974","       value and f_i is the predicted value.\"\"\"","975","    def __dealloc__(self):","976","        \"\"\"Destructor.\"\"\"","977","        free(self.node_medians)","978","","979","    cdef np.ndarray left_child","980","    cdef np.ndarray right_child","981","    cdef DOUBLE_t* node_medians","982","","983","    def __cinit__(self, SIZE_t n_outputs, SIZE_t n_samples):","984","        \"\"\"Initialize parameters for this criterion.","985","","986","        Parameters","987","        ----------","988","        n_outputs: SIZE_t","989","            The number of targets to be predicted","990","","991","        n_samples: SIZE_t","992","            The total number of samples to fit on","993","        \"\"\"","994","","995","        # Default values","996","        self.y = NULL","997","        self.y_stride = 0","998","        self.sample_weight = NULL","999","","1000","        self.samples = NULL","1001","        self.start = 0","1002","        self.pos = 0","1003","        self.end = 0","1004","","1005","        self.n_outputs = n_outputs","1006","        self.n_node_samples = 0","1007","        self.weighted_n_node_samples = 0.0","1008","        self.weighted_n_left = 0.0","1009","        self.weighted_n_right = 0.0","1010","","1011","        # Allocate accumulators. Make sure they are NULL, not uninitialized,","1012","        # before an exception can be raised (which triggers __dealloc__).","1013","        self.node_medians = NULL","1014","","1015","        # Allocate memory for the accumulators","1016","        safe_realloc(&self.node_medians, n_outputs)","1017","","1018","        if (self.node_medians == NULL):","1019","            raise MemoryError()","1020","","1021","        self.left_child = np.empty(n_outputs, dtype='object')","1022","        self.right_child = np.empty(n_outputs, dtype='object')","1023","        # initialize WeightedMedianCalculators","1024","        for k in range(n_outputs):","1025","            self.left_child[k] = WeightedMedianCalculator(n_samples)","1026","            self.right_child[k] = WeightedMedianCalculator(n_samples)","1027","","1028","    cdef void init(self, DOUBLE_t* y, SIZE_t y_stride, DOUBLE_t* sample_weight,","1029","                   double weighted_n_samples, SIZE_t* samples, SIZE_t start,","1030","                   SIZE_t end) nogil:","1031","        \"\"\"Initialize the criterion at node samples[start:end] and","1032","           children samples[start:start] and samples[start:end].\"\"\"","1033","","1034","        cdef SIZE_t i, p, k","1035","        cdef DOUBLE_t y_ik","1036","        cdef DOUBLE_t w = 1.0","1037","","1038","        # Initialize fields","1039","        self.y = y","1040","        self.y_stride = y_stride","1041","        self.sample_weight = sample_weight","1042","        self.samples = samples","1043","        self.start = start","1044","        self.end = end","1045","        self.n_node_samples = end - start","1046","        self.weighted_n_samples = weighted_n_samples","1047","        self.weighted_n_node_samples = 0.","1048","","1049","        cdef void** left_child","1050","        cdef void** right_child","1051","","1052","        left_child = <void**> self.left_child.data","1053","        right_child = <void**> self.right_child.data","1054","","1055","        for k in range(self.n_outputs):","1056","            (<WeightedMedianCalculator> left_child[k]).reset()","1057","            (<WeightedMedianCalculator> right_child[k]).reset()","1058","","1059","        for p in range(start, end):","1060","            i = samples[p]","1061","","1062","            if sample_weight != NULL:","1063","                w = sample_weight[i]","1064","","1065","            for k in range(self.n_outputs):","1066","                y_ik = y[i * y_stride + k]","1067","","1068","                # push all values to the right side,","1069","                # since pos = start initially anyway","1070","                (<WeightedMedianCalculator> right_child[k]).push(y_ik, w)","1071","","1072","            self.weighted_n_node_samples += w","1073","        # calculate the node medians","1074","        for k in range(self.n_outputs):","1075","            self.node_medians[k] = (<WeightedMedianCalculator> right_child[k]).get_median()","1076","","1077","        # Reset to pos=start","1078","        self.reset()","1079","","1080","    cdef void reset(self) nogil:","1081","        \"\"\"Reset the criterion at pos=start.\"\"\"","1082","","1083","        cdef SIZE_t i, k","1084","        cdef DOUBLE_t value","1085","        cdef DOUBLE_t weight","1086","","1087","        cdef void** left_child = <void**> self.left_child.data","1088","        cdef void** right_child = <void**> self.right_child.data","1089","","1090","        self.weighted_n_left = 0.0","1091","        self.weighted_n_right = self.weighted_n_node_samples","1092","        self.pos = self.start","1093","","1094","        # reset the WeightedMedianCalculators, left should have no","1095","        # elements and right should have all elements.","1096","","1097","        for k in range(self.n_outputs):","1098","            # if left has no elements, it's already reset","1099","            for i in range((<WeightedMedianCalculator> left_child[k]).size()):","1100","                # remove everything from left and put it into right","1101","                (<WeightedMedianCalculator> left_child[k]).pop(&value,","1102","                                                               &weight)","1103","                (<WeightedMedianCalculator> right_child[k]).push(value,","1104","                                                                 weight)","1105","","1106","    cdef void reverse_reset(self) nogil:","1107","        \"\"\"Reset the criterion at pos=end.\"\"\"","1108","","1109","        self.weighted_n_right = 0.0","1110","        self.weighted_n_left = self.weighted_n_node_samples","1111","        self.pos = self.end","1112","","1113","        cdef DOUBLE_t value","1114","        cdef DOUBLE_t weight","1115","        cdef void** left_child = <void**> self.left_child.data","1116","        cdef void** right_child = <void**> self.right_child.data","1117","","1118","        # reverse reset the WeightedMedianCalculators, right should have no","1119","        # elements and left should have all elements.","1120","        for k in range(self.n_outputs):","1121","            # if right has no elements, it's already reset","1122","            for i in range((<WeightedMedianCalculator> right_child[k]).size()):","1123","                # remove everything from right and put it into left","1124","                (<WeightedMedianCalculator> right_child[k]).pop(&value,","1125","                                                                &weight)","1126","                (<WeightedMedianCalculator> left_child[k]).push(value,","1127","                                                                weight)","1128","","1129","    cdef void update(self, SIZE_t new_pos) nogil:","1130","        \"\"\"Updated statistics by moving samples[pos:new_pos] to the left.\"\"\"","1131","","1132","        cdef DOUBLE_t* sample_weight = self.sample_weight","1133","        cdef SIZE_t* samples = self.samples","1134","","1135","        cdef void** left_child = <void**> self.left_child.data","1136","        cdef void** right_child = <void**> self.right_child.data","1137","","1138","        cdef DOUBLE_t* y = self.y","1139","        cdef SIZE_t pos = self.pos","1140","        cdef SIZE_t end = self.end","1141","        cdef SIZE_t i, p, k","1142","        cdef DOUBLE_t w = 1.0","1143","        cdef DOUBLE_t y_ik","1144","","1145","        # Update statistics up to new_pos","1146","        #","1147","        # We are going to update right_child and left_child","1148","        # from the direction that require the least amount of","1149","        # computations, i.e. from pos to new_pos or from end to new_pos.","1150","","1151","        if (new_pos - pos) <= (end - new_pos):","1152","            for p in range(pos, new_pos):","1153","                i = samples[p]","1154","","1155","                if sample_weight != NULL:","1156","                    w = sample_weight[i]","1157","","1158","                for k in range(self.n_outputs):","1159","                    y_ik = y[i * self.y_stride + k]","1160","                    # remove y_ik and its weight w from right and add to left","1161","                    (<WeightedMedianCalculator> right_child[k]).remove(y_ik, w)","1162","                    (<WeightedMedianCalculator> left_child[k]).push(y_ik, w)","1163","","1164","                self.weighted_n_left += w","1165","        else:","1166","            self.reverse_reset()","1167","","1168","            for p in range(end - 1, new_pos - 1, -1):","1169","                i = samples[p]","1170","","1171","                if sample_weight != NULL:","1172","                    w = sample_weight[i]","1173","","1174","                for k in range(self.n_outputs):","1175","                    y_ik = y[i * self.y_stride + k]","1176","                    # remove y_ik and its weight w from left and add to right","1177","                    (<WeightedMedianCalculator> left_child[k]).remove(y_ik, w)","1178","                    (<WeightedMedianCalculator> right_child[k]).push(y_ik, w)","1179","","1180","                self.weighted_n_left -= w","1181","","1182","        self.weighted_n_right = (self.weighted_n_node_samples -","1183","                                 self.weighted_n_left)","1184","        self.pos = new_pos","1185","","1186","    cdef void node_value(self, double* dest) nogil:","1187","        \"\"\"Computes the node value of samples[start:end] into dest.\"\"\"","1188","","1189","        cdef SIZE_t k","1190","        for k in range(self.n_outputs):","1191","            dest[k] = <double> self.node_medians[k]","1192","","1193","    cdef double node_impurity(self) nogil:","1194","        \"\"\"Evaluate the impurity of the current node, i.e. the impurity of","1195","           samples[start:end]\"\"\"","1196","","1197","        cdef DOUBLE_t* y = self.y","1198","        cdef DOUBLE_t* sample_weight = self.sample_weight","1199","        cdef SIZE_t* samples = self.samples","1200","        cdef SIZE_t i, p, k","1201","        cdef DOUBLE_t y_ik","1202","        cdef DOUBLE_t w_y_ik","1203","","1204","        cdef double impurity = 0.0","1205","","1206","        for k in range(self.n_outputs):","1207","            for p in range(self.start, self.end):","1208","                i = samples[p]","1209","","1210","                y_ik = y[i * self.y_stride + k]","1211","","1212","                impurity += <double> fabs((<double> y_ik) - <double> self.node_medians[k])","1213","        return impurity \/ (self.weighted_n_node_samples * self.n_outputs)","1214","","1215","    cdef void children_impurity(self, double* impurity_left,","1216","                                double* impurity_right) nogil:","1217","        \"\"\"Evaluate the impurity in children nodes, i.e. the impurity of the","1218","           left child (samples[start:pos]) and the impurity the right child","1219","           (samples[pos:end]).","1220","        \"\"\"","1221","","1222","        cdef DOUBLE_t* y = self.y","1223","        cdef DOUBLE_t* sample_weight = self.sample_weight","1224","        cdef SIZE_t* samples = self.samples","1225","","1226","        cdef SIZE_t start = self.start","1227","        cdef SIZE_t pos = self.pos","1228","        cdef SIZE_t end = self.end","1229","","1230","        cdef SIZE_t i, p, k","1231","        cdef DOUBLE_t y_ik","1232","        cdef DOUBLE_t median","1233","","1234","        cdef void** left_child = <void**> self.left_child.data","1235","        cdef void** right_child = <void**> self.right_child.data","1236","","1237","        impurity_left[0] = 0.0","1238","        impurity_right[0] = 0.0","1239","","1240","        for k in range(self.n_outputs):","1241","            median = (<WeightedMedianCalculator> left_child[k]).get_median()","1242","            for p in range(start, pos):","1243","                i = samples[p]","1244","","1245","                y_ik = y[i * self.y_stride + k]","1246","","1247","                impurity_left[0] += <double>fabs((<double> y_ik) -","1248","                                                 <double> median)","1249","        impurity_left[0] \/= <double>((self.weighted_n_left) * self.n_outputs)","1250","","1251","        for k in range(self.n_outputs):","1252","            median = (<WeightedMedianCalculator> right_child[k]).get_median()","1253","            for p in range(pos, end):","1254","                i = samples[p]","1255","","1256","                y_ik = y[i * self.y_stride + k]","1257","","1258","                impurity_right[0] += <double>fabs((<double> y_ik) -","1259","                                                  <double> median)","1260","        impurity_right[0] \/= <double>((self.weighted_n_right) *","1261","                                      self.n_outputs)","1262",""],"delete":["686","    def __cinit__(self, SIZE_t n_outputs):"]}],"sklearn\/ensemble\/tests\/test_forest.py":[{"add":["161","    for name, criterion in product(FOREST_REGRESSORS, (\"mse\", \"mae\", \"friedman_mse\")):","246","    for name, criterion in product(FOREST_REGRESSORS, [\"mse\", \"friedman_mse\", \"mae\"]):"],"delete":["161","    for name, criterion in product(FOREST_REGRESSORS, (\"mse\", )):","246","    for name, criterion in product(FOREST_REGRESSORS, [\"mse\", \"friedman_mse\"]):"]}],"doc\/whats_new.rst":[{"add":["119","   - Added a new splitting criterion for :class:`tree.DecisionTreeRegressor`,","120","     the mean absolute error. This criterion can also be used in","121","     :class:`ensemble.ExtraTreesRegressor`,","122","     :class:`ensemble.RandomForestRegressor`, and the gradient boosting","123","     estimators. (`#6667","124","     <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/6667>`_) by `Nelson","125","     Liu`_.","126","","152","   - Gradient boosting estimators accept the parameter ``criterion`` to specify","153","     to splitting criterion used in built decision trees. (`#6667","154","     <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/6667>`_) by `Nelson","155","     Liu`_.","156","","4301","","4302",".. _Nelson Liu: https:\/\/github.com\/nelson-liu"],"delete":[]}],"sklearn\/tree\/_utils.pxd":[{"add":["4","#          Nelson Liu <nelson@nelsonliu.me>","36","    (WeightedPQueueRecord*)","37","    (DOUBLE_t*)","109","","110","# =============================================================================","111","# WeightedPQueue data structure","112","# =============================================================================","113","","114","# A record stored in the WeightedPQueue","115","cdef struct WeightedPQueueRecord:","116","    DOUBLE_t data","117","    DOUBLE_t weight","118","","119","cdef class WeightedPQueue:","120","    cdef SIZE_t capacity","121","    cdef SIZE_t array_ptr","122","    cdef WeightedPQueueRecord* array_","123","","124","    cdef bint is_empty(self) nogil","125","    cdef void reset(self) nogil","126","    cdef SIZE_t size(self) nogil","127","    cdef int push(self, DOUBLE_t data, DOUBLE_t weight) nogil","128","    cdef int remove(self, DOUBLE_t data, DOUBLE_t weight) nogil","129","    cdef int pop(self, DOUBLE_t* data, DOUBLE_t* weight) nogil","130","    cdef int peek(self, DOUBLE_t* data, DOUBLE_t* weight) nogil","131","    cdef DOUBLE_t get_weight_from_index(self, SIZE_t index) nogil","132","    cdef DOUBLE_t get_value_from_index(self, SIZE_t index) nogil","133","","134","","135","# =============================================================================","136","# WeightedMedianCalculator data structure","137","# =============================================================================","138","","139","cdef class WeightedMedianCalculator:","140","    cdef SIZE_t initial_capacity","141","    cdef WeightedPQueue samples","142","    cdef DOUBLE_t total_weight","143","    cdef SIZE_t k","144","    cdef DOUBLE_t sum_w_0_k            # represents sum(weights[0:k])","145","                                       # = w[0] + w[1] + ... + w[k-1]","146","","147","    cdef SIZE_t size(self) nogil","148","    cdef int push(self, DOUBLE_t data, DOUBLE_t weight) nogil","149","    cdef void reset(self) nogil","150","    cdef int update_median_parameters_post_push(self, DOUBLE_t data,","151","                                                DOUBLE_t weight,","152","                                                DOUBLE_t original_median) nogil","153","    cdef int remove(self, DOUBLE_t data, DOUBLE_t weight) nogil","154","    cdef int pop(self, DOUBLE_t* data, DOUBLE_t* weight) nogil","155","    cdef int update_median_parameters_post_remove(self, DOUBLE_t data,","156","                                                  DOUBLE_t weight,","157","                                                  DOUBLE_t original_median) nogil","158","    cdef DOUBLE_t get_median(self) nogil"],"delete":[]}],"sklearn\/ensemble\/forest.py":[{"add":["950","        The function to measure the quality of a split. Supported criteria","951","        are \"mse\" for the mean squared error, which is equal to variance","952","        reduction as feature selection criterion, and \"mae\" for the mean","953","        absolute error.","1304","        The function to measure the quality of a split. Supported criteria","1305","        are \"mse\" for the mean squared error, which is equal to variance","1306","        reduction as feature selection criterion, and \"mae\" for the mean","1307","        absolute error."],"delete":["950","        The function to measure the quality of a split. The only supported","951","        criterion is \"mse\" for the mean squared error.","1302","        The function to measure the quality of a split. The only supported","1303","        criterion is \"mse\" for the mean squared error."]}],"sklearn\/tree\/tests\/test_tree.py":[{"add":["50","REG_CRITERIONS = (\"mse\", \"mae\")","1445","","1446","def test_mae():","1447","    # check MAE criterion produces correct results","1448","    # on small toy dataset","1449","    dt_mae = DecisionTreeRegressor(random_state=0, criterion=\"mae\",","1450","                                   max_leaf_nodes=2)","1451","    dt_mae.fit([[3],[5],[3],[8],[5]],[6,7,3,4,3])","1452","    assert_array_equal(dt_mae.tree_.impurity, [1.4, 1.5, 4.0\/3.0])","1453","    assert_array_equal(dt_mae.tree_.value.flat, [4, 4.5, 4.0])","1454","","1455","    dt_mae.fit([[3],[5],[3],[8],[5]],[6,7,3,4,3], [0.6,0.3,0.1,1.0,0.3])","1456","    assert_array_equal(dt_mae.tree_.impurity, [7.0\/2.3, 3.0\/0.7, 4.0\/1.6])","1457","    assert_array_equal(dt_mae.tree_.value.flat, [4.0, 6.0, 4.0])"],"delete":["50","REG_CRITERIONS = (\"mse\", )"]}]}},"e2a2b4d403742c3eb2d0e085631a014b0975d6af":{"changes":{"examples\/linear_model\/plot_ard.py":"MODIFY","sklearn\/linear_model\/sag_fast.pyx":"MODIFY","examples\/ensemble\/plot_ensemble_oob.py":"MODIFY","examples\/hetero_feature_union.py":"MODIFY","sklearn\/linear_model\/sag.py":"MODIFY","benchmarks\/bench_plot_randomized_svd.py":"MODIFY","sklearn\/linear_model\/randomized_l1.py":"MODIFY","examples\/calibration\/plot_calibration_multiclass.py":"MODIFY","examples\/linear_model\/plot_lasso_coordinate_descent_path.py":"MODIFY","sklearn\/linear_model\/sgd_fast.pyx":"MODIFY"},"diff":{"examples\/linear_model\/plot_ard.py":[{"add":["41","# Create noise with a precision alpha of 50."],"delete":["41","# Create noite with a precision alpha of 50."]}],"sklearn\/linear_model\/sag_fast.pyx":[{"add":["266","    # the maximum change in weights, used to compute stopping criteria","268","    # a holder variable for the max weight, used to compute stopping criteria"],"delete":["266","    # the maximum change in weights, used to compute stopping criterea","268","    # a holder variable for the max weight, used to compute stopping criterea"]}],"examples\/ensemble\/plot_ensemble_oob.py":[{"add":["43","# support for parallelized ensembles but is necessary for tracking the OOB"],"delete":["43","# support for paralellised ensembles but is necessary for tracking the OOB"]}],"examples\/hetero_feature_union.py":[{"add":["167","# limit the list of categories to make running this example faster."],"delete":["167","# limit the list of categories to make running this exmaple faster."]}],"sklearn\/linear_model\/sag.py":[{"add":["112","        criteria is not reached. Defaults to 1000.","115","        The stopping criteria for the weights. The iterations will stop when"],"delete":["112","        criterea is not reached. Defaults to 1000.","115","        The stopping criterea for the weights. The iterations will stop when"]}],"benchmarks\/bench_plot_randomized_svd.py":[{"add":["375","    title = \"%s: Frobenius norm diff vs n power iteration\" % (dataset_name)"],"delete":["375","    title = \"%s: frobenius norm diff vs n power iteration\" % (dataset_name)"]}],"sklearn\/linear_model\/randomized_l1.py":[{"add":["548","    # Sort alphas in ascending order","566","    \"\"\"Stability path based on randomized Lasso estimates"],"delete":["548","    # Sort alphas in assending order","566","    \"\"\"Stabiliy path based on randomized Lasso estimates"]}],"examples\/calibration\/plot_calibration_multiclass.py":[{"add":["147","# Plot modifications of calibrator"],"delete":["147","# Ploit modifications of calibrator"]}],"examples\/linear_model\/plot_lasso_coordinate_descent_path.py":[{"add":["42","print(\"Computing regularization path using the positive elastic net...\")"],"delete":["42","print(\"Computing regularization path using the positve elastic net...\")"]}],"sklearn\/linear_model\/sgd_fast.pyx":[{"add":["484","        (4) Passive Aggressive-I, eta = min(alpha, loss\/norm(x))","485","        (5) Passive Aggressive-II, eta = 1.0 \/ (norm(x) + 0.5*alpha)","505","        The averaged weights across iterations","507","        The averaged intercept across iterations"],"delete":["484","        (4) Passive Agressive-I, eta = min(alpha, loss\/norm(x))","485","        (5) Passive Agressive-II, eta = 1.0 \/ (norm(x) + 0.5*alpha)","505","        The averaged weights accross iterations","507","        The averaged intercept accross iterations"]}]}},"65b7d7a8913e362239eb8fd4a673787f21411ef4":{"changes":{"sklearn\/mixture\/tests\/test_gaussian_mixture.py":"MODIFY","sklearn\/mixture\/base.py":"MODIFY","sklearn\/mixture\/gaussian_mixture.py":"MODIFY"},"diff":{"sklearn\/mixture\/tests\/test_gaussian_mixture.py":[{"add":["0","# Author: Wei Xue <xuewei4d@gmail.com>","1","#         Thierry Guillemot <thierry.guillemot.work@gmail.com>","2","# License: BSD 3 clauseimport warnings","3","","20","    _estimate_gaussian_covariances_spherical)","21","from sklearn.mixture.gaussian_mixture import _compute_precision_cholesky","22","from sklearn.mixture.gaussian_mixture import _compute_log_det_cholesky","392","","440","","441","def test_compute_log_det_cholesky():","442","    n_features = 2","443","    rand_data = RandomData(np.random.RandomState(0))","444","","445","    for covar_type in COVARIANCE_TYPE:","446","        covariance = rand_data.covariances[covar_type]","447","","448","        if covar_type == 'full':","449","            predected_det = np.array([linalg.det(cov) for cov in covariance])","450","        elif covar_type == 'tied':","451","            predected_det = linalg.det(covariance)","452","        elif covar_type == 'diag':","453","            predected_det = np.array([np.prod(cov) for cov in covariance])","454","        elif covar_type == 'spherical':","455","            predected_det = covariance ** n_features","456","","457","        # We compute the cholesky decomposition of the covariance matrix","458","        expected_det = _compute_log_det_cholesky(_compute_precision_cholesky(","459","            covariance, covar_type), covar_type, n_features=n_features)","460","        assert_array_almost_equal(expected_det, - .5 * np.log(predected_det))","461","","462","","472","    from sklearn.mixture.gaussian_mixture import _estimate_log_gaussian_prob","489","    log_prob = _estimate_log_gaussian_prob(X, means, precs_full, 'full')","494","    log_prob = _estimate_log_gaussian_prob(X, means, precs_chol_diag, 'diag')","503","    log_prob = _estimate_log_gaussian_prob(X, means, precs_tied, 'tied')","513","    log_prob = _estimate_log_gaussian_prob(X, means,","514","                                           precs_spherical, 'spherical')","751","    g = GaussianMixture(n_components=n_components, n_init=1, max_iter=2,","752","                        reg_covar=0, random_state=random_state,","754","    h = GaussianMixture(n_components=n_components, n_init=1, max_iter=1,","755","                        reg_covar=0, random_state=random_state,","854","            for _ in range(600):","866","            assert_true(gmm.converged_)","867","","901","                              covariance_type=covar_type, random_state=rng,","902","                              n_init=5)","905","        if covar_type == 'full':","909","        elif covar_type == 'tied':"],"delete":["16","    _estimate_gaussian_covariances_spherical,","17","    _compute_precision_cholesky)","443","    from sklearn.mixture.gaussian_mixture import (","444","        _estimate_log_gaussian_prob_full,","445","        _estimate_log_gaussian_prob_tied,","446","        _estimate_log_gaussian_prob_diag,","447","        _estimate_log_gaussian_prob_spherical)","464","    log_prob = _estimate_log_gaussian_prob_full(X, means, precs_full)","469","    log_prob = _estimate_log_gaussian_prob_diag(X, means, precs_chol_diag)","478","    log_prob = _estimate_log_gaussian_prob_tied(X, means, precs_tied)","488","    log_prob = _estimate_log_gaussian_prob_spherical(X, means, precs_spherical)","725","    g = GaussianMixture(n_components=n_components, n_init=1,","726","                        max_iter=2, reg_covar=0, random_state=random_state,","728","    h = GaussianMixture(n_components=n_components, n_init=1,","729","                        max_iter=1, reg_covar=0, random_state=random_state,","828","            for _ in range(300):","873","                              covariance_type=covar_type, random_state=rng)","876","        if covar_type is 'full':","880","        elif covar_type is 'tied':"]}],"sklearn\/mixture\/base.py":[{"add":["4","# License: BSD 3 clause","139","        n_samples, _ = X.shape","148","            resp = random_state.rand(n_samples, self.n_components)","194","        max_lower_bound = -np.infty","197","        n_samples, _ = X.shape","203","                self.lower_bound_ = np.infty","206","                prev_lower_bound = self.lower_bound_","208","                log_prob_norm, log_resp = self._e_step(X)","209","                self._m_step(X, log_resp)","210","                self.lower_bound_ = self._compute_lower_bound(","211","                    log_resp, log_prob_norm)","212","","213","                change = self.lower_bound_ - prev_lower_bound","220","            self._print_verbose_msg_init_end(self.lower_bound_)","222","            if self.lower_bound_ > max_lower_bound:","223","                max_lower_bound = self.lower_bound_","249","        log_prob_norm : array, shape (n_samples,)","250","            log p(X)","252","        log_responsibility : array, shape (n_samples, n_components)","253","            logarithm of the responsibilities","258","    def _m_step(self, X, log_resp):","265","        log_resp : array-like, shape (n_samples, n_components)","351","        _, log_resp = self._estimate_log_prob_resp(X)","417","        return log_prob_norm, log_resp"],"delete":["138","        n_samples = X.shape[0]","147","            resp = random_state.rand(X.shape[0], self.n_components)","193","        max_log_likelihood = -np.infty","201","            current_log_likelihood, resp = self._e_step(X)","204","                prev_log_likelihood = current_log_likelihood","206","                self._m_step(X, resp)","207","                current_log_likelihood, resp = self._e_step(X)","208","                change = current_log_likelihood - prev_log_likelihood","215","            self._print_verbose_msg_init_end(current_log_likelihood)","217","            if current_log_likelihood > max_log_likelihood:","218","                max_log_likelihood = current_log_likelihood","244","        log-likelihood : scalar","246","        responsibility : array, shape (n_samples, n_components)","251","    def _m_step(self, X, resp):","258","        resp : array-like, shape (n_samples, n_components)","344","        _, _, log_resp = self._estimate_log_prob_resp(X)","402","        log_prob : array, shape (n_samples, n_components)","403","            log p(X|Z) + log weights","404","","413","        return log_prob_norm, weighted_log_prob, log_resp"]}],"sklearn\/mixture\/gaussian_mixture.py":[{"add":["4","# License: BSD 3 clause","14","from ..utils.extmath import row_norms","340","def _compute_log_det_cholesky(matrix_chol, covariance_type, n_features):","341","    \"\"\"Compute the log-det of the cholesky decomposition of matrices.","342","","343","    Parameters","344","    ----------","345","    matrix_chol : array-like,","346","        Cholesky decompositions of the matrices.","347","        'full' : shape of (n_components, n_features, n_features)","348","        'tied' : shape of (n_features, n_features)","349","        'diag' : shape of (n_components, n_features)","350","        'spherical' : shape of (n_components,)","351","","352","    covariance_type : {'full', 'tied', 'diag', 'spherical'}","353","","354","    n_features : int","355","        Number of features.","356","","357","    Returns","358","    -------","359","    log_det_precision_chol : array-like, shape (n_components,)","360","        The determinant of the cholesky decomposition.","361","        matrix.","362","    \"\"\"","363","    if covariance_type == 'full':","364","        n_components, _, _ = matrix_chol.shape","365","        log_det_chol = (np.sum(np.log(","366","            matrix_chol.reshape(","367","                n_components, -1)[:, ::n_features + 1]), 1))","368","","369","    elif covariance_type == 'tied':","370","        log_det_chol = (np.sum(np.log(np.diag(matrix_chol))))","371","","372","    elif covariance_type == 'diag':","373","        log_det_chol = (np.sum(np.log(matrix_chol), axis=1))","374","","375","    else:","376","        log_det_chol = n_features * (np.log(matrix_chol))","377","","378","    return log_det_chol","379","","380","","381","def _estimate_log_gaussian_prob(X, means, precisions_chol, covariance_type):","382","    \"\"\"Estimate the log Gaussian probability.","390","    precisions_chol : array-like,","392","        'full' : shape of (n_components, n_features, n_features)","393","        'tied' : shape of (n_features, n_features)","394","        'diag' : shape of (n_components, n_features)","395","        'spherical' : shape of (n_components,)","396","","397","    covariance_type : {'full', 'tied', 'diag', 'spherical'}","405","    # det(precision_chol) is half of det(precision)","406","    log_det = _compute_log_det_cholesky(","407","        precisions_chol, covariance_type, n_features)","409","    if covariance_type == 'full':","410","        log_prob = np.empty((n_samples, n_components))","411","        for k, (mu, prec_chol) in enumerate(zip(means, precisions_chol)):","412","            y = np.dot(X, prec_chol) - np.dot(mu, prec_chol)","413","            log_prob[:, k] = np.sum(np.square(y), axis=1)","415","    elif covariance_type == 'tied':","416","        log_prob = np.empty((n_samples, n_components))","417","        for k, mu in enumerate(means):","418","            y = np.dot(X, precisions_chol) - np.dot(mu, precisions_chol)","419","            log_prob[:, k] = np.sum(np.square(y), axis=1)","421","    elif covariance_type == 'diag':","422","        precisions = precisions_chol ** 2","423","        log_prob = (np.sum((means ** 2 * precisions), 1) -","424","                    2. * np.dot(X, (means * precisions).T) +","425","                    np.dot(X ** 2, precisions.T))","427","    elif covariance_type == 'spherical':","428","        precisions = precisions_chol ** 2","429","        log_prob = (np.sum(means ** 2, 1) * precisions -","430","                    2 * np.dot(X, means.T * precisions) +","431","                    np.outer(row_norms(X, squared=True), precisions))","432","    return -.5 * (n_features * np.log(2 * np.pi) + log_prob) + log_det","468","        The number of initializations to perform. The best results are kept.","556","","557","    lower_bound_ : float","558","        Log-likelihood of the best fit of EM.","634","        log_prob_norm, log_resp = self._estimate_log_prob_resp(X)","647","        return _estimate_log_gaussian_prob(","648","            X, self.means_, self.precisions_cholesky_, self.covariance_type)","653","    def _compute_lower_bound(self, _, log_prob_norm):","654","        return log_prob_norm","655","","660","        return (self.weights_, self.means_, self.covariances_,","661","                self.precisions_cholesky_)","664","        (self.weights_, self.means_, self.covariances_,","665","         self.precisions_cholesky_) = params"],"delete":["338","def _estimate_log_gaussian_prob_full(X, means, precisions_chol):","339","    \"\"\"Estimate the log Gaussian probability for 'full' precision.","347","    precisions_chol : array-like, shape (n_components, n_features, n_features)","356","    log_prob = np.empty((n_samples, n_components))","357","    for k, (mu, prec_chol) in enumerate(zip(means, precisions_chol)):","358","        log_det = -2. * np.sum(np.log(np.diagonal(prec_chol)))","359","        y = np.dot(X - mu, prec_chol)","360","        log_prob[:, k] = -.5 * (n_features * np.log(2. * np.pi) + log_det +","361","                                np.sum(np.square(y), axis=1))","362","    return log_prob","365","def _estimate_log_gaussian_prob_tied(X, means, precision_chol):","366","    \"\"\"Estimate the log Gaussian probability for 'tied' precision.","368","    Parameters","369","    ----------","370","    X : array-like, shape (n_samples, n_features)","372","    means : array-like, shape (n_components, n_features)","373","","374","    precision_chol : array-like, shape (n_features, n_features)","375","        Cholesky decomposition of the precision matrix.","376","","377","    Returns","378","    -------","379","    log_prob : array-like, shape (n_samples, n_components)","380","    \"\"\"","381","    n_samples, n_features = X.shape","382","    n_components, _ = means.shape","383","    log_prob = np.empty((n_samples, n_components))","384","    log_det = -2. * np.sum(np.log(np.diagonal(precision_chol)))","385","    for k, mu in enumerate(means):","386","        y = np.dot(X - mu, precision_chol)","387","        log_prob[:, k] = np.sum(np.square(y), axis=1)","388","    log_prob = -.5 * (n_features * np.log(2. * np.pi) + log_det + log_prob)","389","    return log_prob","390","","391","","392","def _estimate_log_gaussian_prob_diag(X, means, precisions_chol):","393","    \"\"\"Estimate the log Gaussian probability for 'diag' precision.","394","","395","    Parameters","396","    ----------","397","    X : array-like, shape (n_samples, n_features)","398","","399","    means : array-like, shape (n_components, n_features)","400","","401","    precisions_chol : array-like, shape (n_components, n_features)","402","        Cholesky decompositions of the precision matrices.","403","","404","    Returns","405","    -------","406","    log_prob : array-like, shape (n_samples, n_components)","407","    \"\"\"","408","    n_samples, n_features = X.shape","409","    precisions = precisions_chol ** 2","410","    log_prob = -.5 * (n_features * np.log(2. * np.pi) -","411","                      np.sum(np.log(precisions), 1) +","412","                      np.sum((means ** 2 * precisions), 1) -","413","                      2. * np.dot(X, (means * precisions).T) +","414","                      np.dot(X ** 2, precisions.T))","415","    return log_prob","416","","417","","418","def _estimate_log_gaussian_prob_spherical(X, means, precisions_chol):","419","    \"\"\"Estimate the log Gaussian probability for 'spherical' precision.","420","","421","    Parameters","422","    ----------","423","    X : array-like, shape (n_samples, n_features)","424","","425","    means : array-like, shape (n_components, n_features)","426","","427","    precisions_chol : array-like, shape (n_components, )","428","        Cholesky decompositions of the precision matrices.","429","","430","    Returns","431","    -------","432","    log_prob : array-like, shape (n_samples, n_components)","433","    \"\"\"","434","    n_samples, n_features = X.shape","435","    precisions = precisions_chol ** 2","436","    log_prob = -.5 * (n_features * np.log(2 * np.pi) -","437","                      n_features * np.log(precisions) +","438","                      np.sum(means ** 2, 1) * precisions -","439","                      2 * np.dot(X, means.T * precisions) +","440","                      np.outer(np.sum(X ** 2, axis=1), precisions))","441","    return log_prob","477","        The number of initializations to perform. The best results is kept.","640","        log_prob_norm, _, log_resp = self._estimate_log_prob_resp(X)","653","        return {\"full\": _estimate_log_gaussian_prob_full,","654","                \"tied\": _estimate_log_gaussian_prob_tied,","655","                \"diag\": _estimate_log_gaussian_prob_diag,","656","                \"spherical\": _estimate_log_gaussian_prob_spherical","657","                }[self.covariance_type](X, self.means_,","658","                                        self.precisions_cholesky_)","667","        return self.weights_, self.means_, self.precisions_cholesky_","670","        self.weights_, self.means_, self.precisions_cholesky_ = params"]}]}},"db370178d6a3525c881ea161b9373a7f65fb810a":{"changes":{"examples\/cluster\/plot_cluster_comparison.py":"MODIFY"},"diff":{"examples\/cluster\/plot_cluster_comparison.py":[{"add":["5","This example shows characteristics of different","7","but still in 2D. With the exception of the last dataset,","8","the parameters of each of these dataset-algorithm pairs","9","has been tuned to produce good clustering results. Some","10","algorithms are more sensitive to parameter values than","11","others.","13","The last dataset is an example of a 'null' situation for","14","clustering: the data is homogeneous, and there is no good","15","clustering. For this example, the null dataset uses the","16","same parameters as the dataset in the row above it, which","17","represents a mismatch in the parameter values and the","18","data structure.","20","While these examples give some intuition about the","21","algorithms, this intuition might not apply to very high","22","dimensional data.","27","import warnings","32","from sklearn import cluster, datasets, mixture","35","from itertools import cycle, islice","39","# ============","42","# ============","50","# Anisotropicly distributed data","51","random_state = 170","52","X, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)","53","transformation = [[0.6, -0.6], [-0.4, 0.8]]","54","X_aniso = np.dot(X, transformation)","55","aniso = (X_aniso, y)","57","# blobs with varied variances","58","varied = datasets.make_blobs(n_samples=n_samples,","59","                             cluster_std=[1.0, 2.5, 0.5],","60","                             random_state=random_state)","62","# ============","63","# Set up cluster parameters","64","# ============","65","plt.figure(figsize=(9 * 2 + 3, 12.5))","71","default_base = {'quantile': .3,","72","                'eps': .3,","73","                'damping': .9,","74","                'preference': -200,","75","                'n_neighbors': 10,","76","                'n_clusters': 3}","77","","78","datasets = [","79","    (noisy_circles, {'damping': .77, 'preference': -240,","80","                     'quantile': .2, 'n_clusters': 2}),","81","    (noisy_moons, {'damping': .75, 'preference': -220, 'n_clusters': 2}),","82","    (varied, {'eps': .18, 'n_neighbors': 2}),","83","    (aniso, {'eps': .15, 'n_neighbors': 2}),","84","    (blobs, {}),","85","    (no_structure, {})]","86","","87","for i_dataset, (dataset, algo_params) in enumerate(datasets):","88","    # update parameters with dataset-specific values","89","    params = default_base.copy()","90","    params.update(algo_params)","91","","93","","98","    bandwidth = cluster.estimate_bandwidth(X, quantile=params['quantile'])","101","    connectivity = kneighbors_graph(","102","        X, n_neighbors=params['n_neighbors'], include_self=False)","106","    # ============","107","    # Create cluster objects","108","    # ============","110","    two_means = cluster.MiniBatchKMeans(n_clusters=params['n_clusters'])","111","    ward = cluster.AgglomerativeClustering(","112","        n_clusters=params['n_clusters'], linkage='ward',","114","    spectral = cluster.SpectralClustering(","115","        n_clusters=params['n_clusters'], eigen_solver='arpack',","116","        affinity=\"nearest_neighbors\")","117","    dbscan = cluster.DBSCAN(eps=params['eps'])","118","    affinity_propagation = cluster.AffinityPropagation(","119","        damping=params['damping'], preference=params['preference'])","120","    average_linkage = cluster.AgglomerativeClustering(","121","        linkage=\"average\", affinity=\"cityblock\",","122","        n_clusters=params['n_clusters'], connectivity=connectivity)","123","    birch = cluster.Birch(n_clusters=params['n_clusters'])","124","    gmm = mixture.GaussianMixture(","125","        n_components=params['n_clusters'], covariance_type='full')","127","    clustering_algorithms = (","128","        ('MiniBatchKMeans', two_means),","129","        ('AffinityPropagation', affinity_propagation),","130","        ('MeanShift', ms),","131","        ('SpectralClustering', spectral),","132","        ('Ward', ward),","133","        ('AgglomerativeClustering', average_linkage),","134","        ('DBSCAN', dbscan),","135","        ('Birch', birch),","136","        ('GaussianMixture', gmm)","137","    )","139","    for name, algorithm in clustering_algorithms:","141","","142","        # catch warnings related to kneighbors_graph","143","        with warnings.catch_warnings():","144","            warnings.filterwarnings(","145","                \"ignore\",","146","                message=\"the number of connected components of the \" +","147","                \"connectivity matrix is [0-9]{1,2}\" +","148","                \" > 1. Completing it to avoid stopping the tree early.\",","149","                category=UserWarning)","150","            warnings.filterwarnings(","151","                \"ignore\",","152","                message=\"Graph is not fully connected, spectral embedding\" +","153","                \" may not work as expected.\",","154","                category=UserWarning)","155","            algorithm.fit(X)","156","","163","        plt.subplot(len(datasets), len(clustering_algorithms), plot_num)","167","        colors = np.array(list(islice(cycle(['#377eb8', '#ff7f00', '#4daf4a',","168","                                             '#f781bf', '#a65628', '#984ea3',","169","                                             '#999999', '#e41a1c', '#dede00']),","170","                                      int(max(y_pred) + 1))))","171","        plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[y_pred])","172","","173","        plt.xlim(-2.5, 2.5)","174","        plt.ylim(-2.5, 2.5)"],"delete":["5","This example aims at showing characteristics of different","7","but still in 2D. The last dataset is an example of a 'null'","8","situation for clustering: the data is homogeneous, and","9","there is no good clustering.","11","While these examples give some intuition about the algorithms,","12","this intuition might not apply to very high dimensional data.","14","The results could be improved by tweaking the parameters for","15","each clustering strategy, for instance setting the number of","16","clusters for the methods that needs this parameter","17","specified. Note that affinity propagation has a tendency to","18","create many clusters. Thus in this example its two parameters","19","(damping and per-point preference) were set to mitigate this","20","behavior.","29","from sklearn import cluster, datasets","44","colors = np.array([x for x in 'bgrcmykbgrcmykbgrcmykbgrcmyk'])","45","colors = np.hstack([colors] * 20)","47","clustering_names = [","48","    'MiniBatchKMeans', 'AffinityPropagation', 'MeanShift',","49","    'SpectralClustering', 'Ward', 'AgglomerativeClustering',","50","    'DBSCAN', 'Birch']","52","plt.figure(figsize=(len(clustering_names) * 2 + 3, 9.5))","58","datasets = [noisy_circles, noisy_moons, blobs, no_structure]","59","for i_dataset, dataset in enumerate(datasets):","65","    bandwidth = cluster.estimate_bandwidth(X, quantile=0.3)","68","    connectivity = kneighbors_graph(X, n_neighbors=10, include_self=False)","72","    # create clustering estimators","74","    two_means = cluster.MiniBatchKMeans(n_clusters=2)","75","    ward = cluster.AgglomerativeClustering(n_clusters=2, linkage='ward',","76","                                           connectivity=connectivity)","77","    spectral = cluster.SpectralClustering(n_clusters=2,","78","                                          eigen_solver='arpack',","79","                                          affinity=\"nearest_neighbors\")","80","    dbscan = cluster.DBSCAN(eps=.2)","81","    affinity_propagation = cluster.AffinityPropagation(damping=.9,","82","                                                       preference=-200)","83","","84","    average_linkage = cluster.AgglomerativeClustering(","85","        linkage=\"average\", affinity=\"cityblock\", n_clusters=2,","88","    birch = cluster.Birch(n_clusters=2)","89","    clustering_algorithms = [","90","        two_means, affinity_propagation, ms, spectral, ward, average_linkage,","91","        dbscan, birch]","93","    for name, algorithm in zip(clustering_names, clustering_algorithms):","94","        # predict cluster memberships","96","        algorithm.fit(X)","103","        # plot","104","        plt.subplot(4, len(clustering_algorithms), plot_num)","107","        plt.scatter(X[:, 0], X[:, 1], color=colors[y_pred].tolist(), s=10)","109","        if hasattr(algorithm, 'cluster_centers_'):","110","            centers = algorithm.cluster_centers_","111","            center_colors = colors[:len(centers)]","112","            plt.scatter(centers[:, 0], centers[:, 1], s=100, c=center_colors)","113","        plt.xlim(-2, 2)","114","        plt.ylim(-2, 2)"]}]}},"c948319b0ab31a45a365ac4ac22ce9d311c8306a":{"changes":{"sklearn\/manifold\/tests\/test_locally_linear.py":"MODIFY","sklearn\/manifold\/locally_linear.py":"MODIFY"},"diff":{"sklearn\/manifold\/tests\/test_locally_linear.py":[{"add":["136","","137","","138","# regression test for #6033","139","def test_integer_input():","140","    rand = np.random.RandomState(0)","141","    X = rand.randint(0, 100, size=(20, 3))","142","","143","    for method in [\"standard\", \"hessian\", \"modified\", \"ltsa\"]:","144","        clf = manifold.LocallyLinearEmbedding(method=method, n_neighbors=10)","145","        clf.fit(X)  # this previously raised a TypeError"],"delete":[]}],"sklearn\/manifold\/locally_linear.py":[{"add":["628","        X = check_array(X, dtype=float)"],"delete":["628","        X = check_array(X)"]}]}},"a2dac46e71c3b6e1cae3fa8f83867dbb2ddd2487":{"changes":{"doc\/tutorial\/basic\/tutorial.rst":"MODIFY","doc\/tutorial\/statistical_inference\/supervised_learning.rst":"MODIFY"},"diff":{"doc\/tutorial\/basic\/tutorial.rst":[{"add":["310","(an integer array) was used in ``fit``. The second ``predict()`` returns a string"],"delete":["310","(an integer array) was used in ``fit``. The second ``predict`` returns a string"]}],"doc\/tutorial\/statistical_inference\/supervised_learning.rst":[{"add":["189","    >>> # between X and y."],"delete":["189","    >>> # between X and Y."]}]}},"204472af0fb2d3ef038caf303df16ae245449201":{"changes":{"sklearn\/svm\/src\/libsvm\/svm.cpp":"MODIFY"},"diff":{"sklearn\/svm\/src\/libsvm\/svm.cpp":[{"add":["1011","\tif(Gmax+Gmax2 < eps || Gmin_idx == -1)","1263","\tif(max(Gmaxp+Gmaxp2,Gmaxn+Gmaxn2) < eps || Gmin_idx == -1)"],"delete":["1011","\tif(Gmax+Gmax2 < eps)","1263","\tif(max(Gmaxp+Gmaxp2,Gmaxn+Gmaxn2) < eps)"]}]}},"1002de22d950934c20e434baec866469dc6c204f":{"changes":{"sklearn\/calibration.py":"MODIFY"},"diff":{"sklearn\/calibration.py":[{"add":["35","    it is assumed that base_estimator has been fitted already and all","36","    data is used for calibration. Note that data for fitting the","37","    classifier and for calibrating it must be disjoint.","128","        # Check that each cross-validation fold can have at least one"],"delete":["35","    it is it is assumed that base_estimator has been","36","    fitted already and all data is used for calibration. Note that","37","    data for fitting the classifier and for calibrating it must be disjoint.","128","        # Check that we each cross-validation fold can have at least one"]}]}},"dc42bde3e9588ca7ed0232ff70dc8048a1411869":{"changes":{"sklearn\/model_selection\/tests\/test_search.py":"MODIFY","sklearn\/model_selection\/tests\/test_validation.py":"MODIFY","sklearn\/model_selection\/_search.py":"MODIFY"},"diff":{"sklearn\/model_selection\/tests\/test_search.py":[{"add":["154","        assert_true(grid_search.grid_scores_[i][0] == {'foo_param': foo_i})","206","    score_no_scoring = search_no_scoring.score(X, y)","207","    score_accuracy = search_accuracy.score(X, y)","208","    score_no_score_auc = search_no_score_method_auc.score(X, y)","209","    score_auc = search_auc.score(X, y)","210",""],"delete":["22","from sklearn.utils.testing import assert_no_warnings","46","# TODO Import from sklearn.exceptions once merged.","47","from sklearn.base import ChangedBehaviorWarning","157","        assert_true(grid_search.grid_scores_[i][0]","158","                    == {'foo_param': foo_i})","210","    score_no_scoring = assert_no_warnings(search_no_scoring.score, X, y)","211","    score_accuracy = assert_warns(ChangedBehaviorWarning,","212","                                  search_accuracy.score, X, y)","213","    score_no_score_auc = assert_no_warnings(search_no_score_method_auc.score,","214","                                            X, y)","215","    score_auc = assert_warns(ChangedBehaviorWarning,","216","                             search_auc.score, X, y)"]}],"sklearn\/model_selection\/tests\/test_validation.py":[{"add":["381","        return (((y_true == y_pred).sum() - (y_true != y_pred).sum()) \/","382","                y_true.shape[0])"],"delete":["381","        return (((y_true == y_pred).sum() - (y_true != y_pred).sum())","382","                \/ y_true.shape[0])"]}],"sklearn\/model_selection\/_search.py":[{"add":["21","from ..base import MetaEstimatorMixin","235","                    \"than n_iter=%d. For exhaustive searches, use \"","236","                    \"GridSearchCV.\" % (grid_size, self.n_iter))"],"delete":["17","import warnings","22","from ..base import MetaEstimatorMixin, ChangedBehaviorWarning","236","                    \"than n_iter=%d.\" % (grid_size, self.n_iter)","237","                    + \" For exhaustive searches, use GridSearchCV.\")","406","","407","        Notes","408","        -----","409","         * The long-standing behavior of this method changed in version 0.16.","410","         * It no longer uses the metric provided by ``estimator.score`` if the","411","           ``scoring`` parameter was set when fitting.","412","","418","        if self.scoring is not None and hasattr(self.best_estimator_, 'score'):","419","            warnings.warn(\"The long-standing behavior to use the estimator's \"","420","                          \"score function in {0}.score has changed. The \"","421","                          \"scoring parameter is now used.\"","422","                          \"\".format(self.__class__.__name__),","423","                          ChangedBehaviorWarning)"]}]}},"1f381ae72875533ccad4f14c2e490b8339f67d4f":{"changes":{"sklearn\/kernel_ridge.py":"MODIFY"},"diff":{"sklearn\/kernel_ridge.py":[{"add":["71","    dual_coef_ : array, shape = [n_samples] or [n_samples, n_targets]","72","        Representation of weight vector(s) in kernel space"],"delete":["71","    dual_coef_ : array, shape = [n_features] or [n_targets, n_features]","72","        Weight vector(s) in kernel space"]}]}},"f2df3db65d3e4f0c273ace7f0cce99412ab10ce5":{"changes":{"sklearn\/feature_extraction\/text.py":"MODIFY"},"diff":{"sklearn\/feature_extraction\/text.py":[{"add":["995","            self._idf_diag = sp.spdiags(idf, diags=0, m=n_features, ","996","                                        n=n_features, format='csr')"],"delete":["995","            self._idf_diag = sp.spdiags(idf,","996","                                        diags=0, m=n_features, n=n_features)"]}]}},"1b6c84da13678bb0bfae23dc67fe8d7618a6c097":{"changes":{"sklearn\/datasets\/base.py":"MODIFY","doc\/whats_new.rst":"MODIFY"},"diff":{"sklearn\/datasets\/base.py":[{"add":["266","    .. versionadded:: 0.18","267","","279","    .. versionadded:: 0.18","280",""],"delete":["262","    ","263","        .. versionadded:: 0.18","264","     "]}],"doc\/whats_new.rst":[{"add":["232","   - Added new return type ``(data, target)`` : tuple option to","233","     :func:`load_iris` dataset.","234","     (`#7049 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/7049>`_) by","235","     `Manvendra Singh`_.","4321",".. _Ibraim Ganiev: https:\/\/github.com\/olologin"],"delete":["232","   - Added new return type ``(data, target)`` : tuple option to :func:`load_iris` dataset. (`#7049 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/7049>`_) ","233","     By `Manvendra Singh`_ and `Nelson Liu`_.   ","4319",".. _Ibraim Ganiev: https:\/\/github.com\/olologin"]}]}},"a9bf9146e07b9900126884c4412c869a08b5e92a":{"changes":{"sklearn\/metrics\/cluster\/expected_mutual_info_fast.pyx":"MODIFY"},"diff":{"sklearn\/metrics\/cluster\/expected_mutual_info_fast.pyx":[{"add":["15","ctypedef np.float64_t DOUBLE","22","    cdef DOUBLE N, gln_N, emi, term2, term3, gln","23","    cdef np.ndarray[DOUBLE] gln_a, gln_b, gln_Na, gln_Nb, gln_nij, log_Nnij","24","    cdef np.ndarray[DOUBLE] nijs, term1","25","    cdef np.ndarray[DOUBLE, ndim=2] log_ab_outer","29","    N = <DOUBLE>n_samples"],"delete":["15","","22","    cdef float N, gln_N, emi, term2, term3, gln","23","    cdef np.ndarray[double] gln_a, gln_b, gln_Na, gln_Nb, gln_nij, log_Nnij","24","    cdef np.ndarray[double] nijs, term1","25","    cdef np.ndarray[double, ndim=2] log_ab_outer","29","    N = float(n_samples)"]}]}},"331964ead08b14e998bf0eb748dd8b7c226de399":{"changes":{"doc\/themes\/scikit-learn\/layout.html":"MODIFY","doc\/themes\/scikit-learn\/static\/nature.css_t":"MODIFY"},"diff":{"doc\/themes\/scikit-learn\/layout.html":[{"add":["40","    function showMenu() {","41","      var topNav = document.getElementById(\"scikit-navbar\");","42","      if (topNav.className === \"navbar\") {","43","          topNav.className += \" responsive\";","44","      } else {","45","          topNav.className = \"navbar\";","46","      }","47","    };","71","        <div class=\"navbar\" id=\"scikit-navbar\">","97","            <a href=\"javascript:void(0);\" onclick=\"showMenu()\">","98","                <div class=\"nav-icon\">","99","                    <div class=\"hamburger-line\"><\/div>","100","                    <div class=\"hamburger-line\"><\/div>","101","                    <div class=\"hamburger-line\"><\/div>","102","                <\/div>","103","            <\/a>"],"delete":["63","        <div class=\"navbar\">","89",""]}],"doc\/themes\/scikit-learn\/static\/nature.css_t":[{"add":["44","       height: 60px;","97","div.navbar div.nav-icon a,","98","div.navbar div.nav-icon a:link,","99","div.navbar div.nav-icon a:visited,","100","div.navbar div.nav-icon a:hover {","101","    color: white;","102","    text-decoration: none;","103","}","104","div.navbar div.nav-icon {","105","    display: none;","106","}","107","\/* Nav bar collapses for mobile phones and shows the hamburger *\/","108","@media screen and (max-width: 680px) {","109","    div.navbar div.nav-icon {","110","        position: absolute;","111","        display: inline-block;","112","        right: 0;","113","        top: 12px;","114","        margin-right: 10px;","115","        background: #ff9c34;","116","        padding: 5px 10px;","117","        border-radius: 5px;","118","    }","119","    div.navbar ul li {","120","        display: none;","121","    }","122","    div.navbar ul {","123","        visibilty: hidden;","124","        background: #FFFFFF;","125","    }","126","    div.navbar.responsive > ul li.btn-li {","127","        margin-left: 6px;","128","    }","129","    div.navbar.responsive > ul li.btn-li + li {","130","        margin-top: -5px;   ","131","    }","132","    div.navbar.responsive > ul {","133","        visiblity: visible;","134","        position: absolute;","135","        right: 0;","136","        top: 10px;","137","        margin-right: 10px;","138","        background: #ff9c34;","139","    }","140","    div.navbar.responsive > ul li {","141","        display: flex;","142","        justify-content: flex-start;","143","        visibility: visible;","144","        width: 130px;","145","    }","146","    div.navbar .dropdown-menu li {","147","        white-space: normal;","148","    }","149","    div.navbar div.nav-icon .hamburger-line {","150","        background: white;","151","        width: 20px;","152","        height: 2px;","153","        margin-bottom: 5px;","154","        -webkit-transition: .1s ease-in-out;","155","        -moz-transition: .1s ease-in-out;","156","        -o-transition: .1s ease-in-out;","157","        transition: .1s ease-in-out;","158","    }","159","    div.navbar div.nav-icon .hamburger-line:nth-child(1) {","160","        margin-top: 5px;","161","        -webkit-transform-origin: left center;","162","        -moz-transform-origin: left center;","163","        -o-transform-origin: left center;","164","        transform-origin: left center;","165","    }","166","    div.navbar div.nav-icon .hamburger-line:nth-child(2) {","167","        -webkit-transform-origin: left center;","168","        -moz-transform-origin: left center;","169","        -o-transform-origin: left center;","170","        transform-origin: left center;","171","    }","172","    div.navbar div.nav-icon .hamburger-line:nth-child(3) {","173","        -webkit-transform-origin: left center;","174","        -moz-transform-origin: left center;","175","        -o-transform-origin: left center;","176","        transform-origin: left center;","177","    }","178","    div.navbar.responsive div.nav-icon .hamburger-line:nth-child(1) {","179","        -webkit-transform: rotate(45deg);","180","        -moz-transform: rotate(45deg);","181","        -o-transform: rotate(45deg);","182","        transform: rotate(45deg);","183","    }","184","    div.navbar.responsive div.nav-icon .hamburger-line:nth-child(2) {","185","        width: 0;","186","        opacity: 0%;","187","    }","188","    div.navbar.responsive div.nav-icon .hamburger-line:nth-child(3) {","189","        -webkit-transform: rotate(-45deg);","190","        -moz-transform: rotate(-45deg);","191","        -o-transform: rotate(-45deg);","192","        transform: rotate(-45deg);","193","    }","194","}","195",""],"delete":[]}]}},"fa598738570e03c50df5836a4e0a4285b91c957c":{"changes":{"sklearn\/utils\/stats.py":"MODIFY","sklearn\/mixture\/dpgmm.py":"MODIFY","sklearn\/utils\/extmath.py":"MODIFY","sklearn\/datasets\/samples_generator.py":"MODIFY","sklearn\/ensemble\/weight_boosting.py":"MODIFY","sklearn\/decomposition\/pca.py":"MODIFY","sklearn\/manifold\/locally_linear.py":"MODIFY","sklearn\/utils\/tests\/test_extmath.py":"MODIFY","sklearn\/cluster\/k_means_.py":"MODIFY"},"diff":{"sklearn\/utils\/stats.py":[{"add":["3","from ..utils.extmath import stable_cumsum","56","    weight_cdf = stable_cumsum(sample_weight[sorted_idx])"],"delete":["55","    weight_cdf = sample_weight[sorted_idx].cumsum()"]}],"sklearn\/mixture\/dpgmm.py":[{"add":["26","from ..utils.extmath import logsumexp, pinvh, squared_norm, stable_cumsum","464","        cz = stable_cumsum(z[:, ::-1], axis=-1)[:, -2::-1]"],"delete":["26","from ..utils.extmath import logsumexp, pinvh, squared_norm","464","        cz = np.cumsum(z[:, ::-1], axis=-1)[:, -2::-1]"]}],"sklearn\/utils\/extmath.py":[{"add":["27","from ..exceptions import ConvergenceWarning, NonBLASDotWarning","846","def stable_cumsum(arr, axis=None, rtol=1e-05, atol=1e-08):","853","    axis : int, optional","854","        Axis along which the cumulative sum is computed.","855","        The default (None) is to compute the cumsum over the flattened array.","861","    # sum is as unstable as cumsum for numpy < 1.9","862","    if np_version < (1, 9):","863","        return np.cumsum(arr, axis=axis, dtype=np.float64)","864","","865","    out = np.cumsum(arr, axis=axis, dtype=np.float64)","866","    expected = np.sum(arr, axis=axis, dtype=np.float64)","867","    if not np.all(np.isclose(out.take(-1, axis=axis), expected, rtol=rtol,","868","                             atol=atol, equal_nan=True)):","869","        warnings.warn('cumsum was found to be unstable: '","870","                      'its last element does not correspond to sum',","871","                      ConvergenceWarning)"],"delete":["27","from ..exceptions import NonBLASDotWarning","846","def stable_cumsum(arr, rtol=1e-05, atol=1e-08):","858","    out = np.cumsum(arr, dtype=np.float64)","859","    expected = np.sum(arr, dtype=np.float64)","860","    if not np.allclose(out[-1], expected, rtol=rtol, atol=atol):","861","        raise RuntimeError('cumsum was found to be unstable: '","862","                           'its last element does not correspond to sum')"]}],"sklearn\/datasets\/samples_generator.py":[{"add":["1196","        The probability that a coefficient is zero (see notes). Larger values"],"delete":["1196","        The probability that a coefficient is zero (see notes). Larger values "]}],"sklearn\/ensemble\/weight_boosting.py":[{"add":["40","from ..utils.extmath import stable_cumsum","1005","        cdf = stable_cumsum(sample_weight)","1062","        weight_cdf = stable_cumsum(self.estimator_weights_[sorted_idx], axis=1)"],"delete":["1004","        cdf = sample_weight.cumsum()","1061","        weight_cdf = self.estimator_weights_[sorted_idx].cumsum(axis=1)"]}],"sklearn\/decomposition\/pca.py":[{"add":["26","from ..utils.extmath import stable_cumsum","396","            ratio_cumsum = stable_cumsum(explained_variance_ratio_)"],"delete":["395","            ratio_cumsum = explained_variance_ratio_.cumsum()"]}],"sklearn\/manifold\/locally_linear.py":[{"add":["12","from ..utils.extmath import stable_cumsum","423","        evals_cumsum = stable_cumsum(evals, 1)"],"delete":["422","        evals_cumsum = np.cumsum(evals, 1)"]}],"sklearn\/utils\/tests\/test_extmath.py":[{"add":["20","from sklearn.utils.testing import assert_warns","39","from sklearn.exceptions import ConvergenceWarning","658","    assert_warns(ConvergenceWarning, stable_cumsum, r, rtol=0, atol=0)","659","","660","    # test axis parameter","661","    A = np.random.RandomState(36).randint(1000, size=(5, 5, 5))","662","    assert_array_equal(stable_cumsum(A, axis=0), np.cumsum(A, axis=0))","663","    assert_array_equal(stable_cumsum(A, axis=1), np.cumsum(A, axis=1))","664","    assert_array_equal(stable_cumsum(A, axis=2), np.cumsum(A, axis=2))"],"delete":["656","    assert_raise_message(RuntimeError,","657","                         'cumsum was found to be unstable: its last element '","658","                         'does not correspond to sum',","659","                         stable_cumsum, r, rtol=0, atol=0)"]}],"sklearn\/cluster\/k_means_.py":[{"add":["20","from ..utils.extmath import row_norms, squared_norm, stable_cumsum","108","        candidate_ids = np.searchsorted(stable_cumsum(closest_dist_sq),","109","                                        rand_vals)"],"delete":["20","from ..utils.extmath import row_norms, squared_norm","108","        candidate_ids = np.searchsorted(closest_dist_sq.cumsum(), rand_vals)"]}]}},"cbd3bca20f1d19461011b5f59d9416669eb30535":{"changes":{"sklearn\/neighbors\/dist_metrics.pyx":"MODIFY","sklearn\/neighbors\/tests\/test_dist_metrics.py":"MODIFY"},"diff":{"sklearn\/neighbors\/dist_metrics.pyx":[{"add":["1102","            d = self.func(x1arr, x2arr, **self.kwargs)","1103","            try:","1104","                # Cython generates code here that results in a TypeError","1105","                # if d is the wrong type.","1106","                return d","1107","            except TypeError:","1108","                raise TypeError(\"Custom distance function must accept two \"","1109","                                \"vectors and return a float.\")","1110","            "],"delete":["1093","        x = np.random.random(10)","1094","        try:","1095","            d = self.func(x, x, **kwargs)","1096","        except TypeError:","1097","            raise ValueError(\"func must be a callable taking two arrays\")","1098","","1099","        try:","1100","            d = float(d)","1101","        except TypeError:","1102","            raise ValueError(\"func must return a float\")","1103","","1113","            return self.func(x1arr, x2arr, **self.kwargs)"]}],"sklearn\/neighbors\/tests\/test_dist_metrics.py":[{"add":["9","from sklearn.neighbors import BallTree","10","from sklearn.utils.testing import SkipTest, assert_raises_regex","172","","173","","174","def test_bad_pyfunc_metric():","175","    def wrong_distance(x, y):","176","        return \"1\"","177","","178","    X = np.ones((5, 2))","179","    assert_raises_regex(TypeError,","180","                        \"Custom distance function must accept two vectors\",","181","                        BallTree, X, metric=wrong_distance)","182","","183","","184","def test_input_data_size():","185","    # Regression test for #6288","186","    # Previoulsly, a metric requiring a particular input dimension would fail","187","    def custom_metric(x, y):","188","        assert x.shape[0] == 3","189","        return np.sum((x - y) ** 2)","190","","191","    rng = np.random.RandomState(0)","192","    X = rng.rand(10, 3)","193","","194","    pyfunc = DistanceMetric.get_metric(\"pyfunc\", func=dist_func, p=2)","195","    eucl = DistanceMetric.get_metric(\"euclidean\")","196","    assert_array_almost_equal(pyfunc.pairwise(X), eucl.pairwise(X))"],"delete":["9","from sklearn.utils.testing import SkipTest"]}]}}}