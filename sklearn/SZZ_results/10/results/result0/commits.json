{"97e423256176d6c85bd7ff4641b0d0cde625f5bd":{"changes":{"sklearn\/tests\/test_multiclass.py":"MODIFY","sklearn\/multiclass.py":"MODIFY"},"diff":{"sklearn\/tests\/test_multiclass.py":[{"add":["440","    iris_data_list = [list(a) for a in iris.data]","441","    prediction_from_list = ovo.fit(iris_data_list,","442","                                   list(iris.target)).predict(iris_data_list)"],"delete":["440","    prediction_from_list = ovo.fit(iris.data,","441","                                   list(iris.target)).predict(iris.data)"]}],"sklearn\/multiclass.py":[{"add":["48","from .utils.validation import check_X_y","471","        X, y = check_X_y(X, y, accept_sparse=['csr', 'csc'])","514","        X, y = check_X_y(X, y, accept_sparse=['csr', 'csc'])"],"delete":["470","        y = np.asarray(y)","471","        check_consistent_length(X, y)","514","        y = np.asarray(y)","515","        check_consistent_length(X, y)"]}]}},"78dbcb2838bb863dcf2e930c29aeef4f4cde5949":{"changes":{"sklearn\/ensemble\/_gradient_boosting.pyx":"MODIFY","sklearn\/ensemble\/gradient_boosting.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/tree\/_tree.pyx":"MODIFY","sklearn\/tree\/_utils.pxd":"MODIFY","sklearn\/ensemble\/tests\/test_gradient_boosting.py":"MODIFY"},"diff":{"sklearn\/ensemble\/_gradient_boosting.pyx":[{"add":["10","from libc.stdlib cimport free","11","from libc.string cimport memset","12","","17","from scipy.sparse import issparse","18","from scipy.sparse import csr_matrix","19","","22","from sklearn.tree._tree cimport DTYPE_t","23","from sklearn.tree._tree cimport SIZE_t","24","from sklearn.tree._tree cimport INT32_t","25","from sklearn.tree._utils cimport safe_realloc","40","cdef SIZE_t TREE_LEAF = -1","42","cdef void _predict_regression_tree_inplace_fast_dense(DTYPE_t *X,","43","                                                      Node* root_node,","44","                                                      double *value,","45","                                                      double scale,","46","                                                      Py_ssize_t k,","47","                                                      Py_ssize_t K,","48","                                                      Py_ssize_t n_samples,","49","                                                      Py_ssize_t n_features,","50","                                                      float64 *out):","94","        while node.left_child != TREE_LEAF:","101","def _predict_regression_tree_stages_sparse(np.ndarray[object, ndim=2] estimators,","102","                                           object X, double scale,","103","                                           np.ndarray[float64, ndim=2] out):","104","    \"\"\"Predicts output for regression tree inplace and adds scaled value to ``out[i, k]``.","106","    The function assumes that the ndarray that wraps ``X`` is csr_matrix.","107","    \"\"\"","108","    cdef DTYPE_t* X_data = <DTYPE_t*>(<np.ndarray> X.data).data","109","    cdef INT32_t* X_indices = <INT32_t*>(<np.ndarray> X.indices).data","110","    cdef INT32_t* X_indptr = <INT32_t*>(<np.ndarray> X.indptr).data","111","","112","    cdef SIZE_t n_samples = X.shape[0]","113","    cdef SIZE_t n_features = X.shape[1]","114","    cdef SIZE_t n_stages = estimators.shape[0]","115","    cdef SIZE_t n_outputs = estimators.shape[1]","116","","117","    # Initialize output","118","    cdef float64* out_ptr = <float64*> out.data","119","","120","    # Indices and temporary variables","121","    cdef SIZE_t sample_i","122","    cdef SIZE_t feature_i","123","    cdef SIZE_t stage_i","124","    cdef SIZE_t output_i","125","    cdef Node *root_node = NULL","126","    cdef Node *node = NULL","127","    cdef double *value = NULL","128","","129","    cdef Tree tree","130","    cdef Node** nodes = NULL","131","    cdef double** values = NULL","132","    safe_realloc(&nodes, n_stages * n_outputs)","133","    safe_realloc(&values, n_stages * n_outputs)","134","    for stage_i in range(n_stages):","135","        for output_i in range(n_outputs):","136","            tree = estimators[stage_i, output_i].tree_","137","            nodes[stage_i * n_outputs + output_i] = tree.nodes","138","            values[stage_i * n_outputs + output_i] = tree.value","139","","140","    # Initialize auxiliary data-structure","141","    cdef DTYPE_t feature_value = 0.","142","    cdef DTYPE_t* X_sample = NULL","143","","144","    # feature_to_sample as a data structure records the last seen sample","145","    # for each feature; functionally, it is an efficient way to identify","146","    # which features are nonzero in the present sample.","147","    cdef SIZE_t* feature_to_sample = NULL","148","","149","    safe_realloc(&X_sample, n_features)","150","    safe_realloc(&feature_to_sample, n_features)","151","","152","    memset(feature_to_sample, -1, n_features * sizeof(SIZE_t))","153","","154","    # Cycle through all samples","155","    for sample_i in range(n_samples):","156","        for feature_i in range(X_indptr[sample_i], X_indptr[sample_i + 1]):","157","            feature_to_sample[X_indices[feature_i]] = sample_i","158","            X_sample[X_indices[feature_i]] = X_data[feature_i]","159","","160","        # Cycle through all stages","161","        for stage_i in range(n_stages):","162","            # Cycle through all trees","163","            for output_i in range(n_outputs):","164","                root_node = nodes[stage_i * n_outputs + output_i]","165","                value = values[stage_i * n_outputs + output_i]","166","                node = root_node","167","","168","                # While node not a leaf","169","                while node.left_child != TREE_LEAF:","170","                    # ... and node.right_child != TREE_LEAF:","171","                    if feature_to_sample[node.feature] == sample_i:","172","                        feature_value = X_sample[node.feature]","173","                    else:","174","                        feature_value = 0.","175","","176","                    if feature_value <= node.threshold:","177","                        node = root_node + node.left_child","178","                    else:","179","                        node = root_node + node.right_child","180","                out_ptr[sample_i * n_outputs + output_i] += (scale","181","                    * value[node - root_node])","182","","183","    # Free auxiliary arrays","184","    free(X_sample)","185","    free(feature_to_sample)","186","    free(nodes)","187","    free(values)","188","","189","","191","                   object X, double scale,","204","    if issparse(X):","205","        _predict_regression_tree_stages_sparse(estimators, X, scale, out)","206","    else:","207","        if not isinstance(X, np.ndarray):","208","            raise ValueError(\"X should be in np.ndarray or csr_matrix format,\"","209","                             \"got %s\" % type(X))","211","        for i in range(n_estimators):","212","            for k in range(K):","213","                tree = estimators[i, k].tree_","214","","215","                # avoid buffer validation by casting to ndarray","216","                # and get data pointer","217","                # need brackets because of casting operator priority","218","                _predict_regression_tree_inplace_fast_dense(","219","                    <DTYPE_t*> (<np.ndarray> X).data,","220","                    tree.nodes, tree.value,","221","                    scale, k, K, X.shape[0], X.shape[1],","222","                    <float64 *> (<np.ndarray> out).data)","223","                ## out += scale * tree.predict(X).reshape((X.shape[0], 1))","228","                  object X, double scale,","314","            if current_node.left_child == TREE_LEAF:"],"delete":["16","","29","# Define a datatype for the data array","30","DTYPE = np.float32","31","ctypedef np.float32_t DTYPE_t","32","ctypedef np.npy_intp SIZE_t","33","","36","cdef int LEAF = -1","38","cdef void _predict_regression_tree_inplace_fast(DTYPE_t *X,","39","                                                Node* root_node,","40","                                                double *value,","41","                                                double scale,","42","                                                Py_ssize_t k,","43","                                                Py_ssize_t K,","44","                                                Py_ssize_t n_samples,","45","                                                Py_ssize_t n_features,","46","                                                float64 *out):","86","    cdef int32 node_id","91","        while node.left_child != -1 and node.right_child != -1:","99","@cython.nonecheck(False)","101","                   np.ndarray[DTYPE_t, ndim=2, mode='c'] X, double scale,","114","    for i in range(n_estimators):","115","        for k in range(K):","116","            tree = estimators[i, k].tree_","118","            # avoid buffer validation by casting to ndarray","119","            # and get data pointer","120","            # need brackets because of casting operator priority","121","            _predict_regression_tree_inplace_fast(","122","                <DTYPE_t*> X.data,","123","                tree.nodes, tree.value,","124","                scale, k, K, X.shape[0], X.shape[1],","125","                <float64 *> (<np.ndarray> out).data)","126","            ## out += scale * tree.predict(X).reshape((X.shape[0], 1))","129","@cython.nonecheck(False)","132","                  np.ndarray[DTYPE_t, ndim=2] X, double scale,","218","            if current_node.left_child == LEAF:"]}],"sklearn\/ensemble\/gradient_boosting.py":[{"add":["1160","        X : array-like or sparse matrix, shape = [n_samples, n_features]","1161","            The input samples. Internally, it will be converted to","1162","            ``dtype=np.float32`` and if a sparse matrix is provided","1163","            to a sparse ``csr_matrix``.","1173","        X = check_array(X, dtype=DTYPE, order=\"C\",  accept_sparse='csr')","1483","        X : array-like or sparse matrix, shape = [n_samples, n_features]","1484","            The input samples. Internally, it will be converted to","1485","            ``dtype=np.float32`` and if a sparse matrix is provided","1486","            to a sparse ``csr_matrix``.","1496","        X = check_array(X, dtype=DTYPE, order=\"C\",  accept_sparse='csr')","1510","        X : array-like or sparse matrix, shape = [n_samples, n_features]","1511","            The input samples. Internally, it will be converted to","1512","            ``dtype=np.float32`` and if a sparse matrix is provided","1513","            to a sparse ``csr_matrix``.","1532","        X : array-like or sparse matrix, shape = [n_samples, n_features]","1533","            The input samples. Internally, it will be converted to","1534","            ``dtype=np.float32`` and if a sparse matrix is provided","1535","            to a sparse ``csr_matrix``.","1554","        X : array-like or sparse matrix, shape = [n_samples, n_features]","1555","            The input samples. Internally, it will be converted to","1556","            ``dtype=np.float32`` and if a sparse matrix is provided","1557","            to a sparse ``csr_matrix``.","1573","        X : array-like or sparse matrix, shape = [n_samples, n_features]","1574","            The input samples. Internally, it will be converted to","1575","            ``dtype=np.float32`` and if a sparse matrix is provided","1576","            to a sparse ``csr_matrix``.","1603","        X : array-like or sparse matrix, shape = [n_samples, n_features]","1604","            The input samples. Internally, it will be converted to","1605","            ``dtype=np.float32`` and if a sparse matrix is provided","1606","            to a sparse ``csr_matrix``.","1630","        X : array-like or sparse matrix, shape = [n_samples, n_features]","1631","            The input samples. Internally, it will be converted to","1632","            ``dtype=np.float32`` and if a sparse matrix is provided","1633","            to a sparse ``csr_matrix``.","1858","        X : array-like or sparse matrix, shape = [n_samples, n_features]","1859","            The input samples. Internally, it will be converted to","1860","            ``dtype=np.float32`` and if a sparse matrix is provided","1861","            to a sparse ``csr_matrix``.","1868","        X = check_array(X, dtype=DTYPE, order=\"C\",  accept_sparse='csr')","1879","        X : array-like or sparse matrix, shape = [n_samples, n_features]","1880","            The input samples. Internally, it will be converted to","1881","            ``dtype=np.float32`` and if a sparse matrix is provided","1882","            to a sparse ``csr_matrix``."],"delete":["1160","        X : array-like of shape = [n_samples, n_features]","1161","            The input samples.","1171","        X = check_array(X, dtype=DTYPE, order=\"C\")","1481","        X : array-like of shape = [n_samples, n_features]","1482","            The input samples.","1492","        X = check_array(X, dtype=DTYPE, order=\"C\")","1506","        X : array-like of shape = [n_samples, n_features]","1507","            The input samples.","1526","        X : array-like of shape = [n_samples, n_features]","1527","            The input samples.","1546","        X : array-like of shape = [n_samples, n_features]","1547","            The input samples.","1563","        X : array-like of shape = [n_samples, n_features]","1564","            The input samples.","1591","        X : array-like of shape = [n_samples, n_features]","1592","            The input samples.","1616","        X : array-like of shape = [n_samples, n_features]","1617","            The input samples.","1842","        X : array-like of shape = [n_samples, n_features]","1843","            The input samples.","1850","        X = check_array(X, dtype=DTYPE, order=\"C\")","1861","        X : array-like of shape = [n_samples, n_features]","1862","            The input samples."]}],"doc\/whats_new.rst":[{"add":["38","   - :class:`ensemble.GradientBoostingClassifier` and :class:`ensemble.GradientBoostingRegressor`","39","     now support sparse input for prediction.","40","     (`#6101 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/6101>`_)","41","     By `Ibraim Ganiev`_.","42",""],"delete":[]}],"sklearn\/tree\/_tree.pyx":[{"add":["841","        safe_realloc(&X_sample, n_features)","842","        safe_realloc(&feature_to_sample, n_features)","987","        safe_realloc(&X_sample, n_features)","988","        safe_realloc(&feature_to_sample, n_features)"],"delete":["841","        safe_realloc(&X_sample, n_features * sizeof(DTYPE_t))","842","        safe_realloc(&feature_to_sample, n_features * sizeof(SIZE_t))","987","        safe_realloc(&X_sample, n_features * sizeof(DTYPE_t))","988","        safe_realloc(&feature_to_sample, n_features * sizeof(SIZE_t))"]}],"sklearn\/tree\/_utils.pxd":[{"add":["12","from _tree cimport Node ","39","    (DOUBLE_t**)","40","    (Node*)","41","    (Node**)"],"delete":[]}],"sklearn\/ensemble\/tests\/test_gradient_boosting.py":[{"add":["1052","    assert_array_almost_equal(sparse.predict(X_sparse), dense.predict(X))","1053","    assert_array_almost_equal(dense.predict(X_sparse), sparse.predict(X))","1054","","1066","        assert_array_almost_equal(sparse.decision_function(X_sparse),","1067","                                  sparse.decision_function(X))","1068","        assert_array_almost_equal(dense.decision_function(X_sparse),","1069","                                  sparse.decision_function(X))","1070","","1071","        assert_array_almost_equal(","1072","            np.array(sparse.staged_decision_function(X_sparse)),","1073","            np.array(sparse.staged_decision_function(X)))"],"delete":[]}]}},"94246be3e3746b577a4d0f2641f887a5828e2f81":{"changes":{"sklearn\/neighbors\/tests\/test_neighbors.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/neighbors\/classification.py":"MODIFY"},"diff":{"sklearn\/neighbors\/tests\/test_neighbors.py":[{"add":["351","    X = np.array([[1.0, 1.0], [2.0, 2.0], [0.99, 0.99], ","352","                  [0.98, 0.98], [2.01, 2.01]])","353","    y  = np.array([1, 2, 1, 1, 2])","357","    z2 = np.array([[1.4, 1.4], [1.01, 1.01], [2.01, 2.01]])    # one outlier","359","    correct_labels2 = np.array([-1, 1, 2])","373","      "],"delete":["351","    X = np.array([[1.0, 1.0], [2.0, 2.0]])","352","    y = np.array([1, 2])","356","    z2 = np.array([[1.01, 1.01], [1.4, 1.4]])    # one outlier","358","    correct_labels2 = np.array([1, -1])","372",""]}],"doc\/whats_new.rst":[{"add":["281","      ","282","    - Fix bug in :class:`neighbors.RadiusNeighborsClassifier` where an error ","283","      occurred when there were outliers being labelled and a weight function ","284","      (`#6902 <https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/6902>`_). ","285","      By `LeonieBorne <https:\/\/github.com\/LeonieBorne>`_."],"delete":[]}],"sklearn\/neighbors\/classification.py":[{"add":["376","                                 in zip(pred_labels[inliers], weights[inliers])],"],"delete":["376","                                 in zip(pred_labels[inliers], weights)],"]}]}},"31a4691d7671648cb44fcdf0fc9410727200946a":{"changes":{"sklearn\/model_selection\/_split.py":"MODIFY","sklearn\/utils\/random.py":"MODIFY","sklearn\/tests\/test_cross_validation.py":"MODIFY","sklearn\/cross_validation.py":"MODIFY","sklearn\/model_selection\/tests\/test_split.py":"MODIFY","doc\/whats_new.rst":"MODIFY"},"diff":{"sklearn\/model_selection\/_split.py":[{"add":["32","from ..utils.random import choice","1101","def _approximate_mode(class_counts, n_draws, rng):","1102","    \"\"\"Computes approximate mode of multivariate hypergeometric.","1103","","1104","    This is an approximation to the mode of the multivariate","1105","    hypergeometric given by class_counts and n_draws.","1106","    It shouldn't be off by more than one.","1107","","1108","    It is the mostly likely outcome of drawing n_draws many","1109","    samples from the population given by class_counts.","1110","","1111","    Parameters","1112","    ----------","1113","    class_counts : ndarray of int","1114","        Population per class.","1115","    n_draws : int","1116","        Number of draws (samples to draw) from the overall population.","1117","    rng : random state","1118","        Used to break ties.","1119","","1120","    Returns","1121","    -------","1122","    sampled_classes : ndarray of int","1123","        Number of samples drawn from each class.","1124","        np.sum(sampled_classes) == n_draws","1125","","1126","    Examples","1127","    --------","1128","    >>> from sklearn.model_selection._split import _approximate_mode","1129","    >>> _approximate_mode(class_counts=np.array([4, 2]), n_draws=3, rng=0)","1130","    array([2, 1])","1131","    >>> _approximate_mode(class_counts=np.array([5, 2]), n_draws=4, rng=0)","1132","    array([3, 1])","1133","    >>> _approximate_mode(class_counts=np.array([2, 2, 2, 1]),","1134","    ...                   n_draws=2, rng=0)","1135","    array([0, 1, 1, 0])","1136","    >>> _approximate_mode(class_counts=np.array([2, 2, 2, 1]),","1137","    ...                   n_draws=2, rng=42)","1138","    array([1, 1, 0, 0])","1139","    \"\"\"","1140","    # this computes a bad approximation to the mode of the","1141","    # multivariate hypergeometric given by class_counts and n_draws","1142","    continuous = n_draws * class_counts \/ class_counts.sum()","1143","    # floored means we don't overshoot n_samples, but probably undershoot","1144","    floored = np.floor(continuous)","1145","    # we add samples according to how much \"left over\" probability","1146","    # they had, until we arrive at n_samples","1147","    need_to_add = int(n_draws - floored.sum())","1148","    if need_to_add > 0:","1149","        remainder = continuous - floored","1150","        values = np.sort(np.unique(remainder))[::-1]","1151","        # add according to remainder, but break ties","1152","        # randomly to avoid biases","1153","        for value in values:","1154","            inds, = np.where(remainder == value)","1155","            # if we need_to_add less than what's in inds","1156","            # we draw randomly from them.","1157","            # if we need to add more, we add them all and","1158","            # go to the next value","1159","            add_now = min(len(inds), need_to_add)","1160","            inds = choice(inds, size=add_now, replace=False, random_state=rng)","1161","            floored[inds] += 1","1162","            need_to_add -= add_now","1163","            if need_to_add == 0:","1164","                break","1165","    return floored.astype(np.int)","1166","","1167","","1253","            # if there are ties in the class-counts, we want","1254","            # to make sure to break them anew in each iteration","1255","            n_i = _approximate_mode(class_counts, n_train, rng)","1256","            class_counts_remaining = class_counts - n_i","1257","            t_i = _approximate_mode(class_counts_remaining, n_test, rng)","1258",""],"delete":["1183","        p_i = class_counts \/ float(n_samples)","1184","        n_i = np.round(n_train * p_i).astype(int)","1185","        t_i = np.minimum(class_counts - n_i,","1186","                         np.round(n_test * p_i).astype(int))","1198","","1199","            # Because of rounding issues (as n_train and n_test are not","1200","            # dividers of the number of elements per class), we may end","1201","            # up here with less samples in train and test than asked for.","1202","            if len(train) + len(test) < n_train + n_test:","1203","                # We complete by affecting randomly the missing indexes","1204","                missing_indices = np.where(bincount(train + test,","1205","                                                    minlength=len(y)) == 0)[0]","1206","                missing_indices = rng.permutation(missing_indices)","1207","                n_missing_train = n_train - len(train)","1208","                n_missing_test = n_test - len(test)","1209","","1210","                if n_missing_train > 0:","1211","                    train.extend(missing_indices[:n_missing_train])","1212","                if n_missing_test > 0:","1213","                    test.extend(missing_indices[-n_missing_test:])","1214",""]}],"sklearn\/utils\/random.py":[{"add":["125","    if p is not None:","144","        if p is not None:","158","        if p is not None:"],"delete":["125","    if None != p:","144","        if None != p:","158","        if None != p:"]}],"sklearn\/tests\/test_cross_validation.py":[{"add":["481","          np.array([0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2] * 2),","489","        test_size = np.ceil(0.33 * len(y))","490","        train_size = len(y) - test_size","494","            p_train = (np.bincount(np.unique(y[train],","495","                                   return_inverse=True)[1]) \/","496","                       float(len(y[train])))","497","            p_test = (np.bincount(np.unique(y[test],","498","                                  return_inverse=True)[1]) \/","499","                      float(len(y[test])))","501","            assert_equal(len(train) + len(test), y.size)","502","            assert_equal(len(train), train_size)","503","            assert_equal(len(test), test_size)","504","            assert_array_equal(np.lib.arraysetops.intersect1d(train, test), [])"],"delete":["481","          np.array([0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2]),","492","            p_train = (np.bincount(np.unique(y[train], return_inverse=True)[1])","493","                       \/ float(len(y[train])))","494","            p_test = (np.bincount(np.unique(y[test], return_inverse=True)[1])","495","                      \/ float(len(y[test])))","497","            assert_equal(y[train].size + y[test].size, y.size)","498","            assert_array_equal(np.intersect1d(train, test), [])"]}],"sklearn\/cross_validation.py":[{"add":["29","from .utils.random import choice","417","                (\"Cannot have number of folds n_folds={0} greater\"","418","                 \" than the number of labels: {1}.\").format(n_folds,","419","                                                            n_labels))","909","def _approximate_mode(class_counts, n_draws, rng):","910","    \"\"\"Computes approximate mode of multivariate hypergeometric.","911","","912","    This is an approximation to the mode of the multivariate","913","    hypergeometric given by class_counts and n_draws.","914","    It shouldn't be off by more than one.","915","","916","    It is the mostly likely outcome of drawing n_draws many","917","    samples from the population given by class_counts.","918","","919","    Parameters","920","    ----------","921","    class_counts : ndarray of int","922","        Population per class.","923","    n_draws : int","924","        Number of draws (samples to draw) from the overall population.","925","    rng : random state","926","        Used to break ties.","927","","928","    Returns","929","    -------","930","    sampled_classes : ndarray of int","931","        Number of samples drawn from each class.","932","        np.sum(sampled_classes) == n_draws","933","    \"\"\"","934","    # this computes a bad approximation to the mode of the","935","    # multivariate hypergeometric given by class_counts and n_draws","936","    continuous = n_draws * class_counts \/ class_counts.sum()","937","    # floored means we don't overshoot n_samples, but probably undershoot","938","    floored = np.floor(continuous)","939","    # we add samples according to how much \"left over\" probability","940","    # they had, until we arrive at n_samples","941","    need_to_add = int(n_draws - floored.sum())","942","    if need_to_add > 0:","943","        remainder = continuous - floored","944","        values = np.sort(np.unique(remainder))[::-1]","945","        # add according to remainder, but break ties","946","        # randomly to avoid biases","947","        for value in values:","948","            inds, = np.where(remainder == value)","949","            # if we need_to_add less than what's in inds","950","            # we draw randomly from them.","951","            # if we need to add more, we add them all and","952","            # go to the next value","953","            add_now = min(len(inds), need_to_add)","954","            inds = choice(inds, size=add_now, replace=False, random_state=rng)","955","            floored[inds] += 1","956","            need_to_add -= add_now","957","            if need_to_add == 0:","958","                    break","959","    return floored.astype(np.int)","960","","961","","1049","            # if there are ties in the class-counts, we want","1050","            # to make sure to break them anew in each iteration","1051","            n_i = _approximate_mode(cls_count, self.n_train, rng)","1052","            class_counts_remaining = cls_count - n_i","1053","            t_i = _approximate_mode(class_counts_remaining, self.n_test, rng)","1054","","1058","            for i, _ in enumerate(self.classes):","1060","                perm_indices_class_i = np.where(","1061","                    (i == self.y_indices))[0][permutation]","1063","                train.extend(perm_indices_class_i[:n_i[i]])","1064","                test.extend(perm_indices_class_i[n_i[i]:n_i[i] + t_i[i]])"],"delete":["416","                    (\"Cannot have number of folds n_folds={0} greater\"","417","                     \" than the number of labels: {1}.\").format(n_folds,","418","                                                                n_labels))","993","        p_i = cls_count \/ float(self.n)","994","        n_i = np.round(self.n_train * p_i).astype(int)","995","        t_i = np.minimum(cls_count - n_i,","996","                         np.round(self.n_test * p_i).astype(int))","1002","            for i, cls in enumerate(self.classes):","1004","                cls_i = np.where((self.y == cls))[0][permutation]","1006","                train.extend(cls_i[:n_i[i]])","1007","                test.extend(cls_i[n_i[i]:n_i[i] + t_i[i]])","1008","","1009","            # Because of rounding issues (as n_train and n_test are not","1010","            # dividers of the number of elements per class), we may end","1011","            # up here with less samples in train and test than asked for.","1012","            if len(train) + len(test) < self.n_train + self.n_test:","1013","                # We complete by affecting randomly the missing indexes","1014","                missing_idx = np.where(bincount(train + test,","1015","                                                minlength=len(self.y)) == 0,","1016","                                       )[0]","1017","                missing_idx = rng.permutation(missing_idx)","1018","                n_missing_train = self.n_train - len(train)","1019","                n_missing_test = self.n_test - len(test)","1020","","1021","                if n_missing_train > 0:","1022","                    train.extend(missing_idx[:n_missing_train])","1023","                if n_missing_test > 0:","1024","                    test.extend(missing_idx[-n_missing_test:])","1025",""]}],"sklearn\/model_selection\/tests\/test_split.py":[{"add":["537","def test_stratified_shuffle_split_respects_test_size():","538","    y = np.array([0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2])","539","    test_size = 5","540","    train_size = 10","541","    sss = StratifiedShuffleSplit(6, test_size=test_size, train_size=train_size,","542","                                 random_state=0).split(np.ones(len(y)), y)","543","    for train, test in sss:","544","        assert_equal(len(train), train_size)","545","        assert_equal(len(test), test_size)","546","","547","","551","          np.array([0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2] * 2),","553","          np.array([-1] * 800 + [1] * 50),","554","          np.concatenate([[i] * (100 + i) for i in range(11)])","560","        # this is how test-size is computed internally","561","        # in _validate_shuffle_split","562","        test_size = np.ceil(0.33 * len(y))","563","        train_size = len(y) - test_size","574","            assert_equal(len(train) + len(test), y.size)","575","            assert_equal(len(train), train_size)","576","            assert_equal(len(test), test_size)","592","            prob = bf.pmf(count)","593","            assert_true(prob > threshold,","613","        n_train, n_test = _validate_shuffle_split(","614","            n_samples, test_size=1. \/ n_folds, train_size=1. - (1. \/ n_folds))","675","        test_size = 1. \/ 3"],"delete":["540","          np.array([0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2]),","542","          np.array([-1] * 800 + [1] * 50)","558","            assert_equal(y[train].size + y[test].size, y.size)","574","            p = bf.pmf(count)","575","            assert_true(p > threshold,","595","        n_train, n_test = _validate_shuffle_split(n_samples,","596","                                                  test_size=1.\/n_folds,","597","                                                  train_size=1.-(1.\/n_folds))","658","        test_size = 1.\/3"]}],"doc\/whats_new.rst":[{"add":["399","    - Fix in :class:`sklearn.model_selection.StratifiedShuffleSplit` to","400","      return splits of size ``train_size`` and ``test_size`` in all cases","401","      (`#6472 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/6472>`).","402","      By `Andreas M¨¹ller`_.","403",""],"delete":[]}]}},"084ef97f16342e1c5ac630ac46476c08da3bea62":{"changes":{"sklearn\/linear_model\/tests\/test_coordinate_descent.py":"MODIFY","sklearn\/src\/cblas\/cblas_sasum.c":"ADD","sklearn\/linear_model\/coordinate_descent.py":"MODIFY","sklearn\/src\/cblas\/ATL_srefasum.c":"ADD","doc\/whats_new.rst":"MODIFY","sklearn\/src\/cblas\/ATL_dsrefdot.c":"ADD","sklearn\/linear_model\/cd_fast.pyx":"MODIFY","sklearn\/src\/cblas\/cblas_saxpy.c":"ADD"},"diff":{"sklearn\/linear_model\/tests\/test_coordinate_descent.py":[{"add":["672","","673","","674","def test_enet_float_precision():","675","    # Generate dataset","676","    X, y, X_test, y_test = build_dataset(n_samples=20, n_features=10)","677","    # Here we have a small number of iterations, and thus the","678","    # ElasticNet might not converge. This is to speed up tests","679","","680","    for normalize in [True, False]:","681","        for fit_intercept in [True, False]:","682","            coef = {}","683","            intercept = {}","684","            clf = ElasticNet(alpha=0.5, max_iter=100, precompute=False,","685","                            fit_intercept=fit_intercept, normalize=normalize)","686","            for dtype in [np.float64, np.float32]:","687","                X = dtype(X)","688","                y = dtype(y)","689","                ignore_warnings(clf.fit)(X, y)","690","","691","                coef[dtype] = clf.coef_","692","                intercept[dtype] = clf.intercept_","693","","694","                assert_equal(clf.coef_.dtype, dtype)","695","","696","            assert_array_almost_equal(coef[np.float32], coef[np.float64],","697","                                    decimal=4)","698","            assert_array_almost_equal(intercept[np.float32],","699","                                    intercept[np.float64],","700","                                    decimal=4)"],"delete":[]}],"sklearn\/src\/cblas\/cblas_sasum.c":[{"add":[],"delete":[]}],"sklearn\/linear_model\/coordinate_descent.py":[{"add":["374","    # We expect X and y to be already Fortran ordered when bypassing","377","        X = check_array(X, 'csc', dtype=[np.float64, np.float32],","378","                        order='F', copy=copy_X)","379","        y = check_array(y, 'csc', dtype=X.dtype.type, order='F', copy=False,","383","            Xy = check_array(Xy, dtype=X.dtype.type, order='C', copy=False,","385","","399","            X_sparse_scaling = np.asarray(X_sparse_scaling, dtype=X.dtype)","401","            X_sparse_scaling = np.zeros(n_features, dtype=X.dtype)","431","        coefs = np.empty((n_features, n_alphas), dtype=X.dtype)","434","                         dtype=X.dtype)","437","        coef_ = np.asfortranarray(np.zeros(coefs.shape[:-1], dtype=X.dtype))","439","        coef_ = np.asfortranarray(coef_init, dtype=X.dtype)","475","                          ' to increase the number of iterations.' +","476","                          ' Fitting data with very small alpha' +","477","                          ' may cause precision problems.',","670","        # We expect X and y to be float64 or float32 Fortran ordered arrays","673","            X, y = check_X_y(X, y, accept_sparse='csc',","674","                             order='F', dtype=[np.float64, np.float32],","677","            y = check_array(y, order='F', copy=False, dtype=X.dtype.type,","679","","695","            coef_ = np.zeros((n_targets, n_features), dtype=X.dtype,","702","        dual_gaps_ = np.zeros(n_targets, dtype=X.dtype)","732","        # workaround since _set_intercept will cast self.coef_ into float64","733","        self.coef_ = np.asarray(self.coef_, dtype=X.dtype)","734",""],"delete":["374","    # We expect X and y to be already float64 Fortran ordered when bypassing","377","        X = check_array(X, 'csc', dtype=np.float64, order='F', copy=copy_X)","378","        y = check_array(y, 'csc', dtype=np.float64, order='F', copy=False,","382","            Xy = check_array(Xy, dtype=np.float64, order='C', copy=False,","398","            X_sparse_scaling = np.zeros(n_features)","428","        coefs = np.empty((n_features, n_alphas), dtype=np.float64)","431","                         dtype=np.float64)","434","        coef_ = np.asfortranarray(np.zeros(coefs.shape[:-1]))","436","        coef_ = np.asfortranarray(coef_init)","472","                          ' to increase the number of iterations',","665","        # We expect X and y to be already float64 Fortran ordered arrays","668","            y = np.asarray(y, dtype=np.float64)","669","            X, y = check_X_y(X, y, accept_sparse='csc', dtype=np.float64,","670","                             order='F',","673","            y = check_array(y, dtype=np.float64, order='F', copy=False,","690","            coef_ = np.zeros((n_targets, n_features), dtype=np.float64,","697","        dual_gaps_ = np.zeros(n_targets, dtype=np.float64)"]}],"sklearn\/src\/cblas\/ATL_srefasum.c":[{"add":[],"delete":[]}],"doc\/whats_new.rst":[{"add":["265","   - :class:`linear_model.ElasticNet` and :class:`linear_model.Lasso`","266","     now works with ``np.float32`` input data without converting it","267","     into ``np.float64``. This allows to reduce the memory","268","     consumption.","269","     (`#6913 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/6913>`_)","270","     By `YenChen Lin`_."],"delete":[]}],"sklearn\/src\/cblas\/ATL_dsrefdot.c":[{"add":[],"delete":[]}],"sklearn\/linear_model\/cd_fast.pyx":[{"add":["15","from cython cimport floating","20","ctypedef floating (*DOT)(int N, floating *X, int incX, floating *Y,","21","                         int incY) nogil","22","ctypedef void (*AXPY)(int N, floating alpha, floating *X, int incX,","23","                      floating *Y, int incY) nogil","24","ctypedef floating (*ASUM)(int N, floating *X, int incX) nogil","50","cdef inline floating fmax(floating x, floating y) nogil:","56","cdef inline floating fsign(floating f) nogil:","65","cdef floating abs_max(int n, floating* a) nogil:","68","    cdef floating m = fabs(a[0])","69","    cdef floating d","77","cdef floating max(int n, floating* a) nogil:","80","    cdef floating m = a[0]","81","    cdef floating d","89","cdef floating diff_abs_max(int n, floating* a, floating* b) nogil:","92","    cdef floating m = fabs(a[0] - b[0])","93","    cdef floating d","113","    void saxpy \"cblas_saxpy\"(int N, float alpha, float *X, int incX,","114","                             float *Y, int incY) nogil","117","    float sdot \"cblas_sdot\"(int N, float *X, int incX, float *Y,","118","                            int incY) nogil","120","    float sasum \"cblas_sasum\"(int N, float *X, int incX) nogil","122","                           double *X, int incX, double *Y, int incY,","123","                           double *A, int lda) nogil","124","    void dgemv \"cblas_dgemv\"(CBLAS_ORDER Order, CBLAS_TRANSPOSE TransA,","125","                             int M, int N, double alpha, double *A, int lda,","126","                             double *X, int incX, double beta,","127","                             double *Y, int incY) nogil","129","    void dcopy \"cblas_dcopy\"(int N, double *X, int incX, double *Y,","130","                             int incY) nogil","137","def enet_coordinate_descent(np.ndarray[floating, ndim=1] w,","138","                            floating alpha, floating beta,","139","                            np.ndarray[floating, ndim=2, mode='fortran'] X,","140","                            np.ndarray[floating, ndim=1, mode='c'] y,","141","                            int max_iter, floating tol,","152","    # fused types version of BLAS functions","153","    cdef DOT dot","154","    cdef AXPY axpy","155","    cdef ASUM asum","156","","157","    if floating is float:","158","        dtype = np.float32","159","        dot = sdot","160","        axpy = saxpy","161","        asum = sasum","162","    else:","163","        dtype = np.float64","164","        dot = ddot","165","        axpy = daxpy","166","        asum = dasum","167","","173","    cdef unsigned int n_tasks = y.strides[0] \/ sizeof(floating)","176","    cdef np.ndarray[floating, ndim=1] norm_cols_X = (X**2).sum(axis=0)","179","    cdef np.ndarray[floating, ndim=1] R = np.empty(n_samples, dtype=dtype)","180","    cdef np.ndarray[floating, ndim=1] XtA = np.empty(n_features, dtype=dtype)","182","    cdef floating tmp","183","    cdef floating w_ii","184","    cdef floating d_w_max","185","    cdef floating w_max","186","    cdef floating d_w_ii","187","    cdef floating gap = tol + 1.0","188","    cdef floating d_w_tol = tol","189","    cdef floating dual_norm_XtA","190","    cdef floating R_norm2","191","    cdef floating w_norm2","192","    cdef floating l1_norm","193","    cdef floating const","194","    cdef floating A_norm2","202","    cdef floating *X_data = <floating*> X.data","203","    cdef floating *y_data = <floating*> y.data","204","    cdef floating *w_data = <floating*> w.data","205","    cdef floating *R_data = <floating*> R.data","206","    cdef floating *XtA_data = <floating*> XtA.data","207","","215","            R[i] = y[i] - dot(n_features, &X_data[i], n_samples, w_data, 1)","218","        tol *= dot(n_samples, y_data, n_tasks, y_data, n_tasks)","236","                    axpy(n_samples, w_ii, &X_data[ii * n_samples], 1,","237","                         R_data, 1)","240","                tmp = dot(n_samples, &X_data[ii * n_samples], 1, R_data, 1)","250","                    axpy(n_samples, -w[ii], &X_data[ii * n_samples], 1,","251","                         R_data, 1)","261","            if (w_max == 0.0 or","262","                d_w_max \/ w_max < d_w_tol or","263","                n_iter == max_iter - 1):","270","                    XtA[i] = dot(n_samples, &X_data[i * n_samples],","271","                                 1, R_data, 1) - beta * w[i]","274","                    dual_norm_XtA = max(n_features, XtA_data)","276","                    dual_norm_XtA = abs_max(n_features, XtA_data)","279","                R_norm2 = dot(n_samples, R_data, 1, R_data, 1)","282","                w_norm2 = dot(n_features, w_data, 1, w_data, 1)","292","                l1_norm = asum(n_features, w_data, 1)","295","                gap += (alpha * l1_norm","296","                        - const * dot(n_samples, R_data, 1, y_data, n_tasks)","308","def sparse_enet_coordinate_descent(floating [:] w,","309","                            floating alpha, floating beta,","310","                            np.ndarray[floating, ndim=1, mode='c'] X_data,","313","                            np.ndarray[floating, ndim=1] y,","314","                            floating[:] X_mean, int max_iter,","315","                            floating tol, object rng, bint random=0,","331","    cdef floating[:] norm_cols_X","337","    cdef unsigned int n_tasks","340","    cdef floating[:] R = y.copy()","342","    cdef floating[:] X_T_R","343","    cdef floating[:] XtA","345","    # fused types version of BLAS functions","346","    cdef DOT dot","347","    cdef ASUM asum","348","","349","    if floating is float:","350","        dtype = np.float32","351","        n_tasks = y.strides[0] \/ sizeof(float)","352","        dot = sdot","353","        asum = sasum","354","    else:","355","        dtype = np.float64","356","        n_tasks = y.strides[0] \/ sizeof(DOUBLE)","357","        dot = ddot","358","        asum = dasum","359","","360","    norm_cols_X = np.zeros(n_features, dtype=dtype)","361","    X_T_R = np.zeros(n_features, dtype=dtype)","362","    XtA = np.zeros(n_features, dtype=dtype)","363","","364","    cdef floating tmp","365","    cdef floating w_ii","366","    cdef floating d_w_max","367","    cdef floating w_max","368","    cdef floating d_w_ii","369","    cdef floating X_mean_ii","370","    cdef floating R_sum = 0.0","371","    cdef floating R_norm2","372","    cdef floating w_norm2","373","    cdef floating A_norm2","374","    cdef floating l1_norm","375","    cdef floating normalize_sum","376","    cdef floating gap = tol + 1.0","377","    cdef floating d_w_tol = tol","378","    cdef floating dual_norm_XtA","411","        tol *= dot(n_samples, &y[0], 1, &y[0], 1)","499","                R_norm2 = dot(n_samples, &R[0], 1, &R[0], 1)","502","                w_norm2 = dot(n_features, &w[0], 1, &w[0], 1)","511","                l1_norm = asum(n_features, &w[0], 1)","513","                gap += (alpha * l1_norm - const * dot(","515","                            &R[0], 1,","516","                            &y[0], n_tasks"],"delete":["44","cdef inline double fmax(double x, double y) nogil:","50","cdef inline double fsign(double f) nogil:","59","cdef double abs_max(int n, double* a) nogil:","62","    cdef double m = fabs(a[0])","63","    cdef double d","71","cdef double max(int n, double* a) nogil:","74","    cdef double m = a[0]","75","    cdef double d","83","cdef double diff_abs_max(int n, double* a, double* b) nogil:","86","    cdef double m = fabs(a[0] - b[0])","87","    cdef double d","111","                double *X, int incX, double *Y, int incY, double *A, int lda) nogil","112","    void dgemv \"cblas_dgemv\"(CBLAS_ORDER Order,","113","                      CBLAS_TRANSPOSE TransA, int M, int N,","114","                      double alpha, double *A, int lda,","115","                      double *X, int incX, double beta,","116","                      double *Y, int incY) nogil","118","    void dcopy \"cblas_dcopy\"(int N, double *X, int incX, double *Y, int incY) nogil","125","def enet_coordinate_descent(np.ndarray[DOUBLE, ndim=1] w,","126","                            double alpha, double beta,","127","                            np.ndarray[DOUBLE, ndim=2, mode='fortran'] X,","128","                            np.ndarray[DOUBLE, ndim=1, mode='c'] y,","129","                            int max_iter, double tol,","145","    cdef unsigned int n_tasks = y.strides[0] \/ sizeof(DOUBLE)","148","    cdef np.ndarray[DOUBLE, ndim=1] norm_cols_X = (X**2).sum(axis=0)","151","    cdef np.ndarray[DOUBLE, ndim=1] R = np.empty(n_samples)","153","    cdef np.ndarray[DOUBLE, ndim=1] XtA = np.empty(n_features)","154","    cdef double tmp","155","    cdef double w_ii","156","    cdef double d_w_max","157","    cdef double w_max","158","    cdef double d_w_ii","159","    cdef double gap = tol + 1.0","160","    cdef double d_w_tol = tol","161","    cdef double dual_norm_XtA","162","    cdef double R_norm2","163","    cdef double w_norm2","164","    cdef double l1_norm","179","            R[i] = y[i] - ddot(n_features,","180","                               <DOUBLE*>(X.data + i * sizeof(DOUBLE)),","181","                               n_samples, <DOUBLE*>w.data, 1)","184","        tol *= ddot(n_samples, <DOUBLE*>y.data, n_tasks,","185","                    <DOUBLE*>y.data, n_tasks)","203","                    daxpy(n_samples, w_ii,","204","                          <DOUBLE*>(X.data + ii * n_samples * sizeof(DOUBLE)),","205","                          1, <DOUBLE*>R.data, 1)","208","                tmp = ddot(n_samples,","209","                           <DOUBLE*>(X.data + ii * n_samples * sizeof(DOUBLE)),","210","                           1, <DOUBLE*>R.data, 1)","220","                    daxpy(n_samples, -w[ii],","221","                          <DOUBLE*>(X.data + ii * n_samples * sizeof(DOUBLE)),","222","                          1, <DOUBLE*>R.data, 1)","232","            if (w_max == 0.0","233","                    or d_w_max \/ w_max < d_w_tol","234","                    or n_iter == max_iter - 1):","241","                    XtA[i] = ddot(","242","                        n_samples,","243","                        <DOUBLE*>(X.data + i * n_samples *sizeof(DOUBLE)),","244","                        1, <DOUBLE*>R.data, 1) - beta * w[i]","247","                    dual_norm_XtA = max(n_features, <DOUBLE*>XtA.data)","249","                    dual_norm_XtA = abs_max(n_features, <DOUBLE*>XtA.data)","252","                R_norm2 = ddot(n_samples, <DOUBLE*>R.data, 1,","253","                               <DOUBLE*>R.data, 1)","256","                w_norm2 = ddot(n_features, <DOUBLE*>w.data, 1,","257","                               <DOUBLE*>w.data, 1)","267","                l1_norm = dasum(n_features, <DOUBLE*>w.data, 1)","270","                gap += (alpha * l1_norm - const * ddot(","271","                            n_samples,","272","                            <DOUBLE*>R.data, 1,","273","                            <DOUBLE*>y.data, n_tasks)","279","","286","def sparse_enet_coordinate_descent(double[:] w,","287","                            double alpha, double beta,","288","                            np.ndarray[double, ndim=1, mode='c'] X_data,","291","                            np.ndarray[double, ndim=1] y,","292","                            double[:] X_mean, int max_iter,","293","                            double tol, object rng, bint random=0,","309","    cdef double[:] norm_cols_X = np.zeros(n_features, np.float64)","315","    cdef unsigned int n_tasks = y.strides[0] \/ sizeof(DOUBLE)","318","    cdef double[:] R = y.copy()","320","    cdef double[:] X_T_R = np.zeros(n_features)","321","    cdef double[:] XtA = np.zeros(n_features)","323","    cdef double tmp","324","    cdef double w_ii","325","    cdef double d_w_max","326","    cdef double w_max","327","    cdef double d_w_ii","328","    cdef double X_mean_ii","329","    cdef double R_sum = 0.0","330","    cdef double normalize_sum","331","    cdef double gap = tol + 1.0","332","    cdef double d_w_tol = tol","365","        tol *= ddot(n_samples, <DOUBLE*>&y[0], 1, <DOUBLE*>&y[0], 1)","453","                R_norm2 = ddot(n_samples, <DOUBLE*>&R[0], 1, <DOUBLE*>&R[0], 1)","456","                w_norm2 = ddot(n_features, <DOUBLE*>&w[0], 1, <DOUBLE*>&w[0], 1)","465","                l1_norm = dasum(n_features, <DOUBLE*>&w[0], 1)","467","                # The expression inside ddot is equivalent to np.dot(R.T, y)","468","                gap += (alpha * l1_norm - const * ddot(","470","                            <DOUBLE*>&R[0], 1,","471","                            <DOUBLE*>&y[0], n_tasks"]}],"sklearn\/src\/cblas\/cblas_saxpy.c":[{"add":[],"delete":[]}]}},"e17d5c9337a2c9557be1bda085f36d2769c0fdf7":{"changes":{"sklearn\/gaussian_process\/tests\/test_gaussian_process.py":"MODIFY","sklearn\/gaussian_process\/gaussian_process.py":"MODIFY"},"diff":{"sklearn\/gaussian_process\/tests\/test_gaussian_process.py":[{"add":["135","def test_batch_size():","136","    # TypeError when using batch_size on Python 3, see","137","    # https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/7329 for more","138","    # details","139","    gp = GaussianProcess()","140","    gp.fit(X, y)","141","    gp.predict(X, batch_size=1)","142","    gp.predict(X, batch_size=1, eval_MSE=True)","143","","144",""],"delete":[]}],"sklearn\/gaussian_process\/gaussian_process.py":[{"add":["517","                for k in range(max(1, int(n_eval \/ batch_size))):","529","                for k in range(max(1, int(n_eval \/ batch_size))):"],"delete":["517","                for k in range(max(1, n_eval \/ batch_size)):","529","                for k in range(max(1, n_eval \/ batch_size)):"]}]}},"19d6d925edd33288acee343c5eb6afdc80b84dbc":{"changes":{"doc\/whats_new.rst":"MODIFY","sklearn\/preprocessing\/tests\/test_data.py":"MODIFY","sklearn\/preprocessing\/data.py":"MODIFY"},"diff":{"doc\/whats_new.rst":[{"add":["244","     :func:`load_iris` dataset","245","     `#7049 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/7049>`_,","326","","345","    - :func:`_transform_selected` now always passes a copy of `X` to transform","346","      function when `copy=True` (`#7194","347","      <https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/7194>`_). By `Caio","348","      Oliveira <https:\/\/github.com\/caioaao>`_.","349",""],"delete":["244","     :func:`load_iris` dataset ","245","     `#7049 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/7049>`_, ","326","      "]}],"sklearn\/preprocessing\/tests\/test_data.py":[{"add":["1563","def test_transform_selected_copy_arg():","1564","    # transformer that alters X","1565","    def _mutating_transformer(X):","1566","        X[0, 0] = X[0, 0] + 1","1567","        return X","1568","","1569","    original_X = np.asarray([[1, 2], [3, 4]])","1570","    expected_Xtr = [[2, 2], [3, 4]]","1571","","1572","    X = original_X.copy()","1573","    Xtr = _transform_selected(X, _mutating_transformer, copy=True,","1574","                              selected='all')","1575","","1576","    assert_array_equal(toarray(X), toarray(original_X))","1577","    assert_array_equal(toarray(Xtr), expected_Xtr)","1578","","1579",""],"delete":[]}],"sklearn\/preprocessing\/data.py":[{"add":["1696","    X = check_array(X, accept_sparse='csc', copy=copy, dtype=FLOAT_DTYPES)","1697",""],"delete":["1699","    X = check_array(X, accept_sparse='csc', copy=copy, dtype=FLOAT_DTYPES)","1700",""]}]}},"3cc7fead338bded628184ecef25516745a2067e3":{"changes":{"sklearn\/svm\/classes.py":"MODIFY","sklearn\/svm\/tests\/test_svm.py":"MODIFY"},"diff":{"sklearn\/svm\/classes.py":[{"add":["8","from ..utils import check_X_y, column_or_1d","331","    def fit(self, X, y, sample_weight=None):","376","            epsilon=self.epsilon, sample_weight=sample_weight)","768","    sample_weight : array-like, shape = [n_samples]","769","            Individual weights for each sample","770",""],"delete":["8","from ..utils import check_X_y","331","    def fit(self, X, y):","376","            epsilon=self.epsilon)"]}],"sklearn\/svm\/tests\/test_svm.py":[{"add":["10","from numpy.testing import assert_allclose","198","    assert_allclose(np.linalg.norm(lsvr.coef_),","199","                    np.linalg.norm(svr.coef_), 1, 0.0001)","200","    assert_almost_equal(score1, score2, 2)","201","","202","","203","def test_linearsvr_fit_sampleweight():","204","    # check correct result when sample_weight is 1","205","    # check that SVR(kernel='linear') and LinearSVC() give","206","    # comparable results","207","    diabetes = datasets.load_diabetes()","208","    n_samples = len(diabetes.target)","209","    unit_weight = np.ones(n_samples)","210","    lsvr = svm.LinearSVR(C=1e3).fit(diabetes.data, diabetes.target,","211","                                    sample_weight=unit_weight)","212","    score1 = lsvr.score(diabetes.data, diabetes.target)","213","","214","    lsvr_no_weight = svm.LinearSVR(C=1e3).fit(diabetes.data, diabetes.target)","215","    score2 = lsvr_no_weight.score(diabetes.data, diabetes.target)","216","","217","    assert_allclose(np.linalg.norm(lsvr.coef_),","218","                    np.linalg.norm(lsvr_no_weight.coef_), 1, 0.0001)","219","    assert_almost_equal(score1, score2, 2)","220","","221","    # check that fit(X)  = fit([X1, X2, X3],sample_weight = [n1, n2, n3]) where","222","    # X = X1 repeated n1 times, X2 repeated n2 times and so forth","223","    random_state = check_random_state(0)","224","    random_weight = random_state.randint(0, 10, n_samples)","225","    lsvr_unflat = svm.LinearSVR(C=1e3).fit(diabetes.data, diabetes.target,","226","                                           sample_weight=random_weight)","227","    score3 = lsvr_unflat.score(diabetes.data, diabetes.target,","228","                               sample_weight=random_weight)","229","","230","    X_flat = np.repeat(diabetes.data, random_weight, axis=0)","231","    y_flat = np.repeat(diabetes.target, random_weight, axis=0)","232","    lsvr_flat = svm.LinearSVR(C=1e3).fit(X_flat, y_flat)","233","    score4 = lsvr_flat.score(X_flat, y_flat)","234","","235","    assert_almost_equal(score3, score4, 2)","319","                            == clf.predict(iris.data)) > 0.9)","544","                    (loss, penalty, dual) == ('hinge', 'l2', False) or","545","                    (penalty, dual) == ('l1', True) or","546","                    loss == 'foo' or penalty == 'bar'):","604","                                      \" and loss='squared_hinge' is not supported\"),","807","    X = \"foo!\"  # input validation not required when SVM not fitted"],"delete":["8","","13","","27","","200","    assert np.linalg.norm(lsvr.coef_ - svr.coef_) \/ np.linalg.norm(svr.coef_) < .1","201","    assert np.abs(score1 - score2) < 0.1","279","","286","                    == clf.predict(iris.data)) > 0.9)","511","                (loss, penalty, dual) == ('hinge', 'l2', False) or","512","                (penalty, dual) == ('l1', True) or","513","                loss == 'foo' or penalty == 'bar'):","571","                         \" and loss='squared_hinge' is not supported\"),","636","","775","    X = \"foo!\"      # input validation not required when SVM not fitted"]}]}},"b444cc9c6457590b33b365185b3eafb0d312b9be":{"changes":{"sklearn\/model_selection\/_validation.py":"MODIFY","sklearn\/model_selection\/tests\/test_search.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/model_selection\/_search.py":"MODIFY"},"diff":{"sklearn\/model_selection\/_validation.py":[{"add":["0","","145","                   return_parameters=False, return_n_test_samples=False,","146","                   return_times=False, error_score='raise'):","203","    fit_time : float","204","        Time spent for fitting in seconds.","205","","206","    score_time : float","207","        Time spent for scoring in seconds.","240","        # Note fit time as time until error","241","        fit_time = time.time() - start_time","242","        score_time = 0.0","258","        fit_time = time.time() - start_time","260","        score_time = time.time() - start_time - fit_time","267","        end_msg = \"%s -%s\" % (msg, logger.short_format_time(score_time))","270","    ret = [train_score, test_score] if return_train_score else [test_score]","271","","272","    if return_n_test_samples:","273","        ret.append(_num_samples(X_test))","274","    if return_times:","275","        ret.extend([fit_time, score_time])","772","        out = np.array(out)","955","    out = np.asarray(out)"],"delete":["144","                   return_parameters=False, error_score='raise'):","201","    scoring_time : float","202","        Time spent for fitting and scoring in seconds.","254","    scoring_time = time.time() - start_time","255","","259","        end_msg = \"%s -%s\" % (msg, logger.short_format_time(scoring_time))","262","    ret = [train_score] if return_train_score else []","263","    ret.extend([test_score, _num_samples(X_test), scoring_time])","760","        out = np.array(out)[:, :2]","943","    out = np.asarray(out)[:, :2]"]}],"sklearn\/model_selection\/tests\/test_search.py":[{"add":["597","def check_cv_results_array_types(cv_results, param_keys, score_keys):","598","    # Check if the search `cv_results`'s array are of correct types","599","    assert_true(all(isinstance(cv_results[param], np.ma.MaskedArray)","601","    assert_true(all(cv_results[key].dtype == object for key in param_keys))","602","    assert_false(any(isinstance(cv_results[key], np.ma.MaskedArray)","604","    assert_true(all(cv_results[key].dtype == np.float64","605","                    for key in score_keys if not key.startswith('rank')))","606","    assert_true(cv_results['rank_test_score'].dtype == np.int32)","609","def check_cv_results_keys(cv_results, param_keys, score_keys, n_cand):","611","    assert_array_equal(sorted(cv_results.keys()),","613","    assert_true(all(cv_results[key].shape == (n_cand,)","619","    cv_results = search.cv_results_","620","    res_scores = np.vstack(list([cv_results[\"split%d_test_score\" % i]","622","    res_means = cv_results[\"mean_test_score\"]","623","    res_params = cv_results[\"params\"]","636","def test_grid_search_cv_results():","652","    score_keys = ('mean_test_score', 'mean_train_score',","653","                  'rank_test_score',","655","                  'split2_test_score',","656","                  'split0_train_score', 'split1_train_score',","657","                  'split2_train_score',","658","                  'std_test_score', 'std_train_score',","659","                  'mean_fit_time', 'std_fit_time',","660","                  'mean_score_time', 'std_score_time')","665","        cv_results = search.cv_results_","666","        # Check if score and timing are reasonable","667","        assert_true(all(cv_results['rank_test_score'] >= 1))","668","        assert_true(all(cv_results[k] >= 0) for k in score_keys","669","                    if k is not 'rank_test_score')","670","        assert_true(all(cv_results[k] <= 1) for k in score_keys","671","                    if 'time' not in k and","672","                    k is not 'rank_test_score')","673","        # Check cv_results structure","674","        check_cv_results_array_types(cv_results, param_keys, score_keys)","675","        check_cv_results_keys(cv_results, param_keys, score_keys, n_candidates)","677","        cv_results = grid_search.cv_results_","679","        assert_true(all((cv_results['param_C'].mask[i] and","680","                         cv_results['param_gamma'].mask[i] and","681","                         not cv_results['param_degree'].mask[i])","683","                        if cv_results['param_kernel'][i] == 'linear'))","684","        assert_true(all((not cv_results['param_C'].mask[i] and","685","                         not cv_results['param_gamma'].mask[i] and","686","                         cv_results['param_degree'].mask[i])","688","                        if cv_results['param_kernel'][i] == 'rbf'))","692","def test_random_search_cv_results():","705","                                       cv=n_splits, iid=False,","706","                                       param_distributions=params)","714","    score_keys = ('mean_test_score', 'mean_train_score',","715","                  'rank_test_score',","717","                  'split2_test_score',","718","                  'split0_train_score', 'split1_train_score',","719","                  'split2_train_score',","720","                  'std_test_score', 'std_train_score',","721","                  'mean_fit_time', 'std_fit_time',","722","                  'mean_score_time', 'std_score_time')","727","        cv_results = search.cv_results_","729","        check_cv_results_array_types(cv_results, param_keys, score_keys)","730","        check_cv_results_keys(cv_results, param_keys, score_keys, n_cand)","732","        assert_false(any(cv_results['param_C'].mask) or","733","                     any(cv_results['param_gamma'].mask))","760","        test_cv_scores = np.array(list(search.cv_results_['split%d_test_score'","761","                                                          % s_i][0]","762","                                       for s_i in range(search.n_splits_)))","763","        train_cv_scores = np.array(list(search.cv_results_['split%d_train_'","764","                                                           'score' % s_i][0]","765","                                        for s_i in range(search.n_splits_)))","766","        test_mean = search.cv_results_['mean_test_score'][0]","767","        test_std = search.cv_results_['std_test_score'][0]","769","        train_cv_scores = np.array(list(search.cv_results_['split%d_train_'","770","                                                           'score' % s_i][0]","771","                                        for s_i in range(search.n_splits_)))","772","        train_mean = search.cv_results_['mean_train_score'][0]","773","        train_std = search.cv_results_['std_train_score'][0]","774","","775","        # Test the first candidate","777","        assert_array_almost_equal(test_cv_scores, [1, 1. \/ 3.])","778","        assert_array_almost_equal(train_cv_scores, [1, 1])","779","","782","        expected_test_mean = 1 * 1. \/ 4. + 1. \/ 3. * 3. \/ 4.","783","        expected_test_std = np.sqrt(1. \/ 4 * (expected_test_mean - 1) ** 2 +","784","                                    3. \/ 4 * (expected_test_mean - 1. \/ 3.) **","785","                                    2)","786","        assert_almost_equal(test_mean, expected_test_mean)","787","        assert_almost_equal(test_std, expected_test_std)","788","","789","        # For the train scores, we do not take a weighted mean irrespective of","790","        # i.i.d. or not","791","        assert_almost_equal(train_mean, 1)","792","        assert_almost_equal(train_std, 0)","806","        test_cv_scores = np.array(list(search.cv_results_['split%d_test_score'","807","                                                          % s][0]","808","                                       for s in range(search.n_splits_)))","809","        test_mean = search.cv_results_['mean_test_score'][0]","810","        test_std = search.cv_results_['std_test_score'][0]","811","","812","        train_cv_scores = np.array(list(search.cv_results_['split%d_train_'","813","                                                           'score' % s][0]","814","                                        for s in range(search.n_splits_)))","815","        train_mean = search.cv_results_['mean_train_score'][0]","816","        train_std = search.cv_results_['std_train_score'][0]","817","","820","        assert_array_almost_equal(test_cv_scores, [1, 1. \/ 3.])","822","        assert_almost_equal(test_mean, np.mean(test_cv_scores))","823","        assert_almost_equal(test_std, np.std(test_cv_scores))","824","","825","        # For the train scores, we do not take a weighted mean irrespective of","826","        # i.i.d. or not","827","        assert_almost_equal(train_mean, 1)","828","        assert_almost_equal(train_std, 0)","844","        cv_results = search.cv_results_","848","        assert_almost_equal(cv_results['mean_test_score'][0],","849","                            cv_results['mean_test_score'][1])","850","        assert_almost_equal(cv_results['mean_train_score'][0],","851","                            cv_results['mean_train_score'][1])","853","            assert_almost_equal(cv_results['mean_test_score'][1],","854","                                cv_results['mean_test_score'][2])","855","        except AssertionError:","856","            pass","857","        try:","858","            assert_almost_equal(cv_results['mean_train_score'][1],","859","                                cv_results['mean_train_score'][2])","878","@ignore_warnings()","879","def test_search_cv_timing():","880","    svc = LinearSVC(random_state=0)","881","","882","    X = [[1, ], [2, ], [3, ], [4, ]]","883","    y = [0, 1, 1, 0]","884","","885","    gs = GridSearchCV(svc, {'C': [0, 1]}, cv=2, error_score=0)","886","    rs = RandomizedSearchCV(svc, {'C': [0, 1]}, cv=2, error_score=0, n_iter=2)","887","","888","    for search in (gs, rs):","889","        search.fit(X, y)","890","        for key in ['mean_fit_time', 'std_fit_time']:","891","            # NOTE The precision of time.time in windows is not high","892","            # enough for the fit\/score times to be non-zero for trivial X and y","893","            assert_true(np.all(search.cv_results_[key] >= 0))","894","            assert_true(np.all(search.cv_results_[key] < 1))","895","","896","        for key in ['mean_score_time', 'std_score_time']:","897","            assert_true(search.cv_results_[key][1] >= 0)","898","            assert_true(search.cv_results_[key][0] == 0.0)","899","            assert_true(np.all(search.cv_results_[key] < 1))","900","","901","","910","        cv_results = grid_search.fit(X, y).cv_results_","913","        result_keys = list(cv_results.keys())","1120","    X = np.arange(24).reshape(6, -1)","1121","    y = [0, 0, 0, 1, 1, 1]"],"delete":["597","def check_cv_results_array_types(results, param_keys, score_keys):","598","    # Check if the search results' array are of correct types","599","    assert_true(all(isinstance(results[param], np.ma.MaskedArray)","601","    assert_true(all(results[key].dtype == object for key in param_keys))","602","    assert_false(any(isinstance(results[key], np.ma.MaskedArray)","604","    assert_true(all(results[key].dtype == np.float64","605","                    for key in score_keys if key != 'rank_test_score'))","606","    assert_true(results['rank_test_score'].dtype == np.int32)","609","def check_cv_results_keys(results, param_keys, score_keys, n_cand):","611","    assert_array_equal(sorted(results.keys()),","613","    assert_true(all(results[key].shape == (n_cand,)","619","    results = search.cv_results_","620","    res_scores = np.vstack(list([results[\"split%d_test_score\" % i]","622","    res_means = results[\"mean_test_score\"]","623","    res_params = results[\"params\"]","636","def test_grid_search_results():","652","    score_keys = ('mean_test_score', 'rank_test_score',","654","                  'split2_test_score', 'std_test_score')","659","        results = search.cv_results_","660","        # Check results structure","661","        check_cv_results_array_types(results, param_keys, score_keys)","662","        check_cv_results_keys(results, param_keys, score_keys, n_candidates)","664","        results = grid_search.cv_results_","666","        assert_true(all((results['param_C'].mask[i] and","667","                         results['param_gamma'].mask[i] and","668","                         not results['param_degree'].mask[i])","670","                        if results['param_kernel'][i] == 'linear'))","671","        assert_true(all((not results['param_C'].mask[i] and","672","                         not results['param_gamma'].mask[i] and","673","                         results['param_degree'].mask[i])","675","                        if results['param_kernel'][i] == 'rbf'))","679","def test_random_search_results():","692","                                       cv=n_splits,","693","                                       iid=False, param_distributions=params)","701","    score_keys = ('mean_test_score', 'rank_test_score',","703","                  'split2_test_score', 'std_test_score')","708","        results = search.cv_results_","710","        check_cv_results_array_types(results, param_keys, score_keys)","711","        check_cv_results_keys(results, param_keys, score_keys, n_cand)","713","        assert_false(any(results['param_C'].mask) or","714","                     any(results['param_gamma'].mask))","741","        # Test the first candidate","742","        cv_scores = np.array(list(search.cv_results_['split%d_test_score'","743","                                                     % s][0]","744","                                  for s in range(search.n_splits_)))","745","        mean = search.cv_results_['mean_test_score'][0]","746","        std = search.cv_results_['std_test_score'][0]","749","        assert_array_almost_equal(cv_scores, [1, 1. \/ 3.])","752","        expected_mean = 1 * 1. \/ 4. + 1. \/ 3. * 3. \/ 4.","753","        expected_std = np.sqrt(1. \/ 4 * (expected_mean - 1) ** 2 +","754","                               3. \/ 4 * (expected_mean - 1. \/ 3.) ** 2)","755","        assert_almost_equal(mean, expected_mean)","756","        assert_almost_equal(std, expected_std)","770","        cv_scores = np.array(list(search.cv_results_['split%d_test_score'","771","                                                     % s][0]","772","                                  for s in range(search.n_splits_)))","773","        mean = search.cv_results_['mean_test_score'][0]","774","        std = search.cv_results_['std_test_score'][0]","777","        assert_array_almost_equal(cv_scores, [1, 1. \/ 3.])","779","        assert_almost_equal(mean, np.mean(cv_scores))","780","        assert_almost_equal(std, np.std(cv_scores))","796","        results = search.cv_results_","800","        assert_almost_equal(results['mean_test_score'][0],","801","                            results['mean_test_score'][1])","803","            assert_almost_equal(results['mean_test_score'][1],","804","                                results['mean_test_score'][2])","831","        results = grid_search.fit(X, y).cv_results_","834","        result_keys = list(results.keys())","1041","    X = np.arange(20).reshape(5, -1)","1042","    y = [0, 0, 1, 1, 1]"]}],"doc\/whats_new.rst":[{"add":["118","  - Training scores and Timing information","119","","120","    ``cv_results_`` also includes the training scores for each","121","    cross-validation split (with keys such as ``'split0_train_score'``), as","122","    well as their mean (``'mean_train_score'``) and standard deviation","123","    (``'std_train_score'``). To avoid the cost of evaluating training score,","124","    set ``return_train_score=False``.","125","","126","    Additionally the mean and standard deviation of the times taken to split,","127","    train and score the model across all the cross-validation splits is","128","    available at the key ``'mean_time'`` and ``'std_time'`` respectively.","129","","130","Changelog","131","---------","378","   - The training scores and time taken for training followed by scoring for","379","     each search candidate are now available at the ``cv_results_`` dict.","380","     See :ref:`model_selection_changes` for more information.","381","     (`#7324 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/7325>`)","382","     By `Eugene Chen`_ and `Raghav RV`_.","383","","4753","","4754",".. _Eugene Chen: https:\/\/github.com\/eyc88"],"delete":[]}],"sklearn\/model_selection\/_search.py":[{"add":["321","                                              fit_params=fit_params,","322","                                              return_n_test_samples=True,","323","                                              error_score=error_score)","378","                 error_score='raise', return_train_score=True):","390","        self.return_train_score = return_train_score","556","                                  fit_params=self.fit_params,","557","                                  return_train_score=self.return_train_score,","558","                                  return_n_test_samples=True,","559","                                  return_times=True, return_parameters=True,","564","        # if one choose to see train score, \"out\" will contain train score info","565","        if self.return_train_score:","566","            (train_scores, test_scores, test_sample_counts,","567","             fit_time, score_time, parameters) = zip(*out)","568","        else:","569","            (test_scores, test_sample_counts,","570","             fit_time, score_time, parameters) = zip(*out)","575","        results = dict()","576","","577","        def _store(key_name, array, weights=None, splits=False, rank=False):","578","            \"\"\"A small helper to store the scores\/times to the cv_results_\"\"\"","579","            array = np.array(array, dtype=np.float64).reshape(n_candidates,","580","                                                              n_splits)","581","            if splits:","582","                for split_i in range(n_splits):","583","                    results[\"split%d_%s\"","584","                            % (split_i, key_name)] = array[:, split_i]","585","","586","            array_means = np.average(array, axis=1, weights=weights)","587","            results['mean_%s' % key_name] = array_means","588","            # Weighted std is not directly available in numpy","589","            array_stds = np.sqrt(np.average((array -","590","                                             array_means[:, np.newaxis]) ** 2,","591","                                            axis=1, weights=weights))","592","            results['std_%s' % key_name] = array_stds","593","","594","            if rank:","595","                results[\"rank_%s\" % key_name] = np.asarray(","596","                    rankdata(-array_means, method='min'), dtype=np.int32)","597","","598","        # Computed the (weighted) mean and std for test scores alone","603","        _store('test_score', test_scores, splits=True, rank=True,","604","               weights=test_sample_counts if self.iid else None)","605","        _store('train_score', train_scores, splits=True)","606","        _store('fit_time', fit_time)","607","        _store('score_time', score_time)","609","        best_index = np.flatnonzero(results[\"rank_test_score\"] == 1)[0]","624","        results.update(param_results)","627","        results['params'] = candidate_params","629","        self.cv_results_ = results","771","    return_train_score : boolean, default=True","772","        If ``'False'``, the ``cv_results_`` attribute will not include training","773","        scores.","774","","793","           param_grid=..., pre_dispatch=..., refit=..., return_train_score=...,","797","    ['mean_fit_time', 'mean_score_time', 'mean_test_score',...","798","     'mean_train_score', 'param_C', 'param_kernel', 'params',...","799","     'rank_test_score', 'split0_test_score',...","800","     'split0_train_score', 'split1_test_score', 'split1_train_score',...","801","     'split2_test_score', 'split2_train_score',...","802","     'std_fit_time', 'std_score_time', 'std_test_score', 'std_train_score'...]","833","            'split0_test_score'  : [0.8, 0.7, 0.8, 0.9],","834","            'split1_test_score'  : [0.82, 0.5, 0.7, 0.78],","835","            'mean_test_score'    : [0.81, 0.60, 0.75, 0.82],","836","            'std_test_score'     : [0.02, 0.01, 0.03, 0.03],","837","            'rank_test_score'    : [2, 4, 3, 1],","838","            'split0_train_score' : [0.8, 0.9, 0.7],","839","            'split1_train_score' : [0.82, 0.5, 0.7],","840","            'mean_train_score'   : [0.81, 0.7, 0.7],","841","            'std_train_score'    : [0.03, 0.03, 0.04],","842","            'mean_fit_time'      : [0.73, 0.63, 0.43, 0.49],","843","            'std_fit_time'       : [0.01, 0.02, 0.01, 0.01],","844","            'mean_score_time'    : [0.007, 0.06, 0.04, 0.04],","845","            'std_score_time'     : [0.001, 0.002, 0.003, 0.005],","846","            'params'             : [{'kernel': 'poly', 'degree': 2}, ...],","852","        The ``mean_fit_time``, ``std_fit_time``, ``mean_score_time`` and","853","        ``std_score_time`` are all in seconds.","854","","911","                 pre_dispatch='2*n_jobs', error_score='raise',","912","                 return_train_score=True):","916","            pre_dispatch=pre_dispatch, error_score=error_score,","917","            return_train_score=return_train_score)","1051","    return_train_score : boolean, default=True","1052","        If ``'False'``, the ``cv_results_`` attribute will not include training","1053","        scores.","1054","","1079","            'split0_test_score'  : [0.8, 0.9, 0.7],","1080","            'split1_test_score'  : [0.82, 0.5, 0.7],","1081","            'mean_test_score'    : [0.81, 0.7, 0.7],","1082","            'std_test_score'     : [0.02, 0.2, 0.],","1083","            'rank_test_score'    : [3, 1, 1],","1084","            'split0_train_score' : [0.8, 0.9, 0.7],","1085","            'split1_train_score' : [0.82, 0.5, 0.7],","1086","            'mean_train_score'   : [0.81, 0.7, 0.7],","1087","            'std_train_score'    : [0.03, 0.03, 0.04],","1088","            'mean_fit_time'      : [0.73, 0.63, 0.43, 0.49],","1089","            'std_fit_time'       : [0.01, 0.02, 0.01, 0.01],","1090","            'mean_score_time'    : [0.007, 0.06, 0.04, 0.04],","1091","            'std_score_time'     : [0.001, 0.002, 0.003, 0.005],","1098","        The ``mean_fit_time``, ``std_fit_time``, ``mean_score_time`` and","1099","        ``std_score_time`` are all in seconds.","1100","","1154","                 error_score='raise', return_train_score=True):","1159","             estimator=estimator, scoring=scoring, fit_params=fit_params,","1160","             n_jobs=n_jobs, iid=iid, refit=refit, cv=cv, verbose=verbose,","1161","             pre_dispatch=pre_dispatch, error_score=error_score,","1162","             return_train_score=return_train_score)"],"delete":["321","                                              fit_params, error_score)","376","                 error_score='raise'):","553","                                  self.fit_params, return_parameters=True,","558","        test_scores, test_sample_counts, _, parameters = zip(*out)","563","        test_scores = np.array(test_scores,","564","                               dtype=np.float64).reshape(n_candidates,","565","                                                         n_splits)","570","        # Computed the (weighted) mean and std for all the candidates","571","        weights = test_sample_counts if self.iid else None","572","        means = np.average(test_scores, axis=1, weights=weights)","573","        stds = np.sqrt(np.average((test_scores - means[:, np.newaxis]) ** 2,","574","                                  axis=1, weights=weights))","576","        cv_results = dict()","577","        for split_i in range(n_splits):","578","            cv_results[\"split%d_test_score\" % split_i] = test_scores[:,","579","                                                                     split_i]","580","        cv_results[\"mean_test_score\"] = means","581","        cv_results[\"std_test_score\"] = stds","582","","583","        ranks = np.asarray(rankdata(-means, method='min'), dtype=np.int32)","584","","585","        best_index = np.flatnonzero(ranks == 1)[0]","587","        cv_results[\"rank_test_score\"] = ranks","601","        cv_results.update(param_results)","604","        cv_results['params'] = candidate_params","606","        self.cv_results_ = cv_results","766","           param_grid=..., pre_dispatch=..., refit=...,","770","    ['mean_test_score', 'param_C', 'param_kernel', 'params',...","771","     'rank_test_score', 'split0_test_score', 'split1_test_score',...","772","     'split2_test_score', 'std_test_score']","803","            'split0_test_score' : [0.8, 0.7, 0.8, 0.9],","804","            'split1_test_score' : [0.82, 0.5, 0.7, 0.78],","805","            'mean_test_score'   : [0.81, 0.60, 0.75, 0.82],","806","            'std_test_score'    : [0.02, 0.01, 0.03, 0.03],","807","            'rank_test_score'   : [2, 4, 3, 1],","808","            'params'            : [{'kernel': 'poly', 'degree': 2}, ...],","870","                 pre_dispatch='2*n_jobs', error_score='raise'):","874","            pre_dispatch=pre_dispatch, error_score=error_score)","1032","            'split0_test_score' : [0.8, 0.9, 0.7],","1033","            'split1_test_score' : [0.82, 0.5, 0.7],","1034","            'mean_test_score'   : [0.81, 0.7, 0.7],","1035","            'std_test_score'    : [0.02, 0.2, 0.],","1036","            'rank_test_score'   : [3, 1, 1],","1096","                 error_score='raise'):","1097","","1102","            estimator=estimator, scoring=scoring, fit_params=fit_params,","1103","            n_jobs=n_jobs, iid=iid, refit=refit, cv=cv, verbose=verbose,","1104","            pre_dispatch=pre_dispatch, error_score=error_score)"]}]}},"49d126fd8fdf9ed554d9ac953ba88fda09137be3":{"changes":{"sklearn\/utils\/extmath.py":"MODIFY","sklearn\/metrics\/ranking.py":"MODIFY","sklearn\/utils\/tests\/test_extmath.py":"MODIFY"},"diff":{"sklearn\/utils\/extmath.py":[{"add":["844","","845","","846","def stable_cumsum(arr, rtol=1e-05, atol=1e-08):","847","    \"\"\"Use high precision for cumsum and check that final value matches sum","848","","849","    Parameters","850","    ----------","851","    arr : array-like","852","        To be cumulatively summed as flat","853","    rtol : float","854","        Relative tolerance, see ``np.allclose``","855","    atol : float","856","        Absolute tolerance, see ``np.allclose``","857","    \"\"\"","858","    out = np.cumsum(arr, dtype=np.float64)","859","    expected = np.sum(arr, dtype=np.float64)","860","    if not np.allclose(out[-1], expected, rtol=rtol, atol=atol):","861","        raise RuntimeError('cumsum was found to be unstable: '","862","                           'its last element does not correspond to sum')","863","    return out"],"delete":[]}],"sklearn\/metrics\/ranking.py":[{"add":["29","from ..utils.extmath import stable_cumsum","340","    tps = stable_cumsum(y_true * weight)[threshold_idxs]","342","        fps = stable_cumsum(weight)[threshold_idxs] - tps"],"delete":["339","    tps = (y_true * weight).cumsum()[threshold_idxs]","341","        fps = weight.cumsum()[threshold_idxs] - tps"]}],"sklearn\/utils\/tests\/test_extmath.py":[{"add":["19","from sklearn.utils.testing import assert_raise_message","21","from sklearn.utils.testing import SkipTest","22","from sklearn.utils.fixes import np_version","37","from sklearn.utils.extmath import stable_cumsum","649","","650","","651","def test_stable_cumsum():","652","    if np_version < (1, 9):","653","        raise SkipTest(\"Sum is as unstable as cumsum for numpy < 1.9\")","654","    assert_array_equal(stable_cumsum([1, 2, 3]), np.cumsum([1, 2, 3]))","655","    r = np.random.RandomState(0).rand(100000)","656","    assert_raise_message(RuntimeError,","657","                         'cumsum was found to be unstable: its last element '","658","                         'does not correspond to sum',","659","                         stable_cumsum, r, rtol=0, atol=0)"],"delete":[]}]}},"a03db89eba7978cbe8d22573cf64def4df8b5d72":{"changes":{"sklearn\/model_selection\/tests\/test_search.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/model_selection\/_search.py":"MODIFY"},"diff":{"sklearn\/model_selection\/tests\/test_search.py":[{"add":["5","from sklearn.externals.joblib._compat import PY3_OR_LATER","307","def test_grid_search_when_param_grid_includes_range():","308","    # Test that the best estimator contains the right value for foo_param","309","    clf = MockClassifier()","310","    grid_search = None","311","    if PY3_OR_LATER:","312","        grid_search = GridSearchCV(clf, {'foo_param': range(1, 4)})","313","    else:","314","        grid_search = GridSearchCV(clf, {'foo_param': xrange(1, 4)})","315","    grid_search.fit(X, y)","316","    assert_equal(grid_search.best_estimator_.foo_param, 2)","317","","318","","322","    assert_raise_message(","323","        ValueError,","324","        \"Parameter values for parameter (C) need to be a sequence\"","325","        \"(but not a string) or np.ndarray.\",","326","        GridSearchCV, clf, param_dict)","330","    assert_raise_message(","331","        ValueError,","332","        \"Parameter values for parameter (C) need to be a non-empty sequence.\",","333","        GridSearchCV, clf, param_dict)","334","","335","    param_dict = {\"C\": \"1,2,3\"}","336","    clf = SVC()","337","    assert_raise_message(","338","        ValueError,","339","        \"Parameter values for parameter (C) need to be a sequence\"","340","        \"(but not a string) or np.ndarray.\",","341","        GridSearchCV, clf, param_dict)"],"delete":["171","def test_grid_search_incorrect_param_grid():","172","    clf = MockClassifier()","173","    assert_raise_message(","174","        ValueError,","175","        \"Parameter values for parameter (C) need to be a sequence.\",","176","        GridSearchCV, clf, {'C': 1})","177","","178","","179","def test_grid_search_param_grid_includes_sequence_of_a_zero_length():","180","    clf = MockClassifier()","181","    assert_raise_message(","182","        ValueError,","183","        \"Parameter values for parameter (C) need to be a non-empty sequence.\",","184","        GridSearchCV, clf, {'C': []})","185","","186","","325","    assert_raises(ValueError, GridSearchCV, clf, param_dict)","329","    assert_raises(ValueError, GridSearchCV, clf, param_dict)"]}],"doc\/whats_new.rst":[{"add":["298","    - :func: `model_selection.tests._search._check_param_grid` now works correctly with all types","299","      that extends\/implements `Sequence` (except string), including range (Python 3.x) and xrange","300","      (Python 2.x).","301","      (`#7323 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/7323>`_) by `Viacheslav Kovalevskyi`_.","302",""],"delete":[]}],"sklearn\/model_selection\/_search.py":[{"add":["14","from collections import Mapping, namedtuple, Sized, defaultdict, Sequence","334","            if (isinstance(v, six.string_types) or","335","                    not isinstance(v, (np.ndarray, Sequence))):","337","                                 \"to be a sequence(but not a string) or\"","338","                                 \" np.ndarray.\".format(name))"],"delete":["14","from collections import Mapping, namedtuple, Sized, defaultdict","334","            check = [isinstance(v, k) for k in (list, tuple, np.ndarray)]","335","            if True not in check:","337","                                 \"to be a sequence.\".format(name))"]}]}},"7c0ebbc2269612dd79ec1d64d2da866ce567c861":{"changes":{"examples\/classification\/plot_lda_qda.py":"MODIFY"},"diff":{"examples\/classification\/plot_lda_qda.py":[{"add":["70","    alpha = 0.5","71","","73","    plt.plot(X0_tp[:, 0], X0_tp[:, 1], 'o', alpha=alpha,","74","             color='red')","75","    plt.plot(X0_fp[:, 0], X0_fp[:, 1], '*', alpha=alpha,","76","             color='#990000')  # dark red","79","    plt.plot(X1_tp[:, 0], X1_tp[:, 1], 'o', alpha=alpha,","80","             color='blue')","81","    plt.plot(X1_fp[:, 0], X1_fp[:, 1], '*', alpha=alpha,","82","             color='#000099')  # dark blue","112","                              180 + angle, facecolor=color, edgecolor='yellow',","113","                              linewidth=2, zorder=2)"],"delete":["71","    plt.plot(X0_tp[:, 0], X0_tp[:, 1], 'o', color='red')","72","    plt.plot(X0_fp[:, 0], X0_fp[:, 1], '.', color='#990000')  # dark red","75","    plt.plot(X1_tp[:, 0], X1_tp[:, 1], 'o', color='blue')","76","    plt.plot(X1_fp[:, 0], X1_fp[:, 1], '.', color='#000099')  # dark blue","106","                              180 + angle, color=color)"]}]}},"040a76676d325f5d97d46a42ca0ff628883675b3":{"changes":{"sklearn\/manifold\/tests\/test_t_sne.py":"MODIFY","sklearn\/manifold\/t_sne.py":"MODIFY"},"diff":{"sklearn\/manifold\/tests\/test_t_sne.py":[{"add":["9","from sklearn.utils.testing import assert_array_equal","311","    # 'init' must be 'pca', 'random', or numpy array.","312","    m = \"'init' must be 'pca', 'random', or a numpy array\"","316","def test_init_ndarray():","317","    # Initialize TSNE with ndarray and test fit","318","    tsne = TSNE(init=np.zeros((100, 2)))","319","    X_embedded = tsne.fit_transform(np.ones((100, 5)))","320","    assert_array_equal(np.zeros((100, 2)), X_embedded)","321","","322","","323","def test_init_ndarray_precomputed():","324","    # Initialize TSNE with ndarray and metric 'precomputed'","325","    # Make sure no FutureWarning is thrown from _fit","326","    tsne = TSNE(init=np.zeros((100, 2)), metric=\"precomputed\")","327","    tsne.fit(np.zeros((100, 100)))","328","","329",""],"delete":["310","    # 'init' must be 'pca' or 'random'.","311","    m = \"'init' must be 'pca', 'random' or a NumPy array\""]}],"sklearn\/manifold\/t_sne.py":[{"add":["25","from ..externals.six import string_types","570","    init : string or numpy array, optional (default: \"random\")","571","        Initialization of embedding. Possible options are 'random', 'pca',","572","        and a numpy array of shape (n_samples, n_components).","647","        if not ((isinstance(init, string_types) and","648","                init in [\"pca\", \"random\"]) or","649","                isinstance(init, np.ndarray)):","650","            msg = \"'init' must be 'pca', 'random', or a numpy array\"","713","            if isinstance(self.init, string_types) and self.init == 'pca':","769","        if isinstance(self.init, np.ndarray):","770","            X_embedded = self.init","771","        elif self.init == 'pca':"],"delete":["569","    init : string, optional (default: \"random\")","570","        Initialization of embedding. Possible options are 'random' and 'pca'.","645","        if init not in [\"pca\", \"random\"] or isinstance(init, np.ndarray):","646","            msg = \"'init' must be 'pca', 'random' or a NumPy array\"","709","            if self.init == 'pca':","765","        if self.init == 'pca':","769","        elif isinstance(self.init, np.ndarray):","770","            X_embedded = self.init"]}]}},"0c879ba55193f13420934fa01bbcae88cad53c14":{"changes":{"sklearn\/datasets\/base.py":"MODIFY","examples\/decomposition\/plot_sparse_coding.py":"MODIFY","benchmarks\/bench_plot_neighbors.py":"MODIFY","doc\/datasets\/index.rst":"MODIFY","doc\/tutorial\/statistical_inference\/settings.rst":"MODIFY","doc\/tutorial\/text_analytics\/solutions\/exercise_01_language_train_model.py":"MODIFY","examples\/gaussian_process\/plot_gpr_noisy_targets.py":"MODIFY","benchmarks\/bench_sgd_regression.py":"MODIFY","benchmarks\/bench_tree.py":"MODIFY","benchmarks\/bench_plot_parallel_pairwise.py":"MODIFY","benchmarks\/bench_plot_ward.py":"MODIFY","examples\/classification\/plot_digits_classification.py":"MODIFY","doc\/tutorial\/statistical_inference\/supervised_learning.rst":"MODIFY","benchmarks\/bench_plot_omp_lars.py":"MODIFY","benchmarks\/bench_lasso.py":"MODIFY","examples\/gaussian_process\/plot_gpc_isoprobability.py":"MODIFY","benchmarks\/bench_glm.py":"MODIFY","doc\/tutorial\/text_analytics\/skeletons\/exercise_01_language_train_model.py":"MODIFY","benchmarks\/bench_glmnet.py":"MODIFY","sklearn\/ensemble\/partial_dependence.py":"MODIFY","examples\/text\/mlcomp_sparse_document_classification.py":"MODIFY"},"diff":{"sklearn\/datasets\/base.py":[{"add":["417","        >>> import matplotlib.pyplot as plt #doctest: +SKIP","418","        >>> plt.gray() #doctest: +SKIP","419","        >>> plt.matshow(digits.images[0]) #doctest: +SKIP","420","        >>> plt.show() #doctest: +SKIP"],"delete":["417","        >>> import pylab as pl #doctest: +SKIP","418","        >>> pl.gray() #doctest: +SKIP","419","        >>> pl.matshow(digits.images[0]) #doctest: +SKIP","420","        >>> pl.show() #doctest: +SKIP"]}],"examples\/decomposition\/plot_sparse_coding.py":[{"add":["19","import matplotlib.pyplot as plt"],"delete":["19","import matplotlib.pylab as plt"]}],"benchmarks\/bench_plot_neighbors.py":[{"add":["6","import matplotlib.pyplot as plt","108","    plt.figure(figsize=(8, 11))","120","        ax = plt.subplot(sbplt, yscale='log')","121","        plt.grid(True)","133","            c_bar = plt.bar(xvals, build_time[alg] - bottom,","134","                            width, bottom, color='r')","135","            q_bar = plt.bar(xvals, query_time[alg],","136","                            width, build_time[alg], color='b')","141","            plt.text((i + 0.02) \/ len(algorithms), 0.98, alg,","142","                     transform=ax.transAxes,","143","                     ha='left',","144","                     va='top',","145","                     bbox=dict(facecolor='w', edgecolor='w', alpha=0.5))","147","            plt.ylabel('Time (s)')","168","        plt.text(1.01, 0.5, title_string,","169","                 transform=ax.transAxes, rotation=-90,","170","                 ha='left', va='center', fontsize=20)","172","        plt.text(0.99, 0.5, descr_string,","173","                 transform=ax.transAxes, rotation=-90,","174","                 ha='right', va='center')","176","        plt.gcf().suptitle(\"%s data set\" % dataset.capitalize(), fontsize=16)","178","    plt.figlegend((c_bar, q_bar), ('construction', 'N-point query'),","179","                  'upper right')","184","    plt.show()"],"delete":["6","import pylab as pl","108","    pl.figure(figsize=(8, 11))","120","        ax = pl.subplot(sbplt, yscale='log')","121","        pl.grid(True)","133","            c_bar = pl.bar(xvals, build_time[alg] - bottom,","134","                           width, bottom, color='r')","135","            q_bar = pl.bar(xvals, query_time[alg],","136","                           width, build_time[alg], color='b')","141","            pl.text((i + 0.02) \/ len(algorithms), 0.98, alg,","142","                    transform=ax.transAxes,","143","                    ha='left',","144","                    va='top',","145","                    bbox=dict(facecolor='w', edgecolor='w', alpha=0.5))","147","            pl.ylabel('Time (s)')","168","        pl.text(1.01, 0.5, title_string,","169","                transform=ax.transAxes, rotation=-90,","170","                ha='left', va='center', fontsize=20)","172","        pl.text(0.99, 0.5, descr_string,","173","                transform=ax.transAxes, rotation=-90,","174","                ha='right', va='center')","176","        pl.gcf().suptitle(\"%s data set\" % dataset.capitalize(), fontsize=16)","178","    pl.figlegend((c_bar, q_bar), ('construction', 'N-point query'),","179","                 'upper right')","184","    pl.show()"]}],"doc\/datasets\/index.rst":[{"add":["95","  if you plan to use ``matplotlib.pyplpt.imshow`` don't forget to scale to the range"],"delete":["95","  if you plan to use ``pylab.imshow`` don't forget to scale to the range"]}],"doc\/tutorial\/statistical_inference\/settings.rst":[{"add":["31",".. topic:: An example of reshaping data would be the digits dataset","44","        >>> import matplotlib.pyplot as plt #doctest: +SKIP","45","        >>> plt.imshow(digits.images[-1], cmap=plt.cm.gray_r) #doctest: +SKIP"],"delete":["31",".. topic:: An example of reshaping data would be the digits dataset ","44","        >>> import pylab as pl #doctest: +SKIP","45","        >>> pl.imshow(digits.images[-1], cmap=pl.cm.gray_r) #doctest: +SKIP","91",""]}],"doc\/tutorial\/text_analytics\/solutions\/exercise_01_language_train_model.py":[{"add":["56","#import matlotlib.pyplot as plt","57","#plt.matshow(cm, cmap=plt.cm.jet)","58","#plt.show()"],"delete":["56","#import pylab as pl","57","#pl.matshow(cm, cmap=pl.cm.jet)","58","#pl.show()"]}],"examples\/gaussian_process\/plot_gpr_noisy_targets.py":[{"add":["28","from matplotlib import pyplot as plt","63","fig = plt.figure()","64","plt.plot(x, f(x), 'r:', label=u'$f(x) = x\\,\\sin(x)$')","65","plt.plot(X, y, 'r.', markersize=10, label=u'Observations')","66","plt.plot(x, y_pred, 'b-', label=u'Prediction')","67","plt.fill(np.concatenate([x, x[::-1]]),","68","         np.concatenate([y_pred - 1.9600 * sigma,","69","                        (y_pred + 1.9600 * sigma)[::-1]]),","70","         alpha=.5, fc='b', ec='None', label='95% confidence interval')","71","plt.xlabel('$x$')","72","plt.ylabel('$f(x)$')","73","plt.ylim(-10, 20)","74","plt.legend(loc='upper left')","99","fig = plt.figure()","100","plt.plot(x, f(x), 'r:', label=u'$f(x) = x\\,\\sin(x)$')","101","plt.errorbar(X.ravel(), y, dy, fmt='r.', markersize=10, label=u'Observations')","102","plt.plot(x, y_pred, 'b-', label=u'Prediction')","103","plt.fill(np.concatenate([x, x[::-1]]),","104","         np.concatenate([y_pred - 1.9600 * sigma,","105","                        (y_pred + 1.9600 * sigma)[::-1]]),","106","         alpha=.5, fc='b', ec='None', label='95% confidence interval')","107","plt.xlabel('$x$')","108","plt.ylabel('$f(x)$')","109","plt.ylim(-10, 20)","110","plt.legend(loc='upper left')","112","plt.show()"],"delete":["28","from matplotlib import pyplot as pl","63","fig = pl.figure()","64","pl.plot(x, f(x), 'r:', label=u'$f(x) = x\\,\\sin(x)$')","65","pl.plot(X, y, 'r.', markersize=10, label=u'Observations')","66","pl.plot(x, y_pred, 'b-', label=u'Prediction')","67","pl.fill(np.concatenate([x, x[::-1]]),","68","        np.concatenate([y_pred - 1.9600 * sigma,","69","                       (y_pred + 1.9600 * sigma)[::-1]]),","70","        alpha=.5, fc='b', ec='None', label='95% confidence interval')","71","pl.xlabel('$x$')","72","pl.ylabel('$f(x)$')","73","pl.ylim(-10, 20)","74","pl.legend(loc='upper left')","99","fig = pl.figure()","100","pl.plot(x, f(x), 'r:', label=u'$f(x) = x\\,\\sin(x)$')","101","pl.errorbar(X.ravel(), y, dy, fmt='r.', markersize=10, label=u'Observations')","102","pl.plot(x, y_pred, 'b-', label=u'Prediction')","103","pl.fill(np.concatenate([x, x[::-1]]),","104","        np.concatenate([y_pred - 1.9600 * sigma,","105","                       (y_pred + 1.9600 * sigma)[::-1]]),","106","        alpha=.5, fc='b', ec='None', label='95% confidence interval')","107","pl.xlabel('$x$')","108","pl.ylabel('$f(x)$')","109","pl.ylim(-10, 20)","110","pl.legend(loc='upper left')","112","pl.show()"]}],"benchmarks\/bench_sgd_regression.py":[{"add":["13","import matplotlib.pyplot as plt","115","    plt.figure('scikit-learn SGD regression benchmark results',","116","               figsize=(5 * 2, 4 * m))","118","        plt.subplot(m, 2, i + 1)","119","        plt.plot(list_n_samples, np.sqrt(elnet_results[:, j, 0]),","120","                 label=\"ElasticNet\")","121","        plt.plot(list_n_samples, np.sqrt(sgd_results[:, j, 0]),","122","                 label=\"SGDRegressor\")","123","        plt.plot(list_n_samples, np.sqrt(asgd_results[:, j, 0]),","124","                 label=\"A-SGDRegressor\")","125","        plt.plot(list_n_samples, np.sqrt(ridge_results[:, j, 0]),","126","                 label=\"Ridge\")","127","        plt.legend(prop={\"size\": 10})","128","        plt.xlabel(\"n_train\")","129","        plt.ylabel(\"RMSE\")","130","        plt.title(\"Test error - %d features\" % list_n_features[j])","133","        plt.subplot(m, 2, i + 1)","134","        plt.plot(list_n_samples, np.sqrt(elnet_results[:, j, 1]),","135","                 label=\"ElasticNet\")","136","        plt.plot(list_n_samples, np.sqrt(sgd_results[:, j, 1]),","137","                 label=\"SGDRegressor\")","138","        plt.plot(list_n_samples, np.sqrt(asgd_results[:, j, 1]),","139","                 label=\"A-SGDRegressor\")","140","        plt.plot(list_n_samples, np.sqrt(ridge_results[:, j, 1]),","141","                 label=\"Ridge\")","142","        plt.legend(prop={\"size\": 10})","143","        plt.xlabel(\"n_train\")","144","        plt.ylabel(\"Time [sec]\")","145","        plt.title(\"Training time - %d features\" % list_n_features[j])","148","    plt.subplots_adjust(hspace=.30)","150","    plt.show()"],"delete":["13","import pylab as pl","115","    pl.figure('scikit-learn SGD regression benchmark results',","116","              figsize=(5 * 2, 4 * m))","118","        pl.subplot(m, 2, i + 1)","119","        pl.plot(list_n_samples, np.sqrt(elnet_results[:, j, 0]),","120","                label=\"ElasticNet\")","121","        pl.plot(list_n_samples, np.sqrt(sgd_results[:, j, 0]),","122","                label=\"SGDRegressor\")","123","        pl.plot(list_n_samples, np.sqrt(asgd_results[:, j, 0]),","124","                label=\"A-SGDRegressor\")","125","        pl.plot(list_n_samples, np.sqrt(ridge_results[:, j, 0]),","126","                label=\"Ridge\")","127","        pl.legend(prop={\"size\": 10})","128","        pl.xlabel(\"n_train\")","129","        pl.ylabel(\"RMSE\")","130","        pl.title(\"Test error - %d features\" % list_n_features[j])","133","        pl.subplot(m, 2, i + 1)","134","        pl.plot(list_n_samples, np.sqrt(elnet_results[:, j, 1]),","135","                label=\"ElasticNet\")","136","        pl.plot(list_n_samples, np.sqrt(sgd_results[:, j, 1]),","137","                label=\"SGDRegressor\")","138","        pl.plot(list_n_samples, np.sqrt(asgd_results[:, j, 1]),","139","                label=\"A-SGDRegressor\")","140","        pl.plot(list_n_samples, np.sqrt(ridge_results[:, j, 1]),","141","                label=\"Ridge\")","142","        pl.legend(prop={\"size\": 10})","143","        pl.xlabel(\"n_train\")","144","        pl.ylabel(\"Time [sec]\")","145","        pl.title(\"Training time - %d features\" % list_n_features[j])","148","    pl.subplots_adjust(hspace=.30)","150","    pl.show()"]}],"benchmarks\/bench_tree.py":[{"add":["16","import matplotlib.pyplot as plt","86","    plt.figure('scikit-learn tree benchmark results')","87","    plt.subplot(211)","88","    plt.title('Learning with varying number of samples')","89","    plt.plot(xx, scikit_classifier_results, 'g-', label='classification')","90","    plt.plot(xx, scikit_regressor_results, 'r-', label='regression')","91","    plt.legend(loc='upper left')","92","    plt.xlabel('number of samples')","93","    plt.ylabel('Time (s)')","115","    plt.subplot(212)","116","    plt.title('Learning in high dimensional spaces')","117","    plt.plot(xx, scikit_classifier_results, 'g-', label='classification')","118","    plt.plot(xx, scikit_regressor_results, 'r-', label='regression')","119","    plt.legend(loc='upper left')","120","    plt.xlabel('number of dimensions')","121","    plt.ylabel('Time (s)')","122","    plt.axis('tight')","123","    plt.show()"],"delete":["16","import pylab as pl","86","    pl.figure('scikit-learn tree benchmark results')","87","    pl.subplot(211)","88","    pl.title('Learning with varying number of samples')","89","    pl.plot(xx, scikit_classifier_results, 'g-', label='classification')","90","    pl.plot(xx, scikit_regressor_results, 'r-', label='regression')","91","    pl.legend(loc='upper left')","92","    pl.xlabel('number of samples')","93","    pl.ylabel('Time (s)')","115","    pl.subplot(212)","116","    pl.title('Learning in high dimensional spaces')","117","    pl.plot(xx, scikit_classifier_results, 'g-', label='classification')","118","    pl.plot(xx, scikit_regressor_results, 'r-', label='regression')","119","    pl.legend(loc='upper left')","120","    pl.xlabel('number of dimensions')","121","    pl.ylabel('Time (s)')","122","    pl.axis('tight')","123","    pl.show()"]}],"benchmarks\/bench_plot_parallel_pairwise.py":[{"add":["4","import matplotlib.pyplot as plt","27","    plt.figure('scikit-learn parallel %s benchmark results' % func.__name__)","28","    plt.plot(sample_sizes, one_core, label=\"one core\")","29","    plt.plot(sample_sizes, multi_core, label=\"multi core\")","30","    plt.xlabel('n_samples')","31","    plt.ylabel('Time (s)')","32","    plt.title('Parallel %s' % func.__name__)","33","    plt.legend()","43","plt.show()"],"delete":["4","import pylab as pl","27","    pl.figure('scikit-learn parallel %s benchmark results' % func.__name__)","28","    pl.plot(sample_sizes, one_core, label=\"one core\")","29","    pl.plot(sample_sizes, multi_core, label=\"multi core\")","30","    pl.xlabel('n_samples')","31","    pl.ylabel('Time (s)')","32","    pl.title('Parallel %s' % func.__name__)","33","    pl.legend()","43","pl.show()"]}],"benchmarks\/bench_plot_ward.py":[{"add":["8","import matplotlib.pyplot as plt","33","plt.figure(\"scikit-learn Ward's method benchmark results\")","34","plt.imshow(np.log(ratio), aspect='auto', origin=\"lower\")","35","plt.colorbar()","36","plt.contour(ratio, levels=[1, ], colors='k')","37","plt.yticks(range(len(n_features)), n_features.astype(np.int))","38","plt.ylabel('N features')","39","plt.xticks(range(len(n_samples)), n_samples.astype(np.int))","40","plt.xlabel('N samples')","41","plt.title(\"Scikit's time, in units of scipy time (log)\")","42","plt.show()"],"delete":["8","import pylab as pl","33","pl.figure(\"scikit-learn Ward's method benchmark results\")","34","pl.imshow(np.log(ratio), aspect='auto', origin=\"lower\")","35","pl.colorbar()","36","pl.contour(ratio, levels=[1, ], colors='k')","37","pl.yticks(range(len(n_features)), n_features.astype(np.int))","38","pl.ylabel('N features')","39","pl.xticks(range(len(n_samples)), n_samples.astype(np.int))","40","pl.xlabel('N samples')","41","pl.title(\"Scikit's time, in units of scipy time (log)\")","42","pl.show()"]}],"examples\/classification\/plot_digits_classification.py":[{"add":["29","# matplotlib.pyplot.imread.  Note that each image must have the same size. For these"],"delete":["29","# pylab.imread.  Note that each image must have the same size. For these"]}],"doc\/tutorial\/statistical_inference\/supervised_learning.rst":[{"add":["214","    >>> import matplotlib.pyplot as plt # doctest: +SKIP","215","    >>> plt.figure() # doctest: +SKIP","221","    ...    plt.plot(test, regr.predict(test)) # doctest: +SKIP","222","    ...    plt.scatter(this_X, y, s=3)  # doctest: +SKIP","240","    >>> plt.figure() # doctest: +SKIP","246","    ...    plt.plot(test, regr.predict(test)) # doctest: +SKIP","247","    ...    plt.scatter(this_X, y, s=3) # doctest: +SKIP"],"delete":["214","    >>> import pylab as pl # doctest: +SKIP","215","    >>> pl.figure() # doctest: +SKIP","221","    ...    pl.plot(test, regr.predict(test)) # doctest: +SKIP","222","    ...    pl.scatter(this_X, y, s=3)  # doctest: +SKIP","240","    >>> pl.figure() # doctest: +SKIP","246","    ...    pl.plot(test, regr.predict(test)) # doctest: +SKIP","247","    ...    pl.scatter(this_X, y, s=3) # doctest: +SKIP"]}],"benchmarks\/bench_plot_omp_lars.py":[{"add":["107","    import matplotlib.pyplot as plt","108","    fig = plt.figure('scikit-learn OMP vs. LARS benchmark results')","110","        ax = fig.add_subplot(1, 2, i+1)","112","        plt.matshow(timings, fignum=False, vmin=1 - vmax, vmax=1 + vmax)","115","        plt.xlabel('n_samples')","116","        plt.ylabel('n_features')","117","        plt.title(label)","119","    plt.subplots_adjust(0.1, 0.08, 0.96, 0.98, 0.4, 0.63)","120","    ax = plt.axes([0.1, 0.08, 0.8, 0.06])","121","    plt.colorbar(cax=ax, orientation='horizontal')","122","    plt.show()"],"delete":["107","    import pylab as pl","108","    fig = pl.figure('scikit-learn OMP vs. LARS benchmark results')","110","        ax = fig.add_subplot(1, 2, i)","112","        pl.matshow(timings, fignum=False, vmin=1 - vmax, vmax=1 + vmax)","115","        pl.xlabel('n_samples')","116","        pl.ylabel('n_features')","117","        pl.title(label)","119","    pl.subplots_adjust(0.1, 0.08, 0.96, 0.98, 0.4, 0.63)","120","    ax = pl.axes([0.1, 0.08, 0.8, 0.06])","121","    pl.colorbar(cax=ax, orientation='horizontal')","122","    pl.show()"]}],"benchmarks\/bench_lasso.py":[{"add":["61","    import matplotlib.pyplot as plt","70","    plt.figure('scikit-learn LASSO benchmark results')","71","    plt.subplot(211)","72","    plt.plot(list_n_samples, lasso_results, 'b-',","74","    plt.plot(list_n_samples, lars_lasso_results, 'r-',","76","    plt.title('precomputed Gram matrix, %d features, alpha=%s' % (n_features,","77","                            alpha))","78","    plt.legend(loc='upper left')","79","    plt.xlabel('number of samples')","80","    plt.ylabel('Time (s)')","81","    plt.axis('tight')","87","    plt.subplot(212)","88","    plt.plot(list_n_features, lasso_results, 'b-', label='Lasso')","89","    plt.plot(list_n_features, lars_lasso_results, 'r-', label='LassoLars')","90","    plt.title('%d samples, alpha=%s' % (n_samples, alpha))","91","    plt.legend(loc='upper left')","92","    plt.xlabel('number of features')","93","    plt.ylabel('Time (s)')","94","    plt.axis('tight')","95","    plt.show()"],"delete":["61","    import pylab as pl","70","    pl.figure('scikit-learn LASSO benchmark results')","71","    pl.subplot(211)","72","    pl.plot(list_n_samples, lasso_results, 'b-',","74","    pl.plot(list_n_samples, lars_lasso_results, 'r-',","76","    pl.title('precomputed Gram matrix, %d features, alpha=%s' % (n_features, alpha))","77","    pl.legend(loc='upper left')","78","    pl.xlabel('number of samples')","79","    pl.ylabel('Time (s)')","80","    pl.axis('tight')","86","    pl.subplot(212)","87","    pl.plot(list_n_features, lasso_results, 'b-', label='Lasso')","88","    pl.plot(list_n_features, lars_lasso_results, 'r-', label='LassoLars')","89","    pl.title('%d samples, alpha=%s' % (n_samples, alpha))","90","    pl.legend(loc='upper left')","91","    pl.xlabel('number of features')","92","    pl.ylabel('Time (s)')","93","    pl.axis('tight')","94","    pl.show()"]}],"examples\/gaussian_process\/plot_gpc_isoprobability.py":[{"add":["20","from matplotlib import pyplot as plt","66","fig = plt.figure(1)","69","plt.xticks([])","70","plt.yticks([])","73","plt.xlabel('$x_1$')","74","plt.ylabel('$x_2$')","76","cax = plt.imshow(y_prob, cmap=cm.gray_r, alpha=0.8,","77","                 extent=(-lim, lim, -lim, lim))","78","norm = plt.matplotlib.colors.Normalize(vmin=0., vmax=0.9)","79","cb = plt.colorbar(cax, ticks=[0., 0.2, 0.4, 0.6, 0.8, 1.], norm=norm)","81","plt.clim(0, 1)","83","plt.plot(X[y <= 0, 0], X[y <= 0, 1], 'r.', markersize=12)","85","plt.plot(X[y > 0, 0], X[y > 0, 1], 'b.', markersize=12)","87","cs = plt.contour(x1, x2, y_true, [0.], colors='k', linestyles='dashdot')","89","cs = plt.contour(x1, x2, y_prob, [0.666], colors='b',","90","                 linestyles='solid')","91","plt.clabel(cs, fontsize=11)","93","cs = plt.contour(x1, x2, y_prob, [0.5], colors='k',","94","                 linestyles='dashed')","95","plt.clabel(cs, fontsize=11)","97","cs = plt.contour(x1, x2, y_prob, [0.334], colors='r',","98","                 linestyles='solid')","99","plt.clabel(cs, fontsize=11)","101","plt.show()"],"delete":["20","from matplotlib import pyplot as pl","66","fig = pl.figure(1)","69","pl.xticks([])","70","pl.yticks([])","73","pl.xlabel('$x_1$')","74","pl.ylabel('$x_2$')","76","cax = pl.imshow(y_prob, cmap=cm.gray_r, alpha=0.8,","77","                extent=(-lim, lim, -lim, lim))","78","norm = pl.matplotlib.colors.Normalize(vmin=0., vmax=0.9)","79","cb = pl.colorbar(cax, ticks=[0., 0.2, 0.4, 0.6, 0.8, 1.], norm=norm)","81","pl.clim(0, 1)","83","pl.plot(X[y <= 0, 0], X[y <= 0, 1], 'r.', markersize=12)","85","pl.plot(X[y > 0, 0], X[y > 0, 1], 'b.', markersize=12)","87","cs = pl.contour(x1, x2, y_true, [0.], colors='k', linestyles='dashdot')","89","cs = pl.contour(x1, x2, y_prob, [0.666], colors='b',","90","                linestyles='solid')","91","pl.clabel(cs, fontsize=11)","93","cs = pl.contour(x1, x2, y_prob, [0.5], colors='k',","94","                linestyles='dashed')","95","pl.clabel(cs, fontsize=11)","97","cs = pl.contour(x1, x2, y_prob, [0.334], colors='r',","98","                linestyles='solid')","99","pl.clabel(cs, fontsize=11)","101","pl.show()"]}],"benchmarks\/bench_glm.py":[{"add":["14","    import matplotlib.pyplot as plt","48","    plt.figure('scikit-learn GLM benchmark results')","49","    plt.xlabel('Dimensions')","50","    plt.ylabel('Time (s)')","51","    plt.plot(dimensions, time_ridge, color='r')","52","    plt.plot(dimensions, time_ols, color='g')","53","    plt.plot(dimensions, time_lasso, color='b')","55","    plt.legend(['Ridge', 'OLS', 'LassoLars'], loc='upper left')","56","    plt.axis('tight')","57","    plt.show()"],"delete":["14","    import pylab as pl","48","    pl.figure('scikit-learn GLM benchmark results')","49","    pl.xlabel('Dimensions')","50","    pl.ylabel('Time (s)')","51","    pl.plot(dimensions, time_ridge, color='r')","52","    pl.plot(dimensions, time_ols, color='g')","53","    pl.plot(dimensions, time_lasso, color='b')","55","    pl.legend(['Ridge', 'OLS', 'LassoLars'], loc='upper left')","56","    pl.axis('tight')","57","    pl.show()"]}],"doc\/tutorial\/text_analytics\/skeletons\/exercise_01_language_train_model.py":[{"add":["48","#import matplotlib.pyplot as plt","49","#plt.matshow(cm, cmap=plt.cm.jet)","50","#plt.show()"],"delete":["48","#import pylab as pl","49","#pl.matshow(cm, cmap=pl.cm.jet)","50","#pl.show()"]}],"benchmarks\/bench_glmnet.py":[{"add":["49","    # Delayed import of matplotlib.pyplot","50","    import matplotlib.pyplot as plt","78","    plt.clf()","80","    plt.title('Lasso regression on sample dataset (%d features)' % n_features)","81","    plt.plot(xx, scikit_results, 'b-', label='scikit-learn')","82","    plt.plot(xx, glmnet_results, 'r-', label='glmnet')","83","    plt.legend()","84","    plt.xlabel('number of samples to classify')","85","    plt.ylabel('Time (s)')","86","    plt.show()","119","    plt.figure('scikit-learn vs. glmnet benchmark results')","120","    plt.title('Regression in high dimensional spaces (%d samples)' % n_samples)","121","    plt.plot(xx, scikit_results, 'b-', label='scikit-learn')","122","    plt.plot(xx, glmnet_results, 'r-', label='glmnet')","123","    plt.legend()","124","    plt.xlabel('number of features')","125","    plt.ylabel('Time (s)')","126","    plt.axis('tight')","127","    plt.show()"],"delete":["49","    # Delayed import of pylab","50","    import pylab as pl","78","    pl.clf()","80","    pl.title('Lasso regression on sample dataset (%d features)' % n_features)","81","    pl.plot(xx, scikit_results, 'b-', label='scikit-learn')","82","    pl.plot(xx, glmnet_results, 'r-', label='glmnet')","83","    pl.legend()","84","    pl.xlabel('number of samples to classify')","85","    pl.ylabel('Time (s)')","86","    pl.show()","119","    pl.figure('scikit-learn vs. glmnet benchmark results')","120","    pl.title('Regression in high dimensional spaces (%d samples)' % n_samples)","121","    pl.plot(xx, scikit_results, 'b-', label='scikit-learn')","122","    pl.plot(xx, glmnet_results, 'r-', label='glmnet')","123","    pl.legend()","124","    pl.xlabel('number of features')","125","    pl.ylabel('Time (s)')","126","    pl.axis('tight')","127","    pl.show()"]}],"sklearn\/ensemble\/partial_dependence.py":[{"add":["210","        Dict with keywords passed to the ``matplotlib.pyplot.plot`` call.","213","        Dict with keywords passed to the ``matplotlib.pyplot.plot`` call."],"delete":["210","        Dict with keywords passed to the ``pylab.plot`` call.","213","        Dict with keywords passed to the ``pylab.plot`` call."]}],"examples\/text\/mlcomp_sparse_document_classification.py":[{"add":["46","import matplotlib.pyplot as pl"],"delete":["46","import pylab as pl"]}]}},"3b4ae2845aeb9f02059b1a345f46a03312123201":{"changes":{"build_tools\/circle\/build_doc.sh":"MODIFY"},"diff":{"build_tools\/circle\/build_doc.sh":[{"add":["47","  cython nose coverage matplotlib sphinx pillow","48","source activate testenv"],"delete":["47","  cython nose coverage matplotlib sphinx pillow psutil","48","source \/home\/ubuntu\/miniconda\/envs\/testenv\/bin\/activate testenv"]}]}},"ccefc2ec3284d1424e574858621ce9fa76a296fe":{"changes":{"sklearn\/base.py":"MODIFY","sklearn\/tests\/test_base.py":"MODIFY"},"diff":{"sklearn\/base.py":[{"add":["23","def _first_and_last_element(arr):","24","    \"\"\"Returns first and last element of numpy array or sparse matrix.\"\"\"","25","    if isinstance(arr, np.ndarray) or hasattr(arr, 'data'):","26","        # numpy array or sparse matrix with .data attribute","27","        data = arr.data if sparse.issparse(arr) else arr","28","        return data.flat[0], data.flat[-1]","29","    else:","30","        # Sparse matrices without .data attribute. Only dok_matrix at","31","        # the time of writing, in this case indexing is fast","32","        return arr[0, 0], arr[-1, -1]","33","","34","","87","                    and (_first_and_last_element(param1) ==","88","                         _first_and_last_element(param2))","105","                    and (_first_and_last_element(param1) ==","106","                         _first_and_last_element(param2))"],"delete":["75","                    # We have to use '.flat' for 2D arrays","76","                    and param1.flat[0] == param2.flat[0]","77","                    and param1.flat[-1] == param2.flat[-1]","94","                    and param1.data[0] == param2.data[0]","95","                    and param1.data[-1] == param2.data[-1]"]}],"sklearn\/tests\/test_base.py":[{"add":["3","import sys","4","","147","def test_clone_sparse_matrices():","148","    sparse_matrix_classes = [","149","        getattr(sp, name)","150","        for name in dir(sp) if name.endswith('_matrix')]","151","","152","    PY26 = sys.version_info[:2] == (2, 6)","153","    if PY26:","154","        # sp.dok_matrix can not be deepcopied in Python 2.6","155","        sparse_matrix_classes.remove(sp.dok_matrix)","156","","157","    for cls in sparse_matrix_classes:","158","        sparse_matrix = cls(np.eye(5))","159","        clf = MyEstimator(empty=sparse_matrix)","160","        clf_cloned = clone(clf)","161","        assert_true(clf.empty.__class__ is clf_cloned.empty.__class__)","162","        assert_array_equal(clf.empty.toarray(), clf_cloned.empty.toarray())","163","","164",""],"delete":[]}]}},"f485fce970727d99b7dc55273054eec9a56d032a":{"changes":{"sklearn\/linear_model\/ridge.py":"MODIFY"},"diff":{"sklearn\/linear_model\/ridge.py":[{"add":["250","          scipy.sparse.linalg.lsqr. It is the fastest but may not be available"],"delete":["250","          scipy.sparse.linalg.lsqr. It is the fatest but may not be available"]}]}},"ed7af6632c50e8c0637b07095de21a2a1ade2b9a":{"changes":{"doc\/developers\/advanced_installation.rst":"MODIFY"},"diff":{"doc\/developers\/advanced_installation.rst":[{"add":["141","download the source package from","142","`pypi <https:\/\/pypi.python.org\/pypi\/scikit-learn>`_, unpack the sources and","143","cd into the source directory.","145","This packages uses distutils, which is the default way of installing","185","Third party distributions of scikit-learn"],"delete":["141","download the source package from ","142","`pypi <https:\/\/pypi.python.org\/pypi\/scikit-learn>`_,","143",", unpack the sources and cd into the source directory.","145","this packages uses distutils, which is the default way of installing","185","third party distributions of scikit-learn"]}]}},"acf8368ca3b426e504c00d1df3c2d51b7fa89c65":{"changes":{"doc\/faq.rst":"MODIFY"},"diff":{"doc\/faq.rst":[{"add":["252","Why is there no support for deep or reinforcement learning \/ Will there be support for deep or reinforcement learning in scikit-learn?","253","--------------------------------------------------------------------------------------------------------------------------------------","254","Deep learning and reinforcement learning both require a rich vocabulary to","255","define an architecture, with deep learning additionally requiring","256","GPUs for efficient computing. However, neither of these fit within","257","the design constraints of scikit-learn; as a result, deep learning","258","and reinforcement learning are currently out of scope for what","259","scikit-learn seeks to achieve."],"delete":["252","Why is there no support for deep learning \/ Will there be support for deep learning in scikit-learn?","253","----------------------------------------------------------------------------------------------------","254","Deep learning requires a rich vocabulary to define an architecture and the","255","use of GPUs for efficient computing. However, neither of these fit within","256","the design constraints of scikit-learn. As a result, deep learning is","257","currently out of scope for what scikit-learn seeks to achieve."]}]}},"36e67a91af1d63209625d6c36f484bec756310ed":{"changes":{"sklearn\/linear_model\/tests\/test_least_angle.py":"MODIFY","sklearn\/decomposition\/tests\/test_truncated_svd.py":"MODIFY","sklearn\/linear_model\/tests\/test_sgd.py":"MODIFY","sklearn\/ensemble\/tests\/test_gradient_boosting.py":"MODIFY"},"diff":{"sklearn\/linear_model\/tests\/test_least_angle.py":[{"add":["247","","248","    for alpha_min in alphas_min:","250","                                                       alpha_min=alpha_min)","258","    for alpha_min in alphas_min:","260","                                                       alpha_min=alpha_min)"],"delete":["247","    for alphas_min in alphas_min:","249","                                                       alpha_min=0.9)","256","    alphas_min = [10, 0.9, 1e-4]","258","    for alphas_min in alphas_min:","260","                                                       alpha_min=0.9)"]}],"sklearn\/decomposition\/tests\/test_truncated_svd.py":[{"add":["66","        tsvd = TruncatedSVD(n_components=52, random_state=42, algorithm=algo)"],"delete":["66","        tsvd = TruncatedSVD(n_components=52, random_state=42)"]}],"sklearn\/linear_model\/tests\/test_sgd.py":[{"add":["479","            clf = self.factory(loss=loss, alpha=0.01, n_iter=10)"],"delete":["479","            clf = self.factory(loss=\"modified_huber\", alpha=0.01, n_iter=10)"]}],"sklearn\/ensemble\/tests\/test_gradient_boosting.py":[{"add":["1029","        gb = GradientBoostingClassifier(n_estimators=5, loss=loss)"],"delete":["1029","        gb = GradientBoostingClassifier(n_estimators=5)"]}]}},"3250f15967d2c6a8360316246361c8b4b9775468":{"changes":{"doc\/themes\/scikit-learn\/static\/nature.css_t":"MODIFY"},"diff":{"doc\/themes\/scikit-learn\/static\/nature.css_t":[{"add":["118","        z-index: 10;","140","        z-index: 9;","1190","  font-size: 8px;","1269","  padding-bottom: 5px;"],"delete":["1199","a.btn.dropdown-toggle,  a.btn.dropdown-toggle:hover{","1200","  vertical-align: baseline;","1201","}","1202","","1270","  padding-bottom: 8px;"]}]}},"d9b8ece02c66d7d48179a2c04326b8e027fbebf1":{"changes":{"sklearn\/linear_model\/coordinate_descent.py":"MODIFY"},"diff":{"sklearn\/linear_model\/coordinate_descent.py":[{"add":["643","        check_input : boolean, (default=True)","644","            Allow to bypass several input checking.","645","            Don't use this parameter unless you know what you do.","646",""],"delete":[]}]}},"b8be0198b6b66a67721a19d247a8343486578b73":{"changes":{"sklearn\/datasets\/base.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/datasets\/tests\/test_base.py":"MODIFY"},"diff":{"sklearn\/datasets\/base.py":[{"add":["244","def load_iris(return_X_y=False):","260","    Parameters","261","    ----------","262","    ","263","        .. versionadded:: 0.18","264","     ","265","    return_X_y : boolean, default=False.","266","        If True, returns ``(data, target)`` instead of a Bunch object.","267","        See below for more information about the `data` and `target` object.","268","","278","    (data, target) : tuple if ``return_X_y`` is True","279","","309","    if return_X_y:","310","        return data, target","311",""],"delete":["244","def load_iris():"]}],"doc\/whats_new.rst":[{"add":["232","   - Added new return type ``(data, target)`` : tuple option to :func:`load_iris` dataset. (`#7049 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/7049>`_) ","233","     By `Manvendra Singh`_ and `Nelson Liu`_.   ","234","","4312","","4313",".. _Manvendra Singh: https:\/\/github.com\/manu-chroma"],"delete":[]}],"sklearn\/datasets\/tests\/test_base.py":[{"add":["28","from sklearn.utils.testing import assert_array_equal","183","    # test return_X_y option","184","    X_y_tuple = load_iris(return_X_y=True)","185","    bunch = load_iris()","186","    assert_true(isinstance(X_y_tuple, tuple))","187","    assert_array_equal(X_y_tuple[0], bunch.data)","188","    assert_array_equal(X_y_tuple[1], bunch.target)","189",""],"delete":[]}]}}}