{"725ff4a2eeef171cb6b01b568faf23d170566f94":{"changes":{"sklearn\/cluster\/hierarchical.py":"MODIFY"},"diff":{"sklearn\/cluster\/hierarchical.py":[{"add":["743","            # Reassign cluster numbers"],"delete":["743","            # Reasign cluster numbers"]}]}},"979591643397d78fcf27c51e2fbe6493d527a42f":{"changes":{"sklearn\/linear_model\/least_angle.py":"MODIFY"},"diff":{"sklearn\/linear_model\/least_angle.py":[{"add":["65","        ..). Only coefficients up to the smallest alpha value (``alphas_[alphas_ >"],"delete":["65","        ..). Only coeffiencts up to the smallest alpha value (``alphas_[alphas_ >"]}]}},"b32897f1c32ad184cc84634567d87eab83c1e1eb":{"changes":{"sklearn\/linear_model\/tests\/test_ridge.py":"MODIFY","sklearn\/linear_model\/ridge.py":"MODIFY"},"diff":{"sklearn\/linear_model\/tests\/test_ridge.py":[{"add":["299","    fit_intercept = filter_ == DENSE_FILTER","300","    if fit_intercept:","301","        X_diabetes_ = X_diabetes - X_diabetes.mean(0)","302","    else:","303","        X_diabetes_ = X_diabetes","304","    ridge_gcv = _RidgeGCV(fit_intercept=fit_intercept)","305","    ridge = Ridge(alpha=1.0, fit_intercept=fit_intercept)","306","","307","    # because fit_intercept is applied","310","    decomp = ridge_gcv._pre_compute(X_diabetes_, y_diabetes, fit_intercept)","319","        X_new = X_diabetes_[sel]","322","        value = ridge.predict([X_diabetes_[i]])[0]","333","    decomp = ridge_gcv._pre_compute_svd(X_diabetes_, y_diabetes, fit_intercept)"],"delete":["299","    ridge_gcv = _RidgeGCV(fit_intercept=False)","300","    ridge = Ridge(alpha=1.0, fit_intercept=False)","303","    decomp = ridge_gcv._pre_compute(X_diabetes, y_diabetes)","312","        X_new = X_diabetes[sel]","315","        value = ridge.predict([X_diabetes[i]])[0]","326","    decomp = ridge_gcv._pre_compute_svd(X_diabetes, y_diabetes)"]}],"sklearn\/linear_model\/ridge.py":[{"add":["852","    def _pre_compute(self, X, y, centered_kernel=True):","855","        # the following emulates an additional constant regressor","856","        # corresponding to fit_intercept=True","857","        # but this is done only when the features have been centered","858","        if centered_kernel:","859","            K += np.ones_like(K)","883","        w = 1. \/ (v + alpha)","884","        constant_column = np.var(Q, 0) < 1.e-12","885","        # detect constant columns","886","        w[constant_column] = 0  # cancel the regularization for the intercept","887","        w[v == 0] = 0","903","    def _pre_compute_svd(self, X, y, centered_kernel=True):","906","        if centered_kernel:","907","            X = np.hstack((X, np.ones((X.shape[0], 1))))","908","        # to emulate fit_intercept=True situation, add a column on ones","909","        # Note that by centering, the other columns are orthogonal to that one","919","        constant_column = np.var(U, 0) < 1.e-12","920","        # detect columns colinear to ones","922","        w[constant_column] = - (alpha ** -1)","923","        # cancel the regularization for the intercept","991","        if sample_weight is not None:","992","            X, y = _rescale_data(X, y, sample_weight)","993","","994","        centered_kernel = not sparse.issparse(X) and self.fit_intercept","995","","996","        v, Q, QT_y = _pre_compute(X, y, centered_kernel)","1006","                out, c = _errors(alpha, y, v, Q, QT_y)","1008","                out, c = _values(alpha, y, v, Q, QT_y)","1092","                              parameters, fit_params=fit_params, cv=self.cv,","1093","                              scoring=self.scoring)"],"delete":["852","    def _pre_compute(self, X, y):","878","        w = 1.0 \/ (v + alpha)","894","    def _pre_compute_svd(self, X, y):","942","","975","        v, Q, QT_y = _pre_compute(X, y)","984","            weighted_alpha = (sample_weight * alpha","985","                              if sample_weight is not None","986","                              else alpha)","988","                out, c = _errors(weighted_alpha, y, v, Q, QT_y)","990","                out, c = _values(weighted_alpha, y, v, Q, QT_y)","1074","                              parameters, fit_params=fit_params, cv=self.cv)"]}]}},"b444cc9c6457590b33b365185b3eafb0d312b9be":{"changes":{"sklearn\/model_selection\/_validation.py":"MODIFY","sklearn\/model_selection\/tests\/test_search.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/model_selection\/_search.py":"MODIFY"},"diff":{"sklearn\/model_selection\/_validation.py":[{"add":["0","","145","                   return_parameters=False, return_n_test_samples=False,","146","                   return_times=False, error_score='raise'):","203","    fit_time : float","204","        Time spent for fitting in seconds.","205","","206","    score_time : float","207","        Time spent for scoring in seconds.","240","        # Note fit time as time until error","241","        fit_time = time.time() - start_time","242","        score_time = 0.0","258","        fit_time = time.time() - start_time","260","        score_time = time.time() - start_time - fit_time","267","        end_msg = \"%s -%s\" % (msg, logger.short_format_time(score_time))","270","    ret = [train_score, test_score] if return_train_score else [test_score]","271","","272","    if return_n_test_samples:","273","        ret.append(_num_samples(X_test))","274","    if return_times:","275","        ret.extend([fit_time, score_time])","772","        out = np.array(out)","955","    out = np.asarray(out)"],"delete":["144","                   return_parameters=False, error_score='raise'):","201","    scoring_time : float","202","        Time spent for fitting and scoring in seconds.","254","    scoring_time = time.time() - start_time","255","","259","        end_msg = \"%s -%s\" % (msg, logger.short_format_time(scoring_time))","262","    ret = [train_score] if return_train_score else []","263","    ret.extend([test_score, _num_samples(X_test), scoring_time])","760","        out = np.array(out)[:, :2]","943","    out = np.asarray(out)[:, :2]"]}],"sklearn\/model_selection\/tests\/test_search.py":[{"add":["597","def check_cv_results_array_types(cv_results, param_keys, score_keys):","598","    # Check if the search `cv_results`'s array are of correct types","599","    assert_true(all(isinstance(cv_results[param], np.ma.MaskedArray)","601","    assert_true(all(cv_results[key].dtype == object for key in param_keys))","602","    assert_false(any(isinstance(cv_results[key], np.ma.MaskedArray)","604","    assert_true(all(cv_results[key].dtype == np.float64","605","                    for key in score_keys if not key.startswith('rank')))","606","    assert_true(cv_results['rank_test_score'].dtype == np.int32)","609","def check_cv_results_keys(cv_results, param_keys, score_keys, n_cand):","611","    assert_array_equal(sorted(cv_results.keys()),","613","    assert_true(all(cv_results[key].shape == (n_cand,)","619","    cv_results = search.cv_results_","620","    res_scores = np.vstack(list([cv_results[\"split%d_test_score\" % i]","622","    res_means = cv_results[\"mean_test_score\"]","623","    res_params = cv_results[\"params\"]","636","def test_grid_search_cv_results():","652","    score_keys = ('mean_test_score', 'mean_train_score',","653","                  'rank_test_score',","655","                  'split2_test_score',","656","                  'split0_train_score', 'split1_train_score',","657","                  'split2_train_score',","658","                  'std_test_score', 'std_train_score',","659","                  'mean_fit_time', 'std_fit_time',","660","                  'mean_score_time', 'std_score_time')","665","        cv_results = search.cv_results_","666","        # Check if score and timing are reasonable","667","        assert_true(all(cv_results['rank_test_score'] >= 1))","668","        assert_true(all(cv_results[k] >= 0) for k in score_keys","669","                    if k is not 'rank_test_score')","670","        assert_true(all(cv_results[k] <= 1) for k in score_keys","671","                    if 'time' not in k and","672","                    k is not 'rank_test_score')","673","        # Check cv_results structure","674","        check_cv_results_array_types(cv_results, param_keys, score_keys)","675","        check_cv_results_keys(cv_results, param_keys, score_keys, n_candidates)","677","        cv_results = grid_search.cv_results_","679","        assert_true(all((cv_results['param_C'].mask[i] and","680","                         cv_results['param_gamma'].mask[i] and","681","                         not cv_results['param_degree'].mask[i])","683","                        if cv_results['param_kernel'][i] == 'linear'))","684","        assert_true(all((not cv_results['param_C'].mask[i] and","685","                         not cv_results['param_gamma'].mask[i] and","686","                         cv_results['param_degree'].mask[i])","688","                        if cv_results['param_kernel'][i] == 'rbf'))","692","def test_random_search_cv_results():","705","                                       cv=n_splits, iid=False,","706","                                       param_distributions=params)","714","    score_keys = ('mean_test_score', 'mean_train_score',","715","                  'rank_test_score',","717","                  'split2_test_score',","718","                  'split0_train_score', 'split1_train_score',","719","                  'split2_train_score',","720","                  'std_test_score', 'std_train_score',","721","                  'mean_fit_time', 'std_fit_time',","722","                  'mean_score_time', 'std_score_time')","727","        cv_results = search.cv_results_","729","        check_cv_results_array_types(cv_results, param_keys, score_keys)","730","        check_cv_results_keys(cv_results, param_keys, score_keys, n_cand)","732","        assert_false(any(cv_results['param_C'].mask) or","733","                     any(cv_results['param_gamma'].mask))","760","        test_cv_scores = np.array(list(search.cv_results_['split%d_test_score'","761","                                                          % s_i][0]","762","                                       for s_i in range(search.n_splits_)))","763","        train_cv_scores = np.array(list(search.cv_results_['split%d_train_'","764","                                                           'score' % s_i][0]","765","                                        for s_i in range(search.n_splits_)))","766","        test_mean = search.cv_results_['mean_test_score'][0]","767","        test_std = search.cv_results_['std_test_score'][0]","769","        train_cv_scores = np.array(list(search.cv_results_['split%d_train_'","770","                                                           'score' % s_i][0]","771","                                        for s_i in range(search.n_splits_)))","772","        train_mean = search.cv_results_['mean_train_score'][0]","773","        train_std = search.cv_results_['std_train_score'][0]","774","","775","        # Test the first candidate","777","        assert_array_almost_equal(test_cv_scores, [1, 1. \/ 3.])","778","        assert_array_almost_equal(train_cv_scores, [1, 1])","779","","782","        expected_test_mean = 1 * 1. \/ 4. + 1. \/ 3. * 3. \/ 4.","783","        expected_test_std = np.sqrt(1. \/ 4 * (expected_test_mean - 1) ** 2 +","784","                                    3. \/ 4 * (expected_test_mean - 1. \/ 3.) **","785","                                    2)","786","        assert_almost_equal(test_mean, expected_test_mean)","787","        assert_almost_equal(test_std, expected_test_std)","788","","789","        # For the train scores, we do not take a weighted mean irrespective of","790","        # i.i.d. or not","791","        assert_almost_equal(train_mean, 1)","792","        assert_almost_equal(train_std, 0)","806","        test_cv_scores = np.array(list(search.cv_results_['split%d_test_score'","807","                                                          % s][0]","808","                                       for s in range(search.n_splits_)))","809","        test_mean = search.cv_results_['mean_test_score'][0]","810","        test_std = search.cv_results_['std_test_score'][0]","811","","812","        train_cv_scores = np.array(list(search.cv_results_['split%d_train_'","813","                                                           'score' % s][0]","814","                                        for s in range(search.n_splits_)))","815","        train_mean = search.cv_results_['mean_train_score'][0]","816","        train_std = search.cv_results_['std_train_score'][0]","817","","820","        assert_array_almost_equal(test_cv_scores, [1, 1. \/ 3.])","822","        assert_almost_equal(test_mean, np.mean(test_cv_scores))","823","        assert_almost_equal(test_std, np.std(test_cv_scores))","824","","825","        # For the train scores, we do not take a weighted mean irrespective of","826","        # i.i.d. or not","827","        assert_almost_equal(train_mean, 1)","828","        assert_almost_equal(train_std, 0)","844","        cv_results = search.cv_results_","848","        assert_almost_equal(cv_results['mean_test_score'][0],","849","                            cv_results['mean_test_score'][1])","850","        assert_almost_equal(cv_results['mean_train_score'][0],","851","                            cv_results['mean_train_score'][1])","853","            assert_almost_equal(cv_results['mean_test_score'][1],","854","                                cv_results['mean_test_score'][2])","855","        except AssertionError:","856","            pass","857","        try:","858","            assert_almost_equal(cv_results['mean_train_score'][1],","859","                                cv_results['mean_train_score'][2])","878","@ignore_warnings()","879","def test_search_cv_timing():","880","    svc = LinearSVC(random_state=0)","881","","882","    X = [[1, ], [2, ], [3, ], [4, ]]","883","    y = [0, 1, 1, 0]","884","","885","    gs = GridSearchCV(svc, {'C': [0, 1]}, cv=2, error_score=0)","886","    rs = RandomizedSearchCV(svc, {'C': [0, 1]}, cv=2, error_score=0, n_iter=2)","887","","888","    for search in (gs, rs):","889","        search.fit(X, y)","890","        for key in ['mean_fit_time', 'std_fit_time']:","891","            # NOTE The precision of time.time in windows is not high","892","            # enough for the fit\/score times to be non-zero for trivial X and y","893","            assert_true(np.all(search.cv_results_[key] >= 0))","894","            assert_true(np.all(search.cv_results_[key] < 1))","895","","896","        for key in ['mean_score_time', 'std_score_time']:","897","            assert_true(search.cv_results_[key][1] >= 0)","898","            assert_true(search.cv_results_[key][0] == 0.0)","899","            assert_true(np.all(search.cv_results_[key] < 1))","900","","901","","910","        cv_results = grid_search.fit(X, y).cv_results_","913","        result_keys = list(cv_results.keys())","1120","    X = np.arange(24).reshape(6, -1)","1121","    y = [0, 0, 0, 1, 1, 1]"],"delete":["597","def check_cv_results_array_types(results, param_keys, score_keys):","598","    # Check if the search results' array are of correct types","599","    assert_true(all(isinstance(results[param], np.ma.MaskedArray)","601","    assert_true(all(results[key].dtype == object for key in param_keys))","602","    assert_false(any(isinstance(results[key], np.ma.MaskedArray)","604","    assert_true(all(results[key].dtype == np.float64","605","                    for key in score_keys if key != 'rank_test_score'))","606","    assert_true(results['rank_test_score'].dtype == np.int32)","609","def check_cv_results_keys(results, param_keys, score_keys, n_cand):","611","    assert_array_equal(sorted(results.keys()),","613","    assert_true(all(results[key].shape == (n_cand,)","619","    results = search.cv_results_","620","    res_scores = np.vstack(list([results[\"split%d_test_score\" % i]","622","    res_means = results[\"mean_test_score\"]","623","    res_params = results[\"params\"]","636","def test_grid_search_results():","652","    score_keys = ('mean_test_score', 'rank_test_score',","654","                  'split2_test_score', 'std_test_score')","659","        results = search.cv_results_","660","        # Check results structure","661","        check_cv_results_array_types(results, param_keys, score_keys)","662","        check_cv_results_keys(results, param_keys, score_keys, n_candidates)","664","        results = grid_search.cv_results_","666","        assert_true(all((results['param_C'].mask[i] and","667","                         results['param_gamma'].mask[i] and","668","                         not results['param_degree'].mask[i])","670","                        if results['param_kernel'][i] == 'linear'))","671","        assert_true(all((not results['param_C'].mask[i] and","672","                         not results['param_gamma'].mask[i] and","673","                         results['param_degree'].mask[i])","675","                        if results['param_kernel'][i] == 'rbf'))","679","def test_random_search_results():","692","                                       cv=n_splits,","693","                                       iid=False, param_distributions=params)","701","    score_keys = ('mean_test_score', 'rank_test_score',","703","                  'split2_test_score', 'std_test_score')","708","        results = search.cv_results_","710","        check_cv_results_array_types(results, param_keys, score_keys)","711","        check_cv_results_keys(results, param_keys, score_keys, n_cand)","713","        assert_false(any(results['param_C'].mask) or","714","                     any(results['param_gamma'].mask))","741","        # Test the first candidate","742","        cv_scores = np.array(list(search.cv_results_['split%d_test_score'","743","                                                     % s][0]","744","                                  for s in range(search.n_splits_)))","745","        mean = search.cv_results_['mean_test_score'][0]","746","        std = search.cv_results_['std_test_score'][0]","749","        assert_array_almost_equal(cv_scores, [1, 1. \/ 3.])","752","        expected_mean = 1 * 1. \/ 4. + 1. \/ 3. * 3. \/ 4.","753","        expected_std = np.sqrt(1. \/ 4 * (expected_mean - 1) ** 2 +","754","                               3. \/ 4 * (expected_mean - 1. \/ 3.) ** 2)","755","        assert_almost_equal(mean, expected_mean)","756","        assert_almost_equal(std, expected_std)","770","        cv_scores = np.array(list(search.cv_results_['split%d_test_score'","771","                                                     % s][0]","772","                                  for s in range(search.n_splits_)))","773","        mean = search.cv_results_['mean_test_score'][0]","774","        std = search.cv_results_['std_test_score'][0]","777","        assert_array_almost_equal(cv_scores, [1, 1. \/ 3.])","779","        assert_almost_equal(mean, np.mean(cv_scores))","780","        assert_almost_equal(std, np.std(cv_scores))","796","        results = search.cv_results_","800","        assert_almost_equal(results['mean_test_score'][0],","801","                            results['mean_test_score'][1])","803","            assert_almost_equal(results['mean_test_score'][1],","804","                                results['mean_test_score'][2])","831","        results = grid_search.fit(X, y).cv_results_","834","        result_keys = list(results.keys())","1041","    X = np.arange(20).reshape(5, -1)","1042","    y = [0, 0, 1, 1, 1]"]}],"doc\/whats_new.rst":[{"add":["118","  - Training scores and Timing information","119","","120","    ``cv_results_`` also includes the training scores for each","121","    cross-validation split (with keys such as ``'split0_train_score'``), as","122","    well as their mean (``'mean_train_score'``) and standard deviation","123","    (``'std_train_score'``). To avoid the cost of evaluating training score,","124","    set ``return_train_score=False``.","125","","126","    Additionally the mean and standard deviation of the times taken to split,","127","    train and score the model across all the cross-validation splits is","128","    available at the key ``'mean_time'`` and ``'std_time'`` respectively.","129","","130","Changelog","131","---------","378","   - The training scores and time taken for training followed by scoring for","379","     each search candidate are now available at the ``cv_results_`` dict.","380","     See :ref:`model_selection_changes` for more information.","381","     (`#7324 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/7325>`)","382","     By `Eugene Chen`_ and `Raghav RV`_.","383","","4753","","4754",".. _Eugene Chen: https:\/\/github.com\/eyc88"],"delete":[]}],"sklearn\/model_selection\/_search.py":[{"add":["321","                                              fit_params=fit_params,","322","                                              return_n_test_samples=True,","323","                                              error_score=error_score)","378","                 error_score='raise', return_train_score=True):","390","        self.return_train_score = return_train_score","556","                                  fit_params=self.fit_params,","557","                                  return_train_score=self.return_train_score,","558","                                  return_n_test_samples=True,","559","                                  return_times=True, return_parameters=True,","564","        # if one choose to see train score, \"out\" will contain train score info","565","        if self.return_train_score:","566","            (train_scores, test_scores, test_sample_counts,","567","             fit_time, score_time, parameters) = zip(*out)","568","        else:","569","            (test_scores, test_sample_counts,","570","             fit_time, score_time, parameters) = zip(*out)","575","        results = dict()","576","","577","        def _store(key_name, array, weights=None, splits=False, rank=False):","578","            \"\"\"A small helper to store the scores\/times to the cv_results_\"\"\"","579","            array = np.array(array, dtype=np.float64).reshape(n_candidates,","580","                                                              n_splits)","581","            if splits:","582","                for split_i in range(n_splits):","583","                    results[\"split%d_%s\"","584","                            % (split_i, key_name)] = array[:, split_i]","585","","586","            array_means = np.average(array, axis=1, weights=weights)","587","            results['mean_%s' % key_name] = array_means","588","            # Weighted std is not directly available in numpy","589","            array_stds = np.sqrt(np.average((array -","590","                                             array_means[:, np.newaxis]) ** 2,","591","                                            axis=1, weights=weights))","592","            results['std_%s' % key_name] = array_stds","593","","594","            if rank:","595","                results[\"rank_%s\" % key_name] = np.asarray(","596","                    rankdata(-array_means, method='min'), dtype=np.int32)","597","","598","        # Computed the (weighted) mean and std for test scores alone","603","        _store('test_score', test_scores, splits=True, rank=True,","604","               weights=test_sample_counts if self.iid else None)","605","        _store('train_score', train_scores, splits=True)","606","        _store('fit_time', fit_time)","607","        _store('score_time', score_time)","609","        best_index = np.flatnonzero(results[\"rank_test_score\"] == 1)[0]","624","        results.update(param_results)","627","        results['params'] = candidate_params","629","        self.cv_results_ = results","771","    return_train_score : boolean, default=True","772","        If ``'False'``, the ``cv_results_`` attribute will not include training","773","        scores.","774","","793","           param_grid=..., pre_dispatch=..., refit=..., return_train_score=...,","797","    ['mean_fit_time', 'mean_score_time', 'mean_test_score',...","798","     'mean_train_score', 'param_C', 'param_kernel', 'params',...","799","     'rank_test_score', 'split0_test_score',...","800","     'split0_train_score', 'split1_test_score', 'split1_train_score',...","801","     'split2_test_score', 'split2_train_score',...","802","     'std_fit_time', 'std_score_time', 'std_test_score', 'std_train_score'...]","833","            'split0_test_score'  : [0.8, 0.7, 0.8, 0.9],","834","            'split1_test_score'  : [0.82, 0.5, 0.7, 0.78],","835","            'mean_test_score'    : [0.81, 0.60, 0.75, 0.82],","836","            'std_test_score'     : [0.02, 0.01, 0.03, 0.03],","837","            'rank_test_score'    : [2, 4, 3, 1],","838","            'split0_train_score' : [0.8, 0.9, 0.7],","839","            'split1_train_score' : [0.82, 0.5, 0.7],","840","            'mean_train_score'   : [0.81, 0.7, 0.7],","841","            'std_train_score'    : [0.03, 0.03, 0.04],","842","            'mean_fit_time'      : [0.73, 0.63, 0.43, 0.49],","843","            'std_fit_time'       : [0.01, 0.02, 0.01, 0.01],","844","            'mean_score_time'    : [0.007, 0.06, 0.04, 0.04],","845","            'std_score_time'     : [0.001, 0.002, 0.003, 0.005],","846","            'params'             : [{'kernel': 'poly', 'degree': 2}, ...],","852","        The ``mean_fit_time``, ``std_fit_time``, ``mean_score_time`` and","853","        ``std_score_time`` are all in seconds.","854","","911","                 pre_dispatch='2*n_jobs', error_score='raise',","912","                 return_train_score=True):","916","            pre_dispatch=pre_dispatch, error_score=error_score,","917","            return_train_score=return_train_score)","1051","    return_train_score : boolean, default=True","1052","        If ``'False'``, the ``cv_results_`` attribute will not include training","1053","        scores.","1054","","1079","            'split0_test_score'  : [0.8, 0.9, 0.7],","1080","            'split1_test_score'  : [0.82, 0.5, 0.7],","1081","            'mean_test_score'    : [0.81, 0.7, 0.7],","1082","            'std_test_score'     : [0.02, 0.2, 0.],","1083","            'rank_test_score'    : [3, 1, 1],","1084","            'split0_train_score' : [0.8, 0.9, 0.7],","1085","            'split1_train_score' : [0.82, 0.5, 0.7],","1086","            'mean_train_score'   : [0.81, 0.7, 0.7],","1087","            'std_train_score'    : [0.03, 0.03, 0.04],","1088","            'mean_fit_time'      : [0.73, 0.63, 0.43, 0.49],","1089","            'std_fit_time'       : [0.01, 0.02, 0.01, 0.01],","1090","            'mean_score_time'    : [0.007, 0.06, 0.04, 0.04],","1091","            'std_score_time'     : [0.001, 0.002, 0.003, 0.005],","1098","        The ``mean_fit_time``, ``std_fit_time``, ``mean_score_time`` and","1099","        ``std_score_time`` are all in seconds.","1100","","1154","                 error_score='raise', return_train_score=True):","1159","             estimator=estimator, scoring=scoring, fit_params=fit_params,","1160","             n_jobs=n_jobs, iid=iid, refit=refit, cv=cv, verbose=verbose,","1161","             pre_dispatch=pre_dispatch, error_score=error_score,","1162","             return_train_score=return_train_score)"],"delete":["321","                                              fit_params, error_score)","376","                 error_score='raise'):","553","                                  self.fit_params, return_parameters=True,","558","        test_scores, test_sample_counts, _, parameters = zip(*out)","563","        test_scores = np.array(test_scores,","564","                               dtype=np.float64).reshape(n_candidates,","565","                                                         n_splits)","570","        # Computed the (weighted) mean and std for all the candidates","571","        weights = test_sample_counts if self.iid else None","572","        means = np.average(test_scores, axis=1, weights=weights)","573","        stds = np.sqrt(np.average((test_scores - means[:, np.newaxis]) ** 2,","574","                                  axis=1, weights=weights))","576","        cv_results = dict()","577","        for split_i in range(n_splits):","578","            cv_results[\"split%d_test_score\" % split_i] = test_scores[:,","579","                                                                     split_i]","580","        cv_results[\"mean_test_score\"] = means","581","        cv_results[\"std_test_score\"] = stds","582","","583","        ranks = np.asarray(rankdata(-means, method='min'), dtype=np.int32)","584","","585","        best_index = np.flatnonzero(ranks == 1)[0]","587","        cv_results[\"rank_test_score\"] = ranks","601","        cv_results.update(param_results)","604","        cv_results['params'] = candidate_params","606","        self.cv_results_ = cv_results","766","           param_grid=..., pre_dispatch=..., refit=...,","770","    ['mean_test_score', 'param_C', 'param_kernel', 'params',...","771","     'rank_test_score', 'split0_test_score', 'split1_test_score',...","772","     'split2_test_score', 'std_test_score']","803","            'split0_test_score' : [0.8, 0.7, 0.8, 0.9],","804","            'split1_test_score' : [0.82, 0.5, 0.7, 0.78],","805","            'mean_test_score'   : [0.81, 0.60, 0.75, 0.82],","806","            'std_test_score'    : [0.02, 0.01, 0.03, 0.03],","807","            'rank_test_score'   : [2, 4, 3, 1],","808","            'params'            : [{'kernel': 'poly', 'degree': 2}, ...],","870","                 pre_dispatch='2*n_jobs', error_score='raise'):","874","            pre_dispatch=pre_dispatch, error_score=error_score)","1032","            'split0_test_score' : [0.8, 0.9, 0.7],","1033","            'split1_test_score' : [0.82, 0.5, 0.7],","1034","            'mean_test_score'   : [0.81, 0.7, 0.7],","1035","            'std_test_score'    : [0.02, 0.2, 0.],","1036","            'rank_test_score'   : [3, 1, 1],","1096","                 error_score='raise'):","1097","","1102","            estimator=estimator, scoring=scoring, fit_params=fit_params,","1103","            n_jobs=n_jobs, iid=iid, refit=refit, cv=cv, verbose=verbose,","1104","            pre_dispatch=pre_dispatch, error_score=error_score)"]}]}},"9b25f5d37a78e9a28134b917c2768849f529cd6e":{"changes":{"doc\/developers\/advanced_installation.rst":"MODIFY","doc\/modules\/manifold.rst":"MODIFY","sklearn\/decomposition\/online_lda.py":"MODIFY","doc\/modules\/cross_validation.rst":"MODIFY","sklearn\/random_projection.py":"MODIFY","doc\/developers\/contributing.rst":"MODIFY","doc\/datasets\/olivetti_faces.rst":"MODIFY","AUTHORS.rst":"MODIFY","doc\/modules\/svm.rst":"MODIFY","sklearn\/kernel_approximation.py":"MODIFY","examples\/neighbors\/plot_species_kde.py":"MODIFY","sklearn\/datasets\/species_distributions.py":"MODIFY","doc\/modules\/clustering.rst":"MODIFY","doc\/support.rst":"MODIFY","CONTRIBUTING.md":"MODIFY","sklearn\/datasets\/olivetti_faces.py":"MODIFY","doc\/modules\/feature_selection.rst":"MODIFY","doc\/faq.rst":"MODIFY","doc\/related_projects.rst":"MODIFY","sklearn\/gaussian_process\/gaussian_process.py":"MODIFY","doc\/about.rst":"MODIFY","doc\/modules\/model_persistence.rst":"MODIFY","doc\/modules\/naive_bayes.rst":"MODIFY","doc\/modules\/feature_extraction.rst":"MODIFY","doc\/testimonials\/testimonials.rst":"MODIFY","doc\/tutorial\/statistical_inference\/finding_help.rst":"MODIFY","sklearn\/feature_selection\/mutual_info_.py":"MODIFY","doc\/presentations.rst":"MODIFY","doc\/modules\/tree.rst":"MODIFY","sklearn\/cluster\/birch.py":"MODIFY","sklearn\/linear_model\/least_angle.py":"MODIFY"},"diff":{"doc\/developers\/advanced_installation.rst":[{"add":["170","<https:\/\/pip.readthedocs.io\/en\/stable\/installing\/>`_ utility.","281","`microsoft visual c++ express 2008 <http:\/\/download.microsoft.com\/download\/A\/5\/4\/A54BADB6-9C3F-478D-8657-93B3FC9FE62D\/vcsetup.exe>`_","303","  <https:\/\/www.microsoft.com\/en-us\/download\/details.aspx?id=18950>`_","379","<https:\/\/nose.readthedocs.io\/en\/latest\/>`_ library. after"],"delete":["170","<https:\/\/pip.readthedocs.org\/en\/stable\/installing\/>`_ utility.","281","`microsoft visual c++ express 2008 <http:\/\/go.microsoft.com\/?linkid=7729279>`_","303","  <http:\/\/www.microsoft.com\/en-us\/download\/details.aspx?id=18950>`_","379","<https:\/\/nose.readthedocs.org\/en\/latest\/>`_ library. after"]}],"doc\/modules\/manifold.rst":[{"add":["148","     <http:\/\/science.sciencemag.org\/content\/290\/5500\/2319.full>`_"],"delete":["148","     <http:\/\/www.sciencemag.org\/content\/290\/5500\/2319.full>`_"]}],"sklearn\/decomposition\/online_lda.py":[{"add":["7","Link: http:\/\/matthewdhoffman.com\/code\/onlineldavb.tar","243","        http:\/\/matthewdhoffman.com\/\/code\/onlineldavb.tar"],"delete":["7","Link: http:\/\/www.cs.princeton.edu\/~mdhoffma\/code\/onlineldavb.tar","243","        http:\/\/www.cs.princeton.edu\/~mdhoffma\/code\/onlineldavb.tar"]}],"doc\/modules\/cross_validation.rst":[{"add":["345","   <http:\/\/people.csail.mit.edu\/romer\/papers\/CrossVal_SDM08.pdf>`_, SIAM 2008;"],"delete":["345","   <http:\/\/www.siam.org\/proceedings\/datamining\/2008\/dm08_54_Rao.pdf>`_, SIAM 2008;"]}],"sklearn\/random_projection.py":[{"add":["248","           http:\/\/web.stanford.edu\/~hastie\/Papers\/Ping\/KDD06_rp.pdf","583","           http:\/\/web.stanford.edu\/~hastie\/Papers\/Ping\/KDD06_rp.pdf"],"delete":["248","           http:\/\/www.stanford.edu\/~hastie\/Papers\/Ping\/KDD06_rp.pdf","583","           http:\/\/www.stanford.edu\/~hastie\/Papers\/Ping\/KDD06_rp.pdf"]}],"doc\/developers\/contributing.rst":[{"add":["30","We use `Git <https:\/\/git-scm.com\/>`_ for version control and","52","`the setuptool docs <http:\/\/setuptools.readthedocs.io\/en\/latest\/setuptools.html#development-mode>`_)::","131","`Git documentation <https:\/\/git-scm.com\/documentation>`_ on the web.)","227","   <http:\/\/astropy.readthedocs.io\/en\/latest\/development\/workflow\/development_workflow.html>`_","301","<http:\/\/www.sphinx-doc.org\/en\/stable\/>`_,","303","`pillow <http:\/\/pillow.readthedocs.io\/en\/latest\/>`_.","363","purpose, we use the `nose <http:\/\/nose.readthedocs.io\/en\/latest\/>`_","482","      <https:\/\/divmod.readthedocs.io\/en\/latest\/products\/pyflakes.html>`_ to automatically","491","<https:\/\/gist.github.com\/nateGeorge\/5455d2c57fb33c1ae04706f2dc4fee01>`_."],"delete":["30","We use `Git <http:\/\/git-scm.com\/>`_ for version control and","52","`the setuptool docs <https:\/\/pythonhosted.org\/setuptools\/setuptools.html#development-mode>`_)::","131","`Git documentation <http:\/\/git-scm.com\/documentation>`_ on the web.)","227","   <http:\/\/astropy.readthedocs.org\/en\/latest\/development\/workflow\/development_workflow.html>`_","301","<http:\/\/sphinx-doc.org\/>`_,","303","`pillow <http:\/\/pillow.readthedocs.org\/en\/latest\/>`_.","363","purpose, we use the `nose <http:\/\/nose.readthedocs.org\/en\/latest\/>`_","482","      <http:\/\/www.divmod.org\/trac\/wiki\/DivmodPyflakes>`_ to automatically","491","<https:\/\/svn.enthought.com\/enthought\/browser\/sandbox\/docs\/coding_standard.py>`_."]}],"doc\/datasets\/olivetti_faces.rst":[{"add":["7","`This dataset contains a set of face images`_ taken between April 1992 and April","8","1994 at AT&T Laboratories Cambridge. The","13",".. _This dataset contains a set of face images: http:\/\/www.cl.cam.ac.uk\/research\/dtg\/attarchive\/facedatabase.html"],"delete":["7","This dataset contains a set of face images taken between April 1992 and April","8","1994 at AT&T Laboratories Cambridge. The website describing the original","9","dataset is now defunct, but archived copies can be accessed through","10","`the Internet Archive's Wayback Machine`_. The","15",".. _the Internet Archive's Wayback Machine: http:\/\/wayback.archive.org\/web\/*\/http:\/\/www.uk.research.att.com\/facedatabase.html"]}],"AUTHORS.rst":[{"add":["35","  * `Virgile Fritsch <https:\/\/team.inria.fr\/parietal\/vfritsch\/>`_","40","  * `Olivier Grisel <https:\/\/twitter.com\/ogrisel>`_","43","  * `Brian Holt <http:\/\/personal.ee.surrey.ac.uk\/Personal\/B.Holt\/>`_","49","  * `Wei Li <http:\/\/kuantkid.github.io\/>`_","51","  * `Gilles Louppe <http:\/\/glouppe.github.io\/>`_","59","  * `Fabian Pedregosa <http:\/\/fa.bianp.net\/blog\/>`_","60","  * `Peter Prettenhofer <https:\/\/sites.google.com\/site\/peterprettenhofer\/>`_","62","  * `Jake VanderPlas <http:\/\/staff.washington.edu\/jakevdp\/>`_","64","  * `Gael Varoquaux <http:\/\/gael-varoquaux.info\/>`_"],"delete":["35","  * `Virgile Fritsch <http:\/\/parietal.saclay.inria.fr\/Members\/virgile-fritsch>`_","40","  * `Olivier Grisel <http:\/\/twitter.com\/ogrisel>`_","43","  * `Brian Holt <http:\/\/info.ee.surrey.ac.uk\/Personal\/B.Holt\/>`_","49","  * `Wei Li <http:\/\/kuantkid.github.com>`_","51","  * `Gilles Louppe <http:\/\/www.montefiore.ulg.ac.be\/~glouppe>`_","59","  * `Fabian Pedregosa <http:\/\/fseoane.net\/blog\/>`_","60","  * `Peter Prettenhofer <http:\/\/sites.google.com\/site\/peterprettenhofer\/>`_","62","  * `Jake VanderPlas <http:\/\/www.astro.washington.edu\/users\/vanderplas\/>`_","64","  * `Gael Varoquaux <http:\/\/gael-varoquaux.info\/blog\/>`_"]}],"doc\/modules\/svm.rst":[{"add":["621","   <http:\/\/link.springer.com\/article\/10.1007%2FBF00994018>`_,"],"delete":["621","   <http:\/\/www.springerlink.com\/content\/k238jx04hm87j80g\/>`_,"]}],"sklearn\/kernel_approximation.py":[{"add":["52","    (http:\/\/people.eecs.berkeley.edu\/~brecht\/papers\/08.rah.rec.nips.pdf)"],"delete":["52","    (http:\/\/www.eecs.berkeley.edu\/~brecht\/papers\/08.rah.rec.nips.pdf)"]}],"examples\/neighbors\/plot_species_kde.py":[{"add":["25","   <http:\/\/www.iucnredlist.org\/details\/13408\/0>`_ ,"],"delete":["25","   <http:\/\/www.iucnredlist.org\/apps\/redlist\/details\/13408\/0>`_ ,"]}],"sklearn\/datasets\/species_distributions.py":[{"add":["11","   <http:\/\/www.iucnredlist.org\/details\/3038\/0>`_ ,","15","   <http:\/\/www.iucnredlist.org\/details\/13408\/0>`_ ,","184","      <http:\/\/www.iucnredlist.org\/details\/3038\/0>`_ ,","188","      <http:\/\/www.iucnredlist.org\/details\/13408\/0>`_ ,"],"delete":["11","   <http:\/\/www.iucnredlist.org\/apps\/redlist\/details\/3038\/0>`_ ,","15","   <http:\/\/www.iucnredlist.org\/apps\/redlist\/details\/13408\/0>`_ ,","184","      <http:\/\/www.iucnredlist.org\/apps\/redlist\/details\/3038\/0>`_ ,","188","      <http:\/\/www.iucnredlist.org\/apps\/redlist\/details\/13408\/0>`_ ,"]}],"doc\/modules\/clustering.rst":[{"add":["426","cuts <http:\/\/people.eecs.berkeley.edu\/~malik\/papers\/SM-ncut.pdf>`_ problem on","878","   https:\/\/code.google.com\/archive\/p\/jbirch"],"delete":["426","cuts <http:\/\/www.cs.berkeley.edu\/~malik\/papers\/SM-ncut.pdf>`_ problem on","878","   https:\/\/code.google.com\/p\/jbirch\/"]}],"doc\/support.rst":[{"add":["30","  `stack exchange <http:\/\/stats.stackexchange.com\/>`_ is probably a more","100","<https:\/\/sourceforge.net\/projects\/scikit-learn\/files\/documentation\/>`_."],"delete":["30","  `metaoptimize.com\/qa <http:\/\/metaoptimize.com\/qa>`_ is probably a more","100","<http:\/\/sourceforge.net\/projects\/scikit-learn\/files\/documentation\/>`_."]}],"CONTRIBUTING.md":[{"add":["55","[Git documentation](https:\/\/git-scm.com\/documentation) on the web, or ask a friend or another contributor for help.)","217","[pillow](http:\/\/pillow.readthedocs.io\/en\/latest\/)."],"delete":["55","[Git documentation](http:\/\/git-scm.com\/documentation) on the web, or ask a friend or another contributor for help.)","217","[pillow](http:\/\/pillow.readthedocs.org\/en\/latest\/)."]}],"sklearn\/datasets\/olivetti_faces.py":[{"add":["2","The original database was available from","4","    http:\/\/www.cl.cam.ac.uk\/research\/dtg\/attarchive\/facedatabase.html","100","        http:\/\/www.cl.cam.ac.uk\/research\/dtg\/attarchive\/facedatabase.html"],"delete":["2","The original database was available from (now defunct)","4","    http:\/\/www.uk.research.att.com\/facedatabase.html","100","        http:\/\/www.uk.research.att.com\/facedatabase.html"]}],"doc\/modules\/feature_selection.rst":[{"add":["285","        Bootstrap.\" https:\/\/hal.inria.fr\/hal-00354771\/","289","       http:\/\/arxiv.org\/pdf\/0809.2932.pdf"],"delete":["285","        Bootstrap.\" http:\/\/hal.inria.fr\/hal-00354771\/","289","       http:\/\/arxiv.org\/pdf\/0809.2932"]}],"doc\/faq.rst":[{"add":["104","<https:\/\/sourceforge.net\/p\/scikit-learn\/mailman\/scikit-learn-general\/thread\/CAAkaFLWcBG+gtsFQzpTLfZoCsHMDv9UG5WaqT0LwUApte0TVzg@mail.gmail.com\/#msg33104380>`_."],"delete":["104","<http:\/\/sourceforge.net\/p\/scikit-learn\/mailman\/scikit-learn-general\/thread\/CAAkaFLWcBG%2BgtsFQzpTLfZoCsHMDv9UG5WaqT0LwUApte0TVzg%40mail.gmail.com\/#msg33104380>`_."]}],"doc\/related_projects.rst":[{"add":["24","  <https:\/\/skll.readthedocs.io\/en\/latest\/index.html>`_  A command-line","28","- `auto-sklearn <https:\/\/github.com\/automl\/auto-sklearn\/>`_","60","- `lightning <https:\/\/github.com\/scikit-learn-contrib\/lightning>`_ Fast state-of-the-art","75","- `py-earth <https:\/\/github.com\/scikit-learn-contrib\/py-earth>`_ Multivariate adaptive"],"delete":["24","  <https:\/\/skll.readthedocs.org\/en\/latest\/index.html>`_  A command-line","28","- `auto-sklearn <https:\/\/github.com\/automl\/auto-sklearn\/blob\/master\/source\/index.rst>`_","60","- `lightning <http:\/\/www.mblondel.org\/lightning\/>`_ Fast state-of-the-art","75","- `py-earth <https:\/\/github.com\/jcrudy\/py-earth>`_ Multivariate adaptive"]}],"sklearn\/gaussian_process\/gaussian_process.py":[{"add":["210","        http:\/\/www.jstor.org\/stable\/1269548"],"delete":["210","        http:\/\/www.jstor.org\/pss\/1269548"]}],"doc\/about.rst":[{"add":["115","`tinyclues <https:\/\/www.tinyclues.com\/>`_ funded the 2011 international Granada","128","<http:\/\/www.numfocus.org\/board.html>`_. NumFOCUS's mission is to foster","178",".. |tinyclues| image:: https:\/\/www.tinyclues.com\/web\/wp-content\/uploads\/2016\/06\/Tinyclues-PNG-logo.png","180","   :target: https:\/\/www.tinyclues.com\/","183",".. |afpy| image:: https:\/\/www.afpy.org\/logo.png","185","   :target: https:\/\/www.afpy.org","192",".. |FNRS| image:: http:\/\/www.fnrs.be\/en\/images\/FRS-FNRS_rose_transp.png","213","- We would like to thank `Rackspace <https:\/\/www.rackspace.com>`_ for providing","214","  us with a free `Rackspace Cloud <https:\/\/www.rackspace.com\/cloud\/>`_ account to","220","  <http:\/\/shiningpanda.com\/>`_ for free CPU time on their Continuous"],"delete":["115","`tinyclues <http:\/\/www.tinyclues.com\/>`_ funded the 2011 international Granada","128","<http:\/\/www.numfocus.org\/board>`_. NumFOCUS's mission is to foster","178",".. |tinyclues| image:: http:\/\/www.tinyclues.com\/static\/img\/logo.png","180","   :target: http:\/\/www.tinyclues.com\/","183",".. |afpy| image:: http:\/\/www.afpy.org\/logo.png","185","   :target: http:\/\/www.afpy.org","192",".. |FNRS| image:: http:\/\/www.fnrs.be\/uploaddocs\/images\/COMMUNIQUER\/FRS-FNRS_rose_transp.png","213","- We would like to thank `Rackspace <http:\/\/www.rackspace.com>`_ for providing","214","  us with a free `Rackspace Cloud <http:\/\/www.rackspace.com\/cloud\/>`_ account to","220","  <https:\/\/www.shiningpanda-ci.com\/>`_ for free CPU time on their Continuous"]}],"doc\/modules\/model_persistence.rst":[{"add":["16","persistence model, namely `pickle <https:\/\/docs.python.org\/2\/library\/pickle.html>`_::"],"delete":["16","persistence model, namely `pickle <http:\/\/docs.python.org\/2\/library\/pickle.html>`_::"]}],"doc\/modules\/naive_bayes.rst":[{"add":["73","   <http:\/\/www.cs.unb.ca\/~hzhang\/publications\/FLAIRS04ZhangH.pdf>`_"],"delete":["73","   <http:\/\/www.cs.unb.ca\/profs\/hzhang\/publications\/FLAIRS04ZhangH.pdf>`_"]}],"doc\/modules\/feature_extraction.rst":[{"add":["212"," * `MurmurHash3 <https:\/\/github.com\/aappleby\/smhasher>`_."],"delete":["212"," * `MurmurHash3 <http:\/\/code.google.com\/p\/smhasher\/wiki\/MurmurHash3>`_."]}],"doc\/testimonials\/testimonials.rst":[{"add":["66","<https:\/\/team.inria.fr\/visages\/>`_ for medical image analysis, `Privatics","504","    :target: https:\/\/www.brandwatch.com\/peerindex-and-brandwatch","531","`DataRobot <https:\/\/www.datarobot.com>`_","540","    :target: https:\/\/www.datarobot.com"],"delete":["66","<https:\/\/www.irisa.fr\/visages\/index>`_ for medical image analysis, `Privatics","294","","295","`Rangespan <https:\/\/www.rangespan.com>`_","506","    :target: http:\/\/www.peerindex.com\/","533","`DataRobot <http:\/\/www.datarobot.com>`_","542","    :target: http:\/\/www.datarobot.com"]}],"doc\/tutorial\/statistical_inference\/finding_help.rst":[{"add":["23","\t","24","  :Stack Exchange:","26","    The Stack Exchange family of sites hosts `multiple subdomains for Machine Learning questions`_.","30",".. _`multiple subdomains for Machine Learning questions`: http:\/\/meta.stackexchange.com\/questions\/130524\/which-stack-exchange-website-for-machine-learning-and-computational-algorithms","31",""],"delete":["15","  :Metaoptimize\/QA:","16","","17","    A forum for Machine Learning, Natural Language Processing and","18","    other Data Analytics discussions (similar to what Stackoverflow","19","    is for developers): http:\/\/metaoptimize.com\/qa","20","","21","    A good starting point is the discussion on `good freely available","22","    textbooks on machine learning`_","23","","33","","34","","35",".. _`good freely available textbooks on machine learning`: http:\/\/metaoptimize.com\/qa\/questions\/186\/good-freely-available-textbooks-on-machine-learning"]}],"sklearn\/feature_selection\/mutual_info_.py":[{"add":["351","    .. [1] `Mutual Information <https:\/\/en.wikipedia.org\/wiki\/Mutual_information>`_","426","    .. [1] `Mutual Information <https:\/\/en.wikipedia.org\/wiki\/Mutual_information>`_"],"delete":["351","    .. [1] `Mutual Information <http:\/\/en.wikipedia.org\/wiki\/Mutual_information>`_","426","    .. [1] `Mutual Information <http:\/\/en.wikipedia.org\/wiki\/Mutual_information>`_"]}],"doc\/presentations.rst":[{"add":["11","<http:\/\/www.scipy-lectures.org\/>`_. This will help you find your footing a"],"delete":["11","<http:\/\/scipy-lectures.org>`_. This will help you find your footing a"]}],"doc\/modules\/tree.rst":[{"add":["412",".. _CART: https:\/\/en.wikipedia.org\/wiki\/Predictive_analytics#Classification_and_regression_trees_.28CART.29"],"delete":["412",".. _CART: https:\/\/en.wikipedia.org\/wiki\/Predictive_analytics#Classification_and_regression_trees"]}],"sklearn\/cluster\/birch.py":[{"add":["403","      https:\/\/code.google.com\/archive\/p\/jbirch"],"delete":["403","      https:\/\/code.google.com\/p\/jbirch\/"]}],"sklearn\/linear_model\/least_angle.py":[{"add":["143","           <https:\/\/en.wikipedia.org\/wiki\/Lasso_(statistics)>`_"],"delete":["143","           <https:\/\/en.wikipedia.org\/wiki\/Lasso_(statistics)#Lasso_method>`_"]}]}},"c31ad7ada3d46f8b8119685b91d1562f8e710b09":{"changes":{"sklearn\/gaussian_process\/kernels.py":"MODIFY","sklearn\/gaussian_process\/tests\/test_kernels.py":"MODIFY"},"diff":{"sklearn\/gaussian_process\/kernels.py":[{"add":["35","def _check_length_scale(X, length_scale):","36","    length_scale = np.squeeze(length_scale).astype(float)","37","    if np.ndim(length_scale) > 1:","38","        raise ValueError(\"length_scale cannot be of dimension greater than 1\")","39","    if np.ndim(length_scale) == 1 and X.shape[1] != length_scale.shape[0]:","40","        raise ValueError(\"Anisotropic kernel must have the same number of \"","41","                         \"dimensions as data (%d!=%d)\"","42","                         % (length_scale.shape[0], X.shape[1]))","43","    return length_scale","44","","45","","105","    # This is mainly a testing utility to check that two hyperparameters","106","    # are equal.","107","    def __eq__(self, other):","108","        return (self.name == other.name and","109","                self.value_type == other.value_type and","110","                np.all(self.bounds == other.bounds) and","111","                self.n_elements == other.n_elements and","112","                self.fixed == other.fixed)","113","","209","        for attr in dir(self):","211","                r.append(getattr(self, attr))","229","        params = self.get_params()","232","                theta.append(params[hyperparameter.name])","247","        params = self.get_params()","254","                params[hyperparameter.name] = np.exp(","255","                    theta[i:i + hyperparameter.n_elements])","258","                params[hyperparameter.name] = np.exp(theta[i])","265","        self.set_params(**params)","935","    @property","936","    def hyperparameter_constant_value(self):","937","        return Hyperparameter(","938","            \"constant_value\", \"numeric\", self.constant_value_bounds)","1026","    @property","1027","    def hyperparameter_noise_level(self):","1028","        return Hyperparameter(","1029","            \"noise_level\", \"numeric\", self.noise_level_bounds)","1124","        self.length_scale = length_scale","1127","    @property","1128","    def anisotropic(self):","1129","        return np.iterable(self.length_scale) and len(self.length_scale) > 1","1130","","1131","    @property","1132","    def hyperparameter_length_scale(self):","1133","        if self.anisotropic:","1134","            return Hyperparameter(\"length_scale\", \"numeric\",","1135","                                  self.length_scale_bounds,","1136","                                  len(self.length_scale))","1137","        return Hyperparameter(","1138","            \"length_scale\", \"numeric\", self.length_scale_bounds)","1167","        length_scale = _check_length_scale(X, self.length_scale)","1169","            dists = pdist(X \/ length_scale, metric='sqeuclidean')","1178","            dists = cdist(X \/ length_scale, Y \/ length_scale,","1186","            elif not self.anisotropic or length_scale.shape[0] == 1:","1193","                    \/ (length_scale ** 2)","1277","        length_scale = _check_length_scale(X, self.length_scale)","1279","            dists = pdist(X \/ length_scale, metric='euclidean')","1284","            dists = cdist(X \/ length_scale, Y \/ length_scale,","1317","                    \/ (length_scale ** 2)","1387","    @property","1388","    def hyperparameter_length_scale(self):","1389","        return Hyperparameter(","1390","            \"length_scale\", \"numeric\", self.length_scale_bounds)","1391","","1392","    @property","1393","    def hyperparameter_alpha(self):","1394","        return Hyperparameter(\"alpha\", \"numeric\", self.alpha_bounds)","1496","    @property","1497","    def hyperparameter_length_scale(self):","1498","        return Hyperparameter(","1499","            \"length_scale\", \"numeric\", self.length_scale_bounds)","1500","","1501","    @property","1502","    def hyperparameter_periodicity(self):","1503","        return Hyperparameter(","1504","            \"periodicity\", \"numeric\", self.periodicity_bounds)","1602","    @property","1603","    def hyperparameter_sigma_0(self):","1604","        return Hyperparameter(\"sigma_0\", \"numeric\", self.sigma_0_bounds)","1732","        self.pairwise_kernels_kwargs = pairwise_kernels_kwargs","1733","","1734","    @property","1735","    def hyperparameter_gamma(self):","1736","        return Hyperparameter(\"gamma\", \"numeric\", self.gamma_bounds)","1764","        pairwise_kernels_kwargs = self.pairwise_kernels_kwargs","1765","        if self.pairwise_kernels_kwargs is None:","1766","            pairwise_kernels_kwargs = {}","1767","","1771","                             **pairwise_kernels_kwargs)","1780","                        filter_params=True, **pairwise_kernels_kwargs)"],"delete":["189","        for attr, value in sorted(self.__dict__.items()):","191","                r.append(value)","211","                theta.append(getattr(self, hyperparameter.name))","232","                setattr(self, hyperparameter.name,","233","                        np.exp(theta[i:i + hyperparameter.n_elements]))","236","                setattr(self, hyperparameter.name, np.exp(theta[i]))","912","        self.hyperparameter_constant_value = \\","913","            Hyperparameter(\"constant_value\", \"numeric\", constant_value_bounds)","1001","        self.hyperparameter_noise_level = \\","1002","            Hyperparameter(\"noise_level\", \"numeric\", noise_level_bounds)","1097","        if np.iterable(length_scale):","1098","            if len(length_scale) > 1:","1099","                self.anisotropic = True","1100","                self.length_scale = np.asarray(length_scale, dtype=np.float)","1101","            else:","1102","                self.anisotropic = False","1103","                self.length_scale = float(length_scale[0])","1104","        else:","1105","            self.anisotropic = False","1106","            self.length_scale = float(length_scale)","1109","        if self.anisotropic:  # anisotropic length_scale","1110","            self.hyperparameter_length_scale = \\","1111","                Hyperparameter(\"length_scale\", \"numeric\", length_scale_bounds,","1112","                               len(length_scale))","1113","        else:","1114","            self.hyperparameter_length_scale = \\","1115","                Hyperparameter(\"length_scale\", \"numeric\", length_scale_bounds)","1144","        if self.anisotropic and X.shape[1] != self.length_scale.shape[0]:","1145","            raise Exception(\"Anisotropic kernel must have the same number of \"","1146","                            \"dimensions as data (%d!=%d)\"","1147","                            % (self.length_scale.shape[0], X.shape[1]))","1148","","1150","            dists = pdist(X \/ self.length_scale, metric='sqeuclidean')","1159","            dists = cdist(X \/ self.length_scale, Y \/ self.length_scale,","1167","            elif not self.anisotropic or self.length_scale.shape[0] == 1:","1174","                    \/ (self.length_scale ** 2)","1177","            else:","1178","                raise Exception(\"Anisotropic kernels require that the number \"","1179","                                \"of length scales and features match.\")","1261","        if self.anisotropic and X.shape[1] != self.length_scale.shape[0]:","1262","            raise Exception(\"Anisotropic kernel must have the same number of \"","1263","                            \"dimensions as data (%d!=%d)\"","1264","                            % (self.length_scale.shape[0], X.shape[1]))","1265","","1267","            dists = pdist(X \/ self.length_scale, metric='euclidean')","1272","            dists = cdist(X \/ self.length_scale, Y \/ self.length_scale,","1305","                    \/ (self.length_scale ** 2)","1375","        self.hyperparameter_length_scale = \\","1376","            Hyperparameter(\"length_scale\", \"numeric\", length_scale_bounds)","1377","        self.hyperparameter_alpha = \\","1378","            Hyperparameter(\"alpha\", \"numeric\", alpha_bounds)","1480","        self.hyperparameter_length_scale = \\","1481","            Hyperparameter(\"length_scale\", \"numeric\", length_scale_bounds)","1482","        self.hyperparameter_periodicity = \\","1483","            Hyperparameter(\"periodicity\", \"numeric\", periodicity_bounds)","1581","        self.hyperparameter_sigma_0 = \\","1582","            Hyperparameter(\"sigma_0\", \"numeric\", sigma_0_bounds)","1709","","1710","        self.hyperparameter_gamma = \\","1711","            Hyperparameter(\"gamma\", \"numeric\", gamma_bounds)","1712","","1714","        if pairwise_kernels_kwargs is not None:","1715","            self.pairwise_kernels_kwargs = pairwise_kernels_kwargs","1716","        else:","1717","            self.pairwise_kernels_kwargs = {}","1748","                             **self.pairwise_kernels_kwargs)","1757","                        filter_params=True, **self.pairwise_kernels_kwargs)"]}],"sklearn\/gaussian_process\/tests\/test_kernels.py":[{"add":["185","def check_hyperparameters_equal(kernel1, kernel2):","186","    \"\"\"Check that hyperparameters of two kernels are equal\"\"\"","187","    for attr in set(dir(kernel1) + dir(kernel2)):","188","        if attr.startswith(\"hyperparameter_\"):","189","            attr_value1 = getattr(kernel1, attr)","190","            attr_value2 = getattr(kernel2, attr)","191","            assert_equal(attr_value1, attr_value2)","192","","193","","196","    bounds = (1e-5, 1e5)","200","        # XXX: Should this be fixed?","201","        # This differs from the sklearn's estimators equality check.","204","","205","        # Check that all constructor parameters are equal.","206","        assert_equal(kernel.get_params(), kernel_cloned.get_params())","207","","208","        # Check that all hyperparameters are equal.","209","        yield check_hyperparameters_equal, kernel, kernel_cloned","210","","211","        # This test is to verify that using set_params does not","212","        # break clone on kernels.","213","        # This used to break because in kernels such as the RBF, non-trivial","214","        # logic that modified the length scale used to be in the constructor","215","        # See https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/6961","216","        # for more details.","217","        params = kernel.get_params()","218","        # RationalQuadratic kernel is isotropic.","219","        isotropic_kernels = (ExpSineSquared, RationalQuadratic)","220","        if 'length_scale' in params and not isinstance(kernel, isotropic_kernels):","221","            length_scale = params['length_scale']","222","            if np.iterable(length_scale):","223","                params['length_scale'] = length_scale[0]","224","                params['length_scale_bounds'] = bounds","226","                params['length_scale'] = [length_scale] * 2","227","                params['length_scale_bounds'] = bounds * 2","228","            kernel_cloned.set_params(**params)","229","            kernel_cloned_clone = clone(kernel_cloned)","230","            assert_equal(kernel_cloned_clone.get_params(),","231","                         kernel_cloned.get_params())","232","            assert_not_equal(id(kernel_cloned_clone), id(kernel_cloned))","233","            yield check_hyperparameters_equal, kernel_cloned, kernel_cloned_clone"],"delete":["192","        for attr in kernel.__dict__.keys():","193","            attr_value = getattr(kernel, attr)","194","            attr_value_cloned = getattr(kernel_cloned, attr)","195","            if attr.startswith(\"hyperparameter_\"):","196","                assert_equal(attr_value.name, attr_value_cloned.name)","197","                assert_equal(attr_value.value_type,","198","                             attr_value_cloned.value_type)","199","                assert_array_equal(attr_value.bounds,","200","                                   attr_value_cloned.bounds)","201","                assert_equal(attr_value.n_elements,","202","                             attr_value_cloned.n_elements)","203","            elif np.iterable(attr_value):","204","                for i in range(len(attr_value)):","205","                    if np.iterable(attr_value[i]):","206","                        assert_array_equal(attr_value[i],","207","                                           attr_value_cloned[i])","208","                    else:","209","                        assert_equal(attr_value[i], attr_value_cloned[i])","211","                assert_equal(attr_value, attr_value_cloned)","212","            if not isinstance(attr_value, Hashable):","213","                # modifiable attributes must not be identical","214","                assert_not_equal(id(attr_value), id(attr_value_cloned))"]}]}},"040a76676d325f5d97d46a42ca0ff628883675b3":{"changes":{"sklearn\/manifold\/tests\/test_t_sne.py":"MODIFY","sklearn\/manifold\/t_sne.py":"MODIFY"},"diff":{"sklearn\/manifold\/tests\/test_t_sne.py":[{"add":["9","from sklearn.utils.testing import assert_array_equal","311","    # 'init' must be 'pca', 'random', or numpy array.","312","    m = \"'init' must be 'pca', 'random', or a numpy array\"","316","def test_init_ndarray():","317","    # Initialize TSNE with ndarray and test fit","318","    tsne = TSNE(init=np.zeros((100, 2)))","319","    X_embedded = tsne.fit_transform(np.ones((100, 5)))","320","    assert_array_equal(np.zeros((100, 2)), X_embedded)","321","","322","","323","def test_init_ndarray_precomputed():","324","    # Initialize TSNE with ndarray and metric 'precomputed'","325","    # Make sure no FutureWarning is thrown from _fit","326","    tsne = TSNE(init=np.zeros((100, 2)), metric=\"precomputed\")","327","    tsne.fit(np.zeros((100, 100)))","328","","329",""],"delete":["310","    # 'init' must be 'pca' or 'random'.","311","    m = \"'init' must be 'pca', 'random' or a NumPy array\""]}],"sklearn\/manifold\/t_sne.py":[{"add":["25","from ..externals.six import string_types","570","    init : string or numpy array, optional (default: \"random\")","571","        Initialization of embedding. Possible options are 'random', 'pca',","572","        and a numpy array of shape (n_samples, n_components).","647","        if not ((isinstance(init, string_types) and","648","                init in [\"pca\", \"random\"]) or","649","                isinstance(init, np.ndarray)):","650","            msg = \"'init' must be 'pca', 'random', or a numpy array\"","713","            if isinstance(self.init, string_types) and self.init == 'pca':","769","        if isinstance(self.init, np.ndarray):","770","            X_embedded = self.init","771","        elif self.init == 'pca':"],"delete":["569","    init : string, optional (default: \"random\")","570","        Initialization of embedding. Possible options are 'random' and 'pca'.","645","        if init not in [\"pca\", \"random\"] or isinstance(init, np.ndarray):","646","            msg = \"'init' must be 'pca', 'random' or a NumPy array\"","709","            if self.init == 'pca':","765","        if self.init == 'pca':","769","        elif isinstance(self.init, np.ndarray):","770","            X_embedded = self.init"]}]}},"268af6918ec9c1c88242efa75eadacb7eba0870b":{"changes":{"sklearn\/decomposition\/online_lda.py":"MODIFY","sklearn\/decomposition\/tests\/test_online_lda.py":"MODIFY"},"diff":{"sklearn\/decomposition\/online_lda.py":[{"add":["522","                                                       random_init=False,","523","                                                       parallel=parallel)"],"delete":["522","                                                       random_init=False)"]}],"sklearn\/decomposition\/tests\/test_online_lda.py":[{"add":["201","                                        evaluate_every=1,"],"delete":[]}]}},"32d1236f4a4d01a2e1544fe9929e4e78630fa9eb":{"changes":{"sklearn\/ensemble\/bagging.py":"MODIFY","sklearn\/ensemble\/tests\/test_bagging.py":"MODIFY","sklearn\/ensemble\/tests\/test_base.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/ensemble\/base.py":"MODIFY","sklearn\/ensemble\/weight_boosting.py":"MODIFY","sklearn\/ensemble\/forest.py":"MODIFY","sklearn\/ensemble\/tests\/test_weight_boosting.py":"MODIFY"},"diff":{"sklearn\/ensemble\/bagging.py":[{"add":["88","        estimator = ensemble._make_estimator(append=False,","89","                                             random_state=random_state)"],"delete":["88","        estimator = ensemble._make_estimator(append=False)","89","","90","        try:  # Not all estimators accept a random_state","91","            estimator.set_params(random_state=seeds[i])","92","        except ValueError:","93","            pass"]}],"sklearn\/ensemble\/tests\/test_bagging.py":[{"add":["555","    assert_true(isinstance(estimator[0].steps[-1][1].random_state,","556","                           int))"],"delete":[]}],"sklearn\/ensemble\/tests\/test_base.py":[{"add":["7","import numpy as np","12","from sklearn.utils.testing import assert_not_equal","15","from sklearn.ensemble.base import _set_random_states","17","from sklearn.externals.odict import OrderedDict","18","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis","19","from sklearn.pipeline import Pipeline","20","from sklearn.feature_selection import SelectFromModel","25","    ensemble = BaggingClassifier(base_estimator=Perceptron(random_state=None),","26","                                 n_estimators=3)","33","    random_state = np.random.RandomState(3)","34","    ensemble._make_estimator(random_state=random_state)","35","    ensemble._make_estimator(random_state=random_state)","42","    assert_equal(ensemble[0].random_state, None)","43","    assert_true(isinstance(ensemble[1].random_state, int))","44","    assert_true(isinstance(ensemble[2].random_state, int))","45","    assert_not_equal(ensemble[1].random_state, ensemble[2].random_state)","56","","57","","58","def test_set_random_states():","59","    # Linear Discriminant Analysis doesn't have random state: smoke test","60","    _set_random_states(LinearDiscriminantAnalysis(), random_state=17)","61","","62","    clf1 = Perceptron(random_state=None)","63","    assert_equal(clf1.random_state, None)","64","    # check random_state is None still sets","65","    _set_random_states(clf1, None)","66","    assert_true(isinstance(clf1.random_state, int))","67","","68","    # check random_state fixes results in consistent initialisation","69","    _set_random_states(clf1, 3)","70","    assert_true(isinstance(clf1.random_state, int))","71","    clf2 = Perceptron(random_state=None)","72","    _set_random_states(clf2, 3)","73","    assert_equal(clf1.random_state, clf2.random_state)","74","","75","    # nested random_state","76","","77","    def make_steps():","78","        return [('sel', SelectFromModel(Perceptron(random_state=None))),","79","                ('clf', Perceptron(random_state=None))]","80","","81","    est1 = Pipeline(make_steps())","82","    _set_random_states(est1, 3)","83","    assert_true(isinstance(est1.steps[0][1].estimator.random_state, int))","84","    assert_true(isinstance(est1.steps[1][1].random_state, int))","85","    assert_not_equal(est1.get_params()['sel__estimator__random_state'],","86","                     est1.get_params()['clf__random_state'])","87","","88","    # ensure multiple random_state paramaters are invariant to get_params()","89","    # iteration order","90","","91","    class AlphaParamPipeline(Pipeline):","92","        def get_params(self, *args, **kwargs):","93","            params = Pipeline.get_params(self, *args, **kwargs).items()","94","            return OrderedDict(sorted(params))","95","","96","    class RevParamPipeline(Pipeline):","97","        def get_params(self, *args, **kwargs):","98","            params = Pipeline.get_params(self, *args, **kwargs).items()","99","            return OrderedDict(sorted(params, reverse=True))","100","","101","    for cls in [AlphaParamPipeline, RevParamPipeline]:","102","        est2 = cls(make_steps())","103","        _set_random_states(est2, 3)","104","        assert_equal(est1.get_params()['sel__estimator__random_state'],","105","                     est2.get_params()['sel__estimator__random_state'])","106","        assert_equal(est1.get_params()['clf__random_state'],","107","                     est2.get_params()['clf__random_state'])"],"delete":["18","    ensemble = BaggingClassifier(base_estimator=Perceptron(), n_estimators=3)","25","    ensemble._make_estimator()","26","    ensemble._make_estimator()"]}],"doc\/whats_new.rst":[{"add":["492","","493","    - Fix bug where :class:`ensemble.AdaBoostClassifier` and","494","      :class:`ensemble.AdaBoostRegressor` would perform poorly if the","495","      ``random_state`` was fixed","496","      (`#7411 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/7411>`_).","499","    - Fix bug in ensembles with randomization where the ensemble would not","500","      set ``random_state`` on base estimators in a pipeline or similar nesting.","501","      (`#7411 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/7411>`_).","502","      Note, results for :class:`ensemble.BaggingClassifier`","503","      :class:`ensemble.BaggingRegressor`, :class:`ensemble.AdaBoostClassifier`","504","      and :class:`ensemble.AdaBoostRegressor` will now differ from previous","505","      versions. By `Joel Nothman`_.","506",""],"delete":[]}],"sklearn\/ensemble\/base.py":[{"add":["12","from ..utils import _get_n_jobs, check_random_state","13","","14","MAX_RAND_SEED = np.iinfo(np.int32).max","15","","16","","17","def _set_random_states(estimator, random_state=None):","18","    \"\"\"Sets fixed random_state parameters for an estimator","19","","20","    Finds all parameters ending ``random_state`` and sets them to integers","21","    derived from ``random_state``.","22","","23","    Parameters","24","    ----------","25","","26","    estimator : estimator supporting get\/set_params","27","        Estimator with potential randomness managed by random_state","28","        parameters.","29","","30","    random_state : numpy.RandomState or int, optional","31","        Random state used to generate integer values.","32","","33","    Notes","34","    -----","35","    This does not necessarily set *all* ``random_state`` attributes that","36","    control an estimator's randomness, only those accessible through","37","    ``estimator.get_params()``.  ``random_state``s not controlled include","38","    those belonging to:","39","","40","        * cross-validation splitters","41","        * ``scipy.stats`` rvs","42","    \"\"\"","43","    random_state = check_random_state(random_state)","44","    to_set = {}","45","    for key in sorted(estimator.get_params(deep=True)):","46","        if key == 'random_state' or key.endswith('__random_state'):","47","            to_set[key] = random_state.randint(MAX_RAND_SEED)","48","","49","    if to_set:","50","        estimator.set_params(**to_set)","107","    def _make_estimator(self, append=True, random_state=None):","117","        if random_state is not None:","118","            _set_random_states(estimator, random_state)","119",""],"delete":["12","from ..utils import _get_n_jobs","69","    def _make_estimator(self, append=True):"]}],"sklearn\/ensemble\/weight_boosting.py":[{"add":["134","        random_state = check_random_state(self.random_state)","135","","141","                sample_weight,","142","                random_state)","168","    def _boost(self, iboost, X, y, sample_weight, random_state):","188","        random_state : numpy.RandomState","189","            The current random number generator","190","","430","    def _boost(self, iboost, X, y, sample_weight, random_state):","452","        random_state : numpy.RandomState","453","            The current random number generator","454","","470","            return self._boost_real(iboost, X, y, sample_weight, random_state)","473","            return self._boost_discrete(iboost, X, y, sample_weight,","474","                                        random_state)","476","    def _boost_real(self, iboost, X, y, sample_weight, random_state):","478","        estimator = self._make_estimator(random_state=random_state)","534","    def _boost_discrete(self, iboost, X, y, sample_weight, random_state):","536","        estimator = self._make_estimator(random_state=random_state)","961","    def _boost(self, iboost, X, y, sample_weight, random_state):","983","        random_state : numpy.RandomState","984","            The current random number generator","985","","1000","        estimator = self._make_estimator(random_state=random_state)","1006","        uniform_samples = random_state.random_sample(X.shape[0])"],"delete":["139","                sample_weight)","165","    def _boost(self, iboost, X, y, sample_weight):","424","    def _boost(self, iboost, X, y, sample_weight):","461","            return self._boost_real(iboost, X, y, sample_weight)","464","            return self._boost_discrete(iboost, X, y, sample_weight)","466","    def _boost_real(self, iboost, X, y, sample_weight):","468","        estimator = self._make_estimator()","469","","470","        try:","471","            estimator.set_params(random_state=self.random_state)","472","        except ValueError:","473","            pass","529","    def _boost_discrete(self, iboost, X, y, sample_weight):","531","        estimator = self._make_estimator()","532","","533","        try:","534","            estimator.set_params(random_state=self.random_state)","535","        except ValueError:","536","            pass","961","    def _boost(self, iboost, X, y, sample_weight):","997","        estimator = self._make_estimator()","998","","999","        try:","1000","            estimator.set_params(random_state=self.random_state)","1001","        except ValueError:","1002","            pass","1003","","1004","        generator = check_random_state(self.random_state)","1010","        uniform_samples = generator.random_sample(X.shape[0])"]}],"sklearn\/ensemble\/forest.py":[{"add":["306","                tree = self._make_estimator(append=False,","307","                                            random_state=random_state)"],"delete":["306","                tree = self._make_estimator(append=False)","307","                tree.set_params(random_state=random_state.randint(MAX_INT))"]}],"sklearn\/ensemble\/tests\/test_weight_boosting.py":[{"add":["5","from sklearn.utils.testing import assert_equal, assert_true, assert_greater","115","        # Check we used multiple estimators","116","        assert_greater(len(clf.estimators_), 1)","117","        # Check for distinct random states (see issue #7408)","118","        assert_equal(len(set(est.random_state for est in clf.estimators_)),","119","                     len(clf.estimators_))","120","","131","    reg = AdaBoostRegressor(random_state=0)","132","    reg.fit(boston.data, boston.target)","133","    score = reg.score(boston.data, boston.target)","136","    # Check we used multiple estimators","137","    assert_true(len(reg.estimators_) > 1)","138","    # Check for distinct random states (see issue #7408)","139","    assert_equal(len(set(est.random_state for est in reg.estimators_)),","140","                 len(reg.estimators_))","141",""],"delete":["5","from sklearn.utils.testing import assert_equal, assert_true","125","    clf = AdaBoostRegressor(random_state=0)","126","    clf.fit(boston.data, boston.target)","127","    score = clf.score(boston.data, boston.target)"]}]}},"c84ff5e3514d0e48daa9dcaeafb80629b8db3ced":{"changes":{"sklearn\/tree\/_utils.pyx":"MODIFY","sklearn\/tree\/tree.py":"MODIFY","sklearn\/ensemble\/gradient_boosting.py":"MODIFY","sklearn\/tree\/_criterion.pyx":"MODIFY","sklearn\/ensemble\/tests\/test_forest.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/tree\/_utils.pxd":"MODIFY","sklearn\/ensemble\/forest.py":"MODIFY","sklearn\/tree\/tests\/test_tree.py":"MODIFY"},"diff":{"sklearn\/tree\/_utils.pyx":[{"add":["8","#          Nelson Liu <nelson@nelsonliu.me>","15","from libc.stdlib cimport calloc","305","","306","# =============================================================================","307","# WeightedPQueue data structure","308","# =============================================================================","309","","310","cdef class WeightedPQueue:","311","    \"\"\"A priority queue class, always sorted in increasing order.","312","","313","    Attributes","314","    ----------","315","    capacity : SIZE_t","316","        The capacity of the priority queue.","317","","318","    array_ptr : SIZE_t","319","        The water mark of the priority queue; the priority queue grows from","320","        left to right in the array ``array_``. ``array_ptr`` is always","321","        less than ``capacity``.","322","","323","    array_ : WeightedPQueueRecord*","324","        The array of priority queue records. The minimum element is on the","325","        left at index 0, and the maximum element is on the right at index","326","        ``array_ptr-1``.","327","    \"\"\"","328","","329","    def __cinit__(self, SIZE_t capacity):","330","        self.capacity = capacity","331","        self.array_ptr = 0","332","        safe_realloc(&self.array_, capacity)","333","","334","        if self.array_ == NULL:","335","            raise MemoryError()","336","","337","    def __dealloc__(self):","338","        free(self.array_)","339","","340","    cdef void reset(self) nogil:","341","        \"\"\"Reset the WeightedPQueue to its state at construction\"\"\"","342","        self.array_ptr = 0","343","        self.array_ = <WeightedPQueueRecord*> calloc(self.capacity,","344","                                                     sizeof(WeightedPQueueRecord))","345","","346","    cdef bint is_empty(self) nogil:","347","        return self.array_ptr <= 0","348","","349","    cdef SIZE_t size(self) nogil:","350","        return self.array_ptr","351","","352","    cdef int push(self, DOUBLE_t data, DOUBLE_t weight) nogil:","353","        \"\"\"Push record on the array.","354","        Returns 0 if successful; -1 on out of memory error.","355","        \"\"\"","356","        cdef SIZE_t array_ptr = self.array_ptr","357","        cdef WeightedPQueueRecord* array = NULL","358","        cdef SIZE_t i","359","","360","        # Resize if capacity not sufficient","361","        if array_ptr >= self.capacity:","362","            self.capacity *= 2","363","            array = <WeightedPQueueRecord*> realloc(self.array_,","364","                                                    self.capacity *","365","                                                    sizeof(WeightedPQueueRecord))","366","","367","            if array == NULL:","368","                # no free; __dealloc__ handles that","369","                return -1","370","            self.array_ = array","371","","372","        # Put element as last element of array","373","        array = self.array_","374","        array[array_ptr].data = data","375","        array[array_ptr].weight = weight","376","","377","        # bubble last element up according until it is sorted","378","        # in ascending order","379","        i = array_ptr","380","        while(i != 0 and array[i].data < array[i-1].data):","381","            array[i], array[i-1] = array[i-1], array[i]","382","            i -= 1","383","","384","        # Increase element count","385","        self.array_ptr = array_ptr + 1","386","        return 0","387","","388","    cdef int remove(self, DOUBLE_t data, DOUBLE_t weight) nogil:","389","        \"\"\"Remove a specific value\/weight record from the array.","390","        Returns 0 if successful, -1 if record not found.\"\"\"","391","        cdef SIZE_t array_ptr = self.array_ptr","392","        cdef WeightedPQueueRecord* array = self.array_","393","        cdef SIZE_t idx_to_remove = -1","394","        cdef SIZE_t i","395","","396","        if array_ptr <= 0:","397","            return -1","398","","399","        # find element to remove","400","        for i in range(array_ptr):","401","            if array[i].data == data and array[i].weight == weight:","402","                idx_to_remove = i","403","                break","404","","405","        if idx_to_remove == -1:","406","            return -1","407","","408","        # shift the elements after the removed element","409","        # to the left.","410","        for i in range(idx_to_remove, array_ptr-1):","411","            array[i] = array[i+1]","412","","413","        self.array_ptr = array_ptr - 1","414","        return 0","415","","416","    cdef int pop(self, DOUBLE_t* data, DOUBLE_t* weight) nogil:","417","        \"\"\"Remove the top (minimum) element from array.","418","        Returns 0 if successful, -1 if nothing to remove.\"\"\"","419","        cdef SIZE_t array_ptr = self.array_ptr","420","        cdef WeightedPQueueRecord* array = self.array_","421","        cdef SIZE_t i","422","","423","        if array_ptr <= 0:","424","            return -1","425","","426","        data[0] = array[0].data","427","        weight[0] = array[0].weight","428","","429","        # shift the elements after the removed element","430","        # to the left.","431","        for i in range(0, array_ptr-1):","432","            array[i] = array[i+1]","433","","434","        self.array_ptr = array_ptr - 1","435","        return 0","436","","437","    cdef int peek(self, DOUBLE_t* data, DOUBLE_t* weight) nogil:","438","        \"\"\"Write the top element from array to a pointer.","439","        Returns 0 if successful, -1 if nothing to write.\"\"\"","440","        cdef WeightedPQueueRecord* array = self.array_","441","        if self.array_ptr <= 0:","442","            return -1","443","        # Take first value","444","        data[0] = array[0].data","445","        weight[0] = array[0].weight","446","        return 0","447","","448","    cdef DOUBLE_t get_weight_from_index(self, SIZE_t index) nogil:","449","        \"\"\"Given an index between [0,self.current_capacity], access","450","        the appropriate heap and return the requested weight\"\"\"","451","        cdef WeightedPQueueRecord* array = self.array_","452","","453","        # get weight at index","454","        return array[index].weight","455","","456","    cdef DOUBLE_t get_value_from_index(self, SIZE_t index) nogil:","457","        \"\"\"Given an index between [0,self.current_capacity], access","458","        the appropriate heap and return the requested value\"\"\"","459","        cdef WeightedPQueueRecord* array = self.array_","460","","461","        # get value at index","462","        return array[index].data","463","","464","# =============================================================================","465","# WeightedMedianCalculator data structure","466","# =============================================================================","467","","468","cdef class WeightedMedianCalculator:","469","    \"\"\"A class to handle calculation of the weighted median from streams of","470","    data. To do so, it maintains a parameter ``k`` such that the sum of the","471","    weights in the range [0,k) is greater than or equal to half of the total","472","    weight. By minimizing the value of ``k`` that fulfills this constraint,","473","    calculating the median is done by either taking the value of the sample","474","    at index ``k-1`` of ``samples`` (samples[k-1].data) or the average of","475","    the samples at index ``k-1`` and ``k`` of ``samples``","476","    ((samples[k-1] + samples[k]) \/ 2).","477","","478","    Attributes","479","    ----------","480","    initial_capacity : SIZE_t","481","        The initial capacity of the WeightedMedianCalculator.","482","","483","    samples : WeightedPQueue","484","        Holds the samples (consisting of values and their weights) used in the","485","        weighted median calculation.","486","","487","    total_weight : DOUBLE_t","488","        The sum of the weights of items in ``samples``. Represents the total","489","        weight of all samples used in the median calculation.","490","","491","    k : SIZE_t","492","        Index used to calculate the median.","493","","494","    sum_w_0_k : DOUBLE_t","495","        The sum of the weights from samples[0:k]. Used in the weighted","496","        median calculation; minimizing the value of ``k`` such that","497","        ``sum_w_0_k`` >= ``total_weight \/ 2`` provides a mechanism for","498","        calculating the median in constant time.","499","","500","    \"\"\"","501","","502","    def __cinit__(self, SIZE_t initial_capacity):","503","        self.initial_capacity = initial_capacity","504","        self.samples = WeightedPQueue(initial_capacity)","505","        self.total_weight = 0","506","        self.k = 0","507","        self.sum_w_0_k = 0","508","","509","    cdef SIZE_t size(self) nogil:","510","        \"\"\"Return the number of samples in the","511","        WeightedMedianCalculator\"\"\"","512","        return self.samples.size()","513","","514","    cdef void reset(self) nogil:","515","        \"\"\"Reset the WeightedMedianCalculator to its state at construction\"\"\"","516","        self.samples.reset()","517","        self.total_weight = 0","518","        self.k = 0","519","        self.sum_w_0_k = 0","520","","521","    cdef int push(self, DOUBLE_t data, DOUBLE_t weight) nogil:","522","        \"\"\"Push a value and its associated weight","523","        to the WeightedMedianCalculator to be considered","524","        in the median calculation.","525","        \"\"\"","526","        cdef int return_value","527","        cdef DOUBLE_t original_median","528","","529","        if self.size() != 0:","530","            original_median = self.get_median()","531","        return_value = self.samples.push(data, weight)","532","        self.update_median_parameters_post_push(data, weight,","533","                                                original_median)","534","        return return_value","535","","536","    cdef int update_median_parameters_post_push(self, DOUBLE_t data,","537","                                                DOUBLE_t weight,","538","                                                DOUBLE_t original_median) nogil:","539","        \"\"\"Update the parameters used in the median calculation,","540","        namely `k` and `sum_w_0_k` after an insertion\"\"\"","541","","542","        # trivial case of one element.","543","        if self.size() == 1:","544","            self.k = 1","545","            self.total_weight = weight","546","            self.sum_w_0_k = self.total_weight","547","            return 0","548","","549","        # get the original weighted median","550","        self.total_weight += weight","551","","552","        if data < original_median:","553","            # inserting below the median, so increment k and","554","            # then update self.sum_w_0_k accordingly by adding","555","            # the weight that was added.","556","            self.k += 1","557","            # update sum_w_0_k by adding the weight added","558","            self.sum_w_0_k += weight","559","","560","            # minimize k such that sum(W[0:k]) >= total_weight \/ 2","561","            # minimum value of k is 1","562","            while(self.k > 1 and ((self.sum_w_0_k -","563","                                   self.samples.get_weight_from_index(self.k-1))","564","                                  >= self.total_weight \/ 2.0)):","565","                self.k -= 1","566","                self.sum_w_0_k -= self.samples.get_weight_from_index(self.k)","567","            return 0","568","","569","        if data >= original_median:","570","            # inserting above or at the median","571","            # minimize k such that sum(W[0:k]) >= total_weight \/ 2","572","            while(self.k < self.samples.size() and","573","                  (self.sum_w_0_k < self.total_weight \/ 2.0)):","574","                self.k += 1","575","                self.sum_w_0_k += self.samples.get_weight_from_index(self.k-1)","576","            return 0","577","","578","    cdef int remove(self, DOUBLE_t data, DOUBLE_t weight) nogil:","579","        \"\"\"Remove a value from the MedianHeap, removing it","580","        from consideration in the median calculation","581","        \"\"\"","582","        cdef int return_value","583","        cdef DOUBLE_t original_median","584","","585","        if self.size() != 0:","586","            original_median = self.get_median()","587","","588","        return_value = self.samples.remove(data, weight)","589","        self.update_median_parameters_post_remove(data, weight,","590","                                                  original_median)","591","        return return_value","592","","593","    cdef int pop(self, DOUBLE_t* data, DOUBLE_t* weight) nogil:","594","        \"\"\"Pop a value from the MedianHeap, starting from the","595","        left and moving to the right.","596","        \"\"\"","597","        cdef int return_value","598","        cdef double original_median","599","","600","        if self.size() != 0:","601","            original_median = self.get_median()","602","","603","        # no elements to pop","604","        if self.samples.size() == 0:","605","            return -1","606","","607","        return_value = self.samples.pop(data, weight)","608","        self.update_median_parameters_post_remove(data[0],","609","                                                  weight[0],","610","                                                  original_median)","611","        return return_value","612","","613","    cdef int update_median_parameters_post_remove(self, DOUBLE_t data,","614","                                                  DOUBLE_t weight,","615","                                                  double original_median) nogil:","616","        \"\"\"Update the parameters used in the median calculation,","617","        namely `k` and `sum_w_0_k` after a removal\"\"\"","618","        # reset parameters because it there are no elements","619","        if self.samples.size() == 0:","620","            self.k = 0","621","            self.total_weight = 0","622","            self.sum_w_0_k = 0","623","            return 0","624","","625","        # trivial case of one element.","626","        if self.samples.size() == 1:","627","            self.k = 1","628","            self.total_weight -= weight","629","            self.sum_w_0_k = self.total_weight","630","            return 0","631","","632","        # get the current weighted median","633","        self.total_weight -= weight","634","","635","        if data < original_median:","636","            # removing below the median, so decrement k and","637","            # then update self.sum_w_0_k accordingly by subtracting","638","            # the removed weight","639","","640","            self.k -= 1","641","            # update sum_w_0_k by removing the weight at index k","642","            self.sum_w_0_k -= weight","643","","644","            # minimize k such that sum(W[0:k]) >= total_weight \/ 2","645","            # by incrementing k and updating sum_w_0_k accordingly","646","            # until the condition is met.","647","            while(self.k < self.samples.size() and","648","                  (self.sum_w_0_k < self.total_weight \/ 2.0)):","649","                self.k += 1","650","                self.sum_w_0_k += self.samples.get_weight_from_index(self.k-1)","651","            return 0","652","","653","        if data >= original_median:","654","            # removing above the median","655","            # minimize k such that sum(W[0:k]) >= total_weight \/ 2","656","            while(self.k > 1 and ((self.sum_w_0_k -","657","                                   self.samples.get_weight_from_index(self.k-1))","658","                                  >= self.total_weight \/ 2.0)):","659","                self.k -= 1","660","                self.sum_w_0_k -= self.samples.get_weight_from_index(self.k)","661","            return 0","662","","663","    cdef DOUBLE_t get_median(self) nogil:","664","        \"\"\"Write the median to a pointer, taking into account","665","        sample weights.\"\"\"","666","        if self.sum_w_0_k == (self.total_weight \/ 2.0):","667","            # split median","668","            return (self.samples.get_value_from_index(self.k) +","669","                    self.samples.get_value_from_index(self.k-1)) \/ 2.0","670","        if self.sum_w_0_k > (self.total_weight \/ 2.0):","671","            # whole median","672","            return self.samples.get_value_from_index(self.k-1)"],"delete":[]}],"sklearn\/tree\/tree.py":[{"add":["58","CRITERIA_REG = {\"mse\": _criterion.MSE, \"friedman_mse\": _criterion.FriedmanMSE,","59","                \"mae\": _criterion.MAE}","340","                criterion = CRITERIA_REG[self.criterion](self.n_outputs_,","341","                                                         n_samples)","786","        The function to measure the quality of a split. Supported criteria","787","        are \"mse\" for the mean squared error, which is equal to variance","788","        reduction as feature selection criterion, and \"mae\" for the mean","789","        absolute error."],"delete":["58","CRITERIA_REG = {\"mse\": _criterion.MSE, \"friedman_mse\": _criterion.FriedmanMSE}","339","                criterion = CRITERIA_REG[self.criterion](self.n_outputs_)","784","        The function to measure the quality of a split. The only supported","785","        criterion is \"mse\" for the mean squared error, which is equal to","786","        variance reduction as feature selection criterion."]}],"sklearn\/ensemble\/gradient_boosting.py":[{"add":["722","    def __init__(self, loss, learning_rate, n_estimators, criterion,","723","                 min_samples_split, min_samples_leaf, min_weight_fraction_leaf,","731","        self.criterion = criterion","765","                criterion=self.criterion,","1299","    criterion : string, optional (default=\"friedman_mse\")","1300","        The function to measure the quality of a split. Supported criteria","1301","        are \"friedman_mse\" for the mean squared error with improvement","1302","        score by Friedman, \"mse\" for mean squared error, and \"mae\" for","1303","        the mean absolute error. The default value of \"friedman_mse\" is","1304","        generally the best as it can provide a better approximation in","1305","        some cases.","1306","","1437","                 subsample=1.0, criterion='friedman_mse', min_samples_split=2,","1446","            criterion=criterion, min_samples_split=min_samples_split,","1654","    criterion : string, optional (default=\"friedman_mse\")","1655","        The function to measure the quality of a split. Supported criteria","1656","        are \"friedman_mse\" for the mean squared error with improvement","1657","        score by Friedman, \"mse\" for mean squared error, and \"mae\" for","1658","        the mean absolute error. The default value of \"friedman_mse\" is","1659","        generally the best as it can provide a better approximation in","1660","        some cases.","1661","","1791","                 subsample=1.0, criterion='friedman_mse', min_samples_split=2,","1799","            criterion=criterion, min_samples_split=min_samples_split,"],"delete":["722","    def __init__(self, loss, learning_rate, n_estimators, min_samples_split,","723","                 min_samples_leaf, min_weight_fraction_leaf,","764","                criterion='friedman_mse',","1428","                 subsample=1.0, min_samples_split=2,","1437","            min_samples_split=min_samples_split,","1774","                 subsample=1.0, min_samples_split=2,","1782","            min_samples_split=min_samples_split,"]}],"sklearn\/tree\/_criterion.pyx":[{"add":["14","#          Nelson Liu <nelson@nelsonliu.me>","22","from libc.math cimport fabs","31","from ._utils cimport WeightedMedianCalculator","689","    def __cinit__(self, SIZE_t n_outputs, SIZE_t n_samples):","696","","697","        n_samples: SIZE_t","698","            The total number of samples to fit on","970","cdef class MAE(RegressionCriterion):","971","    \"\"\"Mean absolute error impurity criterion","972","","973","       MAE = (1 \/ n)*(\\sum_i |y_i - f_i|), where y_i is the true","974","       value and f_i is the predicted value.\"\"\"","975","    def __dealloc__(self):","976","        \"\"\"Destructor.\"\"\"","977","        free(self.node_medians)","978","","979","    cdef np.ndarray left_child","980","    cdef np.ndarray right_child","981","    cdef DOUBLE_t* node_medians","982","","983","    def __cinit__(self, SIZE_t n_outputs, SIZE_t n_samples):","984","        \"\"\"Initialize parameters for this criterion.","985","","986","        Parameters","987","        ----------","988","        n_outputs: SIZE_t","989","            The number of targets to be predicted","990","","991","        n_samples: SIZE_t","992","            The total number of samples to fit on","993","        \"\"\"","994","","995","        # Default values","996","        self.y = NULL","997","        self.y_stride = 0","998","        self.sample_weight = NULL","999","","1000","        self.samples = NULL","1001","        self.start = 0","1002","        self.pos = 0","1003","        self.end = 0","1004","","1005","        self.n_outputs = n_outputs","1006","        self.n_node_samples = 0","1007","        self.weighted_n_node_samples = 0.0","1008","        self.weighted_n_left = 0.0","1009","        self.weighted_n_right = 0.0","1010","","1011","        # Allocate accumulators. Make sure they are NULL, not uninitialized,","1012","        # before an exception can be raised (which triggers __dealloc__).","1013","        self.node_medians = NULL","1014","","1015","        # Allocate memory for the accumulators","1016","        safe_realloc(&self.node_medians, n_outputs)","1017","","1018","        if (self.node_medians == NULL):","1019","            raise MemoryError()","1020","","1021","        self.left_child = np.empty(n_outputs, dtype='object')","1022","        self.right_child = np.empty(n_outputs, dtype='object')","1023","        # initialize WeightedMedianCalculators","1024","        for k in range(n_outputs):","1025","            self.left_child[k] = WeightedMedianCalculator(n_samples)","1026","            self.right_child[k] = WeightedMedianCalculator(n_samples)","1027","","1028","    cdef void init(self, DOUBLE_t* y, SIZE_t y_stride, DOUBLE_t* sample_weight,","1029","                   double weighted_n_samples, SIZE_t* samples, SIZE_t start,","1030","                   SIZE_t end) nogil:","1031","        \"\"\"Initialize the criterion at node samples[start:end] and","1032","           children samples[start:start] and samples[start:end].\"\"\"","1033","","1034","        cdef SIZE_t i, p, k","1035","        cdef DOUBLE_t y_ik","1036","        cdef DOUBLE_t w = 1.0","1037","","1038","        # Initialize fields","1039","        self.y = y","1040","        self.y_stride = y_stride","1041","        self.sample_weight = sample_weight","1042","        self.samples = samples","1043","        self.start = start","1044","        self.end = end","1045","        self.n_node_samples = end - start","1046","        self.weighted_n_samples = weighted_n_samples","1047","        self.weighted_n_node_samples = 0.","1048","","1049","        cdef void** left_child","1050","        cdef void** right_child","1051","","1052","        left_child = <void**> self.left_child.data","1053","        right_child = <void**> self.right_child.data","1054","","1055","        for k in range(self.n_outputs):","1056","            (<WeightedMedianCalculator> left_child[k]).reset()","1057","            (<WeightedMedianCalculator> right_child[k]).reset()","1058","","1059","        for p in range(start, end):","1060","            i = samples[p]","1061","","1062","            if sample_weight != NULL:","1063","                w = sample_weight[i]","1064","","1065","            for k in range(self.n_outputs):","1066","                y_ik = y[i * y_stride + k]","1067","","1068","                # push all values to the right side,","1069","                # since pos = start initially anyway","1070","                (<WeightedMedianCalculator> right_child[k]).push(y_ik, w)","1071","","1072","            self.weighted_n_node_samples += w","1073","        # calculate the node medians","1074","        for k in range(self.n_outputs):","1075","            self.node_medians[k] = (<WeightedMedianCalculator> right_child[k]).get_median()","1076","","1077","        # Reset to pos=start","1078","        self.reset()","1079","","1080","    cdef void reset(self) nogil:","1081","        \"\"\"Reset the criterion at pos=start.\"\"\"","1082","","1083","        cdef SIZE_t i, k","1084","        cdef DOUBLE_t value","1085","        cdef DOUBLE_t weight","1086","","1087","        cdef void** left_child = <void**> self.left_child.data","1088","        cdef void** right_child = <void**> self.right_child.data","1089","","1090","        self.weighted_n_left = 0.0","1091","        self.weighted_n_right = self.weighted_n_node_samples","1092","        self.pos = self.start","1093","","1094","        # reset the WeightedMedianCalculators, left should have no","1095","        # elements and right should have all elements.","1096","","1097","        for k in range(self.n_outputs):","1098","            # if left has no elements, it's already reset","1099","            for i in range((<WeightedMedianCalculator> left_child[k]).size()):","1100","                # remove everything from left and put it into right","1101","                (<WeightedMedianCalculator> left_child[k]).pop(&value,","1102","                                                               &weight)","1103","                (<WeightedMedianCalculator> right_child[k]).push(value,","1104","                                                                 weight)","1105","","1106","    cdef void reverse_reset(self) nogil:","1107","        \"\"\"Reset the criterion at pos=end.\"\"\"","1108","","1109","        self.weighted_n_right = 0.0","1110","        self.weighted_n_left = self.weighted_n_node_samples","1111","        self.pos = self.end","1112","","1113","        cdef DOUBLE_t value","1114","        cdef DOUBLE_t weight","1115","        cdef void** left_child = <void**> self.left_child.data","1116","        cdef void** right_child = <void**> self.right_child.data","1117","","1118","        # reverse reset the WeightedMedianCalculators, right should have no","1119","        # elements and left should have all elements.","1120","        for k in range(self.n_outputs):","1121","            # if right has no elements, it's already reset","1122","            for i in range((<WeightedMedianCalculator> right_child[k]).size()):","1123","                # remove everything from right and put it into left","1124","                (<WeightedMedianCalculator> right_child[k]).pop(&value,","1125","                                                                &weight)","1126","                (<WeightedMedianCalculator> left_child[k]).push(value,","1127","                                                                weight)","1128","","1129","    cdef void update(self, SIZE_t new_pos) nogil:","1130","        \"\"\"Updated statistics by moving samples[pos:new_pos] to the left.\"\"\"","1131","","1132","        cdef DOUBLE_t* sample_weight = self.sample_weight","1133","        cdef SIZE_t* samples = self.samples","1134","","1135","        cdef void** left_child = <void**> self.left_child.data","1136","        cdef void** right_child = <void**> self.right_child.data","1137","","1138","        cdef DOUBLE_t* y = self.y","1139","        cdef SIZE_t pos = self.pos","1140","        cdef SIZE_t end = self.end","1141","        cdef SIZE_t i, p, k","1142","        cdef DOUBLE_t w = 1.0","1143","        cdef DOUBLE_t y_ik","1144","","1145","        # Update statistics up to new_pos","1146","        #","1147","        # We are going to update right_child and left_child","1148","        # from the direction that require the least amount of","1149","        # computations, i.e. from pos to new_pos or from end to new_pos.","1150","","1151","        if (new_pos - pos) <= (end - new_pos):","1152","            for p in range(pos, new_pos):","1153","                i = samples[p]","1154","","1155","                if sample_weight != NULL:","1156","                    w = sample_weight[i]","1157","","1158","                for k in range(self.n_outputs):","1159","                    y_ik = y[i * self.y_stride + k]","1160","                    # remove y_ik and its weight w from right and add to left","1161","                    (<WeightedMedianCalculator> right_child[k]).remove(y_ik, w)","1162","                    (<WeightedMedianCalculator> left_child[k]).push(y_ik, w)","1163","","1164","                self.weighted_n_left += w","1165","        else:","1166","            self.reverse_reset()","1167","","1168","            for p in range(end - 1, new_pos - 1, -1):","1169","                i = samples[p]","1170","","1171","                if sample_weight != NULL:","1172","                    w = sample_weight[i]","1173","","1174","                for k in range(self.n_outputs):","1175","                    y_ik = y[i * self.y_stride + k]","1176","                    # remove y_ik and its weight w from left and add to right","1177","                    (<WeightedMedianCalculator> left_child[k]).remove(y_ik, w)","1178","                    (<WeightedMedianCalculator> right_child[k]).push(y_ik, w)","1179","","1180","                self.weighted_n_left -= w","1181","","1182","        self.weighted_n_right = (self.weighted_n_node_samples -","1183","                                 self.weighted_n_left)","1184","        self.pos = new_pos","1185","","1186","    cdef void node_value(self, double* dest) nogil:","1187","        \"\"\"Computes the node value of samples[start:end] into dest.\"\"\"","1188","","1189","        cdef SIZE_t k","1190","        for k in range(self.n_outputs):","1191","            dest[k] = <double> self.node_medians[k]","1192","","1193","    cdef double node_impurity(self) nogil:","1194","        \"\"\"Evaluate the impurity of the current node, i.e. the impurity of","1195","           samples[start:end]\"\"\"","1196","","1197","        cdef DOUBLE_t* y = self.y","1198","        cdef DOUBLE_t* sample_weight = self.sample_weight","1199","        cdef SIZE_t* samples = self.samples","1200","        cdef SIZE_t i, p, k","1201","        cdef DOUBLE_t y_ik","1202","        cdef DOUBLE_t w_y_ik","1203","","1204","        cdef double impurity = 0.0","1205","","1206","        for k in range(self.n_outputs):","1207","            for p in range(self.start, self.end):","1208","                i = samples[p]","1209","","1210","                y_ik = y[i * self.y_stride + k]","1211","","1212","                impurity += <double> fabs((<double> y_ik) - <double> self.node_medians[k])","1213","        return impurity \/ (self.weighted_n_node_samples * self.n_outputs)","1214","","1215","    cdef void children_impurity(self, double* impurity_left,","1216","                                double* impurity_right) nogil:","1217","        \"\"\"Evaluate the impurity in children nodes, i.e. the impurity of the","1218","           left child (samples[start:pos]) and the impurity the right child","1219","           (samples[pos:end]).","1220","        \"\"\"","1221","","1222","        cdef DOUBLE_t* y = self.y","1223","        cdef DOUBLE_t* sample_weight = self.sample_weight","1224","        cdef SIZE_t* samples = self.samples","1225","","1226","        cdef SIZE_t start = self.start","1227","        cdef SIZE_t pos = self.pos","1228","        cdef SIZE_t end = self.end","1229","","1230","        cdef SIZE_t i, p, k","1231","        cdef DOUBLE_t y_ik","1232","        cdef DOUBLE_t median","1233","","1234","        cdef void** left_child = <void**> self.left_child.data","1235","        cdef void** right_child = <void**> self.right_child.data","1236","","1237","        impurity_left[0] = 0.0","1238","        impurity_right[0] = 0.0","1239","","1240","        for k in range(self.n_outputs):","1241","            median = (<WeightedMedianCalculator> left_child[k]).get_median()","1242","            for p in range(start, pos):","1243","                i = samples[p]","1244","","1245","                y_ik = y[i * self.y_stride + k]","1246","","1247","                impurity_left[0] += <double>fabs((<double> y_ik) -","1248","                                                 <double> median)","1249","        impurity_left[0] \/= <double>((self.weighted_n_left) * self.n_outputs)","1250","","1251","        for k in range(self.n_outputs):","1252","            median = (<WeightedMedianCalculator> right_child[k]).get_median()","1253","            for p in range(pos, end):","1254","                i = samples[p]","1255","","1256","                y_ik = y[i * self.y_stride + k]","1257","","1258","                impurity_right[0] += <double>fabs((<double> y_ik) -","1259","                                                  <double> median)","1260","        impurity_right[0] \/= <double>((self.weighted_n_right) *","1261","                                      self.n_outputs)","1262",""],"delete":["686","    def __cinit__(self, SIZE_t n_outputs):"]}],"sklearn\/ensemble\/tests\/test_forest.py":[{"add":["161","    for name, criterion in product(FOREST_REGRESSORS, (\"mse\", \"mae\", \"friedman_mse\")):","246","    for name, criterion in product(FOREST_REGRESSORS, [\"mse\", \"friedman_mse\", \"mae\"]):"],"delete":["161","    for name, criterion in product(FOREST_REGRESSORS, (\"mse\", )):","246","    for name, criterion in product(FOREST_REGRESSORS, [\"mse\", \"friedman_mse\"]):"]}],"doc\/whats_new.rst":[{"add":["119","   - Added a new splitting criterion for :class:`tree.DecisionTreeRegressor`,","120","     the mean absolute error. This criterion can also be used in","121","     :class:`ensemble.ExtraTreesRegressor`,","122","     :class:`ensemble.RandomForestRegressor`, and the gradient boosting","123","     estimators. (`#6667","124","     <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/6667>`_) by `Nelson","125","     Liu`_.","126","","152","   - Gradient boosting estimators accept the parameter ``criterion`` to specify","153","     to splitting criterion used in built decision trees. (`#6667","154","     <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/6667>`_) by `Nelson","155","     Liu`_.","156","","4301","","4302",".. _Nelson Liu: https:\/\/github.com\/nelson-liu"],"delete":[]}],"sklearn\/tree\/_utils.pxd":[{"add":["4","#          Nelson Liu <nelson@nelsonliu.me>","36","    (WeightedPQueueRecord*)","37","    (DOUBLE_t*)","109","","110","# =============================================================================","111","# WeightedPQueue data structure","112","# =============================================================================","113","","114","# A record stored in the WeightedPQueue","115","cdef struct WeightedPQueueRecord:","116","    DOUBLE_t data","117","    DOUBLE_t weight","118","","119","cdef class WeightedPQueue:","120","    cdef SIZE_t capacity","121","    cdef SIZE_t array_ptr","122","    cdef WeightedPQueueRecord* array_","123","","124","    cdef bint is_empty(self) nogil","125","    cdef void reset(self) nogil","126","    cdef SIZE_t size(self) nogil","127","    cdef int push(self, DOUBLE_t data, DOUBLE_t weight) nogil","128","    cdef int remove(self, DOUBLE_t data, DOUBLE_t weight) nogil","129","    cdef int pop(self, DOUBLE_t* data, DOUBLE_t* weight) nogil","130","    cdef int peek(self, DOUBLE_t* data, DOUBLE_t* weight) nogil","131","    cdef DOUBLE_t get_weight_from_index(self, SIZE_t index) nogil","132","    cdef DOUBLE_t get_value_from_index(self, SIZE_t index) nogil","133","","134","","135","# =============================================================================","136","# WeightedMedianCalculator data structure","137","# =============================================================================","138","","139","cdef class WeightedMedianCalculator:","140","    cdef SIZE_t initial_capacity","141","    cdef WeightedPQueue samples","142","    cdef DOUBLE_t total_weight","143","    cdef SIZE_t k","144","    cdef DOUBLE_t sum_w_0_k            # represents sum(weights[0:k])","145","                                       # = w[0] + w[1] + ... + w[k-1]","146","","147","    cdef SIZE_t size(self) nogil","148","    cdef int push(self, DOUBLE_t data, DOUBLE_t weight) nogil","149","    cdef void reset(self) nogil","150","    cdef int update_median_parameters_post_push(self, DOUBLE_t data,","151","                                                DOUBLE_t weight,","152","                                                DOUBLE_t original_median) nogil","153","    cdef int remove(self, DOUBLE_t data, DOUBLE_t weight) nogil","154","    cdef int pop(self, DOUBLE_t* data, DOUBLE_t* weight) nogil","155","    cdef int update_median_parameters_post_remove(self, DOUBLE_t data,","156","                                                  DOUBLE_t weight,","157","                                                  DOUBLE_t original_median) nogil","158","    cdef DOUBLE_t get_median(self) nogil"],"delete":[]}],"sklearn\/ensemble\/forest.py":[{"add":["950","        The function to measure the quality of a split. Supported criteria","951","        are \"mse\" for the mean squared error, which is equal to variance","952","        reduction as feature selection criterion, and \"mae\" for the mean","953","        absolute error.","1304","        The function to measure the quality of a split. Supported criteria","1305","        are \"mse\" for the mean squared error, which is equal to variance","1306","        reduction as feature selection criterion, and \"mae\" for the mean","1307","        absolute error."],"delete":["950","        The function to measure the quality of a split. The only supported","951","        criterion is \"mse\" for the mean squared error.","1302","        The function to measure the quality of a split. The only supported","1303","        criterion is \"mse\" for the mean squared error."]}],"sklearn\/tree\/tests\/test_tree.py":[{"add":["50","REG_CRITERIONS = (\"mse\", \"mae\")","1445","","1446","def test_mae():","1447","    # check MAE criterion produces correct results","1448","    # on small toy dataset","1449","    dt_mae = DecisionTreeRegressor(random_state=0, criterion=\"mae\",","1450","                                   max_leaf_nodes=2)","1451","    dt_mae.fit([[3],[5],[3],[8],[5]],[6,7,3,4,3])","1452","    assert_array_equal(dt_mae.tree_.impurity, [1.4, 1.5, 4.0\/3.0])","1453","    assert_array_equal(dt_mae.tree_.value.flat, [4, 4.5, 4.0])","1454","","1455","    dt_mae.fit([[3],[5],[3],[8],[5]],[6,7,3,4,3], [0.6,0.3,0.1,1.0,0.3])","1456","    assert_array_equal(dt_mae.tree_.impurity, [7.0\/2.3, 3.0\/0.7, 4.0\/1.6])","1457","    assert_array_equal(dt_mae.tree_.value.flat, [4.0, 6.0, 4.0])"],"delete":["50","REG_CRITERIONS = (\"mse\", )"]}]}},"3250f15967d2c6a8360316246361c8b4b9775468":{"changes":{"doc\/themes\/scikit-learn\/static\/nature.css_t":"MODIFY"},"diff":{"doc\/themes\/scikit-learn\/static\/nature.css_t":[{"add":["118","        z-index: 10;","140","        z-index: 9;","1190","  font-size: 8px;","1269","  padding-bottom: 5px;"],"delete":["1199","a.btn.dropdown-toggle,  a.btn.dropdown-toggle:hover{","1200","  vertical-align: baseline;","1201","}","1202","","1270","  padding-bottom: 8px;"]}]}},"1cf192b2e59e71bdcc788f051895e4f9fb45a8ee":{"changes":{"sklearn\/discriminant_analysis.py":"MODIFY"},"diff":{"sklearn\/discriminant_analysis.py":[{"add":["146","       Deprecated :class:`lda.LDA` have been moved to :class:`LinearDiscriminantAnalysis`.","558","       Deprecated :class:`qda.QDA` have been moved to :class:`QuadraticDiscriminantAnalysis`.","559","","560","    Read more in the :ref:`User Guide <lda_qda>`."],"delete":["146","       Deprecated :class:`lda.LDA` have been moved to *LinearDiscriminantAnalysis*.","558","       Deprecated :class:`qda.QDA` have been moved to *QuadraticDiscriminantAnalysis*."]}]}},"d9b8ece02c66d7d48179a2c04326b8e027fbebf1":{"changes":{"sklearn\/linear_model\/coordinate_descent.py":"MODIFY"},"diff":{"sklearn\/linear_model\/coordinate_descent.py":[{"add":["643","        check_input : boolean, (default=True)","644","            Allow to bypass several input checking.","645","            Don't use this parameter unless you know what you do.","646",""],"delete":[]}]}},"7bde7e7fa7ff6e0fefd7fbb9ffa98cf407dd181b":{"changes":{"doc\/whats_new.rst":"MODIFY"},"diff":{"doc\/whats_new.rst":[{"add":["127","   - Added weighted impurity-based early stopping criterion for decision tree","128","     growth. (`#6954","129","     <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/6954>`_) by `Nelson","130","     Liu`_"],"delete":["127","     - Added weighted impurity-based early stopping criterion for decision tree","128","       growth. (`#6954","129","       <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/6954>`_) by `Nelson","130","       Liu`_"]}]}},"1220ca3048a0cc184f5d1cd9ee9c03b18e55cb3d":{"changes":{"doc\/modules\/pipeline.rst":"MODIFY"},"diff":{"doc\/modules\/pipeline.rst":[{"add":["32","The :class:`Pipeline` is built using a list of ``(key, value)`` pairs, where","33","the ``key`` is a string containing the name you want to give this step and ``value``"],"delete":["32","The :class:`Pipeline` is build using a list of ``(key, value)`` pairs, where","33","the ``key`` a string containing the name you want to give this step and ``value``"]}]}},"58b35d8b9feb55422ed1b0f210f985860213d858":{"changes":{"doc\/modules\/manifold.rst":"MODIFY","doc\/modules\/gaussian_process.rst":"MODIFY","doc\/tutorial\/statistical_inference\/model_selection.rst":"MODIFY","sklearn\/covariance\/tests\/test_graph_lasso.py":"MODIFY","sklearn\/utils\/estimator_checks.py":"MODIFY","sklearn\/gaussian_process\/gpr.py":"MODIFY","doc\/developers\/contributing.rst":"MODIFY","doc\/datasets\/rcv1_fixture.py":"MODIFY","sklearn\/datasets\/mldata.py":"MODIFY","sklearn\/tests\/test_base.py":"MODIFY","sklearn\/feature_extraction\/image.py":"MODIFY","examples\/hetero_feature_union.py":"MODIFY","doc\/tutorial\/text_analytics\/working_with_text_data_fixture.py":"MODIFY","sklearn\/metrics\/tests\/test_pairwise.py":"MODIFY"},"diff":{"doc\/modules\/manifold.rst":[{"add":["61","Component Analysis (PCA), Independent Component Analysis, Linear","62","Discriminant Analysis, and others.  These algorithms define specific","64","These methods can be powerful, but often miss important non-linear","93","The manifold learning implementations available in scikit-learn are","123","   for this are *Dijkstra's Algorithm*, which is approximately","129","3. **Partial eigenvalue decomposition.**  The embedding is encoded in the","193","","223","","234","   weight matrix from multiple weights.  In practice, the added cost of","249","","273","","310","The graph generated can be considered as a discrete approximation of the","311","low dimensional manifold in the high dimensional space. Minimization of a","312","cost function based on the graph ensures that points close to each other on","313","the manifold are mapped close to each other in the low dimensional space,","328","   :math:`L = D^{-\\frac{1}{2}} (D - A) D^{-\\frac{1}{2}}`.","330","3. **Partial Eigenvalue Decomposition**. Eigenvalue decomposition is","344","     and Data Representation\"","356","tangent space, and performs a global optimization to align these local","423","","458","","501","and select the embedding with the lowest KL divergence.","554","but less accurate results.","562","is the number of output dimensions and :math:`N` is the number of samples. The","563","Barnes-Hut method improves on the exact method where t-SNE complexity is"],"delete":["61","Component Analysis (PCA), Independent Component Analysis, Linear ","62","Discriminant Analysis, and others.  These algorithms define specific ","64","These methods can be powerful, but often miss important non-linear ","93","The manifold learning implementations available in sklearn are","123","   for this are *Dijkstra's Algorithm*, which is approximately ","129","3. **Partial eigenvalue decomposition.**  The embedding is encoded in the ","193","   ","223","   ","234","   weight matrix from multiple weights.  In practice, the added cost of ","249","     ","273","   ","310","The graph generated can be considered as a discrete approximation of the ","311","low dimensional manifold in the high dimensional space. Minimization of a ","312","cost function based on the graph ensures that points close to each other on ","313","the manifold are mapped close to each other in the low dimensional space, ","328","   :math:`L = D^{-\\frac{1}{2}} (D - A) D^{-\\frac{1}{2}}`.  ","330","3. **Partial Eigenvalue Decomposition**. Eigenvalue decomposition is ","344","     and Data Representation\" ","356","tangent space, and performs a global optimization to align these local ","423"," ","458","  ","501","and select the embedding with the lowest KL divergence. ","554","but less accurate results. ","562","is the number of output dimensions and :math:`N` is the number of samples. The ","563","Barnes-Hut method improves on the exact method where t-SNE complexity is "]}],"doc\/modules\/gaussian_process.rst":[{"add":["68","the API of standard scikit-learn estimators, GaussianProcessRegressor:","166","This example is based on Section 5.4.3 of [RW2006]_.","604","      <http:\/\/www.gaussianprocess.org\/gpml\/chapters\/>`_","605","      **Gaussian Processes for Machine Learning**,","606","      Carl Eduard Rasmussen and Christopher K.I. Williams, MIT Press 2006.","607","      Link to an official complete PDF version of the book","608","      `here <http:\/\/www.gaussianprocess.org\/gpml\/chapters\/RW.pdf>`_ .","618","In this section, the implementation of Gaussian processes used in scikit-learn","619","until release 0.16.1 is described. Note that this implementation is deprecated","620","and will be removed in version 0.18."],"delete":["68","the API of standard sklearn estimators, GaussianProcessRegressor:","166","This example is based on Section 5.4.3 of [RW2006]_. ","604","      <http:\/\/www.gaussianprocess.org\/gpml\/chapters\/>`_ ","605","      **Gaussian Processes for Machine Learning**, ","606","      Carl Eduard Rasmussen and Christopher K.I. Williams, MIT Press 2006. ","607","      Link to an official complete PDF version of the book ","608","      `here <http:\/\/www.gaussianprocess.org\/gpml\/chapters\/RW.pdf>`_ . ","618","In this section, the implementation of Gaussian processes used in sklearn until","619","release 0.16.1 is described. Note that this implementation is deprecated and","620","will be removed in version 0.18."]}],"doc\/tutorial\/statistical_inference\/model_selection.rst":[{"add":["209","scikit-learn provides an object that, given data, computes the score","259","algorithm-by-algorithm basis. This is why, for certain estimators,","260","scikit-learn exposes :ref:`cross_validation` estimators that set their","261","parameter automatically by cross-validation::"],"delete":["209","The sklearn provides an object that, given data, computes the score","259","algorithm-by-algorithm basis. This is why for certain estimators the","260","sklearn exposes :ref:`cross_validation` estimators that set their parameter","261","automatically by cross-validation::"]}],"sklearn\/covariance\/tests\/test_graph_lasso.py":[{"add":["63","    # The iris datasets in R and scikit-learn do not match in a few places,","64","    # these values are for the scikit-learn version."],"delete":["63","    # The iris datasets in R and sklearn do not match in a few places, these","64","    # values are for the sklearn version"]}],"sklearn\/utils\/estimator_checks.py":[{"add":["222","    \"\"\"Check if estimator adheres to scikit-learn conventions."],"delete":["222","    \"\"\"Check if estimator adheres to sklearn conventions."]}],"sklearn\/gaussian_process\/gpr.py":[{"add":["25","    In addition to standard scikit-learn estimator API,","26","    GaussianProcessRegressor:"],"delete":["25","    In addition to standard sklearn estimator API, GaussianProcessRegressor:"]}],"doc\/developers\/contributing.rst":[{"add":["873","whether it is just for you or for contributing it to scikit-learn, there are","874","several internals of scikit-learn that you should be aware of in addition to","875","the scikit-learn API outlined above. You can check whether your estimator","931","All scikit-learn estimators have ``get_params`` and ``set_params`` functions."],"delete":["873","whether it is just for you or for contributing it to sklearn, there are several","874","internals of scikit-learn that you should be aware of in addition to the","875","sklearn API outlined above. You can check whether your estimator","931","All sklearn estimator have ``get_params`` and ``set_params`` functions."]}],"doc\/datasets\/rcv1_fixture.py":[{"add":["3","stateless hence will not cache the dataset as regular scikit-learn users would do."],"delete":["3","stateless hence will not cache the dataset as regular sklearn users would do."]}],"sklearn\/datasets\/mldata.py":[{"add":["105","    to respects the scikit-learn axes convention:","207","    # set axes to scikit-learn conventions"],"delete":["105","    to respects the sklearn axes convention:","207","    # set axes to sklearn conventions"]}],"sklearn\/tests\/test_base.py":[{"add":["75","    \"\"\"scikit-learn estimators shouldn't have vargs.\"\"\""],"delete":["75","    \"\"\"Sklearn estimators shouldn't have vargs.\"\"\""]}],"sklearn\/feature_extraction\/image.py":[{"add":["154","    For scikit-learn versions 0.14.1 and prior, return_as=np.ndarray was","155","    handled by returning a dense np.matrix instance.  Going forward, np.ndarray","190","    For scikit-learn versions 0.14.1 and prior, return_as=np.ndarray was","191","    handled by returning a dense np.matrix instance.  Going forward, np.ndarray"],"delete":["154","    For sklearn versions 0.14.1 and prior, return_as=np.ndarray was handled","155","    by returning a dense np.matrix instance.  Going forward, np.ndarray","190","    For sklearn versions 0.14.1 and prior, return_as=np.ndarray was handled","191","    by returning a dense np.matrix instance.  Going forward, np.ndarray"]}],"examples\/hetero_feature_union.py":[{"add":["53","    Please note that this is the opposite convention to scikit-learn feature"],"delete":["53","    Please note that this is the opposite convention to sklearn feature"]}],"doc\/tutorial\/text_analytics\/working_with_text_data_fixture.py":[{"add":["3","stateless hence will not cache the dataset as regular scikit-learn users would."],"delete":["3","stateless hence will not cache the dataset as regular sklearn users would do."]}],"sklearn\/metrics\/tests\/test_pairwise.py":[{"add":["63","    # \"cityblock\" uses scikit-learn metric, cityblock (function) is","64","    # scipy.spatial.","81","    # The string \"cosine\" uses sklearn.metric,","82","    # while the function cosine is scipy.spatial","334","    # Non-euclidean scikit-learn metric"],"delete":["63","    # \"cityblock\" uses sklearn metric, cityblock (function) is scipy.spatial.","80","    # \"cosine\" uses sklearn metric, cosine (function) is scipy.spatial","332","    # Non-euclidean sklearn metric"]}]}},"b8be0198b6b66a67721a19d247a8343486578b73":{"changes":{"sklearn\/datasets\/base.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/datasets\/tests\/test_base.py":"MODIFY"},"diff":{"sklearn\/datasets\/base.py":[{"add":["244","def load_iris(return_X_y=False):","260","    Parameters","261","    ----------","262","    ","263","        .. versionadded:: 0.18","264","     ","265","    return_X_y : boolean, default=False.","266","        If True, returns ``(data, target)`` instead of a Bunch object.","267","        See below for more information about the `data` and `target` object.","268","","278","    (data, target) : tuple if ``return_X_y`` is True","279","","309","    if return_X_y:","310","        return data, target","311",""],"delete":["244","def load_iris():"]}],"doc\/whats_new.rst":[{"add":["232","   - Added new return type ``(data, target)`` : tuple option to :func:`load_iris` dataset. (`#7049 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/7049>`_) ","233","     By `Manvendra Singh`_ and `Nelson Liu`_.   ","234","","4312","","4313",".. _Manvendra Singh: https:\/\/github.com\/manu-chroma"],"delete":[]}],"sklearn\/datasets\/tests\/test_base.py":[{"add":["28","from sklearn.utils.testing import assert_array_equal","183","    # test return_X_y option","184","    X_y_tuple = load_iris(return_X_y=True)","185","    bunch = load_iris()","186","    assert_true(isinstance(X_y_tuple, tuple))","187","    assert_array_equal(X_y_tuple[0], bunch.data)","188","    assert_array_equal(X_y_tuple[1], bunch.target)","189",""],"delete":[]}]}},"c948319b0ab31a45a365ac4ac22ce9d311c8306a":{"changes":{"sklearn\/manifold\/tests\/test_locally_linear.py":"MODIFY","sklearn\/manifold\/locally_linear.py":"MODIFY"},"diff":{"sklearn\/manifold\/tests\/test_locally_linear.py":[{"add":["136","","137","","138","# regression test for #6033","139","def test_integer_input():","140","    rand = np.random.RandomState(0)","141","    X = rand.randint(0, 100, size=(20, 3))","142","","143","    for method in [\"standard\", \"hessian\", \"modified\", \"ltsa\"]:","144","        clf = manifold.LocallyLinearEmbedding(method=method, n_neighbors=10)","145","        clf.fit(X)  # this previously raised a TypeError"],"delete":[]}],"sklearn\/manifold\/locally_linear.py":[{"add":["628","        X = check_array(X, dtype=float)"],"delete":["628","        X = check_array(X)"]}]}},"3b95d5fc2a4415e2dc0370ec45ee5942f50cad03":{"changes":{"sklearn\/cluster\/tests\/test_k_means.py":"MODIFY"},"diff":{"sklearn\/cluster\/tests\/test_k_means.py":[{"add":["802","                # ensure the extracted row is a 2d array","803","                X_test_0 = X_test[0] if is_sparse else X_test[0].reshape(1, -1)","805","                assert_equal(estimator.predict(X_test_0), estimator.labels_[0])"],"delete":["803","                assert_equal(estimator.predict(X_test[0]), estimator.labels_[0])"]}]}},"204472af0fb2d3ef038caf303df16ae245449201":{"changes":{"sklearn\/svm\/src\/libsvm\/svm.cpp":"MODIFY"},"diff":{"sklearn\/svm\/src\/libsvm\/svm.cpp":[{"add":["1011","\tif(Gmax+Gmax2 < eps || Gmin_idx == -1)","1263","\tif(max(Gmaxp+Gmaxp2,Gmaxn+Gmaxn2) < eps || Gmin_idx == -1)"],"delete":["1011","\tif(Gmax+Gmax2 < eps)","1263","\tif(max(Gmaxp+Gmaxp2,Gmaxn+Gmaxn2) < eps)"]}]}},"31a4691d7671648cb44fcdf0fc9410727200946a":{"changes":{"sklearn\/model_selection\/_split.py":"MODIFY","sklearn\/utils\/random.py":"MODIFY","sklearn\/tests\/test_cross_validation.py":"MODIFY","sklearn\/cross_validation.py":"MODIFY","sklearn\/model_selection\/tests\/test_split.py":"MODIFY","doc\/whats_new.rst":"MODIFY"},"diff":{"sklearn\/model_selection\/_split.py":[{"add":["32","from ..utils.random import choice","1101","def _approximate_mode(class_counts, n_draws, rng):","1102","    \"\"\"Computes approximate mode of multivariate hypergeometric.","1103","","1104","    This is an approximation to the mode of the multivariate","1105","    hypergeometric given by class_counts and n_draws.","1106","    It shouldn't be off by more than one.","1107","","1108","    It is the mostly likely outcome of drawing n_draws many","1109","    samples from the population given by class_counts.","1110","","1111","    Parameters","1112","    ----------","1113","    class_counts : ndarray of int","1114","        Population per class.","1115","    n_draws : int","1116","        Number of draws (samples to draw) from the overall population.","1117","    rng : random state","1118","        Used to break ties.","1119","","1120","    Returns","1121","    -------","1122","    sampled_classes : ndarray of int","1123","        Number of samples drawn from each class.","1124","        np.sum(sampled_classes) == n_draws","1125","","1126","    Examples","1127","    --------","1128","    >>> from sklearn.model_selection._split import _approximate_mode","1129","    >>> _approximate_mode(class_counts=np.array([4, 2]), n_draws=3, rng=0)","1130","    array([2, 1])","1131","    >>> _approximate_mode(class_counts=np.array([5, 2]), n_draws=4, rng=0)","1132","    array([3, 1])","1133","    >>> _approximate_mode(class_counts=np.array([2, 2, 2, 1]),","1134","    ...                   n_draws=2, rng=0)","1135","    array([0, 1, 1, 0])","1136","    >>> _approximate_mode(class_counts=np.array([2, 2, 2, 1]),","1137","    ...                   n_draws=2, rng=42)","1138","    array([1, 1, 0, 0])","1139","    \"\"\"","1140","    # this computes a bad approximation to the mode of the","1141","    # multivariate hypergeometric given by class_counts and n_draws","1142","    continuous = n_draws * class_counts \/ class_counts.sum()","1143","    # floored means we don't overshoot n_samples, but probably undershoot","1144","    floored = np.floor(continuous)","1145","    # we add samples according to how much \"left over\" probability","1146","    # they had, until we arrive at n_samples","1147","    need_to_add = int(n_draws - floored.sum())","1148","    if need_to_add > 0:","1149","        remainder = continuous - floored","1150","        values = np.sort(np.unique(remainder))[::-1]","1151","        # add according to remainder, but break ties","1152","        # randomly to avoid biases","1153","        for value in values:","1154","            inds, = np.where(remainder == value)","1155","            # if we need_to_add less than what's in inds","1156","            # we draw randomly from them.","1157","            # if we need to add more, we add them all and","1158","            # go to the next value","1159","            add_now = min(len(inds), need_to_add)","1160","            inds = choice(inds, size=add_now, replace=False, random_state=rng)","1161","            floored[inds] += 1","1162","            need_to_add -= add_now","1163","            if need_to_add == 0:","1164","                break","1165","    return floored.astype(np.int)","1166","","1167","","1253","            # if there are ties in the class-counts, we want","1254","            # to make sure to break them anew in each iteration","1255","            n_i = _approximate_mode(class_counts, n_train, rng)","1256","            class_counts_remaining = class_counts - n_i","1257","            t_i = _approximate_mode(class_counts_remaining, n_test, rng)","1258",""],"delete":["1183","        p_i = class_counts \/ float(n_samples)","1184","        n_i = np.round(n_train * p_i).astype(int)","1185","        t_i = np.minimum(class_counts - n_i,","1186","                         np.round(n_test * p_i).astype(int))","1198","","1199","            # Because of rounding issues (as n_train and n_test are not","1200","            # dividers of the number of elements per class), we may end","1201","            # up here with less samples in train and test than asked for.","1202","            if len(train) + len(test) < n_train + n_test:","1203","                # We complete by affecting randomly the missing indexes","1204","                missing_indices = np.where(bincount(train + test,","1205","                                                    minlength=len(y)) == 0)[0]","1206","                missing_indices = rng.permutation(missing_indices)","1207","                n_missing_train = n_train - len(train)","1208","                n_missing_test = n_test - len(test)","1209","","1210","                if n_missing_train > 0:","1211","                    train.extend(missing_indices[:n_missing_train])","1212","                if n_missing_test > 0:","1213","                    test.extend(missing_indices[-n_missing_test:])","1214",""]}],"sklearn\/utils\/random.py":[{"add":["125","    if p is not None:","144","        if p is not None:","158","        if p is not None:"],"delete":["125","    if None != p:","144","        if None != p:","158","        if None != p:"]}],"sklearn\/tests\/test_cross_validation.py":[{"add":["481","          np.array([0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2] * 2),","489","        test_size = np.ceil(0.33 * len(y))","490","        train_size = len(y) - test_size","494","            p_train = (np.bincount(np.unique(y[train],","495","                                   return_inverse=True)[1]) \/","496","                       float(len(y[train])))","497","            p_test = (np.bincount(np.unique(y[test],","498","                                  return_inverse=True)[1]) \/","499","                      float(len(y[test])))","501","            assert_equal(len(train) + len(test), y.size)","502","            assert_equal(len(train), train_size)","503","            assert_equal(len(test), test_size)","504","            assert_array_equal(np.lib.arraysetops.intersect1d(train, test), [])"],"delete":["481","          np.array([0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2]),","492","            p_train = (np.bincount(np.unique(y[train], return_inverse=True)[1])","493","                       \/ float(len(y[train])))","494","            p_test = (np.bincount(np.unique(y[test], return_inverse=True)[1])","495","                      \/ float(len(y[test])))","497","            assert_equal(y[train].size + y[test].size, y.size)","498","            assert_array_equal(np.intersect1d(train, test), [])"]}],"sklearn\/cross_validation.py":[{"add":["29","from .utils.random import choice","417","                (\"Cannot have number of folds n_folds={0} greater\"","418","                 \" than the number of labels: {1}.\").format(n_folds,","419","                                                            n_labels))","909","def _approximate_mode(class_counts, n_draws, rng):","910","    \"\"\"Computes approximate mode of multivariate hypergeometric.","911","","912","    This is an approximation to the mode of the multivariate","913","    hypergeometric given by class_counts and n_draws.","914","    It shouldn't be off by more than one.","915","","916","    It is the mostly likely outcome of drawing n_draws many","917","    samples from the population given by class_counts.","918","","919","    Parameters","920","    ----------","921","    class_counts : ndarray of int","922","        Population per class.","923","    n_draws : int","924","        Number of draws (samples to draw) from the overall population.","925","    rng : random state","926","        Used to break ties.","927","","928","    Returns","929","    -------","930","    sampled_classes : ndarray of int","931","        Number of samples drawn from each class.","932","        np.sum(sampled_classes) == n_draws","933","    \"\"\"","934","    # this computes a bad approximation to the mode of the","935","    # multivariate hypergeometric given by class_counts and n_draws","936","    continuous = n_draws * class_counts \/ class_counts.sum()","937","    # floored means we don't overshoot n_samples, but probably undershoot","938","    floored = np.floor(continuous)","939","    # we add samples according to how much \"left over\" probability","940","    # they had, until we arrive at n_samples","941","    need_to_add = int(n_draws - floored.sum())","942","    if need_to_add > 0:","943","        remainder = continuous - floored","944","        values = np.sort(np.unique(remainder))[::-1]","945","        # add according to remainder, but break ties","946","        # randomly to avoid biases","947","        for value in values:","948","            inds, = np.where(remainder == value)","949","            # if we need_to_add less than what's in inds","950","            # we draw randomly from them.","951","            # if we need to add more, we add them all and","952","            # go to the next value","953","            add_now = min(len(inds), need_to_add)","954","            inds = choice(inds, size=add_now, replace=False, random_state=rng)","955","            floored[inds] += 1","956","            need_to_add -= add_now","957","            if need_to_add == 0:","958","                    break","959","    return floored.astype(np.int)","960","","961","","1049","            # if there are ties in the class-counts, we want","1050","            # to make sure to break them anew in each iteration","1051","            n_i = _approximate_mode(cls_count, self.n_train, rng)","1052","            class_counts_remaining = cls_count - n_i","1053","            t_i = _approximate_mode(class_counts_remaining, self.n_test, rng)","1054","","1058","            for i, _ in enumerate(self.classes):","1060","                perm_indices_class_i = np.where(","1061","                    (i == self.y_indices))[0][permutation]","1063","                train.extend(perm_indices_class_i[:n_i[i]])","1064","                test.extend(perm_indices_class_i[n_i[i]:n_i[i] + t_i[i]])"],"delete":["416","                    (\"Cannot have number of folds n_folds={0} greater\"","417","                     \" than the number of labels: {1}.\").format(n_folds,","418","                                                                n_labels))","993","        p_i = cls_count \/ float(self.n)","994","        n_i = np.round(self.n_train * p_i).astype(int)","995","        t_i = np.minimum(cls_count - n_i,","996","                         np.round(self.n_test * p_i).astype(int))","1002","            for i, cls in enumerate(self.classes):","1004","                cls_i = np.where((self.y == cls))[0][permutation]","1006","                train.extend(cls_i[:n_i[i]])","1007","                test.extend(cls_i[n_i[i]:n_i[i] + t_i[i]])","1008","","1009","            # Because of rounding issues (as n_train and n_test are not","1010","            # dividers of the number of elements per class), we may end","1011","            # up here with less samples in train and test than asked for.","1012","            if len(train) + len(test) < self.n_train + self.n_test:","1013","                # We complete by affecting randomly the missing indexes","1014","                missing_idx = np.where(bincount(train + test,","1015","                                                minlength=len(self.y)) == 0,","1016","                                       )[0]","1017","                missing_idx = rng.permutation(missing_idx)","1018","                n_missing_train = self.n_train - len(train)","1019","                n_missing_test = self.n_test - len(test)","1020","","1021","                if n_missing_train > 0:","1022","                    train.extend(missing_idx[:n_missing_train])","1023","                if n_missing_test > 0:","1024","                    test.extend(missing_idx[-n_missing_test:])","1025",""]}],"sklearn\/model_selection\/tests\/test_split.py":[{"add":["537","def test_stratified_shuffle_split_respects_test_size():","538","    y = np.array([0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2])","539","    test_size = 5","540","    train_size = 10","541","    sss = StratifiedShuffleSplit(6, test_size=test_size, train_size=train_size,","542","                                 random_state=0).split(np.ones(len(y)), y)","543","    for train, test in sss:","544","        assert_equal(len(train), train_size)","545","        assert_equal(len(test), test_size)","546","","547","","551","          np.array([0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2] * 2),","553","          np.array([-1] * 800 + [1] * 50),","554","          np.concatenate([[i] * (100 + i) for i in range(11)])","560","        # this is how test-size is computed internally","561","        # in _validate_shuffle_split","562","        test_size = np.ceil(0.33 * len(y))","563","        train_size = len(y) - test_size","574","            assert_equal(len(train) + len(test), y.size)","575","            assert_equal(len(train), train_size)","576","            assert_equal(len(test), test_size)","592","            prob = bf.pmf(count)","593","            assert_true(prob > threshold,","613","        n_train, n_test = _validate_shuffle_split(","614","            n_samples, test_size=1. \/ n_folds, train_size=1. - (1. \/ n_folds))","675","        test_size = 1. \/ 3"],"delete":["540","          np.array([0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2]),","542","          np.array([-1] * 800 + [1] * 50)","558","            assert_equal(y[train].size + y[test].size, y.size)","574","            p = bf.pmf(count)","575","            assert_true(p > threshold,","595","        n_train, n_test = _validate_shuffle_split(n_samples,","596","                                                  test_size=1.\/n_folds,","597","                                                  train_size=1.-(1.\/n_folds))","658","        test_size = 1.\/3"]}],"doc\/whats_new.rst":[{"add":["399","    - Fix in :class:`sklearn.model_selection.StratifiedShuffleSplit` to","400","      return splits of size ``train_size`` and ``test_size`` in all cases","401","      (`#6472 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/6472>`).","402","      By `Andreas Mller`_.","403",""],"delete":[]}]}},"3cc7fead338bded628184ecef25516745a2067e3":{"changes":{"sklearn\/svm\/classes.py":"MODIFY","sklearn\/svm\/tests\/test_svm.py":"MODIFY"},"diff":{"sklearn\/svm\/classes.py":[{"add":["8","from ..utils import check_X_y, column_or_1d","331","    def fit(self, X, y, sample_weight=None):","376","            epsilon=self.epsilon, sample_weight=sample_weight)","768","    sample_weight : array-like, shape = [n_samples]","769","            Individual weights for each sample","770",""],"delete":["8","from ..utils import check_X_y","331","    def fit(self, X, y):","376","            epsilon=self.epsilon)"]}],"sklearn\/svm\/tests\/test_svm.py":[{"add":["10","from numpy.testing import assert_allclose","198","    assert_allclose(np.linalg.norm(lsvr.coef_),","199","                    np.linalg.norm(svr.coef_), 1, 0.0001)","200","    assert_almost_equal(score1, score2, 2)","201","","202","","203","def test_linearsvr_fit_sampleweight():","204","    # check correct result when sample_weight is 1","205","    # check that SVR(kernel='linear') and LinearSVC() give","206","    # comparable results","207","    diabetes = datasets.load_diabetes()","208","    n_samples = len(diabetes.target)","209","    unit_weight = np.ones(n_samples)","210","    lsvr = svm.LinearSVR(C=1e3).fit(diabetes.data, diabetes.target,","211","                                    sample_weight=unit_weight)","212","    score1 = lsvr.score(diabetes.data, diabetes.target)","213","","214","    lsvr_no_weight = svm.LinearSVR(C=1e3).fit(diabetes.data, diabetes.target)","215","    score2 = lsvr_no_weight.score(diabetes.data, diabetes.target)","216","","217","    assert_allclose(np.linalg.norm(lsvr.coef_),","218","                    np.linalg.norm(lsvr_no_weight.coef_), 1, 0.0001)","219","    assert_almost_equal(score1, score2, 2)","220","","221","    # check that fit(X)  = fit([X1, X2, X3],sample_weight = [n1, n2, n3]) where","222","    # X = X1 repeated n1 times, X2 repeated n2 times and so forth","223","    random_state = check_random_state(0)","224","    random_weight = random_state.randint(0, 10, n_samples)","225","    lsvr_unflat = svm.LinearSVR(C=1e3).fit(diabetes.data, diabetes.target,","226","                                           sample_weight=random_weight)","227","    score3 = lsvr_unflat.score(diabetes.data, diabetes.target,","228","                               sample_weight=random_weight)","229","","230","    X_flat = np.repeat(diabetes.data, random_weight, axis=0)","231","    y_flat = np.repeat(diabetes.target, random_weight, axis=0)","232","    lsvr_flat = svm.LinearSVR(C=1e3).fit(X_flat, y_flat)","233","    score4 = lsvr_flat.score(X_flat, y_flat)","234","","235","    assert_almost_equal(score3, score4, 2)","319","                            == clf.predict(iris.data)) > 0.9)","544","                    (loss, penalty, dual) == ('hinge', 'l2', False) or","545","                    (penalty, dual) == ('l1', True) or","546","                    loss == 'foo' or penalty == 'bar'):","604","                                      \" and loss='squared_hinge' is not supported\"),","807","    X = \"foo!\"  # input validation not required when SVM not fitted"],"delete":["8","","13","","27","","200","    assert np.linalg.norm(lsvr.coef_ - svr.coef_) \/ np.linalg.norm(svr.coef_) < .1","201","    assert np.abs(score1 - score2) < 0.1","279","","286","                    == clf.predict(iris.data)) > 0.9)","511","                (loss, penalty, dual) == ('hinge', 'l2', False) or","512","                (penalty, dual) == ('l1', True) or","513","                loss == 'foo' or penalty == 'bar'):","571","                         \" and loss='squared_hinge' is not supported\"),","636","","775","    X = \"foo!\"      # input validation not required when SVM not fitted"]}]}},"dc42bde3e9588ca7ed0232ff70dc8048a1411869":{"changes":{"sklearn\/model_selection\/tests\/test_search.py":"MODIFY","sklearn\/model_selection\/tests\/test_validation.py":"MODIFY","sklearn\/model_selection\/_search.py":"MODIFY"},"diff":{"sklearn\/model_selection\/tests\/test_search.py":[{"add":["154","        assert_true(grid_search.grid_scores_[i][0] == {'foo_param': foo_i})","206","    score_no_scoring = search_no_scoring.score(X, y)","207","    score_accuracy = search_accuracy.score(X, y)","208","    score_no_score_auc = search_no_score_method_auc.score(X, y)","209","    score_auc = search_auc.score(X, y)","210",""],"delete":["22","from sklearn.utils.testing import assert_no_warnings","46","# TODO Import from sklearn.exceptions once merged.","47","from sklearn.base import ChangedBehaviorWarning","157","        assert_true(grid_search.grid_scores_[i][0]","158","                    == {'foo_param': foo_i})","210","    score_no_scoring = assert_no_warnings(search_no_scoring.score, X, y)","211","    score_accuracy = assert_warns(ChangedBehaviorWarning,","212","                                  search_accuracy.score, X, y)","213","    score_no_score_auc = assert_no_warnings(search_no_score_method_auc.score,","214","                                            X, y)","215","    score_auc = assert_warns(ChangedBehaviorWarning,","216","                             search_auc.score, X, y)"]}],"sklearn\/model_selection\/tests\/test_validation.py":[{"add":["381","        return (((y_true == y_pred).sum() - (y_true != y_pred).sum()) \/","382","                y_true.shape[0])"],"delete":["381","        return (((y_true == y_pred).sum() - (y_true != y_pred).sum())","382","                \/ y_true.shape[0])"]}],"sklearn\/model_selection\/_search.py":[{"add":["21","from ..base import MetaEstimatorMixin","235","                    \"than n_iter=%d. For exhaustive searches, use \"","236","                    \"GridSearchCV.\" % (grid_size, self.n_iter))"],"delete":["17","import warnings","22","from ..base import MetaEstimatorMixin, ChangedBehaviorWarning","236","                    \"than n_iter=%d.\" % (grid_size, self.n_iter)","237","                    + \" For exhaustive searches, use GridSearchCV.\")","406","","407","        Notes","408","        -----","409","         * The long-standing behavior of this method changed in version 0.16.","410","         * It no longer uses the metric provided by ``estimator.score`` if the","411","           ``scoring`` parameter was set when fitting.","412","","418","        if self.scoring is not None and hasattr(self.best_estimator_, 'score'):","419","            warnings.warn(\"The long-standing behavior to use the estimator's \"","420","                          \"score function in {0}.score has changed. The \"","421","                          \"scoring parameter is now used.\"","422","                          \"\".format(self.__class__.__name__),","423","                          ChangedBehaviorWarning)"]}]}},"f893565773c7783dadb217ff984c8c37801d8509":{"changes":{"doc\/whats_new.rst":"MODIFY","sklearn\/preprocessing\/tests\/test_data.py":"MODIFY","sklearn\/preprocessing\/data.py":"MODIFY"},"diff":{"doc\/whats_new.rst":[{"add":["244","   - :class:`RobustScaler` now accepts ``quantile_range`` parameter.","245","     (`#5929 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/5929>`_)","246","     By `Konstantin Podshumok`_.","247","","4333","","4334",".. _Konstantin Podshumok: https:\/\/github.com\/podshumok"],"delete":[]}],"sklearn\/preprocessing\/tests\/test_data.py":[{"add":["842","def test_robust_scaler_iris_quantiles():","843","    X = iris.data","844","    scaler = RobustScaler(quantile_range=(10, 90))","845","    X_trans = scaler.fit_transform(X)","846","    assert_array_almost_equal(np.median(X_trans, axis=0), 0)","847","    X_trans_inv = scaler.inverse_transform(X_trans)","848","    assert_array_almost_equal(X, X_trans_inv)","849","    q = np.percentile(X_trans, q=(10, 90), axis=0)","850","    q_range = q[1] - q[0]","851","    assert_array_almost_equal(q_range, 1)","852","","853","","854","def test_robust_scaler_invalid_range():","855","    for range_ in [","856","        (-1, 90),","857","        (-2, -3),","858","        (10, 101),","859","        (100.5, 101),","860","        (90, 50),","861","    ]:","862","        scaler = RobustScaler(quantile_range=range_)","863","","864","        assert_raises_regex(ValueError, 'Invalid quantile range: \\(',","865","                            scaler.fit, iris.data)","866","","867",""],"delete":[]}],"sklearn\/preprocessing\/data.py":[{"add":["0","","401","       *minmax_scale* function interface","402","       to :class:`sklearn.preprocessing.MinMaxScaler`.","527","    @deprecated(\"Attribute ``std_`` will be removed in 0.19. \"","528","                \"Use ``scale_`` instead\")","902","    the quantile range (defaults to IQR: Interquartile Range).","903","    The IQR is the range between the 1st quartile (25th quantile)","904","    and the 3rd quartile (75th quantile).","934","    quantile_range : tuple (q_min, q_max), 0.0 < q_min < q_max < 100.0","935","        Default: (25.0, 75.0) = (1st quantile, 3rd quantile) = IQR","936","        Quantile range used to calculate scale_","937","","938","        .. versionadded:: 0.18","939","","972","    def __init__(self, with_centering=True, with_scaling=True,","973","                 quantile_range=(25.0, 75.0), copy=True):","976","        self.quantile_range = quantile_range","1012","","1013","            if not 0 <= self.quantile_range[0] <= self.quantile_range[1] <= 100:","1014","                raise ValueError(\"Invalid quantile range: %s\" %","1015","                                 str(self.quantile_range))","1016","","1017","            q = np.percentile(X, self.quantile_range, axis=0)","1075","def robust_scale(X, axis=0, with_centering=True, with_scaling=True,","1076","                 quantile_range=(25.0, 75.0), copy=True):","1101","    quantile_range : tuple (q_min, q_max), 0.0 < q_min < q_max < 100.0","1102","        Default: (25.0, 75.0) = (1st quantile, 3rd quantile) = IQR","1103","        Quantile range used to calculate scale_","1104","","1105","        .. versionadded:: 0.18","1106","","1131","                     quantile_range=quantile_range, copy=copy)","1750","                  ``X[:, i]``. Each feature value should be","1751","                  in ``range(n_values[i])``"],"delete":["400","       *minmax_scale* function interface to :class:`sklearn.preprocessing.MinMaxScaler`.","525","    @deprecated(\"Attribute ``std_`` will be removed in 0.19. Use ``scale_`` instead\")","899","    the Interquartile Range (IQR). The IQR is the range between the 1st","900","    quartile (25th quantile) and the 3rd quartile (75th quantile).","962","    def __init__(self, with_centering=True, with_scaling=True, copy=True):","1000","            q = np.percentile(X, (25, 75), axis=0)","1058","def robust_scale(X, axis=0, with_centering=True, with_scaling=True, copy=True):","1107","                     copy=copy)","1726","                  ``X[:, i]``. Each feature value should be in ``range(n_values[i])``"]}]}},"f0862f7af379609c61789a8afa68eaa08b765a3c":{"changes":{"sklearn\/mixture\/tests\/test_dpgmm.py":"MODIFY","sklearn\/mixture\/tests\/test_gaussian_mixture.py":"MODIFY","doc\/modules\/classes.rst":"MODIFY","sklearn\/mixture\/__init__.py":"MODIFY","sklearn\/mixture\/bayesian_mixture.py":"ADD","sklearn\/mixture\/base.py":"MODIFY","sklearn\/mixture\/dpgmm.py":"MODIFY","doc\/whats_new.rst":"MODIFY","examples\/mixture\/plot_bayesian_gaussian_mixture.py":"ADD","sklearn\/mixture\/tests\/test_bayesian_mixture.py":"ADD","doc\/modules\/mixture.rst":"MODIFY","sklearn\/mixture\/gaussian_mixture.py":"MODIFY"},"diff":{"sklearn\/mixture\/tests\/test_dpgmm.py":[{"add":["185","    assert_warns_message(DeprecationWarning, \"The VBGMM class is not working \"","186","                         \"correctly and it's better to use \"","187","                         \"sklearn.mixture.BayesianGaussianMixture class \"","188","                         \"instead. VBGMM is deprecated in 0.18 and will be \"","189","                         \"removed in 0.20.\", VBGMM)"],"delete":["185","    assert_warns_message(","186","        DeprecationWarning,","187","        \"The VBGMM class is not working correctly and it's better to not use \"","188","        \"it. VBGMM is deprecated in 0.18 and will be removed in 0.20.\", VBGMM)"]}],"sklearn\/mixture\/tests\/test_gaussian_mixture.py":[{"add":["885","                                 \"Fitting the mixture model failed because \"","886","                                 \"some components have ill-defined empirical \"","887","                                 \"covariance (for instance caused by \"","888","                                 \"singleton or collapsed samples). Try to \"","889","                                 \"decrease the number of components, or \"","890","                                 \"increase reg_covar.\", gmm.fit, X)"],"delete":["885","                                 \"The algorithm has diverged because of too \"","886","                                 \"few samples per components. \"","887","                                 \"Try to decrease the number of components, \"","888","                                 \"or increase reg_covar.\", gmm.fit, X)"]}],"doc\/modules\/classes.rst":[{"add":["956","   mixture.BayesianGaussianMixture"],"delete":["957","   mixture.VBGMM"]}],"sklearn\/mixture\/__init__.py":[{"add":["10","from .bayesian_mixture import BayesianGaussianMixture","20","           'GaussianMixture',","21","           'BayesianGaussianMixture']"],"delete":["19","           'GaussianMixture']"]}],"sklearn\/mixture\/bayesian_mixture.py":[{"add":[],"delete":[]}],"sklearn\/mixture\/base.py":[{"add":["249","            Logarithm of the probability of each sample in X.","252","            Logarithm of the posterior probabilities (or responsibilities) of","253","            the point of each sample in X.","255","        log_prob_norm, log_resp = self._estimate_log_prob_resp(X)","256","        return np.mean(log_prob_norm), log_resp","267","            Logarithm of the posterior probabilities (or responsibilities) of","268","            the point of each sample in X."],"delete":["239","    @abstractmethod","250","            log p(X)","253","            logarithm of the responsibilities","255","        pass"]}],"sklearn\/mixture\/dpgmm.py":[{"add":["633","            \"to use sklearn.mixture.BayesianGaussianMixture class instead. \"","634","            \"VBGMM is deprecated in 0.18 and will be removed in 0.20.\")"],"delete":["633","            \"to not use it. VBGMM is deprecated in 0.18 and \"","634","            \"will be removed in 0.20.\")"]}],"doc\/whats_new.rst":[{"add":["66","    Some parameter names have changed:","67","    The ``n_folds`` parameter in :class:`model_selection.KFold`,","68","    :class:`model_selection.LabelKFold`, and","71","    :class:`model_selection.LabelShuffleSplit`,","72","    and :class:`model_selection.StratifiedShuffleSplit` is now renamed","143","   - Added new cross-validation splitter","144","     :class:`model_selection.TimeSeriesSplit` to handle time series data.","404","   - The old :class:`VBGMM` is deprecated in favor of the new","405","     :class:`BayesianGaussianMixture`. The new class solves the computational","406","     problems of the old class and computes the Variational Bayesian Gaussian","407","     mixture faster than before.","408","     Ref :ref:`b` for more information.","409","     (`#6651 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/6651>`_) by","410","     `Wei Xue`_ and `Thierry Guillemot`_.","411","","415","     (`#6666 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/6666>`_) by","416","     `Wei Xue`_ and `Thierry Guillemot`_.","426","     by the new parameter ``n_splits`` since it can provide a consistent"],"delete":["66","    Some parameter names have changed: ","67","    The ``n_folds`` parameter in :class:`model_selection.KFold`, ","68","    :class:`model_selection.LabelKFold`, and ","71","    :class:`model_selection.LabelShuffleSplit`, ","72","    and :class:`model_selection.StratifiedShuffleSplit` is now renamed ","143","   - Added new cross-validation splitter ","144","     :class:`model_selection.TimeSeriesSplit` to handle time series data. ","407","     By `Wei Xue`_ and `Thierry Guillemot`_.","417","     by the new parameter ``n_splits`` since it can provide a consistent "]}],"examples\/mixture\/plot_bayesian_gaussian_mixture.py":[{"add":[],"delete":[]}],"sklearn\/mixture\/tests\/test_bayesian_mixture.py":[{"add":[],"delete":[]}],"doc\/modules\/mixture.rst":[{"add":["135",".. _bgmm:","137","Bayesian Gaussian Mixture","138","=========================","140","The :class:`BayesianGaussianMixture` object implements a variant of the Gaussian","141","mixture model with variational inference algorithms.","143",".. _variational_inference:","144","","145","Estimation algorithm: variational inference","146","---------------------------------------------","147","","148","Variational inference is an extension of expectation-maximization that","149","maximizes a lower bound on model evidence (including","150","priors) instead of data likelihood. The principle behind","151","variational methods is the same as expectation-maximization (that is","152","both are iterative algorithms that alternate between finding the","153","probabilities for each point to be generated by each mixture and","154","fitting the mixtures to these assigned points), but variational","155","methods add regularization by integrating information from prior","156","distributions. This avoids the singularities often found in","157","expectation-maximization solutions but introduces some subtle biases","158","to the model. Inference is often notably slower, but not usually as","159","much so as to render usage unpractical.","160","","161","Due to its Bayesian nature, the variational algorithm needs more","162","hyper-parameters than expectation-maximization, the most","163","important of these being the concentration parameter ``dirichlet_concentration_prior``. Specifying","164","a high value of prior of the dirichlet concentration leads more often to uniformly-sized mixture","165","components, while specifying small (between 0 and 1) values will lead","166","to some mixture components getting almost all the points while most","167","mixture components will be centered on just a few of the remaining","168","points.","169","","170",".. figure:: ..\/auto_examples\/mixture\/images\/sphx_glr_plot_bayesian_gaussian_mixture_001.png","171","   :target: ..\/auto_examples\/mixture\/plot_bayesian_gaussian_mixture.html","172","   :align: center","173","   :scale: 50%","174","","175",".. topic:: Examples:","176","","177","    * See :ref:`plot_bayesian_gaussian_mixture.py` for a comparaison of","178","      the results of the ``BayesianGaussianMixture`` for different values","179","      of the parameter ``dirichlet_concentration_prior``.","180","","181","Pros and cons of variational inference with :class:BayesianGaussianMixture","182","--------------------------------------------------------------------------","189","   expectation-maximization solutions.","190","","191",":Automatic selection: when `dirichlet_concentration_prior` is small enough and","192","`n_components` is larger than what is found necessary by the model, the","193","Variational Bayesian mixture model has a natural tendency to set some mixture","194","weights values close to zero. This makes it possible to let the model choose a","195","suitable number of effective components automatically."],"delete":["135",".. _vbgmm:","137","VBGMM: variational Gaussian mixtures","138","====================================","140","The :class:`VBGMM` object implements a variant of the Gaussian mixture","141","model with :ref:`variational inference <variational_inference>` algorithms.","143","Pros and cons of class :class:`VBGMM`: variational inference","144","------------------------------------------------------------","151","   expectation-maximization solutions. One can then use full","152","   covariance matrices in high dimensions or in cases where some","153","   components might be centered around a single point without","154","   risking divergence.","170",".. _variational_inference:","171","","172","Estimation algorithm: variational inference","173","---------------------------------------------","174","","175","Variational inference is an extension of expectation-maximization that","176","maximizes a lower bound on model evidence (including","177","priors) instead of data likelihood.  The principle behind","178","variational methods is the same as expectation-maximization (that is","179","both are iterative algorithms that alternate between finding the","180","probabilities for each point to be generated by each mixture and","181","fitting the mixtures to these assigned points), but variational","182","methods add regularization by integrating information from prior","183","distributions. This avoids the singularities often found in","184","expectation-maximization solutions but introduces some subtle biases","185","to the model. Inference is often notably slower, but not usually as","186","much so as to render usage unpractical.","187","","188","Due to its Bayesian nature, the variational algorithm needs more","189","hyper-parameters than expectation-maximization, the most","190","important of these being the concentration parameter ``alpha``. Specifying","191","a high value of alpha leads more often to uniformly-sized mixture","192","components, while specifying small (between 0 and 1) values will lead","193","to some mixture components getting almost all the points while most","194","mixture components will be centered on just a few of the remaining","195","points.","196",""]}],"sklearn\/mixture\/gaussian_mixture.py":[{"add":["194","    covariance \/= nk.sum()","307","        \"Fitting the mixture model failed because some components have \"","308","        \"ill-defined empirical covariance (for instance caused by singleton \"","309","        \"or collapsed samples). Try to decrease the number of components, \"","360","        The determinant of the precision matrix for each component.","457","        lower bound average gain is below this threshold.","558","","559","    See Also","560","    --------","561","    BayesianGaussianMixture : Finite gaussian mixture model fit with a","562","        variational algorithm.","637","    def _m_step(self, X, log_resp):","638","        \"\"\"M step.","640","        Parameters","641","        ----------","642","        X : array-like, shape (n_samples, n_features)","643","","644","        log_resp : array-like, shape (n_samples, n_components)","645","            Logarithm of the posterior probabilities (or responsibilities) of","646","            the point of each sample in X.","647","        \"\"\"","650","            _estimate_gaussian_parameters(X, np.exp(log_resp), self.reg_covar,"],"delete":["191","    n_samples, _ = X.shape","195","    covariance \/= n_samples","308","        \"The algorithm has diverged because of too few samples per \"","309","        \"components. Try to decrease the number of components, \"","360","        The determinant of the cholesky decomposition.","361","        matrix.","458","        log_likelihood average gain is below this threshold.","633","    def _e_step(self, X):","634","        log_prob_norm, log_resp = self._estimate_log_prob_resp(X)","635","        return np.mean(log_prob_norm), np.exp(log_resp)","637","    def _m_step(self, X, resp):","640","            _estimate_gaussian_parameters(X, resp, self.reg_covar,"]}]}},"a9bf9146e07b9900126884c4412c869a08b5e92a":{"changes":{"sklearn\/metrics\/cluster\/expected_mutual_info_fast.pyx":"MODIFY"},"diff":{"sklearn\/metrics\/cluster\/expected_mutual_info_fast.pyx":[{"add":["15","ctypedef np.float64_t DOUBLE","22","    cdef DOUBLE N, gln_N, emi, term2, term3, gln","23","    cdef np.ndarray[DOUBLE] gln_a, gln_b, gln_Na, gln_Nb, gln_nij, log_Nnij","24","    cdef np.ndarray[DOUBLE] nijs, term1","25","    cdef np.ndarray[DOUBLE, ndim=2] log_ab_outer","29","    N = <DOUBLE>n_samples"],"delete":["15","","22","    cdef float N, gln_N, emi, term2, term3, gln","23","    cdef np.ndarray[double] gln_a, gln_b, gln_Na, gln_Nb, gln_nij, log_Nnij","24","    cdef np.ndarray[double] nijs, term1","25","    cdef np.ndarray[double, ndim=2] log_ab_outer","29","    N = float(n_samples)"]}]}},"379f54be688f8ef5c15ad3682a7a8d7dcc471260":{"changes":{"sklearn\/mixture\/gmm.py":"MODIFY","sklearn\/mixture\/tests\/test_gmm.py":"MODIFY"},"diff":{"sklearn\/mixture\/gmm.py":[{"add":["86","    covar : array_like","101","    X : array","102","        Randomly generated sample. The shape depends on `n_samples`:","103","        (n_features,) if `1`","104","        (n_features, n_samples) otherwise","106","    _sample_gaussian(mean, covar, covariance_type='diag', n_samples=1,","107","                     random_state=None)","108","","109","","110","def _sample_gaussian(mean, covar, covariance_type='diag', n_samples=1,","111","                     random_state=None):","433","                X[comp_in_X] = _sample_gaussian("],"delete":["86","    covar : array_like, optional","101","    X : array, shape (n_features, n_samples)","102","        Randomly generated sample","425","                X[comp_in_X] = sample_gaussian("]}],"sklearn\/mixture\/tests\/test_gmm.py":[{"add":["35","    samples = mixture.gmm._sample_gaussian(","43","    samples = mixture.gmm._sample_gaussian(","53","    samples = mixture.gmm._sample_gaussian(","60","    x = mixture.gmm._sample_gaussian(","61","        [0, 0], [[4, 3], [1, .1]], covariance_type='full', random_state=42)"],"delete":["35","    samples = mixture.sample_gaussian(","43","    samples = mixture.sample_gaussian(","53","    samples = mixture.sample_gaussian(","60","    from sklearn.mixture import sample_gaussian","61","    x = sample_gaussian([0, 0], [[4, 3], [1, .1]],","62","                        covariance_type='full', random_state=42)"]}]}},"0cffdfa616af487a255a6adedd516d3880d03462":{"changes":{"sklearn\/mixture\/gmm.py":"MODIFY"},"diff":{"sklearn\/mixture\/gmm.py":[{"add":["684","    if cv.shape[1] == 1:"],"delete":["684","    if covars.shape[1] == 1:"]}]}},"331964ead08b14e998bf0eb748dd8b7c226de399":{"changes":{"doc\/themes\/scikit-learn\/layout.html":"MODIFY","doc\/themes\/scikit-learn\/static\/nature.css_t":"MODIFY"},"diff":{"doc\/themes\/scikit-learn\/layout.html":[{"add":["40","    function showMenu() {","41","      var topNav = document.getElementById(\"scikit-navbar\");","42","      if (topNav.className === \"navbar\") {","43","          topNav.className += \" responsive\";","44","      } else {","45","          topNav.className = \"navbar\";","46","      }","47","    };","71","        <div class=\"navbar\" id=\"scikit-navbar\">","97","            <a href=\"javascript:void(0);\" onclick=\"showMenu()\">","98","                <div class=\"nav-icon\">","99","                    <div class=\"hamburger-line\"><\/div>","100","                    <div class=\"hamburger-line\"><\/div>","101","                    <div class=\"hamburger-line\"><\/div>","102","                <\/div>","103","            <\/a>"],"delete":["63","        <div class=\"navbar\">","89",""]}],"doc\/themes\/scikit-learn\/static\/nature.css_t":[{"add":["44","       height: 60px;","97","div.navbar div.nav-icon a,","98","div.navbar div.nav-icon a:link,","99","div.navbar div.nav-icon a:visited,","100","div.navbar div.nav-icon a:hover {","101","    color: white;","102","    text-decoration: none;","103","}","104","div.navbar div.nav-icon {","105","    display: none;","106","}","107","\/* Nav bar collapses for mobile phones and shows the hamburger *\/","108","@media screen and (max-width: 680px) {","109","    div.navbar div.nav-icon {","110","        position: absolute;","111","        display: inline-block;","112","        right: 0;","113","        top: 12px;","114","        margin-right: 10px;","115","        background: #ff9c34;","116","        padding: 5px 10px;","117","        border-radius: 5px;","118","    }","119","    div.navbar ul li {","120","        display: none;","121","    }","122","    div.navbar ul {","123","        visibilty: hidden;","124","        background: #FFFFFF;","125","    }","126","    div.navbar.responsive > ul li.btn-li {","127","        margin-left: 6px;","128","    }","129","    div.navbar.responsive > ul li.btn-li + li {","130","        margin-top: -5px;   ","131","    }","132","    div.navbar.responsive > ul {","133","        visiblity: visible;","134","        position: absolute;","135","        right: 0;","136","        top: 10px;","137","        margin-right: 10px;","138","        background: #ff9c34;","139","    }","140","    div.navbar.responsive > ul li {","141","        display: flex;","142","        justify-content: flex-start;","143","        visibility: visible;","144","        width: 130px;","145","    }","146","    div.navbar .dropdown-menu li {","147","        white-space: normal;","148","    }","149","    div.navbar div.nav-icon .hamburger-line {","150","        background: white;","151","        width: 20px;","152","        height: 2px;","153","        margin-bottom: 5px;","154","        -webkit-transition: .1s ease-in-out;","155","        -moz-transition: .1s ease-in-out;","156","        -o-transition: .1s ease-in-out;","157","        transition: .1s ease-in-out;","158","    }","159","    div.navbar div.nav-icon .hamburger-line:nth-child(1) {","160","        margin-top: 5px;","161","        -webkit-transform-origin: left center;","162","        -moz-transform-origin: left center;","163","        -o-transform-origin: left center;","164","        transform-origin: left center;","165","    }","166","    div.navbar div.nav-icon .hamburger-line:nth-child(2) {","167","        -webkit-transform-origin: left center;","168","        -moz-transform-origin: left center;","169","        -o-transform-origin: left center;","170","        transform-origin: left center;","171","    }","172","    div.navbar div.nav-icon .hamburger-line:nth-child(3) {","173","        -webkit-transform-origin: left center;","174","        -moz-transform-origin: left center;","175","        -o-transform-origin: left center;","176","        transform-origin: left center;","177","    }","178","    div.navbar.responsive div.nav-icon .hamburger-line:nth-child(1) {","179","        -webkit-transform: rotate(45deg);","180","        -moz-transform: rotate(45deg);","181","        -o-transform: rotate(45deg);","182","        transform: rotate(45deg);","183","    }","184","    div.navbar.responsive div.nav-icon .hamburger-line:nth-child(2) {","185","        width: 0;","186","        opacity: 0%;","187","    }","188","    div.navbar.responsive div.nav-icon .hamburger-line:nth-child(3) {","189","        -webkit-transform: rotate(-45deg);","190","        -moz-transform: rotate(-45deg);","191","        -o-transform: rotate(-45deg);","192","        transform: rotate(-45deg);","193","    }","194","}","195",""],"delete":[]}]}},"873f3eb6b9c6a9f10645cb76745ef19555a2f483":{"changes":{"sklearn\/neural_network\/multilayer_perceptron.py":"MODIFY","sklearn\/neural_network\/tests\/test_mlp.py":"MODIFY"},"diff":{"sklearn\/neural_network\/multilayer_perceptron.py":[{"add":["26","from ..utils.multiclass import _check_partial_fit_first_call, unique_labels","27","from ..utils.multiclass import type_of_target","271","        elif self._label_binarizer.y_type_ == 'multiclass':","492","                y_val = self._label_binarizer.inverse_transform(y_val)","900","        if not incremental:","901","            self._label_binarizer = LabelBinarizer()","902","            self._label_binarizer.fit(y)","903","            self.classes_ = self._label_binarizer.classes_","905","            classes = unique_labels(y)","906","            if np.setdiff1d(classes, self.classes_, assume_unique=True):","911","        y = self._label_binarizer.transform(y)","933","        return self._label_binarizer.inverse_transform(y_pred)","966","        if _check_partial_fit_first_call(self, classes):","967","            self._label_binarizer = LabelBinarizer()","968","            if type_of_target(y).startswith('multilabel'):","969","                self._label_binarizer.fit(y)","970","            else:","971","                self._label_binarizer.fit(classes)"],"delete":["26","from ..utils.multiclass import _check_partial_fit_first_call","270","        elif self.label_binarizer_.y_type_ == 'multiclass':","491","                y_val = self.label_binarizer_.inverse_transform(y_val)","821","    `label_binarizer_` : LabelBinarizer","822","        A LabelBinarizer object trained on the training set.","823","","896","        self.label_binarizer_ = LabelBinarizer()","897","","903","        self.label_binarizer_.fit(y)","905","        if not hasattr(self, 'classes_') or not incremental:","906","            self.classes_ = self.label_binarizer_.classes_","908","            classes = self.label_binarizer_.classes_","909","            if not np.all(np.in1d(classes, self.classes_)):","914","        y = self.label_binarizer_.transform(y)","936","        return self.label_binarizer_.inverse_transform(y_pred)","969","        _check_partial_fit_first_call(self, classes)"]}],"sklearn\/neural_network\/tests\/test_mlp.py":[{"add":["344","def test_partial_fit_unseen_classes():","345","    # Non regression test for bug 6994","346","    # Tests for labeling errors in partial fit","347","","348","    clf = MLPClassifier(random_state=0)","349","    clf.partial_fit([[1], [2], [3]], [\"a\", \"b\", \"c\"],","350","                    classes=[\"a\", \"b\", \"c\", \"d\"])","351","    clf.partial_fit([[4]], [\"d\"])","352","    assert_greater(clf.score([[1], [2], [3], [4]], [\"a\", \"b\", \"c\", \"d\"]), 0)","353","","354",""],"delete":["82","    mlp.classes_ = [0, 1]","91","    mlp.label_binarizer_.y_type_ = 'binary'","92",""]}]}},"97e423256176d6c85bd7ff4641b0d0cde625f5bd":{"changes":{"sklearn\/tests\/test_multiclass.py":"MODIFY","sklearn\/multiclass.py":"MODIFY"},"diff":{"sklearn\/tests\/test_multiclass.py":[{"add":["440","    iris_data_list = [list(a) for a in iris.data]","441","    prediction_from_list = ovo.fit(iris_data_list,","442","                                   list(iris.target)).predict(iris_data_list)"],"delete":["440","    prediction_from_list = ovo.fit(iris.data,","441","                                   list(iris.target)).predict(iris.data)"]}],"sklearn\/multiclass.py":[{"add":["48","from .utils.validation import check_X_y","471","        X, y = check_X_y(X, y, accept_sparse=['csr', 'csc'])","514","        X, y = check_X_y(X, y, accept_sparse=['csr', 'csc'])"],"delete":["470","        y = np.asarray(y)","471","        check_consistent_length(X, y)","514","        y = np.asarray(y)","515","        check_consistent_length(X, y)"]}]}},"54b0e4bf62152ac9abb5b93780a0af3e6f5d1c44":{"changes":{"sklearn\/model_selection\/_split.py":"MODIFY","sklearn\/utils\/metaestimators.py":"MODIFY","sklearn\/model_selection\/_validation.py":"MODIFY","sklearn\/tests\/test_multiclass.py":"MODIFY","sklearn\/model_selection\/tests\/test_split.py":"MODIFY","sklearn\/utils\/multiclass.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/svm\/base.py":"MODIFY","sklearn\/multiclass.py":"MODIFY","sklearn\/utils\/tests\/test_multiclass.py":"MODIFY"},"diff":{"sklearn\/model_selection\/_split.py":[{"add":[],"delete":["1679","","1680","def _safe_split(estimator, X, y, indices, train_indices=None):","1681","    \"\"\"Create subset of dataset and properly handle kernels.\"\"\"","1682","    if (hasattr(estimator, 'kernel') and callable(estimator.kernel) and","1683","            not isinstance(estimator.kernel, GPKernel)):","1684","        # cannot compute the kernel values with custom function","1685","        raise ValueError(\"Cannot use a custom kernel function. \"","1686","                         \"Precompute the kernel matrix instead.\")","1687","","1688","    if not hasattr(X, \"shape\"):","1689","        if getattr(estimator, \"_pairwise\", False):","1690","            raise ValueError(\"Precomputed kernels or affinity matrices have \"","1691","                             \"to be passed as arrays or sparse matrices.\")","1692","        X_subset = [X[index] for index in indices]","1693","    else:","1694","        if getattr(estimator, \"_pairwise\", False):","1695","            # X is a precomputed square kernel matrix","1696","            if X.shape[0] != X.shape[1]:","1697","                raise ValueError(\"X should be a square kernel matrix\")","1698","            if train_indices is None:","1699","                X_subset = X[np.ix_(indices, indices)]","1700","            else:","1701","                X_subset = X[np.ix_(indices, train_indices)]","1702","        else:","1703","            X_subset = safe_indexing(X, indices)","1704","","1705","    if y is not None:","1706","        y_subset = safe_indexing(y, indices)","1707","    else:","1708","        y_subset = None","1709","","1710","    return X_subset, y_subset","1711","","1712",""]}],"sklearn\/utils\/metaestimators.py":[{"add":["7","import numpy as np","8","from ..utils import safe_indexing","80","","81","","82","def _safe_split(estimator, X, y, indices, train_indices=None):","83","    \"\"\"Create subset of dataset and properly handle kernels.\"\"\"","84","    from ..gaussian_process.kernels import Kernel as GPKernel","85","","86","    if (hasattr(estimator, 'kernel') and callable(estimator.kernel) and","87","            not isinstance(estimator.kernel, GPKernel)):","88","        # cannot compute the kernel values with custom function","89","        raise ValueError(\"Cannot use a custom kernel function. \"","90","                         \"Precompute the kernel matrix instead.\")","91","","92","    if not hasattr(X, \"shape\"):","93","        if getattr(estimator, \"_pairwise\", False):","94","            raise ValueError(\"Precomputed kernels or affinity matrices have \"","95","                             \"to be passed as arrays or sparse matrices.\")","96","        X_subset = [X[index] for index in indices]","97","    else:","98","        if getattr(estimator, \"_pairwise\", False):","99","            # X is a precomputed square kernel matrix","100","            if X.shape[0] != X.shape[1]:","101","                raise ValueError(\"X should be a square kernel matrix\")","102","            if train_indices is None:","103","                X_subset = X[np.ix_(indices, indices)]","104","            else:","105","                X_subset = X[np.ix_(indices, train_indices)]","106","        else:","107","            X_subset = safe_indexing(X, indices)","108","","109","    if y is not None:","110","        y_subset = safe_indexing(y, indices)","111","    else:","112","        y_subset = None","113","","114","    return X_subset, y_subset"],"delete":["7",""]}],"sklearn\/model_selection\/_validation.py":[{"add":["25","from ..utils.metaestimators import _safe_split","29","from ._split import check_cv"],"delete":["28","","29","from ._split import check_cv, _safe_split"]}],"sklearn\/tests\/test_multiclass.py":[{"add":["26","from sklearn.model_selection import GridSearchCV, cross_val_score","607","","608","","609","def test_pairwise_indices():","610","    clf_precomputed = svm.SVC(kernel='precomputed')","611","    X, y = iris.data, iris.target","612","","613","    ovr_false = OneVsOneClassifier(clf_precomputed)","614","    linear_kernel = np.dot(X, X.T)","615","    ovr_false.fit(linear_kernel, y)","616","","617","    n_estimators = len(ovr_false.estimators_)","618","    precomputed_indices = ovr_false.pairwise_indices_","619","","620","    for idx in precomputed_indices:","621","        assert_equal(idx.shape[0] * n_estimators \/ (n_estimators - 1),","622","                     linear_kernel.shape[0])","623","","624","","625","def test_pairwise_attribute():","626","    clf_precomputed = svm.SVC(kernel='precomputed')","627","    clf_notprecomputed = svm.SVC()","628","","629","    for MultiClassClassifier in [OneVsRestClassifier, OneVsOneClassifier]:","630","        ovr_false = MultiClassClassifier(clf_notprecomputed)","631","        assert_false(ovr_false._pairwise)","632","","633","        ovr_true = MultiClassClassifier(clf_precomputed)","634","        assert_true(ovr_true._pairwise)","635","","636","","637","def test_pairwise_cross_val_score():","638","    clf_precomputed = svm.SVC(kernel='precomputed')","639","    clf_notprecomputed = svm.SVC(kernel='linear')","640","","641","    X, y = iris.data, iris.target","642","","643","    for MultiClassClassifier in [OneVsRestClassifier, OneVsOneClassifier]:","644","        ovr_false = MultiClassClassifier(clf_notprecomputed)","645","        ovr_true = MultiClassClassifier(clf_precomputed)","646","","647","        linear_kernel = np.dot(X, X.T)","648","        score_precomputed = cross_val_score(ovr_true, linear_kernel, y)","649","        score_linear = cross_val_score(ovr_false, X, y)","650","        assert_array_equal(score_precomputed, score_linear)"],"delete":["26","from sklearn.model_selection import GridSearchCV"]}],"sklearn\/model_selection\/tests\/test_split.py":[{"add":[],"delete":["47","from sklearn.model_selection._split import _safe_split","53","from sklearn.datasets import load_iris","64","iris = load_iris()","848","def test_safe_split_with_precomputed_kernel():","849","    clf = SVC()","850","    clfp = SVC(kernel=\"precomputed\")","851","","852","    X, y = iris.data, iris.target","853","    K = np.dot(X, X.T)","854","","855","    cv = ShuffleSplit(test_size=0.25, random_state=0)","856","    tr, te = list(cv.split(X))[0]","857","","858","    X_tr, y_tr = _safe_split(clf, X, y, tr)","859","    K_tr, y_tr2 = _safe_split(clfp, K, y, tr)","860","    assert_array_almost_equal(K_tr, np.dot(X_tr, X_tr.T))","861","","862","    X_te, y_te = _safe_split(clf, X, y, te, tr)","863","    K_te, y_te2 = _safe_split(clfp, K, y, te, tr)","864","    assert_array_almost_equal(K_te, np.dot(X_te, X_tr.T))","865","","866",""]}],"sklearn\/utils\/multiclass.py":[{"add":["161","        'binary', 'multiclass', 'multiclass-multioutput',","169","    if y_type not in ['binary', 'multiclass', 'multiclass-multioutput',","387","","388","","389","def _ovr_decision_function(predictions, confidences, n_classes):","390","    \"\"\"Compute a continuous, tie-breaking ovr decision function.","391","","392","    It is important to include a continuous value, not only votes,","393","    to make computing AUC or calibration meaningful.","394","","395","    Parameters","396","    ----------","397","    predictions : array-like, shape (n_samples, n_classifiers)","398","        Predicted classes for each binary classifier.","399","","400","    confidences : array-like, shape (n_samples, n_classifiers)","401","        Decision functions or predicted probabilities for positive class","402","        for each binary classifier.","403","","404","    n_classes : int","405","        Number of classes. n_classifiers must be","406","        ``n_classes * (n_classes - 1 ) \/ 2``","407","    \"\"\"","408","    n_samples = predictions.shape[0]","409","    votes = np.zeros((n_samples, n_classes))","410","    sum_of_confidences = np.zeros((n_samples, n_classes))","411","","412","    k = 0","413","    for i in range(n_classes):","414","        for j in range(i + 1, n_classes):","415","            sum_of_confidences[:, i] -= confidences[:, k]","416","            sum_of_confidences[:, j] += confidences[:, k]","417","            votes[predictions[:, k] == 0, i] += 1","418","            votes[predictions[:, k] == 1, j] += 1","419","            k += 1","420","","421","    max_confidences = sum_of_confidences.max()","422","    min_confidences = sum_of_confidences.min()","423","","424","    if max_confidences == min_confidences:","425","        return votes","426","","427","    # Scale the sum_of_confidences to (-0.5, 0.5) and add it with votes.","428","    # The motivation is to use confidence levels as a way to break ties in","429","    # the votes without switching any decision made based on a difference","430","    # of 1 vote.","431","    eps = np.finfo(sum_of_confidences.dtype).eps","432","    max_abs_confidence = max(abs(max_confidences), abs(min_confidences))","433","    scale = (0.5 - eps) \/ max_abs_confidence","434","    return votes + sum_of_confidences * scale"],"delete":["25","","162","        'binary', 'multiclass', 'multiclass-multioutput', ","170","    if y_type not in ['binary', 'multiclass', 'multiclass-multioutput', "]}],"doc\/whats_new.rst":[{"add":["464","    - Cross-validation of :class:`OneVsOneClassifier` and","465","      :class:`OneVsRestClassifier` now works with precomputed kernels.","466","      (`#7350 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/7350\/>`_)","467","      By `Russell Smith`_.","468","","4645","","4646",".. _Russell Smith: https:\/\/github.com\/rsmith54"],"delete":[]}],"sklearn\/svm\/base.py":[{"add":["11","from ..utils.multiclass import _ovr_decision_function"],"delete":["11","from ..multiclass import _ovr_decision_function"]}],"sklearn\/multiclass.py":[{"add":["39","import itertools","50","                               check_classification_targets,","51","                               _ovr_decision_function)","52","from .utils.metaestimators import _safe_split","53","","263","                                 X, next(columns) if self.classes_[i] in","264","                                 self.label_binarizer_.classes_ else","265","                                 np.zeros((1, len(y))))","397","    @property","398","    def _pairwise(self):","399","        \"\"\"Indicate if wrapped estimator is using a precomputed Gram matrix\"\"\"","400","        return getattr(self.estimator, \"_pairwise\", False)","401","","410","    indcond = np.arange(X.shape[0])[cond]","411","    return _fit_binary(estimator,","412","                       _safe_split(estimator, X, None, indices=indcond)[0],","413","                       y_binary, classes=[i, j]), indcond","485","        estimators_indices = list(zip(*(Parallel(n_jobs=self.n_jobs)(","486","            delayed(_fit_ovo_binary)","487","            (self.estimator, X, y, self.classes_[i], self.classes_[j])","488","            for i in range(n_classes) for j in range(i + 1, n_classes)))))","489","","490","        self.estimators_ = estimators_indices[0]","491","        try:","492","            self.pairwise_indices_ = estimators_indices[1] \\","493","                                     if self._pairwise else None","494","        except AttributeError:","495","            self.pairwise_indices_ = None","529","                                      (self.n_classes_ - 1) \/\/ 2)]","533","        combinations = itertools.combinations(range(self.n_classes_), 2)","534","        self.estimators_ = Parallel(","535","            n_jobs=self.n_jobs)(","536","                delayed(_partial_fit_ovo_binary)(","537","                    estimator, X, y, self.classes_[i], self.classes_[j])","538","                for estimator, (i, j) in izip(","539","                        self.estimators_, (combinations)))","540","","541","        self.pairwise_indices_ = None","542","","583","        indices = self.pairwise_indices_","584","        if indices is None:","585","            Xs = [X] * len(self.estimators_)","586","        else:","587","            Xs = [X[:, idx] for idx in indices]","588","","589","        predictions = np.vstack([est.predict(Xi)","590","                                 for est, Xi in zip(self.estimators_, Xs)]).T","591","        confidences = np.vstack([_predict_binary(est, Xi)","592","                                 for est, Xi in zip(self.estimators_, Xs)]).T","593","        Y = _ovr_decision_function(predictions,","594","                                   confidences, len(self.classes_))","595","","596","        return Y","602","    @property","603","    def _pairwise(self):","604","        \"\"\"Indicate if wrapped estimator is using a precomputed Gram matrix\"\"\"","605","        return getattr(self.estimator, \"_pairwise\", False)"],"delete":["49","                               check_classification_targets)","259","            X, next(columns) if self.classes_[i] in","260","            self.label_binarizer_.classes_ else","261","            np.zeros((1, len(y))))","401","    ind = np.arange(X.shape[0])","402","    return _fit_binary(estimator, X[ind[cond]], y_binary, classes=[i, j])","474","        self.estimators_ = Parallel(n_jobs=self.n_jobs)(","475","            delayed(_fit_ovo_binary)(","476","                self.estimator, X, y, self.classes_[i], self.classes_[j])","477","            for i in range(n_classes) for j in range(i + 1, n_classes))","511","                                (self.n_classes_-1) \/\/ 2)]","515","        self.estimators_ = Parallel(n_jobs=self.n_jobs)(","516","            delayed(_partial_fit_ovo_binary)(","517","                estimator, X, y, self.classes_[i], self.classes_[j])","518","            for estimator, (i, j) in izip(self.estimators_, ((i, j) for i","519","                                in range(self.n_classes_) for j in range","520","                                            (i + 1, self.n_classes_))))","561","        predictions = np.vstack([est.predict(X) for est in self.estimators_]).T","562","        confidences = np.vstack([_predict_binary(est, X) for est in self.estimators_]).T","563","        return _ovr_decision_function(predictions, confidences,","564","                                      len(self.classes_))","570","","571","def _ovr_decision_function(predictions, confidences, n_classes):","572","    \"\"\"Compute a continuous, tie-breaking ovr decision function.","573","","574","    It is important to include a continuous value, not only votes,","575","    to make computing AUC or calibration meaningful.","576","","577","    Parameters","578","    ----------","579","    predictions : array-like, shape (n_samples, n_classifiers)","580","        Predicted classes for each binary classifier.","581","","582","    confidences : array-like, shape (n_samples, n_classifiers)","583","        Decision functions or predicted probabilities for positive class","584","        for each binary classifier.","585","","586","    n_classes : int","587","        Number of classes. n_classifiers must be","588","        ``n_classes * (n_classes - 1 ) \/ 2``","589","    \"\"\"","590","    n_samples = predictions.shape[0]","591","    votes = np.zeros((n_samples, n_classes))","592","    sum_of_confidences = np.zeros((n_samples, n_classes))","593","","594","    k = 0","595","    for i in range(n_classes):","596","        for j in range(i + 1, n_classes):","597","            sum_of_confidences[:, i] -= confidences[:, k]","598","            sum_of_confidences[:, j] += confidences[:, k]","599","            votes[predictions[:, k] == 0, i] += 1","600","            votes[predictions[:, k] == 1, j] += 1","601","            k += 1","602","","603","    max_confidences = sum_of_confidences.max()","604","    min_confidences = sum_of_confidences.min()","605","","606","    if max_confidences == min_confidences:","607","        return votes","608","","609","    # Scale the sum_of_confidences to (-0.5, 0.5) and add it with votes.","610","    # The motivation is to use confidence levels as a way to break ties in","611","    # the votes without switching any decision made based on a difference","612","    # of 1 vote.","613","    eps = np.finfo(sum_of_confidences.dtype).eps","614","    max_abs_confidence = max(abs(max_confidences), abs(min_confidences))","615","    scale = (0.5 - eps) \/ max_abs_confidence","616","    return votes + sum_of_confidences * scale"]}],"sklearn\/utils\/tests\/test_multiclass.py":[{"add":["30","from sklearn.utils.metaestimators import _safe_split","31","from sklearn.model_selection import ShuffleSplit","32","from sklearn.svm import SVC","33","from sklearn import datasets","34","","273","                assert_raises_regex(ValueError, msg,","352","","353","","354","def test_safe_split_with_precomputed_kernel():","355","    clf = SVC()","356","    clfp = SVC(kernel=\"precomputed\")","357","","358","    iris = datasets.load_iris()","359","    X, y = iris.data, iris.target","360","    K = np.dot(X, X.T)","361","","362","    cv = ShuffleSplit(test_size=0.25, random_state=0)","363","    train, test = list(cv.split(X))[0]","364","","365","    X_train, y_train = _safe_split(clf, X, y, train)","366","    K_train, y_train2 = _safe_split(clfp, K, y, train)","367","    assert_array_almost_equal(K_train, np.dot(X_train, X_train.T))","368","    assert_array_almost_equal(y_train, y_train2)","369","","370","    X_test, y_test = _safe_split(clf, X, y, test, train)","371","    K_test, y_test2 = _safe_split(clfp, K, y, test, train)","372","    assert_array_almost_equal(K_test, np.dot(X_test, X_train.T))","373","    assert_array_almost_equal(y_test, y_test2)"],"delete":["268","                assert_raises_regex(ValueError, msg, "]}]}},"94246be3e3746b577a4d0f2641f887a5828e2f81":{"changes":{"sklearn\/neighbors\/tests\/test_neighbors.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/neighbors\/classification.py":"MODIFY"},"diff":{"sklearn\/neighbors\/tests\/test_neighbors.py":[{"add":["351","    X = np.array([[1.0, 1.0], [2.0, 2.0], [0.99, 0.99], ","352","                  [0.98, 0.98], [2.01, 2.01]])","353","    y  = np.array([1, 2, 1, 1, 2])","357","    z2 = np.array([[1.4, 1.4], [1.01, 1.01], [2.01, 2.01]])    # one outlier","359","    correct_labels2 = np.array([-1, 1, 2])","373","      "],"delete":["351","    X = np.array([[1.0, 1.0], [2.0, 2.0]])","352","    y = np.array([1, 2])","356","    z2 = np.array([[1.01, 1.01], [1.4, 1.4]])    # one outlier","358","    correct_labels2 = np.array([1, -1])","372",""]}],"doc\/whats_new.rst":[{"add":["281","      ","282","    - Fix bug in :class:`neighbors.RadiusNeighborsClassifier` where an error ","283","      occurred when there were outliers being labelled and a weight function ","284","      (`#6902 <https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/6902>`_). ","285","      By `LeonieBorne <https:\/\/github.com\/LeonieBorne>`_."],"delete":[]}],"sklearn\/neighbors\/classification.py":[{"add":["376","                                 in zip(pred_labels[inliers], weights[inliers])],"],"delete":["376","                                 in zip(pred_labels[inliers], weights)],"]}]}},"19d6d925edd33288acee343c5eb6afdc80b84dbc":{"changes":{"doc\/whats_new.rst":"MODIFY","sklearn\/preprocessing\/tests\/test_data.py":"MODIFY","sklearn\/preprocessing\/data.py":"MODIFY"},"diff":{"doc\/whats_new.rst":[{"add":["244","     :func:`load_iris` dataset","245","     `#7049 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/7049>`_,","326","","345","    - :func:`_transform_selected` now always passes a copy of `X` to transform","346","      function when `copy=True` (`#7194","347","      <https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/7194>`_). By `Caio","348","      Oliveira <https:\/\/github.com\/caioaao>`_.","349",""],"delete":["244","     :func:`load_iris` dataset ","245","     `#7049 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/7049>`_, ","326","      "]}],"sklearn\/preprocessing\/tests\/test_data.py":[{"add":["1563","def test_transform_selected_copy_arg():","1564","    # transformer that alters X","1565","    def _mutating_transformer(X):","1566","        X[0, 0] = X[0, 0] + 1","1567","        return X","1568","","1569","    original_X = np.asarray([[1, 2], [3, 4]])","1570","    expected_Xtr = [[2, 2], [3, 4]]","1571","","1572","    X = original_X.copy()","1573","    Xtr = _transform_selected(X, _mutating_transformer, copy=True,","1574","                              selected='all')","1575","","1576","    assert_array_equal(toarray(X), toarray(original_X))","1577","    assert_array_equal(toarray(Xtr), expected_Xtr)","1578","","1579",""],"delete":[]}],"sklearn\/preprocessing\/data.py":[{"add":["1696","    X = check_array(X, accept_sparse='csc', copy=copy, dtype=FLOAT_DTYPES)","1697",""],"delete":["1699","    X = check_array(X, accept_sparse='csc', copy=copy, dtype=FLOAT_DTYPES)","1700",""]}]}},"485445401622f2cae0921c6678bbfb47c0deca27":{"changes":{"sklearn\/decomposition\/tests\/test_nmf.py":"MODIFY","sklearn\/ensemble\/iforest.py":"MODIFY","sklearn\/decomposition\/nmf.py":"MODIFY","sklearn\/ensemble\/tests\/test_iforest.py":"MODIFY"},"diff":{"sklearn\/decomposition\/tests\/test_nmf.py":[{"add":["9","from sklearn.utils.testing import assert_raise_message, assert_no_warnings","236","    assert_no_warnings(nnmf, A, A, A, np.int64(1))","237","    msg = \"Number of components must be a positive integer; got (n_components=1.5)\"","238","    assert_raise_message(ValueError, msg, nnmf, A, A, A, 1.5)","239","    msg = \"Number of components must be a positive integer; got (n_components='2')\""],"delete":["9","from sklearn.utils.testing import assert_raise_message","135","","237","    msg = \"Number of components must be positive; got (n_components='2')\""]}],"sklearn\/ensemble\/iforest.py":[{"add":["12","import numbers","170","        elif isinstance(self.max_samples, numbers.Integral):","280","    if isinstance(n_samples_leaf, numbers.Integral):"],"delete":["169","        elif isinstance(self.max_samples, six.integer_types):","279","    if isinstance(n_samples_leaf, six.integer_types):"]}],"sklearn\/decomposition\/nmf.py":[{"add":["748","    if not isinstance(n_components, numbers.Integral) or n_components <= 0:","749","        raise ValueError(\"Number of components must be a positive integer;\"","751","    if not isinstance(max_iter, numbers.Integral) or max_iter < 0:","752","        raise ValueError(\"Maximum number of iterations must be a positive integer;\""],"delete":["21","from ..externals import six","749","    if not isinstance(n_components, six.integer_types) or n_components <= 0:","750","        raise ValueError(\"Number of components must be positive;\"","752","    if not isinstance(max_iter, numbers.Number) or max_iter < 0:","753","        raise ValueError(\"Maximum number of iteration must be positive;\""]}],"sklearn\/ensemble\/tests\/test_iforest.py":[{"add":["106","    assert_no_warnings(IsolationForest(max_samples=np.int64(2)).fit, X)","107","    assert_raises(ValueError, IsolationForest(max_samples='foobar').fit, X)","108","    assert_raises(ValueError, IsolationForest(max_samples=1.5).fit, X)"],"delete":["106","    assert_raises(ValueError,","107","                  IsolationForest(max_samples='foobar').fit, X)"]}]}},"a03db89eba7978cbe8d22573cf64def4df8b5d72":{"changes":{"sklearn\/model_selection\/tests\/test_search.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/model_selection\/_search.py":"MODIFY"},"diff":{"sklearn\/model_selection\/tests\/test_search.py":[{"add":["5","from sklearn.externals.joblib._compat import PY3_OR_LATER","307","def test_grid_search_when_param_grid_includes_range():","308","    # Test that the best estimator contains the right value for foo_param","309","    clf = MockClassifier()","310","    grid_search = None","311","    if PY3_OR_LATER:","312","        grid_search = GridSearchCV(clf, {'foo_param': range(1, 4)})","313","    else:","314","        grid_search = GridSearchCV(clf, {'foo_param': xrange(1, 4)})","315","    grid_search.fit(X, y)","316","    assert_equal(grid_search.best_estimator_.foo_param, 2)","317","","318","","322","    assert_raise_message(","323","        ValueError,","324","        \"Parameter values for parameter (C) need to be a sequence\"","325","        \"(but not a string) or np.ndarray.\",","326","        GridSearchCV, clf, param_dict)","330","    assert_raise_message(","331","        ValueError,","332","        \"Parameter values for parameter (C) need to be a non-empty sequence.\",","333","        GridSearchCV, clf, param_dict)","334","","335","    param_dict = {\"C\": \"1,2,3\"}","336","    clf = SVC()","337","    assert_raise_message(","338","        ValueError,","339","        \"Parameter values for parameter (C) need to be a sequence\"","340","        \"(but not a string) or np.ndarray.\",","341","        GridSearchCV, clf, param_dict)"],"delete":["171","def test_grid_search_incorrect_param_grid():","172","    clf = MockClassifier()","173","    assert_raise_message(","174","        ValueError,","175","        \"Parameter values for parameter (C) need to be a sequence.\",","176","        GridSearchCV, clf, {'C': 1})","177","","178","","179","def test_grid_search_param_grid_includes_sequence_of_a_zero_length():","180","    clf = MockClassifier()","181","    assert_raise_message(","182","        ValueError,","183","        \"Parameter values for parameter (C) need to be a non-empty sequence.\",","184","        GridSearchCV, clf, {'C': []})","185","","186","","325","    assert_raises(ValueError, GridSearchCV, clf, param_dict)","329","    assert_raises(ValueError, GridSearchCV, clf, param_dict)"]}],"doc\/whats_new.rst":[{"add":["298","    - :func: `model_selection.tests._search._check_param_grid` now works correctly with all types","299","      that extends\/implements `Sequence` (except string), including range (Python 3.x) and xrange","300","      (Python 2.x).","301","      (`#7323 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/7323>`_) by `Viacheslav Kovalevskyi`_.","302",""],"delete":[]}],"sklearn\/model_selection\/_search.py":[{"add":["14","from collections import Mapping, namedtuple, Sized, defaultdict, Sequence","334","            if (isinstance(v, six.string_types) or","335","                    not isinstance(v, (np.ndarray, Sequence))):","337","                                 \"to be a sequence(but not a string) or\"","338","                                 \" np.ndarray.\".format(name))"],"delete":["14","from collections import Mapping, namedtuple, Sized, defaultdict","334","            check = [isinstance(v, k) for k in (list, tuple, np.ndarray)]","335","            if True not in check:","337","                                 \"to be a sequence.\".format(name))"]}]}},"0c879ba55193f13420934fa01bbcae88cad53c14":{"changes":{"sklearn\/datasets\/base.py":"MODIFY","examples\/decomposition\/plot_sparse_coding.py":"MODIFY","benchmarks\/bench_plot_neighbors.py":"MODIFY","doc\/datasets\/index.rst":"MODIFY","doc\/tutorial\/statistical_inference\/settings.rst":"MODIFY","doc\/tutorial\/text_analytics\/solutions\/exercise_01_language_train_model.py":"MODIFY","examples\/gaussian_process\/plot_gpr_noisy_targets.py":"MODIFY","benchmarks\/bench_sgd_regression.py":"MODIFY","benchmarks\/bench_tree.py":"MODIFY","benchmarks\/bench_plot_parallel_pairwise.py":"MODIFY","benchmarks\/bench_plot_ward.py":"MODIFY","examples\/classification\/plot_digits_classification.py":"MODIFY","doc\/tutorial\/statistical_inference\/supervised_learning.rst":"MODIFY","benchmarks\/bench_plot_omp_lars.py":"MODIFY","benchmarks\/bench_lasso.py":"MODIFY","examples\/gaussian_process\/plot_gpc_isoprobability.py":"MODIFY","benchmarks\/bench_glm.py":"MODIFY","doc\/tutorial\/text_analytics\/skeletons\/exercise_01_language_train_model.py":"MODIFY","benchmarks\/bench_glmnet.py":"MODIFY","sklearn\/ensemble\/partial_dependence.py":"MODIFY","examples\/text\/mlcomp_sparse_document_classification.py":"MODIFY"},"diff":{"sklearn\/datasets\/base.py":[{"add":["417","        >>> import matplotlib.pyplot as plt #doctest: +SKIP","418","        >>> plt.gray() #doctest: +SKIP","419","        >>> plt.matshow(digits.images[0]) #doctest: +SKIP","420","        >>> plt.show() #doctest: +SKIP"],"delete":["417","        >>> import pylab as pl #doctest: +SKIP","418","        >>> pl.gray() #doctest: +SKIP","419","        >>> pl.matshow(digits.images[0]) #doctest: +SKIP","420","        >>> pl.show() #doctest: +SKIP"]}],"examples\/decomposition\/plot_sparse_coding.py":[{"add":["19","import matplotlib.pyplot as plt"],"delete":["19","import matplotlib.pylab as plt"]}],"benchmarks\/bench_plot_neighbors.py":[{"add":["6","import matplotlib.pyplot as plt","108","    plt.figure(figsize=(8, 11))","120","        ax = plt.subplot(sbplt, yscale='log')","121","        plt.grid(True)","133","            c_bar = plt.bar(xvals, build_time[alg] - bottom,","134","                            width, bottom, color='r')","135","            q_bar = plt.bar(xvals, query_time[alg],","136","                            width, build_time[alg], color='b')","141","            plt.text((i + 0.02) \/ len(algorithms), 0.98, alg,","142","                     transform=ax.transAxes,","143","                     ha='left',","144","                     va='top',","145","                     bbox=dict(facecolor='w', edgecolor='w', alpha=0.5))","147","            plt.ylabel('Time (s)')","168","        plt.text(1.01, 0.5, title_string,","169","                 transform=ax.transAxes, rotation=-90,","170","                 ha='left', va='center', fontsize=20)","172","        plt.text(0.99, 0.5, descr_string,","173","                 transform=ax.transAxes, rotation=-90,","174","                 ha='right', va='center')","176","        plt.gcf().suptitle(\"%s data set\" % dataset.capitalize(), fontsize=16)","178","    plt.figlegend((c_bar, q_bar), ('construction', 'N-point query'),","179","                  'upper right')","184","    plt.show()"],"delete":["6","import pylab as pl","108","    pl.figure(figsize=(8, 11))","120","        ax = pl.subplot(sbplt, yscale='log')","121","        pl.grid(True)","133","            c_bar = pl.bar(xvals, build_time[alg] - bottom,","134","                           width, bottom, color='r')","135","            q_bar = pl.bar(xvals, query_time[alg],","136","                           width, build_time[alg], color='b')","141","            pl.text((i + 0.02) \/ len(algorithms), 0.98, alg,","142","                    transform=ax.transAxes,","143","                    ha='left',","144","                    va='top',","145","                    bbox=dict(facecolor='w', edgecolor='w', alpha=0.5))","147","            pl.ylabel('Time (s)')","168","        pl.text(1.01, 0.5, title_string,","169","                transform=ax.transAxes, rotation=-90,","170","                ha='left', va='center', fontsize=20)","172","        pl.text(0.99, 0.5, descr_string,","173","                transform=ax.transAxes, rotation=-90,","174","                ha='right', va='center')","176","        pl.gcf().suptitle(\"%s data set\" % dataset.capitalize(), fontsize=16)","178","    pl.figlegend((c_bar, q_bar), ('construction', 'N-point query'),","179","                 'upper right')","184","    pl.show()"]}],"doc\/datasets\/index.rst":[{"add":["95","  if you plan to use ``matplotlib.pyplpt.imshow`` don't forget to scale to the range"],"delete":["95","  if you plan to use ``pylab.imshow`` don't forget to scale to the range"]}],"doc\/tutorial\/statistical_inference\/settings.rst":[{"add":["31",".. topic:: An example of reshaping data would be the digits dataset","44","        >>> import matplotlib.pyplot as plt #doctest: +SKIP","45","        >>> plt.imshow(digits.images[-1], cmap=plt.cm.gray_r) #doctest: +SKIP"],"delete":["31",".. topic:: An example of reshaping data would be the digits dataset ","44","        >>> import pylab as pl #doctest: +SKIP","45","        >>> pl.imshow(digits.images[-1], cmap=pl.cm.gray_r) #doctest: +SKIP","91",""]}],"doc\/tutorial\/text_analytics\/solutions\/exercise_01_language_train_model.py":[{"add":["56","#import matlotlib.pyplot as plt","57","#plt.matshow(cm, cmap=plt.cm.jet)","58","#plt.show()"],"delete":["56","#import pylab as pl","57","#pl.matshow(cm, cmap=pl.cm.jet)","58","#pl.show()"]}],"examples\/gaussian_process\/plot_gpr_noisy_targets.py":[{"add":["28","from matplotlib import pyplot as plt","63","fig = plt.figure()","64","plt.plot(x, f(x), 'r:', label=u'$f(x) = x\\,\\sin(x)$')","65","plt.plot(X, y, 'r.', markersize=10, label=u'Observations')","66","plt.plot(x, y_pred, 'b-', label=u'Prediction')","67","plt.fill(np.concatenate([x, x[::-1]]),","68","         np.concatenate([y_pred - 1.9600 * sigma,","69","                        (y_pred + 1.9600 * sigma)[::-1]]),","70","         alpha=.5, fc='b', ec='None', label='95% confidence interval')","71","plt.xlabel('$x$')","72","plt.ylabel('$f(x)$')","73","plt.ylim(-10, 20)","74","plt.legend(loc='upper left')","99","fig = plt.figure()","100","plt.plot(x, f(x), 'r:', label=u'$f(x) = x\\,\\sin(x)$')","101","plt.errorbar(X.ravel(), y, dy, fmt='r.', markersize=10, label=u'Observations')","102","plt.plot(x, y_pred, 'b-', label=u'Prediction')","103","plt.fill(np.concatenate([x, x[::-1]]),","104","         np.concatenate([y_pred - 1.9600 * sigma,","105","                        (y_pred + 1.9600 * sigma)[::-1]]),","106","         alpha=.5, fc='b', ec='None', label='95% confidence interval')","107","plt.xlabel('$x$')","108","plt.ylabel('$f(x)$')","109","plt.ylim(-10, 20)","110","plt.legend(loc='upper left')","112","plt.show()"],"delete":["28","from matplotlib import pyplot as pl","63","fig = pl.figure()","64","pl.plot(x, f(x), 'r:', label=u'$f(x) = x\\,\\sin(x)$')","65","pl.plot(X, y, 'r.', markersize=10, label=u'Observations')","66","pl.plot(x, y_pred, 'b-', label=u'Prediction')","67","pl.fill(np.concatenate([x, x[::-1]]),","68","        np.concatenate([y_pred - 1.9600 * sigma,","69","                       (y_pred + 1.9600 * sigma)[::-1]]),","70","        alpha=.5, fc='b', ec='None', label='95% confidence interval')","71","pl.xlabel('$x$')","72","pl.ylabel('$f(x)$')","73","pl.ylim(-10, 20)","74","pl.legend(loc='upper left')","99","fig = pl.figure()","100","pl.plot(x, f(x), 'r:', label=u'$f(x) = x\\,\\sin(x)$')","101","pl.errorbar(X.ravel(), y, dy, fmt='r.', markersize=10, label=u'Observations')","102","pl.plot(x, y_pred, 'b-', label=u'Prediction')","103","pl.fill(np.concatenate([x, x[::-1]]),","104","        np.concatenate([y_pred - 1.9600 * sigma,","105","                       (y_pred + 1.9600 * sigma)[::-1]]),","106","        alpha=.5, fc='b', ec='None', label='95% confidence interval')","107","pl.xlabel('$x$')","108","pl.ylabel('$f(x)$')","109","pl.ylim(-10, 20)","110","pl.legend(loc='upper left')","112","pl.show()"]}],"benchmarks\/bench_sgd_regression.py":[{"add":["13","import matplotlib.pyplot as plt","115","    plt.figure('scikit-learn SGD regression benchmark results',","116","               figsize=(5 * 2, 4 * m))","118","        plt.subplot(m, 2, i + 1)","119","        plt.plot(list_n_samples, np.sqrt(elnet_results[:, j, 0]),","120","                 label=\"ElasticNet\")","121","        plt.plot(list_n_samples, np.sqrt(sgd_results[:, j, 0]),","122","                 label=\"SGDRegressor\")","123","        plt.plot(list_n_samples, np.sqrt(asgd_results[:, j, 0]),","124","                 label=\"A-SGDRegressor\")","125","        plt.plot(list_n_samples, np.sqrt(ridge_results[:, j, 0]),","126","                 label=\"Ridge\")","127","        plt.legend(prop={\"size\": 10})","128","        plt.xlabel(\"n_train\")","129","        plt.ylabel(\"RMSE\")","130","        plt.title(\"Test error - %d features\" % list_n_features[j])","133","        plt.subplot(m, 2, i + 1)","134","        plt.plot(list_n_samples, np.sqrt(elnet_results[:, j, 1]),","135","                 label=\"ElasticNet\")","136","        plt.plot(list_n_samples, np.sqrt(sgd_results[:, j, 1]),","137","                 label=\"SGDRegressor\")","138","        plt.plot(list_n_samples, np.sqrt(asgd_results[:, j, 1]),","139","                 label=\"A-SGDRegressor\")","140","        plt.plot(list_n_samples, np.sqrt(ridge_results[:, j, 1]),","141","                 label=\"Ridge\")","142","        plt.legend(prop={\"size\": 10})","143","        plt.xlabel(\"n_train\")","144","        plt.ylabel(\"Time [sec]\")","145","        plt.title(\"Training time - %d features\" % list_n_features[j])","148","    plt.subplots_adjust(hspace=.30)","150","    plt.show()"],"delete":["13","import pylab as pl","115","    pl.figure('scikit-learn SGD regression benchmark results',","116","              figsize=(5 * 2, 4 * m))","118","        pl.subplot(m, 2, i + 1)","119","        pl.plot(list_n_samples, np.sqrt(elnet_results[:, j, 0]),","120","                label=\"ElasticNet\")","121","        pl.plot(list_n_samples, np.sqrt(sgd_results[:, j, 0]),","122","                label=\"SGDRegressor\")","123","        pl.plot(list_n_samples, np.sqrt(asgd_results[:, j, 0]),","124","                label=\"A-SGDRegressor\")","125","        pl.plot(list_n_samples, np.sqrt(ridge_results[:, j, 0]),","126","                label=\"Ridge\")","127","        pl.legend(prop={\"size\": 10})","128","        pl.xlabel(\"n_train\")","129","        pl.ylabel(\"RMSE\")","130","        pl.title(\"Test error - %d features\" % list_n_features[j])","133","        pl.subplot(m, 2, i + 1)","134","        pl.plot(list_n_samples, np.sqrt(elnet_results[:, j, 1]),","135","                label=\"ElasticNet\")","136","        pl.plot(list_n_samples, np.sqrt(sgd_results[:, j, 1]),","137","                label=\"SGDRegressor\")","138","        pl.plot(list_n_samples, np.sqrt(asgd_results[:, j, 1]),","139","                label=\"A-SGDRegressor\")","140","        pl.plot(list_n_samples, np.sqrt(ridge_results[:, j, 1]),","141","                label=\"Ridge\")","142","        pl.legend(prop={\"size\": 10})","143","        pl.xlabel(\"n_train\")","144","        pl.ylabel(\"Time [sec]\")","145","        pl.title(\"Training time - %d features\" % list_n_features[j])","148","    pl.subplots_adjust(hspace=.30)","150","    pl.show()"]}],"benchmarks\/bench_tree.py":[{"add":["16","import matplotlib.pyplot as plt","86","    plt.figure('scikit-learn tree benchmark results')","87","    plt.subplot(211)","88","    plt.title('Learning with varying number of samples')","89","    plt.plot(xx, scikit_classifier_results, 'g-', label='classification')","90","    plt.plot(xx, scikit_regressor_results, 'r-', label='regression')","91","    plt.legend(loc='upper left')","92","    plt.xlabel('number of samples')","93","    plt.ylabel('Time (s)')","115","    plt.subplot(212)","116","    plt.title('Learning in high dimensional spaces')","117","    plt.plot(xx, scikit_classifier_results, 'g-', label='classification')","118","    plt.plot(xx, scikit_regressor_results, 'r-', label='regression')","119","    plt.legend(loc='upper left')","120","    plt.xlabel('number of dimensions')","121","    plt.ylabel('Time (s)')","122","    plt.axis('tight')","123","    plt.show()"],"delete":["16","import pylab as pl","86","    pl.figure('scikit-learn tree benchmark results')","87","    pl.subplot(211)","88","    pl.title('Learning with varying number of samples')","89","    pl.plot(xx, scikit_classifier_results, 'g-', label='classification')","90","    pl.plot(xx, scikit_regressor_results, 'r-', label='regression')","91","    pl.legend(loc='upper left')","92","    pl.xlabel('number of samples')","93","    pl.ylabel('Time (s)')","115","    pl.subplot(212)","116","    pl.title('Learning in high dimensional spaces')","117","    pl.plot(xx, scikit_classifier_results, 'g-', label='classification')","118","    pl.plot(xx, scikit_regressor_results, 'r-', label='regression')","119","    pl.legend(loc='upper left')","120","    pl.xlabel('number of dimensions')","121","    pl.ylabel('Time (s)')","122","    pl.axis('tight')","123","    pl.show()"]}],"benchmarks\/bench_plot_parallel_pairwise.py":[{"add":["4","import matplotlib.pyplot as plt","27","    plt.figure('scikit-learn parallel %s benchmark results' % func.__name__)","28","    plt.plot(sample_sizes, one_core, label=\"one core\")","29","    plt.plot(sample_sizes, multi_core, label=\"multi core\")","30","    plt.xlabel('n_samples')","31","    plt.ylabel('Time (s)')","32","    plt.title('Parallel %s' % func.__name__)","33","    plt.legend()","43","plt.show()"],"delete":["4","import pylab as pl","27","    pl.figure('scikit-learn parallel %s benchmark results' % func.__name__)","28","    pl.plot(sample_sizes, one_core, label=\"one core\")","29","    pl.plot(sample_sizes, multi_core, label=\"multi core\")","30","    pl.xlabel('n_samples')","31","    pl.ylabel('Time (s)')","32","    pl.title('Parallel %s' % func.__name__)","33","    pl.legend()","43","pl.show()"]}],"benchmarks\/bench_plot_ward.py":[{"add":["8","import matplotlib.pyplot as plt","33","plt.figure(\"scikit-learn Ward's method benchmark results\")","34","plt.imshow(np.log(ratio), aspect='auto', origin=\"lower\")","35","plt.colorbar()","36","plt.contour(ratio, levels=[1, ], colors='k')","37","plt.yticks(range(len(n_features)), n_features.astype(np.int))","38","plt.ylabel('N features')","39","plt.xticks(range(len(n_samples)), n_samples.astype(np.int))","40","plt.xlabel('N samples')","41","plt.title(\"Scikit's time, in units of scipy time (log)\")","42","plt.show()"],"delete":["8","import pylab as pl","33","pl.figure(\"scikit-learn Ward's method benchmark results\")","34","pl.imshow(np.log(ratio), aspect='auto', origin=\"lower\")","35","pl.colorbar()","36","pl.contour(ratio, levels=[1, ], colors='k')","37","pl.yticks(range(len(n_features)), n_features.astype(np.int))","38","pl.ylabel('N features')","39","pl.xticks(range(len(n_samples)), n_samples.astype(np.int))","40","pl.xlabel('N samples')","41","pl.title(\"Scikit's time, in units of scipy time (log)\")","42","pl.show()"]}],"examples\/classification\/plot_digits_classification.py":[{"add":["29","# matplotlib.pyplot.imread.  Note that each image must have the same size. For these"],"delete":["29","# pylab.imread.  Note that each image must have the same size. For these"]}],"doc\/tutorial\/statistical_inference\/supervised_learning.rst":[{"add":["214","    >>> import matplotlib.pyplot as plt # doctest: +SKIP","215","    >>> plt.figure() # doctest: +SKIP","221","    ...    plt.plot(test, regr.predict(test)) # doctest: +SKIP","222","    ...    plt.scatter(this_X, y, s=3)  # doctest: +SKIP","240","    >>> plt.figure() # doctest: +SKIP","246","    ...    plt.plot(test, regr.predict(test)) # doctest: +SKIP","247","    ...    plt.scatter(this_X, y, s=3) # doctest: +SKIP"],"delete":["214","    >>> import pylab as pl # doctest: +SKIP","215","    >>> pl.figure() # doctest: +SKIP","221","    ...    pl.plot(test, regr.predict(test)) # doctest: +SKIP","222","    ...    pl.scatter(this_X, y, s=3)  # doctest: +SKIP","240","    >>> pl.figure() # doctest: +SKIP","246","    ...    pl.plot(test, regr.predict(test)) # doctest: +SKIP","247","    ...    pl.scatter(this_X, y, s=3) # doctest: +SKIP"]}],"benchmarks\/bench_plot_omp_lars.py":[{"add":["107","    import matplotlib.pyplot as plt","108","    fig = plt.figure('scikit-learn OMP vs. LARS benchmark results')","110","        ax = fig.add_subplot(1, 2, i+1)","112","        plt.matshow(timings, fignum=False, vmin=1 - vmax, vmax=1 + vmax)","115","        plt.xlabel('n_samples')","116","        plt.ylabel('n_features')","117","        plt.title(label)","119","    plt.subplots_adjust(0.1, 0.08, 0.96, 0.98, 0.4, 0.63)","120","    ax = plt.axes([0.1, 0.08, 0.8, 0.06])","121","    plt.colorbar(cax=ax, orientation='horizontal')","122","    plt.show()"],"delete":["107","    import pylab as pl","108","    fig = pl.figure('scikit-learn OMP vs. LARS benchmark results')","110","        ax = fig.add_subplot(1, 2, i)","112","        pl.matshow(timings, fignum=False, vmin=1 - vmax, vmax=1 + vmax)","115","        pl.xlabel('n_samples')","116","        pl.ylabel('n_features')","117","        pl.title(label)","119","    pl.subplots_adjust(0.1, 0.08, 0.96, 0.98, 0.4, 0.63)","120","    ax = pl.axes([0.1, 0.08, 0.8, 0.06])","121","    pl.colorbar(cax=ax, orientation='horizontal')","122","    pl.show()"]}],"benchmarks\/bench_lasso.py":[{"add":["61","    import matplotlib.pyplot as plt","70","    plt.figure('scikit-learn LASSO benchmark results')","71","    plt.subplot(211)","72","    plt.plot(list_n_samples, lasso_results, 'b-',","74","    plt.plot(list_n_samples, lars_lasso_results, 'r-',","76","    plt.title('precomputed Gram matrix, %d features, alpha=%s' % (n_features,","77","                            alpha))","78","    plt.legend(loc='upper left')","79","    plt.xlabel('number of samples')","80","    plt.ylabel('Time (s)')","81","    plt.axis('tight')","87","    plt.subplot(212)","88","    plt.plot(list_n_features, lasso_results, 'b-', label='Lasso')","89","    plt.plot(list_n_features, lars_lasso_results, 'r-', label='LassoLars')","90","    plt.title('%d samples, alpha=%s' % (n_samples, alpha))","91","    plt.legend(loc='upper left')","92","    plt.xlabel('number of features')","93","    plt.ylabel('Time (s)')","94","    plt.axis('tight')","95","    plt.show()"],"delete":["61","    import pylab as pl","70","    pl.figure('scikit-learn LASSO benchmark results')","71","    pl.subplot(211)","72","    pl.plot(list_n_samples, lasso_results, 'b-',","74","    pl.plot(list_n_samples, lars_lasso_results, 'r-',","76","    pl.title('precomputed Gram matrix, %d features, alpha=%s' % (n_features, alpha))","77","    pl.legend(loc='upper left')","78","    pl.xlabel('number of samples')","79","    pl.ylabel('Time (s)')","80","    pl.axis('tight')","86","    pl.subplot(212)","87","    pl.plot(list_n_features, lasso_results, 'b-', label='Lasso')","88","    pl.plot(list_n_features, lars_lasso_results, 'r-', label='LassoLars')","89","    pl.title('%d samples, alpha=%s' % (n_samples, alpha))","90","    pl.legend(loc='upper left')","91","    pl.xlabel('number of features')","92","    pl.ylabel('Time (s)')","93","    pl.axis('tight')","94","    pl.show()"]}],"examples\/gaussian_process\/plot_gpc_isoprobability.py":[{"add":["20","from matplotlib import pyplot as plt","66","fig = plt.figure(1)","69","plt.xticks([])","70","plt.yticks([])","73","plt.xlabel('$x_1$')","74","plt.ylabel('$x_2$')","76","cax = plt.imshow(y_prob, cmap=cm.gray_r, alpha=0.8,","77","                 extent=(-lim, lim, -lim, lim))","78","norm = plt.matplotlib.colors.Normalize(vmin=0., vmax=0.9)","79","cb = plt.colorbar(cax, ticks=[0., 0.2, 0.4, 0.6, 0.8, 1.], norm=norm)","81","plt.clim(0, 1)","83","plt.plot(X[y <= 0, 0], X[y <= 0, 1], 'r.', markersize=12)","85","plt.plot(X[y > 0, 0], X[y > 0, 1], 'b.', markersize=12)","87","cs = plt.contour(x1, x2, y_true, [0.], colors='k', linestyles='dashdot')","89","cs = plt.contour(x1, x2, y_prob, [0.666], colors='b',","90","                 linestyles='solid')","91","plt.clabel(cs, fontsize=11)","93","cs = plt.contour(x1, x2, y_prob, [0.5], colors='k',","94","                 linestyles='dashed')","95","plt.clabel(cs, fontsize=11)","97","cs = plt.contour(x1, x2, y_prob, [0.334], colors='r',","98","                 linestyles='solid')","99","plt.clabel(cs, fontsize=11)","101","plt.show()"],"delete":["20","from matplotlib import pyplot as pl","66","fig = pl.figure(1)","69","pl.xticks([])","70","pl.yticks([])","73","pl.xlabel('$x_1$')","74","pl.ylabel('$x_2$')","76","cax = pl.imshow(y_prob, cmap=cm.gray_r, alpha=0.8,","77","                extent=(-lim, lim, -lim, lim))","78","norm = pl.matplotlib.colors.Normalize(vmin=0., vmax=0.9)","79","cb = pl.colorbar(cax, ticks=[0., 0.2, 0.4, 0.6, 0.8, 1.], norm=norm)","81","pl.clim(0, 1)","83","pl.plot(X[y <= 0, 0], X[y <= 0, 1], 'r.', markersize=12)","85","pl.plot(X[y > 0, 0], X[y > 0, 1], 'b.', markersize=12)","87","cs = pl.contour(x1, x2, y_true, [0.], colors='k', linestyles='dashdot')","89","cs = pl.contour(x1, x2, y_prob, [0.666], colors='b',","90","                linestyles='solid')","91","pl.clabel(cs, fontsize=11)","93","cs = pl.contour(x1, x2, y_prob, [0.5], colors='k',","94","                linestyles='dashed')","95","pl.clabel(cs, fontsize=11)","97","cs = pl.contour(x1, x2, y_prob, [0.334], colors='r',","98","                linestyles='solid')","99","pl.clabel(cs, fontsize=11)","101","pl.show()"]}],"benchmarks\/bench_glm.py":[{"add":["14","    import matplotlib.pyplot as plt","48","    plt.figure('scikit-learn GLM benchmark results')","49","    plt.xlabel('Dimensions')","50","    plt.ylabel('Time (s)')","51","    plt.plot(dimensions, time_ridge, color='r')","52","    plt.plot(dimensions, time_ols, color='g')","53","    plt.plot(dimensions, time_lasso, color='b')","55","    plt.legend(['Ridge', 'OLS', 'LassoLars'], loc='upper left')","56","    plt.axis('tight')","57","    plt.show()"],"delete":["14","    import pylab as pl","48","    pl.figure('scikit-learn GLM benchmark results')","49","    pl.xlabel('Dimensions')","50","    pl.ylabel('Time (s)')","51","    pl.plot(dimensions, time_ridge, color='r')","52","    pl.plot(dimensions, time_ols, color='g')","53","    pl.plot(dimensions, time_lasso, color='b')","55","    pl.legend(['Ridge', 'OLS', 'LassoLars'], loc='upper left')","56","    pl.axis('tight')","57","    pl.show()"]}],"doc\/tutorial\/text_analytics\/skeletons\/exercise_01_language_train_model.py":[{"add":["48","#import matplotlib.pyplot as plt","49","#plt.matshow(cm, cmap=plt.cm.jet)","50","#plt.show()"],"delete":["48","#import pylab as pl","49","#pl.matshow(cm, cmap=pl.cm.jet)","50","#pl.show()"]}],"benchmarks\/bench_glmnet.py":[{"add":["49","    # Delayed import of matplotlib.pyplot","50","    import matplotlib.pyplot as plt","78","    plt.clf()","80","    plt.title('Lasso regression on sample dataset (%d features)' % n_features)","81","    plt.plot(xx, scikit_results, 'b-', label='scikit-learn')","82","    plt.plot(xx, glmnet_results, 'r-', label='glmnet')","83","    plt.legend()","84","    plt.xlabel('number of samples to classify')","85","    plt.ylabel('Time (s)')","86","    plt.show()","119","    plt.figure('scikit-learn vs. glmnet benchmark results')","120","    plt.title('Regression in high dimensional spaces (%d samples)' % n_samples)","121","    plt.plot(xx, scikit_results, 'b-', label='scikit-learn')","122","    plt.plot(xx, glmnet_results, 'r-', label='glmnet')","123","    plt.legend()","124","    plt.xlabel('number of features')","125","    plt.ylabel('Time (s)')","126","    plt.axis('tight')","127","    plt.show()"],"delete":["49","    # Delayed import of pylab","50","    import pylab as pl","78","    pl.clf()","80","    pl.title('Lasso regression on sample dataset (%d features)' % n_features)","81","    pl.plot(xx, scikit_results, 'b-', label='scikit-learn')","82","    pl.plot(xx, glmnet_results, 'r-', label='glmnet')","83","    pl.legend()","84","    pl.xlabel('number of samples to classify')","85","    pl.ylabel('Time (s)')","86","    pl.show()","119","    pl.figure('scikit-learn vs. glmnet benchmark results')","120","    pl.title('Regression in high dimensional spaces (%d samples)' % n_samples)","121","    pl.plot(xx, scikit_results, 'b-', label='scikit-learn')","122","    pl.plot(xx, glmnet_results, 'r-', label='glmnet')","123","    pl.legend()","124","    pl.xlabel('number of features')","125","    pl.ylabel('Time (s)')","126","    pl.axis('tight')","127","    pl.show()"]}],"sklearn\/ensemble\/partial_dependence.py":[{"add":["210","        Dict with keywords passed to the ``matplotlib.pyplot.plot`` call.","213","        Dict with keywords passed to the ``matplotlib.pyplot.plot`` call."],"delete":["210","        Dict with keywords passed to the ``pylab.plot`` call.","213","        Dict with keywords passed to the ``pylab.plot`` call."]}],"examples\/text\/mlcomp_sparse_document_classification.py":[{"add":["46","import matplotlib.pyplot as pl"],"delete":["46","import pylab as pl"]}]}},"c98adf7d580d87c0241568fa178c2afd08afc847":{"changes":{"sklearn\/datasets\/base.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/datasets\/tests\/test_base.py":"MODIFY"},"diff":{"sklearn\/datasets\/base.py":[{"add":["320","def load_breast_cancer(return_X_y=False):","334","    Parameters","335","    ----------","336","    return_X_y : boolean, default=False","337","        If True, returns ``(data, target)`` instead of a Bunch object.","338","        See below for more information about the `data` and `target` object.","339","","340","    .. versionadded:: 0.18","341","","351","    (data, target) : tuple if ``return_X_y`` is True","352","","353","    .. versionadded:: 0.18","354","","404","    if return_X_y:","405","        return data, target","406",""],"delete":["320","def load_breast_cancer():"]}],"doc\/whats_new.rst":[{"add":["233","     :func:`load_iris` dataset, ","234","     (`#7049 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/7049>`_)","235","     :func:`load_breast_cancer` dataset","236","     (`#7152 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/7152>`_) by"],"delete":["233","     :func:`load_iris` dataset.","234","     (`#7049 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/7049>`_) by"]}],"sklearn\/datasets\/tests\/test_base.py":[{"add":["198","    # test return_X_y option","199","    X_y_tuple = load_breast_cancer(return_X_y=True)","200","    bunch = load_breast_cancer()","201","    assert_true(isinstance(X_y_tuple, tuple))","202","    assert_array_equal(X_y_tuple[0], bunch.data)","203","    assert_array_equal(X_y_tuple[1], bunch.target)","204",""],"delete":[]}]}},"70d7fecaab23095cf3793977d3c15d1a21a7a868":{"changes":{"sklearn\/model_selection\/tests\/test_validation.py":"MODIFY"},"diff":{"sklearn\/model_selection\/tests\/test_validation.py":[{"add":["7","from time import sleep","65","try:","66","    WindowsError","67","except NameError:","68","    WindowsError = None","69","","70","","790","    scores = np.memmap(tf.name, dtype=np.float64)","791","    score = np.memmap(tf.name, shape=(), mode='r', dtype=np.float64)","798","        # Best effort to release the mmap file handles before deleting the","799","        # backing file under Windows","800","        scores, score = None, None","801","        for _ in range(3):","802","            try:","803","                os.unlink(tf.name)","804","                break","805","            except WindowsError:","806","                sleep(1.)"],"delete":["783","    scores = np.memmap(tf.name, dtype=float)","784","    score = np.memmap(tf.name, shape=(), mode='w+', dtype=float)","791","        os.unlink(tf.name)"]}]}},"9cef05faf5cf7e776b4fc050bfc266d1542c1188":{"changes":{"sklearn\/base.py":"MODIFY"},"diff":{"sklearn\/base.py":[{"add":["255","        (such as pipelines). The latter have parameters of the form"],"delete":["255","        (such as pipelines). The former have parameters of the form"]}]}},"db370178d6a3525c881ea161b9373a7f65fb810a":{"changes":{"examples\/cluster\/plot_cluster_comparison.py":"MODIFY"},"diff":{"examples\/cluster\/plot_cluster_comparison.py":[{"add":["5","This example shows characteristics of different","7","but still in 2D. With the exception of the last dataset,","8","the parameters of each of these dataset-algorithm pairs","9","has been tuned to produce good clustering results. Some","10","algorithms are more sensitive to parameter values than","11","others.","13","The last dataset is an example of a 'null' situation for","14","clustering: the data is homogeneous, and there is no good","15","clustering. For this example, the null dataset uses the","16","same parameters as the dataset in the row above it, which","17","represents a mismatch in the parameter values and the","18","data structure.","20","While these examples give some intuition about the","21","algorithms, this intuition might not apply to very high","22","dimensional data.","27","import warnings","32","from sklearn import cluster, datasets, mixture","35","from itertools import cycle, islice","39","# ============","42","# ============","50","# Anisotropicly distributed data","51","random_state = 170","52","X, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)","53","transformation = [[0.6, -0.6], [-0.4, 0.8]]","54","X_aniso = np.dot(X, transformation)","55","aniso = (X_aniso, y)","57","# blobs with varied variances","58","varied = datasets.make_blobs(n_samples=n_samples,","59","                             cluster_std=[1.0, 2.5, 0.5],","60","                             random_state=random_state)","62","# ============","63","# Set up cluster parameters","64","# ============","65","plt.figure(figsize=(9 * 2 + 3, 12.5))","71","default_base = {'quantile': .3,","72","                'eps': .3,","73","                'damping': .9,","74","                'preference': -200,","75","                'n_neighbors': 10,","76","                'n_clusters': 3}","77","","78","datasets = [","79","    (noisy_circles, {'damping': .77, 'preference': -240,","80","                     'quantile': .2, 'n_clusters': 2}),","81","    (noisy_moons, {'damping': .75, 'preference': -220, 'n_clusters': 2}),","82","    (varied, {'eps': .18, 'n_neighbors': 2}),","83","    (aniso, {'eps': .15, 'n_neighbors': 2}),","84","    (blobs, {}),","85","    (no_structure, {})]","86","","87","for i_dataset, (dataset, algo_params) in enumerate(datasets):","88","    # update parameters with dataset-specific values","89","    params = default_base.copy()","90","    params.update(algo_params)","91","","93","","98","    bandwidth = cluster.estimate_bandwidth(X, quantile=params['quantile'])","101","    connectivity = kneighbors_graph(","102","        X, n_neighbors=params['n_neighbors'], include_self=False)","106","    # ============","107","    # Create cluster objects","108","    # ============","110","    two_means = cluster.MiniBatchKMeans(n_clusters=params['n_clusters'])","111","    ward = cluster.AgglomerativeClustering(","112","        n_clusters=params['n_clusters'], linkage='ward',","114","    spectral = cluster.SpectralClustering(","115","        n_clusters=params['n_clusters'], eigen_solver='arpack',","116","        affinity=\"nearest_neighbors\")","117","    dbscan = cluster.DBSCAN(eps=params['eps'])","118","    affinity_propagation = cluster.AffinityPropagation(","119","        damping=params['damping'], preference=params['preference'])","120","    average_linkage = cluster.AgglomerativeClustering(","121","        linkage=\"average\", affinity=\"cityblock\",","122","        n_clusters=params['n_clusters'], connectivity=connectivity)","123","    birch = cluster.Birch(n_clusters=params['n_clusters'])","124","    gmm = mixture.GaussianMixture(","125","        n_components=params['n_clusters'], covariance_type='full')","127","    clustering_algorithms = (","128","        ('MiniBatchKMeans', two_means),","129","        ('AffinityPropagation', affinity_propagation),","130","        ('MeanShift', ms),","131","        ('SpectralClustering', spectral),","132","        ('Ward', ward),","133","        ('AgglomerativeClustering', average_linkage),","134","        ('DBSCAN', dbscan),","135","        ('Birch', birch),","136","        ('GaussianMixture', gmm)","137","    )","139","    for name, algorithm in clustering_algorithms:","141","","142","        # catch warnings related to kneighbors_graph","143","        with warnings.catch_warnings():","144","            warnings.filterwarnings(","145","                \"ignore\",","146","                message=\"the number of connected components of the \" +","147","                \"connectivity matrix is [0-9]{1,2}\" +","148","                \" > 1. Completing it to avoid stopping the tree early.\",","149","                category=UserWarning)","150","            warnings.filterwarnings(","151","                \"ignore\",","152","                message=\"Graph is not fully connected, spectral embedding\" +","153","                \" may not work as expected.\",","154","                category=UserWarning)","155","            algorithm.fit(X)","156","","163","        plt.subplot(len(datasets), len(clustering_algorithms), plot_num)","167","        colors = np.array(list(islice(cycle(['#377eb8', '#ff7f00', '#4daf4a',","168","                                             '#f781bf', '#a65628', '#984ea3',","169","                                             '#999999', '#e41a1c', '#dede00']),","170","                                      int(max(y_pred) + 1))))","171","        plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[y_pred])","172","","173","        plt.xlim(-2.5, 2.5)","174","        plt.ylim(-2.5, 2.5)"],"delete":["5","This example aims at showing characteristics of different","7","but still in 2D. The last dataset is an example of a 'null'","8","situation for clustering: the data is homogeneous, and","9","there is no good clustering.","11","While these examples give some intuition about the algorithms,","12","this intuition might not apply to very high dimensional data.","14","The results could be improved by tweaking the parameters for","15","each clustering strategy, for instance setting the number of","16","clusters for the methods that needs this parameter","17","specified. Note that affinity propagation has a tendency to","18","create many clusters. Thus in this example its two parameters","19","(damping and per-point preference) were set to mitigate this","20","behavior.","29","from sklearn import cluster, datasets","44","colors = np.array([x for x in 'bgrcmykbgrcmykbgrcmykbgrcmyk'])","45","colors = np.hstack([colors] * 20)","47","clustering_names = [","48","    'MiniBatchKMeans', 'AffinityPropagation', 'MeanShift',","49","    'SpectralClustering', 'Ward', 'AgglomerativeClustering',","50","    'DBSCAN', 'Birch']","52","plt.figure(figsize=(len(clustering_names) * 2 + 3, 9.5))","58","datasets = [noisy_circles, noisy_moons, blobs, no_structure]","59","for i_dataset, dataset in enumerate(datasets):","65","    bandwidth = cluster.estimate_bandwidth(X, quantile=0.3)","68","    connectivity = kneighbors_graph(X, n_neighbors=10, include_self=False)","72","    # create clustering estimators","74","    two_means = cluster.MiniBatchKMeans(n_clusters=2)","75","    ward = cluster.AgglomerativeClustering(n_clusters=2, linkage='ward',","76","                                           connectivity=connectivity)","77","    spectral = cluster.SpectralClustering(n_clusters=2,","78","                                          eigen_solver='arpack',","79","                                          affinity=\"nearest_neighbors\")","80","    dbscan = cluster.DBSCAN(eps=.2)","81","    affinity_propagation = cluster.AffinityPropagation(damping=.9,","82","                                                       preference=-200)","83","","84","    average_linkage = cluster.AgglomerativeClustering(","85","        linkage=\"average\", affinity=\"cityblock\", n_clusters=2,","88","    birch = cluster.Birch(n_clusters=2)","89","    clustering_algorithms = [","90","        two_means, affinity_propagation, ms, spectral, ward, average_linkage,","91","        dbscan, birch]","93","    for name, algorithm in zip(clustering_names, clustering_algorithms):","94","        # predict cluster memberships","96","        algorithm.fit(X)","103","        # plot","104","        plt.subplot(4, len(clustering_algorithms), plot_num)","107","        plt.scatter(X[:, 0], X[:, 1], color=colors[y_pred].tolist(), s=10)","109","        if hasattr(algorithm, 'cluster_centers_'):","110","            centers = algorithm.cluster_centers_","111","            center_colors = colors[:len(centers)]","112","            plt.scatter(centers[:, 0], centers[:, 1], s=100, c=center_colors)","113","        plt.xlim(-2, 2)","114","        plt.ylim(-2, 2)"]}]}},"7b3c467205206fa031de2ae9ddf6f1cddaf1bdd8":{"changes":{"doc\/whats_new.rst":"MODIFY"},"diff":{"doc\/whats_new.rst":[{"add":["331","    - :func:`silhouette_score` now again supports sparse input, and this also fixes","332","      examples\/text\/document_clustering.py. ","333","      By `YenChen Lin`_.","334",""],"delete":[]}]}},"f2df3db65d3e4f0c273ace7f0cce99412ab10ce5":{"changes":{"sklearn\/feature_extraction\/text.py":"MODIFY"},"diff":{"sklearn\/feature_extraction\/text.py":[{"add":["995","            self._idf_diag = sp.spdiags(idf, diags=0, m=n_features, ","996","                                        n=n_features, format='csr')"],"delete":["995","            self._idf_diag = sp.spdiags(idf,","996","                                        diags=0, m=n_features, n=n_features)"]}]}},"1b6c84da13678bb0bfae23dc67fe8d7618a6c097":{"changes":{"sklearn\/datasets\/base.py":"MODIFY","doc\/whats_new.rst":"MODIFY"},"diff":{"sklearn\/datasets\/base.py":[{"add":["266","    .. versionadded:: 0.18","267","","279","    .. versionadded:: 0.18","280",""],"delete":["262","    ","263","        .. versionadded:: 0.18","264","     "]}],"doc\/whats_new.rst":[{"add":["232","   - Added new return type ``(data, target)`` : tuple option to","233","     :func:`load_iris` dataset.","234","     (`#7049 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/7049>`_) by","235","     `Manvendra Singh`_.","4321",".. _Ibraim Ganiev: https:\/\/github.com\/olologin"],"delete":["232","   - Added new return type ``(data, target)`` : tuple option to :func:`load_iris` dataset. (`#7049 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/7049>`_) ","233","     By `Manvendra Singh`_ and `Nelson Liu`_.   ","4319",".. _Ibraim Ganiev: https:\/\/github.com\/olologin"]}]}},"a627a079c6dc924ba93904079e773c95bbc4482d":{"changes":{"sklearn\/neighbors\/tests\/test_neighbors.py":"MODIFY"},"diff":{"sklearn\/neighbors\/tests\/test_neighbors.py":[{"add":["944","        results = {}","960","            results[algorithm] = neigh.kneighbors(test, return_distance=True)","961","        assert_array_almost_equal(results['brute'][0], results['ball_tree'][0])","962","        assert_array_almost_equal(results['brute'][1], results['ball_tree'][1])","963","        if 'kd_tree' in results:","964","            assert_array_almost_equal(results['brute'][0],","965","                                      results['kd_tree'][0])","966","            assert_array_almost_equal(results['brute'][1],","967","                                      results['kd_tree'][1])"],"delete":["944","        results = []","955","","961","            results.append(neigh.kneighbors(test, return_distance=True))","962","","963","        assert_array_almost_equal(results[0][0], results[1][0])","964","        assert_array_almost_equal(results[0][1], results[1][1])"]}]}},"1f182cc0b47e55ba2e5df410a14582deeeed4610":{"changes":{"sklearn\/mixture\/base.py":"MODIFY"},"diff":{"sklearn\/mixture\/base.py":[{"add":["203","                self.lower_bound_ = -np.infty"],"delete":["203","                self.lower_bound_ = np.infty"]}]}},"fa598738570e03c50df5836a4e0a4285b91c957c":{"changes":{"sklearn\/utils\/stats.py":"MODIFY","sklearn\/mixture\/dpgmm.py":"MODIFY","sklearn\/utils\/extmath.py":"MODIFY","sklearn\/datasets\/samples_generator.py":"MODIFY","sklearn\/ensemble\/weight_boosting.py":"MODIFY","sklearn\/decomposition\/pca.py":"MODIFY","sklearn\/manifold\/locally_linear.py":"MODIFY","sklearn\/utils\/tests\/test_extmath.py":"MODIFY","sklearn\/cluster\/k_means_.py":"MODIFY"},"diff":{"sklearn\/utils\/stats.py":[{"add":["3","from ..utils.extmath import stable_cumsum","56","    weight_cdf = stable_cumsum(sample_weight[sorted_idx])"],"delete":["55","    weight_cdf = sample_weight[sorted_idx].cumsum()"]}],"sklearn\/mixture\/dpgmm.py":[{"add":["26","from ..utils.extmath import logsumexp, pinvh, squared_norm, stable_cumsum","464","        cz = stable_cumsum(z[:, ::-1], axis=-1)[:, -2::-1]"],"delete":["26","from ..utils.extmath import logsumexp, pinvh, squared_norm","464","        cz = np.cumsum(z[:, ::-1], axis=-1)[:, -2::-1]"]}],"sklearn\/utils\/extmath.py":[{"add":["27","from ..exceptions import ConvergenceWarning, NonBLASDotWarning","846","def stable_cumsum(arr, axis=None, rtol=1e-05, atol=1e-08):","853","    axis : int, optional","854","        Axis along which the cumulative sum is computed.","855","        The default (None) is to compute the cumsum over the flattened array.","861","    # sum is as unstable as cumsum for numpy < 1.9","862","    if np_version < (1, 9):","863","        return np.cumsum(arr, axis=axis, dtype=np.float64)","864","","865","    out = np.cumsum(arr, axis=axis, dtype=np.float64)","866","    expected = np.sum(arr, axis=axis, dtype=np.float64)","867","    if not np.all(np.isclose(out.take(-1, axis=axis), expected, rtol=rtol,","868","                             atol=atol, equal_nan=True)):","869","        warnings.warn('cumsum was found to be unstable: '","870","                      'its last element does not correspond to sum',","871","                      ConvergenceWarning)"],"delete":["27","from ..exceptions import NonBLASDotWarning","846","def stable_cumsum(arr, rtol=1e-05, atol=1e-08):","858","    out = np.cumsum(arr, dtype=np.float64)","859","    expected = np.sum(arr, dtype=np.float64)","860","    if not np.allclose(out[-1], expected, rtol=rtol, atol=atol):","861","        raise RuntimeError('cumsum was found to be unstable: '","862","                           'its last element does not correspond to sum')"]}],"sklearn\/datasets\/samples_generator.py":[{"add":["1196","        The probability that a coefficient is zero (see notes). Larger values"],"delete":["1196","        The probability that a coefficient is zero (see notes). Larger values "]}],"sklearn\/ensemble\/weight_boosting.py":[{"add":["40","from ..utils.extmath import stable_cumsum","1005","        cdf = stable_cumsum(sample_weight)","1062","        weight_cdf = stable_cumsum(self.estimator_weights_[sorted_idx], axis=1)"],"delete":["1004","        cdf = sample_weight.cumsum()","1061","        weight_cdf = self.estimator_weights_[sorted_idx].cumsum(axis=1)"]}],"sklearn\/decomposition\/pca.py":[{"add":["26","from ..utils.extmath import stable_cumsum","396","            ratio_cumsum = stable_cumsum(explained_variance_ratio_)"],"delete":["395","            ratio_cumsum = explained_variance_ratio_.cumsum()"]}],"sklearn\/manifold\/locally_linear.py":[{"add":["12","from ..utils.extmath import stable_cumsum","423","        evals_cumsum = stable_cumsum(evals, 1)"],"delete":["422","        evals_cumsum = np.cumsum(evals, 1)"]}],"sklearn\/utils\/tests\/test_extmath.py":[{"add":["20","from sklearn.utils.testing import assert_warns","39","from sklearn.exceptions import ConvergenceWarning","658","    assert_warns(ConvergenceWarning, stable_cumsum, r, rtol=0, atol=0)","659","","660","    # test axis parameter","661","    A = np.random.RandomState(36).randint(1000, size=(5, 5, 5))","662","    assert_array_equal(stable_cumsum(A, axis=0), np.cumsum(A, axis=0))","663","    assert_array_equal(stable_cumsum(A, axis=1), np.cumsum(A, axis=1))","664","    assert_array_equal(stable_cumsum(A, axis=2), np.cumsum(A, axis=2))"],"delete":["656","    assert_raise_message(RuntimeError,","657","                         'cumsum was found to be unstable: its last element '","658","                         'does not correspond to sum',","659","                         stable_cumsum, r, rtol=0, atol=0)"]}],"sklearn\/cluster\/k_means_.py":[{"add":["20","from ..utils.extmath import row_norms, squared_norm, stable_cumsum","108","        candidate_ids = np.searchsorted(stable_cumsum(closest_dist_sq),","109","                                        rand_vals)"],"delete":["20","from ..utils.extmath import row_norms, squared_norm","108","        candidate_ids = np.searchsorted(closest_dist_sq.cumsum(), rand_vals)"]}]}},"c0a04a30e59e046f639d316e6204742923910b8d":{"changes":{"doc\/modules\/ensemble.rst":"MODIFY"},"diff":{"doc\/modules\/ensemble.rst":[{"add":["206","might result in models that consume a lot of RAM. The best parameter values"],"delete":["206","might result in models that consume a lot of ram. The best parameter values"]}]}},"89e07351509856e4fcb11fdd752dfcd34044cf57":{"changes":{"sklearn\/mixture\/gmm.py":"MODIFY","sklearn\/mixture\/tests\/test_gmm.py":"MODIFY"},"diff":{"sklearn\/mixture\/gmm.py":[{"add":["26","@deprecated(\"The function log_multivariate_normal_density is deprecated in 0.18\"","27","            \" and will be removed in 0.20.\")","652","@deprecated(\"The class GMM is deprecated in 0.18 and will be \"","653","            \" removed in 0.20. Use class GaussianMixture instead.\")","760","@deprecated(\"The functon distribute_covar_matrix_to_match_covariance_type\"","761","            \"is deprecated in 0.18 and will be removed in 0.20.\")"],"delete":["26","","651","@deprecated(\"The class GMM is deprecated and \"","652","            \"will be removed in 0.20. Use class GaussianMixture instead.\")"]}],"sklearn\/mixture\/tests\/test_gmm.py":[{"add":["14","from sklearn.utils.testing import (assert_greater, assert_raise_message,","15","                                   assert_warns_message, ignore_warnings)","82","    lpr = assert_warns_message(DeprecationWarning, \"The function\"","83","                             \" log_multivariate_normal_density is \"","84","                             \"deprecated in 0.18 and will be removed in 0.20.\",","85","                             mixture.log_multivariate_normal_density,","86","                             X, mu, cv, 'diag')","99","    lpr = assert_warns_message(DeprecationWarning, \"The function\"","100","                             \" log_multivariate_normal_density is \"","101","                             \"deprecated in 0.18 and will be removed in 0.20.\",","102","                             mixture.log_multivariate_normal_density,","103","                             X, mu, spherecv, 'spherical')","116","    lpr = assert_warns_message(DeprecationWarning, \"The function\"","117","                             \" log_multivariate_normal_density is \"","118","                             \"deprecated in 0.18 and will be removed in 0.20.\",","119","                             mixture.log_multivariate_normal_density,","120","                             X, mu, fullcv, 'full')"],"delete":["14","from sklearn.utils.testing import assert_greater","15","from sklearn.utils.testing import assert_raise_message","18","from sklearn.utils.testing import ignore_warnings","83","    lpr = mixture.log_multivariate_normal_density(X, mu, cv, 'diag')","96","    lpr = mixture.log_multivariate_normal_density(X, mu, spherecv,","97","                                                  'spherical')","100","","111","    lpr = mixture.log_multivariate_normal_density(X, mu, fullcv, 'full')"]}]}},"22cf46edbc33a6bae158bb6a5bd7b779285fcfa0":{"changes":{".travis.yml":"MODIFY","sklearn\/cross_decomposition\/pls_.py":"MODIFY"},"diff":{".travis.yml":[{"add":["41","    - DISTRIB=\"conda\" PYTHON_VERSION=\"3.5\" INSTALL_MKL=\"true\""],"delete":["41","    - DISTRIB=\"conda\" PYTHON_VERSION=\"3.5\" INSTALL_MKL=\"false\""]}],"sklearn\/cross_decomposition\/pls_.py":[{"add":["55","        # If y_score only has zeros x_weights will only have zeros. In","56","        # this case add an epsilon to converge to a more acceptable","57","        # solution","58","        if np.dot(x_weights.T, x_weights) < eps:","59","            x_weights += eps"],"delete":[]}]}},"e17d5c9337a2c9557be1bda085f36d2769c0fdf7":{"changes":{"sklearn\/gaussian_process\/tests\/test_gaussian_process.py":"MODIFY","sklearn\/gaussian_process\/gaussian_process.py":"MODIFY"},"diff":{"sklearn\/gaussian_process\/tests\/test_gaussian_process.py":[{"add":["135","def test_batch_size():","136","    # TypeError when using batch_size on Python 3, see","137","    # https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/7329 for more","138","    # details","139","    gp = GaussianProcess()","140","    gp.fit(X, y)","141","    gp.predict(X, batch_size=1)","142","    gp.predict(X, batch_size=1, eval_MSE=True)","143","","144",""],"delete":[]}],"sklearn\/gaussian_process\/gaussian_process.py":[{"add":["517","                for k in range(max(1, int(n_eval \/ batch_size))):","529","                for k in range(max(1, int(n_eval \/ batch_size))):"],"delete":["517","                for k in range(max(1, n_eval \/ batch_size)):","529","                for k in range(max(1, n_eval \/ batch_size)):"]}]}},"59bf211789d29d70642fd64d6d4bef95ff518e8d":{"changes":{"sklearn\/metrics\/pairwise.py":"MODIFY","sklearn\/metrics\/pairwise_fast.pyx":"MODIFY"},"diff":{"sklearn\/metrics\/pairwise.py":[{"add":["895","           parameter ``dense_output`` for dense output."],"delete":["895","           parameter *dense_output* for sparse output."]}],"sklearn\/metrics\/pairwise_fast.pyx":[{"add":["16","ctypedef float [:, :] float_array_2d_t","17","ctypedef double [:, :] double_array_2d_t"],"delete":["16","ctypedef float [:, :] float_array_2d_t ","17","ctypedef double [:, :] double_array_2d_t "]}]}},"52bfd2585e5dec3b2ee2d1d9e2036ba637314cf4":{"changes":{"sklearn\/naive_bayes.py":"MODIFY"},"diff":{"sklearn\/naive_bayes.py":[{"add":["390","                             (unique_y[~unique_y_in_classes], classes))"],"delete":["390","                             (y[~unique_y_in_classes], classes))"]}]}},"814223cbfd9552f1ca5925b291616006f1d269a4":{"changes":{"sklearn\/model_selection\/_validation.py":"MODIFY","sklearn\/cross_validation.py":"MODIFY","sklearn\/model_selection\/tests\/test_validation.py":"MODIFY"},"diff":{"sklearn\/model_selection\/_validation.py":[{"add":["303","    if hasattr(score, 'item'):","304","        try:","305","            # e.g. unwrap memmapped scalars","306","            score = score.item()","307","        except ValueError:","308","            # non-scalar?","309","            pass"],"delete":[]}],"sklearn\/cross_validation.py":[{"add":["1650","    if hasattr(score, 'item'):","1651","        try:","1652","            # e.g. unwrap memmapped scalars","1653","            score = score.item()","1654","        except ValueError:","1655","            # non-scalar?","1656","            pass"],"delete":[]}],"sklearn\/model_selection\/tests\/test_validation.py":[{"add":["5","import tempfile","6","import os","773","","774","","775","def test_score_memmap():","776","    # Ensure a scalar score of memmap type is accepted","777","    iris = load_iris()","778","    X, y = iris.data, iris.target","779","    clf = MockClassifier()","780","    tf = tempfile.NamedTemporaryFile(mode='wb', delete=False)","781","    tf.write(b'Hello world!!!!!')","782","    tf.close()","783","    scores = np.memmap(tf.name, dtype=float)","784","    score = np.memmap(tf.name, shape=(), mode='w+', dtype=float)","785","    try:","786","        cross_val_score(clf, X, y, scoring=lambda est, X, y: score)","787","        # non-scalar should still fail","788","        assert_raises(ValueError, cross_val_score, clf, X, y,","789","                      scoring=lambda est, X, y: scores)","790","    finally:","791","        os.unlink(tf.name)"],"delete":[]}]}},"913967b6ee83420285c8472604aad91621f04e7f":{"changes":{"sklearn\/svm\/libsvm_sparse.pyx":"MODIFY"},"diff":{"sklearn\/svm\/libsvm_sparse.pyx":[{"add":["400","        raise MemoryError(\"We've run out of memory\")"],"delete":["400","        raise MemoryError(\"We've run out of of memory\")"]}]}},"376aa50e70d7b45e115e01654bc0a91b5cb9b60d":{"changes":{"sklearn\/tree\/_tree.pxd":"MODIFY","sklearn\/tree\/tree.py":"MODIFY","sklearn\/ensemble\/gradient_boosting.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/tree\/_tree.pyx":"MODIFY","sklearn\/ensemble\/forest.py":"MODIFY","sklearn\/tree\/tests\/test_tree.py":"MODIFY"},"diff":{"sklearn\/tree\/_tree.pxd":[{"add":["6","#          Nelson Liu <nelson@nelsonliu.me>","98","    cdef double min_impurity_split  # Impurity threshold for early stopping"],"delete":[]}],"sklearn\/tree\/tree.py":[{"add":["12","#          Nelson Liu <nelson@nelsonliu.me>","92","                 min_impurity_split,","104","        self.min_impurity_split = min_impurity_split","309","        if self.min_impurity_split < 0.:","310","            raise ValueError(\"min_impurity_split must be greater than or equal \"","311","                             \"to 0\")","312","","368","                                            max_depth, self.min_impurity_split)","374","                                           max_leaf_nodes, self.min_impurity_split)","617","    min_impurity_split : float, optional (default=1e-7)","618","        Threshold for early stopping in tree growth. A node will split","619","        if its impurity is above the threshold, otherwise it is a leaf.","620","","698","                 min_impurity_split=1e-7,","712","            min_impurity_split=min_impurity_split,","863","    min_impurity_split : float, optional (default=1e-7)","864","        Threshold for early stopping in tree growth. If the impurity","865","        of a node is below the threshold, the node is a leaf.","866","","936","                 min_impurity_split=1e-7,","948","            min_impurity_split=min_impurity_split,","986","                 min_impurity_split=1e-7,","998","            min_impurity_split=min_impurity_split,","1035","                 min_impurity_split=1e-7,","1046","            min_impurity_split=min_impurity_split,"],"delete":["361","                                            max_depth)","367","                                           max_leaf_nodes)"]}],"sklearn\/ensemble\/gradient_boosting.py":[{"add":["724","                 max_depth, min_impurity_split, init, subsample, max_features,","738","        self.min_impurity_split = min_impurity_split","1361","    min_impurity_split : float, optional (default=1e-7)","1362","        Threshold for early stopping in tree growth. A node will split","1363","        if its impurity is above the threshold, otherwise it is a leaf.","1364","","1444","                 max_depth=3, min_impurity_split=1e-7, init=None,","1445","                 random_state=None, max_features=None, verbose=0,","1457","            max_leaf_nodes=max_leaf_nodes,","1458","            min_impurity_split=min_impurity_split,","1459","            warm_start=warm_start,","1720","    min_impurity_split : float, optional (default=1e-7)","1721","        Threshold for early stopping in tree growth. A node will split","1722","        if its impurity is above the threshold, otherwise it is a leaf.","1723","","1804","                 max_depth=3, min_impurity_split=1e-7, init=None, random_state=None,","1814","            max_features=max_features, min_impurity_split=min_impurity_split,"],"delete":["724","                 max_depth, init, subsample, max_features,","1439","                 max_depth=3, init=None, random_state=None,","1440","                 max_features=None, verbose=0,","1452","            max_leaf_nodes=max_leaf_nodes, warm_start=warm_start,","1793","                 max_depth=3, init=None, random_state=None,","1803","            max_features=max_features,"]}],"doc\/whats_new.rst":[{"add":["127","     - Added weighted impurity-based early stopping criterion for decision tree","128","       growth. (`#6954","129","       <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/6954>`_) by `Nelson","130","       Liu`_","131",""],"delete":[]}],"sklearn\/tree\/_tree.pyx":[{"add":["14","#          Nelson Liu <nelson@nelsonliu.me>","133","                  SIZE_t max_depth, double min_impurity_split):","139","        self.min_impurity_split = min_impurity_split","169","        cdef double min_impurity_split = self.min_impurity_split","227","                is_leaf = (is_leaf or","228","                           (impurity <= min_impurity_split))","294","                  SIZE_t max_depth, SIZE_t max_leaf_nodes,","295","                  double min_impurity_split):","302","        self.min_impurity_split = min_impurity_split","428","        cdef double min_impurity_split = self.min_impurity_split","444","                   (impurity <= min_impurity_split))"],"delete":["65","cdef DTYPE_t MIN_IMPURITY_SPLIT = 1e-7","133","                  SIZE_t max_depth):","225","                is_leaf = is_leaf or (impurity <= MIN_IMPURITY_SPLIT)","291","                  SIZE_t max_depth, SIZE_t max_leaf_nodes):","438","                   (impurity <= MIN_IMPURITY_SPLIT))"]}],"sklearn\/ensemble\/forest.py":[{"add":["807","    min_impurity_split : float, optional (default=1e-7)","808","        Threshold for early stopping in tree growth. A node will split","809","        if its impurity is above the threshold, otherwise it is a leaf.","810","","905","                 min_impurity_split=1e-7,","918","                              \"max_features\", \"max_leaf_nodes\", \"min_impurity_split\",","935","        self.min_impurity_split = min_impurity_split","1009","    min_impurity_split : float, optional (default=1e-7)","1010","        Threshold for early stopping in tree growth. A node will split","1011","        if its impurity is above the threshold, otherwise it is a leaf.","1012","","1076","                 min_impurity_split=1e-7,","1088","                              \"max_features\", \"max_leaf_nodes\", \"min_impurity_split\",","1104","        self.min_impurity_split = min_impurity_split","1174","    min_impurity_split : float, optional (default=1e-7)","1175","        Threshold for early stopping in tree growth. A node will split","1176","        if its impurity is above the threshold, otherwise it is a leaf.","1177","","1273","                 min_impurity_split=1e-7,","1286","                              \"max_features\", \"max_leaf_nodes\", \"min_impurity_split\",","1303","        self.min_impurity_split = min_impurity_split","1375","    min_impurity_split : float, optional (default=1e-7)","1376","        Threshold for early stopping in tree growth. A node will split","1377","        if its impurity is above the threshold, otherwise it is a leaf.","1378","","1443","                 min_impurity_split=1e-7,","1455","                              \"max_features\", \"max_leaf_nodes\", \"min_impurity_split\",","1471","        self.min_impurity_split = min_impurity_split","1526","    min_impurity_split : float, optional (default=1e-7)","1527","        Threshold for early stopping in tree growth. A node will split","1528","        if its impurity is above the threshold, otherwise it is a leaf.","1529","","1574","                 min_impurity_split=1e-7,","1585","                              \"max_features\", \"max_leaf_nodes\", \"min_impurity_split\",","1601","        self.min_impurity_split = min_impurity_split"],"delete":["913","                              \"max_features\", \"max_leaf_nodes\",","1077","                              \"max_features\", \"max_leaf_nodes\",","1269","                              \"max_features\", \"max_leaf_nodes\",","1432","                              \"max_features\", \"max_leaf_nodes\",","1556","                              \"max_features\", \"max_leaf_nodes\","]}],"sklearn\/tree\/tests\/test_tree.py":[{"add":["526","        assert_raises(ValueError, TreeEstimator(min_impurity_split=-1.0).fit, X, y)","684","def test_min_impurity_split():","685","    # test if min_impurity_split creates leaves with impurity","686","    # [0, min_impurity_split) when min_samples_leaf = 1 and","687","    # min_samples_split = 2.","688","    X = np.asfortranarray(iris.data.astype(tree._tree.DTYPE))","689","    y = iris.target","690","","691","    # test both DepthFirstTreeBuilder and BestFirstTreeBuilder","692","    # by setting max_leaf_nodes","693","    for max_leaf_nodes, name in product((None, 1000), ALL_TREES.keys()):","694","        TreeEstimator = ALL_TREES[name]","695","        min_impurity_split = .5","696","","697","        # verify leaf nodes without min_impurity_split less than","698","        # impurity 1e-7","699","        est = TreeEstimator(max_leaf_nodes=max_leaf_nodes,","700","                            random_state=0)","701","        assert_less_equal(est.min_impurity_split, 1e-7,","702","                     \"Failed, min_impurity_split = {0} > 1e-7\".format(","703","                         est.min_impurity_split))","704","        est.fit(X, y)","705","        for node in range(est.tree_.node_count):","706","            if (est.tree_.children_left[node] == TREE_LEAF or","707","                est.tree_.children_right[node] == TREE_LEAF):","708","                assert_equal(est.tree_.impurity[node], 0.,","709","                             \"Failed with {0} \"","710","                             \"min_impurity_split={1}\".format(","711","                                 est.tree_.impurity[node],","712","                                 est.min_impurity_split))","713","","714","        # verify leaf nodes have impurity [0,min_impurity_split] when using min_impurity_split","715","        est = TreeEstimator(max_leaf_nodes=max_leaf_nodes,","716","                            min_impurity_split=min_impurity_split,","717","                            random_state=0)","718","        est.fit(X, y)","719","        for node in range(est.tree_.node_count):","720","            if (est.tree_.children_left[node] == TREE_LEAF or","721","                est.tree_.children_right[node] == TREE_LEAF):","722","                assert_greater_equal(est.tree_.impurity[node], 0,","723","                                     \"Failed with {0}, \"","724","                                     \"min_impurity_split={1}\".format(","725","                                         est.tree_.impurity[node],","726","                                         est.min_impurity_split))","727","                assert_less_equal(est.tree_.impurity[node], min_impurity_split,","728","                                  \"Failed with {0}, \"","729","                                  \"min_impurity_split={1}\".format(","730","                                      est.tree_.impurity[node],","731","                                      est.min_impurity_split))","732","","733",""],"delete":[]}]}},"668b329d421e797d0b5dbea9035c5de986da60a5":{"changes":{"sklearn\/linear_model\/tests\/test_huber.py":"MODIFY","sklearn\/linear_model\/huber.py":"MODIFY"},"diff":{"sklearn\/linear_model\/tests\/test_huber.py":[{"add":["11","from sklearn.utils.testing import assert_false","67","    huber = HuberRegressor(fit_intercept=True)","72","    # Rescale coefs before comparing with assert_array_almost_equal to make sure","73","    # that the number of decimal places used is somewhat insensitive to the","74","    # amplitude of the coefficients and therefore to the scale of the data","75","    # and the regularization parameter","76","    scale = max(np.mean(np.abs(huber.coef_)),","77","                np.mean(np.abs(huber.intercept_)))","78","","80","    assert_array_almost_equal(huber.coef_ \/ scale, huber_coef \/ scale)","81","    assert_array_almost_equal(huber.intercept_ \/ scale,","82","                              huber_intercept \/ scale)","90","    sample_weight = np.ones(X.shape[0])","91","    sample_weight[1] = 3","92","    sample_weight[3] = 2","93","    huber.fit(X, y, sample_weight=sample_weight)","94","","95","    assert_array_almost_equal(huber.coef_ \/ scale, huber_coef \/ scale)","96","    assert_array_almost_equal(huber.intercept_ \/ scale,","97","                              huber_intercept \/ scale)","101","    huber_sparse = HuberRegressor(fit_intercept=True)","102","    huber_sparse.fit(X_csr, y, sample_weight=sample_weight)","103","    assert_array_almost_equal(huber_sparse.coef_ \/ scale,","104","                              huber_coef \/ scale)","116","    assert_array_equal(huber.outliers_, huber_sparse.outliers_)","123","    huber = HuberRegressor(fit_intercept=False, alpha=0.0, max_iter=100)","126","    assert_false(np.all(n_outliers_mask_1))","130","    assert_array_equal(n_outliers_mask_2, n_outliers_mask_1)","140","    X, y = make_regression_with_outliers(n_samples=10, n_features=2)","153","        alpha=0.0, loss=\"huber\", shuffle=True, random_state=0, n_iter=10000,"],"delete":["66","    huber = HuberRegressor(fit_intercept=True, alpha=0.1)","72","    assert_array_almost_equal(huber.coef_, huber_coef)","73","    assert_array_almost_equal(huber.intercept_, huber_intercept)","81","    huber.fit(X, y, sample_weight=[1, 3, 1, 2, 1])","82","    assert_array_almost_equal(huber.coef_, huber_coef, 3)","83","    assert_array_almost_equal(huber.intercept_, huber_intercept, 3)","87","    huber_sparse = HuberRegressor(fit_intercept=True, alpha=0.1)","88","    huber_sparse.fit(X_csr, y, sample_weight=[1, 3, 1, 2, 1])","89","    assert_array_almost_equal(huber_sparse.coef_, huber_coef, 3)","107","    huber = HuberRegressor(fit_intercept=False, alpha=0.0, max_iter=100,","108","                           epsilon=1.35)","117","","118","    assert_array_equal(n_outliers_mask_2, n_outliers_mask_1)","125","    X, y = make_regression_with_outliers(n_samples=5, n_features=2)","138","        alpha=0.0, loss=\"huber\", shuffle=True, random_state=0, n_iter=1000000,"]}],"sklearn\/linear_model\/huber.py":[{"add":["247","            # Make sure to initialize the scale parameter to a strictly","248","            # positive value:","249","            parameters[-1] = 1","255","        bounds[-1][0] = np.finfo(np.float64).eps * 10","270","        if dict_['warnflag'] == 2:","271","            raise ValueError(\"HuberRegressor convergence failed:\"","272","                             \" l-BFGS-b solver terminated with %s\"","273","                             % dict_['task'].decode('ascii'))"],"delete":["252","        bounds[-1][0] = 1e-12","267",""]}]}},"42120e50bb1ff4e3d443cabdf9cd680b876aeec8":{"changes":{"examples\/model_selection\/plot_learning_curve.py":"MODIFY","sklearn\/model_selection\/tests\/test_search.py":"MODIFY","sklearn\/model_selection\/tests\/test_split.py":"MODIFY","doc\/modules\/cross_validation.rst":"MODIFY","doc\/tutorial\/statistical_inference\/model_selection.rst":"MODIFY","examples\/model_selection\/plot_roc_crossval.py":"MODIFY","examples\/mixture\/plot_gmm_covariances.py":"MODIFY","sklearn\/model_selection\/_split.py":"MODIFY","examples\/ensemble\/plot_gradient_boosting_oob.py":"MODIFY","sklearn\/model_selection\/tests\/test_validation.py":"MODIFY","doc\/whats_new.rst":"MODIFY","examples\/svm\/plot_rbf_parameters.py":"MODIFY","examples\/svm\/plot_svm_scale_c.py":"MODIFY"},"diff":{"examples\/model_selection\/plot_learning_curve.py":[{"add":["103","cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)","110","cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)"],"delete":["103","cv = ShuffleSplit(n_iter=100, test_size=0.2, random_state=0)","110","cv = ShuffleSplit(n_iter=10, test_size=0.2, random_state=0)"]}],"sklearn\/model_selection\/tests\/test_search.py":[{"add":["456","    cv = KFold(n_splits=3)","468","    cv = KFold(n_splits=3)","599","    n_splits = 3","603","    grid_search = GridSearchCV(SVC(), cv=n_splits, iid=False,","606","    grid_search_iid = GridSearchCV(SVC(), cv=n_splits, iid=True,","647","    n_splits = 3","650","    random_search = RandomizedSearchCV(SVC(), n_iter=n_search_iter,","651","                                       cv=n_splits,","655","                                           cv=n_splits, iid=True,","782","    n_splits = 3","787","        grid_search = GridSearchCV(clf, {'C': Cs}, scoring=score, cv=n_splits)","794","                               for cv_i in range(n_splits)))","797","        cv = StratifiedKFold(n_splits=n_splits)"],"delete":["456","    cv = KFold(n_folds=3)","468","    cv = KFold(n_folds=3)","599","    n_folds = 3","603","    grid_search = GridSearchCV(SVC(), cv=n_folds, iid=False,","606","    grid_search_iid = GridSearchCV(SVC(), cv=n_folds, iid=True,","647","    n_folds = 3","650","    random_search = RandomizedSearchCV(SVC(), n_iter=n_search_iter, cv=n_folds,","654","                                           cv=n_folds, iid=True,","781","    n_folds = 3","786","        grid_search = GridSearchCV(clf, {'C': Cs}, scoring=score, cv=n_folds)","793","                               for cv_i in range(n_folds)))","796","        cv = StratifiedKFold(n_folds=n_folds)"]}],"sklearn\/model_selection\/tests\/test_split.py":[{"add":["134","    n_splits = 2","136","    n_shuffle_splits = 10  # (the default value)","144","    kf = KFold(n_splits)","145","    skf = StratifiedKFold(n_splits)","153","    kf_repr = \"KFold(n_splits=2, random_state=None, shuffle=False)\"","154","    skf_repr = \"StratifiedKFold(n_splits=2, random_state=None, shuffle=False)\"","157","    ss_repr = (\"ShuffleSplit(n_splits=10, random_state=0, test_size=0.1, \"","161","    n_splits_expected = [n_samples, comb(n_samples, p), n_splits, n_splits,","162","                         n_unique_labels, comb(n_unique_labels, p),","163","                         n_shuffle_splits, 2]","170","        assert_equal(n_splits_expected[i], cv.get_n_splits(X, y, labels))","197","def check_cv_coverage(cv, X, y, labels, expected_n_splits=None):","200","    if expected_n_splits is not None:","201","        assert_equal(cv.get_n_splits(X, y, labels), expected_n_splits)","203","        expected_n_splits = cv.get_n_splits(X, y, labels)","213","    assert_equal(iterations, expected_n_splits)","237","        check_cv_coverage(skf_3, X2, y, labels=None, expected_n_splits=3)","240","    # classes are less than n_splits.","255","    # When n_splits is not integer:","262","    assert_raises(TypeError, KFold, n_splits=4, shuffle=None)","269","    check_cv_coverage(kf, X1, y=None, labels=None, expected_n_splits=3)","275","    check_cv_coverage(kf, X2, y=None, labels=None, expected_n_splits=3)","444","    check_cv_coverage(kf0, X_40, y, labels=None, expected_n_splits=5)","459","    n_splits = 3","461","    cv = KFold(n_splits=n_splits, shuffle=False)","470","    cv = KFold(n_splits, shuffle=True, random_state=0)","474","    cv = KFold(n_splits, shuffle=True, random_state=1)","485","    cv = StratifiedKFold(n_splits)","565","    n_splits = 1000","580","        splits = StratifiedShuffleSplit(n_splits=n_splits,","586","        n_splits_actual = 0","588","            n_splits_actual += 1","592","        assert_equal(n_splits_actual, n_splits)","619","    sss = StratifiedShuffleSplit(n_splits=1,","620","                                 test_size=0.5, random_state=0)","622","    train, test = next(iter(sss.split(X=X, y=y)))","656","        n_splits = 6","658","        slo = LabelShuffleSplit(n_splits, test_size=test_size, random_state=0)","664","        assert_equal(slo.get_n_splits(X, y, labels=l), n_splits)","909","    n_splits = 5","917","    ideal_n_labels_per_fold = n_samples \/\/ n_splits","922","    lkf = LabelKFold(n_splits=n_splits)","952","    n_splits = 5","954","    ideal_n_labels_per_fold = n_samples \/\/ n_splits","983","    assert_raises_regexp(ValueError, \"Cannot have number of splits.*greater\",","984","                         next, LabelKFold(n_splits=3).split(X, y, labels))","995","           StratifiedShuffleSplit(n_splits=3, random_state=0)]"],"delete":["134","    n_folds = 2","136","    n_iter = 10  # (the default value)","144","    kf = KFold(n_folds)","145","    skf = StratifiedKFold(n_folds)","153","    kf_repr = \"KFold(n_folds=2, random_state=None, shuffle=False)\"","154","    skf_repr = \"StratifiedKFold(n_folds=2, random_state=None, shuffle=False)\"","157","    ss_repr = (\"ShuffleSplit(n_iter=10, random_state=0, test_size=0.1, \"","161","    n_splits = [n_samples, comb(n_samples, p), n_folds, n_folds,","162","                n_unique_labels, comb(n_unique_labels, p), n_iter, 2]","169","        assert_equal(n_splits[i], cv.get_n_splits(X, y, labels))","196","def check_cv_coverage(cv, X, y, labels, expected_n_iter=None):","199","    if expected_n_iter is not None:","200","        assert_equal(cv.get_n_splits(X, y, labels), expected_n_iter)","202","        expected_n_iter = cv.get_n_splits(X, y, labels)","212","    assert_equal(iterations, expected_n_iter)","236","        check_cv_coverage(skf_3, X2, y, labels=None, expected_n_iter=3)","239","    # classes are less than n_folds.","254","    # When n_folds is not integer:","261","    assert_raises(TypeError, KFold, n_folds=4, shuffle=None)","268","    check_cv_coverage(kf, X1, y=None, labels=None, expected_n_iter=3)","274","    check_cv_coverage(kf, X2, y=None, labels=None, expected_n_iter=3)","443","    check_cv_coverage(kf0, X_40, y, labels=None, expected_n_iter=5)","458","    n_folds = 3","460","    cv = KFold(n_folds=n_folds, shuffle=False)","469","    cv = KFold(n_folds, shuffle=True, random_state=0)","473","    cv = KFold(n_folds, shuffle=True, random_state=1)","484","    cv = StratifiedKFold(n_folds)","564","    n_iter = 1000","579","        splits = StratifiedShuffleSplit(n_iter=n_iter,","585","        n_splits = 0","587","            n_splits += 1","591","        assert_equal(n_splits, n_iter)","618","    splits = StratifiedShuffleSplit(n_iter=1,","619","                                    test_size=0.5, random_state=0)","621","    train, test = next(iter(splits.split(X=X, y=y)))","655","        n_iter = 6","657","        slo = LabelShuffleSplit(n_iter, test_size=test_size, random_state=0)","663","        assert_equal(slo.get_n_splits(X, y, labels=l), n_iter)","908","    n_folds = 5","916","    ideal_n_labels_per_fold = n_samples \/\/ n_folds","921","    lkf = LabelKFold(n_folds=n_folds)","951","    n_folds = 5","953","    ideal_n_labels_per_fold = n_samples \/\/ n_folds","982","    assert_raises_regexp(ValueError, \"Cannot have number of folds.*greater\",","983","                         next, LabelKFold(n_folds=3).split(X, y, labels))","994","           StratifiedShuffleSplit(n_iter=3, random_state=0)]"]}],"doc\/modules\/cross_validation.rst":[{"add":["139","  >>> cv = ShuffleSplit(n_splits=3, test_size=0.3, random_state=0)","226","  >>> kf = KFold(n_splits=2)","255","  >>> skf = StratifiedKFold(n_splits=3)","280","  >>> lkf = LabelKFold(n_splits=3)","456","  >>> ss = ShuffleSplit(n_splits=3, test_size=0.25,","487","  >>> lss = LabelShuffleSplit(n_splits=4, test_size=0.5, random_state=0)"],"delete":["139","  >>> cv = ShuffleSplit(n_iter=3, test_size=0.3, random_state=0)","226","  >>> kf = KFold(n_folds=2)","255","  >>> skf = StratifiedKFold(n_folds=3)","280","  >>> lkf = LabelKFold(n_folds=3)","456","  >>> ss = ShuffleSplit(n_iter=3, test_size=0.25,","487","  >>> lss = LabelShuffleSplit(n_iter=4, test_size=0.5, random_state=0)"]}],"doc\/tutorial\/statistical_inference\/model_selection.rst":[{"add":["63","    >>> k_fold = KFold(n_splits=3)","72","    >>> kfold = KFold(n_splits=3)","108","    - :class:`KFold` **(n_splits, shuffle, random_state)**","112","    - :class:`LabelKFold` **(n_splits, shuffle, random_state)**"],"delete":["63","    >>> k_fold = KFold(n_folds=3)","72","    >>> kfold = KFold(n_folds=3)","108","    - :class:`KFold` **(n_folds, shuffle, random_state)**","112","    - :class:`LabelKFold` **(n_folds, shuffle, random_state)**"]}],"examples\/model_selection\/plot_roc_crossval.py":[{"add":["60","cv = StratifiedKFold(n_splits=6)"],"delete":["60","cv = StratifiedKFold(n_folds=6)"]}],"examples\/mixture\/plot_gmm_covariances.py":[{"add":["71","skf = StratifiedKFold(n_splits=4)"],"delete":["71","skf = StratifiedKFold(n_folds=4)"]}],"sklearn\/model_selection\/_split.py":[{"add":["124","    Note: ``LeaveOneOut()`` is equivalent to ``KFold(n_splits=n)`` and","199","    ``KFold(n_splits=n_samples \/\/ p)`` which creates non-overlapping test sets.","266","    def __init__(self, n_splits, shuffle, random_state):","267","        if not isinstance(n_splits, numbers.Integral):","270","                             % (n_splits, type(n_splits)))","271","        n_splits = int(n_splits)","273","        if n_splits <= 1:","276","                \" train\/test split by setting n_splits=2 or more,\"","277","                \" got n_splits={0}.\".format(n_splits))","283","        self.n_splits = n_splits","313","        if self.n_splits > n_samples:","315","                (\"Cannot have number of splits n_splits={0} greater\"","316","                 \" than the number of samples: {1}.\").format(self.n_splits,","341","        return self.n_splits","357","    n_splits : int, default=3","372","    >>> kf = KFold(n_splits=2)","376","    KFold(n_splits=2, random_state=None, shuffle=False)","386","    The first ``n_samples % n_splits`` folds have size","387","    ``n_samples \/\/ n_splits + 1``, other folds have size","388","    ``n_samples \/\/ n_splits``, where ``n_samples`` is the number of samples.","400","    def __init__(self, n_splits=3, shuffle=False,","402","        super(KFold, self).__init__(n_splits, shuffle, random_state)","410","        n_splits = self.n_splits","411","        fold_sizes = (n_samples \/\/ n_splits) * np.ones(n_splits, dtype=np.int)","412","        fold_sizes[:n_samples % n_splits] += 1","431","    n_splits : int, default=3","440","    >>> label_kfold = LabelKFold(n_splits=2)","444","    LabelKFold(n_splits=2)","466","    def __init__(self, n_splits=3):","467","        super(LabelKFold, self).__init__(n_splits, shuffle=False,","477","        if self.n_splits > n_labels:","478","            raise ValueError(\"Cannot have number of splits n_splits=%d greater\"","480","                             % (self.n_splits, n_labels))","490","        n_samples_per_fold = np.zeros(self.n_splits)","503","        for f in range(self.n_splits):","520","    n_splits : int, default=3","536","    >>> skf = StratifiedKFold(n_splits=2)","540","    StratifiedKFold(n_splits=2, random_state=None, shuffle=False)","550","    All the folds have size ``trunc(n_samples \/ n_splits)``, the last one has","555","    def __init__(self, n_splits=3, shuffle=False, random_state=None):","556","        super(StratifiedKFold, self).__init__(n_splits, shuffle, random_state)","568","        if np.all(self.n_splits > y_counts):","570","                             \" are less than n_splits=%d.\"","571","                             % (self.n_splits))","572","        if self.n_splits > min_labels:","576","                           \" be less than n_splits=%d.\"","577","                           % (min_labels, self.n_splits)), Warning)","584","        # So we pass np.zeroes(max(c, n_splits)) as data to the KFold","586","            KFold(self.n_splits, shuffle=self.shuffle,","587","                  random_state=rng).split(np.zeros(max(count, self.n_splits)))","595","                # KFold(...).split(X[:max(c, n_splits)]) when data is not 100%","607","        for i in range(self.n_splits):","636","","806","    def __init__(self, n_splits=10, test_size=0.1, train_size=None,","809","        self.n_splits = n_splits","865","        return self.n_splits","884","    n_splits : int (default 10)","907","    >>> rs = ShuffleSplit(n_splits=3, test_size=.25, random_state=0)","911","    ShuffleSplit(n_splits=3, random_state=0, test_size=0.25, train_size=None)","918","    >>> rs = ShuffleSplit(n_splits=3, train_size=0.5, test_size=.25,","933","        for i in range(self.n_splits):","958","    ``LabelShuffleSplit(test_size=10, n_splits=100)``.","966","    n_splits : int (default 5)","985","    def __init__(self, n_splits=5, test_size=0.2, train_size=None,","988","            n_splits=n_splits,","1025","    n_splits : int (default 10)","1048","    >>> sss = StratifiedShuffleSplit(n_splits=3, test_size=0.5, random_state=0)","1052","    StratifiedShuffleSplit(n_splits=3, random_state=0, ...)","1062","    def __init__(self, n_splits=10, test_size=0.1, train_size=None,","1065","            n_splits, test_size, train_size, random_state)","1096","        for _ in range(self.n_splits):"],"delete":["124","    Note: ``LeaveOneOut()`` is equivalent to ``KFold(n_folds=n)`` and","199","    ``KFold(n_folds=n_samples \/\/ p)`` which creates non-overlapping test sets.","266","    def __init__(self, n_folds, shuffle, random_state):","267","        if not isinstance(n_folds, numbers.Integral):","270","                             % (n_folds, type(n_folds)))","271","        n_folds = int(n_folds)","273","        if n_folds <= 1:","276","                \" train\/test split by setting n_folds=2 or more,\"","277","                \" got n_folds={0}.\".format(n_folds))","283","        self.n_folds = n_folds","313","        if self.n_folds > n_samples:","315","                (\"Cannot have number of folds n_folds={0} greater\"","316","                 \" than the number of samples: {1}.\").format(self.n_folds,","341","        return self.n_folds","357","    n_folds : int, default=3","372","    >>> kf = KFold(n_folds=2)","376","    KFold(n_folds=2, random_state=None, shuffle=False)","386","    The first ``n_samples % n_folds`` folds have size","387","    ``n_samples \/\/ n_folds + 1``, other folds have size","388","    ``n_samples \/\/ n_folds``, where ``n_samples`` is the number of samples.","400","    def __init__(self, n_folds=3, shuffle=False,","402","        super(KFold, self).__init__(n_folds, shuffle, random_state)","410","        n_folds = self.n_folds","411","        fold_sizes = (n_samples \/\/ n_folds) * np.ones(n_folds, dtype=np.int)","412","        fold_sizes[:n_samples % n_folds] += 1","431","    n_folds : int, default=3","440","    >>> label_kfold = LabelKFold(n_folds=2)","444","    LabelKFold(n_folds=2)","466","    def __init__(self, n_folds=3):","467","        super(LabelKFold, self).__init__(n_folds, shuffle=False,","477","        if self.n_folds > n_labels:","478","            raise ValueError(\"Cannot have number of folds n_folds=%d greater\"","480","                             % (self.n_folds, n_labels))","490","        n_samples_per_fold = np.zeros(self.n_folds)","503","        for f in range(self.n_folds):","520","    n_folds : int, default=3","536","    >>> skf = StratifiedKFold(n_folds=2)","540","    StratifiedKFold(n_folds=2, random_state=None, shuffle=False)","550","    All the folds have size ``trunc(n_samples \/ n_folds)``, the last one has","555","    def __init__(self, n_folds=3, shuffle=False, random_state=None):","556","        super(StratifiedKFold, self).__init__(n_folds, shuffle, random_state)","568","        if np.all(self.n_folds > y_counts):","570","                             \" are less than %d folds.\"","571","                             % (self.n_folds))","572","        if self.n_folds > min_labels:","576","                           \" be less than n_folds=%d.\"","577","                           % (min_labels, self.n_folds)), Warning)","584","        # So we pass np.zeroes(max(c, n_folds)) as data to the KFold","586","            KFold(self.n_folds, shuffle=self.shuffle,","587","                  random_state=rng).split(np.zeros(max(count, self.n_folds)))","595","                # KFold(...).split(X[:max(c, n_folds)]) when data is not 100%","607","        for i in range(self.n_folds):","805","    def __init__(self, n_iter=10, test_size=0.1, train_size=None,","808","        self.n_iter = n_iter","864","        return self.n_iter","883","    n_iter : int (default 10)","906","    >>> rs = ShuffleSplit(n_iter=3, test_size=.25, random_state=0)","910","    ShuffleSplit(n_iter=3, random_state=0, test_size=0.25, train_size=None)","917","    >>> rs = ShuffleSplit(n_iter=3, train_size=0.5, test_size=.25,","932","        for i in range(self.n_iter):","957","    ``LabelShuffleSplit(test_size=10, n_iter=100)``.","965","    n_iter : int (default 5)","984","    def __init__(self, n_iter=5, test_size=0.2, train_size=None,","987","            n_iter=n_iter,","1024","    n_iter : int (default 10)","1047","    >>> sss = StratifiedShuffleSplit(n_iter=3, test_size=0.5, random_state=0)","1051","    StratifiedShuffleSplit(n_iter=3, random_state=0, ...)","1061","    def __init__(self, n_iter=10, test_size=0.1, train_size=None,","1064","            n_iter, test_size, train_size, random_state)","1095","        for _ in range(self.n_iter):"]}],"examples\/ensemble\/plot_gradient_boosting_oob.py":[{"add":["76","def cv_estimate(n_splits=3):","77","    cv = KFold(n_splits=n_splits)","83","    val_scores \/= n_splits"],"delete":["76","def cv_estimate(n_folds=3):","77","    cv = KFold(n_folds=n_folds)","83","    val_scores \/= n_folds"]}],"sklearn\/model_selection\/tests\/test_validation.py":[{"add":["138","# The number of samples per class needs to be > n_splits,","139","# for StratifiedKFold(n_splits=3)","704","    cv = KFold(n_splits=3)"],"delete":["138","# The number of samples per class needs to be > n_folds, for StratifiedKFold(3)","703","    cv = KFold(n_folds=3)"]}],"doc\/whats_new.rst":[{"add":["64","  - **Parameters ``n_folds`` and ``n_iter`` renamed to ``n_splits``**","65","","66","    Some parameter names have changed: ","67","    The ``n_folds`` parameter in :class:`model_selection.KFold`, ","68","    :class:`model_selection.LabelKFold`, and ","69","    :class:`model_selection.StratifiedKFold` is now renamed to ``n_splits``.","70","    The ``n_iter`` parameter in :class:`model_selection.ShuffleSplit`,","71","    :class:`model_selection.LabelShuffleSplit`, ","72","    and :class:`model_selection.StratifiedShuffleSplit` is now renamed ","73","    to ``n_splits``.","74","","373","   - The parameters ``n_iter`` or ``n_folds`` in old CV splitters are replaced","374","     by the new parameter ``n_splits`` since it can provide a consistent ","375","     and unambiguous interface to represent the number of train-test splits.","376","     (`#7187 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/7187>`_)","377","     by `YenChen Lin`_.","378",""],"delete":[]}],"examples\/svm\/plot_rbf_parameters.py":[{"add":["61","smoothed out by increasing the number of CV iterations ``n_splits`` at the","130","cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)"],"delete":["61","smoothed out by increasing the number of CV iterations ``n_iter`` at the","130","cv = StratifiedShuffleSplit(n_iter=5, test_size=0.2, random_state=42)"]}],"examples\/svm\/plot_svm_scale_c.py":[{"add":["130","                            cv=ShuffleSplit(train_size=train_size,","131","                                            n_splits=250, random_state=1))"],"delete":["130","                            cv=ShuffleSplit(train_size=train_size, n_iter=250,","131","                                            random_state=1))"]}]}},"1bcd6c3c5530e721860cae5fc410551b8ca744b0":{"changes":{"sklearn\/multioutput.py":"MODIFY"},"diff":{"sklearn\/multioutput.py":[{"add":["62","","65","        self : object","66","            Returns self."],"delete":["64","        self"]}]}},"78dbcb2838bb863dcf2e930c29aeef4f4cde5949":{"changes":{"sklearn\/ensemble\/_gradient_boosting.pyx":"MODIFY","sklearn\/ensemble\/gradient_boosting.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/tree\/_tree.pyx":"MODIFY","sklearn\/tree\/_utils.pxd":"MODIFY","sklearn\/ensemble\/tests\/test_gradient_boosting.py":"MODIFY"},"diff":{"sklearn\/ensemble\/_gradient_boosting.pyx":[{"add":["10","from libc.stdlib cimport free","11","from libc.string cimport memset","12","","17","from scipy.sparse import issparse","18","from scipy.sparse import csr_matrix","19","","22","from sklearn.tree._tree cimport DTYPE_t","23","from sklearn.tree._tree cimport SIZE_t","24","from sklearn.tree._tree cimport INT32_t","25","from sklearn.tree._utils cimport safe_realloc","40","cdef SIZE_t TREE_LEAF = -1","42","cdef void _predict_regression_tree_inplace_fast_dense(DTYPE_t *X,","43","                                                      Node* root_node,","44","                                                      double *value,","45","                                                      double scale,","46","                                                      Py_ssize_t k,","47","                                                      Py_ssize_t K,","48","                                                      Py_ssize_t n_samples,","49","                                                      Py_ssize_t n_features,","50","                                                      float64 *out):","94","        while node.left_child != TREE_LEAF:","101","def _predict_regression_tree_stages_sparse(np.ndarray[object, ndim=2] estimators,","102","                                           object X, double scale,","103","                                           np.ndarray[float64, ndim=2] out):","104","    \"\"\"Predicts output for regression tree inplace and adds scaled value to ``out[i, k]``.","106","    The function assumes that the ndarray that wraps ``X`` is csr_matrix.","107","    \"\"\"","108","    cdef DTYPE_t* X_data = <DTYPE_t*>(<np.ndarray> X.data).data","109","    cdef INT32_t* X_indices = <INT32_t*>(<np.ndarray> X.indices).data","110","    cdef INT32_t* X_indptr = <INT32_t*>(<np.ndarray> X.indptr).data","111","","112","    cdef SIZE_t n_samples = X.shape[0]","113","    cdef SIZE_t n_features = X.shape[1]","114","    cdef SIZE_t n_stages = estimators.shape[0]","115","    cdef SIZE_t n_outputs = estimators.shape[1]","116","","117","    # Initialize output","118","    cdef float64* out_ptr = <float64*> out.data","119","","120","    # Indices and temporary variables","121","    cdef SIZE_t sample_i","122","    cdef SIZE_t feature_i","123","    cdef SIZE_t stage_i","124","    cdef SIZE_t output_i","125","    cdef Node *root_node = NULL","126","    cdef Node *node = NULL","127","    cdef double *value = NULL","128","","129","    cdef Tree tree","130","    cdef Node** nodes = NULL","131","    cdef double** values = NULL","132","    safe_realloc(&nodes, n_stages * n_outputs)","133","    safe_realloc(&values, n_stages * n_outputs)","134","    for stage_i in range(n_stages):","135","        for output_i in range(n_outputs):","136","            tree = estimators[stage_i, output_i].tree_","137","            nodes[stage_i * n_outputs + output_i] = tree.nodes","138","            values[stage_i * n_outputs + output_i] = tree.value","139","","140","    # Initialize auxiliary data-structure","141","    cdef DTYPE_t feature_value = 0.","142","    cdef DTYPE_t* X_sample = NULL","143","","144","    # feature_to_sample as a data structure records the last seen sample","145","    # for each feature; functionally, it is an efficient way to identify","146","    # which features are nonzero in the present sample.","147","    cdef SIZE_t* feature_to_sample = NULL","148","","149","    safe_realloc(&X_sample, n_features)","150","    safe_realloc(&feature_to_sample, n_features)","151","","152","    memset(feature_to_sample, -1, n_features * sizeof(SIZE_t))","153","","154","    # Cycle through all samples","155","    for sample_i in range(n_samples):","156","        for feature_i in range(X_indptr[sample_i], X_indptr[sample_i + 1]):","157","            feature_to_sample[X_indices[feature_i]] = sample_i","158","            X_sample[X_indices[feature_i]] = X_data[feature_i]","159","","160","        # Cycle through all stages","161","        for stage_i in range(n_stages):","162","            # Cycle through all trees","163","            for output_i in range(n_outputs):","164","                root_node = nodes[stage_i * n_outputs + output_i]","165","                value = values[stage_i * n_outputs + output_i]","166","                node = root_node","167","","168","                # While node not a leaf","169","                while node.left_child != TREE_LEAF:","170","                    # ... and node.right_child != TREE_LEAF:","171","                    if feature_to_sample[node.feature] == sample_i:","172","                        feature_value = X_sample[node.feature]","173","                    else:","174","                        feature_value = 0.","175","","176","                    if feature_value <= node.threshold:","177","                        node = root_node + node.left_child","178","                    else:","179","                        node = root_node + node.right_child","180","                out_ptr[sample_i * n_outputs + output_i] += (scale","181","                    * value[node - root_node])","182","","183","    # Free auxiliary arrays","184","    free(X_sample)","185","    free(feature_to_sample)","186","    free(nodes)","187","    free(values)","188","","189","","191","                   object X, double scale,","204","    if issparse(X):","205","        _predict_regression_tree_stages_sparse(estimators, X, scale, out)","206","    else:","207","        if not isinstance(X, np.ndarray):","208","            raise ValueError(\"X should be in np.ndarray or csr_matrix format,\"","209","                             \"got %s\" % type(X))","211","        for i in range(n_estimators):","212","            for k in range(K):","213","                tree = estimators[i, k].tree_","214","","215","                # avoid buffer validation by casting to ndarray","216","                # and get data pointer","217","                # need brackets because of casting operator priority","218","                _predict_regression_tree_inplace_fast_dense(","219","                    <DTYPE_t*> (<np.ndarray> X).data,","220","                    tree.nodes, tree.value,","221","                    scale, k, K, X.shape[0], X.shape[1],","222","                    <float64 *> (<np.ndarray> out).data)","223","                ## out += scale * tree.predict(X).reshape((X.shape[0], 1))","228","                  object X, double scale,","314","            if current_node.left_child == TREE_LEAF:"],"delete":["16","","29","# Define a datatype for the data array","30","DTYPE = np.float32","31","ctypedef np.float32_t DTYPE_t","32","ctypedef np.npy_intp SIZE_t","33","","36","cdef int LEAF = -1","38","cdef void _predict_regression_tree_inplace_fast(DTYPE_t *X,","39","                                                Node* root_node,","40","                                                double *value,","41","                                                double scale,","42","                                                Py_ssize_t k,","43","                                                Py_ssize_t K,","44","                                                Py_ssize_t n_samples,","45","                                                Py_ssize_t n_features,","46","                                                float64 *out):","86","    cdef int32 node_id","91","        while node.left_child != -1 and node.right_child != -1:","99","@cython.nonecheck(False)","101","                   np.ndarray[DTYPE_t, ndim=2, mode='c'] X, double scale,","114","    for i in range(n_estimators):","115","        for k in range(K):","116","            tree = estimators[i, k].tree_","118","            # avoid buffer validation by casting to ndarray","119","            # and get data pointer","120","            # need brackets because of casting operator priority","121","            _predict_regression_tree_inplace_fast(","122","                <DTYPE_t*> X.data,","123","                tree.nodes, tree.value,","124","                scale, k, K, X.shape[0], X.shape[1],","125","                <float64 *> (<np.ndarray> out).data)","126","            ## out += scale * tree.predict(X).reshape((X.shape[0], 1))","129","@cython.nonecheck(False)","132","                  np.ndarray[DTYPE_t, ndim=2] X, double scale,","218","            if current_node.left_child == LEAF:"]}],"sklearn\/ensemble\/gradient_boosting.py":[{"add":["1160","        X : array-like or sparse matrix, shape = [n_samples, n_features]","1161","            The input samples. Internally, it will be converted to","1162","            ``dtype=np.float32`` and if a sparse matrix is provided","1163","            to a sparse ``csr_matrix``.","1173","        X = check_array(X, dtype=DTYPE, order=\"C\",  accept_sparse='csr')","1483","        X : array-like or sparse matrix, shape = [n_samples, n_features]","1484","            The input samples. Internally, it will be converted to","1485","            ``dtype=np.float32`` and if a sparse matrix is provided","1486","            to a sparse ``csr_matrix``.","1496","        X = check_array(X, dtype=DTYPE, order=\"C\",  accept_sparse='csr')","1510","        X : array-like or sparse matrix, shape = [n_samples, n_features]","1511","            The input samples. Internally, it will be converted to","1512","            ``dtype=np.float32`` and if a sparse matrix is provided","1513","            to a sparse ``csr_matrix``.","1532","        X : array-like or sparse matrix, shape = [n_samples, n_features]","1533","            The input samples. Internally, it will be converted to","1534","            ``dtype=np.float32`` and if a sparse matrix is provided","1535","            to a sparse ``csr_matrix``.","1554","        X : array-like or sparse matrix, shape = [n_samples, n_features]","1555","            The input samples. Internally, it will be converted to","1556","            ``dtype=np.float32`` and if a sparse matrix is provided","1557","            to a sparse ``csr_matrix``.","1573","        X : array-like or sparse matrix, shape = [n_samples, n_features]","1574","            The input samples. Internally, it will be converted to","1575","            ``dtype=np.float32`` and if a sparse matrix is provided","1576","            to a sparse ``csr_matrix``.","1603","        X : array-like or sparse matrix, shape = [n_samples, n_features]","1604","            The input samples. Internally, it will be converted to","1605","            ``dtype=np.float32`` and if a sparse matrix is provided","1606","            to a sparse ``csr_matrix``.","1630","        X : array-like or sparse matrix, shape = [n_samples, n_features]","1631","            The input samples. Internally, it will be converted to","1632","            ``dtype=np.float32`` and if a sparse matrix is provided","1633","            to a sparse ``csr_matrix``.","1858","        X : array-like or sparse matrix, shape = [n_samples, n_features]","1859","            The input samples. Internally, it will be converted to","1860","            ``dtype=np.float32`` and if a sparse matrix is provided","1861","            to a sparse ``csr_matrix``.","1868","        X = check_array(X, dtype=DTYPE, order=\"C\",  accept_sparse='csr')","1879","        X : array-like or sparse matrix, shape = [n_samples, n_features]","1880","            The input samples. Internally, it will be converted to","1881","            ``dtype=np.float32`` and if a sparse matrix is provided","1882","            to a sparse ``csr_matrix``."],"delete":["1160","        X : array-like of shape = [n_samples, n_features]","1161","            The input samples.","1171","        X = check_array(X, dtype=DTYPE, order=\"C\")","1481","        X : array-like of shape = [n_samples, n_features]","1482","            The input samples.","1492","        X = check_array(X, dtype=DTYPE, order=\"C\")","1506","        X : array-like of shape = [n_samples, n_features]","1507","            The input samples.","1526","        X : array-like of shape = [n_samples, n_features]","1527","            The input samples.","1546","        X : array-like of shape = [n_samples, n_features]","1547","            The input samples.","1563","        X : array-like of shape = [n_samples, n_features]","1564","            The input samples.","1591","        X : array-like of shape = [n_samples, n_features]","1592","            The input samples.","1616","        X : array-like of shape = [n_samples, n_features]","1617","            The input samples.","1842","        X : array-like of shape = [n_samples, n_features]","1843","            The input samples.","1850","        X = check_array(X, dtype=DTYPE, order=\"C\")","1861","        X : array-like of shape = [n_samples, n_features]","1862","            The input samples."]}],"doc\/whats_new.rst":[{"add":["38","   - :class:`ensemble.GradientBoostingClassifier` and :class:`ensemble.GradientBoostingRegressor`","39","     now support sparse input for prediction.","40","     (`#6101 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/6101>`_)","41","     By `Ibraim Ganiev`_.","42",""],"delete":[]}],"sklearn\/tree\/_tree.pyx":[{"add":["841","        safe_realloc(&X_sample, n_features)","842","        safe_realloc(&feature_to_sample, n_features)","987","        safe_realloc(&X_sample, n_features)","988","        safe_realloc(&feature_to_sample, n_features)"],"delete":["841","        safe_realloc(&X_sample, n_features * sizeof(DTYPE_t))","842","        safe_realloc(&feature_to_sample, n_features * sizeof(SIZE_t))","987","        safe_realloc(&X_sample, n_features * sizeof(DTYPE_t))","988","        safe_realloc(&feature_to_sample, n_features * sizeof(SIZE_t))"]}],"sklearn\/tree\/_utils.pxd":[{"add":["12","from _tree cimport Node ","39","    (DOUBLE_t**)","40","    (Node*)","41","    (Node**)"],"delete":[]}],"sklearn\/ensemble\/tests\/test_gradient_boosting.py":[{"add":["1052","    assert_array_almost_equal(sparse.predict(X_sparse), dense.predict(X))","1053","    assert_array_almost_equal(dense.predict(X_sparse), sparse.predict(X))","1054","","1066","        assert_array_almost_equal(sparse.decision_function(X_sparse),","1067","                                  sparse.decision_function(X))","1068","        assert_array_almost_equal(dense.decision_function(X_sparse),","1069","                                  sparse.decision_function(X))","1070","","1071","        assert_array_almost_equal(","1072","            np.array(sparse.staged_decision_function(X_sparse)),","1073","            np.array(sparse.staged_decision_function(X)))"],"delete":[]}]}},"1002de22d950934c20e434baec866469dc6c204f":{"changes":{"sklearn\/calibration.py":"MODIFY"},"diff":{"sklearn\/calibration.py":[{"add":["35","    it is assumed that base_estimator has been fitted already and all","36","    data is used for calibration. Note that data for fitting the","37","    classifier and for calibrating it must be disjoint.","128","        # Check that each cross-validation fold can have at least one"],"delete":["35","    it is it is assumed that base_estimator has been","36","    fitted already and all data is used for calibration. Note that","37","    data for fitting the classifier and for calibrating it must be disjoint.","128","        # Check that we each cross-validation fold can have at least one"]}]}},"928f72447eedbc16b927f40a87a67cac03fd4974":{"changes":{"sklearn\/datasets\/_svmlight_format.pyx":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/datasets\/svmlight_format.py":"MODIFY","sklearn\/datasets\/tests\/test_svmlight_format.py":"MODIFY"},"diff":{"sklearn\/datasets\/_svmlight_format.pyx":[{"add":["29","    cdef array.array data, indices, indptr","47","    query = np.arange(0, dtype=np.int64)","82","                query.resize(len(query) + 1)","83","                query[len(query) - 1] = np.int64(value)"],"delete":["29","    cdef array.array data, indices, indptr, query","47","    query = array.array(\"i\")","82","                array.resize_smart(query, len(query) + 1)","83","                query[len(query) - 1] = int(value)"]}],"doc\/whats_new.rst":[{"add":["314","    - :func:`datasets.load_svmlight_file` now is able to read long int QID values.","315","      (`#7101 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/7101>`_)","316","      By `Ibraim Ganiev`_.","317","","4318","","4319",".. _Ibraim Ganiev: https:\/\/github.com\/olologin"],"delete":[]}],"sklearn\/datasets\/svmlight_format.py":[{"add":["168","    query = frombuffer_empty(query, np.int64)"],"delete":["168","    query = frombuffer_empty(query, np.intc)"]}],"sklearn\/datasets\/tests\/test_svmlight_format.py":[{"add":["370","","371","","372","def test_load_with_long_qid():","373","    # load svmfile with longint qid attribute","374","    data = b(\"\"\"","375","    1 qid:0 0:1 1:2 2:3","376","    0 qid:72048431380967004 0:1440446648 1:72048431380967004 2:236784985","377","    0 qid:-9223372036854775807 0:1440446648 1:72048431380967004 2:236784985","378","    3 qid:9223372036854775807  0:1440446648 1:72048431380967004 2:236784985\"\"\")","379","    X, y, qid = load_svmlight_file(BytesIO(data), query_id=True)","380","","381","    true_X = [[1,          2,                 3],","382","             [1440446648, 72048431380967004, 236784985],","383","             [1440446648, 72048431380967004, 236784985],","384","             [1440446648, 72048431380967004, 236784985]]","385","","386","    true_y = [1, 0, 0, 3]","387","    trueQID = [0, 72048431380967004, -9223372036854775807, 9223372036854775807]","388","    assert_array_equal(y, true_y)","389","    assert_array_equal(X.toarray(), true_X)","390","    assert_array_equal(qid, trueQID)","391","","392","    f = BytesIO()","393","    dump_svmlight_file(X, y, f, query_id=qid, zero_based=True)","394","    f.seek(0)","395","    X, y, qid = load_svmlight_file(f, query_id=True, zero_based=True)","396","    assert_array_equal(y, true_y)","397","    assert_array_equal(X.toarray(), true_X)","398","    assert_array_equal(qid, trueQID)","399","","400","    f.seek(0)","401","    X, y = load_svmlight_file(f, query_id=False, zero_based=True)","402","    assert_array_equal(y, true_y)","403","    assert_array_equal(X.toarray(), true_X)"],"delete":[]}]}},"89247fcd7f02bbd92c9da141250ec8b82caf8dd0":{"changes":{"sklearn\/linear_model\/sgd_fast.pyx":"MODIFY"},"diff":{"sklearn\/linear_model\/sgd_fast.pyx":[{"add":["169","cdef class SquaredHinge(Classification):"],"delete":["169","cdef class SquaredHinge(LossFunction):"]}]}},"8994d0ef61f66228fadb142bba2a809178b81b39":{"changes":{"sklearn\/datasets\/base.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/datasets\/tests\/test_base.py":"MODIFY"},"diff":{"sklearn\/datasets\/base.py":[{"add":["266","        .. versionadded:: 0.18","279","        .. versionadded:: 0.18","340","        .. versionadded:: 0.18","353","        .. versionadded:: 0.18","413","def load_digits(n_class=10, return_X_y=False):","433","    return_X_y : boolean, default=False.","434","        If True, returns ``(data, target)`` instead of a Bunch object.","435","        See below for more information about the `data` and `target` object.","436","","437","        .. versionadded:: 0.18","438","","448","    (data, target) : tuple if ``return_X_y`` is True","449","","450","        .. versionadded:: 0.18","451","","470","    target = data[:, -1].astype(np.int)","480","    if return_X_y:","481","        return flat_data, target","482","","484","                 target=target,","490","def load_diabetes(return_X_y=False):","502","    Parameters","503","    ----------","504","    return_X_y : boolean, default=False.","505","        If True, returns ``(data, target)`` instead of a Bunch object.","506","        See below for more information about the `data` and `target` object.","507","","508","        .. versionadded:: 0.18","509","","516","","517","    (data, target) : tuple if ``return_X_y`` is True","518","","519","        .. versionadded:: 0.18    ","524","    ","525","    if return_X_y:","526","        return data, target","527","","531","def load_linnerud(return_X_y=False):","539","    Parameters","540","    ----------","541","    return_X_y : boolean, default=False.","542","        If True, returns ``(data, target)`` instead of a Bunch object.","543","        See below for more information about the `data` and `target` object.","544","","545","        .. versionadded:: 0.18","546","","554","    ","555","    (data, target) : tuple if ``return_X_y`` is True","556","","557","        .. versionadded:: 0.18","572","    if return_X_y:","573","        return data_exercise, data_physiological","574","","581","def load_boston(return_X_y=False):","591","    Parameters","592","    ----------","593","    return_X_y : boolean, default=False.","594","        If True, returns ``(data, target)`` instead of a Bunch object.","595","        See below for more information about the `data` and `target` object.","596","","597","        .. versionadded:: 0.18","598","","606","    (data, target) : tuple if ``return_X_y`` is True","607","","608","        .. versionadded:: 0.18    ","609","","638","    if return_X_y:","639","        return data, target","640",""],"delete":["266","    .. versionadded:: 0.18","279","    .. versionadded:: 0.18","340","    .. versionadded:: 0.18","353","    .. versionadded:: 0.18","413","def load_digits(n_class=10):","460","    target = data[:, -1]","471","                 target=target.astype(np.int),","477","def load_diabetes():","502","def load_linnerud():","537","def load_boston():"]}],"doc\/whats_new.rst":[{"add":["232","   - Added parameter ``return_X_y`` and return type ``(data, target) : tuple`` option to","233","     :func:`load_iris` dataset ","234","     `#7049 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/7049>`_, ","236","     `#7152 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/7152>`_,","237","     :func:`load_digits` dataset,","238","     :func:`load_diabetes` dataset,","239","     :func:`load_linnerud` dataset,","240","     :func:`load_boston` dataset","241","     `#7154 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/7154>`_ by"],"delete":["232","   - Added new return type ``(data, target)`` : tuple option to","233","     :func:`load_iris` dataset, ","234","     (`#7049 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/7049>`_)","236","     (`#7152 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/7152>`_) by"]}],"sklearn\/datasets\/tests\/test_base.py":[{"add":["130","    # test return_X_y option","131","    X_y_tuple = load_digits(return_X_y=True)","132","    bunch = load_digits()","133","    assert_true(isinstance(X_y_tuple, tuple))","134","    assert_array_equal(X_y_tuple[0], bunch.data)","135","    assert_array_equal(X_y_tuple[1], bunch.target)","136","","174","    # test return_X_y option","175","    X_y_tuple = load_diabetes(return_X_y=True)","176","    bunch = load_diabetes()","177","    assert_true(isinstance(X_y_tuple, tuple))","178","    assert_array_equal(X_y_tuple[0], bunch.data)","179","    assert_array_equal(X_y_tuple[1], bunch.target)","180","","189","    # test return_X_y option","190","    X_y_tuple = load_linnerud(return_X_y=True)","191","    bunch = load_linnerud()","192","    assert_true(isinstance(X_y_tuple, tuple))","193","    assert_array_equal(X_y_tuple[0], bunch.data)","194","    assert_array_equal(X_y_tuple[1], bunch.target)","233","    # test return_X_y option","234","    X_y_tuple = load_boston(return_X_y=True)","235","    bunch = load_boston()","236","    assert_true(isinstance(X_y_tuple, tuple))","237","    assert_array_equal(X_y_tuple[0], bunch.data)","238","    assert_array_equal(X_y_tuple[1], bunch.target)"],"delete":[]}]}},"71617558dea900489926d88d80a55f952e6a9e07":{"changes":{"doc\/testimonials\/images\/ottogroup_logo.png":"ADD","doc\/testimonials\/testimonials.rst":"MODIFY"},"diff":{"doc\/testimonials\/images\/ottogroup_logo.png":[{"add":[],"delete":[]}],"doc\/testimonials\/testimonials.rst":[{"add":["800","`Otto Group <https:\/\/ottogroup.com\/>`_","801","-----------------------------------------","802","","803",".. raw:: html","804","","805","   <div class=\"logo\">","806","","807",".. image:: images\/ottogroup_logo.png","808","    :width: 120pt","809","    :target: https:\/\/ottogroup.com","810","","811",".. raw:: html","812","","813","   <\/div>","814","","815","Here at Otto Group, one of global Big Five B2C online retailers, we are using","816","scikit-learn in all aspects of our daily work from data exploration to development","817","of machine learning application to the productive deployment of those services.","818","It helps us to tackle machine learning problems ranging from e-commerce to logistics.","819","It consistent APIs enabled us to build the `Palladium REST-API framework","820","<https:\/\/github.com\/ottogroup\/palladium\/>`_ around it and continuously deliver","821","scikit-learn based services.","822","","823","","824",".. raw:: html","825","","826","  <span class=\"testimonial-author\">","827","","828","Christian Rammig, Head of Data Science, Otto Group","829","","830",".. raw:: html","831","","832","  <\/span>","833",""],"delete":[]}]}},"ed7af6632c50e8c0637b07095de21a2a1ade2b9a":{"changes":{"doc\/developers\/advanced_installation.rst":"MODIFY"},"diff":{"doc\/developers\/advanced_installation.rst":[{"add":["141","download the source package from","142","`pypi <https:\/\/pypi.python.org\/pypi\/scikit-learn>`_, unpack the sources and","143","cd into the source directory.","145","This packages uses distutils, which is the default way of installing","185","Third party distributions of scikit-learn"],"delete":["141","download the source package from ","142","`pypi <https:\/\/pypi.python.org\/pypi\/scikit-learn>`_,","143",", unpack the sources and cd into the source directory.","145","this packages uses distutils, which is the default way of installing","185","third party distributions of scikit-learn"]}]}},"acf8368ca3b426e504c00d1df3c2d51b7fa89c65":{"changes":{"doc\/faq.rst":"MODIFY"},"diff":{"doc\/faq.rst":[{"add":["252","Why is there no support for deep or reinforcement learning \/ Will there be support for deep or reinforcement learning in scikit-learn?","253","--------------------------------------------------------------------------------------------------------------------------------------","254","Deep learning and reinforcement learning both require a rich vocabulary to","255","define an architecture, with deep learning additionally requiring","256","GPUs for efficient computing. However, neither of these fit within","257","the design constraints of scikit-learn; as a result, deep learning","258","and reinforcement learning are currently out of scope for what","259","scikit-learn seeks to achieve."],"delete":["252","Why is there no support for deep learning \/ Will there be support for deep learning in scikit-learn?","253","----------------------------------------------------------------------------------------------------","254","Deep learning requires a rich vocabulary to define an architecture and the","255","use of GPUs for efficient computing. However, neither of these fit within","256","the design constraints of scikit-learn. As a result, deep learning is","257","currently out of scope for what scikit-learn seeks to achieve."]}]}},"5c6409c9e94cf9a1d9f78a53d613fb5904a42cd0":{"changes":{"sklearn\/manifold\/tests\/test_locally_linear.py":"MODIFY","sklearn\/manifold\/tests\/test_t_sne.py":"MODIFY"},"diff":{"sklearn\/manifold\/tests\/test_locally_linear.py":[{"add":["16","# ----------------------------------------------------------------------","35","# ----------------------------------------------------------------------"],"delete":["16","#----------------------------------------------------------------------","35","#----------------------------------------------------------------------"]}],"sklearn\/manifold\/tests\/test_t_sne.py":[{"add":["198","","199","    def fun(params):","200","        return _kl_divergence(params, P, alpha, n_samples, n_components)[0]","201","","202","    def grad(params):","203","        return _kl_divergence(params, P, alpha, n_samples, n_components)[1]","204",""],"delete":["198","    fun = lambda params: _kl_divergence(params, P, alpha, n_samples,","199","                                        n_components)[0]","200","    grad = lambda params: _kl_divergence(params, P, alpha, n_samples,","201","                                         n_components)[1]"]}]}},"f95e5b1a0d2139a94393954675d4a84920653176":{"changes":{"sklearn\/tree\/_utils.pyx":"MODIFY","sklearn\/tree\/_criterion.pyx":"MODIFY","sklearn\/tree\/_criterion.pxd":"MODIFY","sklearn\/tree\/_utils.pxd":"MODIFY"},"diff":{"sklearn\/tree\/_utils.pyx":[{"add":["208","    cdef void heapify_up(self, PriorityHeapRecord* heap, SIZE_t pos) nogil:","209","        \"\"\"Restore heap invariant parent.improvement > child.improvement from","210","           ``pos`` upwards. \"\"\"","211","        if pos == 0:","212","            return","213","","214","        cdef SIZE_t parent_pos = (pos - 1) \/ 2","215","","216","        if heap[parent_pos].improvement < heap[pos].improvement:","217","            heap[parent_pos], heap[pos] = heap[pos], heap[parent_pos]","218","            self.heapify_up(heap, parent_pos)","219","","220","    cdef void heapify_down(self, PriorityHeapRecord* heap, SIZE_t pos,","221","                           SIZE_t heap_length) nogil:","222","        \"\"\"Restore heap invariant parent.improvement > children.improvement from","223","           ``pos`` downwards. \"\"\"","224","        cdef SIZE_t left_pos = 2 * (pos + 1) - 1","225","        cdef SIZE_t right_pos = 2 * (pos + 1)","226","        cdef SIZE_t largest = pos","227","","228","        if (left_pos < heap_length and","229","                heap[left_pos].improvement > heap[largest].improvement):","230","            largest = left_pos","231","","232","        if (right_pos < heap_length and","233","                heap[right_pos].improvement > heap[largest].improvement):","234","            largest = right_pos","235","","236","        if largest != pos:","237","            heap[pos], heap[largest] = heap[largest], heap[pos]","238","            self.heapify_down(heap, largest, heap_length)","239","","276","        self.heapify_up(heap, heap_ptr)","298","            self.heapify_down(heap, 0, heap_ptr - 1)"],"delete":["175","cdef void heapify_up(PriorityHeapRecord* heap, SIZE_t pos) nogil:","176","    \"\"\"Restore heap invariant parent.improvement > child.improvement from","177","       ``pos`` upwards. \"\"\"","178","    if pos == 0:","179","        return","180","","181","    cdef SIZE_t parent_pos = (pos - 1) \/ 2","182","","183","    if heap[parent_pos].improvement < heap[pos].improvement:","184","        heap[parent_pos], heap[pos] = heap[pos], heap[parent_pos]","185","        heapify_up(heap, parent_pos)","186","","187","","188","cdef void heapify_down(PriorityHeapRecord* heap, SIZE_t pos,","189","                       SIZE_t heap_length) nogil:","190","    \"\"\"Restore heap invariant parent.improvement > children.improvement from","191","       ``pos`` downwards. \"\"\"","192","    cdef SIZE_t left_pos = 2 * (pos + 1) - 1","193","    cdef SIZE_t right_pos = 2 * (pos + 1)","194","    cdef SIZE_t largest = pos","195","","196","    if (left_pos < heap_length and","197","            heap[left_pos].improvement > heap[largest].improvement):","198","        largest = left_pos","199","","200","    if (right_pos < heap_length and","201","            heap[right_pos].improvement > heap[largest].improvement):","202","        largest = right_pos","203","","204","    if largest != pos:","205","        heap[pos], heap[largest] = heap[largest], heap[pos]","206","        heapify_down(heap, largest, heap_length)","207","","208","","278","        heapify_up(heap, heap_ptr)","300","            heapify_down(heap, 0, heap_ptr - 1)"]}],"sklearn\/tree\/_criterion.pyx":[{"add":["268","        if (self.sum_total == NULL or","855","        self.weighted_n_right = (self.weighted_n_node_samples -","966","            impurity_right[0] -= (sum_right[k] \/ self.weighted_n_right) ** 2.0","1269","    Uses the formula (35) in Friedman's original Gradient Boosting paper:","1322","        return (diff * diff \/ (self.weighted_n_left * self.weighted_n_right *"],"delete":["268","        if (self.sum_total == NULL or ","855","        self.weighted_n_right = (self.weighted_n_node_samples - ","966","            impurity_right[0] -= (sum_right[k] \/ self.weighted_n_right) ** 2.0 ","1269","    Uses the formula (35) in Friedmans original Gradient Boosting paper:","1322","        return (diff * diff \/ (self.weighted_n_left * self.weighted_n_right * "]}],"sklearn\/tree\/_criterion.pxd":[{"add":["47","                                    # where k is output index."],"delete":["47","                                    # where k is output index. "]}],"sklearn\/tree\/_utils.pxd":[{"add":["108","    cdef void heapify_up(self, PriorityHeapRecord* heap, SIZE_t pos) nogil","109","    cdef void heapify_down(self, PriorityHeapRecord* heap, SIZE_t pos, SIZE_t heap_length) nogil"],"delete":[]}]}},"fa6fafcfdbd3cea4583fd63f45f7b80e76de74e7":{"changes":{"sklearn\/datasets\/tests\/test_20news.py":"MODIFY"},"diff":{"sklearn\/datasets\/tests\/test_20news.py":[{"add":["59","    try:","60","        datasets.fetch_20newsgroups(subset='all',","61","                                    download_if_missing=False)","62","    except IOError:","63","        raise SkipTest(\"Download 20 newsgroups to run this test\")","65","    # test subset = train","68","    assert_equal(bunch.data.shape, (11314, 130107))","72","    # test subset = test","75","    assert_equal(bunch.data.shape, (7532, 130107))","79","    # test subset = all","80","    bunch = datasets.fetch_20newsgroups_vectorized(subset='all')","82","    assert_equal(bunch.data.shape, (11314 + 7532, 130107))"],"delete":["59","    # This test is slow.","60","    raise SkipTest(\"Test too slow.\")","64","    assert_equal(bunch.data.shape, (11314, 107428))","70","    assert_equal(bunch.data.shape, (7532, 107428))","74","    bunch = datasets.fetch_20newsgroups_vectorized(subset=\"all\")","76","    assert_equal(bunch.data.shape, (11314 + 7532, 107428))"]}]}},"cee48cdf8f67bca9408855d2cd5a20fabec60155":{"changes":{"sklearn\/linear_model\/tests\/test_coordinate_descent.py":"MODIFY","examples\/linear_model\/plot_lasso_model_selection.py":"MODIFY","sklearn\/linear_model\/least_angle.py":"MODIFY"},"diff":{"sklearn\/linear_model\/tests\/test_coordinate_descent.py":[{"add":["173","    mse_lars = interpolate.interp1d(lars.cv_alphas_, lars.mse_path_.T)"],"delete":["173","    mse_lars = interpolate.interp1d(lars.cv_alphas_, lars.cv_mse_path_.T)"]}],"examples\/linear_model\/plot_lasso_model_selection.py":[{"add":["140","plt.plot(m_log_alphas, model.mse_path_, ':')","141","plt.plot(m_log_alphas, model.mse_path_.mean(axis=-1), 'k',"],"delete":["140","plt.plot(m_log_alphas, model.cv_mse_path_, ':')","141","plt.plot(m_log_alphas, model.cv_mse_path_.mean(axis=-1), 'k',"]}],"sklearn\/linear_model\/least_angle.py":[{"add":["23","from ..utils import arrayfuncs, as_float_array, check_X_y, deprecated","1058","    mse_path_ : array, shape (n_folds, n_cv_alphas)","1154","        self.mse_path_ = mse_path","1167","    @property","1168","    @deprecated(\"Attribute mse_path_ is deprecated in 0.18 and \"","1169","                \"will be removed in 0.20. Use 'cv_mse_path_' instead\")","1170","    def cv_mse_path_(self):","1171","        return self.mse_path_","1172","","1273","    mse_path_ : array, shape (n_folds, n_cv_alphas)"],"delete":["23","from ..utils import arrayfuncs, as_float_array, check_X_y","1058","    cv_mse_path_ : array, shape (n_folds, n_cv_alphas)","1154","        self.cv_mse_path_ = mse_path","1267","    cv_mse_path_ : array, shape (n_folds, n_cv_alphas)"]}]}},"49fb295561948d63199da8a03ba3ca1535fb7608":{"changes":{"sklearn\/gaussian_process\/kernels.py":"MODIFY","sklearn\/gaussian_process\/tests\/test_kernels.py":"MODIFY"},"diff":{"sklearn\/gaussian_process\/kernels.py":[{"add":["1206","                self.__class__.__name__, np.ravel(self.length_scale)[0])","1350","        else:","1352","                self.__class__.__name__, np.ravel(self.length_scale)[0],","1353","                self.nu)"],"delete":["1206","                self.__class__.__name__, self.length_scale)","1350","        else:  # isotropic","1352","                self.__class__.__name__, self.length_scale, self.nu)"]}],"sklearn\/gaussian_process\/tests\/test_kernels.py":[{"add":["43","           DotProduct(sigma_0=2.0), DotProduct(sigma_0=2.0) ** 2,","44","           RBF(length_scale=[2.0]), Matern(length_scale=[2.0])]","307","","308","","309","def test_repr_kernels():","310","    \"\"\"Smoke-test for repr in kernels.\"\"\"","311","","312","    for kernel in kernels:","313","        repr(kernel)"],"delete":["43","           DotProduct(sigma_0=2.0), DotProduct(sigma_0=2.0) ** 2]"]}]}},"1a5be6df1b66f3fe3831c7440438c919ad424626":{"changes":{"sklearn\/isotonic.py":"MODIFY","sklearn\/tests\/test_isotonic.py":"MODIFY"},"diff":{"sklearn\/isotonic.py":[{"add":["431","        if hasattr(self, '_necessary_X_') and hasattr(self, '_necessary_y_'):","432","            self._build_f(self._necessary_X_, self._necessary_y_)"],"delete":["431","        self._build_f(self._necessary_X_, self._necessary_y_)"]}],"sklearn\/tests\/test_isotonic.py":[{"add":["3","import copy","398","","399","","400","def test_isotonic_copy_before_fit():","401","    # https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/6628","402","    ir = IsotonicRegression()","403","    copy.copy(ir)"],"delete":[]}]}},"71408e09e4bf01a72567f9167a1e33fb9620d786":{"changes":{"doc\/conf.py":"MODIFY","\/dev\/null":"DELETE","doc\/templates\/numpydoc_docstring.rst":"ADD","sklearn\/datasets\/species_distributions.py":"MODIFY","build_tools\/circle\/build_doc.sh":"MODIFY","doc\/sphinxext\/sphinx_gallery\/gen_gallery.py":"MODIFY"},"diff":{"doc\/conf.py":[{"add":["34","    'numpydoc',","40","# this is needed for some reason...","41","# see https:\/\/github.com\/numpy\/numpydoc\/issues\/69","42","numpydoc_class_members_toctree = False","43","","44",""],"delete":["34","    'numpy_ext.numpydoc',"]}],"\/dev\/null":[{"add":[],"delete":[]}],"doc\/templates\/numpydoc_docstring.rst":[{"add":[],"delete":[]}],"sklearn\/datasets\/species_distributions.py":[{"add":["203","    - For an example of using this dataset with scikit-learn, see"],"delete":["203","","204","    * For an example of using this dataset with scikit-learn, see"]}],"build_tools\/circle\/build_doc.sh":[{"add":["111","pip install numpydoc"],"delete":[]}],"doc\/sphinxext\/sphinx_gallery\/gen_gallery.py":[{"add":[],"delete":["295",""]}]}},"a08555a2384884c03d5deb509192a052c06caa85":{"changes":{"doc\/modules\/classes.rst":"MODIFY","sklearn\/model_selection\/tests\/test_search.py":"MODIFY","doc\/modules\/cross_validation.rst":"MODIFY","examples\/model_selection\/plot_multi_metric_evaluation.py":"ADD","sklearn\/metrics\/scorer.py":"MODIFY","sklearn\/metrics\/tests\/test_score_objects.py":"MODIFY","sklearn\/model_selection\/__init__.py":"MODIFY","sklearn\/model_selection\/_validation.py":"MODIFY","doc\/modules\/grid_search.rst":"MODIFY","sklearn\/model_selection\/tests\/test_validation.py":"MODIFY","doc\/whats_new.rst":"MODIFY","doc\/modules\/model_evaluation.rst":"MODIFY","sklearn\/model_selection\/_search.py":"MODIFY"},"diff":{"doc\/modules\/classes.rst":[{"add":["225","   model_selection.cross_validate"],"delete":[]}],"sklearn\/model_selection\/tests\/test_search.py":[{"add":["9","import re","31","from sklearn.base import clone","37","from sklearn.model_selection import fit_grid_point","58","from sklearn.metrics import recall_score","59","from sklearn.metrics import accuracy_score","376","    for scoring in [None, ['accuracy', 'precision']]:","377","        grid_search = GridSearchCV(clf, {'foo_param': [1, 2, 3]}, refit=False)","378","        grid_search.fit(X, y)","379","        assert_true(not hasattr(grid_search, \"best_estimator_\") and","380","                    hasattr(grid_search, \"best_index_\") and","381","                    hasattr(grid_search, \"best_params_\"))","383","        # Make sure the functions predict\/transform etc raise meaningful","384","        # error messages","385","        for fn_name in ('predict', 'predict_proba', 'predict_log_proba',","386","                        'transform', 'inverse_transform'):","387","            assert_raise_message(NotFittedError,","388","                                 ('refit=False. %s is available only after '","389","                                  'refitting on the best parameters'","390","                                  % fn_name), getattr(grid_search, fn_name), X)","391","","392","    # Test that an invalid refit param raises appropriate error messages","393","    for refit in [\"\", 5, True, 'recall', 'accuracy']:","394","        assert_raise_message(ValueError, \"For multi-metric scoring, the \"","395","                             \"parameter refit must be set to a scorer key\",","396","                             GridSearchCV(clf, {}, refit=refit,","397","                                          scoring={'acc': 'accuracy',","398","                                                   'prec': 'precision'}).fit,","399","                             X, y)","639","","640","        def check_df(x):","641","            return isinstance(x, InputFeatureType)","642","","643","        def check_series(x):","644","            return isinstance(x, TargetType)","645","","659","    # Multi-metric evaluation unsupervised","660","    scoring = ['adjusted_rand_score', 'fowlkes_mallows_score']","661","    for refit in ['adjusted_rand_score', 'fowlkes_mallows_score']:","662","        grid_search = GridSearchCV(km, param_grid=dict(n_clusters=[2, 3, 4]),","663","                                   scoring=scoring, refit=refit)","664","        grid_search.fit(X, y)","665","        # Both ARI and FMS can find the right number :)","666","        assert_equal(grid_search.best_params_[\"n_clusters\"], 3)","667","","668","    # Single metric evaluation unsupervised","720","def check_cv_results_array_types(search, param_keys, score_keys):","722","    cv_results = search.cv_results_","730","","731","    scorer_keys = search.scorer_.keys() if search.multimetric_ else ['score']","732","","733","    for key in scorer_keys:","734","        assert_true(cv_results['rank_test_%s' % key].dtype == np.int32)","746","    # TODO Remove test in 0.20","747","    if search.multimetric_:","748","        assert_raise_message(AttributeError, \"not available for multi-metric\",","749","                             getattr, search, 'grid_scores_')","750","    else:","751","        cv_results = search.cv_results_","752","        res_scores = np.vstack(list([cv_results[\"split%d_test_score\" % i]","753","                                     for i in range(search.n_splits_)])).T","754","        res_means = cv_results[\"mean_test_score\"]","755","        res_params = cv_results[\"params\"]","756","        n_cand = len(res_params)","757","        grid_scores = assert_warns(DeprecationWarning, getattr,","758","                                   search, 'grid_scores_')","759","        assert_equal(len(grid_scores), n_cand)","760","        # Check consistency of the structure of grid_scores","761","        for i in range(n_cand):","762","            assert_equal(grid_scores[i].parameters, res_params[i])","763","            assert_array_equal(grid_scores[i].cv_validation_scores,","764","                               res_scores[i, :])","765","            assert_array_equal(grid_scores[i].mean_validation_score,","766","                               res_means[i])","790","    for iid in (False, True):","791","        search = GridSearchCV(SVC(), cv=n_splits, iid=iid, param_grid=params)","792","        search.fit(X, y)","803","        check_cv_results_array_types(search, param_keys, score_keys)","806","        cv_results = search.cv_results_","807","        n_candidates = len(search.cv_results_['params'])","822","    X, y = make_classification(n_samples=50, n_features=4, random_state=42)","827","    params = dict(C=expon(scale=10), gamma=expon(scale=0.1))","840","    for iid in (False, True):","841","        search = RandomizedSearchCV(SVC(), n_iter=n_search_iter, cv=n_splits,","842","                                    iid=iid, param_distributions=params)","843","        search.fit(X, y)","847","        check_cv_results_array_types(search, param_keys, score_keys)","949","def test_grid_search_cv_results_multimetric():","950","    X, y = make_classification(n_samples=50, n_features=4, random_state=42)","951","","952","    n_splits = 3","953","    params = [dict(kernel=['rbf', ], C=[1, 10], gamma=[0.1, 1]),","954","              dict(kernel=['poly', ], degree=[1, 2])]","955","","956","    for iid in (False, True):","957","        grid_searches = []","958","        for scoring in ({'accuracy': make_scorer(accuracy_score),","959","                         'recall': make_scorer(recall_score)},","960","                        'accuracy', 'recall'):","961","            grid_search = GridSearchCV(SVC(), cv=n_splits, iid=iid,","962","                                       param_grid=params, scoring=scoring,","963","                                       refit=False)","964","            grid_search.fit(X, y)","965","            assert_equal(grid_search.iid, iid)","966","            grid_searches.append(grid_search)","967","","968","        compare_cv_results_multimetric_with_single(*grid_searches, iid=iid)","969","","970","","971","def test_random_search_cv_results_multimetric():","972","    X, y = make_classification(n_samples=50, n_features=4, random_state=42)","973","","974","    n_splits = 3","975","    n_search_iter = 30","976","    scoring = ('accuracy', 'recall')","977","","978","    # Scipy 0.12's stats dists do not accept seed, hence we use param grid","979","    params = dict(C=np.logspace(-10, 1), gamma=np.logspace(-5, 0, base=0.1))","980","    for iid in (True, False):","981","        for refit in (True, False):","982","            random_searches = []","983","            for scoring in (('accuracy', 'recall'), 'accuracy', 'recall'):","984","                # If True, for multi-metric pass refit='accuracy'","985","                if refit:","986","                    refit = 'accuracy' if isinstance(scoring, tuple) else refit","987","                clf = SVC(probability=True, random_state=42)","988","                random_search = RandomizedSearchCV(clf, n_iter=n_search_iter,","989","                                                   cv=n_splits, iid=iid,","990","                                                   param_distributions=params,","991","                                                   scoring=scoring,","992","                                                   refit=refit, random_state=0)","993","                random_search.fit(X, y)","994","                random_searches.append(random_search)","995","","996","            compare_cv_results_multimetric_with_single(*random_searches,","997","                                                       iid=iid)","998","            if refit:","999","                compare_refit_methods_when_refit_with_acc(","1000","                    random_searches[0], random_searches[1], refit)","1001","","1002","","1003","def compare_cv_results_multimetric_with_single(","1004","        search_multi, search_acc, search_rec, iid):","1005","    \"\"\"Compare multi-metric cv_results with the ensemble of multiple","1006","    single metric cv_results from single metric grid\/random search\"\"\"","1007","","1008","    assert_equal(search_multi.iid, iid)","1009","    assert_true(search_multi.multimetric_)","1010","    assert_array_equal(sorted(search_multi.scorer_),","1011","                       ('accuracy', 'recall'))","1012","","1013","    cv_results_multi = search_multi.cv_results_","1014","    cv_results_acc_rec = {re.sub('_score$', '_accuracy', k): v","1015","                          for k, v in search_acc.cv_results_.items()}","1016","    cv_results_acc_rec.update({re.sub('_score$', '_recall', k): v","1017","                               for k, v in search_rec.cv_results_.items()})","1018","","1019","    # Check if score and timing are reasonable, also checks if the keys","1020","    # are present","1021","    assert_true(all((np.all(cv_results_multi[k] <= 1) for k in (","1022","                    'mean_score_time', 'std_score_time', 'mean_fit_time',","1023","                    'std_fit_time'))))","1024","","1025","    # Compare the keys, other than time keys, among multi-metric and","1026","    # single metric grid search results. np.testing.assert_equal performs a","1027","    # deep nested comparison of the two cv_results dicts","1028","    np.testing.assert_equal({k: v for k, v in cv_results_multi.items()","1029","                             if not k.endswith('_time')},","1030","                            {k: v for k, v in cv_results_acc_rec.items()","1031","                             if not k.endswith('_time')})","1032","","1033","","1034","def compare_refit_methods_when_refit_with_acc(search_multi, search_acc, refit):","1035","    \"\"\"Compare refit multi-metric search methods with single metric methods\"\"\"","1036","    if refit:","1037","        assert_equal(search_multi.refit, 'accuracy')","1038","    else:","1039","        assert_false(search_multi.refit)","1040","    assert_equal(search_acc.refit, refit)","1041","","1042","    X, y = make_blobs(n_samples=100, n_features=4, random_state=42)","1043","    for method in ('predict', 'predict_proba', 'predict_log_proba'):","1044","        assert_almost_equal(getattr(search_multi, method)(X),","1045","                            getattr(search_acc, method)(X))","1046","    assert_almost_equal(search_multi.score(X, y), search_acc.score(X, y))","1047","    for key in ('best_index_', 'best_score_', 'best_params_'):","1048","        assert_equal(getattr(search_multi, key), getattr(search_acc, key))","1049","","1050","","1157","def test_fit_grid_point():","1158","    X, y = make_classification(random_state=0)","1159","    cv = StratifiedKFold(random_state=0)","1160","    svc = LinearSVC(random_state=0)","1161","    scorer = make_scorer(accuracy_score)","1162","","1163","    for params in ({'C': 0.1}, {'C': 0.01}, {'C': 0.001}):","1164","        for train, test in cv.split(X, y):","1165","            this_scores, this_params, n_test_samples = fit_grid_point(","1166","                X, y, clone(svc), params, train, test,","1167","                scorer, verbose=False)","1168","","1169","            est = clone(svc).set_params(**params)","1170","            est.fit(X[train], y[train])","1171","            expected_score = scorer(est, X[test], y[test])","1172","","1173","            # Test the return values of fit_grid_point","1174","            assert_almost_equal(this_scores, expected_score)","1175","            assert_equal(params, this_params)","1176","            assert_equal(n_test_samples, test.size)","1177","","1178","    # Should raise an error upon multimetric scorer","1179","    assert_raise_message(ValueError, \"scoring value should either be a \"","1180","                         \"callable, string or None.\", fit_grid_point, X, y,","1181","                         svc, params, train, test, {'score': scorer},","1182","                         verbose=True)","1183","","1184","","1429","    np.testing.assert_equal({k: v for k, v in gs.cv_results_.items()","1430","                             if not k.endswith('_time')},","1431","                            {k: v for k, v in gs2.cv_results_.items()","1432","                             if not k.endswith('_time')})"],"delete":["29","from sklearn.externals.six.moves import zip","372","    grid_search = GridSearchCV(clf, {'foo_param': [1, 2, 3]}, refit=False)","373","    grid_search.fit(X, y)","374","    assert_true(not hasattr(grid_search, \"best_estimator_\") and","375","                hasattr(grid_search, \"best_index_\") and","376","                hasattr(grid_search, \"best_params_\"))","378","    # Make sure the predict\/transform etc fns raise meaningfull error msg","379","    for fn_name in ('predict', 'predict_proba', 'predict_log_proba',","380","                    'transform', 'inverse_transform'):","381","        assert_raise_message(NotFittedError,","382","                             ('refit=False. %s is available only after '","383","                              'refitting on the best parameters' % fn_name),","384","                             getattr(grid_search, fn_name), X)","624","        check_df = lambda x: isinstance(x, InputFeatureType)","625","        check_series = lambda x: isinstance(x, TargetType)","638","    grid_search = GridSearchCV(km, param_grid=dict(n_clusters=[2, 3, 4]),","639","                               scoring='adjusted_rand_score')","640","    grid_search.fit(X, y)","641","    # ARI can find the right number :)","642","    assert_equal(grid_search.best_params_[\"n_clusters\"], 3)","647","    # So can FMS ;)","696","def check_cv_results_array_types(cv_results, param_keys, score_keys):","705","    assert_true(cv_results['rank_test_score'].dtype == np.int32)","717","    # TODO Remove in 0.20","718","    cv_results = search.cv_results_","719","    res_scores = np.vstack(list([cv_results[\"split%d_test_score\" % i]","720","                                 for i in range(search.n_splits_)])).T","721","    res_means = cv_results[\"mean_test_score\"]","722","    res_params = cv_results[\"params\"]","723","    n_cand = len(res_params)","724","    grid_scores = assert_warns(DeprecationWarning, getattr,","725","                               search, 'grid_scores_')","726","    assert_equal(len(grid_scores), n_cand)","727","    # Check consistency of the structure of grid_scores","728","    for i in range(n_cand):","729","        assert_equal(grid_scores[i].parameters, res_params[i])","730","        assert_array_equal(grid_scores[i].cv_validation_scores,","731","                           res_scores[i, :])","732","        assert_array_equal(grid_scores[i].mean_validation_score, res_means[i])","743","    grid_search = GridSearchCV(SVC(), cv=n_splits, iid=False,","744","                               param_grid=params)","745","    grid_search.fit(X, y)","746","    grid_search_iid = GridSearchCV(SVC(), cv=n_splits, iid=True,","747","                                   param_grid=params)","748","    grid_search_iid.fit(X, y)","762","    for search, iid in zip((grid_search, grid_search_iid), (False, True)):","773","        check_cv_results_array_types(cv_results, param_keys, score_keys)","776","        cv_results = grid_search.cv_results_","777","        n_candidates = len(grid_search.cv_results_['params'])","792","    # Make a dataset with a lot of noise to get various kind of prediction","793","    # errors across CV folds and parameter settings","794","    X, y = make_classification(n_samples=200, n_features=100, n_informative=3,","795","                               random_state=0)","797","    # scipy.stats dists now supports `seed` but we still support scipy 0.12","798","    # which doesn't support the seed. Hence the assertions in the test for","799","    # random_search alone should not depend on randomization.","802","    params = dict(C=expon(scale=10), gamma=expon(scale=0.1))","803","    random_search = RandomizedSearchCV(SVC(), n_iter=n_search_iter,","804","                                       cv=n_splits, iid=False,","805","                                       param_distributions=params)","806","    random_search.fit(X, y)","807","    random_search_iid = RandomizedSearchCV(SVC(), n_iter=n_search_iter,","808","                                           cv=n_splits, iid=True,","809","                                           param_distributions=params)","810","    random_search_iid.fit(X, y)","824","    for search, iid in zip((random_search, random_search_iid), (False, True)):","828","        check_cv_results_array_types(cv_results, param_keys, score_keys)","1274","    def _pop_time_keys(cv_results):","1275","        for key in ('mean_fit_time', 'std_fit_time',","1276","                    'mean_score_time', 'std_score_time'):","1277","            cv_results.pop(key)","1278","        return cv_results","1279","","1286","    np.testing.assert_equal(_pop_time_keys(gs.cv_results_),","1287","                            _pop_time_keys(gs2.cv_results_))"]}],"doc\/modules\/cross_validation.rst":[{"add":["174","","175",".. _multimetric_cross_validation:","176","","177","The cross_validate function and multiple metric evaluation","178","----------------------------------------------------------","179","","180","The ``cross_validate`` function differs from ``cross_val_score`` in two ways -","181","","182","- It allows specifying multiple metrics for evaluation.","183","","184","- It returns a dict containing training scores, fit-times and score-times in","185","  addition to the test score.","186","","187","For single metric evaluation, where the scoring parameter is a string,","188","callable or None, the keys will be - ``['test_score', 'fit_time', 'score_time']``","189","","190","And for multiple metric evaluation, the return value is a dict with the","191","following keys -","192","``['test_<scorer1_name>', 'test_<scorer2_name>', 'test_<scorer...>', 'fit_time', 'score_time']``","193","","194","``return_train_score`` is set to ``True`` by default. It adds train score keys","195","for all the scorers. If train scores are not needed, this should be set to","196","``False`` explicitly.","197","","198","The multiple metrics can be specified either as a list, tuple or set of","199","predefined scorer names::","200","","201","    >>> from sklearn.model_selection import cross_validate","202","    >>> from sklearn.metrics import recall_score","203","    >>> scoring = ['precision_macro', 'recall_macro']","204","    >>> clf = svm.SVC(kernel='linear', C=1, random_state=0)","205","    >>> scores = cross_validate(clf, iris.data, iris.target, scoring=scoring,","206","    ...                         cv=5, return_train_score=False)","207","    >>> sorted(scores.keys())","208","    ['fit_time', 'score_time', 'test_precision_macro', 'test_recall_macro']","209","    >>> scores['test_recall_macro']                       # doctest: +ELLIPSIS","210","    array([ 0.96...,  1.  ...,  0.96...,  0.96...,  1.        ])","211","","212","Or as a dict mapping scorer name to a predefined or custom scoring function::","213","","214","    >>> from sklearn.metrics.scorer import make_scorer","215","    >>> scoring = {'prec_macro': 'precision_macro',","216","    ...            'rec_micro': make_scorer(recall_score, average='macro')}","217","    >>> scores = cross_validate(clf, iris.data, iris.target, scoring=scoring,","218","    ...                         cv=5, return_train_score=True)","219","    >>> sorted(scores.keys())                 # doctest: +NORMALIZE_WHITESPACE","220","    ['fit_time', 'score_time', 'test_prec_macro', 'test_rec_micro',","221","     'train_prec_macro', 'train_rec_micro']","222","    >>> scores['train_rec_micro']                         # doctest: +ELLIPSIS","223","    array([ 0.97...,  0.97...,  0.99...,  0.98...,  0.98...])","224","","225","Here is an example of ``cross_validate`` using a single metric::","226","","227","    >>> scores = cross_validate(clf, iris.data, iris.target,","228","    ...                         scoring='precision_macro')","229","    >>> sorted(scores.keys())","230","    ['fit_time', 'score_time', 'test_score', 'train_score']","231","","232","","247","  0.973..."],"delete":["188","  0.966..."]}],"examples\/model_selection\/plot_multi_metric_evaluation.py":[{"add":[],"delete":[]}],"sklearn\/metrics\/scorer.py":[{"add":["211","    valid = True","218","            valid = False  # Don't raise here to make the error message elegant","219","        if not valid:","263","    elif callable(scoring):","276","    elif scoring is None:","277","        if hasattr(estimator, 'score'):","278","            return _passthrough_scorer","279","        elif allow_none:","280","            return None","281","        else:","282","            raise TypeError(","283","                \"If no scoring is specified, the estimator passed should \"","284","                \"have a 'score' method. The estimator %r does not.\"","285","                % estimator)","287","        raise ValueError(\"scoring value should either be a callable, string or\"","288","                         \" None. %r was passed\" % scoring)","289","","290","","291","def _check_multimetric_scoring(estimator, scoring=None):","292","    \"\"\"Check the scoring parameter in cases when multiple metrics are allowed","293","","294","    Parameters","295","    ----------","296","    estimator : sklearn estimator instance","297","        The estimator for which the scoring will be applied.","298","","299","    scoring : string, callable, list\/tuple, dict or None, default: None","300","        A single string (see :ref:`scoring_parameter`) or a callable","301","        (see :ref:`scoring`) to evaluate the predictions on the test set.","302","","303","        For evaluating multiple metrics, either give a list of (unique) strings","304","        or a dict with names as keys and callables as values.","305","","306","        NOTE that when using custom scorers, each scorer should return a single","307","        value. Metric functions returning a list\/array of values can be wrapped","308","        into multiple scorers that return one value each.","309","","310","        See :ref:`multivalued_scorer_wrapping` for an example.","311","","312","        If None the estimator's default scorer (if available) is used.","313","        The return value in that case will be ``{'score': <default_scorer>}``.","314","        If the estimator's default scorer is not available, a ``TypeError``","315","        is raised.","316","","317","    Returns","318","    -------","319","    scorers_dict : dict","320","        A dict mapping each scorer name to its validated scorer.","321","","322","    is_multimetric : bool","323","        True if scorer is a list\/tuple or dict of callables","324","        False if scorer is None\/str\/callable","325","    \"\"\"","326","    if callable(scoring) or scoring is None or isinstance(scoring,","327","                                                          six.string_types):","328","        scorers = {\"score\": check_scoring(estimator, scoring=scoring)}","329","        return scorers, False","330","    else:","331","        err_msg_generic = (\"scoring should either be a single string or \"","332","                           \"callable for single metric evaluation or a \"","333","                           \"list\/tuple of strings or a dict of scorer name \"","334","                           \"mapped to the callable for multiple metric \"","335","                           \"evaluation. Got %s of type %s\"","336","                           % (repr(scoring), type(scoring)))","337","","338","        if isinstance(scoring, (list, tuple, set)):","339","            err_msg = (\"The list\/tuple elements must be unique \"","340","                       \"strings of predefined scorers. \")","341","            invalid = False","342","            try:","343","                keys = set(scoring)","344","            except TypeError:","345","                invalid = True","346","            if invalid:","347","                raise ValueError(err_msg)","348","","349","            if len(keys) != len(scoring):","350","                raise ValueError(err_msg + \"Duplicate elements were found in\"","351","                                 \" the given list. %r\" % repr(scoring))","352","            elif len(keys) > 0:","353","                if not all(isinstance(k, six.string_types) for k in keys):","354","                    if any(callable(k) for k in keys):","355","                        raise ValueError(err_msg +","356","                                         \"One or more of the elements were \"","357","                                         \"callables. Use a dict of score name \"","358","                                         \"mapped to the scorer callable. \"","359","                                         \"Got %r\" % repr(scoring))","360","                    else:","361","                        raise ValueError(err_msg +","362","                                         \"Non-string types were found in \"","363","                                         \"the given list. Got %r\"","364","                                         % repr(scoring))","365","                scorers = {scorer: check_scoring(estimator, scoring=scorer)","366","                           for scorer in scoring}","367","            else:","368","                raise ValueError(err_msg +","369","                                 \"Empty list was given. %r\" % repr(scoring))","370","","371","        elif isinstance(scoring, dict):","372","            keys = set(scoring)","373","            if not all(isinstance(k, six.string_types) for k in keys):","374","                raise ValueError(\"Non-string types were found in the keys of \"","375","                                 \"the given dict. scoring=%r\" % repr(scoring))","376","            if len(keys) == 0:","377","                raise ValueError(\"An empty dict was passed. %r\"","378","                                 % repr(scoring))","379","            scorers = {key: check_scoring(estimator, scoring=scorer)","380","                       for key, scorer in scoring.items()}","381","        else:","382","            raise ValueError(err_msg_generic)","383","        return scorers, True"],"delete":["255","    has_scoring = scoring is not None","261","    elif has_scoring:","274","    elif hasattr(estimator, 'score'):","275","        return _passthrough_scorer","276","    elif allow_none:","277","        return None","279","        raise TypeError(","280","            \"If no scoring is specified, the estimator passed should \"","281","            \"have a 'score' method. The estimator %r does not.\" % estimator)"]}],"sklearn\/metrics\/tests\/test_score_objects.py":[{"add":["10","from sklearn.utils.testing import assert_equal","14","from sklearn.utils.testing import assert_false","25","from sklearn.metrics import accuracy_score","26","from sklearn.metrics.scorer import _check_multimetric_scoring","110","    \"\"\"Dummy estimator to test scoring validators\"\"\"","115","    \"\"\"Dummy estimator to test scoring validators\"\"\"","121","    \"\"\"Dummy estimator to test scoring validators\"\"\"","130","    \"\"\"Dummy estimator to test scoring validators\"\"\"","151","def check_scoring_validator_for_single_metric_usecases(scoring_validator):","152","    # Test all branches of single metric usecases","156","    assert_raises_regexp(TypeError, pattern, scoring_validator, estimator)","160","    scorer = scoring_validator(estimator)","168","    assert_raises_regexp(TypeError, pattern, scoring_validator, estimator)","170","    scorer = scoring_validator(estimator, \"accuracy\")","174","    scorer = scoring_validator(estimator, \"accuracy\")","177","    # Test the allow_none parameter for check_scoring alone","178","    if scoring_validator is check_scoring:","179","        estimator = EstimatorWithFit()","180","        scorer = scoring_validator(estimator, allow_none=True)","181","        assert_true(scorer is None)","182","","183","","184","def check_multimetric_scoring_single_metric_wrapper(*args, **kwargs):","185","    # This wraps the _check_multimetric_scoring to take in single metric","186","    # scoring parameter so we can run the tests that we will run for","187","    # check_scoring, for check_multimetric_scoring too for single-metric","188","    # usecases","189","    scorers, is_multi = _check_multimetric_scoring(*args, **kwargs)","190","    # For all single metric use cases, it should register as not multimetric","191","    assert_false(is_multi)","192","    if args[0] is not None:","193","        assert_true(scorers is not None)","194","        names, scorers = zip(*scorers.items())","195","        assert_equal(len(scorers), 1)","196","        assert_equal(names[0], 'score')","197","        scorers = scorers[0]","198","    return scorers","199","","200","","201","def test_check_scoring_and_check_multimetric_scoring():","202","    check_scoring_validator_for_single_metric_usecases(check_scoring)","203","    # To make sure the check_scoring is correctly applied to the constituent","204","    # scorers","205","    check_scoring_validator_for_single_metric_usecases(","206","        check_multimetric_scoring_single_metric_wrapper)","207","","208","    # For multiple metric use cases","209","    # Make sure it works for the valid cases","210","    for scoring in (('accuracy',), ['precision'],","211","                    {'acc': 'accuracy', 'precision': 'precision'},","212","                    ('accuracy', 'precision'), ['precision', 'accuracy'],","213","                    {'accuracy': make_scorer(accuracy_score),","214","                     'precision': make_scorer(precision_score)}):","215","        estimator = LinearSVC(random_state=0)","216","        estimator.fit([[1], [2], [3]], [1, 1, 0])","217","","218","        scorers, is_multi = _check_multimetric_scoring(estimator, scoring)","219","        assert_true(is_multi)","220","        assert_true(isinstance(scorers, dict))","221","        assert_equal(sorted(scorers.keys()), sorted(list(scoring)))","222","        assert_true(all([isinstance(scorer, _PredictScorer)","223","                         for scorer in list(scorers.values())]))","224","","225","        if 'acc' in scoring:","226","            assert_almost_equal(scorers['acc'](","227","                estimator, [[1], [2], [3]], [1, 0, 0]), 2. \/ 3.)","228","        if 'accuracy' in scoring:","229","            assert_almost_equal(scorers['accuracy'](","230","                estimator, [[1], [2], [3]], [1, 0, 0]), 2. \/ 3.)","231","        if 'precision' in scoring:","232","            assert_almost_equal(scorers['precision'](","233","                estimator, [[1], [2], [3]], [1, 0, 0]), 0.5)","234","","235","    estimator = EstimatorWithFitAndPredict()","236","    estimator.fit([[1]], [1])","237","","238","    # Make sure it raises errors when scoring parameter is not valid.","239","    # More weird corner cases are tested at test_validation.py","240","    error_message_regexp = \".*must be unique strings.*\"","241","    for scoring in ((make_scorer(precision_score),  # Tuple of callables","242","                     make_scorer(accuracy_score)), [5],","243","                    (make_scorer(precision_score),), (), ('f1', 'f1')):","244","        assert_raises_regexp(ValueError, error_message_regexp,","245","                             _check_multimetric_scoring, estimator,","246","                             scoring=scoring)"],"delete":["106","    \"\"\"Dummy estimator to test check_scoring\"\"\"","111","    \"\"\"Dummy estimator to test check_scoring\"\"\"","117","    \"\"\"Dummy estimator to test check_scoring\"\"\"","126","    \"\"\"Dummy estimator to test check_scoring\"\"\"","147","def test_check_scoring():","148","    # Test all branches of check_scoring","152","    assert_raises_regexp(TypeError, pattern, check_scoring, estimator)","156","    scorer = check_scoring(estimator)","164","    assert_raises_regexp(TypeError, pattern, check_scoring, estimator)","166","    scorer = check_scoring(estimator, \"accuracy\")","170","    scorer = check_scoring(estimator, \"accuracy\")","173","    estimator = EstimatorWithFit()","174","    scorer = check_scoring(estimator, allow_none=True)","175","    assert_true(scorer is None)"]}],"sklearn\/model_selection\/__init__.py":[{"add":["20","from ._validation import cross_validate","53","           'cross_validate',"],"delete":[]}],"sklearn\/model_selection\/_validation.py":[{"add":["5","# Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>","6","#         Gael Varoquaux <gael.varoquaux@normalesup.org>","8","#         Raghav RV <rvraghav93@gmail.com>","26","from ..externals.six.moves import zip","27","from ..metrics.scorer import check_scoring, _check_multimetric_scoring","32","","33","__all__ = ['cross_validate', 'cross_val_score', 'cross_val_predict',","34","           'permutation_test_score', 'learning_curve', 'validation_curve']","35","","36","","37","def cross_validate(estimator, X, y=None, groups=None, scoring=None, cv=None,","38","                   n_jobs=1, verbose=0, fit_params=None,","39","                   pre_dispatch='2*n_jobs', return_train_score=True):","40","    \"\"\"Evaluate metric(s) by cross-validation and also record fit\/score times.","41","","42","    Read more in the :ref:`User Guide <multimetric_cross_validation>`.","43","","44","    Parameters","45","    ----------","46","    estimator : estimator object implementing 'fit'","47","        The object to use to fit the data.","48","","49","    X : array-like","50","        The data to fit. Can be for example a list, or an array.","51","","52","    y : array-like, optional, default: None","53","        The target variable to try to predict in the case of","54","        supervised learning.","55","","56","    groups : array-like, with shape (n_samples,), optional","57","        Group labels for the samples used while splitting the dataset into","58","        train\/test set.","59","","60","    scoring : string, callable, list\/tuple, dict or None, default: None","61","        A single string (see :ref:`scoring_parameter`) or a callable","62","        (see :ref:`scoring`) to evaluate the predictions on the test set.","63","","64","        For evaluating multiple metrics, either give a list of (unique) strings","65","        or a dict with names as keys and callables as values.","66","","67","        NOTE that when using custom scorers, each scorer should return a single","68","        value. Metric functions returning a list\/array of values can be wrapped","69","        into multiple scorers that return one value each.","70","","71","        See :ref:`multivalued_scorer_wrapping` for an example.","72","","73","        If None, the estimator's default scorer (if available) is used.","74","","75","    cv : int, cross-validation generator or an iterable, optional","76","        Determines the cross-validation splitting strategy.","77","        Possible inputs for cv are:","78","          - None, to use the default 3-fold cross validation,","79","          - integer, to specify the number of folds in a `(Stratified)KFold`,","80","          - An object to be used as a cross-validation generator.","81","          - An iterable yielding train, test splits.","82","","83","        For integer\/None inputs, if the estimator is a classifier and ``y`` is","84","        either binary or multiclass, :class:`StratifiedKFold` is used. In all","85","        other cases, :class:`KFold` is used.","86","","87","        Refer :ref:`User Guide <cross_validation>` for the various","88","        cross-validation strategies that can be used here.","89","","90","    n_jobs : integer, optional","91","        The number of CPUs to use to do the computation. -1 means","92","        'all CPUs'.","93","","94","    verbose : integer, optional","95","        The verbosity level.","96","","97","    fit_params : dict, optional","98","        Parameters to pass to the fit method of the estimator.","99","","100","    pre_dispatch : int, or string, optional","101","        Controls the number of jobs that get dispatched during parallel","102","        execution. Reducing this number can be useful to avoid an","103","        explosion of memory consumption when more jobs get dispatched","104","        than CPUs can process. This parameter can be:","105","","106","            - None, in which case all the jobs are immediately","107","              created and spawned. Use this for lightweight and","108","              fast-running jobs, to avoid delays due to on-demand","109","              spawning of the jobs","110","","111","            - An int, giving the exact number of total jobs that are","112","              spawned","113","","114","            - A string, giving an expression as a function of n_jobs,","115","              as in '2*n_jobs'","116","","117","    return_train_score : boolean, default True","118","        Whether to include train scores in the return dict if ``scoring`` is","119","        of multimetric type.","120","","121","    Returns","122","    -------","123","    scores : dict of float arrays of shape=(n_splits,)","124","        Array of scores of the estimator for each run of the cross validation.","125","","126","        A dict of arrays containing the score\/time arrays for each scorer is","127","        returned. The possible keys for this ``dict`` are:","128","","129","            ``test_score``","130","                The score array for test scores on each cv split.","131","            ``train_score``","132","                The score array for train scores on each cv split.","133","                This is available only if ``return_train_score`` parameter","134","                is ``True``.","135","            ``fit_time``","136","                The time for fitting the estimator on the train","137","                set for each cv split.","138","            ``score_time``","139","                The time for scoring the estimator on the test set for each","140","                cv split. (Note time for scoring on the train set is not","141","                included even if ``return_train_score`` is set to ``True``","142","","143","    Examples","144","    --------","145","    >>> from sklearn import datasets, linear_model","146","    >>> from sklearn.model_selection import cross_val_score","147","    >>> from sklearn.metrics.scorer import make_scorer","148","    >>> from sklearn.metrics import confusion_matrix","149","    >>> from sklearn.svm import LinearSVC","150","    >>> diabetes = datasets.load_diabetes()","151","    >>> X = diabetes.data[:150]","152","    >>> y = diabetes.target[:150]","153","    >>> lasso = linear_model.Lasso()","154","","155","    # single metric evaluation using cross_validate","156","    >>> cv_results = cross_validate(lasso, X, y, return_train_score=False)","157","    >>> sorted(cv_results.keys())                         # doctest: +ELLIPSIS","158","    ['fit_time', 'score_time', 'test_score']","159","    >>> cv_results['test_score']    # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE","160","    array([ 0.33...,  0.08...,  0.03...])","161","","162","    # Multiple metric evaluation using cross_validate","163","    # (Please refer the ``scoring`` parameter doc for more information)","164","    >>> scores = cross_validate(lasso, X, y,","165","    ...                         scoring=('r2', 'neg_mean_squared_error'))","166","    >>> print(scores['test_neg_mean_squared_error'])      # doctest: +ELLIPSIS","167","    [-3635.5... -3573.3... -6114.7...]","168","    >>> print(scores['train_r2'])                         # doctest: +ELLIPSIS","169","    [ 0.28...  0.39...  0.22...]","170","","171","    See Also","172","    ---------","173","    :func:`sklearn.metrics.cross_val_score`:","174","        Run cross-validation for single metric evaluation.","175","","176","    :func:`sklearn.metrics.make_scorer`:","177","        Make a scorer from a performance metric or loss function.","178","","179","    \"\"\"","180","    X, y, groups = indexable(X, y, groups)","181","","182","    cv = check_cv(cv, y, classifier=is_classifier(estimator))","183","    scorers, _ = _check_multimetric_scoring(estimator, scoring=scoring)","184","","185","    # We clone the estimator to make sure that all the folds are","186","    # independent, and that it is pickle-able.","187","    parallel = Parallel(n_jobs=n_jobs, verbose=verbose,","188","                        pre_dispatch=pre_dispatch)","189","    scores = parallel(","190","        delayed(_fit_and_score)(","191","            clone(estimator), X, y, scorers, train, test, verbose, None,","192","            fit_params, return_train_score=return_train_score,","193","            return_times=True)","194","        for train, test in cv.split(X, y, groups))","195","","196","    if return_train_score:","197","        train_scores, test_scores, fit_times, score_times = zip(*scores)","198","        train_scores = _aggregate_score_dicts(train_scores)","199","    else:","200","        test_scores, fit_times, score_times = zip(*scores)","201","    test_scores = _aggregate_score_dicts(test_scores)","202","","203","    ret = dict()","204","    ret['fit_time'] = np.array(fit_times)","205","    ret['score_time'] = np.array(score_times)","206","","207","    for name in scorers:","208","        ret['test_%s' % name] = np.array(test_scores[name])","209","        if return_train_score:","210","            ret['train_%s' % name] = np.array(train_scores[name])","211","","212","    return ret","228","        The data to fit. Can be for example a list, or an array.","304","    :func:`sklearn.model_selection.cross_validate`:","305","        To run cross-validation on multiple metrics and also to return","306","        train scores, fit times and score times.","307","","312","    # To ensure multimetric format is not supported","314","","315","    cv_results = cross_validate(estimator=estimator, X=X, y=y, groups=groups,","316","                                scoring={'score': scorer}, cv=cv,","317","                                return_train_score=False,","318","                                n_jobs=n_jobs, verbose=verbose,","319","                                fit_params=fit_params,","320","                                pre_dispatch=pre_dispatch)","321","    return cv_results['test_score']","342","    scorer : A single callable or dict mapping scorer name to the callable","343","        If it is a single callable, the return value for ``train_scores`` and","344","        ``test_scores`` is a single float.","345","","346","        For a dict, it should be one mapping the scorer name to the scorer","347","        callable object \/ function.","348","","349","        The callable object \/ fn should have signature","379","    return_n_test_samples : boolean, optional, default: False","380","        Whether to return the ``n_test_samples``","381","","382","    return_times : boolean, optional, default: False","383","        Whether to return the fit\/score times.","384","","387","    train_scores : dict of scorer name -> float, optional","388","        Score on training set (for all the scorers),","389","        returned only if `return_train_score` is `True`.","391","    test_scores : dict of scorer name -> float, optional","392","        Score on testing set (for all the scorers).","419","    test_scores = {}","420","    train_scores = {}","429","    is_multimetric = not callable(scorer)","430","    n_scorers = len(scorer.keys()) if is_multimetric else 1","431","","445","            if is_multimetric:","446","                test_scores = dict(zip(scorer.keys(),","447","                                   [error_score, ] * n_scorers))","448","                if return_train_score:","449","                    train_scores = dict(zip(scorer.keys(),","450","                                        [error_score, ] * n_scorers))","451","            else:","452","                test_scores = error_score","453","                if return_train_score:","454","                    train_scores = error_score","465","        # _score will return dict if is_multimetric is True","466","        test_scores = _score(estimator, X_test, y_test, scorer, is_multimetric)","469","            train_scores = _score(estimator, X_train, y_train, scorer,","470","                                  is_multimetric)","473","        if is_multimetric:","474","            for scorer_name, score in test_scores.items():","475","                msg += \", %s=%s\" % (scorer_name, score)","476","        else:","477","            msg += \", score=%s\" % test_scores","483","    ret = [train_scores, test_scores] if return_train_score else [test_scores]","494","def _score(estimator, X_test, y_test, scorer, is_multimetric=False):","495","    \"\"\"Compute the score(s) of an estimator on a given test set.","496","","497","    Will return a single float if is_multimetric is False and a dict of floats,","498","    if is_multimetric is True","499","    \"\"\"","500","    if is_multimetric:","501","        return _multimetric_score(estimator, X_test, y_test, scorer)","503","        if y_test is None:","504","            score = scorer(estimator, X_test)","505","        else:","506","            score = scorer(estimator, X_test, y_test)","507","","508","        if hasattr(score, 'item'):","509","            try:","510","                # e.g. unwrap memmapped scalars","511","                score = score.item()","512","            except ValueError:","513","                # non-scalar?","514","                pass","515","","516","        if not isinstance(score, numbers.Number):","517","            raise ValueError(\"scoring must return a number, got %s (%s) \"","518","                             \"instead. (scorer=%r)\"","519","                             % (str(score), type(score), scorer))","523","def _multimetric_score(estimator, X_test, y_test, scorers):","524","    \"\"\"Return a dict of score for multimetric scoring\"\"\"","525","    scores = {}","526","","527","    for name, scorer in scorers.items():","528","        if y_test is None:","529","            score = scorer(estimator, X_test)","530","        else:","531","            score = scorer(estimator, X_test, y_test)","532","","533","        if hasattr(score, 'item'):","534","            try:","535","                # e.g. unwrap memmapped scalars","536","                score = score.item()","537","            except ValueError:","538","                # non-scalar?","539","                pass","540","        scores[name] = score","541","","542","        if not isinstance(score, numbers.Number):","543","            raise ValueError(\"scoring must return a number, got %s (%s) \"","544","                             \"instead. (scorer=%s)\"","545","                             % (str(score), type(score), name))","546","    return scores","547","","548","","805","        A single string (see :ref:`_scoring_parameter`) or a callable","806","        (see :ref:`_scoring`) to evaluate the predictions on the test set.","807","","808","        If None the estimator's default scorer, if available, is used.","1254","","1255","","1256","def _aggregate_score_dicts(scores):","1257","    \"\"\"Aggregate the list of dict to dict of np ndarray","1258","","1259","    The aggregated output of _fit_and_score will be a list of dict","1260","    of form [{'prec': 0.1, 'acc':1.0}, {'prec': 0.1, 'acc':1.0}, ...]","1261","    Convert it to a dict of array {'prec': np.array([0.1 ...]), ...}","1262","","1263","    Parameters","1264","    ----------","1265","","1266","    scores : list of dict","1267","        List of dicts of the scores for all scorers. This is a flat list,","1268","        assumed originally to be of row major order.","1269","","1270","    Example","1271","    -------","1272","","1273","    >>> scores = [{'a': 1, 'b':10}, {'a': 2, 'b':2}, {'a': 3, 'b':3},","1274","    ...           {'a': 10, 'b': 10}]                         # doctest: +SKIP","1275","    >>> _aggregate_score_dicts(scores)                        # doctest: +SKIP","1276","    {'a': array([1, 2, 3, 10]),","1277","     'b': array([10, 2, 3, 10])}","1278","    \"\"\"","1279","    out = {}","1280","    for key in scores[0]:","1281","        out[key] = np.asarray([score[key] for score in scores])","1282","    return out"],"delete":["5","# Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>,","6","#         Gael Varoquaux <gael.varoquaux@normalesup.org>,","10","","26","from ..metrics.scorer import check_scoring","31","__all__ = ['cross_val_score', 'cross_val_predict', 'permutation_test_score',","32","           'learning_curve', 'validation_curve']","48","        The data to fit. Can be, for example a list, or an array at least 2d.","128","    X, y, groups = indexable(X, y, groups)","129","","130","    cv = check_cv(cv, y, classifier=is_classifier(estimator))","132","    # We clone the estimator to make sure that all the folds are","133","    # independent, and that it is pickle-able.","134","    parallel = Parallel(n_jobs=n_jobs, verbose=verbose,","135","                        pre_dispatch=pre_dispatch)","136","    scores = parallel(delayed(_fit_and_score)(clone(estimator), X, y, scorer,","137","                                              train, test, verbose, None,","138","                                              fit_params)","139","                      for train, test in cv.split(X, y, groups))","140","    return np.array(scores)[:, 0]","161","    scorer : callable","162","        A scorer callable object \/ function with signature","194","    train_score : float, optional","195","        Score on training set, returned only if `return_train_score` is `True`.","197","    test_score : float","198","        Score on test set.","246","            test_score = error_score","247","            if return_train_score:","248","                train_score = error_score","259","        test_score = _score(estimator, X_test, y_test, scorer)","262","            train_score = _score(estimator, X_train, y_train, scorer)","265","        msg += \", score=%f\" % test_score","271","    ret = [train_score, test_score] if return_train_score else [test_score]","282","def _score(estimator, X_test, y_test, scorer):","283","    \"\"\"Compute the score of an estimator on a given test set.\"\"\"","284","    if y_test is None:","285","        score = scorer(estimator, X_test)","287","        score = scorer(estimator, X_test, y_test)","288","    if hasattr(score, 'item'):","289","        try:","290","            # e.g. unwrap memmapped scalars","291","            score = score.item()","292","        except ValueError:","293","            # non-scalar?","294","            pass","295","    if not isinstance(score, numbers.Number):","296","        raise ValueError(\"scoring must return a number, got %s (%s) instead.\"","297","                         % (str(score), type(score)))","557","        A string (see model evaluation documentation) or","558","        a scorer callable object \/ function with signature","559","        ``scorer(estimator, X, y)``.","999",""]}],"doc\/modules\/grid_search.rst":[{"add":["86","    - See :ref:`sphx_glr_auto_examples_model_selection_plot_multi_metric_evaluation`","87","      for an example of :class:`GridSearchCV` being used to evaluate multiple","88","      metrics simultaneously.","89","","167",".. _multimetric_grid_search:","168","","169","Specifying multiple metrics for evaluation","170","------------------------------------------","171","","172","``GridSearchCV`` and ``RandomizedSearchCV`` allow specifying multiple metrics","173","for the ``scoring`` parameter.","174","","175","Multimetric scoring can either be specified as a list of strings of predefined","176","scores names or a dict mapping the scorer name to the scorer function and\/or","177","the predefined scorer name(s). See :ref:`multimetric_scoring` for more details.","178","","179","When specifying multiple metrics, the ``refit`` parameter must be set to the","180","metric (string) for which the ``best_params_`` will be found and used to build","181","the ``best_estimator_`` on the whole dataset. If the search should not be","182","refit, set ``refit=False``. Leaving refit to the default value ``None`` will","183","result in an error when using multiple metrics.","184","","185","See :ref:`sphx_glr_auto_examples_model_selection_plot_multi_metric_evaluation`","186","for an example usage.","187",""],"delete":[]}],"sklearn\/model_selection\/tests\/test_validation.py":[{"add":["18","from sklearn.utils.testing import assert_raises_regex","28","from sklearn.model_selection import cross_validate","46","from sklearn.metrics import accuracy_score","47","from sklearn.metrics import confusion_matrix","48","from sklearn.metrics import precision_recall_fscore_support","50","from sklearn.metrics import r2_score","51","from sklearn.metrics.scorer import check_scoring","65","from sklearn.base import clone","272","def test_cross_validate_invalid_scoring_param():","273","    X, y = make_classification(random_state=0)","274","    estimator = MockClassifier()","275","","276","    # Test the errors","277","    error_message_regexp = \".*must be unique strings.*\"","278","","279","    # List\/tuple of callables should raise a message advising users to use","280","    # dict of names to callables mapping","281","    assert_raises_regex(ValueError, error_message_regexp,","282","                        cross_validate, estimator, X, y,","283","                        scoring=(make_scorer(precision_score),","284","                                 make_scorer(accuracy_score)))","285","    assert_raises_regex(ValueError, error_message_regexp,","286","                        cross_validate, estimator, X, y,","287","                        scoring=(make_scorer(precision_score),))","288","","289","    # So should empty lists\/tuples","290","    assert_raises_regex(ValueError, error_message_regexp + \"Empty list.*\",","291","                        cross_validate, estimator, X, y, scoring=())","292","","293","    # So should duplicated entries","294","    assert_raises_regex(ValueError, error_message_regexp + \"Duplicate.*\",","295","                        cross_validate, estimator, X, y,","296","                        scoring=('f1_micro', 'f1_micro'))","297","","298","    # Nested Lists should raise a generic error message","299","    assert_raises_regex(ValueError, error_message_regexp,","300","                        cross_validate, estimator, X, y,","301","                        scoring=[[make_scorer(precision_score)]])","302","","303","    error_message_regexp = (\".*should either be.*string or callable.*for \"","304","                            \"single.*.*dict.*for multi.*\")","305","","306","    # Empty dict should raise invalid scoring error","307","    assert_raises_regex(ValueError, \"An empty dict\",","308","                        cross_validate, estimator, X, y, scoring=(dict()))","309","","310","    # And so should any other invalid entry","311","    assert_raises_regex(ValueError, error_message_regexp,","312","                        cross_validate, estimator, X, y, scoring=5)","313","","314","    multiclass_scorer = make_scorer(precision_recall_fscore_support)","315","","316","    # Multiclass Scorers that return multiple values are not supported yet","317","    assert_raises_regex(ValueError,","318","                        \"Can't handle mix of binary and continuous\",","319","                        cross_validate, estimator, X, y,","320","                        scoring=multiclass_scorer)","321","    assert_raises_regex(ValueError,","322","                        \"Can't handle mix of binary and continuous\",","323","                        cross_validate, estimator, X, y,","324","                        scoring={\"foo\": multiclass_scorer})","325","","326","    multivalued_scorer = make_scorer(confusion_matrix)","327","","328","    # Multiclass Scorers that return multiple values are not supported yet","329","    assert_raises_regex(ValueError, \"scoring must return a number, got\",","330","                        cross_validate, SVC(), X, y,","331","                        scoring=multivalued_scorer)","332","    assert_raises_regex(ValueError, \"scoring must return a number, got\",","333","                        cross_validate, SVC(), X, y,","334","                        scoring={\"foo\": multivalued_scorer})","335","","336","    assert_raises_regex(ValueError, \"'mse' is not a valid scoring value.\",","337","                        cross_validate, SVC(), X, y, scoring=\"mse\")","338","","339","","340","def test_cross_validate():","341","    # Compute train and test mse\/r2 scores","342","    cv = KFold(n_splits=5)","343","","344","    # Regression","345","    X_reg, y_reg = make_regression(n_samples=30, random_state=0)","346","    reg = Ridge(random_state=0)","347","","348","    # Classification","349","    X_clf, y_clf = make_classification(n_samples=30, random_state=0)","350","    clf = SVC(kernel=\"linear\", random_state=0)","351","","352","    for X, y, est in ((X_reg, y_reg, reg), (X_clf, y_clf, clf)):","353","        # It's okay to evaluate regression metrics on classification too","354","        mse_scorer = check_scoring(est, 'neg_mean_squared_error')","355","        r2_scorer = check_scoring(est, 'r2')","356","        train_mse_scores = []","357","        test_mse_scores = []","358","        train_r2_scores = []","359","        test_r2_scores = []","360","        for train, test in cv.split(X, y):","361","            est = clone(reg).fit(X[train], y[train])","362","            train_mse_scores.append(mse_scorer(est, X[train], y[train]))","363","            train_r2_scores.append(r2_scorer(est, X[train], y[train]))","364","            test_mse_scores.append(mse_scorer(est, X[test], y[test]))","365","            test_r2_scores.append(r2_scorer(est, X[test], y[test]))","366","","367","        train_mse_scores = np.array(train_mse_scores)","368","        test_mse_scores = np.array(test_mse_scores)","369","        train_r2_scores = np.array(train_r2_scores)","370","        test_r2_scores = np.array(test_r2_scores)","371","","372","        scores = (train_mse_scores, test_mse_scores, train_r2_scores,","373","                  test_r2_scores)","374","","375","        yield check_cross_validate_single_metric, est, X, y, scores","376","        yield check_cross_validate_multi_metric, est, X, y, scores","377","","378","","379","def check_cross_validate_single_metric(clf, X, y, scores):","380","    (train_mse_scores, test_mse_scores, train_r2_scores,","381","     test_r2_scores) = scores","382","    # Test single metric evaluation when scoring is string or singleton list","383","    for (return_train_score, dict_len) in ((True, 4), (False, 3)):","384","        # Single metric passed as a string","385","        if return_train_score:","386","            # It must be True by default","387","            mse_scores_dict = cross_validate(clf, X, y, cv=5,","388","                                             scoring='neg_mean_squared_error')","389","            assert_array_almost_equal(mse_scores_dict['train_score'],","390","                                      train_mse_scores)","391","        else:","392","            mse_scores_dict = cross_validate(clf, X, y, cv=5,","393","                                             scoring='neg_mean_squared_error',","394","                                             return_train_score=False)","395","        assert_true(isinstance(mse_scores_dict, dict))","396","        assert_equal(len(mse_scores_dict), dict_len)","397","        assert_array_almost_equal(mse_scores_dict['test_score'],","398","                                  test_mse_scores)","399","","400","        # Single metric passed as a list","401","        if return_train_score:","402","            # It must be True by default","403","            r2_scores_dict = cross_validate(clf, X, y, cv=5, scoring=['r2'])","404","            assert_array_almost_equal(r2_scores_dict['train_r2'],","405","                                      train_r2_scores)","406","        else:","407","            r2_scores_dict = cross_validate(clf, X, y, cv=5, scoring=['r2'],","408","                                            return_train_score=False)","409","        assert_true(isinstance(r2_scores_dict, dict))","410","        assert_equal(len(r2_scores_dict), dict_len)","411","        assert_array_almost_equal(r2_scores_dict['test_r2'], test_r2_scores)","412","","413","","414","def check_cross_validate_multi_metric(clf, X, y, scores):","415","    # Test multimetric evaluation when scoring is a list \/ dict","416","    (train_mse_scores, test_mse_scores, train_r2_scores,","417","     test_r2_scores) = scores","418","    all_scoring = (('r2', 'neg_mean_squared_error'),","419","                   {'r2': make_scorer(r2_score),","420","                    'neg_mean_squared_error': 'neg_mean_squared_error'})","421","","422","    keys_sans_train = set(('test_r2', 'test_neg_mean_squared_error',","423","                           'fit_time', 'score_time'))","424","    keys_with_train = keys_sans_train.union(","425","        set(('train_r2', 'train_neg_mean_squared_error')))","426","","427","    for return_train_score in (True, False):","428","        for scoring in all_scoring:","429","            if return_train_score:","430","                # return_train_score must be True by default","431","                cv_results = cross_validate(clf, X, y, cv=5, scoring=scoring)","432","                assert_array_almost_equal(cv_results['train_r2'],","433","                                          train_r2_scores)","434","                assert_array_almost_equal(","435","                    cv_results['train_neg_mean_squared_error'],","436","                    train_mse_scores)","437","            else:","438","                cv_results = cross_validate(clf, X, y, cv=5, scoring=scoring,","439","                                            return_train_score=False)","440","            assert_true(isinstance(cv_results, dict))","441","            assert_equal(set(cv_results.keys()),","442","                         keys_with_train if return_train_score","443","                         else keys_sans_train)","444","            assert_array_almost_equal(cv_results['test_r2'], test_r2_scores)","445","            assert_array_almost_equal(","446","                cv_results['test_neg_mean_squared_error'], test_mse_scores)","447","","448","            # Make sure all the arrays are of np.ndarray type","449","            assert type(cv_results['test_r2']) == np.ndarray","450","            assert (type(cv_results['test_neg_mean_squared_error']) ==","451","                    np.ndarray)","452","            assert type(cv_results['fit_time'] == np.ndarray)","453","            assert type(cv_results['score_time'] == np.ndarray)","454","","455","            # Ensure all the times are within sane limits","456","            assert np.all(cv_results['fit_time'] >= 0)","457","            assert np.all(cv_results['fit_time'] < 10)","458","            assert np.all(cv_results['score_time'] >= 0)","459","            assert np.all(cv_results['score_time'] < 10)","460","","461","","586","        score = cross_val_score(clf, X, y, scoring=scoring, cv=3)","588","    # Test that score function is called only 3 times (for cv=3)"],"delete":["388","        score = cross_val_score(clf, X, y, scoring=scoring)"]}],"doc\/whats_new.rst":[{"add":["33","   - :class:`model_selection.GridSearchCV` and","34","     :class:`model_selection.RandomizedSearchCV` now support simultaneous","35","     evaluation of multiple metrics. Refer to the","36","     :ref:`multimetric_grid_search` section of the user guide for more","37","     information. :issue:`7388` by `Raghav RV`_","38","","39","   - Added the :func:`model_selection.cross_validate` which allows evaluation","40","     of multiple metrics. This function returns a dict with more useful","41","     information from cross-validation such as the train scores, fit times and","42","     score times.","43","     Refer to :ref:`multimetric_cross_validation` section of the userguide","44","     for more information. :issue:`7388` by `Raghav RV`_","45","     "],"delete":[]}],"doc\/modules\/model_evaluation.rst":[{"add":["212",".. _multimetric_scoring:","213","","214","Using mutiple metric evaluation","215","-------------------------------","216","","217","Scikit-learn also permits evaluation of multiple metrics in ``GridSearchCV``,","218","``RandomizedSearchCV`` and ``cross_validate``.","219","","220","There are two ways to specify multiple scoring metrics for the ``scoring``","221","parameter:","222","","223","- As an iterable of string metrics::","224","      >>> scoring = ['accuracy', 'precision']","225","","226","- As a ``dict`` mapping the scorer name to the scoring function::","227","      >>> from sklearn.metrics import accuracy_score","228","      >>> from sklearn.metrics import make_scorer","229","      >>> scoring = {'accuracy': make_scorer(accuracy_score),","230","      ...            'prec': 'precision'}","231","","232","Note that the dict values can either be scorer functions or one of the","233","predefined metric strings.","234","","235","Currently only those scorer functions that return a single score can be passed","236","inside the dict. Scorer functions that return multiple values are not","237","permitted and will require a wrapper to return a single metric::","238","","239","    >>> from sklearn.model_selection import cross_validate","240","    >>> from sklearn.metrics import confusion_matrix","241","    >>> # A sample toy binary classification dataset","242","    >>> X, y = datasets.make_classification(n_classes=2, random_state=0)","243","    >>> svm = LinearSVC(random_state=0)","244","    >>> tp = lambda y_true, y_pred: confusion_matrix(y_true, y_pred)[0, 0]","245","    >>> tn = lambda y_true, y_pred: confusion_matrix(y_true, y_pred)[0, 0]","246","    >>> fp = lambda y_true, y_pred: confusion_matrix(y_true, y_pred)[1, 0]","247","    >>> fn = lambda y_true, y_pred: confusion_matrix(y_true, y_pred)[0, 1]","248","    >>> scoring = {'tp' : make_scorer(tp), 'tn' : make_scorer(tn),","249","    ...            'fp' : make_scorer(fp), 'fn' : make_scorer(fn)}","250","    >>> cv_results = cross_validate(svm.fit(X, y), X, y, scoring=scoring)","251","    >>> # Getting the test set false positive scores","252","    >>> print(cv_results['test_tp'])          # doctest: +NORMALIZE_WHITESPACE","253","    [12 13 15]","254","    >>> # Getting the test set false negative scores","255","    >>> print(cv_results['test_fn'])          # doctest: +NORMALIZE_WHITESPACE","256","    [5 4 1]"],"delete":[]}],"sklearn\/model_selection\/_search.py":[{"add":["11","#         Raghav RV <rvraghav93@gmail.com>","28","from ._validation import _aggregate_score_dicts","38","from ..metrics.scorer import _check_multimetric_scoring","300","    scorer : callable or None","301","        The scorer callable object \/ function must have its signature as","304","        If ``None`` the estimator's default scorer is used.","305","","321","         Score of this parameter setting on given training \/ test split.","329","    # NOTE we are not using the return value as the scorer by itself should be","330","    # validated before. We use check_scoring only to reject multimetric scorer","331","    check_scoring(estimator, scorer)","332","    scores, n_samples_test = _fit_and_score(estimator, X, y,","333","                                            scorer, train,","334","                                            test, verbose, parameters,","335","                                            fit_params=fit_params,","336","                                            return_n_test_samples=True,","337","                                            error_score=error_score)","338","    return scores, parameters, n_samples_test","430","        self._check_is_fitted('score')","435","        score = self.scorer_[self.refit] if self.multimetric_ else self.scorer_","436","        return score(self.best_estimator_, X, y)","440","            raise NotFittedError('This %s instance was initialized '","441","                                 'with refit=False. %s is '","442","                                 'available only after refitting on the best '","443","                                 'parameters. You can refit an estimator '","444","                                 'manually using the ``best_parameters_`` '","445","                                 'attribute'","446","                                 % (type(self).__name__, method_name))","591","","592","        scorers, self.multimetric_ = _check_multimetric_scoring(","593","            self.estimator, scoring=self.scoring)","594","","595","        if self.multimetric_:","596","            if self.refit is not False and (","597","                    not isinstance(self.refit, six.string_types) or","598","                    # This will work for both dict \/ list (tuple)","599","                    self.refit not in scorers):","600","                raise ValueError(\"For multi-metric scoring, the parameter \"","601","                                 \"refit must be set to a scorer key \"","602","                                 \"to refit an estimator with the best \"","603","                                 \"parameter setting on the whole data and \"","604","                                 \"make the best_* attributes \"","605","                                 \"available for that metric. If this is not \"","606","                                 \"needed, refit should be set to False \"","607","                                 \"explicitly. %r was passed.\" % self.refit)","608","            else:","609","                refit_metric = self.refit","610","        else:","611","            refit_metric = 'score'","629","        )(delayed(_fit_and_score)(clone(base_estimator), X, y, scorers, train,","630","                                  test, self.verbose, parameters,","641","            (train_score_dicts, test_score_dicts, test_sample_counts, fit_time,","644","            (test_score_dicts, test_sample_counts, fit_time,","645","             score_time) = zip(*out)","646","","647","        # test_score_dicts and train_score dicts are lists of dictionaries and","648","        # we make them into dict of lists","649","        test_scores = _aggregate_score_dicts(test_score_dicts)","650","        if self.return_train_score:","651","            train_scores = _aggregate_score_dicts(train_score_dicts)","658","            # We want `array` to have `n_candidates` rows and `n_splits` cols.","663","                    # Uses closure to alter the results","699","        # NOTE test_sample counts (weights) remain the same for all candidates","700","        test_sample_counts = np.array(test_sample_counts[:n_splits],","701","                                      dtype=np.int)","702","        for scorer_name in scorers.keys():","703","            # Computed the (weighted) mean and std for test scores alone","704","            _store('test_%s' % scorer_name, test_scores[scorer_name],","705","                   splits=True, rank=True,","706","                   weights=test_sample_counts if self.iid else None)","707","            if self.return_train_score:","708","                _store('train_%s' % scorer_name, train_scores[scorer_name],","709","                       splits=True)","710","","711","        # For multi-metric evaluation, store the best_index_, best_params_ and","712","        # best_score_ iff refit is one of the scorer names","713","        # In single metric evaluation, refit_metric is \"score\"","714","        if self.refit or not self.multimetric_:","715","            self.best_index_ = results[\"rank_test_%s\" % refit_metric].argmin()","716","            self.best_params_ = candidate_params[self.best_index_]","717","            self.best_score_ = results[\"mean_test_%s\" % refit_metric][","718","                self.best_index_]","721","            self.best_estimator_ = clone(base_estimator).set_params(","722","                **self.best_params_)","724","                self.best_estimator_.fit(X, y, **fit_params)","726","                self.best_estimator_.fit(X, **fit_params)","727","","728","        # Store the only scorer not as a dict for single metric evaluation","729","        self.scorer_ = scorers if self.multimetric_ else scorers['score']","730","","731","        self.cv_results_ = results","732","        self.n_splits_ = n_splits","733","","738","        check_is_fitted(self, 'cv_results_')","739","        if self.multimetric_:","740","            raise AttributeError(\"grid_scores_ attribute is not available for\"","741","                                 \" multi-metric evaluation.\")","792","    scoring : string, callable, list\/tuple, dict or None, default: None","793","        A single string (see :ref:`scoring_parameter`) or a callable","794","        (see :ref:`scoring`) to evaluate the predictions on the test set.","795","","796","        For evaluating multiple metrics, either give a list of (unique) strings","797","        or a dict with names as keys and callables as values.","798","","799","        NOTE that when using custom scorers, each scorer should return a single","800","        value. Metric functions returning a list\/array of values can be wrapped","801","        into multiple scorers that return one value each.","802","","803","        See :ref:`multivalued_scorer_wrapping` for an example.","804","","805","        If None, the estimator's default scorer (if available) is used.","855","    refit : boolean, or string, default=True","856","        Refit an estimator using the best found parameters on the whole","857","        dataset.","858","","859","        For multiple metric evaluation, this needs to be a string denoting the","860","        scorer is used to find the best parameters for refitting the estimator","861","        at the end.","862","","863","        The refitted estimator is made available at the ``best_estimator_``","864","        attribute and permits using ``predict`` directly on this","865","        ``GridSearchCV`` instance.","866","","867","        Also for multiple metric evaluation, the attributes ``best_index_``,","868","        ``best_score_`` and ``best_parameters_`` will only be available if","869","        ``refit`` is set and all of them will be determined w.r.t this specific","870","        scorer.","871","","872","        See ``scoring`` parameter to know more about multiple metric","873","        evaluation.","926","        |param_kernel|param_gamma|param_degree|split0_test_score|...|..rank...|","962","        NOTE","963","","964","        The key ``'params'`` is used to store a list of parameter","965","        settings dicts for all the parameter candidates.","970","        For multi-metric evaluation, the scores for all the scorers are","971","        available in the ``cv_results_`` dict at the keys ending with that","972","        scorer's name (``'_<scorer_name>'``) instead of ``'_score'`` shown","973","        above. ('split0_test_precision', 'mean_train_precision' etc.)","974","","975","    best_estimator_ : estimator or dict","978","        on the left out data. Not available if ``refit=False``.","979","","980","        See ``refit`` parameter for more information on allowed values.","983","        Mean cross-validated score of the best_estimator","984","","985","        For multi-metric evaluation, this is present only if ``refit`` is","986","        specified.","991","        For multi-metric evaluation, this is present only if ``refit`` is","992","        specified.","993","","1002","        For multi-metric evaluation, this is present only if ``refit`` is","1003","        specified.","1004","","1005","    scorer_ : function or a dict","1009","        For multi-metric evaluation, this attribute holds the validated","1010","        ``scoring`` dict which maps the scorer key to the scorer callable.","1011","","1102","    scoring : string, callable, list\/tuple, dict or None, default: None","1103","        A single string (see :ref:`scoring_parameter`) or a callable","1104","        (see :ref:`scoring`) to evaluate the predictions on the test set.","1105","","1106","        For evaluating multiple metrics, either give a list of (unique) strings","1107","        or a dict with names as keys and callables as values.","1108","","1109","        NOTE that when using custom scorers, each scorer should return a single","1110","        value. Metric functions returning a list\/array of values can be wrapped","1111","        into multiple scorers that return one value each.","1112","","1113","        See :ref:`multivalued_scorer_wrapping` for an example.","1114","","1115","        If None, the estimator's default scorer (if available) is used.","1165","    refit : boolean, or string default=True","1166","        Refit an estimator using the best found parameters on the whole","1167","        dataset.","1168","","1169","        For multiple metric evaluation, this needs to be a string denoting the","1170","        scorer that would be used to find the best parameters for refitting","1171","        the estimator at the end.","1172","","1173","        The refitted estimator is made available at the ``best_estimator_``","1174","        attribute and permits using ``predict`` directly on this","1175","        ``RandomizedSearchCV`` instance.","1176","","1177","        Also for multiple metric evaluation, the attributes ``best_index_``,","1178","        ``best_score_`` and ``best_parameters_`` will only be available if","1179","        ``refit`` is set and all of them will be determined w.r.t this specific","1180","        scorer.","1181","","1182","        See ``scoring`` parameter to know more about multiple metric","1183","        evaluation.","1243","            'params'             : [{'kernel' : 'rbf', 'gamma' : 0.1}, ...],","1246","        NOTE","1247","","1248","        The key ``'params'`` is used to store a list of parameter","1249","        settings dicts for all the parameter candidates.","1254","        For multi-metric evaluation, the scores for all the scorers are","1255","        available in the ``cv_results_`` dict at the keys ending with that","1256","        scorer's name (``'_<scorer_name>'``) instead of ``'_score'`` shown","1257","        above. ('split0_test_precision', 'mean_train_precision' etc.)","1258","","1259","    best_estimator_ : estimator or dict","1262","        on the left out data. Not available if ``refit=False``.","1263","","1264","        For multi-metric evaluation, this attribute is present only if","1265","        ``refit`` is specified.","1266","","1267","        See ``refit`` parameter for more information on allowed values.","1270","        Mean cross-validated score of the best_estimator.","1271","","1272","        For multi-metric evaluation, this is not available if ``refit`` is","1273","        ``False``. See ``refit`` parameter for more information.","1278","        For multi-metric evaluation, this is not available if ``refit`` is","1279","        ``False``. See ``refit`` parameter for more information.","1280","","1289","        For multi-metric evaluation, this is not available if ``refit`` is","1290","        ``False``. See ``refit`` parameter for more information.","1291","","1292","    scorer_ : function or a dict","1296","        For multi-metric evaluation, this attribute holds the validated","1297","        ``scoring`` dict which maps the scorer key to the scorer callable.","1298",""],"delete":["297","    scorer : callable or None.","298","        If provided must be a scorer callable object \/ function with signature","316","        Score of this parameter setting on given training \/ test split.","324","    score, n_samples_test, _ = _fit_and_score(estimator, X, y, scorer, train,","325","                                              test, verbose, parameters,","326","                                              fit_params=fit_params,","327","                                              return_n_test_samples=True,","328","                                              error_score=error_score)","329","    return score, parameters, n_samples_test","425","        return self.scorer_(self.best_estimator_, X, y)","429","            raise NotFittedError(('This GridSearchCV instance was initialized '","430","                                  'with refit=False. %s is '","431","                                  'available only after refitting on the best '","432","                                  'parameters. ') % method_name)","577","        self.scorer_ = check_scoring(self.estimator, scoring=self.scoring)","595","        )(delayed(_fit_and_score)(clone(base_estimator), X, y, self.scorer_,","596","                                  train, test, self.verbose, parameters,","607","            (train_scores, test_scores, test_sample_counts, fit_time,","610","            (test_scores, test_sample_counts, fit_time, score_time) = zip(*out)","636","        # Computed the (weighted) mean and std for test scores alone","637","        # NOTE test_sample counts (weights) remain the same for all candidates","638","        test_sample_counts = np.array(test_sample_counts[:n_splits],","639","                                      dtype=np.int)","640","","641","        _store('test_score', test_scores, splits=True, rank=True,","642","               weights=test_sample_counts if self.iid else None)","643","        if self.return_train_score:","644","            _store('train_score', train_scores, splits=True)","647","","648","        best_index = np.flatnonzero(results[\"rank_test_score\"] == 1)[0]","649","        best_parameters = candidate_params[best_index]","650","","666","","670","        self.cv_results_ = results","671","        self.best_index_ = best_index","672","        self.n_splits_ = n_splits","675","            # fit the best estimator using the entire dataset","676","            # clone first to work around broken estimators","677","            best_estimator = clone(base_estimator).set_params(","678","                **best_parameters)","680","                best_estimator.fit(X, y, **fit_params)","682","                best_estimator.fit(X, **fit_params)","683","            self.best_estimator_ = best_estimator","687","    def best_params_(self):","688","        check_is_fitted(self, 'cv_results_')","689","        return self.cv_results_['params'][self.best_index_]","690","","691","    @property","692","    def best_score_(self):","693","        check_is_fitted(self, 'cv_results_')","694","        return self.cv_results_['mean_test_score'][self.best_index_]","695","","696","    @property","704","        check_is_fitted(self, 'cv_results_')","749","    scoring : string, callable or None, default=None","750","        A string (see model evaluation documentation) or","751","        a scorer callable object \/ function with signature","752","        ``scorer(estimator, X, y)``.","753","        If ``None``, the ``score`` method of the estimator is used.","803","    refit : boolean, default=True","804","        Refit the best estimator with the entire dataset.","805","        If \"False\", it is impossible to make predictions using","806","        this GridSearchCV instance after fitting.","859","        |param_kernel|param_gamma|param_degree|split0_test_score|...|rank_....|","895","        NOTE that the key ``'params'`` is used to store a list of parameter","896","        settings dict for all the parameter candidates.","901","    best_estimator_ : estimator","904","        on the left out data. Not available if refit=False.","907","        Score of best_estimator on the left out data.","920","    scorer_ : function","1014","    scoring : string, callable or None, default=None","1015","        A string (see model evaluation documentation) or","1016","        a scorer callable object \/ function with signature","1017","        ``scorer(estimator, X, y)``.","1018","        If ``None``, the ``score`` method of the estimator is used.","1068","    refit : boolean, default=True","1069","        Refit the best estimator with the entire dataset.","1070","        If \"False\", it is impossible to make predictions using","1071","        this RandomizedSearchCV instance after fitting.","1131","            'params' : [{'kernel' : 'rbf', 'gamma' : 0.1}, ...],","1134","        NOTE that the key ``'params'`` is used to store a list of parameter","1135","        settings dict for all the parameter candidates.","1140","    best_estimator_ : estimator","1143","        on the left out data. Not available if refit=False.","1146","        Score of best_estimator on the left out data.","1159","    scorer_ : function"]}]}},"74a9756fa784d1f22873ad23c8b4948c6e290108":{"changes":{"sklearn\/utils\/fixes.py":"MODIFY","sklearn\/utils\/tests\/test_fixes.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/feature_selection\/from_model.py":"MODIFY","sklearn\/feature_selection\/tests\/test_from_model.py":"MODIFY"},"diff":{"sklearn\/utils\/fixes.py":[{"add":["421","","422","if 'axis' not in signature(np.linalg.norm).parameters:","423","","424","    def norm(X, ord=None, axis=None):","425","        \"\"\"","426","        Handles the axis parameter for the norm function","427","        in old versions of numpy (useless for numpy >= 1.8).","428","        \"\"\"","429","","430","        if axis is None or X.ndim == 1:","431","            result = np.linalg.norm(X, ord=ord)","432","            return result","433","","434","        if axis not in (0, 1):","435","            raise NotImplementedError(\"\"\"","436","            The fix that adds axis parameter to the old numpy","437","            norm only works for 1D or 2D arrays.","438","            \"\"\")","439","","440","        if axis == 0:","441","            X = X.T","442","","443","        result = np.zeros(X.shape[0])","444","        for i in range(len(result)):","445","            result[i] = np.linalg.norm(X[i], ord=ord)","446","","447","        return result","448","","449","else:","450","    norm = np.linalg.norm"],"delete":[]}],"sklearn\/utils\/tests\/test_fixes.py":[{"add":["7","import math","19","from sklearn.utils.fixes import norm","70","","71","","72","def test_norm():","73","    X = np.array([[-2, 4, 5],","74","                  [1, 3, -4],","75","                  [0, 0, 8],","76","                  [0, 0, 0]]).astype(float)","77","","78","    # Test various axis and order","79","    assert_equal(math.sqrt(135), norm(X))","80","    assert_array_equal(","81","        np.array([math.sqrt(5), math.sqrt(25), math.sqrt(105)]),","82","        norm(X, axis=0)","83","    )","84","    assert_array_equal(np.array([3, 7, 17]), norm(X, axis=0, ord=1))","85","    assert_array_equal(np.array([2, 4, 8]), norm(X, axis=0, ord=np.inf))","86","    assert_array_equal(np.array([0, 0, 0]), norm(X, axis=0, ord=-np.inf))","87","    assert_array_equal(np.array([11, 8, 8, 0]), norm(X, axis=1, ord=1))","88","","89","    # Test shapes","90","    assert_equal((), norm(X).shape)","91","    assert_equal((3,), norm(X, axis=0).shape)","92","    assert_equal((4,), norm(X, axis=1).shape)"],"delete":[]}],"doc\/whats_new.rst":[{"add":["54","   - Added ``norm_order`` parameter to :class:`feature_selection.SelectFromModel`","55","     to enable selection of the norm order when ``coef_`` is more than 1D","56",""],"delete":[]}],"sklearn\/feature_selection\/from_model.py":[{"add":["12","from ..utils.fixes import norm","15","def _get_feature_importances(estimator, norm_order=1):","24","            importances = norm(estimator.coef_, axis=0, ord=norm_order)","175","    norm_order : non-zero int, inf, -inf, default 1","176","        Order of the norm used to filter the vectors of coefficients below","177","        ``threshold`` in the case where the ``coef_`` attribute of the","178","        estimator is of dimension 2.","179","","190","","191","    def __init__(self, estimator, threshold=None, prefit=False, norm_order=1):","195","        self.norm_order = norm_order","207","        scores = _get_feature_importances(estimator, self.norm_order)"],"delete":["14","def _get_feature_importances(estimator):","23","            importances = np.sum(np.abs(estimator.coef_), axis=0)","184","    def __init__(self, estimator, threshold=None, prefit=False):","199","        scores = _get_feature_importances(estimator)"]}],"sklearn\/feature_selection\/tests\/test_from_model.py":[{"add":["19","from sklearn.utils.fixes import norm","105","@skip_if_32bit","106","def test_feature_importances_2d_coef():","107","    X, y = datasets.make_classification(","108","        n_samples=1000, n_features=10, n_informative=3, n_redundant=0,","109","        n_repeated=0, shuffle=False, random_state=0, n_classes=4)","110","","111","    est = LogisticRegression()","112","    for threshold, func in zip([\"mean\", \"median\"], [np.mean, np.median]):","113","        for order in [1, 2, np.inf]:","114","            # Fit SelectFromModel a multi-class problem","115","            transformer = SelectFromModel(estimator=LogisticRegression(),","116","                                          threshold=threshold,","117","                                          norm_order=order)","118","            transformer.fit(X, y)","119","            assert_true(hasattr(transformer.estimator_, 'coef_'))","120","            X_new = transformer.transform(X)","121","            assert_less(X_new.shape[1], X.shape[1])","122","","123","            # Manually check that the norm is correctly performed","124","            est.fit(X, y)","125","            importances = norm(est.coef_, axis=0, ord=order)","126","            feature_mask = importances > func(importances)","127","            assert_array_equal(X_new, X[:, feature_mask])","128","","129",""],"delete":[]}]}},"277b058713f64f55e787aac55fe0e1bbbd47576f":{"changes":{"sklearn\/ensemble\/bagging.py":"MODIFY","sklearn\/ensemble\/tests\/test_bagging.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/utils\/__init__.py":"MODIFY","sklearn\/ensemble\/tests\/test_iforest.py":"MODIFY"},"diff":{"sklearn\/ensemble\/bagging.py":[{"add":["22","from ..utils import indices_to_mask","36","def _generate_indices(random_state, bootstrap, n_population, n_samples):","37","    \"\"\"Draw randomly sampled indices.\"\"\"","38","    # Draw sample indices","39","    if bootstrap:","40","        indices = random_state.randint(0, n_population, n_samples)","41","    else:","42","        indices = sample_without_replacement(n_population, n_samples,","43","                                             random_state=random_state)","44","","45","    return indices","46","","47","","48","def _generate_bagging_indices(random_state, bootstrap_features,","49","                              bootstrap_samples, n_features, n_samples,","50","                              max_features, max_samples):","51","    \"\"\"Randomly draw feature and sample indices.\"\"\"","52","    # Get valid random state","53","    random_state = check_random_state(random_state)","54","","55","    # Draw indices","56","    feature_indices = _generate_indices(random_state, bootstrap_features,","57","                                        n_features, max_features)","58","    sample_indices = _generate_indices(random_state, bootstrap_samples,","59","                                       n_samples, max_samples)","60","","61","    return feature_indices, sample_indices","62","","63","","65","                               seeds, total_n_estimators, verbose):","69","    max_features = ensemble._max_features","70","    max_samples = ensemble._max_samples","87","        random_state = np.random.RandomState(seeds[i])","90","        try:  # Not all estimators accept a random_state","91","            estimator.set_params(random_state=seeds[i])","95","        # Draw random feature, sample indices","96","        features, indices = _generate_bagging_indices(random_state,","97","                                                      bootstrap_features,","98","                                                      bootstrap, n_features,","99","                                                      n_samples, max_features,","100","                                                      max_samples)","113","                not_indices_mask = ~indices_to_mask(indices, n_samples)","114","                curr_sample_weight[not_indices_mask] = 0","125","    return estimators, estimators_features","253","    def _fit(self, X, y, max_samples=None, max_depth=None, sample_weight=None):","291","        self._n_samples = n_samples","300","        # Validate max_samples","301","        if max_samples is None:","302","            max_samples = self.max_samples","303","        elif not isinstance(max_samples, (numbers.Integral, np.integer)):","309","        # Store validated integer row sampling value","310","        self._max_samples = max_samples","311","","312","        # Validate max_features","321","        # Store validated integer feature sampling value","322","        self._max_features = max_features","323","","324","        # Other checks","364","        self._seeds = seeds","382","            t[1] for t in all_results))","397","    def _get_estimators_indices(self):","398","        # Get drawn indices along both sample and feature axes","399","        for seed in self._seeds:","400","            # Operations accessing random_state must be performed identically","401","            # to those in `_parallel_build_estimators()`","402","            random_state = np.random.RandomState(seed)","403","            feature_indices, sample_indices = _generate_bagging_indices(","404","                random_state, self.bootstrap_features, self.bootstrap,","405","                self.n_features_, self._n_samples, self._max_features,","406","                self._max_samples)","407","","408","            yield feature_indices, sample_indices","409","","410","    @property","411","    def estimators_samples_(self):","412","        \"\"\"The subset of drawn samples for each base estimator.","413","","414","        Returns a dynamically generated list of boolean masks identifying","415","        the samples used for for fitting each member of the ensemble, i.e.,","416","        the in-bag samples.","417","","418","        Note: the list is re-created at each call to the property in order","419","        to reduce the object memory footprint by not storing the sampling","420","        data. Thus fetching the property may be slower than expected.","421","        \"\"\"","422","        sample_masks = []","423","        for _, sample_indices in self._get_estimators_indices():","424","            mask = indices_to_mask(sample_indices, self._n_samples)","425","            sample_masks.append(mask)","426","","427","        return sample_masks","428","","512","        estimator. Each subset is defined by a boolean mask.","580","        n_samples = y.shape[0]","589","            # Create mask for OOB samples","590","            mask = ~samples","875","        estimator. Each subset is defined by a boolean mask.","982","            # Create mask for OOB samples","983","            mask = ~samples"],"delete":["36","                               max_samples, seeds, total_n_estimators, verbose):","40","    max_features = ensemble.max_features","41","","42","    if (not isinstance(max_samples, (numbers.Integral, np.integer)) and","43","            (0.0 < max_samples <= 1.0)):","44","        max_samples = int(max_samples * n_samples)","45","","46","    if (not isinstance(max_features, (numbers.Integral, np.integer)) and","47","            (0.0 < max_features <= 1.0)):","48","        max_features = int(max_features * n_features)","49","","59","    estimators_samples = []","67","        random_state = check_random_state(seeds[i])","68","        seed = random_state.randint(MAX_INT)","71","        try:  # Not all estimator accept a random_state","72","            estimator.set_params(random_state=seed)","76","        # Draw features","77","        if bootstrap_features:","78","            features = random_state.randint(0, n_features, max_features)","79","        else:","80","            features = sample_without_replacement(n_features,","81","                                                  max_features,","82","                                                  random_state=random_state)","92","                indices = random_state.randint(0, n_samples, max_samples)","95","","97","                not_indices = sample_without_replacement(","98","                    n_samples,","99","                    n_samples - max_samples,","100","                    random_state=random_state)","101","","102","                curr_sample_weight[not_indices] = 0","105","            samples = curr_sample_weight > 0.","109","            if bootstrap:","110","                indices = random_state.randint(0, n_samples, max_samples)","111","            else:","112","                indices = sample_without_replacement(n_samples,","113","                                                     max_samples,","114","                                                     random_state=random_state)","115","","116","            sample_counts = bincount(indices, minlength=n_samples)","117","","119","            samples = sample_counts > 0.","122","        estimators_samples.append(samples)","125","    return estimators, estimators_samples, estimators_features","253","    def _fit(self, X, y, max_samples, max_depth=None, sample_weight=None):","299","        # if max_samples is float:","300","        if not isinstance(max_samples, (numbers.Integral, np.integer)):","328","            self.estimators_samples_ = []","362","                max_samples,","371","        self.estimators_samples_ += list(itertools.chain.from_iterable(","372","            t[1] for t in all_results))","374","            t[2] for t in all_results))","472","        estimator.","542","        n_samples = y.shape[0]","549","            mask = np.ones(n_samples, dtype=np.bool)","550","            mask[samples] = False","835","        estimator.","942","            mask = np.ones(n_samples, dtype=np.bool)","943","            mask[samples] = False"]}],"sklearn\/ensemble\/tests\/test_bagging.py":[{"add":["665","","666","","667","def test_oob_score_consistency():","668","    # Make sure OOB scores are identical when random_state, estimator, and ","669","    # training data are fixed and fitting is done twice","670","    X, y = make_hastie_10_2(n_samples=200, random_state=1)","671","    bagging = BaggingClassifier(KNeighborsClassifier(), max_samples=0.5,","672","                                max_features=0.5, oob_score=True,","673","                                random_state=1)","674","    assert_equal(bagging.fit(X, y).oob_score_, bagging.fit(X, y).oob_score_)","675","","676","","677","def test_estimators_samples():","678","    # Check that format of estimators_samples_ is correct and that results","679","    # generated at fit time can be identically reproduced at a later time","680","    # using data saved in object attributes.","681","    X, y = make_hastie_10_2(n_samples=200, random_state=1)","682","    bagging = BaggingClassifier(LogisticRegression(), max_samples=0.5,","683","                                max_features=0.5, random_state=1,","684","                                bootstrap=False)","685","    bagging.fit(X, y)","686","","687","    # Get relevant attributes","688","    estimators_samples = bagging.estimators_samples_","689","    estimators_features = bagging.estimators_features_","690","    estimators = bagging.estimators_","691","","692","    # Test for correct formatting","693","    assert_equal(len(estimators_samples), len(estimators))","694","    assert_equal(len(estimators_samples[0]), len(X))","695","    assert_equal(estimators_samples[0].dtype.kind, 'b')","696","","697","    # Re-fit single estimator to test for consistent sampling","698","    estimator_index = 0","699","    estimator_samples = estimators_samples[estimator_index]","700","    estimator_features = estimators_features[estimator_index]","701","    estimator = estimators[estimator_index]","702","","703","    X_train = (X[estimator_samples])[:, estimator_features]","704","    y_train = y[estimator_samples]","705","","706","    orig_coefs = estimator.coef_","707","    estimator.fit(X_train, y_train)","708","    new_coefs = estimator.coef_","709","","710","    assert_array_almost_equal(orig_coefs, new_coefs)","711","","712","","713","def test_max_samples_consistency():","714","    # Make sure validated max_samples and original max_samples are identical","715","    # when valid integer max_samples supplied by user","716","    max_samples = 100","717","    X, y = make_hastie_10_2(n_samples=2*max_samples, random_state=1)","718","    bagging = BaggingClassifier(KNeighborsClassifier(),","719","                                max_samples=max_samples,","720","                                max_features=0.5, random_state=1)","721","    bagging.fit(X, y)","722","    assert_equal(bagging._max_samples, max_samples)"],"delete":[]}],"doc\/whats_new.rst":[{"add":["259","   - The memory footprint is reduced (sometimes greatly) for :class:`BaseBagging`","260","     and classes that inherit from it, i.e, :class:`BaggingClassifier`,","261","     :class:`BaggingRegressor`, and :class:`IsolationForest`, by dynamically","262","     generating attribute ``estimators_samples_`` only when it is needed.","263","     By `David Staub`_.","264","","265","","4370","","4371",".. _David Staub: https:\/\/github.com\/staubda"],"delete":[]}],"sklearn\/utils\/__init__.py":[{"add":["34","           \"check_symmetric\", \"indices_to_mask\"]","421","","422","","423","def indices_to_mask(indices, mask_length):","424","    \"\"\"Convert list of indices to boolean mask.","425","","426","    Parameters","427","    ----------","428","    indices : list-like","429","        List of integers treated as indices.","430","    mask_length : int","431","        Length of boolean mask to be generated.","432","","433","    Returns","434","    -------","435","    mask : 1d boolean nd-array","436","        Boolean array that is True where indices are present, else False.","437","    \"\"\"","438","    if mask_length <= np.max(indices):","439","        raise ValueError(\"mask_length must be greater than max(indices)\")","440","","441","    mask = np.zeros(mask_length, dtype=np.bool)","442","    mask[indices] = True","443","","444","    return mask"],"delete":["34","           \"check_symmetric\"]"]}],"sklearn\/ensemble\/tests\/test_iforest.py":[{"add":["195","","196","","197","def test_max_samples_consistency():","198","    # Make sure validated max_samples in iforest and BaseBagging are identical","199","    X = iris.data","200","    clf = IsolationForest().fit(X)","201","    assert_equal(clf.max_samples_, clf._max_samples)"],"delete":[]}]}},"fe03879cd32207c5ed671a71a82df64802e06d0e":{"changes":{"sklearn\/model_selection\/_split.py":"MODIFY","sklearn\/model_selection\/_validation.py":"MODIFY","sklearn\/calibration.py":"MODIFY","sklearn\/cross_validation.py":"MODIFY","sklearn\/feature_selection\/rfe.py":"MODIFY","sklearn\/learning_curve.py":"MODIFY","sklearn\/grid_search.py":"MODIFY","sklearn\/linear_model\/ridge.py":"MODIFY","sklearn\/model_selection\/_search.py":"MODIFY"},"diff":{"sklearn\/model_selection\/_split.py":[{"add":["1387","        binary or multiclass, :class:`StratifiedKFold` is used. In all other"],"delete":["1387","        binary or multiclass, :class:`StratifiedKFold` used. In all other"]}],"sklearn\/model_selection\/_validation.py":[{"add":["100","        either binary or multiclass, :class:`StratifiedKFold` is used. In all","348","        either binary or multiclass, :class:`StratifiedKFold` is used. In all","564","        either binary or multiclass, :class:`StratifiedKFold` is used. In all","704","        either binary or multiclass, :class:`StratifiedKFold` is used. In all","917","        either binary or multiclass, :class:`StratifiedKFold` is used. In all"],"delete":["100","        either binary or multiclass, :class:`StratifiedKFold` used. In all","348","        either binary or multiclass, :class:`StratifiedKFold` used. In all","564","        either binary or multiclass, :class:`StratifiedKFold` used. In all","704","        either binary or multiclass, :class:`StratifiedKFold` used. In all","917","        either binary or multiclass, :class:`StratifiedKFold` used. In all"]}],"sklearn\/calibration.py":[{"add":["65","        :class:`sklearn.model_selection.StratifiedKFold` is used. If ``y`` ","66","        is neither binary nor multiclass, :class:`sklearn.model_selection.KFold` ","67","        is used."],"delete":["65","        :class:`StratifiedKFold` used. If ``y`` is neither binary nor","66","        multiclass, :class:`KFold` is used."]}],"sklearn\/cross_validation.py":[{"add":["1228","        either binary or multiclass, :class:`StratifiedKFold` is used. In all","1413","        either binary or multiclass, :class:`StratifiedKFold` is used. In all","1699","        multiclass, :class:`StratifiedKFold` is used. In all other cases,","1773","        either binary or multiclass, :class:`StratifiedKFold` is used. In all"],"delete":["1228","        either binary or multiclass, :class:`StratifiedKFold` used. In all","1413","        either binary or multiclass, :class:`StratifiedKFold` used. In all","1699","        multiclass, :class:`StratifiedKFold` used. In all other cases,","1773","        either binary or multiclass, :class:`StratifiedKFold` used. In all"]}],"sklearn\/feature_selection\/rfe.py":[{"add":["295","        :class:`sklearn.model_selection.StratifiedKFold` is used. If the ","296","        estimator is a classifier or if ``y`` is neither binary nor multiclass, ","297","        :class:`sklearn.model_selection.KFold` is used."],"delete":["295","        :class:`StratifiedKFold` used. If the estimator is a classifier","296","        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used."]}],"sklearn\/learning_curve.py":[{"add":["77","        either binary or multiclass, ","78","        :class:`sklearn.model_selection.StratifiedKFold` is used. In all","79","        other cases, :class:`sklearn.model_selection.KFold` is used.","291","        either binary or multiclass, ","292","        :class:`sklearn.model_selection.StratifiedKFold` is used. In all","293","        other cases, :class:`sklearn.model_selection.KFold` is used."],"delete":["77","        either binary or multiclass, :class:`StratifiedKFold` used. In all","78","        other cases, :class:`KFold` is used.","290","        either binary or multiclass, :class:`StratifiedKFold` used. In all","291","        other cases, :class:`KFold` is used."]}],"sklearn\/grid_search.py":[{"add":["689","        either binary or multiclass, ","690","        :class:`sklearn.model_selection.StratifiedKFold` is used. In all","691","        other cases, :class:`sklearn.model_selection.KFold` is used.","901","        either binary or multiclass, ","902","        :class:`sklearn.model_selection.StratifiedKFold` is used. In all","903","        other cases, :class:`sklearn.model_selection.KFold` is used."],"delete":["689","        either binary or multiclass, :class:`StratifiedKFold` used. In all","690","        other cases, :class:`KFold` is used.","900","        either binary or multiclass, :class:`StratifiedKFold` used. In all","901","        other cases, :class:`KFold` is used."]}],"sklearn\/linear_model\/ridge.py":[{"add":["1133","        :class:`sklearn.model_selection.StratifiedKFold` is used, else, ","1134","        :class:`sklearn.model_selection.KFold` is used."],"delete":["1133","        :class:`StratifiedKFold` used, else, :class:`KFold` is used."]}],"sklearn\/model_selection\/_search.py":[{"add":["708","        either binary or multiclass, :class:`StratifiedKFold` is used. In all","964","        either binary or multiclass, :class:`StratifiedKFold` is used. In all"],"delete":["708","        either binary or multiclass, :class:`StratifiedKFold` used. In all","964","        either binary or multiclass, :class:`StratifiedKFold` used. In all"]}]}},"bd99858a92858e1fde0d4eb678b0d35591ebca01":{"changes":{"sklearn\/model_selection\/_validation.py":"MODIFY","sklearn\/cross_validation.py":"MODIFY"},"diff":{"sklearn\/model_selection\/_validation.py":[{"add":["138","    Examples","139","    --------","140","    >>> from sklearn import datasets, linear_model","141","    >>> from sklearn.cross_validation import cross_val_score","142","    >>> diabetes = datasets.load_diabetes()","143","    >>> X = diabetes.data[:150]","144","    >>> y = diabetes.target[:150]","145","    >>> lasso = linear_model.Lasso()","146","    >>> print(cross_val_score(lasso, X, y))  # doctest: +ELLIPSIS","147","    [ 0.33150734  0.08022311  0.03531764]","148","","377","","378","    Examples","379","    --------","380","    >>> from sklearn import datasets, linear_model","381","    >>> from sklearn.cross_validation import cross_val_predict","382","    >>> diabetes = datasets.load_diabetes()","383","    >>> X = diabetes.data[:150]","384","    >>> y = diabetes.target[:150]","385","    >>> lasso = linear_model.Lasso()","386","    >>> y_pred = cross_val_predict(lasso, X, y)"],"delete":[]}],"sklearn\/cross_validation.py":[{"add":["1265","","1266","    Examples","1267","    --------","1268","    >>> from sklearn import datasets, linear_model","1269","    >>> from sklearn.cross_validation import cross_val_predict","1270","    >>> diabetes = datasets.load_diabetes()","1271","    >>> X = diabetes.data[:150]","1272","    >>> y = diabetes.target[:150]","1273","    >>> lasso = linear_model.Lasso()","1274","    >>> y_pred = cross_val_predict(lasso, X, y)","1451","    Examples","1452","    --------","1453","    >>> from sklearn import datasets, linear_model","1454","    >>> from sklearn.cross_validation import cross_val_score","1455","    >>> diabetes = datasets.load_diabetes()","1456","    >>> X = diabetes.data[:150]","1457","    >>> y = diabetes.target[:150]","1458","    >>> lasso = linear_model.Lasso()","1459","    >>> print(cross_val_score(lasso, X, y))  # doctest:  +ELLIPSIS","1460","    [ 0.33150734  0.08022311  0.03531764]","1461",""],"delete":[]}]}},"3b4ae2845aeb9f02059b1a345f46a03312123201":{"changes":{"build_tools\/circle\/build_doc.sh":"MODIFY"},"diff":{"build_tools\/circle\/build_doc.sh":[{"add":["47","  cython nose coverage matplotlib sphinx pillow","48","source activate testenv"],"delete":["47","  cython nose coverage matplotlib sphinx pillow psutil","48","source \/home\/ubuntu\/miniconda\/envs\/testenv\/bin\/activate testenv"]}]}},"0493d714a0a7fde4c26e92f4f5b7d2ea968e7ea1":{"changes":{"doc\/modules\/preprocessing.rst":"MODIFY"},"diff":{"doc\/modules\/preprocessing.rst":[{"add":["32","than others, it might dominate the objective function and make the"],"delete":["32","that others, it might dominate the objective function and make the"]}]}},"e2a2b4d403742c3eb2d0e085631a014b0975d6af":{"changes":{"examples\/linear_model\/plot_ard.py":"MODIFY","sklearn\/linear_model\/sag_fast.pyx":"MODIFY","examples\/ensemble\/plot_ensemble_oob.py":"MODIFY","examples\/hetero_feature_union.py":"MODIFY","sklearn\/linear_model\/sag.py":"MODIFY","benchmarks\/bench_plot_randomized_svd.py":"MODIFY","sklearn\/linear_model\/randomized_l1.py":"MODIFY","examples\/calibration\/plot_calibration_multiclass.py":"MODIFY","examples\/linear_model\/plot_lasso_coordinate_descent_path.py":"MODIFY","sklearn\/linear_model\/sgd_fast.pyx":"MODIFY"},"diff":{"examples\/linear_model\/plot_ard.py":[{"add":["41","# Create noise with a precision alpha of 50."],"delete":["41","# Create noite with a precision alpha of 50."]}],"sklearn\/linear_model\/sag_fast.pyx":[{"add":["266","    # the maximum change in weights, used to compute stopping criteria","268","    # a holder variable for the max weight, used to compute stopping criteria"],"delete":["266","    # the maximum change in weights, used to compute stopping criterea","268","    # a holder variable for the max weight, used to compute stopping criterea"]}],"examples\/ensemble\/plot_ensemble_oob.py":[{"add":["43","# support for parallelized ensembles but is necessary for tracking the OOB"],"delete":["43","# support for paralellised ensembles but is necessary for tracking the OOB"]}],"examples\/hetero_feature_union.py":[{"add":["167","# limit the list of categories to make running this example faster."],"delete":["167","# limit the list of categories to make running this exmaple faster."]}],"sklearn\/linear_model\/sag.py":[{"add":["112","        criteria is not reached. Defaults to 1000.","115","        The stopping criteria for the weights. The iterations will stop when"],"delete":["112","        criterea is not reached. Defaults to 1000.","115","        The stopping criterea for the weights. The iterations will stop when"]}],"benchmarks\/bench_plot_randomized_svd.py":[{"add":["375","    title = \"%s: Frobenius norm diff vs n power iteration\" % (dataset_name)"],"delete":["375","    title = \"%s: frobenius norm diff vs n power iteration\" % (dataset_name)"]}],"sklearn\/linear_model\/randomized_l1.py":[{"add":["548","    # Sort alphas in ascending order","566","    \"\"\"Stability path based on randomized Lasso estimates"],"delete":["548","    # Sort alphas in assending order","566","    \"\"\"Stabiliy path based on randomized Lasso estimates"]}],"examples\/calibration\/plot_calibration_multiclass.py":[{"add":["147","# Plot modifications of calibrator"],"delete":["147","# Ploit modifications of calibrator"]}],"examples\/linear_model\/plot_lasso_coordinate_descent_path.py":[{"add":["42","print(\"Computing regularization path using the positive elastic net...\")"],"delete":["42","print(\"Computing regularization path using the positve elastic net...\")"]}],"sklearn\/linear_model\/sgd_fast.pyx":[{"add":["484","        (4) Passive Aggressive-I, eta = min(alpha, loss\/norm(x))","485","        (5) Passive Aggressive-II, eta = 1.0 \/ (norm(x) + 0.5*alpha)","505","        The averaged weights across iterations","507","        The averaged intercept across iterations"],"delete":["484","        (4) Passive Agressive-I, eta = min(alpha, loss\/norm(x))","485","        (5) Passive Agressive-II, eta = 1.0 \/ (norm(x) + 0.5*alpha)","505","        The averaged weights accross iterations","507","        The averaged intercept accross iterations"]}]}},"1a8ddbede3ec810cb29c222fcd4332e82523e9f3":{"changes":{"sklearn\/mixture\/tests\/test_dpgmm.py":"MODIFY","sklearn\/mixture\/dpgmm.py":"MODIFY","sklearn\/mixture\/tests\/test_gmm.py":"MODIFY"},"diff":{"sklearn\/mixture\/tests\/test_dpgmm.py":[{"add":["9","from sklearn.utils.testing import assert_warns_message, ignore_warnings","12","from sklearn.mixture.dpgmm import digamma, gammaln","13","from sklearn.mixture.dpgmm import wishart_log_det, wishart_logz","14","","19","@ignore_warnings(category=DeprecationWarning)","37","@ignore_warnings(category=DeprecationWarning)","67","@ignore_warnings(category=DeprecationWarning)","83","@ignore_warnings(category=DeprecationWarning)","99","@ignore_warnings(category=DeprecationWarning)","100","def test_digamma():","101","    assert_warns_message(DeprecationWarning, \"The function digamma is\"","102","                         \" deprecated in 0.18 and will be removed in 0.20. \"","103","                         \"Use scipy.special.digamma instead.\", digamma, 3)","104","","105","","106","@ignore_warnings(category=DeprecationWarning)","107","def test_gammaln():","108","    assert_warns_message(DeprecationWarning, \"The function gammaln\"","109","                         \" is deprecated in 0.18 and will be removed\"","110","                         \" in 0.20. Use scipy.special.gammaln instead.\",","111","                         gammaln, 3)","112","","113","","114","@ignore_warnings(category=DeprecationWarning)","118","    result = assert_warns_message(DeprecationWarning, \"The function \"","119","                                  \"log_normalize is deprecated in 0.18 and\"","120","                                  \" will be removed in 0.20.\",","121","                                  log_normalize, a)","122","    assert np.allclose(v, result, rtol=0.01)","123","","124","","125","@ignore_warnings(category=DeprecationWarning)","126","def test_wishart_log_det():","127","    a = np.array([0.1, 0.8, 0.01, 0.09])","128","    b = np.array([0.2, 0.7, 0.05, 0.1])","129","    assert_warns_message(DeprecationWarning, \"The function \"","130","                         \"wishart_log_det is deprecated in 0.18 and\"","131","                         \" will be removed in 0.20.\",","132","                         wishart_log_det, a, b, 2, 4)","133","","134","","135","@ignore_warnings(category=DeprecationWarning)","136","def test_wishart_logz():","137","    assert_warns_message(DeprecationWarning, \"The function \"","138","                         \"wishart_logz is deprecated in 0.18 and \"","139","                         \"will be removed in 0.20.\", wishart_logz,","140","                         3, np.identity(3), 1, 3)","141","","142","","143","@ignore_warnings(category=DeprecationWarning)","144","def test_DPGMM_deprecation():","145","    assert_warns_message(DeprecationWarning, \"The DPGMM class is\"","146","                         \" not working correctly and it's better \"","147","                         \"to not use it. DPGMM is deprecated in 0.18 \"","148","                         \"and will be removed in 0.20.\", DPGMM)","184","@ignore_warnings(category=DeprecationWarning)","185","def test_VBGMM_deprecation():","186","    assert_warns_message(DeprecationWarning, \"The VBGMM class is\"","187","                         \" not working correctly and it's better\"","188","                         \" to not use it. VBGMM is deprecated in 0.18\"","189","                         \" and will be removed in 0.20.\", VBGMM)","190","","191",""],"delete":["94","    assert np.allclose(v, log_normalize(a), rtol=0.01)"]}],"sklearn\/mixture\/dpgmm.py":[{"add":["18","from ..utils import check_random_state, check_array, deprecated","25","@deprecated(\"The function digamma is deprecated in 0.18 and \"","26","            \"will be removed in 0.20. Use scipy.special.digamma instead.\")","31","@deprecated(\"The function gammaln is deprecated in 0.18 and \"","32","            \"will be removed in 0.20. Use scipy.special.gammaln instead.\")","37","@deprecated(\"The function log_normalize is deprecated in 0.18 and \"","38","            \"will be removed in 0.20.\")","51","@deprecated(\"The function wishart_log_det is deprecated in 0.18 and \"","52","            \"will be removed in 0.20.\")","63","@deprecated(\"The function wishart_logz is deprecated in 0.18 and \"","64","            \"will be removed in 0.20.\")","118","@deprecated(\"The DPGMM class is not working correctly and it's better \"","119","            \"to not use it. DPGMM is deprecated in 0.18 and \"","120","            \"will be removed in 0.20.\")","621","@deprecated(\"The VBGMM class is not working correctly and it's better\"","622","            \" to not use it. VBGMM is deprecated in 0.18 and \"","623","            \"will be removed in 0.20.\")"],"delete":["18","from ..utils import check_random_state, check_array"]}],"sklearn\/mixture\/tests\/test_gmm.py":[{"add":["206","        with ignore_warnings(category=DeprecationWarning):","207","            ll, responsibilities = g.score_samples(X)","228","        with ignore_warnings(category=DeprecationWarning):","229","            samples = g.sample(n)","237","        with ignore_warnings(category=DeprecationWarning):","238","            g.weights_ = self.weights","239","            g.means_ = self.means","240","            g.covars_ = 20 * self.covars[self.covariance_type]","243","        with ignore_warnings(category=DeprecationWarning):","244","            X = g.sample(n_samples=100)","245","            g = self.model(n_components=self.n_components,","246","                           covariance_type=self.covariance_type,","247","                           random_state=rng, min_covar=1e-1,","248","                           n_iter=1, init_params=params)","249","            g.fit(X)","255","        with ignore_warnings(category=DeprecationWarning):","256","            for _ in range(5):","257","                g.params = params","258","                g.init_params = ''","259","                g.fit(X)","260","                trainll.append(self.score(g, X))","261","            g.n_iter = 10","263","            g.params = params","264","            g.fit(X)  # finish fitting","271","        with ignore_warnings(category=DeprecationWarning):","272","            delta_min = np.diff(trainll).min()","291","        with ignore_warnings(category=DeprecationWarning):","292","            g.fit(X)","293","            trainll = g.score(X)","308","        with ignore_warnings(category=DeprecationWarning):","309","            g.fit(X)","310","            trainll = g.score(X)","311","            if isinstance(g, mixture.DPGMM):","312","                self.assertTrue(np.sum(np.abs(trainll \/ 100)) < 5)","313","            else:","314","                self.assertTrue(np.sum(np.abs(trainll \/ 100)) < 2)","319","        with ignore_warnings(category=DeprecationWarning):","320","            return g.score(X).sum()","355","    with ignore_warnings(category=DeprecationWarning):","356","        train1 = g.fit(X).score(X).sum()","357","        g.n_init = 5","358","        train2 = g.fit(X).score(X).sum()","369","        with ignore_warnings(category=DeprecationWarning):","370","            g = mixture.GMM(n_components=n_components, covariance_type=cv_type,","371","                            random_state=rng, min_covar=1e-7, n_iter=1)","372","            g.fit(X)","373","            assert_true(g._n_parameters() == n_params[cv_type])","385","    with ignore_warnings(category=DeprecationWarning):","386","        g_full.fit(X)","387","        g_full_bic = g_full.bic(X)","388","        for cv_type in ['tied', 'diag', 'spherical']:","389","            g = mixture.GMM(n_components=n_components, covariance_type=cv_type,","390","                            random_state=rng, min_covar=1e-7, n_iter=1)","391","            g.fit(X)","392","            assert_array_almost_equal(g.bic(X), g_full_bic)"],"delete":["206","        ll, responsibilities = g.score_samples(X)","227","        samples = g.sample(n)","235","        g.weights_ = self.weights","236","        g.means_ = self.means","237","        g.covars_ = 20 * self.covars[self.covariance_type]","240","        X = g.sample(n_samples=100)","241","        g = self.model(n_components=self.n_components,","242","                       covariance_type=self.covariance_type,","243","                       random_state=rng, min_covar=1e-1,","244","                       n_iter=1, init_params=params)","245","        g.fit(X)","251","        for _ in range(5):","252","            g.params = params","254","            g.fit(X)","255","            trainll.append(self.score(g, X))","256","        g.n_iter = 10","257","        g.init_params = ''","258","        g.params = params","259","        g.fit(X)  # finish fitting","266","        delta_min = np.diff(trainll).min()","285","        g.fit(X)","286","        trainll = g.score(X)","301","        g.fit(X)","302","        trainll = g.score(X)","303","        if isinstance(g, mixture.DPGMM):","304","            self.assertTrue(np.sum(np.abs(trainll \/ 100)) < 5)","305","        else:","306","            self.assertTrue(np.sum(np.abs(trainll \/ 100)) < 2)","311","        return g.score(X).sum()","346","    train1 = g.fit(X).score(X).sum()","347","    g.n_init = 5","348","    train2 = g.fit(X).score(X).sum()","359","        g = mixture.GMM(n_components=n_components, covariance_type=cv_type,","360","                        random_state=rng, min_covar=1e-7, n_iter=1)","361","        g.fit(X)","362","        assert_true(g._n_parameters() == n_params[cv_type])","374","    g_full.fit(X)","375","    g_full_bic = g_full.bic(X)","376","    for cv_type in ['tied', 'diag', 'spherical']:","377","        g = mixture.GMM(n_components=n_components, covariance_type=cv_type,","378","                        random_state=rng, min_covar=1e-7, n_iter=1)","379","        g.fit(X)","380","        assert_array_almost_equal(g.bic(X), g_full_bic)"]}]}},"65b7d7a8913e362239eb8fd4a673787f21411ef4":{"changes":{"sklearn\/mixture\/tests\/test_gaussian_mixture.py":"MODIFY","sklearn\/mixture\/base.py":"MODIFY","sklearn\/mixture\/gaussian_mixture.py":"MODIFY"},"diff":{"sklearn\/mixture\/tests\/test_gaussian_mixture.py":[{"add":["0","# Author: Wei Xue <xuewei4d@gmail.com>","1","#         Thierry Guillemot <thierry.guillemot.work@gmail.com>","2","# License: BSD 3 clauseimport warnings","3","","20","    _estimate_gaussian_covariances_spherical)","21","from sklearn.mixture.gaussian_mixture import _compute_precision_cholesky","22","from sklearn.mixture.gaussian_mixture import _compute_log_det_cholesky","392","","440","","441","def test_compute_log_det_cholesky():","442","    n_features = 2","443","    rand_data = RandomData(np.random.RandomState(0))","444","","445","    for covar_type in COVARIANCE_TYPE:","446","        covariance = rand_data.covariances[covar_type]","447","","448","        if covar_type == 'full':","449","            predected_det = np.array([linalg.det(cov) for cov in covariance])","450","        elif covar_type == 'tied':","451","            predected_det = linalg.det(covariance)","452","        elif covar_type == 'diag':","453","            predected_det = np.array([np.prod(cov) for cov in covariance])","454","        elif covar_type == 'spherical':","455","            predected_det = covariance ** n_features","456","","457","        # We compute the cholesky decomposition of the covariance matrix","458","        expected_det = _compute_log_det_cholesky(_compute_precision_cholesky(","459","            covariance, covar_type), covar_type, n_features=n_features)","460","        assert_array_almost_equal(expected_det, - .5 * np.log(predected_det))","461","","462","","472","    from sklearn.mixture.gaussian_mixture import _estimate_log_gaussian_prob","489","    log_prob = _estimate_log_gaussian_prob(X, means, precs_full, 'full')","494","    log_prob = _estimate_log_gaussian_prob(X, means, precs_chol_diag, 'diag')","503","    log_prob = _estimate_log_gaussian_prob(X, means, precs_tied, 'tied')","513","    log_prob = _estimate_log_gaussian_prob(X, means,","514","                                           precs_spherical, 'spherical')","751","    g = GaussianMixture(n_components=n_components, n_init=1, max_iter=2,","752","                        reg_covar=0, random_state=random_state,","754","    h = GaussianMixture(n_components=n_components, n_init=1, max_iter=1,","755","                        reg_covar=0, random_state=random_state,","854","            for _ in range(600):","866","            assert_true(gmm.converged_)","867","","901","                              covariance_type=covar_type, random_state=rng,","902","                              n_init=5)","905","        if covar_type == 'full':","909","        elif covar_type == 'tied':"],"delete":["16","    _estimate_gaussian_covariances_spherical,","17","    _compute_precision_cholesky)","443","    from sklearn.mixture.gaussian_mixture import (","444","        _estimate_log_gaussian_prob_full,","445","        _estimate_log_gaussian_prob_tied,","446","        _estimate_log_gaussian_prob_diag,","447","        _estimate_log_gaussian_prob_spherical)","464","    log_prob = _estimate_log_gaussian_prob_full(X, means, precs_full)","469","    log_prob = _estimate_log_gaussian_prob_diag(X, means, precs_chol_diag)","478","    log_prob = _estimate_log_gaussian_prob_tied(X, means, precs_tied)","488","    log_prob = _estimate_log_gaussian_prob_spherical(X, means, precs_spherical)","725","    g = GaussianMixture(n_components=n_components, n_init=1,","726","                        max_iter=2, reg_covar=0, random_state=random_state,","728","    h = GaussianMixture(n_components=n_components, n_init=1,","729","                        max_iter=1, reg_covar=0, random_state=random_state,","828","            for _ in range(300):","873","                              covariance_type=covar_type, random_state=rng)","876","        if covar_type is 'full':","880","        elif covar_type is 'tied':"]}],"sklearn\/mixture\/base.py":[{"add":["4","# License: BSD 3 clause","139","        n_samples, _ = X.shape","148","            resp = random_state.rand(n_samples, self.n_components)","194","        max_lower_bound = -np.infty","197","        n_samples, _ = X.shape","203","                self.lower_bound_ = np.infty","206","                prev_lower_bound = self.lower_bound_","208","                log_prob_norm, log_resp = self._e_step(X)","209","                self._m_step(X, log_resp)","210","                self.lower_bound_ = self._compute_lower_bound(","211","                    log_resp, log_prob_norm)","212","","213","                change = self.lower_bound_ - prev_lower_bound","220","            self._print_verbose_msg_init_end(self.lower_bound_)","222","            if self.lower_bound_ > max_lower_bound:","223","                max_lower_bound = self.lower_bound_","249","        log_prob_norm : array, shape (n_samples,)","250","            log p(X)","252","        log_responsibility : array, shape (n_samples, n_components)","253","            logarithm of the responsibilities","258","    def _m_step(self, X, log_resp):","265","        log_resp : array-like, shape (n_samples, n_components)","351","        _, log_resp = self._estimate_log_prob_resp(X)","417","        return log_prob_norm, log_resp"],"delete":["138","        n_samples = X.shape[0]","147","            resp = random_state.rand(X.shape[0], self.n_components)","193","        max_log_likelihood = -np.infty","201","            current_log_likelihood, resp = self._e_step(X)","204","                prev_log_likelihood = current_log_likelihood","206","                self._m_step(X, resp)","207","                current_log_likelihood, resp = self._e_step(X)","208","                change = current_log_likelihood - prev_log_likelihood","215","            self._print_verbose_msg_init_end(current_log_likelihood)","217","            if current_log_likelihood > max_log_likelihood:","218","                max_log_likelihood = current_log_likelihood","244","        log-likelihood : scalar","246","        responsibility : array, shape (n_samples, n_components)","251","    def _m_step(self, X, resp):","258","        resp : array-like, shape (n_samples, n_components)","344","        _, _, log_resp = self._estimate_log_prob_resp(X)","402","        log_prob : array, shape (n_samples, n_components)","403","            log p(X|Z) + log weights","404","","413","        return log_prob_norm, weighted_log_prob, log_resp"]}],"sklearn\/mixture\/gaussian_mixture.py":[{"add":["4","# License: BSD 3 clause","14","from ..utils.extmath import row_norms","340","def _compute_log_det_cholesky(matrix_chol, covariance_type, n_features):","341","    \"\"\"Compute the log-det of the cholesky decomposition of matrices.","342","","343","    Parameters","344","    ----------","345","    matrix_chol : array-like,","346","        Cholesky decompositions of the matrices.","347","        'full' : shape of (n_components, n_features, n_features)","348","        'tied' : shape of (n_features, n_features)","349","        'diag' : shape of (n_components, n_features)","350","        'spherical' : shape of (n_components,)","351","","352","    covariance_type : {'full', 'tied', 'diag', 'spherical'}","353","","354","    n_features : int","355","        Number of features.","356","","357","    Returns","358","    -------","359","    log_det_precision_chol : array-like, shape (n_components,)","360","        The determinant of the cholesky decomposition.","361","        matrix.","362","    \"\"\"","363","    if covariance_type == 'full':","364","        n_components, _, _ = matrix_chol.shape","365","        log_det_chol = (np.sum(np.log(","366","            matrix_chol.reshape(","367","                n_components, -1)[:, ::n_features + 1]), 1))","368","","369","    elif covariance_type == 'tied':","370","        log_det_chol = (np.sum(np.log(np.diag(matrix_chol))))","371","","372","    elif covariance_type == 'diag':","373","        log_det_chol = (np.sum(np.log(matrix_chol), axis=1))","374","","375","    else:","376","        log_det_chol = n_features * (np.log(matrix_chol))","377","","378","    return log_det_chol","379","","380","","381","def _estimate_log_gaussian_prob(X, means, precisions_chol, covariance_type):","382","    \"\"\"Estimate the log Gaussian probability.","390","    precisions_chol : array-like,","392","        'full' : shape of (n_components, n_features, n_features)","393","        'tied' : shape of (n_features, n_features)","394","        'diag' : shape of (n_components, n_features)","395","        'spherical' : shape of (n_components,)","396","","397","    covariance_type : {'full', 'tied', 'diag', 'spherical'}","405","    # det(precision_chol) is half of det(precision)","406","    log_det = _compute_log_det_cholesky(","407","        precisions_chol, covariance_type, n_features)","409","    if covariance_type == 'full':","410","        log_prob = np.empty((n_samples, n_components))","411","        for k, (mu, prec_chol) in enumerate(zip(means, precisions_chol)):","412","            y = np.dot(X, prec_chol) - np.dot(mu, prec_chol)","413","            log_prob[:, k] = np.sum(np.square(y), axis=1)","415","    elif covariance_type == 'tied':","416","        log_prob = np.empty((n_samples, n_components))","417","        for k, mu in enumerate(means):","418","            y = np.dot(X, precisions_chol) - np.dot(mu, precisions_chol)","419","            log_prob[:, k] = np.sum(np.square(y), axis=1)","421","    elif covariance_type == 'diag':","422","        precisions = precisions_chol ** 2","423","        log_prob = (np.sum((means ** 2 * precisions), 1) -","424","                    2. * np.dot(X, (means * precisions).T) +","425","                    np.dot(X ** 2, precisions.T))","427","    elif covariance_type == 'spherical':","428","        precisions = precisions_chol ** 2","429","        log_prob = (np.sum(means ** 2, 1) * precisions -","430","                    2 * np.dot(X, means.T * precisions) +","431","                    np.outer(row_norms(X, squared=True), precisions))","432","    return -.5 * (n_features * np.log(2 * np.pi) + log_prob) + log_det","468","        The number of initializations to perform. The best results are kept.","556","","557","    lower_bound_ : float","558","        Log-likelihood of the best fit of EM.","634","        log_prob_norm, log_resp = self._estimate_log_prob_resp(X)","647","        return _estimate_log_gaussian_prob(","648","            X, self.means_, self.precisions_cholesky_, self.covariance_type)","653","    def _compute_lower_bound(self, _, log_prob_norm):","654","        return log_prob_norm","655","","660","        return (self.weights_, self.means_, self.covariances_,","661","                self.precisions_cholesky_)","664","        (self.weights_, self.means_, self.covariances_,","665","         self.precisions_cholesky_) = params"],"delete":["338","def _estimate_log_gaussian_prob_full(X, means, precisions_chol):","339","    \"\"\"Estimate the log Gaussian probability for 'full' precision.","347","    precisions_chol : array-like, shape (n_components, n_features, n_features)","356","    log_prob = np.empty((n_samples, n_components))","357","    for k, (mu, prec_chol) in enumerate(zip(means, precisions_chol)):","358","        log_det = -2. * np.sum(np.log(np.diagonal(prec_chol)))","359","        y = np.dot(X - mu, prec_chol)","360","        log_prob[:, k] = -.5 * (n_features * np.log(2. * np.pi) + log_det +","361","                                np.sum(np.square(y), axis=1))","362","    return log_prob","365","def _estimate_log_gaussian_prob_tied(X, means, precision_chol):","366","    \"\"\"Estimate the log Gaussian probability for 'tied' precision.","368","    Parameters","369","    ----------","370","    X : array-like, shape (n_samples, n_features)","372","    means : array-like, shape (n_components, n_features)","373","","374","    precision_chol : array-like, shape (n_features, n_features)","375","        Cholesky decomposition of the precision matrix.","376","","377","    Returns","378","    -------","379","    log_prob : array-like, shape (n_samples, n_components)","380","    \"\"\"","381","    n_samples, n_features = X.shape","382","    n_components, _ = means.shape","383","    log_prob = np.empty((n_samples, n_components))","384","    log_det = -2. * np.sum(np.log(np.diagonal(precision_chol)))","385","    for k, mu in enumerate(means):","386","        y = np.dot(X - mu, precision_chol)","387","        log_prob[:, k] = np.sum(np.square(y), axis=1)","388","    log_prob = -.5 * (n_features * np.log(2. * np.pi) + log_det + log_prob)","389","    return log_prob","390","","391","","392","def _estimate_log_gaussian_prob_diag(X, means, precisions_chol):","393","    \"\"\"Estimate the log Gaussian probability for 'diag' precision.","394","","395","    Parameters","396","    ----------","397","    X : array-like, shape (n_samples, n_features)","398","","399","    means : array-like, shape (n_components, n_features)","400","","401","    precisions_chol : array-like, shape (n_components, n_features)","402","        Cholesky decompositions of the precision matrices.","403","","404","    Returns","405","    -------","406","    log_prob : array-like, shape (n_samples, n_components)","407","    \"\"\"","408","    n_samples, n_features = X.shape","409","    precisions = precisions_chol ** 2","410","    log_prob = -.5 * (n_features * np.log(2. * np.pi) -","411","                      np.sum(np.log(precisions), 1) +","412","                      np.sum((means ** 2 * precisions), 1) -","413","                      2. * np.dot(X, (means * precisions).T) +","414","                      np.dot(X ** 2, precisions.T))","415","    return log_prob","416","","417","","418","def _estimate_log_gaussian_prob_spherical(X, means, precisions_chol):","419","    \"\"\"Estimate the log Gaussian probability for 'spherical' precision.","420","","421","    Parameters","422","    ----------","423","    X : array-like, shape (n_samples, n_features)","424","","425","    means : array-like, shape (n_components, n_features)","426","","427","    precisions_chol : array-like, shape (n_components, )","428","        Cholesky decompositions of the precision matrices.","429","","430","    Returns","431","    -------","432","    log_prob : array-like, shape (n_samples, n_components)","433","    \"\"\"","434","    n_samples, n_features = X.shape","435","    precisions = precisions_chol ** 2","436","    log_prob = -.5 * (n_features * np.log(2 * np.pi) -","437","                      n_features * np.log(precisions) +","438","                      np.sum(means ** 2, 1) * precisions -","439","                      2 * np.dot(X, means.T * precisions) +","440","                      np.outer(np.sum(X ** 2, axis=1), precisions))","441","    return log_prob","477","        The number of initializations to perform. The best results is kept.","640","        log_prob_norm, _, log_resp = self._estimate_log_prob_resp(X)","653","        return {\"full\": _estimate_log_gaussian_prob_full,","654","                \"tied\": _estimate_log_gaussian_prob_tied,","655","                \"diag\": _estimate_log_gaussian_prob_diag,","656","                \"spherical\": _estimate_log_gaussian_prob_spherical","657","                }[self.covariance_type](X, self.means_,","658","                                        self.precisions_cholesky_)","667","        return self.weights_, self.means_, self.precisions_cholesky_","670","        self.weights_, self.means_, self.precisions_cholesky_ = params"]}]}},"12d5f078313f280eddfc590296d51473257ea8d2":{"changes":{"sklearn\/utils\/_random.pyx":"MODIFY","sklearn\/linear_model\/tests\/test_logistic.py":"MODIFY"},"diff":{"sklearn\/utils\/_random.pyx":[{"add":[],"delete":["0","# cython: cdivision=True"]}],"sklearn\/linear_model\/tests\/test_logistic.py":[{"add":["632","        penalty=\"l1\", tol=1e-5)","635","        solver=\"liblinear\", fit_intercept=False, penalty=\"l1\", tol=1e-5)"],"delete":["632","        penalty=\"l1\")","635","        solver=\"liblinear\", fit_intercept=False, penalty=\"l1\")"]}]}},"a2dac46e71c3b6e1cae3fa8f83867dbb2ddd2487":{"changes":{"doc\/tutorial\/basic\/tutorial.rst":"MODIFY","doc\/tutorial\/statistical_inference\/supervised_learning.rst":"MODIFY"},"diff":{"doc\/tutorial\/basic\/tutorial.rst":[{"add":["310","(an integer array) was used in ``fit``. The second ``predict()`` returns a string"],"delete":["310","(an integer array) was used in ``fit``. The second ``predict`` returns a string"]}],"doc\/tutorial\/statistical_inference\/supervised_learning.rst":[{"add":["189","    >>> # between X and y."],"delete":["189","    >>> # between X and Y."]}]}},"da118d0cb406c5a968cdeef6416c7b681022329c":{"changes":{"sklearn\/tree\/tree.py":"MODIFY","sklearn\/ensemble\/gradient_boosting.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/ensemble\/forest.py":"MODIFY","sklearn\/tree\/tests\/test_tree.py":"MODIFY"},"diff":{"sklearn\/tree\/tree.py":[{"add":["303","        if sample_weight is None:","304","            min_weight_leaf = (self.min_weight_fraction_leaf *","305","                               n_samples)","306","        else:","595","        The minimum weighted fraction of the sum total of weights (of all","596","        the input samples) required to be at a leaf node. Samples have","597","        equal weight when sample_weight is not provided.","866","        The minimum weighted fraction of the sum total of weights (of all","867","        the input samples) required to be at a leaf node. Samples have","868","        equal weight when sample_weight is not provided."],"delete":["303","        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:","306","        else:","307","            min_weight_leaf = 0.","594","        The minimum weighted fraction of the input samples required to be at a","595","        leaf node.","864","        The minimum weighted fraction of the input samples required to be at a","865","        leaf node."]}],"sklearn\/ensemble\/gradient_boosting.py":[{"add":["1332","        The minimum weighted fraction of the sum total of weights (of all","1333","        the input samples) required to be at a leaf node. Samples have","1334","        equal weight when sample_weight is not provided.","1701","        The minimum weighted fraction of the sum total of weights (of all","1702","        the input samples) required to be at a leaf node. Samples have","1703","        equal weight when sample_weight is not provided."],"delete":["1332","        The minimum weighted fraction of the input samples required to be at a","1333","        leaf node.","1700","        The minimum weighted fraction of the input samples required to be at a","1701","        leaf node."]}],"doc\/whats_new.rst":[{"add":["26","   - The ``min_weight_fraction_leaf`` parameter of tree-based classifiers and","27","     regressors now assumes uniform sample weights by default if the","28","     ``sample_weight`` argument is not passed to the ``fit`` function.","29","     Previously, the parameter was silently ignored. (`#7301","30","     <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/7301>`_) by `Nelson","31","     Liu`_.","32",""],"delete":[]}],"sklearn\/ensemble\/forest.py":[{"add":["809","        The minimum weighted fraction of the sum total of weights (of all","810","        the input samples) required to be at a leaf node. Samples have","811","        equal weight when sample_weight is not provided.","1021","        The minimum weighted fraction of the sum total of weights (of all","1022","        the input samples) required to be at a leaf node. Samples have","1023","        equal weight when sample_weight is not provided.","1193","        The minimum weighted fraction of the sum total of weights (of all","1194","        the input samples) required to be at a leaf node. Samples have","1195","        equal weight when sample_weight is not provided.","1404","        The minimum weighted fraction of the sum total of weights (of all","1405","        the input samples) required to be at a leaf node. Samples have","1406","        equal weight when sample_weight is not provided.","1562","        The minimum weighted fraction of the sum total of weights (of all","1563","        the input samples) required to be at a leaf node. Samples have","1564","        equal weight when sample_weight is not provided."],"delete":["809","        The minimum weighted fraction of the input samples required to be at a","810","        leaf node.","1020","        The minimum weighted fraction of the input samples required to be at a","1021","        leaf node.","1191","        The minimum weighted fraction of the input samples required to be at a","1192","        leaf node.","1401","        The minimum weighted fraction of the input samples required to be at a","1402","        leaf node.","1558","        The minimum weighted fraction of the input samples required to be at a","1559","        leaf node."]}],"sklearn\/tree\/tests\/test_tree.py":[{"add":["672","    # test case with no weights passed in","673","    total_weight = X.shape[0]","674","","675","    for max_leaf_nodes, frac in product((None, 1000), np.linspace(0, 0.5, 6)):","676","        est = TreeEstimator(min_weight_fraction_leaf=frac,","677","                            max_leaf_nodes=max_leaf_nodes,","678","                            random_state=0)","679","        est.fit(X, y)","680","","681","        if sparse:","682","            out = est.tree_.apply(X.tocsr())","683","        else:","684","            out = est.tree_.apply(X)","685","","686","        node_weights = np.bincount(out)","687","        # drop inner nodes","688","        leaf_weights = node_weights[node_weights != 0]","689","        assert_greater_equal(","690","            np.min(leaf_weights),","691","            total_weight * est.min_weight_fraction_leaf,","692","            \"Failed with {0} \"","693","            \"min_weight_fraction_leaf={1}\".format(","694","                name, est.min_weight_fraction_leaf))","695","","707","def check_min_weight_fraction_leaf_with_min_samples_leaf(name, datasets,","708","                                                         sparse=False):","709","    \"\"\"Test the interaction between min_weight_fraction_leaf and min_samples_leaf","710","    when sample_weights is not provided in fit.\"\"\"","711","    if sparse:","712","        X = DATASETS[datasets][\"X_sparse\"].astype(np.float32)","713","    else:","714","        X = DATASETS[datasets][\"X\"].astype(np.float32)","715","    y = DATASETS[datasets][\"y\"]","716","","717","    total_weight = X.shape[0]","718","    TreeEstimator = ALL_TREES[name]","719","    for max_leaf_nodes, frac in product((None, 1000), np.linspace(0, 0.5, 3)):","720","        # test integer min_samples_leaf","721","        est = TreeEstimator(min_weight_fraction_leaf=frac,","722","                            max_leaf_nodes=max_leaf_nodes,","723","                            min_samples_leaf=5,","724","                            random_state=0)","725","        est.fit(X, y)","726","","727","        if sparse:","728","            out = est.tree_.apply(X.tocsr())","729","        else:","730","            out = est.tree_.apply(X)","731","","732","        node_weights = np.bincount(out)","733","        # drop inner nodes","734","        leaf_weights = node_weights[node_weights != 0]","735","        assert_greater_equal(","736","            np.min(leaf_weights),","737","            max((total_weight *","738","                 est.min_weight_fraction_leaf), 5),","739","            \"Failed with {0} \"","740","            \"min_weight_fraction_leaf={1}, \"","741","            \"min_samples_leaf={2}\".format(name,","742","                                          est.min_weight_fraction_leaf,","743","                                          est.min_samples_leaf))","744","    for max_leaf_nodes, frac in product((None, 1000), np.linspace(0, 0.5, 3)):","745","        # test float min_samples_leaf","746","        est = TreeEstimator(min_weight_fraction_leaf=frac,","747","                            max_leaf_nodes=max_leaf_nodes,","748","                            min_samples_leaf=.1,","749","                            random_state=0)","750","        est.fit(X, y)","751","","752","        if sparse:","753","            out = est.tree_.apply(X.tocsr())","754","        else:","755","            out = est.tree_.apply(X)","756","","757","        node_weights = np.bincount(out)","758","        # drop inner nodes","759","        leaf_weights = node_weights[node_weights != 0]","760","        assert_greater_equal(","761","            np.min(leaf_weights),","762","            max((total_weight * est.min_weight_fraction_leaf),","763","                (total_weight * est.min_samples_leaf)),","764","            \"Failed with {0} \"","765","            \"min_weight_fraction_leaf={1}, \"","766","            \"min_samples_leaf={2}\".format(name,","767","                                          est.min_weight_fraction_leaf,","768","                                          est.min_samples_leaf))","769","","770","","771","def test_min_weight_fraction_leaf_with_min_samples_leaf():","772","    # Check on dense input","773","    for name in ALL_TREES:","774","        yield (check_min_weight_fraction_leaf_with_min_samples_leaf,","775","               name, \"iris\")","776","","777","    # Check on sparse input","778","    for name in SPARSE_TREES:","779","        yield (check_min_weight_fraction_leaf_with_min_samples_leaf,","780","               name, \"multilabel\", True)","781","","782",""],"delete":[]}]}},"9b2aac9e5c8749243c73f2377519d2f2c407b095":{"changes":{"sklearn\/linear_model\/tests\/test_theil_sen.py":"MODIFY","doc\/datasets\/labeled_faces_fixture.py":"MODIFY","sklearn\/preprocessing\/tests\/test_function_transformer.py":"MODIFY","sklearn\/utils\/tests\/test_bench.py":"MODIFY","sklearn\/utils\/tests\/test_murmurhash.py":"MODIFY","sklearn\/utils\/tests\/test_testing.py":"MODIFY","sklearn\/svm\/tests\/test_bounds.py":"MODIFY","sklearn\/tests\/test_discriminant_analysis.py":"MODIFY","sklearn\/utils\/tests\/test_fixes.py":"MODIFY","sklearn\/utils\/testing.py":"MODIFY","sklearn\/utils\/tests\/test_fast_dict.py":"MODIFY","sklearn\/neighbors\/tests\/test_dist_metrics.py":"MODIFY","sklearn\/feature_selection\/tests\/test_rfe.py":"MODIFY","sklearn\/cross_decomposition\/tests\/test_pls.py":"MODIFY","sklearn\/feature_extraction\/tests\/test_text.py":"MODIFY","sklearn\/feature_selection\/tests\/test_from_model.py":"MODIFY","doc\/datasets\/twenty_newsgroups_fixture.py":"MODIFY","sklearn\/utils\/tests\/test_metaestimators.py":"MODIFY","sklearn\/feature_extraction\/tests\/test_feature_hasher.py":"MODIFY","sklearn\/tree\/tests\/test_export.py":"MODIFY","sklearn\/datasets\/mldata.py":"MODIFY","sklearn\/mixture\/tests\/test_gmm.py":"MODIFY","sklearn\/gaussian_process\/tests\/test_gaussian_process.py":"MODIFY","sklearn\/metrics\/cluster\/tests\/test_supervised.py":"MODIFY","sklearn\/utils\/sparsetools\/tests\/test_traversal.py":"MODIFY","sklearn\/svm\/tests\/test_svm.py":"MODIFY","sklearn\/svm\/tests\/test_sparse.py":"MODIFY","sklearn\/manifold\/tests\/test_mds.py":"MODIFY","sklearn\/decomposition\/tests\/test_fastica.py":"MODIFY","sklearn\/semi_supervised\/tests\/test_label_propagation.py":"MODIFY","sklearn\/ensemble\/tests\/test_gradient_boosting_loss_functions.py":"MODIFY","sklearn\/utils\/tests\/test_seq_dataset.py":"MODIFY","sklearn\/ensemble\/tests\/test_base.py":"MODIFY","sklearn\/feature_selection\/tests\/test_base.py":"MODIFY","sklearn\/feature_extraction\/tests\/test_image.py":"MODIFY","sklearn\/linear_model\/tests\/test_least_angle.py":"MODIFY","sklearn\/manifold\/tests\/test_locally_linear.py":"MODIFY","sklearn\/manifold\/tests\/test_spectral_embedding.py":"MODIFY","sklearn\/utils\/tests\/test_validation.py":"MODIFY","sklearn\/datasets\/tests\/test_base.py":"MODIFY"},"diff":{"sklearn\/linear_model\/tests\/test_theil_sen.py":[{"add":["21","from sklearn.utils.testing import (","22","        assert_almost_equal, assert_greater, assert_less, raises,","23",")"],"delete":["17","from nose.tools import raises, assert_almost_equal","22","from sklearn.utils.testing import assert_greater, assert_less"]}],"doc\/datasets\/labeled_faces_fixture.py":[{"add":["8","from sklearn.utils.testing import SkipTest"],"delete":["7","from nose import SkipTest"]}],"sklearn\/preprocessing\/tests\/test_function_transformer.py":[{"add":["4","from sklearn.utils.testing import assert_equal, assert_array_equal","23","    assert_array_equal(","50","    assert_array_equal(","80","    assert_array_equal(","92","    assert_array_equal(F.transform(X),","93","                       np.around(X, decimals=3))","104","    assert_array_equal(F.transform(X), np.around(X, decimals=1))","115","    assert_array_equal(F.transform(X), np.around(X, decimals=1))","123","        func=np.sqrt,","124","        inverse_func=np.around, inv_kw_args=dict(decimals=3),","125","    )","126","    assert_array_equal(","127","        F.inverse_transform(F.transform(X)),","128","        np.around(np.sqrt(X), decimals=3),","129","    )"],"delete":["0","from nose.tools import assert_equal","23","    testing.assert_array_equal(","50","    testing.assert_array_equal(","80","    testing.assert_array_equal(","92","    testing.assert_array_equal(F.transform(X),","93","                                  np.around(X, decimals=3))","104","    testing.assert_array_equal(F.transform(X),","105","                                  np.around(X, decimals=1))","116","    testing.assert_array_equal(F.transform(X),","117","                               np.around(X, decimals=1))","125","            func=np.sqrt,","126","            inverse_func=np.around, inv_kw_args=dict(decimals=3))","127","    testing.assert_array_equal(","128","            F.inverse_transform(F.transform(X)),","129","            np.around(np.sqrt(X), decimals=3))"]}],"sklearn\/utils\/tests\/test_bench.py":[{"add":["4","from sklearn.utils.testing import assert_equal"],"delete":["4","from nose.tools import assert_equal"]}],"sklearn\/utils\/tests\/test_murmurhash.py":[{"add":["9","from sklearn.utils.testing import assert_equal, assert_true"],"delete":["9","from nose.tools import assert_equal, assert_true"]}],"sklearn\/utils\/tests\/test_testing.py":[{"add":["5","    assert_raises,"],"delete":["4","from nose.tools import assert_raises","5",""]}],"sklearn\/svm\/tests\/test_bounds.py":[{"add":["9","from sklearn.utils.testing import assert_true, raises","10","from sklearn.utils.testing import assert_raise_message","11","","67","@raises(ValueError)","74","@raises(ValueError)"],"delete":["0","import nose","1","from nose.tools import assert_equal, assert_true","2","from sklearn.utils.testing import clean_warning_registry","3","from sklearn.utils.testing import assert_raise_message","68","@nose.tools.raises(ValueError)","75","@nose.tools.raises(ValueError)"]}],"sklearn\/tests\/test_discriminant_analysis.py":[{"add":["13","from sklearn.utils.testing import SkipTest"],"delete":["2","from nose import SkipTest"]}],"sklearn\/utils\/tests\/test_fixes.py":[{"add":["11","from sklearn.utils.testing import assert_equal, assert_false, assert_true"],"delete":["7","from nose.tools import assert_equal","8","from nose.tools import assert_false","9","from nose.tools import assert_true","12",""]}],"sklearn\/utils\/testing.py":[{"add":["38","import unittest","70","           \"assert_approx_equal\", \"SkipTest\"]","71","","72","","73","_dummy = unittest.TestCase('__init__')","74","assert_equal = _dummy.assertEqual","75","assert_not_equal = _dummy.assertNotEqual","76","assert_true = _dummy.assertTrue","77","assert_false = _dummy.assertFalse","78","assert_raises = _dummy.assertRaises","79","","80","try:","81","    SkipTest = unittest.case.SkipTest","82","except AttributeError:","83","    # Python <= 2.6, we stil need nose here","84","    from nose import SkipTest","88","    assert_dict_equal = _dummy.assertDictEqual","89","    assert_in = _dummy.assertIn","90","    assert_not_in = _dummy.assertNotIn","91","except AttributeError:","92","    # Python <= 2.6","93","","94","    assert_dict_equal = assert_equal","103","    assert_raises_regex = _dummy.assertRaisesRegex","104","except AttributeError:","105","    # for Python 2.6","387","    assert_less = _dummy.assertLess","388","    assert_greater = _dummy.assertGreater","389","except AttributeError:"],"delete":["49","# Conveniently import all assertions in one place.","50","from nose.tools import assert_equal","51","from nose.tools import assert_not_equal","52","from nose.tools import assert_true","53","from nose.tools import assert_false","54","from nose.tools import assert_raises","56","try:","57","    from nose.tools import assert_dict_equal","58","except ImportError:","59","    # Not in old versions of nose, but is only for formatting anyway","60","    assert_dict_equal = assert_equal","61","from nose import SkipTest","81","           \"assert_approx_equal\"]","85","    from nose.tools import assert_in, assert_not_in","86","except ImportError:","87","    # Nose < 1.0.0","96","    from nose.tools import assert_raises_regex","97","except ImportError:","98","    # for Python 2","380","    from nose.tools import assert_less","381","except ImportError:","383","","384","try:","385","    from nose.tools import assert_greater","386","except ImportError:"]}],"sklearn\/utils\/tests\/test_fast_dict.py":[{"add":["5","from sklearn.utils.testing import assert_equal","8",""],"delete":["3","from nose.tools import assert_equal"]}],"sklearn\/neighbors\/tests\/test_dist_metrics.py":[{"add":["9","from sklearn.utils.testing import SkipTest"],"delete":["9","from nose import SkipTest"]}],"sklearn\/feature_selection\/tests\/test_rfe.py":[{"add":["16","from sklearn.utils.testing import assert_greater, assert_equal, assert_true"],"delete":["5","from nose.tools import assert_equal, assert_true","17","from sklearn.utils.testing import assert_greater"]}],"sklearn\/cross_decomposition\/tests\/test_pls.py":[{"add":["1","from sklearn.utils.testing import (assert_equal, assert_array_almost_equal,"],"delete":["1","from sklearn.utils.testing import (assert_array_almost_equal,","6","from nose.tools import assert_equal"]}],"sklearn\/feature_extraction\/tests\/test_text.py":[{"add":["27","from sklearn.utils.testing import (assert_equal, assert_false, assert_true,","28","                                   assert_not_equal, assert_almost_equal,","29","                                   assert_in, assert_less, assert_greater,","31","                                   clean_warning_registry, SkipTest)"],"delete":["23","from nose import SkipTest","24","from nose.tools import assert_equal","25","from nose.tools import assert_false","26","from nose.tools import assert_not_equal","27","from nose.tools import assert_true","28","from nose.tools import assert_almost_equal","33","from sklearn.utils.testing import (assert_in, assert_less, assert_greater,","35","                                   clean_warning_registry)"]}],"sklearn\/feature_selection\/tests\/test_from_model.py":[{"add":["3","from sklearn.utils.testing import assert_true","9","from sklearn.utils.testing import assert_raises"],"delete":["3","from nose.tools import assert_raises, assert_true","4",""]}],"doc\/datasets\/twenty_newsgroups_fixture.py":[{"add":["7","","9","from sklearn.utils.testing import SkipTest"],"delete":["7","from nose import SkipTest"]}],"sklearn\/utils\/tests\/test_metaestimators.py":[{"add":["0","from sklearn.utils.testing import assert_true, assert_false"],"delete":["0","from nose.tools import assert_true, assert_false"]}],"sklearn\/feature_extraction\/tests\/test_feature_hasher.py":[{"add":["3","from numpy.testing import assert_array_equal","6","from sklearn.utils.testing import assert_raises, assert_true, assert_equal","72","    assert_array_equal(x1, x2)"],"delete":["5","","6","from nose.tools import assert_raises, assert_true","7","from numpy.testing import assert_array_equal, assert_equal","73","    assert_equal(x1, x2)"]}],"sklearn\/tree\/tests\/test_export.py":[{"add":["10","from sklearn.utils.testing import assert_in, assert_equal, assert_raises"],"delete":["6","from numpy.testing import assert_equal","7","from nose.tools import assert_raises","8","","13","from sklearn.utils.testing import assert_in"]}],"sklearn\/datasets\/mldata.py":[{"add":["217","# The following is used by test runners to setup the docstring tests fixture"],"delete":["217","# The following is used by nosetests to setup the docstring tests fixture"]}],"sklearn\/mixture\/tests\/test_gmm.py":[{"add":["16","from sklearn.utils.testing import (assert_true, assert_greater,","17","                                   assert_raise_message, assert_warns_message,","18","                                   ignore_warnings)"],"delete":["10","from nose.tools import assert_true","17","from sklearn.utils.testing import (assert_greater, assert_raise_message,","18","                                   assert_warns_message, ignore_warnings)"]}],"sklearn\/gaussian_process\/tests\/test_gaussian_process.py":[{"add":["13","from sklearn.utils.testing import assert_greater, assert_true, raises"],"delete":["7","from nose.tools import raises","8","from nose.tools import assert_true","9","","16","from sklearn.utils.testing import assert_greater"]}],"sklearn\/metrics\/cluster\/tests\/test_supervised.py":[{"add":["14","","15","from sklearn.utils.testing import (","16","        assert_equal, assert_almost_equal, assert_raise_message,","17",")","18","from numpy.testing import assert_array_almost_equal","19",""],"delete":["1","from nose.tools import assert_almost_equal","2","from nose.tools import assert_equal","3","from numpy.testing import assert_array_almost_equal","17","from sklearn.utils.testing import assert_raise_message"]}],"sklearn\/utils\/sparsetools\/tests\/test_traversal.py":[{"add":["4","from sklearn.utils.testing import SkipTest","5",""],"delete":["2","from nose import SkipTest","3",""]}],"sklearn\/svm\/tests\/test_svm.py":[{"add":["17","from sklearn.utils.testing import assert_equal, assert_true, assert_false","21","from sklearn.utils.testing import ignore_warnings, assert_raises"],"delete":["11","from nose.tools import assert_raises, assert_true, assert_equal, assert_false","21","from sklearn.utils.testing import ignore_warnings"]}],"sklearn\/svm\/tests\/test_sparse.py":[{"add":["10","from sklearn.utils.testing import (assert_raises, assert_true, assert_false,","11","                                   assert_warns, assert_raise_message,"],"delete":["0","from nose.tools import assert_raises, assert_true, assert_false","1","","12","from sklearn.utils.testing import (assert_warns, assert_raise_message,"]}],"sklearn\/manifold\/tests\/test_mds.py":[{"add":["4","from sklearn.utils.testing import assert_raises"],"delete":["3","from nose.tools import assert_raises"]}],"sklearn\/decomposition\/tests\/test_fastica.py":[{"add":["15","from sklearn.utils.testing import assert_raises"],"delete":["9","from nose.tools import assert_raises","10",""]}],"sklearn\/semi_supervised\/tests\/test_label_propagation.py":[{"add":["4","from sklearn.utils.testing import assert_equal","29","        assert_equal(clf.transduction_[2], 1)"],"delete":["2","import nose","29","        nose.tools.assert_equal(clf.transduction_[2], 1)"]}],"sklearn\/ensemble\/tests\/test_gradient_boosting_loss_functions.py":[{"add":["10","from sklearn.utils.testing import assert_raises"],"delete":["9","from nose.tools import assert_raises","10",""]}],"sklearn\/utils\/tests\/test_seq_dataset.py":[{"add":["5","from numpy.testing import assert_array_equal","11","from sklearn.utils.testing import assert_equal"],"delete":["10","from numpy.testing import assert_array_equal","11","from nose.tools import assert_equal"]}],"sklearn\/ensemble\/tests\/test_base.py":[{"add":["12","from sklearn.utils.testing import assert_true","13",""],"delete":["9","from nose.tools import assert_true"]}],"sklearn\/feature_selection\/tests\/test_base.py":[{"add":["8","from sklearn.utils.testing import assert_raises, assert_equal"],"delete":["3","from nose.tools import assert_raises, assert_equal"]}],"sklearn\/feature_extraction\/tests\/test_image.py":[{"add":["14","from sklearn.utils.testing import SkipTest, assert_equal, assert_true"],"delete":["8","from nose.tools import assert_equal, assert_true","15","from sklearn.utils.testing import SkipTest"]}],"sklearn\/linear_model\/tests\/test_least_angle.py":[{"add":["4","from sklearn.utils.testing import assert_equal"],"delete":["0","from nose.tools import assert_equal","1",""]}],"sklearn\/manifold\/tests\/test_locally_linear.py":[{"add":["11","from sklearn.utils.testing import assert_raises","12","from sklearn.utils.testing import assert_true"],"delete":["1","from nose.tools import assert_true","131","    from nose.tools import assert_raises"]}],"sklearn\/manifold\/tests\/test_spectral_embedding.py":[{"add":["18","from sklearn.utils.testing import assert_true, assert_equal, assert_raises","19","from sklearn.utils.testing import SkipTest"],"delete":["0","from nose.tools import assert_true","1","from nose.tools import assert_equal","2","","11","from nose.tools import assert_raises","12","from nose.plugins.skip import SkipTest","13",""]}],"sklearn\/utils\/tests\/test_validation.py":[{"add":["11","from sklearn.utils.testing import assert_true, assert_false, assert_equal","12","from sklearn.utils.testing import assert_raises, assert_raises_regexp"],"delete":["10","from nose.tools import assert_raises, assert_true, assert_false, assert_equal","12","from sklearn.utils.testing import assert_raises_regexp"]}],"sklearn\/datasets\/tests\/test_base.py":[{"add":["28","from sklearn.utils.testing import with_setup","86","@with_setup(setup_load_files, teardown_load_files)","95","@with_setup(setup_load_files, teardown_load_files)","106","@with_setup(setup_load_files, teardown_load_files)"],"delete":["4","import nose","86","@nose.tools.with_setup(setup_load_files, teardown_load_files)","95","@nose.tools.with_setup(setup_load_files, teardown_load_files)","106","@nose.tools.with_setup(setup_load_files, teardown_load_files)"]}]}},"084ef97f16342e1c5ac630ac46476c08da3bea62":{"changes":{"sklearn\/linear_model\/tests\/test_coordinate_descent.py":"MODIFY","sklearn\/src\/cblas\/cblas_sasum.c":"ADD","sklearn\/linear_model\/coordinate_descent.py":"MODIFY","sklearn\/src\/cblas\/ATL_srefasum.c":"ADD","doc\/whats_new.rst":"MODIFY","sklearn\/src\/cblas\/ATL_dsrefdot.c":"ADD","sklearn\/linear_model\/cd_fast.pyx":"MODIFY","sklearn\/src\/cblas\/cblas_saxpy.c":"ADD"},"diff":{"sklearn\/linear_model\/tests\/test_coordinate_descent.py":[{"add":["672","","673","","674","def test_enet_float_precision():","675","    # Generate dataset","676","    X, y, X_test, y_test = build_dataset(n_samples=20, n_features=10)","677","    # Here we have a small number of iterations, and thus the","678","    # ElasticNet might not converge. This is to speed up tests","679","","680","    for normalize in [True, False]:","681","        for fit_intercept in [True, False]:","682","            coef = {}","683","            intercept = {}","684","            clf = ElasticNet(alpha=0.5, max_iter=100, precompute=False,","685","                            fit_intercept=fit_intercept, normalize=normalize)","686","            for dtype in [np.float64, np.float32]:","687","                X = dtype(X)","688","                y = dtype(y)","689","                ignore_warnings(clf.fit)(X, y)","690","","691","                coef[dtype] = clf.coef_","692","                intercept[dtype] = clf.intercept_","693","","694","                assert_equal(clf.coef_.dtype, dtype)","695","","696","            assert_array_almost_equal(coef[np.float32], coef[np.float64],","697","                                    decimal=4)","698","            assert_array_almost_equal(intercept[np.float32],","699","                                    intercept[np.float64],","700","                                    decimal=4)"],"delete":[]}],"sklearn\/src\/cblas\/cblas_sasum.c":[{"add":[],"delete":[]}],"sklearn\/linear_model\/coordinate_descent.py":[{"add":["374","    # We expect X and y to be already Fortran ordered when bypassing","377","        X = check_array(X, 'csc', dtype=[np.float64, np.float32],","378","                        order='F', copy=copy_X)","379","        y = check_array(y, 'csc', dtype=X.dtype.type, order='F', copy=False,","383","            Xy = check_array(Xy, dtype=X.dtype.type, order='C', copy=False,","385","","399","            X_sparse_scaling = np.asarray(X_sparse_scaling, dtype=X.dtype)","401","            X_sparse_scaling = np.zeros(n_features, dtype=X.dtype)","431","        coefs = np.empty((n_features, n_alphas), dtype=X.dtype)","434","                         dtype=X.dtype)","437","        coef_ = np.asfortranarray(np.zeros(coefs.shape[:-1], dtype=X.dtype))","439","        coef_ = np.asfortranarray(coef_init, dtype=X.dtype)","475","                          ' to increase the number of iterations.' +","476","                          ' Fitting data with very small alpha' +","477","                          ' may cause precision problems.',","670","        # We expect X and y to be float64 or float32 Fortran ordered arrays","673","            X, y = check_X_y(X, y, accept_sparse='csc',","674","                             order='F', dtype=[np.float64, np.float32],","677","            y = check_array(y, order='F', copy=False, dtype=X.dtype.type,","679","","695","            coef_ = np.zeros((n_targets, n_features), dtype=X.dtype,","702","        dual_gaps_ = np.zeros(n_targets, dtype=X.dtype)","732","        # workaround since _set_intercept will cast self.coef_ into float64","733","        self.coef_ = np.asarray(self.coef_, dtype=X.dtype)","734",""],"delete":["374","    # We expect X and y to be already float64 Fortran ordered when bypassing","377","        X = check_array(X, 'csc', dtype=np.float64, order='F', copy=copy_X)","378","        y = check_array(y, 'csc', dtype=np.float64, order='F', copy=False,","382","            Xy = check_array(Xy, dtype=np.float64, order='C', copy=False,","398","            X_sparse_scaling = np.zeros(n_features)","428","        coefs = np.empty((n_features, n_alphas), dtype=np.float64)","431","                         dtype=np.float64)","434","        coef_ = np.asfortranarray(np.zeros(coefs.shape[:-1]))","436","        coef_ = np.asfortranarray(coef_init)","472","                          ' to increase the number of iterations',","665","        # We expect X and y to be already float64 Fortran ordered arrays","668","            y = np.asarray(y, dtype=np.float64)","669","            X, y = check_X_y(X, y, accept_sparse='csc', dtype=np.float64,","670","                             order='F',","673","            y = check_array(y, dtype=np.float64, order='F', copy=False,","690","            coef_ = np.zeros((n_targets, n_features), dtype=np.float64,","697","        dual_gaps_ = np.zeros(n_targets, dtype=np.float64)"]}],"sklearn\/src\/cblas\/ATL_srefasum.c":[{"add":[],"delete":[]}],"doc\/whats_new.rst":[{"add":["265","   - :class:`linear_model.ElasticNet` and :class:`linear_model.Lasso`","266","     now works with ``np.float32`` input data without converting it","267","     into ``np.float64``. This allows to reduce the memory","268","     consumption.","269","     (`#6913 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/6913>`_)","270","     By `YenChen Lin`_."],"delete":[]}],"sklearn\/src\/cblas\/ATL_dsrefdot.c":[{"add":[],"delete":[]}],"sklearn\/linear_model\/cd_fast.pyx":[{"add":["15","from cython cimport floating","20","ctypedef floating (*DOT)(int N, floating *X, int incX, floating *Y,","21","                         int incY) nogil","22","ctypedef void (*AXPY)(int N, floating alpha, floating *X, int incX,","23","                      floating *Y, int incY) nogil","24","ctypedef floating (*ASUM)(int N, floating *X, int incX) nogil","50","cdef inline floating fmax(floating x, floating y) nogil:","56","cdef inline floating fsign(floating f) nogil:","65","cdef floating abs_max(int n, floating* a) nogil:","68","    cdef floating m = fabs(a[0])","69","    cdef floating d","77","cdef floating max(int n, floating* a) nogil:","80","    cdef floating m = a[0]","81","    cdef floating d","89","cdef floating diff_abs_max(int n, floating* a, floating* b) nogil:","92","    cdef floating m = fabs(a[0] - b[0])","93","    cdef floating d","113","    void saxpy \"cblas_saxpy\"(int N, float alpha, float *X, int incX,","114","                             float *Y, int incY) nogil","117","    float sdot \"cblas_sdot\"(int N, float *X, int incX, float *Y,","118","                            int incY) nogil","120","    float sasum \"cblas_sasum\"(int N, float *X, int incX) nogil","122","                           double *X, int incX, double *Y, int incY,","123","                           double *A, int lda) nogil","124","    void dgemv \"cblas_dgemv\"(CBLAS_ORDER Order, CBLAS_TRANSPOSE TransA,","125","                             int M, int N, double alpha, double *A, int lda,","126","                             double *X, int incX, double beta,","127","                             double *Y, int incY) nogil","129","    void dcopy \"cblas_dcopy\"(int N, double *X, int incX, double *Y,","130","                             int incY) nogil","137","def enet_coordinate_descent(np.ndarray[floating, ndim=1] w,","138","                            floating alpha, floating beta,","139","                            np.ndarray[floating, ndim=2, mode='fortran'] X,","140","                            np.ndarray[floating, ndim=1, mode='c'] y,","141","                            int max_iter, floating tol,","152","    # fused types version of BLAS functions","153","    cdef DOT dot","154","    cdef AXPY axpy","155","    cdef ASUM asum","156","","157","    if floating is float:","158","        dtype = np.float32","159","        dot = sdot","160","        axpy = saxpy","161","        asum = sasum","162","    else:","163","        dtype = np.float64","164","        dot = ddot","165","        axpy = daxpy","166","        asum = dasum","167","","173","    cdef unsigned int n_tasks = y.strides[0] \/ sizeof(floating)","176","    cdef np.ndarray[floating, ndim=1] norm_cols_X = (X**2).sum(axis=0)","179","    cdef np.ndarray[floating, ndim=1] R = np.empty(n_samples, dtype=dtype)","180","    cdef np.ndarray[floating, ndim=1] XtA = np.empty(n_features, dtype=dtype)","182","    cdef floating tmp","183","    cdef floating w_ii","184","    cdef floating d_w_max","185","    cdef floating w_max","186","    cdef floating d_w_ii","187","    cdef floating gap = tol + 1.0","188","    cdef floating d_w_tol = tol","189","    cdef floating dual_norm_XtA","190","    cdef floating R_norm2","191","    cdef floating w_norm2","192","    cdef floating l1_norm","193","    cdef floating const","194","    cdef floating A_norm2","202","    cdef floating *X_data = <floating*> X.data","203","    cdef floating *y_data = <floating*> y.data","204","    cdef floating *w_data = <floating*> w.data","205","    cdef floating *R_data = <floating*> R.data","206","    cdef floating *XtA_data = <floating*> XtA.data","207","","215","            R[i] = y[i] - dot(n_features, &X_data[i], n_samples, w_data, 1)","218","        tol *= dot(n_samples, y_data, n_tasks, y_data, n_tasks)","236","                    axpy(n_samples, w_ii, &X_data[ii * n_samples], 1,","237","                         R_data, 1)","240","                tmp = dot(n_samples, &X_data[ii * n_samples], 1, R_data, 1)","250","                    axpy(n_samples, -w[ii], &X_data[ii * n_samples], 1,","251","                         R_data, 1)","261","            if (w_max == 0.0 or","262","                d_w_max \/ w_max < d_w_tol or","263","                n_iter == max_iter - 1):","270","                    XtA[i] = dot(n_samples, &X_data[i * n_samples],","271","                                 1, R_data, 1) - beta * w[i]","274","                    dual_norm_XtA = max(n_features, XtA_data)","276","                    dual_norm_XtA = abs_max(n_features, XtA_data)","279","                R_norm2 = dot(n_samples, R_data, 1, R_data, 1)","282","                w_norm2 = dot(n_features, w_data, 1, w_data, 1)","292","                l1_norm = asum(n_features, w_data, 1)","295","                gap += (alpha * l1_norm","296","                        - const * dot(n_samples, R_data, 1, y_data, n_tasks)","308","def sparse_enet_coordinate_descent(floating [:] w,","309","                            floating alpha, floating beta,","310","                            np.ndarray[floating, ndim=1, mode='c'] X_data,","313","                            np.ndarray[floating, ndim=1] y,","314","                            floating[:] X_mean, int max_iter,","315","                            floating tol, object rng, bint random=0,","331","    cdef floating[:] norm_cols_X","337","    cdef unsigned int n_tasks","340","    cdef floating[:] R = y.copy()","342","    cdef floating[:] X_T_R","343","    cdef floating[:] XtA","345","    # fused types version of BLAS functions","346","    cdef DOT dot","347","    cdef ASUM asum","348","","349","    if floating is float:","350","        dtype = np.float32","351","        n_tasks = y.strides[0] \/ sizeof(float)","352","        dot = sdot","353","        asum = sasum","354","    else:","355","        dtype = np.float64","356","        n_tasks = y.strides[0] \/ sizeof(DOUBLE)","357","        dot = ddot","358","        asum = dasum","359","","360","    norm_cols_X = np.zeros(n_features, dtype=dtype)","361","    X_T_R = np.zeros(n_features, dtype=dtype)","362","    XtA = np.zeros(n_features, dtype=dtype)","363","","364","    cdef floating tmp","365","    cdef floating w_ii","366","    cdef floating d_w_max","367","    cdef floating w_max","368","    cdef floating d_w_ii","369","    cdef floating X_mean_ii","370","    cdef floating R_sum = 0.0","371","    cdef floating R_norm2","372","    cdef floating w_norm2","373","    cdef floating A_norm2","374","    cdef floating l1_norm","375","    cdef floating normalize_sum","376","    cdef floating gap = tol + 1.0","377","    cdef floating d_w_tol = tol","378","    cdef floating dual_norm_XtA","411","        tol *= dot(n_samples, &y[0], 1, &y[0], 1)","499","                R_norm2 = dot(n_samples, &R[0], 1, &R[0], 1)","502","                w_norm2 = dot(n_features, &w[0], 1, &w[0], 1)","511","                l1_norm = asum(n_features, &w[0], 1)","513","                gap += (alpha * l1_norm - const * dot(","515","                            &R[0], 1,","516","                            &y[0], n_tasks"],"delete":["44","cdef inline double fmax(double x, double y) nogil:","50","cdef inline double fsign(double f) nogil:","59","cdef double abs_max(int n, double* a) nogil:","62","    cdef double m = fabs(a[0])","63","    cdef double d","71","cdef double max(int n, double* a) nogil:","74","    cdef double m = a[0]","75","    cdef double d","83","cdef double diff_abs_max(int n, double* a, double* b) nogil:","86","    cdef double m = fabs(a[0] - b[0])","87","    cdef double d","111","                double *X, int incX, double *Y, int incY, double *A, int lda) nogil","112","    void dgemv \"cblas_dgemv\"(CBLAS_ORDER Order,","113","                      CBLAS_TRANSPOSE TransA, int M, int N,","114","                      double alpha, double *A, int lda,","115","                      double *X, int incX, double beta,","116","                      double *Y, int incY) nogil","118","    void dcopy \"cblas_dcopy\"(int N, double *X, int incX, double *Y, int incY) nogil","125","def enet_coordinate_descent(np.ndarray[DOUBLE, ndim=1] w,","126","                            double alpha, double beta,","127","                            np.ndarray[DOUBLE, ndim=2, mode='fortran'] X,","128","                            np.ndarray[DOUBLE, ndim=1, mode='c'] y,","129","                            int max_iter, double tol,","145","    cdef unsigned int n_tasks = y.strides[0] \/ sizeof(DOUBLE)","148","    cdef np.ndarray[DOUBLE, ndim=1] norm_cols_X = (X**2).sum(axis=0)","151","    cdef np.ndarray[DOUBLE, ndim=1] R = np.empty(n_samples)","153","    cdef np.ndarray[DOUBLE, ndim=1] XtA = np.empty(n_features)","154","    cdef double tmp","155","    cdef double w_ii","156","    cdef double d_w_max","157","    cdef double w_max","158","    cdef double d_w_ii","159","    cdef double gap = tol + 1.0","160","    cdef double d_w_tol = tol","161","    cdef double dual_norm_XtA","162","    cdef double R_norm2","163","    cdef double w_norm2","164","    cdef double l1_norm","179","            R[i] = y[i] - ddot(n_features,","180","                               <DOUBLE*>(X.data + i * sizeof(DOUBLE)),","181","                               n_samples, <DOUBLE*>w.data, 1)","184","        tol *= ddot(n_samples, <DOUBLE*>y.data, n_tasks,","185","                    <DOUBLE*>y.data, n_tasks)","203","                    daxpy(n_samples, w_ii,","204","                          <DOUBLE*>(X.data + ii * n_samples * sizeof(DOUBLE)),","205","                          1, <DOUBLE*>R.data, 1)","208","                tmp = ddot(n_samples,","209","                           <DOUBLE*>(X.data + ii * n_samples * sizeof(DOUBLE)),","210","                           1, <DOUBLE*>R.data, 1)","220","                    daxpy(n_samples, -w[ii],","221","                          <DOUBLE*>(X.data + ii * n_samples * sizeof(DOUBLE)),","222","                          1, <DOUBLE*>R.data, 1)","232","            if (w_max == 0.0","233","                    or d_w_max \/ w_max < d_w_tol","234","                    or n_iter == max_iter - 1):","241","                    XtA[i] = ddot(","242","                        n_samples,","243","                        <DOUBLE*>(X.data + i * n_samples *sizeof(DOUBLE)),","244","                        1, <DOUBLE*>R.data, 1) - beta * w[i]","247","                    dual_norm_XtA = max(n_features, <DOUBLE*>XtA.data)","249","                    dual_norm_XtA = abs_max(n_features, <DOUBLE*>XtA.data)","252","                R_norm2 = ddot(n_samples, <DOUBLE*>R.data, 1,","253","                               <DOUBLE*>R.data, 1)","256","                w_norm2 = ddot(n_features, <DOUBLE*>w.data, 1,","257","                               <DOUBLE*>w.data, 1)","267","                l1_norm = dasum(n_features, <DOUBLE*>w.data, 1)","270","                gap += (alpha * l1_norm - const * ddot(","271","                            n_samples,","272","                            <DOUBLE*>R.data, 1,","273","                            <DOUBLE*>y.data, n_tasks)","279","","286","def sparse_enet_coordinate_descent(double[:] w,","287","                            double alpha, double beta,","288","                            np.ndarray[double, ndim=1, mode='c'] X_data,","291","                            np.ndarray[double, ndim=1] y,","292","                            double[:] X_mean, int max_iter,","293","                            double tol, object rng, bint random=0,","309","    cdef double[:] norm_cols_X = np.zeros(n_features, np.float64)","315","    cdef unsigned int n_tasks = y.strides[0] \/ sizeof(DOUBLE)","318","    cdef double[:] R = y.copy()","320","    cdef double[:] X_T_R = np.zeros(n_features)","321","    cdef double[:] XtA = np.zeros(n_features)","323","    cdef double tmp","324","    cdef double w_ii","325","    cdef double d_w_max","326","    cdef double w_max","327","    cdef double d_w_ii","328","    cdef double X_mean_ii","329","    cdef double R_sum = 0.0","330","    cdef double normalize_sum","331","    cdef double gap = tol + 1.0","332","    cdef double d_w_tol = tol","365","        tol *= ddot(n_samples, <DOUBLE*>&y[0], 1, <DOUBLE*>&y[0], 1)","453","                R_norm2 = ddot(n_samples, <DOUBLE*>&R[0], 1, <DOUBLE*>&R[0], 1)","456","                w_norm2 = ddot(n_features, <DOUBLE*>&w[0], 1, <DOUBLE*>&w[0], 1)","465","                l1_norm = dasum(n_features, <DOUBLE*>&w[0], 1)","467","                # The expression inside ddot is equivalent to np.dot(R.T, y)","468","                gap += (alpha * l1_norm - const * ddot(","470","                            <DOUBLE*>&R[0], 1,","471","                            <DOUBLE*>&y[0], n_tasks"]}],"sklearn\/src\/cblas\/cblas_saxpy.c":[{"add":[],"delete":[]}]}},"1f381ae72875533ccad4f14c2e490b8339f67d4f":{"changes":{"sklearn\/kernel_ridge.py":"MODIFY"},"diff":{"sklearn\/kernel_ridge.py":[{"add":["71","    dual_coef_ : array, shape = [n_samples] or [n_samples, n_targets]","72","        Representation of weight vector(s) in kernel space"],"delete":["71","    dual_coef_ : array, shape = [n_features] or [n_targets, n_features]","72","        Weight vector(s) in kernel space"]}]}},"49d126fd8fdf9ed554d9ac953ba88fda09137be3":{"changes":{"sklearn\/utils\/extmath.py":"MODIFY","sklearn\/metrics\/ranking.py":"MODIFY","sklearn\/utils\/tests\/test_extmath.py":"MODIFY"},"diff":{"sklearn\/utils\/extmath.py":[{"add":["844","","845","","846","def stable_cumsum(arr, rtol=1e-05, atol=1e-08):","847","    \"\"\"Use high precision for cumsum and check that final value matches sum","848","","849","    Parameters","850","    ----------","851","    arr : array-like","852","        To be cumulatively summed as flat","853","    rtol : float","854","        Relative tolerance, see ``np.allclose``","855","    atol : float","856","        Absolute tolerance, see ``np.allclose``","857","    \"\"\"","858","    out = np.cumsum(arr, dtype=np.float64)","859","    expected = np.sum(arr, dtype=np.float64)","860","    if not np.allclose(out[-1], expected, rtol=rtol, atol=atol):","861","        raise RuntimeError('cumsum was found to be unstable: '","862","                           'its last element does not correspond to sum')","863","    return out"],"delete":[]}],"sklearn\/metrics\/ranking.py":[{"add":["29","from ..utils.extmath import stable_cumsum","340","    tps = stable_cumsum(y_true * weight)[threshold_idxs]","342","        fps = stable_cumsum(weight)[threshold_idxs] - tps"],"delete":["339","    tps = (y_true * weight).cumsum()[threshold_idxs]","341","        fps = weight.cumsum()[threshold_idxs] - tps"]}],"sklearn\/utils\/tests\/test_extmath.py":[{"add":["19","from sklearn.utils.testing import assert_raise_message","21","from sklearn.utils.testing import SkipTest","22","from sklearn.utils.fixes import np_version","37","from sklearn.utils.extmath import stable_cumsum","649","","650","","651","def test_stable_cumsum():","652","    if np_version < (1, 9):","653","        raise SkipTest(\"Sum is as unstable as cumsum for numpy < 1.9\")","654","    assert_array_equal(stable_cumsum([1, 2, 3]), np.cumsum([1, 2, 3]))","655","    r = np.random.RandomState(0).rand(100000)","656","    assert_raise_message(RuntimeError,","657","                         'cumsum was found to be unstable: its last element '","658","                         'does not correspond to sum',","659","                         stable_cumsum, r, rtol=0, atol=0)"],"delete":[]}]}},"7c0ebbc2269612dd79ec1d64d2da866ce567c861":{"changes":{"examples\/classification\/plot_lda_qda.py":"MODIFY"},"diff":{"examples\/classification\/plot_lda_qda.py":[{"add":["70","    alpha = 0.5","71","","73","    plt.plot(X0_tp[:, 0], X0_tp[:, 1], 'o', alpha=alpha,","74","             color='red')","75","    plt.plot(X0_fp[:, 0], X0_fp[:, 1], '*', alpha=alpha,","76","             color='#990000')  # dark red","79","    plt.plot(X1_tp[:, 0], X1_tp[:, 1], 'o', alpha=alpha,","80","             color='blue')","81","    plt.plot(X1_fp[:, 0], X1_fp[:, 1], '*', alpha=alpha,","82","             color='#000099')  # dark blue","112","                              180 + angle, facecolor=color, edgecolor='yellow',","113","                              linewidth=2, zorder=2)"],"delete":["71","    plt.plot(X0_tp[:, 0], X0_tp[:, 1], 'o', color='red')","72","    plt.plot(X0_fp[:, 0], X0_fp[:, 1], '.', color='#990000')  # dark red","75","    plt.plot(X1_tp[:, 0], X1_tp[:, 1], 'o', color='blue')","76","    plt.plot(X1_fp[:, 0], X1_fp[:, 1], '.', color='#000099')  # dark blue","106","                              180 + angle, color=color)"]}]}},"ccefc2ec3284d1424e574858621ce9fa76a296fe":{"changes":{"sklearn\/base.py":"MODIFY","sklearn\/tests\/test_base.py":"MODIFY"},"diff":{"sklearn\/base.py":[{"add":["23","def _first_and_last_element(arr):","24","    \"\"\"Returns first and last element of numpy array or sparse matrix.\"\"\"","25","    if isinstance(arr, np.ndarray) or hasattr(arr, 'data'):","26","        # numpy array or sparse matrix with .data attribute","27","        data = arr.data if sparse.issparse(arr) else arr","28","        return data.flat[0], data.flat[-1]","29","    else:","30","        # Sparse matrices without .data attribute. Only dok_matrix at","31","        # the time of writing, in this case indexing is fast","32","        return arr[0, 0], arr[-1, -1]","33","","34","","87","                    and (_first_and_last_element(param1) ==","88","                         _first_and_last_element(param2))","105","                    and (_first_and_last_element(param1) ==","106","                         _first_and_last_element(param2))"],"delete":["75","                    # We have to use '.flat' for 2D arrays","76","                    and param1.flat[0] == param2.flat[0]","77","                    and param1.flat[-1] == param2.flat[-1]","94","                    and param1.data[0] == param2.data[0]","95","                    and param1.data[-1] == param2.data[-1]"]}],"sklearn\/tests\/test_base.py":[{"add":["3","import sys","4","","147","def test_clone_sparse_matrices():","148","    sparse_matrix_classes = [","149","        getattr(sp, name)","150","        for name in dir(sp) if name.endswith('_matrix')]","151","","152","    PY26 = sys.version_info[:2] == (2, 6)","153","    if PY26:","154","        # sp.dok_matrix can not be deepcopied in Python 2.6","155","        sparse_matrix_classes.remove(sp.dok_matrix)","156","","157","    for cls in sparse_matrix_classes:","158","        sparse_matrix = cls(np.eye(5))","159","        clf = MyEstimator(empty=sparse_matrix)","160","        clf_cloned = clone(clf)","161","        assert_true(clf.empty.__class__ is clf_cloned.empty.__class__)","162","        assert_array_equal(clf.empty.toarray(), clf_cloned.empty.toarray())","163","","164",""],"delete":[]}]}},"f485fce970727d99b7dc55273054eec9a56d032a":{"changes":{"sklearn\/linear_model\/ridge.py":"MODIFY"},"diff":{"sklearn\/linear_model\/ridge.py":[{"add":["250","          scipy.sparse.linalg.lsqr. It is the fastest but may not be available"],"delete":["250","          scipy.sparse.linalg.lsqr. It is the fatest but may not be available"]}]}},"c7465f2c0ed10baaa11388cdd9d8698f0f889fc5":{"changes":{"sklearn\/linear_model\/ridge.py":"MODIFY"},"diff":{"sklearn\/linear_model\/ridge.py":[{"add":["212","        Regularization strength; must be a positive float. Regularization","213","        improves the conditioning of the problem and reduces the variance of","214","        the estimates. Larger values specify stronger regularization.","215","        Alpha corresponds to ``C^-1`` in other linear models such as ","216","        LogisticRegression or LinearSVC. If an array is passed, penalties are","217","        assumed to be specific to the targets. Hence they must correspond in","218","        number.","507","        Regularization strength; must be a positive float. Regularization","508","        improves the conditioning of the problem and reduces the variance of","509","        the estimates. Larger values specify stronger regularization.","510","        Alpha corresponds to ``C^-1`` in other linear models such as ","511","        LogisticRegression or LinearSVC. If an array is passed, penalties are","512","        assumed to be specific to the targets. Hence they must correspond in","513","        number.","652","        Regularization strength; must be a positive float. Regularization","653","        improves the conditioning of the problem and reduces the variance of","654","        the estimates. Larger values specify stronger regularization.","655","        Alpha corresponds to ``C^-1`` in other linear models such as ","656","        LogisticRegression or LinearSVC.","1097","        Regularization strength; must be a positive float. Regularization","1098","        improves the conditioning of the problem and reduces the variance of","1099","        the estimates. Larger values specify stronger regularization.","1100","        Alpha corresponds to ``C^-1`` in other linear models such as ","1101","        LogisticRegression or LinearSVC. ","1199","        Regularization strength; must be a positive float. Regularization","1200","        improves the conditioning of the problem and reduces the variance of","1201","        the estimates. Larger values specify stronger regularization.","1202","        Alpha corresponds to ``C^-1`` in other linear models such as ","1203","        LogisticRegression or LinearSVC. "],"delete":["212","        The l_2 penalty to be used. If an array is passed, penalties are","213","        assumed to be specific to targets","502","        Small positive values of alpha improve the conditioning of the problem","503","        and reduce the variance of the estimates.  Alpha corresponds to","504","        ``C^-1`` in other linear models such as LogisticRegression or","505","        LinearSVC. If an array is passed, penalties are assumed to be specific","506","        to the targets. Hence they must correspond in number.","645","        Small positive values of alpha improve the conditioning of the problem","646","        and reduce the variance of the estimates.  Alpha corresponds to","647","        ``C^-1`` in other linear models such as LogisticRegression or","648","        LinearSVC.","1089","        Small positive values of alpha improve the conditioning of the","1090","        problem and reduce the variance of the estimates.","1091","        Alpha corresponds to ``C^-1`` in other linear models such as","1092","        LogisticRegression or LinearSVC.","1190","        Small positive values of alpha improve the conditioning of the","1191","        problem and reduce the variance of the estimates.","1192","        Alpha corresponds to ``C^-1`` in other linear models such as","1193","        LogisticRegression or LinearSVC."]}]}},"36e67a91af1d63209625d6c36f484bec756310ed":{"changes":{"sklearn\/linear_model\/tests\/test_least_angle.py":"MODIFY","sklearn\/decomposition\/tests\/test_truncated_svd.py":"MODIFY","sklearn\/linear_model\/tests\/test_sgd.py":"MODIFY","sklearn\/ensemble\/tests\/test_gradient_boosting.py":"MODIFY"},"diff":{"sklearn\/linear_model\/tests\/test_least_angle.py":[{"add":["247","","248","    for alpha_min in alphas_min:","250","                                                       alpha_min=alpha_min)","258","    for alpha_min in alphas_min:","260","                                                       alpha_min=alpha_min)"],"delete":["247","    for alphas_min in alphas_min:","249","                                                       alpha_min=0.9)","256","    alphas_min = [10, 0.9, 1e-4]","258","    for alphas_min in alphas_min:","260","                                                       alpha_min=0.9)"]}],"sklearn\/decomposition\/tests\/test_truncated_svd.py":[{"add":["66","        tsvd = TruncatedSVD(n_components=52, random_state=42, algorithm=algo)"],"delete":["66","        tsvd = TruncatedSVD(n_components=52, random_state=42)"]}],"sklearn\/linear_model\/tests\/test_sgd.py":[{"add":["479","            clf = self.factory(loss=loss, alpha=0.01, n_iter=10)"],"delete":["479","            clf = self.factory(loss=\"modified_huber\", alpha=0.01, n_iter=10)"]}],"sklearn\/ensemble\/tests\/test_gradient_boosting.py":[{"add":["1029","        gb = GradientBoostingClassifier(n_estimators=5, loss=loss)"],"delete":["1029","        gb = GradientBoostingClassifier(n_estimators=5)"]}]}},"cbd3bca20f1d19461011b5f59d9416669eb30535":{"changes":{"sklearn\/neighbors\/dist_metrics.pyx":"MODIFY","sklearn\/neighbors\/tests\/test_dist_metrics.py":"MODIFY"},"diff":{"sklearn\/neighbors\/dist_metrics.pyx":[{"add":["1102","            d = self.func(x1arr, x2arr, **self.kwargs)","1103","            try:","1104","                # Cython generates code here that results in a TypeError","1105","                # if d is the wrong type.","1106","                return d","1107","            except TypeError:","1108","                raise TypeError(\"Custom distance function must accept two \"","1109","                                \"vectors and return a float.\")","1110","            "],"delete":["1093","        x = np.random.random(10)","1094","        try:","1095","            d = self.func(x, x, **kwargs)","1096","        except TypeError:","1097","            raise ValueError(\"func must be a callable taking two arrays\")","1098","","1099","        try:","1100","            d = float(d)","1101","        except TypeError:","1102","            raise ValueError(\"func must return a float\")","1103","","1113","            return self.func(x1arr, x2arr, **self.kwargs)"]}],"sklearn\/neighbors\/tests\/test_dist_metrics.py":[{"add":["9","from sklearn.neighbors import BallTree","10","from sklearn.utils.testing import SkipTest, assert_raises_regex","172","","173","","174","def test_bad_pyfunc_metric():","175","    def wrong_distance(x, y):","176","        return \"1\"","177","","178","    X = np.ones((5, 2))","179","    assert_raises_regex(TypeError,","180","                        \"Custom distance function must accept two vectors\",","181","                        BallTree, X, metric=wrong_distance)","182","","183","","184","def test_input_data_size():","185","    # Regression test for #6288","186","    # Previoulsly, a metric requiring a particular input dimension would fail","187","    def custom_metric(x, y):","188","        assert x.shape[0] == 3","189","        return np.sum((x - y) ** 2)","190","","191","    rng = np.random.RandomState(0)","192","    X = rng.rand(10, 3)","193","","194","    pyfunc = DistanceMetric.get_metric(\"pyfunc\", func=dist_func, p=2)","195","    eucl = DistanceMetric.get_metric(\"euclidean\")","196","    assert_array_almost_equal(pyfunc.pairwise(X), eucl.pairwise(X))"],"delete":["9","from sklearn.utils.testing import SkipTest"]}]}}}