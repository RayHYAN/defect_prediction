{"5e193ec655d2b7634e35733e2fa0e5bef5e63d45":{"changes":{"sklearn\/decomposition\/_factor_analysis.py":"MODIFY","sklearn\/decomposition\/tests\/test_factor_analysis.py":"MODIFY","examples\/decomposition\/plot_varimax_fa.py":"ADD","doc\/modules\/decomposition.rst":"MODIFY","doc\/whats_new\/v0.24.rst":"MODIFY"},"diff":{"sklearn\/decomposition\/_factor_analysis.py":[{"add":["91","    rotation : None | 'varimax' | 'quartimax'","92","        If not None, apply the indicated rotation. Currently, varimax and","93","        quartimax are implemented. See","94","        `\"The varimax criterion for analytic rotation in factor analysis\"","95","        <https:\/\/link.springer.com\/article\/10.1007%2FBF02289233>`_","96","        H. F. Kaiser, 1958","97","","98","        .. versionadded:: 0.24","99","","153","                 iterated_power=3, rotation=None, random_state=0):","166","        self.rotation = rotation","188","","256","        if self.rotation is not None:","257","            self.components_ = self._rotate(W)","377","","378","    def _rotate(self, components, n_components=None, tol=1e-6):","379","        \"Rotate the factor analysis solution.\"","380","        # note that tol is not exposed","381","        implemented = (\"varimax\", \"quartimax\")","382","        method = self.rotation","383","        if method in implemented:","384","            return _ortho_rotation(components.T, method=method,","385","                                   tol=tol)[:self.n_components]","386","        else:","387","            raise ValueError(\"'method' must be in %s, not %s\"","388","                             % (implemented, method))","389","","390","","391","def _ortho_rotation(components, method='varimax', tol=1e-6, max_iter=100):","392","    \"\"\"Return rotated components.\"\"\"","393","    nrow, ncol = components.shape","394","    rotation_matrix = np.eye(ncol)","395","    var = 0","396","","397","    for _ in range(max_iter):","398","        comp_rot = np.dot(components, rotation_matrix)","399","        if method == \"varimax\":","400","            tmp = comp_rot * np.transpose((comp_rot ** 2).sum(axis=0) \/ nrow)","401","        elif method == \"quartimax\":","402","            tmp = 0","403","        u, s, v = np.linalg.svd(","404","            np.dot(components.T, comp_rot ** 3 - tmp))","405","        rotation_matrix = np.dot(u, v)","406","        var_new = np.sum(s)","407","        if var != 0 and var_new < var * (1 + tol):","408","            break","409","        var = var_new","410","","411","    return np.dot(components, rotation_matrix).T"],"delete":["144","                 iterated_power=3, random_state=0):"]}],"sklearn\/decomposition\/tests\/test_factor_analysis.py":[{"add":["4","from itertools import combinations","5","","10","from sklearn.utils._testing import assert_raises","16","from sklearn.decomposition._factor_analysis import _ortho_rotation","89","","90","    # test rotation","91","    n_components = 2","92","","93","    results, projections = {}, {}","94","    for method in (None, \"varimax\", 'quartimax'):","95","        fa_var = FactorAnalysis(n_components=n_components,","96","                                rotation=method)","97","        results[method] = fa_var.fit_transform(X)","98","        projections[method] = fa_var.get_covariance()","99","    for rot1, rot2 in combinations([None, 'varimax', 'quartimax'], 2):","100","        assert not np.allclose(results[rot1], results[rot2])","101","        assert np.allclose(projections[rot1], projections[rot2], atol=3)","102","","103","    assert_raises(ValueError,","104","                  FactorAnalysis(rotation='not_implemented').fit_transform, X)","105","","106","    # test against R's psych::principal with rotate=\"varimax\"","107","    # (i.e., the values below stem from rotating the components in R)","108","    # R's factor analysis returns quite different values; therefore, we only","109","    # test the rotation itself","110","    factors = np.array(","111","        [[0.89421016, -0.35854928, -0.27770122, 0.03773647],","112","         [-0.45081822, -0.89132754, 0.0932195, -0.01787973],","113","         [0.99500666, -0.02031465, 0.05426497, -0.11539407],","114","         [0.96822861, -0.06299656, 0.24411001, 0.07540887]])","115","    r_solution = np.array([[0.962, 0.052], [-0.141, 0.989],","116","                           [0.949, -0.300], [0.937, -0.251]])","117","    rotated = _ortho_rotation(factors[:, :n_components], method='varimax').T","118","    assert_array_almost_equal(np.abs(rotated), np.abs(r_solution), decimal=3)"],"delete":[]}],"examples\/decomposition\/plot_varimax_fa.py":[{"add":[],"delete":[]}],"doc\/modules\/decomposition.rst":[{"add":["624","Factor Analysis is often followed by a rotation of the factors (with the","625","parameter `rotation`), usually to improve interpretability. For example,","626","Varimax rotation maximizes the sum of the variances of the squared loadings,","627","i.e., it tends to produce sparser factors, which are influenced by only a few","628","features each (the \"simple structure\"). See e.g., the first example below.","632","    * :ref:`sphx_glr_auto_examples_decomposition_plot_varimax_fa.py`","635","","968","    * `\"The varimax criterion for analytic rotation in factor analysis\"","969","      <https:\/\/link.springer.com\/article\/10.1007%2FBF02289233>`_","970","      H. F. Kaiser, 1958"],"delete":[]}],"doc\/whats_new\/v0.24.rst":[{"add":["86","- |Enhancement| :func:`decomposition.FactorAnalysis` now supports the optional","87","  argument `rotation`, which can take the value `None`, `'varimax'` or `'quartimax'.`","88","  :pr:`11064` by :user:`Jona Sassenhagen <jona-sassenhagen>`.","89",""],"delete":[]}]}},"a74d31a339b440817f43e407c59026cfba980fb9":{"changes":{"examples\/cross_decomposition\/plot_compare_cross_decomposition.py":"MODIFY","examples\/exercises\/plot_iris_exercise.py":"MODIFY","examples\/ensemble\/plot_isolation_forest.py":"MODIFY","examples\/decomposition\/plot_kernel_pca.py":"MODIFY","examples\/ensemble\/plot_voting_probas.py":"MODIFY","examples\/datasets\/plot_random_dataset.py":"MODIFY","examples\/ensemble\/plot_adaboost_twoclass.py":"MODIFY","examples\/ensemble\/plot_random_forest_regression_multioutput.py":"MODIFY","examples\/ensemble\/plot_partial_dependence.py":"MODIFY","examples\/ensemble\/plot_forest_iris.py":"MODIFY","examples\/ensemble\/plot_voting_decision_regions.py":"MODIFY","examples\/datasets\/plot_iris_dataset.py":"MODIFY","examples\/ensemble\/plot_random_forest_embedding.py":"MODIFY"},"diff":{"examples\/cross_decomposition\/plot_compare_cross_decomposition.py":[{"add":["63","plt.scatter(X_train_r[:, 0], Y_train_r[:, 0], label=\"train\",","64","            marker=\"o\", c=\"b\", s=25)","65","plt.scatter(X_test_r[:, 0], Y_test_r[:, 0], label=\"test\",","66","            marker=\"o\", c=\"r\", s=25)","76","plt.scatter(X_train_r[:, 1], Y_train_r[:, 1], label=\"train\",","77","            marker=\"o\", c=\"b\", s=25)","78","plt.scatter(X_test_r[:, 1], Y_test_r[:, 1], label=\"test\",","79","            marker=\"o\", c=\"r\", s=25)","90","plt.scatter(X_train_r[:, 0], X_train_r[:, 1], label=\"train\",","91","            marker=\"*\", c=\"b\", s=50)","92","plt.scatter(X_test_r[:, 0], X_test_r[:, 1], label=\"test\",","93","            marker=\"*\", c=\"r\", s=50)","103","plt.scatter(Y_train_r[:, 0], Y_train_r[:, 1], label=\"train\",","104","            marker=\"*\", c=\"b\", s=50)","105","plt.scatter(Y_test_r[:, 0], Y_test_r[:, 1], label=\"test\",","106","            marker=\"*\", c=\"r\", s=50)"],"delete":["63","plt.plot(X_train_r[:, 0], Y_train_r[:, 0], \"ob\", label=\"train\")","64","plt.plot(X_test_r[:, 0], Y_test_r[:, 0], \"or\", label=\"test\")","74","plt.plot(X_train_r[:, 1], Y_train_r[:, 1], \"ob\", label=\"train\")","75","plt.plot(X_test_r[:, 1], Y_test_r[:, 1], \"or\", label=\"test\")","86","plt.plot(X_train_r[:, 0], X_train_r[:, 1], \"*b\", label=\"train\")","87","plt.plot(X_test_r[:, 0], X_test_r[:, 1], \"*r\", label=\"test\")","97","plt.plot(Y_train_r[:, 0], Y_train_r[:, 1], \"*b\", label=\"train\")","98","plt.plot(Y_test_r[:, 0], Y_test_r[:, 1], \"*r\", label=\"test\")"]}],"examples\/exercises\/plot_iris_exercise.py":[{"add":["43","    plt.scatter(X[:, 0], X[:, 1], c=y, zorder=10, cmap=plt.cm.Paired,","44","                edgecolor='k', s=20)","47","    plt.scatter(X_test[:, 0], X_test[:, 1], s=80, facecolors='none',","48","                zorder=10, edgecolor='k')"],"delete":["43","    plt.scatter(X[:, 0], X[:, 1], c=y, zorder=10, cmap=plt.cm.Paired)","46","    plt.scatter(X_test[:, 0], X_test[:, 1], s=80, facecolors='none', zorder=10)"]}],"examples\/ensemble\/plot_isolation_forest.py":[{"add":["58","b1 = plt.scatter(X_train[:, 0], X_train[:, 1], c='white',","59","                 s=20, edgecolor='k')","60","b2 = plt.scatter(X_test[:, 0], X_test[:, 1], c='green',","61","                 s=20, edgecolor='k')","62","c = plt.scatter(X_outliers[:, 0], X_outliers[:, 1], c='red',","63","                s=20, edgecolor='k')"],"delete":["58","b1 = plt.scatter(X_train[:, 0], X_train[:, 1], c='white')","59","b2 = plt.scatter(X_test[:, 0], X_test[:, 1], c='green')","60","c = plt.scatter(X_outliers[:, 0], X_outliers[:, 1], c='red')"]}],"examples\/decomposition\/plot_kernel_pca.py":[{"add":["38","plt.scatter(X[reds, 0], X[reds, 1], c=\"red\",","39","            s=20, edgecolor='k')","40","plt.scatter(X[blues, 0], X[blues, 1], c=\"blue\",","41","            s=20, edgecolor='k')","52","plt.scatter(X_pca[reds, 0], X_pca[reds, 1], c=\"red\",","53","            s=20, edgecolor='k')","54","plt.scatter(X_pca[blues, 0], X_pca[blues, 1], c=\"blue\",","55","            s=20, edgecolor='k')","61","plt.scatter(X_kpca[reds, 0], X_kpca[reds, 1], c=\"red\",","62","            s=20, edgecolor='k')","63","plt.scatter(X_kpca[blues, 0], X_kpca[blues, 1], c=\"blue\",","64","            s=20, edgecolor='k')","70","plt.scatter(X_back[reds, 0], X_back[reds, 1], c=\"red\",","71","            s=20, edgecolor='k')","72","plt.scatter(X_back[blues, 0], X_back[blues, 1], c=\"blue\",","73","            s=20, edgecolor='k')"],"delete":["38","plt.plot(X[reds, 0], X[reds, 1], \"ro\")","39","plt.plot(X[blues, 0], X[blues, 1], \"bo\")","50","plt.plot(X_pca[reds, 0], X_pca[reds, 1], \"ro\")","51","plt.plot(X_pca[blues, 0], X_pca[blues, 1], \"bo\")","57","plt.plot(X_kpca[reds, 0], X_kpca[reds, 1], \"ro\")","58","plt.plot(X_kpca[blues, 0], X_kpca[blues, 1], \"bo\")","64","plt.plot(X_back[reds, 0], X_back[reds, 1], \"ro\")","65","plt.plot(X_back[blues, 0], X_back[blues, 1], \"bo\")"]}],"examples\/ensemble\/plot_voting_probas.py":[{"add":["58","p1 = ax.bar(ind, np.hstack(([class1_1[:-1], [0]])), width,","59","            color='green', edgecolor='k')","60","p2 = ax.bar(ind + width, np.hstack(([class2_1[:-1], [0]])), width,","61","            color='lightgreen', edgecolor='k')","64","p3 = ax.bar(ind, [0, 0, 0, class1_1[-1]], width,","65","            color='blue', edgecolor='k')","66","p4 = ax.bar(ind + width, [0, 0, 0, class2_1[-1]], width,","67","            color='steelblue', edgecolor='k')"],"delete":["58","p1 = ax.bar(ind, np.hstack(([class1_1[:-1], [0]])), width, color='green')","59","p2 = ax.bar(ind + width, np.hstack(([class2_1[:-1], [0]])), width, color='lightgreen')","62","p3 = ax.bar(ind, [0, 0, 0, class1_1[-1]], width, color='blue')","63","p4 = ax.bar(ind + width, [0, 0, 0, class2_1[-1]], width, color='steelblue')"]}],"examples\/datasets\/plot_random_dataset.py":[{"add":["29","plt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1,","30","            s=25, edgecolor='k')","36","plt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1,","37","            s=25, edgecolor='k')","40","plt.title(\"Two informative features, two clusters per class\",","41","          fontsize='small')","43","plt.scatter(X2[:, 0], X2[:, 1], marker='o', c=Y2,","44","            s=25, edgecolor='k')","51","plt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1,","52","            s=25, edgecolor='k')","57","plt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1,","58","            s=25, edgecolor='k')","63","plt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1,","64","            s=25, edgecolor='k')"],"delete":["29","plt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1)","35","plt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1)","38","plt.title(\"Two informative features, two clusters per class\", fontsize='small')","40","plt.scatter(X2[:, 0], X2[:, 1], marker='o', c=Y2)","41","","48","plt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1)","53","plt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1)","58","plt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1)"]}],"examples\/ensemble\/plot_adaboost_twoclass.py":[{"add":["72","                s=20, edgecolor='k',","91","             alpha=.5,","92","             edgecolor='k')"],"delete":["90","             alpha=.5)"]}],"examples\/ensemble\/plot_random_forest_regression_multioutput.py":[{"add":["61","plt.scatter(y_test[:, 0], y_test[:, 1], edgecolor='k',","63","plt.scatter(y_multirf[:, 0], y_multirf[:, 1], edgecolor='k',","66","plt.scatter(y_rf[:, 0], y_rf[:, 1], edgecolor='k',"],"delete":["61","plt.scatter(y_test[:, 0], y_test[:, 1],","63","plt.scatter(y_multirf[:, 0], y_multirf[:, 1],","66","plt.scatter(y_rf[:, 0], y_rf[:, 1],"]}],"examples\/ensemble\/plot_partial_dependence.py":[{"add":["97","    surf = ax.plot_surface(XX, YY, Z, rstride=1, cstride=1,","98","                           cmap=plt.cm.BuPu, edgecolor='k')","105","    plt.suptitle('Partial dependence of house value on median\\n'","106","                 'age and average occupancy')"],"delete":["97","    surf = ax.plot_surface(XX, YY, Z, rstride=1, cstride=1, cmap=plt.cm.BuPu)","104","    plt.suptitle('Partial dependence of house value on median age and '","105","                 'average occupancy')"]}],"examples\/ensemble\/plot_forest_iris.py":[{"add":["12","In the first row, the classifiers are built using the sepal width and","13","the sepal length features only, on the second row using the petal length and","14","sepal length only, and on the third row using the petal width and the","15","petal length only.","18","4 features using 30 estimators and scored using 10 fold cross validation,","19","we see::","26","Increasing `max_depth` for AdaBoost lowers the standard deviation of","27","the scores (but the average score does not improve).","98","        model_title = str(type(model)).split(","99","            \".\")[-1][:-2][:-len(\"Classifier\")]","100","","103","            model_details += \" with {} estimators\".format(","104","                len(model.estimators_))","105","        print(model_details + \" with features\", pair,","106","              \"has a score of\", scores)","127","            # Choose alpha blend level with respect to the number","128","            # of estimators","139","        # surfaces. These points are regularly space and do not have a","140","        # black outline","141","        xx_coarser, yy_coarser = np.meshgrid(","142","            np.arange(x_min, x_max, plot_step_coarser),","143","            np.arange(y_min, y_max, plot_step_coarser))","144","        Z_points_coarser = model.predict(np.c_[xx_coarser.ravel(),","145","                                         yy_coarser.ravel()]","146","                                         ).reshape(xx_coarser.shape)","147","        cs_points = plt.scatter(xx_coarser, yy_coarser, s=15,","148","                                c=Z_points_coarser, cmap=cmap,","149","                                edgecolors=\"none\")","154","                    cmap=ListedColormap(['r', 'y', 'b']),","155","                    edgecolor='k', s=20)"],"delete":["12","In the first row, the classifiers are built using the sepal width and the sepal","13","length features only, on the second row using the petal length and sepal length","14","only, and on the third row using the petal width and the petal length only.","17","4 features using 30 estimators and scored using 10 fold cross validation, we see::","24","Increasing `max_depth` for AdaBoost lowers the standard deviation of the scores (but","25","the average score does not improve).","50","from sklearn.externals.six.moves import xrange","97","        model_title = str(type(model)).split(\".\")[-1][:-2][:-len(\"Classifier\")]","100","            model_details += \" with {} estimators\".format(len(model.estimators_))","101","        print( model_details + \" with features\", pair, \"has a score of\", scores )","122","            # Choose alpha blend level with respect to the number of estimators","133","        # surfaces. These points are regularly space and do not have a black outline","134","        xx_coarser, yy_coarser = np.meshgrid(np.arange(x_min, x_max, plot_step_coarser),","135","                                             np.arange(y_min, y_max, plot_step_coarser))","136","        Z_points_coarser = model.predict(np.c_[xx_coarser.ravel(), yy_coarser.ravel()]).reshape(xx_coarser.shape)","137","        cs_points = plt.scatter(xx_coarser, yy_coarser, s=15, c=Z_points_coarser, cmap=cmap, edgecolors=\"none\")","142","                    cmap=ListedColormap(['r', 'y', 'b']))"]}],"examples\/ensemble\/plot_voting_decision_regions.py":[{"add":["68","    axarr[idx[0], idx[1]].scatter(X[:, 0], X[:, 1], c=y,","69","                                  s=20, edgecolor='k')"],"delete":["68","    axarr[idx[0], idx[1]].scatter(X[:, 0], X[:, 1], c=y, alpha=0.8)"]}],"examples\/datasets\/plot_iris_dataset.py":[{"add":["42","plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired,","43","            edgecolor='k')","58","           cmap=plt.cm.Paired, edgecolor='k')"],"delete":["42","plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)","57","           cmap=plt.cm.Paired)"]}],"examples\/ensemble\/plot_random_forest_embedding.py":[{"add":["16","separate two concentric circles simply based on the principal components","17","of the transformed data with truncated SVD.","59","ax.scatter(X[:, 0], X[:, 1], c=y, s=50, edgecolor='k')","65","ax.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y, s=50, edgecolor='k')","71","# Plot the decision in original space. For that, we will assign a color","72","# to each point in the mesh [x_min, x_max]x[y_min, y_max].","85","ax.scatter(X[:, 0], X[:, 1], c=y, s=50, edgecolor='k')","97","ax.scatter(X[:, 0], X[:, 1], c=y, s=50, edgecolor='k')"],"delete":["16","separate two concentric circles simply based on the principal components of the","17","transformed data with truncated SVD.","59","ax.scatter(X[:, 0], X[:, 1], c=y, s=50)","65","ax.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y, s=50)","71","# Plot the decision in original space. For that, we will assign a color to each","72","# point in the mesh [x_min, x_max]x[y_min, y_max].","85","ax.scatter(X[:, 0], X[:, 1], c=y, s=50)","97","ax.scatter(X[:, 0], X[:, 1], c=y, s=50)"]}]}},"7b543513b80f0e1f6af98b1b52b70f452728b5d7":{"changes":{"doc\/modules\/pipeline.rst":"MODIFY"},"diff":{"doc\/modules\/pipeline.rst":[{"add":["16","and classification. :class:`Pipeline` serves multiple purposes here:"],"delete":["16","and classification. :class:`Pipeline` serves two purposes here:"]}]}},"9accce5519bdc7733c28f52ec25310ab78ec7dfe":{"changes":{"sklearn\/feature_extraction\/text.py":"MODIFY","sklearn\/utils\/validation.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY","sklearn\/feature_extraction\/tests\/test_text.py":"MODIFY","sklearn\/utils\/tests\/test_validation.py":"MODIFY"},"diff":{"sklearn\/feature_extraction\/text.py":[{"add":["1500","            # idf_ being a property, the automatic attributes detection","1501","            # does not work as usual and we need to specify the attribute","1502","            # name:","1503","            check_is_fitted(self, attributes=[\"idf_\"],","1504","                            msg='idf vector is not fitted')","1889","        check_is_fitted(self, msg='The TF-IDF vectorizer is not fitted')"],"delete":["1500","            check_is_fitted(self, msg='idf vector is not fitted')","1885","        check_is_fitted(self, msg='The tfidf vector is not fitted')"]}],"sklearn\/utils\/validation.py":[{"add":["854","def check_is_fitted(estimator, attributes=None, msg=None, all_or_any=all):","861","    This utility is meant to be used internally by estimators themselves,","862","    typically in their own predict \/ transform methods.","863","","869","    attributes : str, list or tuple of str, default=None","870","        Attribute name(s) given as string or a list\/tuple of strings","871","        Eg.: ``[\"coef_\", \"estimator_\", ...], \"coef_\"``","872","","873","        If `None`, `estimator` is considered fitted if there exist an","874","        attribute that ends with a underscore and does not start with double","875","        underscore.","876","","887","    all_or_any : callable, {all, any}, default all","888","        Specify whether all or any of the given attributes must exist.","889","","908","    if attributes is not None:","909","        if not isinstance(attributes, (list, tuple)):","910","            attributes = [attributes]","911","        attrs = all_or_any([hasattr(estimator, attr) for attr in attributes])","912","    else:","913","        attrs = [v for v in vars(estimator)","914","                 if v.endswith(\"_\") and not v.startswith(\"__\")]"],"delete":["854","def check_is_fitted(estimator, msg=None):","894","    attrs = [v for v in vars(estimator)","895","             if (v.endswith(\"_\") or v.startswith(\"_\"))","896","             and not v.startswith(\"__\")]"]}],"doc\/whats_new\/v0.22.rst":[{"add":["70","- |Fix| :func:`utils.check_is_fitted` accepts back an explicit ``attributes``","71","  argument to check for specific attributes as explicit markers of a fitted","72","  estimator. When no explicit ``attributes`` are provided, only the attributes","73","  ending with a single \"_\" are used as \"fitted\" markers. The ``all_or_any``","74","  argument is also no longer deprecated. This change is made to","75","  restore some backward compatibility with the behavior of this utility in","76","  version 0.21. :pr:`15947` by `Thomas Fan`_.","77",""],"delete":[]}],"sklearn\/feature_extraction\/tests\/test_text.py":[{"add":["1101","    vec.fit([\"some text\", \"some other text\"])"],"delete":[]}],"sklearn\/utils\/tests\/test_validation.py":[{"add":["7","from operator import itemgetter","679","def test_check_is_fitted_attributes():","680","    class MyEstimator():","681","        def fit(self, X, y):","682","            return self","683","","684","    msg = \"not fitted\"","685","    est = MyEstimator()","686","","687","    with pytest.raises(NotFittedError, match=msg):","688","        check_is_fitted(est, attributes=[\"a_\", \"b_\"])","689","    with pytest.raises(NotFittedError, match=msg):","690","        check_is_fitted(est, attributes=[\"a_\", \"b_\"], all_or_any=all)","691","    with pytest.raises(NotFittedError, match=msg):","692","        check_is_fitted(est, attributes=[\"a_\", \"b_\"], all_or_any=any)","693","","694","    est.a_ = \"a\"","695","    with pytest.raises(NotFittedError, match=msg):","696","        check_is_fitted(est, attributes=[\"a_\", \"b_\"])","697","    with pytest.raises(NotFittedError, match=msg):","698","        check_is_fitted(est, attributes=[\"a_\", \"b_\"], all_or_any=all)","699","    check_is_fitted(est, attributes=[\"a_\", \"b_\"], all_or_any=any)","700","","701","    est.b_ = \"b\"","702","    check_is_fitted(est, attributes=[\"a_\", \"b_\"])","703","    check_is_fitted(est, attributes=[\"a_\", \"b_\"], all_or_any=all)","704","    check_is_fitted(est, attributes=[\"a_\", \"b_\"], all_or_any=any)","705","","706","","707","@pytest.mark.parametrize(\"wrap\",","708","                         [itemgetter(0), list, tuple],","709","                         ids=[\"single\", \"list\", \"tuple\"])","710","def test_check_is_fitted_with_attributes(wrap):","711","    ard = ARDRegression()","712","    with pytest.raises(NotFittedError, match=\"is not fitted yet\"):","713","        check_is_fitted(ard, wrap([\"coef_\"]))","714","","715","    ard.fit(*make_blobs())","716","","717","    # Does not raise","718","    check_is_fitted(ard, wrap([\"coef_\"]))","719","","720","    # Raises when using attribute that is not defined","721","    with pytest.raises(NotFittedError, match=\"is not fitted yet\"):","722","        check_is_fitted(ard, wrap([\"coef_bad_\"]))","723","","724",""],"delete":["16","from sklearn.utils._testing import assert_warns_message","52","from sklearn.exceptions import DataConversionWarning"]}]}},"623ee42260eb11a695407d60439e18056c22559e":{"changes":{"doc\/modules\/grid_search.rst":"MODIFY"},"diff":{"doc\/modules\/grid_search.rst":[{"add":["16","best :ref:`cross validation <cross_validation>` score."],"delete":["16","best :ref:`cross_validation` score."]}]}},"e2f99b04270d132d0a485fdb43406e46a95bb2ee":{"changes":{"sklearn\/svm\/src\/liblinear\/liblinear_helper.c":"MODIFY","sklearn\/svm\/src\/liblinear\/linear.cpp":"MODIFY"},"diff":{"sklearn\/svm\/src\/liblinear\/liblinear_helper.c":[{"add":["221","    free(problem);"],"delete":[]}],"sklearn\/svm\/src\/liblinear\/linear.cpp":[{"add":["2912","\tif(model_ptr->n_iter != NULL)","2913","\t    free(model_ptr->n_iter);"],"delete":[]}]}},"4e166e2982be77ea2375653754de77159b9e1f80":{"changes":{"sklearn\/compose\/tests\/test_column_transformer.py":"MODIFY","sklearn\/compose\/_column_transformer.py":"MODIFY"},"diff":{"sklearn\/compose\/tests\/test_column_transformer.py":[{"add":["18","from sklearn.externals import six","304","    for col in [1.5, ['string', 1], slice(1, 's'), np.array([1.])]:","554","@pytest.mark.parametrize(","555","    \"key\", [[0], slice(0, 1), np.array([True, False]), ['first'], 'pd-index',","556","            np.array(['first']), np.array(['first'], dtype=object),","557","            slice(None, 'first'), slice('first', 'first')])","561","    if isinstance(key, six.string_types) and key == 'pd-index':","562","        key = pd.Index(['first'])"],"delete":["303","    for col in [1.5, ['string', 1], slice(1, 's')]:","553","@pytest.mark.parametrize(\"key\", [[0], slice(0, 1), np.array([True, False]),","554","                                 ['first'], slice(None, 'first'),","555","                                 slice('first', 'first')])"]}],"sklearn\/compose\/_column_transformer.py":[{"add":["456","    Check that scalar, list or slice is of a certain type.","457","","458","    This is only used in _get_column and _get_column_indices to check","459","    if the `key` (column specification) is fully integer or fully string-like.","460","","461","    Parameters","462","    ----------","463","    key : scalar, list, slice, array-like","464","        The column specification to check","465","    superclass : int or six.string_types","466","        The type for which to check the `key`","477","        if superclass is int:","478","            return key.dtype.kind == 'i'","479","        else:","480","            # superclass = six.string_types","481","            return key.dtype.kind in ('O', 'U', 'S')","510","    elif hasattr(key, 'dtype') and np.issubdtype(key.dtype, np.bool_):","577","    elif hasattr(key, 'dtype') and np.issubdtype(key.dtype, np.bool_):"],"delete":["456","    Check that scalar, list or slice is of certain type.","467","        return key.dtype.kind == 'i'","496","    elif hasattr(key, 'dtype') and np.issubdtype(key.dtype, np.bool):","563","    elif hasattr(key, 'dtype') and np.issubdtype(key.dtype, np.bool):"]}]}},"689f412391582640025938c068d9a6885bc15a26":{"changes":{"sklearn\/ensemble\/voting_classifier.py":"MODIFY","sklearn\/ensemble\/tests\/test_voting_classifier.py":"MODIFY"},"diff":{"sklearn\/ensemble\/voting_classifier.py":[{"add":["210","                axis=1, arr=predictions)"],"delete":["210","                axis=1, arr=predictions.astype('int'))"]}],"sklearn\/ensemble\/tests\/test_voting_classifier.py":[{"add":[],"delete":["25","# A custom classifier based on SVC to return 'float' type class labels","26","class FaultySVC(SVC):","27","    def predict(self, X):","28","        return super(FaultySVC, self).predict(X).astype(float)","29","","30","","372","","373","","374","def test_predict_for_hard_voting():","375","    # Test voting classifier with non-integer (float) prediction","376","    clf1 = FaultySVC(random_state=123)","377","    clf2 = GaussianNB()","378","    clf3 = SVC(probability=True, random_state=123)","379","    eclf1 = VotingClassifier(estimators=[","380","        ('fsvc', clf1), ('gnb', clf2), ('svc', clf3)], weights=[1, 2, 3],","381","        voting='hard')","382","","383","    eclf1.fit(X, y)","384","    eclf1.predict(X)"]}]}},"1557cb8a1910bce6dc33cd5cd3a08c380bdec566":{"changes":{"examples\/applications\/plot_tomography_l1_reconstruction.py":"MODIFY"},"diff":{"examples\/applications\/plot_tomography_l1_reconstruction.py":[{"add":["36","from __future__ import division","53","    floor_x = np.floor((x - orig) \/ dx).astype(np.int64)","115","proj_operator = build_projection_operator(l, l \/\/ 7)"],"delete":["52","    floor_x = np.floor((x - orig) \/ dx)","114","proj_operator = build_projection_operator(l, l \/ 7.)"]}]}},"0bdd8bfb2bec61d9f636adb61c682ce2356b1b3c":{"changes":{"sklearn\/datasets\/base.py":"MODIFY"},"diff":{"sklearn\/datasets\/base.py":[{"add":["501","    This is a copy of the test set of the UCI ML hand-written digits datasets","502","    http:\/\/archive.ics.uci.edu\/ml\/datasets\/Optical+Recognition+of+Handwritten+Digits","503",""],"delete":[]}]}},"a0c2de3f9ee8310da5a827623ce06b50769dfc99":{"changes":{"doc\/whats_new.rst":"MODIFY"},"diff":{"doc\/whats_new.rst":[{"add":["213","   - Fixed a bug when :func:`sklearn.datasets.make_classification` fails ","214","     when generating more than 30 features. :issue:`8159` by","215","     :user:`Herilalaina Rakotoarison <herilalaina>`","216",""],"delete":[]}]}},"d01cdc204ee4972307aca4cc1e1b1e5e6347cc70":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/manifold\/tests\/test_spectral_embedding.py":"MODIFY",".travis.yml":"MODIFY","build_tools\/travis\/install.sh":"MODIFY","sklearn\/cluster\/tests\/test_spectral.py":"MODIFY","sklearn\/manifold\/spectral_embedding_.py":"MODIFY","examples\/cluster\/plot_face_segmentation.py":"MODIFY","sklearn\/cluster\/spectral.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["227","","229","  is smaller than ``n_clusters``. This may occur when the number of distinct","230","  points in the data set is actually smaller than the number of cluster one is","237","- Fixed a bug in :func:`cluster.spectral_clustering` where the normalization of","238","  the spectrum was using a division instead of a multiplication. :issue:`8129`","239","  by :user:`Jan Margeta <jmargeta>`, :user:`Guillaume Lemaitre <glemaitre>`,","240","  and :user:`Devansh D. <devanshdalal>`.","241",""],"delete":["227","  ","229","  is smaller than ``n_clusters``. This may occur when the number of distinct ","230","  points in the data set is actually smaller than the number of cluster one is "]}],"sklearn\/manifold\/tests\/test_spectral_embedding.py":[{"add":["0","import pytest","1","","17","from sklearn.utils.testing import assert_array_almost_equal","18","from sklearn.utils.testing import assert_array_equal","262","    embedding_2 = diffusion_map.T[:n_components]","266","","267","","268","def test_spectral_embedding_first_eigen_vector():","269","    # Test that the first eigenvector of spectral_embedding","270","    # is constant and that the second is not (for a connected graph)","271","    random_state = np.random.RandomState(36)","272","    data = random_state.randn(10, 30)","273","    sims = rbf_kernel(data)","274","    n_components = 2","275","","276","    for seed in range(10):","277","        embedding = spectral_embedding(sims,","278","                                       norm_laplacian=False,","279","                                       n_components=n_components,","280","                                       drop_first=False,","281","                                       random_state=seed)","282","","283","        assert np.std(embedding[:, 0]) == pytest.approx(0)","284","        assert np.std(embedding[:, 1]) > 1e-3"],"delete":["1","from numpy.testing import assert_array_almost_equal","2","from numpy.testing import assert_array_equal","260","    embedding_2 = diffusion_map.T[:n_components] * dd"]}],".travis.yml":[{"add":["41","    # It also runs tests requiring Pandas and PyAMG","44","           CYTHON_VERSION=\"0.26.1\" PYAMG_VERSION=\"3.3.2\" COVERAGE=true"],"delete":["41","    # It also runs tests requiring Pandas.","44","           CYTHON_VERSION=\"0.26.1\" COVERAGE=true"]}],"build_tools\/travis\/install.sh":[{"add":["39","    TO_INSTALL=\"python=$PYTHON_VERSION pip pytest pytest-cov \\","40","                numpy=$NUMPY_VERSION scipy=$SCIPY_VERSION \\","41","                cython=$CYTHON_VERSION\"","42","","44","        TO_INSTALL=\"$TO_INSTALL mkl\"","46","        TO_INSTALL=\"$TO_INSTALL nomkl\"","48","","49","    if [[ -n \"$PANDAS_VERSION\" ]]; then","50","        TO_INSTALL=\"$TO_INSTALL pandas=$PANDAS_VERSION\"","51","    fi","52","","53","    if [[ -n \"$PYAMG_VERSION\" ]]; then","54","        TO_INSTALL=\"$TO_INSTALL pyamg=$PYAMG_VERSION\"","55","    fi","56","","57","    conda create -n testenv --yes $TO_INSTALL"],"delete":["39","    # Configure the conda environment and put it in the path using the","40","    # provided versions","42","        conda create -n testenv --yes python=$PYTHON_VERSION pip \\","43","            pytest pytest-cov numpy=$NUMPY_VERSION scipy=$SCIPY_VERSION \\","44","            mkl cython=$CYTHON_VERSION \\","45","            ${PANDAS_VERSION+pandas=$PANDAS_VERSION}","46","            ","48","        conda create -n testenv --yes python=$PYTHON_VERSION pip \\","49","            pytest pytest-cov numpy=$NUMPY_VERSION scipy=$SCIPY_VERSION \\","50","            nomkl cython=$CYTHON_VERSION \\","51","            ${PANDAS_VERSION+pandas=$PANDAS_VERSION}"]}],"sklearn\/cluster\/tests\/test_spectral.py":[{"add":["1","from __future__ import division","6","from sklearn.externals.six.moves import cPickle","7","","16","from sklearn.feature_extraction import img_to_graph","22","try:","23","    from pyamg import smoothed_aggregation_solver  # noqa","24","    amg_loaded = True","25","except ImportError:","26","    amg_loaded = False","27","","50","                assert adjusted_rand_score(labels, [1, 1, 1, 0, 0, 0, 0]) == 1","52","                model_copy = cPickle.loads(cPickle.dumps(model))","53","                assert model_copy.n_clusters == model.n_clusters","54","                assert model_copy.eigen_solver == model.eigen_solver","100","    assert adjusted_rand_score(y, labels) == 1","114","    assert adjusted_rand_score(y, sp.labels_) == 1","118","    assert adjusted_rand_score(y, labels) == 1","130","            assert (X.shape[0],) == labels.shape","135","    assert (X.shape[0],) == labels.shape","144","    assert (X.shape[0],) == labels.shape","169","            assert adjusted_rand_score(y_true, y_pred) > 0.8","170","","171","","172","def test_spectral_clustering_with_arpack_amg_solvers():","173","    # Test that spectral_clustering is the same for arpack and amg solver","174","    # Based on toy example from plot_segmentation_toy.py","175","","176","    # a small two coin image","177","    x, y = np.indices((40, 40))","178","","179","    center1, center2 = (14, 12), (20, 25)","180","    radius1, radius2 = 8, 7","181","","182","    circle1 = (x - center1[0]) ** 2 + (y - center1[1]) ** 2 < radius1 ** 2","183","    circle2 = (x - center2[0]) ** 2 + (y - center2[1]) ** 2 < radius2 ** 2","184","","185","    circles = circle1 | circle2","186","    mask = circles.copy()","187","    img = circles.astype(float)","188","","189","    graph = img_to_graph(img, mask=mask)","190","    graph.data = np.exp(-graph.data \/ graph.data.std())","191","","192","    labels_arpack = spectral_clustering(","193","        graph, n_clusters=2, eigen_solver='arpack', random_state=0)","194","","195","    assert len(np.unique(labels_arpack)) == 2","196","","197","    if amg_loaded:","198","        labels_amg = spectral_clustering(","199","            graph, n_clusters=2, eigen_solver='amg', random_state=0)","200","        assert adjusted_rand_score(labels_arpack, labels_amg) == 1","201","    else:","202","        assert_raises(","203","            ValueError, spectral_clustering,","204","            graph, n_clusters=2, eigen_solver='amg', random_state=0)"],"delete":["1","","2","from sklearn.externals.six.moves import cPickle","3","","4","dumps, loads = cPickle.dumps, cPickle.loads","13","from sklearn.utils.testing import assert_greater","17","from sklearn.cluster.spectral import spectral_embedding","46","                assert_array_equal(labels, [1, 1, 1, 0, 0, 0, 0])","48","                model_copy = loads(dumps(model))","49","                assert_equal(model_copy.n_clusters, model.n_clusters)","50","                assert_equal(model_copy.eigen_solver, model.eigen_solver)","54","def test_spectral_amg_mode():","55","    # Test the amg mode of SpectralClustering","56","    centers = np.array([","57","        [0., 0., 0.],","58","        [10., 10., 10.],","59","        [20., 20., 20.],","60","    ])","61","    X, true_labels = make_blobs(n_samples=100, centers=centers,","62","                                cluster_std=1., random_state=42)","63","    D = pairwise_distances(X)  # Distance matrix","64","    S = np.max(D) - D  # Similarity matrix","65","    S = sparse.coo_matrix(S)","66","    try:","67","        from pyamg import smoothed_aggregation_solver  # noqa","68","","69","        amg_loaded = True","70","    except ImportError:","71","        amg_loaded = False","72","    if amg_loaded:","73","        labels = spectral_clustering(S, n_clusters=len(centers),","74","                                     random_state=0, eigen_solver=\"amg\")","75","        # We don't care too much that it's good, just that it *worked*.","76","        # There does have to be some lower limit on the performance though.","77","        assert_greater(np.mean(labels == true_labels), .3)","78","    else:","79","        assert_raises(ValueError, spectral_embedding, S,","80","                      n_components=len(centers),","81","                      random_state=0, eigen_solver=\"amg\")","82","","83","","126","    assert_equal(adjusted_rand_score(y, labels), 1)","140","    assert_equal(adjusted_rand_score(y, sp.labels_), 1)","144","    assert_equal(adjusted_rand_score(y, labels), 1)","156","            assert_equal((X.shape[0],), labels.shape)","161","    assert_equal((X.shape[0],), labels.shape)","170","    assert_equal((X.shape[0],), labels.shape)","195","            assert_greater(adjusted_rand_score(y_true, y_pred), 0.8)"]}],"sklearn\/manifold\/spectral_embedding_.py":[{"add":["6","from __future__ import division","7","","273","            embedding = diffusion_map.T[n_components::-1]","274","            if norm_laplacian:","275","                embedding = embedding \/ dd","298","        embedding = diffusion_map.T","299","        if norm_laplacian:","300","            embedding = embedding \/ dd","315","            embedding = diffusion_map.T[:n_components]","316","            if norm_laplacian:","317","                embedding = embedding \/ dd","326","            embedding = diffusion_map.T[:n_components]","327","            if norm_laplacian:","328","                embedding = embedding \/ dd"],"delete":["271","            embedding = diffusion_map.T[n_components::-1] * dd","294","        embedding = diffusion_map.T * dd","309","            embedding = diffusion_map.T[:n_components] * dd","318","            embedding = diffusion_map.T[:n_components] * dd"]}],"examples\/cluster\/plot_face_segmentation.py":[{"add":["65","                                 assign_labels=assign_labels, random_state=42)"],"delete":["65","                                 assign_labels=assign_labels, random_state=1)"]}],"sklearn\/cluster\/spectral.py":[{"add":["258","","259","    # The first eigen vector is constant only for fully connected graphs","260","    # and should be kept for spectral clustering (drop_first = False)","261","    # See spectral_embedding documentation."],"delete":[]}]}},"1c1566edc551c9588efadc325f6f29a3060545f3":{"changes":{"sklearn\/utils\/extmath.py":"MODIFY","sklearn\/decomposition\/pca.py":"MODIFY","sklearn\/utils\/tests\/test_extmath.py":"MODIFY","sklearn\/decomposition\/tests\/test_pca.py":"MODIFY"},"diff":{"sklearn\/utils\/extmath.py":[{"add":["197","    if A.dtype.kind == 'f':","198","        # Ensure f32 is preserved as f32","199","        Q = Q.astype(A.dtype, copy=False)","332",""],"delete":[]}],"sklearn\/decomposition\/pca.py":[{"add":["358","        X = check_array(X, dtype=[np.float64, np.float32], ensure_2d=True,"],"delete":["358","        X = check_array(X, dtype=[np.float64], ensure_2d=True,"]}],"sklearn\/utils\/tests\/test_extmath.py":[{"add":["101","def check_randomized_svd_low_rank(dtype):","107","    decimal = 5 if dtype == np.float32 else 7","108","    dtype = np.dtype(dtype)","114","                             random_state=0).astype(dtype, copy=False)","120","    # Convert the singular values to the specific dtype","121","    U = U.astype(dtype, copy=False)","122","    s = s.astype(dtype, copy=False)","123","    V = V.astype(dtype, copy=False)","124","","127","        Ua, sa, Va = randomized_svd(","128","            X, k, power_iteration_normalizer=normalizer, random_state=0)","129","","130","        # If the input dtype is float, then the output dtype is float of the","131","        # same bit size (f32 is not upcast to f64)","132","        # But if the input dtype is int, the output dtype is float64","133","        if dtype.kind == 'f':","134","            assert Ua.dtype == dtype","135","            assert sa.dtype == dtype","136","            assert Va.dtype == dtype","137","        else:","138","            assert Ua.dtype == np.float64","139","            assert sa.dtype == np.float64","140","            assert Va.dtype == np.float64","141","","148","        assert_almost_equal(s[:k], sa, decimal=decimal)","151","        assert_almost_equal(np.dot(U[:, :k], V[:k, :]), np.dot(Ua, Va),","152","                            decimal=decimal)","161","        if dtype.kind == 'f':","162","            assert Ua.dtype == dtype","163","            assert sa.dtype == dtype","164","            assert Va.dtype == dtype","165","        else:","166","            assert Ua.dtype.kind == 'f'","167","            assert sa.dtype.kind == 'f'","168","            assert Va.dtype.kind == 'f'","169","","170","        assert_almost_equal(s[:rank], sa[:rank], decimal=decimal)","171","","172","","173","def test_randomized_svd_low_rank_all_dtypes():","174","    for dtype in (np.int32, np.int64, np.float32, np.float64):","175","        yield check_randomized_svd_low_rank, dtype"],"delete":["101","def test_randomized_svd_low_rank():","112","                             random_state=0)","120","        Ua, sa, Va = \\","121","            randomized_svd(X, k, power_iteration_normalizer=normalizer,","122","                           random_state=0)","129","        assert_almost_equal(s[:k], sa)","132","        assert_almost_equal(np.dot(U[:, :k], V[:k, :]), np.dot(Ua, Va))","141","        assert_almost_equal(s[:rank], sa[:rank])"]}],"sklearn\/decomposition\/tests\/test_pca.py":[{"add":["600","","601","","602","def test_pca_dtype_preservation():","603","    for svd_solver in solver_list:","604","        yield check_pca_float_dtype_preservation, svd_solver","605","        yield check_pca_int_dtype_upcast_to_double, svd_solver","606","","607","","608","def check_pca_float_dtype_preservation(svd_solver):","609","    # Ensure that PCA does not upscale the dtype when input is float32","610","    X_64 = np.random.RandomState(0).rand(1000, 4).astype(np.float64)","611","    X_32 = X_64.astype(np.float32)","612","","613","    pca_64 = PCA(n_components=3, svd_solver=svd_solver,","614","                 random_state=0).fit(X_64)","615","    pca_32 = PCA(n_components=3, svd_solver=svd_solver,","616","                 random_state=0).fit(X_32)","617","","618","    assert pca_64.components_.dtype == np.float64","619","    assert pca_32.components_.dtype == np.float32","620","    assert pca_64.transform(X_64).dtype == np.float64","621","    assert pca_32.transform(X_32).dtype == np.float32","622","","623","    assert_array_almost_equal(pca_64.components_, pca_32.components_,","624","                              decimal=5)","625","","626","","627","def check_pca_int_dtype_upcast_to_double(svd_solver):","628","    # Ensure that all int types will be upcast to float64","629","    X_i64 = np.random.RandomState(0).randint(0, 1000, (1000, 4))","630","    X_i64 = X_i64.astype(np.int64)","631","    X_i32 = X_i64.astype(np.int32)","632","","633","    pca_64 = PCA(n_components=3, svd_solver=svd_solver,","634","                 random_state=0).fit(X_i64)","635","    pca_32 = PCA(n_components=3, svd_solver=svd_solver,","636","                 random_state=0).fit(X_i32)","637","","638","    assert pca_64.components_.dtype == np.float64","639","    assert pca_32.components_.dtype == np.float64","640","    assert pca_64.transform(X_i64).dtype == np.float64","641","    assert pca_32.transform(X_i32).dtype == np.float64","642","","643","    assert_array_almost_equal(pca_64.components_, pca_32.components_,","644","                              decimal=5)"],"delete":[]}]}},"41651a16199caa39189bef2237584b45e1e85e9d":{"changes":{"sklearn\/compose\/tests\/test_column_transformer.py":"MODIFY","sklearn\/compose\/_column_transformer.py":"MODIFY"},"diff":{"sklearn\/compose\/tests\/test_column_transformer.py":[{"add":["345","                                 n_jobs=3, remainder='drop')","351","    assert_equal(ct.remainder, 'drop')"],"delete":["345","                                 n_jobs=3)"]}],"sklearn\/compose\/_column_transformer.py":[{"add":["611","    remainder : {'passthrough', 'drop'}, default 'passthrough'","612","        By default, all remaining columns that were not specified in","613","        `transformers` will be automatically passed through (default of","614","        ``'passthrough'``). This subset of columns is concatenated with the","615","        output of the transformers.","616","        By using ``remainder='drop'``, only the specified columns in","617","        `transformers` are transformed and combined in the output, and the","618","        non-specified columns are dropped.","619","","652","    remainder = kwargs.pop('remainder', 'passthrough')","657","    return ColumnTransformer(transformer_list, n_jobs=n_jobs,","658","                             remainder=remainder)"],"delete":["647","    return ColumnTransformer(transformer_list, n_jobs=n_jobs)"]}]}},"0e4bdfdfc6934515fc03fae1845f223e947ce28f":{"changes":{"sklearn\/linear_model\/tests\/test_coordinate_descent.py":"MODIFY","sklearn\/linear_model\/coordinate_descent.py":"MODIFY"},"diff":{"sklearn\/linear_model\/tests\/test_coordinate_descent.py":[{"add":["177","def test_lasso_cv_with_some_model_selection():","178","    from sklearn.pipeline import make_pipeline","179","    from sklearn.preprocessing import StandardScaler","180","    from sklearn.model_selection import StratifiedKFold","181","    from sklearn import datasets","182","    from sklearn.linear_model import LassoCV","183","","184","    diabetes = datasets.load_diabetes()","185","    X = diabetes.data","186","    y = diabetes.target","187","","188","    pipe = make_pipeline(","189","        StandardScaler(),","190","        LassoCV(cv=StratifiedKFold(n_splits=5))","191","    )","192","    pipe.fit(X, y)","193","","194",""],"delete":[]}],"sklearn\/linear_model\/coordinate_descent.py":[{"add":["1152","        folds = list(cv.split(X, y))"],"delete":["1152","        folds = list(cv.split(X))"]}]}},"eb1a3c4765948cd1d0cdc7ba2e040aa6a3671c07":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/linear_model\/tests\/test_ridge.py":"MODIFY","sklearn\/linear_model\/ridge.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["258","- Fixed a bug in :class:`linear_model.RidgeClassifierCV` where","259","  the parameter ``store_cv_values`` was not implemented though","260","  it was documented in ``cv_values`` as a way to set up the storage","261","  of cross-validation values for different alphas. :issue:`10297` by ","262","  :user:`Mabel Villalba-Jim¨¦nez <mabelvj>`.","263","  "],"delete":[]}],"sklearn\/linear_model\/tests\/test_ridge.py":[{"add":["591","    rng = np.random.RandomState(42)","604","    assert r.cv_values_.shape == (n_samples, n_alphas)","607","    n_targets = 3","608","    y = rng.randn(n_samples, n_targets)","610","    assert r.cv_values_.shape == (n_samples, n_targets, n_alphas)","611","","612","","613","def test_ridge_classifier_cv_store_cv_values():","614","    x = np.array([[-1.0, -1.0], [-1.0, 0], [-.8, -1.0],","615","                  [1.0, 1.0], [1.0, 0.0]])","616","    y = np.array([1, 1, 1, -1, -1])","617","","618","    n_samples = x.shape[0]","619","    alphas = [1e-1, 1e0, 1e1]","620","    n_alphas = len(alphas)","621","","622","    r = RidgeClassifierCV(alphas=alphas, store_cv_values=True)","623","","624","    # with len(y.shape) == 1","625","    n_targets = 1","626","    r.fit(x, y)","627","    assert r.cv_values_.shape == (n_samples, n_targets, n_alphas)","628","","629","    # with len(y.shape) == 2","630","    y = np.array([[1, 1, 1, -1, -1],","631","                  [1, -1, 1, -1, 1],","632","                  [-1, -1, 1, -1, -1]]).transpose()","633","    n_targets = y.shape[1]","634","    r.fit(x, y)","635","    assert r.cv_values_.shape == (n_samples, n_targets, n_alphas)","658","        assert ridgecv.alpha_ == gs.best_estimator_.alpha"],"delete":["591","    # Test _RidgeCV's store_cv_values attribute.","592","    rng = rng = np.random.RandomState(42)","605","    assert_equal(r.cv_values_.shape, (n_samples, n_alphas))","608","    n_responses = 3","609","    y = rng.randn(n_samples, n_responses)","611","    assert_equal(r.cv_values_.shape, (n_samples, n_responses, n_alphas))","634","        assert_equal(ridgecv.alpha_, gs.best_estimator_.alpha)"]}],"sklearn\/linear_model\/ridge.py":[{"add":["1216","        each alpha should be stored in the ``cv_values_`` attribute (see","1217","        below). This flag is only compatible with ``cv=None`` (i.e. using","1224","        Cross-validation values for each alpha (if ``store_cv_values=True``\\","1225","        and ``cv=None``). After ``fit()`` has been called, this attribute \\","1226","        will contain the mean squared errors (by default) or the values \\","1227","        of the ``{loss,score}_func`` function (if provided in the constructor).","1305","    store_cv_values : boolean, default=False","1306","        Flag indicating if the cross-validation values corresponding to","1307","        each alpha should be stored in the ``cv_values_`` attribute (see","1308","        below). This flag is only compatible with ``cv=None`` (i.e. using","1309","        Generalized Cross-Validation).","1310","","1313","    cv_values_ : array, shape = [n_samples, n_targets, n_alphas], optional","1314","        Cross-validation values for each alpha (if ``store_cv_values=True`` and","1315","        ``cv=None``). After ``fit()`` has been called, this attribute will","1316","        contain the mean squared errors (by default) or the values of the","1317","        ``{loss,score}_func`` function (if provided in the constructor).","1343","                 normalize=False, scoring=None, cv=None, class_weight=None,","1344","                 store_cv_values=False):","1347","            scoring=scoring, cv=cv, store_cv_values=store_cv_values)"],"delete":["1216","        each alpha should be stored in the `cv_values_` attribute (see","1217","        below). This flag is only compatible with `cv=None` (i.e. using","1224","        Cross-validation values for each alpha (if `store_cv_values=True` and \\","1225","        `cv=None`). After `fit()` has been called, this attribute will \\","1226","        contain the mean squared errors (by default) or the values of the \\","1227","        `{loss,score}_func` function (if provided in the constructor).","1307","    cv_values_ : array, shape = [n_samples, n_alphas] or \\","1308","    shape = [n_samples, n_responses, n_alphas], optional","1309","        Cross-validation values for each alpha (if `store_cv_values=True` and","1310","    `cv=None`). After `fit()` has been called, this attribute will contain \\","1311","    the mean squared errors (by default) or the values of the \\","1312","    `{loss,score}_func` function (if provided in the constructor).","1338","                 normalize=False, scoring=None, cv=None, class_weight=None):","1341","            scoring=scoring, cv=cv)"]}]}},"6a3562281103d30c8bb937ee64c3b349c3d76370":{"changes":{"sklearn\/linear_model\/tests\/test_least_angle.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/linear_model\/tests\/test_randomized_l1.py":"MODIFY","sklearn\/linear_model\/least_angle.py":"MODIFY"},"diff":{"sklearn\/linear_model\/tests\/test_least_angle.py":[{"add":["14","from sklearn.utils.testing import assert_warns","432","    X = np.c_[X, rng.randn(X.shape[0], 5)]  # add 5 bad features"],"delete":["14","from sklearn.utils.testing import assert_no_warnings, assert_warns","432","    X = np.c_[X, rng.randn(X.shape[0], 4)]  # add 4 bad features","446","def test_no_warning_for_zero_mse():","447","    # LassoLarsIC should not warn for log of zero MSE.","448","    y = np.arange(10, dtype=float)","449","    X = y.reshape(-1, 1)","450","    lars = linear_model.LassoLarsIC(normalize=False)","451","    assert_no_warnings(lars.fit, X, y)","452","    assert_true(np.any(np.isinf(lars.criterion_)))","453","","454",""]}],"doc\/whats_new.rst":[{"add":["374","   - Fix AIC\/BIC criterion computation in :class:`linear_model.LassoLarsIC`","375","     by `Alexandre Gramfort`_ and :user:`Mehmet Basbug <mehmetbasbug>`.","376",""],"delete":[]}],"sklearn\/linear_model\/tests\/test_randomized_l1.py":[{"add":["12","from sklearn.utils.testing import assert_allclose","97","    assert_allclose(feature_scores, [1., 1., 1., 0.225, 1.], rtol=0.2)"],"delete":["96","    assert_array_equal(feature_scores, X.shape[1] * [1.])"]}],"sklearn\/linear_model\/least_angle.py":[{"add":["1402","        alphas. The alpha which has the smallest information criteria is chosen.","1403","        This value is larger by a factor of ``n_samples`` compared to Eqns. 2.15","1404","        and 2.16 in (Zou et al, 2007).","1405","","1491","        sigma2 = np.var(y)","1504","        eps64 = np.finfo('float64').eps","1505","        self.criterion_ = (n_samples * mean_squared_error \/ (sigma2 + eps64) +","1506","                           K * df)  # Eqns. 2.15--16 in (Zou et al, 2007)"],"delete":["1402","        alphas. The alpha which has the smallest information criteria","1403","        is chosen.","1501","        with np.errstate(divide='ignore'):","1502","            self.criterion_ = n_samples * np.log(mean_squared_error) + K * df"]}]}}}