{"cb1b6c4734b2989ad0492b56c326858d030f3fa2":{"changes":{"benchmarks\/.gitignore":"ADD","sklearn\/neighbors\/quad_tree.pyx":"ADD","examples\/manifold\/plot_t_sne_perplexity.py":"MODIFY","sklearn\/manifold\/tests\/test_t_sne.py":"MODIFY","sklearn\/mixture\/base.py":"MODIFY","sklearn\/tree\/_utils.pxd":"MODIFY","benchmarks\/plot_tsne_mnist.py":"ADD","sklearn\/manifold\/_utils.pyx":"MODIFY","sklearn\/neighbors\/quad_tree.pxd":"ADD","doc\/whats_new.rst":"MODIFY","benchmarks\/bench_tsne_mnist.py":"ADD","sklearn\/manifold\/setup.py":"MODIFY","sklearn\/manifold\/_barnes_hut_tsne.pyx":"MODIFY","sklearn\/neighbors\/tests\/test_quad_tree.py":"ADD","sklearn\/manifold\/t_sne.py":"MODIFY","sklearn\/neighbors\/setup.py":"MODIFY"},"diff":{"benchmarks\/.gitignore":[{"add":[],"delete":[]}],"sklearn\/neighbors\/quad_tree.pyx":[{"add":[],"delete":[]}],"examples\/manifold\/plot_t_sne_perplexity.py":[{"add":["16","visually diverge from S-curve topology on the S-curve dataset even for","30","import numpy as np","37","n_samples = 300","39","(fig, subplots) = plt.subplots(3, 5, figsize=(15, 8))","40","perplexities = [5, 30, 50, 100]","74","ax.scatter(X[:, 0], X[:, 2], c=color, cmap=plt.cm.viridis)","89","    ax.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.viridis)","94","","95","# Another example using a 2D uniform grid","96","x = np.linspace(0, 1, int(np.sqrt(n_samples)))","97","xx, yy = np.meshgrid(x, x)","98","X = np.hstack([","99","    xx.ravel().reshape(-1, 1),","100","    yy.ravel().reshape(-1, 1),","101","])","102","color = xx.ravel()","103","ax = subplots[2][0]","104","ax.scatter(X[:, 0], X[:, 1], c=color, cmap=plt.cm.viridis)","105","ax.xaxis.set_major_formatter(NullFormatter())","106","ax.yaxis.set_major_formatter(NullFormatter())","107","","108","for i, perplexity in enumerate(perplexities):","109","    ax = subplots[2][i + 1]","110","","111","    t0 = time()","112","    tsne = manifold.TSNE(n_components=n_components, init='random',","113","                         random_state=0, perplexity=perplexity)","114","    Y = tsne.fit_transform(X)","115","    t1 = time()","116","    print(\"uniform grid, perplexity=%d in %.2g sec\" % (perplexity, t1 - t0))","117","","118","    ax.set_title(\"Perplexity=%d\" % perplexity)","119","    ax.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.viridis)","120","    ax.xaxis.set_major_formatter(NullFormatter())","121","    ax.yaxis.set_major_formatter(NullFormatter())","122","    ax.axis('tight')","123","","124",""],"delete":["16","visually diverge from S-curve topology on the S-curve dateset even for","36","n_samples = 500","38","(fig, subplots) = plt.subplots(2, 5, figsize=(15, 8))","39","perplexities = [5, 50, 100, 150]","73","ax.scatter(X[:, 0], X[:, 2], c=color, cmap=plt.cm.Spectral)","88","    ax.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.Spectral)"]}],"sklearn\/manifold\/tests\/test_t_sne.py":[{"add":["6","from sklearn.neighbors import NearestNeighbors","13","from sklearn.utils.testing import assert_greater","34","x = np.linspace(0, 1, 10)","35","xx, yy = np.meshgrid(x, x)","36","X_2d_grid = np.hstack([","37","    xx.ravel().reshape(-1, 1),","38","    yy.ravel().reshape(-1, 1),","39","])","40","","41","","62","            min_gain=0.0, min_grad_norm=1e-5, verbose=2)","78","            min_gain=0.0, min_grad_norm=0.0, verbose=2)","94","            min_gain=0.0, min_grad_norm=0.0, verbose=2)","136","    neighbors_nn = np.argsort(distances, axis=1)[:, 1:k].astype(np.int64)","137","    distances_nn = np.array([distances[k, neighbors_nn[k]]","138","                            for k in range(n_samples)])","139","    P2 = _binary_search_perplexity(distances_nn, neighbors_nn,","141","    P_nn = np.array([P1[k, neighbors_nn[k]] for k in range(n_samples)])","142","    assert_array_almost_equal(P_nn, P2, decimal=4)","145","    for k in np.linspace(80, n_samples, 5):","149","        distances_nn = np.array([distances[k, neighbors_nn[k]]","150","                                for k in range(n_samples)])","151","        P2k = _binary_search_perplexity(distances_nn, neighbors_nn,","155","        idx = np.argsort(P2k.ravel())[::-1]","177","        # Convert the sparse matrix to a dense one for testing","178","        P1 = P1.toarray()","197","    distances = np.abs(distances.dot(distances.T))","199","    X_embedded = random_state.randn(n_samples, n_components).astype(np.float32)","239","    X = random_state.randn(50, n_components).astype(np.float32)","242","            tsne = TSNE(n_components=n_components, init=init, random_state=0,","245","            t = trustworthiness(X, X_embedded, n_neighbors=1)","246","            assert_greater(t, 0.9)","254","    for n_iter in [250, 300, 350]:","279","    for i in range(3):","280","        X = random_state.randn(100, 2)","281","        D = squareform(pdist(X), \"sqeuclidean\")","282","        tsne = TSNE(n_components=2, perplexity=2, learning_rate=100.0,","283","                    early_exaggeration=2.0, metric=\"precomputed\",","284","                    random_state=i, verbose=0)","285","        X_embedded = tsne.fit_transform(D)","286","        t = trustworthiness(D, X_embedded, n_neighbors=1,","287","                            precomputed=True)","288","        assert t > .95","312","def test_non_positive_precomputed_distances():","313","    # Precomputed distance matrices must be positive.","314","    bad_dist = np.array([[0., -1.], [1., 0.]])","315","    for method in ['barnes_hut', 'exact']:","316","        tsne = TSNE(metric=\"precomputed\", method=method)","317","        assert_raises_regexp(ValueError, \"All distances .*precomputed.*\",","318","                             tsne.fit_transform, bad_dist)","319","","320","","321","def test_non_positive_computed_distances():","322","    # Computed distance matrices must be positive.","323","    def metric(x, y):","324","        return -1","325","","326","    tsne = TSNE(metric=metric, method='exact')","327","    X = np.array([[0.0, 0.0], [1.0, 1.0]])","328","    assert_raises_regexp(ValueError, \"All distances .*metric given.*\",","329","                         tsne.fit_transform, X)","330","","331","","334","    tsne = TSNE(init=\"not available\")","336","    assert_raises_regexp(ValueError, m, tsne.fit_transform,","337","                         np.array([[0.0], [1.0]]))","356","    tsne = TSNE(metric=\"not available\", method='exact')","360","    tsne = TSNE(metric=\"not available\", method='barnes_hut')","361","    assert_raises_regexp(ValueError, \"Metric 'not available' not valid.*\",","362","                         tsne.fit_transform, np.array([[0.0], [1.0]]))","363","","364","","365","def test_method_not_available():","366","    # 'nethod' must be 'barnes_hut' or 'exact'","367","    tsne = TSNE(method='not available')","368","    assert_raises_regexp(ValueError, \"'method' must be 'barnes_hut' or \",","369","                         tsne.fit_transform, np.array([[0.0], [1.0]]))","370","","371","","372","def test_angle_out_of_range_checks():","373","    # check the angle parameter range","374","    for angle in [-1, -1e-6, 1 + 1e-6, 2]:","375","        tsne = TSNE(angle=angle)","376","        assert_raises_regexp(ValueError, \"'angle' must be between 0.0 - 1.0\",","377","                             tsne.fit_transform, np.array([[0.0], [1.0]]))","378","","388","def test_n_components_range():","389","    # barnes_hut method should only be used with n_components <= 3","390","    tsne = TSNE(n_components=4, method=\"barnes_hut\")","391","    assert_raises_regexp(ValueError, \"'n_components' should be .*\",","392","                         tsne.fit_transform, np.array([[0.0], [1.0]]))","393","","394","","395","def test_early_exaggeration_used():","396","    # check that the ``early_exaggeration`` parameter has an effect","397","    random_state = check_random_state(0)","398","    n_components = 2","399","    methods = ['exact', 'barnes_hut']","400","    X = random_state.randn(25, n_components).astype(np.float32)","401","    for method in methods:","402","        tsne = TSNE(n_components=n_components, perplexity=1,","403","                    learning_rate=100.0, init=\"pca\", random_state=0,","404","                    method=method, early_exaggeration=1.0)","405","        X_embedded1 = tsne.fit_transform(X)","406","        tsne = TSNE(n_components=n_components, perplexity=1,","407","                    learning_rate=100.0, init=\"pca\", random_state=0,","408","                    method=method, early_exaggeration=10.0)","409","        X_embedded2 = tsne.fit_transform(X)","410","","411","        assert not np.allclose(X_embedded1, X_embedded2)","412","","413","","414","def test_n_iter_used():","415","    # check that the ``n_iter`` parameter has an effect","416","    random_state = check_random_state(0)","417","    n_components = 2","418","    methods = ['exact', 'barnes_hut']","419","    X = random_state.randn(25, n_components).astype(np.float32)","420","    for method in methods:","421","        for n_iter in [251, 500]:","422","            tsne = TSNE(n_components=n_components, perplexity=1,","423","                        learning_rate=0.5, init=\"random\", random_state=0,","424","                        method=method, early_exaggeration=1.0, n_iter=n_iter)","425","            tsne.fit_transform(X)","426","","427","            assert tsne.n_iter_ == n_iter - 1","428","","429","","503","    from scipy.sparse import csr_matrix","504","    P = csr_matrix(pij_input)","505","","506","    neighbors = P.indices.astype(np.int64)","507","    indptr = P.indptr.astype(np.int64)","508","","509","    _barnes_hut_tsne.gradient(P.data, pos_output, neighbors, indptr,","530","    assert(\"nearest neighbors...\" in out)","570","            X = random_state.randn(50, 2).astype(dt)","572","                        random_state=0, method=method, verbose=0)","573","            X_embedded = tsne.fit_transform(X)","574","            effective_type = X_embedded.dtype","575","","576","            # tsne cython code is only single precision, so the output will","577","            # always be single precision, irrespectively of the input dtype","578","            assert effective_type == np.float32","593","        distances = abs(distances.dot(distances.T))","596","        P = _joint_probabilities(distances, perplexity, verbose=0)","597","        kl_exact, grad_exact = _kl_divergence(params, P, degrees_of_freedom,","598","                                              n_samples, n_components)","604","        distances_nn = np.array([distances[i, neighbors_nn[i]]","605","                                 for i in range(n_samples)])","606","        assert np.all(distances[0, neighbors_nn[0]] == distances_nn[0]),\\","607","            abs(distances[0, neighbors_nn[0]] - distances_nn[0])","608","        P_bh = _joint_probabilities_nn(distances_nn, neighbors_nn,","609","                                       perplexity, verbose=0)","610","        kl_bh, grad_bh = _kl_divergence_bh(params, P_bh, degrees_of_freedom,","611","                                           n_samples, n_components,","612","                                           angle=angle, skip_num_points=0,","613","                                           verbose=0)","615","        P = squareform(P)","616","        P_bh = P_bh.toarray()","617","        assert_array_almost_equal(P_bh, P, decimal=5)","618","        assert_almost_equal(kl_exact, kl_bh, decimal=3)","625","    X = random_state.randn(100, 10)","626","    for method in [\"barnes_hut\", \"exact\"]:","627","        tsne = TSNE(n_iter_without_progress=-1, verbose=2, learning_rate=1e8,","628","                    random_state=0, method=method, n_iter=351, init=\"random\")","629","        tsne._N_ITER_CHECK = 1","630","        tsne._EXPLORATION_N_ITER = 0","632","        old_stdout = sys.stdout","633","        sys.stdout = StringIO()","634","        try:","635","            tsne.fit_transform(X)","636","        finally:","637","            out = sys.stdout.getvalue()","638","            sys.stdout.close()","639","            sys.stdout = old_stdout","641","        # The output needs to contain the value of n_iter_without_progress","642","        assert_in(\"did not make any progress during the \"","643","                  \"last -1 episodes. Finished.\", out)","676","            line = line.replace('gradient norm = ', '').split(' ')[0]","714","","715","","716","def check_uniform_grid(method, seeds=[0, 1, 2], n_iter=1000):","717","    \"\"\"Make sure that TSNE can approximately recover a uniform 2D grid\"\"\"","718","    for seed in seeds:","719","        tsne = TSNE(n_components=2, init='random', random_state=seed,","720","                    perplexity=10, n_iter=n_iter, method=method)","721","        Y = tsne.fit_transform(X_2d_grid)","722","","723","        # Ensure that the convergence criterion has been triggered","724","        assert tsne.n_iter_ < n_iter","725","","726","        # Ensure that the resulting embedding leads to approximately","727","        # uniformly spaced points: the distance to the closest neighbors","728","        # should be non-zero and approximately constant.","729","        nn = NearestNeighbors(n_neighbors=1).fit(Y)","730","        dist_to_nn = nn.kneighbors(return_distance=True)[0].ravel()","731","        assert dist_to_nn.min() > 0.1","732","","733","        smallest_to_mean = dist_to_nn.min() \/ np.mean(dist_to_nn)","734","        largest_to_mean = dist_to_nn.max() \/ np.mean(dist_to_nn)","735","","736","        try_name = \"{}_{}\".format(method, seed)","737","        assert_greater(smallest_to_mean, .5, msg=try_name)","738","        assert_less(largest_to_mean, 2, msg=try_name)","739","","740","","741","def test_uniform_grid():","742","    for method in ['barnes_hut', 'exact']:","743","        yield check_uniform_grid, method","744","","745","","746","def test_bh_match_exact():","747","    # check that the ``barnes_hut`` method match the exact one when","748","    # ``angle = 0`` and ``perplexity > n_samples \/ 3``","749","    random_state = check_random_state(0)","750","    n_features = 10","751","    X = random_state.randn(30, n_features).astype(np.float32)","752","    X_embeddeds = {}","753","    n_iter = {}","754","    for method in ['exact', 'barnes_hut']:","755","        tsne = TSNE(n_components=2, method=method, learning_rate=1.0,","756","                    init=\"random\", random_state=0, n_iter=251,","757","                    perplexity=30.0, angle=0)","758","        # Kill the early_exaggeration","759","        tsne._EXPLORATION_N_ITER = 0","760","        X_embeddeds[method] = tsne.fit_transform(X)","761","        n_iter[method] = tsne.n_iter_","762","","763","    assert n_iter['exact'] == n_iter['barnes_hut']","764","    assert_array_almost_equal(X_embeddeds['exact'], X_embeddeds['barnes_hut'],","765","                              decimal=3)"],"delete":["52","            min_gain=0.0, min_grad_norm=1e-5, min_error_diff=0.0, verbose=2)","61","    # Error difference","62","    old_stdout = sys.stdout","63","    sys.stdout = StringIO()","64","    try:","65","        _, error, it = _gradient_descent(","66","            ObjectiveSmallGradient(), np.zeros(1), 0, n_iter=100,","67","            n_iter_without_progress=100, momentum=0.0, learning_rate=0.0,","68","            min_gain=0.0, min_grad_norm=0.0, min_error_diff=0.2, verbose=2)","69","    finally:","70","        out = sys.stdout.getvalue()","71","        sys.stdout.close()","72","        sys.stdout = old_stdout","73","    assert_equal(error, 0.9)","74","    assert_equal(it, 1)","75","    assert(\"error difference\" in out)","76","","84","            min_gain=0.0, min_grad_norm=0.0, min_error_diff=-1.0, verbose=2)","100","            min_gain=0.0, min_grad_norm=0.0, min_error_diff=0.0, verbose=2)","142","    neighbors_nn = np.argsort(distances, axis=1)[:, :k].astype(np.int64)","143","    P2 = _binary_search_perplexity(distances, neighbors_nn,","145","    assert_array_almost_equal(P1, P2, decimal=4)","148","    for k in np.linspace(80, n_samples, 10):","152","        P2k = _binary_search_perplexity(distances, neighbors_nn,","195","    distances = distances.dot(distances.T)","197","    X_embedded = random_state.randn(n_samples, n_components)","235","    # The Barnes-Hut approximation uses a different method to estimate","236","    # P_ij using only a number of nearest neighbors instead of all","237","    # points (so that k = 3 * perplexity). As a result we set the","238","    # perplexity=5, so that the number of neighbors is 5%.","241","    X = random_state.randn(100, n_components).astype(np.float32)","244","            tsne = TSNE(n_components=n_components, perplexity=50,","245","                        learning_rate=100.0, init=init, random_state=0,","248","            T = trustworthiness(X, X_embedded, n_neighbors=1)","249","            assert_almost_equal(T, 1.0, decimal=1)","257","    for n_iter in [200, 250, 300]:","282","    X = random_state.randn(100, 2)","283","    D = squareform(pdist(X), \"sqeuclidean\")","284","    tsne = TSNE(n_components=2, perplexity=2, learning_rate=100.0,","285","                metric=\"precomputed\", random_state=0, verbose=0)","286","    X_embedded = tsne.fit_transform(D)","287","    assert_almost_equal(trustworthiness(D, X_embedded, n_neighbors=1,","288","                                        precomputed=True), 1.0, decimal=1)","315","    assert_raises_regexp(ValueError, m, TSNE, init=\"not available\")","334","    tsne = TSNE(metric=\"not available\")","420","    _barnes_hut_tsne.gradient(pij_input, pos_output, neighbors,","441","    assert(\"Computing pairwise distances\" in out)","444","    assert(\"Finished\" in out)","446","    assert(\"Finished\" in out)","483","            X = random_state.randn(100, 2).astype(dt)","485","                        random_state=0, method=method)","486","            tsne.fit_transform(X)","501","        distances = distances.dot(distances.T)","504","        P = _joint_probabilities(distances, perplexity, False)","505","        kl, gradex = _kl_divergence(params, P, degrees_of_freedom, n_samples,","506","                                    n_components)","512","        Pbh = _joint_probabilities_nn(distances, neighbors_nn,","513","                                      perplexity, False)","514","        kl, gradbh = _kl_divergence_bh(params, Pbh, neighbors_nn,","515","                                       degrees_of_freedom, n_samples,","516","                                       n_components, angle=angle,","517","                                       skip_num_points=0, verbose=False)","518","        assert_array_almost_equal(Pbh, P, decimal=5)","519","        assert_array_almost_equal(gradex, gradbh, decimal=5)","521","","522","def test_quadtree_similar_point():","523","    # Introduce a point into a quad tree where a similar point already exists.","524","    # Test will hang if it doesn't complete.","525","    Xs = []","526","","527","    # check the case where points are actually different","528","    Xs.append(np.array([[1, 2], [3, 4]], dtype=np.float32))","529","    # check the case where points are the same on X axis","530","    Xs.append(np.array([[1.0, 2.0], [1.0, 3.0]], dtype=np.float32))","531","    # check the case where points are arbitrarily close on X axis","532","    Xs.append(np.array([[1.00001, 2.0], [1.00002, 3.0]], dtype=np.float32))","533","    # check the case where points are the same on Y axis","534","    Xs.append(np.array([[1.0, 2.0], [3.0, 2.0]], dtype=np.float32))","535","    # check the case where points are arbitrarily close on Y axis","536","    Xs.append(np.array([[1.0, 2.00001], [3.0, 2.00002]], dtype=np.float32))","537","    # check the case where points are arbitrarily close on both axes","538","    Xs.append(np.array([[1.00001, 2.00001], [1.00002, 2.00002]],","539","              dtype=np.float32))","540","","541","    # check the case where points are arbitrarily close on both axes","542","    # close to machine epsilon - x axis","543","    Xs.append(np.array([[1, 0.0003817754041], [2, 0.0003817753750]],","544","              dtype=np.float32))","545","","546","    # check the case where points are arbitrarily close on both axes","547","    # close to machine epsilon - y axis","548","    Xs.append(np.array([[0.0003817754041, 1.0], [0.0003817753750, 2.0]],","549","              dtype=np.float32))","550","","551","    for X in Xs:","552","        counts = np.zeros(3, dtype='int64')","553","        _barnes_hut_tsne.check_quadtree(X, counts)","554","        m = \"Tree consistency failed: unexpected number of points at root node\"","555","        assert_equal(counts[0], counts[1], m)","556","        m = \"Tree consistency failed: unexpected number of points on the tree\"","557","        assert_equal(counts[0], counts[2], m)","558","","559","","560","def test_index_offset():","561","    # Make sure translating between 1D and N-D indices are preserved","562","    assert_equal(_barnes_hut_tsne.test_index2offset(), 1)","563","    assert_equal(_barnes_hut_tsne.test_index_offset(), 1)","570","    X = random_state.randn(100, 2)","571","    tsne = TSNE(n_iter_without_progress=-1, verbose=2,","572","                random_state=1, method='exact')","574","    old_stdout = sys.stdout","575","    sys.stdout = StringIO()","576","    try:","577","        tsne.fit_transform(X)","578","    finally:","579","        out = sys.stdout.getvalue()","580","        sys.stdout.close()","581","        sys.stdout = old_stdout","583","    # The output needs to contain the value of n_iter_without_progress","584","    assert_in(\"did not make any progress during the \"","585","              \"last -1 episodes. Finished.\", out)","618","            line = line.replace('gradient norm = ', '')"]}],"sklearn\/mixture\/base.py":[{"add":["353","            Returns the probability each Gaussian (state) in"],"delete":["353","            Returns the probability of each Gaussian (state) in"]}],"sklearn\/tree\/_utils.pxd":[{"add":["12","from _tree cimport Node","13","from sklearn.neighbors.quad_tree cimport Cell","42","    (Cell*)"],"delete":["12","from _tree cimport Node "]}],"benchmarks\/plot_tsne_mnist.py":[{"add":[],"delete":[]}],"sklearn\/manifold\/_utils.pyx":[{"add":["14","        np.ndarray[np.float32_t, ndim=2] affinities,","15","        np.ndarray[np.int64_t, ndim=2] neighbors,","18","    \"\"\"Binary search for sigmas of conditional Gaussians.","19","","25","    affinities : array-like, shape (n_samples, k)","26","        Distances between training samples and its k nearest neighbors.","28","    neighbors : array-like, shape (n_samples, k) or None","29","        Each row contains the indices to the k nearest neigbors. If this","48","    # Precisions of conditional Gaussian distributions","53","","54","    # Use log scale","61","    cdef long i, j, k, l","62","    cdef long n_neighbors = n_samples","66","        n_neighbors = neighbors.shape[1]","67","","68","    # This array is later used as a 32bit array. It has multiple intermediate","69","    # floating point additions that benefit from the extra precision","70","    cdef np.ndarray[np.float64_t, ndim=2] P = np.zeros(","71","        (n_samples, n_neighbors), dtype=np.float64)","84","            for j in range(n_neighbors):","85","                if j != i or using_neighbors:","86","                    P[i, j] = math.exp(-affinities[i, j] * beta)","88","","92","","93","            for j in range(n_neighbors):","94","                P[i, j] \/= sum_Pi","95","                sum_disti_Pi += affinities[i, j] * P[i, j]","96",""],"delete":["14","        np.ndarray[np.float32_t, ndim=2] affinities, ","15","        np.ndarray[np.int64_t, ndim=2] neighbors, ","18","    \"\"\"Binary search for sigmas of conditional Gaussians. ","19","    ","25","    affinities : array-like, shape (n_samples, n_samples)","26","        Distances between training samples.","28","    neighbors : array-like, shape (n_samples, K) or None","29","        Each row contains the indices to the K nearest neigbors. If this","48","    # This array is later used as a 32bit array. It has multiple intermediate","49","    # floating point additions that benefit from the extra precision","50","    cdef np.ndarray[np.float64_t, ndim=2] P = np.zeros((n_samples, n_samples),","51","                                                       dtype=np.float64)","52","    # Precisions of conditional Gaussian distrubutions","57","    # Now we go to log scale","64","    cdef long i, j, k, l = 0","65","    cdef long K = n_samples","69","        K = neighbors.shape[1]","81","            if using_neighbors:","82","                for k in range(K):","83","                    j = neighbors[i, k]","84","                    P[i, j] = math.exp(-affinities[i, j] * beta)","85","            else:","86","                for j in range(K):","87","                    P[i, j] = math.exp(-affinities[i, j] * beta)","88","            P[i, i] = 0.0","90","            if using_neighbors:","91","                for k in range(K):","92","                    j = neighbors[i, k]","94","            else:","95","                for j in range(K):","96","                    sum_Pi += P[i, j]","100","            if using_neighbors:","101","                for k in range(K):","102","                    j = neighbors[i, k]","103","                    P[i, j] \/= sum_Pi","104","                    sum_disti_Pi += affinities[i, j] * P[i, j]","105","            else:","106","                for j in range(K):","107","                    P[i, j] \/= sum_Pi","108","                    sum_disti_Pi += affinities[i, j] * P[i, j]"]}],"sklearn\/neighbors\/quad_tree.pxd":[{"add":[],"delete":[]}],"doc\/whats_new.rst":[{"add":["21","   * :class:`sklearn.manifold.TSNE` (bug fix)","248","   - Memory improvements for method barnes_hut in :class:`manifold.TSNE`","249","     :issue:`7089` by :user:`Thomas Moreau <tomMoral>` and `Olivier Grisel`_.","250","","251","   - Optimization schedule improvements for so the results are closer to the","252","     one from the reference implementation","253","     `lvdmaaten\/bhtsne <https:\/\/github.com\/lvdmaaten\/bhtsne>`_ by","254","     :user:`Thomas Moreau <tomMoral>` and `Olivier Grisel`_.","255","","489","   - Fixed the implementation of :class:`manifold.TSNE`:","490","      - ``early_exageration`` parameter had no effect and is now used for the","491","        first 250 optimization iterations.","492","      - Fixed the ``InsersionError`` reported in :issue:`8992`.","493","      - Improve the learning schedule to match the one from the reference","494","        implementation `lvdmaaten\/bhtsne <https:\/\/github.com\/lvdmaaten\/bhtsne>`_.","495","     by :user:`Thomas Moreau <tomMoral>` and `Olivier Grisel`_.","496",""],"delete":[]}],"benchmarks\/bench_tsne_mnist.py":[{"add":[],"delete":[]}],"sklearn\/manifold\/setup.py":[{"add":["33",""],"delete":[]}],"sklearn\/manifold\/_barnes_hut_tsne.pyx":[{"add":["14","cimport numpy as np","15","","16","from sklearn.neighbors import quad_tree","17","from sklearn.neighbors cimport quad_tree","24","# Smallest strictly positive value that can be represented by floating","25","# point numbers for different precision levels. This is useful to avoid","26","# taking the log of zero when computing the KL divergence.","27","cdef float FLOAT32_TINY = np.finfo(np.float32).tiny","28","","29","# Useful to void division by zero or divergence to +inf.","30","cdef float FLOAT64_EPS = np.finfo(np.float64).eps","45","cdef float compute_gradient(float[:] val_P,","46","                            float[:, :] pos_reference,","47","                            np.int64_t[:] neighbors,","48","                            np.int64_t[:] indptr,","49","                            float[:, :] tot_force,","50","                            quad_tree._QuadTree qt,","57","    cdef:","58","        long i, coord","59","        int ax","60","        long n_samples = pos_reference.shape[0]","61","        int n_dimensions = qt.n_dimensions","62","        double[1] sum_Q","63","        clock_t t1, t2","64","        float sQ, error","65","","66","    if qt.verbose > 11:","67","        printf(\"[t-SNE] Allocating %li elements in force arrays\\n\",","68","                n_samples * n_dimensions * 2)","69","    cdef float* neg_f = <float*> malloc(sizeof(float) * n_samples * n_dimensions)","70","    cdef float* pos_f = <float*> malloc(sizeof(float) * n_samples * n_dimensions)","74","    compute_gradient_negative(pos_reference, neg_f, qt, sum_Q,","77","    if qt.verbose > 15:","81","    error = compute_gradient_positive(val_P, pos_reference, neighbors, indptr,","82","                                      pos_f, n_dimensions, dof, sQ, start,","83","                                      qt.verbose)","85","    if qt.verbose > 15:","87","    for i in range(start, n_samples):","90","            tot_force[i, ax] = pos_f[coord] - (neg_f[coord] \/ sQ)","91","","94","    return error","97","cdef float compute_gradient_positive(float[:] val_P,","98","                                     float[:, :] pos_reference,","99","                                     np.int64_t[:] neighbors,","100","                                     np.int64_t[:] indptr,","104","                                     double sum_Q,","115","        long n_samples = indptr.shape[0] - 1","116","        float dij, qij, pij","119","        float[3] buff","120","        clock_t t1, t2","121","","123","    for i in range(start, n_samples):","124","        # Init the gradient vector","127","        # Compute the positive interaction for the nearest neighbors","128","        for k in range(indptr[i], indptr[i+1]):","129","            j = neighbors[k]","130","            dij = 0.0","131","            pij = val_P[k]","134","                dij += buff[ax] * buff[ax]","135","            qij = (((1.0 + dij) \/ dof) ** exponent)","136","            dij = pij * qij","137","            qij \/= sum_Q","138","            C += pij * log(max(pij, FLOAT32_TINY)","139","                           \/ max(qij, FLOAT32_TINY))","141","                pos_f[i * n_dimensions + ax] += dij * buff[ax]","149","cdef void compute_gradient_negative(float[:, :] pos_reference,","151","                                    quad_tree._QuadTree qt,","152","                                    double* sum_Q,","154","                                    float theta,","155","                                    long start,","158","        stop = pos_reference.shape[0]","161","        int n_dimensions = qt.n_dimensions","162","        long i, j, idx","166","        long offset = n_dimensions + 2","167","        long* l","168","        float size, dist2s, mult","169","        double qijZ","170","        float[1] iQ","171","        float[3] force, neg_force, pos","174","    summary = <float*> malloc(sizeof(float) * n * offset)","186","        idx = qt.summarize(pos, summary, theta*theta)","190","        # is about 10-15x more expensive than the","193","        for j in range(idx \/\/ offset):","194","","195","            dist2s = summary[j * offset + n_dimensions]","196","            size = summary[j * offset + n_dimensions + 1]","197","            qijZ = ((1.0 + dist2s) \/ dof) ** exponent  # 1\/(1+dist)","198","            sum_Q[0] += size * qijZ   # size of the node * q","199","            mult = size * qijZ * qijZ","201","                neg_force[ax] += mult * summary[j * offset + ax]","207","    if qt.verbose > 20:","208","        printf(\"[t-SNE] Tree: %li clock ticks | \", dta)","209","        printf(\"Force computation: %li clock ticks\\n\", dtb)","210","","211","    # Put sum_Q to machine EPSILON to avoid divisions by 0","212","    sum_Q[0] = max(sum_Q[0], FLOAT64_EPS)","213","    free(summary)","216","def gradient(float[:] val_P,","217","             float[:, :] pos_output,","218","             np.int64_t[:] neighbors,","219","             np.int64_t[:] indptr,","220","             float[:, :] forces,","231","    assert val_P.itemsize == 4","237","    assert n == indptr.shape[0] - 1, m","240","    cdef quad_tree._QuadTree qt = quad_tree._QuadTree(pos_output.shape[1],","241","                                                      verbose)","243","        printf(\"[t-SNE] Inserting %li points\\n\", pos_output.shape[0])","244","    qt.build_tree(pos_output)","250","    C = compute_gradient(val_P, pos_output, neighbors, indptr, forces,","251","                         qt, theta, dof, skip_num_points, -1)","258","    assert qt.cells[0].cumulative_size == qt.n_points, m"],"delete":["13","cimport numpy as np","21","# Round points differing by less than this amount","22","# effectively ignoring differences near the 32bit ","23","# floating point precision","24","cdef float EPSILON = 1e-6","39","cdef extern from \"cblas.h\":","40","    float snrm2 \"cblas_snrm2\"(int N, float *X, int incX) nogil","41","","42","","43","cdef struct Node:","44","    # Keep track of the center of mass","45","    float* barycenter","46","    # If this is a leaf, the position of the point within this leaf ","47","    float* leaf_point_position","48","    # The number of points including all ","49","    # nodes below this one","50","    long cumulative_size","51","    # Number of points at this node","52","    long size","53","    # Index of the point at this node","54","    # Only defined for non-empty leaf nodes","55","    long point_index","56","    # level = 0 is the root node","57","    # And each subdivision adds 1 to the level","58","    long level","59","    # Left edge of this node","60","    float* left_edge","61","    # The center of this node, equal to le + w\/2.0","62","    float* center","63","    # The width of this node -- used to calculate the opening","64","    # angle. Equal to width = re - le","65","    float* width","66","    # The value of the maximum width w","67","    float max_width","68","","69","    # Does this node have children?","70","    # Default to leaf until we add points","71","    int is_leaf","72","    # Array of pointers to pointers of children","73","    Node **children","74","    # Keep a pointer to the parent","75","    Node *parent","76","    # Pointer to the tree this node belongs too","77","    Tree* tree","78","","79","cdef struct Tree:","80","    # Holds a pointer to the root node","81","    Node* root_node ","82","    # Number of dimensions in the output","83","    int n_dimensions","84","    # Total number of cells","85","    long n_cells","86","    # Total number of points","87","    long n_points","88","    # Spit out diagnostic information?","89","    int verbose","90","    # How many cells per node? Should go as 2 ** n_dimensionss","91","    int n_cell_per_node","92","","93","cdef Tree* init_tree(float[:] left_edge, float[:] width, int n_dimensions, ","94","                     int verbose) nogil:","95","    # tree is freed by free_tree","96","    cdef Tree* tree = <Tree*> malloc(sizeof(Tree))","97","    tree.n_dimensions = n_dimensions","98","    tree.n_cells = 0","99","    tree.n_points = 0","100","    tree.verbose = verbose","101","    tree.root_node = create_root(left_edge, width, n_dimensions)","102","    tree.root_node.tree = tree","103","    tree.n_cells += 1","104","    tree.n_cell_per_node = 2 ** n_dimensions","105","    if DEBUGFLAG:","106","        printf(\"[t-SNE] Tree initialised. Left_edge = (%1.9e, %1.9e, %1.9e)\\n\",","107","               left_edge[0], left_edge[1], left_edge[2])","108","        printf(\"[t-SNE] Tree initialised. Width = (%1.9e, %1.9e, %1.9e)\\n\",","109","                width[0], width[1], width[2])","110","    return tree","111","","112","cdef Node* create_root(float[:] left_edge, float[:] width, int n_dimensions) nogil:","113","    # Create a default root node","114","    cdef int ax","115","    cdef int n_cell_per_node = 2 ** n_dimensions","116","    # root is freed by free_tree","117","    root = <Node*> malloc(sizeof(Node))","118","    root.is_leaf = 1","119","    root.parent = NULL","120","    root.level = 0","121","    root.cumulative_size = 0","122","    root.size = 0","123","    root.point_index = -1","124","    root.max_width = 0.0","125","    root.width = <float*> malloc(sizeof(float) * n_dimensions)","126","    root.left_edge = <float*> malloc(sizeof(float) * n_dimensions)","127","    root.center = <float*> malloc(sizeof(float) * n_dimensions)","128","    root.barycenter = <float*> malloc(sizeof(float) * n_dimensions)","129","    root.leaf_point_position= <float*> malloc(sizeof(float) * n_dimensions)","130","    root.children = NULL","131","    for ax in range(n_dimensions):","132","        root.width[ax] = width[ax]","133","        root.left_edge[ax] = left_edge[ax]","134","        root.center[ax] = 0.0","135","        root.barycenter[ax] = 0.","136","        root.leaf_point_position[ax] = -1","137","    for ax in range(n_dimensions):","138","        root.max_width = max(root.max_width, root.width[ax])","139","    if DEBUGFLAG:","140","        printf(\"[t-SNE] Created root node %p\\n\", root)","141","    return root","142","","143","cdef Node* create_child(Node *parent, int[3] offset) nogil:","144","    # Create a new child node with default parameters","145","    cdef int ax","146","    # these children are freed by free_recursive","147","    child = <Node *> malloc(sizeof(Node))","148","    child.is_leaf = 1","149","    child.parent = parent","150","    child.level = parent.level + 1","151","    child.size = 0","152","    child.cumulative_size = 0","153","    child.point_index = -1","154","    child.tree = parent.tree","155","    child.max_width = 0.0","156","    child.width = <float*> malloc(sizeof(float) * parent.tree.n_dimensions)","157","    child.left_edge = <float*> malloc(sizeof(float) * parent.tree.n_dimensions)","158","    child.center = <float*> malloc(sizeof(float) * parent.tree.n_dimensions)","159","    child.barycenter = <float*> malloc(sizeof(float) * parent.tree.n_dimensions)","160","    child.leaf_point_position = <float*> malloc(sizeof(float) * parent.tree.n_dimensions)","161","    child.children = NULL","162","    for ax in range(parent.tree.n_dimensions):","163","        child.width[ax] = parent.width[ax] \/ 2.0","164","        child.left_edge[ax] = parent.left_edge[ax] + offset[ax] * parent.width[ax] \/ 2.0","165","        child.center[ax] = child.left_edge[ax] + child.width[ax] \/ 2.0","166","        child.barycenter[ax] = 0.","167","        child.leaf_point_position[ax] = -1.","168","    for ax in range(parent.tree.n_dimensions):","169","        child.max_width = max(child.max_width, child.width[ax])","170","    child.tree.n_cells += 1","171","    return child","172","","173","cdef Node* select_child(Node *node, float[3] pos, long index) nogil:","174","    # Find which sub-node a position should go into","175","    # And return the appropriate node","176","    cdef int* offset = <int*> malloc(sizeof(int) * node.tree.n_dimensions)","177","    cdef int ax, idx","178","    cdef Node* child","179","    cdef int error","180","    for ax in range(node.tree.n_dimensions):","181","        offset[ax] = (pos[ax] - (node.left_edge[ax] + node.width[ax] \/ 2.0)) > 0.","182","    idx = offset2index(offset, node.tree.n_dimensions)","183","    child = node.children[idx]","184","    if DEBUGFLAG:","185","        printf(\"[t-SNE] Offset [%i, %i] with LE [%f, %f]\\n\",","186","               offset[0], offset[1], child.left_edge[0], child.left_edge[1])","187","    free(offset)","188","    return child","189","","190","","191","cdef inline void index2offset(int* offset, int index, int n_dimensions) nogil:","192","    # Convert a 1D index into N-D index; useful for indexing","193","    # children of a quadtree, octree, N-tree","194","    # Quite likely there's a fancy bitshift way of doing this","195","    # since the offset is equivalent to the binary representation","196","    # of the integer index","197","    # We read the offset array left-to-right","198","    # such that the least significat bit is on the right","199","    cdef int rem, k, shift","200","    for k in range(n_dimensions):","201","        shift = n_dimensions -k -1","202","        rem = ((index >> shift) << shift)","203","        offset[k] = rem > 0","204","        if DEBUGFLAG:","205","            printf(\"i2o index %i k %i rem %i offset\", index, k, rem)","206","            for j in range(n_dimensions):","207","                printf(\" %i\", offset[j])","208","            printf(\" n_dimensions %i\\n\", n_dimensions)","209","        index -= rem","210","","211","","212","cdef inline int offset2index(int* offset, int n_dimensions) nogil:","213","    # Calculate the 1:1 index for a given offset array","214","    # We read the offset array right-to-left","215","    # such that the least significat bit is on the right","216","    cdef int dim","217","    cdef int index = 0","218","    for dim in range(n_dimensions):","219","        index += (2 ** dim) * offset[n_dimensions - dim - 1]","220","        if DEBUGFLAG:","221","            printf(\"o2i index %i dim %i            offset\", index, dim)","222","            for j in range(n_dimensions):","223","                printf(\" %i\", offset[j])","224","            printf(\" n_dimensions %i\\n\", n_dimensions)","225","    return index","226","","227","","228","cdef void subdivide(Node* node) nogil:","229","    # This instantiates 2**n_dimensions = n_cell_per_node nodes for the current node","230","    cdef int idx = 0","231","    cdef int* offset = <int*> malloc(sizeof(int) * node.tree.n_dimensions)","232","    node.is_leaf = False","233","    node.children = <Node**> malloc(sizeof(Node*) * node.tree.n_cell_per_node)","234","    for idx in range(node.tree.n_cell_per_node):","235","        index2offset(offset, idx, node.tree.n_dimensions)","236","        node.children[idx] = create_child(node, offset)","237","    free(offset)","238","","239","","240","cdef int insert(Node *root, float pos[3], long point_index, long depth, long","241","        duplicate_count) nogil:","242","    # Introduce a new point into the tree","243","    # by recursively inserting it and subdividng as necessary","244","    # Carefully treat the case of identical points at the same node","245","    # by increasing the root.size and tracking duplicate_count","246","    cdef Node *child","247","    cdef long i","248","    cdef int ax","249","    cdef int not_identical = 1","250","    cdef int n_dimensions = root.tree.n_dimensions","251","    if DEBUGFLAG:","252","        printf(\"[t-SNE] [d=%i] Inserting pos %i [%f, %f] duplicate_count=%i \"","253","                \"into child %p\\n\", depth, point_index, pos[0], pos[1],","254","                duplicate_count, root)    ","255","    # Increment the total number points including this","256","    # node and below it","257","    root.cumulative_size += duplicate_count","258","    # Evaluate the new center of mass, weighting the previous","259","    # center of mass against the new point data","260","    cdef double frac_seen = <double>(root.cumulative_size - 1) \/ (<double>","261","            root.cumulative_size)","262","    cdef double frac_new  = 1.0 \/ <double> root.cumulative_size","263","    # Assert that duplicate_count > 0","264","    if duplicate_count < 1:","265","        return -1","266","    # Assert that the point is inside the left & right edges","267","    for ax in range(n_dimensions):","268","        root.barycenter[ax] *= frac_seen","269","        if (pos[ax] > (root.left_edge[ax] + root.width[ax] + EPSILON)):","270","            printf(\"[t-SNE] Error: point (%1.9e) is above right edge of node \"","271","                    \"(%1.9e)\\n\", pos[ax], root.left_edge[ax] + root.width[ax])","272","            return -1","273","        if (pos[ax] < root.left_edge[ax] - EPSILON):","274","            printf(\"[t-SNE] Error: point (%1.9e) is below left edge of node \"","275","                   \"(%1.9e)\\n\", pos[ax], root.left_edge[ax])","276","            return -1","277","    for ax in range(n_dimensions):","278","        root.barycenter[ax] += pos[ax] * frac_new","279","","280","    # If this node is unoccupied, fill it.","281","    # Otherwise, we need to insert recursively.","282","    # Two insertion scenarios: ","283","    # 1) Insert into this node if it is a leaf and empty","284","    # 2) Subdivide this node if it is currently occupied","285","    if (root.size == 0) & root.is_leaf:","286","        # Root node is empty and a leaf","287","        if DEBUGFLAG:","288","            printf(\"[t-SNE] [d=%i] Inserting [%f, %f] into blank cell\\n\", depth,","289","                   pos[0], pos[1])","290","        for ax in range(n_dimensions):","291","            root.leaf_point_position[ax] = pos[ax]","292","        root.point_index = point_index","293","        root.size = duplicate_count","294","        return 0","295","    else:","296","        # Root node is occupied or not a leaf","297","        if DEBUGFLAG:","298","            printf(\"[t-SNE] [d=%i] Node %p is occupied or is a leaf.\\n\", depth,","299","                    root)","300","            printf(\"[t-SNE] [d=%i] Node %p leaf = %i. Size %i\\n\", depth, root,","301","                    root.is_leaf, root.size)","302","        if root.is_leaf & (root.size > 0):","303","            # is a leaf node and is occupied","304","            for ax in range(n_dimensions):","305","                not_identical &= (fabsf(pos[ax] - root.leaf_point_position[ax]) < EPSILON)","306","                not_identical &= (root.point_index != point_index)","307","            if not_identical == 1:","308","                root.size += duplicate_count","309","                if DEBUGFLAG:","310","                    printf(\"[t-SNE] Warning: [d=%i] Detected identical \"","311","                            \"points. Returning. Leaf now has size %i\\n\",","312","                            depth, root.size)","313","                return 0","314","        # If necessary, subdivide this node before","315","        # descending","316","        if root.is_leaf:","317","            if DEBUGFLAG:","318","                printf(\"[t-SNE] [d=%i] Subdividing this leaf node %p\\n\", depth,","319","                        root)","320","            subdivide(root)","321","        # We have two points to relocate: the one previously","322","        # at this node, and the new one we're attempting","323","        # to insert","324","        if root.size > 0:","325","            child = select_child(root, root.leaf_point_position, root.point_index)","326","            if DEBUGFLAG:","327","                printf(\"[t-SNE] [d=%i] Relocating old point to node %p\\n\",","328","                        depth, child)","329","            insert(child, root.leaf_point_position, root.point_index, depth + 1, root.size)","330","        # Insert the new point","331","        if DEBUGFLAG:","332","            printf(\"[t-SNE] [d=%i] Selecting node for new point\\n\", depth)","333","        child = select_child(root, pos, point_index)","334","        if root.size > 0:","335","            # Remove the point from this node","336","            for ax in range(n_dimensions):","337","                root.leaf_point_position[ax] = -1            ","338","            root.size = 0","339","            root.point_index = -1            ","340","        return insert(child, pos, point_index, depth + 1, 1)","341","","342","cdef int insert_many(Tree* tree, float[:,:] pos_array) nogil:","343","    # Insert each data point into the tree one at a time","344","    cdef long nrows = pos_array.shape[0]","345","    cdef long i","346","    cdef int ax","347","    cdef float row[3]","348","    cdef long err = 0","349","    for i in range(nrows):","350","        for ax in range(tree.n_dimensions):","351","            row[ax] = pos_array[i, ax]","352","        if DEBUGFLAG:","353","            printf(\"[t-SNE] inserting point %i: [%f, %f]\\n\", i, row[0], row[1])","354","        err = insert(tree.root_node, row, i, 0, 1)","355","        if err != 0:","356","            printf(\"[t-SNE] ERROR\\n%s\", EMPTY_STRING)","357","            return err","358","        tree.n_points += 1","359","    return err","360","","361","cdef int free_tree(Tree* tree) nogil:","362","    cdef int check","363","    cdef long* cnt = <long*> malloc(sizeof(long) * 3)","364","    for i in range(3):","365","        cnt[i] = 0","366","    free_recursive(tree, tree.root_node, cnt)","367","    check = cnt[0] == tree.n_cells","368","    check &= cnt[2] == tree.n_points","369","    free(tree)","370","    free(cnt)","371","    return check","372","","373","cdef void free_post_children(Node *node) nogil:","374","    free(node.width)","375","    free(node.left_edge)","376","    free(node.center)","377","    free(node.barycenter)","378","    free(node.leaf_point_position)","379","    free(node)","380","","381","cdef void free_recursive(Tree* tree, Node *root, long* counts) nogil:","382","    # Free up all of the tree nodes recursively","383","    # while counting the number of nodes visited","384","    # and total number of data points removed","385","    cdef int idx","386","    cdef Node* child","387","    if not root.is_leaf:","388","        for idx in range(tree.n_cell_per_node):","389","            child = root.children[idx]","390","            free_recursive(tree, child, counts)","391","            counts[0] += 1","392","            if child.is_leaf:","393","                counts[1] += 1","394","                if child.size > 0:","395","                    counts[2] +=1","396","            else:","397","                free(child.children)","398","","399","            free_post_children(child)","400","","401","    if root == tree.root_node:","402","        if not root.is_leaf:","403","            free(root.children)","404","","405","        free_post_children(root)","406","","407","cdef long count_points(Node* root, long count) nogil:","408","    # Walk through the whole tree and count the number ","409","    # of points at the leaf nodes","410","    if DEBUGFLAG:","411","        printf(\"[t-SNE] Counting nodes at root node %p\\n\", root)","412","    cdef Node* child","413","    cdef int idx","414","    if root.is_leaf:","415","        count += root.size","416","        if DEBUGFLAG : ","417","            printf(\"[t-SNE] %p is a leaf node, no children\\n\", root)","418","            printf(\"[t-SNE] %i points in node %p\\n\", count, root)","419","        return count","420","    # Otherwise, get the children","421","    for idx in range(root.tree.n_cell_per_node):","422","        child = root.children[idx]","423","        if DEBUGFLAG:","424","            printf(\"[t-SNE] Counting points for child %p\\n\", child)","425","        if child.is_leaf and child.size > 0:","426","            if DEBUGFLAG:","427","                printf(\"[t-SNE] Child has size %d\\n\", child.size)","428","            count += child.size","429","        elif not child.is_leaf:","430","            if DEBUGFLAG:","431","                printf(\"[t-SNE] Child is not a leaf. Descending\\n%s\", EMPTY_STRING)","432","            count = count_points(child, count)","433","        # else case is we have an empty leaf node","434","        # which happens when we create a quadtree for","435","        # one point, and then the other neighboring cells","436","        # don't get filled in","437","    if DEBUGFLAG:","438","        printf(\"[t-SNE] %i points in this node\\n\", count)","439","    return count","440","","441","","442","cdef float compute_gradient(float[:,:] val_P,","443","                            float[:,:] pos_reference,","444","                            np.int64_t[:,:] neighbors,","445","                            float[:,:] tot_force,","446","                            Node* root_node,","453","    cdef long i, coord","454","    cdef int ax","455","    cdef long n = pos_reference.shape[0]","456","    cdef int n_dimensions = root_node.tree.n_dimensions","457","    if root_node.tree.verbose > 11:","458","        printf(\"[t-SNE] Allocating %i elements in force arrays\\n\",","459","                n * n_dimensions * 2)","460","    cdef float* sum_Q = <float*> malloc(sizeof(float))","461","    cdef float* neg_f = <float*> malloc(sizeof(float) * n * n_dimensions)","462","    cdef float* neg_f_fast = <float*> malloc(sizeof(float) * n * n_dimensions)","463","    cdef float* pos_f = <float*> malloc(sizeof(float) * n * n_dimensions)","464","    cdef clock_t t1, t2","465","    cdef float sQ, error","469","    compute_gradient_negative(val_P, pos_reference, neg_f, root_node, sum_Q,","472","    if root_node.tree.verbose > 15:","476","    error = compute_gradient_positive(val_P, pos_reference, neighbors, pos_f,","477","                              n_dimensions, dof, sQ, start, root_node.tree.verbose)","479","    if root_node.tree.verbose > 15:","481","    for i in range(start, n):","484","            tot_force[i, ax] = pos_f[coord] - (neg_f[coord] \/ sum_Q[0])","485","    free(sum_Q)","487","    free(neg_f_fast)","489","    return sQ","492","cdef float compute_gradient_positive(float[:,:] val_P,","493","                                     float[:,:] pos_reference,","494","                                     np.int64_t[:,:] neighbors,","498","                                     float sum_Q,","509","        long K = neighbors.shape[1]","510","        long n = val_P.shape[0]","511","        float[3] buff","512","        float D, Q, pij","515","    cdef clock_t t1, t2","517","    for i in range(start, n):","520","        for k in range(K):","521","            j = neighbors[i, k]","522","            # we don't need to exclude the i==j case since we've ","523","            # already thrown it out from the list of neighbors","524","            D = 0.0","525","            Q = 0.0","526","            pij = val_P[i, j]","529","                D += buff[ax] ** 2.0  ","530","            Q = (((1.0 + D) \/ dof) ** exponent)","531","            D = pij * Q","532","            Q \/= sum_Q","533","            C += pij * log((pij + EPSILON) \/ (Q + EPSILON))","535","                pos_f[i * n_dimensions + ax] += D * buff[ax]","543","","544","cdef void compute_gradient_negative(float[:,:] val_P, ","545","                                    float[:,:] pos_reference,","547","                                    Node *root_node,","548","                                    float* sum_Q,","550","                                    float theta, ","551","                                    long start, ","554","        stop = pos_reference.shape[0] ","557","        long i, j","559","        float* force","560","        float* iQ ","561","        float* pos","562","        float* dist2s","563","        long* sizes","564","        float* deltas","565","        long* l","566","        int n_dimensions = root_node.tree.n_dimensions","567","        float qijZ, mult","568","        long idx, ","572","        float* neg_force","574","    iQ = <float*> malloc(sizeof(float))","575","    force = <float*> malloc(sizeof(float) * n_dimensions)","576","    pos = <float*> malloc(sizeof(float) * n_dimensions)","577","    dist2s = <float*> malloc(sizeof(float) * n)","578","    sizes = <long*> malloc(sizeof(long) * n)","579","    deltas = <float*> malloc(sizeof(float) * n * n_dimensions)","580","    l = <long*> malloc(sizeof(long))","581","    neg_force= <float*> malloc(sizeof(float) * n_dimensions)","590","        l[0] = 0","594","        compute_non_edge_forces(root_node, theta, i, pos, force, dist2s,","595","                                     sizes, deltas, l)","599","        # is about 10-15x more expensive than the ","602","        for j in range(l[0]):","603","            qijZ = ((1.0 + dist2s[j]) \/ dof) ** exponent","604","            sum_Q[0] += sizes[j] * qijZ","605","            mult = sizes[j] * qijZ * qijZ","607","                idx = j * n_dimensions + ax","608","                neg_force[ax] += mult * deltas[idx]","614","    if root_node.tree.verbose > 20:","615","        printf(\"[t-SNE] Tree: %i clock ticks | \", dta)","616","        printf(\"Force computation: %i clock ticks\\n\", dtb)","617","    free(iQ)","618","    free(force)","619","    free(pos)","620","    free(dist2s)","621","    free(sizes)","622","    free(deltas)","623","    free(l)","624","    free(neg_force)","627","cdef void compute_non_edge_forces(Node* node, ","628","                                  float theta,","629","                                  long point_index,","630","                                  float* pos,","631","                                  float* force,","632","                                  float* dist2s,","633","                                  long* sizes,","634","                                  float* deltas,","635","                                  long* l) nogil:","636","    # Compute the t-SNE force on the point in pos given by point_index","637","    cdef:","638","        Node* child","639","        int i, j","640","        int n_dimensions = node.tree.n_dimensions","641","        long idx, idx1","642","        float dist_check","643","    ","644","    # There are no points below this node if cumulative_size == 0","645","    # so do not bother to calculate any force contributions","646","    # Also do not compute self-interactions","647","    if node.cumulative_size > 0 and not (node.is_leaf and (node.point_index ==","648","        point_index)):","649","        # Compute distance between node center of mass and the reference point","650","        # I've tried rewriting this in terms of BLAS functions, but it's about","651","        # 1.5x worse when we do so, probbaly because the vectors are small","652","        idx1 = l[0] * n_dimensions","653","        deltas[idx1] = pos[0] - node.barycenter[0]","654","        idx = idx1","655","        for i in range(1, n_dimensions):","656","            idx += 1","657","            deltas[idx] = pos[i] - node.barycenter[i] ","658","        # do np.sqrt(np.sum(deltas**2.0))","659","        dist2s[l[0]] = snrm2(n_dimensions, &deltas[idx1], 1)","660","        # Check whether we can use this node as a summary","661","        # It's a summary node if the angular size as measured from the point","662","        # is relatively small (w.r.t. to theta) or if it is a leaf node.","663","        # If it can be summarized, we use the cell center of mass ","664","        # Otherwise, we go a higher level of resolution and into the leaves.","665","        if node.is_leaf or ((node.max_width \/ dist2s[l[0]]) < theta):","666","            # Compute the t-SNE force between the reference point and the","667","            # current node","668","            sizes[l[0]] = node.cumulative_size","669","            dist2s[l[0]] = dist2s[l[0]] * dist2s[l[0]]","670","            l[0] += 1","671","        else:","672","            # Recursively apply Barnes-Hut to child nodes","673","            for idx in range(node.tree.n_cell_per_node):","674","                child = node.children[idx]","675","                if child.cumulative_size == 0: ","676","                    continue","677","                compute_non_edge_forces(child, theta,","678","                        point_index, pos, force, dist2s, sizes, deltas,","679","                        l)","680","","681","","682","cdef float compute_error(float[:, :] val_P,","683","                        float[:, :] pos_reference,","684","                        np.int64_t[:,:] neighbors,","685","                        float sum_Q,","686","                        int n_dimensions,","687","                        int verbose) nogil:","688","    cdef int i, j, ax","689","    cdef int I = neighbors.shape[0]","690","    cdef int K = neighbors.shape[1]","691","    cdef float pij, Q","692","    cdef float C = 0.0","693","    cdef clock_t t1, t2","694","    cdef float dt, delta","695","    t1 = clock()","696","    for i in range(I):","697","        for k in range(K):","698","            j = neighbors[i, k]","699","            pij = val_P[i, j]","700","            Q = 0.0","701","            for ax in range(n_dimensions):","702","                delta = (pos_reference[i, ax] - pos_reference[j, ax])","703","                Q += delta * delta","704","            Q = (1.0 \/ (sum_Q + Q * sum_Q))","705","            C += pij * log((pij + EPSILON) \/ (Q + EPSILON))","706","    t2 = clock()","707","    dt = ((float) (t2 - t1))","708","    if verbose > 10:","709","        printf(\"[t-SNE] Computed error=%1.4f in %1.1e ticks\\n\", C, dt)","710","    return C","711","","712","","713","def calculate_edge(pos_output):","714","    # Make the boundaries slightly outside of the data","715","    # to avoid floating point error near the edge","716","    left_edge = np.min(pos_output, axis=0)","717","    right_edge = np.max(pos_output, axis=0) ","718","    center = (right_edge + left_edge) * 0.5","719","    width = np.maximum(np.subtract(right_edge, left_edge), EPSILON)","720","    # Exagerate width to avoid boundary edge","721","    width = width.astype(np.float32) * 1.001","722","    left_edge = center - width \/ 2.0","723","    right_edge = center + width \/ 2.0","724","    return left_edge, right_edge, width","725","","726","def gradient(float[:,:] pij_input, ","727","             float[:,:] pos_output, ","728","             np.int64_t[:,:] neighbors, ","729","             float[:,:] forces, ","740","    left_edge, right_edge, width = calculate_edge(pos_output)","741","    assert width.itemsize == 4","742","    assert pij_input.itemsize == 4","745","    m = \"Number of neighbors must be < # of points - 1\"","746","    assert n - 1 >= neighbors.shape[1], m","747","    m = \"neighbors array and pos_output shapes are incompatible\"","748","    assert n == neighbors.shape[0], m","752","    assert n == pij_input.shape[0], m","753","    m = \"Pij and pos_output shapes are incompatible\"","754","    assert n == pij_input.shape[1], m","757","    cdef Tree* qt = init_tree(left_edge, width, n_dimensions, verbose)","759","        printf(\"[t-SNE] Inserting %i points\\n\", pos_output.shape[0])","760","    err = insert_many(qt, pos_output)","761","    assert err == 0, \"[t-SNE] Insertion failed\"","767","    sum_Q = compute_gradient(pij_input, pos_output, neighbors, forces,","768","                             qt.root_node, theta, dof, skip_num_points, -1)","769","    C = compute_error(pij_input, pos_output, neighbors, sum_Q, n_dimensions,","770","                      verbose)","776","    cdef long count = count_points(qt.root_node, 0)","777","    m = (\"Tree consistency failed: unexpected number of points=%i \"","778","         \"at root node=%i\" % (count, qt.root_node.cumulative_size))","779","    assert count == qt.root_node.cumulative_size, m ","781","    assert count == qt.n_points, m","782","    free_tree(qt)","784","","785","","786","# Helper functions","787","def check_quadtree(X, np.int64_t[:] counts):","788","    \"\"\"","789","    Helper function to access quadtree functions for testing","790","    \"\"\"","791","    ","792","    X = X.astype(np.float32)","793","    left_edge, right_edge, width = calculate_edge(X)","794","    # Initialise a tree","795","    qt = init_tree(left_edge, width, 2, 2)","796","    # Insert data into the tree","797","    insert_many(qt, X)","798","","799","    cdef long count = count_points(qt.root_node, 0)","800","    counts[0] = count","801","    counts[1] = qt.root_node.cumulative_size","802","    counts[2] = qt.n_points","803","    free_tree(qt)","804","    return counts","805","","806","","807","cdef int helper_test_index2offset(int* check, int index, int n_dimensions):","808","    cdef int* offset = <int*> malloc(sizeof(int) * n_dimensions)","809","    cdef int error_check = 1","810","    for i in range(n_dimensions):","811","        offset[i] = 0","812","    index2offset(offset, index, n_dimensions)","813","    for i in range(n_dimensions):","814","        error_check &= offset[i] == check[i]","815","    free(offset)","816","    return error_check","817","","818","","819","def test_index2offset():","820","    ret = 1","821","    ret &= helper_test_index2offset([1, 0, 1], 5, 3) == 1","822","    ret &= helper_test_index2offset([0, 0, 0], 0, 3) == 1","823","    ret &= helper_test_index2offset([0, 0, 1], 1, 3) == 1","824","    ret &= helper_test_index2offset([0, 1, 0], 2, 3) == 1","825","    ret &= helper_test_index2offset([0, 1, 1], 3, 3) == 1","826","    ret &= helper_test_index2offset([1, 0, 0], 4, 3) == 1","827","    return ret","828","","829","","830","def test_index_offset():","831","    cdef int n_dimensions, idx, tidx, k","832","    cdef int error_check = 1","833","    cdef int* offset ","834","    for n_dimensions in range(2, 10):","835","        offset = <int*> malloc(sizeof(int) * n_dimensions)","836","        for k in range(n_dimensions):","837","            offset[k] = 0","838","        for idx in range(2 ** n_dimensions):","839","            index2offset(offset, idx, n_dimensions)","840","            tidx = offset2index(offset, n_dimensions)","841","            error_check &= tidx == idx","842","            assert error_check == 1","843","        free(offset)","844","    return error_check"]}],"sklearn\/neighbors\/tests\/test_quad_tree.py":[{"add":[],"delete":[]}],"sklearn\/manifold\/t_sne.py":[{"add":["10","from time import time","16","from scipy.sparse import csr_matrix","17","from ..neighbors import NearestNeighbors","74","    distances : array, shape (n_samples, k)","75","        Distances of samples to its k nearest neighbors.","76","","77","    neighbors : array, shape (n_samples, k)","78","        Indices of the k nearest-neighbors for each samples.","88","    P : csr sparse matrix, shape (n_samples, n_samples)","89","        Condensed joint probability matrix with only nearest neighbors.","91","    t0 = time()","94","    n_samples, k = neighbors.shape","99","    assert np.all(np.isfinite(conditional_P)), \\","100","        \"All probabilities should be finite\"","101","","102","    # Symmetrize the joint probability distribution using sparse operations","103","    P = csr_matrix((conditional_P.ravel(), neighbors.ravel(),","104","                    range(0, n_samples * k + 1, k)),","105","                   shape=(n_samples, n_samples))","106","    P = P + P.T","107","","108","    # Normalize the joint probability distribution","109","    sum_P = np.maximum(P.sum(), MACHINE_EPSILON)","110","    P \/= sum_P","111","","112","    assert np.all(np.abs(P.data) <= 1.0)","113","    if verbose >= 2:","114","        duration = time() - t0","115","        print(\"[t-SNE] Computed conditional probabilities in {:.3f}s\"","116","              .format(duration))","159","    dist = pdist(X_embedded, \"sqeuclidean\")","160","    dist += 1.","161","    dist \/= degrees_of_freedom","162","    dist **= (degrees_of_freedom + 1.0) \/ -2.0","163","    Q = np.maximum(dist \/ (2.0 * np.sum(dist)), MACHINE_EPSILON)","169","    kl_divergence = 2.0 * np.dot(P, np.log(np.maximum(P, MACHINE_EPSILON) \/ Q))","172","    # pdist always returns double precision distances. Thus we need to take","173","    grad = np.ndarray((n_samples, n_components), dtype=params.dtype)","174","    PQd = squareform((P - Q) * dist)","176","        grad[i] = np.dot(np.ravel(PQd[i], order='K'),","177","                         X_embedded[i] - X_embedded)","185","def _kl_divergence_bh(params, P, degrees_of_freedom, n_samples, n_components,","186","                      angle=0.5, skip_num_points=0, verbose=False):","197","    P : csr sparse matrix, shape (n_samples, n_sample)","198","        Sparse approximate joint probability matrix, computed only for the","199","        k nearest-neighbors and symmetrized.","238","","239","    val_P = P.data.astype(np.float32, copy=False)","240","    neighbors = P.indices.astype(np.int64, copy=False)","241","    indptr = P.indptr.astype(np.int64, copy=False)","244","    error = _barnes_hut_tsne.gradient(val_P, X_embedded, neighbors, indptr,","254","def _gradient_descent(objective, p0, it, n_iter,","255","                      n_iter_check=1, n_iter_without_progress=300,","256","                      momentum=0.8, learning_rate=200.0, min_gain=0.01,","257","                      min_grad_norm=1e-7, verbose=0, args=None, kwargs=None):","282","    n_iter_without_progress : int, optional (default: 300)","286","    momentum : float, within (0.0, 1.0), optional (default: 0.8)","290","    learning_rate : float, optional (default: 200.0)","291","        The learning rate for t-SNE is usually in the range [10.0, 1000.0]. If","292","        the learning rate is too high, the data may look like a 'ball' with any","293","        point approximately equidistant from its nearest neighbours. If the","294","        learning rate is too low, most points may look compressed in a dense","295","        cloud with few outliers.","334","    best_iter = i = it","336","    tic = time()","338","        error, grad = objective(p, *args, **kwargs)","351","            toc = time()","352","            duration = toc - tic","353","            tic = toc","356","                print(\"[t-SNE] Iteration %d: error = %.7f,\"","357","                      \" gradient norm = %.7f\"","358","                      \" (%s iterations in %0.3fs)\"","359","                      % (i + 1, error, grad_norm, n_iter_check, duration))","472","    early_exaggeration : float, optional (default: 12.0)","481","    learning_rate : float, optional (default: 200.0)","482","        The learning rate for t-SNE is usually in the range [10.0, 1000.0]. If","483","        the learning rate is too high, the data may look like a 'ball' with any","484","        point approximately equidistant from its nearest neighbours. If the","485","        learning rate is too low, most points may look compressed in a dense","486","        cloud with few outliers. If the cost function gets stuck in a bad local","487","        minimum increasing the learning rate may help.","491","        least 250.","493","    n_iter_without_progress : int, optional (default: 300)","495","        optimization, used after 250 initial iterations with early","496","        exaggeration. Note that progress is only checked every 50 iterations so","497","        this value is rounded to the next multiple of 50.","504","        be stopped.","572","    >>> X_embedded = TSNE(n_components=2).fit_transform(X)","573","    >>> X_embedded.shape","574","    (4, 2)","589","    # Control the number of exploration iterations with early_exaggeration on","590","    _EXPLORATION_N_ITER = 250","591","","592","    # Control the number of iterations between progress checks","593","    _N_ITER_CHECK = 50","596","                 early_exaggeration=12.0, learning_rate=200.0, n_iter=1000,","597","                 n_iter_without_progress=300, min_grad_norm=1e-7,","640","        if self.metric == \"precomputed\":","641","            if isinstance(self.init, string_types) and self.init == 'pca':","642","                raise ValueError(\"The parameter init=\\\"pca\\\" cannot be \"","643","                                 \"used with metric=\\\"precomputed\\\".\")","644","            if X.shape[0] != X.shape[1]:","645","                raise ValueError(\"X should be a square distance matrix\")","646","            if np.any(X < 0):","647","                raise ValueError(\"All distances should be positive, the \"","648","                                 \"precomputed distances given as X is not \"","649","                                 \"correct\")","659","                            dtype=[np.float32, np.float64])","660","        if self.method == 'barnes_hut' and self.n_components > 3:","661","            raise ValueError(\"'n_components' should be inferior to 4 for the \"","662","                             \"barnes_hut algorithm as it relies on \"","663","                             \"quad-tree or oct-tree.\")","667","            raise ValueError(\"early_exaggeration must be at least 1, but is {}\"","668","                             .format(self.early_exaggeration))","670","        if self.n_iter < 250:","671","            raise ValueError(\"n_iter should be at least 250\")","676","        if self.method == \"exact\":","677","            # Retrieve the distance matrix, either using the precomputed one or","678","            # computing it.","679","            if self.metric == \"precomputed\":","680","                distances = X","682","                if self.verbose:","683","                    print(\"[t-SNE] Computing pairwise distances...\")","684","","685","                if self.metric == \"euclidean\":","686","                    distances = pairwise_distances(X, metric=self.metric,","687","                                                   squared=True)","688","                else:","689","                    distances = pairwise_distances(X, metric=self.metric)","690","","691","                if np.any(distances < 0):","692","                    raise ValueError(\"All distances should be positive, the \"","693","                                     \"metric given is not correct\")","694","","695","            # compute the joint probability distribution for the input space","697","            assert np.all(np.isfinite(P)), \"All probabilities should be finite\"","698","            assert np.all(P >= 0), \"All probabilities should be non-negative\"","699","            assert np.all(P <= 1), (\"All probabilities should be less \"","700","                                    \"or then equal to one\")","701","","702","        else:","703","            # Cpmpute the number of nearest neighbors to find.","704","            # LvdM uses 3 * perplexity as the number of neighbors.","705","            # In the event that we have very small # of points","706","            # set the neighbors to n - 1.","707","            k = min(n_samples - 1, int(3. * self.perplexity + 1))","708","","709","            if self.verbose:","710","                print(\"[t-SNE] Computing {} nearest neighbors...\".format(k))","711","","712","            # Find the nearest neighbors for every point","713","            neighbors_method = 'ball_tree'","714","            if (self.metric == 'precomputed'):","715","                neighbors_method = 'brute'","716","            knn = NearestNeighbors(algorithm=neighbors_method, n_neighbors=k,","717","                                   metric=self.metric)","718","            t0 = time()","719","            knn.fit(X)","720","            duration = time() - t0","721","            if self.verbose:","722","                print(\"[t-SNE] Indexed {} samples in {:.3f}s...\".format(","723","                    n_samples, duration))","724","","725","            t0 = time()","726","            distances_nn, neighbors_nn = knn.kneighbors(","727","                None, n_neighbors=k)","728","            duration = time() - t0","729","            if self.verbose:","730","                print(\"[t-SNE] Computed neighbors for {} samples in {:.3f}s...\"","731","                      .format(n_samples, duration))","732","","733","            # Free the memory used by the ball_tree","734","            del knn","735","","736","            if self.metric == \"euclidean\":","737","                # knn return the euclidean distance but we need it squared","738","                # to be consistent with the 'exact' method. Note that the","739","                # the method was derived using the euclidean method as in the","740","                # input space. Not sure of the implication of using a different","741","                # metric.","742","                distances_nn **= 2","743","","744","            # compute the joint probability distribution for the input space","745","            P = _joint_probabilities_nn(distances_nn, neighbors_nn,","746","                                        self.perplexity, self.verbose)","753","            X_embedded = pca.fit_transform(X).astype(np.float32, copy=False)","755","            # The embedding is initialized with iid samples from Gaussians with","756","            # standard deviation 1e-4.","757","            X_embedded = 1e-4 * random_state.randn(","758","                n_samples, self.n_components).astype(np.float32)","760","            raise ValueError(\"'init' must be 'pca', 'random', or \"","761","                             \"a numpy array\")","762","","763","        # Degrees of freedom of the Student's t-distribution. The suggestion","764","        # degrees_of_freedom = n_components - 1 comes from","765","        # \"Learning a Parametric Embedding by Preserving Local Structure\"","766","        # Laurens van der Maaten, 2009.","767","        degrees_of_freedom = max(self.n_components - 1.0, 1)","780","    def _tsne(self, P, degrees_of_freedom, n_samples, random_state, X_embedded,","781","              neighbors=None, skip_num_points=0):","785","        # we use is batch gradient descent with two stages:","786","        # * initial optimization with early exaggeration and momentum at 0.5","787","        # * final optimization with momentum at 0.8","790","        opt_args = {","791","            \"it\": 0,","792","            \"n_iter_check\": self._N_ITER_CHECK,","793","            \"min_grad_norm\": self.min_grad_norm,","794","            \"learning_rate\": self.learning_rate,","795","            \"verbose\": self.verbose,","796","            \"kwargs\": dict(skip_num_points=skip_num_points),","797","            \"args\": [P, degrees_of_freedom, n_samples, self.n_components],","798","            \"n_iter_without_progress\": self._EXPLORATION_N_ITER,","799","            \"n_iter\": self._EXPLORATION_N_ITER,","800","            \"momentum\": 0.5,","801","        }","805","            # Repeat verbose argument for _kl_divergence_bh","810","        # Learning schedule (part 1): do 250 iteration with lower momentum but","811","        # higher learning rate controlled via the early exageration parameter","818","","819","        # Learning schedule (part 2): disable early exaggeration and finish","820","        # optimization with a higher momentum at 0.8","821","        P \/= self.early_exaggeration","822","        remaining = self.n_iter - self._EXPLORATION_N_ITER","823","        if it < self._EXPLORATION_N_ITER or remaining > 0:","824","            opt_args['n_iter'] = self.n_iter","825","            opt_args['it'] = it + 1","826","            opt_args['momentum'] = 0.8","827","            opt_args['n_iter_without_progress'] = self.n_iter_without_progress","828","            params, kl_divergence, it = _gradient_descent(obj_func, params,","829","                                                          **opt_args)","830",""],"delete":["15","from ..neighbors import BallTree","72","    distances : array, shape (n_samples * (n_samples-1) \/ 2,)","73","        Distances of samples are stored as condensed matrices, i.e.","74","        we omit the diagonal and duplicate entries and store everything","75","        in a one-dimensional array.","85","    P : array, shape (n_samples * (n_samples-1) \/ 2,)","86","        Condensed joint probability matrix.","94","    m = \"All probabilities should be finite\"","95","    assert np.all(np.isfinite(conditional_P)), m","96","    P = conditional_P + conditional_P.T","97","    sum_P = np.maximum(np.sum(P), MACHINE_EPSILON)","98","    P = np.maximum(squareform(P) \/ sum_P, MACHINE_EPSILON)","99","    assert np.all(np.abs(P) <= 1.0)","142","    n = pdist(X_embedded, \"sqeuclidean\")","143","    n += 1.","144","    n \/= degrees_of_freedom","145","    n **= (degrees_of_freedom + 1.0) \/ -2.0","146","    Q = np.maximum(n \/ (2.0 * np.sum(n)), MACHINE_EPSILON)","152","    kl_divergence = 2.0 * np.dot(P, np.log(P \/ Q))","155","    grad = np.ndarray((n_samples, n_components))","156","    PQd = squareform((P - Q) * n)","158","        np.dot(np.ravel(PQd[i], order='K'), X_embedded[i] - X_embedded,","159","               out=grad[i])","167","def _kl_divergence_error(params, P, neighbors, degrees_of_freedom, n_samples,","168","                         n_components):","169","    \"\"\"t-SNE objective function: the absolute error of the","170","    KL divergence of p_ijs and q_ijs.","171","","172","    Parameters","173","    ----------","174","    params : array, shape (n_params,)","175","        Unraveled embedding.","176","","177","    P : array, shape (n_samples * (n_samples-1) \/ 2,)","178","        Condensed joint probability matrix.","179","","180","    neighbors : array (n_samples, K)","181","        The neighbors is not actually required to calculate the","182","        divergence, but is here to match the signature of the","183","        gradient function","184","","185","    degrees_of_freedom : float","186","        Degrees of freedom of the Student's-t distribution.","187","","188","    n_samples : int","189","        Number of samples.","190","","191","    n_components : int","192","        Dimension of the embedded space.","193","","194","    Returns","195","    -------","196","    kl_divergence : float","197","        Kullback-Leibler divergence of p_ij and q_ij.","198","","199","    grad : array, shape (n_params,)","200","        Unraveled gradient of the Kullback-Leibler divergence with respect to","201","        the embedding.","202","    \"\"\"","203","    X_embedded = params.reshape(n_samples, n_components)","204","","205","    # Q is a heavy-tailed distribution: Student's t-distribution","206","    n = pdist(X_embedded, \"sqeuclidean\")","207","    n += 1.","208","    n \/= degrees_of_freedom","209","    n **= (degrees_of_freedom + 1.0) \/ -2.0","210","    Q = np.maximum(n \/ (2.0 * np.sum(n)), MACHINE_EPSILON)","211","","212","    # Optimization trick below: np.dot(x, y) is faster than","213","    # np.sum(x * y) because it calls BLAS","214","","215","    # Objective: C (Kullback-Leibler divergence of P and Q)","216","    if len(P.shape) == 2:","217","        P = squareform(P)","218","    kl_divergence = 2.0 * np.dot(P, np.log(P \/ Q))","219","","220","    return kl_divergence","221","","222","","223","def _kl_divergence_bh(params, P, neighbors, degrees_of_freedom, n_samples,","224","                      n_components, angle=0.5, skip_num_points=0,","225","                      verbose=False):","236","    P : array, shape (n_samples * (n_samples-1) \/ 2,)","237","        Condensed joint probability matrix.","238","","239","    neighbors : int64 array, shape (n_samples, K)","240","        Array with element [i, j] giving the index for the jth","241","        closest neighbor to point i.","280","    neighbors = neighbors.astype(np.int64, copy=False)","281","    if len(P.shape) == 1:","282","        sP = squareform(P).astype(np.float32)","283","    else:","284","        sP = P.astype(np.float32)","287","    error = _barnes_hut_tsne.gradient(sP, X_embedded, neighbors,","297","def _gradient_descent(objective, p0, it, n_iter, objective_error=None,","298","                      n_iter_check=1, n_iter_without_progress=50,","299","                      momentum=0.5, learning_rate=1000.0, min_gain=0.01,","300","                      min_grad_norm=1e-7, min_error_diff=1e-7, verbose=0,","301","                      args=None, kwargs=None):","326","    objective_error : function or callable","327","        Should return a tuple of cost and gradient for a given parameter","328","        vector.","329","","330","    n_iter_without_progress : int, optional (default: 30)","334","    momentum : float, within (0.0, 1.0), optional (default: 0.5)","338","    learning_rate : float, optional (default: 1000.0)","339","        The learning rate should be extremely high for t-SNE! Values in the","340","        range [100.0, 1000.0] are common.","349","    min_error_diff : float, optional (default: 1e-7)","350","        If the absolute difference of two successive cost function values","351","        is below this threshold, the optimization will be aborted.","352","","383","    best_iter = 0","386","        new_error, grad = objective(p, *args, **kwargs)","399","            if new_error is None:","400","                new_error = objective_error(p, *args)","401","            error_diff = np.abs(new_error - error)","402","            error = new_error","405","                m = \"[t-SNE] Iteration %d: error = %.7f, gradient norm = %.7f\"","406","                print(m % (i + 1, error, grad_norm))","422","            if error_diff <= min_error_diff:","423","                if verbose >= 2:","424","                    m = \"[t-SNE] Iteration %d: error difference %f. Finished.\"","425","                    print(m % (i + 1, error_diff))","426","                break","427","","428","        if new_error is not None:","429","            error = new_error","527","    early_exaggeration : float, optional (default: 4.0)","536","    learning_rate : float, optional (default: 1000)","537","        The learning rate can be a critical parameter. It should be","538","        between 100 and 1000. If the cost function increases during initial","539","        optimization, the early exaggeration factor or the learning rate","540","        might be too high. If the cost function gets stuck in a bad local","541","        minimum increasing the learning rate helps sometimes.","545","        least 200.","547","    n_iter_without_progress : int, optional (default: 30)","548","        Only used if method='exact'","550","        optimization. If method='barnes_hut' this parameter is fixed to","551","        a value of 30 and cannot be changed.","557","        Only used if method='exact'","559","        be aborted. If method='barnes_hut' this parameter is fixed to a value","560","        of 1e-3 and cannot be changed.","611","","629","    >>> model = TSNE(n_components=2, random_state=0)","630","    >>> np.set_printoptions(suppress=True)","631","    >>> model.fit_transform(X) # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE","632","    array([[ 0.00017619,  0.00004014],","633","           [ 0.00010268,  0.00020546],","634","           [ 0.00018298, -0.00008335],","635","           [ 0.00009501, -0.00001388]])","652","                 early_exaggeration=4.0, learning_rate=1000.0, n_iter=1000,","653","                 n_iter_without_progress=30, min_grad_norm=1e-7,","656","        if not ((isinstance(init, string_types) and","657","                init in [\"pca\", \"random\"]) or","658","                isinstance(init, np.ndarray)):","659","            msg = \"'init' must be 'pca', 'random', or a numpy array\"","660","            raise ValueError(msg)","710","                            dtype=np.float64)","714","            raise ValueError(\"early_exaggeration must be at least 1, but is \"","715","                             \"%f\" % self.early_exaggeration)","717","        if self.n_iter < 200:","718","            raise ValueError(\"n_iter should be at least 200\")","720","        if self.metric == \"precomputed\":","721","            if isinstance(self.init, string_types) and self.init == 'pca':","722","                raise ValueError(\"The parameter init=\\\"pca\\\" cannot be used \"","723","                                 \"with metric=\\\"precomputed\\\".\")","724","            if X.shape[0] != X.shape[1]:","725","                raise ValueError(\"X should be a square distance matrix\")","726","            distances = X","727","        else:","728","            if self.verbose:","729","                print(\"[t-SNE] Computing pairwise distances...\")","730","","731","            if self.metric == \"euclidean\":","732","                distances = pairwise_distances(X, metric=self.metric,","733","                                               squared=True)","734","            else:","735","                distances = pairwise_distances(X, metric=self.metric)","736","","737","        if not np.all(distances >= 0):","738","            raise ValueError(\"All distances should be positive, either \"","739","                             \"the metric or precomputed distances given \"","740","                             \"as X are not correct\")","741","","742","        # Degrees of freedom of the Student's t-distribution. The suggestion","743","        # degrees_of_freedom = n_components - 1 comes from","744","        # \"Learning a Parametric Embedding by Preserving Local Structure\"","745","        # Laurens van der Maaten, 2009.","746","        degrees_of_freedom = max(self.n_components - 1.0, 1)","748","        # the number of nearest neighbors to find","749","        k = min(n_samples - 1, int(3. * self.perplexity + 1))","752","        if self.method == 'barnes_hut':","753","            if self.verbose:","754","                print(\"[t-SNE] Computing %i nearest neighbors...\" % k)","755","            if self.metric == 'precomputed':","756","                # Use the precomputed distances to find","757","                # the k nearest neighbors and their distances","758","                neighbors_nn = np.argsort(distances, axis=1)[:, :k]","760","                # Find the nearest neighbors for every point","761","                bt = BallTree(X)","762","                # LvdM uses 3 * perplexity as the number of neighbors","763","                # And we add one to not count the data point itself","764","                # In the event that we have very small # of points","765","                # set the neighbors to n - 1","766","                distances_nn, neighbors_nn = bt.query(X, k=k + 1)","767","                neighbors_nn = neighbors_nn[:, 1:]","768","            P = _joint_probabilities_nn(distances, neighbors_nn,","769","                                        self.perplexity, self.verbose)","770","        else:","772","        assert np.all(np.isfinite(P)), \"All probabilities should be finite\"","773","        assert np.all(P >= 0), \"All probabilities should be zero or positive\"","774","        assert np.all(P <= 1), (\"All probabilities should be less \"","775","                                \"or then equal to one\")","782","            X_embedded = pca.fit_transform(X)","784","            X_embedded = None","786","            raise ValueError(\"Unsupported initialization scheme: %s\"","787","                             % self.init)","800","    def _tsne(self, P, degrees_of_freedom, n_samples, random_state,","801","              X_embedded=None, neighbors=None, skip_num_points=0):","805","        # we use is batch gradient descent with three stages:","806","        # * early exaggeration with momentum 0.5","807","        # * early exaggeration with momentum 0.8","808","        # * final optimization with momentum 0.8","809","        # The embedding is initialized with iid samples from Gaussians with","810","        # standard deviation 1e-4.","811","","812","        if X_embedded is None:","813","            # Initialize embedding randomly","814","            X_embedded = 1e-4 * random_state.randn(n_samples,","815","                                                   self.n_components)","818","        opt_args = {\"n_iter\": 50, \"momentum\": 0.5, \"it\": 0,","819","                    \"learning_rate\": self.learning_rate,","820","                    \"n_iter_without_progress\": self.n_iter_without_progress,","821","                    \"verbose\": self.verbose, \"n_iter_check\": 25,","822","                    \"kwargs\": dict(skip_num_points=skip_num_points)}","824","            m = \"Must provide an array of neighbors to use Barnes-Hut\"","825","            assert neighbors is not None, m","827","            objective_error = _kl_divergence_error","828","            sP = squareform(P).astype(np.float32)","829","            neighbors = neighbors.astype(np.int64)","830","            args = [sP, neighbors, degrees_of_freedom, n_samples,","831","                    self.n_components]","832","            opt_args['args'] = args","833","            opt_args['min_grad_norm'] = 1e-3","834","            opt_args['n_iter_without_progress'] = 30","835","            # Don't always calculate the cost since that calculation","836","            # can be nearly as expensive as the gradient","837","            opt_args['objective_error'] = objective_error","842","            opt_args['args'] = [P, degrees_of_freedom, n_samples,","843","                                self.n_components]","844","            opt_args['min_error_diff'] = 0.0","845","            opt_args['min_grad_norm'] = self.min_grad_norm","847","        # Early exaggeration","849","","850","        params, kl_divergence, it = _gradient_descent(obj_func, params,","851","                                                      **opt_args)","852","        opt_args['n_iter'] = 100","853","        opt_args['momentum'] = 0.8","854","        opt_args['it'] = it + 1","863","        # Final optimization","864","        P \/= self.early_exaggeration","865","        opt_args['n_iter'] = self.n_iter","866","        opt_args['it'] = it + 1","867","        params, kl_divergence, it = _gradient_descent(obj_func, params,","868","                                                      **opt_args)","869",""]}],"sklearn\/neighbors\/setup.py":[{"add":["33","    config.add_extension(\"quad_tree\",","34","                         sources=[\"quad_tree.pyx\"],","35","                         include_dirs=[numpy.get_include()],","36","                         libraries=libraries)"],"delete":[]}]}},"526b35f372a9152234a3148c3491429882614c21":{"changes":{"sklearn\/model_selection\/_search.py":"MODIFY"},"diff":{"sklearn\/model_selection\/_search.py":[{"add":["388","        self.fit_params = fit_params","564","        if self.fit_params is not None:","838","           fit_params=None, iid=..., n_jobs=1,"],"delete":["388","        self.fit_params = fit_params if fit_params is not None else {}","564","        if self.fit_params:","838","           fit_params={}, iid=..., n_jobs=1,"]}]}},"50d3fe963c3d2823b2ba2a534b8a50dba620fc5e":{"changes":{"sklearn\/semi_supervised\/tests\/test_label_propagation.py":"MODIFY","sklearn\/semi_supervised\/_label_propagation.py":"MODIFY"},"diff":{"sklearn\/semi_supervised\/tests\/test_label_propagation.py":[{"add":["159","def test_label_propagation_non_zero_normalizer():","160","    # check that we don't divide by zero in case of null normalizer","161","    # non-regression test for","162","    # https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/15946","163","    X = np.array([[100., 100.], [100., 100.], [0., 0.], [0., 0.]])","164","    y = np.array([0, 1, -1, -1])","165","    mdl = label_propagation.LabelSpreading(kernel='knn',","166","                                           max_iter=100,","167","                                           n_neighbors=1)","168","    assert_no_warnings(mdl.fit, X, y)","169","","170",""],"delete":[]}],"sklearn\/semi_supervised\/_label_propagation.py":[{"add":["292","        normalizer[normalizer == 0] = 1"],"delete":[]}]}},"6986e9b5db5fa24b268cfc7d8aab32b8962f4510":{"changes":{"examples\/tree\/plot_tree_regression_multioutput.py":"MODIFY","examples\/linear_model\/plot_sgd_iris.py":"MODIFY","examples\/feature_selection\/plot_feature_selection.py":"MODIFY","examples\/linear_model\/plot_bayesian_ridge.py":"MODIFY","examples\/neighbors\/plot_classification.py":"MODIFY","examples\/feature_selection\/plot_f_test_vs_mi.py":"MODIFY","examples\/neighbors\/plot_lof.py":"MODIFY","examples\/semi_supervised\/plot_label_propagation_versus_svm_iris.py":"MODIFY","examples\/linear_model\/plot_sgd_weighted_samples.py":"MODIFY","examples\/feature_selection\/plot_permutation_test_for_classification.py":"MODIFY","examples\/linear_model\/plot_logistic_multinomial.py":"MODIFY","examples\/neural_networks\/plot_mlp_alpha.py":"MODIFY","examples\/model_selection\/plot_underfitting_overfitting.py":"MODIFY","examples\/mixture\/plot_concentration_prior.py":"MODIFY","examples\/neighbors\/plot_nearest_centroid.py":"MODIFY","examples\/svm\/plot_weighted_samples.py":"MODIFY","examples\/linear_model\/plot_sgd_separating_hyperplane.py":"MODIFY","examples\/tree\/plot_tree_regression.py":"MODIFY"},"diff":{"examples\/tree\/plot_tree_regression_multioutput.py":[{"add":["45","s = 25","46","plt.scatter(y[:, 0], y[:, 1], c=\"navy\", s=s,","47","            edgecolor=\"black\", label=\"data\")","48","plt.scatter(y_1[:, 0], y_1[:, 1], c=\"cornflowerblue\", s=s,","49","            edgecolor=\"black\", label=\"max_depth=2\")","50","plt.scatter(y_2[:, 0], y_2[:, 1], c=\"red\", s=s,","51","            edgecolor=\"black\", label=\"max_depth=5\")","52","plt.scatter(y_3[:, 0], y_3[:, 1], c=\"orange\", s=s,","53","            edgecolor=\"black\", label=\"max_depth=8\")","59","plt.legend(loc=\"best\")"],"delete":["45","plt.scatter(y[:, 0], y[:, 1], c=\"navy\", s=s, label=\"data\")","46","plt.scatter(y_1[:, 0], y_1[:, 1], c=\"cornflowerblue\", s=s, label=\"max_depth=2\")","47","plt.scatter(y_2[:, 0], y_2[:, 1], c=\"c\", s=s, label=\"max_depth=5\")","48","plt.scatter(y_3[:, 0], y_3[:, 1], c=\"orange\", s=s, label=\"max_depth=8\")","54","plt.legend()"]}],"examples\/linear_model\/plot_sgd_iris.py":[{"add":["19","","20","# we only take the first two features. We could","21","# avoid this ugly slicing by using a two-dim dataset","22","X = iris.data[:, :2]","60","                cmap=plt.cm.Paired, edgecolor='black', s=20)"],"delete":["19","X = iris.data[:, :2]  # we only take the first two features. We could","20","                      # avoid this ugly slicing by using a two-dim dataset","58","                cmap=plt.cm.Paired)"]}],"examples\/feature_selection\/plot_feature_selection.py":[{"add":["56","        label=r'Univariate score ($-Log(p_{value})$)', color='darkorange',","57","        edgecolor='black')","68","        color='navy', edgecolor='black')","77","        width=.2, label='SVM weights after selection', color='c',","78","        edgecolor='black')"],"delete":["56","        label=r'Univariate score ($-Log(p_{value})$)', color='darkorange')","67","        color='navy')","76","        width=.2, label='SVM weights after selection', color='c')"]}],"examples\/linear_model\/plot_bayesian_ridge.py":[{"add":["74","plt.hist(clf.coef_, bins=n_features, color='gold', log=True,","75","         edgecolor='black')"],"delete":["74","plt.hist(clf.coef_, bins=n_features, color='gold', log=True)"]}],"examples\/neighbors\/plot_classification.py":[{"add":["19","","20","# we only take the first two features. We could avoid this ugly","21","# slicing by using a two-dim dataset","22","X = iris.data[:, :2]","50","    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold,","51","                edgecolor='k', s=20)"],"delete":["19","X = iris.data[:, :2]  # we only take the first two features. We could","20","                      # avoid this ugly slicing by using a two-dim dataset","48","    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold)"]}],"examples\/feature_selection\/plot_f_test_vs_mi.py":[{"add":["11","y = x_1 + sin(6 * pi * x_2) + 0.1 * N(0, 1), that is the third features is","12","completely irrelevant.","42","    plt.scatter(X[:, i], y, edgecolor='black', s=20)"],"delete":["11","y = x_1 + sin(6 * pi * x_2) + 0.1 * N(0, 1), that is the third features is completely irrelevant.","41","    plt.scatter(X[:, i], y)","48",""]}],"examples\/neighbors\/plot_lof.py":[{"add":["19","print(__doc__)","46","a = plt.scatter(X[:200, 0], X[:200, 1], c='white',","47","                edgecolor='k', s=20)","48","b = plt.scatter(X[200:, 0], X[200:, 1], c='red',","49","                edgecolor='k', s=20)"],"delete":["23","print(__doc__)","46","a = plt.scatter(X[:200, 0], X[:200, 1], c='white')","47","b = plt.scatter(X[200:, 0], X[200:, 1], c='red')"]}],"examples\/semi_supervised\/plot_label_propagation_versus_svm_iris.py":[{"add":["73","    plt.scatter(X[:, 0], X[:, 1], c=colors, edgecolors='black')","77","plt.suptitle(\"Unlabeled points are colored white\", y=0.1)"],"delete":["73","    plt.scatter(X[:, 0], X[:, 1], c=colors, cmap=plt.cm.Paired)","77","plt.text(.90, 0, \"Unlabeled points are colored white\")"]}],"examples\/linear_model\/plot_sgd_weighted_samples.py":[{"add":["26","            cmap=plt.cm.bone, edgecolor='black')","28","# fit the unweighted model","35","# fit the weighted model"],"delete":["26","            cmap=plt.cm.bone)","28","## fit the unweighted model","35","## fit the weighted model"]}],"examples\/feature_selection\/plot_permutation_test_for_classification.py":[{"add":["51","plt.hist(permutation_scores, 20, label='Permutation scores',","52","         edgecolor='black')","55","# plt.vlines(score, ylim[0], ylim[1], linestyle='--',","58","# plt.vlines(1.0 \/ n_classes, ylim[0], ylim[1], linestyle='--',"],"delete":["51","plt.hist(permutation_scores, 20, label='Permutation scores')","54","#plt.vlines(score, ylim[0], ylim[1], linestyle='--',","57","#plt.vlines(1.0 \/ n_classes, ylim[0], ylim[1], linestyle='--',"]}],"examples\/linear_model\/plot_logistic_multinomial.py":[{"add":["52","        plt.scatter(X[idx, 0], X[idx, 1], c=color, cmap=plt.cm.Paired,","53","                    edgecolor='black', s=20)"],"delete":["52","        plt.scatter(X[idx, 0], X[idx, 1], c=color, cmap=plt.cm.Paired)"]}],"examples\/neural_networks\/plot_mlp_alpha.py":[{"add":["97","        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,","98","                   edgecolors='black', s=25)","101","                   alpha=0.6, edgecolors='black', s=25)"],"delete":["97","        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)","100","                   alpha=0.6)"]}],"examples\/model_selection\/plot_underfitting_overfitting.py":[{"add":["31","","32","def true_fun(X):","33","    return np.cos(1.5 * np.pi * X)","34","","62","    plt.scatter(X, y, edgecolor='b', s=20, label=\"Samples\")"],"delete":["36","true_fun = lambda X: np.cos(1.5 * np.pi * X)","59","    plt.scatter(X, y, label=\"Samples\")"]}],"examples\/mixture\/plot_concentration_prior.py":[{"add":["52","                                  180 + angle, edgecolor='black')","73","                align='center', edgecolor='black')"],"delete":["52","                                  180 + angle)","73","                align='center')"]}],"examples\/neighbors\/plot_nearest_centroid.py":[{"add":["20","# we only take the first two features. We could avoid this ugly","21","# slicing by using a two-dim dataset","22","X = iris.data[:, :2]","51","    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold,","52","                edgecolor='b', s=20)"],"delete":["20","X = iris.data[:, :2]  # we only take the first two features. We could","21","                      # avoid this ugly slicing by using a two-dim dataset","50","    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold)"]}],"examples\/svm\/plot_weighted_samples.py":[{"add":["31","                 cmap=plt.cm.bone, edgecolors='black')"],"delete":["31","                 cmap=plt.cm.bone)"]}],"examples\/linear_model\/plot_sgd_separating_hyperplane.py":[{"add":["38","plt.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.Paired,","39","            edgecolor='black', s=20)"],"delete":["38","plt.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.Paired)"]}],"examples\/tree\/plot_tree_regression.py":[{"add":["41","plt.scatter(X, y, s=20, edgecolor=\"black\",","42","            c=\"darkorange\", label=\"data\")","43","plt.plot(X_test, y_1, color=\"cornflowerblue\",","44","         label=\"max_depth=2\", linewidth=2)"],"delete":["41","plt.scatter(X, y, c=\"darkorange\", label=\"data\")","42","plt.plot(X_test, y_1, color=\"cornflowerblue\", label=\"max_depth=2\", linewidth=2)"]}]}},"e9b7883e12806a28499abc364313d852cb580b77":{"changes":{"sklearn\/gaussian_process\/_gpr.py":"MODIFY"},"diff":{"sklearn\/gaussian_process\/_gpr.py":[{"add":["488","                0.5 * np.einsum(\"ijl,jik->kl\", tmp, K_gradient)"],"delete":["488","                0.5 * np.einsum(\"ijl,ijk->kl\", tmp, K_gradient)"]}]}},"dd7a3dd0eb34ecca1f9991a23961a43fc2fb0ac1":{"changes":{"sklearn\/externals\/joblib\/pool.py":"MODIFY","sklearn\/metrics\/ranking.py":"MODIFY","sklearn\/externals\/joblib\/logger.py":"MODIFY","doc\/modules\/mixture.rst":"MODIFY"},"diff":{"sklearn\/externals\/joblib\/pool.py":[{"add":["541","                    # Missing rights in the \/dev\/shm partition,"],"delete":["541","                    # Missing rights in the the \/dev\/shm partition,"]}],"sklearn\/metrics\/ranking.py":[{"add":["596","    # Handle badly formatted array and the degenerate case with one label"],"delete":["596","    # Handle badly formated array and the degenerate case with one label"]}],"sklearn\/externals\/joblib\/logger.py":[{"add":["83","        \"\"\" Return the formatted representation of the object."],"delete":["83","        \"\"\" Return the formated representation of the object."]}],"doc\/modules\/mixture.rst":[{"add":["183","Here, we can see the value of the ``weight_concentration_prior`` parameter"],"delete":["183","Here, we can see the the value of the ``weight_concentration_prior`` parameter"]}]}},"4d5407c89fb2b25ef5cba90470cbdaecf10064cc":{"changes":{"sklearn\/linear_model\/tests\/test_sag.py":"MODIFY"},"diff":{"sklearn\/linear_model\/tests\/test_sag.py":[{"add":["122","               fit_intercept=True, saga=False, random_state=0):","132","    rng = check_random_state(random_state)","370","    max_iter = 100","380","                 alpha=alpha * n_samples, max_iter=max_iter,","381","                 random_state=rng)","390","                                          fit_intercept=fit_intercept,","391","                                          random_state=rng)","396","                                          fit_intercept=fit_intercept,","397","                                          random_state=rng)"],"delete":["122","               fit_intercept=True, saga=False):","132","    rng = np.random.RandomState(77)","370","    max_iter = 50","380","                 alpha=alpha * n_samples, max_iter=max_iter)","389","                                          fit_intercept=fit_intercept)","394","                                          fit_intercept=fit_intercept)"]}]}},"072bfc95c4732f540003bd0cb76854588f6af986":{"changes":{"doc\/whats_new\/v0.22.rst":"MODIFY"},"diff":{"doc\/whats_new\/v0.22.rst":[{"add":["9","**January 2 2020**","55","","69",":mod:`sklearn.naive_bayes`","70","..........................","71","","72","- |Fix| Removed `abstractmethod` decorator for the method `_check_X` in","73","  :class:`naive_bayes.BaseNB` that could break downstream projects inheriting","74","  from this deprecated public base class. :pr:`15996` by","75","  :user:`Brigitta Sip?cz <bsipocz>`.","76","","77",":mod:`sklearn.preprocessing`","78","............................","79","","80","- |Fix| :class:`preprocessing.QuantileTransformer` now guarantees the","81","  `quantiles_` attribute to be completely sorted in non-decreasing manner.","82","  :pr:`15751` by :user:`Tirth Patel <tirthasheshpatel>`.","83","","84",":mod:`sklearn.semi_supervised`","85","..............................","86","","87","- |Fix| :class:`semi_supervised.LabelPropagation` and","88","  :class:`semi_supervised.LabelSpreading` now allow callable kernel function to","89","  return sparse weight matrix.","90","  :pr:`15868` by :user:`Niklas Smedemark-Margulies <nik-sm>`.","91",""],"delete":["9","**In Development**","55","  ","60",":mod:`sklearn.naive_bayes`","61","..........................","62","","63","- |Fix| Removed `abstractmethod` decorator for the method `_check_X` in","64","  :class:`naive_bayes.BaseNB` that could break downstream projects inheriting","65","  from this deprecated public base class. :pr:`15996` by","66","  :user:`Brigitta Sip?cz <bsipocz>`.","67","","68",":mod:`sklearn.semi_supervised`","69","..............................","70","","71","- |Fix| :class:`semi_supervised.LabelPropagation` and","72","  :class:`semi_supervised.LabelSpreading` now allow callable kernel function to","73","  return sparse weight matrix.","74","  :pr:`15868` by :user:`Niklas Smedemark-Margulies <nik-sm>`.","75","","861","- |Fix| :class:`preprocessing.QuantileTransformer` now guarantees the ","862","  `quantiles_` attribute to be completely sorted in non-decreasing manner.","863","  :pr:`15751` by :user:`Tirth Patel <tirthasheshpatel>`.","864",""]}]}},"ceea86765d889ea40d771cbb85333afdd63fd9fe":{"changes":{"sklearn\/metrics\/cluster\/setup.py":"MODIFY"},"diff":{"sklearn\/metrics\/cluster\/setup.py":[{"add":["7","    config = Configuration(\"metrics.cluster\", parent_package, top_path)"],"delete":["7","    config = Configuration(\"metrics\/cluster\", parent_package, top_path)"]}]}},"fa4646749ce47cf4fe8d15575c448948b5625209":{"changes":{"doc\/whats_new\/v0.22.rst":"MODIFY"},"diff":{"doc\/whats_new\/v0.22.rst":[{"add":["21","- |Fix| :class:`cluster.KMeans` with ``algorithm=\"elkan\"`` now uses the same","22","  stopping criterion as with the default ``algorithm=\"full\"``. :pr:`15930` by","24","","53","- |Fix| :func:`inspection.plot_partial_dependence` and"],"delete":["21","- |Fix| :class:`KMeans` with ``algorithm=\"elkan\"`` now uses the same stopping","22","  criterion as with the default ``algorithm=\"full\"``. :pr:`15930` by","24"," ","53","- |Fix| :func:`inspection.plot_partial_dependence` and "]}]}},"f28a90c9aa552740a6fe4d9eccc409f82392019b":{"changes":{"sklearn\/naive_bayes.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY"},"diff":{"sklearn\/naive_bayes.py":[{"add":["53","    def _check_X(self, X):","54","        \"\"\"To be overridden in subclasses with the actual checks.\"\"\"","55","        # Note that this is not marked @abstractmethod as long as the","56","        # deprecated public alias sklearn.naive_bayes.BayesNB exists","57","        # (until 0.24) to preserve backward compat for 3rd party projects","58","        # with existing derived classes.","59","        return X","60",""],"delete":[]}],"doc\/whats_new\/v0.22.rst":[{"add":["63","- |Fix| Removed `abstractmethod` decorator for the method `_check_X` in","64","  :class:`naive_bayes.BaseNB` that could break downstream projects inheriting","65","  from this deprecated public base class. :pr:`15996` by","66","  :user:`Brigitta Sip?cz <bsipocz>`."],"delete":["63","- |Fix| removed abstract method `_check_X` from :class:`naive_bayes.BaseNB`","64","  that could break downstream projects inheriting from this deprecated","65","  public base class. :pr:`15996` by :user:`Brigitta Sip?cz <bsipocz>`."]}]}},"aaf9cf09e51e5c7ee3c331a05385a74392da8ed6":{"changes":{"doc\/modules\/decomposition.rst":"MODIFY"},"diff":{"doc\/modules\/decomposition.rst":[{"add":["850","    b. Draw the observed word :math:`w_{ij} \\sim \\mathrm{Multinomial}(\\beta_{z_{di}})`"],"delete":["850","    b. Draw the observed word :math:`w_{ij} \\sim \\mathrm{Multinomial}(beta_{z_{di}}.)`"]}]}},"2f661d0ed636be8e8ec5f197ece3c2ca6187f752":{"changes":{"sklearn\/neighbors\/classification.py":"MODIFY"},"diff":{"sklearn\/neighbors\/classification.py":[{"add":["235","        Range of parameter space to use by default for :meth:`radius_neighbors`"],"delete":["235","        Range of parameter space to use by default for :meth`radius_neighbors`"]}]}},"c75bccf43ecb2b6619491911f048676cab02aced":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/linear_model\/tests\/test_logistic.py":"MODIFY","sklearn\/linear_model\/logistic.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["334","- Fixed a bug in :class:`linear_model.LogisticRegressionCV` where the","335","  ``score`` method always computes accuracy, not the metric given by","336","  the ``scoring`` parameter.","337","  :issue:`10998` by :user:`Thomas Fan <thomasjpfan>`.","338",""],"delete":[]}],"sklearn\/linear_model\/tests\/test_logistic.py":[{"add":["24","from sklearn.exceptions import ChangedBehaviorWarning","95","def test_logistic_cv_mock_scorer():","96","","97","    class MockScorer(object):","98","        def __init__(self):","99","            self.calls = 0","100","            self.scores = [0.1, 0.4, 0.8, 0.5]","101","","102","        def __call__(self, model, X, y, sample_weight=None):","103","            score = self.scores[self.calls % len(self.scores)]","104","            self.calls += 1","105","            return score","106","","107","    mock_scorer = MockScorer()","108","    Cs = [1, 2, 3, 4]","109","    cv = 2","110","","111","    lr = LogisticRegressionCV(Cs=Cs, scoring=mock_scorer, cv=cv)","112","    lr.fit(X, Y1)","113","","114","    # Cs[2] has the highest score (0.8) from MockScorer","115","    assert lr.C_[0] == Cs[2]","116","","117","    # scorer called 8 times (cv*len(Cs))","118","    assert mock_scorer.calls == cv * len(Cs)","119","","120","    # reset mock_scorer","121","    mock_scorer.calls = 0","122","    with pytest.warns(ChangedBehaviorWarning):","123","        custom_score = lr.score(X, lr.predict(X))","124","","125","    assert custom_score == mock_scorer.scores[0]","126","    assert mock_scorer.calls == 1","127","","128","","129","def test_logistic_cv_score_does_not_warn_by_default():","130","    lr = LogisticRegressionCV(cv=2)","131","    lr.fit(X, Y1)","132","","133","    with pytest.warns(None) as record:","134","        lr.score(X, lr.predict(X))","135","    assert len(record) == 0","136","","137",""],"delete":[]}],"sklearn\/linear_model\/logistic.py":[{"add":["31","from..exceptions import (NotFittedError, ConvergenceWarning,","32","                         ChangedBehaviorWarning)","1792","","1793","    def score(self, X, y, sample_weight=None):","1794","        \"\"\"Returns the score using the `scoring` option on the given","1795","        test data and labels.","1796","","1797","        Parameters","1798","        ----------","1799","        X : array-like, shape = (n_samples, n_features)","1800","            Test samples.","1801","","1802","        y : array-like, shape = (n_samples,)","1803","            True labels for X.","1804","","1805","        sample_weight : array-like, shape = [n_samples], optional","1806","            Sample weights.","1807","","1808","        Returns","1809","        -------","1810","        score : float","1811","            Score of self.predict(X) wrt. y.","1812","","1813","        \"\"\"","1814","","1815","        if self.scoring is not None:","1816","            warnings.warn(\"The long-standing behavior to use the \"","1817","                          \"accuracy score has changed. The scoring \"","1818","                          \"parameter is now used. \"","1819","                          \"This warning will disappear in version 0.22.\",","1820","                          ChangedBehaviorWarning)","1821","        scoring = self.scoring or 'accuracy'","1822","        if isinstance(scoring, six.string_types):","1823","            scoring = get_scorer(scoring)","1824","","1825","        return scoring(self, X, y, sample_weight=sample_weight)"],"delete":["31","from ..exceptions import NotFittedError, ConvergenceWarning"]}]}},"ec4cbeff39d0253f62fd74e50c0dd89c6dff8997":{"changes":{"examples\/manifold\/plot_lle_digits.py":"MODIFY","doc\/modules\/manifold.rst":"MODIFY","sklearn\/manifold\/__init__.py":"MODIFY","doc\/modules\/classes.rst":"MODIFY","examples\/cluster\/plot_cluster_comparison.py":"MODIFY","examples\/cluster\/plot_segmentation_toy.py":"MODIFY","sklearn\/cluster\/tests\/test_spectral.py":"MODIFY","sklearn\/cluster\/spectral.py":"MODIFY","sklearn\/manifold\/tests\/test_spectral_embedding.py":"ADD","sklearn\/cluster\/__init__.py":"MODIFY","examples\/manifold\/plot_compare_methods.py":"MODIFY","sklearn\/manifold\/spectral_embedding.py":"ADD","doc\/whats_new.rst":"MODIFY"},"diff":{"examples\/manifold\/plot_lle_digits.py":[{"add":["99","               \"Principal Components projection of the digits (time %.2fs)\" %","100","               (time() - t0))","111","               \"Linear Discriminant projection of the digits (time %.2fs)\" %","112","               (time() - t0))","122","               \"Isomap projection of the digits (time %.2fs)\" %","123","               (time() - t0))","135","               \"Locally Linear Embedding of the digits (time %.2fs)\" %","136","               (time() - t0))","148","               \"Modified Locally Linear Embedding of the digits (time %.2fs)\" %","149","               (time() - t0))","161","               \"Hessian Locally Linear Embedding of the digits (time %.2fs)\" %","162","               (time() - t0))","174","               \"Local Tangent Space Alignment of the digits (time %.2fs)\" %","175","               (time() - t0))","185","               \"MDS embedding of the digits (time %.2fs)\" %","186","               (time() - t0))","192","                                       max_depth=5)","199","               \"Random forest embedding of the digits (time %.2fs)\" %","200","               (time() - t0))","201","","202","#----------------------------------------------------------------------","203","# Spectral embedding of the digits dataset","204","print \"Computing Spectral embedding\"","205","embedder = manifold.SpectralEmbedding(n_components=2, random_state=0,","206","                                      eigen_solver=\"arpack\")","207","t0 = time()","208","X_se = embedder.fit_transform(X)","209","","210","plot_embedding(X_se,","211","               \"Spectral embedding of the digits (time %.2fs)\" %","212","               (time() - t0))"],"delete":["99","    \"Principal Components projection of the digits (time %.2fs)\" %","100","    (time() - t0))","111","    \"Linear Discriminant projection of the digits (time %.2fs)\" %","112","    (time() - t0))","122","    \"Isomap projection of the digits (time %.2fs)\" %","123","    (time() - t0))","135","    \"Locally Linear Embedding of the digits (time %.2fs)\" %","136","    (time() - t0))","148","    \"Modified Locally Linear Embedding of the digits (time %.2fs)\" %","149","    (time() - t0))","161","    \"Hessian Locally Linear Embedding of the digits (time %.2fs)\" %","162","    (time() - t0))","174","    \"Local Tangent Space Alignment of the digits (time %.2fs)\" %","175","    (time() - t0))","185","    \"MDS embedding of the digits (time %.2fs)\" %","186","    (time() - t0))","192","                                        max_depth=5)","199","    \"Random forest embedding of the digits (time %.2fs)\" %","200","    (time() - t0))"]}],"doc\/modules\/manifold.rst":[{"add":["300","Spectral Embedding","301","====================","302","","303","Spectral Embedding (also known as Laplacian Eigenmaps) is one method","304","to calculate nonlinear embedding. It finds a low dimensional representation","305","of the data using a spectral decomposition of the graph Laplacian.","306","The graph generated can be considered as a discrete approximation of the ","307","low dimensional manifold in the high dimensional space. Minimization of a ","308","cost function based on the graph ensures that points close to each other on ","309","the manifold are mapped close to each other in the low dimensional space, ","310","preserving local distances. Spectral embedding can be  performed with the","311","function :func:`spectral_embedding` or its object-oriented counterpart","312",":class:`SpectralEmbedding`.","313","","314","Complexity","315","----------","316","","317","The Spectral Embedding algorithm comprises three stages:","318","","319","1. **Weighted Graph Construction**. Transform the raw input data into","320","   graph representation using affinity (adjacency) matrix representation.","321","","322","2. **Graph Laplacian Construction**. unnormalized Graph Laplacian","323","   is constructed as :math:`L = D - A` for and normalized one as","324","   :math:`L = D^{-\\frac{1}{2}} (D - A) D^{-\\frac{1}{2}}`.  ","325","","326","3. **Partial Eigenvalue Decomposition**. Eigenvalue decomposition is ","327","   done on graph Laplacian","328","","329","The overall complexity of spectral embedding is","330",":math:`O[D \\log(k) N \\log(N)] + O[D N k^3] + O[d N^2]`.","331","","332","* :math:`N` : number of training data points","333","* :math:`D` : input dimension","334","* :math:`k` : number of nearest neighbors","335","* :math:`d` : output dimension","336","","337",".. topic:: References:","338","","339","   * `\"Laplacian Eigenmaps for Dimensionality Reduction","340","     and Data Representation\" ","341","     <http:\/\/www.cse.ohio-state.edu\/~mbelkin\/papers\/LEM_NC_03.pdf>`_","342","     M. Belkin, P. Niyogi, Neural Computation, June 2003; 15 (6):1373-1396","343","","344",""],"delete":[]}],"sklearn\/manifold\/__init__.py":[{"add":["7","from .spectral_embedding import SpectralEmbedding, spectral_embedding"],"delete":[]}],"doc\/modules\/classes.rst":[{"add":["620","    manifold.SpectralEmbedding","627","    manifold.spectral_embedding"],"delete":[]}],"examples\/cluster\/plot_cluster_comparison.py":[{"add":["50","                   hspace=.01)","54","                                     no_structure]):","75","    spectral = cluster.SpectralClustering(n_clusters=2,","76","                                          eigen_solver='arpack',","77","                                          affinity=\"nearest_neighbors\")","80","                                                       preference=-200)"],"delete":["50","        hspace=.01)","54","                no_structure]):","75","    spectral = cluster.SpectralClustering(n_clusters=2, mode='arpack',","76","            affinity=\"nearest_neighbors\")","79","            preference=-200)"]}],"examples\/cluster\/plot_segmentation_toy.py":[{"add":["72","labels = spectral_clustering(graph, n_clusters=4, eigen_solver='arpack')","90","labels = spectral_clustering(graph, n_clusters=2, eigen_solver='arpack')"],"delete":["72","labels = spectral_clustering(graph, n_clusters=4, mode='arpack')","90","labels = spectral_clustering(graph, n_clusters=2, mode='arpack')"]}],"sklearn\/cluster\/tests\/test_spectral.py":[{"add":["14","from sklearn.cluster.spectral import discretize","18","from sklearn.preprocessing import LabelBinarizer","19","","29","                  ])","31","    for eigen_solver in ('arpack', 'lobpcg'):","35","                                           affinity='precomputed',","36","                                           eigen_solver=eigen_solver,","37","                                           assign_labels=assign_labels","38","                                           ).fit(mat)","47","                assert_equal(model_copy.eigen_solver, model.eigen_solver)","49","                                   model.random_state.get_state()[1])","66","                                 random_state=0, eigen_solver=\"lobpcg\")","91","                                     random_state=0, eigen_solver=\"amg\")","97","                      n_components=len(centers),","98","                      random_state=0, eigen_solver=\"amg\")","114","                  random_state=0, eigen_solver=\"<unknown>\")","130","                  ])","135","                                affinity='precomputed').fit(S).labels_","144","                      cluster_std=0.4)","147","                            random_state=0)","154","","155","","156","def test_discretize(seed=36):","157","    # Test the discretize using a noise assignment matrix","158","    LB = LabelBinarizer()","159","    for n_sample in [50, 100, 150, 500]:","160","        for n_class in range(2, 10):","161","            # random class labels","162","            random_state = np.random.RandomState(seed)","163","            y_true = random_state.random_integers(0, n_class, n_sample)","164","            y_true = np.array(y_true, np.float)","165","            # noise class assignment matrix","166","            y_true_noisy = (LB.fit_transform(y_true)","167","                            + 0.1 * random_state.randn(n_sample, n_class + 1))","168","            y_pred = discretize(y_true_noisy)","169","            assert_greater(adjusted_rand_score(y_true, y_pred), 0.9)"],"delete":["26","                 ])","28","    for mode in ('arpack', 'lobpcg'):","32","                        affinity='precomputed', mode=mode,","33","                        assign_labels=assign_labels).fit(mat)","42","                assert_equal(model_copy.mode, model.mode)","44","                            model.random_state.get_state()[1])","61","                                     random_state=0, mode=\"lobpcg\")","86","                                     random_state=0, mode=\"amg\")","92","                      n_components=len(centers), random_state=0, mode=\"amg\")","108","                  random_state=0, mode=\"<unknown>\")","124","                 ])","129","            affinity='precomputed').fit(S).labels_","138","            cluster_std=0.4)","141","            random_state=0)"]}],"sklearn\/cluster\/spectral.py":[{"add":["2","# Author: Gael Varoquaux gael.varoquaux@normalesup.org","3","#         Brian Cheung","4","#         Wei LI <kuantkid@gmail.com>","15","from ..manifold import spectral_embedding","156","def spectral_clustering(affinity, n_clusters=8, n_components=None,","157","                        eigen_solver=None, random_state=None, n_init=10,","158","                        k=None, eigen_tol=0.0,","159","                        assign_labels='kmeans',","160","                        mode=None):","189","    eigen_solver: {None, 'arpack' or 'amg'}","196","        of the lobpcg eigen vectors decomposition when eigen_solver == 'amg'","204","    eigen_tol : float, optional, default: 0.0","206","        when using arpack eigen_solver.","245","                         \"'kmeans' or 'discretize', but '%s' was given\"","246","                         % assign_labels)","247","","249","        warnings.warn(\"'k' was renamed to n_clusters and will \"","250","                      \"be removed in 0.15.\",","251","                      DeprecationWarning)","253","    if not mode is None:","254","        warnings.warn(\"'mode' was renamed to eigen_solver \"","255","                      \"and will be removed in 0.15.\",","256","                      DeprecationWarning)","257","        eigen_solver = mode","258","","262","                              eigen_solver=eigen_solver,","263","                              random_state=random_state,","264","                              eigen_tol=eigen_tol, drop_first=False)","267","        _, labels, _ = k_means(maps, n_clusters, random_state=random_state,","270","        labels = discretize(maps, random_state=random_state)","312","    eigen_solver: {None, 'arpack' or 'amg'}","319","        of the lobpcg eigen vectors decomposition when eigen_solver == 'amg'","327","    eigen_tol : float, optional, default: 0.0","329","        when using arpack eigen_solver.","379","    def __init__(self, n_clusters=8, eigen_solver=None, random_state=None,","380","                 n_init=10, gamma=1., affinity='rbf', n_neighbors=10, k=None,","381","                 eigen_tol=0.0, assign_labels='kmeans', mode=None):","382","        if k is not None:","383","            warnings.warn(\"'k' was renamed to n_clusters and \"","384","                          \"will be removed in 0.15.\",","385","                          DeprecationWarning)","387","        if mode is not None:","388","            warnings.warn(\"'mode' was renamed to eigen_solver and \"","389","                          \"will be removed in 0.15.\",","390","                          DeprecationWarning)","391","            eigen_solver = mode","392","","394","        self.eigen_solver = eigen_solver","400","        self.eigen_tol = eigen_tol","429","                             \"'nearest_neighbors' or 'precomputed', got '%s'.\"","430","                             % self.affinity)","434","                                           n_clusters=self.n_clusters,","435","                                           eigen_solver=self.eigen_solver,","436","                                           random_state=self.random_state,","437","                                           n_init=self.n_init,","438","                                           eigen_tol=self.eigen_tol,","439","                                           assign_labels=self.assign_labels)"],"delete":["2","# Author: Gael Varoquaux gael.varoquaux@normalesup.org, Brian Cheung","11","from ..utils.graph import graph_laplacian","17","def _set_diag(laplacian, value):","18","    \"\"\"Set the diagonal of the laplacian matrix and convert it to a","19","    sparse format well suited for eigenvalue decomposition","20","","21","    Parameters","22","    ----------","23","    laplacian: array or sparse matrix","24","        The graph laplacian","25","    value: float","26","        The value of the diagonal","27","","28","    Returns","29","    -------","30","    laplacian: array of sparse matrix","31","        An array of matrix in a form that is well suited to fast","32","        eigenvalue decomposition, depending on the band width of the","33","        matrix.","34","    \"\"\"","35","    from scipy import sparse","36","    n_nodes = laplacian.shape[0]","37","    # We need all entries in the diagonal to values","38","    if not sparse.isspmatrix(laplacian):","39","        laplacian.flat[::n_nodes + 1] = value","40","    else:","41","        laplacian = laplacian.tocoo()","42","        diag_idx = (laplacian.row == laplacian.col)","43","        laplacian.data[diag_idx] = value","44","        # If the matrix has a small number of diagonals (as in the","45","        # case of structured matrices comming from images), the","46","        # dia format might be best suited for matvec products:","47","        n_diags = np.unique(laplacian.row - laplacian.col).size","48","        if n_diags <= 7:","49","            # 3 or less outer diagonals on each side","50","            laplacian = laplacian.todia()","51","        else:","52","            # csr has the fastest matvec and is thus best suited to","53","            # arpack","54","            laplacian = laplacian.tocsr()","55","    return laplacian","56","","57","","58","def spectral_embedding(adjacency, n_components=8, mode=None,","59","                       random_state=None, eig_tol=0.0):","60","    \"\"\"Project the sample on the first eigen vectors of the graph Laplacian","61","","62","    The adjacency matrix is used to compute a normalized graph Laplacian","63","    whose spectrum (especially the eigen vectors associated to the","64","    smallest eigen values) has an interpretation in terms of minimal","65","    number of cuts necessary to split the graph into comparably sized","66","    components.","67","","68","    This embedding can also 'work' even if the ``adjacency`` variable is","69","    not strictly the adjacency matrix of a graph but more generally","70","    an affinity or similarity matrix between samples (for instance the","71","    heat kernel of a euclidean distance matrix or a k-NN matrix).","72","","73","    However care must taken to always make the affinity matrix symmetric","74","    so that the eigen vector decomposition works as expected.","75","","76","    Parameters","77","    ----------","78","    adjacency: array-like or sparse matrix, shape: (n_samples, n_samples)","79","        The adjacency matrix of the graph to embed.","80","","81","    n_components: integer, optional","82","        The dimension of the projection subspace.","83","","84","    mode: {None, 'arpack', 'lobpcg', or 'amg'}","85","        The eigenvalue decomposition strategy to use. AMG requires pyamg","86","        to be installed. It can be faster on very large, sparse problems,","87","        but may also lead to instabilities","88","","89","    random_state: int seed, RandomState instance, or None (default)","90","        A pseudo random number generator used for the initialization of the","91","        lobpcg eigen vectors decomposition when mode == 'amg'. By default","92","        arpack is used.","93","","94","    eig_tol : float, optional, default: 0.0","95","        Stopping criterion for eigendecomposition of the Laplacian matrix","96","        when using arpack mode.","97","","98","    Returns","99","    -------","100","    embedding: array, shape: (n_samples, n_components)","101","        The reduced samples","102","","103","    Notes","104","    -----","105","    The graph should contain only one connected component, elsewhere the","106","    results make little sense.","107","","108","    References","109","    ----------","110","    [1] http:\/\/en.wikipedia.org\/wiki\/LOBPCG","111","    [2] LOBPCG: http:\/\/dx.doi.org\/10.1137%2FS1064827500366124","112","    \"\"\"","113","","114","    from scipy import sparse","115","    from ..utils.arpack import eigsh","116","    from scipy.sparse.linalg import lobpcg","117","    from scipy.sparse.linalg.eigen.lobpcg.lobpcg import symeig","118","    try:","119","        from pyamg import smoothed_aggregation_solver","120","    except ImportError:","121","        if mode == \"amg\":","122","            raise ValueError(\"The mode was set to 'amg', but pyamg is \"","123","                             \"not available.\")","124","","125","    random_state = check_random_state(random_state)","126","","127","    n_nodes = adjacency.shape[0]","128","    # XXX: Should we check that the matrices given is symmetric","129","    if mode is None:","130","        mode = 'arpack'","131","    elif not mode in ('arpack', 'lobpcg', 'amg'):","132","        raise ValueError(\"Unknown value for mode: '%s'.\"","133","                         \"Should be 'amg', 'arpack', or 'lobpcg'\" % mode)","134","    laplacian, dd = graph_laplacian(adjacency,","135","                                    normed=True, return_diag=True)","136","    if (mode == 'arpack'","137","        or mode != 'lobpcg' and","138","            (not sparse.isspmatrix(laplacian)","139","             or n_nodes < 5 * n_components)):","140","        # lobpcg used with mode='amg' has bugs for low number of nodes","141","        # for details see the source code in scipy:","142","        # https:\/\/github.com\/scipy\/scipy\/blob\/v0.11.0\/scipy\/sparse\/linalg\/eigen\/lobpcg\/lobpcg.py#L237","143","        # or matlab:","144","        # http:\/\/www.mathworks.com\/matlabcentral\/fileexchange\/48-lobpcg-m","145","        laplacian = _set_diag(laplacian, 0)","146","","147","        # Here we'll use shift-invert mode for fast eigenvalues","148","        # (see http:\/\/docs.scipy.org\/doc\/scipy\/reference\/tutorial\/arpack.html","149","        #  for a short explanation of what this means)","150","        # Because the normalized Laplacian has eigenvalues between 0 and 2,","151","        # I - L has eigenvalues between -1 and 1.  ARPACK is most efficient","152","        # when finding eigenvalues of largest magnitude (keyword which='LM')","153","        # and when these eigenvalues are very large compared to the rest.","154","        # For very large, very sparse graphs, I - L can have many, many","155","        # eigenvalues very near 1.0.  This leads to slow convergence.  So","156","        # instead, we'll use ARPACK's shift-invert mode, asking for the","157","        # eigenvalues near 1.0.  This effectively spreads-out the spectrum","158","        # near 1.0 and leads to much faster convergence: potentially an","159","        # orders-of-magnitude speedup over simply using keyword which='LA'","160","        # in standard mode.","161","        try:","162","            lambdas, diffusion_map = eigsh(-laplacian, k=n_components,","163","                                        sigma=1.0, which='LM',","164","                                        tol=eig_tol)","165","            embedding = diffusion_map.T[::-1] * dd","166","        except RuntimeError:","167","            # When submatrices are exactly singular, an LU decomposition","168","            # in arpack fails. We fallback to lobpcg","169","            mode = \"lobpcg\"","170","","171","    if mode == 'amg':","172","        # Use AMG to get a preconditioner and speed up the eigenvalue","173","        # problem.","174","        laplacian = laplacian.astype(np.float)  # lobpcg needs native floats","175","        ml = smoothed_aggregation_solver(laplacian.tocsr())","176","        M = ml.aspreconditioner()","177","        X = random_state.rand(laplacian.shape[0], n_components)","178","        X[:, 0] = dd.ravel()","179","        lambdas, diffusion_map = lobpcg(laplacian, X, M=M, tol=1.e-12,","180","                                        largest=False)","181","        embedding = diffusion_map.T * dd","182","        if embedding.shape[0] == 1:","183","            raise ValueError","184","    elif mode == \"lobpcg\":","185","        laplacian = laplacian.astype(np.float)  # lobpcg needs native floats","186","        if n_nodes < 5 * n_components + 1:","187","            # see note above under arpack why lopbcg has problems with small","188","            # number of nodes","189","            # lobpcg will fallback to symeig, so we short circuit it","190","            if sparse.isspmatrix(laplacian):","191","                laplacian = laplacian.todense()","192","            lambdas, diffusion_map = symeig(laplacian)","193","            embedding = diffusion_map.T[:n_components] * dd","194","        else:","195","            # lobpcg needs native floats","196","            laplacian = laplacian.astype(np.float)","197","            laplacian = _set_diag(laplacian, 1)","198","            # We increase the number of eigenvectors requested, as lobpcg","199","            # doesn't behave well in low dimension","200","            X = random_state.rand(laplacian.shape[0], n_components + 1)","201","            X[:, 0] = dd.ravel()","202","            lambdas, diffusion_map = lobpcg(laplacian, X, tol=1e-15,","203","                                            largest=False, maxiter=2000)","204","            embedding = diffusion_map.T[:n_components] * dd","205","            if embedding.shape[0] == 1:","206","                raise ValueError","207","    return embedding","208","","209","","344","","348","def spectral_clustering(affinity, n_clusters=8, n_components=None, mode=None,","349","                        random_state=None, n_init=10, k=None, eig_tol=0.0,","350","                        assign_labels='kmeans'):","379","    mode: {None, 'arpack' or 'amg'}","386","        of the lobpcg eigen vectors decomposition when mode == 'amg'","394","    eig_tol : float, optional, default: 0.0","396","        when using arpack mode.","435","                    \"'kmeans' or 'discretize', but '%s' was given\"","436","                    % assign_labels)","438","        warnings.warn(\"'k' was renamed to n_clusters\", DeprecationWarning)","443","                              mode=mode, random_state=random_state,","444","                              eig_tol=eig_tol)","447","        maps = maps[1:]","448","        _, labels, _ = k_means(maps.T, n_clusters, random_state=random_state,","451","        labels = discretize(maps.T, random_state=random_state)","493","    mode: {None, 'arpack' or 'amg'}","500","        of the lobpcg eigen vectors decomposition when mode == 'amg'","508","    eig_tol : float, optional, default: 0.0","510","        when using arpack mode.","560","    def __init__(self, n_clusters=8, mode=None, random_state=None, n_init=10,","561","                 gamma=1., affinity='rbf', n_neighbors=10, k=None, eig_tol=0.0,","562","                 assign_labels='kmeans'):","563","        if not k is None:","564","            warnings.warn(\"'k' was renamed to n_clusters\", DeprecationWarning)","567","        self.mode = mode","573","        self.eig_tol = eig_tol","602","                \"'nearest_neighbors' or 'precomputed', got '%s'.\"","603","                % self.affinity)","607","                            n_clusters=self.n_clusters, mode=self.mode,","608","                            random_state=self.random_state, n_init=self.n_init,","609","                            eig_tol=self.eig_tol,","610","                            assign_labels=self.assign_labels)"]}],"sklearn\/manifold\/tests\/test_spectral_embedding.py":[{"add":[],"delete":[]}],"sklearn\/cluster\/__init__.py":[{"add":["7","    get_bin_seeds","12","from ..utils import deprecated","13","","14","","15","# backward compatibility","16","@deprecated(\"to be removed in 0.15;\"","17","            \" use sklearn.manifold.spectral_embedding instead\")","18","def spectral_embedding(*args, **kwargs):","19","    \"\"\"Deprecated, use ``sklearn.manifold.spectral_embedding`` instead\"\"\"","20","    from ..manifold import spectral_embedding","21","    return spectral_embedding(*args, **kwargs)","22","","39","           'spectral_embedding',"],"delete":["7","     get_bin_seeds"]}],"examples\/manifold\/plot_compare_methods.py":[{"add":["41","            % (1000, n_neighbors), fontsize=14)","95","t0 = time()","96","se = manifold.SpectralEmbedding(n_components=n_components,","97","                                n_neighbors=n_neighbors)","98","Y = se.fit_transform(X)","99","t1 = time()","100","print \"SpectralEmbedding: %.2g sec\" % (t1 - t0)","101","ax = fig.add_subplot(248)","102","pl.scatter(Y[:, 0], Y[:, 1], c=color, cmap=pl.cm.Spectral)","103","pl.title(\"SpectralEmbedding (%.2g sec)\" % (t1 - t0))","104","ax.xaxis.set_major_formatter(NullFormatter())","105","ax.yaxis.set_major_formatter(NullFormatter())","106","pl.axis('tight')","107",""],"delete":["41","               % (1000, n_neighbors), fontsize=14)"]}],"sklearn\/manifold\/spectral_embedding.py":[{"add":[],"delete":[]}],"doc\/whats_new.rst":[{"add":["77","   - New estimator :class:`manifold.SpectralEmbedding` and function","78","     :func:`manifold.spectral_embedding`, implementing the","79","     \"laplacian eigenmaps\" for nonlinear dimensionality reduction by Wei Li.","140","   - :func:`cluster.spectral_embedding` is now in","141","     :func:`manifold.spectral_embedding`.","142","","143","   - Renamed ``eig_tol`` in :func:`manifold.spectral_embedding`,","144","     :class:`cluster.SpectralClustering` to ``eigen_tol``, renamed ``mode``","145","     to ``eigen_solver``","146","","147","   - Renamed ``mode`` in :func:`manifold.spectral_embedding` and","148","     :class:`cluster.SpectralClustering` to ``eigen_solver``."],"delete":[]}]}},"7700b5ac7343a57492e30fd310cc63e8fd653bfa":{"changes":{"sklearn\/cluster\/tests\/test_affinity_propagation.py":"MODIFY","sklearn\/gaussian_process\/gpc.py":"MODIFY","sklearn\/linear_model\/ransac.py":"MODIFY","doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/linear_model\/tests\/test_logistic.py":"MODIFY","sklearn\/decomposition\/tests\/test_fastica.py":"MODIFY","sklearn\/linear_model\/logistic.py":"MODIFY","sklearn\/cluster\/affinity_propagation_.py":"MODIFY","sklearn\/linear_model\/tests\/test_ridge.py":"MODIFY","sklearn\/gaussian_process\/gpr.py":"MODIFY","sklearn\/cross_decomposition\/tests\/test_pls.py":"MODIFY","sklearn\/linear_model\/tests\/test_ransac.py":"MODIFY","sklearn\/cluster\/tests\/test_birch.py":"MODIFY","sklearn\/decomposition\/fastica_.py":"MODIFY","sklearn\/cross_decomposition\/pls_.py":"MODIFY","sklearn\/cluster\/birch.py":"MODIFY","sklearn\/linear_model\/ridge.py":"MODIFY"},"diff":{"sklearn\/cluster\/tests\/test_affinity_propagation.py":[{"add":["135","    af = assert_warns(ConvergenceWarning,","136","                      AffinityPropagation(preference=-10, max_iter=1).fit, X)","140","    to_predict = np.array([[2, 2], [3, 3], [4, 4]])","141","    y = assert_warns(ConvergenceWarning, af.predict, to_predict)","142","    assert_array_equal(np.array([-1, -1, -1]), y)"],"delete":["135","    af = AffinityPropagation(preference=-10, max_iter=1).fit(X)","139","    assert_array_equal(np.array([-1, -1, -1]),","140","                       af.predict(np.array([[2, 2], [3, 3], [4, 4]])))"]}],"sklearn\/gaussian_process\/gpc.py":[{"add":["21","from sklearn.exceptions import ConvergenceWarning","431","                              \" state: %s\" % convergence_dict,","432","                              ConvergenceWarning)"],"delete":["430","                              \" state: %s\" % convergence_dict)"]}],"sklearn\/linear_model\/ransac.py":[{"add":["15","from ..exceptions import ConvergenceWarning","456","                              ConvergenceWarning)"],"delete":["455","                              UserWarning)"]}],"doc\/whats_new\/v0.20.rst":[{"add":["385","Misc","386","","387","- Changed warning type from UserWarning to ConvergenceWarning for failing","388","  convergence in :func:`linear_model.logistic_regression_path`,","389","  :class:`linear_model.RANSACRegressor`, :func:`linear_model.ridge_regression`,","390","  :class:`gaussian_process.GaussianProcessRegressor`,","391","  :class:`gaussian_process.GaussianProcessClassifier`,","392","  :func:`decomposition.fastica`, :class:`cross_decomposition.PLSCanonical`,","393","  :class:`cluster.AffinityPropagation`, and :class:`cluster.Birch`.","394","  :issue:`#10306` by :user:`Jonathan Siebert <jotasi>`.","395",""],"delete":[]}],"sklearn\/linear_model\/tests\/test_logistic.py":[{"add":["314","def test_logistic_regression_path_convergence_fail():","315","    rng = np.random.RandomState(0)","316","    X = np.concatenate((rng.randn(100, 2) + [1, 1], rng.randn(100, 2)))","317","    y = [1] * 100 + [-1] * 100","318","    Cs = [1e3]","319","    assert_warns(ConvergenceWarning, logistic_regression_path,","320","                 X, y, Cs=Cs, tol=0., max_iter=1, random_state=0, verbose=1)","321","","322",""],"delete":[]}],"sklearn\/decomposition\/tests\/test_fastica.py":[{"add":["20","from sklearn.exceptions import ConvergenceWarning","144","def test_fastica_convergence_fail():","145","    # Test the FastICA algorithm on very simple data","146","    # (see test_non_square_fastica).","147","    # Ensure a ConvergenceWarning raised if the tolerance is sufficiently low.","148","    rng = np.random.RandomState(0)","149","","150","    n_samples = 1000","151","    # Generate two sources:","152","    t = np.linspace(0, 100, n_samples)","153","    s1 = np.sin(t)","154","    s2 = np.ceil(np.sin(np.pi * t))","155","    s = np.c_[s1, s2].T","156","    center_and_norm(s)","157","    s1, s2 = s","158","","159","    # Mixing matrix","160","    mixing = rng.randn(6, 2)","161","    m = np.dot(mixing, s)","162","","163","    # Do fastICA with tolerance 0. to ensure failing convergence","164","    ica = FastICA(algorithm=\"parallel\", n_components=2, random_state=rng,","165","                  max_iter=2, tol=0.)","166","    assert_warns(ConvergenceWarning, ica.fit, m.T)","167","","168",""],"delete":[]}],"sklearn\/linear_model\/logistic.py":[{"add":["31","from ..exceptions import NotFittedError, ConvergenceWarning","718","                              \"of iterations.\", ConvergenceWarning)"],"delete":["31","from ..exceptions import NotFittedError","718","                              \"of iterations.\")"]}],"sklearn\/cluster\/affinity_propagation_.py":[{"add":["392","                          \"Labeling every sample as '-1'.\", ConvergenceWarning)"],"delete":["392","                          \"Labeling every sample as '-1'.\")"]}],"sklearn\/linear_model\/tests\/test_ridge.py":[{"add":["16","from sklearn.exceptions import ConvergenceWarning","17","","141","def test_ridge_regression_convergence_fail():","142","    rng = np.random.RandomState(0)","143","    y = rng.randn(5)","144","    X = rng.randn(5, 10)","145","","146","    assert_warns(ConvergenceWarning, ridge_regression,","147","                 X, y, alpha=1.0, solver=\"sparse_cg\",","148","                 tol=0., max_iter=None, verbose=1)","149","","150",""],"delete":[]}],"sklearn\/gaussian_process\/gpr.py":[{"add":["18","from sklearn.exceptions import ConvergenceWarning","464","                              \" state: %s\" % convergence_dict,","465","                              ConvergenceWarning)"],"delete":["463","                              \" state: %s\" % convergence_dict)"]}],"sklearn\/cross_decomposition\/tests\/test_pls.py":[{"add":["5","                                   assert_raise_message, assert_warns)","10","from sklearn.exceptions import ConvergenceWarning","263","def test_convergence_fail():","264","    d = load_linnerud()","265","    X = d.data","266","    Y = d.target","267","    pls_bynipals = pls_.PLSCanonical(n_components=X.shape[1],","268","                                     max_iter=2, tol=1e-10)","269","    assert_warns(ConvergenceWarning, pls_bynipals.fit, X, Y)","270","","271",""],"delete":["5","                                   assert_raise_message)"]}],"sklearn\/linear_model\/tests\/test_ransac.py":[{"add":["15","from sklearn.exceptions import ConvergenceWarning","233","    assert_warns(ConvergenceWarning, ransac_estimator.fit, X, y)"],"delete":["232","    assert_warns(UserWarning, ransac_estimator.fit, X, y)"]}],"sklearn\/cluster\/tests\/test_birch.py":[{"add":["11","from sklearn.exceptions import ConvergenceWarning","96","    assert_warns(ConvergenceWarning, brc4.fit, X)"],"delete":["95","    assert_warns(UserWarning, brc4.fit, X)"]}],"sklearn\/decomposition\/fastica_.py":[{"add":["17","from ..exceptions import ConvergenceWarning","119","                      'tolerance or the maximum number of iterations.',","120","                      ConvergenceWarning)"],"delete":["118","                      'tolerance or the maximum number of iterations.')"]}],"sklearn\/cross_decomposition\/pls_.py":[{"add":["18","from ..exceptions import ConvergenceWarning","77","            warnings.warn('Maximum number of iterations reached',","78","                          ConvergenceWarning)"],"delete":["76","            warnings.warn('Maximum number of iterations reached')"]}],"sklearn\/cluster\/birch.py":[{"add":["17","from ..exceptions import NotFittedError, ConvergenceWarning","628","                    % (len(centroids), self.n_clusters), ConvergenceWarning)"],"delete":["17","from ..exceptions import NotFittedError","628","                    % (len(centroids), self.n_clusters))"]}],"sklearn\/linear_model\/ridge.py":[{"add":["33","from ..exceptions import ConvergenceWarning","76","                          info, ConvergenceWarning)"],"delete":["75","                          info)"]}]}},"1d4e18a1a715acf631898ece5ce6cc300e0edcd3":{"changes":{"examples\/cluster\/plot_kmeans_assumptions.py":"MODIFY","examples\/covariance\/plot_outlier_detection.py":"MODIFY","examples\/classification\/plot_lda_qda.py":"MODIFY","examples\/cluster\/plot_birch_vs_minibatchkmeans.py":"MODIFY","examples\/calibration\/plot_calibration.py":"MODIFY","examples\/plot_johnson_lindenstrauss_bound.py":"MODIFY","examples\/cluster\/plot_kmeans_silhouette_analysis.py":"MODIFY","examples\/cluster\/plot_cluster_iris.py":"MODIFY","examples\/cluster\/plot_ward_structured_vs_unstructured.py":"MODIFY"},"diff":{"examples\/cluster\/plot_kmeans_assumptions.py":[{"add":["36","transformation = [[0.60834549, -0.63667341], [-0.40887718, 0.85253229]]","56","y_pred = KMeans(n_clusters=3,","57","                random_state=random_state).fit_predict(X_filtered)"],"delete":["36","transformation = [[ 0.60834549, -0.63667341], [-0.40887718, 0.85253229]]","56","y_pred = KMeans(n_clusters=3, random_state=random_state).fit_predict(X_filtered)"]}],"examples\/covariance\/plot_outlier_detection.py":[{"add":["109","        b = subplot.scatter(X[:-n_outliers, 0], X[:-n_outliers, 1], c='white',","110","                            s=20, edgecolor='k')","111","        c = subplot.scatter(X[-n_outliers:, 0], X[-n_outliers:, 1], c='black',","112","                            s=20, edgecolor='k')"],"delete":["109","        b = subplot.scatter(X[:-n_outliers, 0], X[:-n_outliers, 1], c='white')","110","        c = subplot.scatter(X[-n_outliers:, 0], X[-n_outliers:, 1], c='black')"]}],"examples\/classification\/plot_lda_qda.py":[{"add":["62","        plt.ylabel('Data with\\n fixed covariance')","66","        plt.ylabel('Data with\\n varying covariances')","78","             color='red', markeredgecolor='k')","80","             color='#990000', markeredgecolor='k')  # dark red","84","             color='blue', markeredgecolor='k')","86","             color='#000099', markeredgecolor='k')  # dark blue","102","             'o', color='black', markersize=10, markeredgecolor='k')","104","             'o', color='black', markersize=10, markeredgecolor='k')","116","                              180 + angle, facecolor=color,","117","                              edgecolor='yellow',","149","plt.suptitle('Linear Discriminant Analysis vs Quadratic Discriminant'","150","             'Analysis')"],"delete":["62","        plt.ylabel('Data with fixed covariance')","66","        plt.ylabel('Data with varying covariances')","78","             color='red')","80","             color='#990000')  # dark red","84","             color='blue')","86","             color='#000099')  # dark blue","102","             'o', color='black', markersize=10)","104","             'o', color='black', markersize=10)","116","                              180 + angle, facecolor=color, edgecolor='yellow',","148","plt.suptitle('Linear Discriminant Analysis vs Quadratic Discriminant Analysis')"]}],"examples\/cluster\/plot_birch_vs_minibatchkmeans.py":[{"add":["70","        ax.scatter(X[mask, 0], X[mask, 1],","71","                   c='w', edgecolor=col, marker='.', alpha=0.5)","73","            ax.scatter(this_centroid[0], this_centroid[1], marker='+',","74","                       c='k', s=25)","94","    ax.scatter(X[mask, 0], X[mask, 1], marker='.',","95","               c='w', edgecolor=col, alpha=0.5)","96","    ax.scatter(this_centroid[0], this_centroid[1], marker='+',","97","               c='k', s=25)"],"delete":["41","   ","71","        ax.plot(X[mask, 0], X[mask, 1], 'w',","72","                markerfacecolor=col, marker='.')","74","            ax.plot(this_centroid[0], this_centroid[1], '+', markerfacecolor=col,","75","                    markeredgecolor='k', markersize=5)","95","    ax.plot(X[mask, 0], X[mask, 1], 'w', markerfacecolor=col, marker='.')","96","    ax.plot(this_centroid[0], this_centroid[1], '+', markeredgecolor='k',","97","            markersize=5)"]}],"examples\/calibration\/plot_calibration.py":[{"add":["17","isotonic calibration. One can observe that only the non-parametric model is","18","able to provide a probability calibration that returns probabilities close","19","to the expected 0.5 for most of the samples belonging to the middle","20","cluster with heterogeneous labels. This results in a significantly improved","21","Brier score.","94","    plt.scatter(this_X[:, 0], this_X[:, 1], s=this_sw * 50, c=color,","95","                alpha=0.5, edgecolor='k',"],"delete":["17","isotonic calibration. One can observe that only the non-parametric model is able","18","to provide a probability calibration that returns probabilities close to the","19","expected 0.5 for most of the samples belonging to the middle cluster with","20","heterogeneous labels. This results in a significantly improved Brier score.","93","    plt.scatter(this_X[:, 0], this_X[:, 1], s=this_sw * 50, c=color, alpha=0.5,"]}],"examples\/plot_johnson_lindenstrauss_bound.py":[{"add":["189","    plt.hist(rates, bins=50, normed=True, range=(0., 2.), edgecolor='k')"],"delete":["189","    plt.hist(rates, bins=50, normed=True, range=(0., 2.))"]}],"examples\/cluster\/plot_kmeans_silhouette_analysis.py":[{"add":["121","                c=colors, edgecolor='k')","126","    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',","127","                c=\"white\", alpha=1, s=200, edgecolor='k')","130","        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,","131","                    s=50, edgecolor='k')"],"delete":["121","                c=colors)","126","    ax2.scatter(centers[:, 0], centers[:, 1],","127","                marker='o', c=\"white\", alpha=1, s=200)","130","        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1, s=50)"]}],"examples\/cluster\/plot_cluster_iris.py":[{"add":["27","# Though the following import is not directly being used, it is required","28","# for 3D projection to work","46","fig = plt.figure(figsize=(8, 6))","48","titles = ['3 clusters', '8 clusters', '3 clusters, bad initialization']","50","    ax = plt.subplot(2, 2, fignum, projection='3d',","51","                     elev=48, azim=134)","55","    ax.scatter(X[:, 3], X[:, 0], X[:, 2],","56","               c=labels.astype(np.float), edgecolor='k')","64","    ax.set_title(titles[fignum - 1])","65","    ax.dist = 12","69","ax = plt.subplot(2, 2, 4, projection='3d',","70","                 elev=48, azim=134)","75","              X[y == label, 0].mean(),","76","              X[y == label, 2].mean() + 2, name,","78","              bbox=dict(alpha=.2, edgecolor='w', facecolor='w'))","81","ax.scatter(X[:, 3], X[:, 0], X[:, 2], c=y, edgecolor='k')","89","ax.set_title('Ground Truth')","90","ax.dist = 12","91","","92","fig.tight_layout()","93","fig.show()"],"delete":["29","","45","","48","    fig = plt.figure(fignum, figsize=(4, 3))","49","    plt.clf()","50","    ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)","51","","52","    plt.cla()","56","    ax.scatter(X[:, 3], X[:, 0], X[:, 2], c=labels.astype(np.float))","67","fig = plt.figure(fignum, figsize=(4, 3))","68","plt.clf()","69","ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)","70","","71","plt.cla()","72","","77","              X[y == label, 0].mean() + 1.5,","78","              X[y == label, 2].mean(), name,","80","              bbox=dict(alpha=.5, edgecolor='w', facecolor='w'))","83","ax.scatter(X[:, 3], X[:, 0], X[:, 2], c=y)","91","plt.show()"]}],"examples\/cluster\/plot_ward_structured_vs_unstructured.py":[{"add":["59","    ax.scatter(X[label == l, 0], X[label == l, 1], X[label == l, 2],","60","               color=plt.cm.jet(np.float(l) \/ np.max(label + 1)),","61","               s=20, edgecolor='k')","87","    ax.scatter(X[label == l, 0], X[label == l, 1], X[label == l, 2],","88","               color=plt.cm.jet(float(l) \/ np.max(label + 1)),","89","               s=20, edgecolor='k')"],"delete":["59","    ax.plot3D(X[label == l, 0], X[label == l, 1], X[label == l, 2],","60","              'o', color=plt.cm.jet(np.float(l) \/ np.max(label + 1)))","86","    ax.plot3D(X[label == l, 0], X[label == l, 1], X[label == l, 2],","87","              'o', color=plt.cm.jet(float(l) \/ np.max(label + 1)))"]}]}}}