{"eb3ad2dd5d5244fa384998596a2d6820aa8b16d7":{"changes":{"sklearn\/metrics\/_plot\/confusion_matrix.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY","sklearn\/metrics\/_plot\/tests\/test_plot_confusion_matrix.py":"MODIFY"},"diff":{"sklearn\/metrics\/_plot\/confusion_matrix.py":[{"add":["197","                     cmap=cmap, ax=ax, xticks_rotation=xticks_rotation,","198","                     values_format=values_format)"],"delete":["197","                     cmap=cmap, ax=ax, xticks_rotation=xticks_rotation)"]}],"doc\/whats_new\/v0.22.rst":[{"add":["56","- |Fix| Fixed a bug in :func:`metrics.plot_confusion_matrix` to correctly","57","  pass the `values_format` parameter to the :class:`ConfusionMatrixDisplay`","58","  plot() call. :pr:`15937` by :user:`Stephen Blystone <blynotes>`.","59",""],"delete":[]}],"sklearn\/metrics\/_plot\/tests\/test_plot_confusion_matrix.py":[{"add":["247","","248","","249","@pytest.mark.parametrize(\"values_format\", ['e', 'n'])","250","def test_confusion_matrix_text_format(pyplot, data, y_pred, n_classes,","251","                                      fitted_clf, values_format):","252","    # Make sure plot text is formatted with 'values_format'.","253","    X, y = data","254","    cm = confusion_matrix(y, y_pred)","255","    disp = plot_confusion_matrix(fitted_clf, X, y,","256","                                 include_values=True,","257","                                 values_format=values_format)","258","","259","    assert disp.text_.shape == (n_classes, n_classes)","260","","261","    expected_text = np.array([format(v, values_format)","262","                              for v in cm.ravel()])","263","    text_text = np.array([","264","        t.get_text() for t in disp.text_.ravel()])","265","    assert_array_equal(expected_text, text_text)"],"delete":[]}]}},"0788cd0c6a91c0d1cae17340cdf5d2af3c59ec57":{"changes":{"examples\/covariance\/plot_outlier_detection.py":"MODIFY","doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/covariance\/elliptic_envelope.py":"ADD","doc\/modules\/outlier_detection.rst":"MODIFY","doc\/whats_new\/_contributors.rst":"MODIFY","sklearn\/neighbors\/tests\/test_lof.py":"MODIFY","sklearn\/covariance\/tests\/test_elliptic_envelope.py":"ADD","sklearn\/ensemble\/tests\/test_iforest.py":"MODIFY","\/dev\/null":"DELETE","sklearn\/ensemble\/iforest.py":"MODIFY","sklearn\/covariance\/__init__.py":"MODIFY","sklearn\/covariance\/tests\/test_robust_covariance.py":"MODIFY","sklearn\/svm\/classes.py":"MODIFY","sklearn\/svm\/tests\/test_svm.py":"MODIFY","examples\/applications\/plot_species_distribution_modeling.py":"MODIFY","sklearn\/neighbors\/lof.py":"MODIFY"},"diff":{"examples\/covariance\/plot_outlier_detection.py":[{"add":["106","        subplot.contourf(xx, yy, Z, levels=np.linspace(Z.min(), 0, 7),","108","        a = subplot.contour(xx, yy, Z, levels=[0],","110","        subplot.contourf(xx, yy, Z, levels=[0, Z.max()],"],"delete":["97","        threshold = stats.scoreatpercentile(scores_pred,","98","                                            100 * outliers_fraction)","108","        subplot.contourf(xx, yy, Z, levels=np.linspace(Z.min(), threshold, 7),","110","        a = subplot.contour(xx, yy, Z, levels=[threshold],","112","        subplot.contourf(xx, yy, Z, levels=[threshold, Z.max()],"]}],"doc\/whats_new\/v0.20.rst":[{"add":["31","  ","343","Outlier Detection models","344","","345","- More consistent outlier detection API:","346","  Add a ``score_samples`` method in :class:`svm.OneClassSVM`,","347","  :class:`ensemble.IsolationForest`, :class:`neighbors.LocalOutlierFactor`,","348","  :class:`covariance.EllipticEnvelope`. It allows to access raw score","349","  functions from original papers. A new ``offset_`` parameter allows to link","350","  ``score_samples`` and ``decision_function`` methods.","351","  The ``contamination`` parameter of :class:`ensemble.IsolationForest` and","352","  :class:`neighbors.LocalOutlierFactor` ``decision_function`` methods is used","353","  to define this ``offset_`` such that outliers (resp. inliers) have negative (resp.","354","  positive) ``decision_function`` values. By default, ``contamination`` is","355","  kept unchanged to 0.1 for a deprecation period. In 0.22, it will be set to \"auto\",","356","  thus using method-specific score offsets.","357","  In :class:`covariance.EllipticEnvelope` ``decision_function`` method, the","358","  ``raw_values`` parameter is deprecated as the shifted Mahalanobis distance","359","  will be always returned in 0.22. :issue:`9015` by `Nicolas Goix`_.","360","","361",""],"delete":["31",""]}],"sklearn\/covariance\/elliptic_envelope.py":[{"add":[],"delete":[]}],"doc\/modules\/outlier_detection.rst":[{"add":["29","``predict`` method::","33","Inliers are labeled 1, while outliers are labeled -1. The predict method","34","makes use of a threshold on the raw scoring function computed by the","35","estimator. This scoring function is accessible through the ``score_samples``","36","method, while the threshold can be controlled by the ``contamination``","37","parameter.","38","","39","The ``decision_function`` method is also defined from the scoring function,","40","in such a way that negative values are outliers and non-negative ones are","41","inliers::","42","","43","    estimator.decision_function(X_test)","44","","45","Note that :class:`neighbors.LocalOutlierFactor` does not support","46","``predict`` and ``decision_function`` methods, as this algorithm is","47","purely transductive and is thus not designed to deal with new data."],"delete":["29","`predict` method::","33","Inliers are labeled 1, while outliers are labeled -1."]}],"doc\/whats_new\/_contributors.rst":[{"add":["108",".. _Nicolas Goix: http:\/\/ngoix.github.io"],"delete":["108",".. _Nicolas Goix: https:\/\/perso.telecom-paristech.fr\/~goix\/"]}],"sklearn\/neighbors\/tests\/test_lof.py":[{"add":["17","from sklearn.utils.testing import assert_warns_message, assert_raises","73","    clf1 = neighbors.LocalOutlierFactor(n_neighbors=2,","74","                                        contamination=0.1).fit(X_train)","75","    clf2 = neighbors.LocalOutlierFactor(n_neighbors=2).fit(X_train)","79","    assert_array_almost_equal(-clf1.negative_outlier_factor_, [s_0, s_1, s_1])","80","    assert_array_almost_equal(-clf2.negative_outlier_factor_, [s_0, s_1, s_1])","82","    assert_array_almost_equal(-clf1._score_samples([[2., 2.]]), [s_0])","83","    assert_array_almost_equal(-clf2._score_samples([[2., 2.]]), [s_0])","84","    # check predict(one sample already in train)","85","    assert_array_almost_equal(-clf1._score_samples([[1., 1.]]), [s_1])","86","    assert_array_almost_equal(-clf2._score_samples([[1., 1.]]), [s_1])","124","","125","","126","def test_score_samples():","127","    X_train = [[1, 1], [1, 2], [2, 1]]","128","    clf1 = neighbors.LocalOutlierFactor(n_neighbors=2,","129","                                        contamination=0.1).fit(X_train)","130","    clf2 = neighbors.LocalOutlierFactor(n_neighbors=2).fit(X_train)","131","    assert_array_equal(clf1._score_samples([[2., 2.]]),","132","                       clf1._decision_function([[2., 2.]]) + clf1.offset_)","133","    assert_array_equal(clf2._score_samples([[2., 2.]]),","134","                       clf2._decision_function([[2., 2.]]) + clf2.offset_)","135","    assert_array_equal(clf1._score_samples([[2., 2.]]),","136","                       clf2._score_samples([[2., 2.]]))","137","","138","","139","def test_contamination():","140","    X = [[1, 1], [1, 0]]","141","    clf = neighbors.LocalOutlierFactor(contamination=0.6)","142","    assert_raises(ValueError, clf.fit, X)","143","","144","","145","def test_deprecation():","146","    assert_warns_message(DeprecationWarning,","147","                         'default contamination parameter 0.1 will change '","148","                         'in version 0.22 to \"auto\"',","149","                         neighbors.LocalOutlierFactor, )"],"delete":["17","from sklearn.utils.testing import assert_warns_message","73","    clf = neighbors.LocalOutlierFactor(n_neighbors=2).fit(X_train)","77","    assert_array_almost_equal(-clf.negative_outlier_factor_, [s_0, s_1, s_1])","79","    assert_array_almost_equal(-clf._decision_function([[2., 2.]]), [s_0])","80","    # # check predict(one sample already in train)","81","    assert_array_almost_equal(-clf._decision_function([[1., 1.]]), [s_1])"]}],"sklearn\/covariance\/tests\/test_elliptic_envelope.py":[{"add":[],"delete":[]}],"sklearn\/ensemble\/tests\/test_iforest.py":[{"add":["112","    # test X_test n_features match X_train one:","113","    assert_raises(ValueError, IsolationForest().fit(X).predict, X[:, 1:])","114","","191","    # Test IsolationForest","192","    for contamination in [0.25, \"auto\"]:","193","        clf = IsolationForest(random_state=rng, contamination=contamination)","194","        clf.fit(X)","195","        decision_func = - clf.decision_function(X)","196","        pred = clf.predict(X)","197","        # assert detect outliers:","198","        assert_greater(np.min(decision_func[-2:]), np.max(decision_func[:-2]))","199","        assert_array_equal(pred, 6 * [1] + 2 * [-1])","231","","232","","233","def test_score_samples():","234","    X_train = [[1, 1], [1, 2], [2, 1]]","235","    clf1 = IsolationForest(contamination=0.1).fit(X_train)","236","    clf2 = IsolationForest().fit(X_train)","237","    assert_array_equal(clf1.score_samples([[2., 2.]]),","238","                       clf1.decision_function([[2., 2.]]) + clf1.offset_)","239","    assert_array_equal(clf2.score_samples([[2., 2.]]),","240","                       clf2.decision_function([[2., 2.]]) + clf2.offset_)","241","    assert_array_equal(clf1.score_samples([[2., 2.]]),","242","                       clf2.score_samples([[2., 2.]]))","243","","244","","245","def test_deprecation():","246","    assert_warns_message(DeprecationWarning,","247","                         'default contamination parameter 0.1 will change '","248","                         'in version 0.22 to \"auto\"',","249","                         IsolationForest, )","250","    X = [[0.0], [1.0]]","251","    clf = IsolationForest().fit(X)","252","    assert_warns_message(DeprecationWarning,","253","                         \"threshold_ attribute is deprecated in 0.20 and will\"","254","                         \" be removed in 0.22.\",","255","                         getattr, clf, \"threshold_\")"],"delete":["188","    # Test LOF","189","    clf = IsolationForest(random_state=rng, contamination=0.25)","190","    clf.fit(X)","191","    decision_func = - clf.decision_function(X)","192","    pred = clf.predict(X)","193","","194","    # assert detect outliers:","195","    assert_greater(np.min(decision_func[-2:]), np.max(decision_func[:-2]))","196","    assert_array_equal(pred, 6 * [1] + 2 * [-1])"]}],"\/dev\/null":[{"add":[],"delete":[]}],"sklearn\/ensemble\/iforest.py":[{"add":["8","import warnings","18","from ..utils.validation import check_is_fitted","68","        on the decision function. If 'auto', the decision function threshold is","69","        determined as in the original paper.","108","    offset_ : float","109","        Offset used to define the decision function from the raw scores.","110","        We have the relation: decision_function = score_samples - offset_.","111","        The offset is set to 0.5 as in the original paper, except when a","112","        contamination parameter different than \"auto\" is provided. In that","113","        case, the offset is defined in such a way we obtain the expected","114","        number of outliers (samples with decision function < 0) in training.","115","","129","                 contamination=\"legacy\",","149","","150","        if contamination == \"legacy\":","151","            warnings.warn('default contamination parameter 0.1 will change '","152","                          'in version 0.22 to \"auto\". This will change the '","153","                          'predict method behavior.',","154","                          DeprecationWarning)","218","        if self.contamination == \"auto\":","219","            # 0.5 plays a special role as described in the original paper.","220","            # we take the opposite as we consider the opposite of their score.","221","            self.offset_ = -0.5","222","            # need to save (depreciated) threshold_ in this case:","223","            self._threshold_ = sp.stats.scoreatpercentile(","224","                self.score_samples(X), 100. * 0.1)","225","        elif self.contamination == \"legacy\":  # to be rm in 0.22","226","            self.offset_ = sp.stats.scoreatpercentile(","227","                self.score_samples(X), 100. * 0.1)","228","        else:","229","            self.offset_ = sp.stats.scoreatpercentile(","230","                self.score_samples(X), 100. * self.contamination)","247","            For each observation, tells whether or not (+1 or -1) it should","250","        check_is_fitted(self, [\"offset_\"])","253","        is_inlier[self.decision_function(X) < 0] = -1","276","        scores : array, shape (n_samples,)","278","            The lower, the more abnormal. Negative scores represent outliers,","279","            positive scores represent inliers.","282","        # We substract self.offset_ to make 0 be the threshold value for being","283","        # an outlier:","284","","285","        return self.score_samples(X) - self.offset_","286","","287","    def score_samples(self, X):","288","        \"\"\"Opposite of the anomaly score defined in the original paper.","289","","290","        The anomaly score of an input sample is computed as","291","        the mean anomaly score of the trees in the forest.","292","","293","        The measure of normality of an observation given a tree is the depth","294","        of the leaf containing this observation, which is equivalent to","295","        the number of splittings required to isolate this point. In case of","296","        several observations n_left in the leaf, the average path length of","297","        a n_left samples isolation tree is added.","298","","299","        Parameters","300","        ----------","301","        X : {array-like, sparse matrix}, shape (n_samples, n_features)","302","            The training input samples. Sparse matrices are accepted only if","303","            they are supported by the base estimator.","304","","305","        Returns","306","        -------","307","        scores : array, shape (n_samples,)","308","            The anomaly score of the input samples.","309","            The lower, the more abnormal.","310","        \"\"\"","312","        check_is_fitted(self, [\"estimators_\"])","313","","316","        if self.n_features_ != X.shape[1]:","317","            raise ValueError(\"Number of features of the model must \"","318","                             \"match the input. Model n_features is {0} and \"","319","                             \"input n_features is {1}.\"","320","                             \"\".format(self.n_features_, X.shape[1]))","345","        scores = 2 ** (-depths.mean(axis=1) \/ _average_path_length(","346","            self.max_samples_))","349","        # abnormal)","350","        return -scores","351","","352","    @property","353","    def threshold_(self):","354","        warnings.warn(\"threshold_ attribute is deprecated in 0.20 and will\"","355","                      \" be removed in 0.22.\", DeprecationWarning)","356","        if self.contamination == 'auto':","357","            return self._threshold_","358","        return self.offset_","367","    n_samples_leaf : array-like, shape (n_samples, n_estimators), or int."],"delete":["66","        on the decision function.","118","                 contamination=0.1,","201","        self.threshold_ = -sp.stats.scoreatpercentile(","202","            -self.decision_function(X), 100. * (1. - self.contamination))","219","            For each observations, tells whether or not (+1 or -1) it should","224","        is_inlier[self.decision_function(X) <= self.threshold_] = -1","247","        scores : array of shape (n_samples,)","249","            The lower, the more abnormal.","279","        scores = 2 ** (-depths.mean(axis=1) \/ _average_path_length(self.max_samples_))","282","        # abnormal) and add 0.5 (this value plays a special role as described","283","        # in the original paper) to give a sense to scores = 0:","284","        return 0.5 - scores","293","    n_samples_leaf : array-like of shape (n_samples, n_estimators), or int."]}],"sklearn\/covariance\/__init__.py":[{"add":["15","from .elliptic_envelope import EllipticEnvelope"],"delete":["15","from .outlier_detection import EllipticEnvelope"]}],"sklearn\/covariance\/tests\/test_robust_covariance.py":[{"add":["14","from sklearn.covariance import empirical_covariance, MinCovDet"],"delete":["10","from sklearn.utils.testing import assert_almost_equal","12","from sklearn.utils.testing import assert_raises","14","from sklearn.exceptions import NotFittedError","17","from sklearn.covariance import empirical_covariance, MinCovDet, \\","18","    EllipticEnvelope","139","","140","","141","def test_outlier_detection():","142","    rnd = np.random.RandomState(0)","143","    X = rnd.randn(100, 10)","144","    clf = EllipticEnvelope(contamination=0.1)","145","    assert_raises(NotFittedError, clf.predict, X)","146","    assert_raises(NotFittedError, clf.decision_function, X)","147","    clf.fit(X)","148","    y_pred = clf.predict(X)","149","    decision = clf.decision_function(X, raw_values=True)","150","    decision_transformed = clf.decision_function(X, raw_values=False)","151","","152","    assert_array_almost_equal(","153","        decision, clf.mahalanobis(X))","154","    assert_array_almost_equal(clf.mahalanobis(X), clf.dist_)","155","    assert_almost_equal(clf.score(X, np.ones(100)),","156","                        (100 - y_pred[y_pred == -1].size) \/ 100.)","157","    assert(sum(y_pred == -1) == sum(decision_transformed < 0))"]}],"sklearn\/svm\/classes.py":[{"add":["1058","    offset_ : float","1059","        Offset used to define the decision function from the raw scores.","1060","        We have the relation: decision_function = score_samples - offset_.","1061","        The offset is the opposite of intercept_ and is provided for","1062","        consistency with other outlier detection algorithms.","1063","","1107","        self.offset_ = -self._intercept_","1121","        dec : array-like, shape (n_samples,)","1124","        dec = self._decision_function(X).ravel()","1127","    def score_samples(self, X):","1128","        \"\"\"Raw scoring function of the samples.","1129","","1130","        Parameters","1131","        ----------","1132","        X : array-like, shape (n_samples, n_features)","1133","","1134","        Returns","1135","        -------","1136","        score_samples : array-like, shape (n_samples,)","1137","            Returns the (unshifted) scoring function of the samples.","1138","        \"\"\"","1139","        return self.decision_function(X) + self.offset_","1140",""],"delete":["1114","        X : array-like, shape (n_samples,)","1117","        dec = self._decision_function(X)"]}],"sklearn\/svm\/tests\/test_svm.py":[{"add":["282","def test_oneclass_score_samples():","283","    X_train = [[1, 1], [1, 2], [2, 1]]","284","    clf = svm.OneClassSVM().fit(X_train)","285","    assert_array_equal(clf.score_samples([[2., 2.]]),","286","                       clf.decision_function([[2., 2.]]) + clf.offset_)","287","","288",""],"delete":[]}],"examples\/applications\/plot_species_distribution_modeling.py":[{"add":["170","        pred = clf.decision_function((coverages_land - mean) \/ std)","194","        pred_test = clf.decision_function((species.cov_test - mean) \/ std)"],"delete":["170","        pred = clf.decision_function((coverages_land - mean) \/ std)[:, 0]","194","        pred_test = clf.decision_function((species.cov_test - mean)","195","                                          \/ std)[:, 0]"]}],"sklearn\/neighbors\/lof.py":[{"add":["5","import warnings","97","        threshold on the decision function. If \"auto\", the decision function","98","        threshold is determined as in the original paper.","109","        The opposite LOF of the training samples. The higher, the more normal.","110","        Inliers tend to have a LOF score close to 1 (negative_outlier_factor_","111","        close to -1), while outliers tend to have a larger LOF score.","121","    offset_ : float","122","        Offset used to obtain binary labels from the raw scores.","123","        Observations having a negative_outlier_factor smaller than offset_ are","124","        detected as abnormal.","125","        The offset is set to -1.5 (inliers score around -1), except when a","126","        contamination parameter different than \"auto\" is provided. In that","127","        case, the offset is defined in such a way we obtain the expected","128","        number of outliers in training.","129","","137","                 contamination=\"legacy\", n_jobs=1):","144","        if contamination == \"legacy\":","145","            warnings.warn('default contamination parameter 0.1 will change '","146","                          'in version 0.22 to \"auto\". This will change the '","147","                          'predict method behavior.',","148","                          DeprecationWarning)","184","        if self.contamination not in [\"auto\", \"legacy\"]:  # rm legacy in 0.22","185","            if not(0. < self.contamination <= .5):","186","                raise ValueError(\"contamination must be in (0, 0.5], \"","187","                                 \"got: %f\" % self.contamination)","193","            warnings.warn(\"n_neighbors (%s) is greater than the \"","194","                          \"total number of samples (%s). n_neighbors \"","195","                          \"will be set to (n_samples - 1) for estimation.\"","196","                          % (self.n_neighbors, n_samples))","205","        # Compute lof score over training samples to define offset_:","211","        if self.contamination == \"auto\":","212","            # inliers score around -1 (the higher, the less abnormal).","213","            self.offset_ = -1.5","214","        elif self.contamination == \"legacy\":  # to rm in 0.22","215","            self.offset_ = scoreatpercentile(","216","                self.negative_outlier_factor_, 100. * 0.1)","217","        else:","218","            self.offset_ = scoreatpercentile(","219","                self.negative_outlier_factor_, 100. * self.contamination)","229","        this method is kept private. In particular, fit(X)._predict(X) is not","230","        the same as fit_predict(X).","244","        check_is_fitted(self, [\"offset_\", \"negative_outlier_factor_\",","250","            is_inlier[self._decision_function(X) < 0] = -1","253","            is_inlier[self.negative_outlier_factor_ < self.offset_] = -1","258","        \"\"\"Shifted opposite of the Local Outlier Factor of X","259","","260","        Bigger is better, i.e. large values correspond to inliers.","261","","262","        The shift offset allows a zero threshold for being an outlier.","263","        The argument X is supposed to contain *new data*: if X contains a","264","        point from training, it consider the later in its own neighborhood.","265","        Also, the samples in X are not considered in the neighborhood of any","266","        point.","267","        This method is kept private as the predict method is.","268","        The decision function on training data is available by considering the","269","        the negative_outlier_factor_ attribute.","270","","271","        Parameters","272","        ----------","273","        X : array-like, shape (n_samples, n_features)","274","            The query sample or samples to compute the Local Outlier Factor","275","            w.r.t. the training samples.","276","","277","        Returns","278","        -------","279","        shifted_opposite_lof_scores : array, shape (n_samples,)","280","            The shifted opposite of the Local Outlier Factor of each input","281","            samples. The lower, the more abnormal. Negative scores represent","282","            outliers, positive scores represent inliers.","283","        \"\"\"","284","        return self._score_samples(X) - self.offset_","285","","286","    def _score_samples(self, X):","287","        \"\"\"Opposite of the Local Outlier Factor of X (as bigger is","288","        better, i.e. large values correspond to inliers).","294","        This method is kept private as the predict method is.","308","        check_is_fitted(self, [\"offset_\", \"negative_outlier_factor_\","],"delete":["5","from warnings import warn","97","        threshold on the decision function.","108","        The opposite LOF of the training samples. The lower, the more abnormal.","109","        Inliers tend to have a LOF score close to 1, while outliers tend","110","        to have a larger LOF score.","127","                 contamination=0.1, n_jobs=1):","169","        if not (0. < self.contamination <= .5):","170","            raise ValueError(\"contamination must be in (0, 0.5]\")","176","            warn(\"n_neighbors (%s) is greater than the \"","177","                 \"total number of samples (%s). n_neighbors \"","178","                 \"will be set to (n_samples - 1) for estimation.\"","179","                 % (self.n_neighbors, n_samples))","188","        # Compute lof score over training samples to define threshold_:","194","        self.threshold_ = -scoreatpercentile(","195","            -self.negative_outlier_factor_, 100. * (1. - self.contamination))","205","        this method is kept private.","219","        check_is_fitted(self, [\"threshold_\", \"negative_outlier_factor_\",","225","            is_inlier[self._decision_function(X) <= self.threshold_] = -1","228","            is_inlier[self.negative_outlier_factor_ <= self.threshold_] = -1","233","        \"\"\"Opposite of the Local Outlier Factor of X (as bigger is better,","234","        i.e. large values correspond to inliers).","240","        The decision function on training data is available by considering the","241","        opposite of the negative_outlier_factor_ attribute.","255","        check_is_fitted(self, [\"threshold_\", \"negative_outlier_factor_\",","257",""]}]}},"20cb37e8f6e1eb6859239bac6307fcc213ddd52e":{"changes":{"sklearn\/kernel_ridge.py":"MODIFY","sklearn\/base.py":"MODIFY"},"diff":{"sklearn\/kernel_ridge.py":[{"add":["50","        should return a floating point number. Set to \"precomputed\" in","51","        order to pass a precomputed kernel matrix to the estimator","52","        methods instead of samples.","77","        Training data, which is also required for prediction. If","78","        kernel == \"precomputed\" this is instead the precomputed","79","        training matrix, shape = [n_samples, n_samples].","136","            Training data. If kernel == \"precomputed\" this is instead","137","            a precomputed kernel matrix, shape = [n_samples,","138","            n_samples].","181","            Samples. If kernel == \"precomputed\" this is instead a","182","            precomputed kernel matrix, shape = [n_samples,","183","            n_samples_fitted], where n_samples_fitted is the number of","184","            samples used in the fitting for this estimator."],"delete":["50","        should return a floating point number.","75","        Training data, which is also required for prediction","132","            Training data","175","            Samples."]}],"sklearn\/base.py":[{"add":["356","            Test samples. For some estimators this may be a","357","            precomputed kernel matrix instead, shape = (n_samples,","358","            n_samples_fitted], where n_samples_fitted is the number of","359","            samples used in the fitting for the estimator."],"delete":["356","            Test samples."]}]}},"22aafe04e82aec77c7e8e1347969c822355af634":{"changes":{"sklearn\/cluster\/bicluster.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/utils\/estimator_checks.py":"MODIFY"},"diff":{"sklearn\/cluster\/bicluster.py":[{"add":["112","    def fit(self, X, y=None):"],"delete":["112","    def fit(self, X):"]}],"doc\/whats_new.rst":[{"add":["421","   - :class:`cluster.bicluster.SpectralCoClustering` and","422","     :class:`cluster.bicluster.SpectralBiclustering` now accept ``y`` in fit.","423","     :issue:`6126` by `Andreas M¨¹ller`_.","424",""],"delete":[]}],"sklearn\/utils\/estimator_checks.py":[{"add":["472","    estimator.fit(X, y)"],"delete":["472","    # should be just `estimator.fit(X, y)`","473","    # after merging #6141","474","    if name in ['SpectralBiclustering']:","475","        estimator.fit(X)","476","    else:","477","        estimator.fit(X, y)"]}]}},"328b04f43e3dc22a4262e62e13312a0e98c2637f":{"changes":{"doc\/tutorial\/machine_learning_map\/pyparsing.py":"MODIFY","sklearn\/datasets\/samples_generator.py":"MODIFY","sklearn\/tests\/test_multioutput.py":"MODIFY","sklearn\/cluster\/tests\/test_hierarchical.py":"MODIFY"},"diff":{"doc\/tutorial\/machine_learning_map\/pyparsing.py":[{"add":["1304","           Fail action fn is a callable function that takes the arguments\r"],"delete":["1304","           Fail acton fn is a callable function that takes the arguments\r"]}],"sklearn\/datasets\/samples_generator.py":[{"add":["707","        If int, it is the total number of points equally divided among"],"delete":["707","        If int, it is the the total number of points equally divided among"]}],"sklearn\/tests\/test_multioutput.py":[{"add":["206","def test_multi_output_classification_partial_fit_no_first_classes_exception():"],"delete":["206","def test_mutli_output_classifiation_partial_fit_no_first_classes_exception():"]}],"sklearn\/cluster\/tests\/test_hierarchical.py":[{"add":["298","            # Sort the order of child nodes per row for consistency"],"delete":["298","            # Sort the order of of child nodes per row for consistency"]}]}},"a18eea48da429accded76a0e8cef7ac1a2906fb5":{"changes":{"sklearn\/pipeline.py":"MODIFY"},"diff":{"sklearn\/pipeline.py":[{"add":["646","    >>> union = FeatureUnion([(\"pca\", PCA(n_components=1)),","650","    array([[ 1.5       ,  3.0...,  0.8...],","651","           [-1.5       ,  5.7..., -0.4...]])"],"delete":["646","    >>> union = FeatureUnion([(\"pca\", PCA(n_components=2)),","650","    array([[ 1.5       ,  0.        ,  3.0...,  0.8...],","651","           [-1.5       ,  0.        ,  5.7..., -0.4...]])"]}]}},"9a5e160332bb79cd7e7c8015c7dd25a654788528":{"changes":{"sklearn\/utils\/estimator_checks.py":"MODIFY"},"diff":{"sklearn\/utils\/estimator_checks.py":[{"add":["1106","                if n_classes == 2:","1110","                if n_classes == 3:"],"delete":["1106","                if n_classes is 2:","1110","                if (n_classes is 3 and not isinstance(classifier, BaseLibSVM)):","1111","                    # 1on1 of LibSVM works differently"]}]}},"90adab947dd5e29bf83e0cdf954e09f4880abd55":{"changes":{"sklearn\/svm\/tests\/test_sparse.py":"MODIFY"},"diff":{"sklearn\/svm\/tests\/test_sparse.py":[{"add":["51","    assert_array_almost_equal(dense_svm.dual_coef_,","52","                              sparse_svm.dual_coef_.toarray())","57","    assert_array_almost_equal(dense_svm.predict(X_test_dense),","58","                              sparse_svm.predict(X_test))","126","    def kfunc(x, y):","127","        return safe_sparse_dot(x, y.T)","149","    # Test decision_function","151","    # Sanity check, test that decision_function implemented in python","152","    # returns the same as the one in libsvm","267","    # Check that sparse OneClassSVM gives the same result as dense OneClassSVM","278","            clf = svm.OneClassSVM(kernel=kernel)","279","            sp_clf = svm.OneClassSVM(kernel=kernel)"],"delete":["51","    assert_array_almost_equal(dense_svm.dual_coef_, sparse_svm.dual_coef_.toarray())","56","    assert_array_almost_equal(dense_svm.predict(X_test_dense), sparse_svm.predict(X_test))","124","    kfunc = lambda x, y: safe_sparse_dot(x, y.T)","146","    #Test decision_function","148","    #Sanity check, test that decision_function implemented in python","149","    #returns the same as the one in libsvm","264","    \"\"\"Check that sparse OneClassSVM gives the same result as dense OneClassSVM\"\"\"","275","            clf = svm.OneClassSVM(kernel=kernel, random_state=0)","276","            sp_clf = svm.OneClassSVM(kernel=kernel, random_state=0)"]}]}},"b4984e27dbc15305e0f8644ef9c49d5c457a33c1":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/model_selection\/tests\/test_search.py":"MODIFY","sklearn\/model_selection\/_search.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["617","- Invalid input for :class:`model_selection.ParameterGrid` now raises TypeError.","618","  :issue:`10928` by :user:`Solutus Immensus <solutusimmensus>`","619",""],"delete":[]}],"sklearn\/model_selection\/tests\/test_search.py":[{"add":["14","import pytest","129","@pytest.mark.parametrize(","130","    \"input, error_type, error_message\",","131","    [(0, TypeError, 'Parameter grid is not a dict or a list (0)'),","132","     ([{'foo': [0]}, 0], TypeError, 'Parameter grid is not a dict (0)'),","133","     ({'foo': 0}, TypeError, \"Parameter grid value is not iterable \"","134","      \"(key='foo', value=0)\")]","135",")","136","def test_validate_parameter_grid_input(input, error_type, error_message):","137","    with pytest.raises(error_type, message=error_message):","138","        ParameterGrid(input)","139","","141",""],"delete":[]}],"sklearn\/model_selection\/_search.py":[{"add":["15","from collections import Mapping, namedtuple, defaultdict, Sequence, Iterable","92","        if not isinstance(param_grid, (Mapping, Iterable)):","93","            raise TypeError('Parameter grid is not a dict or '","94","                            'a list ({!r})'.format(param_grid))","95","","100","","101","        # check if all entries are dictionaries of lists","102","        for grid in param_grid:","103","            if not isinstance(grid, dict):","104","                raise TypeError('Parameter grid is not a '","105","                                'dict ({!r})'.format(grid))","106","            for key in grid:","107","                if not isinstance(grid[key], Iterable):","108","                    raise TypeError('Parameter grid value is not iterable '","109","                                    '(key={!r}, value={!r})'","110","                                    .format(key, grid[key]))","111",""],"delete":["15","from collections import Mapping, namedtuple, defaultdict, Sequence"]}]}},"d788dcb1990fffd765c73bea1b63854f0ca6fe20":{"changes":{"sklearn\/utils\/tests\/test_testing.py":"MODIFY","sklearn\/utils\/testing.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/utils\/estimator_checks.py":"MODIFY","sklearn\/tests\/test_common.py":"MODIFY","sklearn\/utils\/tests\/test_estimator_checks.py":"MODIFY"},"diff":{"sklearn\/utils\/tests\/test_testing.py":[{"add":["3","import numpy as np","4","from scipy import sparse","17","    assert_allclose_dense_sparse,","55","def test_assert_allclose_dense_sparse():","56","    x = np.arange(9).reshape(3, 3)","57","    msg = \"Not equal to tolerance \"","58","    y = sparse.csc_matrix(x)","59","    for X in [x, y]:","60","        # basic compare","61","        assert_raise_message(AssertionError, msg, assert_allclose_dense_sparse,","62","                             X, X * 2)","63","        assert_allclose_dense_sparse(X, X)","64","","65","    assert_raise_message(ValueError, \"Can only compare two sparse\",","66","                         assert_allclose_dense_sparse, x, y)","67","","68","    A = sparse.diags(np.ones(5), offsets=0).tocsr()","69","    B = sparse.csr_matrix(np.ones((1, 5)))","70","","71","    assert_raise_message(AssertionError, \"Arrays are not equal\",","72","                         assert_allclose_dense_sparse, B, A)","73","","74","","198","# `clean_warning_registry()` is called internally by assert_warns"],"delete":["175","#`clean_warning_registry()` is called internally by assert_warns"]}],"sklearn\/utils\/testing.py":[{"add":["377","def assert_allclose_dense_sparse(x, y, rtol=1e-07, atol=0, err_msg=''):","378","    \"\"\"Assert allclose for sparse and dense data.","379","","380","    Both x and y need to be either sparse or dense, they","381","    can't be mixed.","382","","383","    Parameters","384","    ----------","385","    x : array-like or sparse matrix","386","        First array to compare.","387","","388","    y : array-like or sparse matrix","389","        Second array to compare.","390","","391","    err_msg : string, default=''","392","        Error message to raise.","393","    \"\"\"","394","    if sp.sparse.issparse(x) and sp.sparse.issparse(y):","395","        x = x.tocsr()","396","        y = y.tocsr()","397","        x.sum_duplicates()","398","        y.sum_duplicates()","399","        assert_array_equal(x.indices, y.indices, err_msg=err_msg)","400","        assert_array_equal(x.indptr, y.indptr, err_msg=err_msg)","401","        assert_allclose(x.data, y.data, rtol=rtol, atol=atol, err_msg=err_msg)","402","    elif not sp.sparse.issparse(x) and not sp.sparse.issparse(y):","403","        # both dense","404","        assert_allclose(x, y, rtol=rtol, atol=atol, err_msg=err_msg)","405","    else:","406","        raise ValueError(\"Can only compare two sparse matrices,\"","407","                         \" not a sparse matrix and an array.\")","408","","409",""],"delete":[]}],"doc\/whats_new.rst":[{"add":["365","   - All checks in ``utils.estimator_checks``, in particular","366","     :func:`utils.estimator_checks.check_estimator` now accept estimator","367","     instances. Most other checks do not accept","368","     estimator classes any more. :issue:`9019` by `Andreas M¨¹ller`_.","369",""],"delete":[]}],"sklearn\/utils\/estimator_checks.py":[{"add":["24","from sklearn.utils.testing import assert_allclose","25","from sklearn.utils.testing import assert_allclose_dense_sparse","34","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis","52","from sklearn.utils.validation import has_fit_parameter, _num_samples","70","def _yield_non_meta_checks(name, estimator):","96","    if hasattr(estimator, 'sparsify'):","106","def _yield_classifier_checks(name, classifier):","126","    if 'class_weight' in classifier.get_params().keys():","135","def check_supervised_y_no_nan(name, estimator_orig):","137","    estimator = clone(estimator_orig)","141","    y = multioutput_estimator_convert_y_2d(estimator, y)","146","        estimator.fit(X, y)","149","            raise ValueError(\"Estimator {0} raised error as expected, but \"","157","def _yield_regressor_checks(name, regressor):","176","def _yield_transformer_checks(name, transformer):","196","def _yield_clustering_checks(name, clusterer):","206","def _yield_all_checks(name, estimator):","207","    for check in _yield_non_meta_checks(name, estimator):","209","    if isinstance(estimator, ClassifierMixin):","210","        for check in _yield_classifier_checks(name, estimator):","212","    if isinstance(estimator, RegressorMixin):","213","        for check in _yield_regressor_checks(name, estimator):","215","    if isinstance(estimator, TransformerMixin):","216","        for check in _yield_transformer_checks(name, estimator):","218","    if isinstance(estimator, ClusterMixin):","219","        for check in _yield_clustering_checks(name, estimator):","240","    This test can be applied to classes or instances.","241","    Classes currently have some additional tests that related to construction,","242","    while passing instances allows the testing of multiple options.","243","","246","    estimator : estimator object or class","247","        Estimator to check. Estimator is a class object or instance.","250","    if isinstance(Estimator, type):","251","        # got a class","252","        name = Estimator.__name__","253","        check_parameters_default_constructible(name, Estimator)","254","        check_no_fit_attributes_set_in_init(name, Estimator)","255","        estimator = Estimator()","256","    else:","257","        # got an instance","258","        estimator = Estimator","259","        name = type(estimator).__name__","260","","261","    for check in _yield_all_checks(name, estimator):","263","            check(name, estimator)","282","def set_checking_parameters(estimator):","292","        # LinearSVR, LinearSVC","293","        if estimator.__class__.__name__ in ['LinearSVR', 'LinearSVC']:","329","        estimator.set_params(n_components=2)","352","def check_estimator_sparse_data(name, estimator_orig):","358","    # catch deprecation warnings","359","    with ignore_warnings(category=DeprecationWarning):","360","        estimator = clone(estimator_orig)","361","    y = multioutput_estimator_convert_y_2d(estimator, y)","367","                estimator = clone(estimator).set_params(with_mean=False)","369","                estimator = clone(estimator)","395","def check_sample_weights_pandas_series(name, estimator_orig):","398","    estimator = clone(estimator_orig)","417","def check_sample_weights_list(name, estimator_orig):","420","    if has_fit_parameter(estimator_orig, \"sample_weight\"):","421","        estimator = clone(estimator_orig)","425","        y = multioutput_estimator_convert_y_2d(estimator, y)","432","def check_dtype_object(name, estimator_orig):","437","    estimator = clone(estimator_orig)","438","    y = multioutput_estimator_convert_y_2d(estimator, y)","459","def check_dict_unchanged(name, estimator_orig):","473","    estimator = clone(estimator_orig)","474","    y = multioutput_estimator_convert_y_2d(estimator, y)","500","@ignore_warnings(category=DeprecationWarning)","501","def check_dont_overwrite_parameters(name, estimator_orig):","503","    if hasattr(estimator_orig.__init__, \"deprecated_original\"):","506","    estimator = clone(estimator_orig)","510","    y = multioutput_estimator_convert_y_2d(estimator, y)","550","@ignore_warnings(category=DeprecationWarning)","551","def check_fit2d_predict1d(name, estimator_orig):","556","    estimator = clone(estimator_orig)","557","    y = multioutput_estimator_convert_y_2d(estimator, y)","575","def check_fit2d_1sample(name, estimator_orig):","580","    estimator = clone(estimator_orig)","581","    y = multioutput_estimator_convert_y_2d(estimator, y)","596","def check_fit2d_1feature(name, estimator_orig):","601","    estimator = clone(estimator_orig)","602","    y = multioutput_estimator_convert_y_2d(estimator, y)","617","def check_fit1d_1feature(name, estimator_orig):","622","    estimator = clone(estimator_orig)","623","    y = multioutput_estimator_convert_y_2d(estimator, y)","639","def check_fit1d_1sample(name, estimator_orig):","644","    estimator = clone(estimator_orig)","645","    y = multioutput_estimator_convert_y_2d(estimator, y)","661","def check_transformer_general(name, transformer):","666","    _check_transformer(name, transformer, X, y)","667","    _check_transformer(name, transformer, X.tolist(), y.tolist())","671","def check_transformer_data_not_an_array(name, transformer):","680","    _check_transformer(name, transformer, this_X, this_y)","683","@ignore_warnings(category=DeprecationWarning)","684","def check_transformers_unfitted(name, transformer):","687","    transformer = clone(transformer)","692","def _check_transformer(name, transformer_orig, X, y):","702","    transformer = clone(transformer_orig)","734","                assert_allclose_dense_sparse(","735","                    x_pred, x_pred2, atol=1e-2,","736","                    err_msg=\"fit_transform and transform outcomes \"","737","                            \"not consistent in %s\"","738","                    % transformer)","739","                assert_allclose_dense_sparse(","740","                    x_pred, x_pred3, atol=1e-2,","741","                    err_msg=\"consecutive fit_transform outcomes \"","742","                            \"not consistent in %s\"","743","                    % transformer)","745","            assert_allclose_dense_sparse(","746","                X_pred, X_pred2,","747","                err_msg=\"fit_transform and transform outcomes \"","748","                        \"not consistent in %s\"","749","                % transformer, atol=1e-2)","750","            assert_allclose_dense_sparse(","751","                X_pred, X_pred3, atol=1e-2,","752","                err_msg=\"consecutive fit_transform outcomes \"","753","                        \"not consistent in %s\"","754","                % transformer)","755","            assert_equal(_num_samples(X_pred2), n_samples)","756","            assert_equal(_num_samples(X_pred3), n_samples)","765","def check_pipeline_consistency(name, estimator_orig):","779","    estimator = clone(estimator_orig)","780","    y = multioutput_estimator_convert_y_2d(estimator, y)","794","            assert_allclose_dense_sparse(result, result_pipe)","798","def check_fit_score_takes_y(name, estimator_orig):","804","    estimator = clone(estimator_orig)","805","    y = multioutput_estimator_convert_y_2d(estimator, y)","814","            if args[0] == \"self\":","815","                # if_delegate_has_method makes methods into functions","816","                # with an explicit \"self\", so need to shift arguments","817","                args = args[1:]","821","                        % (func_name, type(estimator).__name__, args))","825","def check_estimators_dtypes(name, estimator_orig):","832","    y = multioutput_estimator_convert_y_2d(estimator_orig, y)","837","        estimator = clone(estimator_orig)","847","def check_estimators_empty_data_messages(name, estimator_orig):","848","    e = clone(estimator_orig)","859","    y = multioutput_estimator_convert_y_2d(e, np.array([1, 0, 1]))","865","@ignore_warnings(category=DeprecationWarning)","866","def check_estimators_nan_inf(name, estimator_orig):","876","    y = multioutput_estimator_convert_y_2d(estimator_orig, y)","885","            estimator = clone(estimator_orig)","892","                    print(error_string_fit, estimator, e)","896","                print(error_string_fit, estimator, exc)","900","                raise AssertionError(error_string_fit, estimator)","910","                        print(error_string_predict, estimator, e)","914","                    print(error_string_predict, estimator, exc)","917","                    raise AssertionError(error_string_predict, estimator)","925","                        print(error_string_transform, estimator, e)","929","                    print(error_string_transform, estimator, exc)","932","                    raise AssertionError(error_string_transform, estimator)","936","def check_estimators_pickle(name, estimator_orig):","947","    estimator = clone(estimator_orig)","949","    # some estimators only take multioutputs","950","    y = multioutput_estimator_convert_y_2d(estimator, y)","962","    if estimator.__module__.startswith('sklearn.'):","968","        assert_allclose_dense_sparse(result[method], unpickled_result)","971","@ignore_warnings(category=DeprecationWarning)","972","def check_estimators_partial_fit_n_features(name, estimator_orig):","974","    if not hasattr(estimator_orig, 'partial_fit'):","976","    estimator = clone(estimator_orig)","981","        if isinstance(estimator, ClassifierMixin):","983","            estimator.partial_fit(X, y, classes=classes)","985","            estimator.partial_fit(X, y)","989","    assert_raises(ValueError, estimator.partial_fit, X[:, :-1], y)","992","@ignore_warnings(category=DeprecationWarning)","993","def check_clustering(name, clusterer_orig):","994","    clusterer = clone(clusterer_orig)","1000","    if hasattr(clusterer, \"n_clusters\"):","1001","        clusterer.set_params(n_clusters=3)","1002","    set_random_state(clusterer)","1004","        clusterer.set_params(preference=-100)","1005","        clusterer.set_params(max_iter=100)","1008","    clusterer.fit(X)","1010","    clusterer.fit(X.tolist())","1012","    assert_equal(clusterer.labels_.shape, (n_samples,))","1013","    pred = clusterer.labels_","1019","    set_random_state(clusterer)","1021","        pred2 = clusterer.fit_predict(X)","1025","@ignore_warnings(category=DeprecationWarning)","1026","def check_clusterer_compute_labels_predict(name, clusterer_orig):","1029","    clusterer = clone(clusterer_orig)","1042","@ignore_warnings(category=DeprecationWarning)","1043","def check_classifiers_one_label(name, classifier_orig):","1053","        classifier = clone(classifier_orig)","1059","                print(error_string_fit, classifier, e)","1065","            print(error_string_fit, classifier, exc)","1072","            print(error_string_predict, classifier, exc)","1077","def check_classifiers_train(name, classifier_orig):","1088","        classifier = clone(classifier_orig)","1136","            assert_allclose(np.sum(y_prob, axis=1), np.ones(n_samples))","1144","                assert_allclose(y_log_prob, np.log(y_prob), 8)","1149","def check_estimators_fit_returns_self(name, estimator_orig):","1155","    estimator = clone(estimator_orig)","1156","    y = multioutput_estimator_convert_y_2d(estimator, y)","1164","def check_estimators_unfitted(name, estimator_orig):","1175","    est = clone(estimator_orig)","1196","def check_supervised_y_2d(name, estimator_orig):","1203","    estimator = clone(estimator_orig)","1224","    assert_allclose(y_pred.ravel(), y_pred_2d.ravel())","1227","@ignore_warnings(category=DeprecationWarning)","1228","def check_classifiers_classes(name, classifier_orig):","1245","        classifier = clone(classifier_orig)","1262","def check_regressors_int(name, regressor_orig):","1267","    y = multioutput_estimator_convert_y_2d(regressor_orig, y)","1270","    regressor_1 = clone(regressor_orig)","1271","    regressor_2 = clone(regressor_orig)","1286","    assert_allclose(pred1, pred2, atol=1e-2, err_msg=name)","1290","def check_regressors_train(name, regressor_orig):","1294","    regressor = clone(regressor_orig)","1295","    y = multioutput_estimator_convert_y_2d(regressor, y)","1325","def check_regressors_no_decision_function(name, regressor_orig):","1329","    regressor = clone(regressor_orig)","1330","    y = multioutput_estimator_convert_y_2d(regressor, X[:, 0])","1348","@ignore_warnings(category=DeprecationWarning)","1349","def check_class_weight_classifiers(name, classifier_orig):","1370","        classifier = clone(classifier_orig).set_params(","1371","            class_weight=class_weight)","1383","@ignore_warnings(category=DeprecationWarning)","1384","def check_class_weight_balanced_classifiers(name, classifier_orig, X_train,","1385","                                            y_train, X_test, y_test, weights):","1386","    classifier = clone(classifier_orig)","1401","@ignore_warnings(category=DeprecationWarning)","1404","    # this is run on classes, not instances, though this should be changed","1409","    classifier = Classifier()","1429","    assert_allclose(coef_balanced, coef_manual)","1433","def check_estimators_overwrite_params(name, estimator_orig):","1437","    estimator = clone(estimator_orig)","1438","    y = multioutput_estimator_convert_y_2d(estimator, y)","1466","@ignore_warnings(category=DeprecationWarning)","1469","    # this check works on classes, not instances","1485","@ignore_warnings(category=DeprecationWarning)","1486","def check_sparsify_coefficients(name, estimator_orig):","1490","    est = clone(estimator_orig)","1508","@ignore_warnings(category=DeprecationWarning)","1509","def check_classifier_data_not_an_array(name, estimator_orig):","1512","    y = multioutput_estimator_convert_y_2d(estimator_orig, y)","1513","    check_estimators_data_not_an_array(name, estimator_orig, X, y)","1517","def check_regressor_data_not_an_array(name, estimator_orig):","1518","    X, y = _boston_subset(n_samples=50)","1519","    y = multioutput_estimator_convert_y_2d(estimator_orig, y)","1520","    check_estimators_data_not_an_array(name, estimator_orig, X, y)","1521","","1522","","1523","@ignore_warnings(category=DeprecationWarning)","1524","def check_estimators_data_not_an_array(name, estimator_orig, X, y):","1529","    estimator_1 = clone(estimator_orig)","1530","    estimator_2 = clone(estimator_orig)","1542","    assert_allclose(pred1, pred2, atol=1e-2, err_msg=name)","1546","    # this check works on classes, not instances","1608","def multioutput_estimator_convert_y_2d(estimator, y):","1611","    if \"MultiTask\" in estimator.__class__.__name__:","1617","def check_non_transformer_estimators_n_iter(name, estimator_orig):","1635","        estimator = clone(estimator_orig).set_params(alpha=0.)","1637","        estimator = clone(estimator_orig)","1641","        y_ = multioutput_estimator_convert_y_2d(estimator, y_)","1656","def check_transformer_n_iter(name, estimator_orig):","1659","    estimator = clone(estimator_orig)","1682","def check_get_params_invariance(name, estimator_orig):","1697","    e = clone(estimator_orig)","1706","@ignore_warnings(category=DeprecationWarning)","1707","def check_classifiers_regression_target(name, estimator_orig):","1712","    e = clone(estimator_orig)","1718","def check_decision_proba_consistency(name, estimator_orig):","1726","    estimator = clone(estimator_orig)"],"delete":["24","from sklearn.utils.testing import assert_array_almost_equal","39","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis","51","from sklearn.utils.validation import has_fit_parameter","69","def _yield_non_meta_checks(name, Estimator):","95","    if hasattr(Estimator, 'sparsify'):","105","def _yield_classifier_checks(name, Classifier):","125","    if 'class_weight' in Classifier().get_params().keys():","134","def check_supervised_y_no_nan(name, Estimator):","136","","140","    y = multioutput_estimator_convert_y_2d(name, y)","145","        Estimator().fit(X, y)","148","            raise ValueError(\"Estimator {0} raised warning as expected, but \"","156","def _yield_regressor_checks(name, Regressor):","175","def _yield_transformer_checks(name, Transformer):","195","def _yield_clustering_checks(name, Clusterer):","205","def _yield_all_checks(name, Estimator):","206","    for check in _yield_non_meta_checks(name, Estimator):","208","    if issubclass(Estimator, ClassifierMixin):","209","        for check in _yield_classifier_checks(name, Estimator):","211","    if issubclass(Estimator, RegressorMixin):","212","        for check in _yield_regressor_checks(name, Estimator):","214","    if issubclass(Estimator, TransformerMixin):","215","        for check in _yield_transformer_checks(name, Estimator):","217","    if issubclass(Estimator, ClusterMixin):","218","        for check in _yield_clustering_checks(name, Estimator):","227","    yield check_no_fit_attributes_set_in_init","242","    Estimator : class","243","        Class to check. Estimator is a class object (not an instance).","246","    name = Estimator.__name__","247","    check_parameters_default_constructible(name, Estimator)","248","    for check in _yield_all_checks(name, Estimator):","250","            check(name, Estimator)","269","def set_testing_parameters(estimator):","279","        # LinearSVR","280","        if estimator.__class__.__name__ == 'LinearSVR':","316","        estimator.set_params(n_components=1)","339","def check_estimator_sparse_data(name, Estimator):","350","                estimator = Estimator(with_mean=False)","352","                estimator = Estimator()","353","        set_testing_parameters(estimator)","379","def check_sample_weights_pandas_series(name, Estimator):","382","    estimator = Estimator()","401","def check_sample_weights_list(name, Estimator):","404","    estimator = Estimator()","405","    if has_fit_parameter(estimator, \"sample_weight\"):","409","        y = multioutput_estimator_convert_y_2d(name, y)","416","def check_dtype_object(name, Estimator):","421","    y = multioutput_estimator_convert_y_2d(name, y)","422","    estimator = Estimator()","423","    set_testing_parameters(estimator)","444","def check_dict_unchanged(name, Estimator):","458","    y = multioutput_estimator_convert_y_2d(name, y)","459","    estimator = Estimator()","460","    set_testing_parameters(estimator)","486","def check_dont_overwrite_parameters(name, Estimator):","488","    if hasattr(Estimator.__init__, \"deprecated_original\"):","494","    y = multioutput_estimator_convert_y_2d(name, y)","495","    estimator = Estimator()","496","    set_testing_parameters(estimator)","536","def check_fit2d_predict1d(name, Estimator):","541","    y = multioutput_estimator_convert_y_2d(name, y)","542","    estimator = Estimator()","543","    set_testing_parameters(estimator)","561","def check_fit2d_1sample(name, Estimator):","566","    y = multioutput_estimator_convert_y_2d(name, y)","567","    estimator = Estimator()","568","    set_testing_parameters(estimator)","583","def check_fit2d_1feature(name, Estimator):","588","    y = multioutput_estimator_convert_y_2d(name, y)","589","    estimator = Estimator()","590","    set_testing_parameters(estimator)","605","def check_fit1d_1feature(name, Estimator):","610","    y = multioutput_estimator_convert_y_2d(name, y)","611","    estimator = Estimator()","612","    set_testing_parameters(estimator)","628","def check_fit1d_1sample(name, Estimator):","633","    y = multioutput_estimator_convert_y_2d(name, y)","634","    estimator = Estimator()","635","    set_testing_parameters(estimator)","651","def check_transformer_general(name, Transformer):","656","    _check_transformer(name, Transformer, X, y)","657","    _check_transformer(name, Transformer, X.tolist(), y.tolist())","661","def check_transformer_data_not_an_array(name, Transformer):","670","    _check_transformer(name, Transformer, this_X, this_y)","673","def check_transformers_unfitted(name, Transformer):","676","    with ignore_warnings(category=DeprecationWarning):","677","        transformer = Transformer()","682","def _check_transformer(name, Transformer, X, y):","692","    # catch deprecation warnings","693","    transformer = Transformer()","695","    set_testing_parameters(transformer)","726","                assert_array_almost_equal(","727","                    x_pred, x_pred2, 2,","728","                    \"fit_transform and transform outcomes not consistent in %s\"","729","                    % Transformer)","730","                assert_array_almost_equal(","731","                    x_pred, x_pred3, 2,","732","                    \"consecutive fit_transform outcomes not consistent in %s\"","733","                    % Transformer)","735","            assert_array_almost_equal(","736","                X_pred, X_pred2, 2,","737","                \"fit_transform and transform outcomes not consistent in %s\"","738","                % Transformer)","739","            assert_array_almost_equal(","740","                X_pred, X_pred3, 2,","741","                \"consecutive fit_transform outcomes not consistent in %s\"","742","                % Transformer)","743","            assert_equal(len(X_pred2), n_samples)","744","            assert_equal(len(X_pred3), n_samples)","753","def check_pipeline_consistency(name, Estimator):","767","    y = multioutput_estimator_convert_y_2d(name, y)","768","    estimator = Estimator()","769","    set_testing_parameters(estimator)","783","            assert_array_almost_equal(result, result_pipe)","787","def check_fit_score_takes_y(name, Estimator):","793","    y = multioutput_estimator_convert_y_2d(name, y)","794","    estimator = Estimator()","795","    set_testing_parameters(estimator)","807","                        % (func_name, Estimator.__name__, args))","811","def check_estimators_dtypes(name, Estimator):","818","    y = multioutput_estimator_convert_y_2d(name, y)","823","        estimator = Estimator()","824","        set_testing_parameters(estimator)","834","def check_estimators_empty_data_messages(name, Estimator):","835","    e = Estimator()","836","    set_testing_parameters(e)","847","    y = multioutput_estimator_convert_y_2d(name, np.array([1, 0, 1]))","853","def check_estimators_nan_inf(name, Estimator):","863","    y = multioutput_estimator_convert_y_2d(name, y)","872","            estimator = Estimator()","873","            set_testing_parameters(estimator)","880","                    print(error_string_fit, Estimator, e)","884","                print(error_string_fit, Estimator, exc)","888","                raise AssertionError(error_string_fit, Estimator)","898","                        print(error_string_predict, Estimator, e)","902","                    print(error_string_predict, Estimator, exc)","905","                    raise AssertionError(error_string_predict, Estimator)","913","                        print(error_string_transform, Estimator, e)","917","                    print(error_string_transform, Estimator, exc)","920","                    raise AssertionError(error_string_transform, Estimator)","924","def check_estimators_pickle(name, Estimator):","935","    # some estimators only take multioutputs","936","    y = multioutput_estimator_convert_y_2d(name, y)","938","    estimator = Estimator()","941","    set_testing_parameters(estimator)","951","    if Estimator.__module__.startswith('sklearn.'):","957","        assert_array_almost_equal(result[method], unpickled_result)","960","def check_estimators_partial_fit_n_features(name, Alg):","962","    if not hasattr(Alg, 'partial_fit'):","966","    with ignore_warnings(category=DeprecationWarning):","967","        alg = Alg()","968","    if not hasattr(alg, 'partial_fit'):","969","        # check again as for mlp this depends on algorithm","970","        return","972","    set_testing_parameters(alg)","974","        if isinstance(alg, ClassifierMixin):","976","            alg.partial_fit(X, y, classes=classes)","978","            alg.partial_fit(X, y)","982","    assert_raises(ValueError, alg.partial_fit, X[:, :-1], y)","985","def check_clustering(name, Alg):","991","    with ignore_warnings(category=DeprecationWarning):","992","        alg = Alg()","993","    set_testing_parameters(alg)","994","    if hasattr(alg, \"n_clusters\"):","995","        alg.set_params(n_clusters=3)","996","    set_random_state(alg)","998","        alg.set_params(preference=-100)","999","        alg.set_params(max_iter=100)","1002","    alg.fit(X)","1004","    alg.fit(X.tolist())","1006","    assert_equal(alg.labels_.shape, (n_samples,))","1007","    pred = alg.labels_","1013","    set_random_state(alg)","1015","        pred2 = alg.fit_predict(X)","1019","def check_clusterer_compute_labels_predict(name, Clusterer):","1022","    clusterer = Clusterer()","1035","def check_classifiers_one_label(name, Classifier):","1045","        classifier = Classifier()","1046","        set_testing_parameters(classifier)","1052","                print(error_string_fit, Classifier, e)","1058","            print(error_string_fit, Classifier, exc)","1065","            print(error_string_predict, Classifier, exc)","1070","def check_classifiers_train(name, Classifier):","1081","        classifier = Classifier()","1084","        set_testing_parameters(classifier)","1130","            assert_array_almost_equal(np.sum(y_prob, axis=1),","1131","                                      np.ones(n_samples))","1139","                assert_array_almost_equal(y_log_prob, np.log(y_prob), 8)","1144","def check_estimators_fit_returns_self(name, Estimator):","1147","    y = multioutput_estimator_convert_y_2d(name, y)","1151","    estimator = Estimator()","1153","    set_testing_parameters(estimator)","1160","def check_estimators_unfitted(name, Estimator):","1171","    est = Estimator()","1192","def check_supervised_y_2d(name, Estimator):","1199","    estimator = Estimator()","1200","    set_testing_parameters(estimator)","1221","    assert_array_almost_equal(y_pred.ravel(), y_pred_2d.ravel())","1224","def check_classifiers_classes(name, Classifier):","1241","        with ignore_warnings(category=DeprecationWarning):","1242","            classifier = Classifier()","1245","        set_testing_parameters(classifier)","1260","def check_regressors_int(name, Regressor):","1265","    y = multioutput_estimator_convert_y_2d(name, y)","1268","    regressor_1 = Regressor()","1269","    regressor_2 = Regressor()","1270","    set_testing_parameters(regressor_1)","1271","    set_testing_parameters(regressor_2)","1286","    assert_array_almost_equal(pred1, pred2, 2, name)","1290","def check_regressors_train(name, Regressor):","1294","    y = multioutput_estimator_convert_y_2d(name, y)","1296","    # catch deprecation warnings","1297","    regressor = Regressor()","1298","    set_testing_parameters(regressor)","1327","def check_regressors_no_decision_function(name, Regressor):","1331","    y = multioutput_estimator_convert_y_2d(name, X[:, 0])","1332","    regressor = Regressor()","1334","    set_testing_parameters(regressor)","1351","def check_class_weight_classifiers(name, Classifier):","1372","        with ignore_warnings(category=DeprecationWarning):","1373","            classifier = Classifier(class_weight=class_weight)","1385","def check_class_weight_balanced_classifiers(name, Classifier, X_train, y_train,","1386","                                            X_test, y_test, weights):","1387","    with ignore_warnings(category=DeprecationWarning):","1388","        classifier = Classifier()","1409","    with ignore_warnings(category=DeprecationWarning):","1410","        classifier = Classifier()","1430","    assert_array_almost_equal(coef_balanced, coef_manual)","1434","def check_estimators_overwrite_params(name, Estimator):","1436","    y = multioutput_estimator_convert_y_2d(name, y)","1439","    estimator = Estimator()","1441","    set_testing_parameters(estimator)","1485","def check_sparsify_coefficients(name, Estimator):","1489","    est = Estimator()","1507","def check_classifier_data_not_an_array(name, Estimator):","1510","    y = multioutput_estimator_convert_y_2d(name, y)","1511","    check_estimators_data_not_an_array(name, Estimator, X, y)","1512","","1513","","1514","def check_regressor_data_not_an_array(name, Estimator):","1515","    X, y = _boston_subset(n_samples=50)","1516","    y = multioutput_estimator_convert_y_2d(name, y)","1517","    check_estimators_data_not_an_array(name, Estimator, X, y)","1521","def check_estimators_data_not_an_array(name, Estimator, X, y):","1526","    estimator_1 = Estimator()","1527","    estimator_2 = Estimator()","1528","    set_testing_parameters(estimator_1)","1529","    set_testing_parameters(estimator_2)","1541","    assert_array_almost_equal(pred1, pred2, 2, name)","1606","def multioutput_estimator_convert_y_2d(name, y):","1609","    if \"MultiTask\" in name:","1615","def check_non_transformer_estimators_n_iter(name, Estimator):","1633","        estimator = Estimator(alpha=0.)","1635","        estimator = Estimator()","1639","        y_ = multioutput_estimator_convert_y_2d(name, y_)","1654","def check_transformer_n_iter(name, Estimator):","1657","    estimator = Estimator()","1680","def check_get_params_invariance(name, estimator):","1695","    if name in ('FeatureUnion', 'Pipeline'):","1696","        e = estimator([('clf', T())])","1697","","1698","    elif name in ('GridSearchCV', 'RandomizedSearchCV', 'SelectFromModel'):","1699","        return","1700","","1701","    else:","1702","        e = estimator()","1711","def check_classifiers_regression_target(name, Estimator):","1716","    e = Estimator()","1722","def check_decision_proba_consistency(name, Estimator):","1730","    estimator = Estimator()","1731","","1732","    set_testing_parameters(estimator)"]}],"sklearn\/tests\/test_common.py":[{"add":["30","    set_checking_parameters,","32","    check_no_fit_attributes_set_in_init,","67","        estimator = Estimator()","68","        # check this on class","69","        yield _named_check(","70","            check_no_fit_attributes_set_in_init, name), name, Estimator","71","","72","        for check in _yield_all_checks(name, estimator):","73","            set_checking_parameters(estimator)","74","            yield _named_check(check, name), name, estimator"],"delete":["65","        for check in _yield_all_checks(name, Estimator):","66","            yield _named_check(check, name), name, Estimator"]}],"sklearn\/utils\/tests\/test_estimator_checks.py":[{"add":["4","from sklearn.externals import joblib","7","from sklearn.utils.testing import (assert_raises_regex, assert_true,","8","                                   assert_equal)","10","from sklearn.utils.estimator_checks import set_random_state","11","from sklearn.utils.estimator_checks import set_checking_parameters","14","from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier","15","from sklearn.linear_model import LinearRegression, SGDClassifier","16","from sklearn.mixture import GaussianMixture","17","from sklearn.cluster import MiniBatchKMeans","18","from sklearn.decomposition import NMF","130","    assert_raises_regex(TypeError, msg, check_estimator, object())","134","    assert_raises_regex(AttributeError, msg, check_estimator, BaseEstimator())","137","    assert_raises_regex(AssertionError, msg, check_estimator,","138","                        BaseBadClassifier)","139","    assert_raises_regex(AssertionError, msg, check_estimator,","140","                        BaseBadClassifier())","153","    assert_raises_regex(AssertionError, msg, check_estimator,","154","                        NoCheckinPredict())","175","    msg = \"Estimator %s doesn't seem to fail gracefully on sparse data\" % name","192","    check_estimator(AdaBoostClassifier())","194","    check_estimator(MultiTaskElasticNet())","195","","196","","197","def test_check_estimator_clones():","198","    # check that check_estimator doesn't modify the estimator it receives","199","    from sklearn.datasets import load_iris","200","    iris = load_iris()","201","","202","    for Estimator in [GaussianMixture, LinearRegression,","203","                      RandomForestClassifier, NMF, SGDClassifier,","204","                      MiniBatchKMeans]:","205","        est = Estimator()","206","        set_checking_parameters(est)","207","        set_random_state(est)","208","        # without fitting","209","        old_hash = joblib.hash(est)","210","        check_estimator(est)","211","        assert_equal(old_hash, joblib.hash(est))","212","","213","        est = Estimator()","214","        set_checking_parameters(est)","215","        set_random_state(est)","216","        # with fitting","217","        est.fit(iris.data + 10, iris.target)","218","        old_hash = joblib.hash(est)","219","        check_estimator(est)","220","        assert_equal(old_hash, joblib.hash(est))","228","                        \"estimator\", NoSparseClassifier())","232","    check_estimators_unfitted(\"estimator\", CorrectNotFittedErrorClassifier())"],"delete":["6","from sklearn.utils.testing import assert_raises_regex, assert_true","10","from sklearn.ensemble import AdaBoostClassifier","127","    assert_raises_regex(AssertionError, msg, check_estimator, BaseBadClassifier)","160","    msg = \"Estimator \" + name + \" doesn't seem to fail gracefully on sparse data\"","185","                        \"estimator\", NoSparseClassifier)","189","    check_estimators_unfitted(\"estimator\", CorrectNotFittedErrorClassifier)"]}]}},"5fcf6f486fb0134c91f5a8741fe7e450539883f0":{"changes":{"sklearn\/svm\/base.py":"MODIFY"},"diff":{"sklearn\/svm\/base.py":[{"add":["768","    \"\"\"Used by Logistic Regression (and CV) and LinearSVC\/LinearSVR.","893","    y_ind = np.require(y_ind, requirements=\"W\")"],"delete":["768","    \"\"\"Used by Logistic Regression (and CV) and LinearSVC."]}]}},"08f04d95261d989b26683d0ba5bad4c8eb7c3040":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/ensemble\/gradient_boosting.py":"MODIFY","doc\/modules\/ensemble.rst":"MODIFY","sklearn\/ensemble\/tests\/test_gradient_boosting.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["53","- :class:`ensemble.gradient_boosting.GradientBoostingClassifier` (bug fix affecting feature importances)","433","- Fixed a bug in :class:`ensemble.gradient_boosting.GradientBoostingRegressor`","434","  and :class:`ensemble.gradient_boosting.GradientBoostingClassifier` to have","435","  feature importances summed and then normalized, rather than normalizing on a","436","  per-tree basis. The previous behavior over-weighted the Gini importance of","437","  features that appear in later stages. This issue only affected feature","438","  importances. :issue:`11176` by :user:`Gil Forsyth <gforsyth>`.","439",""],"delete":[]}],"sklearn\/ensemble\/gradient_boosting.py":[{"add":["1231","            stage_sum = sum(tree.tree_.compute_feature_importances(","1232","                normalize=False) for tree in stage) \/ len(stage)","1236","        importances \/= importances.sum()"],"delete":["1231","            stage_sum = sum(tree.feature_importances_","1232","                            for tree in stage) \/ len(stage)"]}],"doc\/modules\/ensemble.rst":[{"add":["784","    array([0.10..., 0.10..., 0.11..., ..."],"delete":["784","    array([0.11, 0.1 , 0.11, ..."]}],"sklearn\/ensemble\/tests\/test_gradient_boosting.py":[{"add":["14","from sklearn.datasets import make_classification, fetch_california_housing","454","def test_feature_importance_regression():","455","    \"\"\"Test that Gini importance is calculated correctly.","456","","457","    This test follows the example from [1]_ (pg. 373).","458","","459","    .. [1] Friedman, J., Hastie, T., & Tibshirani, R. (2001). The elements","460","       of statistical learning. New York: Springer series in statistics.","461","    \"\"\"","462","    california = fetch_california_housing()","463","    X, y = california.data, california.target","464","    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)","465","","466","    reg = GradientBoostingRegressor(loss='huber', learning_rate=0.1,","467","                                    max_leaf_nodes=6, n_estimators=100,","468","                                    random_state=0)","469","    reg.fit(X_train, y_train)","470","    sorted_idx = np.argsort(reg.feature_importances_)[::-1]","471","    sorted_features = [california.feature_names[s] for s in sorted_idx]","472","","473","    # The most important feature is the median income by far.","474","    assert sorted_features[0] == 'MedInc'","475","","476","    # The three subsequent features are the following. Their relative ordering","477","    # might change a bit depending on the randomness of the trees and the","478","    # train \/ test split.","479","    assert set(sorted_features[1:4]) == {'Longitude', 'AveOccup', 'Latitude'}","480","","481",""],"delete":["14","from sklearn.datasets import make_classification"]}]}},"7c6486f09ccd3b80350331b879e3d1ea297918b0":{"changes":{"sklearn\/multioutput.py":"MODIFY","sklearn\/tests\/test_multioutput.py":"MODIFY"},"diff":{"sklearn\/multioutput.py":[{"add":["222","    @if_delegate_has_method('estimator')"],"delete":[]}],"sklearn\/tests\/test_multioutput.py":[{"add":["64","    assert_false(hasattr(MultiOutputRegressor(Lasso), 'partial_fit'))"],"delete":["69",""]}]}},"9a9cf19c065a6e3af5912a531b735f766b5dc433":{"changes":{"sklearn\/neighbors\/binary_tree.pxi":"MODIFY"},"diff":{"sklearn\/neighbors\/binary_tree.pxi":[{"add":["160","cdef extern from \"numpy\/arrayobject.h\":","161","    void PyArray_ENABLEFLAGS(np.ndarray arr, int flags)","162","","1506","                    PyArray_ENABLEFLAGS(indices_npy[i], np.NPY_OWNDATA)","1513","                    PyArray_ENABLEFLAGS(distances_npy[i], np.NPY_OWNDATA)","1526","                    PyArray_ENABLEFLAGS(indices_npy[i], np.NPY_OWNDATA)"],"delete":["1503","                    np.PyArray_UpdateFlags(indices_npy[i], indices_npy[i].flags.num | np.NPY_OWNDATA)","1510","                    np.PyArray_UpdateFlags(distances_npy[i], distances_npy[i].flags.num | np.NPY_OWNDATA)","1523","                    np.PyArray_UpdateFlags(indices_npy[i], indices_npy[i].flags.num | np.NPY_OWNDATA)"]}]}},"b42125bec308cccf4c911db65123870b04c08158":{"changes":{"sklearn\/metrics\/_plot\/confusion_matrix.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY","sklearn\/metrics\/_plot\/tests\/test_plot_confusion_matrix.py":"MODIFY"},"diff":{"sklearn\/metrics\/_plot\/confusion_matrix.py":[{"add":["95","            thresh = (cm.max() + cm.min()) \/ 2.0"],"delete":["95","            thresh = (cm.max() - cm.min()) \/ 2."]}],"doc\/whats_new\/v0.22.rst":[{"add":["44","- |Fix| :func:`metrics.plot_confusion_matrix` now colors the label color","45","  correctly to maximize contrast with its background. :pr:`15936` by","46","  `Thomas Fan`_ and :user:`DizietAsahi`.","47",""],"delete":[]}],"sklearn\/metrics\/_plot\/tests\/test_plot_confusion_matrix.py":[{"add":["202","    # off-diagonal text is white","211","    # off-diagonal text is black","215","    # Regression test for #15920","216","    cm = np.array([[19, 34], [32, 58]])","217","    disp = ConfusionMatrixDisplay(cm, display_labels=[0, 1])","218","","219","    disp.plot(cmap=pyplot.cm.Blues)","220","    min_color = pyplot.cm.Blues(0)","221","    max_color = pyplot.cm.Blues(255)","222","    assert_allclose(disp.text_[0, 0].get_color(), max_color)","223","    assert_allclose(disp.text_[0, 1].get_color(), max_color)","224","    assert_allclose(disp.text_[1, 0].get_color(), max_color)","225","    assert_allclose(disp.text_[1, 1].get_color(), min_color)","226","","227","","228",""],"delete":["202","    # oof-diagonal text is white","211","    # oof-diagonal text is black"]}]}},"989f9c764734efc21bbff21b0202b52a1112bfc3":{"changes":{"sklearn\/utils\/estimator_checks.py":"MODIFY","sklearn\/utils\/tests\/test_estimator_checks.py":"MODIFY"},"diff":{"sklearn\/utils\/estimator_checks.py":[{"add":["678","def _apply_on_subsets(func, X):","689","    if sparse.issparse(result_full):","690","        result_full = result_full.A","691","        result_by_batch = [x.A for x in result_by_batch]","727","            result_full, result_by_batch = _apply_on_subsets("],"delete":["678","def _apply_func(func, X):","724","            result_full, result_by_batch = _apply_func("]}],"sklearn\/utils\/tests\/test_estimator_checks.py":[{"add":["163","class SparseTransformer(BaseEstimator):","164","    def fit(self, X, y=None):","165","        self.X_shape_ = check_array(X).shape","166","        return self","167","","168","    def fit_transform(self, X, y=None):","169","        return self.fit(X, y).transform(X)","170","","171","    def transform(self, X):","172","        X = check_array(X)","173","        if X.shape[1] != self.X_shape_[1]:","174","            raise ValueError('Bad number of features')","175","        return sp.csr_matrix(X)","176","","177","","252","    # non-regression test for estimators transforming to sparse data","253","    check_estimator(SparseTransformer())","254",""],"delete":[]}]}}}