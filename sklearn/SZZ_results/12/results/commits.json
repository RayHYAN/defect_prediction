{"5e193ec655d2b7634e35733e2fa0e5bef5e63d45":{"changes":{"sklearn\/decomposition\/_factor_analysis.py":"MODIFY","sklearn\/decomposition\/tests\/test_factor_analysis.py":"MODIFY","examples\/decomposition\/plot_varimax_fa.py":"ADD","doc\/modules\/decomposition.rst":"MODIFY","doc\/whats_new\/v0.24.rst":"MODIFY"},"diff":{"sklearn\/decomposition\/_factor_analysis.py":[{"add":["91","    rotation : None | 'varimax' | 'quartimax'","92","        If not None, apply the indicated rotation. Currently, varimax and","93","        quartimax are implemented. See","94","        `\"The varimax criterion for analytic rotation in factor analysis\"","95","        <https:\/\/link.springer.com\/article\/10.1007%2FBF02289233>`_","96","        H. F. Kaiser, 1958","97","","98","        .. versionadded:: 0.24","99","","153","                 iterated_power=3, rotation=None, random_state=0):","166","        self.rotation = rotation","188","","256","        if self.rotation is not None:","257","            self.components_ = self._rotate(W)","377","","378","    def _rotate(self, components, n_components=None, tol=1e-6):","379","        \"Rotate the factor analysis solution.\"","380","        # note that tol is not exposed","381","        implemented = (\"varimax\", \"quartimax\")","382","        method = self.rotation","383","        if method in implemented:","384","            return _ortho_rotation(components.T, method=method,","385","                                   tol=tol)[:self.n_components]","386","        else:","387","            raise ValueError(\"'method' must be in %s, not %s\"","388","                             % (implemented, method))","389","","390","","391","def _ortho_rotation(components, method='varimax', tol=1e-6, max_iter=100):","392","    \"\"\"Return rotated components.\"\"\"","393","    nrow, ncol = components.shape","394","    rotation_matrix = np.eye(ncol)","395","    var = 0","396","","397","    for _ in range(max_iter):","398","        comp_rot = np.dot(components, rotation_matrix)","399","        if method == \"varimax\":","400","            tmp = comp_rot * np.transpose((comp_rot ** 2).sum(axis=0) \/ nrow)","401","        elif method == \"quartimax\":","402","            tmp = 0","403","        u, s, v = np.linalg.svd(","404","            np.dot(components.T, comp_rot ** 3 - tmp))","405","        rotation_matrix = np.dot(u, v)","406","        var_new = np.sum(s)","407","        if var != 0 and var_new < var * (1 + tol):","408","            break","409","        var = var_new","410","","411","    return np.dot(components, rotation_matrix).T"],"delete":["144","                 iterated_power=3, random_state=0):"]}],"sklearn\/decomposition\/tests\/test_factor_analysis.py":[{"add":["4","from itertools import combinations","5","","10","from sklearn.utils._testing import assert_raises","16","from sklearn.decomposition._factor_analysis import _ortho_rotation","89","","90","    # test rotation","91","    n_components = 2","92","","93","    results, projections = {}, {}","94","    for method in (None, \"varimax\", 'quartimax'):","95","        fa_var = FactorAnalysis(n_components=n_components,","96","                                rotation=method)","97","        results[method] = fa_var.fit_transform(X)","98","        projections[method] = fa_var.get_covariance()","99","    for rot1, rot2 in combinations([None, 'varimax', 'quartimax'], 2):","100","        assert not np.allclose(results[rot1], results[rot2])","101","        assert np.allclose(projections[rot1], projections[rot2], atol=3)","102","","103","    assert_raises(ValueError,","104","                  FactorAnalysis(rotation='not_implemented').fit_transform, X)","105","","106","    # test against R's psych::principal with rotate=\"varimax\"","107","    # (i.e., the values below stem from rotating the components in R)","108","    # R's factor analysis returns quite different values; therefore, we only","109","    # test the rotation itself","110","    factors = np.array(","111","        [[0.89421016, -0.35854928, -0.27770122, 0.03773647],","112","         [-0.45081822, -0.89132754, 0.0932195, -0.01787973],","113","         [0.99500666, -0.02031465, 0.05426497, -0.11539407],","114","         [0.96822861, -0.06299656, 0.24411001, 0.07540887]])","115","    r_solution = np.array([[0.962, 0.052], [-0.141, 0.989],","116","                           [0.949, -0.300], [0.937, -0.251]])","117","    rotated = _ortho_rotation(factors[:, :n_components], method='varimax').T","118","    assert_array_almost_equal(np.abs(rotated), np.abs(r_solution), decimal=3)"],"delete":[]}],"examples\/decomposition\/plot_varimax_fa.py":[{"add":[],"delete":[]}],"doc\/modules\/decomposition.rst":[{"add":["624","Factor Analysis is often followed by a rotation of the factors (with the","625","parameter `rotation`), usually to improve interpretability. For example,","626","Varimax rotation maximizes the sum of the variances of the squared loadings,","627","i.e., it tends to produce sparser factors, which are influenced by only a few","628","features each (the \"simple structure\"). See e.g., the first example below.","632","    * :ref:`sphx_glr_auto_examples_decomposition_plot_varimax_fa.py`","635","","968","    * `\"The varimax criterion for analytic rotation in factor analysis\"","969","      <https:\/\/link.springer.com\/article\/10.1007%2FBF02289233>`_","970","      H. F. Kaiser, 1958"],"delete":[]}],"doc\/whats_new\/v0.24.rst":[{"add":["86","- |Enhancement| :func:`decomposition.FactorAnalysis` now supports the optional","87","  argument `rotation`, which can take the value `None`, `'varimax'` or `'quartimax'.`","88","  :pr:`11064` by :user:`Jona Sassenhagen <jona-sassenhagen>`.","89",""],"delete":[]}]}},"eb3ad2dd5d5244fa384998596a2d6820aa8b16d7":{"changes":{"sklearn\/metrics\/_plot\/confusion_matrix.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY","sklearn\/metrics\/_plot\/tests\/test_plot_confusion_matrix.py":"MODIFY"},"diff":{"sklearn\/metrics\/_plot\/confusion_matrix.py":[{"add":["197","                     cmap=cmap, ax=ax, xticks_rotation=xticks_rotation,","198","                     values_format=values_format)"],"delete":["197","                     cmap=cmap, ax=ax, xticks_rotation=xticks_rotation)"]}],"doc\/whats_new\/v0.22.rst":[{"add":["56","- |Fix| Fixed a bug in :func:`metrics.plot_confusion_matrix` to correctly","57","  pass the `values_format` parameter to the :class:`ConfusionMatrixDisplay`","58","  plot() call. :pr:`15937` by :user:`Stephen Blystone <blynotes>`.","59",""],"delete":[]}],"sklearn\/metrics\/_plot\/tests\/test_plot_confusion_matrix.py":[{"add":["247","","248","","249","@pytest.mark.parametrize(\"values_format\", ['e', 'n'])","250","def test_confusion_matrix_text_format(pyplot, data, y_pred, n_classes,","251","                                      fitted_clf, values_format):","252","    # Make sure plot text is formatted with 'values_format'.","253","    X, y = data","254","    cm = confusion_matrix(y, y_pred)","255","    disp = plot_confusion_matrix(fitted_clf, X, y,","256","                                 include_values=True,","257","                                 values_format=values_format)","258","","259","    assert disp.text_.shape == (n_classes, n_classes)","260","","261","    expected_text = np.array([format(v, values_format)","262","                              for v in cm.ravel()])","263","    text_text = np.array([","264","        t.get_text() for t in disp.text_.ravel()])","265","    assert_array_equal(expected_text, text_text)"],"delete":[]}]}},"95aa2952e1544a5a2e0f14d366bae1bfd8e9195a":{"changes":{"sklearn\/utils\/fixes.py":"MODIFY","sklearn\/mixture\/gmm.py":"MODIFY","sklearn\/naive_bayes.py":"MODIFY","sklearn\/linear_model\/logistic.py":"MODIFY","sklearn\/model_selection\/tests\/test_split.py":"MODIFY","sklearn\/decomposition\/online_lda.py":"MODIFY","sklearn\/ensemble\/tests\/test_forest.py":"MODIFY","sklearn\/mixture\/base.py":"MODIFY","sklearn\/metrics\/cluster\/supervised.py":"MODIFY","sklearn\/model_selection\/_split.py":"MODIFY","sklearn\/ensemble\/gradient_boosting.py":"MODIFY","sklearn\/mixture\/dpgmm.py":"MODIFY","sklearn\/utils\/extmath.py":"MODIFY","sklearn\/utils\/tests\/test_random.py":"MODIFY","sklearn\/linear_model\/tests\/test_sag.py":"MODIFY"},"diff":{"sklearn\/utils\/fixes.py":[{"add":["36","","143","    from scipy.sparse.linalg import lsqr as sparse_lsqr  # noqa","144","","145","","146","try:  # SciPy >= 0.19","147","    from scipy.special import comb, logsumexp","148","except ImportError:","149","    from scipy.misc import comb, logsumexp  # noqa","153","    \"\"\"Workaround for Python 2 limitations of pickling instance methods\"\"\""],"delete":["13","import sys","14","import functools","144","    from scipy.sparse.linalg import lsqr as sparse_lsqr","148","    \"\"\"Helper to workaround Python 2 limitations of pickling instance methods\"\"\""]}],"sklearn\/mixture\/gmm.py":[{"add":["24","from ..utils.fixes import logsumexp"],"delete":["21","from scipy.misc import logsumexp"]}],"sklearn\/naive_bayes.py":[{"add":["29","from .utils.fixes import logsumexp"],"delete":["21","from scipy.misc import logsumexp"]}],"sklearn\/linear_model\/logistic.py":[{"add":["28","from ..utils.fixes import logsumexp"],"delete":["17","from scipy.misc import logsumexp"]}],"sklearn\/model_selection\/tests\/test_split.py":[{"add":["58","from sklearn.utils.fixes import comb","59",""],"delete":["7","from scipy.misc import comb"]}],"sklearn\/decomposition\/online_lda.py":[{"add":["21","from ..utils.fixes import logsumexp"],"delete":["15","from scipy.misc import logsumexp"]}],"sklearn\/ensemble\/tests\/test_forest.py":[{"add":["43","from sklearn.utils.fixes import comb"],"delete":["16","from scipy.misc import comb"]}],"sklearn\/mixture\/base.py":[{"add":["20","from ..utils.fixes import logsumexp"],"delete":["13","from scipy.misc import logsumexp"]}],"sklearn\/metrics\/cluster\/supervised.py":[{"add":["24","from ...utils.fixes import comb"],"delete":["20","from scipy.misc import comb"]}],"sklearn\/model_selection\/_split.py":[{"add":["30","from ..utils.fixes import signature, comb"],"delete":["24","from scipy.misc import comb","31","from ..utils.fixes import signature"]}],"sklearn\/ensemble\/gradient_boosting.py":[{"add":["57","from ..utils.fixes import logsumexp"],"delete":["29","from ..base import BaseEstimator","42","from scipy.misc import logsumexp"]}],"sklearn\/mixture\/dpgmm.py":[{"add":["27","from ..utils.fixes import logsumexp"],"delete":["23","from scipy.misc import logsumexp"]}],"sklearn\/utils\/extmath.py":[{"add":["22","from .fixes import logsumexp as scipy_logsumexp"],"delete":["19","from scipy.misc import logsumexp as scipy_logsumexp"]}],"sklearn\/utils\/tests\/test_random.py":[{"add":["7","from sklearn.utils.fixes import comb","90","        n_expected = comb(n_population, n_samples, exact=True)"],"delete":["4","from scipy.misc import comb as combinations","90","        n_expected = combinations(n_population, n_samples, exact=True)"]}],"sklearn\/linear_model\/tests\/test_sag.py":[{"add":["15","from sklearn.utils.fixes import logsumexp"],"delete":["8","from scipy.misc import logsumexp"]}]}},"7b543513b80f0e1f6af98b1b52b70f452728b5d7":{"changes":{"doc\/modules\/pipeline.rst":"MODIFY"},"diff":{"doc\/modules\/pipeline.rst":[{"add":["16","and classification. :class:`Pipeline` serves multiple purposes here:"],"delete":["16","and classification. :class:`Pipeline` serves two purposes here:"]}]}},"20cb37e8f6e1eb6859239bac6307fcc213ddd52e":{"changes":{"sklearn\/kernel_ridge.py":"MODIFY","sklearn\/base.py":"MODIFY"},"diff":{"sklearn\/kernel_ridge.py":[{"add":["50","        should return a floating point number. Set to \"precomputed\" in","51","        order to pass a precomputed kernel matrix to the estimator","52","        methods instead of samples.","77","        Training data, which is also required for prediction. If","78","        kernel == \"precomputed\" this is instead the precomputed","79","        training matrix, shape = [n_samples, n_samples].","136","            Training data. If kernel == \"precomputed\" this is instead","137","            a precomputed kernel matrix, shape = [n_samples,","138","            n_samples].","181","            Samples. If kernel == \"precomputed\" this is instead a","182","            precomputed kernel matrix, shape = [n_samples,","183","            n_samples_fitted], where n_samples_fitted is the number of","184","            samples used in the fitting for this estimator."],"delete":["50","        should return a floating point number.","75","        Training data, which is also required for prediction","132","            Training data","175","            Samples."]}],"sklearn\/base.py":[{"add":["356","            Test samples. For some estimators this may be a","357","            precomputed kernel matrix instead, shape = (n_samples,","358","            n_samples_fitted], where n_samples_fitted is the number of","359","            samples used in the fitting for the estimator."],"delete":["356","            Test samples."]}]}},"6986e9b5db5fa24b268cfc7d8aab32b8962f4510":{"changes":{"examples\/tree\/plot_tree_regression_multioutput.py":"MODIFY","examples\/linear_model\/plot_sgd_iris.py":"MODIFY","examples\/feature_selection\/plot_feature_selection.py":"MODIFY","examples\/linear_model\/plot_bayesian_ridge.py":"MODIFY","examples\/neighbors\/plot_classification.py":"MODIFY","examples\/feature_selection\/plot_f_test_vs_mi.py":"MODIFY","examples\/neighbors\/plot_lof.py":"MODIFY","examples\/semi_supervised\/plot_label_propagation_versus_svm_iris.py":"MODIFY","examples\/linear_model\/plot_sgd_weighted_samples.py":"MODIFY","examples\/feature_selection\/plot_permutation_test_for_classification.py":"MODIFY","examples\/linear_model\/plot_logistic_multinomial.py":"MODIFY","examples\/neural_networks\/plot_mlp_alpha.py":"MODIFY","examples\/model_selection\/plot_underfitting_overfitting.py":"MODIFY","examples\/mixture\/plot_concentration_prior.py":"MODIFY","examples\/neighbors\/plot_nearest_centroid.py":"MODIFY","examples\/svm\/plot_weighted_samples.py":"MODIFY","examples\/linear_model\/plot_sgd_separating_hyperplane.py":"MODIFY","examples\/tree\/plot_tree_regression.py":"MODIFY"},"diff":{"examples\/tree\/plot_tree_regression_multioutput.py":[{"add":["45","s = 25","46","plt.scatter(y[:, 0], y[:, 1], c=\"navy\", s=s,","47","            edgecolor=\"black\", label=\"data\")","48","plt.scatter(y_1[:, 0], y_1[:, 1], c=\"cornflowerblue\", s=s,","49","            edgecolor=\"black\", label=\"max_depth=2\")","50","plt.scatter(y_2[:, 0], y_2[:, 1], c=\"red\", s=s,","51","            edgecolor=\"black\", label=\"max_depth=5\")","52","plt.scatter(y_3[:, 0], y_3[:, 1], c=\"orange\", s=s,","53","            edgecolor=\"black\", label=\"max_depth=8\")","59","plt.legend(loc=\"best\")"],"delete":["45","plt.scatter(y[:, 0], y[:, 1], c=\"navy\", s=s, label=\"data\")","46","plt.scatter(y_1[:, 0], y_1[:, 1], c=\"cornflowerblue\", s=s, label=\"max_depth=2\")","47","plt.scatter(y_2[:, 0], y_2[:, 1], c=\"c\", s=s, label=\"max_depth=5\")","48","plt.scatter(y_3[:, 0], y_3[:, 1], c=\"orange\", s=s, label=\"max_depth=8\")","54","plt.legend()"]}],"examples\/linear_model\/plot_sgd_iris.py":[{"add":["19","","20","# we only take the first two features. We could","21","# avoid this ugly slicing by using a two-dim dataset","22","X = iris.data[:, :2]","60","                cmap=plt.cm.Paired, edgecolor='black', s=20)"],"delete":["19","X = iris.data[:, :2]  # we only take the first two features. We could","20","                      # avoid this ugly slicing by using a two-dim dataset","58","                cmap=plt.cm.Paired)"]}],"examples\/feature_selection\/plot_feature_selection.py":[{"add":["56","        label=r'Univariate score ($-Log(p_{value})$)', color='darkorange',","57","        edgecolor='black')","68","        color='navy', edgecolor='black')","77","        width=.2, label='SVM weights after selection', color='c',","78","        edgecolor='black')"],"delete":["56","        label=r'Univariate score ($-Log(p_{value})$)', color='darkorange')","67","        color='navy')","76","        width=.2, label='SVM weights after selection', color='c')"]}],"examples\/linear_model\/plot_bayesian_ridge.py":[{"add":["74","plt.hist(clf.coef_, bins=n_features, color='gold', log=True,","75","         edgecolor='black')"],"delete":["74","plt.hist(clf.coef_, bins=n_features, color='gold', log=True)"]}],"examples\/neighbors\/plot_classification.py":[{"add":["19","","20","# we only take the first two features. We could avoid this ugly","21","# slicing by using a two-dim dataset","22","X = iris.data[:, :2]","50","    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold,","51","                edgecolor='k', s=20)"],"delete":["19","X = iris.data[:, :2]  # we only take the first two features. We could","20","                      # avoid this ugly slicing by using a two-dim dataset","48","    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold)"]}],"examples\/feature_selection\/plot_f_test_vs_mi.py":[{"add":["11","y = x_1 + sin(6 * pi * x_2) + 0.1 * N(0, 1), that is the third features is","12","completely irrelevant.","42","    plt.scatter(X[:, i], y, edgecolor='black', s=20)"],"delete":["11","y = x_1 + sin(6 * pi * x_2) + 0.1 * N(0, 1), that is the third features is completely irrelevant.","41","    plt.scatter(X[:, i], y)","48",""]}],"examples\/neighbors\/plot_lof.py":[{"add":["19","print(__doc__)","46","a = plt.scatter(X[:200, 0], X[:200, 1], c='white',","47","                edgecolor='k', s=20)","48","b = plt.scatter(X[200:, 0], X[200:, 1], c='red',","49","                edgecolor='k', s=20)"],"delete":["23","print(__doc__)","46","a = plt.scatter(X[:200, 0], X[:200, 1], c='white')","47","b = plt.scatter(X[200:, 0], X[200:, 1], c='red')"]}],"examples\/semi_supervised\/plot_label_propagation_versus_svm_iris.py":[{"add":["73","    plt.scatter(X[:, 0], X[:, 1], c=colors, edgecolors='black')","77","plt.suptitle(\"Unlabeled points are colored white\", y=0.1)"],"delete":["73","    plt.scatter(X[:, 0], X[:, 1], c=colors, cmap=plt.cm.Paired)","77","plt.text(.90, 0, \"Unlabeled points are colored white\")"]}],"examples\/linear_model\/plot_sgd_weighted_samples.py":[{"add":["26","            cmap=plt.cm.bone, edgecolor='black')","28","# fit the unweighted model","35","# fit the weighted model"],"delete":["26","            cmap=plt.cm.bone)","28","## fit the unweighted model","35","## fit the weighted model"]}],"examples\/feature_selection\/plot_permutation_test_for_classification.py":[{"add":["51","plt.hist(permutation_scores, 20, label='Permutation scores',","52","         edgecolor='black')","55","# plt.vlines(score, ylim[0], ylim[1], linestyle='--',","58","# plt.vlines(1.0 \/ n_classes, ylim[0], ylim[1], linestyle='--',"],"delete":["51","plt.hist(permutation_scores, 20, label='Permutation scores')","54","#plt.vlines(score, ylim[0], ylim[1], linestyle='--',","57","#plt.vlines(1.0 \/ n_classes, ylim[0], ylim[1], linestyle='--',"]}],"examples\/linear_model\/plot_logistic_multinomial.py":[{"add":["52","        plt.scatter(X[idx, 0], X[idx, 1], c=color, cmap=plt.cm.Paired,","53","                    edgecolor='black', s=20)"],"delete":["52","        plt.scatter(X[idx, 0], X[idx, 1], c=color, cmap=plt.cm.Paired)"]}],"examples\/neural_networks\/plot_mlp_alpha.py":[{"add":["97","        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,","98","                   edgecolors='black', s=25)","101","                   alpha=0.6, edgecolors='black', s=25)"],"delete":["97","        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)","100","                   alpha=0.6)"]}],"examples\/model_selection\/plot_underfitting_overfitting.py":[{"add":["31","","32","def true_fun(X):","33","    return np.cos(1.5 * np.pi * X)","34","","62","    plt.scatter(X, y, edgecolor='b', s=20, label=\"Samples\")"],"delete":["36","true_fun = lambda X: np.cos(1.5 * np.pi * X)","59","    plt.scatter(X, y, label=\"Samples\")"]}],"examples\/mixture\/plot_concentration_prior.py":[{"add":["52","                                  180 + angle, edgecolor='black')","73","                align='center', edgecolor='black')"],"delete":["52","                                  180 + angle)","73","                align='center')"]}],"examples\/neighbors\/plot_nearest_centroid.py":[{"add":["20","# we only take the first two features. We could avoid this ugly","21","# slicing by using a two-dim dataset","22","X = iris.data[:, :2]","51","    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold,","52","                edgecolor='b', s=20)"],"delete":["20","X = iris.data[:, :2]  # we only take the first two features. We could","21","                      # avoid this ugly slicing by using a two-dim dataset","50","    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold)"]}],"examples\/svm\/plot_weighted_samples.py":[{"add":["31","                 cmap=plt.cm.bone, edgecolors='black')"],"delete":["31","                 cmap=plt.cm.bone)"]}],"examples\/linear_model\/plot_sgd_separating_hyperplane.py":[{"add":["38","plt.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.Paired,","39","            edgecolor='black', s=20)"],"delete":["38","plt.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.Paired)"]}],"examples\/tree\/plot_tree_regression.py":[{"add":["41","plt.scatter(X, y, s=20, edgecolor=\"black\",","42","            c=\"darkorange\", label=\"data\")","43","plt.plot(X_test, y_1, color=\"cornflowerblue\",","44","         label=\"max_depth=2\", linewidth=2)"],"delete":["41","plt.scatter(X, y, c=\"darkorange\", label=\"data\")","42","plt.plot(X_test, y_1, color=\"cornflowerblue\", label=\"max_depth=2\", linewidth=2)"]}]}},"a18eea48da429accded76a0e8cef7ac1a2906fb5":{"changes":{"sklearn\/pipeline.py":"MODIFY"},"diff":{"sklearn\/pipeline.py":[{"add":["646","    >>> union = FeatureUnion([(\"pca\", PCA(n_components=1)),","650","    array([[ 1.5       ,  3.0...,  0.8...],","651","           [-1.5       ,  5.7..., -0.4...]])"],"delete":["646","    >>> union = FeatureUnion([(\"pca\", PCA(n_components=2)),","650","    array([[ 1.5       ,  0.        ,  3.0...,  0.8...],","651","           [-1.5       ,  0.        ,  5.7..., -0.4...]])"]}]}},"623ee42260eb11a695407d60439e18056c22559e":{"changes":{"doc\/modules\/grid_search.rst":"MODIFY"},"diff":{"doc\/modules\/grid_search.rst":[{"add":["16","best :ref:`cross validation <cross_validation>` score."],"delete":["16","best :ref:`cross_validation` score."]}]}},"689f412391582640025938c068d9a6885bc15a26":{"changes":{"sklearn\/ensemble\/voting_classifier.py":"MODIFY","sklearn\/ensemble\/tests\/test_voting_classifier.py":"MODIFY"},"diff":{"sklearn\/ensemble\/voting_classifier.py":[{"add":["210","                axis=1, arr=predictions)"],"delete":["210","                axis=1, arr=predictions.astype('int'))"]}],"sklearn\/ensemble\/tests\/test_voting_classifier.py":[{"add":[],"delete":["25","# A custom classifier based on SVC to return 'float' type class labels","26","class FaultySVC(SVC):","27","    def predict(self, X):","28","        return super(FaultySVC, self).predict(X).astype(float)","29","","30","","372","","373","","374","def test_predict_for_hard_voting():","375","    # Test voting classifier with non-integer (float) prediction","376","    clf1 = FaultySVC(random_state=123)","377","    clf2 = GaussianNB()","378","    clf3 = SVC(probability=True, random_state=123)","379","    eclf1 = VotingClassifier(estimators=[","380","        ('fsvc', clf1), ('gnb', clf2), ('svc', clf3)], weights=[1, 2, 3],","381","        voting='hard')","382","","383","    eclf1.fit(X, y)","384","    eclf1.predict(X)"]}]}},"ceea86765d889ea40d771cbb85333afdd63fd9fe":{"changes":{"sklearn\/metrics\/cluster\/setup.py":"MODIFY"},"diff":{"sklearn\/metrics\/cluster\/setup.py":[{"add":["7","    config = Configuration(\"metrics.cluster\", parent_package, top_path)"],"delete":["7","    config = Configuration(\"metrics\/cluster\", parent_package, top_path)"]}]}},"b4984e27dbc15305e0f8644ef9c49d5c457a33c1":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/model_selection\/tests\/test_search.py":"MODIFY","sklearn\/model_selection\/_search.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["617","- Invalid input for :class:`model_selection.ParameterGrid` now raises TypeError.","618","  :issue:`10928` by :user:`Solutus Immensus <solutusimmensus>`","619",""],"delete":[]}],"sklearn\/model_selection\/tests\/test_search.py":[{"add":["14","import pytest","129","@pytest.mark.parametrize(","130","    \"input, error_type, error_message\",","131","    [(0, TypeError, 'Parameter grid is not a dict or a list (0)'),","132","     ([{'foo': [0]}, 0], TypeError, 'Parameter grid is not a dict (0)'),","133","     ({'foo': 0}, TypeError, \"Parameter grid value is not iterable \"","134","      \"(key='foo', value=0)\")]","135",")","136","def test_validate_parameter_grid_input(input, error_type, error_message):","137","    with pytest.raises(error_type, message=error_message):","138","        ParameterGrid(input)","139","","141",""],"delete":[]}],"sklearn\/model_selection\/_search.py":[{"add":["15","from collections import Mapping, namedtuple, defaultdict, Sequence, Iterable","92","        if not isinstance(param_grid, (Mapping, Iterable)):","93","            raise TypeError('Parameter grid is not a dict or '","94","                            'a list ({!r})'.format(param_grid))","95","","100","","101","        # check if all entries are dictionaries of lists","102","        for grid in param_grid:","103","            if not isinstance(grid, dict):","104","                raise TypeError('Parameter grid is not a '","105","                                'dict ({!r})'.format(grid))","106","            for key in grid:","107","                if not isinstance(grid[key], Iterable):","108","                    raise TypeError('Parameter grid value is not iterable '","109","                                    '(key={!r}, value={!r})'","110","                                    .format(key, grid[key]))","111",""],"delete":["15","from collections import Mapping, namedtuple, defaultdict, Sequence"]}]}},"0bdd8bfb2bec61d9f636adb61c682ce2356b1b3c":{"changes":{"sklearn\/datasets\/base.py":"MODIFY"},"diff":{"sklearn\/datasets\/base.py":[{"add":["501","    This is a copy of the test set of the UCI ML hand-written digits datasets","502","    http:\/\/archive.ics.uci.edu\/ml\/datasets\/Optical+Recognition+of+Handwritten+Digits","503",""],"delete":[]}]}},"a4f8e3d2a266fe4a253b449214806562ab83dda5":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/preprocessing\/tests\/test_data.py":"MODIFY","sklearn\/preprocessing\/data.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["515","- Fix ``fit`` and ``partial_fit`` in :class:`preprocessing.StandardScaler` in","516","  the rare case when `with_mean=False` and `with_std=False` which was crashing","517","  by calling ``fit`` more than once and giving inconsistent results for","518","  ``mean_`` whether the input was a sparse or a dense matrix. ``mean_`` will be","519","  set to ``None`` with both sparse and dense inputs. ``n_samples_seen_`` will","520","  be also reported for both input types.","521","  :issue:`11235` by :user:`Guillaume Lemaitre <glemaitre>`.","522",""],"delete":[]}],"sklearn\/preprocessing\/tests\/test_data.py":[{"add":["9","import itertools","63","from sklearn.base import clone","705","def _check_identity_scalers_attributes(scaler_1, scaler_2):","706","    assert scaler_1.mean_ is scaler_2.mean_ is None","707","    assert scaler_1.var_ is scaler_2.var_ is None","708","    assert scaler_1.scale_ is scaler_2.scale_ is None","709","    assert scaler_1.n_samples_seen_ == scaler_2.n_samples_seen_","710","","711","","712","def test_scaler_return_identity():","713","    # test that the scaler return identity when with_mean and with_std are","714","    # False","715","    X_dense = np.array([[0, 1, 3],","716","                        [5, 6, 0],","717","                        [8, 0, 10]],","718","                       dtype=np.float64)","719","    X_csr = sparse.csr_matrix(X_dense)","720","    X_csc = X_csr.tocsc()","721","","722","    transformer_dense = StandardScaler(with_mean=False, with_std=False)","723","    X_trans_dense = transformer_dense.fit_transform(X_dense)","724","","725","    transformer_csr = clone(transformer_dense)","726","    X_trans_csr = transformer_csr.fit_transform(X_csr)","727","","728","    transformer_csc = clone(transformer_dense)","729","    X_trans_csc = transformer_csc.fit_transform(X_csc)","730","","731","    assert_allclose(X_trans_csr.toarray(), X_csr.toarray())","732","    assert_allclose(X_trans_csc.toarray(), X_csc.toarray())","733","    assert_allclose(X_trans_dense, X_dense)","734","","735","    for trans_1, trans_2 in itertools.combinations([transformer_dense,","736","                                                    transformer_csr,","737","                                                    transformer_csc],","738","                                                   2):","739","        _check_identity_scalers_attributes(trans_1, trans_2)","740","","741","    transformer_dense.partial_fit(X_dense)","742","    transformer_csr.partial_fit(X_csr)","743","    transformer_csc.partial_fit(X_csc)","744","","745","    for trans_1, trans_2 in itertools.combinations([transformer_dense,","746","                                                    transformer_csr,","747","                                                    transformer_csc],","748","                                                   2):","749","        _check_identity_scalers_attributes(trans_1, trans_2)","750","","751","    transformer_dense.fit(X_dense)","752","    transformer_csr.fit(X_csr)","753","    transformer_csc.fit(X_csc)","754","","755","    for trans_1, trans_2 in itertools.combinations([transformer_dense,","756","                                                    transformer_csr,","757","                                                    transformer_csc],","758","                                                   2):","759","        _check_identity_scalers_attributes(trans_1, trans_2)","760","","761",""],"delete":[]}],"sklearn\/preprocessing\/data.py":[{"add":["654","                if not hasattr(self, 'n_samples_seen_'):","655","                    self.n_samples_seen_ = X.shape[0]","656","                else:","657","                    self.n_samples_seen_ += X.shape[0]","668","            if not self.with_mean and not self.with_std:","669","                self.mean_ = None","670","                self.var_ = None","671","                self.n_samples_seen_ += X.shape[0]","672","            else:","673","                self.mean_, self.var_, self.n_samples_seen_ = \\","674","                    _incremental_mean_and_var(X, self.mean_, self.var_,","675","                                              self.n_samples_seen_)"],"delete":["664","            self.mean_, self.var_, self.n_samples_seen_ = \\","665","                _incremental_mean_and_var(X, self.mean_, self.var_,","666","                                          self.n_samples_seen_)"]}]}},"399f1b27615aa0f4b0901e7164fe043c7f5ecf5b":{"changes":{"sklearn\/datasets\/base.py":"MODIFY","doc\/modules\/clustering.rst":"MODIFY","doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/datasets\/descr\/iris.rst":"MODIFY","sklearn\/covariance\/tests\/test_graph_lasso.py":"MODIFY","sklearn\/covariance\/tests\/test_graphical_lasso.py":"MODIFY","doc\/modules\/model_evaluation.rst":"MODIFY","sklearn\/datasets\/data\/iris.csv":"MODIFY"},"diff":{"sklearn\/datasets\/base.py":[{"add":["362","    Notes","363","    -----","364","        .. versionchanged:: 0.20","365","            Fixed two wrong data points according to Fisher's paper.","366","            The new version is the same as in R, but not as in the UCI","367","            Machine Learning Repository.","368","","710","    Notes","711","    -----"],"delete":[]}],"doc\/modules\/clustering.rst":[{"add":["1569","  561.62...","1638","  0.6619..."],"delete":["1569","  560.39...","1638","  0.6623..."]}],"doc\/whats_new\/v0.20.rst":[{"add":["480","- Fixed a bug in :func:`dataset.load_iris` which had two wrong data points.","481","  :issue:`11082` by :user:`Sadhana Srinivasan <rotuna>`","482","  and :user:`Hanmin Qin <qinhanmin2014>`.","483",""],"delete":[]}],"sklearn\/datasets\/descr\/iris.rst":[{"add":["34","The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken","35","from Fisher's paper. Note that it's the same as in R, but not as in the UCI","36","Machine Learning Repository, which has two wrong data points."],"delete":["34","This is a copy of UCI ML iris datasets.","35","http:\/\/archive.ics.uci.edu\/ml\/datasets\/Iris","36","","37","The famous Iris database, first used by Sir R.A. Fisher"]}],"sklearn\/covariance\/tests\/test_graph_lasso.py":[{"add":["69","    # (need to set penalize.diagonal to FALSE)","71","        [0.68112222, 0.0000000, 0.265820, 0.02464314],","72","        [0.00000000, 0.1887129, 0.000000, 0.00000000],","73","        [0.26582000, 0.0000000, 3.095503, 0.28697200],","74","        [0.02464314, 0.0000000, 0.286972, 0.57713289]","77","        [1.5190747, 0.000000, -0.1304475, 0.0000000],","78","        [0.0000000, 5.299055, 0.0000000, 0.0000000],","79","        [-0.1304475, 0.000000, 0.3498624, -0.1683946],","80","        [0.0000000, 0.000000, -0.1683946, 1.8164353]"],"delete":["69","    # The iris datasets in R and scikit-learn do not match in a few places,","70","    # these values are for the scikit-learn version.","72","        [0.68112222, 0.0, 0.2651911, 0.02467558],","73","        [0.00, 0.1867507, 0.0, 0.00],","74","        [0.26519111, 0.0, 3.0924249, 0.28774489],","75","        [0.02467558, 0.0, 0.2877449, 0.57853156]","78","        [1.5188780, 0.0, -0.1302515, 0.0],","79","        [0.0, 5.354733, 0.0, 0.0],","80","        [-0.1302515, 0.0, 0.3502322, -0.1686399],","81","        [0.0, 0.0, -0.1686399, 1.8123908]"]}],"sklearn\/covariance\/tests\/test_graphical_lasso.py":[{"add":["67","    # (need to set penalize.diagonal to FALSE)","69","        [0.68112222, 0.0000000, 0.265820, 0.02464314],","70","        [0.00000000, 0.1887129, 0.000000, 0.00000000],","71","        [0.26582000, 0.0000000, 3.095503, 0.28697200],","72","        [0.02464314, 0.0000000, 0.286972, 0.57713289]","73","        ])","75","        [1.5190747, 0.000000, -0.1304475, 0.0000000],","76","        [0.0000000, 5.299055, 0.0000000, 0.0000000],","77","        [-0.1304475, 0.000000, 0.3498624, -0.1683946],","78","        [0.0000000, 0.000000, -0.1683946, 1.8164353]","79","        ])"],"delete":["67","    # The iris datasets in R and scikit-learn do not match in a few places,","68","    # these values are for the scikit-learn version.","70","        [0.68112222, 0.0, 0.2651911, 0.02467558],","71","        [0.00, 0.1867507, 0.0, 0.00],","72","        [0.26519111, 0.0, 3.0924249, 0.28774489],","73","        [0.02467558, 0.0, 0.2877449, 0.57853156]","74","    ])","76","        [1.5188780, 0.0, -0.1302515, 0.0],","77","        [0.0, 5.354733, 0.0, 0.0],","78","        [-0.1302515, 0.0, 0.3502322, -0.1686399],","79","        [0.0, 0.0, -0.1686399, 1.8123908]","80","    ])"]}],"doc\/modules\/model_evaluation.rst":[{"add":["102","    array([-0.10..., -0.16..., -0.07...])"],"delete":["102","    array([-0.09..., -0.16..., -0.07...])"]}],"sklearn\/datasets\/data\/iris.csv":[{"add":["35","4.9,3.1,1.5,0.2,0","38","4.9,3.6,1.4,0.1,0"],"delete":["35","4.9,3.1,1.5,0.1,0","38","4.9,3.1,1.5,0.1,0"]}]}},"41651a16199caa39189bef2237584b45e1e85e9d":{"changes":{"sklearn\/compose\/tests\/test_column_transformer.py":"MODIFY","sklearn\/compose\/_column_transformer.py":"MODIFY"},"diff":{"sklearn\/compose\/tests\/test_column_transformer.py":[{"add":["345","                                 n_jobs=3, remainder='drop')","351","    assert_equal(ct.remainder, 'drop')"],"delete":["345","                                 n_jobs=3)"]}],"sklearn\/compose\/_column_transformer.py":[{"add":["611","    remainder : {'passthrough', 'drop'}, default 'passthrough'","612","        By default, all remaining columns that were not specified in","613","        `transformers` will be automatically passed through (default of","614","        ``'passthrough'``). This subset of columns is concatenated with the","615","        output of the transformers.","616","        By using ``remainder='drop'``, only the specified columns in","617","        `transformers` are transformed and combined in the output, and the","618","        non-specified columns are dropped.","619","","652","    remainder = kwargs.pop('remainder', 'passthrough')","657","    return ColumnTransformer(transformer_list, n_jobs=n_jobs,","658","                             remainder=remainder)"],"delete":["647","    return ColumnTransformer(transformer_list, n_jobs=n_jobs)"]}]}},"32b88d8e2dff3b2ebd93c8cdc7511e72a33d71bf":{"changes":{"sklearn\/linear_model\/tests\/test_least_angle.py":"MODIFY","sklearn\/linear_model\/least_angle.py":"MODIFY"},"diff":{"sklearn\/linear_model\/tests\/test_least_angle.py":[{"add":["0","import warnings","1","","9","from sklearn.utils.testing import assert_false","407","    assert_false(hasattr(lars_cv, 'n_nonzero_coefs'))","408","","409","","410","def test_lars_cv_max_iter():","411","    with warnings.catch_warnings(record=True) as w:","412","        X = diabetes.data","413","        y = diabetes.target","414","        rng = np.random.RandomState(42)","415","        x = rng.randn(len(y))","416","        X = np.c_[X, x, x]  # add correlated features","417","        lars_cv = linear_model.LassoLarsCV(max_iter=5)","418","        lars_cv.fit(X, y)","419","    assert_true(len(w) == 0)"],"delete":[]}],"sklearn\/linear_model\/least_angle.py":[{"add":["301","                              'the smallest cholesky pivot element being %.3e.'","302","                              ' Reduce max_iter or increase eps parameters.'","305","","373","        g1 = arrayfuncs.min_pos((C - Cov) \/ (AA - corr_eq_dir + tiny32))","377","            g2 = arrayfuncs.min_pos((C + Cov) \/ (AA + corr_eq_dir + tiny32))","611","    def _fit(self, X, y, max_iter, alpha, fit_path, Xy=None):","612","        \"\"\"Auxiliary method to fit the model using X, y as training data\"\"\"","638","        if fit_path:","674","","678","    def fit(self, X, y, Xy=None):","679","        \"\"\"Fit the model using X, y as training data.","680","","681","        Parameters","682","        ----------","683","        X : array-like, shape (n_samples, n_features)","684","            Training data.","685","","686","        y : array-like, shape (n_samples,) or (n_samples, n_targets)","687","            Target values.","688","","689","        Xy : array-like, shape (n_samples,) or (n_samples, n_targets), \\","690","                optional","691","            Xy = np.dot(X.T, y) that can be precomputed. It is useful","692","            only when the Gram matrix is precomputed.","693","","694","        Returns","695","        -------","696","        self : object","697","            returns an instance of self.","698","        \"\"\"","699","        X, y = check_X_y(X, y, y_numeric=True, multi_output=True)","700","","701","        alpha = getattr(self, 'alpha', 0.)","702","        if hasattr(self, 'n_nonzero_coefs'):","703","            alpha = 0.  # n_nonzero_coefs parametrization takes priority","704","            max_iter = self.n_nonzero_coefs","705","        else:","706","            max_iter = self.max_iter","707","","708","        self._fit(X, y, max_iter=max_iter, alpha=alpha, fit_path=self.fit_path,","709","                  Xy=Xy)","710","","711","        return self","712","","1157","        self._fit(X, y, max_iter=self.max_iter, alpha=best_alpha,","1158","                  Xy=None, fit_path=True)","1162","    @deprecated(\"Attribute alpha is deprecated in 0.19 and \"","1163","                \"will be removed in 0.21. See 'alpha_' instead\")","1298","    def __init__(self, fit_intercept=True, verbose=False, max_iter=500,","1299","                 normalize=True, precompute='auto', cv=None,","1300","                 max_n_alphas=1000, n_jobs=1, eps=np.finfo(np.float).eps,","1301","                 copy_X=True, positive=False):","1302","        self.fit_intercept = fit_intercept","1303","        self.verbose = verbose","1304","        self.max_iter = max_iter","1305","        self.normalize = normalize","1306","        self.precompute = precompute","1307","        self.cv = cv","1308","        self.max_n_alphas = max_n_alphas","1309","        self.n_jobs = n_jobs","1310","        self.eps = eps","1311","        self.copy_X = copy_X","1312","        self.positive = positive","1313","        # XXX : we don't use super(LarsCV, self).__init__","1314","        # to avoid setting n_nonzero_coefs","1315",""],"delete":["197","    tiny = np.finfo(np.float).tiny  # to avoid division by 0 warning","302","                              'the smallest cholesky pivot element being %.3e'","372","        g1 = arrayfuncs.min_pos((C - Cov) \/ (AA - corr_eq_dir + tiny))","376","            g2 = arrayfuncs.min_pos((C + Cov) \/ (AA + corr_eq_dir + tiny))","610","    def fit(self, X, y, Xy=None):","611","        \"\"\"Fit the model using X, y as training data.","612","","613","        parameters","614","        ----------","615","        X : array-like, shape (n_samples, n_features)","616","            Training data.","617","","618","        y : array-like, shape (n_samples,) or (n_samples, n_targets)","619","            Target values.","620","","621","        Xy : array-like, shape (n_samples,) or (n_samples, n_targets), \\","622","                optional","623","            Xy = np.dot(X.T, y) that can be precomputed. It is useful","624","            only when the Gram matrix is precomputed.","625","","626","        returns","627","        -------","628","        self : object","629","            returns an instance of self.","630","        \"\"\"","631","        X, y = check_X_y(X, y, y_numeric=True, multi_output=True)","644","        alpha = getattr(self, 'alpha', 0.)","645","        if hasattr(self, 'n_nonzero_coefs'):","646","            alpha = 0.  # n_nonzero_coefs parametrization takes priority","647","            max_iter = self.n_nonzero_coefs","648","        else:","649","            max_iter = self.max_iter","650","","664","        if self.fit_path:","1147","        Lars.fit(self, X, y)"]}]}},"a6028fc059982f04cb12256016702387e11964f5":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/preprocessing\/tests\/test_data.py":"MODIFY","sklearn\/preprocessing\/data.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["498","- Fix bug in :class:`preprocessing.OneHotEncoder` which discarded the ``dtype``","499","  when returning a sparse matrix output. :issue:`11042` by :user:`Daniel","500","  Morales <DanielMorales9>`.","501",""],"delete":[]}],"sklearn\/preprocessing\/tests\/test_data.py":[{"add":["1911","def _check_transform_selected(X, X_expected, dtype, sel):","1913","        Xtr = _transform_selected(M, Binarizer().transform, dtype, sel)","1917","@pytest.mark.parametrize(\"output_dtype\", [np.int32, np.float32, np.float64])","1918","@pytest.mark.parametrize(\"input_dtype\", [np.int32, np.float32, np.float64])","1919","def test_transform_selected(output_dtype, input_dtype):","1920","    X = np.asarray([[3, 2, 1], [0, 1, 1]], dtype=input_dtype)","1922","    X_expected = np.asarray([[1, 2, 1], [0, 1, 1]], dtype=output_dtype)","1923","    _check_transform_selected(X, X_expected, output_dtype, [0])","1924","    _check_transform_selected(X, X_expected, output_dtype,","1925","                              [True, False, False])","1927","    X_expected = np.asarray([[1, 1, 1], [0, 1, 1]], dtype=output_dtype)","1928","    _check_transform_selected(X, X_expected, output_dtype, [0, 1, 2])","1929","    _check_transform_selected(X, X_expected, output_dtype, [True, True, True])","1930","    _check_transform_selected(X, X_expected, output_dtype, \"all\")","1932","    _check_transform_selected(X, X, output_dtype, [])","1933","    _check_transform_selected(X, X, output_dtype, [False, False, False])","1936","@pytest.mark.parametrize(\"output_dtype\", [np.int32, np.float32, np.float64])","1937","@pytest.mark.parametrize(\"input_dtype\", [np.int32, np.float32, np.float64])","1938","def test_transform_selected_copy_arg(output_dtype, input_dtype):","1944","    original_X = np.asarray([[1, 2], [3, 4]], dtype=input_dtype)","1945","    expected_Xtr = np.asarray([[2, 2], [3, 4]], dtype=output_dtype)","1948","    Xtr = _transform_selected(X, _mutating_transformer, output_dtype,","1949","                              copy=True, selected='all')","1994","@pytest.mark.parametrize(\"output_dtype\", [np.int32, np.float32, np.float64])","1995","@pytest.mark.parametrize(\"input_dtype\",  [np.int32, np.float32, np.float64])","1996","@pytest.mark.parametrize(\"sparse\", [True, False])","1997","def test_one_hot_encoder_preserve_type(input_dtype, output_dtype, sparse):","1998","    X = np.array([[0, 1, 0, 0], [1, 2, 0, 0]], dtype=input_dtype)","1999","    transformer = OneHotEncoder(categorical_features=[0, 1],","2000","                                dtype=output_dtype, sparse=sparse)","2001","    X_trans = transformer.fit_transform(X)","2002","    assert X_trans.dtype == output_dtype","2003","","2004",""],"delete":["1911","def _check_transform_selected(X, X_expected, sel):","1913","        Xtr = _transform_selected(M, Binarizer().transform, sel)","1917","def test_transform_selected():","1918","    X = [[3, 2, 1], [0, 1, 1]]","1920","    X_expected = [[1, 2, 1], [0, 1, 1]]","1921","    _check_transform_selected(X, X_expected, [0])","1922","    _check_transform_selected(X, X_expected, [True, False, False])","1924","    X_expected = [[1, 1, 1], [0, 1, 1]]","1925","    _check_transform_selected(X, X_expected, [0, 1, 2])","1926","    _check_transform_selected(X, X_expected, [True, True, True])","1927","    _check_transform_selected(X, X_expected, \"all\")","1929","    _check_transform_selected(X, X, [])","1930","    _check_transform_selected(X, X, [False, False, False])","1933","def test_transform_selected_copy_arg():","1939","    original_X = np.asarray([[1, 2], [3, 4]])","1940","    expected_Xtr = [[2, 2], [3, 4]]","1943","    Xtr = _transform_selected(X, _mutating_transformer, copy=True,","1944","                              selected='all')"]}],"sklearn\/preprocessing\/data.py":[{"add":["1827","def _transform_selected(X, transform, dtype, selected=\"all\", copy=True):","1838","    dtype : number type","1839","        Desired dtype of output.","1840","","1874","        # The columns of X which are not transformed need","1875","        # to be casted to the desire dtype before concatenation.","1876","        # Otherwise, the stacking will cast to the higher-precision dtype.","1877","        X_not_sel = X[:, ind[not_sel]].astype(dtype)","2069","        return _transform_selected(X, self._fit_transform, self.dtype,","2125","        return _transform_selected(X, self._transform, self.dtype,"],"delete":["1827","def _transform_selected(X, transform, selected=\"all\", copy=True):","1871","        X_not_sel = X[:, ind[not_sel]]","2063","        return _transform_selected(X, self._fit_transform,","2119","        return _transform_selected(X, self._transform,"]}]}},"ad281b8791f451a622e1bfb568e02d07c6e2b1cb":{"changes":{"sklearn\/neighbors\/classification.py":"MODIFY"},"diff":{"sklearn\/neighbors\/classification.py":[{"add":["368","            pred_labels = np.zeros(len(neigh_ind), dtype=object)","369","            pred_labels[:] = [_y[ind, k] for ind in neigh_ind]"],"delete":["368","            pred_labels = np.array([_y[ind, k] for ind in neigh_ind],","369","                                   dtype=object)"]}]}},"1d4e18a1a715acf631898ece5ce6cc300e0edcd3":{"changes":{"examples\/cluster\/plot_kmeans_assumptions.py":"MODIFY","examples\/covariance\/plot_outlier_detection.py":"MODIFY","examples\/classification\/plot_lda_qda.py":"MODIFY","examples\/cluster\/plot_birch_vs_minibatchkmeans.py":"MODIFY","examples\/calibration\/plot_calibration.py":"MODIFY","examples\/plot_johnson_lindenstrauss_bound.py":"MODIFY","examples\/cluster\/plot_kmeans_silhouette_analysis.py":"MODIFY","examples\/cluster\/plot_cluster_iris.py":"MODIFY","examples\/cluster\/plot_ward_structured_vs_unstructured.py":"MODIFY"},"diff":{"examples\/cluster\/plot_kmeans_assumptions.py":[{"add":["36","transformation = [[0.60834549, -0.63667341], [-0.40887718, 0.85253229]]","56","y_pred = KMeans(n_clusters=3,","57","                random_state=random_state).fit_predict(X_filtered)"],"delete":["36","transformation = [[ 0.60834549, -0.63667341], [-0.40887718, 0.85253229]]","56","y_pred = KMeans(n_clusters=3, random_state=random_state).fit_predict(X_filtered)"]}],"examples\/covariance\/plot_outlier_detection.py":[{"add":["109","        b = subplot.scatter(X[:-n_outliers, 0], X[:-n_outliers, 1], c='white',","110","                            s=20, edgecolor='k')","111","        c = subplot.scatter(X[-n_outliers:, 0], X[-n_outliers:, 1], c='black',","112","                            s=20, edgecolor='k')"],"delete":["109","        b = subplot.scatter(X[:-n_outliers, 0], X[:-n_outliers, 1], c='white')","110","        c = subplot.scatter(X[-n_outliers:, 0], X[-n_outliers:, 1], c='black')"]}],"examples\/classification\/plot_lda_qda.py":[{"add":["62","        plt.ylabel('Data with\\n fixed covariance')","66","        plt.ylabel('Data with\\n varying covariances')","78","             color='red', markeredgecolor='k')","80","             color='#990000', markeredgecolor='k')  # dark red","84","             color='blue', markeredgecolor='k')","86","             color='#000099', markeredgecolor='k')  # dark blue","102","             'o', color='black', markersize=10, markeredgecolor='k')","104","             'o', color='black', markersize=10, markeredgecolor='k')","116","                              180 + angle, facecolor=color,","117","                              edgecolor='yellow',","149","plt.suptitle('Linear Discriminant Analysis vs Quadratic Discriminant'","150","             'Analysis')"],"delete":["62","        plt.ylabel('Data with fixed covariance')","66","        plt.ylabel('Data with varying covariances')","78","             color='red')","80","             color='#990000')  # dark red","84","             color='blue')","86","             color='#000099')  # dark blue","102","             'o', color='black', markersize=10)","104","             'o', color='black', markersize=10)","116","                              180 + angle, facecolor=color, edgecolor='yellow',","148","plt.suptitle('Linear Discriminant Analysis vs Quadratic Discriminant Analysis')"]}],"examples\/cluster\/plot_birch_vs_minibatchkmeans.py":[{"add":["70","        ax.scatter(X[mask, 0], X[mask, 1],","71","                   c='w', edgecolor=col, marker='.', alpha=0.5)","73","            ax.scatter(this_centroid[0], this_centroid[1], marker='+',","74","                       c='k', s=25)","94","    ax.scatter(X[mask, 0], X[mask, 1], marker='.',","95","               c='w', edgecolor=col, alpha=0.5)","96","    ax.scatter(this_centroid[0], this_centroid[1], marker='+',","97","               c='k', s=25)"],"delete":["41","   ","71","        ax.plot(X[mask, 0], X[mask, 1], 'w',","72","                markerfacecolor=col, marker='.')","74","            ax.plot(this_centroid[0], this_centroid[1], '+', markerfacecolor=col,","75","                    markeredgecolor='k', markersize=5)","95","    ax.plot(X[mask, 0], X[mask, 1], 'w', markerfacecolor=col, marker='.')","96","    ax.plot(this_centroid[0], this_centroid[1], '+', markeredgecolor='k',","97","            markersize=5)"]}],"examples\/calibration\/plot_calibration.py":[{"add":["17","isotonic calibration. One can observe that only the non-parametric model is","18","able to provide a probability calibration that returns probabilities close","19","to the expected 0.5 for most of the samples belonging to the middle","20","cluster with heterogeneous labels. This results in a significantly improved","21","Brier score.","94","    plt.scatter(this_X[:, 0], this_X[:, 1], s=this_sw * 50, c=color,","95","                alpha=0.5, edgecolor='k',"],"delete":["17","isotonic calibration. One can observe that only the non-parametric model is able","18","to provide a probability calibration that returns probabilities close to the","19","expected 0.5 for most of the samples belonging to the middle cluster with","20","heterogeneous labels. This results in a significantly improved Brier score.","93","    plt.scatter(this_X[:, 0], this_X[:, 1], s=this_sw * 50, c=color, alpha=0.5,"]}],"examples\/plot_johnson_lindenstrauss_bound.py":[{"add":["189","    plt.hist(rates, bins=50, normed=True, range=(0., 2.), edgecolor='k')"],"delete":["189","    plt.hist(rates, bins=50, normed=True, range=(0., 2.))"]}],"examples\/cluster\/plot_kmeans_silhouette_analysis.py":[{"add":["121","                c=colors, edgecolor='k')","126","    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',","127","                c=\"white\", alpha=1, s=200, edgecolor='k')","130","        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,","131","                    s=50, edgecolor='k')"],"delete":["121","                c=colors)","126","    ax2.scatter(centers[:, 0], centers[:, 1],","127","                marker='o', c=\"white\", alpha=1, s=200)","130","        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1, s=50)"]}],"examples\/cluster\/plot_cluster_iris.py":[{"add":["27","# Though the following import is not directly being used, it is required","28","# for 3D projection to work","46","fig = plt.figure(figsize=(8, 6))","48","titles = ['3 clusters', '8 clusters', '3 clusters, bad initialization']","50","    ax = plt.subplot(2, 2, fignum, projection='3d',","51","                     elev=48, azim=134)","55","    ax.scatter(X[:, 3], X[:, 0], X[:, 2],","56","               c=labels.astype(np.float), edgecolor='k')","64","    ax.set_title(titles[fignum - 1])","65","    ax.dist = 12","69","ax = plt.subplot(2, 2, 4, projection='3d',","70","                 elev=48, azim=134)","75","              X[y == label, 0].mean(),","76","              X[y == label, 2].mean() + 2, name,","78","              bbox=dict(alpha=.2, edgecolor='w', facecolor='w'))","81","ax.scatter(X[:, 3], X[:, 0], X[:, 2], c=y, edgecolor='k')","89","ax.set_title('Ground Truth')","90","ax.dist = 12","91","","92","fig.tight_layout()","93","fig.show()"],"delete":["29","","45","","48","    fig = plt.figure(fignum, figsize=(4, 3))","49","    plt.clf()","50","    ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)","51","","52","    plt.cla()","56","    ax.scatter(X[:, 3], X[:, 0], X[:, 2], c=labels.astype(np.float))","67","fig = plt.figure(fignum, figsize=(4, 3))","68","plt.clf()","69","ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)","70","","71","plt.cla()","72","","77","              X[y == label, 0].mean() + 1.5,","78","              X[y == label, 2].mean(), name,","80","              bbox=dict(alpha=.5, edgecolor='w', facecolor='w'))","83","ax.scatter(X[:, 3], X[:, 0], X[:, 2], c=y)","91","plt.show()"]}],"examples\/cluster\/plot_ward_structured_vs_unstructured.py":[{"add":["59","    ax.scatter(X[label == l, 0], X[label == l, 1], X[label == l, 2],","60","               color=plt.cm.jet(np.float(l) \/ np.max(label + 1)),","61","               s=20, edgecolor='k')","87","    ax.scatter(X[label == l, 0], X[label == l, 1], X[label == l, 2],","88","               color=plt.cm.jet(float(l) \/ np.max(label + 1)),","89","               s=20, edgecolor='k')"],"delete":["59","    ax.plot3D(X[label == l, 0], X[label == l, 1], X[label == l, 2],","60","              'o', color=plt.cm.jet(np.float(l) \/ np.max(label + 1)))","86","    ax.plot3D(X[label == l, 0], X[label == l, 1], X[label == l, 2],","87","              'o', color=plt.cm.jet(float(l) \/ np.max(label + 1)))"]}]}},"eb1a3c4765948cd1d0cdc7ba2e040aa6a3671c07":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/linear_model\/tests\/test_ridge.py":"MODIFY","sklearn\/linear_model\/ridge.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["258","- Fixed a bug in :class:`linear_model.RidgeClassifierCV` where","259","  the parameter ``store_cv_values`` was not implemented though","260","  it was documented in ``cv_values`` as a way to set up the storage","261","  of cross-validation values for different alphas. :issue:`10297` by ","262","  :user:`Mabel Villalba-Jimnez <mabelvj>`.","263","  "],"delete":[]}],"sklearn\/linear_model\/tests\/test_ridge.py":[{"add":["591","    rng = np.random.RandomState(42)","604","    assert r.cv_values_.shape == (n_samples, n_alphas)","607","    n_targets = 3","608","    y = rng.randn(n_samples, n_targets)","610","    assert r.cv_values_.shape == (n_samples, n_targets, n_alphas)","611","","612","","613","def test_ridge_classifier_cv_store_cv_values():","614","    x = np.array([[-1.0, -1.0], [-1.0, 0], [-.8, -1.0],","615","                  [1.0, 1.0], [1.0, 0.0]])","616","    y = np.array([1, 1, 1, -1, -1])","617","","618","    n_samples = x.shape[0]","619","    alphas = [1e-1, 1e0, 1e1]","620","    n_alphas = len(alphas)","621","","622","    r = RidgeClassifierCV(alphas=alphas, store_cv_values=True)","623","","624","    # with len(y.shape) == 1","625","    n_targets = 1","626","    r.fit(x, y)","627","    assert r.cv_values_.shape == (n_samples, n_targets, n_alphas)","628","","629","    # with len(y.shape) == 2","630","    y = np.array([[1, 1, 1, -1, -1],","631","                  [1, -1, 1, -1, 1],","632","                  [-1, -1, 1, -1, -1]]).transpose()","633","    n_targets = y.shape[1]","634","    r.fit(x, y)","635","    assert r.cv_values_.shape == (n_samples, n_targets, n_alphas)","658","        assert ridgecv.alpha_ == gs.best_estimator_.alpha"],"delete":["591","    # Test _RidgeCV's store_cv_values attribute.","592","    rng = rng = np.random.RandomState(42)","605","    assert_equal(r.cv_values_.shape, (n_samples, n_alphas))","608","    n_responses = 3","609","    y = rng.randn(n_samples, n_responses)","611","    assert_equal(r.cv_values_.shape, (n_samples, n_responses, n_alphas))","634","        assert_equal(ridgecv.alpha_, gs.best_estimator_.alpha)"]}],"sklearn\/linear_model\/ridge.py":[{"add":["1216","        each alpha should be stored in the ``cv_values_`` attribute (see","1217","        below). This flag is only compatible with ``cv=None`` (i.e. using","1224","        Cross-validation values for each alpha (if ``store_cv_values=True``\\","1225","        and ``cv=None``). After ``fit()`` has been called, this attribute \\","1226","        will contain the mean squared errors (by default) or the values \\","1227","        of the ``{loss,score}_func`` function (if provided in the constructor).","1305","    store_cv_values : boolean, default=False","1306","        Flag indicating if the cross-validation values corresponding to","1307","        each alpha should be stored in the ``cv_values_`` attribute (see","1308","        below). This flag is only compatible with ``cv=None`` (i.e. using","1309","        Generalized Cross-Validation).","1310","","1313","    cv_values_ : array, shape = [n_samples, n_targets, n_alphas], optional","1314","        Cross-validation values for each alpha (if ``store_cv_values=True`` and","1315","        ``cv=None``). After ``fit()`` has been called, this attribute will","1316","        contain the mean squared errors (by default) or the values of the","1317","        ``{loss,score}_func`` function (if provided in the constructor).","1343","                 normalize=False, scoring=None, cv=None, class_weight=None,","1344","                 store_cv_values=False):","1347","            scoring=scoring, cv=cv, store_cv_values=store_cv_values)"],"delete":["1216","        each alpha should be stored in the `cv_values_` attribute (see","1217","        below). This flag is only compatible with `cv=None` (i.e. using","1224","        Cross-validation values for each alpha (if `store_cv_values=True` and \\","1225","        `cv=None`). After `fit()` has been called, this attribute will \\","1226","        contain the mean squared errors (by default) or the values of the \\","1227","        `{loss,score}_func` function (if provided in the constructor).","1307","    cv_values_ : array, shape = [n_samples, n_alphas] or \\","1308","    shape = [n_samples, n_responses, n_alphas], optional","1309","        Cross-validation values for each alpha (if `store_cv_values=True` and","1310","    `cv=None`). After `fit()` has been called, this attribute will contain \\","1311","    the mean squared errors (by default) or the values of the \\","1312","    `{loss,score}_func` function (if provided in the constructor).","1338","                 normalize=False, scoring=None, cv=None, class_weight=None):","1341","            scoring=scoring, cv=cv)"]}]}},"00f3910678d2b64ddc5be81255a093b97396c580":{"changes":{"doc\/conf.py":"MODIFY","doc\/templates\/index.html":"MODIFY","doc\/themes\/scikit-learn-modern\/layout.html":"MODIFY"},"diff":{"doc\/conf.py":[{"add":["90","version = \".\".join(version.split(\".\")[:2])"],"delete":[]}],"doc\/templates\/index.html":[{"add":["10","        <a class=\"btn sk-landing-btn mb-1\" href=\"whats_new\/v{{ version }}.html\" role=\"button\">What's New in {{ release }}<\/a>","158","        <li><strong>January 2020.<\/strong> scikit-learn 0.22.1 is available for download (<a href=\"whats_new\/v0.22.html#version-0-22-1\">Changelog<\/a>)."],"delete":["10","        <a class=\"btn sk-landing-btn mb-1\" href=\"whats_new\/v{{ version }}.html\" role=\"button\">What's New in {{ version }}<\/a>"]}],"doc\/themes\/scikit-learn-modern\/layout.html":[{"add":["79","          <strong>scikit-learn {{ release }}<\/strong><br\/>"],"delete":["79","          <strong>scikit-learn {{ version }}<\/strong><br\/>"]}]}},"0788cd0c6a91c0d1cae17340cdf5d2af3c59ec57":{"changes":{"examples\/covariance\/plot_outlier_detection.py":"MODIFY","doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/covariance\/elliptic_envelope.py":"ADD","doc\/modules\/outlier_detection.rst":"MODIFY","doc\/whats_new\/_contributors.rst":"MODIFY","sklearn\/neighbors\/tests\/test_lof.py":"MODIFY","sklearn\/covariance\/tests\/test_elliptic_envelope.py":"ADD","sklearn\/ensemble\/tests\/test_iforest.py":"MODIFY","\/dev\/null":"DELETE","sklearn\/ensemble\/iforest.py":"MODIFY","sklearn\/covariance\/__init__.py":"MODIFY","sklearn\/covariance\/tests\/test_robust_covariance.py":"MODIFY","sklearn\/svm\/classes.py":"MODIFY","sklearn\/svm\/tests\/test_svm.py":"MODIFY","examples\/applications\/plot_species_distribution_modeling.py":"MODIFY","sklearn\/neighbors\/lof.py":"MODIFY"},"diff":{"examples\/covariance\/plot_outlier_detection.py":[{"add":["106","        subplot.contourf(xx, yy, Z, levels=np.linspace(Z.min(), 0, 7),","108","        a = subplot.contour(xx, yy, Z, levels=[0],","110","        subplot.contourf(xx, yy, Z, levels=[0, Z.max()],"],"delete":["97","        threshold = stats.scoreatpercentile(scores_pred,","98","                                            100 * outliers_fraction)","108","        subplot.contourf(xx, yy, Z, levels=np.linspace(Z.min(), threshold, 7),","110","        a = subplot.contour(xx, yy, Z, levels=[threshold],","112","        subplot.contourf(xx, yy, Z, levels=[threshold, Z.max()],"]}],"doc\/whats_new\/v0.20.rst":[{"add":["31","  ","343","Outlier Detection models","344","","345","- More consistent outlier detection API:","346","  Add a ``score_samples`` method in :class:`svm.OneClassSVM`,","347","  :class:`ensemble.IsolationForest`, :class:`neighbors.LocalOutlierFactor`,","348","  :class:`covariance.EllipticEnvelope`. It allows to access raw score","349","  functions from original papers. A new ``offset_`` parameter allows to link","350","  ``score_samples`` and ``decision_function`` methods.","351","  The ``contamination`` parameter of :class:`ensemble.IsolationForest` and","352","  :class:`neighbors.LocalOutlierFactor` ``decision_function`` methods is used","353","  to define this ``offset_`` such that outliers (resp. inliers) have negative (resp.","354","  positive) ``decision_function`` values. By default, ``contamination`` is","355","  kept unchanged to 0.1 for a deprecation period. In 0.22, it will be set to \"auto\",","356","  thus using method-specific score offsets.","357","  In :class:`covariance.EllipticEnvelope` ``decision_function`` method, the","358","  ``raw_values`` parameter is deprecated as the shifted Mahalanobis distance","359","  will be always returned in 0.22. :issue:`9015` by `Nicolas Goix`_.","360","","361",""],"delete":["31",""]}],"sklearn\/covariance\/elliptic_envelope.py":[{"add":[],"delete":[]}],"doc\/modules\/outlier_detection.rst":[{"add":["29","``predict`` method::","33","Inliers are labeled 1, while outliers are labeled -1. The predict method","34","makes use of a threshold on the raw scoring function computed by the","35","estimator. This scoring function is accessible through the ``score_samples``","36","method, while the threshold can be controlled by the ``contamination``","37","parameter.","38","","39","The ``decision_function`` method is also defined from the scoring function,","40","in such a way that negative values are outliers and non-negative ones are","41","inliers::","42","","43","    estimator.decision_function(X_test)","44","","45","Note that :class:`neighbors.LocalOutlierFactor` does not support","46","``predict`` and ``decision_function`` methods, as this algorithm is","47","purely transductive and is thus not designed to deal with new data."],"delete":["29","`predict` method::","33","Inliers are labeled 1, while outliers are labeled -1."]}],"doc\/whats_new\/_contributors.rst":[{"add":["108",".. _Nicolas Goix: http:\/\/ngoix.github.io"],"delete":["108",".. _Nicolas Goix: https:\/\/perso.telecom-paristech.fr\/~goix\/"]}],"sklearn\/neighbors\/tests\/test_lof.py":[{"add":["17","from sklearn.utils.testing import assert_warns_message, assert_raises","73","    clf1 = neighbors.LocalOutlierFactor(n_neighbors=2,","74","                                        contamination=0.1).fit(X_train)","75","    clf2 = neighbors.LocalOutlierFactor(n_neighbors=2).fit(X_train)","79","    assert_array_almost_equal(-clf1.negative_outlier_factor_, [s_0, s_1, s_1])","80","    assert_array_almost_equal(-clf2.negative_outlier_factor_, [s_0, s_1, s_1])","82","    assert_array_almost_equal(-clf1._score_samples([[2., 2.]]), [s_0])","83","    assert_array_almost_equal(-clf2._score_samples([[2., 2.]]), [s_0])","84","    # check predict(one sample already in train)","85","    assert_array_almost_equal(-clf1._score_samples([[1., 1.]]), [s_1])","86","    assert_array_almost_equal(-clf2._score_samples([[1., 1.]]), [s_1])","124","","125","","126","def test_score_samples():","127","    X_train = [[1, 1], [1, 2], [2, 1]]","128","    clf1 = neighbors.LocalOutlierFactor(n_neighbors=2,","129","                                        contamination=0.1).fit(X_train)","130","    clf2 = neighbors.LocalOutlierFactor(n_neighbors=2).fit(X_train)","131","    assert_array_equal(clf1._score_samples([[2., 2.]]),","132","                       clf1._decision_function([[2., 2.]]) + clf1.offset_)","133","    assert_array_equal(clf2._score_samples([[2., 2.]]),","134","                       clf2._decision_function([[2., 2.]]) + clf2.offset_)","135","    assert_array_equal(clf1._score_samples([[2., 2.]]),","136","                       clf2._score_samples([[2., 2.]]))","137","","138","","139","def test_contamination():","140","    X = [[1, 1], [1, 0]]","141","    clf = neighbors.LocalOutlierFactor(contamination=0.6)","142","    assert_raises(ValueError, clf.fit, X)","143","","144","","145","def test_deprecation():","146","    assert_warns_message(DeprecationWarning,","147","                         'default contamination parameter 0.1 will change '","148","                         'in version 0.22 to \"auto\"',","149","                         neighbors.LocalOutlierFactor, )"],"delete":["17","from sklearn.utils.testing import assert_warns_message","73","    clf = neighbors.LocalOutlierFactor(n_neighbors=2).fit(X_train)","77","    assert_array_almost_equal(-clf.negative_outlier_factor_, [s_0, s_1, s_1])","79","    assert_array_almost_equal(-clf._decision_function([[2., 2.]]), [s_0])","80","    # # check predict(one sample already in train)","81","    assert_array_almost_equal(-clf._decision_function([[1., 1.]]), [s_1])"]}],"sklearn\/covariance\/tests\/test_elliptic_envelope.py":[{"add":[],"delete":[]}],"sklearn\/ensemble\/tests\/test_iforest.py":[{"add":["112","    # test X_test n_features match X_train one:","113","    assert_raises(ValueError, IsolationForest().fit(X).predict, X[:, 1:])","114","","191","    # Test IsolationForest","192","    for contamination in [0.25, \"auto\"]:","193","        clf = IsolationForest(random_state=rng, contamination=contamination)","194","        clf.fit(X)","195","        decision_func = - clf.decision_function(X)","196","        pred = clf.predict(X)","197","        # assert detect outliers:","198","        assert_greater(np.min(decision_func[-2:]), np.max(decision_func[:-2]))","199","        assert_array_equal(pred, 6 * [1] + 2 * [-1])","231","","232","","233","def test_score_samples():","234","    X_train = [[1, 1], [1, 2], [2, 1]]","235","    clf1 = IsolationForest(contamination=0.1).fit(X_train)","236","    clf2 = IsolationForest().fit(X_train)","237","    assert_array_equal(clf1.score_samples([[2., 2.]]),","238","                       clf1.decision_function([[2., 2.]]) + clf1.offset_)","239","    assert_array_equal(clf2.score_samples([[2., 2.]]),","240","                       clf2.decision_function([[2., 2.]]) + clf2.offset_)","241","    assert_array_equal(clf1.score_samples([[2., 2.]]),","242","                       clf2.score_samples([[2., 2.]]))","243","","244","","245","def test_deprecation():","246","    assert_warns_message(DeprecationWarning,","247","                         'default contamination parameter 0.1 will change '","248","                         'in version 0.22 to \"auto\"',","249","                         IsolationForest, )","250","    X = [[0.0], [1.0]]","251","    clf = IsolationForest().fit(X)","252","    assert_warns_message(DeprecationWarning,","253","                         \"threshold_ attribute is deprecated in 0.20 and will\"","254","                         \" be removed in 0.22.\",","255","                         getattr, clf, \"threshold_\")"],"delete":["188","    # Test LOF","189","    clf = IsolationForest(random_state=rng, contamination=0.25)","190","    clf.fit(X)","191","    decision_func = - clf.decision_function(X)","192","    pred = clf.predict(X)","193","","194","    # assert detect outliers:","195","    assert_greater(np.min(decision_func[-2:]), np.max(decision_func[:-2]))","196","    assert_array_equal(pred, 6 * [1] + 2 * [-1])"]}],"\/dev\/null":[{"add":[],"delete":[]}],"sklearn\/ensemble\/iforest.py":[{"add":["8","import warnings","18","from ..utils.validation import check_is_fitted","68","        on the decision function. If 'auto', the decision function threshold is","69","        determined as in the original paper.","108","    offset_ : float","109","        Offset used to define the decision function from the raw scores.","110","        We have the relation: decision_function = score_samples - offset_.","111","        The offset is set to 0.5 as in the original paper, except when a","112","        contamination parameter different than \"auto\" is provided. In that","113","        case, the offset is defined in such a way we obtain the expected","114","        number of outliers (samples with decision function < 0) in training.","115","","129","                 contamination=\"legacy\",","149","","150","        if contamination == \"legacy\":","151","            warnings.warn('default contamination parameter 0.1 will change '","152","                          'in version 0.22 to \"auto\". This will change the '","153","                          'predict method behavior.',","154","                          DeprecationWarning)","218","        if self.contamination == \"auto\":","219","            # 0.5 plays a special role as described in the original paper.","220","            # we take the opposite as we consider the opposite of their score.","221","            self.offset_ = -0.5","222","            # need to save (depreciated) threshold_ in this case:","223","            self._threshold_ = sp.stats.scoreatpercentile(","224","                self.score_samples(X), 100. * 0.1)","225","        elif self.contamination == \"legacy\":  # to be rm in 0.22","226","            self.offset_ = sp.stats.scoreatpercentile(","227","                self.score_samples(X), 100. * 0.1)","228","        else:","229","            self.offset_ = sp.stats.scoreatpercentile(","230","                self.score_samples(X), 100. * self.contamination)","247","            For each observation, tells whether or not (+1 or -1) it should","250","        check_is_fitted(self, [\"offset_\"])","253","        is_inlier[self.decision_function(X) < 0] = -1","276","        scores : array, shape (n_samples,)","278","            The lower, the more abnormal. Negative scores represent outliers,","279","            positive scores represent inliers.","282","        # We substract self.offset_ to make 0 be the threshold value for being","283","        # an outlier:","284","","285","        return self.score_samples(X) - self.offset_","286","","287","    def score_samples(self, X):","288","        \"\"\"Opposite of the anomaly score defined in the original paper.","289","","290","        The anomaly score of an input sample is computed as","291","        the mean anomaly score of the trees in the forest.","292","","293","        The measure of normality of an observation given a tree is the depth","294","        of the leaf containing this observation, which is equivalent to","295","        the number of splittings required to isolate this point. In case of","296","        several observations n_left in the leaf, the average path length of","297","        a n_left samples isolation tree is added.","298","","299","        Parameters","300","        ----------","301","        X : {array-like, sparse matrix}, shape (n_samples, n_features)","302","            The training input samples. Sparse matrices are accepted only if","303","            they are supported by the base estimator.","304","","305","        Returns","306","        -------","307","        scores : array, shape (n_samples,)","308","            The anomaly score of the input samples.","309","            The lower, the more abnormal.","310","        \"\"\"","312","        check_is_fitted(self, [\"estimators_\"])","313","","316","        if self.n_features_ != X.shape[1]:","317","            raise ValueError(\"Number of features of the model must \"","318","                             \"match the input. Model n_features is {0} and \"","319","                             \"input n_features is {1}.\"","320","                             \"\".format(self.n_features_, X.shape[1]))","345","        scores = 2 ** (-depths.mean(axis=1) \/ _average_path_length(","346","            self.max_samples_))","349","        # abnormal)","350","        return -scores","351","","352","    @property","353","    def threshold_(self):","354","        warnings.warn(\"threshold_ attribute is deprecated in 0.20 and will\"","355","                      \" be removed in 0.22.\", DeprecationWarning)","356","        if self.contamination == 'auto':","357","            return self._threshold_","358","        return self.offset_","367","    n_samples_leaf : array-like, shape (n_samples, n_estimators), or int."],"delete":["66","        on the decision function.","118","                 contamination=0.1,","201","        self.threshold_ = -sp.stats.scoreatpercentile(","202","            -self.decision_function(X), 100. * (1. - self.contamination))","219","            For each observations, tells whether or not (+1 or -1) it should","224","        is_inlier[self.decision_function(X) <= self.threshold_] = -1","247","        scores : array of shape (n_samples,)","249","            The lower, the more abnormal.","279","        scores = 2 ** (-depths.mean(axis=1) \/ _average_path_length(self.max_samples_))","282","        # abnormal) and add 0.5 (this value plays a special role as described","283","        # in the original paper) to give a sense to scores = 0:","284","        return 0.5 - scores","293","    n_samples_leaf : array-like of shape (n_samples, n_estimators), or int."]}],"sklearn\/covariance\/__init__.py":[{"add":["15","from .elliptic_envelope import EllipticEnvelope"],"delete":["15","from .outlier_detection import EllipticEnvelope"]}],"sklearn\/covariance\/tests\/test_robust_covariance.py":[{"add":["14","from sklearn.covariance import empirical_covariance, MinCovDet"],"delete":["10","from sklearn.utils.testing import assert_almost_equal","12","from sklearn.utils.testing import assert_raises","14","from sklearn.exceptions import NotFittedError","17","from sklearn.covariance import empirical_covariance, MinCovDet, \\","18","    EllipticEnvelope","139","","140","","141","def test_outlier_detection():","142","    rnd = np.random.RandomState(0)","143","    X = rnd.randn(100, 10)","144","    clf = EllipticEnvelope(contamination=0.1)","145","    assert_raises(NotFittedError, clf.predict, X)","146","    assert_raises(NotFittedError, clf.decision_function, X)","147","    clf.fit(X)","148","    y_pred = clf.predict(X)","149","    decision = clf.decision_function(X, raw_values=True)","150","    decision_transformed = clf.decision_function(X, raw_values=False)","151","","152","    assert_array_almost_equal(","153","        decision, clf.mahalanobis(X))","154","    assert_array_almost_equal(clf.mahalanobis(X), clf.dist_)","155","    assert_almost_equal(clf.score(X, np.ones(100)),","156","                        (100 - y_pred[y_pred == -1].size) \/ 100.)","157","    assert(sum(y_pred == -1) == sum(decision_transformed < 0))"]}],"sklearn\/svm\/classes.py":[{"add":["1058","    offset_ : float","1059","        Offset used to define the decision function from the raw scores.","1060","        We have the relation: decision_function = score_samples - offset_.","1061","        The offset is the opposite of intercept_ and is provided for","1062","        consistency with other outlier detection algorithms.","1063","","1107","        self.offset_ = -self._intercept_","1121","        dec : array-like, shape (n_samples,)","1124","        dec = self._decision_function(X).ravel()","1127","    def score_samples(self, X):","1128","        \"\"\"Raw scoring function of the samples.","1129","","1130","        Parameters","1131","        ----------","1132","        X : array-like, shape (n_samples, n_features)","1133","","1134","        Returns","1135","        -------","1136","        score_samples : array-like, shape (n_samples,)","1137","            Returns the (unshifted) scoring function of the samples.","1138","        \"\"\"","1139","        return self.decision_function(X) + self.offset_","1140",""],"delete":["1114","        X : array-like, shape (n_samples,)","1117","        dec = self._decision_function(X)"]}],"sklearn\/svm\/tests\/test_svm.py":[{"add":["282","def test_oneclass_score_samples():","283","    X_train = [[1, 1], [1, 2], [2, 1]]","284","    clf = svm.OneClassSVM().fit(X_train)","285","    assert_array_equal(clf.score_samples([[2., 2.]]),","286","                       clf.decision_function([[2., 2.]]) + clf.offset_)","287","","288",""],"delete":[]}],"examples\/applications\/plot_species_distribution_modeling.py":[{"add":["170","        pred = clf.decision_function((coverages_land - mean) \/ std)","194","        pred_test = clf.decision_function((species.cov_test - mean) \/ std)"],"delete":["170","        pred = clf.decision_function((coverages_land - mean) \/ std)[:, 0]","194","        pred_test = clf.decision_function((species.cov_test - mean)","195","                                          \/ std)[:, 0]"]}],"sklearn\/neighbors\/lof.py":[{"add":["5","import warnings","97","        threshold on the decision function. If \"auto\", the decision function","98","        threshold is determined as in the original paper.","109","        The opposite LOF of the training samples. The higher, the more normal.","110","        Inliers tend to have a LOF score close to 1 (negative_outlier_factor_","111","        close to -1), while outliers tend to have a larger LOF score.","121","    offset_ : float","122","        Offset used to obtain binary labels from the raw scores.","123","        Observations having a negative_outlier_factor smaller than offset_ are","124","        detected as abnormal.","125","        The offset is set to -1.5 (inliers score around -1), except when a","126","        contamination parameter different than \"auto\" is provided. In that","127","        case, the offset is defined in such a way we obtain the expected","128","        number of outliers in training.","129","","137","                 contamination=\"legacy\", n_jobs=1):","144","        if contamination == \"legacy\":","145","            warnings.warn('default contamination parameter 0.1 will change '","146","                          'in version 0.22 to \"auto\". This will change the '","147","                          'predict method behavior.',","148","                          DeprecationWarning)","184","        if self.contamination not in [\"auto\", \"legacy\"]:  # rm legacy in 0.22","185","            if not(0. < self.contamination <= .5):","186","                raise ValueError(\"contamination must be in (0, 0.5], \"","187","                                 \"got: %f\" % self.contamination)","193","            warnings.warn(\"n_neighbors (%s) is greater than the \"","194","                          \"total number of samples (%s). n_neighbors \"","195","                          \"will be set to (n_samples - 1) for estimation.\"","196","                          % (self.n_neighbors, n_samples))","205","        # Compute lof score over training samples to define offset_:","211","        if self.contamination == \"auto\":","212","            # inliers score around -1 (the higher, the less abnormal).","213","            self.offset_ = -1.5","214","        elif self.contamination == \"legacy\":  # to rm in 0.22","215","            self.offset_ = scoreatpercentile(","216","                self.negative_outlier_factor_, 100. * 0.1)","217","        else:","218","            self.offset_ = scoreatpercentile(","219","                self.negative_outlier_factor_, 100. * self.contamination)","229","        this method is kept private. In particular, fit(X)._predict(X) is not","230","        the same as fit_predict(X).","244","        check_is_fitted(self, [\"offset_\", \"negative_outlier_factor_\",","250","            is_inlier[self._decision_function(X) < 0] = -1","253","            is_inlier[self.negative_outlier_factor_ < self.offset_] = -1","258","        \"\"\"Shifted opposite of the Local Outlier Factor of X","259","","260","        Bigger is better, i.e. large values correspond to inliers.","261","","262","        The shift offset allows a zero threshold for being an outlier.","263","        The argument X is supposed to contain *new data*: if X contains a","264","        point from training, it consider the later in its own neighborhood.","265","        Also, the samples in X are not considered in the neighborhood of any","266","        point.","267","        This method is kept private as the predict method is.","268","        The decision function on training data is available by considering the","269","        the negative_outlier_factor_ attribute.","270","","271","        Parameters","272","        ----------","273","        X : array-like, shape (n_samples, n_features)","274","            The query sample or samples to compute the Local Outlier Factor","275","            w.r.t. the training samples.","276","","277","        Returns","278","        -------","279","        shifted_opposite_lof_scores : array, shape (n_samples,)","280","            The shifted opposite of the Local Outlier Factor of each input","281","            samples. The lower, the more abnormal. Negative scores represent","282","            outliers, positive scores represent inliers.","283","        \"\"\"","284","        return self._score_samples(X) - self.offset_","285","","286","    def _score_samples(self, X):","287","        \"\"\"Opposite of the Local Outlier Factor of X (as bigger is","288","        better, i.e. large values correspond to inliers).","294","        This method is kept private as the predict method is.","308","        check_is_fitted(self, [\"offset_\", \"negative_outlier_factor_\","],"delete":["5","from warnings import warn","97","        threshold on the decision function.","108","        The opposite LOF of the training samples. The lower, the more abnormal.","109","        Inliers tend to have a LOF score close to 1, while outliers tend","110","        to have a larger LOF score.","127","                 contamination=0.1, n_jobs=1):","169","        if not (0. < self.contamination <= .5):","170","            raise ValueError(\"contamination must be in (0, 0.5]\")","176","            warn(\"n_neighbors (%s) is greater than the \"","177","                 \"total number of samples (%s). n_neighbors \"","178","                 \"will be set to (n_samples - 1) for estimation.\"","179","                 % (self.n_neighbors, n_samples))","188","        # Compute lof score over training samples to define threshold_:","194","        self.threshold_ = -scoreatpercentile(","195","            -self.negative_outlier_factor_, 100. * (1. - self.contamination))","205","        this method is kept private.","219","        check_is_fitted(self, [\"threshold_\", \"negative_outlier_factor_\",","225","            is_inlier[self._decision_function(X) <= self.threshold_] = -1","228","            is_inlier[self.negative_outlier_factor_ <= self.threshold_] = -1","233","        \"\"\"Opposite of the Local Outlier Factor of X (as bigger is better,","234","        i.e. large values correspond to inliers).","240","        The decision function on training data is available by considering the","241","        opposite of the negative_outlier_factor_ attribute.","255","        check_is_fitted(self, [\"threshold_\", \"negative_outlier_factor_\",","257",""]}]}},"cb1b6c4734b2989ad0492b56c326858d030f3fa2":{"changes":{"benchmarks\/.gitignore":"ADD","sklearn\/neighbors\/quad_tree.pyx":"ADD","examples\/manifold\/plot_t_sne_perplexity.py":"MODIFY","sklearn\/manifold\/tests\/test_t_sne.py":"MODIFY","sklearn\/mixture\/base.py":"MODIFY","sklearn\/tree\/_utils.pxd":"MODIFY","benchmarks\/plot_tsne_mnist.py":"ADD","sklearn\/manifold\/_utils.pyx":"MODIFY","sklearn\/neighbors\/quad_tree.pxd":"ADD","doc\/whats_new.rst":"MODIFY","benchmarks\/bench_tsne_mnist.py":"ADD","sklearn\/manifold\/setup.py":"MODIFY","sklearn\/manifold\/_barnes_hut_tsne.pyx":"MODIFY","sklearn\/neighbors\/tests\/test_quad_tree.py":"ADD","sklearn\/manifold\/t_sne.py":"MODIFY","sklearn\/neighbors\/setup.py":"MODIFY"},"diff":{"benchmarks\/.gitignore":[{"add":[],"delete":[]}],"sklearn\/neighbors\/quad_tree.pyx":[{"add":[],"delete":[]}],"examples\/manifold\/plot_t_sne_perplexity.py":[{"add":["16","visually diverge from S-curve topology on the S-curve dataset even for","30","import numpy as np","37","n_samples = 300","39","(fig, subplots) = plt.subplots(3, 5, figsize=(15, 8))","40","perplexities = [5, 30, 50, 100]","74","ax.scatter(X[:, 0], X[:, 2], c=color, cmap=plt.cm.viridis)","89","    ax.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.viridis)","94","","95","# Another example using a 2D uniform grid","96","x = np.linspace(0, 1, int(np.sqrt(n_samples)))","97","xx, yy = np.meshgrid(x, x)","98","X = np.hstack([","99","    xx.ravel().reshape(-1, 1),","100","    yy.ravel().reshape(-1, 1),","101","])","102","color = xx.ravel()","103","ax = subplots[2][0]","104","ax.scatter(X[:, 0], X[:, 1], c=color, cmap=plt.cm.viridis)","105","ax.xaxis.set_major_formatter(NullFormatter())","106","ax.yaxis.set_major_formatter(NullFormatter())","107","","108","for i, perplexity in enumerate(perplexities):","109","    ax = subplots[2][i + 1]","110","","111","    t0 = time()","112","    tsne = manifold.TSNE(n_components=n_components, init='random',","113","                         random_state=0, perplexity=perplexity)","114","    Y = tsne.fit_transform(X)","115","    t1 = time()","116","    print(\"uniform grid, perplexity=%d in %.2g sec\" % (perplexity, t1 - t0))","117","","118","    ax.set_title(\"Perplexity=%d\" % perplexity)","119","    ax.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.viridis)","120","    ax.xaxis.set_major_formatter(NullFormatter())","121","    ax.yaxis.set_major_formatter(NullFormatter())","122","    ax.axis('tight')","123","","124",""],"delete":["16","visually diverge from S-curve topology on the S-curve dateset even for","36","n_samples = 500","38","(fig, subplots) = plt.subplots(2, 5, figsize=(15, 8))","39","perplexities = [5, 50, 100, 150]","73","ax.scatter(X[:, 0], X[:, 2], c=color, cmap=plt.cm.Spectral)","88","    ax.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.Spectral)"]}],"sklearn\/manifold\/tests\/test_t_sne.py":[{"add":["6","from sklearn.neighbors import NearestNeighbors","13","from sklearn.utils.testing import assert_greater","34","x = np.linspace(0, 1, 10)","35","xx, yy = np.meshgrid(x, x)","36","X_2d_grid = np.hstack([","37","    xx.ravel().reshape(-1, 1),","38","    yy.ravel().reshape(-1, 1),","39","])","40","","41","","62","            min_gain=0.0, min_grad_norm=1e-5, verbose=2)","78","            min_gain=0.0, min_grad_norm=0.0, verbose=2)","94","            min_gain=0.0, min_grad_norm=0.0, verbose=2)","136","    neighbors_nn = np.argsort(distances, axis=1)[:, 1:k].astype(np.int64)","137","    distances_nn = np.array([distances[k, neighbors_nn[k]]","138","                            for k in range(n_samples)])","139","    P2 = _binary_search_perplexity(distances_nn, neighbors_nn,","141","    P_nn = np.array([P1[k, neighbors_nn[k]] for k in range(n_samples)])","142","    assert_array_almost_equal(P_nn, P2, decimal=4)","145","    for k in np.linspace(80, n_samples, 5):","149","        distances_nn = np.array([distances[k, neighbors_nn[k]]","150","                                for k in range(n_samples)])","151","        P2k = _binary_search_perplexity(distances_nn, neighbors_nn,","155","        idx = np.argsort(P2k.ravel())[::-1]","177","        # Convert the sparse matrix to a dense one for testing","178","        P1 = P1.toarray()","197","    distances = np.abs(distances.dot(distances.T))","199","    X_embedded = random_state.randn(n_samples, n_components).astype(np.float32)","239","    X = random_state.randn(50, n_components).astype(np.float32)","242","            tsne = TSNE(n_components=n_components, init=init, random_state=0,","245","            t = trustworthiness(X, X_embedded, n_neighbors=1)","246","            assert_greater(t, 0.9)","254","    for n_iter in [250, 300, 350]:","279","    for i in range(3):","280","        X = random_state.randn(100, 2)","281","        D = squareform(pdist(X), \"sqeuclidean\")","282","        tsne = TSNE(n_components=2, perplexity=2, learning_rate=100.0,","283","                    early_exaggeration=2.0, metric=\"precomputed\",","284","                    random_state=i, verbose=0)","285","        X_embedded = tsne.fit_transform(D)","286","        t = trustworthiness(D, X_embedded, n_neighbors=1,","287","                            precomputed=True)","288","        assert t > .95","312","def test_non_positive_precomputed_distances():","313","    # Precomputed distance matrices must be positive.","314","    bad_dist = np.array([[0., -1.], [1., 0.]])","315","    for method in ['barnes_hut', 'exact']:","316","        tsne = TSNE(metric=\"precomputed\", method=method)","317","        assert_raises_regexp(ValueError, \"All distances .*precomputed.*\",","318","                             tsne.fit_transform, bad_dist)","319","","320","","321","def test_non_positive_computed_distances():","322","    # Computed distance matrices must be positive.","323","    def metric(x, y):","324","        return -1","325","","326","    tsne = TSNE(metric=metric, method='exact')","327","    X = np.array([[0.0, 0.0], [1.0, 1.0]])","328","    assert_raises_regexp(ValueError, \"All distances .*metric given.*\",","329","                         tsne.fit_transform, X)","330","","331","","334","    tsne = TSNE(init=\"not available\")","336","    assert_raises_regexp(ValueError, m, tsne.fit_transform,","337","                         np.array([[0.0], [1.0]]))","356","    tsne = TSNE(metric=\"not available\", method='exact')","360","    tsne = TSNE(metric=\"not available\", method='barnes_hut')","361","    assert_raises_regexp(ValueError, \"Metric 'not available' not valid.*\",","362","                         tsne.fit_transform, np.array([[0.0], [1.0]]))","363","","364","","365","def test_method_not_available():","366","    # 'nethod' must be 'barnes_hut' or 'exact'","367","    tsne = TSNE(method='not available')","368","    assert_raises_regexp(ValueError, \"'method' must be 'barnes_hut' or \",","369","                         tsne.fit_transform, np.array([[0.0], [1.0]]))","370","","371","","372","def test_angle_out_of_range_checks():","373","    # check the angle parameter range","374","    for angle in [-1, -1e-6, 1 + 1e-6, 2]:","375","        tsne = TSNE(angle=angle)","376","        assert_raises_regexp(ValueError, \"'angle' must be between 0.0 - 1.0\",","377","                             tsne.fit_transform, np.array([[0.0], [1.0]]))","378","","388","def test_n_components_range():","389","    # barnes_hut method should only be used with n_components <= 3","390","    tsne = TSNE(n_components=4, method=\"barnes_hut\")","391","    assert_raises_regexp(ValueError, \"'n_components' should be .*\",","392","                         tsne.fit_transform, np.array([[0.0], [1.0]]))","393","","394","","395","def test_early_exaggeration_used():","396","    # check that the ``early_exaggeration`` parameter has an effect","397","    random_state = check_random_state(0)","398","    n_components = 2","399","    methods = ['exact', 'barnes_hut']","400","    X = random_state.randn(25, n_components).astype(np.float32)","401","    for method in methods:","402","        tsne = TSNE(n_components=n_components, perplexity=1,","403","                    learning_rate=100.0, init=\"pca\", random_state=0,","404","                    method=method, early_exaggeration=1.0)","405","        X_embedded1 = tsne.fit_transform(X)","406","        tsne = TSNE(n_components=n_components, perplexity=1,","407","                    learning_rate=100.0, init=\"pca\", random_state=0,","408","                    method=method, early_exaggeration=10.0)","409","        X_embedded2 = tsne.fit_transform(X)","410","","411","        assert not np.allclose(X_embedded1, X_embedded2)","412","","413","","414","def test_n_iter_used():","415","    # check that the ``n_iter`` parameter has an effect","416","    random_state = check_random_state(0)","417","    n_components = 2","418","    methods = ['exact', 'barnes_hut']","419","    X = random_state.randn(25, n_components).astype(np.float32)","420","    for method in methods:","421","        for n_iter in [251, 500]:","422","            tsne = TSNE(n_components=n_components, perplexity=1,","423","                        learning_rate=0.5, init=\"random\", random_state=0,","424","                        method=method, early_exaggeration=1.0, n_iter=n_iter)","425","            tsne.fit_transform(X)","426","","427","            assert tsne.n_iter_ == n_iter - 1","428","","429","","503","    from scipy.sparse import csr_matrix","504","    P = csr_matrix(pij_input)","505","","506","    neighbors = P.indices.astype(np.int64)","507","    indptr = P.indptr.astype(np.int64)","508","","509","    _barnes_hut_tsne.gradient(P.data, pos_output, neighbors, indptr,","530","    assert(\"nearest neighbors...\" in out)","570","            X = random_state.randn(50, 2).astype(dt)","572","                        random_state=0, method=method, verbose=0)","573","            X_embedded = tsne.fit_transform(X)","574","            effective_type = X_embedded.dtype","575","","576","            # tsne cython code is only single precision, so the output will","577","            # always be single precision, irrespectively of the input dtype","578","            assert effective_type == np.float32","593","        distances = abs(distances.dot(distances.T))","596","        P = _joint_probabilities(distances, perplexity, verbose=0)","597","        kl_exact, grad_exact = _kl_divergence(params, P, degrees_of_freedom,","598","                                              n_samples, n_components)","604","        distances_nn = np.array([distances[i, neighbors_nn[i]]","605","                                 for i in range(n_samples)])","606","        assert np.all(distances[0, neighbors_nn[0]] == distances_nn[0]),\\","607","            abs(distances[0, neighbors_nn[0]] - distances_nn[0])","608","        P_bh = _joint_probabilities_nn(distances_nn, neighbors_nn,","609","                                       perplexity, verbose=0)","610","        kl_bh, grad_bh = _kl_divergence_bh(params, P_bh, degrees_of_freedom,","611","                                           n_samples, n_components,","612","                                           angle=angle, skip_num_points=0,","613","                                           verbose=0)","615","        P = squareform(P)","616","        P_bh = P_bh.toarray()","617","        assert_array_almost_equal(P_bh, P, decimal=5)","618","        assert_almost_equal(kl_exact, kl_bh, decimal=3)","625","    X = random_state.randn(100, 10)","626","    for method in [\"barnes_hut\", \"exact\"]:","627","        tsne = TSNE(n_iter_without_progress=-1, verbose=2, learning_rate=1e8,","628","                    random_state=0, method=method, n_iter=351, init=\"random\")","629","        tsne._N_ITER_CHECK = 1","630","        tsne._EXPLORATION_N_ITER = 0","632","        old_stdout = sys.stdout","633","        sys.stdout = StringIO()","634","        try:","635","            tsne.fit_transform(X)","636","        finally:","637","            out = sys.stdout.getvalue()","638","            sys.stdout.close()","639","            sys.stdout = old_stdout","641","        # The output needs to contain the value of n_iter_without_progress","642","        assert_in(\"did not make any progress during the \"","643","                  \"last -1 episodes. Finished.\", out)","676","            line = line.replace('gradient norm = ', '').split(' ')[0]","714","","715","","716","def check_uniform_grid(method, seeds=[0, 1, 2], n_iter=1000):","717","    \"\"\"Make sure that TSNE can approximately recover a uniform 2D grid\"\"\"","718","    for seed in seeds:","719","        tsne = TSNE(n_components=2, init='random', random_state=seed,","720","                    perplexity=10, n_iter=n_iter, method=method)","721","        Y = tsne.fit_transform(X_2d_grid)","722","","723","        # Ensure that the convergence criterion has been triggered","724","        assert tsne.n_iter_ < n_iter","725","","726","        # Ensure that the resulting embedding leads to approximately","727","        # uniformly spaced points: the distance to the closest neighbors","728","        # should be non-zero and approximately constant.","729","        nn = NearestNeighbors(n_neighbors=1).fit(Y)","730","        dist_to_nn = nn.kneighbors(return_distance=True)[0].ravel()","731","        assert dist_to_nn.min() > 0.1","732","","733","        smallest_to_mean = dist_to_nn.min() \/ np.mean(dist_to_nn)","734","        largest_to_mean = dist_to_nn.max() \/ np.mean(dist_to_nn)","735","","736","        try_name = \"{}_{}\".format(method, seed)","737","        assert_greater(smallest_to_mean, .5, msg=try_name)","738","        assert_less(largest_to_mean, 2, msg=try_name)","739","","740","","741","def test_uniform_grid():","742","    for method in ['barnes_hut', 'exact']:","743","        yield check_uniform_grid, method","744","","745","","746","def test_bh_match_exact():","747","    # check that the ``barnes_hut`` method match the exact one when","748","    # ``angle = 0`` and ``perplexity > n_samples \/ 3``","749","    random_state = check_random_state(0)","750","    n_features = 10","751","    X = random_state.randn(30, n_features).astype(np.float32)","752","    X_embeddeds = {}","753","    n_iter = {}","754","    for method in ['exact', 'barnes_hut']:","755","        tsne = TSNE(n_components=2, method=method, learning_rate=1.0,","756","                    init=\"random\", random_state=0, n_iter=251,","757","                    perplexity=30.0, angle=0)","758","        # Kill the early_exaggeration","759","        tsne._EXPLORATION_N_ITER = 0","760","        X_embeddeds[method] = tsne.fit_transform(X)","761","        n_iter[method] = tsne.n_iter_","762","","763","    assert n_iter['exact'] == n_iter['barnes_hut']","764","    assert_array_almost_equal(X_embeddeds['exact'], X_embeddeds['barnes_hut'],","765","                              decimal=3)"],"delete":["52","            min_gain=0.0, min_grad_norm=1e-5, min_error_diff=0.0, verbose=2)","61","    # Error difference","62","    old_stdout = sys.stdout","63","    sys.stdout = StringIO()","64","    try:","65","        _, error, it = _gradient_descent(","66","            ObjectiveSmallGradient(), np.zeros(1), 0, n_iter=100,","67","            n_iter_without_progress=100, momentum=0.0, learning_rate=0.0,","68","            min_gain=0.0, min_grad_norm=0.0, min_error_diff=0.2, verbose=2)","69","    finally:","70","        out = sys.stdout.getvalue()","71","        sys.stdout.close()","72","        sys.stdout = old_stdout","73","    assert_equal(error, 0.9)","74","    assert_equal(it, 1)","75","    assert(\"error difference\" in out)","76","","84","            min_gain=0.0, min_grad_norm=0.0, min_error_diff=-1.0, verbose=2)","100","            min_gain=0.0, min_grad_norm=0.0, min_error_diff=0.0, verbose=2)","142","    neighbors_nn = np.argsort(distances, axis=1)[:, :k].astype(np.int64)","143","    P2 = _binary_search_perplexity(distances, neighbors_nn,","145","    assert_array_almost_equal(P1, P2, decimal=4)","148","    for k in np.linspace(80, n_samples, 10):","152","        P2k = _binary_search_perplexity(distances, neighbors_nn,","195","    distances = distances.dot(distances.T)","197","    X_embedded = random_state.randn(n_samples, n_components)","235","    # The Barnes-Hut approximation uses a different method to estimate","236","    # P_ij using only a number of nearest neighbors instead of all","237","    # points (so that k = 3 * perplexity). As a result we set the","238","    # perplexity=5, so that the number of neighbors is 5%.","241","    X = random_state.randn(100, n_components).astype(np.float32)","244","            tsne = TSNE(n_components=n_components, perplexity=50,","245","                        learning_rate=100.0, init=init, random_state=0,","248","            T = trustworthiness(X, X_embedded, n_neighbors=1)","249","            assert_almost_equal(T, 1.0, decimal=1)","257","    for n_iter in [200, 250, 300]:","282","    X = random_state.randn(100, 2)","283","    D = squareform(pdist(X), \"sqeuclidean\")","284","    tsne = TSNE(n_components=2, perplexity=2, learning_rate=100.0,","285","                metric=\"precomputed\", random_state=0, verbose=0)","286","    X_embedded = tsne.fit_transform(D)","287","    assert_almost_equal(trustworthiness(D, X_embedded, n_neighbors=1,","288","                                        precomputed=True), 1.0, decimal=1)","315","    assert_raises_regexp(ValueError, m, TSNE, init=\"not available\")","334","    tsne = TSNE(metric=\"not available\")","420","    _barnes_hut_tsne.gradient(pij_input, pos_output, neighbors,","441","    assert(\"Computing pairwise distances\" in out)","444","    assert(\"Finished\" in out)","446","    assert(\"Finished\" in out)","483","            X = random_state.randn(100, 2).astype(dt)","485","                        random_state=0, method=method)","486","            tsne.fit_transform(X)","501","        distances = distances.dot(distances.T)","504","        P = _joint_probabilities(distances, perplexity, False)","505","        kl, gradex = _kl_divergence(params, P, degrees_of_freedom, n_samples,","506","                                    n_components)","512","        Pbh = _joint_probabilities_nn(distances, neighbors_nn,","513","                                      perplexity, False)","514","        kl, gradbh = _kl_divergence_bh(params, Pbh, neighbors_nn,","515","                                       degrees_of_freedom, n_samples,","516","                                       n_components, angle=angle,","517","                                       skip_num_points=0, verbose=False)","518","        assert_array_almost_equal(Pbh, P, decimal=5)","519","        assert_array_almost_equal(gradex, gradbh, decimal=5)","521","","522","def test_quadtree_similar_point():","523","    # Introduce a point into a quad tree where a similar point already exists.","524","    # Test will hang if it doesn't complete.","525","    Xs = []","526","","527","    # check the case where points are actually different","528","    Xs.append(np.array([[1, 2], [3, 4]], dtype=np.float32))","529","    # check the case where points are the same on X axis","530","    Xs.append(np.array([[1.0, 2.0], [1.0, 3.0]], dtype=np.float32))","531","    # check the case where points are arbitrarily close on X axis","532","    Xs.append(np.array([[1.00001, 2.0], [1.00002, 3.0]], dtype=np.float32))","533","    # check the case where points are the same on Y axis","534","    Xs.append(np.array([[1.0, 2.0], [3.0, 2.0]], dtype=np.float32))","535","    # check the case where points are arbitrarily close on Y axis","536","    Xs.append(np.array([[1.0, 2.00001], [3.0, 2.00002]], dtype=np.float32))","537","    # check the case where points are arbitrarily close on both axes","538","    Xs.append(np.array([[1.00001, 2.00001], [1.00002, 2.00002]],","539","              dtype=np.float32))","540","","541","    # check the case where points are arbitrarily close on both axes","542","    # close to machine epsilon - x axis","543","    Xs.append(np.array([[1, 0.0003817754041], [2, 0.0003817753750]],","544","              dtype=np.float32))","545","","546","    # check the case where points are arbitrarily close on both axes","547","    # close to machine epsilon - y axis","548","    Xs.append(np.array([[0.0003817754041, 1.0], [0.0003817753750, 2.0]],","549","              dtype=np.float32))","550","","551","    for X in Xs:","552","        counts = np.zeros(3, dtype='int64')","553","        _barnes_hut_tsne.check_quadtree(X, counts)","554","        m = \"Tree consistency failed: unexpected number of points at root node\"","555","        assert_equal(counts[0], counts[1], m)","556","        m = \"Tree consistency failed: unexpected number of points on the tree\"","557","        assert_equal(counts[0], counts[2], m)","558","","559","","560","def test_index_offset():","561","    # Make sure translating between 1D and N-D indices are preserved","562","    assert_equal(_barnes_hut_tsne.test_index2offset(), 1)","563","    assert_equal(_barnes_hut_tsne.test_index_offset(), 1)","570","    X = random_state.randn(100, 2)","571","    tsne = TSNE(n_iter_without_progress=-1, verbose=2,","572","                random_state=1, method='exact')","574","    old_stdout = sys.stdout","575","    sys.stdout = StringIO()","576","    try:","577","        tsne.fit_transform(X)","578","    finally:","579","        out = sys.stdout.getvalue()","580","        sys.stdout.close()","581","        sys.stdout = old_stdout","583","    # The output needs to contain the value of n_iter_without_progress","584","    assert_in(\"did not make any progress during the \"","585","              \"last -1 episodes. Finished.\", out)","618","            line = line.replace('gradient norm = ', '')"]}],"sklearn\/mixture\/base.py":[{"add":["353","            Returns the probability each Gaussian (state) in"],"delete":["353","            Returns the probability of each Gaussian (state) in"]}],"sklearn\/tree\/_utils.pxd":[{"add":["12","from _tree cimport Node","13","from sklearn.neighbors.quad_tree cimport Cell","42","    (Cell*)"],"delete":["12","from _tree cimport Node "]}],"benchmarks\/plot_tsne_mnist.py":[{"add":[],"delete":[]}],"sklearn\/manifold\/_utils.pyx":[{"add":["14","        np.ndarray[np.float32_t, ndim=2] affinities,","15","        np.ndarray[np.int64_t, ndim=2] neighbors,","18","    \"\"\"Binary search for sigmas of conditional Gaussians.","19","","25","    affinities : array-like, shape (n_samples, k)","26","        Distances between training samples and its k nearest neighbors.","28","    neighbors : array-like, shape (n_samples, k) or None","29","        Each row contains the indices to the k nearest neigbors. If this","48","    # Precisions of conditional Gaussian distributions","53","","54","    # Use log scale","61","    cdef long i, j, k, l","62","    cdef long n_neighbors = n_samples","66","        n_neighbors = neighbors.shape[1]","67","","68","    # This array is later used as a 32bit array. It has multiple intermediate","69","    # floating point additions that benefit from the extra precision","70","    cdef np.ndarray[np.float64_t, ndim=2] P = np.zeros(","71","        (n_samples, n_neighbors), dtype=np.float64)","84","            for j in range(n_neighbors):","85","                if j != i or using_neighbors:","86","                    P[i, j] = math.exp(-affinities[i, j] * beta)","88","","92","","93","            for j in range(n_neighbors):","94","                P[i, j] \/= sum_Pi","95","                sum_disti_Pi += affinities[i, j] * P[i, j]","96",""],"delete":["14","        np.ndarray[np.float32_t, ndim=2] affinities, ","15","        np.ndarray[np.int64_t, ndim=2] neighbors, ","18","    \"\"\"Binary search for sigmas of conditional Gaussians. ","19","    ","25","    affinities : array-like, shape (n_samples, n_samples)","26","        Distances between training samples.","28","    neighbors : array-like, shape (n_samples, K) or None","29","        Each row contains the indices to the K nearest neigbors. If this","48","    # This array is later used as a 32bit array. It has multiple intermediate","49","    # floating point additions that benefit from the extra precision","50","    cdef np.ndarray[np.float64_t, ndim=2] P = np.zeros((n_samples, n_samples),","51","                                                       dtype=np.float64)","52","    # Precisions of conditional Gaussian distrubutions","57","    # Now we go to log scale","64","    cdef long i, j, k, l = 0","65","    cdef long K = n_samples","69","        K = neighbors.shape[1]","81","            if using_neighbors:","82","                for k in range(K):","83","                    j = neighbors[i, k]","84","                    P[i, j] = math.exp(-affinities[i, j] * beta)","85","            else:","86","                for j in range(K):","87","                    P[i, j] = math.exp(-affinities[i, j] * beta)","88","            P[i, i] = 0.0","90","            if using_neighbors:","91","                for k in range(K):","92","                    j = neighbors[i, k]","94","            else:","95","                for j in range(K):","96","                    sum_Pi += P[i, j]","100","            if using_neighbors:","101","                for k in range(K):","102","                    j = neighbors[i, k]","103","                    P[i, j] \/= sum_Pi","104","                    sum_disti_Pi += affinities[i, j] * P[i, j]","105","            else:","106","                for j in range(K):","107","                    P[i, j] \/= sum_Pi","108","                    sum_disti_Pi += affinities[i, j] * P[i, j]"]}],"sklearn\/neighbors\/quad_tree.pxd":[{"add":[],"delete":[]}],"doc\/whats_new.rst":[{"add":["21","   * :class:`sklearn.manifold.TSNE` (bug fix)","248","   - Memory improvements for method barnes_hut in :class:`manifold.TSNE`","249","     :issue:`7089` by :user:`Thomas Moreau <tomMoral>` and `Olivier Grisel`_.","250","","251","   - Optimization schedule improvements for so the results are closer to the","252","     one from the reference implementation","253","     `lvdmaaten\/bhtsne <https:\/\/github.com\/lvdmaaten\/bhtsne>`_ by","254","     :user:`Thomas Moreau <tomMoral>` and `Olivier Grisel`_.","255","","489","   - Fixed the implementation of :class:`manifold.TSNE`:","490","      - ``early_exageration`` parameter had no effect and is now used for the","491","        first 250 optimization iterations.","492","      - Fixed the ``InsersionError`` reported in :issue:`8992`.","493","      - Improve the learning schedule to match the one from the reference","494","        implementation `lvdmaaten\/bhtsne <https:\/\/github.com\/lvdmaaten\/bhtsne>`_.","495","     by :user:`Thomas Moreau <tomMoral>` and `Olivier Grisel`_.","496",""],"delete":[]}],"benchmarks\/bench_tsne_mnist.py":[{"add":[],"delete":[]}],"sklearn\/manifold\/setup.py":[{"add":["33",""],"delete":[]}],"sklearn\/manifold\/_barnes_hut_tsne.pyx":[{"add":["14","cimport numpy as np","15","","16","from sklearn.neighbors import quad_tree","17","from sklearn.neighbors cimport quad_tree","24","# Smallest strictly positive value that can be represented by floating","25","# point numbers for different precision levels. This is useful to avoid","26","# taking the log of zero when computing the KL divergence.","27","cdef float FLOAT32_TINY = np.finfo(np.float32).tiny","28","","29","# Useful to void division by zero or divergence to +inf.","30","cdef float FLOAT64_EPS = np.finfo(np.float64).eps","45","cdef float compute_gradient(float[:] val_P,","46","                            float[:, :] pos_reference,","47","                            np.int64_t[:] neighbors,","48","                            np.int64_t[:] indptr,","49","                            float[:, :] tot_force,","50","                            quad_tree._QuadTree qt,","57","    cdef:","58","        long i, coord","59","        int ax","60","        long n_samples = pos_reference.shape[0]","61","        int n_dimensions = qt.n_dimensions","62","        double[1] sum_Q","63","        clock_t t1, t2","64","        float sQ, error","65","","66","    if qt.verbose > 11:","67","        printf(\"[t-SNE] Allocating %li elements in force arrays\\n\",","68","                n_samples * n_dimensions * 2)","69","    cdef float* neg_f = <float*> malloc(sizeof(float) * n_samples * n_dimensions)","70","    cdef float* pos_f = <float*> malloc(sizeof(float) * n_samples * n_dimensions)","74","    compute_gradient_negative(pos_reference, neg_f, qt, sum_Q,","77","    if qt.verbose > 15:","81","    error = compute_gradient_positive(val_P, pos_reference, neighbors, indptr,","82","                                      pos_f, n_dimensions, dof, sQ, start,","83","                                      qt.verbose)","85","    if qt.verbose > 15:","87","    for i in range(start, n_samples):","90","            tot_force[i, ax] = pos_f[coord] - (neg_f[coord] \/ sQ)","91","","94","    return error","97","cdef float compute_gradient_positive(float[:] val_P,","98","                                     float[:, :] pos_reference,","99","                                     np.int64_t[:] neighbors,","100","                                     np.int64_t[:] indptr,","104","                                     double sum_Q,","115","        long n_samples = indptr.shape[0] - 1","116","        float dij, qij, pij","119","        float[3] buff","120","        clock_t t1, t2","121","","123","    for i in range(start, n_samples):","124","        # Init the gradient vector","127","        # Compute the positive interaction for the nearest neighbors","128","        for k in range(indptr[i], indptr[i+1]):","129","            j = neighbors[k]","130","            dij = 0.0","131","            pij = val_P[k]","134","                dij += buff[ax] * buff[ax]","135","            qij = (((1.0 + dij) \/ dof) ** exponent)","136","            dij = pij * qij","137","            qij \/= sum_Q","138","            C += pij * log(max(pij, FLOAT32_TINY)","139","                           \/ max(qij, FLOAT32_TINY))","141","                pos_f[i * n_dimensions + ax] += dij * buff[ax]","149","cdef void compute_gradient_negative(float[:, :] pos_reference,","151","                                    quad_tree._QuadTree qt,","152","                                    double* sum_Q,","154","                                    float theta,","155","                                    long start,","158","        stop = pos_reference.shape[0]","161","        int n_dimensions = qt.n_dimensions","162","        long i, j, idx","166","        long offset = n_dimensions + 2","167","        long* l","168","        float size, dist2s, mult","169","        double qijZ","170","        float[1] iQ","171","        float[3] force, neg_force, pos","174","    summary = <float*> malloc(sizeof(float) * n * offset)","186","        idx = qt.summarize(pos, summary, theta*theta)","190","        # is about 10-15x more expensive than the","193","        for j in range(idx \/\/ offset):","194","","195","            dist2s = summary[j * offset + n_dimensions]","196","            size = summary[j * offset + n_dimensions + 1]","197","            qijZ = ((1.0 + dist2s) \/ dof) ** exponent  # 1\/(1+dist)","198","            sum_Q[0] += size * qijZ   # size of the node * q","199","            mult = size * qijZ * qijZ","201","                neg_force[ax] += mult * summary[j * offset + ax]","207","    if qt.verbose > 20:","208","        printf(\"[t-SNE] Tree: %li clock ticks | \", dta)","209","        printf(\"Force computation: %li clock ticks\\n\", dtb)","210","","211","    # Put sum_Q to machine EPSILON to avoid divisions by 0","212","    sum_Q[0] = max(sum_Q[0], FLOAT64_EPS)","213","    free(summary)","216","def gradient(float[:] val_P,","217","             float[:, :] pos_output,","218","             np.int64_t[:] neighbors,","219","             np.int64_t[:] indptr,","220","             float[:, :] forces,","231","    assert val_P.itemsize == 4","237","    assert n == indptr.shape[0] - 1, m","240","    cdef quad_tree._QuadTree qt = quad_tree._QuadTree(pos_output.shape[1],","241","                                                      verbose)","243","        printf(\"[t-SNE] Inserting %li points\\n\", pos_output.shape[0])","244","    qt.build_tree(pos_output)","250","    C = compute_gradient(val_P, pos_output, neighbors, indptr, forces,","251","                         qt, theta, dof, skip_num_points, -1)","258","    assert qt.cells[0].cumulative_size == qt.n_points, m"],"delete":["13","cimport numpy as np","21","# Round points differing by less than this amount","22","# effectively ignoring differences near the 32bit ","23","# floating point precision","24","cdef float EPSILON = 1e-6","39","cdef extern from \"cblas.h\":","40","    float snrm2 \"cblas_snrm2\"(int N, float *X, int incX) nogil","41","","42","","43","cdef struct Node:","44","    # Keep track of the center of mass","45","    float* barycenter","46","    # If this is a leaf, the position of the point within this leaf ","47","    float* leaf_point_position","48","    # The number of points including all ","49","    # nodes below this one","50","    long cumulative_size","51","    # Number of points at this node","52","    long size","53","    # Index of the point at this node","54","    # Only defined for non-empty leaf nodes","55","    long point_index","56","    # level = 0 is the root node","57","    # And each subdivision adds 1 to the level","58","    long level","59","    # Left edge of this node","60","    float* left_edge","61","    # The center of this node, equal to le + w\/2.0","62","    float* center","63","    # The width of this node -- used to calculate the opening","64","    # angle. Equal to width = re - le","65","    float* width","66","    # The value of the maximum width w","67","    float max_width","68","","69","    # Does this node have children?","70","    # Default to leaf until we add points","71","    int is_leaf","72","    # Array of pointers to pointers of children","73","    Node **children","74","    # Keep a pointer to the parent","75","    Node *parent","76","    # Pointer to the tree this node belongs too","77","    Tree* tree","78","","79","cdef struct Tree:","80","    # Holds a pointer to the root node","81","    Node* root_node ","82","    # Number of dimensions in the output","83","    int n_dimensions","84","    # Total number of cells","85","    long n_cells","86","    # Total number of points","87","    long n_points","88","    # Spit out diagnostic information?","89","    int verbose","90","    # How many cells per node? Should go as 2 ** n_dimensionss","91","    int n_cell_per_node","92","","93","cdef Tree* init_tree(float[:] left_edge, float[:] width, int n_dimensions, ","94","                     int verbose) nogil:","95","    # tree is freed by free_tree","96","    cdef Tree* tree = <Tree*> malloc(sizeof(Tree))","97","    tree.n_dimensions = n_dimensions","98","    tree.n_cells = 0","99","    tree.n_points = 0","100","    tree.verbose = verbose","101","    tree.root_node = create_root(left_edge, width, n_dimensions)","102","    tree.root_node.tree = tree","103","    tree.n_cells += 1","104","    tree.n_cell_per_node = 2 ** n_dimensions","105","    if DEBUGFLAG:","106","        printf(\"[t-SNE] Tree initialised. Left_edge = (%1.9e, %1.9e, %1.9e)\\n\",","107","               left_edge[0], left_edge[1], left_edge[2])","108","        printf(\"[t-SNE] Tree initialised. Width = (%1.9e, %1.9e, %1.9e)\\n\",","109","                width[0], width[1], width[2])","110","    return tree","111","","112","cdef Node* create_root(float[:] left_edge, float[:] width, int n_dimensions) nogil:","113","    # Create a default root node","114","    cdef int ax","115","    cdef int n_cell_per_node = 2 ** n_dimensions","116","    # root is freed by free_tree","117","    root = <Node*> malloc(sizeof(Node))","118","    root.is_leaf = 1","119","    root.parent = NULL","120","    root.level = 0","121","    root.cumulative_size = 0","122","    root.size = 0","123","    root.point_index = -1","124","    root.max_width = 0.0","125","    root.width = <float*> malloc(sizeof(float) * n_dimensions)","126","    root.left_edge = <float*> malloc(sizeof(float) * n_dimensions)","127","    root.center = <float*> malloc(sizeof(float) * n_dimensions)","128","    root.barycenter = <float*> malloc(sizeof(float) * n_dimensions)","129","    root.leaf_point_position= <float*> malloc(sizeof(float) * n_dimensions)","130","    root.children = NULL","131","    for ax in range(n_dimensions):","132","        root.width[ax] = width[ax]","133","        root.left_edge[ax] = left_edge[ax]","134","        root.center[ax] = 0.0","135","        root.barycenter[ax] = 0.","136","        root.leaf_point_position[ax] = -1","137","    for ax in range(n_dimensions):","138","        root.max_width = max(root.max_width, root.width[ax])","139","    if DEBUGFLAG:","140","        printf(\"[t-SNE] Created root node %p\\n\", root)","141","    return root","142","","143","cdef Node* create_child(Node *parent, int[3] offset) nogil:","144","    # Create a new child node with default parameters","145","    cdef int ax","146","    # these children are freed by free_recursive","147","    child = <Node *> malloc(sizeof(Node))","148","    child.is_leaf = 1","149","    child.parent = parent","150","    child.level = parent.level + 1","151","    child.size = 0","152","    child.cumulative_size = 0","153","    child.point_index = -1","154","    child.tree = parent.tree","155","    child.max_width = 0.0","156","    child.width = <float*> malloc(sizeof(float) * parent.tree.n_dimensions)","157","    child.left_edge = <float*> malloc(sizeof(float) * parent.tree.n_dimensions)","158","    child.center = <float*> malloc(sizeof(float) * parent.tree.n_dimensions)","159","    child.barycenter = <float*> malloc(sizeof(float) * parent.tree.n_dimensions)","160","    child.leaf_point_position = <float*> malloc(sizeof(float) * parent.tree.n_dimensions)","161","    child.children = NULL","162","    for ax in range(parent.tree.n_dimensions):","163","        child.width[ax] = parent.width[ax] \/ 2.0","164","        child.left_edge[ax] = parent.left_edge[ax] + offset[ax] * parent.width[ax] \/ 2.0","165","        child.center[ax] = child.left_edge[ax] + child.width[ax] \/ 2.0","166","        child.barycenter[ax] = 0.","167","        child.leaf_point_position[ax] = -1.","168","    for ax in range(parent.tree.n_dimensions):","169","        child.max_width = max(child.max_width, child.width[ax])","170","    child.tree.n_cells += 1","171","    return child","172","","173","cdef Node* select_child(Node *node, float[3] pos, long index) nogil:","174","    # Find which sub-node a position should go into","175","    # And return the appropriate node","176","    cdef int* offset = <int*> malloc(sizeof(int) * node.tree.n_dimensions)","177","    cdef int ax, idx","178","    cdef Node* child","179","    cdef int error","180","    for ax in range(node.tree.n_dimensions):","181","        offset[ax] = (pos[ax] - (node.left_edge[ax] + node.width[ax] \/ 2.0)) > 0.","182","    idx = offset2index(offset, node.tree.n_dimensions)","183","    child = node.children[idx]","184","    if DEBUGFLAG:","185","        printf(\"[t-SNE] Offset [%i, %i] with LE [%f, %f]\\n\",","186","               offset[0], offset[1], child.left_edge[0], child.left_edge[1])","187","    free(offset)","188","    return child","189","","190","","191","cdef inline void index2offset(int* offset, int index, int n_dimensions) nogil:","192","    # Convert a 1D index into N-D index; useful for indexing","193","    # children of a quadtree, octree, N-tree","194","    # Quite likely there's a fancy bitshift way of doing this","195","    # since the offset is equivalent to the binary representation","196","    # of the integer index","197","    # We read the offset array left-to-right","198","    # such that the least significat bit is on the right","199","    cdef int rem, k, shift","200","    for k in range(n_dimensions):","201","        shift = n_dimensions -k -1","202","        rem = ((index >> shift) << shift)","203","        offset[k] = rem > 0","204","        if DEBUGFLAG:","205","            printf(\"i2o index %i k %i rem %i offset\", index, k, rem)","206","            for j in range(n_dimensions):","207","                printf(\" %i\", offset[j])","208","            printf(\" n_dimensions %i\\n\", n_dimensions)","209","        index -= rem","210","","211","","212","cdef inline int offset2index(int* offset, int n_dimensions) nogil:","213","    # Calculate the 1:1 index for a given offset array","214","    # We read the offset array right-to-left","215","    # such that the least significat bit is on the right","216","    cdef int dim","217","    cdef int index = 0","218","    for dim in range(n_dimensions):","219","        index += (2 ** dim) * offset[n_dimensions - dim - 1]","220","        if DEBUGFLAG:","221","            printf(\"o2i index %i dim %i            offset\", index, dim)","222","            for j in range(n_dimensions):","223","                printf(\" %i\", offset[j])","224","            printf(\" n_dimensions %i\\n\", n_dimensions)","225","    return index","226","","227","","228","cdef void subdivide(Node* node) nogil:","229","    # This instantiates 2**n_dimensions = n_cell_per_node nodes for the current node","230","    cdef int idx = 0","231","    cdef int* offset = <int*> malloc(sizeof(int) * node.tree.n_dimensions)","232","    node.is_leaf = False","233","    node.children = <Node**> malloc(sizeof(Node*) * node.tree.n_cell_per_node)","234","    for idx in range(node.tree.n_cell_per_node):","235","        index2offset(offset, idx, node.tree.n_dimensions)","236","        node.children[idx] = create_child(node, offset)","237","    free(offset)","238","","239","","240","cdef int insert(Node *root, float pos[3], long point_index, long depth, long","241","        duplicate_count) nogil:","242","    # Introduce a new point into the tree","243","    # by recursively inserting it and subdividng as necessary","244","    # Carefully treat the case of identical points at the same node","245","    # by increasing the root.size and tracking duplicate_count","246","    cdef Node *child","247","    cdef long i","248","    cdef int ax","249","    cdef int not_identical = 1","250","    cdef int n_dimensions = root.tree.n_dimensions","251","    if DEBUGFLAG:","252","        printf(\"[t-SNE] [d=%i] Inserting pos %i [%f, %f] duplicate_count=%i \"","253","                \"into child %p\\n\", depth, point_index, pos[0], pos[1],","254","                duplicate_count, root)    ","255","    # Increment the total number points including this","256","    # node and below it","257","    root.cumulative_size += duplicate_count","258","    # Evaluate the new center of mass, weighting the previous","259","    # center of mass against the new point data","260","    cdef double frac_seen = <double>(root.cumulative_size - 1) \/ (<double>","261","            root.cumulative_size)","262","    cdef double frac_new  = 1.0 \/ <double> root.cumulative_size","263","    # Assert that duplicate_count > 0","264","    if duplicate_count < 1:","265","        return -1","266","    # Assert that the point is inside the left & right edges","267","    for ax in range(n_dimensions):","268","        root.barycenter[ax] *= frac_seen","269","        if (pos[ax] > (root.left_edge[ax] + root.width[ax] + EPSILON)):","270","            printf(\"[t-SNE] Error: point (%1.9e) is above right edge of node \"","271","                    \"(%1.9e)\\n\", pos[ax], root.left_edge[ax] + root.width[ax])","272","            return -1","273","        if (pos[ax] < root.left_edge[ax] - EPSILON):","274","            printf(\"[t-SNE] Error: point (%1.9e) is below left edge of node \"","275","                   \"(%1.9e)\\n\", pos[ax], root.left_edge[ax])","276","            return -1","277","    for ax in range(n_dimensions):","278","        root.barycenter[ax] += pos[ax] * frac_new","279","","280","    # If this node is unoccupied, fill it.","281","    # Otherwise, we need to insert recursively.","282","    # Two insertion scenarios: ","283","    # 1) Insert into this node if it is a leaf and empty","284","    # 2) Subdivide this node if it is currently occupied","285","    if (root.size == 0) & root.is_leaf:","286","        # Root node is empty and a leaf","287","        if DEBUGFLAG:","288","            printf(\"[t-SNE] [d=%i] Inserting [%f, %f] into blank cell\\n\", depth,","289","                   pos[0], pos[1])","290","        for ax in range(n_dimensions):","291","            root.leaf_point_position[ax] = pos[ax]","292","        root.point_index = point_index","293","        root.size = duplicate_count","294","        return 0","295","    else:","296","        # Root node is occupied or not a leaf","297","        if DEBUGFLAG:","298","            printf(\"[t-SNE] [d=%i] Node %p is occupied or is a leaf.\\n\", depth,","299","                    root)","300","            printf(\"[t-SNE] [d=%i] Node %p leaf = %i. Size %i\\n\", depth, root,","301","                    root.is_leaf, root.size)","302","        if root.is_leaf & (root.size > 0):","303","            # is a leaf node and is occupied","304","            for ax in range(n_dimensions):","305","                not_identical &= (fabsf(pos[ax] - root.leaf_point_position[ax]) < EPSILON)","306","                not_identical &= (root.point_index != point_index)","307","            if not_identical == 1:","308","                root.size += duplicate_count","309","                if DEBUGFLAG:","310","                    printf(\"[t-SNE] Warning: [d=%i] Detected identical \"","311","                            \"points. Returning. Leaf now has size %i\\n\",","312","                            depth, root.size)","313","                return 0","314","        # If necessary, subdivide this node before","315","        # descending","316","        if root.is_leaf:","317","            if DEBUGFLAG:","318","                printf(\"[t-SNE] [d=%i] Subdividing this leaf node %p\\n\", depth,","319","                        root)","320","            subdivide(root)","321","        # We have two points to relocate: the one previously","322","        # at this node, and the new one we're attempting","323","        # to insert","324","        if root.size > 0:","325","            child = select_child(root, root.leaf_point_position, root.point_index)","326","            if DEBUGFLAG:","327","                printf(\"[t-SNE] [d=%i] Relocating old point to node %p\\n\",","328","                        depth, child)","329","            insert(child, root.leaf_point_position, root.point_index, depth + 1, root.size)","330","        # Insert the new point","331","        if DEBUGFLAG:","332","            printf(\"[t-SNE] [d=%i] Selecting node for new point\\n\", depth)","333","        child = select_child(root, pos, point_index)","334","        if root.size > 0:","335","            # Remove the point from this node","336","            for ax in range(n_dimensions):","337","                root.leaf_point_position[ax] = -1            ","338","            root.size = 0","339","            root.point_index = -1            ","340","        return insert(child, pos, point_index, depth + 1, 1)","341","","342","cdef int insert_many(Tree* tree, float[:,:] pos_array) nogil:","343","    # Insert each data point into the tree one at a time","344","    cdef long nrows = pos_array.shape[0]","345","    cdef long i","346","    cdef int ax","347","    cdef float row[3]","348","    cdef long err = 0","349","    for i in range(nrows):","350","        for ax in range(tree.n_dimensions):","351","            row[ax] = pos_array[i, ax]","352","        if DEBUGFLAG:","353","            printf(\"[t-SNE] inserting point %i: [%f, %f]\\n\", i, row[0], row[1])","354","        err = insert(tree.root_node, row, i, 0, 1)","355","        if err != 0:","356","            printf(\"[t-SNE] ERROR\\n%s\", EMPTY_STRING)","357","            return err","358","        tree.n_points += 1","359","    return err","360","","361","cdef int free_tree(Tree* tree) nogil:","362","    cdef int check","363","    cdef long* cnt = <long*> malloc(sizeof(long) * 3)","364","    for i in range(3):","365","        cnt[i] = 0","366","    free_recursive(tree, tree.root_node, cnt)","367","    check = cnt[0] == tree.n_cells","368","    check &= cnt[2] == tree.n_points","369","    free(tree)","370","    free(cnt)","371","    return check","372","","373","cdef void free_post_children(Node *node) nogil:","374","    free(node.width)","375","    free(node.left_edge)","376","    free(node.center)","377","    free(node.barycenter)","378","    free(node.leaf_point_position)","379","    free(node)","380","","381","cdef void free_recursive(Tree* tree, Node *root, long* counts) nogil:","382","    # Free up all of the tree nodes recursively","383","    # while counting the number of nodes visited","384","    # and total number of data points removed","385","    cdef int idx","386","    cdef Node* child","387","    if not root.is_leaf:","388","        for idx in range(tree.n_cell_per_node):","389","            child = root.children[idx]","390","            free_recursive(tree, child, counts)","391","            counts[0] += 1","392","            if child.is_leaf:","393","                counts[1] += 1","394","                if child.size > 0:","395","                    counts[2] +=1","396","            else:","397","                free(child.children)","398","","399","            free_post_children(child)","400","","401","    if root == tree.root_node:","402","        if not root.is_leaf:","403","            free(root.children)","404","","405","        free_post_children(root)","406","","407","cdef long count_points(Node* root, long count) nogil:","408","    # Walk through the whole tree and count the number ","409","    # of points at the leaf nodes","410","    if DEBUGFLAG:","411","        printf(\"[t-SNE] Counting nodes at root node %p\\n\", root)","412","    cdef Node* child","413","    cdef int idx","414","    if root.is_leaf:","415","        count += root.size","416","        if DEBUGFLAG : ","417","            printf(\"[t-SNE] %p is a leaf node, no children\\n\", root)","418","            printf(\"[t-SNE] %i points in node %p\\n\", count, root)","419","        return count","420","    # Otherwise, get the children","421","    for idx in range(root.tree.n_cell_per_node):","422","        child = root.children[idx]","423","        if DEBUGFLAG:","424","            printf(\"[t-SNE] Counting points for child %p\\n\", child)","425","        if child.is_leaf and child.size > 0:","426","            if DEBUGFLAG:","427","                printf(\"[t-SNE] Child has size %d\\n\", child.size)","428","            count += child.size","429","        elif not child.is_leaf:","430","            if DEBUGFLAG:","431","                printf(\"[t-SNE] Child is not a leaf. Descending\\n%s\", EMPTY_STRING)","432","            count = count_points(child, count)","433","        # else case is we have an empty leaf node","434","        # which happens when we create a quadtree for","435","        # one point, and then the other neighboring cells","436","        # don't get filled in","437","    if DEBUGFLAG:","438","        printf(\"[t-SNE] %i points in this node\\n\", count)","439","    return count","440","","441","","442","cdef float compute_gradient(float[:,:] val_P,","443","                            float[:,:] pos_reference,","444","                            np.int64_t[:,:] neighbors,","445","                            float[:,:] tot_force,","446","                            Node* root_node,","453","    cdef long i, coord","454","    cdef int ax","455","    cdef long n = pos_reference.shape[0]","456","    cdef int n_dimensions = root_node.tree.n_dimensions","457","    if root_node.tree.verbose > 11:","458","        printf(\"[t-SNE] Allocating %i elements in force arrays\\n\",","459","                n * n_dimensions * 2)","460","    cdef float* sum_Q = <float*> malloc(sizeof(float))","461","    cdef float* neg_f = <float*> malloc(sizeof(float) * n * n_dimensions)","462","    cdef float* neg_f_fast = <float*> malloc(sizeof(float) * n * n_dimensions)","463","    cdef float* pos_f = <float*> malloc(sizeof(float) * n * n_dimensions)","464","    cdef clock_t t1, t2","465","    cdef float sQ, error","469","    compute_gradient_negative(val_P, pos_reference, neg_f, root_node, sum_Q,","472","    if root_node.tree.verbose > 15:","476","    error = compute_gradient_positive(val_P, pos_reference, neighbors, pos_f,","477","                              n_dimensions, dof, sQ, start, root_node.tree.verbose)","479","    if root_node.tree.verbose > 15:","481","    for i in range(start, n):","484","            tot_force[i, ax] = pos_f[coord] - (neg_f[coord] \/ sum_Q[0])","485","    free(sum_Q)","487","    free(neg_f_fast)","489","    return sQ","492","cdef float compute_gradient_positive(float[:,:] val_P,","493","                                     float[:,:] pos_reference,","494","                                     np.int64_t[:,:] neighbors,","498","                                     float sum_Q,","509","        long K = neighbors.shape[1]","510","        long n = val_P.shape[0]","511","        float[3] buff","512","        float D, Q, pij","515","    cdef clock_t t1, t2","517","    for i in range(start, n):","520","        for k in range(K):","521","            j = neighbors[i, k]","522","            # we don't need to exclude the i==j case since we've ","523","            # already thrown it out from the list of neighbors","524","            D = 0.0","525","            Q = 0.0","526","            pij = val_P[i, j]","529","                D += buff[ax] ** 2.0  ","530","            Q = (((1.0 + D) \/ dof) ** exponent)","531","            D = pij * Q","532","            Q \/= sum_Q","533","            C += pij * log((pij + EPSILON) \/ (Q + EPSILON))","535","                pos_f[i * n_dimensions + ax] += D * buff[ax]","543","","544","cdef void compute_gradient_negative(float[:,:] val_P, ","545","                                    float[:,:] pos_reference,","547","                                    Node *root_node,","548","                                    float* sum_Q,","550","                                    float theta, ","551","                                    long start, ","554","        stop = pos_reference.shape[0] ","557","        long i, j","559","        float* force","560","        float* iQ ","561","        float* pos","562","        float* dist2s","563","        long* sizes","564","        float* deltas","565","        long* l","566","        int n_dimensions = root_node.tree.n_dimensions","567","        float qijZ, mult","568","        long idx, ","572","        float* neg_force","574","    iQ = <float*> malloc(sizeof(float))","575","    force = <float*> malloc(sizeof(float) * n_dimensions)","576","    pos = <float*> malloc(sizeof(float) * n_dimensions)","577","    dist2s = <float*> malloc(sizeof(float) * n)","578","    sizes = <long*> malloc(sizeof(long) * n)","579","    deltas = <float*> malloc(sizeof(float) * n * n_dimensions)","580","    l = <long*> malloc(sizeof(long))","581","    neg_force= <float*> malloc(sizeof(float) * n_dimensions)","590","        l[0] = 0","594","        compute_non_edge_forces(root_node, theta, i, pos, force, dist2s,","595","                                     sizes, deltas, l)","599","        # is about 10-15x more expensive than the ","602","        for j in range(l[0]):","603","            qijZ = ((1.0 + dist2s[j]) \/ dof) ** exponent","604","            sum_Q[0] += sizes[j] * qijZ","605","            mult = sizes[j] * qijZ * qijZ","607","                idx = j * n_dimensions + ax","608","                neg_force[ax] += mult * deltas[idx]","614","    if root_node.tree.verbose > 20:","615","        printf(\"[t-SNE] Tree: %i clock ticks | \", dta)","616","        printf(\"Force computation: %i clock ticks\\n\", dtb)","617","    free(iQ)","618","    free(force)","619","    free(pos)","620","    free(dist2s)","621","    free(sizes)","622","    free(deltas)","623","    free(l)","624","    free(neg_force)","627","cdef void compute_non_edge_forces(Node* node, ","628","                                  float theta,","629","                                  long point_index,","630","                                  float* pos,","631","                                  float* force,","632","                                  float* dist2s,","633","                                  long* sizes,","634","                                  float* deltas,","635","                                  long* l) nogil:","636","    # Compute the t-SNE force on the point in pos given by point_index","637","    cdef:","638","        Node* child","639","        int i, j","640","        int n_dimensions = node.tree.n_dimensions","641","        long idx, idx1","642","        float dist_check","643","    ","644","    # There are no points below this node if cumulative_size == 0","645","    # so do not bother to calculate any force contributions","646","    # Also do not compute self-interactions","647","    if node.cumulative_size > 0 and not (node.is_leaf and (node.point_index ==","648","        point_index)):","649","        # Compute distance between node center of mass and the reference point","650","        # I've tried rewriting this in terms of BLAS functions, but it's about","651","        # 1.5x worse when we do so, probbaly because the vectors are small","652","        idx1 = l[0] * n_dimensions","653","        deltas[idx1] = pos[0] - node.barycenter[0]","654","        idx = idx1","655","        for i in range(1, n_dimensions):","656","            idx += 1","657","            deltas[idx] = pos[i] - node.barycenter[i] ","658","        # do np.sqrt(np.sum(deltas**2.0))","659","        dist2s[l[0]] = snrm2(n_dimensions, &deltas[idx1], 1)","660","        # Check whether we can use this node as a summary","661","        # It's a summary node if the angular size as measured from the point","662","        # is relatively small (w.r.t. to theta) or if it is a leaf node.","663","        # If it can be summarized, we use the cell center of mass ","664","        # Otherwise, we go a higher level of resolution and into the leaves.","665","        if node.is_leaf or ((node.max_width \/ dist2s[l[0]]) < theta):","666","            # Compute the t-SNE force between the reference point and the","667","            # current node","668","            sizes[l[0]] = node.cumulative_size","669","            dist2s[l[0]] = dist2s[l[0]] * dist2s[l[0]]","670","            l[0] += 1","671","        else:","672","            # Recursively apply Barnes-Hut to child nodes","673","            for idx in range(node.tree.n_cell_per_node):","674","                child = node.children[idx]","675","                if child.cumulative_size == 0: ","676","                    continue","677","                compute_non_edge_forces(child, theta,","678","                        point_index, pos, force, dist2s, sizes, deltas,","679","                        l)","680","","681","","682","cdef float compute_error(float[:, :] val_P,","683","                        float[:, :] pos_reference,","684","                        np.int64_t[:,:] neighbors,","685","                        float sum_Q,","686","                        int n_dimensions,","687","                        int verbose) nogil:","688","    cdef int i, j, ax","689","    cdef int I = neighbors.shape[0]","690","    cdef int K = neighbors.shape[1]","691","    cdef float pij, Q","692","    cdef float C = 0.0","693","    cdef clock_t t1, t2","694","    cdef float dt, delta","695","    t1 = clock()","696","    for i in range(I):","697","        for k in range(K):","698","            j = neighbors[i, k]","699","            pij = val_P[i, j]","700","            Q = 0.0","701","            for ax in range(n_dimensions):","702","                delta = (pos_reference[i, ax] - pos_reference[j, ax])","703","                Q += delta * delta","704","            Q = (1.0 \/ (sum_Q + Q * sum_Q))","705","            C += pij * log((pij + EPSILON) \/ (Q + EPSILON))","706","    t2 = clock()","707","    dt = ((float) (t2 - t1))","708","    if verbose > 10:","709","        printf(\"[t-SNE] Computed error=%1.4f in %1.1e ticks\\n\", C, dt)","710","    return C","711","","712","","713","def calculate_edge(pos_output):","714","    # Make the boundaries slightly outside of the data","715","    # to avoid floating point error near the edge","716","    left_edge = np.min(pos_output, axis=0)","717","    right_edge = np.max(pos_output, axis=0) ","718","    center = (right_edge + left_edge) * 0.5","719","    width = np.maximum(np.subtract(right_edge, left_edge), EPSILON)","720","    # Exagerate width to avoid boundary edge","721","    width = width.astype(np.float32) * 1.001","722","    left_edge = center - width \/ 2.0","723","    right_edge = center + width \/ 2.0","724","    return left_edge, right_edge, width","725","","726","def gradient(float[:,:] pij_input, ","727","             float[:,:] pos_output, ","728","             np.int64_t[:,:] neighbors, ","729","             float[:,:] forces, ","740","    left_edge, right_edge, width = calculate_edge(pos_output)","741","    assert width.itemsize == 4","742","    assert pij_input.itemsize == 4","745","    m = \"Number of neighbors must be < # of points - 1\"","746","    assert n - 1 >= neighbors.shape[1], m","747","    m = \"neighbors array and pos_output shapes are incompatible\"","748","    assert n == neighbors.shape[0], m","752","    assert n == pij_input.shape[0], m","753","    m = \"Pij and pos_output shapes are incompatible\"","754","    assert n == pij_input.shape[1], m","757","    cdef Tree* qt = init_tree(left_edge, width, n_dimensions, verbose)","759","        printf(\"[t-SNE] Inserting %i points\\n\", pos_output.shape[0])","760","    err = insert_many(qt, pos_output)","761","    assert err == 0, \"[t-SNE] Insertion failed\"","767","    sum_Q = compute_gradient(pij_input, pos_output, neighbors, forces,","768","                             qt.root_node, theta, dof, skip_num_points, -1)","769","    C = compute_error(pij_input, pos_output, neighbors, sum_Q, n_dimensions,","770","                      verbose)","776","    cdef long count = count_points(qt.root_node, 0)","777","    m = (\"Tree consistency failed: unexpected number of points=%i \"","778","         \"at root node=%i\" % (count, qt.root_node.cumulative_size))","779","    assert count == qt.root_node.cumulative_size, m ","781","    assert count == qt.n_points, m","782","    free_tree(qt)","784","","785","","786","# Helper functions","787","def check_quadtree(X, np.int64_t[:] counts):","788","    \"\"\"","789","    Helper function to access quadtree functions for testing","790","    \"\"\"","791","    ","792","    X = X.astype(np.float32)","793","    left_edge, right_edge, width = calculate_edge(X)","794","    # Initialise a tree","795","    qt = init_tree(left_edge, width, 2, 2)","796","    # Insert data into the tree","797","    insert_many(qt, X)","798","","799","    cdef long count = count_points(qt.root_node, 0)","800","    counts[0] = count","801","    counts[1] = qt.root_node.cumulative_size","802","    counts[2] = qt.n_points","803","    free_tree(qt)","804","    return counts","805","","806","","807","cdef int helper_test_index2offset(int* check, int index, int n_dimensions):","808","    cdef int* offset = <int*> malloc(sizeof(int) * n_dimensions)","809","    cdef int error_check = 1","810","    for i in range(n_dimensions):","811","        offset[i] = 0","812","    index2offset(offset, index, n_dimensions)","813","    for i in range(n_dimensions):","814","        error_check &= offset[i] == check[i]","815","    free(offset)","816","    return error_check","817","","818","","819","def test_index2offset():","820","    ret = 1","821","    ret &= helper_test_index2offset([1, 0, 1], 5, 3) == 1","822","    ret &= helper_test_index2offset([0, 0, 0], 0, 3) == 1","823","    ret &= helper_test_index2offset([0, 0, 1], 1, 3) == 1","824","    ret &= helper_test_index2offset([0, 1, 0], 2, 3) == 1","825","    ret &= helper_test_index2offset([0, 1, 1], 3, 3) == 1","826","    ret &= helper_test_index2offset([1, 0, 0], 4, 3) == 1","827","    return ret","828","","829","","830","def test_index_offset():","831","    cdef int n_dimensions, idx, tidx, k","832","    cdef int error_check = 1","833","    cdef int* offset ","834","    for n_dimensions in range(2, 10):","835","        offset = <int*> malloc(sizeof(int) * n_dimensions)","836","        for k in range(n_dimensions):","837","            offset[k] = 0","838","        for idx in range(2 ** n_dimensions):","839","            index2offset(offset, idx, n_dimensions)","840","            tidx = offset2index(offset, n_dimensions)","841","            error_check &= tidx == idx","842","            assert error_check == 1","843","        free(offset)","844","    return error_check"]}],"sklearn\/neighbors\/tests\/test_quad_tree.py":[{"add":[],"delete":[]}],"sklearn\/manifold\/t_sne.py":[{"add":["10","from time import time","16","from scipy.sparse import csr_matrix","17","from ..neighbors import NearestNeighbors","74","    distances : array, shape (n_samples, k)","75","        Distances of samples to its k nearest neighbors.","76","","77","    neighbors : array, shape (n_samples, k)","78","        Indices of the k nearest-neighbors for each samples.","88","    P : csr sparse matrix, shape (n_samples, n_samples)","89","        Condensed joint probability matrix with only nearest neighbors.","91","    t0 = time()","94","    n_samples, k = neighbors.shape","99","    assert np.all(np.isfinite(conditional_P)), \\","100","        \"All probabilities should be finite\"","101","","102","    # Symmetrize the joint probability distribution using sparse operations","103","    P = csr_matrix((conditional_P.ravel(), neighbors.ravel(),","104","                    range(0, n_samples * k + 1, k)),","105","                   shape=(n_samples, n_samples))","106","    P = P + P.T","107","","108","    # Normalize the joint probability distribution","109","    sum_P = np.maximum(P.sum(), MACHINE_EPSILON)","110","    P \/= sum_P","111","","112","    assert np.all(np.abs(P.data) <= 1.0)","113","    if verbose >= 2:","114","        duration = time() - t0","115","        print(\"[t-SNE] Computed conditional probabilities in {:.3f}s\"","116","              .format(duration))","159","    dist = pdist(X_embedded, \"sqeuclidean\")","160","    dist += 1.","161","    dist \/= degrees_of_freedom","162","    dist **= (degrees_of_freedom + 1.0) \/ -2.0","163","    Q = np.maximum(dist \/ (2.0 * np.sum(dist)), MACHINE_EPSILON)","169","    kl_divergence = 2.0 * np.dot(P, np.log(np.maximum(P, MACHINE_EPSILON) \/ Q))","172","    # pdist always returns double precision distances. Thus we need to take","173","    grad = np.ndarray((n_samples, n_components), dtype=params.dtype)","174","    PQd = squareform((P - Q) * dist)","176","        grad[i] = np.dot(np.ravel(PQd[i], order='K'),","177","                         X_embedded[i] - X_embedded)","185","def _kl_divergence_bh(params, P, degrees_of_freedom, n_samples, n_components,","186","                      angle=0.5, skip_num_points=0, verbose=False):","197","    P : csr sparse matrix, shape (n_samples, n_sample)","198","        Sparse approximate joint probability matrix, computed only for the","199","        k nearest-neighbors and symmetrized.","238","","239","    val_P = P.data.astype(np.float32, copy=False)","240","    neighbors = P.indices.astype(np.int64, copy=False)","241","    indptr = P.indptr.astype(np.int64, copy=False)","244","    error = _barnes_hut_tsne.gradient(val_P, X_embedded, neighbors, indptr,","254","def _gradient_descent(objective, p0, it, n_iter,","255","                      n_iter_check=1, n_iter_without_progress=300,","256","                      momentum=0.8, learning_rate=200.0, min_gain=0.01,","257","                      min_grad_norm=1e-7, verbose=0, args=None, kwargs=None):","282","    n_iter_without_progress : int, optional (default: 300)","286","    momentum : float, within (0.0, 1.0), optional (default: 0.8)","290","    learning_rate : float, optional (default: 200.0)","291","        The learning rate for t-SNE is usually in the range [10.0, 1000.0]. If","292","        the learning rate is too high, the data may look like a 'ball' with any","293","        point approximately equidistant from its nearest neighbours. If the","294","        learning rate is too low, most points may look compressed in a dense","295","        cloud with few outliers.","334","    best_iter = i = it","336","    tic = time()","338","        error, grad = objective(p, *args, **kwargs)","351","            toc = time()","352","            duration = toc - tic","353","            tic = toc","356","                print(\"[t-SNE] Iteration %d: error = %.7f,\"","357","                      \" gradient norm = %.7f\"","358","                      \" (%s iterations in %0.3fs)\"","359","                      % (i + 1, error, grad_norm, n_iter_check, duration))","472","    early_exaggeration : float, optional (default: 12.0)","481","    learning_rate : float, optional (default: 200.0)","482","        The learning rate for t-SNE is usually in the range [10.0, 1000.0]. If","483","        the learning rate is too high, the data may look like a 'ball' with any","484","        point approximately equidistant from its nearest neighbours. If the","485","        learning rate is too low, most points may look compressed in a dense","486","        cloud with few outliers. If the cost function gets stuck in a bad local","487","        minimum increasing the learning rate may help.","491","        least 250.","493","    n_iter_without_progress : int, optional (default: 300)","495","        optimization, used after 250 initial iterations with early","496","        exaggeration. Note that progress is only checked every 50 iterations so","497","        this value is rounded to the next multiple of 50.","504","        be stopped.","572","    >>> X_embedded = TSNE(n_components=2).fit_transform(X)","573","    >>> X_embedded.shape","574","    (4, 2)","589","    # Control the number of exploration iterations with early_exaggeration on","590","    _EXPLORATION_N_ITER = 250","591","","592","    # Control the number of iterations between progress checks","593","    _N_ITER_CHECK = 50","596","                 early_exaggeration=12.0, learning_rate=200.0, n_iter=1000,","597","                 n_iter_without_progress=300, min_grad_norm=1e-7,","640","        if self.metric == \"precomputed\":","641","            if isinstance(self.init, string_types) and self.init == 'pca':","642","                raise ValueError(\"The parameter init=\\\"pca\\\" cannot be \"","643","                                 \"used with metric=\\\"precomputed\\\".\")","644","            if X.shape[0] != X.shape[1]:","645","                raise ValueError(\"X should be a square distance matrix\")","646","            if np.any(X < 0):","647","                raise ValueError(\"All distances should be positive, the \"","648","                                 \"precomputed distances given as X is not \"","649","                                 \"correct\")","659","                            dtype=[np.float32, np.float64])","660","        if self.method == 'barnes_hut' and self.n_components > 3:","661","            raise ValueError(\"'n_components' should be inferior to 4 for the \"","662","                             \"barnes_hut algorithm as it relies on \"","663","                             \"quad-tree or oct-tree.\")","667","            raise ValueError(\"early_exaggeration must be at least 1, but is {}\"","668","                             .format(self.early_exaggeration))","670","        if self.n_iter < 250:","671","            raise ValueError(\"n_iter should be at least 250\")","676","        if self.method == \"exact\":","677","            # Retrieve the distance matrix, either using the precomputed one or","678","            # computing it.","679","            if self.metric == \"precomputed\":","680","                distances = X","682","                if self.verbose:","683","                    print(\"[t-SNE] Computing pairwise distances...\")","684","","685","                if self.metric == \"euclidean\":","686","                    distances = pairwise_distances(X, metric=self.metric,","687","                                                   squared=True)","688","                else:","689","                    distances = pairwise_distances(X, metric=self.metric)","690","","691","                if np.any(distances < 0):","692","                    raise ValueError(\"All distances should be positive, the \"","693","                                     \"metric given is not correct\")","694","","695","            # compute the joint probability distribution for the input space","697","            assert np.all(np.isfinite(P)), \"All probabilities should be finite\"","698","            assert np.all(P >= 0), \"All probabilities should be non-negative\"","699","            assert np.all(P <= 1), (\"All probabilities should be less \"","700","                                    \"or then equal to one\")","701","","702","        else:","703","            # Cpmpute the number of nearest neighbors to find.","704","            # LvdM uses 3 * perplexity as the number of neighbors.","705","            # In the event that we have very small # of points","706","            # set the neighbors to n - 1.","707","            k = min(n_samples - 1, int(3. * self.perplexity + 1))","708","","709","            if self.verbose:","710","                print(\"[t-SNE] Computing {} nearest neighbors...\".format(k))","711","","712","            # Find the nearest neighbors for every point","713","            neighbors_method = 'ball_tree'","714","            if (self.metric == 'precomputed'):","715","                neighbors_method = 'brute'","716","            knn = NearestNeighbors(algorithm=neighbors_method, n_neighbors=k,","717","                                   metric=self.metric)","718","            t0 = time()","719","            knn.fit(X)","720","            duration = time() - t0","721","            if self.verbose:","722","                print(\"[t-SNE] Indexed {} samples in {:.3f}s...\".format(","723","                    n_samples, duration))","724","","725","            t0 = time()","726","            distances_nn, neighbors_nn = knn.kneighbors(","727","                None, n_neighbors=k)","728","            duration = time() - t0","729","            if self.verbose:","730","                print(\"[t-SNE] Computed neighbors for {} samples in {:.3f}s...\"","731","                      .format(n_samples, duration))","732","","733","            # Free the memory used by the ball_tree","734","            del knn","735","","736","            if self.metric == \"euclidean\":","737","                # knn return the euclidean distance but we need it squared","738","                # to be consistent with the 'exact' method. Note that the","739","                # the method was derived using the euclidean method as in the","740","                # input space. Not sure of the implication of using a different","741","                # metric.","742","                distances_nn **= 2","743","","744","            # compute the joint probability distribution for the input space","745","            P = _joint_probabilities_nn(distances_nn, neighbors_nn,","746","                                        self.perplexity, self.verbose)","753","            X_embedded = pca.fit_transform(X).astype(np.float32, copy=False)","755","            # The embedding is initialized with iid samples from Gaussians with","756","            # standard deviation 1e-4.","757","            X_embedded = 1e-4 * random_state.randn(","758","                n_samples, self.n_components).astype(np.float32)","760","            raise ValueError(\"'init' must be 'pca', 'random', or \"","761","                             \"a numpy array\")","762","","763","        # Degrees of freedom of the Student's t-distribution. The suggestion","764","        # degrees_of_freedom = n_components - 1 comes from","765","        # \"Learning a Parametric Embedding by Preserving Local Structure\"","766","        # Laurens van der Maaten, 2009.","767","        degrees_of_freedom = max(self.n_components - 1.0, 1)","780","    def _tsne(self, P, degrees_of_freedom, n_samples, random_state, X_embedded,","781","              neighbors=None, skip_num_points=0):","785","        # we use is batch gradient descent with two stages:","786","        # * initial optimization with early exaggeration and momentum at 0.5","787","        # * final optimization with momentum at 0.8","790","        opt_args = {","791","            \"it\": 0,","792","            \"n_iter_check\": self._N_ITER_CHECK,","793","            \"min_grad_norm\": self.min_grad_norm,","794","            \"learning_rate\": self.learning_rate,","795","            \"verbose\": self.verbose,","796","            \"kwargs\": dict(skip_num_points=skip_num_points),","797","            \"args\": [P, degrees_of_freedom, n_samples, self.n_components],","798","            \"n_iter_without_progress\": self._EXPLORATION_N_ITER,","799","            \"n_iter\": self._EXPLORATION_N_ITER,","800","            \"momentum\": 0.5,","801","        }","805","            # Repeat verbose argument for _kl_divergence_bh","810","        # Learning schedule (part 1): do 250 iteration with lower momentum but","811","        # higher learning rate controlled via the early exageration parameter","818","","819","        # Learning schedule (part 2): disable early exaggeration and finish","820","        # optimization with a higher momentum at 0.8","821","        P \/= self.early_exaggeration","822","        remaining = self.n_iter - self._EXPLORATION_N_ITER","823","        if it < self._EXPLORATION_N_ITER or remaining > 0:","824","            opt_args['n_iter'] = self.n_iter","825","            opt_args['it'] = it + 1","826","            opt_args['momentum'] = 0.8","827","            opt_args['n_iter_without_progress'] = self.n_iter_without_progress","828","            params, kl_divergence, it = _gradient_descent(obj_func, params,","829","                                                          **opt_args)","830",""],"delete":["15","from ..neighbors import BallTree","72","    distances : array, shape (n_samples * (n_samples-1) \/ 2,)","73","        Distances of samples are stored as condensed matrices, i.e.","74","        we omit the diagonal and duplicate entries and store everything","75","        in a one-dimensional array.","85","    P : array, shape (n_samples * (n_samples-1) \/ 2,)","86","        Condensed joint probability matrix.","94","    m = \"All probabilities should be finite\"","95","    assert np.all(np.isfinite(conditional_P)), m","96","    P = conditional_P + conditional_P.T","97","    sum_P = np.maximum(np.sum(P), MACHINE_EPSILON)","98","    P = np.maximum(squareform(P) \/ sum_P, MACHINE_EPSILON)","99","    assert np.all(np.abs(P) <= 1.0)","142","    n = pdist(X_embedded, \"sqeuclidean\")","143","    n += 1.","144","    n \/= degrees_of_freedom","145","    n **= (degrees_of_freedom + 1.0) \/ -2.0","146","    Q = np.maximum(n \/ (2.0 * np.sum(n)), MACHINE_EPSILON)","152","    kl_divergence = 2.0 * np.dot(P, np.log(P \/ Q))","155","    grad = np.ndarray((n_samples, n_components))","156","    PQd = squareform((P - Q) * n)","158","        np.dot(np.ravel(PQd[i], order='K'), X_embedded[i] - X_embedded,","159","               out=grad[i])","167","def _kl_divergence_error(params, P, neighbors, degrees_of_freedom, n_samples,","168","                         n_components):","169","    \"\"\"t-SNE objective function: the absolute error of the","170","    KL divergence of p_ijs and q_ijs.","171","","172","    Parameters","173","    ----------","174","    params : array, shape (n_params,)","175","        Unraveled embedding.","176","","177","    P : array, shape (n_samples * (n_samples-1) \/ 2,)","178","        Condensed joint probability matrix.","179","","180","    neighbors : array (n_samples, K)","181","        The neighbors is not actually required to calculate the","182","        divergence, but is here to match the signature of the","183","        gradient function","184","","185","    degrees_of_freedom : float","186","        Degrees of freedom of the Student's-t distribution.","187","","188","    n_samples : int","189","        Number of samples.","190","","191","    n_components : int","192","        Dimension of the embedded space.","193","","194","    Returns","195","    -------","196","    kl_divergence : float","197","        Kullback-Leibler divergence of p_ij and q_ij.","198","","199","    grad : array, shape (n_params,)","200","        Unraveled gradient of the Kullback-Leibler divergence with respect to","201","        the embedding.","202","    \"\"\"","203","    X_embedded = params.reshape(n_samples, n_components)","204","","205","    # Q is a heavy-tailed distribution: Student's t-distribution","206","    n = pdist(X_embedded, \"sqeuclidean\")","207","    n += 1.","208","    n \/= degrees_of_freedom","209","    n **= (degrees_of_freedom + 1.0) \/ -2.0","210","    Q = np.maximum(n \/ (2.0 * np.sum(n)), MACHINE_EPSILON)","211","","212","    # Optimization trick below: np.dot(x, y) is faster than","213","    # np.sum(x * y) because it calls BLAS","214","","215","    # Objective: C (Kullback-Leibler divergence of P and Q)","216","    if len(P.shape) == 2:","217","        P = squareform(P)","218","    kl_divergence = 2.0 * np.dot(P, np.log(P \/ Q))","219","","220","    return kl_divergence","221","","222","","223","def _kl_divergence_bh(params, P, neighbors, degrees_of_freedom, n_samples,","224","                      n_components, angle=0.5, skip_num_points=0,","225","                      verbose=False):","236","    P : array, shape (n_samples * (n_samples-1) \/ 2,)","237","        Condensed joint probability matrix.","238","","239","    neighbors : int64 array, shape (n_samples, K)","240","        Array with element [i, j] giving the index for the jth","241","        closest neighbor to point i.","280","    neighbors = neighbors.astype(np.int64, copy=False)","281","    if len(P.shape) == 1:","282","        sP = squareform(P).astype(np.float32)","283","    else:","284","        sP = P.astype(np.float32)","287","    error = _barnes_hut_tsne.gradient(sP, X_embedded, neighbors,","297","def _gradient_descent(objective, p0, it, n_iter, objective_error=None,","298","                      n_iter_check=1, n_iter_without_progress=50,","299","                      momentum=0.5, learning_rate=1000.0, min_gain=0.01,","300","                      min_grad_norm=1e-7, min_error_diff=1e-7, verbose=0,","301","                      args=None, kwargs=None):","326","    objective_error : function or callable","327","        Should return a tuple of cost and gradient for a given parameter","328","        vector.","329","","330","    n_iter_without_progress : int, optional (default: 30)","334","    momentum : float, within (0.0, 1.0), optional (default: 0.5)","338","    learning_rate : float, optional (default: 1000.0)","339","        The learning rate should be extremely high for t-SNE! Values in the","340","        range [100.0, 1000.0] are common.","349","    min_error_diff : float, optional (default: 1e-7)","350","        If the absolute difference of two successive cost function values","351","        is below this threshold, the optimization will be aborted.","352","","383","    best_iter = 0","386","        new_error, grad = objective(p, *args, **kwargs)","399","            if new_error is None:","400","                new_error = objective_error(p, *args)","401","            error_diff = np.abs(new_error - error)","402","            error = new_error","405","                m = \"[t-SNE] Iteration %d: error = %.7f, gradient norm = %.7f\"","406","                print(m % (i + 1, error, grad_norm))","422","            if error_diff <= min_error_diff:","423","                if verbose >= 2:","424","                    m = \"[t-SNE] Iteration %d: error difference %f. Finished.\"","425","                    print(m % (i + 1, error_diff))","426","                break","427","","428","        if new_error is not None:","429","            error = new_error","527","    early_exaggeration : float, optional (default: 4.0)","536","    learning_rate : float, optional (default: 1000)","537","        The learning rate can be a critical parameter. It should be","538","        between 100 and 1000. If the cost function increases during initial","539","        optimization, the early exaggeration factor or the learning rate","540","        might be too high. If the cost function gets stuck in a bad local","541","        minimum increasing the learning rate helps sometimes.","545","        least 200.","547","    n_iter_without_progress : int, optional (default: 30)","548","        Only used if method='exact'","550","        optimization. If method='barnes_hut' this parameter is fixed to","551","        a value of 30 and cannot be changed.","557","        Only used if method='exact'","559","        be aborted. If method='barnes_hut' this parameter is fixed to a value","560","        of 1e-3 and cannot be changed.","611","","629","    >>> model = TSNE(n_components=2, random_state=0)","630","    >>> np.set_printoptions(suppress=True)","631","    >>> model.fit_transform(X) # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE","632","    array([[ 0.00017619,  0.00004014],","633","           [ 0.00010268,  0.00020546],","634","           [ 0.00018298, -0.00008335],","635","           [ 0.00009501, -0.00001388]])","652","                 early_exaggeration=4.0, learning_rate=1000.0, n_iter=1000,","653","                 n_iter_without_progress=30, min_grad_norm=1e-7,","656","        if not ((isinstance(init, string_types) and","657","                init in [\"pca\", \"random\"]) or","658","                isinstance(init, np.ndarray)):","659","            msg = \"'init' must be 'pca', 'random', or a numpy array\"","660","            raise ValueError(msg)","710","                            dtype=np.float64)","714","            raise ValueError(\"early_exaggeration must be at least 1, but is \"","715","                             \"%f\" % self.early_exaggeration)","717","        if self.n_iter < 200:","718","            raise ValueError(\"n_iter should be at least 200\")","720","        if self.metric == \"precomputed\":","721","            if isinstance(self.init, string_types) and self.init == 'pca':","722","                raise ValueError(\"The parameter init=\\\"pca\\\" cannot be used \"","723","                                 \"with metric=\\\"precomputed\\\".\")","724","            if X.shape[0] != X.shape[1]:","725","                raise ValueError(\"X should be a square distance matrix\")","726","            distances = X","727","        else:","728","            if self.verbose:","729","                print(\"[t-SNE] Computing pairwise distances...\")","730","","731","            if self.metric == \"euclidean\":","732","                distances = pairwise_distances(X, metric=self.metric,","733","                                               squared=True)","734","            else:","735","                distances = pairwise_distances(X, metric=self.metric)","736","","737","        if not np.all(distances >= 0):","738","            raise ValueError(\"All distances should be positive, either \"","739","                             \"the metric or precomputed distances given \"","740","                             \"as X are not correct\")","741","","742","        # Degrees of freedom of the Student's t-distribution. The suggestion","743","        # degrees_of_freedom = n_components - 1 comes from","744","        # \"Learning a Parametric Embedding by Preserving Local Structure\"","745","        # Laurens van der Maaten, 2009.","746","        degrees_of_freedom = max(self.n_components - 1.0, 1)","748","        # the number of nearest neighbors to find","749","        k = min(n_samples - 1, int(3. * self.perplexity + 1))","752","        if self.method == 'barnes_hut':","753","            if self.verbose:","754","                print(\"[t-SNE] Computing %i nearest neighbors...\" % k)","755","            if self.metric == 'precomputed':","756","                # Use the precomputed distances to find","757","                # the k nearest neighbors and their distances","758","                neighbors_nn = np.argsort(distances, axis=1)[:, :k]","760","                # Find the nearest neighbors for every point","761","                bt = BallTree(X)","762","                # LvdM uses 3 * perplexity as the number of neighbors","763","                # And we add one to not count the data point itself","764","                # In the event that we have very small # of points","765","                # set the neighbors to n - 1","766","                distances_nn, neighbors_nn = bt.query(X, k=k + 1)","767","                neighbors_nn = neighbors_nn[:, 1:]","768","            P = _joint_probabilities_nn(distances, neighbors_nn,","769","                                        self.perplexity, self.verbose)","770","        else:","772","        assert np.all(np.isfinite(P)), \"All probabilities should be finite\"","773","        assert np.all(P >= 0), \"All probabilities should be zero or positive\"","774","        assert np.all(P <= 1), (\"All probabilities should be less \"","775","                                \"or then equal to one\")","782","            X_embedded = pca.fit_transform(X)","784","            X_embedded = None","786","            raise ValueError(\"Unsupported initialization scheme: %s\"","787","                             % self.init)","800","    def _tsne(self, P, degrees_of_freedom, n_samples, random_state,","801","              X_embedded=None, neighbors=None, skip_num_points=0):","805","        # we use is batch gradient descent with three stages:","806","        # * early exaggeration with momentum 0.5","807","        # * early exaggeration with momentum 0.8","808","        # * final optimization with momentum 0.8","809","        # The embedding is initialized with iid samples from Gaussians with","810","        # standard deviation 1e-4.","811","","812","        if X_embedded is None:","813","            # Initialize embedding randomly","814","            X_embedded = 1e-4 * random_state.randn(n_samples,","815","                                                   self.n_components)","818","        opt_args = {\"n_iter\": 50, \"momentum\": 0.5, \"it\": 0,","819","                    \"learning_rate\": self.learning_rate,","820","                    \"n_iter_without_progress\": self.n_iter_without_progress,","821","                    \"verbose\": self.verbose, \"n_iter_check\": 25,","822","                    \"kwargs\": dict(skip_num_points=skip_num_points)}","824","            m = \"Must provide an array of neighbors to use Barnes-Hut\"","825","            assert neighbors is not None, m","827","            objective_error = _kl_divergence_error","828","            sP = squareform(P).astype(np.float32)","829","            neighbors = neighbors.astype(np.int64)","830","            args = [sP, neighbors, degrees_of_freedom, n_samples,","831","                    self.n_components]","832","            opt_args['args'] = args","833","            opt_args['min_grad_norm'] = 1e-3","834","            opt_args['n_iter_without_progress'] = 30","835","            # Don't always calculate the cost since that calculation","836","            # can be nearly as expensive as the gradient","837","            opt_args['objective_error'] = objective_error","842","            opt_args['args'] = [P, degrees_of_freedom, n_samples,","843","                                self.n_components]","844","            opt_args['min_error_diff'] = 0.0","845","            opt_args['min_grad_norm'] = self.min_grad_norm","847","        # Early exaggeration","849","","850","        params, kl_divergence, it = _gradient_descent(obj_func, params,","851","                                                      **opt_args)","852","        opt_args['n_iter'] = 100","853","        opt_args['momentum'] = 0.8","854","        opt_args['it'] = it + 1","863","        # Final optimization","864","        P \/= self.early_exaggeration","865","        opt_args['n_iter'] = self.n_iter","866","        opt_args['it'] = it + 1","867","        params, kl_divergence, it = _gradient_descent(obj_func, params,","868","                                                      **opt_args)","869",""]}],"sklearn\/neighbors\/setup.py":[{"add":["33","    config.add_extension(\"quad_tree\",","34","                         sources=[\"quad_tree.pyx\"],","35","                         include_dirs=[numpy.get_include()],","36","                         libraries=libraries)"],"delete":[]}]}},"22aafe04e82aec77c7e8e1347969c822355af634":{"changes":{"sklearn\/cluster\/bicluster.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/utils\/estimator_checks.py":"MODIFY"},"diff":{"sklearn\/cluster\/bicluster.py":[{"add":["112","    def fit(self, X, y=None):"],"delete":["112","    def fit(self, X):"]}],"doc\/whats_new.rst":[{"add":["421","   - :class:`cluster.bicluster.SpectralCoClustering` and","422","     :class:`cluster.bicluster.SpectralBiclustering` now accept ``y`` in fit.","423","     :issue:`6126` by `Andreas Mller`_.","424",""],"delete":[]}],"sklearn\/utils\/estimator_checks.py":[{"add":["472","    estimator.fit(X, y)"],"delete":["472","    # should be just `estimator.fit(X, y)`","473","    # after merging #6141","474","    if name in ['SpectralBiclustering']:","475","        estimator.fit(X)","476","    else:","477","        estimator.fit(X, y)"]}]}},"9accce5519bdc7733c28f52ec25310ab78ec7dfe":{"changes":{"sklearn\/feature_extraction\/text.py":"MODIFY","sklearn\/utils\/validation.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY","sklearn\/feature_extraction\/tests\/test_text.py":"MODIFY","sklearn\/utils\/tests\/test_validation.py":"MODIFY"},"diff":{"sklearn\/feature_extraction\/text.py":[{"add":["1500","            # idf_ being a property, the automatic attributes detection","1501","            # does not work as usual and we need to specify the attribute","1502","            # name:","1503","            check_is_fitted(self, attributes=[\"idf_\"],","1504","                            msg='idf vector is not fitted')","1889","        check_is_fitted(self, msg='The TF-IDF vectorizer is not fitted')"],"delete":["1500","            check_is_fitted(self, msg='idf vector is not fitted')","1885","        check_is_fitted(self, msg='The tfidf vector is not fitted')"]}],"sklearn\/utils\/validation.py":[{"add":["854","def check_is_fitted(estimator, attributes=None, msg=None, all_or_any=all):","861","    This utility is meant to be used internally by estimators themselves,","862","    typically in their own predict \/ transform methods.","863","","869","    attributes : str, list or tuple of str, default=None","870","        Attribute name(s) given as string or a list\/tuple of strings","871","        Eg.: ``[\"coef_\", \"estimator_\", ...], \"coef_\"``","872","","873","        If `None`, `estimator` is considered fitted if there exist an","874","        attribute that ends with a underscore and does not start with double","875","        underscore.","876","","887","    all_or_any : callable, {all, any}, default all","888","        Specify whether all or any of the given attributes must exist.","889","","908","    if attributes is not None:","909","        if not isinstance(attributes, (list, tuple)):","910","            attributes = [attributes]","911","        attrs = all_or_any([hasattr(estimator, attr) for attr in attributes])","912","    else:","913","        attrs = [v for v in vars(estimator)","914","                 if v.endswith(\"_\") and not v.startswith(\"__\")]"],"delete":["854","def check_is_fitted(estimator, msg=None):","894","    attrs = [v for v in vars(estimator)","895","             if (v.endswith(\"_\") or v.startswith(\"_\"))","896","             and not v.startswith(\"__\")]"]}],"doc\/whats_new\/v0.22.rst":[{"add":["70","- |Fix| :func:`utils.check_is_fitted` accepts back an explicit ``attributes``","71","  argument to check for specific attributes as explicit markers of a fitted","72","  estimator. When no explicit ``attributes`` are provided, only the attributes","73","  ending with a single \"_\" are used as \"fitted\" markers. The ``all_or_any``","74","  argument is also no longer deprecated. This change is made to","75","  restore some backward compatibility with the behavior of this utility in","76","  version 0.21. :pr:`15947` by `Thomas Fan`_.","77",""],"delete":[]}],"sklearn\/feature_extraction\/tests\/test_text.py":[{"add":["1101","    vec.fit([\"some text\", \"some other text\"])"],"delete":[]}],"sklearn\/utils\/tests\/test_validation.py":[{"add":["7","from operator import itemgetter","679","def test_check_is_fitted_attributes():","680","    class MyEstimator():","681","        def fit(self, X, y):","682","            return self","683","","684","    msg = \"not fitted\"","685","    est = MyEstimator()","686","","687","    with pytest.raises(NotFittedError, match=msg):","688","        check_is_fitted(est, attributes=[\"a_\", \"b_\"])","689","    with pytest.raises(NotFittedError, match=msg):","690","        check_is_fitted(est, attributes=[\"a_\", \"b_\"], all_or_any=all)","691","    with pytest.raises(NotFittedError, match=msg):","692","        check_is_fitted(est, attributes=[\"a_\", \"b_\"], all_or_any=any)","693","","694","    est.a_ = \"a\"","695","    with pytest.raises(NotFittedError, match=msg):","696","        check_is_fitted(est, attributes=[\"a_\", \"b_\"])","697","    with pytest.raises(NotFittedError, match=msg):","698","        check_is_fitted(est, attributes=[\"a_\", \"b_\"], all_or_any=all)","699","    check_is_fitted(est, attributes=[\"a_\", \"b_\"], all_or_any=any)","700","","701","    est.b_ = \"b\"","702","    check_is_fitted(est, attributes=[\"a_\", \"b_\"])","703","    check_is_fitted(est, attributes=[\"a_\", \"b_\"], all_or_any=all)","704","    check_is_fitted(est, attributes=[\"a_\", \"b_\"], all_or_any=any)","705","","706","","707","@pytest.mark.parametrize(\"wrap\",","708","                         [itemgetter(0), list, tuple],","709","                         ids=[\"single\", \"list\", \"tuple\"])","710","def test_check_is_fitted_with_attributes(wrap):","711","    ard = ARDRegression()","712","    with pytest.raises(NotFittedError, match=\"is not fitted yet\"):","713","        check_is_fitted(ard, wrap([\"coef_\"]))","714","","715","    ard.fit(*make_blobs())","716","","717","    # Does not raise","718","    check_is_fitted(ard, wrap([\"coef_\"]))","719","","720","    # Raises when using attribute that is not defined","721","    with pytest.raises(NotFittedError, match=\"is not fitted yet\"):","722","        check_is_fitted(ard, wrap([\"coef_bad_\"]))","723","","724",""],"delete":["16","from sklearn.utils._testing import assert_warns_message","52","from sklearn.exceptions import DataConversionWarning"]}]}},"fa5926afae6e6d47ea34049b0203438c3900de94":{"changes":{"doc\/faq.rst":"MODIFY"},"diff":{"doc\/faq.rst":[{"add":["27","issues <easy_issues>`. Please do not contact the contributors of scikit-learn directly","28","regarding contributing to scikit-learn."],"delete":["27","issues <easy_issues>`."]}]}},"90adab947dd5e29bf83e0cdf954e09f4880abd55":{"changes":{"sklearn\/svm\/tests\/test_sparse.py":"MODIFY"},"diff":{"sklearn\/svm\/tests\/test_sparse.py":[{"add":["51","    assert_array_almost_equal(dense_svm.dual_coef_,","52","                              sparse_svm.dual_coef_.toarray())","57","    assert_array_almost_equal(dense_svm.predict(X_test_dense),","58","                              sparse_svm.predict(X_test))","126","    def kfunc(x, y):","127","        return safe_sparse_dot(x, y.T)","149","    # Test decision_function","151","    # Sanity check, test that decision_function implemented in python","152","    # returns the same as the one in libsvm","267","    # Check that sparse OneClassSVM gives the same result as dense OneClassSVM","278","            clf = svm.OneClassSVM(kernel=kernel)","279","            sp_clf = svm.OneClassSVM(kernel=kernel)"],"delete":["51","    assert_array_almost_equal(dense_svm.dual_coef_, sparse_svm.dual_coef_.toarray())","56","    assert_array_almost_equal(dense_svm.predict(X_test_dense), sparse_svm.predict(X_test))","124","    kfunc = lambda x, y: safe_sparse_dot(x, y.T)","146","    #Test decision_function","148","    #Sanity check, test that decision_function implemented in python","149","    #returns the same as the one in libsvm","264","    \"\"\"Check that sparse OneClassSVM gives the same result as dense OneClassSVM\"\"\"","275","            clf = svm.OneClassSVM(kernel=kernel, random_state=0)","276","            sp_clf = svm.OneClassSVM(kernel=kernel, random_state=0)"]}]}},"e2f99b04270d132d0a485fdb43406e46a95bb2ee":{"changes":{"sklearn\/svm\/src\/liblinear\/liblinear_helper.c":"MODIFY","sklearn\/svm\/src\/liblinear\/linear.cpp":"MODIFY"},"diff":{"sklearn\/svm\/src\/liblinear\/liblinear_helper.c":[{"add":["221","    free(problem);"],"delete":[]}],"sklearn\/svm\/src\/liblinear\/linear.cpp":[{"add":["2912","\tif(model_ptr->n_iter != NULL)","2913","\t    free(model_ptr->n_iter);"],"delete":[]}]}},"dd7a3dd0eb34ecca1f9991a23961a43fc2fb0ac1":{"changes":{"sklearn\/externals\/joblib\/pool.py":"MODIFY","sklearn\/metrics\/ranking.py":"MODIFY","sklearn\/externals\/joblib\/logger.py":"MODIFY","doc\/modules\/mixture.rst":"MODIFY"},"diff":{"sklearn\/externals\/joblib\/pool.py":[{"add":["541","                    # Missing rights in the \/dev\/shm partition,"],"delete":["541","                    # Missing rights in the the \/dev\/shm partition,"]}],"sklearn\/metrics\/ranking.py":[{"add":["596","    # Handle badly formatted array and the degenerate case with one label"],"delete":["596","    # Handle badly formated array and the degenerate case with one label"]}],"sklearn\/externals\/joblib\/logger.py":[{"add":["83","        \"\"\" Return the formatted representation of the object."],"delete":["83","        \"\"\" Return the formated representation of the object."]}],"doc\/modules\/mixture.rst":[{"add":["183","Here, we can see the value of the ``weight_concentration_prior`` parameter"],"delete":["183","Here, we can see the the value of the ``weight_concentration_prior`` parameter"]}]}},"072bfc95c4732f540003bd0cb76854588f6af986":{"changes":{"doc\/whats_new\/v0.22.rst":"MODIFY"},"diff":{"doc\/whats_new\/v0.22.rst":[{"add":["9","**January 2 2020**","55","","69",":mod:`sklearn.naive_bayes`","70","..........................","71","","72","- |Fix| Removed `abstractmethod` decorator for the method `_check_X` in","73","  :class:`naive_bayes.BaseNB` that could break downstream projects inheriting","74","  from this deprecated public base class. :pr:`15996` by","75","  :user:`Brigitta Sip?cz <bsipocz>`.","76","","77",":mod:`sklearn.preprocessing`","78","............................","79","","80","- |Fix| :class:`preprocessing.QuantileTransformer` now guarantees the","81","  `quantiles_` attribute to be completely sorted in non-decreasing manner.","82","  :pr:`15751` by :user:`Tirth Patel <tirthasheshpatel>`.","83","","84",":mod:`sklearn.semi_supervised`","85","..............................","86","","87","- |Fix| :class:`semi_supervised.LabelPropagation` and","88","  :class:`semi_supervised.LabelSpreading` now allow callable kernel function to","89","  return sparse weight matrix.","90","  :pr:`15868` by :user:`Niklas Smedemark-Margulies <nik-sm>`.","91",""],"delete":["9","**In Development**","55","  ","60",":mod:`sklearn.naive_bayes`","61","..........................","62","","63","- |Fix| Removed `abstractmethod` decorator for the method `_check_X` in","64","  :class:`naive_bayes.BaseNB` that could break downstream projects inheriting","65","  from this deprecated public base class. :pr:`15996` by","66","  :user:`Brigitta Sip?cz <bsipocz>`.","67","","68",":mod:`sklearn.semi_supervised`","69","..............................","70","","71","- |Fix| :class:`semi_supervised.LabelPropagation` and","72","  :class:`semi_supervised.LabelSpreading` now allow callable kernel function to","73","  return sparse weight matrix.","74","  :pr:`15868` by :user:`Niklas Smedemark-Margulies <nik-sm>`.","75","","861","- |Fix| :class:`preprocessing.QuantileTransformer` now guarantees the ","862","  `quantiles_` attribute to be completely sorted in non-decreasing manner.","863","  :pr:`15751` by :user:`Tirth Patel <tirthasheshpatel>`.","864",""]}]}},"1557cb8a1910bce6dc33cd5cd3a08c380bdec566":{"changes":{"examples\/applications\/plot_tomography_l1_reconstruction.py":"MODIFY"},"diff":{"examples\/applications\/plot_tomography_l1_reconstruction.py":[{"add":["36","from __future__ import division","53","    floor_x = np.floor((x - orig) \/ dx).astype(np.int64)","115","proj_operator = build_projection_operator(l, l \/\/ 7)"],"delete":["52","    floor_x = np.floor((x - orig) \/ dx)","114","proj_operator = build_projection_operator(l, l \/ 7.)"]}]}},"f28a90c9aa552740a6fe4d9eccc409f82392019b":{"changes":{"sklearn\/naive_bayes.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY"},"diff":{"sklearn\/naive_bayes.py":[{"add":["53","    def _check_X(self, X):","54","        \"\"\"To be overridden in subclasses with the actual checks.\"\"\"","55","        # Note that this is not marked @abstractmethod as long as the","56","        # deprecated public alias sklearn.naive_bayes.BayesNB exists","57","        # (until 0.24) to preserve backward compat for 3rd party projects","58","        # with existing derived classes.","59","        return X","60",""],"delete":[]}],"doc\/whats_new\/v0.22.rst":[{"add":["63","- |Fix| Removed `abstractmethod` decorator for the method `_check_X` in","64","  :class:`naive_bayes.BaseNB` that could break downstream projects inheriting","65","  from this deprecated public base class. :pr:`15996` by","66","  :user:`Brigitta Sip?cz <bsipocz>`."],"delete":["63","- |Fix| removed abstract method `_check_X` from :class:`naive_bayes.BaseNB`","64","  that could break downstream projects inheriting from this deprecated","65","  public base class. :pr:`15996` by :user:`Brigitta Sip?cz <bsipocz>`."]}]}},"d788dcb1990fffd765c73bea1b63854f0ca6fe20":{"changes":{"sklearn\/utils\/tests\/test_testing.py":"MODIFY","sklearn\/utils\/testing.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/utils\/estimator_checks.py":"MODIFY","sklearn\/tests\/test_common.py":"MODIFY","sklearn\/utils\/tests\/test_estimator_checks.py":"MODIFY"},"diff":{"sklearn\/utils\/tests\/test_testing.py":[{"add":["3","import numpy as np","4","from scipy import sparse","17","    assert_allclose_dense_sparse,","55","def test_assert_allclose_dense_sparse():","56","    x = np.arange(9).reshape(3, 3)","57","    msg = \"Not equal to tolerance \"","58","    y = sparse.csc_matrix(x)","59","    for X in [x, y]:","60","        # basic compare","61","        assert_raise_message(AssertionError, msg, assert_allclose_dense_sparse,","62","                             X, X * 2)","63","        assert_allclose_dense_sparse(X, X)","64","","65","    assert_raise_message(ValueError, \"Can only compare two sparse\",","66","                         assert_allclose_dense_sparse, x, y)","67","","68","    A = sparse.diags(np.ones(5), offsets=0).tocsr()","69","    B = sparse.csr_matrix(np.ones((1, 5)))","70","","71","    assert_raise_message(AssertionError, \"Arrays are not equal\",","72","                         assert_allclose_dense_sparse, B, A)","73","","74","","198","# `clean_warning_registry()` is called internally by assert_warns"],"delete":["175","#`clean_warning_registry()` is called internally by assert_warns"]}],"sklearn\/utils\/testing.py":[{"add":["377","def assert_allclose_dense_sparse(x, y, rtol=1e-07, atol=0, err_msg=''):","378","    \"\"\"Assert allclose for sparse and dense data.","379","","380","    Both x and y need to be either sparse or dense, they","381","    can't be mixed.","382","","383","    Parameters","384","    ----------","385","    x : array-like or sparse matrix","386","        First array to compare.","387","","388","    y : array-like or sparse matrix","389","        Second array to compare.","390","","391","    err_msg : string, default=''","392","        Error message to raise.","393","    \"\"\"","394","    if sp.sparse.issparse(x) and sp.sparse.issparse(y):","395","        x = x.tocsr()","396","        y = y.tocsr()","397","        x.sum_duplicates()","398","        y.sum_duplicates()","399","        assert_array_equal(x.indices, y.indices, err_msg=err_msg)","400","        assert_array_equal(x.indptr, y.indptr, err_msg=err_msg)","401","        assert_allclose(x.data, y.data, rtol=rtol, atol=atol, err_msg=err_msg)","402","    elif not sp.sparse.issparse(x) and not sp.sparse.issparse(y):","403","        # both dense","404","        assert_allclose(x, y, rtol=rtol, atol=atol, err_msg=err_msg)","405","    else:","406","        raise ValueError(\"Can only compare two sparse matrices,\"","407","                         \" not a sparse matrix and an array.\")","408","","409",""],"delete":[]}],"doc\/whats_new.rst":[{"add":["365","   - All checks in ``utils.estimator_checks``, in particular","366","     :func:`utils.estimator_checks.check_estimator` now accept estimator","367","     instances. Most other checks do not accept","368","     estimator classes any more. :issue:`9019` by `Andreas Mller`_.","369",""],"delete":[]}],"sklearn\/utils\/estimator_checks.py":[{"add":["24","from sklearn.utils.testing import assert_allclose","25","from sklearn.utils.testing import assert_allclose_dense_sparse","34","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis","52","from sklearn.utils.validation import has_fit_parameter, _num_samples","70","def _yield_non_meta_checks(name, estimator):","96","    if hasattr(estimator, 'sparsify'):","106","def _yield_classifier_checks(name, classifier):","126","    if 'class_weight' in classifier.get_params().keys():","135","def check_supervised_y_no_nan(name, estimator_orig):","137","    estimator = clone(estimator_orig)","141","    y = multioutput_estimator_convert_y_2d(estimator, y)","146","        estimator.fit(X, y)","149","            raise ValueError(\"Estimator {0} raised error as expected, but \"","157","def _yield_regressor_checks(name, regressor):","176","def _yield_transformer_checks(name, transformer):","196","def _yield_clustering_checks(name, clusterer):","206","def _yield_all_checks(name, estimator):","207","    for check in _yield_non_meta_checks(name, estimator):","209","    if isinstance(estimator, ClassifierMixin):","210","        for check in _yield_classifier_checks(name, estimator):","212","    if isinstance(estimator, RegressorMixin):","213","        for check in _yield_regressor_checks(name, estimator):","215","    if isinstance(estimator, TransformerMixin):","216","        for check in _yield_transformer_checks(name, estimator):","218","    if isinstance(estimator, ClusterMixin):","219","        for check in _yield_clustering_checks(name, estimator):","240","    This test can be applied to classes or instances.","241","    Classes currently have some additional tests that related to construction,","242","    while passing instances allows the testing of multiple options.","243","","246","    estimator : estimator object or class","247","        Estimator to check. Estimator is a class object or instance.","250","    if isinstance(Estimator, type):","251","        # got a class","252","        name = Estimator.__name__","253","        check_parameters_default_constructible(name, Estimator)","254","        check_no_fit_attributes_set_in_init(name, Estimator)","255","        estimator = Estimator()","256","    else:","257","        # got an instance","258","        estimator = Estimator","259","        name = type(estimator).__name__","260","","261","    for check in _yield_all_checks(name, estimator):","263","            check(name, estimator)","282","def set_checking_parameters(estimator):","292","        # LinearSVR, LinearSVC","293","        if estimator.__class__.__name__ in ['LinearSVR', 'LinearSVC']:","329","        estimator.set_params(n_components=2)","352","def check_estimator_sparse_data(name, estimator_orig):","358","    # catch deprecation warnings","359","    with ignore_warnings(category=DeprecationWarning):","360","        estimator = clone(estimator_orig)","361","    y = multioutput_estimator_convert_y_2d(estimator, y)","367","                estimator = clone(estimator).set_params(with_mean=False)","369","                estimator = clone(estimator)","395","def check_sample_weights_pandas_series(name, estimator_orig):","398","    estimator = clone(estimator_orig)","417","def check_sample_weights_list(name, estimator_orig):","420","    if has_fit_parameter(estimator_orig, \"sample_weight\"):","421","        estimator = clone(estimator_orig)","425","        y = multioutput_estimator_convert_y_2d(estimator, y)","432","def check_dtype_object(name, estimator_orig):","437","    estimator = clone(estimator_orig)","438","    y = multioutput_estimator_convert_y_2d(estimator, y)","459","def check_dict_unchanged(name, estimator_orig):","473","    estimator = clone(estimator_orig)","474","    y = multioutput_estimator_convert_y_2d(estimator, y)","500","@ignore_warnings(category=DeprecationWarning)","501","def check_dont_overwrite_parameters(name, estimator_orig):","503","    if hasattr(estimator_orig.__init__, \"deprecated_original\"):","506","    estimator = clone(estimator_orig)","510","    y = multioutput_estimator_convert_y_2d(estimator, y)","550","@ignore_warnings(category=DeprecationWarning)","551","def check_fit2d_predict1d(name, estimator_orig):","556","    estimator = clone(estimator_orig)","557","    y = multioutput_estimator_convert_y_2d(estimator, y)","575","def check_fit2d_1sample(name, estimator_orig):","580","    estimator = clone(estimator_orig)","581","    y = multioutput_estimator_convert_y_2d(estimator, y)","596","def check_fit2d_1feature(name, estimator_orig):","601","    estimator = clone(estimator_orig)","602","    y = multioutput_estimator_convert_y_2d(estimator, y)","617","def check_fit1d_1feature(name, estimator_orig):","622","    estimator = clone(estimator_orig)","623","    y = multioutput_estimator_convert_y_2d(estimator, y)","639","def check_fit1d_1sample(name, estimator_orig):","644","    estimator = clone(estimator_orig)","645","    y = multioutput_estimator_convert_y_2d(estimator, y)","661","def check_transformer_general(name, transformer):","666","    _check_transformer(name, transformer, X, y)","667","    _check_transformer(name, transformer, X.tolist(), y.tolist())","671","def check_transformer_data_not_an_array(name, transformer):","680","    _check_transformer(name, transformer, this_X, this_y)","683","@ignore_warnings(category=DeprecationWarning)","684","def check_transformers_unfitted(name, transformer):","687","    transformer = clone(transformer)","692","def _check_transformer(name, transformer_orig, X, y):","702","    transformer = clone(transformer_orig)","734","                assert_allclose_dense_sparse(","735","                    x_pred, x_pred2, atol=1e-2,","736","                    err_msg=\"fit_transform and transform outcomes \"","737","                            \"not consistent in %s\"","738","                    % transformer)","739","                assert_allclose_dense_sparse(","740","                    x_pred, x_pred3, atol=1e-2,","741","                    err_msg=\"consecutive fit_transform outcomes \"","742","                            \"not consistent in %s\"","743","                    % transformer)","745","            assert_allclose_dense_sparse(","746","                X_pred, X_pred2,","747","                err_msg=\"fit_transform and transform outcomes \"","748","                        \"not consistent in %s\"","749","                % transformer, atol=1e-2)","750","            assert_allclose_dense_sparse(","751","                X_pred, X_pred3, atol=1e-2,","752","                err_msg=\"consecutive fit_transform outcomes \"","753","                        \"not consistent in %s\"","754","                % transformer)","755","            assert_equal(_num_samples(X_pred2), n_samples)","756","            assert_equal(_num_samples(X_pred3), n_samples)","765","def check_pipeline_consistency(name, estimator_orig):","779","    estimator = clone(estimator_orig)","780","    y = multioutput_estimator_convert_y_2d(estimator, y)","794","            assert_allclose_dense_sparse(result, result_pipe)","798","def check_fit_score_takes_y(name, estimator_orig):","804","    estimator = clone(estimator_orig)","805","    y = multioutput_estimator_convert_y_2d(estimator, y)","814","            if args[0] == \"self\":","815","                # if_delegate_has_method makes methods into functions","816","                # with an explicit \"self\", so need to shift arguments","817","                args = args[1:]","821","                        % (func_name, type(estimator).__name__, args))","825","def check_estimators_dtypes(name, estimator_orig):","832","    y = multioutput_estimator_convert_y_2d(estimator_orig, y)","837","        estimator = clone(estimator_orig)","847","def check_estimators_empty_data_messages(name, estimator_orig):","848","    e = clone(estimator_orig)","859","    y = multioutput_estimator_convert_y_2d(e, np.array([1, 0, 1]))","865","@ignore_warnings(category=DeprecationWarning)","866","def check_estimators_nan_inf(name, estimator_orig):","876","    y = multioutput_estimator_convert_y_2d(estimator_orig, y)","885","            estimator = clone(estimator_orig)","892","                    print(error_string_fit, estimator, e)","896","                print(error_string_fit, estimator, exc)","900","                raise AssertionError(error_string_fit, estimator)","910","                        print(error_string_predict, estimator, e)","914","                    print(error_string_predict, estimator, exc)","917","                    raise AssertionError(error_string_predict, estimator)","925","                        print(error_string_transform, estimator, e)","929","                    print(error_string_transform, estimator, exc)","932","                    raise AssertionError(error_string_transform, estimator)","936","def check_estimators_pickle(name, estimator_orig):","947","    estimator = clone(estimator_orig)","949","    # some estimators only take multioutputs","950","    y = multioutput_estimator_convert_y_2d(estimator, y)","962","    if estimator.__module__.startswith('sklearn.'):","968","        assert_allclose_dense_sparse(result[method], unpickled_result)","971","@ignore_warnings(category=DeprecationWarning)","972","def check_estimators_partial_fit_n_features(name, estimator_orig):","974","    if not hasattr(estimator_orig, 'partial_fit'):","976","    estimator = clone(estimator_orig)","981","        if isinstance(estimator, ClassifierMixin):","983","            estimator.partial_fit(X, y, classes=classes)","985","            estimator.partial_fit(X, y)","989","    assert_raises(ValueError, estimator.partial_fit, X[:, :-1], y)","992","@ignore_warnings(category=DeprecationWarning)","993","def check_clustering(name, clusterer_orig):","994","    clusterer = clone(clusterer_orig)","1000","    if hasattr(clusterer, \"n_clusters\"):","1001","        clusterer.set_params(n_clusters=3)","1002","    set_random_state(clusterer)","1004","        clusterer.set_params(preference=-100)","1005","        clusterer.set_params(max_iter=100)","1008","    clusterer.fit(X)","1010","    clusterer.fit(X.tolist())","1012","    assert_equal(clusterer.labels_.shape, (n_samples,))","1013","    pred = clusterer.labels_","1019","    set_random_state(clusterer)","1021","        pred2 = clusterer.fit_predict(X)","1025","@ignore_warnings(category=DeprecationWarning)","1026","def check_clusterer_compute_labels_predict(name, clusterer_orig):","1029","    clusterer = clone(clusterer_orig)","1042","@ignore_warnings(category=DeprecationWarning)","1043","def check_classifiers_one_label(name, classifier_orig):","1053","        classifier = clone(classifier_orig)","1059","                print(error_string_fit, classifier, e)","1065","            print(error_string_fit, classifier, exc)","1072","            print(error_string_predict, classifier, exc)","1077","def check_classifiers_train(name, classifier_orig):","1088","        classifier = clone(classifier_orig)","1136","            assert_allclose(np.sum(y_prob, axis=1), np.ones(n_samples))","1144","                assert_allclose(y_log_prob, np.log(y_prob), 8)","1149","def check_estimators_fit_returns_self(name, estimator_orig):","1155","    estimator = clone(estimator_orig)","1156","    y = multioutput_estimator_convert_y_2d(estimator, y)","1164","def check_estimators_unfitted(name, estimator_orig):","1175","    est = clone(estimator_orig)","1196","def check_supervised_y_2d(name, estimator_orig):","1203","    estimator = clone(estimator_orig)","1224","    assert_allclose(y_pred.ravel(), y_pred_2d.ravel())","1227","@ignore_warnings(category=DeprecationWarning)","1228","def check_classifiers_classes(name, classifier_orig):","1245","        classifier = clone(classifier_orig)","1262","def check_regressors_int(name, regressor_orig):","1267","    y = multioutput_estimator_convert_y_2d(regressor_orig, y)","1270","    regressor_1 = clone(regressor_orig)","1271","    regressor_2 = clone(regressor_orig)","1286","    assert_allclose(pred1, pred2, atol=1e-2, err_msg=name)","1290","def check_regressors_train(name, regressor_orig):","1294","    regressor = clone(regressor_orig)","1295","    y = multioutput_estimator_convert_y_2d(regressor, y)","1325","def check_regressors_no_decision_function(name, regressor_orig):","1329","    regressor = clone(regressor_orig)","1330","    y = multioutput_estimator_convert_y_2d(regressor, X[:, 0])","1348","@ignore_warnings(category=DeprecationWarning)","1349","def check_class_weight_classifiers(name, classifier_orig):","1370","        classifier = clone(classifier_orig).set_params(","1371","            class_weight=class_weight)","1383","@ignore_warnings(category=DeprecationWarning)","1384","def check_class_weight_balanced_classifiers(name, classifier_orig, X_train,","1385","                                            y_train, X_test, y_test, weights):","1386","    classifier = clone(classifier_orig)","1401","@ignore_warnings(category=DeprecationWarning)","1404","    # this is run on classes, not instances, though this should be changed","1409","    classifier = Classifier()","1429","    assert_allclose(coef_balanced, coef_manual)","1433","def check_estimators_overwrite_params(name, estimator_orig):","1437","    estimator = clone(estimator_orig)","1438","    y = multioutput_estimator_convert_y_2d(estimator, y)","1466","@ignore_warnings(category=DeprecationWarning)","1469","    # this check works on classes, not instances","1485","@ignore_warnings(category=DeprecationWarning)","1486","def check_sparsify_coefficients(name, estimator_orig):","1490","    est = clone(estimator_orig)","1508","@ignore_warnings(category=DeprecationWarning)","1509","def check_classifier_data_not_an_array(name, estimator_orig):","1512","    y = multioutput_estimator_convert_y_2d(estimator_orig, y)","1513","    check_estimators_data_not_an_array(name, estimator_orig, X, y)","1517","def check_regressor_data_not_an_array(name, estimator_orig):","1518","    X, y = _boston_subset(n_samples=50)","1519","    y = multioutput_estimator_convert_y_2d(estimator_orig, y)","1520","    check_estimators_data_not_an_array(name, estimator_orig, X, y)","1521","","1522","","1523","@ignore_warnings(category=DeprecationWarning)","1524","def check_estimators_data_not_an_array(name, estimator_orig, X, y):","1529","    estimator_1 = clone(estimator_orig)","1530","    estimator_2 = clone(estimator_orig)","1542","    assert_allclose(pred1, pred2, atol=1e-2, err_msg=name)","1546","    # this check works on classes, not instances","1608","def multioutput_estimator_convert_y_2d(estimator, y):","1611","    if \"MultiTask\" in estimator.__class__.__name__:","1617","def check_non_transformer_estimators_n_iter(name, estimator_orig):","1635","        estimator = clone(estimator_orig).set_params(alpha=0.)","1637","        estimator = clone(estimator_orig)","1641","        y_ = multioutput_estimator_convert_y_2d(estimator, y_)","1656","def check_transformer_n_iter(name, estimator_orig):","1659","    estimator = clone(estimator_orig)","1682","def check_get_params_invariance(name, estimator_orig):","1697","    e = clone(estimator_orig)","1706","@ignore_warnings(category=DeprecationWarning)","1707","def check_classifiers_regression_target(name, estimator_orig):","1712","    e = clone(estimator_orig)","1718","def check_decision_proba_consistency(name, estimator_orig):","1726","    estimator = clone(estimator_orig)"],"delete":["24","from sklearn.utils.testing import assert_array_almost_equal","39","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis","51","from sklearn.utils.validation import has_fit_parameter","69","def _yield_non_meta_checks(name, Estimator):","95","    if hasattr(Estimator, 'sparsify'):","105","def _yield_classifier_checks(name, Classifier):","125","    if 'class_weight' in Classifier().get_params().keys():","134","def check_supervised_y_no_nan(name, Estimator):","136","","140","    y = multioutput_estimator_convert_y_2d(name, y)","145","        Estimator().fit(X, y)","148","            raise ValueError(\"Estimator {0} raised warning as expected, but \"","156","def _yield_regressor_checks(name, Regressor):","175","def _yield_transformer_checks(name, Transformer):","195","def _yield_clustering_checks(name, Clusterer):","205","def _yield_all_checks(name, Estimator):","206","    for check in _yield_non_meta_checks(name, Estimator):","208","    if issubclass(Estimator, ClassifierMixin):","209","        for check in _yield_classifier_checks(name, Estimator):","211","    if issubclass(Estimator, RegressorMixin):","212","        for check in _yield_regressor_checks(name, Estimator):","214","    if issubclass(Estimator, TransformerMixin):","215","        for check in _yield_transformer_checks(name, Estimator):","217","    if issubclass(Estimator, ClusterMixin):","218","        for check in _yield_clustering_checks(name, Estimator):","227","    yield check_no_fit_attributes_set_in_init","242","    Estimator : class","243","        Class to check. Estimator is a class object (not an instance).","246","    name = Estimator.__name__","247","    check_parameters_default_constructible(name, Estimator)","248","    for check in _yield_all_checks(name, Estimator):","250","            check(name, Estimator)","269","def set_testing_parameters(estimator):","279","        # LinearSVR","280","        if estimator.__class__.__name__ == 'LinearSVR':","316","        estimator.set_params(n_components=1)","339","def check_estimator_sparse_data(name, Estimator):","350","                estimator = Estimator(with_mean=False)","352","                estimator = Estimator()","353","        set_testing_parameters(estimator)","379","def check_sample_weights_pandas_series(name, Estimator):","382","    estimator = Estimator()","401","def check_sample_weights_list(name, Estimator):","404","    estimator = Estimator()","405","    if has_fit_parameter(estimator, \"sample_weight\"):","409","        y = multioutput_estimator_convert_y_2d(name, y)","416","def check_dtype_object(name, Estimator):","421","    y = multioutput_estimator_convert_y_2d(name, y)","422","    estimator = Estimator()","423","    set_testing_parameters(estimator)","444","def check_dict_unchanged(name, Estimator):","458","    y = multioutput_estimator_convert_y_2d(name, y)","459","    estimator = Estimator()","460","    set_testing_parameters(estimator)","486","def check_dont_overwrite_parameters(name, Estimator):","488","    if hasattr(Estimator.__init__, \"deprecated_original\"):","494","    y = multioutput_estimator_convert_y_2d(name, y)","495","    estimator = Estimator()","496","    set_testing_parameters(estimator)","536","def check_fit2d_predict1d(name, Estimator):","541","    y = multioutput_estimator_convert_y_2d(name, y)","542","    estimator = Estimator()","543","    set_testing_parameters(estimator)","561","def check_fit2d_1sample(name, Estimator):","566","    y = multioutput_estimator_convert_y_2d(name, y)","567","    estimator = Estimator()","568","    set_testing_parameters(estimator)","583","def check_fit2d_1feature(name, Estimator):","588","    y = multioutput_estimator_convert_y_2d(name, y)","589","    estimator = Estimator()","590","    set_testing_parameters(estimator)","605","def check_fit1d_1feature(name, Estimator):","610","    y = multioutput_estimator_convert_y_2d(name, y)","611","    estimator = Estimator()","612","    set_testing_parameters(estimator)","628","def check_fit1d_1sample(name, Estimator):","633","    y = multioutput_estimator_convert_y_2d(name, y)","634","    estimator = Estimator()","635","    set_testing_parameters(estimator)","651","def check_transformer_general(name, Transformer):","656","    _check_transformer(name, Transformer, X, y)","657","    _check_transformer(name, Transformer, X.tolist(), y.tolist())","661","def check_transformer_data_not_an_array(name, Transformer):","670","    _check_transformer(name, Transformer, this_X, this_y)","673","def check_transformers_unfitted(name, Transformer):","676","    with ignore_warnings(category=DeprecationWarning):","677","        transformer = Transformer()","682","def _check_transformer(name, Transformer, X, y):","692","    # catch deprecation warnings","693","    transformer = Transformer()","695","    set_testing_parameters(transformer)","726","                assert_array_almost_equal(","727","                    x_pred, x_pred2, 2,","728","                    \"fit_transform and transform outcomes not consistent in %s\"","729","                    % Transformer)","730","                assert_array_almost_equal(","731","                    x_pred, x_pred3, 2,","732","                    \"consecutive fit_transform outcomes not consistent in %s\"","733","                    % Transformer)","735","            assert_array_almost_equal(","736","                X_pred, X_pred2, 2,","737","                \"fit_transform and transform outcomes not consistent in %s\"","738","                % Transformer)","739","            assert_array_almost_equal(","740","                X_pred, X_pred3, 2,","741","                \"consecutive fit_transform outcomes not consistent in %s\"","742","                % Transformer)","743","            assert_equal(len(X_pred2), n_samples)","744","            assert_equal(len(X_pred3), n_samples)","753","def check_pipeline_consistency(name, Estimator):","767","    y = multioutput_estimator_convert_y_2d(name, y)","768","    estimator = Estimator()","769","    set_testing_parameters(estimator)","783","            assert_array_almost_equal(result, result_pipe)","787","def check_fit_score_takes_y(name, Estimator):","793","    y = multioutput_estimator_convert_y_2d(name, y)","794","    estimator = Estimator()","795","    set_testing_parameters(estimator)","807","                        % (func_name, Estimator.__name__, args))","811","def check_estimators_dtypes(name, Estimator):","818","    y = multioutput_estimator_convert_y_2d(name, y)","823","        estimator = Estimator()","824","        set_testing_parameters(estimator)","834","def check_estimators_empty_data_messages(name, Estimator):","835","    e = Estimator()","836","    set_testing_parameters(e)","847","    y = multioutput_estimator_convert_y_2d(name, np.array([1, 0, 1]))","853","def check_estimators_nan_inf(name, Estimator):","863","    y = multioutput_estimator_convert_y_2d(name, y)","872","            estimator = Estimator()","873","            set_testing_parameters(estimator)","880","                    print(error_string_fit, Estimator, e)","884","                print(error_string_fit, Estimator, exc)","888","                raise AssertionError(error_string_fit, Estimator)","898","                        print(error_string_predict, Estimator, e)","902","                    print(error_string_predict, Estimator, exc)","905","                    raise AssertionError(error_string_predict, Estimator)","913","                        print(error_string_transform, Estimator, e)","917","                    print(error_string_transform, Estimator, exc)","920","                    raise AssertionError(error_string_transform, Estimator)","924","def check_estimators_pickle(name, Estimator):","935","    # some estimators only take multioutputs","936","    y = multioutput_estimator_convert_y_2d(name, y)","938","    estimator = Estimator()","941","    set_testing_parameters(estimator)","951","    if Estimator.__module__.startswith('sklearn.'):","957","        assert_array_almost_equal(result[method], unpickled_result)","960","def check_estimators_partial_fit_n_features(name, Alg):","962","    if not hasattr(Alg, 'partial_fit'):","966","    with ignore_warnings(category=DeprecationWarning):","967","        alg = Alg()","968","    if not hasattr(alg, 'partial_fit'):","969","        # check again as for mlp this depends on algorithm","970","        return","972","    set_testing_parameters(alg)","974","        if isinstance(alg, ClassifierMixin):","976","            alg.partial_fit(X, y, classes=classes)","978","            alg.partial_fit(X, y)","982","    assert_raises(ValueError, alg.partial_fit, X[:, :-1], y)","985","def check_clustering(name, Alg):","991","    with ignore_warnings(category=DeprecationWarning):","992","        alg = Alg()","993","    set_testing_parameters(alg)","994","    if hasattr(alg, \"n_clusters\"):","995","        alg.set_params(n_clusters=3)","996","    set_random_state(alg)","998","        alg.set_params(preference=-100)","999","        alg.set_params(max_iter=100)","1002","    alg.fit(X)","1004","    alg.fit(X.tolist())","1006","    assert_equal(alg.labels_.shape, (n_samples,))","1007","    pred = alg.labels_","1013","    set_random_state(alg)","1015","        pred2 = alg.fit_predict(X)","1019","def check_clusterer_compute_labels_predict(name, Clusterer):","1022","    clusterer = Clusterer()","1035","def check_classifiers_one_label(name, Classifier):","1045","        classifier = Classifier()","1046","        set_testing_parameters(classifier)","1052","                print(error_string_fit, Classifier, e)","1058","            print(error_string_fit, Classifier, exc)","1065","            print(error_string_predict, Classifier, exc)","1070","def check_classifiers_train(name, Classifier):","1081","        classifier = Classifier()","1084","        set_testing_parameters(classifier)","1130","            assert_array_almost_equal(np.sum(y_prob, axis=1),","1131","                                      np.ones(n_samples))","1139","                assert_array_almost_equal(y_log_prob, np.log(y_prob), 8)","1144","def check_estimators_fit_returns_self(name, Estimator):","1147","    y = multioutput_estimator_convert_y_2d(name, y)","1151","    estimator = Estimator()","1153","    set_testing_parameters(estimator)","1160","def check_estimators_unfitted(name, Estimator):","1171","    est = Estimator()","1192","def check_supervised_y_2d(name, Estimator):","1199","    estimator = Estimator()","1200","    set_testing_parameters(estimator)","1221","    assert_array_almost_equal(y_pred.ravel(), y_pred_2d.ravel())","1224","def check_classifiers_classes(name, Classifier):","1241","        with ignore_warnings(category=DeprecationWarning):","1242","            classifier = Classifier()","1245","        set_testing_parameters(classifier)","1260","def check_regressors_int(name, Regressor):","1265","    y = multioutput_estimator_convert_y_2d(name, y)","1268","    regressor_1 = Regressor()","1269","    regressor_2 = Regressor()","1270","    set_testing_parameters(regressor_1)","1271","    set_testing_parameters(regressor_2)","1286","    assert_array_almost_equal(pred1, pred2, 2, name)","1290","def check_regressors_train(name, Regressor):","1294","    y = multioutput_estimator_convert_y_2d(name, y)","1296","    # catch deprecation warnings","1297","    regressor = Regressor()","1298","    set_testing_parameters(regressor)","1327","def check_regressors_no_decision_function(name, Regressor):","1331","    y = multioutput_estimator_convert_y_2d(name, X[:, 0])","1332","    regressor = Regressor()","1334","    set_testing_parameters(regressor)","1351","def check_class_weight_classifiers(name, Classifier):","1372","        with ignore_warnings(category=DeprecationWarning):","1373","            classifier = Classifier(class_weight=class_weight)","1385","def check_class_weight_balanced_classifiers(name, Classifier, X_train, y_train,","1386","                                            X_test, y_test, weights):","1387","    with ignore_warnings(category=DeprecationWarning):","1388","        classifier = Classifier()","1409","    with ignore_warnings(category=DeprecationWarning):","1410","        classifier = Classifier()","1430","    assert_array_almost_equal(coef_balanced, coef_manual)","1434","def check_estimators_overwrite_params(name, Estimator):","1436","    y = multioutput_estimator_convert_y_2d(name, y)","1439","    estimator = Estimator()","1441","    set_testing_parameters(estimator)","1485","def check_sparsify_coefficients(name, Estimator):","1489","    est = Estimator()","1507","def check_classifier_data_not_an_array(name, Estimator):","1510","    y = multioutput_estimator_convert_y_2d(name, y)","1511","    check_estimators_data_not_an_array(name, Estimator, X, y)","1512","","1513","","1514","def check_regressor_data_not_an_array(name, Estimator):","1515","    X, y = _boston_subset(n_samples=50)","1516","    y = multioutput_estimator_convert_y_2d(name, y)","1517","    check_estimators_data_not_an_array(name, Estimator, X, y)","1521","def check_estimators_data_not_an_array(name, Estimator, X, y):","1526","    estimator_1 = Estimator()","1527","    estimator_2 = Estimator()","1528","    set_testing_parameters(estimator_1)","1529","    set_testing_parameters(estimator_2)","1541","    assert_array_almost_equal(pred1, pred2, 2, name)","1606","def multioutput_estimator_convert_y_2d(name, y):","1609","    if \"MultiTask\" in name:","1615","def check_non_transformer_estimators_n_iter(name, Estimator):","1633","        estimator = Estimator(alpha=0.)","1635","        estimator = Estimator()","1639","        y_ = multioutput_estimator_convert_y_2d(name, y_)","1654","def check_transformer_n_iter(name, Estimator):","1657","    estimator = Estimator()","1680","def check_get_params_invariance(name, estimator):","1695","    if name in ('FeatureUnion', 'Pipeline'):","1696","        e = estimator([('clf', T())])","1697","","1698","    elif name in ('GridSearchCV', 'RandomizedSearchCV', 'SelectFromModel'):","1699","        return","1700","","1701","    else:","1702","        e = estimator()","1711","def check_classifiers_regression_target(name, Estimator):","1716","    e = Estimator()","1722","def check_decision_proba_consistency(name, Estimator):","1730","    estimator = Estimator()","1731","","1732","    set_testing_parameters(estimator)"]}],"sklearn\/tests\/test_common.py":[{"add":["30","    set_checking_parameters,","32","    check_no_fit_attributes_set_in_init,","67","        estimator = Estimator()","68","        # check this on class","69","        yield _named_check(","70","            check_no_fit_attributes_set_in_init, name), name, Estimator","71","","72","        for check in _yield_all_checks(name, estimator):","73","            set_checking_parameters(estimator)","74","            yield _named_check(check, name), name, estimator"],"delete":["65","        for check in _yield_all_checks(name, Estimator):","66","            yield _named_check(check, name), name, Estimator"]}],"sklearn\/utils\/tests\/test_estimator_checks.py":[{"add":["4","from sklearn.externals import joblib","7","from sklearn.utils.testing import (assert_raises_regex, assert_true,","8","                                   assert_equal)","10","from sklearn.utils.estimator_checks import set_random_state","11","from sklearn.utils.estimator_checks import set_checking_parameters","14","from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier","15","from sklearn.linear_model import LinearRegression, SGDClassifier","16","from sklearn.mixture import GaussianMixture","17","from sklearn.cluster import MiniBatchKMeans","18","from sklearn.decomposition import NMF","130","    assert_raises_regex(TypeError, msg, check_estimator, object())","134","    assert_raises_regex(AttributeError, msg, check_estimator, BaseEstimator())","137","    assert_raises_regex(AssertionError, msg, check_estimator,","138","                        BaseBadClassifier)","139","    assert_raises_regex(AssertionError, msg, check_estimator,","140","                        BaseBadClassifier())","153","    assert_raises_regex(AssertionError, msg, check_estimator,","154","                        NoCheckinPredict())","175","    msg = \"Estimator %s doesn't seem to fail gracefully on sparse data\" % name","192","    check_estimator(AdaBoostClassifier())","194","    check_estimator(MultiTaskElasticNet())","195","","196","","197","def test_check_estimator_clones():","198","    # check that check_estimator doesn't modify the estimator it receives","199","    from sklearn.datasets import load_iris","200","    iris = load_iris()","201","","202","    for Estimator in [GaussianMixture, LinearRegression,","203","                      RandomForestClassifier, NMF, SGDClassifier,","204","                      MiniBatchKMeans]:","205","        est = Estimator()","206","        set_checking_parameters(est)","207","        set_random_state(est)","208","        # without fitting","209","        old_hash = joblib.hash(est)","210","        check_estimator(est)","211","        assert_equal(old_hash, joblib.hash(est))","212","","213","        est = Estimator()","214","        set_checking_parameters(est)","215","        set_random_state(est)","216","        # with fitting","217","        est.fit(iris.data + 10, iris.target)","218","        old_hash = joblib.hash(est)","219","        check_estimator(est)","220","        assert_equal(old_hash, joblib.hash(est))","228","                        \"estimator\", NoSparseClassifier())","232","    check_estimators_unfitted(\"estimator\", CorrectNotFittedErrorClassifier())"],"delete":["6","from sklearn.utils.testing import assert_raises_regex, assert_true","10","from sklearn.ensemble import AdaBoostClassifier","127","    assert_raises_regex(AssertionError, msg, check_estimator, BaseBadClassifier)","160","    msg = \"Estimator \" + name + \" doesn't seem to fail gracefully on sparse data\"","185","                        \"estimator\", NoSparseClassifier)","189","    check_estimators_unfitted(\"estimator\", CorrectNotFittedErrorClassifier)"]}]}},"5fcf6f486fb0134c91f5a8741fe7e450539883f0":{"changes":{"sklearn\/svm\/base.py":"MODIFY"},"diff":{"sklearn\/svm\/base.py":[{"add":["768","    \"\"\"Used by Logistic Regression (and CV) and LinearSVC\/LinearSVR.","893","    y_ind = np.require(y_ind, requirements=\"W\")"],"delete":["768","    \"\"\"Used by Logistic Regression (and CV) and LinearSVC."]}]}},"2f661d0ed636be8e8ec5f197ece3c2ca6187f752":{"changes":{"sklearn\/neighbors\/classification.py":"MODIFY"},"diff":{"sklearn\/neighbors\/classification.py":[{"add":["235","        Range of parameter space to use by default for :meth:`radius_neighbors`"],"delete":["235","        Range of parameter space to use by default for :meth`radius_neighbors`"]}]}},"d01cdc204ee4972307aca4cc1e1b1e5e6347cc70":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/manifold\/tests\/test_spectral_embedding.py":"MODIFY",".travis.yml":"MODIFY","build_tools\/travis\/install.sh":"MODIFY","sklearn\/cluster\/tests\/test_spectral.py":"MODIFY","sklearn\/manifold\/spectral_embedding_.py":"MODIFY","examples\/cluster\/plot_face_segmentation.py":"MODIFY","sklearn\/cluster\/spectral.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["227","","229","  is smaller than ``n_clusters``. This may occur when the number of distinct","230","  points in the data set is actually smaller than the number of cluster one is","237","- Fixed a bug in :func:`cluster.spectral_clustering` where the normalization of","238","  the spectrum was using a division instead of a multiplication. :issue:`8129`","239","  by :user:`Jan Margeta <jmargeta>`, :user:`Guillaume Lemaitre <glemaitre>`,","240","  and :user:`Devansh D. <devanshdalal>`.","241",""],"delete":["227","  ","229","  is smaller than ``n_clusters``. This may occur when the number of distinct ","230","  points in the data set is actually smaller than the number of cluster one is "]}],"sklearn\/manifold\/tests\/test_spectral_embedding.py":[{"add":["0","import pytest","1","","17","from sklearn.utils.testing import assert_array_almost_equal","18","from sklearn.utils.testing import assert_array_equal","262","    embedding_2 = diffusion_map.T[:n_components]","266","","267","","268","def test_spectral_embedding_first_eigen_vector():","269","    # Test that the first eigenvector of spectral_embedding","270","    # is constant and that the second is not (for a connected graph)","271","    random_state = np.random.RandomState(36)","272","    data = random_state.randn(10, 30)","273","    sims = rbf_kernel(data)","274","    n_components = 2","275","","276","    for seed in range(10):","277","        embedding = spectral_embedding(sims,","278","                                       norm_laplacian=False,","279","                                       n_components=n_components,","280","                                       drop_first=False,","281","                                       random_state=seed)","282","","283","        assert np.std(embedding[:, 0]) == pytest.approx(0)","284","        assert np.std(embedding[:, 1]) > 1e-3"],"delete":["1","from numpy.testing import assert_array_almost_equal","2","from numpy.testing import assert_array_equal","260","    embedding_2 = diffusion_map.T[:n_components] * dd"]}],".travis.yml":[{"add":["41","    # It also runs tests requiring Pandas and PyAMG","44","           CYTHON_VERSION=\"0.26.1\" PYAMG_VERSION=\"3.3.2\" COVERAGE=true"],"delete":["41","    # It also runs tests requiring Pandas.","44","           CYTHON_VERSION=\"0.26.1\" COVERAGE=true"]}],"build_tools\/travis\/install.sh":[{"add":["39","    TO_INSTALL=\"python=$PYTHON_VERSION pip pytest pytest-cov \\","40","                numpy=$NUMPY_VERSION scipy=$SCIPY_VERSION \\","41","                cython=$CYTHON_VERSION\"","42","","44","        TO_INSTALL=\"$TO_INSTALL mkl\"","46","        TO_INSTALL=\"$TO_INSTALL nomkl\"","48","","49","    if [[ -n \"$PANDAS_VERSION\" ]]; then","50","        TO_INSTALL=\"$TO_INSTALL pandas=$PANDAS_VERSION\"","51","    fi","52","","53","    if [[ -n \"$PYAMG_VERSION\" ]]; then","54","        TO_INSTALL=\"$TO_INSTALL pyamg=$PYAMG_VERSION\"","55","    fi","56","","57","    conda create -n testenv --yes $TO_INSTALL"],"delete":["39","    # Configure the conda environment and put it in the path using the","40","    # provided versions","42","        conda create -n testenv --yes python=$PYTHON_VERSION pip \\","43","            pytest pytest-cov numpy=$NUMPY_VERSION scipy=$SCIPY_VERSION \\","44","            mkl cython=$CYTHON_VERSION \\","45","            ${PANDAS_VERSION+pandas=$PANDAS_VERSION}","46","            ","48","        conda create -n testenv --yes python=$PYTHON_VERSION pip \\","49","            pytest pytest-cov numpy=$NUMPY_VERSION scipy=$SCIPY_VERSION \\","50","            nomkl cython=$CYTHON_VERSION \\","51","            ${PANDAS_VERSION+pandas=$PANDAS_VERSION}"]}],"sklearn\/cluster\/tests\/test_spectral.py":[{"add":["1","from __future__ import division","6","from sklearn.externals.six.moves import cPickle","7","","16","from sklearn.feature_extraction import img_to_graph","22","try:","23","    from pyamg import smoothed_aggregation_solver  # noqa","24","    amg_loaded = True","25","except ImportError:","26","    amg_loaded = False","27","","50","                assert adjusted_rand_score(labels, [1, 1, 1, 0, 0, 0, 0]) == 1","52","                model_copy = cPickle.loads(cPickle.dumps(model))","53","                assert model_copy.n_clusters == model.n_clusters","54","                assert model_copy.eigen_solver == model.eigen_solver","100","    assert adjusted_rand_score(y, labels) == 1","114","    assert adjusted_rand_score(y, sp.labels_) == 1","118","    assert adjusted_rand_score(y, labels) == 1","130","            assert (X.shape[0],) == labels.shape","135","    assert (X.shape[0],) == labels.shape","144","    assert (X.shape[0],) == labels.shape","169","            assert adjusted_rand_score(y_true, y_pred) > 0.8","170","","171","","172","def test_spectral_clustering_with_arpack_amg_solvers():","173","    # Test that spectral_clustering is the same for arpack and amg solver","174","    # Based on toy example from plot_segmentation_toy.py","175","","176","    # a small two coin image","177","    x, y = np.indices((40, 40))","178","","179","    center1, center2 = (14, 12), (20, 25)","180","    radius1, radius2 = 8, 7","181","","182","    circle1 = (x - center1[0]) ** 2 + (y - center1[1]) ** 2 < radius1 ** 2","183","    circle2 = (x - center2[0]) ** 2 + (y - center2[1]) ** 2 < radius2 ** 2","184","","185","    circles = circle1 | circle2","186","    mask = circles.copy()","187","    img = circles.astype(float)","188","","189","    graph = img_to_graph(img, mask=mask)","190","    graph.data = np.exp(-graph.data \/ graph.data.std())","191","","192","    labels_arpack = spectral_clustering(","193","        graph, n_clusters=2, eigen_solver='arpack', random_state=0)","194","","195","    assert len(np.unique(labels_arpack)) == 2","196","","197","    if amg_loaded:","198","        labels_amg = spectral_clustering(","199","            graph, n_clusters=2, eigen_solver='amg', random_state=0)","200","        assert adjusted_rand_score(labels_arpack, labels_amg) == 1","201","    else:","202","        assert_raises(","203","            ValueError, spectral_clustering,","204","            graph, n_clusters=2, eigen_solver='amg', random_state=0)"],"delete":["1","","2","from sklearn.externals.six.moves import cPickle","3","","4","dumps, loads = cPickle.dumps, cPickle.loads","13","from sklearn.utils.testing import assert_greater","17","from sklearn.cluster.spectral import spectral_embedding","46","                assert_array_equal(labels, [1, 1, 1, 0, 0, 0, 0])","48","                model_copy = loads(dumps(model))","49","                assert_equal(model_copy.n_clusters, model.n_clusters)","50","                assert_equal(model_copy.eigen_solver, model.eigen_solver)","54","def test_spectral_amg_mode():","55","    # Test the amg mode of SpectralClustering","56","    centers = np.array([","57","        [0., 0., 0.],","58","        [10., 10., 10.],","59","        [20., 20., 20.],","60","    ])","61","    X, true_labels = make_blobs(n_samples=100, centers=centers,","62","                                cluster_std=1., random_state=42)","63","    D = pairwise_distances(X)  # Distance matrix","64","    S = np.max(D) - D  # Similarity matrix","65","    S = sparse.coo_matrix(S)","66","    try:","67","        from pyamg import smoothed_aggregation_solver  # noqa","68","","69","        amg_loaded = True","70","    except ImportError:","71","        amg_loaded = False","72","    if amg_loaded:","73","        labels = spectral_clustering(S, n_clusters=len(centers),","74","                                     random_state=0, eigen_solver=\"amg\")","75","        # We don't care too much that it's good, just that it *worked*.","76","        # There does have to be some lower limit on the performance though.","77","        assert_greater(np.mean(labels == true_labels), .3)","78","    else:","79","        assert_raises(ValueError, spectral_embedding, S,","80","                      n_components=len(centers),","81","                      random_state=0, eigen_solver=\"amg\")","82","","83","","126","    assert_equal(adjusted_rand_score(y, labels), 1)","140","    assert_equal(adjusted_rand_score(y, sp.labels_), 1)","144","    assert_equal(adjusted_rand_score(y, labels), 1)","156","            assert_equal((X.shape[0],), labels.shape)","161","    assert_equal((X.shape[0],), labels.shape)","170","    assert_equal((X.shape[0],), labels.shape)","195","            assert_greater(adjusted_rand_score(y_true, y_pred), 0.8)"]}],"sklearn\/manifold\/spectral_embedding_.py":[{"add":["6","from __future__ import division","7","","273","            embedding = diffusion_map.T[n_components::-1]","274","            if norm_laplacian:","275","                embedding = embedding \/ dd","298","        embedding = diffusion_map.T","299","        if norm_laplacian:","300","            embedding = embedding \/ dd","315","            embedding = diffusion_map.T[:n_components]","316","            if norm_laplacian:","317","                embedding = embedding \/ dd","326","            embedding = diffusion_map.T[:n_components]","327","            if norm_laplacian:","328","                embedding = embedding \/ dd"],"delete":["271","            embedding = diffusion_map.T[n_components::-1] * dd","294","        embedding = diffusion_map.T * dd","309","            embedding = diffusion_map.T[:n_components] * dd","318","            embedding = diffusion_map.T[:n_components] * dd"]}],"examples\/cluster\/plot_face_segmentation.py":[{"add":["65","                                 assign_labels=assign_labels, random_state=42)"],"delete":["65","                                 assign_labels=assign_labels, random_state=1)"]}],"sklearn\/cluster\/spectral.py":[{"add":["258","","259","    # The first eigen vector is constant only for fully connected graphs","260","    # and should be kept for spectral clustering (drop_first = False)","261","    # See spectral_embedding documentation."],"delete":[]}]}},"f4b230cc65f2a9091545779cb8595848765904fc":{"changes":{"examples\/decomposition\/plot_image_denoising.py":"MODIFY"},"diff":{"examples\/decomposition\/plot_image_denoising.py":[{"add":["55","face = face \/ 255."],"delete":["55","face = face \/ 255"]}]}},"08f04d95261d989b26683d0ba5bad4c8eb7c3040":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/ensemble\/gradient_boosting.py":"MODIFY","doc\/modules\/ensemble.rst":"MODIFY","sklearn\/ensemble\/tests\/test_gradient_boosting.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["53","- :class:`ensemble.gradient_boosting.GradientBoostingClassifier` (bug fix affecting feature importances)","433","- Fixed a bug in :class:`ensemble.gradient_boosting.GradientBoostingRegressor`","434","  and :class:`ensemble.gradient_boosting.GradientBoostingClassifier` to have","435","  feature importances summed and then normalized, rather than normalizing on a","436","  per-tree basis. The previous behavior over-weighted the Gini importance of","437","  features that appear in later stages. This issue only affected feature","438","  importances. :issue:`11176` by :user:`Gil Forsyth <gforsyth>`.","439",""],"delete":[]}],"sklearn\/ensemble\/gradient_boosting.py":[{"add":["1231","            stage_sum = sum(tree.tree_.compute_feature_importances(","1232","                normalize=False) for tree in stage) \/ len(stage)","1236","        importances \/= importances.sum()"],"delete":["1231","            stage_sum = sum(tree.feature_importances_","1232","                            for tree in stage) \/ len(stage)"]}],"doc\/modules\/ensemble.rst":[{"add":["784","    array([0.10..., 0.10..., 0.11..., ..."],"delete":["784","    array([0.11, 0.1 , 0.11, ..."]}],"sklearn\/ensemble\/tests\/test_gradient_boosting.py":[{"add":["14","from sklearn.datasets import make_classification, fetch_california_housing","454","def test_feature_importance_regression():","455","    \"\"\"Test that Gini importance is calculated correctly.","456","","457","    This test follows the example from [1]_ (pg. 373).","458","","459","    .. [1] Friedman, J., Hastie, T., & Tibshirani, R. (2001). The elements","460","       of statistical learning. New York: Springer series in statistics.","461","    \"\"\"","462","    california = fetch_california_housing()","463","    X, y = california.data, california.target","464","    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)","465","","466","    reg = GradientBoostingRegressor(loss='huber', learning_rate=0.1,","467","                                    max_leaf_nodes=6, n_estimators=100,","468","                                    random_state=0)","469","    reg.fit(X_train, y_train)","470","    sorted_idx = np.argsort(reg.feature_importances_)[::-1]","471","    sorted_features = [california.feature_names[s] for s in sorted_idx]","472","","473","    # The most important feature is the median income by far.","474","    assert sorted_features[0] == 'MedInc'","475","","476","    # The three subsequent features are the following. Their relative ordering","477","    # might change a bit depending on the randomness of the trees and the","478","    # train \/ test split.","479","    assert set(sorted_features[1:4]) == {'Longitude', 'AveOccup', 'Latitude'}","480","","481",""],"delete":["14","from sklearn.datasets import make_classification"]}]}},"0e4bdfdfc6934515fc03fae1845f223e947ce28f":{"changes":{"sklearn\/linear_model\/tests\/test_coordinate_descent.py":"MODIFY","sklearn\/linear_model\/coordinate_descent.py":"MODIFY"},"diff":{"sklearn\/linear_model\/tests\/test_coordinate_descent.py":[{"add":["177","def test_lasso_cv_with_some_model_selection():","178","    from sklearn.pipeline import make_pipeline","179","    from sklearn.preprocessing import StandardScaler","180","    from sklearn.model_selection import StratifiedKFold","181","    from sklearn import datasets","182","    from sklearn.linear_model import LassoCV","183","","184","    diabetes = datasets.load_diabetes()","185","    X = diabetes.data","186","    y = diabetes.target","187","","188","    pipe = make_pipeline(","189","        StandardScaler(),","190","        LassoCV(cv=StratifiedKFold(n_splits=5))","191","    )","192","    pipe.fit(X, y)","193","","194",""],"delete":[]}],"sklearn\/linear_model\/coordinate_descent.py":[{"add":["1152","        folds = list(cv.split(X, y))"],"delete":["1152","        folds = list(cv.split(X))"]}]}},"bdb00d17b9b82601c2c6865c2df6de1f5ce357bb":{"changes":{"sklearn\/covariance\/tests\/test_covariance.py":"MODIFY","sklearn\/utils\/extmath.py":"MODIFY"},"diff":{"sklearn\/covariance\/tests\/test_covariance.py":[{"add":["123","    lw_cov_from_mle, lw_shrinkage_from_mle = ledoit_wolf(X_centered,","126","    assert_almost_equal(lw_shrinkage_from_mle, lw.shrinkage_)","136","    lw_cov_from_mle, lw_shrinkage_from_mle = ledoit_wolf(X_1d,","139","    assert_almost_equal(lw_shrinkage_from_mle, lw.shrinkage_)","157","    lw_cov_from_mle, lw_shrinkage_from_mle = ledoit_wolf(X)","159","    assert_almost_equal(lw_shrinkage_from_mle, lw.shrinkage_)","169","    lw_cov_from_mle, lw_shrinkage_from_mle = ledoit_wolf(X_1d)","171","    assert_almost_equal(lw_shrinkage_from_mle, lw.shrinkage_)","245","    oa_cov_from_mle, oa_shrinkage_from_mle = oas(X_centered,","248","    assert_almost_equal(oa_shrinkage_from_mle, oa.shrinkage_)","258","    oa_cov_from_mle, oa_shrinkage_from_mle = oas(X_1d, assume_centered=True)","260","    assert_almost_equal(oa_shrinkage_from_mle, oa.shrinkage_)","276","    oa_cov_from_mle, oa_shrinkage_from_mle = oas(X)","278","    assert_almost_equal(oa_shrinkage_from_mle, oa.shrinkage_)","288","    oa_cov_from_mle, oa_shrinkage_from_mle = oas(X_1d)","290","    assert_almost_equal(oa_shrinkage_from_mle, oa.shrinkage_)"],"delete":["123","    lw_cov_from_mle, lw_shinkrage_from_mle = ledoit_wolf(X_centered,","126","    assert_almost_equal(lw_shinkrage_from_mle, lw.shrinkage_)","136","    lw_cov_from_mle, lw_shinkrage_from_mle = ledoit_wolf(X_1d,","139","    assert_almost_equal(lw_shinkrage_from_mle, lw.shrinkage_)","157","    lw_cov_from_mle, lw_shinkrage_from_mle = ledoit_wolf(X)","159","    assert_almost_equal(lw_shinkrage_from_mle, lw.shrinkage_)","169","    lw_cov_from_mle, lw_shinkrage_from_mle = ledoit_wolf(X_1d)","171","    assert_almost_equal(lw_shinkrage_from_mle, lw.shrinkage_)","245","    oa_cov_from_mle, oa_shinkrage_from_mle = oas(X_centered,","248","    assert_almost_equal(oa_shinkrage_from_mle, oa.shrinkage_)","258","    oa_cov_from_mle, oa_shinkrage_from_mle = oas(X_1d, assume_centered=True)","260","    assert_almost_equal(oa_shinkrage_from_mle, oa.shrinkage_)","276","    oa_cov_from_mle, oa_shinkrage_from_mle = oas(X)","278","    assert_almost_equal(oa_shinkrage_from_mle, oa.shrinkage_)","288","    oa_cov_from_mle, oa_shinkrage_from_mle = oas(X_1d)","290","    assert_almost_equal(oa_shinkrage_from_mle, oa.shrinkage_)"]}],"sklearn\/utils\/extmath.py":[{"add":["365","        # Checks if the number of iterations is explicitly specified"],"delete":["365","        # Checks if the number of iterations is explicitely specified"]}]}},"6a3562281103d30c8bb937ee64c3b349c3d76370":{"changes":{"sklearn\/linear_model\/tests\/test_least_angle.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/linear_model\/tests\/test_randomized_l1.py":"MODIFY","sklearn\/linear_model\/least_angle.py":"MODIFY"},"diff":{"sklearn\/linear_model\/tests\/test_least_angle.py":[{"add":["14","from sklearn.utils.testing import assert_warns","432","    X = np.c_[X, rng.randn(X.shape[0], 5)]  # add 5 bad features"],"delete":["14","from sklearn.utils.testing import assert_no_warnings, assert_warns","432","    X = np.c_[X, rng.randn(X.shape[0], 4)]  # add 4 bad features","446","def test_no_warning_for_zero_mse():","447","    # LassoLarsIC should not warn for log of zero MSE.","448","    y = np.arange(10, dtype=float)","449","    X = y.reshape(-1, 1)","450","    lars = linear_model.LassoLarsIC(normalize=False)","451","    assert_no_warnings(lars.fit, X, y)","452","    assert_true(np.any(np.isinf(lars.criterion_)))","453","","454",""]}],"doc\/whats_new.rst":[{"add":["374","   - Fix AIC\/BIC criterion computation in :class:`linear_model.LassoLarsIC`","375","     by `Alexandre Gramfort`_ and :user:`Mehmet Basbug <mehmetbasbug>`.","376",""],"delete":[]}],"sklearn\/linear_model\/tests\/test_randomized_l1.py":[{"add":["12","from sklearn.utils.testing import assert_allclose","97","    assert_allclose(feature_scores, [1., 1., 1., 0.225, 1.], rtol=0.2)"],"delete":["96","    assert_array_equal(feature_scores, X.shape[1] * [1.])"]}],"sklearn\/linear_model\/least_angle.py":[{"add":["1402","        alphas. The alpha which has the smallest information criteria is chosen.","1403","        This value is larger by a factor of ``n_samples`` compared to Eqns. 2.15","1404","        and 2.16 in (Zou et al, 2007).","1405","","1491","        sigma2 = np.var(y)","1504","        eps64 = np.finfo('float64').eps","1505","        self.criterion_ = (n_samples * mean_squared_error \/ (sigma2 + eps64) +","1506","                           K * df)  # Eqns. 2.15--16 in (Zou et al, 2007)"],"delete":["1402","        alphas. The alpha which has the smallest information criteria","1403","        is chosen.","1501","        with np.errstate(divide='ignore'):","1502","            self.criterion_ = n_samples * np.log(mean_squared_error) + K * df"]}]}},"a74d31a339b440817f43e407c59026cfba980fb9":{"changes":{"examples\/cross_decomposition\/plot_compare_cross_decomposition.py":"MODIFY","examples\/exercises\/plot_iris_exercise.py":"MODIFY","examples\/ensemble\/plot_isolation_forest.py":"MODIFY","examples\/decomposition\/plot_kernel_pca.py":"MODIFY","examples\/ensemble\/plot_voting_probas.py":"MODIFY","examples\/datasets\/plot_random_dataset.py":"MODIFY","examples\/ensemble\/plot_adaboost_twoclass.py":"MODIFY","examples\/ensemble\/plot_random_forest_regression_multioutput.py":"MODIFY","examples\/ensemble\/plot_partial_dependence.py":"MODIFY","examples\/ensemble\/plot_forest_iris.py":"MODIFY","examples\/ensemble\/plot_voting_decision_regions.py":"MODIFY","examples\/datasets\/plot_iris_dataset.py":"MODIFY","examples\/ensemble\/plot_random_forest_embedding.py":"MODIFY"},"diff":{"examples\/cross_decomposition\/plot_compare_cross_decomposition.py":[{"add":["63","plt.scatter(X_train_r[:, 0], Y_train_r[:, 0], label=\"train\",","64","            marker=\"o\", c=\"b\", s=25)","65","plt.scatter(X_test_r[:, 0], Y_test_r[:, 0], label=\"test\",","66","            marker=\"o\", c=\"r\", s=25)","76","plt.scatter(X_train_r[:, 1], Y_train_r[:, 1], label=\"train\",","77","            marker=\"o\", c=\"b\", s=25)","78","plt.scatter(X_test_r[:, 1], Y_test_r[:, 1], label=\"test\",","79","            marker=\"o\", c=\"r\", s=25)","90","plt.scatter(X_train_r[:, 0], X_train_r[:, 1], label=\"train\",","91","            marker=\"*\", c=\"b\", s=50)","92","plt.scatter(X_test_r[:, 0], X_test_r[:, 1], label=\"test\",","93","            marker=\"*\", c=\"r\", s=50)","103","plt.scatter(Y_train_r[:, 0], Y_train_r[:, 1], label=\"train\",","104","            marker=\"*\", c=\"b\", s=50)","105","plt.scatter(Y_test_r[:, 0], Y_test_r[:, 1], label=\"test\",","106","            marker=\"*\", c=\"r\", s=50)"],"delete":["63","plt.plot(X_train_r[:, 0], Y_train_r[:, 0], \"ob\", label=\"train\")","64","plt.plot(X_test_r[:, 0], Y_test_r[:, 0], \"or\", label=\"test\")","74","plt.plot(X_train_r[:, 1], Y_train_r[:, 1], \"ob\", label=\"train\")","75","plt.plot(X_test_r[:, 1], Y_test_r[:, 1], \"or\", label=\"test\")","86","plt.plot(X_train_r[:, 0], X_train_r[:, 1], \"*b\", label=\"train\")","87","plt.plot(X_test_r[:, 0], X_test_r[:, 1], \"*r\", label=\"test\")","97","plt.plot(Y_train_r[:, 0], Y_train_r[:, 1], \"*b\", label=\"train\")","98","plt.plot(Y_test_r[:, 0], Y_test_r[:, 1], \"*r\", label=\"test\")"]}],"examples\/exercises\/plot_iris_exercise.py":[{"add":["43","    plt.scatter(X[:, 0], X[:, 1], c=y, zorder=10, cmap=plt.cm.Paired,","44","                edgecolor='k', s=20)","47","    plt.scatter(X_test[:, 0], X_test[:, 1], s=80, facecolors='none',","48","                zorder=10, edgecolor='k')"],"delete":["43","    plt.scatter(X[:, 0], X[:, 1], c=y, zorder=10, cmap=plt.cm.Paired)","46","    plt.scatter(X_test[:, 0], X_test[:, 1], s=80, facecolors='none', zorder=10)"]}],"examples\/ensemble\/plot_isolation_forest.py":[{"add":["58","b1 = plt.scatter(X_train[:, 0], X_train[:, 1], c='white',","59","                 s=20, edgecolor='k')","60","b2 = plt.scatter(X_test[:, 0], X_test[:, 1], c='green',","61","                 s=20, edgecolor='k')","62","c = plt.scatter(X_outliers[:, 0], X_outliers[:, 1], c='red',","63","                s=20, edgecolor='k')"],"delete":["58","b1 = plt.scatter(X_train[:, 0], X_train[:, 1], c='white')","59","b2 = plt.scatter(X_test[:, 0], X_test[:, 1], c='green')","60","c = plt.scatter(X_outliers[:, 0], X_outliers[:, 1], c='red')"]}],"examples\/decomposition\/plot_kernel_pca.py":[{"add":["38","plt.scatter(X[reds, 0], X[reds, 1], c=\"red\",","39","            s=20, edgecolor='k')","40","plt.scatter(X[blues, 0], X[blues, 1], c=\"blue\",","41","            s=20, edgecolor='k')","52","plt.scatter(X_pca[reds, 0], X_pca[reds, 1], c=\"red\",","53","            s=20, edgecolor='k')","54","plt.scatter(X_pca[blues, 0], X_pca[blues, 1], c=\"blue\",","55","            s=20, edgecolor='k')","61","plt.scatter(X_kpca[reds, 0], X_kpca[reds, 1], c=\"red\",","62","            s=20, edgecolor='k')","63","plt.scatter(X_kpca[blues, 0], X_kpca[blues, 1], c=\"blue\",","64","            s=20, edgecolor='k')","70","plt.scatter(X_back[reds, 0], X_back[reds, 1], c=\"red\",","71","            s=20, edgecolor='k')","72","plt.scatter(X_back[blues, 0], X_back[blues, 1], c=\"blue\",","73","            s=20, edgecolor='k')"],"delete":["38","plt.plot(X[reds, 0], X[reds, 1], \"ro\")","39","plt.plot(X[blues, 0], X[blues, 1], \"bo\")","50","plt.plot(X_pca[reds, 0], X_pca[reds, 1], \"ro\")","51","plt.plot(X_pca[blues, 0], X_pca[blues, 1], \"bo\")","57","plt.plot(X_kpca[reds, 0], X_kpca[reds, 1], \"ro\")","58","plt.plot(X_kpca[blues, 0], X_kpca[blues, 1], \"bo\")","64","plt.plot(X_back[reds, 0], X_back[reds, 1], \"ro\")","65","plt.plot(X_back[blues, 0], X_back[blues, 1], \"bo\")"]}],"examples\/ensemble\/plot_voting_probas.py":[{"add":["58","p1 = ax.bar(ind, np.hstack(([class1_1[:-1], [0]])), width,","59","            color='green', edgecolor='k')","60","p2 = ax.bar(ind + width, np.hstack(([class2_1[:-1], [0]])), width,","61","            color='lightgreen', edgecolor='k')","64","p3 = ax.bar(ind, [0, 0, 0, class1_1[-1]], width,","65","            color='blue', edgecolor='k')","66","p4 = ax.bar(ind + width, [0, 0, 0, class2_1[-1]], width,","67","            color='steelblue', edgecolor='k')"],"delete":["58","p1 = ax.bar(ind, np.hstack(([class1_1[:-1], [0]])), width, color='green')","59","p2 = ax.bar(ind + width, np.hstack(([class2_1[:-1], [0]])), width, color='lightgreen')","62","p3 = ax.bar(ind, [0, 0, 0, class1_1[-1]], width, color='blue')","63","p4 = ax.bar(ind + width, [0, 0, 0, class2_1[-1]], width, color='steelblue')"]}],"examples\/datasets\/plot_random_dataset.py":[{"add":["29","plt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1,","30","            s=25, edgecolor='k')","36","plt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1,","37","            s=25, edgecolor='k')","40","plt.title(\"Two informative features, two clusters per class\",","41","          fontsize='small')","43","plt.scatter(X2[:, 0], X2[:, 1], marker='o', c=Y2,","44","            s=25, edgecolor='k')","51","plt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1,","52","            s=25, edgecolor='k')","57","plt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1,","58","            s=25, edgecolor='k')","63","plt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1,","64","            s=25, edgecolor='k')"],"delete":["29","plt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1)","35","plt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1)","38","plt.title(\"Two informative features, two clusters per class\", fontsize='small')","40","plt.scatter(X2[:, 0], X2[:, 1], marker='o', c=Y2)","41","","48","plt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1)","53","plt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1)","58","plt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1)"]}],"examples\/ensemble\/plot_adaboost_twoclass.py":[{"add":["72","                s=20, edgecolor='k',","91","             alpha=.5,","92","             edgecolor='k')"],"delete":["90","             alpha=.5)"]}],"examples\/ensemble\/plot_random_forest_regression_multioutput.py":[{"add":["61","plt.scatter(y_test[:, 0], y_test[:, 1], edgecolor='k',","63","plt.scatter(y_multirf[:, 0], y_multirf[:, 1], edgecolor='k',","66","plt.scatter(y_rf[:, 0], y_rf[:, 1], edgecolor='k',"],"delete":["61","plt.scatter(y_test[:, 0], y_test[:, 1],","63","plt.scatter(y_multirf[:, 0], y_multirf[:, 1],","66","plt.scatter(y_rf[:, 0], y_rf[:, 1],"]}],"examples\/ensemble\/plot_partial_dependence.py":[{"add":["97","    surf = ax.plot_surface(XX, YY, Z, rstride=1, cstride=1,","98","                           cmap=plt.cm.BuPu, edgecolor='k')","105","    plt.suptitle('Partial dependence of house value on median\\n'","106","                 'age and average occupancy')"],"delete":["97","    surf = ax.plot_surface(XX, YY, Z, rstride=1, cstride=1, cmap=plt.cm.BuPu)","104","    plt.suptitle('Partial dependence of house value on median age and '","105","                 'average occupancy')"]}],"examples\/ensemble\/plot_forest_iris.py":[{"add":["12","In the first row, the classifiers are built using the sepal width and","13","the sepal length features only, on the second row using the petal length and","14","sepal length only, and on the third row using the petal width and the","15","petal length only.","18","4 features using 30 estimators and scored using 10 fold cross validation,","19","we see::","26","Increasing `max_depth` for AdaBoost lowers the standard deviation of","27","the scores (but the average score does not improve).","98","        model_title = str(type(model)).split(","99","            \".\")[-1][:-2][:-len(\"Classifier\")]","100","","103","            model_details += \" with {} estimators\".format(","104","                len(model.estimators_))","105","        print(model_details + \" with features\", pair,","106","              \"has a score of\", scores)","127","            # Choose alpha blend level with respect to the number","128","            # of estimators","139","        # surfaces. These points are regularly space and do not have a","140","        # black outline","141","        xx_coarser, yy_coarser = np.meshgrid(","142","            np.arange(x_min, x_max, plot_step_coarser),","143","            np.arange(y_min, y_max, plot_step_coarser))","144","        Z_points_coarser = model.predict(np.c_[xx_coarser.ravel(),","145","                                         yy_coarser.ravel()]","146","                                         ).reshape(xx_coarser.shape)","147","        cs_points = plt.scatter(xx_coarser, yy_coarser, s=15,","148","                                c=Z_points_coarser, cmap=cmap,","149","                                edgecolors=\"none\")","154","                    cmap=ListedColormap(['r', 'y', 'b']),","155","                    edgecolor='k', s=20)"],"delete":["12","In the first row, the classifiers are built using the sepal width and the sepal","13","length features only, on the second row using the petal length and sepal length","14","only, and on the third row using the petal width and the petal length only.","17","4 features using 30 estimators and scored using 10 fold cross validation, we see::","24","Increasing `max_depth` for AdaBoost lowers the standard deviation of the scores (but","25","the average score does not improve).","50","from sklearn.externals.six.moves import xrange","97","        model_title = str(type(model)).split(\".\")[-1][:-2][:-len(\"Classifier\")]","100","            model_details += \" with {} estimators\".format(len(model.estimators_))","101","        print( model_details + \" with features\", pair, \"has a score of\", scores )","122","            # Choose alpha blend level with respect to the number of estimators","133","        # surfaces. These points are regularly space and do not have a black outline","134","        xx_coarser, yy_coarser = np.meshgrid(np.arange(x_min, x_max, plot_step_coarser),","135","                                             np.arange(y_min, y_max, plot_step_coarser))","136","        Z_points_coarser = model.predict(np.c_[xx_coarser.ravel(), yy_coarser.ravel()]).reshape(xx_coarser.shape)","137","        cs_points = plt.scatter(xx_coarser, yy_coarser, s=15, c=Z_points_coarser, cmap=cmap, edgecolors=\"none\")","142","                    cmap=ListedColormap(['r', 'y', 'b']))"]}],"examples\/ensemble\/plot_voting_decision_regions.py":[{"add":["68","    axarr[idx[0], idx[1]].scatter(X[:, 0], X[:, 1], c=y,","69","                                  s=20, edgecolor='k')"],"delete":["68","    axarr[idx[0], idx[1]].scatter(X[:, 0], X[:, 1], c=y, alpha=0.8)"]}],"examples\/datasets\/plot_iris_dataset.py":[{"add":["42","plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired,","43","            edgecolor='k')","58","           cmap=plt.cm.Paired, edgecolor='k')"],"delete":["42","plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)","57","           cmap=plt.cm.Paired)"]}],"examples\/ensemble\/plot_random_forest_embedding.py":[{"add":["16","separate two concentric circles simply based on the principal components","17","of the transformed data with truncated SVD.","59","ax.scatter(X[:, 0], X[:, 1], c=y, s=50, edgecolor='k')","65","ax.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y, s=50, edgecolor='k')","71","# Plot the decision in original space. For that, we will assign a color","72","# to each point in the mesh [x_min, x_max]x[y_min, y_max].","85","ax.scatter(X[:, 0], X[:, 1], c=y, s=50, edgecolor='k')","97","ax.scatter(X[:, 0], X[:, 1], c=y, s=50, edgecolor='k')"],"delete":["16","separate two concentric circles simply based on the principal components of the","17","transformed data with truncated SVD.","59","ax.scatter(X[:, 0], X[:, 1], c=y, s=50)","65","ax.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y, s=50)","71","# Plot the decision in original space. For that, we will assign a color to each","72","# point in the mesh [x_min, x_max]x[y_min, y_max].","85","ax.scatter(X[:, 0], X[:, 1], c=y, s=50)","97","ax.scatter(X[:, 0], X[:, 1], c=y, s=50)"]}]}},"50d3fe963c3d2823b2ba2a534b8a50dba620fc5e":{"changes":{"sklearn\/semi_supervised\/tests\/test_label_propagation.py":"MODIFY","sklearn\/semi_supervised\/_label_propagation.py":"MODIFY"},"diff":{"sklearn\/semi_supervised\/tests\/test_label_propagation.py":[{"add":["159","def test_label_propagation_non_zero_normalizer():","160","    # check that we don't divide by zero in case of null normalizer","161","    # non-regression test for","162","    # https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/15946","163","    X = np.array([[100., 100.], [100., 100.], [0., 0.], [0., 0.]])","164","    y = np.array([0, 1, -1, -1])","165","    mdl = label_propagation.LabelSpreading(kernel='knn',","166","                                           max_iter=100,","167","                                           n_neighbors=1)","168","    assert_no_warnings(mdl.fit, X, y)","169","","170",""],"delete":[]}],"sklearn\/semi_supervised\/_label_propagation.py":[{"add":["292","        normalizer[normalizer == 0] = 1"],"delete":[]}]}},"328b04f43e3dc22a4262e62e13312a0e98c2637f":{"changes":{"doc\/tutorial\/machine_learning_map\/pyparsing.py":"MODIFY","sklearn\/datasets\/samples_generator.py":"MODIFY","sklearn\/tests\/test_multioutput.py":"MODIFY","sklearn\/cluster\/tests\/test_hierarchical.py":"MODIFY"},"diff":{"doc\/tutorial\/machine_learning_map\/pyparsing.py":[{"add":["1304","           Fail action fn is a callable function that takes the arguments\r"],"delete":["1304","           Fail acton fn is a callable function that takes the arguments\r"]}],"sklearn\/datasets\/samples_generator.py":[{"add":["707","        If int, it is the total number of points equally divided among"],"delete":["707","        If int, it is the the total number of points equally divided among"]}],"sklearn\/tests\/test_multioutput.py":[{"add":["206","def test_multi_output_classification_partial_fit_no_first_classes_exception():"],"delete":["206","def test_mutli_output_classifiation_partial_fit_no_first_classes_exception():"]}],"sklearn\/cluster\/tests\/test_hierarchical.py":[{"add":["298","            # Sort the order of child nodes per row for consistency"],"delete":["298","            # Sort the order of of child nodes per row for consistency"]}]}},"9a5e160332bb79cd7e7c8015c7dd25a654788528":{"changes":{"sklearn\/utils\/estimator_checks.py":"MODIFY"},"diff":{"sklearn\/utils\/estimator_checks.py":[{"add":["1106","                if n_classes == 2:","1110","                if n_classes == 3:"],"delete":["1106","                if n_classes is 2:","1110","                if (n_classes is 3 and not isinstance(classifier, BaseLibSVM)):","1111","                    # 1on1 of LibSVM works differently"]}]}},"e9b7883e12806a28499abc364313d852cb580b77":{"changes":{"sklearn\/gaussian_process\/_gpr.py":"MODIFY"},"diff":{"sklearn\/gaussian_process\/_gpr.py":[{"add":["488","                0.5 * np.einsum(\"ijl,jik->kl\", tmp, K_gradient)"],"delete":["488","                0.5 * np.einsum(\"ijl,ijk->kl\", tmp, K_gradient)"]}]}},"aaf9cf09e51e5c7ee3c331a05385a74392da8ed6":{"changes":{"doc\/modules\/decomposition.rst":"MODIFY"},"diff":{"doc\/modules\/decomposition.rst":[{"add":["850","    b. Draw the observed word :math:`w_{ij} \\sim \\mathrm{Multinomial}(\\beta_{z_{di}})`"],"delete":["850","    b. Draw the observed word :math:`w_{ij} \\sim \\mathrm{Multinomial}(beta_{z_{di}}.)`"]}]}},"71770dc464810d1327c633f92b55e58b4d6a0bb6":{"changes":{"sklearn\/utils\/metaestimators.py":"MODIFY"},"diff":{"sklearn\/utils\/metaestimators.py":[{"add":["154","    Labels y will always be indexed only along the first axis.","163","        Data to be indexed. If ``estimator._pairwise is True``,","167","        Targets to be indexed.","180","    X_subset : array-like, sparse matrix or list","181","        Indexed data.","183","    y_subset : array-like, sparse matrix or list","184","        Indexed targets."],"delete":["154","    Labels y will always be sliced only along the last axis.","163","        Data to be sliced. If ``estimator._pairwise is True``,","167","        Targets to be sliced.","180","    X_sliced : array-like, sparse matrix or list","181","        Sliced data.","183","    y_sliced : array-like, sparse matrix or list","184","        Sliced targets."]}]}},"1c1566edc551c9588efadc325f6f29a3060545f3":{"changes":{"sklearn\/utils\/extmath.py":"MODIFY","sklearn\/decomposition\/pca.py":"MODIFY","sklearn\/utils\/tests\/test_extmath.py":"MODIFY","sklearn\/decomposition\/tests\/test_pca.py":"MODIFY"},"diff":{"sklearn\/utils\/extmath.py":[{"add":["197","    if A.dtype.kind == 'f':","198","        # Ensure f32 is preserved as f32","199","        Q = Q.astype(A.dtype, copy=False)","332",""],"delete":[]}],"sklearn\/decomposition\/pca.py":[{"add":["358","        X = check_array(X, dtype=[np.float64, np.float32], ensure_2d=True,"],"delete":["358","        X = check_array(X, dtype=[np.float64], ensure_2d=True,"]}],"sklearn\/utils\/tests\/test_extmath.py":[{"add":["101","def check_randomized_svd_low_rank(dtype):","107","    decimal = 5 if dtype == np.float32 else 7","108","    dtype = np.dtype(dtype)","114","                             random_state=0).astype(dtype, copy=False)","120","    # Convert the singular values to the specific dtype","121","    U = U.astype(dtype, copy=False)","122","    s = s.astype(dtype, copy=False)","123","    V = V.astype(dtype, copy=False)","124","","127","        Ua, sa, Va = randomized_svd(","128","            X, k, power_iteration_normalizer=normalizer, random_state=0)","129","","130","        # If the input dtype is float, then the output dtype is float of the","131","        # same bit size (f32 is not upcast to f64)","132","        # But if the input dtype is int, the output dtype is float64","133","        if dtype.kind == 'f':","134","            assert Ua.dtype == dtype","135","            assert sa.dtype == dtype","136","            assert Va.dtype == dtype","137","        else:","138","            assert Ua.dtype == np.float64","139","            assert sa.dtype == np.float64","140","            assert Va.dtype == np.float64","141","","148","        assert_almost_equal(s[:k], sa, decimal=decimal)","151","        assert_almost_equal(np.dot(U[:, :k], V[:k, :]), np.dot(Ua, Va),","152","                            decimal=decimal)","161","        if dtype.kind == 'f':","162","            assert Ua.dtype == dtype","163","            assert sa.dtype == dtype","164","            assert Va.dtype == dtype","165","        else:","166","            assert Ua.dtype.kind == 'f'","167","            assert sa.dtype.kind == 'f'","168","            assert Va.dtype.kind == 'f'","169","","170","        assert_almost_equal(s[:rank], sa[:rank], decimal=decimal)","171","","172","","173","def test_randomized_svd_low_rank_all_dtypes():","174","    for dtype in (np.int32, np.int64, np.float32, np.float64):","175","        yield check_randomized_svd_low_rank, dtype"],"delete":["101","def test_randomized_svd_low_rank():","112","                             random_state=0)","120","        Ua, sa, Va = \\","121","            randomized_svd(X, k, power_iteration_normalizer=normalizer,","122","                           random_state=0)","129","        assert_almost_equal(s[:k], sa)","132","        assert_almost_equal(np.dot(U[:, :k], V[:k, :]), np.dot(Ua, Va))","141","        assert_almost_equal(s[:rank], sa[:rank])"]}],"sklearn\/decomposition\/tests\/test_pca.py":[{"add":["600","","601","","602","def test_pca_dtype_preservation():","603","    for svd_solver in solver_list:","604","        yield check_pca_float_dtype_preservation, svd_solver","605","        yield check_pca_int_dtype_upcast_to_double, svd_solver","606","","607","","608","def check_pca_float_dtype_preservation(svd_solver):","609","    # Ensure that PCA does not upscale the dtype when input is float32","610","    X_64 = np.random.RandomState(0).rand(1000, 4).astype(np.float64)","611","    X_32 = X_64.astype(np.float32)","612","","613","    pca_64 = PCA(n_components=3, svd_solver=svd_solver,","614","                 random_state=0).fit(X_64)","615","    pca_32 = PCA(n_components=3, svd_solver=svd_solver,","616","                 random_state=0).fit(X_32)","617","","618","    assert pca_64.components_.dtype == np.float64","619","    assert pca_32.components_.dtype == np.float32","620","    assert pca_64.transform(X_64).dtype == np.float64","621","    assert pca_32.transform(X_32).dtype == np.float32","622","","623","    assert_array_almost_equal(pca_64.components_, pca_32.components_,","624","                              decimal=5)","625","","626","","627","def check_pca_int_dtype_upcast_to_double(svd_solver):","628","    # Ensure that all int types will be upcast to float64","629","    X_i64 = np.random.RandomState(0).randint(0, 1000, (1000, 4))","630","    X_i64 = X_i64.astype(np.int64)","631","    X_i32 = X_i64.astype(np.int32)","632","","633","    pca_64 = PCA(n_components=3, svd_solver=svd_solver,","634","                 random_state=0).fit(X_i64)","635","    pca_32 = PCA(n_components=3, svd_solver=svd_solver,","636","                 random_state=0).fit(X_i32)","637","","638","    assert pca_64.components_.dtype == np.float64","639","    assert pca_32.components_.dtype == np.float64","640","    assert pca_64.transform(X_i64).dtype == np.float64","641","    assert pca_32.transform(X_i32).dtype == np.float64","642","","643","    assert_array_almost_equal(pca_64.components_, pca_32.components_,","644","                              decimal=5)"],"delete":[]}]}},"7c6486f09ccd3b80350331b879e3d1ea297918b0":{"changes":{"sklearn\/multioutput.py":"MODIFY","sklearn\/tests\/test_multioutput.py":"MODIFY"},"diff":{"sklearn\/multioutput.py":[{"add":["222","    @if_delegate_has_method('estimator')"],"delete":[]}],"sklearn\/tests\/test_multioutput.py":[{"add":["64","    assert_false(hasattr(MultiOutputRegressor(Lasso), 'partial_fit'))"],"delete":["69",""]}]}},"93563b0ac1d96403eff9988dfcb680d42c6304aa":{"changes":{"sklearn\/model_selection\/_split.py":"MODIFY","sklearn\/model_selection\/tests\/test_split.py":"MODIFY"},"diff":{"sklearn\/model_selection\/_split.py":[{"add":["1480","","1481","        if y.ndim == 2:","1482","            # for multi-label y, map each distinct row to its string repr:","1483","            y = np.array([str(row) for row in y])","1484",""],"delete":[]}],"sklearn\/model_selection\/tests\/test_split.py":[{"add":["665","    train, test = next(sss.split(X=X, y=y))","667","    # no overlap","670","    # complete partition","671","    assert_array_equal(np.union1d(train, test), np.arange(len(y)))","672","","673","","674","def test_stratified_shuffle_split_multilabel():","675","    # fix for issue 9037","676","    for y in [np.array([[0, 1], [1, 0], [1, 0], [0, 1]]),","677","              np.array([[0, 1], [1, 1], [1, 1], [0, 1]])]:","678","        X = np.ones_like(y)","679","        sss = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=0)","680","        train, test = next(sss.split(X=X, y=y))","681","        y_train = y[train]","682","        y_test = y[test]","683","","684","        # no overlap","685","        assert_array_equal(np.intersect1d(train, test), [])","686","","687","        # complete partition","688","        assert_array_equal(np.union1d(train, test), np.arange(len(y)))","689","","690","        # correct stratification of entire rows","691","        # (by design, here y[:, 0] uniquely determines the entire row of y)","692","        expected_ratio = np.mean(y[:, 0])","693","        assert_equal(expected_ratio, np.mean(y_train[:, 0]))","694","        assert_equal(expected_ratio, np.mean(y_test[:, 0]))","695",""],"delete":["665","    train, test = next(iter(sss.split(X=X, y=y)))"]}]}},"9a9cf19c065a6e3af5912a531b735f766b5dc433":{"changes":{"sklearn\/neighbors\/binary_tree.pxi":"MODIFY"},"diff":{"sklearn\/neighbors\/binary_tree.pxi":[{"add":["160","cdef extern from \"numpy\/arrayobject.h\":","161","    void PyArray_ENABLEFLAGS(np.ndarray arr, int flags)","162","","1506","                    PyArray_ENABLEFLAGS(indices_npy[i], np.NPY_OWNDATA)","1513","                    PyArray_ENABLEFLAGS(distances_npy[i], np.NPY_OWNDATA)","1526","                    PyArray_ENABLEFLAGS(indices_npy[i], np.NPY_OWNDATA)"],"delete":["1503","                    np.PyArray_UpdateFlags(indices_npy[i], indices_npy[i].flags.num | np.NPY_OWNDATA)","1510","                    np.PyArray_UpdateFlags(distances_npy[i], distances_npy[i].flags.num | np.NPY_OWNDATA)","1523","                    np.PyArray_UpdateFlags(indices_npy[i], indices_npy[i].flags.num | np.NPY_OWNDATA)"]}]}},"6451a097889eea6df43cc11cbfed59b0106e6171":{"changes":{"sklearn\/neural_network\/tests\/test_mlp.py":"MODIFY"},"diff":{"sklearn\/neural_network\/tests\/test_mlp.py":[{"add":["566","@ignore_warnings(category=RuntimeWarning)"],"delete":["566","@ignore_warnings(RuntimeError)"]}]}},"b42125bec308cccf4c911db65123870b04c08158":{"changes":{"sklearn\/metrics\/_plot\/confusion_matrix.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY","sklearn\/metrics\/_plot\/tests\/test_plot_confusion_matrix.py":"MODIFY"},"diff":{"sklearn\/metrics\/_plot\/confusion_matrix.py":[{"add":["95","            thresh = (cm.max() + cm.min()) \/ 2.0"],"delete":["95","            thresh = (cm.max() - cm.min()) \/ 2."]}],"doc\/whats_new\/v0.22.rst":[{"add":["44","- |Fix| :func:`metrics.plot_confusion_matrix` now colors the label color","45","  correctly to maximize contrast with its background. :pr:`15936` by","46","  `Thomas Fan`_ and :user:`DizietAsahi`.","47",""],"delete":[]}],"sklearn\/metrics\/_plot\/tests\/test_plot_confusion_matrix.py":[{"add":["202","    # off-diagonal text is white","211","    # off-diagonal text is black","215","    # Regression test for #15920","216","    cm = np.array([[19, 34], [32, 58]])","217","    disp = ConfusionMatrixDisplay(cm, display_labels=[0, 1])","218","","219","    disp.plot(cmap=pyplot.cm.Blues)","220","    min_color = pyplot.cm.Blues(0)","221","    max_color = pyplot.cm.Blues(255)","222","    assert_allclose(disp.text_[0, 0].get_color(), max_color)","223","    assert_allclose(disp.text_[0, 1].get_color(), max_color)","224","    assert_allclose(disp.text_[1, 0].get_color(), max_color)","225","    assert_allclose(disp.text_[1, 1].get_color(), min_color)","226","","227","","228",""],"delete":["202","    # oof-diagonal text is white","211","    # oof-diagonal text is black"]}]}},"989f9c764734efc21bbff21b0202b52a1112bfc3":{"changes":{"sklearn\/utils\/estimator_checks.py":"MODIFY","sklearn\/utils\/tests\/test_estimator_checks.py":"MODIFY"},"diff":{"sklearn\/utils\/estimator_checks.py":[{"add":["678","def _apply_on_subsets(func, X):","689","    if sparse.issparse(result_full):","690","        result_full = result_full.A","691","        result_by_batch = [x.A for x in result_by_batch]","727","            result_full, result_by_batch = _apply_on_subsets("],"delete":["678","def _apply_func(func, X):","724","            result_full, result_by_batch = _apply_func("]}],"sklearn\/utils\/tests\/test_estimator_checks.py":[{"add":["163","class SparseTransformer(BaseEstimator):","164","    def fit(self, X, y=None):","165","        self.X_shape_ = check_array(X).shape","166","        return self","167","","168","    def fit_transform(self, X, y=None):","169","        return self.fit(X, y).transform(X)","170","","171","    def transform(self, X):","172","        X = check_array(X)","173","        if X.shape[1] != self.X_shape_[1]:","174","            raise ValueError('Bad number of features')","175","        return sp.csr_matrix(X)","176","","177","","252","    # non-regression test for estimators transforming to sparse data","253","    check_estimator(SparseTransformer())","254",""],"delete":[]}]}},"526b35f372a9152234a3148c3491429882614c21":{"changes":{"sklearn\/model_selection\/_search.py":"MODIFY"},"diff":{"sklearn\/model_selection\/_search.py":[{"add":["388","        self.fit_params = fit_params","564","        if self.fit_params is not None:","838","           fit_params=None, iid=..., n_jobs=1,"],"delete":["388","        self.fit_params = fit_params if fit_params is not None else {}","564","        if self.fit_params:","838","           fit_params={}, iid=..., n_jobs=1,"]}]}},"52b6a669669019d63bbe03f32d919a62538779b9":{"changes":{"doc\/modules\/clustering.rst":"MODIFY","doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/metrics\/cluster\/tests\/test_supervised.py":"MODIFY","sklearn\/metrics\/cluster\/supervised.py":"MODIFY"},"diff":{"doc\/modules\/clustering.rst":[{"add":["1160","measure are available, **Normalized Mutual Information (NMI)** and **Adjusted","1161","Mutual Information (AMI)**. NMI is often used in the literature, while AMI was","1214","- **Upper bound  of 1**:  Values close to zero indicate two label","1216","  indicate significant agreement. Further, an AMI of exactly 1 indicates","1270",".. math:: \\text{NMI}(U, V) = \\frac{\\text{MI}(U, V)}{\\text{mean}(H(U), H(V))}","1278","following equation [VEB2009]_. In this equation,","1291",".. math:: \\text{AMI} = \\frac{\\text{MI} - E[\\text{MI}]}{\\text{mean}(H(U), H(V)) - E[\\text{MI}]}","1292","","1293","For normalized mutual information and adjusted mutual information, the normalizing","1294","value is typically some *generalized* mean of the entropies of each clustering.","1295","Various generalized means exist, and no firm rules exist for preferring one over the","1296","others.  The decision is largely a field-by-field basis; for instance, in community","1297","detection, the arithmetic mean is most common. Each","1298","normalizing method provides \"qualitatively similar behaviours\" [YAT2016]_. In our","1299","implementation, this is controlled by the ``average_method`` parameter.","1300","","1301","Vinh et al. (2010) named variants of NMI and AMI by their averaging method [VEB2010]_. Their","1302","'sqrt' and 'sum' averages are the geometric and arithmetic means; we use these","1303","more broadly common names.","1312"," * [VEB2009] Vinh, Epps, and Bailey, (2009). \"Information theoretic measures","1318"," * [VEB2010] Vinh, Epps, and Bailey, (2010). \"Information Theoretic Measures for","1320","   Correction for Chance\". JMLR","1321","   <http:\/\/jmlr.csail.mit.edu\/papers\/volume11\/vinh10a\/vinh10a.pdf>","1328","   ","1329"," * [YAT2016] Yang, Algesheimer, and Tessone, (2016). \"A comparative analysis of","1330","   community","1331","   detection algorithms on artificial networks\". Scientific Reports 6: 30750.","1332","   `doi:10.1038\/srep30750 <https:\/\/www.nature.com\/articles\/srep30750>`_.","1333","   ","1334","   ","1374","discussed above, with the aggregation function being the arithmetic mean [B2011]_.","1549","- **Upper-bounded at 1**:  Values close to zero indicate two label"],"delete":["1160","measure are available, **Normalized Mutual Information(NMI)** and **Adjusted","1161","Mutual Information(AMI)**. NMI is often used in the literature while AMI was","1214","- **Bounded range [0, 1]**:  Values close to zero indicate two label","1216","  indicate significant agreement. Further, values of exactly 0 indicate","1217","  **purely** independent label assignments and a AMI of exactly 1 indicates","1220","- **No assumption is made on the cluster structure**: can be used","1221","  to compare clustering algorithms such as k-means which assumes isotropic","1222","  blob shapes with results of spectral clustering algorithms which can","1223","  find cluster with \"folded\" shapes.","1224","","1276",".. math:: \\text{NMI}(U, V) = \\frac{\\text{MI}(U, V)}{\\sqrt{H(U)H(V)}}","1284","following equation, from Vinh, Epps, and Bailey, (2009). In this equation,","1297",".. math:: \\text{AMI} = \\frac{\\text{MI} - E[\\text{MI}]}{\\max(H(U), H(V)) - E[\\text{MI}]}","1306"," * Vinh, Epps, and Bailey, (2009). \"Information theoretic measures","1312"," * Vinh, Epps, and Bailey, (2010). Information Theoretic Measures for","1314","   Correction for Chance, JMLR","1315","   http:\/\/jmlr.csail.mit.edu\/papers\/volume11\/vinh10a\/vinh10a.pdf","1361","discussed above normalized by the sum of the label entropies [B2011]_.","1536","- **Bounded range [0, 1]**:  Values close to zero indicate two label"]}],"doc\/whats_new\/v0.20.rst":[{"add":["208","- Added control over the normalization in ","209","  :func:`metrics.normalized_mutual_information_score` and","210","  :func:`metrics.adjusted_mutual_information_score` via the ``average_method``","211","  parameter. In version 0.22, the default normalizer for each will become","212","  the *arithmetic* mean of the entropies of each clustering. :issue:`11124` by","213","  :user:`Arya McCarthy <aryamccarthy>`.","800","- In :func:`metrics.normalized_mutual_information_score` and","801","  :func:`metrics.adjusted_mutual_information_score`, ","802","  warn that ``average_method``","803","  will have a new default value. In version 0.22, the default normalizer for each ","804","  will become the *arithmetic* mean of the entropies of each clustering. Currently,","805","  :func:`metrics.normalized_mutual_information_score` uses the default of","806","  ``average_method='geometric'``, and :func:`metrics.adjusted_mutual_information_score`","807","  uses the default of ``average_method='max'`` to match their behaviors in","808","  version 0.19.","809","  :issue:`11124` by :user:`Arya McCarthy <aryamccarthy>`.","810",""],"delete":[]}],"sklearn\/metrics\/cluster\/tests\/test_supervised.py":[{"add":["14","from sklearn.metrics.cluster.supervised import _generalized_average","19","        assert_warns_message, ignore_warnings","34","def test_future_warning():","35","    score_funcs_with_changing_means = [","36","        normalized_mutual_info_score,","37","        adjusted_mutual_info_score,","38","    ]","39","    warning_msg = \"The behavior of \"","40","    args = [0, 0, 0], [0, 0, 0]","41","    for score_func in score_funcs_with_changing_means:","42","        assert_warns_message(FutureWarning, warning_msg, score_func, *args)","43","","44","","45","@ignore_warnings(category=FutureWarning)","62","def test_generalized_average():","63","    a, b = 1, 2","64","    methods = [\"min\", \"geometric\", \"arithmetic\", \"max\"]","65","    means = [_generalized_average(a, b, method) for method in methods]","66","    assert means[0] <= means[1] <= means[2] <= means[3]","67","    c, d = 12, 12","68","    means = [_generalized_average(c, d, method) for method in methods]","69","    assert means[0] == means[1] == means[2] == means[3]","70","","71","","72","@ignore_warnings(category=FutureWarning)","82","    score_funcs_with_changing_means = [","83","        normalized_mutual_info_score,","84","        adjusted_mutual_info_score,","85","    ]","86","    means = {\"min\", \"geometric\", \"arithmetic\", \"max\"}","87","    for score_func in score_funcs_with_changing_means:","88","        for mean in means:","89","            assert score_func([], [], mean) == 1.0","90","            assert score_func([0], [1], mean) == 1.0","91","            assert score_func([0, 0, 0], [0, 0, 0], mean) == 1.0","92","            assert score_func([0, 1, 0], [42, 7, 42], mean) == 1.0","93","            assert score_func([0., 1., 0.], [42., 7., 42.], mean) == 1.0","94","            assert score_func([0., 1., 2.], [42., 7., 2.], mean) == 1.0","95","            assert score_func([0, 1, 2], [42, 7, 2], mean) == 1.0","128","def test_non_consecutive_labels():","150","@ignore_warnings(category=FutureWarning)","164","@ignore_warnings(category=FutureWarning)","178","@ignore_warnings(category=FutureWarning)","259","@ignore_warnings(category=FutureWarning)","269","        for method in [\"min\", \"geometric\", \"arithmetic\", \"max\"]:","270","            assert adjusted_mutual_info_score(labels_a, labels_b,","271","                                              method) == 0.0","272","            assert normalized_mutual_info_score(labels_a, labels_b,","273","                                                method) == 0.0","285","        avg = 'arithmetic'","286","        assert_almost_equal(v_measure_score(labels_a, labels_b),","287","                            normalized_mutual_info_score(labels_a, labels_b,","288","                                                         average_method=avg)","289","                            )"],"delete":["89","def test_non_consicutive_labels():"]}],"sklearn\/metrics\/cluster\/supervised.py":[{"add":["13","#          Arya McCarthy <arya@jhu.edu>","19","import warnings","63","def _generalized_average(U, V, average_method):","64","    \"\"\"Return a particular mean of two numbers.\"\"\"","65","    if average_method == \"min\":","66","        return min(U, V)","67","    elif average_method == \"geometric\":","68","        return np.sqrt(U * V)","69","    elif average_method == \"arithmetic\":","70","        return np.mean([U, V])","71","    elif average_method == \"max\":","72","        return max(U, V)","73","    else:","74","        raise ValueError(\"'average_method' must be 'min', 'geometric', \"","75","                         \"'arithmetic', or 'max'\")","76","","77","","264","    homogeneity and completeness. V-Measure is identical to","265","    :func:`normalized_mutual_info_score` with the arithmetic averaging","266","    method.","465","    This score is identical to :func:`normalized_mutual_info_score` with","466","    the ``'arithmetic'`` option for averaging.","481","","508","    normalized_mutual_info_score","641","def adjusted_mutual_info_score(labels_true, labels_pred,","642","                               average_method='warn'):","651","        AMI(U, V) = [MI(U, V) - E(MI(U, V))] \/ [avg(H(U), H(V)) - E(MI(U, V))]","675","    average_method : string, optional (default: 'warn')","676","        How to compute the normalizer in the denominator. Possible options","677","        are 'min', 'geometric', 'arithmetic', and 'max'.","678","        If 'warn', 'max' will be used. The default will change to","679","        'arithmetic' in version 0.22.","680","","681","        .. versionadded:: 0.20","682","","685","    ami: float (upperlimited by 1.0)","724","    if average_method == 'warn':","725","        warnings.warn(\"The behavior of AMI will change in version 0.22. \"","726","                      \"To match the behavior of 'v_measure_score', AMI will \"","727","                      \"use average_method='arithmetic' by default.\",","728","                      FutureWarning)","729","        average_method = 'max'","748","    normalizer = _generalized_average(h_true, h_pred, average_method)","749","    denominator = normalizer - emi","750","    # Avoid 0.0 \/ 0.0 when expectation equals maximum, i.e a perfect match.","751","    # normalizer should always be >= emi, but because of floating-point","752","    # representation, sometimes emi is slightly larger. Correct this","753","    # by preserving the sign.","754","    if denominator < 0:","755","        denominator = min(denominator, -np.finfo('float64').eps)","756","    else:","757","        denominator = max(denominator, np.finfo('float64').eps)","758","    ami = (mi - emi) \/ denominator","762","def normalized_mutual_info_score(labels_true, labels_pred,","763","                                 average_method='warn'):","769","    information is normalized by some generalized mean of ``H(labels_true)``","770","    and ``H(labels_pred))``, defined by the `average_method`.","794","    average_method : string, optional (default: 'warn')","795","        How to compute the normalizer in the denominator. Possible options","796","        are 'min', 'geometric', 'arithmetic', and 'max'.","797","        If 'warn', 'geometric' will be used. The default will change to","798","        'arithmetic' in version 0.22.","799","","800","        .. versionadded:: 0.20","801","","809","    v_measure_score: V-Measure (NMI with arithmetic mean option.)","833","    if average_method == 'warn':","834","        warnings.warn(\"The behavior of NMI will change in version 0.22. \"","835","                      \"To match the behavior of 'v_measure_score', NMI will \"","836","                      \"use average_method='arithmetic' by default.\",","837","                      FutureWarning)","838","        average_method = 'geometric'","855","    normalizer = _generalized_average(h_true, h_pred, average_method)","856","    # Avoid 0.0 \/ 0.0 when either entropy is zero.","857","    normalizer = max(normalizer, np.finfo('float64').eps)","858","    nmi = mi \/ normalizer"],"delete":["247","    homogeneity and completeness.","446","    This score is identical to :func:`normalized_mutual_info_score`.","619","def adjusted_mutual_info_score(labels_true, labels_pred):","628","        AMI(U, V) = [MI(U, V) - E(MI(U, V))] \/ [max(H(U), H(V)) - E(MI(U, V))]","654","    ami: float(upperlimited by 1.0)","711","    ami = (mi - emi) \/ (max(h_true, h_pred) - emi)","715","def normalized_mutual_info_score(labels_true, labels_pred):","721","    information is normalized by ``sqrt(H(labels_true) * H(labels_pred))``.","791","    nmi = mi \/ max(np.sqrt(h_true * h_pred), 1e-10)"]}]}},"4e166e2982be77ea2375653754de77159b9e1f80":{"changes":{"sklearn\/compose\/tests\/test_column_transformer.py":"MODIFY","sklearn\/compose\/_column_transformer.py":"MODIFY"},"diff":{"sklearn\/compose\/tests\/test_column_transformer.py":[{"add":["18","from sklearn.externals import six","304","    for col in [1.5, ['string', 1], slice(1, 's'), np.array([1.])]:","554","@pytest.mark.parametrize(","555","    \"key\", [[0], slice(0, 1), np.array([True, False]), ['first'], 'pd-index',","556","            np.array(['first']), np.array(['first'], dtype=object),","557","            slice(None, 'first'), slice('first', 'first')])","561","    if isinstance(key, six.string_types) and key == 'pd-index':","562","        key = pd.Index(['first'])"],"delete":["303","    for col in [1.5, ['string', 1], slice(1, 's')]:","553","@pytest.mark.parametrize(\"key\", [[0], slice(0, 1), np.array([True, False]),","554","                                 ['first'], slice(None, 'first'),","555","                                 slice('first', 'first')])"]}],"sklearn\/compose\/_column_transformer.py":[{"add":["456","    Check that scalar, list or slice is of a certain type.","457","","458","    This is only used in _get_column and _get_column_indices to check","459","    if the `key` (column specification) is fully integer or fully string-like.","460","","461","    Parameters","462","    ----------","463","    key : scalar, list, slice, array-like","464","        The column specification to check","465","    superclass : int or six.string_types","466","        The type for which to check the `key`","477","        if superclass is int:","478","            return key.dtype.kind == 'i'","479","        else:","480","            # superclass = six.string_types","481","            return key.dtype.kind in ('O', 'U', 'S')","510","    elif hasattr(key, 'dtype') and np.issubdtype(key.dtype, np.bool_):","577","    elif hasattr(key, 'dtype') and np.issubdtype(key.dtype, np.bool_):"],"delete":["456","    Check that scalar, list or slice is of certain type.","467","        return key.dtype.kind == 'i'","496","    elif hasattr(key, 'dtype') and np.issubdtype(key.dtype, np.bool):","563","    elif hasattr(key, 'dtype') and np.issubdtype(key.dtype, np.bool):"]}]}},"4d5407c89fb2b25ef5cba90470cbdaecf10064cc":{"changes":{"sklearn\/linear_model\/tests\/test_sag.py":"MODIFY"},"diff":{"sklearn\/linear_model\/tests\/test_sag.py":[{"add":["122","               fit_intercept=True, saga=False, random_state=0):","132","    rng = check_random_state(random_state)","370","    max_iter = 100","380","                 alpha=alpha * n_samples, max_iter=max_iter,","381","                 random_state=rng)","390","                                          fit_intercept=fit_intercept,","391","                                          random_state=rng)","396","                                          fit_intercept=fit_intercept,","397","                                          random_state=rng)"],"delete":["122","               fit_intercept=True, saga=False):","132","    rng = np.random.RandomState(77)","370","    max_iter = 50","380","                 alpha=alpha * n_samples, max_iter=max_iter)","389","                                          fit_intercept=fit_intercept)","394","                                          fit_intercept=fit_intercept)"]}]}},"fa4646749ce47cf4fe8d15575c448948b5625209":{"changes":{"doc\/whats_new\/v0.22.rst":"MODIFY"},"diff":{"doc\/whats_new\/v0.22.rst":[{"add":["21","- |Fix| :class:`cluster.KMeans` with ``algorithm=\"elkan\"`` now uses the same","22","  stopping criterion as with the default ``algorithm=\"full\"``. :pr:`15930` by","24","","53","- |Fix| :func:`inspection.plot_partial_dependence` and"],"delete":["21","- |Fix| :class:`KMeans` with ``algorithm=\"elkan\"`` now uses the same stopping","22","  criterion as with the default ``algorithm=\"full\"``. :pr:`15930` by","24"," ","53","- |Fix| :func:`inspection.plot_partial_dependence` and "]}]}},"a0c2de3f9ee8310da5a827623ce06b50769dfc99":{"changes":{"doc\/whats_new.rst":"MODIFY"},"diff":{"doc\/whats_new.rst":[{"add":["213","   - Fixed a bug when :func:`sklearn.datasets.make_classification` fails ","214","     when generating more than 30 features. :issue:`8159` by","215","     :user:`Herilalaina Rakotoarison <herilalaina>`","216",""],"delete":[]}]}},"c62338feb5de4b6eabf055fd1643a3e022fe4547":{"changes":{"sklearn\/tree\/tests\/test_export.py":"MODIFY","sklearn\/tree\/export.py":"MODIFY"},"diff":{"sklearn\/tree\/tests\/test_export.py":[{"add":["235","    # Check error when argument is not an estimator","236","    message = \"is not an estimator instance\"","237","    assert_raise_message(TypeError, message,","238","                         export_graphviz, clf.fit(X, y).tree_)","239",""],"delete":["212","    assert_equal(contents1, contents2)","213",""]}],"sklearn\/tree\/export.py":[{"add":["70","","71","","462","        recurse(decision_tree.tree_, 0, criterion=decision_tree.criterion)"],"delete":["460","        if isinstance(decision_tree, _tree.Tree):","461","            recurse(decision_tree, 0, criterion=\"impurity\")","462","        else:","463","            recurse(decision_tree.tree_, 0, criterion=decision_tree.criterion)"]}]}},"528b044a22f0afad4ebca2197589b62b869326c1":{"changes":{"doc\/whats_new\/v0.23.rst":"MODIFY","sklearn\/compose\/tests\/test_column_transformer.py":"MODIFY","sklearn\/compose\/_column_transformer.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.23.rst":[{"add":["59",":mod:`sklearn.compose`","60","......................","61","","62","- |Fix| :class:`compose.ColumnTransformer` method ``get_feature_names`` now","63","  returns correct results when one of the transformer steps applies on an","64","  empty list of columns :pr:`15963` by `Roman Yurchak`_.","65",""],"delete":[]}],"sklearn\/compose\/tests\/test_column_transformer.py":[{"add":["1268","","1269","","1270","@pytest.mark.parametrize(","1271","    'empty_col', [[], np.array([], dtype=np.int), lambda x: []],","1272","    ids=['list', 'array', 'callable']","1273",")","1274","def test_feature_names_empty_columns(empty_col):","1275","    pd = pytest.importorskip('pandas')","1276","","1277","    df = pd.DataFrame({\"col1\": [\"a\", \"a\", \"b\"], \"col2\": [\"z\", \"z\", \"z\"]})","1278","","1279","    ct = ColumnTransformer(","1280","        transformers=[","1281","            (\"ohe\", OneHotEncoder(), [\"col1\", \"col2\"]),","1282","            (\"empty_features\", OneHotEncoder(), empty_col),","1283","        ],","1284","    )","1285","","1286","    ct.fit(df)","1287","    assert ct.get_feature_names() == ['ohe__x0_a', 'ohe__x0_b', 'ohe__x1_z']"],"delete":[]}],"sklearn\/compose\/_column_transformer.py":[{"add":["352","        for name, trans, column, _ in self._iter(fitted=True):","353","            if trans == 'drop' or (","354","                    hasattr(column, '__len__') and not len(column)):"],"delete":["352","        for name, trans, _, _ in self._iter(fitted=True):","353","            if trans == 'drop':"]}]}},"c75bccf43ecb2b6619491911f048676cab02aced":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/linear_model\/tests\/test_logistic.py":"MODIFY","sklearn\/linear_model\/logistic.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["334","- Fixed a bug in :class:`linear_model.LogisticRegressionCV` where the","335","  ``score`` method always computes accuracy, not the metric given by","336","  the ``scoring`` parameter.","337","  :issue:`10998` by :user:`Thomas Fan <thomasjpfan>`.","338",""],"delete":[]}],"sklearn\/linear_model\/tests\/test_logistic.py":[{"add":["24","from sklearn.exceptions import ChangedBehaviorWarning","95","def test_logistic_cv_mock_scorer():","96","","97","    class MockScorer(object):","98","        def __init__(self):","99","            self.calls = 0","100","            self.scores = [0.1, 0.4, 0.8, 0.5]","101","","102","        def __call__(self, model, X, y, sample_weight=None):","103","            score = self.scores[self.calls % len(self.scores)]","104","            self.calls += 1","105","            return score","106","","107","    mock_scorer = MockScorer()","108","    Cs = [1, 2, 3, 4]","109","    cv = 2","110","","111","    lr = LogisticRegressionCV(Cs=Cs, scoring=mock_scorer, cv=cv)","112","    lr.fit(X, Y1)","113","","114","    # Cs[2] has the highest score (0.8) from MockScorer","115","    assert lr.C_[0] == Cs[2]","116","","117","    # scorer called 8 times (cv*len(Cs))","118","    assert mock_scorer.calls == cv * len(Cs)","119","","120","    # reset mock_scorer","121","    mock_scorer.calls = 0","122","    with pytest.warns(ChangedBehaviorWarning):","123","        custom_score = lr.score(X, lr.predict(X))","124","","125","    assert custom_score == mock_scorer.scores[0]","126","    assert mock_scorer.calls == 1","127","","128","","129","def test_logistic_cv_score_does_not_warn_by_default():","130","    lr = LogisticRegressionCV(cv=2)","131","    lr.fit(X, Y1)","132","","133","    with pytest.warns(None) as record:","134","        lr.score(X, lr.predict(X))","135","    assert len(record) == 0","136","","137",""],"delete":[]}],"sklearn\/linear_model\/logistic.py":[{"add":["31","from..exceptions import (NotFittedError, ConvergenceWarning,","32","                         ChangedBehaviorWarning)","1792","","1793","    def score(self, X, y, sample_weight=None):","1794","        \"\"\"Returns the score using the `scoring` option on the given","1795","        test data and labels.","1796","","1797","        Parameters","1798","        ----------","1799","        X : array-like, shape = (n_samples, n_features)","1800","            Test samples.","1801","","1802","        y : array-like, shape = (n_samples,)","1803","            True labels for X.","1804","","1805","        sample_weight : array-like, shape = [n_samples], optional","1806","            Sample weights.","1807","","1808","        Returns","1809","        -------","1810","        score : float","1811","            Score of self.predict(X) wrt. y.","1812","","1813","        \"\"\"","1814","","1815","        if self.scoring is not None:","1816","            warnings.warn(\"The long-standing behavior to use the \"","1817","                          \"accuracy score has changed. The scoring \"","1818","                          \"parameter is now used. \"","1819","                          \"This warning will disappear in version 0.22.\",","1820","                          ChangedBehaviorWarning)","1821","        scoring = self.scoring or 'accuracy'","1822","        if isinstance(scoring, six.string_types):","1823","            scoring = get_scorer(scoring)","1824","","1825","        return scoring(self, X, y, sample_weight=sample_weight)"],"delete":["31","from ..exceptions import NotFittedError, ConvergenceWarning"]}]}},"ec4cbeff39d0253f62fd74e50c0dd89c6dff8997":{"changes":{"examples\/manifold\/plot_lle_digits.py":"MODIFY","doc\/modules\/manifold.rst":"MODIFY","sklearn\/manifold\/__init__.py":"MODIFY","doc\/modules\/classes.rst":"MODIFY","examples\/cluster\/plot_cluster_comparison.py":"MODIFY","examples\/cluster\/plot_segmentation_toy.py":"MODIFY","sklearn\/cluster\/tests\/test_spectral.py":"MODIFY","sklearn\/cluster\/spectral.py":"MODIFY","sklearn\/manifold\/tests\/test_spectral_embedding.py":"ADD","sklearn\/cluster\/__init__.py":"MODIFY","examples\/manifold\/plot_compare_methods.py":"MODIFY","sklearn\/manifold\/spectral_embedding.py":"ADD","doc\/whats_new.rst":"MODIFY"},"diff":{"examples\/manifold\/plot_lle_digits.py":[{"add":["99","               \"Principal Components projection of the digits (time %.2fs)\" %","100","               (time() - t0))","111","               \"Linear Discriminant projection of the digits (time %.2fs)\" %","112","               (time() - t0))","122","               \"Isomap projection of the digits (time %.2fs)\" %","123","               (time() - t0))","135","               \"Locally Linear Embedding of the digits (time %.2fs)\" %","136","               (time() - t0))","148","               \"Modified Locally Linear Embedding of the digits (time %.2fs)\" %","149","               (time() - t0))","161","               \"Hessian Locally Linear Embedding of the digits (time %.2fs)\" %","162","               (time() - t0))","174","               \"Local Tangent Space Alignment of the digits (time %.2fs)\" %","175","               (time() - t0))","185","               \"MDS embedding of the digits (time %.2fs)\" %","186","               (time() - t0))","192","                                       max_depth=5)","199","               \"Random forest embedding of the digits (time %.2fs)\" %","200","               (time() - t0))","201","","202","#----------------------------------------------------------------------","203","# Spectral embedding of the digits dataset","204","print \"Computing Spectral embedding\"","205","embedder = manifold.SpectralEmbedding(n_components=2, random_state=0,","206","                                      eigen_solver=\"arpack\")","207","t0 = time()","208","X_se = embedder.fit_transform(X)","209","","210","plot_embedding(X_se,","211","               \"Spectral embedding of the digits (time %.2fs)\" %","212","               (time() - t0))"],"delete":["99","    \"Principal Components projection of the digits (time %.2fs)\" %","100","    (time() - t0))","111","    \"Linear Discriminant projection of the digits (time %.2fs)\" %","112","    (time() - t0))","122","    \"Isomap projection of the digits (time %.2fs)\" %","123","    (time() - t0))","135","    \"Locally Linear Embedding of the digits (time %.2fs)\" %","136","    (time() - t0))","148","    \"Modified Locally Linear Embedding of the digits (time %.2fs)\" %","149","    (time() - t0))","161","    \"Hessian Locally Linear Embedding of the digits (time %.2fs)\" %","162","    (time() - t0))","174","    \"Local Tangent Space Alignment of the digits (time %.2fs)\" %","175","    (time() - t0))","185","    \"MDS embedding of the digits (time %.2fs)\" %","186","    (time() - t0))","192","                                        max_depth=5)","199","    \"Random forest embedding of the digits (time %.2fs)\" %","200","    (time() - t0))"]}],"doc\/modules\/manifold.rst":[{"add":["300","Spectral Embedding","301","====================","302","","303","Spectral Embedding (also known as Laplacian Eigenmaps) is one method","304","to calculate nonlinear embedding. It finds a low dimensional representation","305","of the data using a spectral decomposition of the graph Laplacian.","306","The graph generated can be considered as a discrete approximation of the ","307","low dimensional manifold in the high dimensional space. Minimization of a ","308","cost function based on the graph ensures that points close to each other on ","309","the manifold are mapped close to each other in the low dimensional space, ","310","preserving local distances. Spectral embedding can be  performed with the","311","function :func:`spectral_embedding` or its object-oriented counterpart","312",":class:`SpectralEmbedding`.","313","","314","Complexity","315","----------","316","","317","The Spectral Embedding algorithm comprises three stages:","318","","319","1. **Weighted Graph Construction**. Transform the raw input data into","320","   graph representation using affinity (adjacency) matrix representation.","321","","322","2. **Graph Laplacian Construction**. unnormalized Graph Laplacian","323","   is constructed as :math:`L = D - A` for and normalized one as","324","   :math:`L = D^{-\\frac{1}{2}} (D - A) D^{-\\frac{1}{2}}`.  ","325","","326","3. **Partial Eigenvalue Decomposition**. Eigenvalue decomposition is ","327","   done on graph Laplacian","328","","329","The overall complexity of spectral embedding is","330",":math:`O[D \\log(k) N \\log(N)] + O[D N k^3] + O[d N^2]`.","331","","332","* :math:`N` : number of training data points","333","* :math:`D` : input dimension","334","* :math:`k` : number of nearest neighbors","335","* :math:`d` : output dimension","336","","337",".. topic:: References:","338","","339","   * `\"Laplacian Eigenmaps for Dimensionality Reduction","340","     and Data Representation\" ","341","     <http:\/\/www.cse.ohio-state.edu\/~mbelkin\/papers\/LEM_NC_03.pdf>`_","342","     M. Belkin, P. Niyogi, Neural Computation, June 2003; 15 (6):1373-1396","343","","344",""],"delete":[]}],"sklearn\/manifold\/__init__.py":[{"add":["7","from .spectral_embedding import SpectralEmbedding, spectral_embedding"],"delete":[]}],"doc\/modules\/classes.rst":[{"add":["620","    manifold.SpectralEmbedding","627","    manifold.spectral_embedding"],"delete":[]}],"examples\/cluster\/plot_cluster_comparison.py":[{"add":["50","                   hspace=.01)","54","                                     no_structure]):","75","    spectral = cluster.SpectralClustering(n_clusters=2,","76","                                          eigen_solver='arpack',","77","                                          affinity=\"nearest_neighbors\")","80","                                                       preference=-200)"],"delete":["50","        hspace=.01)","54","                no_structure]):","75","    spectral = cluster.SpectralClustering(n_clusters=2, mode='arpack',","76","            affinity=\"nearest_neighbors\")","79","            preference=-200)"]}],"examples\/cluster\/plot_segmentation_toy.py":[{"add":["72","labels = spectral_clustering(graph, n_clusters=4, eigen_solver='arpack')","90","labels = spectral_clustering(graph, n_clusters=2, eigen_solver='arpack')"],"delete":["72","labels = spectral_clustering(graph, n_clusters=4, mode='arpack')","90","labels = spectral_clustering(graph, n_clusters=2, mode='arpack')"]}],"sklearn\/cluster\/tests\/test_spectral.py":[{"add":["14","from sklearn.cluster.spectral import discretize","18","from sklearn.preprocessing import LabelBinarizer","19","","29","                  ])","31","    for eigen_solver in ('arpack', 'lobpcg'):","35","                                           affinity='precomputed',","36","                                           eigen_solver=eigen_solver,","37","                                           assign_labels=assign_labels","38","                                           ).fit(mat)","47","                assert_equal(model_copy.eigen_solver, model.eigen_solver)","49","                                   model.random_state.get_state()[1])","66","                                 random_state=0, eigen_solver=\"lobpcg\")","91","                                     random_state=0, eigen_solver=\"amg\")","97","                      n_components=len(centers),","98","                      random_state=0, eigen_solver=\"amg\")","114","                  random_state=0, eigen_solver=\"<unknown>\")","130","                  ])","135","                                affinity='precomputed').fit(S).labels_","144","                      cluster_std=0.4)","147","                            random_state=0)","154","","155","","156","def test_discretize(seed=36):","157","    # Test the discretize using a noise assignment matrix","158","    LB = LabelBinarizer()","159","    for n_sample in [50, 100, 150, 500]:","160","        for n_class in range(2, 10):","161","            # random class labels","162","            random_state = np.random.RandomState(seed)","163","            y_true = random_state.random_integers(0, n_class, n_sample)","164","            y_true = np.array(y_true, np.float)","165","            # noise class assignment matrix","166","            y_true_noisy = (LB.fit_transform(y_true)","167","                            + 0.1 * random_state.randn(n_sample, n_class + 1))","168","            y_pred = discretize(y_true_noisy)","169","            assert_greater(adjusted_rand_score(y_true, y_pred), 0.9)"],"delete":["26","                 ])","28","    for mode in ('arpack', 'lobpcg'):","32","                        affinity='precomputed', mode=mode,","33","                        assign_labels=assign_labels).fit(mat)","42","                assert_equal(model_copy.mode, model.mode)","44","                            model.random_state.get_state()[1])","61","                                     random_state=0, mode=\"lobpcg\")","86","                                     random_state=0, mode=\"amg\")","92","                      n_components=len(centers), random_state=0, mode=\"amg\")","108","                  random_state=0, mode=\"<unknown>\")","124","                 ])","129","            affinity='precomputed').fit(S).labels_","138","            cluster_std=0.4)","141","            random_state=0)"]}],"sklearn\/cluster\/spectral.py":[{"add":["2","# Author: Gael Varoquaux gael.varoquaux@normalesup.org","3","#         Brian Cheung","4","#         Wei LI <kuantkid@gmail.com>","15","from ..manifold import spectral_embedding","156","def spectral_clustering(affinity, n_clusters=8, n_components=None,","157","                        eigen_solver=None, random_state=None, n_init=10,","158","                        k=None, eigen_tol=0.0,","159","                        assign_labels='kmeans',","160","                        mode=None):","189","    eigen_solver: {None, 'arpack' or 'amg'}","196","        of the lobpcg eigen vectors decomposition when eigen_solver == 'amg'","204","    eigen_tol : float, optional, default: 0.0","206","        when using arpack eigen_solver.","245","                         \"'kmeans' or 'discretize', but '%s' was given\"","246","                         % assign_labels)","247","","249","        warnings.warn(\"'k' was renamed to n_clusters and will \"","250","                      \"be removed in 0.15.\",","251","                      DeprecationWarning)","253","    if not mode is None:","254","        warnings.warn(\"'mode' was renamed to eigen_solver \"","255","                      \"and will be removed in 0.15.\",","256","                      DeprecationWarning)","257","        eigen_solver = mode","258","","262","                              eigen_solver=eigen_solver,","263","                              random_state=random_state,","264","                              eigen_tol=eigen_tol, drop_first=False)","267","        _, labels, _ = k_means(maps, n_clusters, random_state=random_state,","270","        labels = discretize(maps, random_state=random_state)","312","    eigen_solver: {None, 'arpack' or 'amg'}","319","        of the lobpcg eigen vectors decomposition when eigen_solver == 'amg'","327","    eigen_tol : float, optional, default: 0.0","329","        when using arpack eigen_solver.","379","    def __init__(self, n_clusters=8, eigen_solver=None, random_state=None,","380","                 n_init=10, gamma=1., affinity='rbf', n_neighbors=10, k=None,","381","                 eigen_tol=0.0, assign_labels='kmeans', mode=None):","382","        if k is not None:","383","            warnings.warn(\"'k' was renamed to n_clusters and \"","384","                          \"will be removed in 0.15.\",","385","                          DeprecationWarning)","387","        if mode is not None:","388","            warnings.warn(\"'mode' was renamed to eigen_solver and \"","389","                          \"will be removed in 0.15.\",","390","                          DeprecationWarning)","391","            eigen_solver = mode","392","","394","        self.eigen_solver = eigen_solver","400","        self.eigen_tol = eigen_tol","429","                             \"'nearest_neighbors' or 'precomputed', got '%s'.\"","430","                             % self.affinity)","434","                                           n_clusters=self.n_clusters,","435","                                           eigen_solver=self.eigen_solver,","436","                                           random_state=self.random_state,","437","                                           n_init=self.n_init,","438","                                           eigen_tol=self.eigen_tol,","439","                                           assign_labels=self.assign_labels)"],"delete":["2","# Author: Gael Varoquaux gael.varoquaux@normalesup.org, Brian Cheung","11","from ..utils.graph import graph_laplacian","17","def _set_diag(laplacian, value):","18","    \"\"\"Set the diagonal of the laplacian matrix and convert it to a","19","    sparse format well suited for eigenvalue decomposition","20","","21","    Parameters","22","    ----------","23","    laplacian: array or sparse matrix","24","        The graph laplacian","25","    value: float","26","        The value of the diagonal","27","","28","    Returns","29","    -------","30","    laplacian: array of sparse matrix","31","        An array of matrix in a form that is well suited to fast","32","        eigenvalue decomposition, depending on the band width of the","33","        matrix.","34","    \"\"\"","35","    from scipy import sparse","36","    n_nodes = laplacian.shape[0]","37","    # We need all entries in the diagonal to values","38","    if not sparse.isspmatrix(laplacian):","39","        laplacian.flat[::n_nodes + 1] = value","40","    else:","41","        laplacian = laplacian.tocoo()","42","        diag_idx = (laplacian.row == laplacian.col)","43","        laplacian.data[diag_idx] = value","44","        # If the matrix has a small number of diagonals (as in the","45","        # case of structured matrices comming from images), the","46","        # dia format might be best suited for matvec products:","47","        n_diags = np.unique(laplacian.row - laplacian.col).size","48","        if n_diags <= 7:","49","            # 3 or less outer diagonals on each side","50","            laplacian = laplacian.todia()","51","        else:","52","            # csr has the fastest matvec and is thus best suited to","53","            # arpack","54","            laplacian = laplacian.tocsr()","55","    return laplacian","56","","57","","58","def spectral_embedding(adjacency, n_components=8, mode=None,","59","                       random_state=None, eig_tol=0.0):","60","    \"\"\"Project the sample on the first eigen vectors of the graph Laplacian","61","","62","    The adjacency matrix is used to compute a normalized graph Laplacian","63","    whose spectrum (especially the eigen vectors associated to the","64","    smallest eigen values) has an interpretation in terms of minimal","65","    number of cuts necessary to split the graph into comparably sized","66","    components.","67","","68","    This embedding can also 'work' even if the ``adjacency`` variable is","69","    not strictly the adjacency matrix of a graph but more generally","70","    an affinity or similarity matrix between samples (for instance the","71","    heat kernel of a euclidean distance matrix or a k-NN matrix).","72","","73","    However care must taken to always make the affinity matrix symmetric","74","    so that the eigen vector decomposition works as expected.","75","","76","    Parameters","77","    ----------","78","    adjacency: array-like or sparse matrix, shape: (n_samples, n_samples)","79","        The adjacency matrix of the graph to embed.","80","","81","    n_components: integer, optional","82","        The dimension of the projection subspace.","83","","84","    mode: {None, 'arpack', 'lobpcg', or 'amg'}","85","        The eigenvalue decomposition strategy to use. AMG requires pyamg","86","        to be installed. It can be faster on very large, sparse problems,","87","        but may also lead to instabilities","88","","89","    random_state: int seed, RandomState instance, or None (default)","90","        A pseudo random number generator used for the initialization of the","91","        lobpcg eigen vectors decomposition when mode == 'amg'. By default","92","        arpack is used.","93","","94","    eig_tol : float, optional, default: 0.0","95","        Stopping criterion for eigendecomposition of the Laplacian matrix","96","        when using arpack mode.","97","","98","    Returns","99","    -------","100","    embedding: array, shape: (n_samples, n_components)","101","        The reduced samples","102","","103","    Notes","104","    -----","105","    The graph should contain only one connected component, elsewhere the","106","    results make little sense.","107","","108","    References","109","    ----------","110","    [1] http:\/\/en.wikipedia.org\/wiki\/LOBPCG","111","    [2] LOBPCG: http:\/\/dx.doi.org\/10.1137%2FS1064827500366124","112","    \"\"\"","113","","114","    from scipy import sparse","115","    from ..utils.arpack import eigsh","116","    from scipy.sparse.linalg import lobpcg","117","    from scipy.sparse.linalg.eigen.lobpcg.lobpcg import symeig","118","    try:","119","        from pyamg import smoothed_aggregation_solver","120","    except ImportError:","121","        if mode == \"amg\":","122","            raise ValueError(\"The mode was set to 'amg', but pyamg is \"","123","                             \"not available.\")","124","","125","    random_state = check_random_state(random_state)","126","","127","    n_nodes = adjacency.shape[0]","128","    # XXX: Should we check that the matrices given is symmetric","129","    if mode is None:","130","        mode = 'arpack'","131","    elif not mode in ('arpack', 'lobpcg', 'amg'):","132","        raise ValueError(\"Unknown value for mode: '%s'.\"","133","                         \"Should be 'amg', 'arpack', or 'lobpcg'\" % mode)","134","    laplacian, dd = graph_laplacian(adjacency,","135","                                    normed=True, return_diag=True)","136","    if (mode == 'arpack'","137","        or mode != 'lobpcg' and","138","            (not sparse.isspmatrix(laplacian)","139","             or n_nodes < 5 * n_components)):","140","        # lobpcg used with mode='amg' has bugs for low number of nodes","141","        # for details see the source code in scipy:","142","        # https:\/\/github.com\/scipy\/scipy\/blob\/v0.11.0\/scipy\/sparse\/linalg\/eigen\/lobpcg\/lobpcg.py#L237","143","        # or matlab:","144","        # http:\/\/www.mathworks.com\/matlabcentral\/fileexchange\/48-lobpcg-m","145","        laplacian = _set_diag(laplacian, 0)","146","","147","        # Here we'll use shift-invert mode for fast eigenvalues","148","        # (see http:\/\/docs.scipy.org\/doc\/scipy\/reference\/tutorial\/arpack.html","149","        #  for a short explanation of what this means)","150","        # Because the normalized Laplacian has eigenvalues between 0 and 2,","151","        # I - L has eigenvalues between -1 and 1.  ARPACK is most efficient","152","        # when finding eigenvalues of largest magnitude (keyword which='LM')","153","        # and when these eigenvalues are very large compared to the rest.","154","        # For very large, very sparse graphs, I - L can have many, many","155","        # eigenvalues very near 1.0.  This leads to slow convergence.  So","156","        # instead, we'll use ARPACK's shift-invert mode, asking for the","157","        # eigenvalues near 1.0.  This effectively spreads-out the spectrum","158","        # near 1.0 and leads to much faster convergence: potentially an","159","        # orders-of-magnitude speedup over simply using keyword which='LA'","160","        # in standard mode.","161","        try:","162","            lambdas, diffusion_map = eigsh(-laplacian, k=n_components,","163","                                        sigma=1.0, which='LM',","164","                                        tol=eig_tol)","165","            embedding = diffusion_map.T[::-1] * dd","166","        except RuntimeError:","167","            # When submatrices are exactly singular, an LU decomposition","168","            # in arpack fails. We fallback to lobpcg","169","            mode = \"lobpcg\"","170","","171","    if mode == 'amg':","172","        # Use AMG to get a preconditioner and speed up the eigenvalue","173","        # problem.","174","        laplacian = laplacian.astype(np.float)  # lobpcg needs native floats","175","        ml = smoothed_aggregation_solver(laplacian.tocsr())","176","        M = ml.aspreconditioner()","177","        X = random_state.rand(laplacian.shape[0], n_components)","178","        X[:, 0] = dd.ravel()","179","        lambdas, diffusion_map = lobpcg(laplacian, X, M=M, tol=1.e-12,","180","                                        largest=False)","181","        embedding = diffusion_map.T * dd","182","        if embedding.shape[0] == 1:","183","            raise ValueError","184","    elif mode == \"lobpcg\":","185","        laplacian = laplacian.astype(np.float)  # lobpcg needs native floats","186","        if n_nodes < 5 * n_components + 1:","187","            # see note above under arpack why lopbcg has problems with small","188","            # number of nodes","189","            # lobpcg will fallback to symeig, so we short circuit it","190","            if sparse.isspmatrix(laplacian):","191","                laplacian = laplacian.todense()","192","            lambdas, diffusion_map = symeig(laplacian)","193","            embedding = diffusion_map.T[:n_components] * dd","194","        else:","195","            # lobpcg needs native floats","196","            laplacian = laplacian.astype(np.float)","197","            laplacian = _set_diag(laplacian, 1)","198","            # We increase the number of eigenvectors requested, as lobpcg","199","            # doesn't behave well in low dimension","200","            X = random_state.rand(laplacian.shape[0], n_components + 1)","201","            X[:, 0] = dd.ravel()","202","            lambdas, diffusion_map = lobpcg(laplacian, X, tol=1e-15,","203","                                            largest=False, maxiter=2000)","204","            embedding = diffusion_map.T[:n_components] * dd","205","            if embedding.shape[0] == 1:","206","                raise ValueError","207","    return embedding","208","","209","","344","","348","def spectral_clustering(affinity, n_clusters=8, n_components=None, mode=None,","349","                        random_state=None, n_init=10, k=None, eig_tol=0.0,","350","                        assign_labels='kmeans'):","379","    mode: {None, 'arpack' or 'amg'}","386","        of the lobpcg eigen vectors decomposition when mode == 'amg'","394","    eig_tol : float, optional, default: 0.0","396","        when using arpack mode.","435","                    \"'kmeans' or 'discretize', but '%s' was given\"","436","                    % assign_labels)","438","        warnings.warn(\"'k' was renamed to n_clusters\", DeprecationWarning)","443","                              mode=mode, random_state=random_state,","444","                              eig_tol=eig_tol)","447","        maps = maps[1:]","448","        _, labels, _ = k_means(maps.T, n_clusters, random_state=random_state,","451","        labels = discretize(maps.T, random_state=random_state)","493","    mode: {None, 'arpack' or 'amg'}","500","        of the lobpcg eigen vectors decomposition when mode == 'amg'","508","    eig_tol : float, optional, default: 0.0","510","        when using arpack mode.","560","    def __init__(self, n_clusters=8, mode=None, random_state=None, n_init=10,","561","                 gamma=1., affinity='rbf', n_neighbors=10, k=None, eig_tol=0.0,","562","                 assign_labels='kmeans'):","563","        if not k is None:","564","            warnings.warn(\"'k' was renamed to n_clusters\", DeprecationWarning)","567","        self.mode = mode","573","        self.eig_tol = eig_tol","602","                \"'nearest_neighbors' or 'precomputed', got '%s'.\"","603","                % self.affinity)","607","                            n_clusters=self.n_clusters, mode=self.mode,","608","                            random_state=self.random_state, n_init=self.n_init,","609","                            eig_tol=self.eig_tol,","610","                            assign_labels=self.assign_labels)"]}],"sklearn\/manifold\/tests\/test_spectral_embedding.py":[{"add":[],"delete":[]}],"sklearn\/cluster\/__init__.py":[{"add":["7","    get_bin_seeds","12","from ..utils import deprecated","13","","14","","15","# backward compatibility","16","@deprecated(\"to be removed in 0.15;\"","17","            \" use sklearn.manifold.spectral_embedding instead\")","18","def spectral_embedding(*args, **kwargs):","19","    \"\"\"Deprecated, use ``sklearn.manifold.spectral_embedding`` instead\"\"\"","20","    from ..manifold import spectral_embedding","21","    return spectral_embedding(*args, **kwargs)","22","","39","           'spectral_embedding',"],"delete":["7","     get_bin_seeds"]}],"examples\/manifold\/plot_compare_methods.py":[{"add":["41","            % (1000, n_neighbors), fontsize=14)","95","t0 = time()","96","se = manifold.SpectralEmbedding(n_components=n_components,","97","                                n_neighbors=n_neighbors)","98","Y = se.fit_transform(X)","99","t1 = time()","100","print \"SpectralEmbedding: %.2g sec\" % (t1 - t0)","101","ax = fig.add_subplot(248)","102","pl.scatter(Y[:, 0], Y[:, 1], c=color, cmap=pl.cm.Spectral)","103","pl.title(\"SpectralEmbedding (%.2g sec)\" % (t1 - t0))","104","ax.xaxis.set_major_formatter(NullFormatter())","105","ax.yaxis.set_major_formatter(NullFormatter())","106","pl.axis('tight')","107",""],"delete":["41","               % (1000, n_neighbors), fontsize=14)"]}],"sklearn\/manifold\/spectral_embedding.py":[{"add":[],"delete":[]}],"doc\/whats_new.rst":[{"add":["77","   - New estimator :class:`manifold.SpectralEmbedding` and function","78","     :func:`manifold.spectral_embedding`, implementing the","79","     \"laplacian eigenmaps\" for nonlinear dimensionality reduction by Wei Li.","140","   - :func:`cluster.spectral_embedding` is now in","141","     :func:`manifold.spectral_embedding`.","142","","143","   - Renamed ``eig_tol`` in :func:`manifold.spectral_embedding`,","144","     :class:`cluster.SpectralClustering` to ``eigen_tol``, renamed ``mode``","145","     to ``eigen_solver``","146","","147","   - Renamed ``mode`` in :func:`manifold.spectral_embedding` and","148","     :class:`cluster.SpectralClustering` to ``eigen_solver``."],"delete":[]}]}},"5763284ef9e04ffc52cf759a59bdc6bb81107616":{"changes":{"sklearn\/neighbors\/tests\/test_neighbors.py":"MODIFY","sklearn\/neighbors\/base.py":"MODIFY"},"diff":{"sklearn\/neighbors\/tests\/test_neighbors.py":[{"add":["20","from sklearn.utils.testing import assert_raises_regex","111","def test_n_neighbors_datatype():","112","    # Test to check whether n_neighbors is integer","113","    X = [[1, 1], [1, 1], [1, 1]]","114","    expected_msg = \"n_neighbors does not take .*float.* \" \\","115","                   \"value, enter integer value\"","116","    msg = \"Expected n_neighbors > 0. Got -3\"","117","","118","    neighbors_ = neighbors.NearestNeighbors(n_neighbors=3.)","119","    assert_raises_regex(TypeError, expected_msg, neighbors_.fit, X)","120","    assert_raises_regex(ValueError, msg,","121","                        neighbors_.kneighbors, X=X, n_neighbors=-3)","122","    assert_raises_regex(TypeError, expected_msg,","123","                        neighbors_.kneighbors, X=X, n_neighbors=3.)","124","","125",""],"delete":[]}],"sklearn\/neighbors\/base.py":[{"add":["260","            else:","261","                if not np.issubdtype(type(self.n_neighbors), np.integer):","262","                    raise TypeError(","263","                        \"n_neighbors does not take %s value, \"","264","                        \"enter integer value\" %","265","                        type(self.n_neighbors))","335","        elif n_neighbors <= 0:","336","            raise ValueError(","337","                \"Expected n_neighbors > 0. Got %d\" %","338","                n_neighbors","339","            )","340","        else:","341","            if not np.issubdtype(type(n_neighbors), np.integer):","342","                raise TypeError(","343","                    \"n_neighbors does not take %s value, \"","344","                    \"enter integer value\" %","345","                    type(n_neighbors))"],"delete":[]}]}},"7700b5ac7343a57492e30fd310cc63e8fd653bfa":{"changes":{"sklearn\/cluster\/tests\/test_affinity_propagation.py":"MODIFY","sklearn\/gaussian_process\/gpc.py":"MODIFY","sklearn\/linear_model\/ransac.py":"MODIFY","doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/linear_model\/tests\/test_logistic.py":"MODIFY","sklearn\/decomposition\/tests\/test_fastica.py":"MODIFY","sklearn\/linear_model\/logistic.py":"MODIFY","sklearn\/cluster\/affinity_propagation_.py":"MODIFY","sklearn\/linear_model\/tests\/test_ridge.py":"MODIFY","sklearn\/gaussian_process\/gpr.py":"MODIFY","sklearn\/cross_decomposition\/tests\/test_pls.py":"MODIFY","sklearn\/linear_model\/tests\/test_ransac.py":"MODIFY","sklearn\/cluster\/tests\/test_birch.py":"MODIFY","sklearn\/decomposition\/fastica_.py":"MODIFY","sklearn\/cross_decomposition\/pls_.py":"MODIFY","sklearn\/cluster\/birch.py":"MODIFY","sklearn\/linear_model\/ridge.py":"MODIFY"},"diff":{"sklearn\/cluster\/tests\/test_affinity_propagation.py":[{"add":["135","    af = assert_warns(ConvergenceWarning,","136","                      AffinityPropagation(preference=-10, max_iter=1).fit, X)","140","    to_predict = np.array([[2, 2], [3, 3], [4, 4]])","141","    y = assert_warns(ConvergenceWarning, af.predict, to_predict)","142","    assert_array_equal(np.array([-1, -1, -1]), y)"],"delete":["135","    af = AffinityPropagation(preference=-10, max_iter=1).fit(X)","139","    assert_array_equal(np.array([-1, -1, -1]),","140","                       af.predict(np.array([[2, 2], [3, 3], [4, 4]])))"]}],"sklearn\/gaussian_process\/gpc.py":[{"add":["21","from sklearn.exceptions import ConvergenceWarning","431","                              \" state: %s\" % convergence_dict,","432","                              ConvergenceWarning)"],"delete":["430","                              \" state: %s\" % convergence_dict)"]}],"sklearn\/linear_model\/ransac.py":[{"add":["15","from ..exceptions import ConvergenceWarning","456","                              ConvergenceWarning)"],"delete":["455","                              UserWarning)"]}],"doc\/whats_new\/v0.20.rst":[{"add":["385","Misc","386","","387","- Changed warning type from UserWarning to ConvergenceWarning for failing","388","  convergence in :func:`linear_model.logistic_regression_path`,","389","  :class:`linear_model.RANSACRegressor`, :func:`linear_model.ridge_regression`,","390","  :class:`gaussian_process.GaussianProcessRegressor`,","391","  :class:`gaussian_process.GaussianProcessClassifier`,","392","  :func:`decomposition.fastica`, :class:`cross_decomposition.PLSCanonical`,","393","  :class:`cluster.AffinityPropagation`, and :class:`cluster.Birch`.","394","  :issue:`#10306` by :user:`Jonathan Siebert <jotasi>`.","395",""],"delete":[]}],"sklearn\/linear_model\/tests\/test_logistic.py":[{"add":["314","def test_logistic_regression_path_convergence_fail():","315","    rng = np.random.RandomState(0)","316","    X = np.concatenate((rng.randn(100, 2) + [1, 1], rng.randn(100, 2)))","317","    y = [1] * 100 + [-1] * 100","318","    Cs = [1e3]","319","    assert_warns(ConvergenceWarning, logistic_regression_path,","320","                 X, y, Cs=Cs, tol=0., max_iter=1, random_state=0, verbose=1)","321","","322",""],"delete":[]}],"sklearn\/decomposition\/tests\/test_fastica.py":[{"add":["20","from sklearn.exceptions import ConvergenceWarning","144","def test_fastica_convergence_fail():","145","    # Test the FastICA algorithm on very simple data","146","    # (see test_non_square_fastica).","147","    # Ensure a ConvergenceWarning raised if the tolerance is sufficiently low.","148","    rng = np.random.RandomState(0)","149","","150","    n_samples = 1000","151","    # Generate two sources:","152","    t = np.linspace(0, 100, n_samples)","153","    s1 = np.sin(t)","154","    s2 = np.ceil(np.sin(np.pi * t))","155","    s = np.c_[s1, s2].T","156","    center_and_norm(s)","157","    s1, s2 = s","158","","159","    # Mixing matrix","160","    mixing = rng.randn(6, 2)","161","    m = np.dot(mixing, s)","162","","163","    # Do fastICA with tolerance 0. to ensure failing convergence","164","    ica = FastICA(algorithm=\"parallel\", n_components=2, random_state=rng,","165","                  max_iter=2, tol=0.)","166","    assert_warns(ConvergenceWarning, ica.fit, m.T)","167","","168",""],"delete":[]}],"sklearn\/linear_model\/logistic.py":[{"add":["31","from ..exceptions import NotFittedError, ConvergenceWarning","718","                              \"of iterations.\", ConvergenceWarning)"],"delete":["31","from ..exceptions import NotFittedError","718","                              \"of iterations.\")"]}],"sklearn\/cluster\/affinity_propagation_.py":[{"add":["392","                          \"Labeling every sample as '-1'.\", ConvergenceWarning)"],"delete":["392","                          \"Labeling every sample as '-1'.\")"]}],"sklearn\/linear_model\/tests\/test_ridge.py":[{"add":["16","from sklearn.exceptions import ConvergenceWarning","17","","141","def test_ridge_regression_convergence_fail():","142","    rng = np.random.RandomState(0)","143","    y = rng.randn(5)","144","    X = rng.randn(5, 10)","145","","146","    assert_warns(ConvergenceWarning, ridge_regression,","147","                 X, y, alpha=1.0, solver=\"sparse_cg\",","148","                 tol=0., max_iter=None, verbose=1)","149","","150",""],"delete":[]}],"sklearn\/gaussian_process\/gpr.py":[{"add":["18","from sklearn.exceptions import ConvergenceWarning","464","                              \" state: %s\" % convergence_dict,","465","                              ConvergenceWarning)"],"delete":["463","                              \" state: %s\" % convergence_dict)"]}],"sklearn\/cross_decomposition\/tests\/test_pls.py":[{"add":["5","                                   assert_raise_message, assert_warns)","10","from sklearn.exceptions import ConvergenceWarning","263","def test_convergence_fail():","264","    d = load_linnerud()","265","    X = d.data","266","    Y = d.target","267","    pls_bynipals = pls_.PLSCanonical(n_components=X.shape[1],","268","                                     max_iter=2, tol=1e-10)","269","    assert_warns(ConvergenceWarning, pls_bynipals.fit, X, Y)","270","","271",""],"delete":["5","                                   assert_raise_message)"]}],"sklearn\/linear_model\/tests\/test_ransac.py":[{"add":["15","from sklearn.exceptions import ConvergenceWarning","233","    assert_warns(ConvergenceWarning, ransac_estimator.fit, X, y)"],"delete":["232","    assert_warns(UserWarning, ransac_estimator.fit, X, y)"]}],"sklearn\/cluster\/tests\/test_birch.py":[{"add":["11","from sklearn.exceptions import ConvergenceWarning","96","    assert_warns(ConvergenceWarning, brc4.fit, X)"],"delete":["95","    assert_warns(UserWarning, brc4.fit, X)"]}],"sklearn\/decomposition\/fastica_.py":[{"add":["17","from ..exceptions import ConvergenceWarning","119","                      'tolerance or the maximum number of iterations.',","120","                      ConvergenceWarning)"],"delete":["118","                      'tolerance or the maximum number of iterations.')"]}],"sklearn\/cross_decomposition\/pls_.py":[{"add":["18","from ..exceptions import ConvergenceWarning","77","            warnings.warn('Maximum number of iterations reached',","78","                          ConvergenceWarning)"],"delete":["76","            warnings.warn('Maximum number of iterations reached')"]}],"sklearn\/cluster\/birch.py":[{"add":["17","from ..exceptions import NotFittedError, ConvergenceWarning","628","                    % (len(centroids), self.n_clusters), ConvergenceWarning)"],"delete":["17","from ..exceptions import NotFittedError","628","                    % (len(centroids), self.n_clusters))"]}],"sklearn\/linear_model\/ridge.py":[{"add":["33","from ..exceptions import ConvergenceWarning","76","                          info, ConvergenceWarning)"],"delete":["75","                          info)"]}]}}}