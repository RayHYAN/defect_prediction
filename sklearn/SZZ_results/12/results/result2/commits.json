{"95aa2952e1544a5a2e0f14d366bae1bfd8e9195a":{"changes":{"sklearn\/utils\/fixes.py":"MODIFY","sklearn\/mixture\/gmm.py":"MODIFY","sklearn\/naive_bayes.py":"MODIFY","sklearn\/linear_model\/logistic.py":"MODIFY","sklearn\/model_selection\/tests\/test_split.py":"MODIFY","sklearn\/decomposition\/online_lda.py":"MODIFY","sklearn\/ensemble\/tests\/test_forest.py":"MODIFY","sklearn\/mixture\/base.py":"MODIFY","sklearn\/metrics\/cluster\/supervised.py":"MODIFY","sklearn\/model_selection\/_split.py":"MODIFY","sklearn\/ensemble\/gradient_boosting.py":"MODIFY","sklearn\/mixture\/dpgmm.py":"MODIFY","sklearn\/utils\/extmath.py":"MODIFY","sklearn\/utils\/tests\/test_random.py":"MODIFY","sklearn\/linear_model\/tests\/test_sag.py":"MODIFY"},"diff":{"sklearn\/utils\/fixes.py":[{"add":["36","","143","    from scipy.sparse.linalg import lsqr as sparse_lsqr  # noqa","144","","145","","146","try:  # SciPy >= 0.19","147","    from scipy.special import comb, logsumexp","148","except ImportError:","149","    from scipy.misc import comb, logsumexp  # noqa","153","    \"\"\"Workaround for Python 2 limitations of pickling instance methods\"\"\""],"delete":["13","import sys","14","import functools","144","    from scipy.sparse.linalg import lsqr as sparse_lsqr","148","    \"\"\"Helper to workaround Python 2 limitations of pickling instance methods\"\"\""]}],"sklearn\/mixture\/gmm.py":[{"add":["24","from ..utils.fixes import logsumexp"],"delete":["21","from scipy.misc import logsumexp"]}],"sklearn\/naive_bayes.py":[{"add":["29","from .utils.fixes import logsumexp"],"delete":["21","from scipy.misc import logsumexp"]}],"sklearn\/linear_model\/logistic.py":[{"add":["28","from ..utils.fixes import logsumexp"],"delete":["17","from scipy.misc import logsumexp"]}],"sklearn\/model_selection\/tests\/test_split.py":[{"add":["58","from sklearn.utils.fixes import comb","59",""],"delete":["7","from scipy.misc import comb"]}],"sklearn\/decomposition\/online_lda.py":[{"add":["21","from ..utils.fixes import logsumexp"],"delete":["15","from scipy.misc import logsumexp"]}],"sklearn\/ensemble\/tests\/test_forest.py":[{"add":["43","from sklearn.utils.fixes import comb"],"delete":["16","from scipy.misc import comb"]}],"sklearn\/mixture\/base.py":[{"add":["20","from ..utils.fixes import logsumexp"],"delete":["13","from scipy.misc import logsumexp"]}],"sklearn\/metrics\/cluster\/supervised.py":[{"add":["24","from ...utils.fixes import comb"],"delete":["20","from scipy.misc import comb"]}],"sklearn\/model_selection\/_split.py":[{"add":["30","from ..utils.fixes import signature, comb"],"delete":["24","from scipy.misc import comb","31","from ..utils.fixes import signature"]}],"sklearn\/ensemble\/gradient_boosting.py":[{"add":["57","from ..utils.fixes import logsumexp"],"delete":["29","from ..base import BaseEstimator","42","from scipy.misc import logsumexp"]}],"sklearn\/mixture\/dpgmm.py":[{"add":["27","from ..utils.fixes import logsumexp"],"delete":["23","from scipy.misc import logsumexp"]}],"sklearn\/utils\/extmath.py":[{"add":["22","from .fixes import logsumexp as scipy_logsumexp"],"delete":["19","from scipy.misc import logsumexp as scipy_logsumexp"]}],"sklearn\/utils\/tests\/test_random.py":[{"add":["7","from sklearn.utils.fixes import comb","90","        n_expected = comb(n_population, n_samples, exact=True)"],"delete":["4","from scipy.misc import comb as combinations","90","        n_expected = combinations(n_population, n_samples, exact=True)"]}],"sklearn\/linear_model\/tests\/test_sag.py":[{"add":["15","from sklearn.utils.fixes import logsumexp"],"delete":["8","from scipy.misc import logsumexp"]}]}},"52b6a669669019d63bbe03f32d919a62538779b9":{"changes":{"doc\/modules\/clustering.rst":"MODIFY","doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/metrics\/cluster\/tests\/test_supervised.py":"MODIFY","sklearn\/metrics\/cluster\/supervised.py":"MODIFY"},"diff":{"doc\/modules\/clustering.rst":[{"add":["1160","measure are available, **Normalized Mutual Information (NMI)** and **Adjusted","1161","Mutual Information (AMI)**. NMI is often used in the literature, while AMI was","1214","- **Upper bound  of 1**:  Values close to zero indicate two label","1216","  indicate significant agreement. Further, an AMI of exactly 1 indicates","1270",".. math:: \\text{NMI}(U, V) = \\frac{\\text{MI}(U, V)}{\\text{mean}(H(U), H(V))}","1278","following equation [VEB2009]_. In this equation,","1291",".. math:: \\text{AMI} = \\frac{\\text{MI} - E[\\text{MI}]}{\\text{mean}(H(U), H(V)) - E[\\text{MI}]}","1292","","1293","For normalized mutual information and adjusted mutual information, the normalizing","1294","value is typically some *generalized* mean of the entropies of each clustering.","1295","Various generalized means exist, and no firm rules exist for preferring one over the","1296","others.  The decision is largely a field-by-field basis; for instance, in community","1297","detection, the arithmetic mean is most common. Each","1298","normalizing method provides \"qualitatively similar behaviours\" [YAT2016]_. In our","1299","implementation, this is controlled by the ``average_method`` parameter.","1300","","1301","Vinh et al. (2010) named variants of NMI and AMI by their averaging method [VEB2010]_. Their","1302","'sqrt' and 'sum' averages are the geometric and arithmetic means; we use these","1303","more broadly common names.","1312"," * [VEB2009] Vinh, Epps, and Bailey, (2009). \"Information theoretic measures","1318"," * [VEB2010] Vinh, Epps, and Bailey, (2010). \"Information Theoretic Measures for","1320","   Correction for Chance\". JMLR","1321","   <http:\/\/jmlr.csail.mit.edu\/papers\/volume11\/vinh10a\/vinh10a.pdf>","1328","   ","1329"," * [YAT2016] Yang, Algesheimer, and Tessone, (2016). \"A comparative analysis of","1330","   community","1331","   detection algorithms on artificial networks\". Scientific Reports 6: 30750.","1332","   `doi:10.1038\/srep30750 <https:\/\/www.nature.com\/articles\/srep30750>`_.","1333","   ","1334","   ","1374","discussed above, with the aggregation function being the arithmetic mean [B2011]_.","1549","- **Upper-bounded at 1**:  Values close to zero indicate two label"],"delete":["1160","measure are available, **Normalized Mutual Information(NMI)** and **Adjusted","1161","Mutual Information(AMI)**. NMI is often used in the literature while AMI was","1214","- **Bounded range [0, 1]**:  Values close to zero indicate two label","1216","  indicate significant agreement. Further, values of exactly 0 indicate","1217","  **purely** independent label assignments and a AMI of exactly 1 indicates","1220","- **No assumption is made on the cluster structure**: can be used","1221","  to compare clustering algorithms such as k-means which assumes isotropic","1222","  blob shapes with results of spectral clustering algorithms which can","1223","  find cluster with \"folded\" shapes.","1224","","1276",".. math:: \\text{NMI}(U, V) = \\frac{\\text{MI}(U, V)}{\\sqrt{H(U)H(V)}}","1284","following equation, from Vinh, Epps, and Bailey, (2009). In this equation,","1297",".. math:: \\text{AMI} = \\frac{\\text{MI} - E[\\text{MI}]}{\\max(H(U), H(V)) - E[\\text{MI}]}","1306"," * Vinh, Epps, and Bailey, (2009). \"Information theoretic measures","1312"," * Vinh, Epps, and Bailey, (2010). Information Theoretic Measures for","1314","   Correction for Chance, JMLR","1315","   http:\/\/jmlr.csail.mit.edu\/papers\/volume11\/vinh10a\/vinh10a.pdf","1361","discussed above normalized by the sum of the label entropies [B2011]_.","1536","- **Bounded range [0, 1]**:  Values close to zero indicate two label"]}],"doc\/whats_new\/v0.20.rst":[{"add":["208","- Added control over the normalization in ","209","  :func:`metrics.normalized_mutual_information_score` and","210","  :func:`metrics.adjusted_mutual_information_score` via the ``average_method``","211","  parameter. In version 0.22, the default normalizer for each will become","212","  the *arithmetic* mean of the entropies of each clustering. :issue:`11124` by","213","  :user:`Arya McCarthy <aryamccarthy>`.","800","- In :func:`metrics.normalized_mutual_information_score` and","801","  :func:`metrics.adjusted_mutual_information_score`, ","802","  warn that ``average_method``","803","  will have a new default value. In version 0.22, the default normalizer for each ","804","  will become the *arithmetic* mean of the entropies of each clustering. Currently,","805","  :func:`metrics.normalized_mutual_information_score` uses the default of","806","  ``average_method='geometric'``, and :func:`metrics.adjusted_mutual_information_score`","807","  uses the default of ``average_method='max'`` to match their behaviors in","808","  version 0.19.","809","  :issue:`11124` by :user:`Arya McCarthy <aryamccarthy>`.","810",""],"delete":[]}],"sklearn\/metrics\/cluster\/tests\/test_supervised.py":[{"add":["14","from sklearn.metrics.cluster.supervised import _generalized_average","19","        assert_warns_message, ignore_warnings","34","def test_future_warning():","35","    score_funcs_with_changing_means = [","36","        normalized_mutual_info_score,","37","        adjusted_mutual_info_score,","38","    ]","39","    warning_msg = \"The behavior of \"","40","    args = [0, 0, 0], [0, 0, 0]","41","    for score_func in score_funcs_with_changing_means:","42","        assert_warns_message(FutureWarning, warning_msg, score_func, *args)","43","","44","","45","@ignore_warnings(category=FutureWarning)","62","def test_generalized_average():","63","    a, b = 1, 2","64","    methods = [\"min\", \"geometric\", \"arithmetic\", \"max\"]","65","    means = [_generalized_average(a, b, method) for method in methods]","66","    assert means[0] <= means[1] <= means[2] <= means[3]","67","    c, d = 12, 12","68","    means = [_generalized_average(c, d, method) for method in methods]","69","    assert means[0] == means[1] == means[2] == means[3]","70","","71","","72","@ignore_warnings(category=FutureWarning)","82","    score_funcs_with_changing_means = [","83","        normalized_mutual_info_score,","84","        adjusted_mutual_info_score,","85","    ]","86","    means = {\"min\", \"geometric\", \"arithmetic\", \"max\"}","87","    for score_func in score_funcs_with_changing_means:","88","        for mean in means:","89","            assert score_func([], [], mean) == 1.0","90","            assert score_func([0], [1], mean) == 1.0","91","            assert score_func([0, 0, 0], [0, 0, 0], mean) == 1.0","92","            assert score_func([0, 1, 0], [42, 7, 42], mean) == 1.0","93","            assert score_func([0., 1., 0.], [42., 7., 42.], mean) == 1.0","94","            assert score_func([0., 1., 2.], [42., 7., 2.], mean) == 1.0","95","            assert score_func([0, 1, 2], [42, 7, 2], mean) == 1.0","128","def test_non_consecutive_labels():","150","@ignore_warnings(category=FutureWarning)","164","@ignore_warnings(category=FutureWarning)","178","@ignore_warnings(category=FutureWarning)","259","@ignore_warnings(category=FutureWarning)","269","        for method in [\"min\", \"geometric\", \"arithmetic\", \"max\"]:","270","            assert adjusted_mutual_info_score(labels_a, labels_b,","271","                                              method) == 0.0","272","            assert normalized_mutual_info_score(labels_a, labels_b,","273","                                                method) == 0.0","285","        avg = 'arithmetic'","286","        assert_almost_equal(v_measure_score(labels_a, labels_b),","287","                            normalized_mutual_info_score(labels_a, labels_b,","288","                                                         average_method=avg)","289","                            )"],"delete":["89","def test_non_consicutive_labels():"]}],"sklearn\/metrics\/cluster\/supervised.py":[{"add":["13","#          Arya McCarthy <arya@jhu.edu>","19","import warnings","63","def _generalized_average(U, V, average_method):","64","    \"\"\"Return a particular mean of two numbers.\"\"\"","65","    if average_method == \"min\":","66","        return min(U, V)","67","    elif average_method == \"geometric\":","68","        return np.sqrt(U * V)","69","    elif average_method == \"arithmetic\":","70","        return np.mean([U, V])","71","    elif average_method == \"max\":","72","        return max(U, V)","73","    else:","74","        raise ValueError(\"'average_method' must be 'min', 'geometric', \"","75","                         \"'arithmetic', or 'max'\")","76","","77","","264","    homogeneity and completeness. V-Measure is identical to","265","    :func:`normalized_mutual_info_score` with the arithmetic averaging","266","    method.","465","    This score is identical to :func:`normalized_mutual_info_score` with","466","    the ``'arithmetic'`` option for averaging.","481","","508","    normalized_mutual_info_score","641","def adjusted_mutual_info_score(labels_true, labels_pred,","642","                               average_method='warn'):","651","        AMI(U, V) = [MI(U, V) - E(MI(U, V))] \/ [avg(H(U), H(V)) - E(MI(U, V))]","675","    average_method : string, optional (default: 'warn')","676","        How to compute the normalizer in the denominator. Possible options","677","        are 'min', 'geometric', 'arithmetic', and 'max'.","678","        If 'warn', 'max' will be used. The default will change to","679","        'arithmetic' in version 0.22.","680","","681","        .. versionadded:: 0.20","682","","685","    ami: float (upperlimited by 1.0)","724","    if average_method == 'warn':","725","        warnings.warn(\"The behavior of AMI will change in version 0.22. \"","726","                      \"To match the behavior of 'v_measure_score', AMI will \"","727","                      \"use average_method='arithmetic' by default.\",","728","                      FutureWarning)","729","        average_method = 'max'","748","    normalizer = _generalized_average(h_true, h_pred, average_method)","749","    denominator = normalizer - emi","750","    # Avoid 0.0 \/ 0.0 when expectation equals maximum, i.e a perfect match.","751","    # normalizer should always be >= emi, but because of floating-point","752","    # representation, sometimes emi is slightly larger. Correct this","753","    # by preserving the sign.","754","    if denominator < 0:","755","        denominator = min(denominator, -np.finfo('float64').eps)","756","    else:","757","        denominator = max(denominator, np.finfo('float64').eps)","758","    ami = (mi - emi) \/ denominator","762","def normalized_mutual_info_score(labels_true, labels_pred,","763","                                 average_method='warn'):","769","    information is normalized by some generalized mean of ``H(labels_true)``","770","    and ``H(labels_pred))``, defined by the `average_method`.","794","    average_method : string, optional (default: 'warn')","795","        How to compute the normalizer in the denominator. Possible options","796","        are 'min', 'geometric', 'arithmetic', and 'max'.","797","        If 'warn', 'geometric' will be used. The default will change to","798","        'arithmetic' in version 0.22.","799","","800","        .. versionadded:: 0.20","801","","809","    v_measure_score: V-Measure (NMI with arithmetic mean option.)","833","    if average_method == 'warn':","834","        warnings.warn(\"The behavior of NMI will change in version 0.22. \"","835","                      \"To match the behavior of 'v_measure_score', NMI will \"","836","                      \"use average_method='arithmetic' by default.\",","837","                      FutureWarning)","838","        average_method = 'geometric'","855","    normalizer = _generalized_average(h_true, h_pred, average_method)","856","    # Avoid 0.0 \/ 0.0 when either entropy is zero.","857","    normalizer = max(normalizer, np.finfo('float64').eps)","858","    nmi = mi \/ normalizer"],"delete":["247","    homogeneity and completeness.","446","    This score is identical to :func:`normalized_mutual_info_score`.","619","def adjusted_mutual_info_score(labels_true, labels_pred):","628","        AMI(U, V) = [MI(U, V) - E(MI(U, V))] \/ [max(H(U), H(V)) - E(MI(U, V))]","654","    ami: float(upperlimited by 1.0)","711","    ami = (mi - emi) \/ (max(h_true, h_pred) - emi)","715","def normalized_mutual_info_score(labels_true, labels_pred):","721","    information is normalized by ``sqrt(H(labels_true) * H(labels_pred))``.","791","    nmi = mi \/ max(np.sqrt(h_true * h_pred), 1e-10)"]}]}},"fa5926afae6e6d47ea34049b0203438c3900de94":{"changes":{"doc\/faq.rst":"MODIFY"},"diff":{"doc\/faq.rst":[{"add":["27","issues <easy_issues>`. Please do not contact the contributors of scikit-learn directly","28","regarding contributing to scikit-learn."],"delete":["27","issues <easy_issues>`."]}]}},"71770dc464810d1327c633f92b55e58b4d6a0bb6":{"changes":{"sklearn\/utils\/metaestimators.py":"MODIFY"},"diff":{"sklearn\/utils\/metaestimators.py":[{"add":["154","    Labels y will always be indexed only along the first axis.","163","        Data to be indexed. If ``estimator._pairwise is True``,","167","        Targets to be indexed.","180","    X_subset : array-like, sparse matrix or list","181","        Indexed data.","183","    y_subset : array-like, sparse matrix or list","184","        Indexed targets."],"delete":["154","    Labels y will always be sliced only along the last axis.","163","        Data to be sliced. If ``estimator._pairwise is True``,","167","        Targets to be sliced.","180","    X_sliced : array-like, sparse matrix or list","181","        Sliced data.","183","    y_sliced : array-like, sparse matrix or list","184","        Sliced targets."]}]}},"a4f8e3d2a266fe4a253b449214806562ab83dda5":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/preprocessing\/tests\/test_data.py":"MODIFY","sklearn\/preprocessing\/data.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["515","- Fix ``fit`` and ``partial_fit`` in :class:`preprocessing.StandardScaler` in","516","  the rare case when `with_mean=False` and `with_std=False` which was crashing","517","  by calling ``fit`` more than once and giving inconsistent results for","518","  ``mean_`` whether the input was a sparse or a dense matrix. ``mean_`` will be","519","  set to ``None`` with both sparse and dense inputs. ``n_samples_seen_`` will","520","  be also reported for both input types.","521","  :issue:`11235` by :user:`Guillaume Lemaitre <glemaitre>`.","522",""],"delete":[]}],"sklearn\/preprocessing\/tests\/test_data.py":[{"add":["9","import itertools","63","from sklearn.base import clone","705","def _check_identity_scalers_attributes(scaler_1, scaler_2):","706","    assert scaler_1.mean_ is scaler_2.mean_ is None","707","    assert scaler_1.var_ is scaler_2.var_ is None","708","    assert scaler_1.scale_ is scaler_2.scale_ is None","709","    assert scaler_1.n_samples_seen_ == scaler_2.n_samples_seen_","710","","711","","712","def test_scaler_return_identity():","713","    # test that the scaler return identity when with_mean and with_std are","714","    # False","715","    X_dense = np.array([[0, 1, 3],","716","                        [5, 6, 0],","717","                        [8, 0, 10]],","718","                       dtype=np.float64)","719","    X_csr = sparse.csr_matrix(X_dense)","720","    X_csc = X_csr.tocsc()","721","","722","    transformer_dense = StandardScaler(with_mean=False, with_std=False)","723","    X_trans_dense = transformer_dense.fit_transform(X_dense)","724","","725","    transformer_csr = clone(transformer_dense)","726","    X_trans_csr = transformer_csr.fit_transform(X_csr)","727","","728","    transformer_csc = clone(transformer_dense)","729","    X_trans_csc = transformer_csc.fit_transform(X_csc)","730","","731","    assert_allclose(X_trans_csr.toarray(), X_csr.toarray())","732","    assert_allclose(X_trans_csc.toarray(), X_csc.toarray())","733","    assert_allclose(X_trans_dense, X_dense)","734","","735","    for trans_1, trans_2 in itertools.combinations([transformer_dense,","736","                                                    transformer_csr,","737","                                                    transformer_csc],","738","                                                   2):","739","        _check_identity_scalers_attributes(trans_1, trans_2)","740","","741","    transformer_dense.partial_fit(X_dense)","742","    transformer_csr.partial_fit(X_csr)","743","    transformer_csc.partial_fit(X_csc)","744","","745","    for trans_1, trans_2 in itertools.combinations([transformer_dense,","746","                                                    transformer_csr,","747","                                                    transformer_csc],","748","                                                   2):","749","        _check_identity_scalers_attributes(trans_1, trans_2)","750","","751","    transformer_dense.fit(X_dense)","752","    transformer_csr.fit(X_csr)","753","    transformer_csc.fit(X_csc)","754","","755","    for trans_1, trans_2 in itertools.combinations([transformer_dense,","756","                                                    transformer_csr,","757","                                                    transformer_csc],","758","                                                   2):","759","        _check_identity_scalers_attributes(trans_1, trans_2)","760","","761",""],"delete":[]}],"sklearn\/preprocessing\/data.py":[{"add":["654","                if not hasattr(self, 'n_samples_seen_'):","655","                    self.n_samples_seen_ = X.shape[0]","656","                else:","657","                    self.n_samples_seen_ += X.shape[0]","668","            if not self.with_mean and not self.with_std:","669","                self.mean_ = None","670","                self.var_ = None","671","                self.n_samples_seen_ += X.shape[0]","672","            else:","673","                self.mean_, self.var_, self.n_samples_seen_ = \\","674","                    _incremental_mean_and_var(X, self.mean_, self.var_,","675","                                              self.n_samples_seen_)"],"delete":["664","            self.mean_, self.var_, self.n_samples_seen_ = \\","665","                _incremental_mean_and_var(X, self.mean_, self.var_,","666","                                          self.n_samples_seen_)"]}]}},"c62338feb5de4b6eabf055fd1643a3e022fe4547":{"changes":{"sklearn\/tree\/tests\/test_export.py":"MODIFY","sklearn\/tree\/export.py":"MODIFY"},"diff":{"sklearn\/tree\/tests\/test_export.py":[{"add":["235","    # Check error when argument is not an estimator","236","    message = \"is not an estimator instance\"","237","    assert_raise_message(TypeError, message,","238","                         export_graphviz, clf.fit(X, y).tree_)","239",""],"delete":["212","    assert_equal(contents1, contents2)","213",""]}],"sklearn\/tree\/export.py":[{"add":["70","","71","","462","        recurse(decision_tree.tree_, 0, criterion=decision_tree.criterion)"],"delete":["460","        if isinstance(decision_tree, _tree.Tree):","461","            recurse(decision_tree, 0, criterion=\"impurity\")","462","        else:","463","            recurse(decision_tree.tree_, 0, criterion=decision_tree.criterion)"]}]}},"399f1b27615aa0f4b0901e7164fe043c7f5ecf5b":{"changes":{"sklearn\/datasets\/base.py":"MODIFY","doc\/modules\/clustering.rst":"MODIFY","doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/datasets\/descr\/iris.rst":"MODIFY","sklearn\/covariance\/tests\/test_graph_lasso.py":"MODIFY","sklearn\/covariance\/tests\/test_graphical_lasso.py":"MODIFY","doc\/modules\/model_evaluation.rst":"MODIFY","sklearn\/datasets\/data\/iris.csv":"MODIFY"},"diff":{"sklearn\/datasets\/base.py":[{"add":["362","    Notes","363","    -----","364","        .. versionchanged:: 0.20","365","            Fixed two wrong data points according to Fisher's paper.","366","            The new version is the same as in R, but not as in the UCI","367","            Machine Learning Repository.","368","","710","    Notes","711","    -----"],"delete":[]}],"doc\/modules\/clustering.rst":[{"add":["1569","  561.62...","1638","  0.6619..."],"delete":["1569","  560.39...","1638","  0.6623..."]}],"doc\/whats_new\/v0.20.rst":[{"add":["480","- Fixed a bug in :func:`dataset.load_iris` which had two wrong data points.","481","  :issue:`11082` by :user:`Sadhana Srinivasan <rotuna>`","482","  and :user:`Hanmin Qin <qinhanmin2014>`.","483",""],"delete":[]}],"sklearn\/datasets\/descr\/iris.rst":[{"add":["34","The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken","35","from Fisher's paper. Note that it's the same as in R, but not as in the UCI","36","Machine Learning Repository, which has two wrong data points."],"delete":["34","This is a copy of UCI ML iris datasets.","35","http:\/\/archive.ics.uci.edu\/ml\/datasets\/Iris","36","","37","The famous Iris database, first used by Sir R.A. Fisher"]}],"sklearn\/covariance\/tests\/test_graph_lasso.py":[{"add":["69","    # (need to set penalize.diagonal to FALSE)","71","        [0.68112222, 0.0000000, 0.265820, 0.02464314],","72","        [0.00000000, 0.1887129, 0.000000, 0.00000000],","73","        [0.26582000, 0.0000000, 3.095503, 0.28697200],","74","        [0.02464314, 0.0000000, 0.286972, 0.57713289]","77","        [1.5190747, 0.000000, -0.1304475, 0.0000000],","78","        [0.0000000, 5.299055, 0.0000000, 0.0000000],","79","        [-0.1304475, 0.000000, 0.3498624, -0.1683946],","80","        [0.0000000, 0.000000, -0.1683946, 1.8164353]"],"delete":["69","    # The iris datasets in R and scikit-learn do not match in a few places,","70","    # these values are for the scikit-learn version.","72","        [0.68112222, 0.0, 0.2651911, 0.02467558],","73","        [0.00, 0.1867507, 0.0, 0.00],","74","        [0.26519111, 0.0, 3.0924249, 0.28774489],","75","        [0.02467558, 0.0, 0.2877449, 0.57853156]","78","        [1.5188780, 0.0, -0.1302515, 0.0],","79","        [0.0, 5.354733, 0.0, 0.0],","80","        [-0.1302515, 0.0, 0.3502322, -0.1686399],","81","        [0.0, 0.0, -0.1686399, 1.8123908]"]}],"sklearn\/covariance\/tests\/test_graphical_lasso.py":[{"add":["67","    # (need to set penalize.diagonal to FALSE)","69","        [0.68112222, 0.0000000, 0.265820, 0.02464314],","70","        [0.00000000, 0.1887129, 0.000000, 0.00000000],","71","        [0.26582000, 0.0000000, 3.095503, 0.28697200],","72","        [0.02464314, 0.0000000, 0.286972, 0.57713289]","73","        ])","75","        [1.5190747, 0.000000, -0.1304475, 0.0000000],","76","        [0.0000000, 5.299055, 0.0000000, 0.0000000],","77","        [-0.1304475, 0.000000, 0.3498624, -0.1683946],","78","        [0.0000000, 0.000000, -0.1683946, 1.8164353]","79","        ])"],"delete":["67","    # The iris datasets in R and scikit-learn do not match in a few places,","68","    # these values are for the scikit-learn version.","70","        [0.68112222, 0.0, 0.2651911, 0.02467558],","71","        [0.00, 0.1867507, 0.0, 0.00],","72","        [0.26519111, 0.0, 3.0924249, 0.28774489],","73","        [0.02467558, 0.0, 0.2877449, 0.57853156]","74","    ])","76","        [1.5188780, 0.0, -0.1302515, 0.0],","77","        [0.0, 5.354733, 0.0, 0.0],","78","        [-0.1302515, 0.0, 0.3502322, -0.1686399],","79","        [0.0, 0.0, -0.1686399, 1.8123908]","80","    ])"]}],"doc\/modules\/model_evaluation.rst":[{"add":["102","    array([-0.10..., -0.16..., -0.07...])"],"delete":["102","    array([-0.09..., -0.16..., -0.07...])"]}],"sklearn\/datasets\/data\/iris.csv":[{"add":["35","4.9,3.1,1.5,0.2,0","38","4.9,3.6,1.4,0.1,0"],"delete":["35","4.9,3.1,1.5,0.1,0","38","4.9,3.1,1.5,0.1,0"]}]}},"f4b230cc65f2a9091545779cb8595848765904fc":{"changes":{"examples\/decomposition\/plot_image_denoising.py":"MODIFY"},"diff":{"examples\/decomposition\/plot_image_denoising.py":[{"add":["55","face = face \/ 255."],"delete":["55","face = face \/ 255"]}]}},"528b044a22f0afad4ebca2197589b62b869326c1":{"changes":{"doc\/whats_new\/v0.23.rst":"MODIFY","sklearn\/compose\/tests\/test_column_transformer.py":"MODIFY","sklearn\/compose\/_column_transformer.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.23.rst":[{"add":["59",":mod:`sklearn.compose`","60","......................","61","","62","- |Fix| :class:`compose.ColumnTransformer` method ``get_feature_names`` now","63","  returns correct results when one of the transformer steps applies on an","64","  empty list of columns :pr:`15963` by `Roman Yurchak`_.","65",""],"delete":[]}],"sklearn\/compose\/tests\/test_column_transformer.py":[{"add":["1268","","1269","","1270","@pytest.mark.parametrize(","1271","    'empty_col', [[], np.array([], dtype=np.int), lambda x: []],","1272","    ids=['list', 'array', 'callable']","1273",")","1274","def test_feature_names_empty_columns(empty_col):","1275","    pd = pytest.importorskip('pandas')","1276","","1277","    df = pd.DataFrame({\"col1\": [\"a\", \"a\", \"b\"], \"col2\": [\"z\", \"z\", \"z\"]})","1278","","1279","    ct = ColumnTransformer(","1280","        transformers=[","1281","            (\"ohe\", OneHotEncoder(), [\"col1\", \"col2\"]),","1282","            (\"empty_features\", OneHotEncoder(), empty_col),","1283","        ],","1284","    )","1285","","1286","    ct.fit(df)","1287","    assert ct.get_feature_names() == ['ohe__x0_a', 'ohe__x0_b', 'ohe__x1_z']"],"delete":[]}],"sklearn\/compose\/_column_transformer.py":[{"add":["352","        for name, trans, column, _ in self._iter(fitted=True):","353","            if trans == 'drop' or (","354","                    hasattr(column, '__len__') and not len(column)):"],"delete":["352","        for name, trans, _, _ in self._iter(fitted=True):","353","            if trans == 'drop':"]}]}},"5763284ef9e04ffc52cf759a59bdc6bb81107616":{"changes":{"sklearn\/neighbors\/tests\/test_neighbors.py":"MODIFY","sklearn\/neighbors\/base.py":"MODIFY"},"diff":{"sklearn\/neighbors\/tests\/test_neighbors.py":[{"add":["20","from sklearn.utils.testing import assert_raises_regex","111","def test_n_neighbors_datatype():","112","    # Test to check whether n_neighbors is integer","113","    X = [[1, 1], [1, 1], [1, 1]]","114","    expected_msg = \"n_neighbors does not take .*float.* \" \\","115","                   \"value, enter integer value\"","116","    msg = \"Expected n_neighbors > 0. Got -3\"","117","","118","    neighbors_ = neighbors.NearestNeighbors(n_neighbors=3.)","119","    assert_raises_regex(TypeError, expected_msg, neighbors_.fit, X)","120","    assert_raises_regex(ValueError, msg,","121","                        neighbors_.kneighbors, X=X, n_neighbors=-3)","122","    assert_raises_regex(TypeError, expected_msg,","123","                        neighbors_.kneighbors, X=X, n_neighbors=3.)","124","","125",""],"delete":[]}],"sklearn\/neighbors\/base.py":[{"add":["260","            else:","261","                if not np.issubdtype(type(self.n_neighbors), np.integer):","262","                    raise TypeError(","263","                        \"n_neighbors does not take %s value, \"","264","                        \"enter integer value\" %","265","                        type(self.n_neighbors))","335","        elif n_neighbors <= 0:","336","            raise ValueError(","337","                \"Expected n_neighbors > 0. Got %d\" %","338","                n_neighbors","339","            )","340","        else:","341","            if not np.issubdtype(type(n_neighbors), np.integer):","342","                raise TypeError(","343","                    \"n_neighbors does not take %s value, \"","344","                    \"enter integer value\" %","345","                    type(n_neighbors))"],"delete":[]}]}},"32b88d8e2dff3b2ebd93c8cdc7511e72a33d71bf":{"changes":{"sklearn\/linear_model\/tests\/test_least_angle.py":"MODIFY","sklearn\/linear_model\/least_angle.py":"MODIFY"},"diff":{"sklearn\/linear_model\/tests\/test_least_angle.py":[{"add":["0","import warnings","1","","9","from sklearn.utils.testing import assert_false","407","    assert_false(hasattr(lars_cv, 'n_nonzero_coefs'))","408","","409","","410","def test_lars_cv_max_iter():","411","    with warnings.catch_warnings(record=True) as w:","412","        X = diabetes.data","413","        y = diabetes.target","414","        rng = np.random.RandomState(42)","415","        x = rng.randn(len(y))","416","        X = np.c_[X, x, x]  # add correlated features","417","        lars_cv = linear_model.LassoLarsCV(max_iter=5)","418","        lars_cv.fit(X, y)","419","    assert_true(len(w) == 0)"],"delete":[]}],"sklearn\/linear_model\/least_angle.py":[{"add":["301","                              'the smallest cholesky pivot element being %.3e.'","302","                              ' Reduce max_iter or increase eps parameters.'","305","","373","        g1 = arrayfuncs.min_pos((C - Cov) \/ (AA - corr_eq_dir + tiny32))","377","            g2 = arrayfuncs.min_pos((C + Cov) \/ (AA + corr_eq_dir + tiny32))","611","    def _fit(self, X, y, max_iter, alpha, fit_path, Xy=None):","612","        \"\"\"Auxiliary method to fit the model using X, y as training data\"\"\"","638","        if fit_path:","674","","678","    def fit(self, X, y, Xy=None):","679","        \"\"\"Fit the model using X, y as training data.","680","","681","        Parameters","682","        ----------","683","        X : array-like, shape (n_samples, n_features)","684","            Training data.","685","","686","        y : array-like, shape (n_samples,) or (n_samples, n_targets)","687","            Target values.","688","","689","        Xy : array-like, shape (n_samples,) or (n_samples, n_targets), \\","690","                optional","691","            Xy = np.dot(X.T, y) that can be precomputed. It is useful","692","            only when the Gram matrix is precomputed.","693","","694","        Returns","695","        -------","696","        self : object","697","            returns an instance of self.","698","        \"\"\"","699","        X, y = check_X_y(X, y, y_numeric=True, multi_output=True)","700","","701","        alpha = getattr(self, 'alpha', 0.)","702","        if hasattr(self, 'n_nonzero_coefs'):","703","            alpha = 0.  # n_nonzero_coefs parametrization takes priority","704","            max_iter = self.n_nonzero_coefs","705","        else:","706","            max_iter = self.max_iter","707","","708","        self._fit(X, y, max_iter=max_iter, alpha=alpha, fit_path=self.fit_path,","709","                  Xy=Xy)","710","","711","        return self","712","","1157","        self._fit(X, y, max_iter=self.max_iter, alpha=best_alpha,","1158","                  Xy=None, fit_path=True)","1162","    @deprecated(\"Attribute alpha is deprecated in 0.19 and \"","1163","                \"will be removed in 0.21. See 'alpha_' instead\")","1298","    def __init__(self, fit_intercept=True, verbose=False, max_iter=500,","1299","                 normalize=True, precompute='auto', cv=None,","1300","                 max_n_alphas=1000, n_jobs=1, eps=np.finfo(np.float).eps,","1301","                 copy_X=True, positive=False):","1302","        self.fit_intercept = fit_intercept","1303","        self.verbose = verbose","1304","        self.max_iter = max_iter","1305","        self.normalize = normalize","1306","        self.precompute = precompute","1307","        self.cv = cv","1308","        self.max_n_alphas = max_n_alphas","1309","        self.n_jobs = n_jobs","1310","        self.eps = eps","1311","        self.copy_X = copy_X","1312","        self.positive = positive","1313","        # XXX : we don't use super(LarsCV, self).__init__","1314","        # to avoid setting n_nonzero_coefs","1315",""],"delete":["197","    tiny = np.finfo(np.float).tiny  # to avoid division by 0 warning","302","                              'the smallest cholesky pivot element being %.3e'","372","        g1 = arrayfuncs.min_pos((C - Cov) \/ (AA - corr_eq_dir + tiny))","376","            g2 = arrayfuncs.min_pos((C + Cov) \/ (AA + corr_eq_dir + tiny))","610","    def fit(self, X, y, Xy=None):","611","        \"\"\"Fit the model using X, y as training data.","612","","613","        parameters","614","        ----------","615","        X : array-like, shape (n_samples, n_features)","616","            Training data.","617","","618","        y : array-like, shape (n_samples,) or (n_samples, n_targets)","619","            Target values.","620","","621","        Xy : array-like, shape (n_samples,) or (n_samples, n_targets), \\","622","                optional","623","            Xy = np.dot(X.T, y) that can be precomputed. It is useful","624","            only when the Gram matrix is precomputed.","625","","626","        returns","627","        -------","628","        self : object","629","            returns an instance of self.","630","        \"\"\"","631","        X, y = check_X_y(X, y, y_numeric=True, multi_output=True)","644","        alpha = getattr(self, 'alpha', 0.)","645","        if hasattr(self, 'n_nonzero_coefs'):","646","            alpha = 0.  # n_nonzero_coefs parametrization takes priority","647","            max_iter = self.n_nonzero_coefs","648","        else:","649","            max_iter = self.max_iter","650","","664","        if self.fit_path:","1147","        Lars.fit(self, X, y)"]}]}},"a6028fc059982f04cb12256016702387e11964f5":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/preprocessing\/tests\/test_data.py":"MODIFY","sklearn\/preprocessing\/data.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["498","- Fix bug in :class:`preprocessing.OneHotEncoder` which discarded the ``dtype``","499","  when returning a sparse matrix output. :issue:`11042` by :user:`Daniel","500","  Morales <DanielMorales9>`.","501",""],"delete":[]}],"sklearn\/preprocessing\/tests\/test_data.py":[{"add":["1911","def _check_transform_selected(X, X_expected, dtype, sel):","1913","        Xtr = _transform_selected(M, Binarizer().transform, dtype, sel)","1917","@pytest.mark.parametrize(\"output_dtype\", [np.int32, np.float32, np.float64])","1918","@pytest.mark.parametrize(\"input_dtype\", [np.int32, np.float32, np.float64])","1919","def test_transform_selected(output_dtype, input_dtype):","1920","    X = np.asarray([[3, 2, 1], [0, 1, 1]], dtype=input_dtype)","1922","    X_expected = np.asarray([[1, 2, 1], [0, 1, 1]], dtype=output_dtype)","1923","    _check_transform_selected(X, X_expected, output_dtype, [0])","1924","    _check_transform_selected(X, X_expected, output_dtype,","1925","                              [True, False, False])","1927","    X_expected = np.asarray([[1, 1, 1], [0, 1, 1]], dtype=output_dtype)","1928","    _check_transform_selected(X, X_expected, output_dtype, [0, 1, 2])","1929","    _check_transform_selected(X, X_expected, output_dtype, [True, True, True])","1930","    _check_transform_selected(X, X_expected, output_dtype, \"all\")","1932","    _check_transform_selected(X, X, output_dtype, [])","1933","    _check_transform_selected(X, X, output_dtype, [False, False, False])","1936","@pytest.mark.parametrize(\"output_dtype\", [np.int32, np.float32, np.float64])","1937","@pytest.mark.parametrize(\"input_dtype\", [np.int32, np.float32, np.float64])","1938","def test_transform_selected_copy_arg(output_dtype, input_dtype):","1944","    original_X = np.asarray([[1, 2], [3, 4]], dtype=input_dtype)","1945","    expected_Xtr = np.asarray([[2, 2], [3, 4]], dtype=output_dtype)","1948","    Xtr = _transform_selected(X, _mutating_transformer, output_dtype,","1949","                              copy=True, selected='all')","1994","@pytest.mark.parametrize(\"output_dtype\", [np.int32, np.float32, np.float64])","1995","@pytest.mark.parametrize(\"input_dtype\",  [np.int32, np.float32, np.float64])","1996","@pytest.mark.parametrize(\"sparse\", [True, False])","1997","def test_one_hot_encoder_preserve_type(input_dtype, output_dtype, sparse):","1998","    X = np.array([[0, 1, 0, 0], [1, 2, 0, 0]], dtype=input_dtype)","1999","    transformer = OneHotEncoder(categorical_features=[0, 1],","2000","                                dtype=output_dtype, sparse=sparse)","2001","    X_trans = transformer.fit_transform(X)","2002","    assert X_trans.dtype == output_dtype","2003","","2004",""],"delete":["1911","def _check_transform_selected(X, X_expected, sel):","1913","        Xtr = _transform_selected(M, Binarizer().transform, sel)","1917","def test_transform_selected():","1918","    X = [[3, 2, 1], [0, 1, 1]]","1920","    X_expected = [[1, 2, 1], [0, 1, 1]]","1921","    _check_transform_selected(X, X_expected, [0])","1922","    _check_transform_selected(X, X_expected, [True, False, False])","1924","    X_expected = [[1, 1, 1], [0, 1, 1]]","1925","    _check_transform_selected(X, X_expected, [0, 1, 2])","1926","    _check_transform_selected(X, X_expected, [True, True, True])","1927","    _check_transform_selected(X, X_expected, \"all\")","1929","    _check_transform_selected(X, X, [])","1930","    _check_transform_selected(X, X, [False, False, False])","1933","def test_transform_selected_copy_arg():","1939","    original_X = np.asarray([[1, 2], [3, 4]])","1940","    expected_Xtr = [[2, 2], [3, 4]]","1943","    Xtr = _transform_selected(X, _mutating_transformer, copy=True,","1944","                              selected='all')"]}],"sklearn\/preprocessing\/data.py":[{"add":["1827","def _transform_selected(X, transform, dtype, selected=\"all\", copy=True):","1838","    dtype : number type","1839","        Desired dtype of output.","1840","","1874","        # The columns of X which are not transformed need","1875","        # to be casted to the desire dtype before concatenation.","1876","        # Otherwise, the stacking will cast to the higher-precision dtype.","1877","        X_not_sel = X[:, ind[not_sel]].astype(dtype)","2069","        return _transform_selected(X, self._fit_transform, self.dtype,","2125","        return _transform_selected(X, self._transform, self.dtype,"],"delete":["1827","def _transform_selected(X, transform, selected=\"all\", copy=True):","1871","        X_not_sel = X[:, ind[not_sel]]","2063","        return _transform_selected(X, self._fit_transform,","2119","        return _transform_selected(X, self._transform,"]}]}},"ad281b8791f451a622e1bfb568e02d07c6e2b1cb":{"changes":{"sklearn\/neighbors\/classification.py":"MODIFY"},"diff":{"sklearn\/neighbors\/classification.py":[{"add":["368","            pred_labels = np.zeros(len(neigh_ind), dtype=object)","369","            pred_labels[:] = [_y[ind, k] for ind in neigh_ind]"],"delete":["368","            pred_labels = np.array([_y[ind, k] for ind in neigh_ind],","369","                                   dtype=object)"]}]}},"93563b0ac1d96403eff9988dfcb680d42c6304aa":{"changes":{"sklearn\/model_selection\/_split.py":"MODIFY","sklearn\/model_selection\/tests\/test_split.py":"MODIFY"},"diff":{"sklearn\/model_selection\/_split.py":[{"add":["1480","","1481","        if y.ndim == 2:","1482","            # for multi-label y, map each distinct row to its string repr:","1483","            y = np.array([str(row) for row in y])","1484",""],"delete":[]}],"sklearn\/model_selection\/tests\/test_split.py":[{"add":["665","    train, test = next(sss.split(X=X, y=y))","667","    # no overlap","670","    # complete partition","671","    assert_array_equal(np.union1d(train, test), np.arange(len(y)))","672","","673","","674","def test_stratified_shuffle_split_multilabel():","675","    # fix for issue 9037","676","    for y in [np.array([[0, 1], [1, 0], [1, 0], [0, 1]]),","677","              np.array([[0, 1], [1, 1], [1, 1], [0, 1]])]:","678","        X = np.ones_like(y)","679","        sss = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=0)","680","        train, test = next(sss.split(X=X, y=y))","681","        y_train = y[train]","682","        y_test = y[test]","683","","684","        # no overlap","685","        assert_array_equal(np.intersect1d(train, test), [])","686","","687","        # complete partition","688","        assert_array_equal(np.union1d(train, test), np.arange(len(y)))","689","","690","        # correct stratification of entire rows","691","        # (by design, here y[:, 0] uniquely determines the entire row of y)","692","        expected_ratio = np.mean(y[:, 0])","693","        assert_equal(expected_ratio, np.mean(y_train[:, 0]))","694","        assert_equal(expected_ratio, np.mean(y_test[:, 0]))","695",""],"delete":["665","    train, test = next(iter(sss.split(X=X, y=y)))"]}]}},"6451a097889eea6df43cc11cbfed59b0106e6171":{"changes":{"sklearn\/neural_network\/tests\/test_mlp.py":"MODIFY"},"diff":{"sklearn\/neural_network\/tests\/test_mlp.py":[{"add":["566","@ignore_warnings(category=RuntimeWarning)"],"delete":["566","@ignore_warnings(RuntimeError)"]}]}},"bdb00d17b9b82601c2c6865c2df6de1f5ce357bb":{"changes":{"sklearn\/covariance\/tests\/test_covariance.py":"MODIFY","sklearn\/utils\/extmath.py":"MODIFY"},"diff":{"sklearn\/covariance\/tests\/test_covariance.py":[{"add":["123","    lw_cov_from_mle, lw_shrinkage_from_mle = ledoit_wolf(X_centered,","126","    assert_almost_equal(lw_shrinkage_from_mle, lw.shrinkage_)","136","    lw_cov_from_mle, lw_shrinkage_from_mle = ledoit_wolf(X_1d,","139","    assert_almost_equal(lw_shrinkage_from_mle, lw.shrinkage_)","157","    lw_cov_from_mle, lw_shrinkage_from_mle = ledoit_wolf(X)","159","    assert_almost_equal(lw_shrinkage_from_mle, lw.shrinkage_)","169","    lw_cov_from_mle, lw_shrinkage_from_mle = ledoit_wolf(X_1d)","171","    assert_almost_equal(lw_shrinkage_from_mle, lw.shrinkage_)","245","    oa_cov_from_mle, oa_shrinkage_from_mle = oas(X_centered,","248","    assert_almost_equal(oa_shrinkage_from_mle, oa.shrinkage_)","258","    oa_cov_from_mle, oa_shrinkage_from_mle = oas(X_1d, assume_centered=True)","260","    assert_almost_equal(oa_shrinkage_from_mle, oa.shrinkage_)","276","    oa_cov_from_mle, oa_shrinkage_from_mle = oas(X)","278","    assert_almost_equal(oa_shrinkage_from_mle, oa.shrinkage_)","288","    oa_cov_from_mle, oa_shrinkage_from_mle = oas(X_1d)","290","    assert_almost_equal(oa_shrinkage_from_mle, oa.shrinkage_)"],"delete":["123","    lw_cov_from_mle, lw_shinkrage_from_mle = ledoit_wolf(X_centered,","126","    assert_almost_equal(lw_shinkrage_from_mle, lw.shrinkage_)","136","    lw_cov_from_mle, lw_shinkrage_from_mle = ledoit_wolf(X_1d,","139","    assert_almost_equal(lw_shinkrage_from_mle, lw.shrinkage_)","157","    lw_cov_from_mle, lw_shinkrage_from_mle = ledoit_wolf(X)","159","    assert_almost_equal(lw_shinkrage_from_mle, lw.shrinkage_)","169","    lw_cov_from_mle, lw_shinkrage_from_mle = ledoit_wolf(X_1d)","171","    assert_almost_equal(lw_shinkrage_from_mle, lw.shrinkage_)","245","    oa_cov_from_mle, oa_shinkrage_from_mle = oas(X_centered,","248","    assert_almost_equal(oa_shinkrage_from_mle, oa.shrinkage_)","258","    oa_cov_from_mle, oa_shinkrage_from_mle = oas(X_1d, assume_centered=True)","260","    assert_almost_equal(oa_shinkrage_from_mle, oa.shrinkage_)","276","    oa_cov_from_mle, oa_shinkrage_from_mle = oas(X)","278","    assert_almost_equal(oa_shinkrage_from_mle, oa.shrinkage_)","288","    oa_cov_from_mle, oa_shinkrage_from_mle = oas(X_1d)","290","    assert_almost_equal(oa_shinkrage_from_mle, oa.shrinkage_)"]}],"sklearn\/utils\/extmath.py":[{"add":["365","        # Checks if the number of iterations is explicitly specified"],"delete":["365","        # Checks if the number of iterations is explicitely specified"]}]}},"00f3910678d2b64ddc5be81255a093b97396c580":{"changes":{"doc\/conf.py":"MODIFY","doc\/templates\/index.html":"MODIFY","doc\/themes\/scikit-learn-modern\/layout.html":"MODIFY"},"diff":{"doc\/conf.py":[{"add":["90","version = \".\".join(version.split(\".\")[:2])"],"delete":[]}],"doc\/templates\/index.html":[{"add":["10","        <a class=\"btn sk-landing-btn mb-1\" href=\"whats_new\/v{{ version }}.html\" role=\"button\">What's New in {{ release }}<\/a>","158","        <li><strong>January 2020.<\/strong> scikit-learn 0.22.1 is available for download (<a href=\"whats_new\/v0.22.html#version-0-22-1\">Changelog<\/a>)."],"delete":["10","        <a class=\"btn sk-landing-btn mb-1\" href=\"whats_new\/v{{ version }}.html\" role=\"button\">What's New in {{ version }}<\/a>"]}],"doc\/themes\/scikit-learn-modern\/layout.html":[{"add":["79","          <strong>scikit-learn {{ release }}<\/strong><br\/>"],"delete":["79","          <strong>scikit-learn {{ version }}<\/strong><br\/>"]}]}}}