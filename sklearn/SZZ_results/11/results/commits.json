{"60f887e65c7140f29ff9ebe73a0945884d1d685d":{"changes":{"sklearn\/ensemble\/tests\/test_iforest.py":"MODIFY"},"diff":{"sklearn\/ensemble\/tests\/test_iforest.py":[{"add":["8","import pytest","9","","108","    # note that assert_no_warnings does not apply since it enables a","109","    # PendingDeprecationWarning triggered by scipy.sparse's use of","110","    # np.matrix. See issue #11251.","111","    with pytest.warns(None) as record:","112","        IsolationForest(max_samples='auto').fit(X)","113","    user_warnings = [each for each in record","114","                     if issubclass(each.category, UserWarning)]","115","    assert len(user_warnings) == 0","116","    with pytest.warns(None) as record:","117","        IsolationForest(max_samples=np.int64(2)).fit(X)","118","    user_warnings = [each for each in record","119","                     if issubclass(each.category, UserWarning)]","120","    assert len(user_warnings) == 0","121",""],"delete":["17","from sklearn.utils.testing import assert_no_warnings","107","    assert_no_warnings(IsolationForest(max_samples='auto').fit, X)","108","    assert_no_warnings(IsolationForest(max_samples=np.int64(2)).fit, X)"]}]}},"2f5bb34b6d6093efe0ab1c10857ef07fd8aa503b":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/decomposition\/tests\/test_kernel_pca.py":"MODIFY","sklearn\/decomposition\/kernel_pca.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["95","- |Fix| Fixed a bug in :class:`decomposition.KernelPCA`, `fit().transform()`","96","  now produces the correct output (the same as `fit_transform()`) in case","97","  of non-removed zero eigenvalues (`remove_zero_eig=False`).","98","  `fit_inverse_transform` was also accelerated by using the same trick as","99","  `fit_transform` to compute the transform of `X`.","100","  :issue:`12143` by :user:`Sylvain Mari¨¦ <smarie>`","101",""],"delete":[]}],"sklearn\/decomposition\/tests\/test_kernel_pca.py":[{"add":["157","def test_leave_zero_eig():","158","    \"\"\"This test checks that fit().transform() returns the same result as","159","    fit_transform() in case of non-removed zero eigenvalue.","160","    Non-regression test for issue #12141 (PR #12143)\"\"\"","161","    X_fit = np.array([[1, 1], [0, 0]])","162","","163","    # Assert that even with all np warnings on, there is no div by zero warning","164","    with pytest.warns(None) as record:","165","        with np.errstate(all='warn'):","166","            k = KernelPCA(n_components=2, remove_zero_eig=False,","167","                          eigen_solver=\"dense\")","168","            # Fit, then transform","169","            A = k.fit(X_fit).transform(X_fit)","170","            # Do both at once","171","            B = k.fit_transform(X_fit)","172","            # Compare","173","            assert_array_almost_equal(np.abs(A), np.abs(B))","174","","175","    for w in record:","176","        # There might be warnings about the kernel being badly conditioned,","177","        # but there should not be warnings about division by zero.","178","        # (Numpy division by zero warning can have many message variants, but","179","        # at least we know that it is a RuntimeWarning so lets check only this)","180","        assert not issubclass(w.category, RuntimeWarning)","181","","182",""],"delete":[]}],"sklearn\/decomposition\/kernel_pca.py":[{"add":["222","        # remove eigenvectors with a zero eigenvalue (null space) if required","227","        # Maintenance note on Eigenvectors normalization","228","        # ----------------------------------------------","229","        # there is a link between","230","        # the eigenvectors of K=Phi(X)'Phi(X) and the ones of Phi(X)Phi(X)'","231","        # if v is an eigenvector of K","232","        #                      then Phi(X)v  is an eigenvector of Phi(X)Phi(X)'","233","        # if u is an eigenvector of Phi(X)Phi(X)'","234","        #                      then Phi(X)'u is an eigenvector of Phi(X)Phi(X)'","235","        #","236","        # At this stage our self.alphas_ (the v) have norm 1, we need to scale","237","        # them so that eigenvectors in kernel feature space (the u) have norm=1","238","        # instead","239","        #","240","        # We COULD scale them here:","241","        #       self.alphas_ = self.alphas_ \/ np.sqrt(self.lambdas_)","242","        #","243","        # But choose to perform that LATER when needed, in `fit()` and in","244","        # `transform()`.","245","","279","            # no need to use the kernel to transform X, use shortcut expression","280","            X_transformed = self.alphas_ * np.sqrt(self.lambdas_)","281","","302","        # no need to use the kernel to transform X, use shortcut expression","323","        # Compute centered gram matrix between X and training data X_fit_","325","","326","        # scale eigenvectors (properly account for null-space for dot product)","327","        non_zeros = np.flatnonzero(self.lambdas_)","328","        scaled_alphas = np.zeros_like(self.alphas_)","329","        scaled_alphas[:, non_zeros] = (self.alphas_[:, non_zeros]","330","                                       \/ np.sqrt(self.lambdas_[non_zeros]))","331","","332","        # Project with a scalar product between K and the scaled eigenvectors","333","        return np.dot(K, scaled_alphas)"],"delete":["222","        # remove eigenvectors with a zero eigenvalue","260","            sqrt_lambdas = np.diag(np.sqrt(self.lambdas_))","261","            X_transformed = np.dot(self.alphas_, sqrt_lambdas)","303","        return np.dot(K, self.alphas_ \/ np.sqrt(self.lambdas_))"]}]}},"680ab517dfec4c60857f503f50b5faf512d37c56":{"changes":{"sklearn\/base.py":"MODIFY","sklearn\/tests\/test_base.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/utils\/mocking.py":"MODIFY"},"diff":{"sklearn\/base.py":[{"add":["75","        if param1 is param2:","76","            # this should always happen","77","            continue","114","            # fall back on standard equality","115","            equality_test = param1 == param2","116","        if equality_test:","117","            warnings.warn(\"Estimator %s modifies parameters in __init__.\"","118","                          \" This behavior is deprecated as of 0.18 and \"","119","                          \"support for this behavior will be removed in 0.20.\"","120","                          % type(estimator).__name__, DeprecationWarning)","121","        else:"],"delete":["111","            new_obj_val = new_object_params[name]","112","            params_set_val = params_set[name]","113","            # The following construct is required to check equality on special","114","            # singletons such as np.nan that are not equal to them-selves:","115","            equality_test = (new_obj_val == params_set_val or","116","                             new_obj_val is params_set_val)","117","        if not equality_test:"]}],"sklearn\/tests\/test_base.py":[{"add":["14","from sklearn.utils.testing import assert_warns_message","22","from sklearn.base import TransformerMixin","23","from sklearn.utils.mocking import MockDataFrame","24","","47","class ModifyInitParams(BaseEstimator):","48","    \"\"\"Deprecated behavior.","49","    Equal parameters but with a type cast.","50","    Doesn't fulfill a is a","51","    \"\"\"","52","    def __init__(self, a=np.array([0])):","53","        self.a = a.copy()","54","","55","","160","def test_clone_copy_init_params():","161","    # test for deprecation warning when copying or casting an init parameter","162","    est = ModifyInitParams()","163","    message = (\"Estimator ModifyInitParams modifies parameters in __init__. \"","164","               \"This behavior is deprecated as of 0.18 and support \"","165","               \"for this behavior will be removed in 0.20.\")","166","","167","    assert_warns_message(DeprecationWarning, message, clone, est)","168","","169","","277","","278","","279","def test_clone_pandas_dataframe():","280","","281","    class DummyEstimator(BaseEstimator, TransformerMixin):","282","        \"\"\"This is a dummy class for generating numerical features","283","","284","        This feature extractor extracts numerical features from pandas data","285","        frame.","286","","287","        Parameters","288","        ----------","289","","290","        df: pandas data frame","291","            The pandas data frame parameter.","292","","293","        Notes","294","        -----","295","        \"\"\"","296","        def __init__(self, df=None, scalar_param=1):","297","            self.df = df","298","            self.scalar_param = scalar_param","299","","300","        def fit(self, X, y=None):","301","            pass","302","","303","        def transform(self, X, y=None):","304","            pass","305","","306","    # build and clone estimator","307","    d = np.arange(10)","308","    df = MockDataFrame(d)","309","    e = DummyEstimator(df, scalar_param=1)","310","    cloned_e = clone(e)","311","","312","    # the test","313","    assert_true((e.df == cloned_e.df).values.all())","314","    assert_equal(e.scalar_param, cloned_e.scalar_param)"],"delete":[]}],"doc\/whats_new.rst":[{"add":["295","   - Simplification of the ``clone`` function, deprecate support for estimators","296","     that modify parameters in ``__init__``.","297","     (`#5540 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/5540>_`)","298","     By `Andreas M¨¹ller`_.","299",""],"delete":[]}],"sklearn\/utils\/mocking.py":[{"add":["20","        self.values = array","35","    def __eq__(self, other):","36","        return MockDataFrame(self.array == other.array)","37",""],"delete":[]}]}},"58228cb6d35b069c701a5baf1e25510f76fd5ef3":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/compose\/tests\/test_column_transformer.py":"MODIFY","sklearn\/compose\/_column_transformer.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["29",":mod:`sklearn.compose`","30","......................","31","","32","- |Fix| Fixed an issue in :class:`compose.ColumnTransformer` when stacking","33","  columns with types not convertible to a numeric.","34","  :issue:`11912` by :user:`Adrin Jalali <adrinjalali>`.","35",""],"delete":[]}],"sklearn\/compose\/tests\/test_column_transformer.py":[{"add":["370","def test_column_transformer_mixed_cols_sparse():","371","    df = np.array([['a', 1, True],","372","                   ['b', 2, False]],","373","                  dtype='O')","374","","375","    ct = make_column_transformer(","376","        ([0], OneHotEncoder()),","377","        ([1, 2], 'passthrough'),","378","        sparse_threshold=1.0","379","    )","380","","381","    # this shouldn't fail, since boolean can be coerced into a numeric","382","    # See: https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/11912","383","    X_trans = ct.fit_transform(df)","384","    assert X_trans.getformat() == 'csr'","385","    assert_array_equal(X_trans.toarray(), np.array([[1, 0, 1, 1],","386","                                                    [0, 1, 2, 0]]))","387","","388","    ct = make_column_transformer(","389","        ([0], OneHotEncoder()),","390","        ([0], 'passthrough'),","391","        sparse_threshold=1.0","392","    )","393","    with pytest.raises(ValueError,","394","                       match=\"For a sparse output, all columns should\"):","395","        # this fails since strings `a` and `b` cannot be","396","        # coerced into a numeric.","397","        ct.fit_transform(df)","398","","399",""],"delete":[]}],"sklearn\/compose\/_column_transformer.py":[{"add":["514","            try:","515","                # since all columns should be numeric before stacking them","516","                # in a sparse matrix, `check_array` is used for the","517","                # dtype conversion if necessary.","518","                converted_Xs = [check_array(X,","519","                                            accept_sparse=True,","520","                                            force_all_finite=False)","521","                                for X in Xs]","522","            except ValueError:","523","                raise ValueError(\"For a sparse output, all columns should\"","524","                                 \" be a numeric or convertible to a numeric.\")","525","","526","            return sparse.hstack(converted_Xs).tocsr()"],"delete":["514","            return sparse.hstack(Xs).tocsr()"]}]}},"911137f0de9499a140e4ba1f4746de2770b8a53e":{"changes":{"sklearn\/feature_selection\/_rfe.py":"MODIFY","sklearn\/feature_selection\/tests\/test_rfe.py":"MODIFY","doc\/whats_new\/v0.24.rst":"MODIFY"},"diff":{"sklearn\/feature_selection\/_rfe.py":[{"add":["9","import numbers","60","    n_features_to_select : int or float, default=None","61","        The number of features to select. If `None`, half of the features are","62","        selected. If integer, the parameter is the absolute number of features","63","        to select. If float between 0 and 1, it is the fraction of features to","64","        select.","186","        error_msg = (\"n_features_to_select must be either None, a \"","187","                     \"positive integer representing the absolute \"","188","                     \"number of features or a float in (0.0, 1.0] \"","189","                     \"representing a percentage of features to \"","190","                     f\"select. Got {self.n_features_to_select}\")","191","","196","        elif self.n_features_to_select < 0:","197","            raise ValueError(error_msg)","198","        elif isinstance(self.n_features_to_select, numbers.Integral):  # int","200","        elif self.n_features_to_select > 1.0:  # float > 1","201","            raise ValueError(error_msg)","202","        else:  # float","203","            n_features_to_select = int(n_features * self.n_features_to_select)"],"delete":["59","    n_features_to_select : int, default=None","60","        The number of features to select. If `None`, half of the features","61","        are selected.","187","        else:"]}],"sklearn\/feature_selection\/tests\/test_rfe.py":[{"add":["111","@pytest.mark.parametrize(\"n_features_to_select\", [-1, 2.1])","112","def test_rfe_invalid_n_features_errors(n_features_to_select):","113","    clf = SVC(kernel=\"linear\")","114","","115","    iris = load_iris()","116","    rfe = RFE(estimator=clf, n_features_to_select=n_features_to_select,","117","              step=0.1)","118","    msg = f\"n_features_to_select must be .+ Got {n_features_to_select}\"","119","    with pytest.raises(ValueError, match=msg):","120","        rfe.fit(iris.data, iris.target)","121","","122","","123","def test_rfe_percent_n_features():","124","    # test that the results are the same","125","    generator = check_random_state(0)","126","    iris = load_iris()","127","    X = np.c_[iris.data, generator.normal(size=(len(iris.data), 6))]","128","    y = iris.target","129","    # there are 10 features in the data. We select 40%.","130","    clf = SVC(kernel=\"linear\")","131","    rfe_num = RFE(estimator=clf, n_features_to_select=4, step=0.1)","132","    rfe_num.fit(X, y)","133","","134","    rfe_perc = RFE(estimator=clf, n_features_to_select=0.4, step=0.1)","135","    rfe_perc.fit(X, y)","136","","137","    assert_array_equal(rfe_perc.ranking_, rfe_num.ranking_)","138","    assert_array_equal(rfe_perc.support_, rfe_num.support_)","139","","140",""],"delete":[]}],"doc\/whats_new\/v0.24.rst":[{"add":["96","- |Enhancement| Added the option for the number of n_features_to_select to be","97","  given as a float representing the percentage of features to select.","98","  :pr:`17090` by :user:`Lisa Schwetlick <lschwetlick>` and","99","  :user:`Marija Vlajic Wheeler <marijavlajic>`.","100","","252",""],"delete":[]}]}},"28b1e00665d0fd1590186f0f58734d8e9ace4468":{"changes":{"sklearn\/linear_model\/_coordinate_descent.py":"MODIFY"},"diff":{"sklearn\/linear_model\/_coordinate_descent.py":[{"add":["400","        If set to False, the input validation checks are skipped (including the","401","        Gram matrix when provided). It is assumed that they are handled","402","        by the caller."],"delete":["400","        Skip input validation checks, including the Gram matrix when provided","401","        assuming there are handled by the caller when check_input=False."]}]}},"d6d1d63fa6b098c72953a6827aae475f611936ed":{"changes":{"sklearn\/svm\/tests\/test_svm.py":"MODIFY","sklearn\/svm\/src\/libsvm\/svm.cpp":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY"},"diff":{"sklearn\/svm\/tests\/test_svm.py":[{"add":["424","@pytest.mark.parametrize(\"estimator\", [svm.SVC(C=1e-2), svm.NuSVC()])","425","def test_svm_classifier_sided_sample_weight(estimator):","426","    # fit a linear SVM and check that giving more weight to opposed samples","427","    # in the space will flip the decision toward these samples.","428","    X = [[-2, 0], [-1, -1], [0, -2], [0, 2], [1, 1], [2, 0]]","429","    estimator.set_params(kernel='linear')","431","    # check that with unit weights, a sample is supposed to be predicted on","432","    # the boundary","433","    sample_weight = [1] * 6","434","    estimator.fit(X, Y, sample_weight=sample_weight)","435","    y_pred = estimator.decision_function([[-1., 1.]])","436","    assert y_pred == pytest.approx(0)","438","    # give more weights to opposed samples","439","    sample_weight = [10., .1, .1, .1, .1, 10]","440","    estimator.fit(X, Y, sample_weight=sample_weight)","441","    y_pred = estimator.decision_function([[-1., 1.]])","442","    assert y_pred < 0","443","","444","    sample_weight = [1., .1, 10., 10., .1, .1]","445","    estimator.fit(X, Y, sample_weight=sample_weight)","446","    y_pred = estimator.decision_function([[-1., 1.]])","447","    assert y_pred > 0","448","","449","","450","@pytest.mark.parametrize(","451","    \"estimator\",","452","    [svm.SVR(C=1e-2), svm.NuSVR(C=1e-2)]","453",")","454","def test_svm_regressor_sided_sample_weight(estimator):","455","    # similar test to test_svm_classifier_sided_sample_weight but for","456","    # SVM regressors","457","    X = [[-2, 0], [-1, -1], [0, -2], [0, 2], [1, 1], [2, 0]]","458","    estimator.set_params(kernel='linear')","459","","460","    # check that with unit weights, a sample is supposed to be predicted on","461","    # the boundary","462","    sample_weight = [1] * 6","463","    estimator.fit(X, Y, sample_weight=sample_weight)","464","    y_pred = estimator.predict([[-1., 1.]])","465","    assert y_pred == pytest.approx(1.5)","466","","467","    # give more weights to opposed samples","468","    sample_weight = [10., .1, .1, .1, .1, 10]","469","    estimator.fit(X, Y, sample_weight=sample_weight)","470","    y_pred = estimator.predict([[-1., 1.]])","471","    assert y_pred < 1.5","472","","473","    sample_weight = [1., .1, 10., 10., .1, .1]","474","    estimator.fit(X, Y, sample_weight=sample_weight)","475","    y_pred = estimator.predict([[-1., 1.]])","476","    assert y_pred > 1.5","477","","478","","479","def test_svm_equivalence_sample_weight_C():","486","    assert_allclose(dual_coef_no_weight, clf.dual_coef_)","487","","488","","489","@pytest.mark.parametrize(","490","    \"Estimator, err_msg\",","491","    [(svm.SVC,","492","      'Invalid input - all samples have zero or negative weights.'),","493","     (svm.NuSVC, '(negative dimensions are not allowed|nu is infeasible)'),","494","     (svm.SVR,","495","      'Invalid input - all samples have zero or negative weights.'),","496","     (svm.NuSVR,","497","      'Invalid input - all samples have zero or negative weights.'),","498","     (svm.OneClassSVM,","499","      'Invalid input - all samples have zero or negative weights.')","500","     ],","501","    ids=['SVC', 'NuSVC', 'SVR', 'NuSVR', 'OneClassSVM']","502",")","503","@pytest.mark.parametrize(","504","    \"sample_weight\",","505","    [[0] * len(Y), [-0.3] * len(Y)],","506","    ids=['weights-are-zero', 'weights-are-negative']","507",")","508","def test_negative_sample_weights_mask_all_samples(Estimator,","509","                                                  err_msg, sample_weight):","510","    est = Estimator(kernel='linear')","511","    with pytest.raises(ValueError, match=err_msg):","512","        est.fit(X, Y, sample_weight=sample_weight)","513","","514","","515","@pytest.mark.parametrize(","516","    \"Classifier, err_msg\",","517","    [(svm.SVC,","518","     'Invalid input - all samples with positive weights have the same label'),","519","     (svm.NuSVC, 'specified nu is infeasible')],","520","    ids=['SVC', 'NuSVC']","521",")","522","@pytest.mark.parametrize(","523","    \"sample_weight\",","524","    [[0, -0.5, 0, 1, 1, 1],","525","     [1, 1, 1, 0, -0.1, -0.3]],","526","    ids=['mask-label-1', 'mask-label-2']","527",")","528","def test_negative_weights_svc_leave_just_one_label(Classifier,","529","                                                   err_msg,","530","                                                   sample_weight):","531","    clf = Classifier(kernel='linear')","532","    with pytest.raises(ValueError, match=err_msg):","533","        clf.fit(X, Y, sample_weight=sample_weight)","534","","535","","536","@pytest.mark.parametrize(","537","    \"Classifier, model\",","538","    [(svm.SVC, {'when-left': [0.3998,  0.4], 'when-right': [0.4,  0.3999]}),","539","     (svm.NuSVC, {'when-left': [0.3333,  0.3333],","540","      'when-right': [0.3333, 0.3333]})],","541","    ids=['SVC', 'NuSVC']","542",")","543","@pytest.mark.parametrize(","544","    \"sample_weight, mask_side\",","545","    [([1, -0.5, 1, 1, 1, 1], 'when-left'),","546","     ([1, 1, 1, 0, 1, 1], 'when-right')],","547","    ids=['partial-mask-label-1', 'partial-mask-label-2']","548",")","549","def test_negative_weights_svc_leave_two_labels(Classifier, model,","550","                                               sample_weight, mask_side):","551","    clf = Classifier(kernel='linear')","552","    clf.fit(X, Y, sample_weight=sample_weight)","553","    assert_allclose(clf.coef_, [model[mask_side]], rtol=1e-3)","554","","555","","556","@pytest.mark.parametrize(","557","    \"Estimator\",","558","    [svm.SVC, svm.NuSVC, svm.NuSVR],","559","    ids=['SVC', 'NuSVC', 'NuSVR']","560",")","561","@pytest.mark.parametrize(","562","    \"sample_weight\",","563","    [[1, -0.5, 1, 1, 1, 1], [1, 1, 1, 0, 1, 1]],","564","    ids=['partial-mask-label-1', 'partial-mask-label-2']","565",")","566","def test_negative_weight_equal_coeffs(Estimator, sample_weight):","567","    # model generates equal coefficients","568","    est = Estimator(kernel='linear')","569","    est.fit(X, Y, sample_weight=sample_weight)","570","    coef = np.abs(est.coef_).ravel()","571","    assert coef[0] == pytest.approx(coef[1], rel=1e-3)"],"delete":["424","def test_sample_weights():","425","    # Test weights on individual samples","426","    # TODO: check on NuSVR, OneClass, etc.","427","    clf = svm.SVC()","428","    clf.fit(X, Y)","429","    assert_array_equal(clf.predict([X[2]]), [1.])","431","    sample_weight = [.1] * 3 + [10] * 3","432","    clf.fit(X, Y, sample_weight=sample_weight)","433","    assert_array_equal(clf.predict([X[2]]), [2.])","441","    assert_array_almost_equal(dual_coef_no_weight, clf.dual_coef_)"]}],"sklearn\/svm\/src\/libsvm\/svm.cpp":[{"add":["3103","\tif(svm_type == C_SVC ||","3104","\t   svm_type == EPSILON_SVR ||","3105","\t   svm_type == NU_SVR ||","3106","\t   svm_type == ONE_CLASS)","3107","\t{","3108","\t\tPREFIX(problem) newprob;","3109","\t\t\/\/ filter samples with negative and null weights ","3110","\t\tremove_zero_weight(&newprob, prob);","3111","","3112","\t\tchar* msg = NULL;","3113","\t\t\/\/ all samples were removed","3114","\t\tif(newprob.l == 0)","3115","\t\t\tmsg =  \"Invalid input - all samples have zero or negative weights.\";","3116","\t\telse if(prob->l != newprob.l && ","3117","\t\t        svm_type == C_SVC)","3118","\t\t{","3119","\t\t\tbool only_one_label = true;","3120","\t\t\tint first_label = newprob.y[0];","3121","\t\t\tfor(int i=1;i<newprob.l;i++)","3122","\t\t\t{","3123","\t\t\t\tif(newprob.y[i] != first_label)","3124","\t\t\t\t{","3125","\t\t\t\t\tonly_one_label = false;","3126","\t\t\t\t\tbreak;","3127","\t\t\t\t}","3128","\t\t\t}","3129","\t\t\tif(only_one_label == true)","3130","\t\t\t\tmsg = \"Invalid input - all samples with positive weights have the same label.\";","3131","\t\t}","3132","","3133","\t\tfree(newprob.x);","3134","\t\tfree(newprob.y);","3135","\t\tfree(newprob.W);","3136","\t\tif(msg != NULL)","3137","\t\t\treturn msg;","3138","\t}"],"delete":[]}],"doc\/whats_new\/v0.22.rst":[{"add":["518","- |Fix| :class:`svm.SVC`, :class:`svm.SVR`, :class:`svm.NuSVR` and","519","  :class:`svm.OneClassSVM` when received values negative or zero","520","  for parameter ``sample_weight`` in method fit(), generated an","521","  invalid model. This behavior occured only in some border scenarios.","522","  Now in these cases, fit() will fail with an Exception.","523","  :pr:`14286` by :user:`Alex Shacked <alexshacked>`.","524",""],"delete":[]}]}},"c71a1c21d14fc7a98493acb1f3d315db720ca4ac":{"changes":{"doc\/whats_new\/v0.23.rst":"MODIFY"},"diff":{"doc\/whats_new\/v0.23.rst":[{"add":["447","- |Fix| :func:`model_selection.cross_val_predict` supports","448","  `method=\"predict_proba\"` when `y=None`.:pr:`15918` by","449","  :user:`Luca Kubin <lkubin>`."],"delete":["447","- |Fix| :func: `cross_val_predict` supports `method=\"predict_proba\"`","448","  when `y=None`.","449","  :pr:`15918` by :user:`Luca Kubin <lkubin>`."]}]}},"f15ebb953b7b8971093962514187f447df4c716c":{"changes":{"sklearn\/compose\/tests\/test_column_transformer.py":"MODIFY","sklearn\/compose\/_column_transformer.py":"MODIFY"},"diff":{"sklearn\/compose\/tests\/test_column_transformer.py":[{"add":["20","from sklearn.exceptions import NotFittedError, DataConversionWarning","280","def test_column_transformer_list():","281","    X_list = [","282","        [1, float('nan'), 'a'],","283","        [0, 0, 'b']","284","    ]","285","    expected_result = np.array([","286","        [1, float('nan'), 1, 0],","287","        [-1, 0, 0, 1],","288","    ])","289","","290","    ct = ColumnTransformer([","291","        ('numerical', StandardScaler(), [0, 1]),","292","        ('categorical', OneHotEncoder(), [2]),","293","    ])","294","","295","    with pytest.warns(DataConversionWarning):","296","        # TODO: this warning is not very useful in this case, would be good","297","        # to get rid of it","298","        assert_array_equal(ct.fit_transform(X_list), expected_result)","299","        assert_array_equal(ct.fit(X_list).transform(X_list), expected_result)","300","","301",""],"delete":["20","from sklearn.exceptions import NotFittedError"]}],"sklearn\/compose\/_column_transformer.py":[{"add":["18","from ..pipeline import _fit_transform_one, _transform_one, _name_estimators","22","from ..utils.validation import check_array, check_is_fitted","437","        X = _check_X(X)","487","        X = _check_X(X)","514","def _check_X(X):","515","    \"\"\"Use check_array only on lists and other non-array-likes \/ sparse\"\"\"","516","    if hasattr(X, '__array__') or sparse.issparse(X):","517","        return X","518","    return check_array(X, force_all_finite='allow-nan', dtype=np.object)","519","","520",""],"delete":["18","from ..pipeline import (","19","    _fit_one_transformer, _fit_transform_one, _transform_one, _name_estimators)","23","from ..utils.validation import check_is_fitted"]}]}},"c7ce6757d35d50c59b9e3d59ece755e95dd0e602":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/metrics\/pairwise.py":"MODIFY","sklearn\/metrics\/tests\/test_pairwise.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["4",".. _changes_0_21_1:","5","","6","Version 0.21.1","7","==============","8","","9","**May 2019**","10","","11","","12","This is a bug-fix release with some minor documentation improvements and","13","enhancements to features released in 0.21.0.","14","","15","Changelog","16","---------","17","","18",":mod:`sklearn.metrics`","19","......................","20","","21","- |Fix| Fixed a bug in :class:`metrics.pairwise_distances` where it would raise","22","  ``AttributeError`` for boolean metrics when ``X`` had a boolean dtype and","23","  ``Y == None``.","24","  :issue:`13864` by :user:`Paresh Mathur <rick2047>`.","25","","26",""],"delete":[]}],"sklearn\/metrics\/pairwise.py":[{"add":["308","        10 * 2 ** 17)","317","    batch_size = (-tmp + np.sqrt(tmp ** 2 + 4 * maxmem)) \/ 2","902","    np.tanh(K, K)  # compute tanh in-place","935","    np.exp(K, K)  # exponentiate K in-place","969","    np.exp(K, K)  # exponentiate K in-place","1547","        if (dtype == bool and","1548","                (X.dtype != bool or (Y is not None and Y.dtype != bool))):"],"delete":["308","        10 * 2**17)","317","    batch_size = (-tmp + np.sqrt(tmp**2 + 4 * maxmem)) \/ 2","902","    np.tanh(K, K)   # compute tanh in-place","935","    np.exp(K, K)    # exponentiate K in-place","969","    np.exp(K, K)    # exponentiate K in-place","1547","        if dtype == bool and (X.dtype != bool or Y.dtype != bool):","1578",""]}],"sklearn\/metrics\/tests\/test_pairwise.py":[{"add":["175","    # Check that the warning is raised if X is boolean by Y is not boolean:","176","    with pytest.warns(DataConversionWarning, match=msg):","177","        pairwise_distances(X.astype(bool), Y=Y, metric=metric)","178","","179","    # Check that no warning is raised if X is already boolean and Y is None:","180","    with pytest.warns(None) as records:","181","        pairwise_distances(X.astype(bool), metric=metric)","182","    assert len(records) == 0","183",""],"delete":[]}]}},"c50dc0ec9afd5b90288decb3ddf88802a841a368":{"changes":{"sklearn\/utils\/deprecation.py":"MODIFY","sklearn\/utils\/tests\/test_validation.py":"MODIFY","sklearn\/utils\/tests\/test_estimator_checks.py":"MODIFY"},"diff":{"sklearn\/utils\/deprecation.py":[{"add":["80","        # Add a reference to the wrapped function so that we can introspect","81","        # on function arguments in Python 2 (already works in Python 3)","82","        wrapped.__wrapped__ = fun"],"delete":[]}],"sklearn\/utils\/tests\/test_validation.py":[{"add":["24","from sklearn.utils import deprecated","566","    class TestClassWithDeprecatedFitMethod:","567","        @deprecated(\"Deprecated for the purpose of testing has_fit_parameter\")","568","        def fit(self, X, y, sample_weight=None):","569","            pass","570","","571","    assert has_fit_parameter(TestClassWithDeprecatedFitMethod,","572","                             \"sample_weight\"), \\","573","        \"has_fit_parameter fails for class with deprecated fit method.\"","574",""],"delete":[]}],"sklearn\/utils\/tests\/test_estimator_checks.py":[{"add":["11","from sklearn.utils import deprecated","18","from sklearn.utils.estimator_checks import check_fit_score_takes_y","180","def test_check_fit_score_takes_y_works_on_deprecated_fit():","181","    # Tests that check_fit_score_takes_y works on a class with","182","    # a deprecated fit method","183","","184","    class TestEstimatorWithDeprecatedFitMethod(BaseEstimator):","185","        @deprecated(\"Deprecated for the purpose of testing \"","186","                    \"check_fit_score_takes_y\")","187","        def fit(self, X, y):","188","            return self","189","","190","    check_fit_score_takes_y(\"test\", TestEstimatorWithDeprecatedFitMethod())","191","","192",""],"delete":[]}]}},"a35b892522499bfe7a0e5fdfdfbd15752e63fbb0":{"changes":{"sklearn\/datasets\/_openml.py":"MODIFY","doc\/developers\/contributing.rst":"MODIFY"},"diff":{"sklearn\/datasets\/_openml.py":[{"add":["508","        def postprocess(frame):  # type:ignore","529","        def postprocess(X, y, nominal_attributes):  # type:ignore"],"delete":["508","        def postprocess(frame):","529","        def postprocess(X, y, nominal_attributes):"]}],"doc\/developers\/contributing.rst":[{"add":["433","  must not produce new errors in your pull request. Using `# type: ignore`","434","  annotation can be a workaround for a few cases that are not supported by","435","  mypy, in particular,"],"delete":["433","  must not produce new errors in your pull request. Using `# type: ignore` annotation can be a workaround for a few cases that are not supported by mypy, in particular,"]}]}},"fb76de72e7560aaa739872e94bee761777e54c0a":{"changes":{"examples\/impute\/plot_missing_values.py":"MODIFY","doc\/developers\/contributing.rst":"MODIFY"},"diff":{"examples\/impute\/plot_missing_values.py":[{"add":["8","In this example we will investigate different imputation techniques:","10","- imputation by the constant value 0","11","- imputation by the mean value of each feature combined with a missing-ness","12","  indicator auxiliary variable","13","- k nearest neighbor imputation","14","- iterative imputation","16","We will use two datasets: Diabetes dataset which consists of 10 feature","17","variables collected from diabetes patients with an aim to predict disease","18","progression and California Housing dataset for which the target is the median","19","house value for California districts.","20","","21","As neither of these datasets have missing values, we will remove some","22","values to create new versions with artificially missing data. The performance","23","of","24",":class:`~sklearn.ensemble.RandomForestRegressor` on the full original dataset","25","is then compared the performance on the altered datasets with the artificially","26","missing values imputed using different techniques.","27","","31","# Authors: Maria Telenczuk  <https:\/\/github.com\/maikia>","32","# License: BSD 3 clause","34","###############################################################################","35","# Download the data and make missing values sets","36","################################################","37","#","38","# First we download the two datasets. Diabetes dataset is shipped with","39","# scikit-learn. It has 442 entries, each with 10 features. California Housing","40","# dataset is much larger with 20640 entries and 8 features. It needs to be","41","# downloaded. We will only use the first 400 entries for the sake of speeding","42","# up the calculations but feel free to use the whole dataset.","43","#","44","","45","import numpy as np","46","","47","from sklearn.datasets import fetch_california_housing","49","","50","","51","rng = np.random.RandomState(42)","52","","53","X_diabetes, y_diabetes = load_diabetes(return_X_y=True)","54","X_california, y_california = fetch_california_housing(return_X_y=True)","55","X_california = X_california[:400]","56","y_california = y_california[:400]","57","","58","","59","def add_missing_values(X_full, y_full):","60","    n_samples, n_features = X_full.shape","61","","62","    # Add missing values in 75% of the lines","63","    missing_rate = 0.75","64","    n_missing_samples = int(n_samples * missing_rate)","65","","66","    missing_samples = np.zeros(n_samples, dtype=np.bool)","67","    missing_samples[: n_missing_samples] = True","68","","69","    rng.shuffle(missing_samples)","70","    missing_features = rng.randint(0, n_features, n_missing_samples)","71","    X_missing = X_full.copy()","72","    X_missing[missing_samples, missing_features] = np.nan","73","    y_missing = y_full.copy()","74","","75","    return X_missing, y_missing","76","","77","","78","X_miss_california, y_miss_california = add_missing_values(","79","    X_california, y_california)","80","","81","X_miss_diabetes, y_miss_diabetes = add_missing_values(","82","    X_diabetes, y_diabetes)","83","","84","","85","###############################################################################","86","# Impute the missing data and score","87","# #################################","88","# Now we will write a function which will score the results on the differently","89","# imputed data. Let's look at each imputer separately:","90","#","94","from sklearn.ensemble import RandomForestRegressor","95","","96","# To use the experimental IterativeImputer, we need to explicitly ask for it:","97","from sklearn.experimental import enable_iterative_imputer  # noqa","98","from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer","99","from sklearn.model_selection import cross_val_score","100","from sklearn.pipeline import make_pipeline","101","","102","","104","regressor = RandomForestRegressor(random_state=0)","105","","106","###############################################################################","107","# Missing information","108","# -------------------","109","# In addition to imputing the missing values, the imputers have an","110","# `add_indicator` parameter that marks the values that were missing, which","111","# might carry some information.","112","#","116","    estimator = make_pipeline(imputer, regressor)","128","","129","mses_california = np.zeros(5)","130","stds_california = np.zeros(5)","131","mses_diabetes = np.zeros(5)","132","stds_diabetes = np.zeros(5)","133","","134","###############################################################################","135","# Estimate the score","136","# ------------------","137","# First, we want to estimate the score on the original data:","138","#","139","","140","","141","def get_full_score(X_full, y_full):","142","    full_scores = cross_val_score(regressor, X_full, y_full,","143","                                  scoring='neg_mean_squared_error',","144","                                  cv=N_SPLITS)","145","    return full_scores.mean(), full_scores.std()","146","","147","","148","mses_california[0], stds_california[0] = get_full_score(X_california,","149","                                                        y_california)","150","mses_diabetes[0], stds_diabetes[0] = get_full_score(X_diabetes, y_diabetes)","151","","152","","153","###############################################################################","154","# Replace missing values by 0","155","# ---------------------------","156","#","157","# Now we will estimate the score on the data where the missing values are","158","# replaced by 0:","159","#","160","","161","","162","def get_impute_zero_score(X_missing, y_missing):","163","","164","    imputer = SimpleImputer(missing_values=np.nan, add_indicator=True,","165","                            strategy='constant', fill_value=0)","166","    zero_impute_scores = get_scores_for_imputer(imputer, X_missing, y_missing)","167","    return zero_impute_scores.mean(), zero_impute_scores.std()","168","","169","","170","mses_california[1], stds_california[1] = get_impute_zero_score(","171","    X_miss_california, y_miss_california)","172","mses_diabetes[1], stds_diabetes[1] = get_impute_zero_score(X_miss_diabetes,","173","                                                           y_miss_diabetes)","174","","175","","176","###############################################################################","177","# kNN-imputation of the missing values","178","# ------------------------------------","179","#","180","# :class:`sklearn.impute.KNNImputer` imputes missing values using the weighted","181","# or unweighted mean of the desired number of nearest neighbors.","182","","183","def get_impute_knn_score(X_missing, y_missing):","184","    imputer = KNNImputer(missing_values=np.nan, add_indicator=True)","185","    knn_impute_scores = get_scores_for_imputer(imputer, X_missing, y_missing)","186","    return knn_impute_scores.mean(), knn_impute_scores.std()","187","","188","","189","mses_california[2], stds_california[2] = get_impute_knn_score(","190","    X_miss_california, y_miss_california)","191","mses_diabetes[2], stds_diabetes[2] = get_impute_knn_score(X_miss_diabetes,","192","                                                          y_miss_diabetes)","193","","194","","195","###############################################################################","196","# Impute missing values with mean","197","# -------------------------------","198","#","199","","200","def get_impute_mean(X_missing, y_missing):","201","    imputer = SimpleImputer(missing_values=np.nan, strategy=\"mean\",","202","                            add_indicator=True)","203","    mean_impute_scores = get_scores_for_imputer(imputer, X_missing, y_missing)","204","    return mean_impute_scores.mean(), mean_impute_scores.std()","205","","206","","207","mses_california[3], stds_california[3] = get_impute_mean(X_miss_california,","208","                                                         y_miss_california)","209","mses_diabetes[3], stds_diabetes[3] = get_impute_mean(X_miss_diabetes,","210","                                                     y_miss_diabetes)","211","","212","","213","###############################################################################","214","# Iterative imputation of the missing values","215","# ------------------------------------------","216","#","217","# Another option is the :class:`sklearn.impute.IterativeImputer`. This uses","218","# round-robin linear regression, modeling each feature with missing values as a","219","# function of other features, in turn.","220","# The version implemented assumes Gaussian (output) variables. If your features","221","# are obviously non-normal, consider transforming them to look more normal","222","# to potentially improve performance.","223","#","224","","225","def get_impute_iterative(X_missing, y_missing):","226","    imputer = IterativeImputer(missing_values=np.nan, add_indicator=True,","227","                               random_state=0, n_nearest_features=5,","228","                               sample_posterior=True)","229","    iterative_impute_scores = get_scores_for_imputer(imputer,","230","                                                     X_missing,","231","                                                     y_missing)","232","    return iterative_impute_scores.mean(), iterative_impute_scores.std()","233","","234","","235","mses_california[4], stds_california[4] = get_impute_iterative(","236","    X_miss_california, y_miss_california)","237","mses_diabetes[4], stds_diabetes[4] = get_impute_iterative(X_miss_diabetes,","238","                                                          y_miss_diabetes)","239","","240","mses_diabetes = mses_diabetes * -1","241","mses_california = mses_california * -1","242","","243","###############################################################################","244","# Plot the results","245","# ################","246","#","247","# Finally we are going to visualize the score:","248","#","249","","250","import matplotlib.pyplot as plt","251","","252","","253","n_bars = len(mses_diabetes)","254","xval = np.arange(n_bars)","255","","273","# plot california dataset results","276","    ax2.barh(j, mses_california[j], xerr=stds_california[j],","279","ax2.set_title('Imputation Techniques with California Data')","286","","287","# You can also try different techniques. For instance, the median is a more","288","# robust estimator for data with high magnitude variables which could dominate","289","# results (otherwise known as a 'long tail')."],"delete":["7","The median is a more robust estimator for data with high magnitude variables","8","which could dominate results (otherwise known as a 'long tail').","10","With ``KNNImputer``, missing values can be imputed using the weighted","11","or unweighted mean of the desired number of nearest neighbors.","13","Another option is the :class:`sklearn.impute.IterativeImputer`. This uses","14","round-robin linear regression, treating every variable as an output in","15","turn. The version implemented assumes Gaussian (output) variables. If your","16","features are obviously non-Normal, consider transforming them to look more","17","Normal so as to potentially improve performance.","19","In addition of using an imputing method, we can also keep an indication of the","20","missing information using :func:`sklearn.impute.MissingIndicator` which might","21","carry some information.","25","import numpy as np","26","import matplotlib.pyplot as plt","28","# To use the experimental IterativeImputer, we need to explicitly ask for it:","29","from sklearn.experimental import enable_iterative_imputer  # noqa","31","from sklearn.datasets import load_boston","32","from sklearn.ensemble import RandomForestRegressor","33","from sklearn.pipeline import make_pipeline, make_union","34","from sklearn.impute import (","35","    SimpleImputer, KNNImputer, IterativeImputer, MissingIndicator)","36","from sklearn.model_selection import cross_val_score","41","REGRESSOR = RandomForestRegressor(random_state=0)","45","    estimator = make_pipeline(","46","        make_union(imputer, MissingIndicator(missing_values=0)),","47","        REGRESSOR)","54","def get_results(dataset):","55","    X_full, y_full = dataset.data, dataset.target","56","    n_samples = X_full.shape[0]","57","    n_features = X_full.shape[1]","58","","59","    # Estimate the score on the entire dataset, with no missing values","60","    full_scores = cross_val_score(REGRESSOR, X_full, y_full,","61","                                  scoring='neg_mean_squared_error',","62","                                  cv=N_SPLITS)","63","","64","    # Add missing values in 75% of the lines","65","    missing_rate = 0.75","66","    n_missing_samples = int(np.floor(n_samples * missing_rate))","67","    missing_samples = np.hstack((np.zeros(n_samples - n_missing_samples,","68","                                          dtype=np.bool),","69","                                 np.ones(n_missing_samples,","70","                                         dtype=np.bool)))","71","    rng.shuffle(missing_samples)","72","    missing_features = rng.randint(0, n_features, n_missing_samples)","73","    X_missing = X_full.copy()","74","    X_missing[np.where(missing_samples)[0], missing_features] = 0","75","    y_missing = y_full.copy()","76","","77","    # Estimate the score after replacing missing values by 0","78","    imputer = SimpleImputer(missing_values=0,","79","                            strategy='constant',","80","                            fill_value=0)","81","    zero_impute_scores = get_scores_for_imputer(imputer, X_missing, y_missing)","82","","83","    # Estimate the score after imputation (mean strategy) of the missing values","84","    imputer = SimpleImputer(missing_values=0, strategy=\"mean\")","85","    mean_impute_scores = get_scores_for_imputer(imputer, X_missing, y_missing)","86","","87","    # Estimate the score after kNN-imputation of the missing values","88","    imputer = KNNImputer(missing_values=0)","89","    knn_impute_scores = get_scores_for_imputer(imputer, X_missing, y_missing)","90","","91","    # Estimate the score after iterative imputation of the missing values","92","    imputer = IterativeImputer(missing_values=0,","93","                               random_state=0,","94","                               n_nearest_features=5,","95","                               sample_posterior=True)","96","    iterative_impute_scores = get_scores_for_imputer(imputer,","97","                                                     X_missing,","98","                                                     y_missing)","99","","100","    return ((full_scores.mean(), full_scores.std()),","101","            (zero_impute_scores.mean(), zero_impute_scores.std()),","102","            (mean_impute_scores.mean(), mean_impute_scores.std()),","103","            (knn_impute_scores.mean(), knn_impute_scores.std()),","104","            (iterative_impute_scores.mean(), iterative_impute_scores.std()))","105","","106","","107","results_diabetes = np.array(get_results(load_diabetes()))","108","mses_diabetes = results_diabetes[:, 0] * -1","109","stds_diabetes = results_diabetes[:, 1]","110","","111","results_boston = np.array(get_results(load_boston()))","112","mses_boston = results_boston[:, 0] * -1","113","stds_boston = results_boston[:, 1]","114","","115","n_bars = len(mses_diabetes)","116","xval = np.arange(n_bars)","117","","140","# plot boston results","143","    ax2.barh(j, mses_boston[j], xerr=stds_boston[j],","146","ax2.set_title('Imputation Techniques with Boston Data')"]}],"doc\/developers\/contributing.rst":[{"add":["436","","437","  - when importing C or Cython modules","438","  - on properties with decorators"],"delete":["436","   - when importing C or Cython modules","437","   - on properties with decorators"]}]}},"76ef8b0ef07f9c03b97d29a51e1543be7720e85a":{"changes":{"sklearn\/utils\/class_weight.py":"MODIFY","sklearn\/utils\/multiclass.py":"MODIFY","sklearn\/preprocessing\/_data.py":"MODIFY","sklearn\/neural_network\/_rbm.py":"MODIFY","sklearn\/feature_selection\/_univariate_selection.py":"MODIFY","sklearn\/utils\/__init__.py":"MODIFY","sklearn\/linear_model\/_stochastic_gradient.py":"MODIFY","sklearn\/feature_selection\/tests\/test_base.py":"MODIFY","sklearn\/utils\/validation.py":"MODIFY","sklearn\/manifold\/_spectral_embedding.py":"MODIFY","sklearn\/utils\/tests\/test_class_weight.py":"MODIFY","sklearn\/ensemble\/_forest.py":"MODIFY","sklearn\/utils\/tests\/test_validation.py":"MODIFY","sklearn\/linear_model\/_coordinate_descent.py":"MODIFY","sklearn\/neighbors\/_nca.py":"MODIFY"},"diff":{"sklearn\/utils\/class_weight.py":[{"add":["6","from .validation import _deprecate_positional_args","7","","73","@_deprecate_positional_args","74","def compute_sample_weight(class_weight, y, *, indices=None):"],"delete":["71","def compute_sample_weight(class_weight, y, indices=None):"]}],"sklearn\/utils\/multiclass.py":[{"add":["29","    return np.arange(","30","        check_array(y, accept_sparse=['csr', 'csc', 'coo']).shape[1]","31","    )","87","            len(set(check_array(y,","88","                                accept_sparse=['csr', 'csc', 'coo']).shape[1]"],"delete":["29","    return np.arange(check_array(y, ['csr', 'csc', 'coo']).shape[1])","85","            len(set(check_array(y, ['csr', 'csc', 'coo']).shape[1]"]}],"sklearn\/preprocessing\/_data.py":[{"add":["1709","    X = check_array(X, accept_sparse=sparse_format, copy=copy,"],"delete":["1709","    X = check_array(X, sparse_format, copy=copy,"]}],"sklearn\/neural_network\/_rbm.py":[{"add":["359","                                            n_batches, n_samples=n_samples))"],"delete":["359","                                            n_batches, n_samples))"]}],"sklearn\/feature_selection\/_univariate_selection.py":[{"add":["148","    X, y = check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'])","279","    X, y = check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'],","280","                     dtype=np.float64)"],"delete":["148","    X, y = check_X_y(X, y, ['csr', 'csc', 'coo'])","279","    X, y = check_X_y(X, y, ['csr', 'csc', 'coo'], dtype=np.float64)"]}],"sklearn\/utils\/__init__.py":[{"add":["31","                         check_symmetric, check_scalar,","32","                         _deprecate_positional_args)","317","    return _safe_indexing(X, indices, axis=axis)","320","def _safe_indexing(X, indices, *, axis=0):","687","@_deprecate_positional_args","688","def safe_sqr(X, *, copy=True):","727","@_deprecate_positional_args","728","def gen_batches(n, batch_size, *, min_batch_size=0):","777","@_deprecate_positional_args","778","def gen_even_slices(n, n_packs, *, n_samples=None):","963","@_deprecate_positional_args","964","def get_chunk_n_rows(row_bytes, *, max_n_rows=None, working_memory=None):"],"delete":["31","                         check_symmetric, check_scalar)","316","    return _safe_indexing(X, indices, axis)","319","def _safe_indexing(X, indices, axis=0):","686","def safe_sqr(X, copy=True):","725","def gen_batches(n, batch_size, min_batch_size=0):","774","def gen_even_slices(n, n_packs, n_samples=None):","959","def get_chunk_n_rows(row_bytes, max_n_rows=None,","960","                     working_memory=None):"]}],"sklearn\/linear_model\/_stochastic_gradient.py":[{"add":["489","        X, y = check_X_y(X, y, accept_sparse='csr', dtype=np.float64,","490","                         order=\"C\", accept_large_sparse=False)"],"delete":["489","        X, y = check_X_y(X, y, 'csr', dtype=np.float64, order=\"C\",","490","                         accept_large_sparse=False)"]}],"sklearn\/feature_selection\/tests\/test_base.py":[{"add":["17","        X = check_array(X, accept_sparse='csc')"],"delete":["17","        X = check_array(X, 'csc')"]}],"sklearn\/utils\/validation.py":[{"add":["38","def _deprecate_positional_args(f):","39","    \"\"\"Decorator for methods that issues warnings for positional arguments","40","","41","    Using the keyword-only argument syntax in pep 3102, arguments after the","42","    * will issue a warning when passed as a positional argument.","43","","44","    Parameters","45","    ----------","46","    f : function","47","        function to check arguments on","48","    \"\"\"","49","    sig = signature(f)","50","    kwonly_args = []","51","    all_args = []","52","","53","    for name, param in sig.parameters.items():","54","        if param.kind == Parameter.POSITIONAL_OR_KEYWORD:","55","            all_args.append(name)","56","        elif param.kind == Parameter.KEYWORD_ONLY:","57","            kwonly_args.append(name)","58","","59","    @wraps(f)","60","    def inner_f(*args, **kwargs):","61","        extra_args = len(args) - len(all_args)","62","        if extra_args > 0:","63","            # ignore first 'self' argument for instance methods","64","            args_msg = ['{}={}'.format(name, arg)","65","                        for name, arg in zip(kwonly_args[:extra_args],","66","                                             args[-extra_args:])]","67","            warnings.warn(\"Pass {} as keyword args. From version 0.25 \"","68","                          \"passing these as positional arguments will \"","69","                          \"result in an error\".format(\", \".join(args_msg)),","70","                          FutureWarning)","71","        kwargs.update({k: arg for k, arg in zip(sig.parameters, args)})","72","        return f(**kwargs)","73","    return inner_f","74","","75","","107","@_deprecate_positional_args","108","def assert_all_finite(X, *, allow_nan=False):","120","@_deprecate_positional_args","121","def as_float_array(X, *, copy=True, force_all_finite=True):","155","        return check_array(X, accept_sparse=['csr', 'csc', 'coo'],","156","                           dtype=np.float64, copy=copy,","157","                           force_all_finite=force_all_finite, ensure_2d=False)","391","@_deprecate_positional_args","392","def check_array(array, accept_sparse=False, *, accept_large_sparse=True,","663","@_deprecate_positional_args","664","def check_X_y(X, y, accept_sparse=False, *, accept_large_sparse=True,","776","        y = check_array(y, accept_sparse='csr', force_all_finite=True,","777","                        ensure_2d=False, dtype=None)","789","@_deprecate_positional_args","790","def column_or_1d(y, *, warn=False):","870","@_deprecate_positional_args","871","def check_symmetric(array, *, tol=1E-10, raise_warning=True,","927","@_deprecate_positional_args","928","def check_is_fitted(estimator, attributes=None, *, msg=None, all_or_any=all):","1021","def check_scalar(x, name, target_type, *, min_val=None, max_val=None):"],"delete":["69","def assert_all_finite(X, allow_nan=False):","81","def as_float_array(X, copy=True, force_all_finite=True):","115","        return check_array(X, ['csr', 'csc', 'coo'], dtype=np.float64,","116","                           copy=copy, force_all_finite=force_all_finite,","117","                           ensure_2d=False)","351","def check_array(array, accept_sparse=False, accept_large_sparse=True,","622","def check_X_y(X, y, accept_sparse=False, accept_large_sparse=True,","734","        y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,","735","                        dtype=None)","747","def column_or_1d(y, warn=False):","827","def check_symmetric(array, tol=1E-10, raise_warning=True,","883","def check_is_fitted(estimator, attributes=None, msg=None, all_or_any=all):","976","def check_scalar(x, name, target_type, min_val=None, max_val=None):","1270","def _deprecate_positional_args(f):","1271","    \"\"\"Decorator for methods that issues warnings for positional arguments","1272","","1273","    Using the keyword-only argument syntax in pep 3102, arguments after the","1274","    * will issue a warning when passed as a positional argument.","1275","","1276","    Parameters","1277","    ----------","1278","    f : function","1279","        function to check arguments on","1280","    \"\"\"","1281","    sig = signature(f)","1282","    kwonly_args = []","1283","    all_args = []","1284","","1285","    for name, param in sig.parameters.items():","1286","        if param.kind == Parameter.POSITIONAL_OR_KEYWORD:","1287","            all_args.append(name)","1288","        elif param.kind == Parameter.KEYWORD_ONLY:","1289","            kwonly_args.append(name)","1290","","1291","    @wraps(f)","1292","    def inner_f(*args, **kwargs):","1293","        extra_args = len(args) - len(all_args)","1294","        if extra_args > 0:","1295","            # ignore first 'self' argument for instance methods","1296","            args_msg = ['{}={}'.format(name, arg)","1297","                        for name, arg in zip(kwonly_args[:extra_args],","1298","                                             args[-extra_args:])]","1299","            warnings.warn(\"Pass {} as keyword args. From version 0.25 \"","1300","                          \"passing these as positional arguments will \"","1301","                          \"result in an error\".format(\", \".join(args_msg)),","1302","                          FutureWarning)","1303","        kwargs.update({k: arg for k, arg in zip(sig.parameters, args)})","1304","        return f(**kwargs)","1305","    return inner_f","1306","","1307",""]}],"sklearn\/manifold\/_spectral_embedding.py":[{"add":["303","        ml = smoothed_aggregation_solver(check_array(laplacian,","304","                                                     accept_sparse='csr'))"],"delete":["303","        ml = smoothed_aggregation_solver(check_array(laplacian, 'csr'))"]}],"sklearn\/utils\/tests\/test_class_weight.py":[{"add":["194","    sample_weight = compute_sample_weight(\"balanced\", y, indices=range(6))","199","    sample_weight = compute_sample_weight(\"balanced\", y, indices=range(6))","204","    sample_weight = compute_sample_weight(\"balanced\", y, indices=range(4))","210","    sample_weight = compute_sample_weight(\"balanced\", y,","211","                                          indices=[0, 1, 1, 2, 2, 3])","217","    sample_weight = compute_sample_weight(\"balanced\", y,","218","                                          indices=[0, 1, 1, 2, 2, 3])","223","    sample_weight = compute_sample_weight(\"balanced\", y, indices=range(6))","228","    sample_weight = compute_sample_weight(\"balanced\", y, indices=range(6))","241","        compute_sample_weight(\"ni\", y, indices=range(4))","245","        compute_sample_weight(\"ni\", y_, indices=range(4))","249","        compute_sample_weight({1: 2, 2: 1}, y, indices=range(4))"],"delete":["194","    sample_weight = compute_sample_weight(\"balanced\", y, range(6))","199","    sample_weight = compute_sample_weight(\"balanced\", y, range(6))","204","    sample_weight = compute_sample_weight(\"balanced\", y, range(4))","210","    sample_weight = compute_sample_weight(\"balanced\", y, [0, 1, 1, 2, 2, 3])","216","    sample_weight = compute_sample_weight(\"balanced\", y, [0, 1, 1, 2, 2, 3])","221","    sample_weight = compute_sample_weight(\"balanced\", y, range(6))","226","    sample_weight = compute_sample_weight(\"balanced\", y, range(6))","239","        compute_sample_weight(\"ni\", y, range(4))","243","        compute_sample_weight(\"ni\", y_, range(4))","247","        compute_sample_weight({1: 2, 2: 1}, y, range(4))"]}],"sklearn\/ensemble\/_forest.py":[{"add":["161","                curr_sample_weight *= compute_sample_weight('auto', y,","162","                                                            indices=indices)","164","            curr_sample_weight *= compute_sample_weight('balanced', y,","165","                                                        indices=indices)"],"delete":["161","                curr_sample_weight *= compute_sample_weight('auto', y, indices)","163","            curr_sample_weight *= compute_sample_weight('balanced', y, indices)"]}],"sklearn\/utils\/tests\/test_validation.py":[{"add":["65","    assert as_float_array(X, copy=False) is not X","914","        check_scalar(x, \"test_name\", target_type=target_type,","915","                     min_val=min_val, max_val=max_val)"],"delete":["65","    assert as_float_array(X, False) is not X","914","        check_scalar(x, \"test_name\", target_type, min_val, max_val)"]}],"sklearn\/linear_model\/_coordinate_descent.py":[{"add":["133","        X = check_array(X, accept_sparse='csc',","437","        X = check_array(X, accept_sparse='csc', dtype=[np.float64, np.float32],","439","        y = check_array(y, accept_sparse='csc', dtype=X.dtype.type,","440","                        order='F', copy=False, ensure_2d=False)","1097","    X_train = check_array(X_train, accept_sparse='csc', dtype=dtype,","1098","                          order=X_order)"],"delete":["133","        X = check_array(X, 'csc',","437","        X = check_array(X, 'csc', dtype=[np.float64, np.float32],","439","        y = check_array(y, 'csc', dtype=X.dtype.type, order='F', copy=False,","440","                        ensure_2d=False)","1097","    X_train = check_array(X_train, 'csc', dtype=dtype, order=X_order)"]}],"sklearn\/neighbors\/_nca.py":[{"add":["310","                self.n_components, 'n_components', numbers.Integral, min_val=1)","329","        check_scalar(self.max_iter, 'max_iter', numbers.Integral, min_val=1)","330","        check_scalar(self.tol, 'tol', numbers.Real, min_val=0.)","331","        check_scalar(self.verbose, 'verbose', numbers.Integral, min_val=0)"],"delete":["310","                self.n_components, 'n_components', numbers.Integral, 1)","329","        check_scalar(self.max_iter, 'max_iter', numbers.Integral, 1)","330","        check_scalar(self.tol, 'tol', numbers.Real, 0.)","331","        check_scalar(self.verbose, 'verbose', numbers.Integral, 0)"]}]}},"3f8743f47b61a269e8bfff2322cb544170976574":{"changes":{"sklearn\/__init__.py":"MODIFY","sklearn\/neighbors\/tests\/test_neighbors.py":"MODIFY","sklearn\/setup.py":"MODIFY","sklearn\/tests\/test_multiclass.py":"MODIFY","sklearn\/utils\/testing.py":"MODIFY","sklearn\/covariance\/graph_lasso_.py":"MODIFY","sklearn\/learning_curve.py":"MODIFY","sklearn\/tests\/test_naive_bayes.py":"MODIFY","sklearn\/tests\/test_metaestimators.py":"MODIFY","sklearn\/tests\/test_base.py":"MODIFY","sklearn\/model_selection\/_search.py":"ADD","sklearn\/cross_validation.py":"MODIFY","sklearn\/tree\/tree.py":"MODIFY","sklearn\/linear_model\/logistic.py":"MODIFY","sklearn\/ensemble\/tests\/test_forest.py":"MODIFY","sklearn\/ensemble\/tests\/test_voting_classifier.py":"MODIFY","sklearn\/linear_model\/omp.py":"MODIFY","sklearn\/ensemble\/tests\/test_weight_boosting.py":"MODIFY","sklearn\/metrics\/tests\/test_score_objects.py":"MODIFY","sklearn\/model_selection\/_split.py":"ADD","sklearn\/preprocessing\/tests\/test_imputation.py":"MODIFY","sklearn\/linear_model\/tests\/test_least_angle.py":"MODIFY","sklearn\/tests\/test_grid_search.py":"MODIFY","sklearn\/model_selection\/tests\/test_validation.py":"ADD","doc\/whats_new.rst":"MODIFY","sklearn\/tests\/test_learning_curve.py":"MODIFY","sklearn\/grid_search.py":"MODIFY","sklearn\/exceptions.py":"MODIFY","sklearn\/linear_model\/tests\/test_logistic.py":"MODIFY","sklearn\/model_selection\/tests\/test_search.py":"ADD","sklearn\/linear_model\/coordinate_descent.py":"MODIFY","sklearn\/model_selection\/tests\/test_split.py":"ADD","sklearn\/linear_model\/tests\/test_ridge.py":"MODIFY","sklearn\/feature_selection\/tests\/test_rfe.py":"MODIFY","sklearn\/neighbors\/tests\/test_kde.py":"MODIFY","sklearn\/feature_extraction\/tests\/test_text.py":"MODIFY","sklearn\/metrics\/scorer.py":"MODIFY","sklearn\/calibration.py":"MODIFY","sklearn\/tests\/test_cross_validation.py":"MODIFY","sklearn\/svm\/tests\/test_svm.py":"MODIFY","sklearn\/linear_model\/ridge.py":"MODIFY","sklearn\/feature_selection\/rfe.py":"MODIFY","sklearn\/decomposition\/tests\/test_kernel_pca.py":"MODIFY","sklearn\/utils\/estimator_checks.py":"MODIFY","sklearn\/cluster\/tests\/test_bicluster.py":"MODIFY","sklearn\/model_selection\/__init__.py":"ADD","sklearn\/ensemble\/tests\/test_bagging.py":"MODIFY","sklearn\/model_selection\/_validation.py":"ADD","sklearn\/linear_model\/least_angle.py":"MODIFY"},"diff":{"sklearn\/__init__.py":[{"add":["64","               'lda', 'learning_curve', 'linear_model', 'manifold', 'metrics',","65","               'mixture', 'model_selection', 'multiclass',"],"delete":["64","               'lda', 'learning_curve',","65","               'linear_model', 'manifold', 'metrics', 'mixture', 'multiclass',"]}],"sklearn\/neighbors\/tests\/test_neighbors.py":[{"add":["8","from sklearn.model_selection import train_test_split","9","from sklearn.model_selection import cross_val_score"],"delete":["8","from sklearn.cross_validation import train_test_split, cross_val_score"]}],"sklearn\/setup.py":[{"add":["51","    config.add_subpackage('model_selection')","52","    config.add_subpackage('model_selection\/tests')"],"delete":[]}],"sklearn\/tests\/test_multiclass.py":[{"add":["23","from sklearn.model_selection import GridSearchCV"],"delete":["23","from sklearn.grid_search import GridSearchCV"]}],"sklearn\/utils\/testing.py":[{"add":["22","from operator import itemgetter","607","        if (\".tests.\" in modname):","651","    # itemgetter is used to ensure the sort does not extend to the 2nd item of","652","    # the tuple","653","    return sorted(set(estimators), key=itemgetter(0))"],"delete":["606","        if \".tests.\" in modname:","650","    return sorted(set(estimators))"]}],"sklearn\/covariance\/graph_lasso_.py":[{"add":["23","from ..model_selection import check_cv, cross_val_score","582","        cv = check_cv(self.cv, y, classifier=False)","614","                )(delayed(graph_lasso_path)(X[train], alphas=alphas,","615","                                            X_test=X[test], mode=self.mode,","616","                                            tol=self.tol,","617","                                            enet_tol=self.enet_tol,","618","                                            max_iter=int(.1 * self.max_iter),","619","                                            verbose=inner_verbose)","620","                  for train, test in cv.split(X, y))"],"delete":["23","from ..cross_validation import check_cv, cross_val_score","582","        cv = check_cv(self.cv, X, y, classifier=False)","614","                )(","615","                    delayed(graph_lasso_path)(","616","                        X[train], alphas=alphas,","617","                        X_test=X[test], mode=self.mode,","618","                        tol=self.tol, enet_tol=self.enet_tol,","619","                        max_iter=int(.1 * self.max_iter),","620","                        verbose=inner_verbose)","621","                    for train, test in cv)"]}],"sklearn\/learning_curve.py":[{"add":["19","warnings.warn(\"This module has been deprecated in favor of the \"","20","              \"model_selection module into which all the functions are moved.\"","21","              \" This module will be removed in 0.19\",","22","              DeprecationWarning)","23","","24",""],"delete":[]}],"sklearn\/tests\/test_naive_bayes.py":[{"add":["6","","7","from sklearn.model_selection import train_test_split","8","from sklearn.model_selection import cross_val_score"],"delete":["6","from sklearn.cross_validation import cross_val_score, train_test_split"]}],"sklearn\/tests\/test_metaestimators.py":[{"add":["11","from sklearn.model_selection import GridSearchCV, RandomizedSearchCV"],"delete":["11","from sklearn.grid_search import GridSearchCV, RandomizedSearchCV"]}],"sklearn\/tests\/test_base.py":[{"add":["16","from sklearn.model_selection import GridSearchCV"],"delete":["16","from sklearn.grid_search import GridSearchCV"]}],"sklearn\/model_selection\/_search.py":[{"add":[],"delete":[]}],"sklearn\/cross_validation.py":[{"add":["36","","37","warnings.warn(\"This module has been deprecated in favor of the \"","38","              \"model_selection module into which all the refactored classes \"","39","              \"and functions are moved. Also note that the interface of the \"","40","              \"new CV iterators are different from that of this module. \"","41","              \"This module will be removed in 0.19.\", DeprecationWarning)","42","","43","","314","    StratifiedKFold take label information into account to avoid building"],"delete":["306","    StratifiedKFold: take label information into account to avoid building"]}],"sklearn\/tree\/tree.py":[{"add":["636","    >>> from sklearn.model_selection import cross_val_score","856","    >>> from sklearn.model_selection import cross_val_score"],"delete":["636","    >>> from sklearn.cross_validation import cross_val_score","856","    >>> from sklearn.cross_validation import cross_val_score"]}],"sklearn\/linear_model\/logistic.py":[{"add":["0","","35","from ..model_selection import check_cv","1312","        See the module :mod:`sklearn.model_selection` module for the","1509","        cv = check_cv(self.cv, y, classifier=True)","1510","        folds = list(cv.split(X, y))"],"delete":["34","from ..cross_validation import check_cv","1311","        See the module :mod:`sklearn.cross_validation` module for the","1508","        cv = check_cv(self.cv, X, y, classifier=True)","1509","        folds = list(cv)"]}],"sklearn\/ensemble\/tests\/test_forest.py":[{"add":["40","from sklearn.model_selection import GridSearchCV"],"delete":["40","from sklearn.grid_search import GridSearchCV"]}],"sklearn\/ensemble\/tests\/test_voting_classifier.py":[{"add":["9","from sklearn.model_selection import GridSearchCV","11","from sklearn.model_selection import cross_val_score","29","    scores = cross_val_score(eclf, X, y, cv=5, scoring='accuracy')","53","    scores = cross_val_score(eclf, X, y, cv=5, scoring='accuracy')"],"delete":["9","from sklearn.grid_search import GridSearchCV","11","from sklearn import cross_validation","29","    scores = cross_validation.cross_val_score(eclf,","30","                                              X,","31","                                              y,","32","                                              cv=5,","33","                                              scoring='accuracy')","57","    scores = cross_validation.cross_val_score(eclf,","58","                                              X,","59","                                              y,","60","                                              cv=5,","61","                                              scoring='accuracy')"]}],"sklearn\/linear_model\/omp.py":[{"add":["17","from ..model_selection import check_cv","837","        cv = check_cv(self.cv, classifier=False)","845","            for train, test in cv.split(X))"],"delete":["17","from ..cross_validation import check_cv","837","        cv = check_cv(self.cv, X, y, classifier=False)","845","            for train, test in cv)"]}],"sklearn\/ensemble\/tests\/test_weight_boosting.py":[{"add":["9","from sklearn.model_selection import train_test_split","10","from sklearn.model_selection import GridSearchCV"],"delete":["9","from sklearn.cross_validation import train_test_split","10","from sklearn.grid_search import GridSearchCV"]}],"sklearn\/metrics\/tests\/test_score_objects.py":[{"add":["29","from sklearn.model_selection import train_test_split, cross_val_score","30","from sklearn.model_selection import GridSearchCV"],"delete":["29","from sklearn.cross_validation import train_test_split, cross_val_score","30","from sklearn.grid_search import GridSearchCV"]}],"sklearn\/model_selection\/_split.py":[{"add":[],"delete":[]}],"sklearn\/preprocessing\/tests\/test_imputation.py":[{"add":["12","from sklearn.model_selection import GridSearchCV","271","    gs = GridSearchCV(pipeline, parameters)"],"delete":["12","from sklearn import grid_search","271","    gs = grid_search.GridSearchCV(pipeline, parameters)"]}],"sklearn\/linear_model\/tests\/test_least_angle.py":[{"add":["5","from sklearn.model_selection import train_test_split"],"delete":["5","from sklearn.cross_validation import train_test_split"]}],"sklearn\/tests\/test_grid_search.py":[{"add":["10","import warnings","44","","45","from sklearn.exceptions import ChangedBehaviorWarning","47","","48","with warnings.catch_warnings():","49","    warnings.simplefilter('ignore')","50","    from sklearn.grid_search import (GridSearchCV, RandomizedSearchCV,","51","                                     ParameterGrid, ParameterSampler)","52","    from sklearn.cross_validation import KFold, StratifiedKFold","53",""],"delete":["35","from sklearn.grid_search import GridSearchCV, RandomizedSearchCV","36","from sklearn.grid_search import ParameterGrid, ParameterSampler","37","from sklearn.exceptions import ChangedBehaviorWarning","46","from sklearn.cross_validation import KFold, StratifiedKFold"]}],"sklearn\/model_selection\/tests\/test_validation.py":[{"add":[],"delete":[]}],"doc\/whats_new.rst":[{"add":["27","   - The cross-validation iterators are now modified as cross-validation splitters","28","     which expose a ``split`` method that takes in the data and yields a generator","29","     for the different splits. This change makes it possible to do nested cross-validation","30","     with ease. (`#4294 https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/4294>`_) by `Raghav R V`_.","31","","32","   - The :mod:`cross_validation`, :mod:`grid_search` and :mod:`learning_curve`","33","     have been deprecated and the classes and functions have been reorganized into","34","     the :mod:`model_selection` module. (`#4294 https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/4294>`_) by `Raghav R V`_.","35","","36",""],"delete":[]}],"sklearn\/tests\/test_learning_curve.py":[{"add":["15","","16","with warnings.catch_warnings():","17","    warnings.simplefilter('ignore')","18","    from sklearn.learning_curve import learning_curve, validation_curve","19","    from sklearn.cross_validation import KFold","20",""],"delete":["9","from sklearn.learning_curve import learning_curve, validation_curve","16","from sklearn.cross_validation import KFold"]}],"sklearn\/grid_search.py":[{"add":["39","warnings.warn(\"This module has been deprecated in favor of the \"","40","              \"model_selection module into which all the refactored classes \"","41","              \"and functions are moved. This module will be removed in 0.19.\",","42","              DeprecationWarning)","43","","44",""],"delete":[]}],"sklearn\/exceptions.py":[{"add":["87","    >>> from sklearn.model_selection import GridSearchCV"],"delete":["87","    >>> from sklearn.grid_search import GridSearchCV"]}],"sklearn\/linear_model\/tests\/test_logistic.py":[{"add":["26","from sklearn.model_selection import StratifiedKFold","456","    # The cv indices from stratified kfold (where stratification is done based","457","    # on the fine-grained iris classes, i.e, before the classes 0 and 1 are","458","    # conflated) is used for both clf and clf1","459","    cv = StratifiedKFold(3)","460","    precomputed_folds = list(cv.split(train, target))","461","","462","    # Train clf on the original dataset where classes 0 and 1 are separated","463","    clf = LogisticRegressionCV(cv=precomputed_folds)","466","    # Conflate classes 0 and 1 and train clf1 on this modifed dataset","467","    clf1 = LogisticRegressionCV(cv=precomputed_folds)","472","    # Ensure that what OvR learns for class2 is same regardless of whether","473","    # classes 0 and 1 are separated or not"],"delete":["26","from sklearn.cross_validation import StratifiedKFold","456","    # Use pre-defined fold as folds generated for different y","457","    cv = StratifiedKFold(target, 3)","458","    clf = LogisticRegressionCV(cv=cv)","461","    clf1 = LogisticRegressionCV(cv=cv)"]}],"sklearn\/model_selection\/tests\/test_search.py":[{"add":[],"delete":[]}],"sklearn\/linear_model\/coordinate_descent.py":[{"add":["19","from ..model_selection import check_cv","1122","        cv = check_cv(self.cv)","1125","        folds = list(cv.split(X))"],"delete":["19","from ..cross_validation import check_cv","1122","        cv = check_cv(self.cv, X)","1125","        folds = list(cv)"]}],"sklearn\/model_selection\/tests\/test_split.py":[{"add":[],"delete":[]}],"sklearn\/linear_model\/tests\/test_ridge.py":[{"add":["31","from sklearn.model_selection import GridSearchCV","32","from sklearn.model_selection import KFold","366","    cv = KFold(5)","405","    cv = KFold(5)","569","        cv = KFold(5)"],"delete":["31","from sklearn.grid_search import GridSearchCV","32","","33","from sklearn.cross_validation import KFold","360","    n_samples = X_diabetes.shape[0]","361","","369","    cv = KFold(n_samples, 5)","408","    n_samples = X_iris.shape[0]","409","    cv = KFold(n_samples, 5)","573","        cv = KFold(n_samples, 5)"]}],"sklearn\/feature_selection\/tests\/test_rfe.py":[{"add":["14","from sklearn.model_selection import cross_val_score"],"delete":["14","from sklearn.cross_validation import cross_val_score"]}],"sklearn\/neighbors\/tests\/test_kde.py":[{"add":["7","from sklearn.model_selection import GridSearchCV"],"delete":["7","from sklearn.grid_search import GridSearchCV"]}],"sklearn\/feature_extraction\/tests\/test_text.py":[{"add":["14","from sklearn.model_selection import train_test_split","15","from sklearn.model_selection import cross_val_score","16","from sklearn.model_selection import GridSearchCV"],"delete":["14","from sklearn.cross_validation import train_test_split","15","from sklearn.cross_validation import cross_val_score","16","from sklearn.grid_search import GridSearchCV"]}],"sklearn\/metrics\/scorer.py":[{"add":["6",":class:`sklearn.model_selection.GridSearchCV` or","7",":func:`sklearn.model_selection.cross_val_score` as the ``scoring``","8","parameter, to specify how a model should be evaluated.","296","    >>> from sklearn.model_selection import GridSearchCV"],"delete":["6",":class:`sklearn.grid_search.GridSearchCV` or","7",":func:`sklearn.cross_validation.cross_val_score` as the ``scoring`` parameter,","8","to specify how a model should be evaluated.","296","    >>> from sklearn.grid_search import GridSearchCV"]}],"sklearn\/calibration.py":[{"add":["24","from .model_selection import check_cv","154","            cv = check_cv(self.cv, y, classifier=True)","165","            for train, test in cv.split(X, y):"],"delete":["24","from .cross_validation import check_cv","154","            cv = check_cv(self.cv, X, y, classifier=True)","165","            for train, test in cv:"]}],"sklearn\/tests\/test_cross_validation.py":[{"add":["24","with warnings.catch_warnings():","25","    warnings.simplefilter('ignore')","26","    from sklearn import cross_validation as cval","27",""],"delete":["24","from sklearn import cross_validation as cval"]}],"sklearn\/svm\/tests\/test_svm.py":[{"add":["15","from sklearn.model_selection import train_test_split"],"delete":["15","from sklearn.cross_validation import train_test_split"]}],"sklearn\/linear_model\/ridge.py":[{"add":["30","from ..model_selection import GridSearchCV"],"delete":["30","from ..grid_search import GridSearchCV"]}],"sklearn\/feature_selection\/rfe.py":[{"add":["16","from ..model_selection import check_cv","17","from ..model_selection._validation import _safe_split, _score","375","        cv = check_cv(self.cv, y, is_classifier(self.estimator))","384","        for n, (train, test) in enumerate(cv.split(X, y)):","416","        # Fixing a normalization error, n is equal to get_n_splits(X, y) - 1","417","        # here, the scores are normalized by get_n_splits(X, y)","418","        self.grid_scores_ = scores \/ cv.get_n_splits(X, y)"],"delete":["16","from ..cross_validation import check_cv","17","from ..cross_validation import _safe_split, _score","375","        cv = check_cv(self.cv, X, y, is_classifier(self.estimator))","384","        for n, (train, test) in enumerate(cv):","416","        # Fixing a normalization error, n is equal to len(cv) - 1","417","        # here, the scores are normalized by len(cv)","418","        self.grid_scores_ = scores \/ len(cv)"]}],"sklearn\/decomposition\/tests\/test_kernel_pca.py":[{"add":["11","from sklearn.model_selection import GridSearchCV"],"delete":["11","from sklearn.grid_search import GridSearchCV"]}],"sklearn\/utils\/estimator_checks.py":[{"add":["45","from sklearn.model_selection import train_test_split"],"delete":["45","from sklearn.cross_validation import train_test_split"]}],"sklearn\/cluster\/tests\/test_bicluster.py":[{"add":["5","from sklearn.model_selection import ParameterGrid"],"delete":["5","from sklearn.grid_search import ParameterGrid"]}],"sklearn\/model_selection\/__init__.py":[{"add":[],"delete":[]}],"sklearn\/ensemble\/tests\/test_bagging.py":[{"add":["23","from sklearn.model_selection import GridSearchCV, ParameterGrid","31","from sklearn.model_selection import train_test_split"],"delete":["23","from sklearn.grid_search import GridSearchCV, ParameterGrid","31","from sklearn.cross_validation import train_test_split"]}],"sklearn\/model_selection\/_validation.py":[{"add":[],"delete":[]}],"sklearn\/linear_model\/least_angle.py":[{"add":["24","from ..model_selection import check_cv","1081","        cv = check_cv(self.cv, classifier=False)","1091","            for train, test in cv.split(X, y))"],"delete":["24","from ..cross_validation import check_cv","1081","        cv = check_cv(self.cv, X, y, classifier=False)","1091","            for train, test in cv)"]}]}},"31ee1a8e6ed9b78408f7b807b5a0cab7a4fd35e5":{"changes":{"sklearn\/linear_model\/tests\/test_logistic.py":"MODIFY","sklearn\/linear_model\/logistic.py":"MODIFY","doc\/whats_new.rst":"MODIFY"},"diff":{"sklearn\/linear_model\/tests\/test_logistic.py":[{"add":["29","from sklearn.preprocessing import LabelEncoder","401","def test_multinomial_logistic_regression_string_inputs():","402","    # Test with string labels for LogisticRegression(CV)","403","    n_samples, n_features, n_classes = 50, 5, 3","404","    X_ref, y = make_classification(n_samples=n_samples, n_features=n_features,","405","                                   n_classes=n_classes, n_informative=3)","406","    y_str = LabelEncoder().fit(['bar', 'baz', 'foo']).inverse_transform(y)","407","    # For numerical labels, let y values be taken from set (-1, 0, 1)","408","    y = np.array(y) - 1","409","    # Test for string labels","410","    lr = LogisticRegression(solver='lbfgs', multi_class='multinomial')","411","    lr_cv = LogisticRegressionCV(solver='lbfgs', multi_class='multinomial')","412","    lr_str = LogisticRegression(solver='lbfgs', multi_class='multinomial')","413","    lr_cv_str = LogisticRegressionCV(solver='lbfgs', multi_class='multinomial')","414","","415","    lr.fit(X_ref, y)","416","    lr_cv.fit(X_ref, y)","417","    lr_str.fit(X_ref, y_str)","418","    lr_cv_str.fit(X_ref, y_str)","419","","420","    assert_array_almost_equal(lr.coef_, lr_str.coef_)","421","    assert_equal(sorted(lr_str.classes_), ['bar', 'baz', 'foo'])","422","    assert_array_almost_equal(lr_cv.coef_, lr_cv_str.coef_)","423","    assert_equal(sorted(lr_str.classes_), ['bar', 'baz', 'foo'])","424","    assert_equal(sorted(lr_cv_str.classes_), ['bar', 'baz', 'foo'])","425","","426","    # The predictions should be in original labels","427","    assert_equal(sorted(np.unique(lr_str.predict(X_ref))),","428","                 ['bar', 'baz', 'foo'])","429","    assert_equal(sorted(np.unique(lr_cv_str.predict(X_ref))),","430","                 ['bar', 'baz', 'foo'])","431","","432","    # Make sure class weights can be given with string labels","433","    lr_cv_str = LogisticRegression(","434","        solver='lbfgs', class_weight={'bar': 1, 'baz': 2, 'foo': 0},","435","        multi_class='multinomial').fit(X_ref, y_str)","436","    assert_equal(sorted(np.unique(lr_cv_str.predict(X_ref))), ['bar', 'baz'])","437","","438",""],"delete":[]}],"sklearn\/linear_model\/logistic.py":[{"add":["1558","        check_classification_targets(y)","1559","","1560","        class_weight = self.class_weight","1561","        if class_weight and not(isinstance(class_weight, dict) or","1562","                                class_weight in ['balanced', 'auto']):","1563","            # 'auto' is deprecated and will be removed in 0.19","1564","            raise ValueError(\"class_weight provided should be a \"","1565","                             \"dict or 'balanced'\")","1566","","1567","        # Encode for string labels","1568","        label_encoder = LabelEncoder().fit(y)","1569","        y = label_encoder.transform(y)","1570","        if isinstance(class_weight, dict):","1571","            class_weight = dict((label_encoder.transform([cls])[0], v)","1572","                                for cls, v in class_weight.items())","1573","","1574","        # The original class labels","1575","        classes = self.classes_ = label_encoder.classes_","1576","        encoded_labels = label_encoder.transform(label_encoder.classes_)","1587","        # Use the label encoded classes","1588","        n_classes = len(encoded_labels)","1593","                             \" class: %r\" % classes[0])","1594","","1599","            encoded_labels = encoded_labels[1:]","1600","            classes = classes[1:]","1605","            iter_encoded_labels = iter_classes = [None]","1606","        else:","1607","            iter_encoded_labels = encoded_labels","1608","            iter_classes = classes","1611","        if class_weight in (\"auto\", \"balanced\"):","1612","            class_weight = compute_class_weight(class_weight,","1613","                                                np.arange(len(self.classes_)),","1614","                                                y)","1615","            class_weight = dict(enumerate(class_weight))","1635","            for label in iter_encoded_labels","1666","        self.coefs_paths_ = dict(zip(classes, coefs_paths))","1668","        self.scores_ = dict(zip(classes, scores))","1679","        for index, (cls, encoded_label) in enumerate(","1680","                zip(iter_classes, iter_encoded_labels)):","1681","","1683","                # The scores_ \/ coefs_paths_ dict have unencoded class","1684","                # labels as their keys","1685","                scores = self.scores_[cls]","1686","                coefs_paths = self.coefs_paths_[cls]","1699","                # Note that y is label encoded and hence pos_class must be","1700","                # the encoded label \/ None (for 'multinomial')","1702","                    X, y, pos_class=encoded_label, Cs=[C_], solver=self.solver,"],"delete":["0","","30","from ..exceptions import DataConversionWarning","927","    # To deal with object dtypes, we need to convert into an array of floats.","928","    y_test = check_array(y_test, dtype=np.float64, ensure_2d=False)","929","","1569","        check_classification_targets(y)","1570","","1571","        if y.ndim == 2 and y.shape[1] == 1:","1572","            warnings.warn(","1573","                \"A column-vector y was passed when a 1d array was\"","1574","                \" expected. Please change the shape of y to \"","1575","                \"(n_samples, ), for example using ravel().\",","1576","                DataConversionWarning)","1577","            y = np.ravel(y)","1578","","1579","        check_consistent_length(X, y)","1580","","1585","        self._enc = LabelEncoder()","1586","        self._enc.fit(y)","1587","","1588","        labels = self.classes_ = np.unique(y)","1589","        n_classes = len(labels)","1594","                             \" class: %r\" % self.classes_[0])","1599","            labels = labels[1:]","1603","        iter_labels = labels","1605","            iter_labels = [None]","1606","","1607","        if self.class_weight and not(isinstance(self.class_weight, dict) or","1608","                                     self.class_weight in","1609","                                     ['balanced', 'auto']):","1610","            # 'auto' is deprecated and will be removed in 0.19","1611","            raise ValueError(\"class_weight provided should be a \"","1612","                             \"dict or 'balanced'\")","1615","        if self.class_weight in (\"auto\", \"balanced\"):","1616","            classes = np.unique(y)","1617","            class_weight = compute_class_weight(self.class_weight, classes, y)","1618","            class_weight = dict(zip(classes, class_weight))","1619","        else:","1620","            class_weight = self.class_weight","1640","            for label in iter_labels","1671","        self.coefs_paths_ = dict(zip(labels, coefs_paths))","1673","        self.scores_ = dict(zip(labels, scores))","1684","        for index, label in enumerate(iter_labels):","1686","                scores = self.scores_[label]","1687","                coefs_paths = self.coefs_paths_[label]","1701","                    X, y, pos_class=label, Cs=[C_], solver=self.solver,"]}],"doc\/whats_new.rst":[{"add":["99","   - :class:`sklearn.linear_model.LogisticRegressionCV` now correctly handles","100","     string labels. :issue:`5874` by `Raghav RV`_.","101","","102",""],"delete":[]}]}},"bbb1e3b6e9af85d04a01465abde7620e2ac5a77b":{"changes":{"doc\/whats_new\/v0.23.rst":"MODIFY","sklearn\/metrics\/_plot\/precision_recall_curve.py":"MODIFY","sklearn\/metrics\/_plot\/tests\/test_plot_precision_recall.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.23.rst":[{"add":["254","- |Fix| Fixed a bug in :func:`metrics.plot_precision_recall_curve` where the","255","  name of the estimator was passed in the","256","  :class:`metrics.PrecisionRecallDisplay` instead of the parameter `name`. It","257","  results in a different plot when calling","258","  :meth:`metrics.PrecisionRecallDisplay.plot` for the subsequent times.","259","  :pr:`#16505` by :user:`Guillaume Lemaitre <glemaitre>`.","260",""],"delete":[]}],"sklearn\/metrics\/_plot\/precision_recall_curve.py":[{"add":["143","    classification_error = (\"{} should be a binary classifier\".format(","165","    name = name if name is not None else estimator.__class__.__name__","166","    viz = PrecisionRecallDisplay(","167","        precision=precision, recall=recall,","168","        average_precision=average_precision, estimator_name=name","169","    )"],"delete":["143","    classification_error = (\"{} should be a binary classifer\".format(","165","    viz = PrecisionRecallDisplay(precision, recall, average_precision,","166","                                 estimator.__class__.__name__)"]}],"sklearn\/metrics\/_plot\/tests\/test_plot_precision_recall.py":[{"add":["39","    msg = \"DecisionTreeClassifier should be a binary classifier\"","44","    msg = \"DecisionTreeRegressor should be a binary classifier\"","155","","156","","157","def test_plot_precision_recall_curve_estimator_name_multiple_calls(pyplot):","158","    # non-regression test checking that the `name` used when calling","159","    # `plot_roc_curve` is used as well when calling `disp.plot()`","160","    X, y = make_classification(n_classes=2, n_samples=50, random_state=0)","161","    clf_name = \"my hand-crafted name\"","162","    clf = LogisticRegression().fit(X, y)","163","    disp = plot_precision_recall_curve(clf, X, y, name=clf_name)","164","    assert disp.estimator_name == clf_name","165","    pyplot.close(\"all\")","166","    disp.plot()","167","    assert clf_name in disp.line_.get_label()","168","    pyplot.close(\"all\")","169","    clf_name = \"another_name\"","170","    disp.plot(name=clf_name)","171","    assert clf_name in disp.line_.get_label()"],"delete":["39","    msg = \"DecisionTreeClassifier should be a binary classifer\"","44","    msg = \"DecisionTreeRegressor should be a binary classifer\""]}]}},"7243cc3687073e0a98f00d3dd6bc57007bf401ef":{"changes":{"sklearn\/covariance\/shrunk_covariance_.py":"MODIFY","sklearn\/covariance\/empirical_covariance_.py":"MODIFY"},"diff":{"sklearn\/covariance\/shrunk_covariance_.py":[{"add":["73","        If True, data will not be centered before computation.","76","        If False, data will be centered before computation.","177","        If True, data will not be centered before computation.","180","        If False, data will be centered before computation.","272","        If True, data will not be centered before computation.","275","        If False, data will be centered before computation.","341","        If True, data will not be centered before computation.","344","        If False (default), data will be centered before computation.","450","      If True, data will not be centered before computation.","453","      If False, data will be centered before computation.","527","        If True, data will not be centered before computation.","530","        If False (default), data will be centered before computation."],"delete":["73","        If True, data are not centered before computation.","76","        If False, data are centered before computation.","177","        If True, data are not centered before computation.","180","        If False, data are centered before computation.","272","        If True, data are not centered before computation.","275","        If False, data are centered before computation.","341","        If True, data are not centered before computation.","344","        If False (default), data are centered before computation.","450","      If True, data are not centered before computation.","453","      If False, data are centered before computation.","527","        If True, data are not centered before computation.","530","        If False (default), data are centered before computation."]}],"sklearn\/covariance\/empirical_covariance_.py":[{"add":["58","        If True, data will not be centered before computation.","61","        If False, data will be centered before computation."],"delete":["58","        If True, data are not centered before computation.","61","        If False, data are centered before computation."]}]}},"bb1299b534887964cde73b38d28aa7bdeec778a4":{"changes":{"sklearn\/linear_model\/tests\/test_least_angle.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/linear_model\/randomized_l1.py":"MODIFY","sklearn\/linear_model\/tests\/test_randomized_l1.py":"MODIFY","sklearn\/linear_model\/least_angle.py":"MODIFY"},"diff":{"sklearn\/linear_model\/tests\/test_least_angle.py":[{"add":["174","def test_lars_precompute():","175","    # Check for different values of precompute","176","    X, y = diabetes.data, diabetes.target","177","    G = np.dot(X.T, X)","178","    for classifier in [linear_model.Lars, linear_model.LarsCV,","179","                       linear_model.LassoLarsIC]:","180","        clf = classifier(precompute=G)","181","        output_1 = ignore_warnings(clf.fit)(X, y).coef_","182","        for precompute in [True, False, 'auto', None]:","183","            clf = classifier(precompute=precompute)","184","            output_2 = clf.fit(X, y).coef_","185","            assert_array_almost_equal(output_1, output_2, decimal=8)","186","","187",""],"delete":[]}],"doc\/whats_new.rst":[{"add":["380","   - Fixed a bug in :class:`linear_model.RandomizedLasso`,","381","     :class:`linear_model.Lars`, :class:`linear_model.LarsLasso`,","382","     :class:`linear_model.LarsCV` and :class:`linear_model.LarsLassoCV`,","383","     where the parameter ``precompute`` were not used consistently accross","384","     classes, and some values proposed in the docstring could raise errors.","385","     :issue:`5359` by `Tom Dupre la Tour`_.","386",""],"delete":[]}],"sklearn\/linear_model\/randomized_l1.py":[{"add":["159","","233","    precompute : True | False | 'auto' | array-like","234","        Whether to use a precomputed Gram matrix to speed up calculations.","235","        If set to 'auto' let us decide.","236","        The Gram matrix can also be passed as argument, but it will be used","237","        only for the selection of parameter alpha, if alpha is 'aic' or 'bic'.","346","","347","        precompute = self.precompute","348","        # A precomputed Gram array is useless, since _randomized_lasso","349","        # change X a each iteration","350","        if hasattr(precompute, '__array__'):","351","            precompute = 'auto'","352","        assert precompute in (True, False, None, 'auto')","355","                                       precompute=precompute)"],"delete":["232","    precompute : True | False | 'auto'","233","        Whether to use a precomputed Gram matrix to speed up","234","        calculations. If set to 'auto' let us decide. The Gram","235","        matrix can also be passed as argument.","336","        assert self.precompute in (True, False, None, 'auto')","347","                                       precompute=self.precompute)"]}],"sklearn\/linear_model\/tests\/test_randomized_l1.py":[{"add":["61","    n_resampling = 20","65","                          scaling=scaling, n_resampling=n_resampling,","72","                          scaling=scaling, n_resampling=n_resampling,","96","                          scaling=scaling, n_resampling=100)","107","def test_randomized_lasso_precompute():","108","    # Check randomized lasso for different values of precompute","109","    n_resampling = 20","110","    alpha = 1","111","    random_state = 42","112","","113","    G = np.dot(X.T, X)","114","","115","    clf = RandomizedLasso(alpha=alpha, random_state=random_state,","116","                          precompute=G, n_resampling=n_resampling)","117","    feature_scores_1 = clf.fit(X, y).scores_","118","","119","    for precompute in [True, False, None, 'auto']:","120","        clf = RandomizedLasso(alpha=alpha, random_state=random_state,","121","                              precompute=precompute, n_resampling=n_resampling)","122","        feature_scores_2 = clf.fit(X, y).scores_","123","        assert_array_equal(feature_scores_1, feature_scores_2)","124","","125",""],"delete":["64","                          scaling=scaling,","71","                          scaling=scaling,","95","                          scaling=scaling)"]}],"sklearn\/linear_model\/least_angle.py":[{"add":["172","    if Gram is None or Gram is False:","173","        Gram = None","179","","180","    elif isinstance(Gram, string_types) and Gram == 'auto' or Gram is True:","181","        if Gram is True or X.shape[0] > X.shape[1]:","183","        else:","184","            Gram = None","603","    def _get_gram(self, precompute, X, y):","604","        if (not hasattr(precompute, '__array__')) and (","605","                (precompute is True) or","606","                (precompute == 'auto' and X.shape[0] > X.shape[1]) or","607","                (precompute == 'auto' and y.shape[1] > 1)):","608","            precompute = np.dot(X.T, X)","609","","610","        return precompute","626","        Gram = self._get_gram(self.precompute, X, y)","996","    precompute : True | False | 'auto'","998","        calculations. If set to ``'auto'`` let us decide. The Gram matrix","999","        cannot be passed as argument since we will use only subsets of X.","1104","        # As we use cross-validation, the Gram matrix is not precomputed here","1105","        Gram = self.precompute","1106","        if hasattr(Gram, '__array__'):","1107","            warnings.warn(\"Parameter 'precompute' cannot be an array in \"","1108","                          \"%s. Automatically switch to 'auto' instead.\"","1109","                          % self.__class__.__name__)","1110","            Gram = 'auto'","1214","    precompute : True | False | 'auto'","1216","        calculations. If set to ``'auto'`` let us decide. The Gram matrix","1217","        cannot be passed as argument since we will use only subsets of X.","1473","        Gram = self.precompute"],"delete":["172","    if Gram is None:","178","    elif isinstance(Gram, string_types) and Gram == 'auto':","179","        Gram = None","180","        if X.shape[0] > X.shape[1]:","600","    def _get_gram(self):","601","        # precompute if n_samples > n_features","602","        precompute = self.precompute","603","        if hasattr(precompute, '__array__'):","604","            Gram = precompute","605","        elif precompute == 'auto':","606","            Gram = 'auto'","607","        else:","608","            Gram = None","609","        return Gram","625","        precompute = self.precompute","626","        if not hasattr(precompute, '__array__') and (","627","                precompute is True or","628","                (precompute == 'auto' and X.shape[0] > X.shape[1]) or","629","                (precompute == 'auto' and y.shape[1] > 1)):","630","            Gram = np.dot(X.T, X)","631","        else:","632","            Gram = self._get_gram()","1002","    precompute : True | False | 'auto' | array-like","1004","        calculations. If set to ``'auto'`` let us decide. The Gram","1005","        matrix can also be passed as argument.","1110","        Gram = 'auto' if self.precompute else None","1214","    precompute : True | False | 'auto' | array-like","1216","        calculations. If set to ``'auto'`` let us decide. The Gram","1217","        matrix can also be passed as argument.","1473","        Gram = self._get_gram()"]}]}},"ca78d75e751d8d57c08fb48fc8f437d18d454e43":{"changes":{"sklearn\/datasets\/_openml.py":"MODIFY","sklearn\/tree\/_classes.py":"MODIFY","sklearn\/datasets\/_rcv1.py":"MODIFY","doc\/datasets\/index.rst":"MODIFY","sklearn\/datasets\/_base.py":"MODIFY","sklearn\/datasets\/_california_housing.py":"MODIFY","sklearn\/datasets\/_species_distributions.py":"MODIFY","sklearn\/ensemble\/_stacking.py":"MODIFY","sklearn\/inspection\/_permutation_importance.py":"MODIFY","sklearn\/utils\/__init__.py":"MODIFY","sklearn\/datasets\/_covtype.py":"MODIFY","sklearn\/datasets\/_lfw.py":"MODIFY","sklearn\/ensemble\/_voting.py":"MODIFY","sklearn\/datasets\/_olivetti_faces.py":"MODIFY","sklearn\/pipeline.py":"MODIFY","sklearn\/datasets\/_kddcup99.py":"MODIFY","sklearn\/compose\/_column_transformer.py":"MODIFY","sklearn\/datasets\/_twenty_newsgroups.py":"MODIFY"},"diff":{"sklearn\/datasets\/_openml.py":[{"add":["581","    data : :class:`~sklearn.utils.Bunch`","582","        Dictionary-like object, with the following attributes."],"delete":["581","    data : Bunch","582","        Dictionary-like object, with attributes:"]}],"sklearn\/tree\/_classes.py":[{"add":["547","        ccp_path : :class:`~sklearn.utils.Bunch`","548","            Dictionary-like object, with the following attributes."],"delete":["547","        ccp_path : Bunch","548","            Dictionary-like object, with attributes:"]}],"sklearn\/datasets\/_rcv1.py":[{"add":["129","    dataset : :class:`~sklearn.utils.Bunch`","130","        Dictionary-like object, with the following attributes.","132","        data : scipy csr array, dtype np.float64, shape (804414, 47236)","133","            The array has 0.16% of non zero values.","134","        target : scipy csr array, dtype np.uint8, shape (804414, 103)","135","            Each sample has a value of 1 in its categories, and 0 in others.","136","            The array has 3.15% of non zero values.","137","        sample_id : numpy array, dtype np.uint32, shape (804414,)","138","            Identification number of each sample, as ordered in dataset.data.","139","        target_names : numpy array, dtype object, length (103)","140","            Names of each target (RCV1 topics), as ordered in dataset.target.","141","        DESCR : string","142","            Description of the RCV1 dataset."],"delete":["129","    dataset : dict-like object with the following attributes:","131","    dataset.data : scipy csr array, dtype np.float64, shape (804414, 47236)","132","        The array has 0.16% of non zero values.","133","","134","    dataset.target : scipy csr array, dtype np.uint8, shape (804414, 103)","135","        Each sample has a value of 1 in its categories, and 0 in others.","136","        The array has 3.15% of non zero values.","137","","138","    dataset.sample_id : numpy array, dtype np.uint32, shape (804414,)","139","        Identification number of each sample, as ordered in dataset.data.","140","","141","    dataset.target_names : numpy array, dtype object, length (103)","142","        Names of each target (RCV1 topics), as ordered in dataset.target.","143","","144","    dataset.DESCR : string","145","        Description of the RCV1 dataset."]}],"doc\/datasets\/index.rst":[{"add":["23","There are three main kinds of dataset interfaces that can be used to get","25","","26","**The dataset loaders.** They can be used to load small standard datasets,","27","described in the :ref:`toy_datasets` section.","32","Both loaders and fetchers functions return a :class:`sklearn.utils.Bunch`","33","object holding at least two items:","34","an array of shape ``n_samples`` * ``n_features`` with","35","key ``data`` (except for 20newsgroups) and a numpy array of","38","The Bunch object is a dictionary that exposes its keys are attributes.","39","For more information about Bunch object, see :class:`sklearn.utils.Bunch`:","40","","42","to be a tuple containing only the data and the target, by setting the","45","The datasets also contain a full description in their ``DESCR`` attribute and","46","some contain ``feature_names`` and ``target_names``. See the dataset","47","descriptions below for details.","49","**The dataset generation functions.** They can be used to generate controlled","56","In addition, there are also miscellaneous tools to load datasets of other","58","section.","65","scikit-learn comes with a few small standard datasets that do not require to","66","download any file from some external website.","491","Here are some recommended ways to load standard columnar data into a","492","format usable by scikit-learn:","493","","494","* `pandas.io <https:\/\/pandas.pydata.org\/pandas-docs\/stable\/io.html>`_","499","* `scipy.io <https:\/\/docs.scipy.org\/doc\/scipy\/reference\/io.html>`_","500","  specializes in binary formats often used in scientific computing","514","  `Imageio <https:\/\/imageio.readthedocs.io\/en\/latest\/userapi.html>`_","516","* `scipy.io.wavfile.read","517","  <https:\/\/docs.scipy.org\/doc\/scipy-0.14.0\/reference\/generated\/scipy.io.wavfile.read.html>`_","520","Categorical (or nominal) features stored as strings (common in pandas DataFrames)","525","Note: if you manage your own numerical data it is recommended to use an","527","such as H5Py, PyTables and pandas provides a Python interface for reading and"],"delete":["23","There are three main kinds of dataset interfaces that can be used to get ","25","  ","26","**The dataset loaders.** They can be used to load small standard datasets, ","27","described in the :ref:`toy_datasets` section.  ","32","Both loaders and fetchers functions return a dictionary-like object holding ","33","at least two items: an array of shape ``n_samples`` * ``n_features`` with ","34","key ``data`` (except for 20newsgroups) and a numpy array of ","38","to be a tuple containing only the data and the target, by setting the ","41","The datasets also contain a full description in their ``DESCR`` attribute and ","42","some contain ``feature_names`` and ``target_names``. See the dataset ","43","descriptions below for details.  ","45","**The dataset generation functions.** They can be used to generate controlled ","52","In addition, there are also miscellaneous tools to load datasets of other ","54","section. ","61","scikit-learn comes with a few small standard datasets that do not require to ","62","download any file from some external website. ","486"," ","487","Here are some recommended ways to load standard columnar data into a ","488","format usable by scikit-learn: ","490","* `pandas.io <https:\/\/pandas.pydata.org\/pandas-docs\/stable\/io.html>`_ ","495","* `scipy.io <https:\/\/docs.scipy.org\/doc\/scipy\/reference\/io.html>`_ ","496","  specializes in binary formats often used in scientific computing ","510","  `Imageio <https:\/\/imageio.readthedocs.io\/en\/latest\/userapi.html>`_ ","512","* `scipy.io.wavfile.read ","513","  <https:\/\/docs.scipy.org\/doc\/scipy-0.14.0\/reference\/generated\/scipy.io.wavfile.read.html>`_ ","516","Categorical (or nominal) features stored as strings (common in pandas DataFrames) ","521","Note: if you manage your own numerical data it is recommended to use an ","523","such as H5Py, PyTables and pandas provides a Python interface for reading and "]}],"sklearn\/datasets\/_base.py":[{"add":["165","    data : :class:`~sklearn.utils.Bunch`","166","        Dictionary-like object, with the following attributes.","167","","168","        data : list of str","169","            Only present when `load_content=True`.","170","            The raw text data to learn.","171","        target : ndarray","172","            The target labels (integer index).","173","        target_names : list","174","            The names of target classes.","175","        DESCR : str","176","            The full description of the dataset.","177","        filenames: ndarray","178","            The filenames holding the dataset.","305","    data : :class:`~sklearn.utils.Bunch`","306","        Dictionary-like object, with the following attributes.","419","    data : :class:`~sklearn.utils.Bunch`","420","        Dictionary-like object, with the following attributes.","531","    data : :class:`~sklearn.utils.Bunch`","532","        Dictionary-like object, with the following attributes.","655","    data : :class:`~sklearn.utils.Bunch`","656","        Dictionary-like object, with the following attributes.","769","    data : :class:`~sklearn.utils.Bunch`","770","        Dictionary-like object, with the following attributes.","863","    data : :class:`~sklearn.utils.Bunch`","864","        Dictionary-like object, with the following attributes.","953","    data : :class:`~sklearn.utils.Bunch`","954","        Dictionary-like object, with the following attributes.","955","","956","        data : ndarray of shape (506, 13)","957","            The data matrix.","958","        target : ndarray of shape (506, )","959","            The regression target.","960","        filename : str","961","            The physical location of boston csv dataset.","962","","963","            .. versionadded:: 0.20","964","        DESCR : str","965","            The full description of the dataset.","966","        feature_names : ndarray","967","            The names of features","1026","    data : :class:`~sklearn.utils.Bunch`","1027","        Dictionary-like object, with the following attributes.","1028","","1029","        images : list of ndarray of shape (427, 640, 3)","1030","            The two sample image.","1031","        filenames : list","1032","            The filenames for the images.","1033","        DESCR : str","1034","            The full description of the dataset."],"delete":["165","    data : Bunch","166","        Dictionary-like object, the interesting attributes are: either","167","        data, the raw text data to learn, or 'filenames', the files","168","        holding it, 'target', the classification labels (integer index),","169","        'target_names', the meaning of the labels, and 'DESCR', the full","170","        description of the dataset.","297","    data : Bunch","298","        Dictionary-like object, with attributes:","411","    data : Bunch","412","        Dictionary-like object, with attributes:","523","    data : Bunch","524","        Dictionary-like object, with attributes:","647","    data : Bunch","648","        Dictionary-like object, with attributes:","761","    data : Bunch","762","        Dictionary-like object, with attributes:","855","    data : Bunch","856","        Dictionary-like object, with attributes:","945","    data : Bunch","946","        Dictionary-like object, the interesting attributes are:","947","        'data', the data to learn, 'target', the regression targets,","948","        'DESCR', the full description of the dataset,","949","        and 'filename', the physical location of boston","950","        csv dataset (added in version `0.20`).","1009","    data : Bunch","1010","        Dictionary-like object with the following attributes : 'images', the","1011","        two sample images, 'filenames', the file names for the images, and","1012","        'DESCR' the full description of the dataset."]}],"sklearn\/datasets\/_california_housing.py":[{"add":["89","    dataset : :class:`~sklearn.utils.Bunch`","90","        Dictionary-like object, with the following attributes.","92","        data : ndarray, shape (20640, 8)","93","            Each row corresponding to the 8 feature values in order.","94","            If ``as_frame`` is True, ``data`` is a pandas object.","95","        target : numpy array of shape (20640,)","96","            Each value corresponds to the average","97","            house value in units of 100,000.","98","            If ``as_frame`` is True, ``target`` is a pandas object.","99","        feature_names : list of length 8","100","            Array of ordered feature names used in the dataset.","101","        DESCR : string","102","            Description of the California housing dataset."],"delete":["89","    dataset : dict-like object with the following attributes:","91","    dataset.data : ndarray, shape [20640, 8]","92","        Each row corresponding to the 8 feature values in order.","93","        If ``as_frame`` is True, ``data`` is a pandas object.","94","","95","    dataset.target : numpy array of shape (20640,)","96","        Each value corresponds to the average house value in units of 100,000.","97","        If ``as_frame`` is True, ``target`` is a pandas object.","98","","99","    dataset.feature_names : array of length 8","100","        Array of ordered feature names used in the dataset.","101","","102","    dataset.DESCR : string","103","        Description of the California housing dataset."]}],"sklearn\/datasets\/_species_distributions.py":[{"add":["157","    data : :class:`~sklearn.utils.Bunch`","158","        Dictionary-like object, with the following attributes.","160","        coverages : array, shape = [14, 1592, 1212]","161","            These represent the 14 features measured","162","            at each point of the map grid.","163","            The latitude\/longitude values for the grid are discussed below.","164","            Missing data is represented by the value -9999.","165","        train : record array, shape = (1624,)","166","            The training points for the data.  Each point has three fields:","168","            - train['species'] is the species name","169","            - train['dd long'] is the longitude, in degrees","170","            - train['dd lat'] is the latitude, in degrees","171","        test : record array, shape = (620,)","172","            The test points for the data.  Same format as the training data.","173","        Nx, Ny : integers","174","            The number of longitudes (x) and latitudes (y) in the grid","175","        x_left_lower_corner, y_left_lower_corner : floats","176","            The (x,y) position of the lower-left corner, in degrees","177","        grid_size : float","178","            The spacing between points of the grid, in degrees"],"delete":["157","    The data is returned as a Bunch object with the following attributes:","159","    coverages : array, shape = [14, 1592, 1212]","160","        These represent the 14 features measured at each point of the map grid.","161","        The latitude\/longitude values for the grid are discussed below.","162","        Missing data is represented by the value -9999.","164","    train : record array, shape = (1624,)","165","        The training points for the data.  Each point has three fields:","166","","167","        - train['species'] is the species name","168","        - train['dd long'] is the longitude, in degrees","169","        - train['dd lat'] is the latitude, in degrees","170","","171","    test : record array, shape = (620,)","172","        The test points for the data.  Same format as the training data.","173","","174","    Nx, Ny : integers","175","        The number of longitudes (x) and latitudes (y) in the grid","176","","177","    x_left_lower_corner, y_left_lower_corner : floats","178","        The (x,y) position of the lower-left corner, in degrees","179","","180","    grid_size : float","181","        The spacing between points of the grid, in degrees"]}],"sklearn\/ensemble\/_stacking.py":[{"add":["325","    named_estimators_ : :class:`~sklearn.utils.Bunch`","573","    named_estimators_ : :class:`~sklearn.utils.Bunch`","576",""],"delete":["325","    named_estimators_ : Bunch","573","    named_estimators_ : Bunch"]}],"sklearn\/inspection\/_permutation_importance.py":[{"add":["88","    result : :class:`~sklearn.utils.Bunch`","89","        Dictionary-like object, with the following attributes."],"delete":["88","    result : Bunch","89","        Dictionary-like object, with attributes:"]}],"sklearn\/utils\/__init__.py":[{"add":["66","    Examples","67","    --------"],"delete":["66","","78",""]}],"sklearn\/datasets\/_covtype.py":[{"add":["83","    dataset : :class:`~sklearn.utils.Bunch`","84","        Dictionary-like object, with the following attributes.","86","        data : numpy array of shape (581012, 54)","87","            Each row corresponds to the 54 features in the dataset.","88","        target : numpy array of shape (581012,)","89","            Each value corresponds to one of","90","            the 7 forest covertypes with values","91","            ranging between 1 to 7.","92","        DESCR : str","93","            Description of the forest covertype dataset."],"delete":["83","    dataset : dict-like object with the following attributes:","85","    dataset.data : numpy array of shape (581012, 54)","86","        Each row corresponds to the 54 features in the dataset.","87","","88","    dataset.target : numpy array of shape (581012,)","89","        Each value corresponds to one of the 7 forest covertypes with values","90","        ranging between 1 to 7.","91","","92","    dataset.DESCR : string","93","        Description of the forest covertype dataset."]}],"sklearn\/datasets\/_lfw.py":[{"add":["274","    dataset : :class:`~sklearn.utils.Bunch`","275","        Dictionary-like object, with the following attributes.","277","        data : numpy array of shape (13233, 2914)","278","            Each row corresponds to a ravelled face image","279","            of original size 62 x 47 pixels.","280","            Changing the ``slice_`` or resize parameters will change the","281","            shape of the output.","282","        images : numpy array of shape (13233, 62, 47)","283","            Each row is a face image corresponding to one of the 5749 people in","284","            the dataset. Changing the ``slice_``","285","            or resize parameters will change the shape of the output.","286","        target : numpy array of shape (13233,)","287","            Labels associated to each face image.","288","            Those labels range from 0-5748 and correspond to the person IDs.","289","        DESCR : string","290","            Description of the Labeled Faces in the Wild (LFW) dataset.","447","    data : :class:`~sklearn.utils.Bunch`","448","        Dictionary-like object, with the following attributes.","450","        data : ndarray of shape (2200, 5828). Shape depends on ``subset``.","451","            Each row corresponds to 2 ravel'd face images","452","            of original size 62 x 47 pixels.","453","            Changing the ``slice_``, ``resize`` or ``subset`` parameters","454","            will change the shape of the output.","455","        pairs : ndarray of shape (2200, 2, 62, 47). Shape depends on ``subset``","456","            Each row has 2 face images corresponding","457","            to same or different person from the dataset","458","            containing 5749 people. Changing the ``slice_``,","459","            ``resize`` or ``subset`` parameters will change the shape of the","460","            output.","461","        target : numpy array of shape (2200,). Shape depends on ``subset``.","462","            Labels associated to each pair of images.","463","            The two label values being different persons or the same person.","464","        DESCR : string","465","            Description of the Labeled Faces in the Wild (LFW) dataset."],"delete":["274","    dataset : dict-like object with the following attributes:","276","    dataset.data : numpy array of shape (13233, 2914)","277","        Each row corresponds to a ravelled face image of original size 62 x 47","278","        pixels. Changing the ``slice_`` or resize parameters will change the","279","        shape of the output.","280","","281","    dataset.images : numpy array of shape (13233, 62, 47)","282","        Each row is a face image corresponding to one of the 5749 people in","283","        the dataset. Changing the ``slice_`` or resize parameters will change","284","        the shape of the output.","285","","286","    dataset.target : numpy array of shape (13233,)","287","        Labels associated to each face image. Those labels range from 0-5748","288","        and correspond to the person IDs.","289","","290","    dataset.DESCR : string","291","        Description of the Labeled Faces in the Wild (LFW) dataset.","448","    The data is returned as a Bunch object with the following attributes:","450","    data : numpy array of shape (2200, 5828). Shape depends on ``subset``.","451","        Each row corresponds to 2 ravel'd face images of original size 62 x 47","452","        pixels. Changing the ``slice_``, ``resize`` or ``subset`` parameters","453","        will change the shape of the output.","454","","455","    pairs : numpy array of shape (2200, 2, 62, 47). Shape depends on ``subset``","456","        Each row has 2 face images corresponding to same or different person","457","        from the dataset containing 5749 people. Changing the ``slice_``,","458","        ``resize`` or ``subset`` parameters will change the shape of the","459","        output.","460","","461","    target : numpy array of shape (2200,). Shape depends on ``subset``.","462","        Labels associated to each pair of images. The two label values being","463","        different persons or the same person.","464","","465","    DESCR : string","466","        Description of the Labeled Faces in the Wild (LFW) dataset."]}],"sklearn\/ensemble\/_voting.py":[{"add":["144","    named_estimators_ : :class:`~sklearn.utils.Bunch`","147",""],"delete":["144","    named_estimators_ : Bunch"]}],"sklearn\/datasets\/_olivetti_faces.py":[{"add":["79","    data : :class:`~sklearn.utils.Bunch`","80","        Dictionary-like object, with the following attributes.","81","","82","        data: ndarray, shape (400, 4096)","83","            Each row corresponds to a ravelled","84","            face image of original size 64 x 64 pixels.","85","        images : ndarray, shape (400, 64, 64)","86","            Each row is a face image","87","            corresponding to one of the 40 subjects of the dataset.","88","        target : ndarray, shape (400,)","89","            Labels associated to each face image.","90","            Those labels are ranging from 0-39 and correspond to the","91","            Subject IDs.","92","        DESCR : str","93","            Description of the modified Olivetti Faces Dataset."],"delete":["79","    bunch : Bunch object with the following attributes:","80","        - data: ndarray, shape (400, 4096). Each row corresponds to a ravelled","81","          face image of original size 64 x 64 pixels.","82","        - images : ndarray, shape (400, 64, 64). Each row is a face image","83","          corresponding to one of the 40 subjects of the dataset.","84","        - target : ndarray, shape (400,). Labels associated to each face image.","85","          Those labels are ranging from 0-39 and correspond to the","86","          Subject IDs.","87","        - DESCR : string. Description of the modified Olivetti Faces Dataset."]}],"sklearn\/pipeline.py":[{"add":["74","    named_steps : :class:`~sklearn.utils.Bunch`","75","        Dictionary-like object, with the following attributes."],"delete":["74","    named_steps : bunch object, a dictionary with attribute access"]}],"sklearn\/datasets\/_kddcup99.py":[{"add":["98","    data : :class:`~sklearn.utils.Bunch`","99","        Dictionary-like object, with the following attributes.","100","","101","        data : ndarray of shape (494021, 41)","102","            The data matrix to learn.","103","        target : ndarray of shape (494021,)","104","            The regression target for each sample.","105","        DESCR : str","106","            The full description of the dataset.","196","    dataset : :class:`~sklearn.utils.Bunch`","197","        Dictionary-like object, with the following attributes.","198","","199","        data : numpy array of shape (494021, 41)","201","        target : numpy array of shape (494021,)","204","        DESCR : string"],"delete":["98","    data : Bunch","99","        Dictionary-like object, the interesting attributes are:","100","         - 'data', the data to learn.","101","         - 'target', the regression target for each sample.","102","         - 'DESCR', a description of the dataset.","192","    dataset : dict-like object with the following attributes:","193","        dataset.data : numpy array of shape (494021, 41)","195","        dataset.target : numpy array of shape (494021,)","198","        dataset.DESCR : string"]}],"sklearn\/compose\/_column_transformer.py":[{"add":["126","    named_transformers_ : :class:`~sklearn.utils.Bunch`"],"delete":["126","    named_transformers_ : Bunch"]}],"sklearn\/datasets\/_twenty_newsgroups.py":[{"add":["216","    bunch : :class:`~sklearn.utils.Bunch`","217","        Dictionary-like object, with the following attributes.","218","","219","        data : list, length [n_samples]","220","            The data list to learn.","221","        target: array, shape [n_samples]","222","            The target labels.","223","        filenames: list, length [n_samples]","224","            The path to the location of the data.","225","        DESCR: str","226","            The full description of the dataset.","227","        target_names: list, length [n_classes]","228","            The names of target classes.","392","    bunch : :class:`~sklearn.utils.Bunch`","393","        Dictionary-like object, with the following attributes.","394","","395","        data: sparse matrix, shape [n_samples, n_features]","396","            The data matrix to learn.","397","        target: array, shape [n_samples]","398","            The target labels.","399","        target_names: list, length [n_classes]","400","            The names of target classes.","401","        DESCR: str","402","            The full description of the dataset."],"delete":["216","    bunch : Bunch object with the following attribute:","217","        - data: list, length [n_samples]","218","        - target: array, shape [n_samples]","219","        - filenames: list, length [n_samples]","220","        - DESCR: a description of the dataset.","221","        - target_names: a list of categories of the returned data,","222","          length [n_classes]. This depends on the `categories` parameter.","386","    bunch : Bunch object with the following attribute:","387","        - bunch.data: sparse matrix, shape [n_samples, n_features]","388","        - bunch.target: array, shape [n_samples]","389","        - bunch.target_names: a list of categories of the returned data,","390","          length [n_classes].","391","        - bunch.DESCR: a description of the dataset."]}]}},"da0bdab4ac6841533e136c6ba24dd3bc0bb75188":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/inspection\/partial_dependence.py":"MODIFY","sklearn\/inspection\/tests\/test_partial_dependence.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["113",":mod:`sklearn.inspection`","114",".........................","115","","116","- |Fix| Fixed a bug in :func:`inspection.partial_dependence` to only check","117","  classifier and not regressor for the multiclass-multioutput case.","118","  :pr:`14309` by :user:`Guillaume Lemaitre <glemaitre>`.","119",""],"delete":[]}],"sklearn\/inspection\/partial_dependence.py":[{"add":["288","    if is_classifier(estimator):","289","        if not hasattr(estimator, 'classes_'):","290","            raise ValueError(","291","                \"'estimator' parameter must be a fitted estimator\"","292","            )","293","        if isinstance(estimator.classes_[0], np.ndarray):","294","            raise ValueError(","295","                'Multiclass-multioutput estimators are not supported'","296","            )"],"delete":["288","    if (hasattr(estimator, 'classes_') and","289","            isinstance(estimator.classes_[0], np.ndarray)):","290","        raise ValueError('Multiclass-multioutput estimators are not supported')"]}],"sklearn\/inspection\/tests\/test_partial_dependence.py":[{"add":["23","from sklearn.tree import DecisionTreeRegressor","61","    (DecisionTreeRegressor, 'brute', regression_data),","291","        # simulate that we have some classes","292","        self.classes_ = [0, 1]"],"delete":["263","@pytest.mark.filterwarnings('ignore:The default value of ')  # 0.22"]}]}},"88b49e5caf01ee8e6d803f8daca2bf1666219b0b":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/neighbors\/tests\/test_neighbors.py":"MODIFY","sklearn\/neighbors\/base.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["25","- |Fix| force the parallelism backend to :code:`threading` for","26","  :class:`neighbors.KDTree` and :class:`neighbors.BallTree` in Python 2.7 to","27","  avoid pickling errors caused by the serialization of their methods.","28","  :issue:`12171` by :user:`Thomas Moreau <tomMoral>`","29",""],"delete":[]}],"sklearn\/neighbors\/tests\/test_neighbors.py":[{"add":["29","from sklearn.externals.joblib import parallel_backend","30","","1320","@pytest.mark.parametrize('backend', ['loky', 'multiprocessing', 'threading'])","1321","@pytest.mark.parametrize('algorithm', ALGORITHMS)","1322","def test_knn_forcing_backend(backend, algorithm):","1323","    # Non-regression test which ensure the knn methods are properly working","1324","    # even when forcing the global joblib backend.","1325","    with parallel_backend(backend):","1326","        X, y = datasets.make_classification(n_samples=30, n_features=5,","1327","                                            n_redundant=0, random_state=0)","1328","        X_train, X_test, y_train, y_test = train_test_split(X, y)","1329","","1330","        clf = neighbors.KNeighborsClassifier(n_neighbors=3,","1331","                                             algorithm=algorithm,","1332","                                             n_jobs=3)","1333","        clf.fit(X_train, y_train)","1334","        clf.predict(X_test)","1335","        clf.kneighbors(X_test)","1336","        clf.kneighbors_graph(X_test, mode='distance').toarray()","1337","","1338",""],"delete":[]}],"sklearn\/neighbors\/base.py":[{"add":["11","import sys","432","            if (sys.version_info < (3,) or","433","                    LooseVersion(joblib_version) < LooseVersion('0.12')):"],"delete":["431","            if LooseVersion(joblib_version) < LooseVersion('0.12'):"]}]}},"553b5fb8f84ba05c8397f26dd079deece2b05029":{"changes":{"examples\/manifold\/plot_t_sne_perplexity.py":"MODIFY","examples\/manifold\/plot_compare_methods.py":"MODIFY"},"diff":{"examples\/manifold\/plot_t_sne_perplexity.py":[{"add":["2","t-SNE: The effect of various perplexity values on the shape"],"delete":["2"," t-SNE: The effect of various perplexity values on the shape"]}],"examples\/manifold\/plot_compare_methods.py":[{"add":["2","Comparison of Manifold Learning methods"],"delete":["2"," Comparison of Manifold Learning methods"]}]}},"b3e122a0b5d7d2bb7770c4ab30557b05cb550f46":{"changes":{"sklearn\/ensemble\/voting_classifier.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/ensemble\/tests\/test_voting_classifier.py":"MODIFY"},"diff":{"sklearn\/ensemble\/voting_classifier.py":[{"add":["21","from ..externals.joblib import Parallel, delayed","22","from ..utils.validation import has_fit_parameter, check_is_fitted","23","","24","","25","def _parallel_fit_estimator(estimator, X, y, sample_weight):","26","    \"\"\"Private function used to fit an estimator within a job.\"\"\"","27","    if sample_weight is not None:","28","        estimator.fit(X, y, sample_weight)","29","    else:","30","        estimator.fit(X, y)","31","    return estimator","59","    n_jobs : int, optional (default=1)","60","        The number of jobs to run in parallel for ``fit``.","61","        If -1, then the number of jobs is set to the number of cores.","62","","102","    def __init__(self, estimators, voting='hard', weights=None, n_jobs=1):","107","        self.n_jobs = n_jobs","109","    def fit(self, X, y, sample_weight=None):","121","        sample_weight : array-like, shape = [n_samples] or None","122","            Sample weights. If None, then samples are equally weighted.","123","            Note that this is supported only if all underlying estimators","124","            support sample weights.","125","","148","        if sample_weight is not None:","149","            for name, step in self.estimators:","150","                if not has_fit_parameter(step, 'sample_weight'):","151","                    raise ValueError('Underlying estimator \\'%s\\' does not support'","152","                                     ' sample weights.' % name)","153","","159","        transformed_y = self.le_.transform(y)","160","","161","        self.estimators_ = Parallel(n_jobs=self.n_jobs)(","162","                delayed(_parallel_fit_estimator)(clone(clf), X, transformed_y,","163","                    sample_weight)","164","                    for _, clf in self.estimators)","244","          array-like = [n_samples, n_classifiers]"],"delete":["21","from ..utils.validation import check_is_fitted","88","    def __init__(self, estimators, voting='hard', weights=None):","89","","95","    def fit(self, X, y):","134","        for name, clf in self.estimators:","135","            fitted_clf = clone(clf).fit(X, self.le_.transform(y))","136","            self.estimators_.append(fitted_clf)","216","          array-like = [n_classifiers, n_samples]"]}],"doc\/whats_new.rst":[{"add":["283","   - Added ``n_jobs`` and ``sample_weights`` parameters for :class:`VotingClassifier`","284","     to fit underlying estimators in parallel.","285","     (`#5805 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/5805>`_)","286","     By `Ibraim Ganiev`_.","287",""],"delete":[]}],"sklearn\/ensemble\/tests\/test_voting_classifier.py":[{"add":["0","\"\"\"Testing for the VotingClassifier\"\"\"","3","from sklearn.utils.testing import assert_almost_equal, assert_array_equal","17","from sklearn.neighbors import KNeighborsClassifier","210","","211","","212","def test_parallel_predict():","213","    \"\"\"Check parallel backend of VotingClassifier on toy dataset.\"\"\"","214","    clf1 = LogisticRegression(random_state=123)","215","    clf2 = RandomForestClassifier(random_state=123)","216","    clf3 = GaussianNB()","217","    X = np.array([[-1.1, -1.5], [-1.2, -1.4], [-3.4, -2.2], [1.1, 1.2]])","218","    y = np.array([1, 1, 2, 2])","219","","220","    eclf1 = VotingClassifier(estimators=[","221","        ('lr', clf1), ('rf', clf2), ('gnb', clf3)],","222","        voting='soft',","223","        n_jobs=1).fit(X, y)","224","    eclf2 = VotingClassifier(estimators=[","225","        ('lr', clf1), ('rf', clf2), ('gnb', clf3)],","226","        voting='soft',","227","        n_jobs=2).fit(X, y)","228","","229","    assert_array_equal(eclf1.predict(X), eclf2.predict(X))","230","    assert_array_equal(eclf1.predict_proba(X), eclf2.predict_proba(X))","231","","232","","233","def test_sample_weight():","234","    \"\"\"Tests sample_weight parameter of VotingClassifier\"\"\"","235","    clf1 = LogisticRegression(random_state=123)","236","    clf2 = RandomForestClassifier(random_state=123)","237","    clf3 = SVC(probability=True, random_state=123)","238","    eclf1 = VotingClassifier(estimators=[","239","        ('lr', clf1), ('rf', clf2), ('svc', clf3)],","240","        voting='soft').fit(X, y, sample_weight=np.ones((len(y),)))","241","    eclf2 = VotingClassifier(estimators=[","242","        ('lr', clf1), ('rf', clf2), ('svc', clf3)],","243","        voting='soft').fit(X, y)","244","    assert_array_equal(eclf1.predict(X), eclf2.predict(X))","245","    assert_array_equal(eclf1.predict_proba(X), eclf2.predict_proba(X))","246","","247","    sample_weight = np.random.RandomState(123).uniform(size=(len(y),))","248","    eclf3 = VotingClassifier(estimators=[('lr', clf1)], voting='soft')","249","    eclf3.fit(X, y, sample_weight)","250","    clf1.fit(X, y, sample_weight)","251","    assert_array_equal(eclf3.predict(X), clf1.predict(X))","252","    assert_array_equal(eclf3.predict_proba(X), clf1.predict_proba(X))","253","","254","    clf4 = KNeighborsClassifier()","255","    eclf3 = VotingClassifier(estimators=[","256","        ('lr', clf1), ('svc', clf3), ('knn', clf4)],","257","        voting='soft')","258","    msg = ('Underlying estimator \\'knn\\' does not support sample weights.')","259","    assert_raise_message(ValueError, msg, eclf3.fit, X, y, sample_weight)"],"delete":["0","\"\"\"Testing for the boost module (sklearn.ensemble.boost).\"\"\"","3","from sklearn.utils.testing import assert_almost_equal"]}]}},"62a0bcdbe133a4ede5616996ec8ba0044d1bf47b":{"changes":{"sklearn\/decomposition\/tests\/test_fastica.py":"MODIFY"},"diff":{"sklearn\/decomposition\/tests\/test_fastica.py":[{"add":["5","import pytest","53","@pytest.mark.parametrize(\"add_noise\", [True, False])","54","@pytest.mark.parametrize(\"seed\", range(1))","55","def test_fastica_simple(add_noise, seed):","57","    rng = np.random.RandomState(seed)","87","            k_, mixing_, s_ = fastica(m.T, fun=nl, algorithm=algo,","88","                                      random_state=rng)","92","            pca = PCA(n_components=2, whiten=True, random_state=rng)","93","            X = pca.fit_transform(m.T)","94","            k_, mixing_, s_ = fastica(X, fun=nl, algorithm=algo, whiten=False,","95","                                      random_state=rng)","121","    _, _, sources_fun = fastica(m.T, fun=nl, algorithm=algo,","122","                                random_state=seed)","123","    ica = FastICA(fun=nl, algorithm=algo, random_state=seed)","134","        ica = FastICA(fun=fn, algorithm=algo)"],"delete":["52","def test_fastica_simple(add_noise=False):","54","    rng = np.random.RandomState(0)","84","            k_, mixing_, s_ = fastica(m.T, fun=nl, algorithm=algo)","88","            X = PCA(n_components=2, whiten=True).fit_transform(m.T)","89","            k_, mixing_, s_ = fastica(X, fun=nl, algorithm=algo, whiten=False)","115","    _, _, sources_fun = fastica(m.T, fun=nl, algorithm=algo, random_state=0)","116","    ica = FastICA(fun=nl, algorithm=algo, random_state=0)","127","        ica = FastICA(fun=fn, algorithm=algo, random_state=0)"]}]}},"eb1f5f29df4bcb64fa3a96e3018aefcbe99dffab":{"changes":{"sklearn\/ensemble\/_hist_gradient_boosting\/tests\/__init__.py":"ADD","MANIFEST.in":"MODIFY"},"diff":{"sklearn\/ensemble\/_hist_gradient_boosting\/tests\/__init__.py":[{"add":[],"delete":[]}],"MANIFEST.in":[{"add":["3","recursive-include sklearn *.c *.h *.pyx *.pxd *.pxi *.tp"],"delete":["3","recursive-include sklearn *.c *.h *.pyx *.pxd *.pxi"]}]}},"6da44dd6b37cb64202b8baed148ed83294001396":{"changes":{".github\/ISSUE_TEMPLATE\/bug_report.md":"MODIFY"},"diff":{".github\/ISSUE_TEMPLATE\/bug_report.md":[{"add":["4","labels: 'Bug: triage'"],"delete":["4","labels: Bug"]}]}},"fc7d6e698668b983cee2867b1bf3c65f1384e4cf":{"changes":{"examples\/multioutput\/plot_classifier_chain_yeast.py":"MODIFY",".circleci\/config.yml":"MODIFY","README.rst":"MODIFY","examples\/linear_model\/plot_logistic.py":"MODIFY","examples\/ensemble\/plot_voting_probas.py":"MODIFY","examples\/linear_model\/plot_iris_logistic.py":"MODIFY","examples\/linear_model\/plot_logistic_path.py":"MODIFY","examples\/neural_networks\/plot_rbm_logistic_classification.py":"MODIFY","examples\/ensemble\/plot_feature_transformation.py":"MODIFY","examples\/linear_model\/plot_logistic_l1_l2_sparsity.py":"MODIFY","examples\/compose\/plot_digits_pipe.py":"MODIFY","examples\/exercises\/plot_digits_classification_exercise.py":"MODIFY","examples\/calibration\/plot_compare_calibration.py":"MODIFY","examples\/compose\/plot_column_transformer_mixed_types.py":"MODIFY","examples\/classification\/plot_classification_probability.py":"MODIFY"},"diff":{"examples\/multioutput\/plot_classifier_chain_yeast.py":[{"add":["56","base_lr = LogisticRegression(solver='lbfgs')","57","ovr = OneVsRestClassifier(base_lr)","64","chains = [ClassifierChain(base_lr, order='random', random_state=i)"],"delete":["56","ovr = OneVsRestClassifier(LogisticRegression())","63","chains = [ClassifierChain(LogisticRegression(), order='random', random_state=i)"]}],".circleci\/config.yml":[{"add":["43","      - PYTHON_VERSION: \"2\"","44","      - NUMPY_VERSION: \"1.10\"","45","      - SCIPY_VERSION: \"0.16\"","46","      - MATPLOTLIB_VERSION: \"1.4\"","47","      - SCIKIT_IMAGE_VERSION: \"0.11\"","48","      - PANDAS_VERSION: \"0.17.1\""],"delete":["43","      - PYTHON_VERSION: 2","44","      - NUMPY_VERSION: 1.8.2","45","      # XXX: plot_gpc_xor.py fails with scipy 0.13.3","46","      - SCIPY_VERSION: 0.14","47","      - MATPLOTLIB_VERSION: 1.3","48","      - SCIKIT_IMAGE_VERSION: 0.9.3","49","      - PANDAS_VERSION: 0.13.1"]}],"README.rst":[{"add":["58","For running the examples Matplotlib >= 1.4 is required. A few examples","59","require scikit-image >= 0.11.3 and a few examples require pandas >= 0.17.1."],"delete":["58","For running the examples Matplotlib >= 1.3.1 is required. A few examples","59","require scikit-image >= 0.9.3 and a few examples require pandas >= 0.13.1."]}],"examples\/linear_model\/plot_logistic.py":[{"add":["25","# General a toy dataset:s it's just a straight line with some Gaussian noise:","35","","36","# Fit the classifier","37","clf = linear_model.LogisticRegression(C=1e5, solver='lbfgs')","49","","50","","67","plt.tight_layout()"],"delete":["25","# this is our test set, it's just a straight line with some","26","# Gaussian noise","36","# run the classifier","37","clf = linear_model.LogisticRegression(C=1e5)"]}],"examples\/ensemble\/plot_voting_probas.py":[{"add":["31","clf1 = LogisticRegression(solver='lbfgs', max_iter=1000, random_state=123)","81","plt.tight_layout()"],"delete":["31","clf1 = LogisticRegression(random_state=123)"]}],"examples\/linear_model\/plot_iris_logistic.py":[{"add":["9","first two dimensions (sepal length and width) of the `iris","10","<https:\/\/en.wikipedia.org\/wiki\/Iris_flower_data_set>`_ dataset. The datapoints","11","are colored according to their labels.","22","from sklearn.linear_model import LogisticRegression","23","from sklearn import datasets","30","logreg = LogisticRegression(C=1e5, solver='lbfgs', multi_class='multinomial')","39","h = .02  # step size in the mesh"],"delete":["9","`iris <https:\/\/en.wikipedia.org\/wiki\/Iris_flower_data_set>`_ dataset. The","10","datapoints are colored according to their labels.","15","","22","from sklearn import linear_model, datasets","29","h = .02  # step size in the mesh","30","","31","logreg = linear_model.LogisticRegression(C=1e5)"]}],"examples\/linear_model\/plot_logistic_path.py":[{"add":["2","==============================================","3","Regularization path of L1- Logistic Regression","4","==============================================","6","","7","Train l1-penalized logistic regression models on a binary classification","8","problem derived from the Iris dataset.","9","","10","The models are ordered from strongest regularized to least regularized. The 4","11","coefficients of the models are collected and plotted as a \"regularization","12","path\": on the left-hand side of the figure (strong regularizers), all the","13","coefficients are exactly 0. When regularization gets progressively looser,","14","coefficients can get non-zero values one after the other.","15","","16","Here we choose the SAGA solver because it can efficiently optimize for the","17","Logistic Regression loss with a non-smooth, sparsity inducing l1 penalty.","18","","19","Also note that we set a low value for the tolerance to make sure that the model","20","has converged before collecting the coefficients.","21","","22","We also use warm_start=True which means that the coefficients of the models are","23","reused to initialize the next model fit to speed-up the computation of the","24","full-path.","32","from time import time","47","X \/= X.max()  # Normalize X to speed-up convergence","52","cs = l1_min_c(X, y, loss='log') * np.logspace(0, 7, 16)","56","start = time()","57","clf = linear_model.LogisticRegression(penalty='l1', solver='saga',","58","                                      tol=1e-6, max_iter=int(1e6),","59","                                      warm_start=True)","65","print(\"This took %0.3fs\" % (time() - start))","68","plt.plot(np.log10(cs), coefs_, marker='o')"],"delete":["2","=================================","3","Path with L1- Logistic Regression","4","=================================","6","Computes path on IRIS dataset.","14","from datetime import datetime","29","X -= np.mean(X, 0)","34","cs = l1_min_c(X, y, loss='log') * np.logspace(0, 3)","38","start = datetime.now()","39","clf = linear_model.LogisticRegression(C=1.0, penalty='l1', tol=1e-6)","45","print(\"This took \", datetime.now() - start)","48","plt.plot(np.log10(cs), coefs_)"]}],"examples\/neural_networks\/plot_rbm_logistic_classification.py":[{"add":["42","from sklearn.base import clone","70","    def shift(x, w):","71","        return convolve(x.reshape((8, 8)), mode='constant', weights=w).ravel()","72","","79","","86","X_train, X_test, Y_train, Y_test = train_test_split(","87","    X, Y, test_size=0.2, random_state=0)","90","logistic = linear_model.LogisticRegression(solver='lbfgs', max_iter=10000,","91","                                           multi_class='multinomial')","94","rbm_features_classifier = Pipeline(","95","    steps=[('rbm', rbm), ('logistic', logistic)])","108","logistic.C = 6000","111","rbm_features_classifier.fit(X_train, Y_train)","113","# Training the Logistic regression classifier directly on the pixel","114","raw_pixel_classifier = clone(logistic)","115","raw_pixel_classifier.C = 100.","116","raw_pixel_classifier.fit(X_train, Y_train)","121","Y_pred = rbm_features_classifier.predict(X_test)","123","    metrics.classification_report(Y_test, Y_pred)))","125","Y_pred = raw_pixel_classifier.predict(X_test)","127","    metrics.classification_report(Y_test, Y_pred)))"],"delete":["69","    shift = lambda x, w: convolve(x.reshape((8, 8)), mode='constant',","70","                                  weights=w).ravel()","83","X_train, X_test, Y_train, Y_test = train_test_split(X, Y,","84","                                                    test_size=0.2,","85","                                                    random_state=0)","88","logistic = linear_model.LogisticRegression()","91","classifier = Pipeline(steps=[('rbm', rbm), ('logistic', logistic)])","104","logistic.C = 6000.0","107","classifier.fit(X_train, Y_train)","109","# Training Logistic regression","110","logistic_classifier = linear_model.LogisticRegression(C=100.0)","111","logistic_classifier.fit(X_train, Y_train)","116","print()","118","    metrics.classification_report(","119","        Y_test,","120","        classifier.predict(X_test))))","123","    metrics.classification_report(","124","        Y_test,","125","        logistic_classifier.predict(X_test))))"]}],"examples\/ensemble\/plot_feature_transformation.py":[{"add":["44","","49","X_train, X_train_lr, y_train, y_train_lr = train_test_split(","50","    X_train, y_train, test_size=0.5)","56","rt_lm = LogisticRegression(solver='lbfgs', max_iter=1000)","65","rf_lm = LogisticRegression(solver='lbfgs', max_iter=1000)","73","# Supervised transformation based on gradient boosted trees","76","grd_lm = LogisticRegression(solver='lbfgs', max_iter=1000)"],"delete":["48","X_train, X_train_lr, y_train, y_train_lr = train_test_split(X_train,","49","                                                            y_train,","50","                                                            test_size=0.5)","56","rt_lm = LogisticRegression()","65","rf_lm = LogisticRegression()","75","grd_lm = LogisticRegression()","84","","89",""]}],"examples\/linear_model\/plot_logistic_l1_l2_sparsity.py":[{"add":["39","for i, C in enumerate((1, 0.1, 0.01)):","41","    clf_l1_LR = LogisticRegression(C=C, penalty='l1', tol=0.01, solver='saga')","42","    clf_l2_LR = LogisticRegression(C=C, penalty='l2', tol=0.01, solver='saga')"],"delete":["39","for i, C in enumerate((100, 1, 0.01)):","41","    clf_l1_LR = LogisticRegression(C=C, penalty='l1', tol=0.01)","42","    clf_l2_LR = LogisticRegression(C=C, penalty='l2', tol=0.01)"]}],"examples\/compose\/plot_digits_pipe.py":[{"add":["24","import pandas as pd","26","from sklearn import datasets","27","from sklearn.decomposition import PCA","28","from sklearn.linear_model import SGDClassifier","33","# Define a pipeline to search for the best combination of PCA truncation","34","# and classifier regularization.","35","logistic = SGDClassifier(loss='log', penalty='l2', early_stopping=True,","36","                         max_iter=10000, tol=1e-5, random_state=0)","37","pca = PCA()","44","# Parameters of pipelines can be set using \u2018__\u2019 separated parameter names:","45","param_grid = {","46","    'pca__n_components': [5, 20, 30, 40, 50, 64],","47","    'logistic__alpha': np.logspace(-4, 4, 5),","48","}","49","search = GridSearchCV(pipe, param_grid, iid=False, cv=5,","50","                      return_train_score=False)","51","search.fit(X_digits, y_digits)","52","print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)","53","print(search.best_params_)","54","","58","fig, (ax0, ax1) = plt.subplots(nrows=2, sharex=True, figsize=(6, 6))","59","ax0.plot(pca.explained_variance_ratio_, linewidth=2)","60","ax0.set_ylabel('PCA explained variance')","62","ax0.axvline(search.best_estimator_.named_steps['pca'].n_components,","64","ax0.legend(prop=dict(size=12))","65","","66","# For each number of components, find the best classifier results","67","results = pd.DataFrame(search.cv_results_)","68","components_col = 'param_pca__n_components'","69","best_clfs = results.groupby(components_col).apply(","70","    lambda g: g.nlargest(1, 'mean_test_score'))","71","","72","best_clfs.plot(x=components_col, y='mean_test_score', yerr='std_test_score',","73","               legend=False, ax=ax1)","74","ax1.set_ylabel('Classification accuracy (val)')","75","ax1.set_xlabel('n_components')","76","","77","plt.tight_layout()"],"delete":["25","from sklearn import linear_model, decomposition, datasets","29","logistic = linear_model.LogisticRegression()","31","pca = decomposition.PCA()","41","plt.figure(1, figsize=(4, 3))","42","plt.clf()","43","plt.axes([.2, .2, .7, .7])","44","plt.plot(pca.explained_variance_, linewidth=2)","45","plt.axis('tight')","46","plt.xlabel('n_components')","47","plt.ylabel('explained_variance_')","49","# Prediction","50","n_components = [20, 40, 64]","51","Cs = np.logspace(-4, 4, 3)","52","","53","# Parameters of pipelines can be set using \u2018__\u2019 separated parameter names:","54","estimator = GridSearchCV(pipe,","55","                         dict(pca__n_components=n_components,","56","                              logistic__C=Cs), cv=5)","57","estimator.fit(X_digits, y_digits)","58","","59","plt.axvline(estimator.best_estimator_.named_steps['pca'].n_components,","61","plt.legend(prop=dict(size=12))"]}],"examples\/exercises\/plot_digits_classification_exercise.py":[{"add":["17","X_digits = digits.data \/ digits.data.max()","28","logistic = linear_model.LogisticRegression(solver='lbfgs', max_iter=1000,","29","                                           multi_class='multinomial')"],"delete":["17","X_digits = digits.data","28","logistic = linear_model.LogisticRegression()"]}],"examples\/calibration\/plot_compare_calibration.py":[{"add":["77","lr = LogisticRegression(solver='lbfgs')"],"delete":["77","lr = LogisticRegression()"]}],"examples\/compose\/plot_column_transformer_mixed_types.py":[{"add":["73","                      ('classifier', LogisticRegression(solver='lbfgs'))])"],"delete":["73","                      ('classifier', LogisticRegression())])"]}],"examples\/classification\/plot_classification_probability.py":[{"add":["5","Plot the classification probability for different classifiers. We use a 3 class","6","dataset, and we classify it with a Support Vector classifier, L1 and L2","7","penalized logistic regression with either a One-Vs-Rest or multinomial setting,","8","and Gaussian process classification.","10","Linear SVC is not a probabilistic classifier by default but it has a built-in","11","calibration option enabled in this example (`probability=True`).","12","","13","The logistic regression with One-Vs-Rest is not a multiclass classifier out of","14","the box. As a result it has more trouble in separating class 2 and 3 than the","15","other estimators.","25","from sklearn.metrics import accuracy_score","38","C = 10","41","# Create different classifiers.","42","classifiers = {","43","    'L1 logistic': LogisticRegression(C=C, penalty='l1',","44","                                      solver='saga',","45","                                      multi_class='multinomial',","46","                                      max_iter=10000),","47","    'L2 logistic (Multinomial)': LogisticRegression(C=C, penalty='l2',","48","                                                    solver='saga',","49","                                                    multi_class='multinomial',","50","                                                    max_iter=10000),","51","    'L2 logistic (OvR)': LogisticRegression(C=C, penalty='l2',","52","                                            solver='saga',","53","                                            multi_class='ovr',","54","                                            max_iter=10000),","55","    'Linear SVC': SVC(kernel='linear', C=C, probability=True,","56","                      random_state=0),","57","    'GPC': GaussianProcessClassifier(kernel)","58","}","74","    accuracy = accuracy_score(y, y_pred)","75","    print(\"Accuracy (train) for %s: %0.1f%% \" % (name, accuracy * 100))","77","    # View probabilities:"],"delete":["5","Plot the classification probability for different classifiers. We use a 3","6","class dataset, and we classify it with a Support Vector classifier, L1","7","and L2 penalized logistic regression with either a One-Vs-Rest or multinomial","8","setting, and Gaussian process classification.","10","The logistic regression is not a multiclass classifier out of the box. As","11","a result it can identify only the first class.","33","C = 1.0","36","# Create different classifiers. The logistic regression cannot do","37","# multiclass out of the box.","38","classifiers = {'L1 logistic': LogisticRegression(C=C, penalty='l1'),","39","               'L2 logistic (OvR)': LogisticRegression(C=C, penalty='l2'),","40","               'Linear SVC': SVC(kernel='linear', C=C, probability=True,","41","                                 random_state=0),","42","               'L2 logistic (Multinomial)': LogisticRegression(","43","                C=C, solver='lbfgs', multi_class='multinomial'),","44","               'GPC': GaussianProcessClassifier(kernel)","45","               }","61","    classif_rate = np.mean(y_pred.ravel() == y.ravel()) * 100","62","    print(\"classif_rate for %s : %f \" % (name, classif_rate))","64","    # View probabilities="]}]}},"11934e1838b87936da98de31350d88158a8640bb":{"changes":{"sklearn\/cluster\/tests\/test_birch.py":"MODIFY","sklearn\/cluster\/_birch.py":"MODIFY","doc\/whats_new\/v0.23.rst":"MODIFY"},"diff":{"sklearn\/cluster\/tests\/test_birch.py":[{"add":["161","","162","","163","def test_birch_n_clusters_long_int():","164","    # Check that birch supports n_clusters with np.int64 dtype, for instance","165","    # coming from np.arange. #16484","166","    X, _ = make_blobs(random_state=0)","167","    n_clusters = np.int64(5)","168","    Birch(n_clusters=n_clusters).fit(X)"],"delete":[]}],"sklearn\/cluster\/_birch.py":[{"add":["6","import numbers","620","        if isinstance(clusterer, numbers.Integral):"],"delete":["619","        if isinstance(clusterer, int):"]}],"doc\/whats_new\/v0.23.rst":[{"add":["62","- |Fix| Fixed a bug in :class:`cluster.Birch` where the `n_clusters` parameter","63","  could not have a `np.int64` type. :pr:`16484`","64","  by :user:`Jeremie du Boisberranger <jeremiedbb>`.","65",""],"delete":[]}]}},"70e1558607b4b0bdeb94f5af4aae115e4108e202":{"changes":{"doc\/modules\/manifold.rst":"MODIFY","doc\/modules\/multiclass.rst":"MODIFY","doc\/modules\/compose.rst":"MODIFY","doc\/modules\/feature_selection.rst":"MODIFY","doc\/modules\/preprocessing.rst":"MODIFY","doc\/faq.rst":"MODIFY","doc\/related_projects.rst":"MODIFY","doc\/modules\/metrics.rst":"MODIFY","doc\/developers\/contributing.rst":"MODIFY","doc\/modules\/random_projection.rst":"MODIFY","doc\/modules\/feature_extraction.rst":"MODIFY","doc\/developers\/utilities.rst":"MODIFY","doc\/whats_new\/older_versions.rst":"MODIFY","doc\/modules\/neighbors.rst":"MODIFY","doc\/modules\/svm.rst":"MODIFY","doc\/modules\/density.rst":"MODIFY","doc\/computing\/computational_performance.rst":"MODIFY"},"diff":{"doc\/modules\/manifold.rst":[{"add":["118","   :class:`~sklearn.neighbors.BallTree` for efficient neighbor search.","581","  projection for instance using :class:`~sklearn.decomposition.TruncatedSVD`"],"delete":["118","   :class:`sklearn.neighbors.BallTree` for efficient neighbor search.","581","  projection for instance using :class:`sklearn.decomposition.TruncatedSVD`"]}],"doc\/modules\/multiclass.rst":[{"add":["62","  :class:`~sklearn.multioutput.MultiOutputClassifier`. This approach treats","179",".. currentmodule:: sklearn","180","","183","  - :class:`naive_bayes.BernoulliNB`","184","  - :class:`tree.DecisionTreeClassifier`","185","  - :class:`tree.ExtraTreeClassifier`","186","  - :class:`ensemble.ExtraTreesClassifier`","187","  - :class:`naive_bayes.GaussianNB`","188","  - :class:`neighbors.KNeighborsClassifier`","189","  - :class:`semi_supervised.LabelPropagation`","190","  - :class:`semi_supervised.LabelSpreading`","191","  - :class:`discriminant_analysis.LinearDiscriminantAnalysis`","192","  - :class:`svm.LinearSVC` (setting multi_class=\"crammer_singer\")","193","  - :class:`linear_model.LogisticRegression` (setting multi_class=\"multinomial\")","194","  - :class:`linear_model.LogisticRegressionCV` (setting multi_class=\"multinomial\")","195","  - :class:`neural_network.MLPClassifier`","196","  - :class:`neighbors.NearestCentroid`","197","  - :class:`discriminant_analysis.QuadraticDiscriminantAnalysis`","198","  - :class:`neighbors.RadiusNeighborsClassifier`","199","  - :class:`ensemble.RandomForestClassifier`","200","  - :class:`linear_model.RidgeClassifier`","201","  - :class:`linear_model.RidgeClassifierCV`","206","  - :class:`svm.NuSVC`","207","  - :class:`svm.SVC`.","208","  - :class:`gaussian_process.GaussianProcessClassifier` (setting multi_class = \"one_vs_one\")","213","  - :class:`ensemble.GradientBoostingClassifier`","214","  - :class:`gaussian_process.GaussianProcessClassifier` (setting multi_class = \"one_vs_rest\")","215","  - :class:`svm.LinearSVC` (setting multi_class=\"ovr\")","216","  - :class:`linear_model.LogisticRegression` (setting multi_class=\"ovr\")","217","  - :class:`linear_model.LogisticRegressionCV` (setting multi_class=\"ovr\")","218","  - :class:`linear_model.SGDClassifier`","219","  - :class:`linear_model.Perceptron`","220","  - :class:`linear_model.PassiveAggressiveClassifier`","225","  - :class:`tree.DecisionTreeClassifier`","226","  - :class:`tree.ExtraTreeClassifier`","227","  - :class:`ensemble.ExtraTreesClassifier`","228","  - :class:`neighbors.KNeighborsClassifier`","229","  - :class:`neural_network.MLPClassifier`","230","  - :class:`neighbors.RadiusNeighborsClassifier`","231","  - :class:`ensemble.RandomForestClassifier`","232","  - :class:`linear_model.RidgeClassifierCV`","237","  - :class:`tree.DecisionTreeClassifier`","238","  - :class:`tree.ExtraTreeClassifier`","239","  - :class:`ensemble.ExtraTreesClassifier`","240","  - :class:`neighbors.KNeighborsClassifier`","241","  - :class:`neighbors.RadiusNeighborsClassifier`","242","  - :class:`ensemble.RandomForestClassifier`","261","The :class:`~preprocessing.MultiLabelBinarizer` transformer can be used","262","to convert between a collection of collections of labels and the indicator format."],"delete":["62","  :class:`sklearn.multioutput.MultiOutputClassifier`. This approach treats","181","  - :class:`sklearn.naive_bayes.BernoulliNB`","182","  - :class:`sklearn.tree.DecisionTreeClassifier`","183","  - :class:`sklearn.tree.ExtraTreeClassifier`","184","  - :class:`sklearn.ensemble.ExtraTreesClassifier`","185","  - :class:`sklearn.naive_bayes.GaussianNB`","186","  - :class:`sklearn.neighbors.KNeighborsClassifier`","187","  - :class:`sklearn.semi_supervised.LabelPropagation`","188","  - :class:`sklearn.semi_supervised.LabelSpreading`","189","  - :class:`sklearn.discriminant_analysis.LinearDiscriminantAnalysis`","190","  - :class:`sklearn.svm.LinearSVC` (setting multi_class=\"crammer_singer\")","191","  - :class:`sklearn.linear_model.LogisticRegression` (setting multi_class=\"multinomial\")","192","  - :class:`sklearn.linear_model.LogisticRegressionCV` (setting multi_class=\"multinomial\")","193","  - :class:`sklearn.neural_network.MLPClassifier`","194","  - :class:`sklearn.neighbors.NearestCentroid`","195","  - :class:`sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis`","196","  - :class:`sklearn.neighbors.RadiusNeighborsClassifier`","197","  - :class:`sklearn.ensemble.RandomForestClassifier`","198","  - :class:`sklearn.linear_model.RidgeClassifier`","199","  - :class:`sklearn.linear_model.RidgeClassifierCV`","204","  - :class:`sklearn.svm.NuSVC`","205","  - :class:`sklearn.svm.SVC`.","206","  - :class:`sklearn.gaussian_process.GaussianProcessClassifier` (setting multi_class = \"one_vs_one\")","211","  - :class:`sklearn.ensemble.GradientBoostingClassifier`","212","  - :class:`sklearn.gaussian_process.GaussianProcessClassifier` (setting multi_class = \"one_vs_rest\")","213","  - :class:`sklearn.svm.LinearSVC` (setting multi_class=\"ovr\")","214","  - :class:`sklearn.linear_model.LogisticRegression` (setting multi_class=\"ovr\")","215","  - :class:`sklearn.linear_model.LogisticRegressionCV` (setting multi_class=\"ovr\")","216","  - :class:`sklearn.linear_model.SGDClassifier`","217","  - :class:`sklearn.linear_model.Perceptron`","218","  - :class:`sklearn.linear_model.PassiveAggressiveClassifier`","223","  - :class:`sklearn.tree.DecisionTreeClassifier`","224","  - :class:`sklearn.tree.ExtraTreeClassifier`","225","  - :class:`sklearn.ensemble.ExtraTreesClassifier`","226","  - :class:`sklearn.neighbors.KNeighborsClassifier`","227","  - :class:`sklearn.neural_network.MLPClassifier`","228","  - :class:`sklearn.neighbors.RadiusNeighborsClassifier`","229","  - :class:`sklearn.ensemble.RandomForestClassifier`","230","  - :class:`sklearn.linear_model.RidgeClassifierCV`","235","  - :class:`sklearn.tree.DecisionTreeClassifier`","236","  - :class:`sklearn.tree.ExtraTreeClassifier`","237","  - :class:`sklearn.ensemble.ExtraTreesClassifier`","238","  - :class:`sklearn.neighbors.KNeighborsClassifier`","239","  - :class:`sklearn.neighbors.RadiusNeighborsClassifier`","240","  - :class:`sklearn.ensemble.RandomForestClassifier`","259","The :class:`MultiLabelBinarizer <sklearn.preprocessing.MultiLabelBinarizer>`","260","transformer can be used to convert between a collection of collections of","261","labels and the indicator format."]}],"doc\/modules\/compose.rst":[{"add":["332","see the related class :class:`~sklearn.compose.ColumnTransformer`","418","variable using :class:`~sklearn.preprocessing.OneHotEncoder` but apply a","419",":class:`~sklearn.feature_extraction.text.CountVectorizer` to the ``'title'`` column.","453","However, :class:`~sklearn.preprocessing.OneHotEncoder`","537","`display` option in :func:`~sklearn.set_config`::"],"delete":["332","see the related class :class:`sklearn.compose.ColumnTransformer`","418","variable using :class:`preprocessing.OneHotEncoder","419","<sklearn.preprocessing.OneHotEncoder>` but apply a","420",":class:`feature_extraction.text.CountVectorizer","421","<sklearn.feature_extraction.text.CountVectorizer>` to the ``'title'`` column.","455","However, :class:`preprocessing.OneHotEncoder <sklearn.preprocessing.OneHotEncoder>`","539","`display` option in :func:`sklearn.set_config`::"]}],"doc\/modules\/feature_selection.rst":[{"add":["176","they can be used along with :class:`~feature_selection.SelectFromModel`","178","for this purpose are the :class:`~linear_model.Lasso` for regression, and","179","of :class:`~linear_model.LogisticRegression` and :class:`~svm.LinearSVC`","237","features (when coupled with the :class:`~feature_selection.SelectFromModel`","269","to use a :class:`~pipeline.Pipeline`::","277","In this snippet we make use of a :class:`~svm.LinearSVC`","278","coupled with :class:`~feature_selection.SelectFromModel`","280","Then, a :class:`~ensemble.RandomForestClassifier` is trained on the","284","See the :class:`~pipeline.Pipeline` examples for more details."],"delete":["176","they can be used along with :class:`feature_selection.SelectFromModel`","178","for this purpose are the :class:`linear_model.Lasso` for regression, and","179","of :class:`linear_model.LogisticRegression` and :class:`svm.LinearSVC`","237","features (when coupled with the :class:`sklearn.feature_selection.SelectFromModel`","269","to use a :class:`sklearn.pipeline.Pipeline`::","277","In this snippet we make use of a :class:`sklearn.svm.LinearSVC`","278","coupled with :class:`sklearn.feature_selection.SelectFromModel`","280","Then, a :class:`sklearn.ensemble.RandomForestClassifier` is trained on the","284","See the :class:`sklearn.pipeline.Pipeline` examples for more details."]}],"doc\/modules\/preprocessing.rst":[{"add":["78",":class:`~sklearn.pipeline.Pipeline`::","237","  To address this issue you can use :class:`~sklearn.decomposition.PCA` with","435",":class:`~sklearn.pipeline.Pipeline`::","639","in a :class:`~sklearn.pipeline.Pipeline`.","667","this is the case for the :class:`~sklearn.neural_network.BernoulliRBM`.","676",":class:`~sklearn.pipeline.Pipeline`. The ``fit`` method does nothing","761","Note that polynomial features are used implicitly in `kernel methods <https:\/\/en.wikipedia.org\/wiki\/Kernel_method>`_ (e.g., :class:`~sklearn.svm.SVC`, :class:`~sklearn.decomposition.KernelPCA`) when using polynomial :ref:`svm_kernels`."],"delete":["78",":class:`sklearn.pipeline.Pipeline`::","237","  To address this issue you can use :class:`sklearn.decomposition.PCA` with","435",":class:`sklearn.pipeline.Pipeline`::","639","in a :class:`sklearn.pipeline.Pipeline`.","667","this is the case for the :class:`sklearn.neural_network.BernoulliRBM`.","676",":class:`sklearn.pipeline.Pipeline`. The ``fit`` method does nothing","761","Note that polynomial features are used implicitly in `kernel methods <https:\/\/en.wikipedia.org\/wiki\/Kernel_method>`_ (e.g., :class:`sklearn.svm.SVC`, :class:`sklearn.decomposition.KernelPCA`) when using polynomial :ref:`svm_kernels`."]}],"doc\/faq.rst":[{"add":["329","in :mod:`sklearn.neural_network`. We will only accept bug fixes for this module.","402",":class:`~compose.TransformedTargetRegressor`,"],"delete":["329","in `sklearn.neural_network`. We will only accept bug fixes for this module.","402",":class:`sklearn.compose.TransformedTargetRegressor`,"]}],"doc\/related_projects.rst":[{"add":["202","  :class:`~sklearn.decomposition.LatentDirichletAllocation` implementation uses"],"delete":["202","  :class:`sklearn.decomposition.LatentDirichletAllocation` implementation uses"]}],"doc\/modules\/metrics.rst":[{"add":["188",":class:`~sklearn.svm.SVC` with ``kernel=\"precomputed\"``::"],"delete":["188",":class:`sklearn.svm.SVC` with ``kernel=\"precomputed\"``::"]}],"doc\/developers\/contributing.rst":[{"add":["6",".. currentmodule:: sklearn","7","","1169","  :class:`~linear_model.LinearRegression`, what you're looking for","1176","  at least from :class:`~base.BaseEstimator`, and","1177","  from a ``Mixin`` class (e.g. :class:`~base.ClassifierMixin`) that enables default","1178","  behaviour depending on the nature of the estimator (classifier, regressor,","1179","  transformer, etc.)."],"delete":["1167","  :class:`sklearn.linear_model.LinearRegression`, what you're looking for","1174","  at least from :class:`BaseEstimator <sklearn.base.BaseEstimator>`, and","1175","  from a ``Mixin`` class (e.g. :class:`ClassifierMixin","1176","  <sklearn.base.ClassifierMixin>`) that enables default behaviour depending","1177","  on the nature of the estimator (classifier, regressor, transformer, etc.)."]}],"doc\/modules\/random_projection.rst":[{"add":["54",":func:`johnson_lindenstrauss_min_dim` estimates","92","The :class:`GaussianRandomProjection` reduces the","113","The :class:`SparseRandomProjection` reduces the"],"delete":["54",":func:`sklearn.random_projection.johnson_lindenstrauss_min_dim` estimates","92","The :class:`sklearn.random_projection.GaussianRandomProjection` reduces the","113","The :class:`sklearn.random_projection.SparseRandomProjection` reduces the"]}],"doc\/modules\/feature_extraction.rst":[{"add":["104",":class:`~text.TfidfTransformer` for normalization)::","152",":class:`~sklearn.naive_bayes.MultinomialNB` or","153",":class:`~sklearn.feature_selection.chi2`","171","but unlike :class:`~text.CountVectorizer`,","823",":class:`~sklearn.feature_extraction.FeatureHasher` class and the text"],"delete":["104",":class:`text.TfidfTransformer` for normalization)::","152",":class:`sklearn.naive_bayes.MultinomialNB` or","153",":class:`sklearn.feature_selection.chi2`","171","but unlike :class:`text.CountVectorizer`,","823",":class:`sklearn.feature_extraction.FeatureHasher` class and the text"]}],"doc\/developers\/utilities.rst":[{"add":["100","  (used in :func:`~sklearn.linear_model.lars_path`)  Remove an","123","  Used in :func:`~sklearn.cluster.k_means`.","143","  :class:`~sklearn.cluster.KMeans`.","148","  :class:`~sklearn.preprocessing.Normalizer`.","153","  :class:`~sklearn.preprocessing.StandardScaler`.","168","  (used in :class:`~sklearn.manifold.Isomap`)","195","  to ``n``.  Used in :func:`~sklearn.decomposition.dict_learning` and","196","  :func:`~sklearn.cluster.k_means`.","233","- :class:`~sklearn.exceptions.ConvergenceWarning`: Custom warning to catch"],"delete":["100","  (used in :func:`sklearn.linear_model.lars_path`)  Remove an","123","  Used in :func:`sklearn.cluster.k_means`.","143","  :class:`sklearn.cluster.KMeans`.","148","  :class:`sklearn.preprocessing.Normalizer`.","153","  :class:`sklearn.preprocessing.StandardScaler`.","168","  (used in :class:`sklearn.manifold.Isomap`)","195","  to ``n``.  Used in :func:`sklearn.decomposition.dict_learning` and","196","  :func:`sklearn.cluster.k_means`.","233","- :class:`sklearn.exceptions.ConvergenceWarning`: Custom warning to catch"]}],"doc\/whats_new\/older_versions.rst":[{"add":["64","- :class:`~ensemble.GradientBoostingRegressor` and","65","  :class:`~ensemble.GradientBoostingClassifier` now support feature subsampling","69","  :class:`~ensemble.GradientBoostingRegressor`, by `Peter Prettenhofer`_.","75","- Added :class:`~preprocessing.LabelEncoder`, a simple utility class to","116","- In :class:`~feature_extraction.text.CountVectorizer`, added an option to","123","- Fixes in :class:`~decomposition.ProbabilisticPCA` score function by Wei Li.","138","- In :class:`hmm` objects, like :class:`~hmm.GaussianHMM`,","139","  :class:`~hmm.MultinomialHMM`, etc., all parameters must be passed to the","154","- In :class:`~feature_extraction.text.CountVectorizer` the parameters","158","- In :class:`~feature_extraction.text.CountVectorizer`, words that appear","169","- Grid of alphas used for fitting :class:`~linear_model.LassoCV` and","170","  :class:`~linear_model.ElasticNetCV` is now stored","178","- :class:`~ensemble.GradientBoostingClassifier` now supports","179","  :meth:`~ensemble.GradientBoostingClassifier.staged_predict_proba`, and","180","  :meth:`~ensemble.GradientBoostingClassifier.staged_predict`.","182","- :class:`~svm.sparse.SVC` and other sparse SVM classes are now deprecated.","187","  input data, in particular :class:`~cluster.SpectralClustering` and","188","  :class:`~cluster.AffinityPropagation` which previously expected affinity matrices.","264","  (:class:`~feature_extraction.DictVectorizer`) by `Lars Buitinck`_.","268","  :func:`~metrics.precision_score`, :func:`metrics.recall_score` and","269","  :func:`~metrics.f1_score` by `Satrajit Ghosh`_.","284","- Added :class:`~sklearn.cross_validation.StratifiedShuffleSplit`, which is","285","  a :class:`~sklearn.cross_validation.ShuffleSplit` with balanced splits,","288","- :class:`~sklearn.neighbors.NearestCentroid` classifier added, along with a","304","  :class:`~linear_model.LogisticRegression` merged by `Lars Buitinck`_.","320","- :class:`~svm.SVC` members ``coef_`` and ``intercept_`` changed sign for","326","  :class:`~linear_model.RidgeCV`, by Reuben Fletcher-Costin.","342","- :class:`~covariance.EllipticEnvelop` is now deprecated - Please use :class:`~covariance.EllipticEnvelope`","352","- In :class:`~mixture.GMM`, :class:`~mixture.DPGMM` and :class:`~mixture.VBGMM`,","372","- In :class:`~svm.LinearSVC`, the meaning of the ``multi_class`` parameter","377","- Class :class:`~feature_selection.text.Vectorizer` is deprecated and","378","  replaced by :class:`~feature_selection.text.TfidfVectorizer`.","383","  to :class:`~feature_selection.text.TfidfVectorizer` and","384","  :class:`~feature_selection.text.CountVectorizer`, in particular the","403","- Class :class:`~feature_selection.text.TfidfVectorizer` now derives directly","404","  from :class:`~feature_selection.text.CountVectorizer` to make grid","510","  :func:`~sklearn.metrics.silhouette_score` by Robert Layton.","521","  :func:`~sklearn.metrics.adjusted_mutual_info_score` by Robert Layton.","550","  (:func:`~sklearn.datasets.fetch_20newsgroups_vectorized`) by","558","- Make :func:`~sklearn.preprocessing.scale` and","559","  :class:`~sklearn.preprocessing.Scaler` work on sparse matrices by","568","- :class:`~sklearn.cross_validation.ShuffleSplit` can subsample the train","587","- The SVMlight dataset loader :func:`~sklearn.datasets.load_svmlight_file` no","613","  :func:`~sklearn.decomposition.sparse_encode`, and the shapes of the arrays","618","  files generated using :func:`~sklearn.datasets.dump_svmlight_file` should be","624","- :func:`~sklearn.utils.extmath.fast_svd` has been renamed","625","  :func:`~sklearn.utils.extmath.randomized_svd` and the default","772","- Implementation of :class:`~linear_model.LassoLarsCV`","774","  :class:`~linear_model.LassoLarsIC` (BIC\/AIC model","808","  from :class:`~base.BaseEstimator`.","950","- :class:`~decomposition.PCA` is now usable from the Pipeline object by `Olivier Grisel`_.","961","  :class:`~discriminant_analysis.LinearDiscriminantAnalysis` By `Mathieu Blondel`_.","1032","  :class:`~linear_model.RidgeCV` [`Mathieu Blondel`_]","1044","- Performance improvements for :class:`~cluster.KMeans` [`Gael","1049","- Refactoring of :class:`~neighbors.NeighborsClassifier` and","1057","- Documentation improvements: Added :class:`~pca.RandomizedPCA` and","1058","  :class:`~linear_model.LogisticRegression` to the class","1065","  dense and sparse variants, like :class:`~svm.LinearSVC` or","1066","  :class:`~linear_model.LogisticRegression` [`Fabian Pedregosa`_].","1070","  :class:`~pca.RandomizedPCA` [`James Bergstra`_].","1074","- Allow input sequences of different lengths in :class:`~hmm.GaussianHMM`","1137","  multi_class in :class:`~svm.LinearSVC`)","1143","  (:class:`~grid_search.GridSearchCV`) as in modules","1163","  :class:`~linear_model.LogisticRegression` model.","1220","  ``svm`` and ``linear_model`` (see :class:`~svm.sparse.SVC`,","1221","  :class:`~svm.sparse.SVR`, :class:`~svm.sparse.LinearSVC`,","1222","  :class:`~linear_model.sparse.Lasso`, :class:`~linear_model.sparse.ElasticNet`)","1224","- New :class:`~pipeline.Pipeline` object to compose different estimators.","1230","  linear_model module (:class:`~linear_model.LassoCV`, :class:`~linear_model.ElasticNetCV`,","1235","  :class:`~linear_model.lars_path`, :class:`~linear_model.Lars` and","1236","  :class:`~linear_model.LassoLars`.","1239","  :class:`~hmm.GaussianHMM`, :class:`~hmm.MultinomialHMM`,","1240","  :class:`~hmm.GMMHMM`)"],"delete":["64","- :class:`ensemble.GradientBoostingRegressor` and","65","  :class:`ensemble.GradientBoostingClassifier` now support feature subsampling","69","  :class:`ensemble.GradientBoostingRegressor`, by `Peter Prettenhofer`_.","75","- Added :class:`preprocessing.LabelEncoder`, a simple utility class to","116","- In :class:`feature_extraction.text.CountVectorizer`, added an option to","123","- Fixes in :class:`decomposition.ProbabilisticPCA` score function by Wei Li.","138","- In :class:`hmm` objects, like :class:`hmm.GaussianHMM`,","139","  :class:`hmm.MultinomialHMM`, etc., all parameters must be passed to the","154","- In :class:`feature_extraction.text.CountVectorizer` the parameters","158","- In :class:`feature_extraction.text.CountVectorizer`, words that appear","169","- Grid of alphas used for fitting :class:`linear_model.LassoCV` and","170","  :class:`linear_model.ElasticNetCV` is now stored","178","- :class:`ensemble.GradientBoostingClassifier` now supports","179","  :meth:`ensemble.GradientBoostingClassifier.staged_predict_proba`, and","180","  :meth:`ensemble.GradientBoostingClassifier.staged_predict`.","182","- :class:`svm.sparse.SVC` and other sparse SVM classes are now deprecated.","187","  input data, in particular :class:`cluster.SpectralClustering` and","188","  :class:`cluster.AffinityPropagation` which previously expected affinity matrices.","264","  (:class:`feature_extraction.DictVectorizer`) by `Lars Buitinck`_.","268","  :func:`metrics.precision_score`, :func:`metrics.recall_score` and","269","  :func:`metrics.f1_score` by `Satrajit Ghosh`_.","284","- Added :class:`sklearn.cross_validation.StratifiedShuffleSplit`, which is","285","  a :class:`sklearn.cross_validation.ShuffleSplit` with balanced splits,","288","- :class:`sklearn.neighbors.NearestCentroid` classifier added, along with a","304","  :class:`linear_model.LogisticRegression` merged by `Lars Buitinck`_.","320","- :class:`svm.SVC` members ``coef_`` and ``intercept_`` changed sign for","326","  :class:`linear_model.RidgeCV`, by Reuben Fletcher-Costin.","342","- :class:`covariance.EllipticEnvelop` is now deprecated - Please use :class:`covariance.EllipticEnvelope`","352","- In :class:`mixture.GMM`, :class:`mixture.DPGMM` and :class:`mixture.VBGMM`,","372","- In :class:`svm.LinearSVC`, the meaning of the ``multi_class`` parameter","377","- Class :class:`feature_selection.text.Vectorizer` is deprecated and","378","  replaced by :class:`feature_selection.text.TfidfVectorizer`.","383","  to :class:`feature_selection.text.TfidfVectorizer` and","384","  :class:`feature_selection.text.CountVectorizer`, in particular the","403","- Class :class:`feature_selection.text.TfidfVectorizer` now derives directly","404","  from :class:`feature_selection.text.CountVectorizer` to make grid","510","  :func:`sklearn.metrics.silhouette_score` by Robert Layton.","521","  :func:`sklearn.metrics.adjusted_mutual_info_score` by Robert Layton.","550","  (:func:`sklearn.datasets.fetch_20newsgroups_vectorized`) by","558","- Make :func:`sklearn.preprocessing.scale` and","559","  :class:`sklearn.preprocessing.Scaler` work on sparse matrices by","568","- :class:`sklearn.cross_validation.ShuffleSplit` can subsample the train","587","- The SVMlight dataset loader :func:`sklearn.datasets.load_svmlight_file` no","613","  :func:`sklearn.decomposition.sparse_encode`, and the shapes of the arrays","618","  files generated using :func:`sklearn.datasets.dump_svmlight_file` should be","624","- :func:`sklearn.utils.extmath.fast_svd` has been renamed","625","  :func:`sklearn.utils.extmath.randomized_svd` and the default","772","- Implementation of :class:`linear_model.LassoLarsCV`","774","  :class:`linear_model.LassoLarsIC` (BIC\/AIC model","808","  from :class:`base.BaseEstimator`.","950","- :class:`decomposition.PCA` is now usable from the Pipeline object by `Olivier Grisel`_.","961","  :class:`discriminant_analysis.LinearDiscriminantAnalysis` By `Mathieu Blondel`_.","1032","  :class:`linear_model.RidgeCV` [`Mathieu Blondel`_]","1044","- Performance improvements for :class:`cluster.KMeans` [`Gael","1049","- Refactoring of :class:`neighbors.NeighborsClassifier` and","1057","- Documentation improvements: Added :class:`pca.RandomizedPCA` and","1058","  :class:`linear_model.LogisticRegression` to the class","1065","  dense and sparse variants, like :class:`svm.LinearSVC` or","1066","  :class:`linear_model.LogisticRegression` [`Fabian Pedregosa`_].","1070","  :class:`pca.RandomizedPCA` [`James Bergstra`_].","1074","- Allow input sequences of different lengths in :class:`hmm.GaussianHMM`","1137","  multi_class in :class:`svm.LinearSVC`)","1143","  (:class:`grid_search.GridSearchCV`) as in modules","1163","  :class:`linear_model.LogisticRegression` model.","1220","  ``svm`` and ``linear_model`` (see :class:`svm.sparse.SVC`,","1221","  :class:`svm.sparse.SVR`, :class:`svm.sparse.LinearSVC`,","1222","  :class:`linear_model.sparse.Lasso`, :class:`linear_model.sparse.ElasticNet`)","1224","- New :class:`pipeline.Pipeline` object to compose different estimators.","1230","  linear_model module (:class:`linear_model.LassoCV`, :class:`linear_model.ElasticNetCV`,","1235","  :class:`linear_model.lars_path`, :class:`linear_model.Lars` and","1236","  :class:`linear_model.LassoLars`.","1239","  :class:`hmm.GaussianHMM`, :class:`hmm.MultinomialHMM`,","1240","  :class:`hmm.GMMHMM`)"]}],"doc\/modules\/neighbors.rst":[{"add":["112","unsupervised learning: in particular, see :class:`~sklearn.manifold.Isomap`,","113",":class:`~sklearn.manifold.LocallyLinearEmbedding`, and","114",":class:`~sklearn.cluster.SpectralClustering`.","341","and are computed using the class :class:`BallTree`.","473","similar to the label updating phase of the :class:`~sklearn.cluster.KMeans` algorithm.","477","assumed. See Linear Discriminant Analysis (:class:`~sklearn.discriminant_analysis.LinearDiscriminantAnalysis`)","478","and Quadratic Discriminant Analysis (:class:`~sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis`)","708","(:class:`~sklearn.decomposition.PCA`), Linear Discriminant Analysis","709","(:class:`~sklearn.discriminant_analysis.LinearDiscriminantAnalysis`) and"],"delete":["112","unsupervised learning: in particular, see :class:`sklearn.manifold.Isomap`,","113",":class:`sklearn.manifold.LocallyLinearEmbedding`, and","114",":class:`sklearn.cluster.SpectralClustering`.","341","and are computed using the class :class:`sklearn.neighbors.BallTree`.","473","similar to the label updating phase of the :class:`sklearn.cluster.KMeans` algorithm.","477","assumed. See Linear Discriminant Analysis (:class:`sklearn.discriminant_analysis.LinearDiscriminantAnalysis`)","478","and Quadratic Discriminant Analysis (:class:`sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis`)","708","(:class:`sklearn.decomposition.PCA`), Linear Discriminant Analysis","709","(:class:`sklearn.discriminant_analysis.LinearDiscriminantAnalysis`) and"]}],"doc\/modules\/svm.rst":[{"add":["502","is advised to use :class:`~sklearn.model_selection.GridSearchCV` with","669","    estimator used is :class:`~sklearn.linear_model.Ridge` regression,"],"delete":["502","is advised to use :class:`sklearn.model_selection.GridSearchCV` with ","669","    estimator used is :class:`sklearn.linear_model.Ridge <ridge>` regression,"]}],"doc\/modules\/density.rst":[{"add":["10","Gaussian Mixtures (:class:`~sklearn.mixture.GaussianMixture`), and","12","(:class:`~sklearn.neighbors.KernelDensity`).","60",":class:`~sklearn.neighbors.KernelDensity` estimator, which uses the","102",":class:`~sklearn.neighbors.KernelDensity` implements several common kernel","138","metrics (see :class:`~sklearn.neighbors.DistanceMetric` for a list of available metrics), though"],"delete":["10","Gaussian Mixtures (:class:`sklearn.mixture.GaussianMixture`), and","12","(:class:`sklearn.neighbors.KernelDensity`).","60",":class:`sklearn.neighbors.KernelDensity` estimator, which uses the","102",":class:`sklearn.neighbors.KernelDensity` implements several common kernel","138","metrics (see :class:`sklearn.neighbors.DistanceMetric` for a list of available metrics), though"]}],"doc\/computing\/computational_performance.rst":[{"add":["6",".. currentmodule:: sklearn","7","","84","scikit-learn, or configure it in Python with :func:`set_config`.","93",":func:`~utils.assert_all_finite` within the context.","165",":class:`~linear_model.SGDClassifier` with the","187",":class:`~svm.NuSVR` was used to influence the number of","200",":class:`~ensemble.GradientBoostingRegressor`.","307","working memory (defaulting to 1GB) using :func:`set_config` or","316",":func:`~metrics.pairwise_distances_chunked`, which facilitates computing"],"delete":["82","scikit-learn, or configure it in Python with :func:`sklearn.set_config`.","91",":func:`sklearn.utils.assert_all_finite` within the context.","163",":class:`sklearn.linear_model.SGDClassifier` with the","185",":class:`sklearn.svm.NuSVR` was used to influence the number of","198",":class:`sklearn.ensemble.gradient_boosting.GradientBoostingRegressor`.","305","working memory (defaulting to 1GB) using :func:`sklearn.set_config` or","314",":func:`metric.pairwise_distances_chunked`, which facilitates computing"]}]}},"94c70ff235a19312063ef089ef587957f40db656":{"changes":{"sklearn\/preprocessing\/_encoders.py":"MODIFY"},"diff":{"sklearn\/preprocessing\/_encoders.py":[{"add":["423","            raise ValueError(\"OneHotEncoder in legacy mode cannot handle \"","424","                             \"categories encoded as negative integers. \"","425","                             \"Please set categories='auto' explicitly to \"","426","                             \"be able to use arbitrary integer values as \"","427","                             \"category identifiers.\")","510","            raise ValueError(\"OneHotEncoder in legacy mode cannot handle \"","511","                             \"categories encoded as negative integers. \"","512","                             \"Please set categories='auto' explicitly to \"","513","                             \"be able to use arbitrary integer values as \"","514","                             \"category identifiers.\")"],"delete":["423","            raise ValueError(\"X needs to contain only non-negative integers.\")","506","            raise ValueError(\"X needs to contain only non-negative integers.\")"]}]}},"93e09aaae68ec2fc2d7b78818364ca868442e61e":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/cluster\/hierarchical.py":"MODIFY","sklearn\/cluster\/tests\/test_hierarchical.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["78","- |API| The ``n_components_`` attribute in :class:`cluster.AgglomerativeClustering`","79","  and :class:`cluster.FeatureAgglomeration` has been renamed to","80","  ``n_connected_components_``.","81","  :issue:`13427` by :user:`Stephane Couvreur <scouvreur>`.","82",""],"delete":[]}],"sklearn\/cluster\/hierarchical.py":[{"add":["25","from ..utils import deprecated","56","    n_connected_components, labels = connected_components(connectivity)","58","    if n_connected_components > 1:","61","                      \"stopping the tree early.\" % n_connected_components,","64","        for i in range(n_connected_components):","77","    return connectivity, n_connected_components","81","                         n_connected_components, return_distance):","127","        return children_, n_connected_components, n_samples, parent, distances","128","    return children_, n_connected_components, n_samples, parent","179","    n_connected_components : int","241","    connectivity, n_connected_components = _fix_connectivity(","242","                                                X, connectivity,","243","                                                affinity='euclidean')","336","        return children, n_connected_components, n_leaves, parent, distances","338","        return children, n_connected_components, n_leaves, parent","399","    n_connected_components : int","470","    connectivity, n_connected_components = _fix_connectivity(","471","                                                X, connectivity,","472","                                                affinity=affinity)","500","                                    n_clusters, n_connected_components,","501","                                    return_distance)","571","        return children, n_connected_components, n_leaves, parent, distances","572","    return children, n_connected_components, n_leaves, parent","721","    n_connected_components_ : int","760","    @property","761","    @deprecated(\"The ``n_components_`` attribute was deprecated \"","762","                \"in favor of ``n_connected_components_`` in 0.21 \"","763","                \"and will be removed in 0.23.\")","764","    def n_components_(self):","765","        return self.n_connected_components_","766","","830","        (self.children_, self.n_connected_components_, self.n_leaves_,","831","            parents) = memory.cache(tree_builder)(X, connectivity,","832","                                                  n_clusters=n_clusters,","833","                                                  **kwargs)","834","","914","    n_connected_components_ : int"],"delete":["29","","56","    n_components, labels = connected_components(connectivity)","58","    if n_components > 1:","61","                      \"stopping the tree early.\" % n_components,","64","        for i in range(n_components):","77","    return connectivity, n_components","81","                         n_components, return_distance):","127","        return children_, n_components, n_samples, parent, distances","128","    return children_, n_components, n_samples, parent","179","    n_components : int","241","    connectivity, n_components = _fix_connectivity(X, connectivity,","242","                                                   affinity='euclidean')","335","        return children, n_components, n_leaves, parent, distances","337","        return children, n_components, n_leaves, parent","398","    n_components : int","469","    connectivity, n_components = _fix_connectivity(X, connectivity,","470","                                                   affinity=affinity)","471","","499","                                    n_clusters, n_components, return_distance)","569","        return children, n_components, n_leaves, parent, distances","570","    return children, n_components, n_leaves, parent","719","    n_components_ : int","821","        self.children_, self.n_components_, self.n_leaves_, parents = \\","822","            memory.cache(tree_builder)(X, connectivity,","823","                                       n_clusters=n_clusters,","824","                                       **kwargs)","904","    n_components_ : int"]}],"sklearn\/cluster\/tests\/test_hierarchical.py":[{"add":["600","","601","","602","def test_n_components_deprecation():","603","    # Test that a Deprecation warning is thrown when n_components_","604","    # attribute is accessed","605","","606","    X = np.array([[1, 2], [1, 4], [1, 0], [4, 2]])","607","    agc = AgglomerativeClustering().fit(X)","608","","609","    match = (\"``n_components_`` attribute was deprecated \"","610","             \"in favor of ``n_connected_components_``\")","611","    with pytest.warns(DeprecationWarning, match=match):","612","        n = agc.n_components_","613","    assert n == agc.n_connected_components_"],"delete":[]}]}},"02fad7ab24959e59e8f7791bd9c3a353115ba7c8":{"changes":{"doc\/conftest.py":"MODIFY"},"diff":{"doc\/conftest.py":[{"add":["72","    is_index = fname.endswith('datasets\/index.rst')","73","    if fname.endswith('datasets\/labeled_faces.rst') or is_index:","75","    elif fname.endswith('datasets\/mldata.rst') or is_index:","77","    elif fname.endswith('datasets\/rcv1.rst') or is_index:","79","    elif fname.endswith('datasets\/twenty_newsgroups.rst') or is_index:","81","    elif fname.endswith('tutorial\/text_analytics\/working_with_text_data.rst')\\","82","            or is_index:","84","    elif fname.endswith('modules\/compose.rst') or is_index:"],"delete":["72","    if fname.endswith('datasets\/labeled_faces.rst'):","74","    elif fname.endswith('datasets\/mldata.rst'):","76","    elif fname.endswith('datasets\/rcv1.rst'):","78","    elif fname.endswith('datasets\/twenty_newsgroups.rst'):","80","    elif fname.endswith('tutorial\/text_analytics\/working_with_text_data.rst'):","82","    elif fname.endswith('modules\/compose.rst'):"]}]}},"4f3c60c82e1540fb2f384d25952e3d25e81b73ab":{"changes":{"sklearn\/ensemble\/iforest.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/ensemble\/tests\/test_iforest.py":"MODIFY"},"diff":{"sklearn\/ensemble\/iforest.py":[{"add":["250","        X = check_array(X, accept_sparse='csr')","256","        if self._max_features == X.shape[1]:","257","            subsample_features = False","258","        else:","259","            subsample_features = True","260","","261","        for i, (tree, features) in enumerate(zip(self.estimators_,","262","                                                 self.estimators_features_)):","263","            if subsample_features:","264","                X_subset = X[:, features]","265","            else:","266","                X_subset = X","267","            leaves_index = tree.apply(X_subset)","268","            node_indicator = tree.decision_path(X_subset)","270","            depths[:, i] = np.ravel(node_indicator.sum(axis=1))","271","            depths[:, i] -= 1"],"delete":["250","        X = self.estimators_[0]._validate_X_predict(X, check_input=True)","256","        for i, tree in enumerate(self.estimators_):","257","            leaves_index = tree.apply(X)","258","            node_indicator = tree.decision_path(X)","260","            depths[:, i] = np.asarray(node_indicator.sum(axis=1)).reshape(-1) - 1"]}],"doc\/whats_new.rst":[{"add":["138","  ","139","   - Fixed a bug where :class:`sklearn.ensemble.IsolationForest` fails when ","140","     ``max_features`` is less than 1.","141","     :issue:`5732` by :user:`Ishank Gulati <IshankGulati>`."],"delete":[]}],"sklearn\/ensemble\/tests\/test_iforest.py":[{"add":["202","","203","","204","def test_iforest_subsampled_features():","205","    # It tests non-regression for #5732 which failed at predict.","206","    rng = check_random_state(0)","207","    X_train, X_test, y_train, y_test = train_test_split(boston.data[:50],","208","                                                        boston.target[:50],","209","                                                        random_state=rng)","210","    clf = IsolationForest(max_features=0.8)","211","    clf.fit(X_train, y_train)","212","    clf.predict(X_test)"],"delete":[]}]}},"9cd13a1fa16708f94a0d821ac2865fa7d981cad8":{"changes":{"sklearn\/calibration.py":"MODIFY","sklearn\/multioutput.py":"MODIFY","sklearn\/model_selection\/tests\/test_search.py":"MODIFY","sklearn\/discriminant_analysis.py":"MODIFY","sklearn\/svm\/_classes.py":"MODIFY","sklearn\/linear_model\/_logistic.py":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_gradient_boosting.py":"MODIFY","sklearn\/ensemble\/tests\/test_stacking.py":"MODIFY","sklearn\/metrics\/_plot\/tests\/test_plot_precision_recall.py":"MODIFY"},"diff":{"sklearn\/calibration.py":[{"add":["34","class CalibratedClassifierCV(ClassifierMixin,","35","                             MetaEstimatorMixin,","36","                             BaseEstimator):"],"delete":["34","class CalibratedClassifierCV(BaseEstimator, ClassifierMixin,","35","                             MetaEstimatorMixin):"]}],"sklearn\/multioutput.py":[{"add":["62","class _MultiOutputEstimator(MetaEstimatorMixin,","63","                            BaseEstimator,"],"delete":["62","class _MultiOutputEstimator(BaseEstimator, MetaEstimatorMixin,"]}],"sklearn\/model_selection\/tests\/test_search.py":[{"add":["1823","    class TestEstimator(ClassifierMixin, BaseEstimator):"],"delete":["1823","    class TestEstimator(BaseEstimator, ClassifierMixin):"]}],"sklearn\/discriminant_analysis.py":[{"add":["130","class LinearDiscriminantAnalysis(LinearClassifierMixin,","131","                                 TransformerMixin,","132","                                 BaseEstimator):"],"delete":["130","class LinearDiscriminantAnalysis(BaseEstimator, LinearClassifierMixin,","131","                                 TransformerMixin):"]}],"sklearn\/svm\/_classes.py":[{"add":["12","class LinearSVC(LinearClassifierMixin,","13","                SparseCoefMixin,","14","                BaseEstimator):"],"delete":["12","class LinearSVC(BaseEstimator, LinearClassifierMixin,","13","                SparseCoefMixin):"]}],"sklearn\/linear_model\/_logistic.py":[{"add":["1010","class LogisticRegression(LinearClassifierMixin,","1011","                         SparseCoefMixin,","1012","                         BaseEstimator):","1501","class LogisticRegressionCV(LogisticRegression,","1502","                           LinearClassifierMixin,","1503","                           BaseEstimator):"],"delete":["1010","class LogisticRegression(BaseEstimator, LinearClassifierMixin,","1011","                         SparseCoefMixin):","1500","class LogisticRegressionCV(LogisticRegression, BaseEstimator,","1501","                           LinearClassifierMixin):"]}],"sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_gradient_boosting.py":[{"add":["394","    class MinMaxImputer(TransformerMixin, BaseEstimator):"],"delete":["394","    class MinMaxImputer(BaseEstimator, TransformerMixin):"]}],"sklearn\/ensemble\/tests\/test_stacking.py":[{"add":["264","class NoWeightRegressor(RegressorMixin, BaseEstimator):","273","class NoWeightClassifier(ClassifierMixin, BaseEstimator):"],"delete":["264","class NoWeightRegressor(BaseEstimator, RegressorMixin):","273","class NoWeightClassifier(BaseEstimator, ClassifierMixin):"]}],"sklearn\/metrics\/_plot\/tests\/test_plot_precision_recall.py":[{"add":["65","    class MyClassifier(ClassifierMixin, BaseEstimator):"],"delete":["65","    class MyClassifier(BaseEstimator, ClassifierMixin):"]}]}},"9b1928dbc233c052366f5686432e017022bcaa5d":{"changes":{".github\/labeler.yml":"ADD",".github\/workflows\/labeler.yml":"ADD"},"diff":{".github\/labeler.yml":[{"add":[],"delete":[]}],".github\/workflows\/labeler.yml":[{"add":[],"delete":[]}]}},"84bc8d341e5a0a0d0b20b8acda58a33d72a2c23a":{"changes":{"sklearn\/svm\/_base.py":"MODIFY","doc\/whats_new\/v0.23.rst":"MODIFY","sklearn\/svm\/tests\/test_svm.py":"MODIFY"},"diff":{"sklearn\/svm\/_base.py":[{"add":["16","from ..utils.validation import _num_samples","17","from ..utils.validation import _check_sample_weight, check_consistent_length","146","        if callable(self.kernel):","147","            check_consistent_length(X, y)","148","        else:","149","            X, y = check_X_y(X, y, dtype=np.float64,","150","                             order='C', accept_sparse='csr',","151","                             accept_large_sparse=False)","152","","161","        n_samples = _num_samples(X)","162","        if solver_type != 2 and n_samples != y.shape[0]:","165","                             (n_samples, y.shape[0]))","167","        if self.kernel == \"precomputed\" and n_samples != X.shape[1]:","172","        if sample_weight.shape[0] > 0 and sample_weight.shape[0] != n_samples:","179","        kernel = 'precomputed' if callable(self.kernel) else self.kernel","180","","181","        if kernel == 'precomputed':","182","            # unused but needs to be a float for cython code that ignores","183","            # it anyway","184","            self._gamma = 0.","185","        elif isinstance(self.gamma, str):","209","        self.shape_fit_ = X.shape if hasattr(X, \"shape\") else (n_samples, )","453","        if not callable(self.kernel):","454","            X = check_array(X, accept_sparse='csr', dtype=np.float64,","455","                            order=\"C\", accept_large_sparse=False)","456","","472","        elif not callable(self.kernel) and X.shape[1] != self.shape_fit_[1]:","475","                             (X.shape[1], self.shape_fit_[1]))","931","            raise ValueError(\"Intercept scaling is %r but needs to be greater \"","932","                             \"than 0. To disable fitting an intercept,\""],"delete":["16","from ..utils.validation import _check_sample_weight","145","        X, y = check_X_y(X, y, dtype=np.float64,","146","                         order='C', accept_sparse='csr',","147","                         accept_large_sparse=False)","156","        if solver_type != 2 and X.shape[0] != y.shape[0]:","159","                             (X.shape[0], y.shape[0]))","161","        if self.kernel == \"precomputed\" and X.shape[0] != X.shape[1]:","166","        if sample_weight.shape[0] > 0 and sample_weight.shape[0] != X.shape[0]:","173","        if isinstance(self.gamma, str):","189","        kernel = self.kernel","190","        if callable(kernel):","191","            kernel = 'precomputed'","192","","201","        self.shape_fit_ = X.shape","445","        X = check_array(X, accept_sparse='csr', dtype=np.float64, order=\"C\",","446","                        accept_large_sparse=False)","456","        n_samples, n_features = X.shape","463","        elif n_features != self.shape_fit_[1]:","466","                             (n_features, self.shape_fit_[1]))","922","            raise ValueError(\"Intercept scaling is %r but needs to be greater than 0.\"","923","                             \" To disable fitting an intercept,\""]}],"doc\/whats_new\/v0.23.rst":[{"add":["25","- list models here","212","- |Fix| Fix use of custom kernel not taking float entries such as string","213","  kernels in :class:`svm.SVC` and :class:`svm.SVR`. Note that custom kennels","214","  are now expected to validate their input where they previously received","215","  valid numeric arrays.","216","  :pr:`11296` by `Alexandre Gramfort`_ and  :user:`Georgi Peev <georgipeev>`.","217",""],"delete":["25","- models come here"]}],"sklearn\/svm\/tests\/test_svm.py":[{"add":["22","from sklearn.utils._testing import assert_raise_message","25","from sklearn.utils.validation import _num_samples","128","    clf.fit(np.array(X), Y)","545","    [(svm.SVC, {'when-left': [0.3998, 0.4], 'when-right': [0.4, 0.3999]}),","546","     (svm.NuSVC, {'when-left': [0.3333, 0.3333],","684","                             iris.target.astype(np.float64), 5,","685","                             kernel='linear',","686","                             random_seed=0)","983","    assert_warns(ConvergenceWarning, a.fit, np.array(X), Y)","1029","        assert_array_almost_equal(","1030","            svr.predict(X), np.dot(X, svr.coef_.ravel()) + svr.intercept_","1031","        )","1098","    ))","1252","","1253","","1254","@pytest.mark.parametrize(\"Estimator\", [svm.SVC, svm.SVR])","1255","def test_custom_kernel_not_array_input(Estimator):","1256","    \"\"\"Test using a custom kernel that is not fed with array-like for floats\"\"\"","1257","    data = [\"A A\", \"A\", \"B\", \"B B\", \"A B\"]","1258","    X = np.array([[2, 0], [1, 0], [0, 1], [0, 2], [1, 1]])  # count encoding","1259","    y = np.array([1, 1, 2, 2, 1])","1260","","1261","    def string_kernel(X1, X2):","1262","        assert isinstance(X1[0], str)","1263","        n_samples1 = _num_samples(X1)","1264","        n_samples2 = _num_samples(X2)","1265","        K = np.zeros((n_samples1, n_samples2))","1266","        for ii in range(n_samples1):","1267","            for jj in range(ii, n_samples2):","1268","                K[ii, jj] = X1[ii].count('A') * X2[jj].count('A')","1269","                K[ii, jj] += X1[ii].count('B') * X2[jj].count('B')","1270","                K[jj, ii] = K[ii, jj]","1271","        return K","1272","","1273","    K = string_kernel(data, data)","1274","    assert_array_equal(np.dot(X, X.T), K)","1275","","1276","    svc1 = Estimator(kernel=string_kernel).fit(data, y)","1277","    svc2 = Estimator(kernel='linear').fit(X, y)","1278","    svc3 = Estimator(kernel='precomputed').fit(K, y)","1279","","1280","    assert svc1.score(data, y) == svc3.score(K, y)","1281","    assert svc1.score(data, y) == svc2.score(X, y)","1282","    if hasattr(svc1, 'decision_function'):  # classifier","1283","        assert_allclose(svc1.decision_function(data),","1284","                        svc2.decision_function(X))","1285","        assert_allclose(svc1.decision_function(data),","1286","                        svc3.decision_function(K))","1287","        assert_array_equal(svc1.predict(data), svc2.predict(X))","1288","        assert_array_equal(svc1.predict(data), svc3.predict(K))","1289","    else:  # regressor","1290","        assert_allclose(svc1.predict(data), svc2.predict(X))","1291","        assert_allclose(svc1.predict(data), svc3.predict(K))"],"delete":["22","from sklearn.utils._testing import assert_warns_message, assert_raise_message","127","    clf.fit(X, Y)","544","    [(svm.SVC, {'when-left': [0.3998,  0.4], 'when-right': [0.4,  0.3999]}),","545","     (svm.NuSVC, {'when-left': [0.3333,  0.3333],","683","                                iris.target.astype(np.float64), 5,","684","                                kernel='linear',","685","                                random_seed=0)","982","    assert_warns(ConvergenceWarning, a.fit, X, Y)","1028","        assert_array_almost_equal(svr.predict(X),","1029","                                  np.dot(X, svr.coef_.ravel()) + svr.intercept_)","1096","        ))"]}]}},"788a458bba353c2cf3cfa5a15d6f68315149ef9e":{"changes":{"benchmarks\/bench_lof.py":"ADD","examples\/covariance\/plot_outlier_detection.py":"MODIFY","doc\/modules\/classes.rst":"MODIFY","doc\/modules\/outlier_detection.rst":"MODIFY","sklearn\/neighbors\/tests\/test_lof.py":"ADD","sklearn\/neighbors\/unsupervised.py":"MODIFY","doc\/whats_new.rst":"MODIFY","examples\/neighbors\/plot_lof.py":"ADD","sklearn\/neighbors\/classification.py":"MODIFY","sklearn\/neighbors\/__init__.py":"MODIFY","sklearn\/neighbors\/lof.py":"ADD","sklearn\/neighbors\/regression.py":"MODIFY"},"diff":{"benchmarks\/bench_lof.py":[{"add":[],"delete":[]}],"examples\/covariance\/plot_outlier_detection.py":[{"add":["20","- using the Local Outlier Factor to measure the local deviation of a given","21","  data point with respect to its neighbors by comparing their local density.","22","","41","from sklearn.neighbors import LocalOutlierFactor","42","","43","print(__doc__)","59","                                        random_state=rng),","60","    \"Local Outlier Factor\": LocalOutlierFactor(","61","        n_neighbors=35,","62","        contamination=outliers_fraction)}","65","xx, yy = np.meshgrid(np.linspace(-7, 7, 100), np.linspace(-7, 7, 100))","82","    plt.figure(figsize=(9, 7))","85","        if clf_name == \"Local Outlier Factor\":","86","            y_pred = clf.fit_predict(X)","87","            scores_pred = clf.negative_outlier_factor_","88","        else:","89","            clf.fit(X)","90","            scores_pred = clf.decision_function(X)","91","            y_pred = clf.predict(X)","96","        if clf_name == \"Local Outlier Factor\":","97","            # decision_function is private for LOF","98","            Z = clf._decision_function(np.c_[xx.ravel(), yy.ravel()])","99","        else:","100","            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])","102","        subplot = plt.subplot(2, 2, i + 1)","115","            prop=matplotlib.font_manager.FontProperties(size=10),","117","        subplot.set_xlabel(\"%d. %s (errors: %d)\" % (i + 1, clf_name, n_errors))","120","    plt.subplots_adjust(0.04, 0.1, 0.96, 0.94, 0.1, 0.26)","121","    plt.suptitle(\"Outlier detection\")"],"delete":["29","print(__doc__)","54","                                        random_state=rng)}","57","xx, yy = np.meshgrid(np.linspace(-7, 7, 500), np.linspace(-7, 7, 500))","74","    plt.figure(figsize=(10.8, 3.6))","77","        clf.fit(X)","78","        scores_pred = clf.decision_function(X)","81","        y_pred = clf.predict(X)","84","        Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])","86","        subplot = plt.subplot(1, 3, i + 1)","99","            prop=matplotlib.font_manager.FontProperties(size=11),","101","        subplot.set_title(\"%d. %s (errors: %d)\" % (i + 1, clf_name, n_errors))","104","    plt.subplots_adjust(0.04, 0.1, 0.96, 0.92, 0.1, 0.26)"]}],"doc\/modules\/classes.rst":[{"add":["1052","   neighbors.LocalOutlierFactor","1053","\t      "],"delete":["1052",""]}],"doc\/modules\/outlier_detection.rst":[{"add":["167","     :class:`neighbors.LocalOutlierFactor`,","170","     :class:`covariance.EllipticEnvelope`.","178","Local Outlier Factor","179","--------------------","180","Another efficient way to perform outlier detection on moderately high dimensional","181","datasets is to use the Local Outlier Factor (LOF) algorithm.","182","","183","The :class:`neighbors.LocalOutlierFactor` (LOF) algorithm computes a score","184","(called local outlier factor) reflecting the degree of abnormality of the","185","observations.","186","It measures the local density deviation of a given data point with respect to","187","its neighbors. The idea is to detect the samples that have a substantially","188","lower density than their neighbors.","189","","190","In practice the local density is obtained from the k-nearest neighbors.","191","The LOF score of an observation is equal to the ratio of the","192","average local density of his k-nearest neighbors, and its own local density:","193","a normal instance is expected to have a local density similar to that of its","194","neighbors, while abnormal data are expected to have much smaller local density.","195","","196","The number k of neighbors considered, (alias parameter n_neighbors) is typically","197","chosen 1) greater than the minimum number of objects a cluster has to contain,","198","so that other objects can be local outliers relative to this cluster, and 2)","199","smaller than the maximum number of close by objects that can potentially be","200","local outliers.","201","In practice, such informations are generally not available, and taking","202","n_neighbors=20 appears to work well in general.","203","When the proportion of outliers is high (i.e. greater than 10 \\%, as in the","204","example below), n_neighbors should be greater (n_neighbors=35 in the example","205","below).","206","","207","The strength of the LOF algorithm is that it takes both local and global","208","properties of datasets into consideration: it can perform well even in datasets","209","where abnormal samples have different underlying densities.","210","The question is not, how isolated the sample is, but how isolated it is","211","with respect to the surrounding neighborhood.","212","","213","This strategy is illustrated below.","214","","215",".. figure:: ..\/auto_examples\/neighbors\/images\/sphx_glr_plot_lof_001.png","216","   :target: ..\/auto_examples\/neighbors\/plot_lof.html","217","   :align: center","218","   :scale: 75%","219","","220",".. topic:: Examples:","221","","222","   * See :ref:`sphx_glr_auto_example_neighbors_plot_lof.py` for","223","     an illustration of the use of :class:`neighbors.LocalOutlierFactor`.","224","","225","   * See :ref:`sphx_glr_auto_example_covariance_plot_outlier_detection.py` for a","226","     comparison with other anomaly detection methods.","227","","228",".. topic:: References:","229","","230","   .. [BKNS2000]  Breunig, Kriegel, Ng, and Sander (2000)","231","      `LOF: identifying density-based local outliers.","232","      <http:\/\/www.dbs.ifi.lmu.de\/Publikationen\/Papers\/LOF.pdf>`_","233","      Proc. ACM SIGMOD","234","","235","One-class SVM versus Elliptic Envelope versus Isolation Forest versus LOF","236","-------------------------------------------------------------------------","248","multiple modes and :class:`ensemble.IsolationForest` and","249",":class:`neighbors.LocalOutlierFactor` perform well in every cases.","263",".. list-table:: **Comparing One-class SVM, Isolation Forest, LOF, and Elliptic Envelope**","274","\tand :class:`neighbors.LocalOutlierFactor` perform as well.","275","      - |outlier1| ","280","        inliers. However, we can see that :class:`ensemble.IsolationForest`,","281","\t:class:`svm.OneClassSVM` and :class:`neighbors.LocalOutlierFactor`","282","\thave difficulties to detect the two modes,","284","        tends to overfit: because it has no model of inliers, it","292","        approximation as well as :class:`ensemble.IsolationForest`","293","        and :class:`neighbors.LocalOutlierFactor`,","301","     an outlier detection method), the :class:`ensemble.IsolationForest`,","302","     the :class:`neighbors.LocalOutlierFactor`","303","     and a covariance-based outlier detection :class:`covariance.EllipticEnvelope`."],"delete":["169","     :class:`covariance.MinCovDet`.","177","One-class SVM versus Elliptic Envelope versus Isolation Forest","178","--------------------------------------------------------------","190","multiple modes and :class:`ensemble.IsolationForest` performs well in every cases.","204",".. list-table:: **Comparing One-class SVM approach, and elliptic envelope**","215","\tperforms as well.","216","      - |outlier1|","221","        inliers. However, we can see that both :class:`ensemble.IsolationForest`","222","\tand :class:`svm.OneClassSVM` have difficulties to detect the two modes,","224","        tends to overfit: because it has not model of inliers, it","232","        approximation as well as :class:`ensemble.IsolationForest`,","240","     an outlier detection method), the :class:`ensemble.IsolationForest`","241","     and a covariance-based outlier","242","     detection with :class:`covariance.MinCovDet`."]}],"sklearn\/neighbors\/tests\/test_lof.py":[{"add":[],"delete":[]}],"sklearn\/neighbors\/unsupervised.py":[{"add":["17","        Number of neighbors to use by default for :meth:`kneighbors` queries.","79","        Affects only :meth:`kneighbors` and :meth:`kneighbors_graph` methods."],"delete":["17","        Number of neighbors to use by default for :meth:`k_neighbors` queries.","79","        Affects only :meth:`k_neighbors` and :meth:`kneighbors_graph` methods."]}],"doc\/whats_new.rst":[{"add":["18","   - Added the :class:`neighbors.LocalOutlierFactor` class for anomaly detection based","19","     on nearest neighbors. By `Nicolas Goix`_ and `Alexandre Gramfort`_.","20","","4745",".. _Nicolas Goix: https:\/\/perso.telecom-paristech.fr\/~goix\/"],"delete":["4742",".. _Nicolas Goix: https:\/\/webperso.telecom-paristech.fr\/front\/frontoffice.php?SP_ID=241"]}],"examples\/neighbors\/plot_lof.py":[{"add":[],"delete":[]}],"sklearn\/neighbors\/classification.py":[{"add":["31","        Number of neighbors to use by default for :meth:`kneighbors` queries."],"delete":["31","        Number of neighbors to use by default for :meth:`k_neighbors` queries."]}],"sklearn\/neighbors\/__init__.py":[{"add":["15","from .lof import LocalOutlierFactor","29","           'LSHForest',","30","           'LocalOutlierFactor']"],"delete":["28","           'LSHForest']"]}],"sklearn\/neighbors\/lof.py":[{"add":[],"delete":[]}],"sklearn\/neighbors\/regression.py":[{"add":["31","        Number of neighbors to use by default for :meth:`kneighbors` queries."],"delete":["31","        Number of neighbors to use by default for :meth:`k_neighbors` queries."]}]}},"9f015c8a14a67d248599dc376d33ec612dd9dbb9":{"changes":{"sklearn\/utils\/validation.py":"MODIFY"},"diff":{"sklearn\/utils\/validation.py":[{"add":["509","                dtypes_orig[i] = np.dtype(np.object)"],"delete":["509","                dtypes_orig[i] = np.object"]}]}},"73caba5dabfc0157ce2631b3f94ec1fb78d3ded3":{"changes":{"sklearn\/metrics\/_classification.py":"MODIFY","sklearn\/metrics\/tests\/test_classification.py":"MODIFY","doc\/whats_new\/v0.24.rst":"MODIFY"},"diff":{"sklearn\/metrics\/_classification.py":[{"add":["1485","            zero_division_value = np.float64(1.0)","1486","            if zero_division in [\"warn\", 0]:","1487","                zero_division_value = np.float64(0.0)","1492","            if pred_sum.sum() == 0:","1493","                return (zero_division_value,","1494","                        zero_division_value,","1495","                        zero_division_value,","1496","                        None)","1497","            else:","1498","                return (np.float64(0.0),","1499","                        zero_division_value,","1500","                        np.float64(0.0),","1501","                        None)"],"delete":["1485","            zero_division_value = 0.0 if zero_division in [\"warn\", 0] else 1.0","1490","            return (zero_division_value if pred_sum.sum() == 0 else 0,","1491","                    zero_division_value,","1492","                    zero_division_value if pred_sum.sum() == 0 else 0,","1493","                    None)"]}],"sklearn\/metrics\/tests\/test_classification.py":[{"add":["155","def test_classification_report_output_dict_empty_input():","156","    report = classification_report(y_true=[], y_pred=[], output_dict=True)","157","    expected_report = {'accuracy': 0.0,","158","                       'macro avg': {'f1-score': np.nan,","159","                                     'precision': np.nan,","160","                                     'recall': np.nan,","161","                                     'support': 0},","162","                       'weighted avg': {'f1-score': 0.0,","163","                                        'precision': 0.0,","164","                                        'recall': 0.0,","165","                                        'support': 0}}","166","    assert isinstance(report, dict)","167","    # assert the 2 dicts are equal.","168","    assert report.keys() == expected_report.keys()","169","    for key in expected_report:","170","        if key == 'accuracy':","171","            assert isinstance(report[key], float)","172","            assert report[key] == expected_report[key]","173","        else:","174","            assert report[key].keys() == expected_report[key].keys()","175","            for metric in expected_report[key]:","176","                assert_almost_equal(expected_report[key][metric],","177","                                    report[key][metric])","178","","179",""],"delete":[]}],"doc\/whats_new\/v0.24.rst":[{"add":["253","  ","254","- |Fix| Fixed a bug in ","255","  :func:`metrics.classification_report` which was raising AttributeError","256","  when called with `output_dict=True` for 0-length values.","257","  :pr:`17777` by :user:`Shubhanshu Mishra <napsternxg>`"],"delete":[]}]}},"5431a1a9bfaf1927fade31492ac2b63423ad6384":{"changes":{"sklearn\/model_selection\/_split.py":"MODIFY","sklearn\/model_selection\/tests\/test_validation.py":"MODIFY"},"diff":{"sklearn\/model_selection\/_split.py":[{"add":["424","        if n_splits == 'warn':","495","        if n_splits == 'warn':","596","        if n_splits == 'warn':","750","        if n_splits == 'warn':","1941","    if cv is None or cv == 'warn':"],"delete":["424","        if n_splits is 'warn':","495","        if n_splits is 'warn':","596","        if n_splits is 'warn':","750","        if n_splits is 'warn':","1941","    if cv is None or cv is 'warn':"]}],"sklearn\/model_selection\/tests\/test_validation.py":[{"add":["283","@pytest.mark.filterwarnings('ignore:You should specify a value for')  # 0.22","284","def test_cross_validate_many_jobs():","285","    # regression test for #12154: cv='warn' with n_jobs>1 trigger a copy of","286","    # the parameters leading to a failure in check_cv due to cv is 'warn'","287","    # instead of cv == 'warn'.","288","    X, y = load_iris(return_X_y=True)","289","    clf = SVC(gamma='auto')","290","    grid = GridSearchCV(clf, param_grid={'C': [1, 10]})","291","    cross_validate(grid, X, y, n_jobs=2)","292","","293",""],"delete":[]}]}},"5925fb9e4fb372f254b5401c6011215132fa61ee":{"changes":{"sklearn\/ensemble\/tests\/test_voting.py":"MODIFY","sklearn\/utils\/estimator_checks.py":"MODIFY","sklearn\/tests\/test_common.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY","sklearn\/ensemble\/voting.py":"MODIFY"},"diff":{"sklearn\/ensemble\/tests\/test_voting.py":[{"add":["8","from sklearn.utils.estimator_checks import check_estimator","9","from sklearn.utils.estimator_checks import check_no_attributes_set_in_init","17","from sklearn.tree import DecisionTreeClassifier","18","from sklearn.tree import DecisionTreeRegressor","514","","515","","516","@pytest.mark.parametrize(","517","    \"estimator\",","518","    [VotingRegressor(","519","        estimators=[('lr', LinearRegression()),","520","                    ('tree', DecisionTreeRegressor(random_state=0))]),","521","     VotingClassifier(","522","         estimators=[('lr', LogisticRegression(random_state=0)),","523","                     ('tree', DecisionTreeClassifier(random_state=0))])],","524","    ids=['VotingRegressor', 'VotingClassifier']","525",")","526","def test_check_estimators_voting_estimator(estimator):","527","    # FIXME: to be removed when meta-estimators can be specified themselves","528","    # their testing parameters (for required parameters).","529","    check_estimator(estimator)","530","    check_no_attributes_set_in_init(estimator.__class__.__name__, estimator)"],"delete":[]}],"sklearn\/utils\/estimator_checks.py":[{"add":[],"delete":["32",""]}],"sklearn\/tests\/test_common.py":[{"add":["25","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"],"delete":["27","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"]}],"doc\/whats_new\/v0.22.rst":[{"add":["117","- |Fix| Run by default","118","  :func:`utils.estimator_checks.check_estimator` on both","119","  :class:`ensemble.VotingClassifier` and :class:`ensemble.VotingRegressor`. It","120","  leads to solve issues regarding shape consistency during `predict` which was","121","  failing when the underlying estimators were not outputting consistent array","122","  dimensions. Note that it should be replaced by refactoring the common tests","123","  in the future.","124","  :pr:`14305` by :user:`Guillaume Lemaitre <glemaitre>`.","125",""],"delete":[]}],"sklearn\/ensemble\/voting.py":[{"add":["17","import numpy as np","18","","26","from ..utils import Bunch","29","from ..utils.multiclass import check_classification_targets","30","from ..utils.validation import column_or_1d","72","        return np.asarray([est.predict(X) for est in self.estimators_]).T","269","        check_classification_targets(y)","460","        y = column_or_1d(y, warn=True)"],"delete":["15","import numpy as np","27","from ..utils import Bunch","69","        return np.asarray([clf.predict(X) for clf in self.estimators_]).T"]}]}},"174f4aea6e544f2126ab2d920ec51d31d148d703":{"changes":{"sklearn\/linear_model\/tests\/test_omp.py":"MODIFY","doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/decomposition\/tests\/test_dict_learning.py":"MODIFY","sklearn\/linear_model\/omp.py":"MODIFY"},"diff":{"sklearn\/linear_model\/tests\/test_omp.py":[{"add":["106","def test_orthogonal_mp_gram_readonly():","107","    # Non-regression test for:","108","    # https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/5956","109","    idx, = gamma[:, 0].nonzero()","110","    G_readonly = G.copy()","111","    G_readonly.setflags(write=False)","112","    Xy_readonly = Xy.copy()","113","    Xy_readonly.setflags(write=False)","114","    gamma_gram = orthogonal_mp_gram(G_readonly, Xy_readonly[:, 0], 5,","115","                                    copy_Gram=False, copy_Xy=False)","116","    assert_array_equal(idx, np.flatnonzero(gamma_gram))","117","    assert_array_almost_equal(gamma[:, 0], gamma_gram, decimal=2)","118","","119",""],"delete":[]}],"doc\/whats_new\/v0.20.rst":[{"add":["508","- Fixed a bug in :class:`decomposition.SparseCoder` when running OMP sparse","509","  coding in parallel using readonly memory mapped datastructures. :issue:`5956`","510","  by :user:`Vighnesh Birodkar <vighneshbirodkar>` and","511","  :user:`Olivier Grisel <ogrisel>`.","512",""],"delete":[]}],"sklearn\/decomposition\/tests\/test_dict_learning.py":[{"add":["0","from __future__ import division","369","","370","","371","def test_sparse_coder_parallel_mmap():","372","    # Non-regression test for:","373","    # https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/5956","374","    # Test that SparseCoder does not error by passing reading only","375","    # arrays to child processes","376","","377","    rng = np.random.RandomState(777)","378","    n_components, n_features = 40, 64","379","    init_dict = rng.rand(n_components, n_features)","380","    # Ensure that `data` is >2M. Joblib memory maps arrays","381","    # if they are larger than 1MB. The 4 accounts for float32","382","    # data type","383","    n_samples = int(2e6) \/\/ (4 * n_features)","384","    data = np.random.rand(n_samples, n_features).astype(np.float32)","385","","386","    sc = SparseCoder(init_dict, transform_algorithm='omp', n_jobs=2)","387","    sc.fit_transform(data)"],"delete":[]}],"sklearn\/linear_model\/omp.py":[{"add":["193","    if copy_Xy or not Xy.flags.writeable:","493","    if copy_Xy or not Xy.flags.writeable:","494","        # Make the copy once instead of many times in _gram_omp itself.","495","        Xy = Xy.copy()","520","            copy_Gram=copy_Gram, copy_Xy=False,"],"delete":["193","    if copy_Xy:","517","            copy_Gram=copy_Gram, copy_Xy=copy_Xy,"]}]}},"eaf0a044fdc084ebeeb9bbfbcf42e6df2b1491bb":{"changes":{"sklearn\/svm\/src\/liblinear\/liblinear_helper.c":"MODIFY","sklearn\/svm\/src\/liblinear\/linear.h":"MODIFY","sklearn\/svm\/src\/newrand\/newrand.h":"ADD","doc\/whats_new\/v0.23.rst":"MODIFY","sklearn\/svm\/src\/libsvm\/svm.cpp":"MODIFY","sklearn\/svm\/setup.py":"MODIFY","sklearn\/svm\/src\/liblinear\/linear.cpp":"MODIFY","sklearn\/metrics\/tests\/test_ranking.py":"MODIFY","sklearn\/svm\/src\/libsvm\/LIBSVM_CHANGES":"MODIFY"},"diff":{"sklearn\/svm\/src\/liblinear\/liblinear_helper.c":[{"add":["184","    set_seed(seed);"],"delete":["184","    srand(seed);"]}],"sklearn\/svm\/src\/liblinear\/linear.h":[{"add":["51","void set_seed(unsigned seed);","52",""],"delete":[]}],"sklearn\/svm\/src\/newrand\/newrand.h":[{"add":[],"delete":[]}],"doc\/whats_new\/v0.23.rst":[{"add":["27","- Any model using the :func:`svm.libsvm` or the :func:`svm.liblinear` solver,","28","  including :class:`svm.LinearSVC`, :class:`svm.LinearSVR`,","29","  :class:`svm.NuSVC`, :class:`svm.NuSVR`, :class:`svm.OneClassSVM`,","30","  :class:`svm.SVC`, :class:`svm.SVR`, :class:`linear_model.LogisticRegression`.","31","  |Efficiency| |Fix|","32","","304","  or 'd'). :pr:`16159` by :user:`Rick Mackenbach <Rick-Mackenbach>` and","386","- |Fix| |Efficiency| Improved ``libsvm`` and ``liblinear`` random number","387","  generators used to randomly select coordinates in the coordinate descent","388","  algorithms. Platform-dependent C ``rand()`` was used, which is only able to","389","  generate numbers up to ``32767`` on windows platform (see this `blog","390","  post <https:\/\/codeforces.com\/blog\/entry\/61587>`) and also has poor","391","  randomization power as suggested by `this presentation","392","  <https:\/\/channel9.msdn.com\/Events\/GoingNative\/2013\/rand-Considered-Harmful>`.","393","  It was replaced with C++11 ``mt19937``, a Mersenne Twister that correctly","394","  generates 31bits\/63bits random numbers on all platforms. In addition, the","395","  crude \"modulo\" postprocessor used to get a random number in a bounded","396","  interval was replaced by the tweaked Lemire method as suggested by `this blog","397","  post <http:\/\/www.pcg-random.org\/posts\/bounded-rands.html>`.","398","  Any model using the :func:`svm.libsvm` or the :func:`svm.liblinear` solver,","399","  including :class:`svm.LinearSVC`, :class:`svm.LinearSVR`,","400","  :class:`svm.NuSVC`, :class:`svm.NuSVR`, :class:`svm.OneClassSVM`,","401","  :class:`svm.SVC`, :class:`svm.SVR`, :class:`linear_model.LogisticRegression`,","402","  is affected. In particular users can expect a better convergence when the","403","  number of samples (LibSVM) or the number of features (LibLinear) is large.","404","  :pr:`13511` by :user:`Sylvain Mari¨¦ <smarie>`.","405",""],"delete":["298","  or 'd'). :pr:`16159` by :user:`Rick Mackenbach <Rick-Mackenbach>` and "]}],"sklearn\/svm\/src\/libsvm\/svm.cpp":[{"add":["50","   Modified 2020:","51","","52","   - Improved random number generator by using a mersenne twister + tweaked","53","     lemire postprocessor. This fixed a convergence issue on windows targets.","54","     Sylvain Marie,","55","     see <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/13511#issuecomment-481729756>","56","","66","#include <climits>","67","#include <random>","69","#include \"..\/newrand\/newrand.h\"","2105","\t\tint j = i+bounded_rand_int(prob->l-i);","2360","        set_seed(param->random_seed);","2640","        set_seed(param->random_seed);","2662","\t\t\t\tint j = i+bounded_rand_int(count[c]-i);","2699","\t\t\tint j = i+bounded_rand_int(l-i);"],"delete":["2095","\t\tint j = i+rand()%(prob->l-i);","2350","        srand(param->random_seed);","2630","        srand(param->random_seed);","2652","\t\t\t\tint j = i+rand()%(count[c]-i);","2689","\t\t\tint j = i+rand()%(l-i);"]}],"sklearn\/svm\/setup.py":[{"add":["18","                                join('src', 'libsvm', 'svm.h'),","19","                                join('src', 'newrand', 'newrand.h')],","23","                       # Use C++11 to use the random number generator fix","24","                       extra_compiler_args=['-std=c++11'],","31","                      join('src', 'libsvm', 'svm.h'),","32","                      join('src', 'newrand', 'newrand.h')]","37","                                       join('src', 'libsvm'),","38","                                       join('src', 'newrand')],","48","    # precompile liblinear to use C++11 flag","49","    config.add_library('liblinear-skl',","50","                       sources=[join('src', 'liblinear', 'linear.cpp'),","51","                                join('src', 'liblinear', 'tron.cpp')],","52","                       depends=[join('src', 'liblinear', 'linear.h'),","53","                                join('src', 'liblinear', 'tron.h'),","54","                                join('src', 'newrand', 'newrand.h')],","55","                       # Force C++ linking in case gcc is picked up instead","56","                       # of g++ under windows with some versions of MinGW","57","                       extra_link_args=['-lstdc++'],","58","                       # Use C++11 to use the random number generator fix","59","                       extra_compiler_args=['-std=c++11'],","60","                       )","62","    liblinear_sources = ['_liblinear.pyx']","64","                         join('src', 'newrand', 'newrand.h'),","69","                         libraries=['liblinear-skl'] + libraries,","71","                                       join('.', 'src', 'newrand'),","85","                                       join(\"src\", \"libsvm\"),","86","                                       join(\"src\", \"newrand\")],","88","                                  join('src', 'newrand', 'newrand.h'),"],"delete":["18","                                join('src', 'libsvm', 'svm.h')],","28","                      join('src', 'libsvm', 'svm.h')]","33","                                       join('src', 'libsvm')],","43","    liblinear_sources = ['_liblinear.pyx',","44","                         join('src', 'liblinear', '*.cpp')]","51","                         libraries=libraries,","66","                                       join(\"src\", \"libsvm\")],"]}],"sklearn\/svm\/src\/liblinear\/linear.cpp":[{"add":["0","\/*","4","","24","","25","   Modified 2020:","26","   - Improved random number generator by using a mersenne twister + tweaked","27","     lemire postprocessor. This fixed a convergence issue on windows targets.","28","     Sylvain Marie","29","     See <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/13511#issuecomment-481729756>","30","","41","#include <climits>","42","#include <random>","43","#include \"..\/newrand\/newrand.h\"","44","","469","\/\/ A coordinate descent algorithm for","474","\/\/","477","\/\/  C^m_i = C if m  = y_i,","478","\/\/  C^m_i = 0 if m != y_i,","479","\/\/  and w_m(\\alpha) = \\sum_i \\alpha^m_i x_i","481","\/\/ Given:","592","\t\/\/ Initial alpha can be set here. Note that","628","\t\t\tint j = i+bounded_rand_int(active_size-i);","788","\/\/ A coordinate descent algorithm for","793","\/\/","795","\/\/  D is a diagonal matrix","806","\/\/ Given:","811","\/\/","901","\t\t\tint j = i+bounded_rand_int(active_size-i);","1022","\/\/ A coordinate descent algorithm for","1027","\/\/","1029","\/\/  D is a diagonal matrix","1038","\/\/ Given:","1044","\/\/ See Algorithm 4 of Ho and Lin, 2012","1120","\t\t\tint j = i+bounded_rand_int(active_size-i);","1266","\/\/ A coordinate descent algorithm for","1271","\/\/","1272","\/\/  where Qij = yi yj xi^T xj and","1276","\/\/ Given:","1346","\t\t\tint j = i+bounded_rand_int(l-i);","1534","\t\t\tint i = j+bounded_rand_int(active_size-j);","1916","\t\t\t\tint i = j+bounded_rand_int(QP_active_size-j);","2247","","2254","                        j++;","2607","\t\tint j = i+bounded_rand_int(l-i);"],"delete":["0","\/* ","4","   ","458","\/\/ A coordinate descent algorithm for ","463","\/\/ ","466","\/\/  C^m_i = C if m  = y_i, ","467","\/\/  C^m_i = 0 if m != y_i, ","468","\/\/  and w_m(\\alpha) = \\sum_i \\alpha^m_i x_i ","470","\/\/ Given: ","581","\t\/\/ Initial alpha can be set here. Note that ","617","\t\t\tint j = i+rand()%(active_size-i);","777","\/\/ A coordinate descent algorithm for ","782","\/\/ ","784","\/\/  D is a diagonal matrix ","795","\/\/ Given: ","800","\/\/ ","890","\t\t\tint j = i+rand()%(active_size-i);","1011","\/\/ A coordinate descent algorithm for ","1016","\/\/ ","1018","\/\/  D is a diagonal matrix ","1027","\/\/ Given: ","1033","\/\/ See Algorithm 4 of Ho and Lin, 2012   ","1109","\t\t\tint j = i+rand()%(active_size-i);","1255","\/\/ A coordinate descent algorithm for ","1260","\/\/ ","1261","\/\/  where Qij = yi yj xi^T xj and ","1265","\/\/ Given: ","1335","\t\t\tint j = i+rand()%(l-i);","1523","\t\t\tint i = j+rand()%(active_size-j);","1905","\t\t\t\tint i = j+rand()%(QP_active_size-j);","2236","        ","2243","                        j++;      ","2596","\t\tint j = i+rand()%(l-i);","3059",""]}],"sklearn\/metrics\/tests\/test_ranking.py":[{"add":["739","    # `_average_precision` is not very precise in case of 0.5 ties: be tolerant","741","                        precision_recall_auc, decimal=2)"],"delete":["740","                        precision_recall_auc, decimal=3)"]}],"sklearn\/svm\/src\/libsvm\/LIBSVM_CHANGES":[{"add":["6","  * Improved random number generator (fix on windows, enhancement on other","7","    platforms). See <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/13511#issuecomment-481729756>"],"delete":[]}]}},"5fc5c6e62e163a20b890b64cb1efa8ed151bbc18":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/feature_extraction\/text.py":"MODIFY","sklearn\/feature_extraction\/tests\/test_text.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["53",":mod:`sklearn.feature_extraction.text`","54","......................................","55","","56","- |Fix| Fixed a bug in :class:`feature_extraction.text.CountVectorizer` which ","57","  would result in the sparse feature matrix having conflicting `indptr` and","58","  `indices` precisions under very large vocabularies. :issue:`11295` by","59","  :user:`Gabriel Vacaliuc <gvacaliuc>`.","60",""],"delete":[]}],"sklearn\/feature_extraction\/text.py":[{"add":["33","from ..utils import _IS_32BIT","874","        map_index = np.empty(len(sorted_features), dtype=X.indices.dtype)","964","            if _IS_32BIT:","967","                                  'which is unsupported with 32 bit Python.')","968","                                 .format(indptr[-1]))","969","            indices_dtype = np.int64"],"delete":["873","        map_index = np.empty(len(sorted_features), dtype=np.int32)","963","            if sp_version >= (0, 14):","964","                indices_dtype = np.int64","965","            else:","968","                                  ' which is unsupported with scipy {}. '","969","                                  'Please upgrade to scipy >=0.14')","970","                                 .format(indptr[-1], '.'.join(sp_version)))"]}],"sklearn\/feature_extraction\/tests\/test_text.py":[{"add":["38","                                   fails_if_pypy, assert_allclose_dense_sparse,","39","                                   skip_if_32bit)","1147","@skip_if_32bit","1148","def test_countvectorizer_sort_features_64bit_sparse_indices():","1149","    \"\"\"","1150","    Check that CountVectorizer._sort_features preserves the dtype of its sparse","1151","    feature matrix.","1152","","1153","    This test is skipped on 32bit platforms, see:","1154","        https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/11295","1155","    for more details.","1156","    \"\"\"","1157","","1158","    X = sparse.csr_matrix((5, 5), dtype=np.int64)","1159","","1160","    # force indices and indptr to int64.","1161","    INDICES_DTYPE = np.int64","1162","    X.indices = X.indices.astype(INDICES_DTYPE)","1163","    X.indptr = X.indptr.astype(INDICES_DTYPE)","1164","","1165","    vocabulary = {","1166","            \"scikit-learn\": 0,","1167","            \"is\": 1,","1168","            \"great!\": 2","1169","            }","1170","","1171","    Xs = CountVectorizer()._sort_features(X, vocabulary)","1172","","1173","    assert INDICES_DTYPE == Xs.indices.dtype","1174","","1175",""],"delete":["38","                                   fails_if_pypy, assert_allclose_dense_sparse)"]}]}},"2a1e9686eeb203f5fddf44fd06414db8ab6a554a":{"changes":{"setup.py":"MODIFY"},"diff":{"setup.py":[{"add":["16","    # Python 2 compat: just to be able to declare that Python >=3.5 is needed.","17","    import __builtin__ as builtins","19","# This is a bit (!) hackish: we are setting a global variable so that the","20","# main sklearn __init__ can detect if it is being loaded by the setup","21","# routine, to avoid attempting to load components that aren't built yet:","22","# the numpy distutils extensions that are used by scikit-learn to","23","# recursively build the compiled extensions in sub-packages is based on the","24","# Python import machinery.","25","builtins.__SKLEARN_SETUP__ = True","26","","115","cmdclass = {'clean': CleanCommand}","116","","120","try:","121","    from numpy.distutils.command.build_ext import build_ext  # noqa","123","    class build_ext_subclass(build_ext):","124","        def build_extensions(self):","125","            from sklearn._build_utils.openmp_helpers import get_openmp_flag","127","            if not os.getenv('SKLEARN_NO_OPENMP'):","128","                openmp_flag = get_openmp_flag(self.compiler)","130","                for e in self.extensions:","131","                    e.extra_compile_args += openmp_flag","132","                    e.extra_link_args += openmp_flag","134","            build_ext.build_extensions(self)","136","    cmdclass['build_ext'] = build_ext_subclass","138","except ImportError:","139","    # Numpy should not be a dependency just to be able to introspect","140","    # that python 3.5 is required.","141","    pass","230","                    python_requires=\">=3.5\",","256","        if sys.version_info < (3, 5):","257","            raise RuntimeError(","258","                \"Scikit-learn requires Python 3.5 or later. The current\"","259","                \" Python version is %s installed in %s.\"","260","                % (platform.python_version(), sys.executable))","261",""],"delete":["15","    # This is a bit (!) hackish: we are setting a global variable so that the","16","    # main sklearn __init__ can detect if it is being loaded by the setup","17","    # routine, to avoid attempting to load components that aren't built yet:","18","    # the numpy distutils extensions that are used by scikit-learn to","19","    # recursively build the compiled extensions in sub-packages is based on the","20","    # Python import machinery.","21","    builtins.__SKLEARN_SETUP__ = True","23","    # Python 2 is not support but we will raise an explicit error message next.","24","    pass","26","if sys.version_info < (3, 5):","27","    raise RuntimeError(\"Scikit-learn requires Python 3.5 or later. The current\"","28","                       \" Python version is %s installed in %s.\"","29","                       % (platform.python_version(), sys.executable))","121","from numpy.distutils.command.build_ext import build_ext  # noqa","124","class build_ext_subclass(build_ext):","125","    def build_extensions(self):","126","        from sklearn._build_utils.openmp_helpers import get_openmp_flag","128","        if not os.getenv('SKLEARN_NO_OPENMP'):","129","            openmp_flag = get_openmp_flag(self.compiler)","131","            for e in self.extensions:","132","                e.extra_compile_args += openmp_flag","133","                e.extra_link_args += openmp_flag","135","        build_ext.build_extensions(self)","137","","138","cmdclass = {'clean': CleanCommand, 'build_ext': build_ext_subclass}"]}]}},"89da7f71a66585978c51f4d510be9583d8020066":{"changes":{"sklearn\/metrics\/regression.py":"MODIFY","sklearn\/metrics\/scorer.py":"MODIFY","sklearn\/metrics\/tests\/test_score_objects.py":"MODIFY","sklearn\/metrics\/tests\/test_common.py":"MODIFY","doc\/modules\/classes.rst":"MODIFY","doc\/modules\/model_evaluation.rst":"MODIFY","sklearn\/metrics\/__init__.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY","sklearn\/metrics\/tests\/test_regression.py":"MODIFY"},"diff":{"sklearn\/metrics\/regression.py":[{"add":["21","#          Christian Lorentzen <lorentzen.ch@googlemail.com>","26","from scipy.special import xlogy","42","    \"explained_variance_score\",","43","    \"mean_tweedie_deviance\",","44","    \"mean_poisson_deviance\",","45","    \"mean_gamma_deviance\",","49","def _check_reg_targets(y_true, y_pred, multioutput, dtype=\"numeric\"):","79","    dtype: str or list, default=\"numeric\"","80","        the dtype argument passed to check_array","84","    y_true = check_array(y_true, ensure_2d=False, dtype=dtype)","85","    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)","618","","619","","620","def mean_tweedie_deviance(y_true, y_pred, sample_weight=None, p=0):","621","    \"\"\"Mean Tweedie deviance regression loss.","622","","623","    Read more in the :ref:`User Guide <mean_tweedie_deviance>`.","624","","625","    Parameters","626","    ----------","627","    y_true : array-like of shape (n_samples,)","628","        Ground truth (correct) target values.","629","","630","    y_pred : array-like of shape (n_samples,)","631","        Estimated target values.","632","","633","    sample_weight : array-like, shape (n_samples,), optional","634","        Sample weights.","635","","636","    p : float, optional","637","        Tweedie power parameter. Either p ¡Ü 0 or p ¡Ý 1.","638","","639","        The higher `p` the less weight is given to extreme","640","        deviations between true and predicted targets.","641","","642","        - p < 0: Extreme stable distribution. Requires: y_pred > 0.","643","        - p = 0 : Normal distribution, output corresponds to","644","          mean_squared_error. y_true and y_pred can be any real numbers.","645","        - p = 1 : Poisson distribution. Requires: y_true ¡Ý 0 and y_pred > 0.","646","        - 1 < p < 2 : Compound Poisson distribution. Requires: y_true ¡Ý 0","647","          and y_pred > 0.","648","        - p = 2 : Gamma distribution. Requires: y_true > 0 and y_pred > 0.","649","        - p = 3 : Inverse Gaussian distribution. Requires: y_true > 0","650","          and y_pred > 0.","651","        - otherwise : Positive stable distribution. Requires: y_true > 0","652","          and y_pred > 0.","653","","654","    Returns","655","    -------","656","    loss : float","657","        A non-negative floating point value (the best value is 0.0).","658","","659","    Examples","660","    --------","661","    >>> from sklearn.metrics import mean_tweedie_deviance","662","    >>> y_true = [2, 0, 1, 4]","663","    >>> y_pred = [0.5, 0.5, 2., 2.]","664","    >>> mean_tweedie_deviance(y_true, y_pred, p=1)","665","    1.4260...","666","    \"\"\"","667","    y_type, y_true, y_pred, _ = _check_reg_targets(","668","        y_true, y_pred, None, dtype=[np.float64, np.float32])","669","    if y_type == 'continuous-multioutput':","670","        raise ValueError(\"Multioutput not supported in mean_tweedie_deviance\")","671","    check_consistent_length(y_true, y_pred, sample_weight)","672","","673","    if sample_weight is not None:","674","        sample_weight = column_or_1d(sample_weight)","675","        sample_weight = sample_weight[:, np.newaxis]","676","","677","    message = (\"Mean Tweedie deviance error with p={} can only be used on \"","678","               .format(p))","679","    if p < 0:","680","        # 'Extreme stable', y_true any realy number, y_pred > 0","681","        if (y_pred <= 0).any():","682","            raise ValueError(message + \"strictly positive y_pred.\")","683","        dev = 2 * (np.power(np.maximum(y_true, 0), 2-p)\/((1-p) * (2-p)) -","684","                   y_true * np.power(y_pred, 1-p)\/(1-p) +","685","                   np.power(y_pred, 2-p)\/(2-p))","686","    elif p == 0:","687","        # Normal distribution, y_true and y_pred any real number","688","        dev = (y_true - y_pred)**2","689","    elif p < 1:","690","        raise ValueError(\"Tweedie deviance is only defined for p<=0 and \"","691","                         \"p>=1.\")","692","    elif p == 1:","693","        # Poisson distribution, y_true >= 0, y_pred > 0","694","        if (y_true < 0).any() or (y_pred <= 0).any():","695","            raise ValueError(message + \"non-negative y_true and strictly \"","696","                             \"positive y_pred.\")","697","        dev = 2 * (xlogy(y_true, y_true\/y_pred) - y_true + y_pred)","698","    elif p == 2:","699","        # Gamma distribution, y_true and y_pred > 0","700","        if (y_true <= 0).any() or (y_pred <= 0).any():","701","            raise ValueError(message + \"strictly positive y_true and y_pred.\")","702","        dev = 2 * (np.log(y_pred\/y_true) + y_true\/y_pred - 1)","703","    else:","704","        if p < 2:","705","            # 1 < p < 2 is Compound Poisson, y_true >= 0, y_pred > 0","706","            if (y_true < 0).any() or (y_pred <= 0).any():","707","                raise ValueError(message + \"non-negative y_true and strictly \"","708","                                           \"positive y_pred.\")","709","        else:","710","            if (y_true <= 0).any() or (y_pred <= 0).any():","711","                raise ValueError(message + \"strictly positive y_true and \"","712","                                           \"y_pred.\")","713","","714","        dev = 2 * (np.power(y_true, 2-p)\/((1-p) * (2-p)) -","715","                   y_true * np.power(y_pred, 1-p)\/(1-p) +","716","                   np.power(y_pred, 2-p)\/(2-p))","717","","718","    return np.average(dev, weights=sample_weight)","719","","720","","721","def mean_poisson_deviance(y_true, y_pred, sample_weight=None):","722","    \"\"\"Mean Poisson deviance regression loss.","723","","724","    Poisson deviance is equivalent to the Tweedie deviance with","725","    the power parameter `p=1`.","726","","727","    Read more in the :ref:`User Guide <mean_tweedie_deviance>`.","728","","729","    Parameters","730","    ----------","731","    y_true : array-like of shape (n_samples,)","732","        Ground truth (correct) target values. Requires y_true ¡Ý 0.","733","","734","    y_pred : array-like of shape (n_samples,)","735","        Estimated target values. Requires y_pred > 0.","736","","737","    sample_weight : array-like, shape (n_samples,), optional","738","        Sample weights.","739","","740","    Returns","741","    -------","742","    loss : float","743","        A non-negative floating point value (the best value is 0.0).","744","","745","    Examples","746","    --------","747","    >>> from sklearn.metrics import mean_poisson_deviance","748","    >>> y_true = [2, 0, 1, 4]","749","    >>> y_pred = [0.5, 0.5, 2., 2.]","750","    >>> mean_poisson_deviance(y_true, y_pred)","751","    1.4260...","752","    \"\"\"","753","    return mean_tweedie_deviance(","754","        y_true, y_pred, sample_weight=sample_weight, p=1","755","    )","756","","757","","758","def mean_gamma_deviance(y_true, y_pred, sample_weight=None):","759","    \"\"\"Mean Gamma deviance regression loss.","760","","761","    Gamma deviance is equivalent to the Tweedie deviance with","762","    the power parameter `p=2`. It is invariant to scaling of","763","    the target variable, and mesures relative errors.","764","","765","    Read more in the :ref:`User Guide <mean_tweedie_deviance>`.","766","","767","    Parameters","768","    ----------","769","    y_true : array-like of shape (n_samples,)","770","        Ground truth (correct) target values. Requires y_true > 0.","771","","772","    y_pred : array-like of shape (n_samples,)","773","        Estimated target values. Requires y_pred > 0.","774","","775","    sample_weight : array-like, shape (n_samples,), optional","776","        Sample weights.","777","","778","    Returns","779","    -------","780","    loss : float","781","        A non-negative floating point value (the best value is 0.0).","782","","783","    Examples","784","    --------","785","    >>> from sklearn.metrics import mean_gamma_deviance","786","    >>> y_true = [2, 0.5, 1, 4]","787","    >>> y_pred = [0.5, 0.5, 2., 2.]","788","    >>> mean_gamma_deviance(y_true, y_pred)","789","    1.0568...","790","    \"\"\"","791","    return mean_tweedie_deviance(","792","        y_true, y_pred, sample_weight=sample_weight, p=2","793","    )"],"delete":["40","    \"explained_variance_score\"","44","def _check_reg_targets(y_true, y_pred, multioutput):","77","    y_true = check_array(y_true, ensure_2d=False)","78","    y_pred = check_array(y_pred, ensure_2d=False)"]}],"sklearn\/metrics\/scorer.py":[{"add":["26","               mean_squared_error, mean_squared_log_error,","27","               mean_tweedie_deviance, accuracy_score,","497","neg_mean_poisson_deviance_scorer = make_scorer(","498","    mean_tweedie_deviance, p=1., greater_is_better=False","499",")","500","","501","neg_mean_gamma_deviance_scorer = make_scorer(","502","    mean_tweedie_deviance, p=2., greater_is_better=False","503",")","551","               neg_mean_poisson_deviance=neg_mean_poisson_deviance_scorer,","552","               neg_mean_gamma_deviance=neg_mean_gamma_deviance_scorer,"],"delete":["26","               mean_squared_error, mean_squared_log_error, accuracy_score,","494",""]}],"sklearn\/metrics\/tests\/test_score_objects.py":[{"add":["45","                      'max_error', 'neg_mean_poisson_deviance',","46","                      'neg_mean_gamma_deviance']","70","REQUIRE_POSITIVE_Y_SCORERS = ['neg_mean_poisson_deviance',","71","                              'neg_mean_gamma_deviance']","72","","73","","74","def _require_positive_y(y):","75","    \"\"\"Make targets strictly positive\"\"\"","76","    offset = abs(y.min()) + 1","77","    y = y + offset","78","    return y","79","","84","    # some of the regressions scorers require strictly positive input.","85","    sensible_regr.fit(X_train, y_train + 1)","491","        if name in REQUIRE_POSITIVE_Y_SCORERS:","492","            target = _require_positive_y(target)","519","","520","    if name in REQUIRE_POSITIVE_Y_SCORERS:","521","        y_mm_1 = _require_positive_y(y_mm)","522","        y_ml_mm_1 = _require_positive_y(y_ml_mm)","523","    else:","524","        y_mm_1, y_ml_mm_1 = y_mm, y_ml_mm","525","","526","    # UndefinedMetricWarning for P \/ R scores","527","    with ignore_warnings():","528","        scorer, estimator = SCORERS[name], ESTIMATORS[name]","529","        if name in MULTILABEL_ONLY_SCORERS:","530","            score = scorer(estimator, X_mm, y_ml_mm_1)","531","        else:","532","            score = scorer(estimator, X_mm, y_mm_1)","533","        assert isinstance(score, numbers.Number), name"],"delete":["45","                      'max_error']","73","    sensible_regr.fit(X_train, y_train)","500","@ignore_warnings  # UndefinedMetricWarning for P \/ R scores","501","def check_scorer_memmap(scorer_name):","502","    scorer, estimator = SCORERS[scorer_name], ESTIMATORS[scorer_name]","503","    if scorer_name in MULTILABEL_ONLY_SCORERS:","504","        score = scorer(estimator, X_mm, y_ml_mm)","505","    else:","506","        score = scorer(estimator, X_mm, y_mm)","507","    assert isinstance(score, numbers.Number), scorer_name","508","","509","","515","    check_scorer_memmap(name)"]}],"sklearn\/metrics\/tests\/test_common.py":[{"add":["46","from sklearn.metrics import mean_tweedie_deviance","47","from sklearn.metrics import mean_poisson_deviance","48","from sklearn.metrics import mean_gamma_deviance","102","    \"mean_normal_deviance\": partial(mean_tweedie_deviance, p=0),","103","    \"mean_poisson_deviance\": mean_poisson_deviance,","104","    \"mean_gamma_deviance\": mean_gamma_deviance,","105","    \"mean_compound_poisson_deviance\":","106","    partial(mean_tweedie_deviance, p=1.4),","444","    \"cohen_kappa_score\", \"mean_normal_deviance\"","466","    \"macro_recall_score\", \"log_loss\", \"hinge_loss\",","467","    \"mean_gamma_deviance\", \"mean_poisson_deviance\",","468","    \"mean_compound_poisson_deviance\"","480","METRICS_REQUIRE_POSITIVE_Y = {","481","    \"mean_poisson_deviance\",","482","    \"mean_gamma_deviance\",","483","    \"mean_compound_poisson_deviance\",","484","}","487","def _require_positive_targets(y1, y2):","488","    \"\"\"Make targets strictly positive\"\"\"","489","    offset = abs(min(y1.min(), y2.min())) + 1","490","    y1 += offset","491","    y2 += offset","492","    return y1, y2","493","","494","","495","def test_symmetry_consistency():","507","","508","@pytest.mark.parametrize(\"name\", sorted(SYMMETRIC_METRICS))","509","def test_symmetric_metric(name):","510","    # Test the symmetry of score and loss functions","511","    random_state = check_random_state(0)","512","    y_true = random_state.randint(0, 2, size=(20, ))","513","    y_pred = random_state.randint(0, 2, size=(20, ))","514","","515","    if name in METRICS_REQUIRE_POSITIVE_Y:","516","        y_true, y_pred = _require_positive_targets(y_true, y_pred)","517","","518","    y_true_bin = random_state.randint(0, 2, size=(20, 25))","519","    y_pred_bin = random_state.randint(0, 2, size=(20, 25))","520","","521","    metric = ALL_METRICS[name]","522","    if name in METRIC_UNDEFINED_BINARY:","523","        if name in MULTILABELS_METRICS:","524","            assert_allclose(metric(y_true_bin, y_pred_bin),","525","                            metric(y_pred_bin, y_true_bin),","527","        else:","528","            assert False, \"This case is currently unhandled\"","529","    else:","530","        assert_allclose(metric(y_true, y_pred),","531","                        metric(y_pred, y_true),","532","                        err_msg=\"%s is not symmetric\" % name)","535","@pytest.mark.parametrize(\"name\", sorted(NOT_SYMMETRIC_METRICS))","536","def test_not_symmetric_metric(name):","537","    # Test the symmetry of score and loss functions","538","    random_state = check_random_state(0)","539","    y_true = random_state.randint(0, 2, size=(20, ))","540","    y_pred = random_state.randint(0, 2, size=(20, ))","541","","542","    if name in METRICS_REQUIRE_POSITIVE_Y:","543","        y_true, y_pred = _require_positive_targets(y_true, y_pred)","544","","545","    metric = ALL_METRICS[name]","546","","547","    # use context manager to supply custom error message","548","    with assert_raises(AssertionError) as cm:","549","        assert_array_equal(metric(y_true, y_pred), metric(y_pred, y_true))","550","        cm.msg = (\"%s seems to be symmetric\" % name)","560","    if name in METRICS_REQUIRE_POSITIVE_Y:","561","        y_true, y_pred = _require_positive_targets(y_true, y_pred)","562","","616","    if name in METRICS_REQUIRE_POSITIVE_Y:","617","        y1, y2 = _require_positive_targets(y1, y2)","618","","807","    if name in METRICS_REQUIRE_POSITIVE_Y:","808","        values = [1, 2]","809","    else:","810","        values = [0, 1]","811","    for i, j in product(values, repeat=2):"],"delete":["436","    \"cohen_kappa_score\",","458","    \"macro_recall_score\", \"log_loss\", \"hinge_loss\"","471","@ignore_warnings","472","def test_symmetry():","473","    # Test the symmetry of score and loss functions","474","    random_state = check_random_state(0)","475","    y_true = random_state.randint(0, 2, size=(20, ))","476","    y_pred = random_state.randint(0, 2, size=(20, ))","478","    y_true_bin = random_state.randint(0, 2, size=(20, 25))","479","    y_pred_bin = random_state.randint(0, 2, size=(20, 25))","491","    # Symmetric metric","492","    for name in SYMMETRIC_METRICS:","493","        metric = ALL_METRICS[name]","494","        if name in METRIC_UNDEFINED_BINARY:","495","            if name in MULTILABELS_METRICS:","496","                assert_allclose(metric(y_true_bin, y_pred_bin),","497","                                metric(y_pred_bin, y_true_bin),","498","                                err_msg=\"%s is not symmetric\" % name)","499","            else:","500","                assert False, \"This case is currently unhandled\"","501","        else:","502","            assert_allclose(metric(y_true, y_pred),","503","                            metric(y_pred, y_true),","506","    # Not symmetric metrics","507","    for name in NOT_SYMMETRIC_METRICS:","508","        metric = ALL_METRICS[name]","510","        # use context manager to supply custom error message","511","        with assert_raises(AssertionError) as cm:","512","            assert_array_equal(metric(y_true, y_pred), metric(y_pred, y_true))","513","            cm.msg = (\"%s seems to be symmetric\" % name)","764","    for i, j in product([0, 1], repeat=2):"]}],"doc\/modules\/classes.rst":[{"add":["905","   metrics.mean_poisson_deviance","906","   metrics.mean_gamma_deviance","907","   metrics.mean_tweedie_deviance"],"delete":[]}],"doc\/modules\/model_evaluation.rst":[{"add":["93","'neg_mean_poisson_deviance'       :func:`metrics.mean_poisson_deviance`","94","'neg_mean_gamma_deviance'         :func:`metrics.mean_gamma_deviance`","1961","","1962",".. _mean_tweedie_deviance:","1963","","1964","Mean Poisson, Gamma, and Tweedie deviances","1965","------------------------------------------","1966","The :func:`mean_tweedie_deviance` function computes the `mean Tweedie","1967","deviance error","1968","<https:\/\/en.wikipedia.org\/wiki\/Tweedie_distribution#The_Tweedie_deviance>`_","1969","with power parameter `p`. This is a metric that elicits predicted expectation","1970","values of regression targets.","1971","","1972","Following special cases exist,","1973","","1974","- when `p=0` it is equivalent to :func:`mean_squared_error`.","1975","- when `p=1` it is equivalent to :func:`mean_poisson_deviance`.","1976","- when `p=2` it is equivalent to :func:`mean_gamma_deviance`.","1977","","1978","If :math:`\\hat{y}_i` is the predicted value of the :math:`i`-th sample,","1979","and :math:`y_i` is the corresponding true value, then the mean Tweedie","1980","deviance error (D) estimated over :math:`n_{\\text{samples}}` is defined as","1981","","1982",".. math::","1983","","1984","  \\text{D}(y, \\hat{y}) = \\frac{1}{n_\\text{samples}}","1985","  \\sum_{i=0}^{n_\\text{samples} - 1}","1986","  \\begin{cases}","1987","  (y_i-\\hat{y}_i)^2, & \\text{for }p=0\\text{ (Normal)}\\\\","1988","  2(y_i \\log(y\/\\hat{y}_i) + \\hat{y}_i - y_i),  & \\text{for }p=1\\text{ (Poisson)}\\\\","1989","  2(\\log(\\hat{y}_i\/y_i) + y_i\/\\hat{y}_i - 1),  & \\text{for }p=2\\text{ (Gamma)}\\\\","1990","  2\\left(\\frac{\\max(y_i,0)^{2-p}}{(1-p)(2-p)}-","1991","  \\frac{y\\,\\hat{y}^{1-p}_i}{1-p}+\\frac{\\hat{y}^{2-p}_i}{2-p}\\right),","1992","  & \\text{otherwise}","1993","  \\end{cases}","1994","","1995","Tweedie deviance is a homogeneous function of degree ``2-p``.","1996","Thus, Gamma distribution with `p=2` means that simultaneously scaling `y_true`","1997","and `y_pred` has no effect on the deviance. For Poisson distribution `p=1`","1998","the deviance scales linearly, and for Normal distribution (`p=0`),","1999","quadratically.  In general, the higher `p` the less weight is given to extreme","2000","deviations between true and predicted targets.","2001","","2002","For instance, let's compare the two predictions 1.0 and 100 that are both","2003","50% of their corresponding true value.","2004","","2005","The mean squared error (``p=0``) is very sensitive to the","2006","prediction difference of the second point,::","2007","","2008","    >>> from sklearn.metrics import mean_tweedie_deviance","2009","    >>> mean_tweedie_deviance([1.0], [1.5], p=0)","2010","    0.25","2011","    >>> mean_tweedie_deviance([100.], [150.], p=0)","2012","    2500.0","2013","","2014","If we increase ``p`` to 1,::","2015","","2016","    >>> mean_tweedie_deviance([1.0], [1.5], p=1)","2017","    0.18...","2018","    >>> mean_tweedie_deviance([100.], [150.], p=1)","2019","    18.9...","2020","","2021","the difference in errors decreases. Finally, by setting, ``p=2``::","2022","","2023","    >>> mean_tweedie_deviance([1.0], [1.5], p=2)","2024","    0.14...","2025","    >>> mean_tweedie_deviance([100.], [150.], p=2)","2026","    0.14...","2027","","2028","we would get identical errors. The deviance when `p=2` is thus only","2029","sensitive to relative errors.","2030",""],"delete":[]}],"sklearn\/metrics\/__init__.py":[{"add":["66","from .regression import mean_tweedie_deviance","67","from .regression import mean_poisson_deviance","68","from .regression import mean_gamma_deviance","115","    'mean_poisson_deviance',","116","    'mean_gamma_deviance',","117","    'mean_tweedie_deviance',"],"delete":[]}],"doc\/whats_new\/v0.22.rst":[{"add":["133","  ","134","- |Feature| Add :class:`metrics.mean_tweedie_deviance` measuring the","135","  Tweedie deviance for a power parameter ``p``. Also add mean Poisson deviance","136","  :class:`metrics.mean_poisson_deviance` and mean Gamma deviance","137","  :class:`metrics.mean_gamma_deviance` that are special cases of the Tweedie","138","  deviance for `p=1` and `p=2` respectively.","139","  :pr:`13938` by :user:`Christian Lorentzen <lorentzenchr>` and","140","  `Roman Yurchak`_."],"delete":[]}],"sklearn\/metrics\/tests\/test_regression.py":[{"add":["2","from numpy.testing import assert_allclose","18","from sklearn.metrics import mean_tweedie_deviance","38","    assert_almost_equal(mean_tweedie_deviance(y_true, y_pred, p=0),","39","                        mean_squared_error(y_true, y_pred))","40","","41","    # Tweedie deviance needs positive y_pred, except for p=0,","42","    # p>=2 needs positive y_true","43","    # results evaluated by sympy","44","    y_true = np.arange(1, 1 + n_samples)","45","    y_pred = 2 * y_true","46","    n = n_samples","47","    assert_almost_equal(mean_tweedie_deviance(y_true, y_pred, p=-1),","48","                        5\/12 * n * (n**2 + 2 * n + 1))","49","    assert_almost_equal(mean_tweedie_deviance(y_true, y_pred, p=1),","50","                        (n + 1) * (1 - np.log(2)))","51","    assert_almost_equal(mean_tweedie_deviance(y_true, y_pred, p=2),","52","                        2 * np.log(2) - 1)","53","    assert_almost_equal(mean_tweedie_deviance(y_true, y_pred, p=3\/2),","54","                        ((6 * np.sqrt(2) - 8) \/ n) * np.sqrt(y_true).sum())","55","    assert_almost_equal(mean_tweedie_deviance(y_true, y_pred, p=3),","56","                        np.sum(1 \/ y_true) \/ (4 * n))","98","    # Tweedie deviance error","99","    p = -1.2","100","    assert_allclose(mean_tweedie_deviance([0], [1.], p=p),","101","                    2.\/(2.-p), rtol=1e-3)","102","    with pytest.raises(ValueError,","103","                       match=\"can only be used on strictly positive y_pred.\"):","104","        mean_tweedie_deviance([0.], [0.], p=p)","105","    assert_almost_equal(mean_tweedie_deviance([0.], [0.], p=0), 0.00, 2)","106","","107","    msg = \"only be used on non-negative y_true and strictly positive y_pred.\"","108","    with pytest.raises(ValueError, match=msg):","109","        mean_tweedie_deviance([0.], [0.], p=1.0)","110","","111","    p = 1.5","112","    assert_allclose(mean_tweedie_deviance([0.], [1.], p=p), 2.\/(2.-p))","113","    msg = \"only be used on non-negative y_true and strictly positive y_pred.\"","114","    with pytest.raises(ValueError, match=msg):","115","        mean_tweedie_deviance([0.], [0.], p=p)","116","    p = 2.","117","    assert_allclose(mean_tweedie_deviance([1.], [1.], p=p), 0.00,","118","                    atol=1e-8)","119","    msg = \"can only be used on strictly positive y_true and y_pred.\"","120","    with pytest.raises(ValueError, match=msg):","121","        mean_tweedie_deviance([0.], [0.], p=p)","122","    p = 3.","123","    assert_allclose(mean_tweedie_deviance([1.], [1.], p=p),","124","                    0.00, atol=1e-8)","125","","126","    msg = \"can only be used on strictly positive y_true and y_pred.\"","127","    with pytest.raises(ValueError, match=msg):","128","        mean_tweedie_deviance([0.], [0.], p=p)","129","","130","    with pytest.raises(ValueError,","131","                       match=\"deviance is only defined for p<=0 and p>=1.\"):","132","        mean_tweedie_deviance([0.], [0.], p=0.5)","133","","261","","262","","263","def test_tweedie_deviance_continuity():","264","    n_samples = 100","265","","266","    y_true = np.random.RandomState(0).rand(n_samples) + 0.1","267","    y_pred = np.random.RandomState(1).rand(n_samples) + 0.1","268","","269","    assert_allclose(mean_tweedie_deviance(y_true, y_pred, p=0 - 1e-10),","270","                    mean_tweedie_deviance(y_true, y_pred, p=0))","271","","272","    # Ws we get closer to the limit, with 1e-12 difference the absolute","273","    # tolerance to pass the below check increases. There are likely","274","    # numerical precision issues on the edges of different definition","275","    # regions.","276","    assert_allclose(mean_tweedie_deviance(y_true, y_pred, p=1 + 1e-10),","277","                    mean_tweedie_deviance(y_true, y_pred, p=1),","278","                    atol=1e-6)","279","","280","    assert_allclose(mean_tweedie_deviance(y_true, y_pred, p=2 - 1e-10),","281","                    mean_tweedie_deviance(y_true, y_pred, p=2),","282","                    atol=1e-6)","283","","284","    assert_allclose(mean_tweedie_deviance(y_true, y_pred, p=2 + 1e-10),","285","                    mean_tweedie_deviance(y_true, y_pred, p=2),","286","                    atol=1e-6)"],"delete":[]}]}},"0eda10a598b3ae988f7d8c116089045474248d62":{"changes":{"doc\/modules\/linear_model.rst":"MODIFY","doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/linear_model\/tests\/test_bayes.py":"MODIFY","sklearn\/linear_model\/bayes.py":"MODIFY"},"diff":{"doc\/modules\/linear_model.rst":[{"add":["579","where :math:`\\alpha` is again treated as a random variable that is to be","580","estimated from the data.","616","conjugate prior for the precision of the Gaussian. The resulting model is","617","called *Bayesian Ridge Regression*, and is similar to the classical","618",":class:`Ridge`.","620","The parameters :math:`w`, :math:`\\alpha` and :math:`\\lambda` are estimated","621","jointly during the fit of the model, the regularization parameters","622",":math:`\\alpha` and :math:`\\lambda` being estimated by maximizing the","623","*log marginal likelihood*. The scikit-learn implementation","624","is based on the algorithm described in Appendix A of (Tipping, 2001)","625","where the update of the parameters :math:`\\alpha` and :math:`\\lambda` is done","626","as suggested in (MacKay, 1992).","628","The remaining hyperparameters are the parameters :math:`\\alpha_1`,","629",":math:`\\alpha_2`, :math:`\\lambda_1` and :math:`\\lambda_2` of the gamma priors","630","over :math:`\\alpha` and :math:`\\lambda`. These are usually chosen to be","631","*non-informative*. By default :math:`\\alpha_1 = \\alpha_2 =  \\lambda_1 = \\lambda_2 = 10^{-6}`.","670",".. topic:: References:","672","    * Section 3.3 in Christopher M. Bishop: Pattern Recognition and Machine Learning, 2006","674","    * David J. C. MacKay, `Bayesian Interpolation <http:\/\/citeseerx.ist.psu.edu\/viewdoc\/download?doi=10.1.1.27.9072&rep=rep1&type=pdf>`_, 1992.","675","","676","    * Michael E. Tipping, `Sparse Bayesian Learning and the Relevance Vector Machine <http:\/\/www.jmlr.org\/papers\/volume1\/tipping01a\/tipping01a.pdf>`_, 2001."],"delete":["579","Alpha is again treated as a random variable that is to be estimated from the","580","data.","616","conjugate prior for the precision of the Gaussian.","618","The resulting model is called *Bayesian Ridge Regression*, and is similar to the","619","classical :class:`Ridge`.  The parameters :math:`w`, :math:`\\alpha` and","620",":math:`\\lambda` are estimated jointly during the fit of the model.  The","621","remaining hyperparameters are the parameters of the gamma priors over","622",":math:`\\alpha` and :math:`\\lambda`.  These are usually chosen to be","623","*non-informative*.  The parameters are estimated by maximizing the *marginal","624","log likelihood*.","626","By default :math:`\\alpha_1 = \\alpha_2 =  \\lambda_1 = \\lambda_2 = 10^{-6}`.","665",".. topic:: References","667","  * More details can be found in the article `Bayesian Interpolation","668","    <http:\/\/citeseerx.ist.psu.edu\/viewdoc\/download?doi=10.1.1.27.9072&rep=rep1&type=pdf>`_","669","    by MacKay, David J. C."]}],"doc\/whats_new\/v0.21.rst":[{"add":["19","- :class:`linear_model.BayesianRidge` |Fix|","236",":mod:`sklearn.linear_model`","237","...........................","238","","239","- |Fix| Fixed the posterior mean, posterior covariance and returned","240","  regularization parameters in :class:`linear_model.BayesianRidge`. The","241","  posterior mean and the posterior covariance were not the ones computed","242","  with the last update of the regularization parameters and the returned","243","  regularization parameters were not the final ones. Also fixed the formula of","244","  the log marginal likelihood used to compute the score when","245","  `compute_score=True`. :issue:`12174` by","246","  :user:`Albert Thomas <albertcthomas>`.","247",""],"delete":["19","..","20","    please add class and reason here (see version 0.20 what's new)","21","","29",""]}],"sklearn\/linear_model\/tests\/test_bayes.py":[{"add":["5","from math import log","7","import numpy as np","8","from scipy.linalg import pinvh","9","","14","from sklearn.utils.testing import assert_raise_message","19","from sklearn.utils.extmath import fast_logdet","20","","21","diabetes = datasets.load_diabetes()","24","def test_n_iter():","25","    \"\"\"Check value of n_iter.\"\"\"","26","    X = np.array([[1], [2], [6], [8], [10]])","27","    y = np.array([1, 2, 6, 8, 10])","28","    clf = BayesianRidge(n_iter=0)","29","    msg = \"n_iter should be greater than or equal to 1.\"","30","    assert_raise_message(ValueError, msg, clf.fit, X, y)","31","","32","","33","def test_bayesian_ridge_scores():","34","    \"\"\"Check scores attribute shape\"\"\"","40","    assert clf.scores_.shape == (clf.n_iter_ + 1,)","41","","42","","43","def test_bayesian_ridge_score_values():","44","    \"\"\"Check value of score on toy example.","45","","46","    Compute log marginal likelihood with equation (36) in Sparse Bayesian","47","    Learning and the Relevance Vector Machine (Tipping, 2001):","48","","49","    - 0.5 * (log |Id\/alpha + X.X^T\/lambda| +","50","             y^T.(Id\/alpha + X.X^T\/lambda).y + n * log(2 * pi))","51","    + lambda_1 * log(lambda) - lambda_2 * lambda","52","    + alpha_1 * log(alpha) - alpha_2 * alpha","53","","54","    and check equality with the score computed during training.","55","    \"\"\"","56","","57","    X, y = diabetes.data, diabetes.target","58","    n_samples = X.shape[0]","59","    # check with initial values of alpha and lambda (see code for the values)","60","    eps = np.finfo(np.float64).eps","61","    alpha_ = 1. \/ (np.var(y) + eps)","62","    lambda_ = 1.","63","","64","    # value of the parameters of the Gamma hyperpriors","65","    alpha_1 = 0.1","66","    alpha_2 = 0.1","67","    lambda_1 = 0.1","68","    lambda_2 = 0.1","69","","70","    # compute score using formula of docstring","71","    score = lambda_1 * log(lambda_) - lambda_2 * lambda_","72","    score += alpha_1 * log(alpha_) - alpha_2 * alpha_","73","    M = 1. \/ alpha_ * np.eye(n_samples) + 1. \/ lambda_ * np.dot(X, X.T)","74","    M_inv = pinvh(M)","75","    score += - 0.5 * (fast_logdet(M) + np.dot(y.T, np.dot(M_inv, y)) +","76","                      n_samples * log(2 * np.pi))","77","","78","    # compute score with BayesianRidge","79","    clf = BayesianRidge(alpha_1=alpha_1, alpha_2=alpha_2,","80","                        lambda_1=lambda_1, lambda_2=lambda_2,","81","                        n_iter=1, fit_intercept=False, compute_score=True)","83","","84","    assert_almost_equal(clf.scores_[0], score, decimal=9)"],"delete":["5","import numpy as np","7","from sklearn.utils.testing import assert_array_equal","12","from sklearn.utils.testing import SkipTest","19","def test_bayesian_on_diabetes():","20","    # Test BayesianRidge on diabetes","21","    raise SkipTest(\"test_bayesian_on_diabetes is broken\")","22","    diabetes = datasets.load_diabetes()","26","","27","    # Test with more samples than features","29","    # Test that scores are increasing at each iteration","30","    assert_array_equal(np.diff(clf.scores_) > 0, True)","32","    # Test with more features than samples","33","    X = X[:5, :]","34","    y = y[:5]","36","    # Test that scores are increasing at each iteration","37","    assert_array_equal(np.diff(clf.scores_) > 0, True)"]}],"sklearn\/linear_model\/bayes.py":[{"add":["23","    \"\"\"Bayesian ridge regression.","25","    Fit a Bayesian ridge model. See the Notes section for details on this","26","    implementation and the optimization of the regularization parameters","34","        Maximum number of iterations.  Default is 300. Should be greater than","35","        or equal to 1.","59","        If True, compute the log marginal likelihood at each iteration of the","60","        optimization. Default is False.","63","        Whether to calculate the intercept for this model. If set","85","    coef_ : array, shape = (n_features,)","86","        Coefficients of the regression model (mean of distribution).","89","       Estimated precision of the noise.","92","       Estimated precision of the weights.","95","        Estimated variance-covariance matrix of the weights.","97","    scores_ : array, shape = (n_iter_ + 1,)","98","        If computed_score is True, value of the log marginal likelihood (to be","99","        maximized) at each iteration of the optimization. The array starts","100","        with the value of the log marginal likelihood obtained for the initial","101","        values of alpha and lambda and ends with the value obtained for the","102","        estimated alpha and lambda.","103","","104","    n_iter_ : int","105","        The actual number of iterations to reach the stopping criterion.","121","    There exist several strategies to perform Bayesian ridge regression. This","122","    implementation is based on the algorithm described in Appendix A of","123","    (Tipping, 2001) where updates of the regularization parameters are done as","124","    suggested in (MacKay, 1992). Note that according to A New","125","    View of Automatic Relevance Determination (Wipf and Nagarajan, 2008) these","126","    update rules do not guarantee that the marginal likelihood is increasing","127","    between two consecutive iterations of the optimization.","134","    M. E. Tipping, Sparse Bayesian Learning and the Relevance Vector Machine,","135","    Journal of Machine Learning Research, Vol. 1, 2001.","174","","175","        if self.n_iter < 1:","176","            raise ValueError('n_iter should be greater than or equal to 1.'","177","                             ' Got {!r}.'.format(self.n_iter))","178","","215","            # update posterior mean coef_ based on alpha_ and lambda_ and","216","            # compute corresponding rmse","217","            coef_, rmse_ = self._update_coef_(X, y, n_samples, n_features,","218","                                              XT_y, U, Vh, eigen_vals_,","219","                                              alpha_, lambda_)","220","            if self.compute_score:","221","                # compute the log marginal likelihood","222","                s = self._log_marginal_likelihood(n_samples, n_features,","223","                                                  eigen_vals_,","224","                                                  alpha_, lambda_,","225","                                                  coef_, rmse_)","226","                self.scores_.append(s)","228","            # Update alpha and lambda according to (MacKay, 1992)","229","            gamma_ = np.sum((alpha_ * eigen_vals_) \/","230","                            (lambda_ + alpha_ * eigen_vals_))","243","        self.n_iter_ = iter_ + 1","244","","245","        # return regularization parameters and corresponding posterior mean,","246","        # log marginal likelihood and posterior covariance","247","        self.alpha_ = alpha_","248","        self.lambda_ = lambda_","249","        self.coef_, rmse_ = self._update_coef_(X, y, n_samples, n_features,","250","                                               XT_y, U, Vh, eigen_vals_,","251","                                               alpha_, lambda_)","252","        if self.compute_score:","253","            # compute the log marginal likelihood","254","            s = self._log_marginal_likelihood(n_samples, n_features,","255","                                              eigen_vals_,","256","                                              alpha_, lambda_,","257","                                              coef_, rmse_)","258","            self.scores_.append(s)","259","            self.scores_ = np.array(self.scores_)","260","","261","        # posterior covariance is given by 1\/alpha_ * scaled_sigma_","262","        scaled_sigma_ = np.dot(Vh.T,","263","                               Vh \/ (eigen_vals_ +","264","                                     lambda_ \/ alpha_)[:, np.newaxis])","265","        self.sigma_ = (1. \/ alpha_) * scaled_sigma_","268","","303","    def _update_coef_(self, X, y, n_samples, n_features, XT_y, U, Vh,","304","                      eigen_vals_, alpha_, lambda_):","305","        \"\"\"Update posterior mean and compute corresponding rmse.","306","","307","        Posterior mean is given by coef_ = scaled_sigma_ * X.T * y where","308","        scaled_sigma_ = (lambda_\/alpha_ * np.eye(n_features)","309","                         + np.dot(X.T, X))^-1","310","        \"\"\"","311","","312","        if n_samples > n_features:","313","            coef_ = np.dot(Vh.T,","314","                           Vh \/ (eigen_vals_ +","315","                                 lambda_ \/ alpha_)[:, np.newaxis])","316","            coef_ = np.dot(coef_, XT_y)","317","        else:","318","            coef_ = np.dot(X.T, np.dot(","319","                U \/ (eigen_vals_ + lambda_ \/ alpha_)[None, :], U.T))","320","            coef_ = np.dot(coef_, y)","321","","322","        rmse_ = np.sum((y - np.dot(X, coef_)) ** 2)","323","","324","        return coef_, rmse_","325","","326","    def _log_marginal_likelihood(self, n_samples, n_features, eigen_vals,","327","                                 alpha_, lambda_, coef, rmse):","328","        \"\"\"Log marginal likelihood.\"\"\"","329","        alpha_1 = self.alpha_1","330","        alpha_2 = self.alpha_2","331","        lambda_1 = self.lambda_1","332","        lambda_2 = self.lambda_2","333","","334","        # compute the log of the determinant of the posterior covariance.","335","        # posterior covariance is given by","336","        # sigma = (lambda_ * np.eye(n_features) + alpha_ * np.dot(X.T, X))^-1","337","        if n_samples > n_features:","338","            logdet_sigma = - np.sum(np.log(lambda_ + alpha_ * eigen_vals))","339","        else:","340","            logdet_sigma = np.full(n_features, lambda_,","341","                                   dtype=np.array(lambda_).dtype)","342","            logdet_sigma[:n_samples] += alpha_ * eigen_vals","343","            logdet_sigma = - np.sum(np.log(logdet_sigma))","344","","345","        score = lambda_1 * log(lambda_) - lambda_2 * lambda_","346","        score += alpha_1 * log(alpha_) - alpha_2 * alpha_","347","        score += 0.5 * (n_features * log(lambda_) +","348","                        n_samples * log(alpha_) -","349","                        alpha_ * rmse -","350","                        lambda_ * np.sum(coef ** 2) +","351","                        logdet_sigma -","352","                        n_samples * log(2 * np.pi))","353","","354","        return score","355",""],"delete":["23","    \"\"\"Bayesian ridge regression","25","    Fit a Bayesian ridge model and optimize the regularization parameters","33","        Maximum number of iterations.  Default is 300.","57","        If True, compute the objective function at each step of the model.","58","        Default is False","61","        whether to calculate the intercept for this model. If set","83","    coef_ : array, shape = (n_features)","84","        Coefficients of the regression model (mean of distribution)","87","       estimated precision of the noise.","90","       estimated precision of the weights.","93","        estimated variance-covariance matrix of the weights","95","    scores_ : float","96","        if computed, value of the objective function (to be maximized)","112","    For an example, see :ref:`examples\/linear_model\/plot_bayesian_ridge.py","113","    <sphx_glr_auto_examples_linear_model_plot_bayesian_ridge.py>`.","120","    R. Salakhutdinov, Lecture notes on Statistical Machine Learning,","121","    http:\/\/www.utstat.toronto.edu\/~rsalakhu\/sta4273\/notes\/Lecture2.pdf#page=15","122","    Their beta is our ``self.alpha_``","123","    Their alpha is our ``self.lambda_``","198","            # Compute mu and sigma","199","            # sigma_ = lambda_ \/ alpha_ * np.eye(n_features) + np.dot(X.T, X)","200","            # coef_ = sigma_^-1 * XT * y","201","            if n_samples > n_features:","202","                coef_ = np.dot(Vh.T,","203","                               Vh \/ (eigen_vals_ +","204","                                     lambda_ \/ alpha_)[:, np.newaxis])","205","                coef_ = np.dot(coef_, XT_y)","206","                if self.compute_score:","207","                    logdet_sigma_ = - np.sum(","208","                        np.log(lambda_ + alpha_ * eigen_vals_))","209","            else:","210","                coef_ = np.dot(X.T, np.dot(","211","                    U \/ (eigen_vals_ + lambda_ \/ alpha_)[None, :], U.T))","212","                coef_ = np.dot(coef_, y)","213","                if self.compute_score:","214","                    logdet_sigma_ = np.full(n_features, lambda_,","215","                                            dtype=np.array(lambda_).dtype)","216","                    logdet_sigma_[:n_samples] += alpha_ * eigen_vals_","217","                    logdet_sigma_ = - np.sum(np.log(logdet_sigma_))","219","            # Preserve the alpha and lambda values that were used to","220","            # calculate the final coefficients","221","            self.alpha_ = alpha_","222","            self.lambda_ = lambda_","223","","224","            # Update alpha and lambda","225","            rmse_ = np.sum((y - np.dot(X, coef_)) ** 2)","226","            gamma_ = (np.sum((alpha_ * eigen_vals_) \/","227","                      (lambda_ + alpha_ * eigen_vals_)))","233","            # Compute the objective function","234","            if self.compute_score:","235","                s = lambda_1 * log(lambda_) - lambda_2 * lambda_","236","                s += alpha_1 * log(alpha_) - alpha_2 * alpha_","237","                s += 0.5 * (n_features * log(lambda_) +","238","                            n_samples * log(alpha_) -","239","                            alpha_ * rmse_ -","240","                            (lambda_ * np.sum(coef_ ** 2)) -","241","                            logdet_sigma_ -","242","                            n_samples * log(2 * np.pi))","243","                self.scores_.append(s)","244","","252","        self.coef_ = coef_","253","        sigma_ = np.dot(Vh.T,","254","                        Vh \/ (eigen_vals_ + lambda_ \/ alpha_)[:, np.newaxis])","255","        self.sigma_ = (1. \/ alpha_) * sigma_"]}]}},"62301aa81b72fd93a5fccc5db0b2a742b95aea7d":{"changes":{"sklearn\/decomposition\/dict_learning.py":"MODIFY","sklearn\/decomposition\/tests\/test_dict_learning.py":"MODIFY"},"diff":{"sklearn\/decomposition\/dict_learning.py":[{"add":["765","    dictionary = np.require(dictionary, requirements='W')","766",""],"delete":[]}],"sklearn\/decomposition\/tests\/test_dict_learning.py":[{"add":["267","def test_dict_learning_online_readonly_initialization():","268","    n_components = 12","269","    rng = np.random.RandomState(0)","270","    V = rng.randn(n_components, n_features)","271","    V.setflags(write=False)","272","    MiniBatchDictionaryLearning(n_components, n_iter=1, dict_init=V,","273","                                random_state=0, shuffle=False).fit(X)","274","","275",""],"delete":[]}]}},"1d3a553b2dfbe5cc8d32b306fe62855671fe9ae4":{"changes":{"sklearn\/linear_model\/tests\/test_coordinate_descent.py":"MODIFY","doc\/whats_new\/v0.23.rst":"MODIFY","sklearn\/linear_model\/_coordinate_descent.py":"MODIFY"},"diff":{"sklearn\/linear_model\/tests\/test_coordinate_descent.py":[{"add":["9","import joblib","10","from distutils.version import LooseVersion","13","from sklearn.datasets import make_regression","1025","","1026","","1027","@pytest.mark.parametrize(\"backend\", [\"loky\", \"threading\"])","1028","@pytest.mark.parametrize(\"estimator\",","1029","                         [ElasticNetCV, MultiTaskElasticNetCV,","1030","                          LassoCV, MultiTaskLassoCV])","1031","def test_linear_models_cv_fit_for_all_backends(backend, estimator):","1032","    # LinearModelsCV.fit performs inplace operations on input data which is","1033","    # memmapped when using loky backend, causing an error due to unexpected","1034","    # behavior of fancy indexing of read-only memmaps (cf. numpy#14132).","1035","","1036","    if joblib.__version__ < LooseVersion('0.12') and backend == 'loky':","1037","        pytest.skip('loky backend does not exist in joblib <0.12')","1038","","1039","    # Create a problem sufficiently large to cause memmapping (1MB).","1040","    n_targets = 1 + (estimator in (MultiTaskElasticNetCV, MultiTaskLassoCV))","1041","    X, y = make_regression(20000, 10, n_targets=n_targets)","1042","","1043","    with joblib.parallel_backend(backend=backend):","1044","        estimator(n_jobs=2, cv=3).fit(X, y)"],"delete":[]}],"doc\/whats_new\/v0.23.rst":[{"add":["386","- |Fix| Fixed a bug in :class:`linear_model.ElasticNetCV`,","387","  :class:`linear_model.MultitaskElasticNetCV`, :class:`linear_model.LassoCV`","388","  and :class:`linear_model.MultitaskLassoCV` where fitting would fail when","389","  using joblib loky backend. :pr:`14264` by","390","  :user:`J¨¦r¨¦mie du Boisberranger <jeremiedbb>`.","391",""],"delete":[]}],"sklearn\/linear_model\/_coordinate_descent.py":[{"add":["1070","","1071","    if not sparse.issparse(X):","1072","        for array, array_input in ((X_train, X), (y_train, y),","1073","                                   (X_test, X), (y_test, y)):","1074","            if array.base is not array_input and not array.flags['WRITEABLE']:","1075","                # fancy indexing should create a writable copy but it doesn't","1076","                # for read-only memmaps (cf. numpy#14132).","1077","                array.setflags(write=True)","1078",""],"delete":[]}]}},"0e4f85fe72932c5adeb7de8652a7b76482190341":{"changes":{".github\/labeler.yml":"ADD",".github\/workflows\/labeler.yml":"ADD"},"diff":{".github\/labeler.yml":[{"add":[],"delete":[]}],".github\/workflows\/labeler.yml":[{"add":[],"delete":[]}]}},"e93cde573960f737f84d07dbbc5f7bc7b9aa176b":{"changes":{"sklearn\/preprocessing\/_encoders.py":"MODIFY","sklearn\/preprocessing\/tests\/test_encoders.py":"MODIFY"},"diff":{"sklearn\/preprocessing\/_encoders.py":[{"add":["129","                le.classes_ = np.array(self._categories[i], dtype=X.dtype)"],"delete":["129","                le.classes_ = np.array(self._categories[i])"]}],"sklearn\/preprocessing\/tests\/test_encoders.py":[{"add":["406","@pytest.mark.parametrize(\"X, cat_exp, cat_dtype\", [","407","    ([['abc', 55], ['def', 55]], [['abc', 'def'], [55]], np.object_),","408","    (np.array([[1, 2], [3, 2]]), [[1, 3], [2]], np.integer),","409","    (np.array([['A', 'cat'], ['B', 'cat']], dtype=object),","410","     [['A', 'B'], ['cat']], np.object_),","411","    (np.array([['A', 'cat'], ['B', 'cat']]),","412","     [['A', 'B'], ['cat']], np.str_)","413","    ], ids=['mixed', 'numeric', 'object', 'string'])","414","def test_one_hot_encoder_categories(X, cat_exp, cat_dtype):","417","        enc = OneHotEncoder(categories='auto')","423","            assert np.issubdtype(res.dtype, cat_dtype)","426","@pytest.mark.parametrize(\"X, X2, cats, cat_dtype\", [","427","    (np.array([['a', 'b']], dtype=object).T,","428","     np.array([['a', 'd']], dtype=object).T,","429","     [['a', 'b', 'c']], np.object_),","430","    (np.array([[1, 2]], dtype='int64').T,","431","     np.array([[1, 4]], dtype='int64').T,","432","     [[1, 2, 3]], np.int64),","433","    (np.array([['a', 'b']], dtype=object).T,","434","     np.array([['a', 'd']], dtype=object).T,","435","     [np.array(['a', 'b', 'c'])], np.object_),","436","    ], ids=['object', 'numeric', 'object-string-cat'])","437","def test_one_hot_encoder_specified_categories(X, X2, cats, cat_dtype):","438","    enc = OneHotEncoder(categories=cats)","442","    assert list(enc.categories[0]) == list(cats[0])","443","    assert enc.categories_[0].tolist() == list(cats[0])","444","    # manually specified categories should have same dtype as","445","    # the data when coerced from lists","446","    assert enc.categories_[0].dtype == cat_dtype","447","","448","    # when specifying categories manually, unknown categories should already","449","    # raise when fitting","450","    enc = OneHotEncoder(categories=cats)","451","    assert_raises(ValueError, enc.fit, X2)","452","    enc = OneHotEncoder(categories=cats, handle_unknown='ignore')","453","    exp = np.array([[1., 0., 0.], [0., 0., 0.]])","454","    assert_array_equal(enc.fit(X2).transform(X2).toarray(), exp)","455","","456","","457","def test_one_hot_encoder_unsorted_categories():","458","    X = np.array([['a', 'b']], dtype=object).T","465","","466","def test_one_hot_encoder_specified_categories_mixed_columns():","474","    assert np.issubdtype(enc.categories_[0].dtype, np.object_)","476","    # integer categories but from object dtype data","477","    assert np.issubdtype(enc.categories_[1].dtype, np.object_)"],"delete":["406","def test_one_hot_encoder_categories():","407","    X = [['abc', 1, 55], ['def', 2, 55]]","408","","411","        enc = OneHotEncoder()","415","        cat_exp = [['abc', 'def'], [1, 2], [55]]","420","def test_one_hot_encoder_specified_categories():","421","    X = np.array([['a', 'b']], dtype=object).T","422","","423","    enc = OneHotEncoder(categories=[['a', 'b', 'c']])","427","    assert enc.categories[0] == ['a', 'b', 'c']","428","    assert enc.categories_[0].tolist() == ['a', 'b', 'c']","429","    assert np.issubdtype(enc.categories_[0].dtype, np.str_)","443","    assert np.issubdtype(enc.categories_[0].dtype, np.str_)","445","    assert np.issubdtype(enc.categories_[1].dtype, np.integer)","446","","447","    # when specifying categories manually, unknown categories should already","448","    # raise when fitting","449","    X = np.array([['a', 'b', 'c']]).T","450","    enc = OneHotEncoder(categories=[['a', 'b']])","451","    assert_raises(ValueError, enc.fit, X)","452","    enc = OneHotEncoder(categories=[['a', 'b']], handle_unknown='ignore')","453","    exp = np.array([[1., 0.], [0., 1.], [0., 0.]])","454","    assert_array_equal(enc.fit(X).transform(X).toarray(), exp)"]}]}},"e405505877c8597a82a10ef46a731c11ab565293":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/cluster\/_k_means.pyx":"MODIFY","sklearn\/cluster\/tests\/test_k_means.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["92","- |Fix| Fixed a bug in :class:`KMeans` where empty clusters weren't correctly","93","  relocated when using sample weights. :issue:`13486`","94","  by :user:`J¨¦r¨¦mie du Boisberranger <jeremiedbb>`.","95",""],"delete":[]}],"sklearn\/cluster\/_k_means.pyx":[{"add":["311","            new_center = X[far_index] * sample_weight[far_index]"],"delete":["311","            new_center = X[far_index]"]}],"sklearn\/cluster\/tests\/test_k_means.py":[{"add":["11","from sklearn.utils.testing import assert_allclose","925","","926","","927","def test_k_means_empty_cluster_relocated():","928","    # check that empty clusters are correctly relocated when using sample","929","    # weights (#13486)","930","    X = np.array([[-1], [1]])","931","    sample_weight = [1.9, 0.1]","932","    init = np.array([[-1], [10]])","933","","934","    km = KMeans(n_clusters=2, init=init, n_init=1)","935","    km.fit(X, sample_weight=sample_weight)","936","","937","    assert len(set(km.labels_)) == 2","938","    assert_allclose(km.cluster_centers_, [[-1], [1]])"],"delete":[]}]}},"1888a96908301a9d71e37ce8c08bfd64298e9192":{"changes":{"sklearn\/utils\/validation.py":"MODIFY","sklearn\/utils\/_mocking.py":"MODIFY","sklearn\/ensemble\/_weight_boosting.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY","sklearn\/ensemble\/tests\/test_gradient_boosting.py":"MODIFY","sklearn\/ensemble\/tests\/test_weight_boosting.py":"MODIFY"},"diff":{"sklearn\/utils\/validation.py":[{"add":["1064","            sample_weight, accept_sparse=False, ensure_2d=False, dtype=dtype,","1065","            order=\"C\""],"delete":["1064","                sample_weight, accept_sparse=False,","1065","                ensure_2d=False, dtype=dtype, order=\"C\""]}],"sklearn\/utils\/_mocking.py":[{"add":["137","","138","","139","class NoSampleWeightWrapper(BaseEstimator):","140","    \"\"\"Wrap estimator which will not expose `sample_weight`.","141","","142","    Parameters","143","    ----------","144","    est : estimator, default=None","145","        The estimator to wrap.","146","    \"\"\"","147","    def __init__(self, est=None):","148","        self.est = est","149","","150","    def fit(self, X, y):","151","        return self.est.fit(X, y)","152","","153","    def predict(self, X):","154","        return self.est.predict(X)","155","","156","    def predict_proba(self, X):","157","        return self.est.predict_proba(X)","158","","159","    def _more_tags(self):","160","        return {'_skip_test': True}  # pragma: no cover"],"delete":[]}],"sklearn\/ensemble\/_weight_boosting.py":[{"add":["40","from ..utils.validation import _check_sample_weight","120","        sample_weight = _check_sample_weight(sample_weight, X, np.float64)","121","        sample_weight \/= sample_weight.sum()","122","        if np.any(sample_weight < 0):","123","            raise ValueError(\"sample_weight cannot contain negative weights\")","1022","        bootstrap_idx = random_state.choice(","1023","            np.arange(_num_samples(X)), size=_num_samples(X), replace=True,","1024","            p=sample_weight","1025","        )","1035","        sample_mask = sample_weight > 0","1036","        masked_sample_weight = sample_weight[sample_mask]","1037","        masked_error_vector = error_vect[sample_mask]","1039","        error_max = masked_error_vector.max()","1040","        if error_max != 0:","1041","            masked_error_vector \/= error_max","1044","            masked_error_vector **= 2","1046","            masked_error_vector = 1. - np.exp(-masked_error_vector)","1049","        estimator_error = (masked_sample_weight * masked_error_vector).sum()","1067","            sample_weight[sample_mask] *= np.power(","1068","                beta, (1. - masked_error_vector) * self.learning_rate","1069","            )"],"delete":["119","        if sample_weight is None:","120","            # Initialize weights to 1 \/ n_samples","121","            sample_weight = np.empty(_num_samples(X), dtype=np.float64)","122","            sample_weight[:] = 1. \/ _num_samples(X)","123","        else:","124","            sample_weight = check_array(sample_weight, ensure_2d=False)","125","            # Normalize existing weights","126","            sample_weight = sample_weight \/ sample_weight.sum(dtype=np.float64)","127","","128","            # Check that the sample weights sum is positive","129","            if sample_weight.sum() <= 0:","130","                raise ValueError(","131","                    \"Attempting to fit with a non-positive \"","132","                    \"weighted number of samples.\")","1031","        # For NumPy >= 1.7.0 use np.random.choice","1032","        cdf = stable_cumsum(sample_weight)","1033","        cdf \/= cdf[-1]","1034","        uniform_samples = random_state.random_sample(_num_samples(X))","1035","        bootstrap_idx = cdf.searchsorted(uniform_samples, side='right')","1036","        # searchsorted returns a scalar","1037","        bootstrap_idx = np.array(bootstrap_idx, copy=False)","1047","        error_max = error_vect.max()","1049","        if error_max != 0.:","1050","            error_vect \/= error_max","1053","            error_vect **= 2","1055","            error_vect = 1. - np.exp(- error_vect)","1058","        estimator_error = (sample_weight * error_vect).sum()","1076","            sample_weight *= np.power(","1077","                beta,","1078","                (1. - error_vect) * self.learning_rate)"]}],"doc\/whats_new\/v0.22.rst":[{"add":["257","- |Fix| :class:`ensemble.AdaBoostRegressor` where the loss should be normalized","258","  by the max of the samples with non-null weights only.","259","  :pr:`14294` by :user:`Guillaume Lemaitre <glemaitre>`.","260",""],"delete":[]}],"sklearn\/ensemble\/tests\/test_gradient_boosting.py":[{"add":["26","from sklearn.utils._mocking import NoSampleWeightWrapper","1319","    init_est = NoSampleWeightWrapper(init_estimator())"],"delete":["1294","class _NoSampleWeightWrapper(BaseEstimator):","1295","    def __init__(self, est):","1296","        self.est = est","1297","","1298","    def fit(self, X, y):","1299","        self.est.fit(X, y)","1300","","1301","    def predict(self, X):","1302","        return self.est.predict(X)","1303","","1304","    def predict_proba(self, X):","1305","        return self.est.predict_proba(X)","1306","","1307","","1332","    init_est = _NoSampleWeightWrapper(init_estimator())"]}],"sklearn\/ensemble\/tests\/test_weight_boosting.py":[{"add":["10","","11","from sklearn.utils.testing import assert_array_equal, assert_array_less","12","from sklearn.utils.testing import assert_array_almost_equal","13","from sklearn.utils.testing import assert_raises, assert_raises_regexp","14","","15","from sklearn.base import BaseEstimator","16","from sklearn.base import clone","17","from sklearn.dummy import DummyClassifier, DummyRegressor","18","from sklearn.linear_model import LinearRegression","19","from sklearn.model_selection import train_test_split","20","from sklearn.model_selection import GridSearchCV","21","from sklearn.ensemble import AdaBoostClassifier","22","from sklearn.ensemble import AdaBoostRegressor","23","from sklearn.ensemble._weight_boosting import _samme_proba","27","from sklearn.utils._mocking import NoSampleWeightWrapper","144","@pytest.mark.parametrize('loss', ['linear', 'square', 'exponential'])","145","def test_boston(loss):","147","    reg = AdaBoostRegressor(loss=loss, random_state=0)","500","@pytest.mark.parametrize(\"algorithm\", ['SAMME', 'SAMME.R'])","501","def test_adaboostclassifier_without_sample_weight(algorithm):","502","    X, y = iris.data, iris.target","503","    base_estimator = NoSampleWeightWrapper(DummyClassifier())","504","    clf = AdaBoostClassifier(","505","        base_estimator=base_estimator, algorithm=algorithm","506","    )","507","    err_msg = (\"{} doesn't support sample_weight\"","508","               .format(base_estimator.__class__.__name__))","509","    with pytest.raises(ValueError, match=err_msg):","510","        clf.fit(X, y)","511","","512","","513","def test_adaboostregressor_sample_weight():","514","    # check that giving weight will have an influence on the error computed","515","    # for a weak learner","516","    rng = np.random.RandomState(42)","517","    X = np.linspace(0, 100, num=1000)","518","    y = (.8 * X + 0.2) + (rng.rand(X.shape[0]) * 0.0001)","519","    X = X.reshape(-1, 1)","520","","521","    # add an arbitrary outlier","522","    X[-1] *= 10","523","    y[-1] = 10000","524","","525","    # random_state=0 ensure that the underlying boostrap will use the outlier","526","    regr_no_outlier = AdaBoostRegressor(","527","        base_estimator=LinearRegression(), n_estimators=1, random_state=0","528","    )","529","    regr_with_weight = clone(regr_no_outlier)","530","    regr_with_outlier = clone(regr_no_outlier)","531","","532","    # fit 3 models:","533","    # - a model containing the outlier","534","    # - a model without the outlier","535","    # - a model containing the outlier but with a null sample-weight","536","    regr_with_outlier.fit(X, y)","537","    regr_no_outlier.fit(X[:-1], y[:-1])","538","    sample_weight = np.ones_like(y)","539","    sample_weight[-1] = 0","540","    regr_with_weight.fit(X, y, sample_weight=sample_weight)","541","","542","    score_with_outlier = regr_with_outlier.score(X[:-1], y[:-1])","543","    score_no_outlier = regr_no_outlier.score(X[:-1], y[:-1])","544","    score_with_weight = regr_with_weight.score(X[:-1], y[:-1])","545","","546","    assert score_with_outlier < score_no_outlier","547","    assert score_with_outlier < score_with_weight","548","    assert score_no_outlier == pytest.approx(score_with_weight)","549","","565","","566","","567","@pytest.mark.parametrize(","568","    'model, X, y',","569","    [(AdaBoostClassifier(), iris.data, iris.target),","570","     (AdaBoostRegressor(), boston.data, boston.target)]","571",")","572","def test_adaboost_negative_weight_error(model, X, y):","573","    sample_weight = np.ones_like(y)","574","    sample_weight[-1] = -10","575","","576","    err_msg = \"sample_weight cannot contain negative weight\"","577","    with pytest.raises(ValueError, match=err_msg):","578","        model.fit(X, y, sample_weight=sample_weight)"],"delete":["5","from sklearn.utils.testing import assert_array_equal, assert_array_less","6","from sklearn.utils.testing import assert_array_almost_equal","7","from sklearn.utils.testing import assert_raises, assert_raises_regexp","8","","9","from sklearn.base import BaseEstimator","10","from sklearn.model_selection import train_test_split","11","from sklearn.model_selection import GridSearchCV","12","from sklearn.ensemble import AdaBoostClassifier","13","from sklearn.ensemble import AdaBoostRegressor","14","from sklearn.ensemble._weight_boosting import _samme_proba","139","def test_boston():","141","    reg = AdaBoostRegressor(random_state=0)","306","def test_sample_weight_missing():","307","    from sklearn.cluster import KMeans","308","","309","    clf = AdaBoostClassifier(KMeans(), algorithm=\"SAMME\")","310","    assert_raises(ValueError, clf.fit, X, y_regr)","311","","312","    clf = AdaBoostRegressor(KMeans())","313","    assert_raises(ValueError, clf.fit, X, y_regr)","314","","315","","488","","489","    from sklearn.dummy import DummyClassifier, DummyRegressor","490",""]}]}},"dc7a68575b4eb60020e46c73b3e5fc5825aacebf":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/cluster\/tests\/test_k_means.py":"MODIFY","sklearn\/cluster\/_k_means_elkan.pyx":"MODIFY","sklearn\/cluster\/k_means_.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["547","- Fixed a bug in :func:`cluster.k_means_elkan` where the returned `iteration`","548","  was 1 less than the correct value. Also added the missing `n_iter_` attribute","549","  in the docstring of :class:`cluster.KMeans`. :issue:`11353` by","550","  :user:`Jeremie du Boisberranger <jeremiedbb>`.","551",""],"delete":[]}],"sklearn\/cluster\/tests\/test_k_means.py":[{"add":["981","","982","","983","def test_iter_attribute():","984","    # Regression test on bad n_iter_ value. Previous bug n_iter_ was one off","985","    # it's right value (#11340).","986","    estimator = KMeans(algorithm=\"elkan\", max_iter=1)","987","    estimator.fit(np.random.rand(10, 10))","988","    assert estimator.n_iter_ == 1"],"delete":[]}],"sklearn\/cluster\/_k_means_elkan.pyx":[{"add":["260","    return centers_, labels_, iteration + 1"],"delete":["260","    return centers_, labels_, iteration"]}],"sklearn\/cluster\/k_means_.py":[{"add":["861","    n_iter_ : int","862","        Number of iterations run.","863",""],"delete":[]}]}},"f1acf834685f8bcd1bcdd903e9c40b7515fe0a67":{"changes":{"sklearn\/utils\/_mocking.py":"MODIFY","doc\/whats_new\/v0.23.rst":"MODIFY","sklearn\/ensemble\/_stacking.py":"MODIFY","sklearn\/ensemble\/tests\/test_stacking.py":"MODIFY"},"diff":{"sklearn\/utils\/_mocking.py":[{"add":["97","        self.n_features_in_ = len(X)"],"delete":[]}],"doc\/whats_new\/v0.23.rst":[{"add":["184","- |Fix| Fixed a bug in :class:`ensemble.StackingClassifier` and","185","  :class:`ensemble.StackingRegressor` where the `sample_weight`","186","  argument was not being passed to `cross_val_predict` when","187","  evaluating the base estimators on cross-validation folds","188","  to obtain the input to the meta estimator.","189","  :pr:`16539` by :user:`Bill DeRose <wderose>`.","190",""],"delete":[]}],"sklearn\/ensemble\/_stacking.py":[{"add":["124","            .. versionchanged:: 0.23","125","               when not None, `sample_weight` is passed to all underlying","126","               estimators","127","","172","        fit_params = ({\"sample_weight\": sample_weight}","173","                      if sample_weight is not None","174","                      else None)","178","                                       fit_params=fit_params,","192","        _fit_single_estimator(self.final_estimator_, X_meta, y,","193","                              sample_weight=sample_weight)"],"delete":["168","","185","        if sample_weight is not None:","186","            try:","187","                self.final_estimator_.fit(","188","                    X_meta, y, sample_weight=sample_weight","189","                )","190","            except TypeError as exc:","191","                if \"unexpected keyword argument 'sample_weight'\" in str(exc):","192","                    raise TypeError(","193","                        \"Underlying estimator {} does not support sample \"","194","                        \"weights.\"","195","                        .format(self.final_estimator_.__class__.__name__)","196","                    ) from exc","197","                raise","198","        else:","199","            self.final_estimator_.fit(X_meta, y)"]}],"sklearn\/ensemble\/tests\/test_stacking.py":[{"add":["40","from sklearn.utils._mocking import CheckingClassifier","442","def test_stacking_classifier_sample_weight_fit_param():","443","    # check sample_weight is passed to all invocations of fit","444","    stacker = StackingClassifier(","445","        estimators=[","446","            ('lr', CheckingClassifier(expected_fit_params=['sample_weight']))","447","        ],","448","        final_estimator=CheckingClassifier(","449","            expected_fit_params=['sample_weight']","450","        )","451","    )","452","    stacker.fit(X_iris, y_iris, sample_weight=np.ones(X_iris.shape[0]))","453","","454",""],"delete":[]}]}},"b8c402722bb0a3dc307443e719ab7b5a037d6de5":{"changes":{"sklearn\/utils\/sparsefuncs_fast.pyx":"MODIFY","doc\/whats_new\/v0.23.rst":"MODIFY","sklearn\/utils\/tests\/test_sparsefuncs.py":"MODIFY","sklearn\/preprocessing\/tests\/test_data.py":"MODIFY"},"diff":{"sklearn\/utils\/sparsefuncs_fast.pyx":[{"add":["336","        if new_n[i] > 0:","337","            updated_n[i] = last_n[i] + new_n[i]","338","            last_over_new_n[i] = dtype(last_n[i]) \/ dtype(new_n[i])","339","            # Unnormalized stats","340","            last_mean[i] *= last_n[i]","341","            last_var[i] *= last_n[i]","342","            new_mean[i] *= new_n[i]","343","            new_var[i] *= new_n[i]","344","            # Update stats","345","            updated_var[i] = (","346","                last_var[i] + new_var[i] +","347","                last_over_new_n[i] \/ updated_n[i] *","348","                (last_mean[i] \/ last_over_new_n[i] - new_mean[i])**2","349","            )","350","            updated_mean[i] = (last_mean[i] + new_mean[i]) \/ updated_n[i]","351","            updated_var[i] \/= updated_n[i]","352","        else:","353","            updated_var[i] = last_var[i]","354","            updated_mean[i] = last_mean[i]","355","            updated_n[i] = last_n[i]"],"delete":["336","        updated_n[i] = last_n[i] + new_n[i]","337","        last_over_new_n[i] = last_n[i] \/ new_n[i]","338","","339","    # Unnormalized stats","340","    for i in range(n_features):","341","        last_mean[i] *= last_n[i]","342","        last_var[i] *= last_n[i]","343","        new_mean[i] *= new_n[i]","344","        new_var[i] *= new_n[i]","345","","346","    # Update stats","347","    for i in range(n_features):","348","        updated_var[i] = (last_var[i] + new_var[i] +","349","                          last_over_new_n[i] \/ updated_n[i] *","350","                          (last_mean[i] \/ last_over_new_n[i] - new_mean[i])**2)","351","        updated_mean[i] = (last_mean[i] + new_mean[i]) \/ updated_n[i]","352","        updated_var[i] \/= updated_n[i]"]}],"doc\/whats_new\/v0.23.rst":[{"add":["358","- |Fix| Fix a bug in :class:`preprocessing.StandardScaler` which was incorrectly","359","  computing statistics when calling `partial_fit` on sparse inputs.","360","  :pr:`16466` by :user:`Guillaume Lemaitre <glemaitre>`.","361",""],"delete":[]}],"sklearn\/utils\/tests\/test_sparsefuncs.py":[{"add":["153","@pytest.mark.parametrize(","154","    \"X1, X2\",","155","    [","156","        (sp.random(5, 2, density=0.8, format='csr', random_state=0),","157","         sp.random(13, 2, density=0.8, format='csr', random_state=0)),","158","        (sp.random(5, 2, density=0.8, format='csr', random_state=0),","159","         sp.hstack([sp.csr_matrix(np.full((13, 1), fill_value=np.nan)),","160","                    sp.random(13, 1, density=0.8, random_state=42)],","161","                   format=\"csr\"))","162","    ]","163",")","164","def test_incr_mean_variance_axis_equivalence_mean_variance(X1, X2):","165","    # non-regression test for:","166","    # https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/16448","167","    # check that computing the incremental mean and variance is equivalent to","168","    # computing the mean and variance on the stacked dataset.","169","    axis = 0","170","    last_mean, last_var = np.zeros(X1.shape[1]), np.zeros(X1.shape[1])","171","    last_n = np.zeros(X1.shape[1], dtype=np.int64)","172","    updated_mean, updated_var, updated_n = incr_mean_variance_axis(","173","        X1, axis, last_mean, last_var, last_n","174","    )","175","    updated_mean, updated_var, updated_n = incr_mean_variance_axis(","176","        X2, axis, updated_mean, updated_var, updated_n","177","    )","178","    X = sp.vstack([X1, X2])","179","    assert_allclose(updated_mean, np.nanmean(X.A, axis=axis))","180","    assert_allclose(updated_var, np.nanvar(X.A, axis=axis))","181","    assert_allclose(updated_n, np.count_nonzero(~np.isnan(X.A), axis=0))","182","","183","","184","def test_incr_mean_variance_no_new_n():","185","    # check the behaviour when we update the variance with an empty matrix","186","    axis = 0","187","    X1 = sp.random(5, 1, density=0.8, random_state=0).tocsr()","188","    X2 = sp.random(0, 1, density=0.8, random_state=0).tocsr()","189","    last_mean, last_var = np.zeros(X1.shape[1]), np.zeros(X1.shape[1])","190","    last_n = np.zeros(X1.shape[1], dtype=np.int64)","191","    last_mean, last_var, last_n = incr_mean_variance_axis(","192","        X1, axis, last_mean, last_var, last_n","193","    )","194","    # update statistic with a column which should ignored","195","    updated_mean, updated_var, updated_n = incr_mean_variance_axis(","196","        X2, axis, last_mean, last_var, last_n","197","    )","198","    assert_allclose(updated_mean, last_mean)","199","    assert_allclose(updated_var, last_var)","200","    assert_allclose(updated_n, last_n)","201","","202",""],"delete":[]}],"sklearn\/preprocessing\/tests\/test_data.py":[{"add":["2473","","2474","","2475","@pytest.mark.parametrize(","2476","    \"X_2\",","2477","    [sparse.random(10, 1, density=0.8, random_state=0),","2478","     sparse.csr_matrix(np.full((10, 1), fill_value=np.nan))]","2479",")","2480","def test_standard_scaler_sparse_partial_fit_finite_variance(X_2):","2481","    # non-regression test for:","2482","    # https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/16448","2483","    X_1 = sparse.random(5, 1, density=0.8)","2484","    scaler = StandardScaler(with_mean=False)","2485","    scaler.fit(X_1).partial_fit(X_2)","2486","    assert np.isfinite(scaler.var_[0])"],"delete":[]}]}},"661a8b42bd93a4b77a14d89aa7b96c2df158cf9d":{"changes":{"sklearn\/compose\/tests\/test_column_transformer.py":"MODIFY","sklearn\/compose\/_column_transformer.py":"MODIFY"},"diff":{"sklearn\/compose\/tests\/test_column_transformer.py":[{"add":["455","                                 n_jobs=3, remainder='drop',","456","                                 sparse_threshold=0.3)","461","    assert_equal(ct.sparse_threshold, 0.3)"],"delete":["455","                                 n_jobs=3, remainder='drop')"]}],"sklearn\/compose\/_column_transformer.py":[{"add":["691","    sparse_threshold : float, default = 0.3","692","        If the transformed output consists of a mix of sparse and dense data,","693","        it will be stacked as a sparse matrix if the density is lower than this","694","        value. Use ``sparse_threshold=0`` to always return dense.","695","        When the transformed output consists of all sparse or all dense data,","696","        the stacked result will be sparse or dense, respectively, and this","697","        keyword will be ignored.","698","","735","    sparse_threshold = kwargs.pop('sparse_threshold', 0.3)","741","                             remainder=remainder,","742","                             sparse_threshold=sparse_threshold)"],"delete":["732","                             remainder=remainder)"]}]}},"f23b940dcabdc86b8b71dc8a9a90ef91505407cc":{"changes":{"sklearn\/base.py":"MODIFY","sklearn\/tests\/test_base.py":"MODIFY"},"diff":{"sklearn\/base.py":[{"add":["439","    @property","441","        \"\"\"HTML representation of estimator.","442","","443","        This is redundant with the logic of `_repr_mimebundle_`. The latter","444","        should be favorted in the long term, `_repr_html_` is only","445","        implemented for consumers who do not interpret `_repr_mimbundle_`.","446","        \"\"\"","447","        if get_config()[\"display\"] != 'diagram':","448","            raise AttributeError(\"_repr_html_ is only defined when the \"","449","                                 \"'display' configuration option is set to \"","450","                                 \"'diagram'\")","451","        return self._repr_html_inner","452","","453","    def _repr_html_inner(self):","454","        \"\"\"This function is returned by the @property `_repr_html_` to make","455","        `hasattr(estimator, \"_repr_html_\") return `True` or `False` depending","456","        on `get_config()[\"display\"]`.","457","        \"\"\""],"delete":["440","        \"\"\"HTML representation of estimator\"\"\""]}],"sklearn\/tests\/test_base.py":[{"add":["527","","528","","529","def test_repr_html_wraps():","530","    # Checks the display configuration flag controls the html output","531","    tree = DecisionTreeClassifier()","532","    msg = \"_repr_html_ is only defined when\"","533","    with pytest.raises(AttributeError, match=msg):","534","        output = tree._repr_html_()","535","","536","    with config_context(display='diagram'):","537","        output = tree._repr_html_()","538","        assert \"<style>\" in output"],"delete":[]}]}},"0ad7481bb1636252e957efcf82fff6160f0bacfc":{"changes":{"doc\/whats_new\/v0.23.rst":"MODIFY","sklearn\/metrics\/_plot\/roc_curve.py":"MODIFY","sklearn\/metrics\/_plot\/tests\/test_plot_roc_curve.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.23.rst":[{"add":["260","- |Fix| Fixed a bug in :func:`metrics.plot_roc_curve` where","261","  the name of the estimator was passed in the :class:`metrics.RocCurveDisplay`","262","  instead of the parameter `name`. It results in a different plot when calling","263","  :meth:`metrics.RocCurveDisplay.plot` for the subsequent times.","264","  :pr:`16500` by :user:`Guillaume Lemaitre <glemaitre>`.","265",""],"delete":[]}],"sklearn\/metrics\/_plot\/roc_curve.py":[{"add":["167","    classification_error = (","168","        \"{} should be a binary classifier\".format(estimator.__class__.__name__)","169","    )","188","    name = estimator.__class__.__name__ if name is None else name","189","    viz = RocCurveDisplay(","190","        fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name=name","191","    )"],"delete":["167","    classification_error = (\"{} should be a binary classifer\".format(","168","        estimator.__class__.__name__))","187","    viz = RocCurveDisplay(fpr, tpr, roc_auc, estimator.__class__.__name__)"]}],"sklearn\/metrics\/_plot\/tests\/test_plot_roc_curve.py":[{"add":["38","    msg = \"DecisionTreeClassifier should be a binary classifier\"","133","    assert clf.__class__.__name__ in disp.line_.get_label()","135","","136","","137","def test_plot_roc_curve_estimator_name_multiple_calls(pyplot, data_binary):","138","    # non-regression test checking that the `name` used when calling","139","    # `plot_roc_curve` is used as well when calling `disp.plot()`","140","    X, y = data_binary","141","    clf_name = \"my hand-crafted name\"","142","    clf = LogisticRegression().fit(X, y)","143","    disp = plot_roc_curve(clf, X, y, name=clf_name)","144","    assert disp.estimator_name == clf_name","145","    pyplot.close(\"all\")","146","    disp.plot()","147","    assert clf_name in disp.line_.get_label()","148","    pyplot.close(\"all\")","149","    clf_name = \"another_name\"","150","    disp.plot(name=clf_name)","151","    assert clf_name in disp.line_.get_label()"],"delete":["38","    msg = \"DecisionTreeClassifier should be a binary classifer\""]}]}},"e8a42aee5d5b3f2d25f3dddbc0c55c96da03e230":{"changes":{"build_tools\/azure\/install.sh":"MODIFY"},"diff":{"build_tools\/azure\/install.sh":[{"add":[],"delete":["100","    # TODO: Remove pin when https:\/\/github.com\/python-pillow\/Pillow\/issues\/4518 gets fixed","101","    python -m pip install \"pillow>=4.3.0,!=7.1.0,!=7.1.1\"","102",""]}]}},"bf8eff3feaded1464e81dcc0c4b7f9a3975c014f":{"changes":{"sklearn\/cluster\/tests\/test_k_means.py":"MODIFY","sklearn\/cluster\/k_means_.py":"MODIFY"},"diff":{"sklearn\/cluster\/tests\/test_k_means.py":[{"add":["944","","945","","946","def test_minibatch_kmeans_partial_fit_int_data():","947","    # Issue GH #14314","948","    X = np.array([[-1], [1]], dtype=np.int)","949","    km = MiniBatchKMeans(n_clusters=2)","950","    km.partial_fit(X)","951","    assert km.cluster_centers_.dtype.kind == \"f\""],"delete":[]}],"sklearn\/cluster\/k_means_.py":[{"add":["1424","    array([[2. , 1. ],","1425","           [3.5, 4.5]])","1669","        X = check_array(X, accept_sparse=\"csr\", order=\"C\",","1670","                        dtype=[np.float64, np.float32])"],"delete":["1424","    array([[1, 1],","1425","           [3, 4]])","1669","        X = check_array(X, accept_sparse=\"csr\", order=\"C\")"]}]}},"1add39ff110bab171df998d978cc88bab0232e54":{"changes":{"sklearn\/feature_selection\/univariate_selection.py":"MODIFY","sklearn\/feature_selection\/tests\/test_feature_select.py":"MODIFY"},"diff":{"sklearn\/feature_selection\/univariate_selection.py":[{"add":["498","        m = float(len(self.pvalues_))","499","        selected = sv[sv < alpha \/ m * np.arange(len(self.pvalues_))]"],"delete":["498","        selected = sv[sv < alpha * np.arange(len(self.pvalues_))]"]}],"sklearn\/feature_selection\/tests\/test_feature_select.py":[{"add":["297","    alpha = 0.01","298","    univariate_filter = SelectFdr(f_classif, alpha=alpha)","301","        f_classif, mode='fdr', param=alpha).fit(X, y).transform(X)","308","    num_false_positives = np.sum(support[5:] == 1)","309","    num_true_positives = np.sum(support[:5] == 1)","310","","311","    false_discovery_rate = float(num_false_positives) \/ \\","312","        (num_true_positives + num_false_positives)","313","    # We have that","314","    #  FDR = E(num_false_positives \/ (true_positives + false_positives))","315","    #      <= alpha","316","    assert(false_discovery_rate < alpha)","317","","323","    with the fwe heuristic","457","    with the fdr heuristic.","459","    Tests that the scaling factors","460","    \"\"\"","461","    def scale_invariance(n_samples, n_features, n_informative, alpha):","462","        X, y = make_regression(n_samples=n_samples, n_features=n_features,","463","                               n_informative=n_informative, shuffle=False,","464","                               random_state=0)","465","","466","        univariate_filter = SelectFdr(f_regression, alpha=alpha)","467","        X_r = univariate_filter.fit(X, y).transform(X)","468","        X_r2 = GenericUnivariateSelect(","469","            f_regression, mode='fdr', param=alpha).fit(X, y).transform(X)","470","        assert_array_equal(X_r, X_r2)","471","        support = univariate_filter.get_support()","472","        num_false_positives = np.sum(support[n_informative:] == 1)","473","        num_true_positives = np.sum(support[:n_informative] == 1)","474","","475","        false_discovery_rate = float(num_false_positives) \/ \\","476","            (num_true_positives + num_false_positives)","477","        # We have that","478","        #  FDR = E(num_false_positives \/ (true_positives + false_positives))","479","        #      <= alpha","480","        assert(false_discovery_rate < alpha)","481","","482","    feature_scaling_factors = (5, 10, 20)","483","    for scale_factor in feature_scaling_factors:","484","        scale_invariance(2000, 5 * scale_factor, 1 * scale_factor, 0.05)"],"delete":["297","","298","    univariate_filter = SelectFdr(f_classif, alpha=0.0001)","301","        f_classif, mode='fdr', param=0.0001).fit(X, y).transform(X)","313","    with the fpr heuristic","447","    with the fdr heuristic","448","    \"\"\"","449","    X, y = make_regression(n_samples=200, n_features=20,","450","                           n_informative=5, shuffle=False, random_state=0)","452","    univariate_filter = SelectFdr(f_regression, alpha=0.01)","453","    X_r = univariate_filter.fit(X, y).transform(X)","454","    X_r2 = GenericUnivariateSelect(","455","        f_regression, mode='fdr', param=0.01).fit(X, y).transform(X)","456","    assert_array_equal(X_r, X_r2)","457","    support = univariate_filter.get_support()","458","    gtruth = np.zeros(20)","459","    gtruth[:5] = 1","460","    assert_array_equal(support, gtruth)"]}]}},"1fe00b58949a6b9bce45e9e15eb8b9c138bd6a2e":{"changes":{"sklearn\/utils\/tests\/test_pprint.py":"MODIFY","sklearn\/base.py":"MODIFY"},"diff":{"sklearn\/utils\/tests\/test_pprint.py":[{"add":["461","def test_bruteforce_ellipsis():","462","    # Check that the bruteforce ellipsis (used when the number of non-blank","463","    # characters exceeds N_CHAR_MAX) renders correctly.","465","    lr = LogisticRegression()","466","","467","    # test when the left and right side of the ellipsis aren't on the same","468","    # line.","469","    expected = \"\"\"","470","LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,","471","                   in...","472","                   multi_class='warn', n_jobs=None, penalty='l2',","473","                   random_state=None, solver='warn', tol=0.0001, verbose=0,","474","                   warm_start=False)\"\"\"","475","","476","    expected = expected[1:]  # remove first \\n","477","    assert expected == lr.__repr__(N_CHAR_MAX=150)","478","","479","    # test with very small N_CHAR_MAX","480","    # Note that N_CHAR_MAX is not strictly enforced, but it's normal: to avoid","481","    # weird reprs we still keep the whole line of the right part (after the","482","    # ellipsis).","483","    expected = \"\"\"","484","Lo...","485","                   warm_start=False)\"\"\"","486","","487","    expected = expected[1:]  # remove first \\n","488","    assert expected == lr.__repr__(N_CHAR_MAX=4)","489","","490","    # test with N_CHAR_MAX == number of non-blank characters: In this case we","491","    # don't want ellipsis","492","    full_repr = lr.__repr__(N_CHAR_MAX=float('inf'))","493","    n_nonblank = len(''.join(full_repr.split()))","494","    assert lr.__repr__(N_CHAR_MAX=n_nonblank) == full_repr","495","    assert '...' not in full_repr","496","","497","    # test with N_CHAR_MAX == number of non-blank characters - 10: the left and","498","    # right side of the ellispsis are on different lines. In this case we","499","    # want to expend the whole line of the right side","500","    expected = \"\"\"","501","LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,","502","                   intercept_scaling=1, l1_ratio=None, max_i...","503","                   multi_class='warn', n_jobs=None, penalty='l2',","504","                   random_state=None, solver='warn', tol=0.0001, verbose=0,","505","                   warm_start=False)\"\"\"","506","    expected = expected[1:]  # remove first \\n","507","    assert expected == lr.__repr__(N_CHAR_MAX=n_nonblank - 10)","508","","509","    # test with N_CHAR_MAX == number of non-blank characters - 10: the left and","510","    # right side of the ellispsis are on the same line. In this case we don't","511","    # want to expend the whole line of the right side, just add the ellispsis","512","    # between the 2 sides.","513","    expected = \"\"\"","514","LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,","515","                   intercept_scaling=1, l1_ratio=None, max_iter...,","516","                   multi_class='warn', n_jobs=None, penalty='l2',","517","                   random_state=None, solver='warn', tol=0.0001, verbose=0,","518","                   warm_start=False)\"\"\"","519","    expected = expected[1:]  # remove first \\n","520","    assert expected == lr.__repr__(N_CHAR_MAX=n_nonblank - 4)","521","","522","    # test with N_CHAR_MAX == number of non-blank characters - 2: the left and","523","    # right side of the ellispsis are on the same line, but adding the ellipsis","524","    # would actually make the repr longer. So we don't add the ellipsis.","525","    expected = \"\"\"","526","LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,","527","                   intercept_scaling=1, l1_ratio=None, max_iter=100,","528","                   multi_class='warn', n_jobs=None, penalty='l2',","529","                   random_state=None, solver='warn', tol=0.0001, verbose=0,","530","                   warm_start=False)\"\"\"","531","    expected = expected[1:]  # remove first \\n","532","    assert expected == lr.__repr__(N_CHAR_MAX=n_nonblank - 2)"],"delete":["461","def test_length_constraint():","462","    # When repr is still too long, use bruteforce ellipsis","463","    # repr is a very long line so we don't check for equality here, just that","464","    # ellipsis has been done. It's not the ellipsis from before because the","465","    # number of elements in the dict is only 1.","466","    vocabulary = {0: 'hello' * 1000}","467","    vectorizer = CountVectorizer(vocabulary=vocabulary)","468","    repr_ = vectorizer.__repr__()","469","    assert '...' in repr_"]}],"sklearn\/base.py":[{"add":["10","import re","236","    def __repr__(self, N_CHAR_MAX=700):","237","        # N_CHAR_MAX is the (approximate) maximum number of non-blank","238","        # characters to render. We pass it as an optional parameter to ease","239","        # the tests.","240","","252","        # Use bruteforce ellipsis when there are a lot of non-blank characters","253","        n_nonblank = len(''.join(repr_.split()))","254","        if n_nonblank > N_CHAR_MAX:","255","            lim = N_CHAR_MAX \/\/ 2  # apprx number of chars to keep on both ends","256","            regex = r'^(\\s*\\S){%d}' % lim","257","            # The regex '^(\\s*\\S){%d}' % n","258","            # matches from the start of the string until the nth non-blank","259","            # character:","260","            # - ^ matches the start of string","261","            # - (pattern){n} matches n repetitions of pattern","262","            # - \\s*\\S matches a non-blank char following zero or more blanks","263","            left_lim = re.match(regex, repr_).end()","264","            right_lim = re.match(regex, repr_[::-1]).end()","265","","266","            if '\\n' in repr_[left_lim:-right_lim]:","267","                # The left side and right side aren't on the same line.","268","                # To avoid weird cuts, e.g.:","269","                # categoric...ore',","270","                # we need to start the right side with an appropriate newline","271","                # character so that it renders properly as:","272","                # categoric...","273","                # handle_unknown='ignore',","274","                # so we add [^\\n]*\\n which matches until the next \\n","275","                regex += r'[^\\n]*\\n'","276","                right_lim = re.match(regex, repr_[::-1]).end()","277","","278","            ellipsis = '...'","279","            if left_lim + len(ellipsis) < len(repr_) - right_lim:","280","                # Only add ellipsis if it results in a shorter repr","281","                repr_ = repr_[:left_lim] + '...' + repr_[-right_lim:]","282",""],"delete":["235","    def __repr__(self):","238","        N_CHAR_MAX = 700  # number of non-whitespace or newline chars","248","        # Use bruteforce ellipsis if string is very long","249","        if len(''.join(repr_.split())) > N_CHAR_MAX:  # check non-blank chars","250","            lim = N_CHAR_MAX \/\/ 2","251","            repr_ = repr_[:lim] + '...' + repr_[-lim:]"]}]}},"b4d7c96054519e958e7d8b9612fd74f46d807b80":{"changes":{"sklearn\/model_selection\/tests\/test_search.py":"MODIFY","sklearn\/model_selection\/_search.py":"MODIFY"},"diff":{"sklearn\/model_selection\/tests\/test_search.py":[{"add":["653","@pytest.mark.parametrize('out_bound_value', [-1, 2])","654","@pytest.mark.parametrize('search_cv', [RandomizedSearchCV, GridSearchCV])","655","def test_refit_callable_out_bound(out_bound_value, search_cv):","664","        return out_bound_value","669","    clf = search_cv(LinearSVC(random_state=42), {'C': [0.1, 1]},","670","                    scoring='precision', refit=refit_callable_out_bound, cv=5)"],"delete":["653","def test_refit_callable_out_bound():","662","        return -1","667","    clf = GridSearchCV(LinearSVC(random_state=42), {'C': [0.1, 1]},","668","                       scoring='precision', refit=refit_callable_out_bound,","669","                       cv=5)"]}],"sklearn\/model_selection\/_search.py":[{"add":["700","                if (self.best_index_ < 0 or","701","                   self.best_index_ >= len(results[\"params\"])):"],"delete":["700","                if self.best_index_ < 0 or self.best_index_ >= len(results):"]}]}},"986a49bbe018aa8060f53c146fc06f278f80b7a6":{"changes":{"sklearn\/model_selection\/_validation.py":"MODIFY","sklearn\/cross_validation.py":"MODIFY","sklearn\/model_selection\/tests\/test_validation.py":"MODIFY"},"diff":{"sklearn\/model_selection\/_validation.py":[{"add":["624","        X_train, y_train = _safe_split(estimator, X, y, train)","625","        X_test, y_test = _safe_split(estimator, X, y, test, train)","626","        estimator.fit(X_train, y_train)","627","        avg_score.append(scorer(estimator, X_test, y_test))","640","    return safe_indexing(y, indices)"],"delete":["624","        estimator.fit(X[train], y[train])","625","        avg_score.append(scorer(estimator, X[test], y[test]))","638","    return y[indices]"]}],"sklearn\/cross_validation.py":[{"add":["1758","        X_train, y_train = _safe_split(estimator, X, y, train)","1759","        X_test, y_test = _safe_split(estimator, X, y, test, train)","1760","        estimator.fit(X_train, y_train)","1761","        avg_score.append(scorer(estimator, X_test, y_test))","1774","    return safe_indexing(y, ind)"],"delete":["1758","        estimator.fit(X[train], y[train])","1759","        avg_score.append(scorer(estimator, X[test], y[test]))","1772","    return y[ind]"]}],"sklearn\/model_selection\/tests\/test_validation.py":[{"add":["968","","969","","970","def test_permutation_test_score_pandas():","971","    # check permutation_test_score doesn't destroy pandas dataframe","972","    types = [(MockDataFrame, MockDataFrame)]","973","    try:","974","        from pandas import Series, DataFrame","975","        types.append((Series, DataFrame))","976","    except ImportError:","977","        pass","978","    for TargetType, InputFeatureType in types:","979","        # X dataframe, y series","980","        iris = load_iris()","981","        X, y = iris.data, iris.target","982","        X_df, y_ser = InputFeatureType(X), TargetType(y)","983","        check_df = lambda x: isinstance(x, InputFeatureType)","984","        check_series = lambda x: isinstance(x, TargetType)","985","        clf = CheckingClassifier(check_X=check_df, check_y=check_series)","986","        permutation_test_score(clf, X_df, y_ser)"],"delete":[]}]}},"1b119c46937f29b1b29fb8eaaee6910beb7807d0":{"changes":{"doc\/authors.rst":"MODIFY","build_tools\/generate_authors_table.py":"MODIFY"},"diff":{"doc\/authors.rst":[{"add":["9","    <p>J¨¦r¨¦mie du Boisberranger<\/p>"],"delete":["9","    <p>J¨¦r¨¦mie Du Boisberranger<\/p>"]}],"build_tools\/generate_authors_table.py":[{"add":["13","from os import path","21","REPO_FOLDER = Path(path.abspath(__file__)).parent.parent"],"delete":["20","REPO_FOLDER = Path(__file__).parent.parent","102","        'jeremiedbb': 'J¨¦r¨¦mie Du Boisberranger',"]}]}},"3b35104c93cb53f67fb5f52ae2fece76ef7144da":{"changes":{"sklearn\/utils\/seq_dataset.pxd.tp":"MODIFY","sklearn\/svm\/libsvm.pyx":"MODIFY","sklearn\/manifold\/_barnes_hut_tsne.pyx":"MODIFY","sklearn\/utils\/seq_dataset.pyx.tp":"MODIFY"},"diff":{"sklearn\/utils\/seq_dataset.pxd.tp":[{"add":["0","# cython: language_level=3"],"delete":["23",""]}],"sklearn\/svm\/libsvm.pyx":[{"add":[],"delete":["36","from . cimport libsvm"]}],"sklearn\/manifold\/_barnes_hut_tsne.pyx":[{"add":["18","from sklearn.neighbors.quad_tree cimport _QuadTree","51","                            _QuadTree qt,","165","                                    _QuadTree qt,","263","    cdef _QuadTree qt = _QuadTree(pos_output.shape[1], verbose)"],"delete":["18","from sklearn.neighbors import quad_tree","19","from sklearn.neighbors cimport quad_tree","52","                            quad_tree._QuadTree qt,","166","                                    quad_tree._QuadTree qt,","264","    cdef quad_tree._QuadTree qt = quad_tree._QuadTree(pos_output.shape[1],","265","                                                      verbose)"]}],"sklearn\/utils\/seq_dataset.pyx.tp":[{"add":["0","# cython: language_level=3","1","# cython: cdivision=True","2","# cython: boundscheck=False","3","# cython: wraparound=False"],"delete":["28","","38","# cython: cdivision=True","39","# cython: boundscheck=False","40","# cython: wraparound=False","41",""]}]}},"5a9ce9fe3fe6cdf2574f0142e3f38698155f707a":{"changes":{".travis.yml":"MODIFY","build_tools\/travis\/install.sh":"MODIFY","doc\/developers\/contributing.rst":"MODIFY","sklearn\/preprocessing\/tests\/test_common.py":"MODIFY"},"diff":{".travis.yml":[{"add":["58","       env: DISTRIB=\"scipy-dev\"","59","       if: type = cron OR commit_message ~ \/\\[scipy-dev\\]\/"],"delete":["58","       env: DISTRIB=\"scipy-dev-wheels\"","59","       if: type = cron"]}],"build_tools\/travis\/install.sh":[{"add":["80","elif [[ \"$DISTRIB\" == \"scipy-dev\" ]]; then"],"delete":["80","elif [[ \"$DISTRIB\" == \"scipy-dev-wheels\" ]]; then"]}],"doc\/developers\/contributing.rst":[{"add":["402","     [scipy-dev]            Add a Travis build with our dependencies (numpy, scipy, etc ...) development builds"],"delete":[]}],"sklearn\/preprocessing\/tests\/test_common.py":[{"add":["0","import warnings","1","","114","                warnings.simplefilter('ignore', PendingDeprecationWarning)","119","                warnings.simplefilter('ignore', PendingDeprecationWarning)"],"delete":[]}]}},"86c5998dbb1951ce6ba52e42da88aeb7e6e9f0a4":{"changes":{"sklearn\/neighbors\/base.py":"MODIFY","sklearn\/svm\/base.py":"MODIFY","sklearn\/utils\/estimator_checks.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY","sklearn\/preprocessing\/data.py":"MODIFY"},"diff":{"sklearn\/neighbors\/base.py":[{"add":["214","        # Precomputed matrix X must be squared","215","        if self.metric == 'precomputed' and X.shape[0] != X.shape[1]:","216","            raise ValueError(\"Precomputed matrix must be a square matrix.\"","217","                             \" Input is a {}x{} matrix.\"","218","                             .format(X.shape[0], X.shape[1]))","219",""],"delete":[]}],"sklearn\/svm\/base.py":[{"add":["161","            raise ValueError(\"Precomputed matrix must be a square matrix.\"","162","                             \" Input is a {}x{} matrix.\"","163","                             .format(X.shape[0], X.shape[1]))"],"delete":["161","            raise ValueError(\"X.shape[0] should be equal to X.shape[1]\")"]}],"sklearn\/utils\/estimator_checks.py":[{"add":["98","    if _is_pairwise(estimator):","99","        # Check that pairwise estimator throws error on non-square input","100","        yield check_nonsquare_error","101","","880","    X = pairwise_estimator_convert_X(X, estimator_orig)","881","","969","    X = pairwise_estimator_convert_X(X, transformer)","985","    X = pairwise_estimator_convert_X(X, transformer)","1068","        if hasattr(X, 'shape') and \\","1069","           not _safe_tags(transformer, \"stateless\") and \\","1070","           X.ndim == 2 and X.shape[1] > 1:","1071","","1078","                transformer.transform(X[:, :-1])","1261","def check_nonsquare_error(name, estimator_orig):","1262","    \"\"\"Test that error is thrown when non-square data provided\"\"\"","1263","","1264","    X, y = make_blobs(n_samples=20, n_features=10)","1265","    estimator = clone(estimator_orig)","1266","","1267","    with assert_raises(ValueError, msg=\"The pairwise estimator {}\"","1268","                       \" does not raise an error on non-square data\"","1269","                       .format(name)):","1270","        estimator.fit(X, y)","1271","","1272","","1273","@ignore_warnings","1913","","1914","    X = rng.normal(size=(10, 4))","1915","    X = pairwise_estimator_convert_X(X, regressor_orig)"],"delete":["1060","        if hasattr(X, 'T') and not _safe_tags(transformer, \"stateless\"):","1067","                transformer.transform(X.T)","1888","    X = rng.normal(size=(10, 4))"]}],"doc\/whats_new\/v0.22.rst":[{"add":["215","- |Enhancement| SVM now throws more specific error when fit on non-square data","216","  and kernel = precomputed.  :class:`svm.BaseLibSVM`","217","  :pr:`14336` by :user:`Gregory Dexter <gdex1>`.","218","  ","227"," ","228","- |Fix| KernelCenterer now throws error when fit on non-square ","229","  class:`preprocessing.KernelCenterer`","230","  :pr:`14336` by :user:`Gregory Dexter <gdex1>`.","255",":mod:`sklearn.neighbors`","256",".............................","257","","258","- |Fix| KNearestRegressor now throws error when fit on non-square data and","259","  metric = precomputed.  :class:`neighbors.NeighborsBase`","260","  :pr:`14336` by :user:`Gregory Dexter <gdex1>`.  ","261","","271","  ","294","- |Fix| Added check_transformer_data_not_an_array to checks where missing","295","  |Fix| Added check that pairwise estimators raise error on non-square data","296","  :pr:`14336` by :user:`Gregory Dexter <gdex1>`.","297",""],"delete":["215","","224","","258",""]}],"sklearn\/preprocessing\/data.py":[{"add":["1988","","1990","","1991","        if K.shape[0] != K.shape[1]:","1992","            raise ValueError(\"Kernel matrix must be a square matrix.\"","1993","                             \" Input is a {}x{} matrix.\"","1994","                             .format(K.shape[0], K.shape[1]))","1995",""],"delete":[]}]}},"496e7106fa8fff9e955620ec8b3b74d1bba59453":{"changes":{".github\/workflows\/labeler.yml":"MODIFY"},"diff":{".github\/workflows\/labeler.yml":[{"add":["8","    - uses: thomasjpfan\/labeler@v2.2.0"],"delete":["8","    - uses: thomasjpfan\/labeler@master"]}]}},"4e2e1fac0da592863b767356e30cee37f752b1bf":{"changes":{"sklearn\/preprocessing\/tests\/test_label.py":"MODIFY","doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/preprocessing\/label.py":"MODIFY"},"diff":{"sklearn\/preprocessing\/tests\/test_label.py":[{"add":["383","def test_multilabel_binarizer_multiple_calls():","384","    inp = [(2, 3), (1,), (1, 2)]","385","    indicator_mat = np.array([[0, 1, 1],","386","                              [1, 0, 0],","387","                              [1, 0, 1]])","388","","389","    indicator_mat2 = np.array([[0, 1, 1],","390","                               [1, 0, 0],","391","                               [1, 1, 0]])","392","","393","    # first call","394","    mlb = MultiLabelBinarizer(classes=[1, 3, 2])","395","    assert_array_equal(mlb.fit_transform(inp), indicator_mat)","396","    # second call change class","397","    mlb.classes = [1, 2, 3]","398","    assert_array_equal(mlb.fit_transform(inp), indicator_mat2)","399","","400",""],"delete":[]}],"doc\/whats_new\/v0.21.rst":[{"add":["49","  ","53","- |Efficiency| Make :class:`preprocessing.MultiLabelBinarizer` to cache class mappings instead ","54","  of calculating it every time on the fly.","55","  :issue:`12116` by :user:`Ekaterina Krivich <kiote>` and `Joel Nothman`_.","56",""],"delete":["49",""]}],"sklearn\/preprocessing\/label.py":[{"add":["826","        self._cached_dict = None","856","        self._cached_dict = None","857","","901","        class_to_index = self._build_cache()","909","    def _build_cache(self):","910","        if self._cached_dict is None:","911","            self._cached_dict = dict(zip(self.classes_,","912","                                         range(len(self.classes_))))","913","","914","        return self._cached_dict","915",""],"delete":["898","        class_to_index = dict(zip(self.classes_, range(len(self.classes_))))"]}]}},"819d8ef8b7532dac0f7cb00f5d3905eed9237b90":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/cluster\/dbscan_.py":"MODIFY","sklearn\/cluster\/tests\/test_dbscan.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["19","- :class:`cluster.DBSCAN` (bug fix)","50","- |Fix| Fixed a bug in :class:`cluster.DBSCAN` with precomputed sparse neighbors","51","  graph, which would add explicitly zeros on the diagonal even when already","52","  present. :issue:`12105` by `Tom Dupre la Tour`_.","53",""],"delete":["19","- please add class and reason here (see version 0.20 what's new)"]}],"doc\/whats_new\/v0.20.rst":[{"add":["49","- |Fix| Fixed a bug in :class:`cluster.DBSCAN` with precomputed sparse neighbors","50","  graph, which would add explicitly zeros on the diagonal even when already","51","  present. :issue:`12105` by `Tom Dupre la Tour`_.","52","","669","  average for multilabel data. :issue:`11679` by :user:`Alexander Pacha <apacha>`.","923","","1045","  ``missing_values='NaN'`` should now be"],"delete":["665","  average for multilabel data. :issue:`11679` by :user:`Alexander Pacha <apacha>`. ","919","  ","1041","  ``missing_values='NaN'``?should now be"]}],"sklearn\/cluster\/dbscan_.py":[{"add":["16","from ..utils.testing import ignore_warnings","145","","146","        # set the diagonal to explicit values, as a point is its own neighbor","147","        with ignore_warnings():","148","            X.setdiag(X.diagonal())  # XXX: modifies X's internals in-place","149","","152","        masked_indptr = np.concatenate(([0], np.cumsum(X_mask)))","153","        masked_indptr = masked_indptr[X.indptr[1:-1]]"],"delete":["146","        masked_indptr = np.concatenate(([0], np.cumsum(X_mask)))[X.indptr[1:]]","148","        # insert the diagonal: a point is its own neighbor, but 0 distance","149","        # means absence from sparse matrix data","150","        masked_indices = np.insert(masked_indices, masked_indptr,","151","                                   np.arange(X.shape[0]))","152","        masked_indptr = masked_indptr[:-1] + np.arange(1, X.shape[0])"]}],"sklearn\/cluster\/tests\/test_dbscan.py":[{"add":["83","@pytest.mark.parametrize('include_self', [False, True])","84","def test_dbscan_sparse_precomputed(include_self):","87","    X_ = X if include_self else None","88","    D_sparse = nn.radius_neighbors_graph(X=X_, mode='distance')","101","@pytest.mark.parametrize('use_sparse', [True, False])","102","@pytest.mark.parametrize('metric', ['precomputed', 'minkowski'])","103","def test_dbscan_input_not_modified(use_sparse, metric):","104","    # test that the input is not modified by dbscan","105","    X = np.random.RandomState(0).rand(10, 10)","106","    X = sparse.csr_matrix(X) if use_sparse else X","107","    X_copy = X.copy()","108","    dbscan(X, metric=metric)","109","","110","    if use_sparse:","111","        assert_array_equal(X.toarray(), X_copy.toarray())","112","    else:","113","        assert_array_equal(X, X_copy)","114","","115",""],"delete":["83","def test_dbscan_sparse_precomputed():","86","    D_sparse = nn.radius_neighbors_graph(mode='distance')"]}]}},"f1a3312cd78df5226c093561ffc086710af5c463":{"changes":{"sklearn\/model_selection\/_split.py":"MODIFY"},"diff":{"sklearn\/model_selection\/_split.py":[{"add":["78","        Yields","79","        ------","303","        Yields","304","        ------","649","        Yields","650","        ------","736","        Yields","737","        ------","1008","        Yields","1009","        ------","1184","        Yields","1185","        ------","1605","        Yields","1606","        ------","1765","        Yields","1766","        ------","1849","        Yields","1850","        ------"],"delete":["78","        Returns","79","        -------","303","        Returns","304","        -------","649","        Returns","650","        -------","736","        Returns","737","        -------","1008","        Returns","1009","        -------","1184","        Returns","1185","        -------","1605","        Returns","1606","        -------","1765","        Returns","1766","        -------","1849","        Returns","1850","        -------"]}]}},"36c3c2bf8de66d236ffc78de43145140580c0c2d":{"changes":{"sklearn\/datasets\/tests\/test_common.py":"MODIFY"},"diff":{"sklearn\/datasets\/tests\/test_common.py":[{"add":["96","            if name in markers_fetch:","97","                marks.append(markers_fetch[name])"],"delete":["96","            marks.append(markers_fetch.get(name, pytest.mark.basic))"]}]}},"06ac22d06f54353ea5d5bba244371474c7baf938":{"changes":{"sklearn\/utils\/metaestimators.py":"MODIFY","sklearn\/compose\/tests\/test_column_transformer.py":"MODIFY"},"diff":{"sklearn\/utils\/metaestimators.py":[{"add":["43","        items = getattr(self, attr)","44","        names = []","45","        if items:","46","            names, _ = zip(*items)"],"delete":["43","        names, _ = zip(*getattr(self, attr))"]}],"sklearn\/compose\/tests\/test_column_transformer.py":[{"add":["796","def test_column_transformer_no_estimators_set_params():","797","    ct = ColumnTransformer([]).set_params(n_jobs=2)","798","    assert ct.n_jobs == 2","799","","800",""],"delete":[]}]}},"572e43d149aaeb29a35f4dd706ca851d38970b2d":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/metrics\/ranking.py":"MODIFY","sklearn\/metrics\/tests\/test_ranking.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["441","- |Fix| Fixed a bug in :func:`metrics.label_ranking_average_precision_score` ","442","  where sample_weight wasn't taken into account for samples with degenerate","443","  labels.","444","  :issue:`13447` by :user:`Dan Ellis <dpwe>`.","445",""],"delete":[]}],"sklearn\/metrics\/ranking.py":[{"add":["729","            aux = 1.","730","        else:","731","            scores_i = y_score[i]","732","            rank = rankdata(scores_i, 'max')[relevant]","733","            L = rankdata(scores_i[relevant], 'max')","734","            aux = (L \/ rank).mean()"],"delete":["729","            out += 1.","730","            continue","732","        scores_i = y_score[i]","733","        rank = rankdata(scores_i, 'max')[relevant]","734","        L = rankdata(scores_i[relevant], 'max')","735","        aux = (L \/ rank).mean()"]}],"sklearn\/metrics\/tests\/test_ranking.py":[{"add":["954","def test_lrap_sample_weighting_zero_labels():","955","    # Degenerate sample labeling (e.g., zero labels for a sample) is a valid","956","    # special case for lrap (the sample is considered to achieve perfect","957","    # precision), but this case is not tested in test_common.","958","    # For these test samples, the APs are 0.5, 0.75, and 1.0 (default for zero","959","    # labels).","960","    y_true = np.array([[1, 0, 0, 0], [1, 0, 0, 1], [0, 0, 0, 0]],","961","                      dtype=np.bool)","962","    y_score = np.array([[0.3, 0.4, 0.2, 0.1], [0.1, 0.2, 0.3, 0.4],","963","                        [0.4, 0.3, 0.2, 0.1]])","964","    samplewise_lraps = np.array([0.5, 0.75, 1.0])","965","    sample_weight = np.array([1.0, 1.0, 0.0])","966","","967","    assert_almost_equal(","968","        label_ranking_average_precision_score(y_true, y_score,","969","                                              sample_weight=sample_weight),","970","        np.sum(sample_weight * samplewise_lraps) \/ np.sum(sample_weight))","971","","972",""],"delete":[]}]}},"81601fb4b8ca43b89e867ec038d9ba5a48d01a49":{"changes":{"examples\/compose\/plot_column_transformer.py":"MODIFY"},"diff":{"examples\/compose\/plot_column_transformer.py":[{"add":["91","    # Use ColumnTransformer to combine the features from subject and body"],"delete":["91","    # Use C toolumnTransformer to combine the features from subject and body"]}]}},"ae0dcddce8f8bc6b4a8af16353c0e6e7cb362bc1":{"changes":{"benchmarks\/bench_hist_gradient_boosting.py":"MODIFY"},"diff":{"benchmarks\/bench_hist_gradient_boosting.py":[{"add":["81","                    early_stopping=False,"],"delete":["81","                    n_iter_no_change=None,"]}]}},"01bc8b18acf4123f3213786dccfef95d8ad1d24c":{"changes":{"sklearn\/utils\/_random.pyx":"MODIFY","build_tools\/circle\/build_doc.sh":"MODIFY","sklearn\/linear_model\/tests\/test_logistic.py":"MODIFY","sklearn\/tree\/_utils.pyx":"MODIFY","sklearn\/tests\/test_multioutput.py":"MODIFY","sklearn\/tree\/_utils.pxd":"MODIFY","sklearn\/utils\/_random.pxd":"MODIFY","sklearn\/utils\/seq_dataset.pyx.tp":"MODIFY","sklearn\/utils\/tests\/test_seq_dataset.py":"MODIFY","sklearn\/linear_model\/stochastic_gradient.py":"MODIFY","doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/linear_model\/perceptron.py":"MODIFY","sklearn\/kernel_approximation.py":"MODIFY","sklearn\/utils\/tests\/test_random.py":"MODIFY","doc\/tutorial\/text_analytics\/working_with_text_data.rst":"MODIFY","sklearn\/linear_model\/tests\/test_sgd.py":"MODIFY","sklearn\/linear_model\/cd_fast.pyx":"MODIFY","sklearn\/linear_model\/tests\/test_passive_aggressive.py":"MODIFY","sklearn\/linear_model\/passive_aggressive.py":"MODIFY"},"diff":{"sklearn\/utils\/_random.pyx":[{"add":["15","    * Fast rand_r alternative based on xor shifts","25","cdef UINT32_t DEFAULT_SEED = 1","26","","314","","315","","316","def _our_rand_r_py(seed):","317","    \"\"\"Python utils to test the our_rand_r function\"\"\"","318","    cdef UINT32_t my_seed = seed","319","    return our_rand_r(&my_seed)"],"delete":["15",""]}],"build_tools\/circle\/build_doc.sh":[{"add":["126","pip install \"sphinx-gallery>=0.2,<0.3\""],"delete":["126","pip install sphinx-gallery"]}],"sklearn\/linear_model\/tests\/test_logistic.py":[{"add":["1673","                               random_state=1)","1677","        penalty='elasticnet', random_state=1, fit_intercept=False, tol=-np.inf,","1680","        penalty='elasticnet', random_state=1, fit_intercept=False, tol=1e-5,"],"delete":["1673","                               random_state=0)","1677","        penalty='elasticnet', random_state=0, fit_intercept=False, tol=-np.inf,","1680","        penalty='elasticnet', random_state=0, fit_intercept=False, tol=1e-5,"]}],"sklearn\/tree\/_utils.pyx":[{"add":["23","from sklearn.utils cimport _random","24","","67","    return low + _random.our_rand_r(random_state) % (high - low)","73","    return ((high - low) * <double> _random.our_rand_r(random_state) \/"],"delete":["55","# rand_r replacement using a 32bit XorShift generator","56","# See https:\/\/www.jstatsoft.org\/v08\/i14\/paper for details","57","cdef inline UINT32_t our_rand_r(UINT32_t* seed) nogil:","58","    seed[0] ^= <UINT32_t>(seed[0] << 13)","59","    seed[0] ^= <UINT32_t>(seed[0] >> 17)","60","    seed[0] ^= <UINT32_t>(seed[0] << 5)","61","","62","    return seed[0] % (<UINT32_t>RAND_R_MAX + 1)","63","","64","","75","    return low + our_rand_r(random_state) % (high - low)","81","    return ((high - low) * <double> our_rand_r(random_state) \/"]}],"sklearn\/tests\/test_multioutput.py":[{"add":["334","    sgd_linear_clf = SGDClassifier(random_state=1, max_iter=20)","341","    sgd_linear_clf = SGDClassifier(random_state=1, max_iter=20)"],"delete":["334","    sgd_linear_clf = SGDClassifier(random_state=1, max_iter=5)","341","    sgd_linear_clf = SGDClassifier(random_state=1, max_iter=5)"]}],"sklearn\/tree\/_utils.pxd":[{"add":["25",""],"delete":[]}],"sklearn\/utils\/_random.pxd":[{"add":["9","ctypedef np.npy_uint32 UINT32_t","11","cdef inline UINT32_t DEFAULT_SEED = 1","12","","13","cdef enum:","14","    # Max value for our rand_r replacement (near the bottom).","15","    # We don't use RAND_MAX because it's different across platforms and","16","    # particularly tiny on Windows\/MSVC.","17","    RAND_R_MAX = 0x7FFFFFFF","24","# rand_r replacement using a 32bit XorShift generator","25","# See http:\/\/www.jstatsoft.org\/v08\/i14\/paper for details","26","cdef inline UINT32_t our_rand_r(UINT32_t* seed) nogil:","27","    \"\"\"Generate a pseudo-random np.uint32 from a np.uint32 seed\"\"\"","28","    # seed shouldn't ever be 0.","29","    if (seed[0] == 0): seed[0] = DEFAULT_SEED","30","","31","    seed[0] ^= <UINT32_t>(seed[0] << 13)","32","    seed[0] ^= <UINT32_t>(seed[0] >> 17)","33","    seed[0] ^= <UINT32_t>(seed[0] << 5)","34","","35","    # Note: we must be careful with the final line cast to np.uint32 so that","36","    # the function behaves consistently across platforms.","37","    #","38","    # The following cast might yield different results on different platforms:","39","    # wrong_cast = <UINT32_t> RAND_R_MAX + 1","40","    #","41","    # We can use:","42","    # good_cast = <UINT32_t>(RAND_R_MAX + 1)","43","    # or:","44","    # cdef np.uint32_t another_good_cast = <UINT32_t>RAND_R_MAX + 1","45","    return seed[0] % <UINT32_t>(RAND_R_MAX + 1)"],"delete":[]}],"sklearn\/utils\/seq_dataset.pyx.tp":[{"add":["48","from sklearn.utils cimport _random","158","            j = i + _random.our_rand_r(&seed) % (n - i)","172","        cdef int current_index = _random.our_rand_r(&self.seed) % n"],"delete":["157","            j = i + our_rand_r(&seed) % (n - i)","171","        cdef int current_index = our_rand_r(&self.seed) % n","359","","360","cdef enum:","361","    RAND_R_MAX = 0x7FFFFFFF","362","","363","","364","# rand_r replacement using a 32bit XorShift generator","365","# See https:\/\/www.jstatsoft.org\/v08\/i14\/paper for details","366","# XXX copied over from sklearn\/tree\/_tree.pyx, should refactor","367","cdef inline np.uint32_t our_rand_r(np.uint32_t* seed) nogil:","368","    seed[0] ^= <np.uint32_t>(seed[0] << 13)","369","    seed[0] ^= <np.uint32_t>(seed[0] >> 17)","370","    seed[0] ^= <np.uint32_t>(seed[0] << 5)","371","","372","    return seed[0] % (<np.uint32_t>RAND_R_MAX + 1)"]}],"sklearn\/utils\/tests\/test_seq_dataset.py":[{"add":["6","import pytest","8","from numpy.testing import assert_array_equal","9","from sklearn.utils.seq_dataset import (","10","    ArrayDataset32, ArrayDataset64, CSRDataset32, CSRDataset64)","13","from sklearn.utils.testing import assert_allclose","96","    for i in [132, 50, 9, 18, 58]:","99","        assert idx1 == i","100","        assert idx2 == i","106","    idx_next = [63, 91, 148, 87, 29]","107","    idx_shuffle = [137, 125, 56, 121, 127]","108","    for i, j in zip(idx_next, idx_shuffle):","111","        assert idx1 == i","112","        assert idx2 == i","116","        assert idx1 == j","117","        assert idx2 == j"],"delete":["5","import pytest","7","from numpy.testing import assert_array_equal","8","from sklearn.utils.testing import assert_allclose","10","","11","from sklearn.utils.seq_dataset import ArrayDataset64","12","from sklearn.utils.seq_dataset import ArrayDataset32","13","from sklearn.utils.seq_dataset import CSRDataset64","14","from sklearn.utils.seq_dataset import CSRDataset32","99","    for i in range(5):","102","        assert idx1 == idx2","108","    for i in range(5):","111","        assert idx1 == idx2","115","        assert idx1 == idx2"]}],"sklearn\/linear_model\/stochastic_gradient.py":[{"add":["44","MAX_INT = np.iinfo(np.int32).max","45","","326","               pos_weight, neg_weight, sample_weight, validation_mask=None,","327","               random_state=None):","371","","372","    random_state : int, RandomState instance or None, optional (default=None)","373","        If int, random_state is the seed used by the random number generator;","374","        If RandomState instance, random_state is the random number generator;","375","        If None, the random number generator is the RandomState instance used","376","        by `np.random`.","383","","384","    random_state = check_random_state(random_state)","385","    dataset, intercept_decay = make_dataset(","386","        X, y_i, sample_weight, random_state=random_state)","399","    seed = random_state.randint(MAX_INT)","570","                                              sample_weight,","571","                                              random_state=self.random_state)","602","        # Pick the random seed for each job outside of fit_binary to avoid","603","        # sharing the estimator random state between threads which could lead","604","        # to non-deterministic behavior","605","        random_state = check_random_state(self.random_state)","606","        seeds = random_state.randint(MAX_INT, size=len(self.classes_))","612","                                validation_mask=validation_mask,","613","                                random_state=seed)","614","            for i, seed in enumerate(seeds))"],"delete":["324","               pos_weight, neg_weight, sample_weight, validation_mask=None):","374","    dataset, intercept_decay = make_dataset(X, y_i, sample_weight)","385","    # XXX should have random_state_!","386","    random_state = check_random_state(est.random_state)","389","    seed = random_state.randint(0, np.iinfo(np.int32).max)","560","                                              sample_weight)","596","                                validation_mask=validation_mask)","597","            for i in range(len(self.classes_)))"]}],"doc\/whats_new\/v0.21.rst":[{"add":["32","- :class:`linear_model.SGDClassifier` and any derived classifiers. |Fix|","33","- Any model using the :func:`linear_model.sag.sag_solver` function with a `0`","34","  seed, including :class:`linear_model.LogisticRegression`,","35","  :class:`linear_model.LogisticRegressionCV`, :class:`linear_model.Ridge`,","36","  and :class:`linear_model.RidgeCV` with 'sag' solver. |Fix|","311","- |Fix| Fixed a bug in","312","  :class:`linear_model.stochastic_gradient.BaseSGDClassifier` that was not","313","  deterministic when trained in a multi-class setting on several threads.","314","  :issue:`13422` by :user:`Cl¨¦ment Doumouro <ClemDoum>`.","315","","562","- |Efficiency| Memory copies are avoided when casting arrays to a different","563","  dtype in multiple estimators. :issue:`11973` by :user:`Roman Yurchak","564","  <rth>`.","565","- |Fix| Fixed a bug in the implementation of the :func:`our_rand_r`","566","  helper function that was not behaving consistently across platforms.","567","  :issue:`13422` by :user:`Madhura Parikh <jdnc>` and","568","  :user:`Cl¨¦ment Doumouro <ClemDoum>`."],"delete":["546","- |Efficiency| Memory copies are avoided when casting arrays to a different","547","  dtype in multiple estimators. :issue:`11973` by :user:`Roman Yurchak","548","  <rth>`."]}],"sklearn\/linear_model\/perceptron.py":[{"add":["130","    0.939..."],"delete":["130","    0.946..."]}],"sklearn\/kernel_approximation.py":[{"add":["293","    0.9499..."],"delete":["293","    0.9543..."]}],"sklearn\/utils\/tests\/test_random.py":[{"add":["4","from sklearn.utils.fixes import comb","5","from sklearn.utils.random import random_choice_csc, sample_without_replacement","6","from sklearn.utils._random import _our_rand_r_py","7","from sklearn.utils.testing import (assert_equal, assert_raises)","177","","178","","179","def test_our_rand_r():","180","    assert 131541053 == _our_rand_r_py(1273642419)","181","    assert 270369 == _our_rand_r_py(0)"],"delete":["0","","4","from sklearn.utils.random import sample_without_replacement","5","from sklearn.utils.random import random_choice_csc","6","from sklearn.utils.fixes import comb","8","from sklearn.utils.testing import (","9","    assert_raises,","10","    assert_equal)"]}],"doc\/tutorial\/text_analytics\/working_with_text_data.rst":[{"add":["369","  0.9101...","380","             alt.atheism       0.95      0.80      0.87       319","381","           comp.graphics       0.87      0.98      0.92       389","382","                 sci.med       0.94      0.89      0.91       396","386","               macro avg       0.91      0.91      0.91      1502","387","            weighted avg       0.91      0.91      0.91      1502","391","  array([[256,  11,  16,  36],","392","         [  4, 380,   3,   2],","393","         [  5,  35, 353,   3],","394","         [  5,  11,   4, 378]])","472","  vect__ngram_range: (1, 1)"],"delete":["369","  0.9127...","380","             alt.atheism       0.95      0.81      0.87       319","381","           comp.graphics       0.88      0.97      0.92       389","382","                 sci.med       0.94      0.90      0.92       396","386","               macro avg       0.92      0.91      0.91      1502","387","            weighted avg       0.92      0.91      0.91      1502","391","  array([[258,  11,  15,  35],","392","         [  4, 379,   3,   3],","393","         [  5,  33, 355,   3],","394","         [  5,  10,   4, 379]])","395","","473","  vect__ngram_range: (1, 2)"]}],"sklearn\/linear_model\/tests\/test_sgd.py":[{"add":["1034","    random_state = np.random.RandomState(1)","1036","                eta0=0.1, loss=\"epsilon_insensitive\",","1037","                random_state=random_state)","1042","                eta0=0.1, loss=\"squared_epsilon_insensitive\",","1043","                random_state=random_state)","1047","    clf = klass(alpha=0.01, loss=\"huber\", random_state=random_state)","1052","                loss=\"squared_loss\", random_state=random_state)"],"delete":["1035","                eta0=0.1, loss=\"epsilon_insensitive\")","1040","                eta0=0.1, loss=\"squared_epsilon_insensitive\")","1044","    clf = klass(alpha=0.01, loss=\"huber\")","1049","                loss=\"squared_loss\")"]}],"sklearn\/linear_model\/cd_fast.pyx":[{"add":["26","from sklearn.utils cimport _random ","27","","44","    return _random.our_rand_r(random_state) % end"],"delete":["40","cdef inline UINT32_t our_rand_r(UINT32_t* seed) nogil:","41","    seed[0] ^= <UINT32_t>(seed[0] << 13)","42","    seed[0] ^= <UINT32_t>(seed[0] >> 17)","43","    seed[0] ^= <UINT32_t>(seed[0] << 5)","44","","45","    return seed[0] % (<UINT32_t>RAND_R_MAX + 1)","46","","47","","50","    return our_rand_r(random_state) % end"]}],"sklearn\/linear_model\/tests\/test_passive_aggressive.py":[{"add":["78","                    random_state=1, average=average, tol=None)"],"delete":["78","                    random_state=0, average=average, tol=None)"]}],"sklearn\/linear_model\/passive_aggressive.py":[{"add":["141","    [[0.26642044 0.45070924 0.67251877 0.64185414]]","143","    [1.84127814]"],"delete":["141","    [[-0.6543424   1.54603022  1.35361642  0.22199435]]","143","    [0.63310933]"]}]}},"2a5c845a60b3fb28704c3f04e8128613459f6222":{"changes":{"sklearn\/feature_selection\/tests\/test_mutual_info.py":"MODIFY","sklearn\/feature_selection\/mutual_info_.py":"MODIFY"},"diff":{"sklearn\/feature_selection\/tests\/test_mutual_info.py":[{"add":["185","        assert_raises(ValueError, mutual_info, X_csr, y,","187","        assert_raises(ValueError, mutual_info, X, y,","188","                      discrete_features='manual')","189","        assert_raises(ValueError, mutual_info, X_csr, y,","190","                      discrete_features=[True, False, True])","191","        assert_raises(IndexError, mutual_info, X, y,","192","                      discrete_features=[True, False, True, False])","193","        assert_raises(IndexError, mutual_info, X, y, discrete_features=[1, 4])","197","        mi_3 = mutual_info(X_csr, y, discrete_features='auto', random_state=0)","198","        mi_4 = mutual_info(X_csr, y, discrete_features=True, random_state=0)","199","        mi_5 = mutual_info(X, y, discrete_features=[True, False, True],","201","        mi_6 = mutual_info(X, y, discrete_features=[0, 2], random_state=0)","205","        assert_array_equal(mi_5, mi_6)"],"delete":["185","        assert_raises(ValueError, mutual_info_regression, X_csr, y,","190","","191","        mi_3 = mutual_info(X_csr, y, discrete_features='auto',","193","        mi_4 = mutual_info(X_csr, y, discrete_features=True,","194","                           random_state=0)"]}],"sklearn\/feature_selection\/mutual_info_.py":[{"add":["12","from ..utils.validation import check_array, check_X_y","249","    if isinstance(discrete_features, (str, bool)):","250","        if isinstance(discrete_features, str):","251","            if discrete_features == 'auto':","252","                discrete_features = issparse(X)","253","            else:","254","                raise ValueError(\"Invalid string value for discrete_features.\")","258","        discrete_features = check_array(discrete_features, ensure_2d=False)"],"delete":["12","from ..utils.validation import check_X_y","249","    if discrete_features == 'auto':","250","        discrete_features = issparse(X)","251","","252","    if isinstance(discrete_features, bool):","256","        discrete_features = np.asarray(discrete_features)"]}]}},"0b818d3bcf77c94bef0e39802f41b8b486c9c304":{"changes":{"doc\/faq.rst":"MODIFY"},"diff":{"doc\/faq.rst":[{"add":["10","scikit-learn, but not scikit or SciKit nor sci-kit learn.","11","Also not scikits.learn or scikits-learn, which were previously used.","253","","263","","278","","279","How do I set a ``random_state`` for an entire execution?","280","----------------------------------------------------","281","","282","For testing and replicability, it is often important to have the entire execution","283","controlled by a single seed for the pseudo-random number generator used in","284","algorithms that have a randomized component. Scikit-learn does not use its own","285","global random state; whenever a RandomState instance or an integer random seed","286","is not provided as an argument, it relies on the numpy global random state,","287","which can be set using :func:`numpy.random.seed`.","288","For example, to set an execution's numpy global random state to 42, one could","289","execute the following in his or her script::","290","","291","    import numpy as np","292","    np.random.seed(42)","293","","294","However, a global random state is prone to modification by other code during","295","execution. Thus, the only way to ensure replicability is to pass ``RandomState``","296","instances everywhere and ensure that both estimators and cross-validation","297","splitters have their ``random_state`` parameter set."],"delete":["10","scikit-learn, but not scikit or SciKit nor sci-kit learn. Also not scikits.learn or scikits-learn, which where previously used.","203","","251","","261",""]}]}},"ab399a675ab54b27ed16d79f0bc7ce6f5240346e":{"changes":{"doc\/developers\/contributing.rst":"MODIFY"},"diff":{"doc\/developers\/contributing.rst":[{"add":["215",".. note::","216","","217","  In the above setup, your ``origin`` remote repository points to","218","  ``YourLogin\/scikit-learn.git``. If you wish to fetch\/merge from the main","219","  repository instead of your forked one, you will need to add another remote","220","  to use instead of ``origin``. If we choose the name ``upstream`` for it, the","221","  command will be::","222","","223","        $ git remote add upstream https:\/\/github.com\/scikit-learn\/scikit-learn.git","224","","225","  And in order to fetch the new remote and base your work on the latest changes","226","  of it you can::","227","","228","        $ git fetch upstream","229","        $ git checkout -b my-feature upstream\/master","230","","260","to merge ``master``. For that, you first need to fetch the ``upstream``, and","261","then merge its ``master`` into your branch::","263","  $ git fetch upstream","264","  $ git merge upstream\/master"],"delete":["237",".. note::","238","","239","  In the above setup, your ``origin`` remote repository points to","240","  YourLogin\/scikit-learn.git. If you wish to fetch\/merge from the main","241","  repository instead of your forked one, you will need to add another remote","242","  to use instead of ``origin``. If we choose the name ``upstream`` for it, the","243","  command will be::","244","","245","        $ git remote add upstream https:\/\/github.com\/scikit-learn\/scikit-learn.git","253","to merge ``master``. The command will be::","255","  $ git merge master","256","","257","with ``master`` being synchronized with the ``upstream``."]}]}},"08b82f8f46cca860d7d4afc47d803765b9a0f48c":{"changes":{"sklearn\/ensemble\/tests\/test_forest.py":"MODIFY"},"diff":{"sklearn\/ensemble\/tests\/test_forest.py":[{"add":["127","        assert score > 0.95, (\"Failed with max_features=None, \"","128","                              \"criterion %s and score = %f\" % (c, score))","134","        assert score > 0.95, (\"Failed with max_features=6, \"","135","                              \"criterion %s and score = %f\" % (c, score))","141","        assert score > 0.95, (\"Failed with max_features=None, \"","142","                              \"criterion %s and score = %f\" % (c, score))","148","        assert score > 0.95, (\"Failed with max_features=6, \"","149","                              \"criterion %s and score = %f\" % (c, score))"],"delete":["127","        assert score < 3, (\"Failed with max_features=None, \"","128","                           \"criterion %s and score = %f\" % (c, score))","134","        assert score < 3, (\"Failed with max_features=None, \"","135","                           \"criterion %s and score = %f\" % (c, score))","141","        assert score < 3, (\"Failed with max_features=None, \"","142","                           \"criterion %s and score = %f\" % (c, score))","148","        assert score < 3, (\"Failed with max_features=None, \"","149","                           \"criterion %s and score = %f\" % (c, score))"]}]}},"9b42b0cc7d5cf6978805619bc2433e3888c38d0c":{"changes":{"sklearn\/utils\/estimator_checks.py":"MODIFY","doc\/whats_new\/v0.24.rst":"MODIFY","sklearn\/utils\/tests\/test_estimator_checks.py":"MODIFY"},"diff":{"sklearn\/utils\/estimator_checks.py":[{"add":["703","    y = (4 * rng.rand(40)).astype(int)","708","    tags = estimator_orig._get_tags()","806","        y = np.arange(n_samples) % 3","897","    y = (X[:, 0] * 4).astype(int)","1360","    y = np.arange(n_samples) % 3","1563","    y = _enforce_estimator_tags_y(estimator_orig, y)","2030","    X, y = make_blobs(random_state=0, n_samples=21)","2072","    y = np.arange(n_samples) % 3","2390","    X, y = make_blobs(random_state=0, n_samples=21)","2461","    y = np.array([1, 1, 1, 2, 2, 2, 3, 3, 3])","2462","    y = _enforce_estimator_tags_y(estimator_orig, y)","2486","    y = np.array([1, 1, 1, 2, 2, 2, 1, 1, 1, 2, 2, 2])","2622","    # Estimators with a `binary_only` tag only accept up to two unique y values","2623","    if estimator._get_tags()[\"binary_only\"] and y.size > 0:","2624","        y = np.where(y == y.flat[0], y, y.flat[0] + 1)"],"delete":["703","    tags = estimator_orig._get_tags()","704","    if tags['binary_only']:","705","        y = (2 * rng.rand(40)).astype(int)","706","    else:","707","        y = (4 * rng.rand(40)).astype(int)","809","        if estimator._get_tags()['binary_only']:","810","            y = np.arange(n_samples) % 2","811","        else:","812","            y = np.arange(n_samples) % 3","903","    if tags['binary_only']:","904","        y = (X[:, 0] * 2).astype(int)","905","    else:","906","        y = (X[:, 0] * 4).astype(int)","1000","    if estimator._get_tags()['binary_only']:","1001","        y[y == 2] = 1","1052","    if tags['binary_only']:","1053","        y[y == 2] = 1","1102","    if estimator_orig._get_tags()['binary_only']:","1103","        y[y == 2] = 1","1375","    if estimator_orig._get_tags()['binary_only']:","1376","        y = np.arange(n_samples) % 2","1377","    else:","1378","        y = np.arange(n_samples) % 3","1408","    if estimator_orig._get_tags()['binary_only']:","1409","        y[y == 2] = 1","2049","    if estimator_orig._get_tags()['binary_only']:","2050","        n_centers = 2","2051","    else:","2052","        n_centers = 3","2053","    X, y = make_blobs(random_state=0, n_samples=21, centers=n_centers)","2095","    if tags['binary_only']:","2096","        y = np.arange(n_samples) % 2","2097","    else:","2098","        y = np.arange(n_samples) % 3","2416","    if estimator_orig._get_tags()['binary_only']:","2417","        n_centers = 2","2418","    else:","2419","        n_centers = 3","2420","    X, y = make_blobs(random_state=0, n_samples=21, centers=n_centers)","2491","    y = [1, 1, 1, 2, 2, 2, 3, 3, 3]","2515","    y = [1, 1, 1, 2, 2, 2, 1, 1, 1, 2, 2, 2]"]}],"doc\/whats_new\/v0.24.rst":[{"add":["296",":mod:`sklearn.utils`","297",".........................","298","","299","- |Fix| Fix :func:`utils.estimator_checks.check_estimator` so that all test","300","  cases support the `binary_only` estimator tag.","301","  :pr:`17812` by :user:`Bruno Charron <brcharron>`.","302",""],"delete":[]}],"sklearn\/utils\/tests\/test_estimator_checks.py":[{"add":["308","class UntaggedBinaryClassifier(SGDClassifier):","310","    def fit(self, X, y, coef_init=None, intercept_init=None,","311","            sample_weight=None):","312","        super().fit(X, y, coef_init, intercept_init, sample_weight)","313","        if len(self.classes_) > 2:","314","            raise ValueError('Only 2 classes are supported')","315","        return self","316","","317","    def partial_fit(self, X, y, classes=None, sample_weight=None):","318","        super().partial_fit(X=X, y=y, classes=classes,","319","                            sample_weight=sample_weight)","320","        if len(self.classes_) > 2:"],"delete":["36","from sklearn.tree import DecisionTreeClassifier","309","class UntaggedBinaryClassifier(DecisionTreeClassifier):","311","    def fit(self, X, y, sample_weight=None):","312","        super().fit(X, y, sample_weight)","313","        if np.all(self.n_classes_ > 2):"]}]}},"8d3b4ff3eec890396a3d7a806bbe944f55a89cb4":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/ensemble\/tests\/test_voting.py":"MODIFY","sklearn\/ensemble\/voting.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["321","- |Fix| :class:`ensemble.VotingClassifier` and","322","  :class:`ensemble.VotingRegressor` were failing during ``fit`` in one","323","  of the estimators was set to ``None`` and ``sample_weight`` was not ``None``.","324","  :pr:`13779` by :user:`Guillaume Lemaitre <glemaitre>`.","325",""],"delete":[]}],"sklearn\/ensemble\/tests\/test_voting.py":[{"add":["10","from sklearn.linear_model import LinearRegression","14","from sklearn.ensemble import RandomForestRegressor","511","","512","","513","@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22","514","@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22","515","@pytest.mark.parametrize(","516","    \"X, y, voter\",","517","    [(X, y, VotingClassifier(","518","        [('lr', LogisticRegression()),","519","         ('rf', RandomForestClassifier(n_estimators=5))])),","520","     (X_r, y_r, VotingRegressor(","521","         [('lr', LinearRegression()),","522","          ('rf', RandomForestRegressor(n_estimators=5))]))]","523",")","524","def test_none_estimator_with_weights(X, y, voter):","525","    # check that an estimator can be set to None and passing some weight","526","    # regression test for","527","    # https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/13777","528","    voter.fit(X, y, sample_weight=np.ones(y.shape))","529","    voter.set_params(lr=None)","530","    voter.fit(X, y, sample_weight=np.ones(y.shape))","531","    y_pred = voter.predict(X)","532","    assert y_pred.shape == y.shape"],"delete":[]}],"sklearn\/ensemble\/voting.py":[{"add":["80","                if step is None:","81","                    continue"],"delete":[]}]}},"db4e2440c55e2d43083b061f742159c7158c6c96":{"changes":{"setup.py":"MODIFY"},"diff":{"setup.py":[{"add":["12","import traceback","156","        traceback.print_exc()","176","        traceback.print_exc()"],"delete":["5","import subprocess","6",""]}]}},"c303ed8ef27c278633d1fa4d869b51dfa2418ca6":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/mixture\/tests\/test_gaussian_mixture.py":"MODIFY","sklearn\/mixture\/base.py":"MODIFY","sklearn\/mixture\/tests\/test_bayesian_mixture.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["707","- Added function :func:`fit_predict` to :class:`mixture.GaussianMixture` and","708","  :class:`mixture.GaussianMixture`, which is essentially equivalent to calling","709","  :func:`fit` and :func:`predict`. :issue:`10336` by","710","  :user:`Shu Haoran <haoranShu>` and :user:`Andrew Peng <Andrew-peng>`.","711",""],"delete":[]}],"sklearn\/mixture\/tests\/test_gaussian_mixture.py":[{"add":["5","import copy","572","def test_gaussian_mixture_fit_predict():","573","    rng = np.random.RandomState(0)","574","    rand_data = RandomData(rng)","575","    for covar_type in COVARIANCE_TYPE:","576","        X = rand_data.X[covar_type]","577","        Y = rand_data.Y","578","        g = GaussianMixture(n_components=rand_data.n_components,","579","                            random_state=rng, weights_init=rand_data.weights,","580","                            means_init=rand_data.means,","581","                            precisions_init=rand_data.precisions[covar_type],","582","                            covariance_type=covar_type)","583","","584","        # check if fit_predict(X) is equivalent to fit(X).predict(X)","585","        f = copy.deepcopy(g)","586","        Y_pred1 = f.fit(X).predict(X)","587","        Y_pred2 = g.fit_predict(X)","588","        assert_array_equal(Y_pred1, Y_pred2)","589","        assert_greater(adjusted_rand_score(Y, Y_pred2), .95)","590","","591",""],"delete":[]}],"sklearn\/mixture\/base.py":[{"add":["174","        The method fits the model `n_init` times and set the parameters with","190","        self.fit_predict(X, y)","191","        return self","192","","193","    def fit_predict(self, X, y=None):","194","        \"\"\"Estimate model parameters using X and predict the labels for X.","195","","196","        The method fits the model n_init times and sets the parameters with","197","        which the model has the largest likelihood or lower bound. Within each","198","        trial, the method iterates between E-step and M-step for `max_iter`","199","        times until the change of likelihood or lower bound is less than","200","        `tol`, otherwise, a `ConvergenceWarning` is raised. After fitting, it","201","        predicts the most probable label for the input data points.","202","","203","        .. versionadded:: 0.20","204","","205","        Parameters","206","        ----------","207","        X : array-like, shape (n_samples, n_features)","208","            List of n_features-dimensional data points. Each row","209","            corresponds to a single data point.","210","","211","        Returns","212","        -------","213","        labels : array, shape (n_samples,)","214","            Component labels.","215","        \"\"\"","268","        return log_resp.argmax(axis=1)"],"delete":["174","        The method fit the model `n_init` times and set the parameters with","242","        return self"]}],"sklearn\/mixture\/tests\/test_bayesian_mixture.py":[{"add":["3","import copy","10","from sklearn.utils.testing import assert_array_equal","11","","12","from sklearn.metrics.cluster import adjusted_rand_score","20","from sklearn.exceptions import ConvergenceWarning, NotFittedError","425","","426","","427","def test_bayesian_mixture_fit_predict():","428","    rng = np.random.RandomState(0)","429","    rand_data = RandomData(rng, scale=7)","430","    n_components = 2 * rand_data.n_components","431","","432","    for covar_type in COVARIANCE_TYPE:","433","        bgmm1 = BayesianGaussianMixture(n_components=n_components,","434","                                        max_iter=100, random_state=rng,","435","                                        tol=1e-3, reg_covar=0)","436","        bgmm1.covariance_type = covar_type","437","        bgmm2 = copy.deepcopy(bgmm1)","438","        X = rand_data.X[covar_type]","439","","440","        Y_pred1 = bgmm1.fit(X).predict(X)","441","        Y_pred2 = bgmm2.fit_predict(X)","442","        assert_array_equal(Y_pred1, Y_pred2)","443","","444","","445","def test_bayesian_mixture_predict_predict_proba():","446","    # this is the same test as test_gaussian_mixture_predict_predict_proba()","447","    rng = np.random.RandomState(0)","448","    rand_data = RandomData(rng)","449","    for prior_type in PRIOR_TYPE:","450","        for covar_type in COVARIANCE_TYPE:","451","            X = rand_data.X[covar_type]","452","            Y = rand_data.Y","453","            bgmm = BayesianGaussianMixture(","454","                n_components=rand_data.n_components,","455","                random_state=rng,","456","                weight_concentration_prior_type=prior_type,","457","                covariance_type=covar_type)","458","","459","            # Check a warning message arrive if we don't do fit","460","            assert_raise_message(NotFittedError,","461","                                 \"This BayesianGaussianMixture instance\"","462","                                 \" is not fitted yet. Call 'fit' with \"","463","                                 \"appropriate arguments before using \"","464","                                 \"this method.\", bgmm.predict, X)","465","","466","            bgmm.fit(X)","467","            Y_pred = bgmm.predict(X)","468","            Y_pred_proba = bgmm.predict_proba(X).argmax(axis=1)","469","            assert_array_equal(Y_pred, Y_pred_proba)","470","            assert_greater_equal(adjusted_rand_score(Y, Y_pred), .95)"],"delete":["16","from sklearn.exceptions import ConvergenceWarning"]}]}},"da0cb32270ce18963799906a8a0a75216749e21c":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/utils\/class_weight.py":"MODIFY","sklearn\/utils\/tests\/test_class_weight.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["19","- |Fix| Fixed a bug mostly affecting :class:`ensemble.RandomForestClassifier`","20","  where ``class_weight='balanced_subsample'`` failed with more than 32 classes.","21","  :issue:`12165` by `Joel Nothman`_.","22",""],"delete":[]}],"sklearn\/utils\/class_weight.py":[{"add":["152","            weight_k = np.take(compute_class_weight(class_weight_k,","153","                                                    classes_subsample,","154","                                                    y_subsample),","155","                               np.searchsorted(classes_subsample,","156","                                               classes_full),","157","                               mode='clip')"],"delete":["152","            weight_k = np.choose(np.searchsorted(classes_subsample,","153","                                                 classes_full),","154","                                 compute_class_weight(class_weight_k,","155","                                                      classes_subsample,","156","                                                      y_subsample),","157","                                 mode='clip')"]}],"sklearn\/utils\/tests\/test_class_weight.py":[{"add":["253","","254","","255","def test_compute_sample_weight_more_than_32():","256","    # Non-regression smoke test for #12146","257","    y = np.arange(50)  # more than 32 distinct classes","258","    indices = np.arange(50)  # use subsampling","259","    weight = compute_sample_weight('balanced', y, indices=indices)","260","    assert_array_almost_equal(weight, np.ones(y.shape[0]))"],"delete":[]}]}},"8442eead9ed3fef8ce03db7f2cccec3078dd685f":{"changes":{"doc\/modules\/multiclass.rst":"MODIFY"},"diff":{"doc\/modules\/multiclass.rst":[{"add":["38","    several joint classification tasks. This is both a generalization","40","    problems is restricted to binary classification, ","41","    as well as a generalization of the multi-class classification task. ","42","    *The output format is a 2d numpy array or sparse matrix.*","45","    For instance, a sample could be assigned \"pear\" for an output variable that","46","    takes possible values in a finite set of species such as \"pear\", \"apple\"; ","47","    and \"blue\" or \"green\" for a second output variable that takes possible values","48","    in a finite set of colors such as \"green\", \"red\", \"blue\", \"yellow\"...","51","    multiclass or multi-task classification tasks,","52","    support the multi-label classification task as a special case.","65","if you're using one of these, unless you want custom multiclass behavior:","121","interpretability. Since each class is represented by one and only one classifier, ","122","it is possible to gain knowledge about the class by inspecting its","214","one-vs-one. With these strategies, each class is represented in a Euclidean"],"delete":["38","    several joint classification tasks. This is a generalization","40","    problem is restricted to binary classification, and of the multi-class","41","    classification task. *The output format is a 2d numpy array or sparse","42","    matrix.*","45","    For instance a sample could be assigned \"pear\" for an output variable that","46","    takes possible values in a finite set of species such as \"pear\", \"apple\",","47","    \"orange\" and \"green\" for a second output variable that takes possible values","48","    in a finite set of colors such as \"green\", \"red\", \"orange\", \"yellow\"...","51","    multiclass or multi-task classification task","52","    supports the multi-label classification task as a special case.","65","if you're using one of these unless you want custom multiclass behavior:","121","interpretability. Since each class is represented by one and one classifier","122","only, it is possible to gain knowledge about the class by inspecting its","214","one-vs-one. With these strategies, each class is represented in a euclidean"]}]}},"b55d12e60369300b97501296e87e13cd15a0e5b7":{"changes":{"sklearn\/ensemble\/tests\/test_voting.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY","sklearn\/ensemble\/voting.py":"MODIFY"},"diff":{"sklearn\/ensemble\/tests\/test_voting.py":[{"add":["70","    with pytest.raises(AttributeError, match=msg):","71","        eclf.predict_proba","72","","73","    assert not hasattr(eclf, \"predict_proba\")","74","    eclf.fit(X, y)","75","    assert not hasattr(eclf, \"predict_proba\")"],"delete":["70","    assert_raise_message(AttributeError, msg, eclf.predict_proba, X)"]}],"doc\/whats_new\/v0.22.rst":[{"add":["71","- |Feature| :class:`ensemble.HistGradientBoostingClassifier` and","72","  :class:`ensemble.HistGradientBoostingRegressor` have an additional","73","  parameter called `warm_start` that enables warm starting. :pr:`14012` by","74","  :user:`Johann Faouzi <johannfaouzi>`.","75","","81","- |Fix| :func:`ensemble.VotingClassifier.predict_proba` will no longer be","82","  present when `voting='hard'`. :pr:`14287` by `Thomas Fan`_."],"delete":["76","- |Feature| :class:`ensemble.HistGradientBoostingClassifier` and","77","  :class:`ensemble.HistGradientBoostingRegressor` have an additional","78","  parameter called `warm_start` that enables warm starting. :pr:`14012` by","79","  :user:`Johann Faouzi <johannfaouzi>`."]}],"sklearn\/ensemble\/voting.py":[{"add":["334","        if self.voting == 'hard':","335","            raise AttributeError(\"predict_proba is not available when\"","336","                                 \" voting=%r\" % self.voting)"],"delete":["315","        if self.voting == 'hard':","316","            raise AttributeError(\"predict_proba is not available when\"","317","                                 \" voting=%r\" % self.voting)"]}]}},"d6b368e81892115d4be0697dde0666dca38ee89a":{"changes":{"sklearn\/ensemble\/gradient_boosting.py":"MODIFY","sklearn\/ensemble\/tests\/test_gradient_boosting.py":"MODIFY"},"diff":{"sklearn\/ensemble\/gradient_boosting.py":[{"add":["1480","                # XXX clean this once we have a support_sample_weight tag","1481","                if sample_weight_is_none:","1482","                    self.init_.fit(X, y)","1483","                else:","1484","                    msg = (\"The initial estimator {} does not support sample \"","1485","                           \"weights.\".format(self.init_.__class__.__name__))","1486","                    try:","1487","                        self.init_.fit(X, y, sample_weight=sample_weight)","1488","                    except TypeError:  # regular estimator without SW support","1489","                        raise ValueError(msg)","1490","                    except ValueError as e:","1491","                        if 'not enough values to unpack' in str(e):  # pipeline","1492","                            raise ValueError(msg) from e","1493","                        else:  # regular estimator whose input checking failed","1494","                            raise"],"delete":["1480","                try:","1481","                    self.init_.fit(X, y, sample_weight=sample_weight)","1482","                except TypeError:","1483","                    if sample_weight_is_none:","1484","                        self.init_.fit(X, y)","1485","                    else:","1486","                        raise ValueError(","1487","                            \"The initial estimator {} does not support sample \"","1488","                            \"weights.\".format(self.init_.__class__.__name__))","1493",""]}],"sklearn\/ensemble\/tests\/test_gradient_boosting.py":[{"add":["41","from sklearn.pipeline import make_pipeline","42","from sklearn.linear_model import LinearRegression","43","from sklearn.svm import NuSVR","1383","def test_gradient_boosting_with_init_pipeline():","1384","    # Check that the init estimator can be a pipeline (see issue #13466)","1385","","1386","    X, y = make_regression(random_state=0)","1387","    init = make_pipeline(LinearRegression())","1388","    gb = GradientBoostingRegressor(init=init)","1389","    gb.fit(X, y)  # pipeline without sample_weight works fine","1390","","1391","    with pytest.raises(","1392","            ValueError,","1393","            match='The initial estimator Pipeline does not support sample '","1394","                  'weights'):","1395","        gb.fit(X, y, sample_weight=np.ones(X.shape[0]))","1396","","1397","    # Passing sample_weight to a pipeline raises a ValueError. This test makes","1398","    # sure we make the distinction between ValueError raised by a pipeline that","1399","    # was passed sample_weight, and a ValueError raised by a regular estimator","1400","    # whose input checking failed.","1401","    with pytest.raises(","1402","            ValueError,","1403","            match='nu <= 0 or nu > 1'):","1404","        # Note that NuSVR properly supports sample_weight","1405","        init = NuSVR(gamma='auto', nu=1.5)","1406","        gb = GradientBoostingRegressor(init=init)","1407","        gb.fit(X, y, sample_weight=np.ones(X.shape[0]))","1408","","1409",""],"delete":[]}]}},"7d5a54b3291cf1af3e29bc5ed41a429610c9ed8a":{"changes":{"sklearn\/linear_model\/tests\/test_coordinate_descent.py":"MODIFY","sklearn\/linear_model\/_coordinate_descent.py":"MODIFY"},"diff":{"sklearn\/linear_model\/tests\/test_coordinate_descent.py":[{"add":["60","@pytest.mark.parametrize('l1_ratio', (-1, 2, None, 10, 'something_wrong'))","61","def test_l1_ratio_param_invalid(l1_ratio):","62","    # Check that correct error is raised when l1_ratio in ElasticNet","63","    # is outside the correct range","64","    X = np.array([[-1.], [0.], [1.]])","65","    Y = [-1, 0, 1]       # just a straight line","66","","67","    msg = \"l1_ratio must be between 0 and 1; got l1_ratio=\"","68","    clf = ElasticNet(alpha=0.1, l1_ratio=l1_ratio)","69","    with pytest.raises(ValueError, match=msg):","70","        clf.fit(X, Y)","71","","72",""],"delete":[]}],"sklearn\/linear_model\/_coordinate_descent.py":[{"add":["757","        if (not isinstance(self.l1_ratio, numbers.Number) or","758","                self.l1_ratio < 0 or self.l1_ratio > 1):","759","            raise ValueError(\"l1_ratio must be between 0 and 1; \"","760","                             f\"got l1_ratio={self.l1_ratio}\")","761",""],"delete":[]}]}},"6d4ae1b61ea8927efe39e27752e15a3cf2263e33":{"changes":{"sklearn\/semi_supervised\/tests\/test_label_propagation.py":"MODIFY","sklearn\/semi_supervised\/label_propagation.py":"MODIFY"},"diff":{"sklearn\/semi_supervised\/tests\/test_label_propagation.py":[{"add":["11","from sklearn.exceptions import ConvergenceWarning","73","    lp_default_y = lp_default.fit(X, y).transduction_","111","                                             gamma=0.1)","112","    clf.fit(X, y)","149","","150","","151","def test_convergence_warning():","152","    # This is a non-regression test for #5774","153","    X = np.array([[1., 0.], [0., 1.], [1., 2.5]])","154","    y = np.array([0, 1, -1])","155","    mdl = label_propagation.LabelSpreading(kernel='rbf', max_iter=1)","156","    assert_warns(ConvergenceWarning, mdl.fit, X, y)","157","    assert_equal(mdl.n_iter_, mdl.max_iter)","158","","159","    mdl = label_propagation.LabelPropagation(kernel='rbf', max_iter=1)","160","    assert_warns(ConvergenceWarning, mdl.fit, X, y)","161","    assert_equal(mdl.n_iter_, mdl.max_iter)","162","","163","    mdl = label_propagation.LabelSpreading(kernel='rbf', max_iter=500)","164","    assert_no_warnings(mdl.fit, X, y)","165","","166","    mdl = label_propagation.LabelPropagation(kernel='rbf', max_iter=500)","167","    assert_no_warnings(mdl.fit, X, y)"],"delete":["72","    lp_default_y = assert_no_warnings(lp_default.fit, X, y).transduction_","110","                                             gamma=0.1).fit(X, y)"]}],"sklearn\/semi_supervised\/label_propagation.py":[{"add":["36",">>> rng = np.random.RandomState(42)","37",">>> random_unlabeled_points = rng.rand(len(iris.target)) < 0.3","55","#          Utkarsh Upadhyay <mail@musicallyut.in>","70","from ..exceptions import ConvergenceWarning","94","    max_iter : integer","264","","265","        for self.n_iter_ in range(self.max_iter):","266","            if np.abs(self.label_distributions_ - l_previous).sum() < self.tol:","267","                break","268","","284","        else:","285","            warnings.warn(","286","                'max_iter=%d was reached without convergence.' % self.max_iter,","287","                category=ConvergenceWarning","288","            )","289","            self.n_iter_ += 1","327","    max_iter : integer","361","    >>> rng = np.random.RandomState(42)","362","    >>> random_unlabeled_points = rng.rand(len(iris.target)) < 0.3","444","    max_iter : integer","478","    >>> rng = np.random.RandomState(42)","479","    >>> random_unlabeled_points = rng.rand(len(iris.target)) < 0.3"],"delete":["36",">>> random_unlabeled_points = np.where(np.random.randint(0, 2,","37","...        size=len(iris.target)))","69","","70","","71","# Helper functions","72","","73","def _not_converged(y_truth, y_prediction, tol=1e-3):","74","    \"\"\"basic convergence check\"\"\"","75","    return np.abs(y_truth - y_prediction).sum() > tol","99","    max_iter : float","266","        remaining_iter = self.max_iter","270","        while (_not_converged(self.label_distributions_, l_previous, self.tol)","271","               and remaining_iter > 1):","287","            remaining_iter -= 1","296","        self.n_iter_ = self.max_iter - remaining_iter","326","    max_iter : float","360","    >>> random_unlabeled_points = np.where(np.random.randint(0, 2,","361","    ...    size=len(iris.target)))","443","    max_iter : float","477","    >>> random_unlabeled_points = np.where(np.random.randint(0, 2,","478","    ...    size=len(iris.target)))"]}]}}}