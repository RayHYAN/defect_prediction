{"58228cb6d35b069c701a5baf1e25510f76fd5ef3":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/compose\/tests\/test_column_transformer.py":"MODIFY","sklearn\/compose\/_column_transformer.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["29",":mod:`sklearn.compose`","30","......................","31","","32","- |Fix| Fixed an issue in :class:`compose.ColumnTransformer` when stacking","33","  columns with types not convertible to a numeric.","34","  :issue:`11912` by :user:`Adrin Jalali <adrinjalali>`.","35",""],"delete":[]}],"sklearn\/compose\/tests\/test_column_transformer.py":[{"add":["370","def test_column_transformer_mixed_cols_sparse():","371","    df = np.array([['a', 1, True],","372","                   ['b', 2, False]],","373","                  dtype='O')","374","","375","    ct = make_column_transformer(","376","        ([0], OneHotEncoder()),","377","        ([1, 2], 'passthrough'),","378","        sparse_threshold=1.0","379","    )","380","","381","    # this shouldn't fail, since boolean can be coerced into a numeric","382","    # See: https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/11912","383","    X_trans = ct.fit_transform(df)","384","    assert X_trans.getformat() == 'csr'","385","    assert_array_equal(X_trans.toarray(), np.array([[1, 0, 1, 1],","386","                                                    [0, 1, 2, 0]]))","387","","388","    ct = make_column_transformer(","389","        ([0], OneHotEncoder()),","390","        ([0], 'passthrough'),","391","        sparse_threshold=1.0","392","    )","393","    with pytest.raises(ValueError,","394","                       match=\"For a sparse output, all columns should\"):","395","        # this fails since strings `a` and `b` cannot be","396","        # coerced into a numeric.","397","        ct.fit_transform(df)","398","","399",""],"delete":[]}],"sklearn\/compose\/_column_transformer.py":[{"add":["514","            try:","515","                # since all columns should be numeric before stacking them","516","                # in a sparse matrix, `check_array` is used for the","517","                # dtype conversion if necessary.","518","                converted_Xs = [check_array(X,","519","                                            accept_sparse=True,","520","                                            force_all_finite=False)","521","                                for X in Xs]","522","            except ValueError:","523","                raise ValueError(\"For a sparse output, all columns should\"","524","                                 \" be a numeric or convertible to a numeric.\")","525","","526","            return sparse.hstack(converted_Xs).tocsr()"],"delete":["514","            return sparse.hstack(Xs).tocsr()"]}]}},"f1a3312cd78df5226c093561ffc086710af5c463":{"changes":{"sklearn\/model_selection\/_split.py":"MODIFY"},"diff":{"sklearn\/model_selection\/_split.py":[{"add":["78","        Yields","79","        ------","303","        Yields","304","        ------","649","        Yields","650","        ------","736","        Yields","737","        ------","1008","        Yields","1009","        ------","1184","        Yields","1185","        ------","1605","        Yields","1606","        ------","1765","        Yields","1766","        ------","1849","        Yields","1850","        ------"],"delete":["78","        Returns","79","        -------","303","        Returns","304","        -------","649","        Returns","650","        -------","736","        Returns","737","        -------","1008","        Returns","1009","        -------","1184","        Returns","1185","        -------","1605","        Returns","1606","        -------","1765","        Returns","1766","        -------","1849","        Returns","1850","        -------"]}]}},"dc7a68575b4eb60020e46c73b3e5fc5825aacebf":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/cluster\/tests\/test_k_means.py":"MODIFY","sklearn\/cluster\/_k_means_elkan.pyx":"MODIFY","sklearn\/cluster\/k_means_.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["547","- Fixed a bug in :func:`cluster.k_means_elkan` where the returned `iteration`","548","  was 1 less than the correct value. Also added the missing `n_iter_` attribute","549","  in the docstring of :class:`cluster.KMeans`. :issue:`11353` by","550","  :user:`Jeremie du Boisberranger <jeremiedbb>`.","551",""],"delete":[]}],"sklearn\/cluster\/tests\/test_k_means.py":[{"add":["981","","982","","983","def test_iter_attribute():","984","    # Regression test on bad n_iter_ value. Previous bug n_iter_ was one off","985","    # it's right value (#11340).","986","    estimator = KMeans(algorithm=\"elkan\", max_iter=1)","987","    estimator.fit(np.random.rand(10, 10))","988","    assert estimator.n_iter_ == 1"],"delete":[]}],"sklearn\/cluster\/_k_means_elkan.pyx":[{"add":["260","    return centers_, labels_, iteration + 1"],"delete":["260","    return centers_, labels_, iteration"]}],"sklearn\/cluster\/k_means_.py":[{"add":["861","    n_iter_ : int","862","        Number of iterations run.","863",""],"delete":[]}]}},"911137f0de9499a140e4ba1f4746de2770b8a53e":{"changes":{"sklearn\/feature_selection\/_rfe.py":"MODIFY","sklearn\/feature_selection\/tests\/test_rfe.py":"MODIFY","doc\/whats_new\/v0.24.rst":"MODIFY"},"diff":{"sklearn\/feature_selection\/_rfe.py":[{"add":["9","import numbers","60","    n_features_to_select : int or float, default=None","61","        The number of features to select. If `None`, half of the features are","62","        selected. If integer, the parameter is the absolute number of features","63","        to select. If float between 0 and 1, it is the fraction of features to","64","        select.","186","        error_msg = (\"n_features_to_select must be either None, a \"","187","                     \"positive integer representing the absolute \"","188","                     \"number of features or a float in (0.0, 1.0] \"","189","                     \"representing a percentage of features to \"","190","                     f\"select. Got {self.n_features_to_select}\")","191","","196","        elif self.n_features_to_select < 0:","197","            raise ValueError(error_msg)","198","        elif isinstance(self.n_features_to_select, numbers.Integral):  # int","200","        elif self.n_features_to_select > 1.0:  # float > 1","201","            raise ValueError(error_msg)","202","        else:  # float","203","            n_features_to_select = int(n_features * self.n_features_to_select)"],"delete":["59","    n_features_to_select : int, default=None","60","        The number of features to select. If `None`, half of the features","61","        are selected.","187","        else:"]}],"sklearn\/feature_selection\/tests\/test_rfe.py":[{"add":["111","@pytest.mark.parametrize(\"n_features_to_select\", [-1, 2.1])","112","def test_rfe_invalid_n_features_errors(n_features_to_select):","113","    clf = SVC(kernel=\"linear\")","114","","115","    iris = load_iris()","116","    rfe = RFE(estimator=clf, n_features_to_select=n_features_to_select,","117","              step=0.1)","118","    msg = f\"n_features_to_select must be .+ Got {n_features_to_select}\"","119","    with pytest.raises(ValueError, match=msg):","120","        rfe.fit(iris.data, iris.target)","121","","122","","123","def test_rfe_percent_n_features():","124","    # test that the results are the same","125","    generator = check_random_state(0)","126","    iris = load_iris()","127","    X = np.c_[iris.data, generator.normal(size=(len(iris.data), 6))]","128","    y = iris.target","129","    # there are 10 features in the data. We select 40%.","130","    clf = SVC(kernel=\"linear\")","131","    rfe_num = RFE(estimator=clf, n_features_to_select=4, step=0.1)","132","    rfe_num.fit(X, y)","133","","134","    rfe_perc = RFE(estimator=clf, n_features_to_select=0.4, step=0.1)","135","    rfe_perc.fit(X, y)","136","","137","    assert_array_equal(rfe_perc.ranking_, rfe_num.ranking_)","138","    assert_array_equal(rfe_perc.support_, rfe_num.support_)","139","","140",""],"delete":[]}],"doc\/whats_new\/v0.24.rst":[{"add":["96","- |Enhancement| Added the option for the number of n_features_to_select to be","97","  given as a float representing the percentage of features to select.","98","  :pr:`17090` by :user:`Lisa Schwetlick <lschwetlick>` and","99","  :user:`Marija Vlajic Wheeler <marijavlajic>`.","100","","252",""],"delete":[]}]}},"6da44dd6b37cb64202b8baed148ed83294001396":{"changes":{".github\/ISSUE_TEMPLATE\/bug_report.md":"MODIFY"},"diff":{".github\/ISSUE_TEMPLATE\/bug_report.md":[{"add":["4","labels: 'Bug: triage'"],"delete":["4","labels: Bug"]}]}},"c71a1c21d14fc7a98493acb1f3d315db720ca4ac":{"changes":{"doc\/whats_new\/v0.23.rst":"MODIFY"},"diff":{"doc\/whats_new\/v0.23.rst":[{"add":["447","- |Fix| :func:`model_selection.cross_val_predict` supports","448","  `method=\"predict_proba\"` when `y=None`.:pr:`15918` by","449","  :user:`Luca Kubin <lkubin>`."],"delete":["447","- |Fix| :func: `cross_val_predict` supports `method=\"predict_proba\"`","448","  when `y=None`.","449","  :pr:`15918` by :user:`Luca Kubin <lkubin>`."]}]}},"02fad7ab24959e59e8f7791bd9c3a353115ba7c8":{"changes":{"doc\/conftest.py":"MODIFY"},"diff":{"doc\/conftest.py":[{"add":["72","    is_index = fname.endswith('datasets\/index.rst')","73","    if fname.endswith('datasets\/labeled_faces.rst') or is_index:","75","    elif fname.endswith('datasets\/mldata.rst') or is_index:","77","    elif fname.endswith('datasets\/rcv1.rst') or is_index:","79","    elif fname.endswith('datasets\/twenty_newsgroups.rst') or is_index:","81","    elif fname.endswith('tutorial\/text_analytics\/working_with_text_data.rst')\\","82","            or is_index:","84","    elif fname.endswith('modules\/compose.rst') or is_index:"],"delete":["72","    if fname.endswith('datasets\/labeled_faces.rst'):","74","    elif fname.endswith('datasets\/mldata.rst'):","76","    elif fname.endswith('datasets\/rcv1.rst'):","78","    elif fname.endswith('datasets\/twenty_newsgroups.rst'):","80","    elif fname.endswith('tutorial\/text_analytics\/working_with_text_data.rst'):","82","    elif fname.endswith('modules\/compose.rst'):"]}]}},"f23b940dcabdc86b8b71dc8a9a90ef91505407cc":{"changes":{"sklearn\/base.py":"MODIFY","sklearn\/tests\/test_base.py":"MODIFY"},"diff":{"sklearn\/base.py":[{"add":["439","    @property","441","        \"\"\"HTML representation of estimator.","442","","443","        This is redundant with the logic of `_repr_mimebundle_`. The latter","444","        should be favorted in the long term, `_repr_html_` is only","445","        implemented for consumers who do not interpret `_repr_mimbundle_`.","446","        \"\"\"","447","        if get_config()[\"display\"] != 'diagram':","448","            raise AttributeError(\"_repr_html_ is only defined when the \"","449","                                 \"'display' configuration option is set to \"","450","                                 \"'diagram'\")","451","        return self._repr_html_inner","452","","453","    def _repr_html_inner(self):","454","        \"\"\"This function is returned by the @property `_repr_html_` to make","455","        `hasattr(estimator, \"_repr_html_\") return `True` or `False` depending","456","        on `get_config()[\"display\"]`.","457","        \"\"\""],"delete":["440","        \"\"\"HTML representation of estimator\"\"\""]}],"sklearn\/tests\/test_base.py":[{"add":["527","","528","","529","def test_repr_html_wraps():","530","    # Checks the display configuration flag controls the html output","531","    tree = DecisionTreeClassifier()","532","    msg = \"_repr_html_ is only defined when\"","533","    with pytest.raises(AttributeError, match=msg):","534","        output = tree._repr_html_()","535","","536","    with config_context(display='diagram'):","537","        output = tree._repr_html_()","538","        assert \"<style>\" in output"],"delete":[]}]}},"9b1928dbc233c052366f5686432e017022bcaa5d":{"changes":{".github\/labeler.yml":"ADD",".github\/workflows\/labeler.yml":"ADD"},"diff":{".github\/labeler.yml":[{"add":[],"delete":[]}],".github\/workflows\/labeler.yml":[{"add":[],"delete":[]}]}},"c50dc0ec9afd5b90288decb3ddf88802a841a368":{"changes":{"sklearn\/utils\/deprecation.py":"MODIFY","sklearn\/utils\/tests\/test_validation.py":"MODIFY","sklearn\/utils\/tests\/test_estimator_checks.py":"MODIFY"},"diff":{"sklearn\/utils\/deprecation.py":[{"add":["80","        # Add a reference to the wrapped function so that we can introspect","81","        # on function arguments in Python 2 (already works in Python 3)","82","        wrapped.__wrapped__ = fun"],"delete":[]}],"sklearn\/utils\/tests\/test_validation.py":[{"add":["24","from sklearn.utils import deprecated","566","    class TestClassWithDeprecatedFitMethod:","567","        @deprecated(\"Deprecated for the purpose of testing has_fit_parameter\")","568","        def fit(self, X, y, sample_weight=None):","569","            pass","570","","571","    assert has_fit_parameter(TestClassWithDeprecatedFitMethod,","572","                             \"sample_weight\"), \\","573","        \"has_fit_parameter fails for class with deprecated fit method.\"","574",""],"delete":[]}],"sklearn\/utils\/tests\/test_estimator_checks.py":[{"add":["11","from sklearn.utils import deprecated","18","from sklearn.utils.estimator_checks import check_fit_score_takes_y","180","def test_check_fit_score_takes_y_works_on_deprecated_fit():","181","    # Tests that check_fit_score_takes_y works on a class with","182","    # a deprecated fit method","183","","184","    class TestEstimatorWithDeprecatedFitMethod(BaseEstimator):","185","        @deprecated(\"Deprecated for the purpose of testing \"","186","                    \"check_fit_score_takes_y\")","187","        def fit(self, X, y):","188","            return self","189","","190","    check_fit_score_takes_y(\"test\", TestEstimatorWithDeprecatedFitMethod())","191","","192",""],"delete":[]}]}},"788a458bba353c2cf3cfa5a15d6f68315149ef9e":{"changes":{"benchmarks\/bench_lof.py":"ADD","examples\/covariance\/plot_outlier_detection.py":"MODIFY","doc\/modules\/classes.rst":"MODIFY","doc\/modules\/outlier_detection.rst":"MODIFY","sklearn\/neighbors\/tests\/test_lof.py":"ADD","sklearn\/neighbors\/unsupervised.py":"MODIFY","doc\/whats_new.rst":"MODIFY","examples\/neighbors\/plot_lof.py":"ADD","sklearn\/neighbors\/classification.py":"MODIFY","sklearn\/neighbors\/__init__.py":"MODIFY","sklearn\/neighbors\/lof.py":"ADD","sklearn\/neighbors\/regression.py":"MODIFY"},"diff":{"benchmarks\/bench_lof.py":[{"add":[],"delete":[]}],"examples\/covariance\/plot_outlier_detection.py":[{"add":["20","- using the Local Outlier Factor to measure the local deviation of a given","21","  data point with respect to its neighbors by comparing their local density.","22","","41","from sklearn.neighbors import LocalOutlierFactor","42","","43","print(__doc__)","59","                                        random_state=rng),","60","    \"Local Outlier Factor\": LocalOutlierFactor(","61","        n_neighbors=35,","62","        contamination=outliers_fraction)}","65","xx, yy = np.meshgrid(np.linspace(-7, 7, 100), np.linspace(-7, 7, 100))","82","    plt.figure(figsize=(9, 7))","85","        if clf_name == \"Local Outlier Factor\":","86","            y_pred = clf.fit_predict(X)","87","            scores_pred = clf.negative_outlier_factor_","88","        else:","89","            clf.fit(X)","90","            scores_pred = clf.decision_function(X)","91","            y_pred = clf.predict(X)","96","        if clf_name == \"Local Outlier Factor\":","97","            # decision_function is private for LOF","98","            Z = clf._decision_function(np.c_[xx.ravel(), yy.ravel()])","99","        else:","100","            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])","102","        subplot = plt.subplot(2, 2, i + 1)","115","            prop=matplotlib.font_manager.FontProperties(size=10),","117","        subplot.set_xlabel(\"%d. %s (errors: %d)\" % (i + 1, clf_name, n_errors))","120","    plt.subplots_adjust(0.04, 0.1, 0.96, 0.94, 0.1, 0.26)","121","    plt.suptitle(\"Outlier detection\")"],"delete":["29","print(__doc__)","54","                                        random_state=rng)}","57","xx, yy = np.meshgrid(np.linspace(-7, 7, 500), np.linspace(-7, 7, 500))","74","    plt.figure(figsize=(10.8, 3.6))","77","        clf.fit(X)","78","        scores_pred = clf.decision_function(X)","81","        y_pred = clf.predict(X)","84","        Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])","86","        subplot = plt.subplot(1, 3, i + 1)","99","            prop=matplotlib.font_manager.FontProperties(size=11),","101","        subplot.set_title(\"%d. %s (errors: %d)\" % (i + 1, clf_name, n_errors))","104","    plt.subplots_adjust(0.04, 0.1, 0.96, 0.92, 0.1, 0.26)"]}],"doc\/modules\/classes.rst":[{"add":["1052","   neighbors.LocalOutlierFactor","1053","\t      "],"delete":["1052",""]}],"doc\/modules\/outlier_detection.rst":[{"add":["167","     :class:`neighbors.LocalOutlierFactor`,","170","     :class:`covariance.EllipticEnvelope`.","178","Local Outlier Factor","179","--------------------","180","Another efficient way to perform outlier detection on moderately high dimensional","181","datasets is to use the Local Outlier Factor (LOF) algorithm.","182","","183","The :class:`neighbors.LocalOutlierFactor` (LOF) algorithm computes a score","184","(called local outlier factor) reflecting the degree of abnormality of the","185","observations.","186","It measures the local density deviation of a given data point with respect to","187","its neighbors. The idea is to detect the samples that have a substantially","188","lower density than their neighbors.","189","","190","In practice the local density is obtained from the k-nearest neighbors.","191","The LOF score of an observation is equal to the ratio of the","192","average local density of his k-nearest neighbors, and its own local density:","193","a normal instance is expected to have a local density similar to that of its","194","neighbors, while abnormal data are expected to have much smaller local density.","195","","196","The number k of neighbors considered, (alias parameter n_neighbors) is typically","197","chosen 1) greater than the minimum number of objects a cluster has to contain,","198","so that other objects can be local outliers relative to this cluster, and 2)","199","smaller than the maximum number of close by objects that can potentially be","200","local outliers.","201","In practice, such informations are generally not available, and taking","202","n_neighbors=20 appears to work well in general.","203","When the proportion of outliers is high (i.e. greater than 10 \\%, as in the","204","example below), n_neighbors should be greater (n_neighbors=35 in the example","205","below).","206","","207","The strength of the LOF algorithm is that it takes both local and global","208","properties of datasets into consideration: it can perform well even in datasets","209","where abnormal samples have different underlying densities.","210","The question is not, how isolated the sample is, but how isolated it is","211","with respect to the surrounding neighborhood.","212","","213","This strategy is illustrated below.","214","","215",".. figure:: ..\/auto_examples\/neighbors\/images\/sphx_glr_plot_lof_001.png","216","   :target: ..\/auto_examples\/neighbors\/plot_lof.html","217","   :align: center","218","   :scale: 75%","219","","220",".. topic:: Examples:","221","","222","   * See :ref:`sphx_glr_auto_example_neighbors_plot_lof.py` for","223","     an illustration of the use of :class:`neighbors.LocalOutlierFactor`.","224","","225","   * See :ref:`sphx_glr_auto_example_covariance_plot_outlier_detection.py` for a","226","     comparison with other anomaly detection methods.","227","","228",".. topic:: References:","229","","230","   .. [BKNS2000]  Breunig, Kriegel, Ng, and Sander (2000)","231","      `LOF: identifying density-based local outliers.","232","      <http:\/\/www.dbs.ifi.lmu.de\/Publikationen\/Papers\/LOF.pdf>`_","233","      Proc. ACM SIGMOD","234","","235","One-class SVM versus Elliptic Envelope versus Isolation Forest versus LOF","236","-------------------------------------------------------------------------","248","multiple modes and :class:`ensemble.IsolationForest` and","249",":class:`neighbors.LocalOutlierFactor` perform well in every cases.","263",".. list-table:: **Comparing One-class SVM, Isolation Forest, LOF, and Elliptic Envelope**","274","\tand :class:`neighbors.LocalOutlierFactor` perform as well.","275","      - |outlier1| ","280","        inliers. However, we can see that :class:`ensemble.IsolationForest`,","281","\t:class:`svm.OneClassSVM` and :class:`neighbors.LocalOutlierFactor`","282","\thave difficulties to detect the two modes,","284","        tends to overfit: because it has no model of inliers, it","292","        approximation as well as :class:`ensemble.IsolationForest`","293","        and :class:`neighbors.LocalOutlierFactor`,","301","     an outlier detection method), the :class:`ensemble.IsolationForest`,","302","     the :class:`neighbors.LocalOutlierFactor`","303","     and a covariance-based outlier detection :class:`covariance.EllipticEnvelope`."],"delete":["169","     :class:`covariance.MinCovDet`.","177","One-class SVM versus Elliptic Envelope versus Isolation Forest","178","--------------------------------------------------------------","190","multiple modes and :class:`ensemble.IsolationForest` performs well in every cases.","204",".. list-table:: **Comparing One-class SVM approach, and elliptic envelope**","215","\tperforms as well.","216","      - |outlier1|","221","        inliers. However, we can see that both :class:`ensemble.IsolationForest`","222","\tand :class:`svm.OneClassSVM` have difficulties to detect the two modes,","224","        tends to overfit: because it has not model of inliers, it","232","        approximation as well as :class:`ensemble.IsolationForest`,","240","     an outlier detection method), the :class:`ensemble.IsolationForest`","241","     and a covariance-based outlier","242","     detection with :class:`covariance.MinCovDet`."]}],"sklearn\/neighbors\/tests\/test_lof.py":[{"add":[],"delete":[]}],"sklearn\/neighbors\/unsupervised.py":[{"add":["17","        Number of neighbors to use by default for :meth:`kneighbors` queries.","79","        Affects only :meth:`kneighbors` and :meth:`kneighbors_graph` methods."],"delete":["17","        Number of neighbors to use by default for :meth:`k_neighbors` queries.","79","        Affects only :meth:`k_neighbors` and :meth:`kneighbors_graph` methods."]}],"doc\/whats_new.rst":[{"add":["18","   - Added the :class:`neighbors.LocalOutlierFactor` class for anomaly detection based","19","     on nearest neighbors. By `Nicolas Goix`_ and `Alexandre Gramfort`_.","20","","4745",".. _Nicolas Goix: https:\/\/perso.telecom-paristech.fr\/~goix\/"],"delete":["4742",".. _Nicolas Goix: https:\/\/webperso.telecom-paristech.fr\/front\/frontoffice.php?SP_ID=241"]}],"examples\/neighbors\/plot_lof.py":[{"add":[],"delete":[]}],"sklearn\/neighbors\/classification.py":[{"add":["31","        Number of neighbors to use by default for :meth:`kneighbors` queries."],"delete":["31","        Number of neighbors to use by default for :meth:`k_neighbors` queries."]}],"sklearn\/neighbors\/__init__.py":[{"add":["15","from .lof import LocalOutlierFactor","29","           'LSHForest',","30","           'LocalOutlierFactor']"],"delete":["28","           'LSHForest']"]}],"sklearn\/neighbors\/lof.py":[{"add":[],"delete":[]}],"sklearn\/neighbors\/regression.py":[{"add":["31","        Number of neighbors to use by default for :meth:`kneighbors` queries."],"delete":["31","        Number of neighbors to use by default for :meth:`k_neighbors` queries."]}]}},"73caba5dabfc0157ce2631b3f94ec1fb78d3ded3":{"changes":{"sklearn\/metrics\/_classification.py":"MODIFY","sklearn\/metrics\/tests\/test_classification.py":"MODIFY","doc\/whats_new\/v0.24.rst":"MODIFY"},"diff":{"sklearn\/metrics\/_classification.py":[{"add":["1485","            zero_division_value = np.float64(1.0)","1486","            if zero_division in [\"warn\", 0]:","1487","                zero_division_value = np.float64(0.0)","1492","            if pred_sum.sum() == 0:","1493","                return (zero_division_value,","1494","                        zero_division_value,","1495","                        zero_division_value,","1496","                        None)","1497","            else:","1498","                return (np.float64(0.0),","1499","                        zero_division_value,","1500","                        np.float64(0.0),","1501","                        None)"],"delete":["1485","            zero_division_value = 0.0 if zero_division in [\"warn\", 0] else 1.0","1490","            return (zero_division_value if pred_sum.sum() == 0 else 0,","1491","                    zero_division_value,","1492","                    zero_division_value if pred_sum.sum() == 0 else 0,","1493","                    None)"]}],"sklearn\/metrics\/tests\/test_classification.py":[{"add":["155","def test_classification_report_output_dict_empty_input():","156","    report = classification_report(y_true=[], y_pred=[], output_dict=True)","157","    expected_report = {'accuracy': 0.0,","158","                       'macro avg': {'f1-score': np.nan,","159","                                     'precision': np.nan,","160","                                     'recall': np.nan,","161","                                     'support': 0},","162","                       'weighted avg': {'f1-score': 0.0,","163","                                        'precision': 0.0,","164","                                        'recall': 0.0,","165","                                        'support': 0}}","166","    assert isinstance(report, dict)","167","    # assert the 2 dicts are equal.","168","    assert report.keys() == expected_report.keys()","169","    for key in expected_report:","170","        if key == 'accuracy':","171","            assert isinstance(report[key], float)","172","            assert report[key] == expected_report[key]","173","        else:","174","            assert report[key].keys() == expected_report[key].keys()","175","            for metric in expected_report[key]:","176","                assert_almost_equal(expected_report[key][metric],","177","                                    report[key][metric])","178","","179",""],"delete":[]}],"doc\/whats_new\/v0.24.rst":[{"add":["253","  ","254","- |Fix| Fixed a bug in ","255","  :func:`metrics.classification_report` which was raising AttributeError","256","  when called with `output_dict=True` for 0-length values.","257","  :pr:`17777` by :user:`Shubhanshu Mishra <napsternxg>`"],"delete":[]}]}},"2a5c845a60b3fb28704c3f04e8128613459f6222":{"changes":{"sklearn\/feature_selection\/tests\/test_mutual_info.py":"MODIFY","sklearn\/feature_selection\/mutual_info_.py":"MODIFY"},"diff":{"sklearn\/feature_selection\/tests\/test_mutual_info.py":[{"add":["185","        assert_raises(ValueError, mutual_info, X_csr, y,","187","        assert_raises(ValueError, mutual_info, X, y,","188","                      discrete_features='manual')","189","        assert_raises(ValueError, mutual_info, X_csr, y,","190","                      discrete_features=[True, False, True])","191","        assert_raises(IndexError, mutual_info, X, y,","192","                      discrete_features=[True, False, True, False])","193","        assert_raises(IndexError, mutual_info, X, y, discrete_features=[1, 4])","197","        mi_3 = mutual_info(X_csr, y, discrete_features='auto', random_state=0)","198","        mi_4 = mutual_info(X_csr, y, discrete_features=True, random_state=0)","199","        mi_5 = mutual_info(X, y, discrete_features=[True, False, True],","201","        mi_6 = mutual_info(X, y, discrete_features=[0, 2], random_state=0)","205","        assert_array_equal(mi_5, mi_6)"],"delete":["185","        assert_raises(ValueError, mutual_info_regression, X_csr, y,","190","","191","        mi_3 = mutual_info(X_csr, y, discrete_features='auto',","193","        mi_4 = mutual_info(X_csr, y, discrete_features=True,","194","                           random_state=0)"]}],"sklearn\/feature_selection\/mutual_info_.py":[{"add":["12","from ..utils.validation import check_array, check_X_y","249","    if isinstance(discrete_features, (str, bool)):","250","        if isinstance(discrete_features, str):","251","            if discrete_features == 'auto':","252","                discrete_features = issparse(X)","253","            else:","254","                raise ValueError(\"Invalid string value for discrete_features.\")","258","        discrete_features = check_array(discrete_features, ensure_2d=False)"],"delete":["12","from ..utils.validation import check_X_y","249","    if discrete_features == 'auto':","250","        discrete_features = issparse(X)","251","","252","    if isinstance(discrete_features, bool):","256","        discrete_features = np.asarray(discrete_features)"]}]}},"b4d7c96054519e958e7d8b9612fd74f46d807b80":{"changes":{"sklearn\/model_selection\/tests\/test_search.py":"MODIFY","sklearn\/model_selection\/_search.py":"MODIFY"},"diff":{"sklearn\/model_selection\/tests\/test_search.py":[{"add":["653","@pytest.mark.parametrize('out_bound_value', [-1, 2])","654","@pytest.mark.parametrize('search_cv', [RandomizedSearchCV, GridSearchCV])","655","def test_refit_callable_out_bound(out_bound_value, search_cv):","664","        return out_bound_value","669","    clf = search_cv(LinearSVC(random_state=42), {'C': [0.1, 1]},","670","                    scoring='precision', refit=refit_callable_out_bound, cv=5)"],"delete":["653","def test_refit_callable_out_bound():","662","        return -1","667","    clf = GridSearchCV(LinearSVC(random_state=42), {'C': [0.1, 1]},","668","                       scoring='precision', refit=refit_callable_out_bound,","669","                       cv=5)"]}],"sklearn\/model_selection\/_search.py":[{"add":["700","                if (self.best_index_ < 0 or","701","                   self.best_index_ >= len(results[\"params\"])):"],"delete":["700","                if self.best_index_ < 0 or self.best_index_ >= len(results):"]}]}},"8d3b4ff3eec890396a3d7a806bbe944f55a89cb4":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/ensemble\/tests\/test_voting.py":"MODIFY","sklearn\/ensemble\/voting.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["321","- |Fix| :class:`ensemble.VotingClassifier` and","322","  :class:`ensemble.VotingRegressor` were failing during ``fit`` in one","323","  of the estimators was set to ``None`` and ``sample_weight`` was not ``None``.","324","  :pr:`13779` by :user:`Guillaume Lemaitre <glemaitre>`.","325",""],"delete":[]}],"sklearn\/ensemble\/tests\/test_voting.py":[{"add":["10","from sklearn.linear_model import LinearRegression","14","from sklearn.ensemble import RandomForestRegressor","511","","512","","513","@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22","514","@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22","515","@pytest.mark.parametrize(","516","    \"X, y, voter\",","517","    [(X, y, VotingClassifier(","518","        [('lr', LogisticRegression()),","519","         ('rf', RandomForestClassifier(n_estimators=5))])),","520","     (X_r, y_r, VotingRegressor(","521","         [('lr', LinearRegression()),","522","          ('rf', RandomForestRegressor(n_estimators=5))]))]","523",")","524","def test_none_estimator_with_weights(X, y, voter):","525","    # check that an estimator can be set to None and passing some weight","526","    # regression test for","527","    # https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/13777","528","    voter.fit(X, y, sample_weight=np.ones(y.shape))","529","    voter.set_params(lr=None)","530","    voter.fit(X, y, sample_weight=np.ones(y.shape))","531","    y_pred = voter.predict(X)","532","    assert y_pred.shape == y.shape"],"delete":[]}],"sklearn\/ensemble\/voting.py":[{"add":["80","                if step is None:","81","                    continue"],"delete":[]}]}},"db4e2440c55e2d43083b061f742159c7158c6c96":{"changes":{"setup.py":"MODIFY"},"diff":{"setup.py":[{"add":["12","import traceback","156","        traceback.print_exc()","176","        traceback.print_exc()"],"delete":["5","import subprocess","6",""]}]}},"7243cc3687073e0a98f00d3dd6bc57007bf401ef":{"changes":{"sklearn\/covariance\/shrunk_covariance_.py":"MODIFY","sklearn\/covariance\/empirical_covariance_.py":"MODIFY"},"diff":{"sklearn\/covariance\/shrunk_covariance_.py":[{"add":["73","        If True, data will not be centered before computation.","76","        If False, data will be centered before computation.","177","        If True, data will not be centered before computation.","180","        If False, data will be centered before computation.","272","        If True, data will not be centered before computation.","275","        If False, data will be centered before computation.","341","        If True, data will not be centered before computation.","344","        If False (default), data will be centered before computation.","450","      If True, data will not be centered before computation.","453","      If False, data will be centered before computation.","527","        If True, data will not be centered before computation.","530","        If False (default), data will be centered before computation."],"delete":["73","        If True, data are not centered before computation.","76","        If False, data are centered before computation.","177","        If True, data are not centered before computation.","180","        If False, data are centered before computation.","272","        If True, data are not centered before computation.","275","        If False, data are centered before computation.","341","        If True, data are not centered before computation.","344","        If False (default), data are centered before computation.","450","      If True, data are not centered before computation.","453","      If False, data are centered before computation.","527","        If True, data are not centered before computation.","530","        If False (default), data are centered before computation."]}],"sklearn\/covariance\/empirical_covariance_.py":[{"add":["58","        If True, data will not be centered before computation.","61","        If False, data will be centered before computation."],"delete":["58","        If True, data are not centered before computation.","61","        If False, data are centered before computation."]}]}},"da0cb32270ce18963799906a8a0a75216749e21c":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/utils\/class_weight.py":"MODIFY","sklearn\/utils\/tests\/test_class_weight.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["19","- |Fix| Fixed a bug mostly affecting :class:`ensemble.RandomForestClassifier`","20","  where ``class_weight='balanced_subsample'`` failed with more than 32 classes.","21","  :issue:`12165` by `Joel Nothman`_.","22",""],"delete":[]}],"sklearn\/utils\/class_weight.py":[{"add":["152","            weight_k = np.take(compute_class_weight(class_weight_k,","153","                                                    classes_subsample,","154","                                                    y_subsample),","155","                               np.searchsorted(classes_subsample,","156","                                               classes_full),","157","                               mode='clip')"],"delete":["152","            weight_k = np.choose(np.searchsorted(classes_subsample,","153","                                                 classes_full),","154","                                 compute_class_weight(class_weight_k,","155","                                                      classes_subsample,","156","                                                      y_subsample),","157","                                 mode='clip')"]}],"sklearn\/utils\/tests\/test_class_weight.py":[{"add":["253","","254","","255","def test_compute_sample_weight_more_than_32():","256","    # Non-regression smoke test for #12146","257","    y = np.arange(50)  # more than 32 distinct classes","258","    indices = np.arange(50)  # use subsampling","259","    weight = compute_sample_weight('balanced', y, indices=indices)","260","    assert_array_almost_equal(weight, np.ones(y.shape[0]))"],"delete":[]}]}},"89da7f71a66585978c51f4d510be9583d8020066":{"changes":{"sklearn\/metrics\/regression.py":"MODIFY","sklearn\/metrics\/scorer.py":"MODIFY","sklearn\/metrics\/tests\/test_score_objects.py":"MODIFY","sklearn\/metrics\/tests\/test_common.py":"MODIFY","doc\/modules\/classes.rst":"MODIFY","doc\/modules\/model_evaluation.rst":"MODIFY","sklearn\/metrics\/__init__.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY","sklearn\/metrics\/tests\/test_regression.py":"MODIFY"},"diff":{"sklearn\/metrics\/regression.py":[{"add":["21","#          Christian Lorentzen <lorentzen.ch@googlemail.com>","26","from scipy.special import xlogy","42","    \"explained_variance_score\",","43","    \"mean_tweedie_deviance\",","44","    \"mean_poisson_deviance\",","45","    \"mean_gamma_deviance\",","49","def _check_reg_targets(y_true, y_pred, multioutput, dtype=\"numeric\"):","79","    dtype: str or list, default=\"numeric\"","80","        the dtype argument passed to check_array","84","    y_true = check_array(y_true, ensure_2d=False, dtype=dtype)","85","    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)","618","","619","","620","def mean_tweedie_deviance(y_true, y_pred, sample_weight=None, p=0):","621","    \"\"\"Mean Tweedie deviance regression loss.","622","","623","    Read more in the :ref:`User Guide <mean_tweedie_deviance>`.","624","","625","    Parameters","626","    ----------","627","    y_true : array-like of shape (n_samples,)","628","        Ground truth (correct) target values.","629","","630","    y_pred : array-like of shape (n_samples,)","631","        Estimated target values.","632","","633","    sample_weight : array-like, shape (n_samples,), optional","634","        Sample weights.","635","","636","    p : float, optional","637","        Tweedie power parameter. Either p ¡Ü 0 or p ¡Ý 1.","638","","639","        The higher `p` the less weight is given to extreme","640","        deviations between true and predicted targets.","641","","642","        - p < 0: Extreme stable distribution. Requires: y_pred > 0.","643","        - p = 0 : Normal distribution, output corresponds to","644","          mean_squared_error. y_true and y_pred can be any real numbers.","645","        - p = 1 : Poisson distribution. Requires: y_true ¡Ý 0 and y_pred > 0.","646","        - 1 < p < 2 : Compound Poisson distribution. Requires: y_true ¡Ý 0","647","          and y_pred > 0.","648","        - p = 2 : Gamma distribution. Requires: y_true > 0 and y_pred > 0.","649","        - p = 3 : Inverse Gaussian distribution. Requires: y_true > 0","650","          and y_pred > 0.","651","        - otherwise : Positive stable distribution. Requires: y_true > 0","652","          and y_pred > 0.","653","","654","    Returns","655","    -------","656","    loss : float","657","        A non-negative floating point value (the best value is 0.0).","658","","659","    Examples","660","    --------","661","    >>> from sklearn.metrics import mean_tweedie_deviance","662","    >>> y_true = [2, 0, 1, 4]","663","    >>> y_pred = [0.5, 0.5, 2., 2.]","664","    >>> mean_tweedie_deviance(y_true, y_pred, p=1)","665","    1.4260...","666","    \"\"\"","667","    y_type, y_true, y_pred, _ = _check_reg_targets(","668","        y_true, y_pred, None, dtype=[np.float64, np.float32])","669","    if y_type == 'continuous-multioutput':","670","        raise ValueError(\"Multioutput not supported in mean_tweedie_deviance\")","671","    check_consistent_length(y_true, y_pred, sample_weight)","672","","673","    if sample_weight is not None:","674","        sample_weight = column_or_1d(sample_weight)","675","        sample_weight = sample_weight[:, np.newaxis]","676","","677","    message = (\"Mean Tweedie deviance error with p={} can only be used on \"","678","               .format(p))","679","    if p < 0:","680","        # 'Extreme stable', y_true any realy number, y_pred > 0","681","        if (y_pred <= 0).any():","682","            raise ValueError(message + \"strictly positive y_pred.\")","683","        dev = 2 * (np.power(np.maximum(y_true, 0), 2-p)\/((1-p) * (2-p)) -","684","                   y_true * np.power(y_pred, 1-p)\/(1-p) +","685","                   np.power(y_pred, 2-p)\/(2-p))","686","    elif p == 0:","687","        # Normal distribution, y_true and y_pred any real number","688","        dev = (y_true - y_pred)**2","689","    elif p < 1:","690","        raise ValueError(\"Tweedie deviance is only defined for p<=0 and \"","691","                         \"p>=1.\")","692","    elif p == 1:","693","        # Poisson distribution, y_true >= 0, y_pred > 0","694","        if (y_true < 0).any() or (y_pred <= 0).any():","695","            raise ValueError(message + \"non-negative y_true and strictly \"","696","                             \"positive y_pred.\")","697","        dev = 2 * (xlogy(y_true, y_true\/y_pred) - y_true + y_pred)","698","    elif p == 2:","699","        # Gamma distribution, y_true and y_pred > 0","700","        if (y_true <= 0).any() or (y_pred <= 0).any():","701","            raise ValueError(message + \"strictly positive y_true and y_pred.\")","702","        dev = 2 * (np.log(y_pred\/y_true) + y_true\/y_pred - 1)","703","    else:","704","        if p < 2:","705","            # 1 < p < 2 is Compound Poisson, y_true >= 0, y_pred > 0","706","            if (y_true < 0).any() or (y_pred <= 0).any():","707","                raise ValueError(message + \"non-negative y_true and strictly \"","708","                                           \"positive y_pred.\")","709","        else:","710","            if (y_true <= 0).any() or (y_pred <= 0).any():","711","                raise ValueError(message + \"strictly positive y_true and \"","712","                                           \"y_pred.\")","713","","714","        dev = 2 * (np.power(y_true, 2-p)\/((1-p) * (2-p)) -","715","                   y_true * np.power(y_pred, 1-p)\/(1-p) +","716","                   np.power(y_pred, 2-p)\/(2-p))","717","","718","    return np.average(dev, weights=sample_weight)","719","","720","","721","def mean_poisson_deviance(y_true, y_pred, sample_weight=None):","722","    \"\"\"Mean Poisson deviance regression loss.","723","","724","    Poisson deviance is equivalent to the Tweedie deviance with","725","    the power parameter `p=1`.","726","","727","    Read more in the :ref:`User Guide <mean_tweedie_deviance>`.","728","","729","    Parameters","730","    ----------","731","    y_true : array-like of shape (n_samples,)","732","        Ground truth (correct) target values. Requires y_true ¡Ý 0.","733","","734","    y_pred : array-like of shape (n_samples,)","735","        Estimated target values. Requires y_pred > 0.","736","","737","    sample_weight : array-like, shape (n_samples,), optional","738","        Sample weights.","739","","740","    Returns","741","    -------","742","    loss : float","743","        A non-negative floating point value (the best value is 0.0).","744","","745","    Examples","746","    --------","747","    >>> from sklearn.metrics import mean_poisson_deviance","748","    >>> y_true = [2, 0, 1, 4]","749","    >>> y_pred = [0.5, 0.5, 2., 2.]","750","    >>> mean_poisson_deviance(y_true, y_pred)","751","    1.4260...","752","    \"\"\"","753","    return mean_tweedie_deviance(","754","        y_true, y_pred, sample_weight=sample_weight, p=1","755","    )","756","","757","","758","def mean_gamma_deviance(y_true, y_pred, sample_weight=None):","759","    \"\"\"Mean Gamma deviance regression loss.","760","","761","    Gamma deviance is equivalent to the Tweedie deviance with","762","    the power parameter `p=2`. It is invariant to scaling of","763","    the target variable, and mesures relative errors.","764","","765","    Read more in the :ref:`User Guide <mean_tweedie_deviance>`.","766","","767","    Parameters","768","    ----------","769","    y_true : array-like of shape (n_samples,)","770","        Ground truth (correct) target values. Requires y_true > 0.","771","","772","    y_pred : array-like of shape (n_samples,)","773","        Estimated target values. Requires y_pred > 0.","774","","775","    sample_weight : array-like, shape (n_samples,), optional","776","        Sample weights.","777","","778","    Returns","779","    -------","780","    loss : float","781","        A non-negative floating point value (the best value is 0.0).","782","","783","    Examples","784","    --------","785","    >>> from sklearn.metrics import mean_gamma_deviance","786","    >>> y_true = [2, 0.5, 1, 4]","787","    >>> y_pred = [0.5, 0.5, 2., 2.]","788","    >>> mean_gamma_deviance(y_true, y_pred)","789","    1.0568...","790","    \"\"\"","791","    return mean_tweedie_deviance(","792","        y_true, y_pred, sample_weight=sample_weight, p=2","793","    )"],"delete":["40","    \"explained_variance_score\"","44","def _check_reg_targets(y_true, y_pred, multioutput):","77","    y_true = check_array(y_true, ensure_2d=False)","78","    y_pred = check_array(y_pred, ensure_2d=False)"]}],"sklearn\/metrics\/scorer.py":[{"add":["26","               mean_squared_error, mean_squared_log_error,","27","               mean_tweedie_deviance, accuracy_score,","497","neg_mean_poisson_deviance_scorer = make_scorer(","498","    mean_tweedie_deviance, p=1., greater_is_better=False","499",")","500","","501","neg_mean_gamma_deviance_scorer = make_scorer(","502","    mean_tweedie_deviance, p=2., greater_is_better=False","503",")","551","               neg_mean_poisson_deviance=neg_mean_poisson_deviance_scorer,","552","               neg_mean_gamma_deviance=neg_mean_gamma_deviance_scorer,"],"delete":["26","               mean_squared_error, mean_squared_log_error, accuracy_score,","494",""]}],"sklearn\/metrics\/tests\/test_score_objects.py":[{"add":["45","                      'max_error', 'neg_mean_poisson_deviance',","46","                      'neg_mean_gamma_deviance']","70","REQUIRE_POSITIVE_Y_SCORERS = ['neg_mean_poisson_deviance',","71","                              'neg_mean_gamma_deviance']","72","","73","","74","def _require_positive_y(y):","75","    \"\"\"Make targets strictly positive\"\"\"","76","    offset = abs(y.min()) + 1","77","    y = y + offset","78","    return y","79","","84","    # some of the regressions scorers require strictly positive input.","85","    sensible_regr.fit(X_train, y_train + 1)","491","        if name in REQUIRE_POSITIVE_Y_SCORERS:","492","            target = _require_positive_y(target)","519","","520","    if name in REQUIRE_POSITIVE_Y_SCORERS:","521","        y_mm_1 = _require_positive_y(y_mm)","522","        y_ml_mm_1 = _require_positive_y(y_ml_mm)","523","    else:","524","        y_mm_1, y_ml_mm_1 = y_mm, y_ml_mm","525","","526","    # UndefinedMetricWarning for P \/ R scores","527","    with ignore_warnings():","528","        scorer, estimator = SCORERS[name], ESTIMATORS[name]","529","        if name in MULTILABEL_ONLY_SCORERS:","530","            score = scorer(estimator, X_mm, y_ml_mm_1)","531","        else:","532","            score = scorer(estimator, X_mm, y_mm_1)","533","        assert isinstance(score, numbers.Number), name"],"delete":["45","                      'max_error']","73","    sensible_regr.fit(X_train, y_train)","500","@ignore_warnings  # UndefinedMetricWarning for P \/ R scores","501","def check_scorer_memmap(scorer_name):","502","    scorer, estimator = SCORERS[scorer_name], ESTIMATORS[scorer_name]","503","    if scorer_name in MULTILABEL_ONLY_SCORERS:","504","        score = scorer(estimator, X_mm, y_ml_mm)","505","    else:","506","        score = scorer(estimator, X_mm, y_mm)","507","    assert isinstance(score, numbers.Number), scorer_name","508","","509","","515","    check_scorer_memmap(name)"]}],"sklearn\/metrics\/tests\/test_common.py":[{"add":["46","from sklearn.metrics import mean_tweedie_deviance","47","from sklearn.metrics import mean_poisson_deviance","48","from sklearn.metrics import mean_gamma_deviance","102","    \"mean_normal_deviance\": partial(mean_tweedie_deviance, p=0),","103","    \"mean_poisson_deviance\": mean_poisson_deviance,","104","    \"mean_gamma_deviance\": mean_gamma_deviance,","105","    \"mean_compound_poisson_deviance\":","106","    partial(mean_tweedie_deviance, p=1.4),","444","    \"cohen_kappa_score\", \"mean_normal_deviance\"","466","    \"macro_recall_score\", \"log_loss\", \"hinge_loss\",","467","    \"mean_gamma_deviance\", \"mean_poisson_deviance\",","468","    \"mean_compound_poisson_deviance\"","480","METRICS_REQUIRE_POSITIVE_Y = {","481","    \"mean_poisson_deviance\",","482","    \"mean_gamma_deviance\",","483","    \"mean_compound_poisson_deviance\",","484","}","487","def _require_positive_targets(y1, y2):","488","    \"\"\"Make targets strictly positive\"\"\"","489","    offset = abs(min(y1.min(), y2.min())) + 1","490","    y1 += offset","491","    y2 += offset","492","    return y1, y2","493","","494","","495","def test_symmetry_consistency():","507","","508","@pytest.mark.parametrize(\"name\", sorted(SYMMETRIC_METRICS))","509","def test_symmetric_metric(name):","510","    # Test the symmetry of score and loss functions","511","    random_state = check_random_state(0)","512","    y_true = random_state.randint(0, 2, size=(20, ))","513","    y_pred = random_state.randint(0, 2, size=(20, ))","514","","515","    if name in METRICS_REQUIRE_POSITIVE_Y:","516","        y_true, y_pred = _require_positive_targets(y_true, y_pred)","517","","518","    y_true_bin = random_state.randint(0, 2, size=(20, 25))","519","    y_pred_bin = random_state.randint(0, 2, size=(20, 25))","520","","521","    metric = ALL_METRICS[name]","522","    if name in METRIC_UNDEFINED_BINARY:","523","        if name in MULTILABELS_METRICS:","524","            assert_allclose(metric(y_true_bin, y_pred_bin),","525","                            metric(y_pred_bin, y_true_bin),","527","        else:","528","            assert False, \"This case is currently unhandled\"","529","    else:","530","        assert_allclose(metric(y_true, y_pred),","531","                        metric(y_pred, y_true),","532","                        err_msg=\"%s is not symmetric\" % name)","535","@pytest.mark.parametrize(\"name\", sorted(NOT_SYMMETRIC_METRICS))","536","def test_not_symmetric_metric(name):","537","    # Test the symmetry of score and loss functions","538","    random_state = check_random_state(0)","539","    y_true = random_state.randint(0, 2, size=(20, ))","540","    y_pred = random_state.randint(0, 2, size=(20, ))","541","","542","    if name in METRICS_REQUIRE_POSITIVE_Y:","543","        y_true, y_pred = _require_positive_targets(y_true, y_pred)","544","","545","    metric = ALL_METRICS[name]","546","","547","    # use context manager to supply custom error message","548","    with assert_raises(AssertionError) as cm:","549","        assert_array_equal(metric(y_true, y_pred), metric(y_pred, y_true))","550","        cm.msg = (\"%s seems to be symmetric\" % name)","560","    if name in METRICS_REQUIRE_POSITIVE_Y:","561","        y_true, y_pred = _require_positive_targets(y_true, y_pred)","562","","616","    if name in METRICS_REQUIRE_POSITIVE_Y:","617","        y1, y2 = _require_positive_targets(y1, y2)","618","","807","    if name in METRICS_REQUIRE_POSITIVE_Y:","808","        values = [1, 2]","809","    else:","810","        values = [0, 1]","811","    for i, j in product(values, repeat=2):"],"delete":["436","    \"cohen_kappa_score\",","458","    \"macro_recall_score\", \"log_loss\", \"hinge_loss\"","471","@ignore_warnings","472","def test_symmetry():","473","    # Test the symmetry of score and loss functions","474","    random_state = check_random_state(0)","475","    y_true = random_state.randint(0, 2, size=(20, ))","476","    y_pred = random_state.randint(0, 2, size=(20, ))","478","    y_true_bin = random_state.randint(0, 2, size=(20, 25))","479","    y_pred_bin = random_state.randint(0, 2, size=(20, 25))","491","    # Symmetric metric","492","    for name in SYMMETRIC_METRICS:","493","        metric = ALL_METRICS[name]","494","        if name in METRIC_UNDEFINED_BINARY:","495","            if name in MULTILABELS_METRICS:","496","                assert_allclose(metric(y_true_bin, y_pred_bin),","497","                                metric(y_pred_bin, y_true_bin),","498","                                err_msg=\"%s is not symmetric\" % name)","499","            else:","500","                assert False, \"This case is currently unhandled\"","501","        else:","502","            assert_allclose(metric(y_true, y_pred),","503","                            metric(y_pred, y_true),","506","    # Not symmetric metrics","507","    for name in NOT_SYMMETRIC_METRICS:","508","        metric = ALL_METRICS[name]","510","        # use context manager to supply custom error message","511","        with assert_raises(AssertionError) as cm:","512","            assert_array_equal(metric(y_true, y_pred), metric(y_pred, y_true))","513","            cm.msg = (\"%s seems to be symmetric\" % name)","764","    for i, j in product([0, 1], repeat=2):"]}],"doc\/modules\/classes.rst":[{"add":["905","   metrics.mean_poisson_deviance","906","   metrics.mean_gamma_deviance","907","   metrics.mean_tweedie_deviance"],"delete":[]}],"doc\/modules\/model_evaluation.rst":[{"add":["93","'neg_mean_poisson_deviance'       :func:`metrics.mean_poisson_deviance`","94","'neg_mean_gamma_deviance'         :func:`metrics.mean_gamma_deviance`","1961","","1962",".. _mean_tweedie_deviance:","1963","","1964","Mean Poisson, Gamma, and Tweedie deviances","1965","------------------------------------------","1966","The :func:`mean_tweedie_deviance` function computes the `mean Tweedie","1967","deviance error","1968","<https:\/\/en.wikipedia.org\/wiki\/Tweedie_distribution#The_Tweedie_deviance>`_","1969","with power parameter `p`. This is a metric that elicits predicted expectation","1970","values of regression targets.","1971","","1972","Following special cases exist,","1973","","1974","- when `p=0` it is equivalent to :func:`mean_squared_error`.","1975","- when `p=1` it is equivalent to :func:`mean_poisson_deviance`.","1976","- when `p=2` it is equivalent to :func:`mean_gamma_deviance`.","1977","","1978","If :math:`\\hat{y}_i` is the predicted value of the :math:`i`-th sample,","1979","and :math:`y_i` is the corresponding true value, then the mean Tweedie","1980","deviance error (D) estimated over :math:`n_{\\text{samples}}` is defined as","1981","","1982",".. math::","1983","","1984","  \\text{D}(y, \\hat{y}) = \\frac{1}{n_\\text{samples}}","1985","  \\sum_{i=0}^{n_\\text{samples} - 1}","1986","  \\begin{cases}","1987","  (y_i-\\hat{y}_i)^2, & \\text{for }p=0\\text{ (Normal)}\\\\","1988","  2(y_i \\log(y\/\\hat{y}_i) + \\hat{y}_i - y_i),  & \\text{for }p=1\\text{ (Poisson)}\\\\","1989","  2(\\log(\\hat{y}_i\/y_i) + y_i\/\\hat{y}_i - 1),  & \\text{for }p=2\\text{ (Gamma)}\\\\","1990","  2\\left(\\frac{\\max(y_i,0)^{2-p}}{(1-p)(2-p)}-","1991","  \\frac{y\\,\\hat{y}^{1-p}_i}{1-p}+\\frac{\\hat{y}^{2-p}_i}{2-p}\\right),","1992","  & \\text{otherwise}","1993","  \\end{cases}","1994","","1995","Tweedie deviance is a homogeneous function of degree ``2-p``.","1996","Thus, Gamma distribution with `p=2` means that simultaneously scaling `y_true`","1997","and `y_pred` has no effect on the deviance. For Poisson distribution `p=1`","1998","the deviance scales linearly, and for Normal distribution (`p=0`),","1999","quadratically.  In general, the higher `p` the less weight is given to extreme","2000","deviations between true and predicted targets.","2001","","2002","For instance, let's compare the two predictions 1.0 and 100 that are both","2003","50% of their corresponding true value.","2004","","2005","The mean squared error (``p=0``) is very sensitive to the","2006","prediction difference of the second point,::","2007","","2008","    >>> from sklearn.metrics import mean_tweedie_deviance","2009","    >>> mean_tweedie_deviance([1.0], [1.5], p=0)","2010","    0.25","2011","    >>> mean_tweedie_deviance([100.], [150.], p=0)","2012","    2500.0","2013","","2014","If we increase ``p`` to 1,::","2015","","2016","    >>> mean_tweedie_deviance([1.0], [1.5], p=1)","2017","    0.18...","2018","    >>> mean_tweedie_deviance([100.], [150.], p=1)","2019","    18.9...","2020","","2021","the difference in errors decreases. Finally, by setting, ``p=2``::","2022","","2023","    >>> mean_tweedie_deviance([1.0], [1.5], p=2)","2024","    0.14...","2025","    >>> mean_tweedie_deviance([100.], [150.], p=2)","2026","    0.14...","2027","","2028","we would get identical errors. The deviance when `p=2` is thus only","2029","sensitive to relative errors.","2030",""],"delete":[]}],"sklearn\/metrics\/__init__.py":[{"add":["66","from .regression import mean_tweedie_deviance","67","from .regression import mean_poisson_deviance","68","from .regression import mean_gamma_deviance","115","    'mean_poisson_deviance',","116","    'mean_gamma_deviance',","117","    'mean_tweedie_deviance',"],"delete":[]}],"doc\/whats_new\/v0.22.rst":[{"add":["133","  ","134","- |Feature| Add :class:`metrics.mean_tweedie_deviance` measuring the","135","  Tweedie deviance for a power parameter ``p``. Also add mean Poisson deviance","136","  :class:`metrics.mean_poisson_deviance` and mean Gamma deviance","137","  :class:`metrics.mean_gamma_deviance` that are special cases of the Tweedie","138","  deviance for `p=1` and `p=2` respectively.","139","  :pr:`13938` by :user:`Christian Lorentzen <lorentzenchr>` and","140","  `Roman Yurchak`_."],"delete":[]}],"sklearn\/metrics\/tests\/test_regression.py":[{"add":["2","from numpy.testing import assert_allclose","18","from sklearn.metrics import mean_tweedie_deviance","38","    assert_almost_equal(mean_tweedie_deviance(y_true, y_pred, p=0),","39","                        mean_squared_error(y_true, y_pred))","40","","41","    # Tweedie deviance needs positive y_pred, except for p=0,","42","    # p>=2 needs positive y_true","43","    # results evaluated by sympy","44","    y_true = np.arange(1, 1 + n_samples)","45","    y_pred = 2 * y_true","46","    n = n_samples","47","    assert_almost_equal(mean_tweedie_deviance(y_true, y_pred, p=-1),","48","                        5\/12 * n * (n**2 + 2 * n + 1))","49","    assert_almost_equal(mean_tweedie_deviance(y_true, y_pred, p=1),","50","                        (n + 1) * (1 - np.log(2)))","51","    assert_almost_equal(mean_tweedie_deviance(y_true, y_pred, p=2),","52","                        2 * np.log(2) - 1)","53","    assert_almost_equal(mean_tweedie_deviance(y_true, y_pred, p=3\/2),","54","                        ((6 * np.sqrt(2) - 8) \/ n) * np.sqrt(y_true).sum())","55","    assert_almost_equal(mean_tweedie_deviance(y_true, y_pred, p=3),","56","                        np.sum(1 \/ y_true) \/ (4 * n))","98","    # Tweedie deviance error","99","    p = -1.2","100","    assert_allclose(mean_tweedie_deviance([0], [1.], p=p),","101","                    2.\/(2.-p), rtol=1e-3)","102","    with pytest.raises(ValueError,","103","                       match=\"can only be used on strictly positive y_pred.\"):","104","        mean_tweedie_deviance([0.], [0.], p=p)","105","    assert_almost_equal(mean_tweedie_deviance([0.], [0.], p=0), 0.00, 2)","106","","107","    msg = \"only be used on non-negative y_true and strictly positive y_pred.\"","108","    with pytest.raises(ValueError, match=msg):","109","        mean_tweedie_deviance([0.], [0.], p=1.0)","110","","111","    p = 1.5","112","    assert_allclose(mean_tweedie_deviance([0.], [1.], p=p), 2.\/(2.-p))","113","    msg = \"only be used on non-negative y_true and strictly positive y_pred.\"","114","    with pytest.raises(ValueError, match=msg):","115","        mean_tweedie_deviance([0.], [0.], p=p)","116","    p = 2.","117","    assert_allclose(mean_tweedie_deviance([1.], [1.], p=p), 0.00,","118","                    atol=1e-8)","119","    msg = \"can only be used on strictly positive y_true and y_pred.\"","120","    with pytest.raises(ValueError, match=msg):","121","        mean_tweedie_deviance([0.], [0.], p=p)","122","    p = 3.","123","    assert_allclose(mean_tweedie_deviance([1.], [1.], p=p),","124","                    0.00, atol=1e-8)","125","","126","    msg = \"can only be used on strictly positive y_true and y_pred.\"","127","    with pytest.raises(ValueError, match=msg):","128","        mean_tweedie_deviance([0.], [0.], p=p)","129","","130","    with pytest.raises(ValueError,","131","                       match=\"deviance is only defined for p<=0 and p>=1.\"):","132","        mean_tweedie_deviance([0.], [0.], p=0.5)","133","","261","","262","","263","def test_tweedie_deviance_continuity():","264","    n_samples = 100","265","","266","    y_true = np.random.RandomState(0).rand(n_samples) + 0.1","267","    y_pred = np.random.RandomState(1).rand(n_samples) + 0.1","268","","269","    assert_allclose(mean_tweedie_deviance(y_true, y_pred, p=0 - 1e-10),","270","                    mean_tweedie_deviance(y_true, y_pred, p=0))","271","","272","    # Ws we get closer to the limit, with 1e-12 difference the absolute","273","    # tolerance to pass the below check increases. There are likely","274","    # numerical precision issues on the edges of different definition","275","    # regions.","276","    assert_allclose(mean_tweedie_deviance(y_true, y_pred, p=1 + 1e-10),","277","                    mean_tweedie_deviance(y_true, y_pred, p=1),","278","                    atol=1e-6)","279","","280","    assert_allclose(mean_tweedie_deviance(y_true, y_pred, p=2 - 1e-10),","281","                    mean_tweedie_deviance(y_true, y_pred, p=2),","282","                    atol=1e-6)","283","","284","    assert_allclose(mean_tweedie_deviance(y_true, y_pred, p=2 + 1e-10),","285","                    mean_tweedie_deviance(y_true, y_pred, p=2),","286","                    atol=1e-6)"],"delete":[]}]}},"1b119c46937f29b1b29fb8eaaee6910beb7807d0":{"changes":{"doc\/authors.rst":"MODIFY","build_tools\/generate_authors_table.py":"MODIFY"},"diff":{"doc\/authors.rst":[{"add":["9","    <p>J¨¦r¨¦mie du Boisberranger<\/p>"],"delete":["9","    <p>J¨¦r¨¦mie Du Boisberranger<\/p>"]}],"build_tools\/generate_authors_table.py":[{"add":["13","from os import path","21","REPO_FOLDER = Path(path.abspath(__file__)).parent.parent"],"delete":["20","REPO_FOLDER = Path(__file__).parent.parent","102","        'jeremiedbb': 'J¨¦r¨¦mie Du Boisberranger',"]}]}},"ca78d75e751d8d57c08fb48fc8f437d18d454e43":{"changes":{"sklearn\/datasets\/_openml.py":"MODIFY","sklearn\/tree\/_classes.py":"MODIFY","sklearn\/datasets\/_rcv1.py":"MODIFY","doc\/datasets\/index.rst":"MODIFY","sklearn\/datasets\/_base.py":"MODIFY","sklearn\/datasets\/_california_housing.py":"MODIFY","sklearn\/datasets\/_species_distributions.py":"MODIFY","sklearn\/ensemble\/_stacking.py":"MODIFY","sklearn\/inspection\/_permutation_importance.py":"MODIFY","sklearn\/utils\/__init__.py":"MODIFY","sklearn\/datasets\/_covtype.py":"MODIFY","sklearn\/datasets\/_lfw.py":"MODIFY","sklearn\/ensemble\/_voting.py":"MODIFY","sklearn\/datasets\/_olivetti_faces.py":"MODIFY","sklearn\/pipeline.py":"MODIFY","sklearn\/datasets\/_kddcup99.py":"MODIFY","sklearn\/compose\/_column_transformer.py":"MODIFY","sklearn\/datasets\/_twenty_newsgroups.py":"MODIFY"},"diff":{"sklearn\/datasets\/_openml.py":[{"add":["581","    data : :class:`~sklearn.utils.Bunch`","582","        Dictionary-like object, with the following attributes."],"delete":["581","    data : Bunch","582","        Dictionary-like object, with attributes:"]}],"sklearn\/tree\/_classes.py":[{"add":["547","        ccp_path : :class:`~sklearn.utils.Bunch`","548","            Dictionary-like object, with the following attributes."],"delete":["547","        ccp_path : Bunch","548","            Dictionary-like object, with attributes:"]}],"sklearn\/datasets\/_rcv1.py":[{"add":["129","    dataset : :class:`~sklearn.utils.Bunch`","130","        Dictionary-like object, with the following attributes.","132","        data : scipy csr array, dtype np.float64, shape (804414, 47236)","133","            The array has 0.16% of non zero values.","134","        target : scipy csr array, dtype np.uint8, shape (804414, 103)","135","            Each sample has a value of 1 in its categories, and 0 in others.","136","            The array has 3.15% of non zero values.","137","        sample_id : numpy array, dtype np.uint32, shape (804414,)","138","            Identification number of each sample, as ordered in dataset.data.","139","        target_names : numpy array, dtype object, length (103)","140","            Names of each target (RCV1 topics), as ordered in dataset.target.","141","        DESCR : string","142","            Description of the RCV1 dataset."],"delete":["129","    dataset : dict-like object with the following attributes:","131","    dataset.data : scipy csr array, dtype np.float64, shape (804414, 47236)","132","        The array has 0.16% of non zero values.","133","","134","    dataset.target : scipy csr array, dtype np.uint8, shape (804414, 103)","135","        Each sample has a value of 1 in its categories, and 0 in others.","136","        The array has 3.15% of non zero values.","137","","138","    dataset.sample_id : numpy array, dtype np.uint32, shape (804414,)","139","        Identification number of each sample, as ordered in dataset.data.","140","","141","    dataset.target_names : numpy array, dtype object, length (103)","142","        Names of each target (RCV1 topics), as ordered in dataset.target.","143","","144","    dataset.DESCR : string","145","        Description of the RCV1 dataset."]}],"doc\/datasets\/index.rst":[{"add":["23","There are three main kinds of dataset interfaces that can be used to get","25","","26","**The dataset loaders.** They can be used to load small standard datasets,","27","described in the :ref:`toy_datasets` section.","32","Both loaders and fetchers functions return a :class:`sklearn.utils.Bunch`","33","object holding at least two items:","34","an array of shape ``n_samples`` * ``n_features`` with","35","key ``data`` (except for 20newsgroups) and a numpy array of","38","The Bunch object is a dictionary that exposes its keys are attributes.","39","For more information about Bunch object, see :class:`sklearn.utils.Bunch`:","40","","42","to be a tuple containing only the data and the target, by setting the","45","The datasets also contain a full description in their ``DESCR`` attribute and","46","some contain ``feature_names`` and ``target_names``. See the dataset","47","descriptions below for details.","49","**The dataset generation functions.** They can be used to generate controlled","56","In addition, there are also miscellaneous tools to load datasets of other","58","section.","65","scikit-learn comes with a few small standard datasets that do not require to","66","download any file from some external website.","491","Here are some recommended ways to load standard columnar data into a","492","format usable by scikit-learn:","493","","494","* `pandas.io <https:\/\/pandas.pydata.org\/pandas-docs\/stable\/io.html>`_","499","* `scipy.io <https:\/\/docs.scipy.org\/doc\/scipy\/reference\/io.html>`_","500","  specializes in binary formats often used in scientific computing","514","  `Imageio <https:\/\/imageio.readthedocs.io\/en\/latest\/userapi.html>`_","516","* `scipy.io.wavfile.read","517","  <https:\/\/docs.scipy.org\/doc\/scipy-0.14.0\/reference\/generated\/scipy.io.wavfile.read.html>`_","520","Categorical (or nominal) features stored as strings (common in pandas DataFrames)","525","Note: if you manage your own numerical data it is recommended to use an","527","such as H5Py, PyTables and pandas provides a Python interface for reading and"],"delete":["23","There are three main kinds of dataset interfaces that can be used to get ","25","  ","26","**The dataset loaders.** They can be used to load small standard datasets, ","27","described in the :ref:`toy_datasets` section.  ","32","Both loaders and fetchers functions return a dictionary-like object holding ","33","at least two items: an array of shape ``n_samples`` * ``n_features`` with ","34","key ``data`` (except for 20newsgroups) and a numpy array of ","38","to be a tuple containing only the data and the target, by setting the ","41","The datasets also contain a full description in their ``DESCR`` attribute and ","42","some contain ``feature_names`` and ``target_names``. See the dataset ","43","descriptions below for details.  ","45","**The dataset generation functions.** They can be used to generate controlled ","52","In addition, there are also miscellaneous tools to load datasets of other ","54","section. ","61","scikit-learn comes with a few small standard datasets that do not require to ","62","download any file from some external website. ","486"," ","487","Here are some recommended ways to load standard columnar data into a ","488","format usable by scikit-learn: ","490","* `pandas.io <https:\/\/pandas.pydata.org\/pandas-docs\/stable\/io.html>`_ ","495","* `scipy.io <https:\/\/docs.scipy.org\/doc\/scipy\/reference\/io.html>`_ ","496","  specializes in binary formats often used in scientific computing ","510","  `Imageio <https:\/\/imageio.readthedocs.io\/en\/latest\/userapi.html>`_ ","512","* `scipy.io.wavfile.read ","513","  <https:\/\/docs.scipy.org\/doc\/scipy-0.14.0\/reference\/generated\/scipy.io.wavfile.read.html>`_ ","516","Categorical (or nominal) features stored as strings (common in pandas DataFrames) ","521","Note: if you manage your own numerical data it is recommended to use an ","523","such as H5Py, PyTables and pandas provides a Python interface for reading and "]}],"sklearn\/datasets\/_base.py":[{"add":["165","    data : :class:`~sklearn.utils.Bunch`","166","        Dictionary-like object, with the following attributes.","167","","168","        data : list of str","169","            Only present when `load_content=True`.","170","            The raw text data to learn.","171","        target : ndarray","172","            The target labels (integer index).","173","        target_names : list","174","            The names of target classes.","175","        DESCR : str","176","            The full description of the dataset.","177","        filenames: ndarray","178","            The filenames holding the dataset.","305","    data : :class:`~sklearn.utils.Bunch`","306","        Dictionary-like object, with the following attributes.","419","    data : :class:`~sklearn.utils.Bunch`","420","        Dictionary-like object, with the following attributes.","531","    data : :class:`~sklearn.utils.Bunch`","532","        Dictionary-like object, with the following attributes.","655","    data : :class:`~sklearn.utils.Bunch`","656","        Dictionary-like object, with the following attributes.","769","    data : :class:`~sklearn.utils.Bunch`","770","        Dictionary-like object, with the following attributes.","863","    data : :class:`~sklearn.utils.Bunch`","864","        Dictionary-like object, with the following attributes.","953","    data : :class:`~sklearn.utils.Bunch`","954","        Dictionary-like object, with the following attributes.","955","","956","        data : ndarray of shape (506, 13)","957","            The data matrix.","958","        target : ndarray of shape (506, )","959","            The regression target.","960","        filename : str","961","            The physical location of boston csv dataset.","962","","963","            .. versionadded:: 0.20","964","        DESCR : str","965","            The full description of the dataset.","966","        feature_names : ndarray","967","            The names of features","1026","    data : :class:`~sklearn.utils.Bunch`","1027","        Dictionary-like object, with the following attributes.","1028","","1029","        images : list of ndarray of shape (427, 640, 3)","1030","            The two sample image.","1031","        filenames : list","1032","            The filenames for the images.","1033","        DESCR : str","1034","            The full description of the dataset."],"delete":["165","    data : Bunch","166","        Dictionary-like object, the interesting attributes are: either","167","        data, the raw text data to learn, or 'filenames', the files","168","        holding it, 'target', the classification labels (integer index),","169","        'target_names', the meaning of the labels, and 'DESCR', the full","170","        description of the dataset.","297","    data : Bunch","298","        Dictionary-like object, with attributes:","411","    data : Bunch","412","        Dictionary-like object, with attributes:","523","    data : Bunch","524","        Dictionary-like object, with attributes:","647","    data : Bunch","648","        Dictionary-like object, with attributes:","761","    data : Bunch","762","        Dictionary-like object, with attributes:","855","    data : Bunch","856","        Dictionary-like object, with attributes:","945","    data : Bunch","946","        Dictionary-like object, the interesting attributes are:","947","        'data', the data to learn, 'target', the regression targets,","948","        'DESCR', the full description of the dataset,","949","        and 'filename', the physical location of boston","950","        csv dataset (added in version `0.20`).","1009","    data : Bunch","1010","        Dictionary-like object with the following attributes : 'images', the","1011","        two sample images, 'filenames', the file names for the images, and","1012","        'DESCR' the full description of the dataset."]}],"sklearn\/datasets\/_california_housing.py":[{"add":["89","    dataset : :class:`~sklearn.utils.Bunch`","90","        Dictionary-like object, with the following attributes.","92","        data : ndarray, shape (20640, 8)","93","            Each row corresponding to the 8 feature values in order.","94","            If ``as_frame`` is True, ``data`` is a pandas object.","95","        target : numpy array of shape (20640,)","96","            Each value corresponds to the average","97","            house value in units of 100,000.","98","            If ``as_frame`` is True, ``target`` is a pandas object.","99","        feature_names : list of length 8","100","            Array of ordered feature names used in the dataset.","101","        DESCR : string","102","            Description of the California housing dataset."],"delete":["89","    dataset : dict-like object with the following attributes:","91","    dataset.data : ndarray, shape [20640, 8]","92","        Each row corresponding to the 8 feature values in order.","93","        If ``as_frame`` is True, ``data`` is a pandas object.","94","","95","    dataset.target : numpy array of shape (20640,)","96","        Each value corresponds to the average house value in units of 100,000.","97","        If ``as_frame`` is True, ``target`` is a pandas object.","98","","99","    dataset.feature_names : array of length 8","100","        Array of ordered feature names used in the dataset.","101","","102","    dataset.DESCR : string","103","        Description of the California housing dataset."]}],"sklearn\/datasets\/_species_distributions.py":[{"add":["157","    data : :class:`~sklearn.utils.Bunch`","158","        Dictionary-like object, with the following attributes.","160","        coverages : array, shape = [14, 1592, 1212]","161","            These represent the 14 features measured","162","            at each point of the map grid.","163","            The latitude\/longitude values for the grid are discussed below.","164","            Missing data is represented by the value -9999.","165","        train : record array, shape = (1624,)","166","            The training points for the data.  Each point has three fields:","168","            - train['species'] is the species name","169","            - train['dd long'] is the longitude, in degrees","170","            - train['dd lat'] is the latitude, in degrees","171","        test : record array, shape = (620,)","172","            The test points for the data.  Same format as the training data.","173","        Nx, Ny : integers","174","            The number of longitudes (x) and latitudes (y) in the grid","175","        x_left_lower_corner, y_left_lower_corner : floats","176","            The (x,y) position of the lower-left corner, in degrees","177","        grid_size : float","178","            The spacing between points of the grid, in degrees"],"delete":["157","    The data is returned as a Bunch object with the following attributes:","159","    coverages : array, shape = [14, 1592, 1212]","160","        These represent the 14 features measured at each point of the map grid.","161","        The latitude\/longitude values for the grid are discussed below.","162","        Missing data is represented by the value -9999.","164","    train : record array, shape = (1624,)","165","        The training points for the data.  Each point has three fields:","166","","167","        - train['species'] is the species name","168","        - train['dd long'] is the longitude, in degrees","169","        - train['dd lat'] is the latitude, in degrees","170","","171","    test : record array, shape = (620,)","172","        The test points for the data.  Same format as the training data.","173","","174","    Nx, Ny : integers","175","        The number of longitudes (x) and latitudes (y) in the grid","176","","177","    x_left_lower_corner, y_left_lower_corner : floats","178","        The (x,y) position of the lower-left corner, in degrees","179","","180","    grid_size : float","181","        The spacing between points of the grid, in degrees"]}],"sklearn\/ensemble\/_stacking.py":[{"add":["325","    named_estimators_ : :class:`~sklearn.utils.Bunch`","573","    named_estimators_ : :class:`~sklearn.utils.Bunch`","576",""],"delete":["325","    named_estimators_ : Bunch","573","    named_estimators_ : Bunch"]}],"sklearn\/inspection\/_permutation_importance.py":[{"add":["88","    result : :class:`~sklearn.utils.Bunch`","89","        Dictionary-like object, with the following attributes."],"delete":["88","    result : Bunch","89","        Dictionary-like object, with attributes:"]}],"sklearn\/utils\/__init__.py":[{"add":["66","    Examples","67","    --------"],"delete":["66","","78",""]}],"sklearn\/datasets\/_covtype.py":[{"add":["83","    dataset : :class:`~sklearn.utils.Bunch`","84","        Dictionary-like object, with the following attributes.","86","        data : numpy array of shape (581012, 54)","87","            Each row corresponds to the 54 features in the dataset.","88","        target : numpy array of shape (581012,)","89","            Each value corresponds to one of","90","            the 7 forest covertypes with values","91","            ranging between 1 to 7.","92","        DESCR : str","93","            Description of the forest covertype dataset."],"delete":["83","    dataset : dict-like object with the following attributes:","85","    dataset.data : numpy array of shape (581012, 54)","86","        Each row corresponds to the 54 features in the dataset.","87","","88","    dataset.target : numpy array of shape (581012,)","89","        Each value corresponds to one of the 7 forest covertypes with values","90","        ranging between 1 to 7.","91","","92","    dataset.DESCR : string","93","        Description of the forest covertype dataset."]}],"sklearn\/datasets\/_lfw.py":[{"add":["274","    dataset : :class:`~sklearn.utils.Bunch`","275","        Dictionary-like object, with the following attributes.","277","        data : numpy array of shape (13233, 2914)","278","            Each row corresponds to a ravelled face image","279","            of original size 62 x 47 pixels.","280","            Changing the ``slice_`` or resize parameters will change the","281","            shape of the output.","282","        images : numpy array of shape (13233, 62, 47)","283","            Each row is a face image corresponding to one of the 5749 people in","284","            the dataset. Changing the ``slice_``","285","            or resize parameters will change the shape of the output.","286","        target : numpy array of shape (13233,)","287","            Labels associated to each face image.","288","            Those labels range from 0-5748 and correspond to the person IDs.","289","        DESCR : string","290","            Description of the Labeled Faces in the Wild (LFW) dataset.","447","    data : :class:`~sklearn.utils.Bunch`","448","        Dictionary-like object, with the following attributes.","450","        data : ndarray of shape (2200, 5828). Shape depends on ``subset``.","451","            Each row corresponds to 2 ravel'd face images","452","            of original size 62 x 47 pixels.","453","            Changing the ``slice_``, ``resize`` or ``subset`` parameters","454","            will change the shape of the output.","455","        pairs : ndarray of shape (2200, 2, 62, 47). Shape depends on ``subset``","456","            Each row has 2 face images corresponding","457","            to same or different person from the dataset","458","            containing 5749 people. Changing the ``slice_``,","459","            ``resize`` or ``subset`` parameters will change the shape of the","460","            output.","461","        target : numpy array of shape (2200,). Shape depends on ``subset``.","462","            Labels associated to each pair of images.","463","            The two label values being different persons or the same person.","464","        DESCR : string","465","            Description of the Labeled Faces in the Wild (LFW) dataset."],"delete":["274","    dataset : dict-like object with the following attributes:","276","    dataset.data : numpy array of shape (13233, 2914)","277","        Each row corresponds to a ravelled face image of original size 62 x 47","278","        pixels. Changing the ``slice_`` or resize parameters will change the","279","        shape of the output.","280","","281","    dataset.images : numpy array of shape (13233, 62, 47)","282","        Each row is a face image corresponding to one of the 5749 people in","283","        the dataset. Changing the ``slice_`` or resize parameters will change","284","        the shape of the output.","285","","286","    dataset.target : numpy array of shape (13233,)","287","        Labels associated to each face image. Those labels range from 0-5748","288","        and correspond to the person IDs.","289","","290","    dataset.DESCR : string","291","        Description of the Labeled Faces in the Wild (LFW) dataset.","448","    The data is returned as a Bunch object with the following attributes:","450","    data : numpy array of shape (2200, 5828). Shape depends on ``subset``.","451","        Each row corresponds to 2 ravel'd face images of original size 62 x 47","452","        pixels. Changing the ``slice_``, ``resize`` or ``subset`` parameters","453","        will change the shape of the output.","454","","455","    pairs : numpy array of shape (2200, 2, 62, 47). Shape depends on ``subset``","456","        Each row has 2 face images corresponding to same or different person","457","        from the dataset containing 5749 people. Changing the ``slice_``,","458","        ``resize`` or ``subset`` parameters will change the shape of the","459","        output.","460","","461","    target : numpy array of shape (2200,). Shape depends on ``subset``.","462","        Labels associated to each pair of images. The two label values being","463","        different persons or the same person.","464","","465","    DESCR : string","466","        Description of the Labeled Faces in the Wild (LFW) dataset."]}],"sklearn\/ensemble\/_voting.py":[{"add":["144","    named_estimators_ : :class:`~sklearn.utils.Bunch`","147",""],"delete":["144","    named_estimators_ : Bunch"]}],"sklearn\/datasets\/_olivetti_faces.py":[{"add":["79","    data : :class:`~sklearn.utils.Bunch`","80","        Dictionary-like object, with the following attributes.","81","","82","        data: ndarray, shape (400, 4096)","83","            Each row corresponds to a ravelled","84","            face image of original size 64 x 64 pixels.","85","        images : ndarray, shape (400, 64, 64)","86","            Each row is a face image","87","            corresponding to one of the 40 subjects of the dataset.","88","        target : ndarray, shape (400,)","89","            Labels associated to each face image.","90","            Those labels are ranging from 0-39 and correspond to the","91","            Subject IDs.","92","        DESCR : str","93","            Description of the modified Olivetti Faces Dataset."],"delete":["79","    bunch : Bunch object with the following attributes:","80","        - data: ndarray, shape (400, 4096). Each row corresponds to a ravelled","81","          face image of original size 64 x 64 pixels.","82","        - images : ndarray, shape (400, 64, 64). Each row is a face image","83","          corresponding to one of the 40 subjects of the dataset.","84","        - target : ndarray, shape (400,). Labels associated to each face image.","85","          Those labels are ranging from 0-39 and correspond to the","86","          Subject IDs.","87","        - DESCR : string. Description of the modified Olivetti Faces Dataset."]}],"sklearn\/pipeline.py":[{"add":["74","    named_steps : :class:`~sklearn.utils.Bunch`","75","        Dictionary-like object, with the following attributes."],"delete":["74","    named_steps : bunch object, a dictionary with attribute access"]}],"sklearn\/datasets\/_kddcup99.py":[{"add":["98","    data : :class:`~sklearn.utils.Bunch`","99","        Dictionary-like object, with the following attributes.","100","","101","        data : ndarray of shape (494021, 41)","102","            The data matrix to learn.","103","        target : ndarray of shape (494021,)","104","            The regression target for each sample.","105","        DESCR : str","106","            The full description of the dataset.","196","    dataset : :class:`~sklearn.utils.Bunch`","197","        Dictionary-like object, with the following attributes.","198","","199","        data : numpy array of shape (494021, 41)","201","        target : numpy array of shape (494021,)","204","        DESCR : string"],"delete":["98","    data : Bunch","99","        Dictionary-like object, the interesting attributes are:","100","         - 'data', the data to learn.","101","         - 'target', the regression target for each sample.","102","         - 'DESCR', a description of the dataset.","192","    dataset : dict-like object with the following attributes:","193","        dataset.data : numpy array of shape (494021, 41)","195","        dataset.target : numpy array of shape (494021,)","198","        dataset.DESCR : string"]}],"sklearn\/compose\/_column_transformer.py":[{"add":["126","    named_transformers_ : :class:`~sklearn.utils.Bunch`"],"delete":["126","    named_transformers_ : Bunch"]}],"sklearn\/datasets\/_twenty_newsgroups.py":[{"add":["216","    bunch : :class:`~sklearn.utils.Bunch`","217","        Dictionary-like object, with the following attributes.","218","","219","        data : list, length [n_samples]","220","            The data list to learn.","221","        target: array, shape [n_samples]","222","            The target labels.","223","        filenames: list, length [n_samples]","224","            The path to the location of the data.","225","        DESCR: str","226","            The full description of the dataset.","227","        target_names: list, length [n_classes]","228","            The names of target classes.","392","    bunch : :class:`~sklearn.utils.Bunch`","393","        Dictionary-like object, with the following attributes.","394","","395","        data: sparse matrix, shape [n_samples, n_features]","396","            The data matrix to learn.","397","        target: array, shape [n_samples]","398","            The target labels.","399","        target_names: list, length [n_classes]","400","            The names of target classes.","401","        DESCR: str","402","            The full description of the dataset."],"delete":["216","    bunch : Bunch object with the following attribute:","217","        - data: list, length [n_samples]","218","        - target: array, shape [n_samples]","219","        - filenames: list, length [n_samples]","220","        - DESCR: a description of the dataset.","221","        - target_names: a list of categories of the returned data,","222","          length [n_classes]. This depends on the `categories` parameter.","386","    bunch : Bunch object with the following attribute:","387","        - bunch.data: sparse matrix, shape [n_samples, n_features]","388","        - bunch.target: array, shape [n_samples]","389","        - bunch.target_names: a list of categories of the returned data,","390","          length [n_classes].","391","        - bunch.DESCR: a description of the dataset."]}]}},"1d3a553b2dfbe5cc8d32b306fe62855671fe9ae4":{"changes":{"sklearn\/linear_model\/tests\/test_coordinate_descent.py":"MODIFY","doc\/whats_new\/v0.23.rst":"MODIFY","sklearn\/linear_model\/_coordinate_descent.py":"MODIFY"},"diff":{"sklearn\/linear_model\/tests\/test_coordinate_descent.py":[{"add":["9","import joblib","10","from distutils.version import LooseVersion","13","from sklearn.datasets import make_regression","1025","","1026","","1027","@pytest.mark.parametrize(\"backend\", [\"loky\", \"threading\"])","1028","@pytest.mark.parametrize(\"estimator\",","1029","                         [ElasticNetCV, MultiTaskElasticNetCV,","1030","                          LassoCV, MultiTaskLassoCV])","1031","def test_linear_models_cv_fit_for_all_backends(backend, estimator):","1032","    # LinearModelsCV.fit performs inplace operations on input data which is","1033","    # memmapped when using loky backend, causing an error due to unexpected","1034","    # behavior of fancy indexing of read-only memmaps (cf. numpy#14132).","1035","","1036","    if joblib.__version__ < LooseVersion('0.12') and backend == 'loky':","1037","        pytest.skip('loky backend does not exist in joblib <0.12')","1038","","1039","    # Create a problem sufficiently large to cause memmapping (1MB).","1040","    n_targets = 1 + (estimator in (MultiTaskElasticNetCV, MultiTaskLassoCV))","1041","    X, y = make_regression(20000, 10, n_targets=n_targets)","1042","","1043","    with joblib.parallel_backend(backend=backend):","1044","        estimator(n_jobs=2, cv=3).fit(X, y)"],"delete":[]}],"doc\/whats_new\/v0.23.rst":[{"add":["386","- |Fix| Fixed a bug in :class:`linear_model.ElasticNetCV`,","387","  :class:`linear_model.MultitaskElasticNetCV`, :class:`linear_model.LassoCV`","388","  and :class:`linear_model.MultitaskLassoCV` where fitting would fail when","389","  using joblib loky backend. :pr:`14264` by","390","  :user:`J¨¦r¨¦mie du Boisberranger <jeremiedbb>`.","391",""],"delete":[]}],"sklearn\/linear_model\/_coordinate_descent.py":[{"add":["1070","","1071","    if not sparse.issparse(X):","1072","        for array, array_input in ((X_train, X), (y_train, y),","1073","                                   (X_test, X), (y_test, y)):","1074","            if array.base is not array_input and not array.flags['WRITEABLE']:","1075","                # fancy indexing should create a writable copy but it doesn't","1076","                # for read-only memmaps (cf. numpy#14132).","1077","                array.setflags(write=True)","1078",""],"delete":[]}]}},"da0bdab4ac6841533e136c6ba24dd3bc0bb75188":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/inspection\/partial_dependence.py":"MODIFY","sklearn\/inspection\/tests\/test_partial_dependence.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["113",":mod:`sklearn.inspection`","114",".........................","115","","116","- |Fix| Fixed a bug in :func:`inspection.partial_dependence` to only check","117","  classifier and not regressor for the multiclass-multioutput case.","118","  :pr:`14309` by :user:`Guillaume Lemaitre <glemaitre>`.","119",""],"delete":[]}],"sklearn\/inspection\/partial_dependence.py":[{"add":["288","    if is_classifier(estimator):","289","        if not hasattr(estimator, 'classes_'):","290","            raise ValueError(","291","                \"'estimator' parameter must be a fitted estimator\"","292","            )","293","        if isinstance(estimator.classes_[0], np.ndarray):","294","            raise ValueError(","295","                'Multiclass-multioutput estimators are not supported'","296","            )"],"delete":["288","    if (hasattr(estimator, 'classes_') and","289","            isinstance(estimator.classes_[0], np.ndarray)):","290","        raise ValueError('Multiclass-multioutput estimators are not supported')"]}],"sklearn\/inspection\/tests\/test_partial_dependence.py":[{"add":["23","from sklearn.tree import DecisionTreeRegressor","61","    (DecisionTreeRegressor, 'brute', regression_data),","291","        # simulate that we have some classes","292","        self.classes_ = [0, 1]"],"delete":["263","@pytest.mark.filterwarnings('ignore:The default value of ')  # 0.22"]}]}},"496e7106fa8fff9e955620ec8b3b74d1bba59453":{"changes":{".github\/workflows\/labeler.yml":"MODIFY"},"diff":{".github\/workflows\/labeler.yml":[{"add":["8","    - uses: thomasjpfan\/labeler@v2.2.0"],"delete":["8","    - uses: thomasjpfan\/labeler@master"]}]}}}