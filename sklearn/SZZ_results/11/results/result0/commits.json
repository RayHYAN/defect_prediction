{"4e2e1fac0da592863b767356e30cee37f752b1bf":{"changes":{"sklearn\/preprocessing\/tests\/test_label.py":"MODIFY","doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/preprocessing\/label.py":"MODIFY"},"diff":{"sklearn\/preprocessing\/tests\/test_label.py":[{"add":["383","def test_multilabel_binarizer_multiple_calls():","384","    inp = [(2, 3), (1,), (1, 2)]","385","    indicator_mat = np.array([[0, 1, 1],","386","                              [1, 0, 0],","387","                              [1, 0, 1]])","388","","389","    indicator_mat2 = np.array([[0, 1, 1],","390","                               [1, 0, 0],","391","                               [1, 1, 0]])","392","","393","    # first call","394","    mlb = MultiLabelBinarizer(classes=[1, 3, 2])","395","    assert_array_equal(mlb.fit_transform(inp), indicator_mat)","396","    # second call change class","397","    mlb.classes = [1, 2, 3]","398","    assert_array_equal(mlb.fit_transform(inp), indicator_mat2)","399","","400",""],"delete":[]}],"doc\/whats_new\/v0.21.rst":[{"add":["49","  ","53","- |Efficiency| Make :class:`preprocessing.MultiLabelBinarizer` to cache class mappings instead ","54","  of calculating it every time on the fly.","55","  :issue:`12116` by :user:`Ekaterina Krivich <kiote>` and `Joel Nothman`_.","56",""],"delete":["49",""]}],"sklearn\/preprocessing\/label.py":[{"add":["826","        self._cached_dict = None","856","        self._cached_dict = None","857","","901","        class_to_index = self._build_cache()","909","    def _build_cache(self):","910","        if self._cached_dict is None:","911","            self._cached_dict = dict(zip(self.classes_,","912","                                         range(len(self.classes_))))","913","","914","        return self._cached_dict","915",""],"delete":["898","        class_to_index = dict(zip(self.classes_, range(len(self.classes_))))"]}]}},"680ab517dfec4c60857f503f50b5faf512d37c56":{"changes":{"sklearn\/base.py":"MODIFY","sklearn\/tests\/test_base.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/utils\/mocking.py":"MODIFY"},"diff":{"sklearn\/base.py":[{"add":["75","        if param1 is param2:","76","            # this should always happen","77","            continue","114","            # fall back on standard equality","115","            equality_test = param1 == param2","116","        if equality_test:","117","            warnings.warn(\"Estimator %s modifies parameters in __init__.\"","118","                          \" This behavior is deprecated as of 0.18 and \"","119","                          \"support for this behavior will be removed in 0.20.\"","120","                          % type(estimator).__name__, DeprecationWarning)","121","        else:"],"delete":["111","            new_obj_val = new_object_params[name]","112","            params_set_val = params_set[name]","113","            # The following construct is required to check equality on special","114","            # singletons such as np.nan that are not equal to them-selves:","115","            equality_test = (new_obj_val == params_set_val or","116","                             new_obj_val is params_set_val)","117","        if not equality_test:"]}],"sklearn\/tests\/test_base.py":[{"add":["14","from sklearn.utils.testing import assert_warns_message","22","from sklearn.base import TransformerMixin","23","from sklearn.utils.mocking import MockDataFrame","24","","47","class ModifyInitParams(BaseEstimator):","48","    \"\"\"Deprecated behavior.","49","    Equal parameters but with a type cast.","50","    Doesn't fulfill a is a","51","    \"\"\"","52","    def __init__(self, a=np.array([0])):","53","        self.a = a.copy()","54","","55","","160","def test_clone_copy_init_params():","161","    # test for deprecation warning when copying or casting an init parameter","162","    est = ModifyInitParams()","163","    message = (\"Estimator ModifyInitParams modifies parameters in __init__. \"","164","               \"This behavior is deprecated as of 0.18 and support \"","165","               \"for this behavior will be removed in 0.20.\")","166","","167","    assert_warns_message(DeprecationWarning, message, clone, est)","168","","169","","277","","278","","279","def test_clone_pandas_dataframe():","280","","281","    class DummyEstimator(BaseEstimator, TransformerMixin):","282","        \"\"\"This is a dummy class for generating numerical features","283","","284","        This feature extractor extracts numerical features from pandas data","285","        frame.","286","","287","        Parameters","288","        ----------","289","","290","        df: pandas data frame","291","            The pandas data frame parameter.","292","","293","        Notes","294","        -----","295","        \"\"\"","296","        def __init__(self, df=None, scalar_param=1):","297","            self.df = df","298","            self.scalar_param = scalar_param","299","","300","        def fit(self, X, y=None):","301","            pass","302","","303","        def transform(self, X, y=None):","304","            pass","305","","306","    # build and clone estimator","307","    d = np.arange(10)","308","    df = MockDataFrame(d)","309","    e = DummyEstimator(df, scalar_param=1)","310","    cloned_e = clone(e)","311","","312","    # the test","313","    assert_true((e.df == cloned_e.df).values.all())","314","    assert_equal(e.scalar_param, cloned_e.scalar_param)"],"delete":[]}],"doc\/whats_new.rst":[{"add":["295","   - Simplification of the ``clone`` function, deprecate support for estimators","296","     that modify parameters in ``__init__``.","297","     (`#5540 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/5540>_`)","298","     By `Andreas M¨¹ller`_.","299",""],"delete":[]}],"sklearn\/utils\/mocking.py":[{"add":["20","        self.values = array","35","    def __eq__(self, other):","36","        return MockDataFrame(self.array == other.array)","37",""],"delete":[]}]}},"e405505877c8597a82a10ef46a731c11ab565293":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/cluster\/_k_means.pyx":"MODIFY","sklearn\/cluster\/tests\/test_k_means.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["92","- |Fix| Fixed a bug in :class:`KMeans` where empty clusters weren't correctly","93","  relocated when using sample weights. :issue:`13486`","94","  by :user:`J¨¦r¨¦mie du Boisberranger <jeremiedbb>`.","95",""],"delete":[]}],"sklearn\/cluster\/_k_means.pyx":[{"add":["311","            new_center = X[far_index] * sample_weight[far_index]"],"delete":["311","            new_center = X[far_index]"]}],"sklearn\/cluster\/tests\/test_k_means.py":[{"add":["11","from sklearn.utils.testing import assert_allclose","925","","926","","927","def test_k_means_empty_cluster_relocated():","928","    # check that empty clusters are correctly relocated when using sample","929","    # weights (#13486)","930","    X = np.array([[-1], [1]])","931","    sample_weight = [1.9, 0.1]","932","    init = np.array([[-1], [10]])","933","","934","    km = KMeans(n_clusters=2, init=init, n_init=1)","935","    km.fit(X, sample_weight=sample_weight)","936","","937","    assert len(set(km.labels_)) == 2","938","    assert_allclose(km.cluster_centers_, [[-1], [1]])"],"delete":[]}]}},"62a0bcdbe133a4ede5616996ec8ba0044d1bf47b":{"changes":{"sklearn\/decomposition\/tests\/test_fastica.py":"MODIFY"},"diff":{"sklearn\/decomposition\/tests\/test_fastica.py":[{"add":["5","import pytest","53","@pytest.mark.parametrize(\"add_noise\", [True, False])","54","@pytest.mark.parametrize(\"seed\", range(1))","55","def test_fastica_simple(add_noise, seed):","57","    rng = np.random.RandomState(seed)","87","            k_, mixing_, s_ = fastica(m.T, fun=nl, algorithm=algo,","88","                                      random_state=rng)","92","            pca = PCA(n_components=2, whiten=True, random_state=rng)","93","            X = pca.fit_transform(m.T)","94","            k_, mixing_, s_ = fastica(X, fun=nl, algorithm=algo, whiten=False,","95","                                      random_state=rng)","121","    _, _, sources_fun = fastica(m.T, fun=nl, algorithm=algo,","122","                                random_state=seed)","123","    ica = FastICA(fun=nl, algorithm=algo, random_state=seed)","134","        ica = FastICA(fun=fn, algorithm=algo)"],"delete":["52","def test_fastica_simple(add_noise=False):","54","    rng = np.random.RandomState(0)","84","            k_, mixing_, s_ = fastica(m.T, fun=nl, algorithm=algo)","88","            X = PCA(n_components=2, whiten=True).fit_transform(m.T)","89","            k_, mixing_, s_ = fastica(X, fun=nl, algorithm=algo, whiten=False)","115","    _, _, sources_fun = fastica(m.T, fun=nl, algorithm=algo, random_state=0)","116","    ica = FastICA(fun=nl, algorithm=algo, random_state=0)","127","        ica = FastICA(fun=fn, algorithm=algo, random_state=0)"]}]}},"1888a96908301a9d71e37ce8c08bfd64298e9192":{"changes":{"sklearn\/utils\/validation.py":"MODIFY","sklearn\/utils\/_mocking.py":"MODIFY","sklearn\/ensemble\/_weight_boosting.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY","sklearn\/ensemble\/tests\/test_gradient_boosting.py":"MODIFY","sklearn\/ensemble\/tests\/test_weight_boosting.py":"MODIFY"},"diff":{"sklearn\/utils\/validation.py":[{"add":["1064","            sample_weight, accept_sparse=False, ensure_2d=False, dtype=dtype,","1065","            order=\"C\""],"delete":["1064","                sample_weight, accept_sparse=False,","1065","                ensure_2d=False, dtype=dtype, order=\"C\""]}],"sklearn\/utils\/_mocking.py":[{"add":["137","","138","","139","class NoSampleWeightWrapper(BaseEstimator):","140","    \"\"\"Wrap estimator which will not expose `sample_weight`.","141","","142","    Parameters","143","    ----------","144","    est : estimator, default=None","145","        The estimator to wrap.","146","    \"\"\"","147","    def __init__(self, est=None):","148","        self.est = est","149","","150","    def fit(self, X, y):","151","        return self.est.fit(X, y)","152","","153","    def predict(self, X):","154","        return self.est.predict(X)","155","","156","    def predict_proba(self, X):","157","        return self.est.predict_proba(X)","158","","159","    def _more_tags(self):","160","        return {'_skip_test': True}  # pragma: no cover"],"delete":[]}],"sklearn\/ensemble\/_weight_boosting.py":[{"add":["40","from ..utils.validation import _check_sample_weight","120","        sample_weight = _check_sample_weight(sample_weight, X, np.float64)","121","        sample_weight \/= sample_weight.sum()","122","        if np.any(sample_weight < 0):","123","            raise ValueError(\"sample_weight cannot contain negative weights\")","1022","        bootstrap_idx = random_state.choice(","1023","            np.arange(_num_samples(X)), size=_num_samples(X), replace=True,","1024","            p=sample_weight","1025","        )","1035","        sample_mask = sample_weight > 0","1036","        masked_sample_weight = sample_weight[sample_mask]","1037","        masked_error_vector = error_vect[sample_mask]","1039","        error_max = masked_error_vector.max()","1040","        if error_max != 0:","1041","            masked_error_vector \/= error_max","1044","            masked_error_vector **= 2","1046","            masked_error_vector = 1. - np.exp(-masked_error_vector)","1049","        estimator_error = (masked_sample_weight * masked_error_vector).sum()","1067","            sample_weight[sample_mask] *= np.power(","1068","                beta, (1. - masked_error_vector) * self.learning_rate","1069","            )"],"delete":["119","        if sample_weight is None:","120","            # Initialize weights to 1 \/ n_samples","121","            sample_weight = np.empty(_num_samples(X), dtype=np.float64)","122","            sample_weight[:] = 1. \/ _num_samples(X)","123","        else:","124","            sample_weight = check_array(sample_weight, ensure_2d=False)","125","            # Normalize existing weights","126","            sample_weight = sample_weight \/ sample_weight.sum(dtype=np.float64)","127","","128","            # Check that the sample weights sum is positive","129","            if sample_weight.sum() <= 0:","130","                raise ValueError(","131","                    \"Attempting to fit with a non-positive \"","132","                    \"weighted number of samples.\")","1031","        # For NumPy >= 1.7.0 use np.random.choice","1032","        cdf = stable_cumsum(sample_weight)","1033","        cdf \/= cdf[-1]","1034","        uniform_samples = random_state.random_sample(_num_samples(X))","1035","        bootstrap_idx = cdf.searchsorted(uniform_samples, side='right')","1036","        # searchsorted returns a scalar","1037","        bootstrap_idx = np.array(bootstrap_idx, copy=False)","1047","        error_max = error_vect.max()","1049","        if error_max != 0.:","1050","            error_vect \/= error_max","1053","            error_vect **= 2","1055","            error_vect = 1. - np.exp(- error_vect)","1058","        estimator_error = (sample_weight * error_vect).sum()","1076","            sample_weight *= np.power(","1077","                beta,","1078","                (1. - error_vect) * self.learning_rate)"]}],"doc\/whats_new\/v0.22.rst":[{"add":["257","- |Fix| :class:`ensemble.AdaBoostRegressor` where the loss should be normalized","258","  by the max of the samples with non-null weights only.","259","  :pr:`14294` by :user:`Guillaume Lemaitre <glemaitre>`.","260",""],"delete":[]}],"sklearn\/ensemble\/tests\/test_gradient_boosting.py":[{"add":["26","from sklearn.utils._mocking import NoSampleWeightWrapper","1319","    init_est = NoSampleWeightWrapper(init_estimator())"],"delete":["1294","class _NoSampleWeightWrapper(BaseEstimator):","1295","    def __init__(self, est):","1296","        self.est = est","1297","","1298","    def fit(self, X, y):","1299","        self.est.fit(X, y)","1300","","1301","    def predict(self, X):","1302","        return self.est.predict(X)","1303","","1304","    def predict_proba(self, X):","1305","        return self.est.predict_proba(X)","1306","","1307","","1332","    init_est = _NoSampleWeightWrapper(init_estimator())"]}],"sklearn\/ensemble\/tests\/test_weight_boosting.py":[{"add":["10","","11","from sklearn.utils.testing import assert_array_equal, assert_array_less","12","from sklearn.utils.testing import assert_array_almost_equal","13","from sklearn.utils.testing import assert_raises, assert_raises_regexp","14","","15","from sklearn.base import BaseEstimator","16","from sklearn.base import clone","17","from sklearn.dummy import DummyClassifier, DummyRegressor","18","from sklearn.linear_model import LinearRegression","19","from sklearn.model_selection import train_test_split","20","from sklearn.model_selection import GridSearchCV","21","from sklearn.ensemble import AdaBoostClassifier","22","from sklearn.ensemble import AdaBoostRegressor","23","from sklearn.ensemble._weight_boosting import _samme_proba","27","from sklearn.utils._mocking import NoSampleWeightWrapper","144","@pytest.mark.parametrize('loss', ['linear', 'square', 'exponential'])","145","def test_boston(loss):","147","    reg = AdaBoostRegressor(loss=loss, random_state=0)","500","@pytest.mark.parametrize(\"algorithm\", ['SAMME', 'SAMME.R'])","501","def test_adaboostclassifier_without_sample_weight(algorithm):","502","    X, y = iris.data, iris.target","503","    base_estimator = NoSampleWeightWrapper(DummyClassifier())","504","    clf = AdaBoostClassifier(","505","        base_estimator=base_estimator, algorithm=algorithm","506","    )","507","    err_msg = (\"{} doesn't support sample_weight\"","508","               .format(base_estimator.__class__.__name__))","509","    with pytest.raises(ValueError, match=err_msg):","510","        clf.fit(X, y)","511","","512","","513","def test_adaboostregressor_sample_weight():","514","    # check that giving weight will have an influence on the error computed","515","    # for a weak learner","516","    rng = np.random.RandomState(42)","517","    X = np.linspace(0, 100, num=1000)","518","    y = (.8 * X + 0.2) + (rng.rand(X.shape[0]) * 0.0001)","519","    X = X.reshape(-1, 1)","520","","521","    # add an arbitrary outlier","522","    X[-1] *= 10","523","    y[-1] = 10000","524","","525","    # random_state=0 ensure that the underlying boostrap will use the outlier","526","    regr_no_outlier = AdaBoostRegressor(","527","        base_estimator=LinearRegression(), n_estimators=1, random_state=0","528","    )","529","    regr_with_weight = clone(regr_no_outlier)","530","    regr_with_outlier = clone(regr_no_outlier)","531","","532","    # fit 3 models:","533","    # - a model containing the outlier","534","    # - a model without the outlier","535","    # - a model containing the outlier but with a null sample-weight","536","    regr_with_outlier.fit(X, y)","537","    regr_no_outlier.fit(X[:-1], y[:-1])","538","    sample_weight = np.ones_like(y)","539","    sample_weight[-1] = 0","540","    regr_with_weight.fit(X, y, sample_weight=sample_weight)","541","","542","    score_with_outlier = regr_with_outlier.score(X[:-1], y[:-1])","543","    score_no_outlier = regr_no_outlier.score(X[:-1], y[:-1])","544","    score_with_weight = regr_with_weight.score(X[:-1], y[:-1])","545","","546","    assert score_with_outlier < score_no_outlier","547","    assert score_with_outlier < score_with_weight","548","    assert score_no_outlier == pytest.approx(score_with_weight)","549","","565","","566","","567","@pytest.mark.parametrize(","568","    'model, X, y',","569","    [(AdaBoostClassifier(), iris.data, iris.target),","570","     (AdaBoostRegressor(), boston.data, boston.target)]","571",")","572","def test_adaboost_negative_weight_error(model, X, y):","573","    sample_weight = np.ones_like(y)","574","    sample_weight[-1] = -10","575","","576","    err_msg = \"sample_weight cannot contain negative weight\"","577","    with pytest.raises(ValueError, match=err_msg):","578","        model.fit(X, y, sample_weight=sample_weight)"],"delete":["5","from sklearn.utils.testing import assert_array_equal, assert_array_less","6","from sklearn.utils.testing import assert_array_almost_equal","7","from sklearn.utils.testing import assert_raises, assert_raises_regexp","8","","9","from sklearn.base import BaseEstimator","10","from sklearn.model_selection import train_test_split","11","from sklearn.model_selection import GridSearchCV","12","from sklearn.ensemble import AdaBoostClassifier","13","from sklearn.ensemble import AdaBoostRegressor","14","from sklearn.ensemble._weight_boosting import _samme_proba","139","def test_boston():","141","    reg = AdaBoostRegressor(random_state=0)","306","def test_sample_weight_missing():","307","    from sklearn.cluster import KMeans","308","","309","    clf = AdaBoostClassifier(KMeans(), algorithm=\"SAMME\")","310","    assert_raises(ValueError, clf.fit, X, y_regr)","311","","312","    clf = AdaBoostRegressor(KMeans())","313","    assert_raises(ValueError, clf.fit, X, y_regr)","314","","315","","488","","489","    from sklearn.dummy import DummyClassifier, DummyRegressor","490",""]}]}},"28b1e00665d0fd1590186f0f58734d8e9ace4468":{"changes":{"sklearn\/linear_model\/_coordinate_descent.py":"MODIFY"},"diff":{"sklearn\/linear_model\/_coordinate_descent.py":[{"add":["400","        If set to False, the input validation checks are skipped (including the","401","        Gram matrix when provided). It is assumed that they are handled","402","        by the caller."],"delete":["400","        Skip input validation checks, including the Gram matrix when provided","401","        assuming there are handled by the caller when check_input=False."]}]}},"fc7d6e698668b983cee2867b1bf3c65f1384e4cf":{"changes":{"examples\/multioutput\/plot_classifier_chain_yeast.py":"MODIFY",".circleci\/config.yml":"MODIFY","README.rst":"MODIFY","examples\/linear_model\/plot_logistic.py":"MODIFY","examples\/ensemble\/plot_voting_probas.py":"MODIFY","examples\/linear_model\/plot_iris_logistic.py":"MODIFY","examples\/linear_model\/plot_logistic_path.py":"MODIFY","examples\/neural_networks\/plot_rbm_logistic_classification.py":"MODIFY","examples\/ensemble\/plot_feature_transformation.py":"MODIFY","examples\/linear_model\/plot_logistic_l1_l2_sparsity.py":"MODIFY","examples\/compose\/plot_digits_pipe.py":"MODIFY","examples\/exercises\/plot_digits_classification_exercise.py":"MODIFY","examples\/calibration\/plot_compare_calibration.py":"MODIFY","examples\/compose\/plot_column_transformer_mixed_types.py":"MODIFY","examples\/classification\/plot_classification_probability.py":"MODIFY"},"diff":{"examples\/multioutput\/plot_classifier_chain_yeast.py":[{"add":["56","base_lr = LogisticRegression(solver='lbfgs')","57","ovr = OneVsRestClassifier(base_lr)","64","chains = [ClassifierChain(base_lr, order='random', random_state=i)"],"delete":["56","ovr = OneVsRestClassifier(LogisticRegression())","63","chains = [ClassifierChain(LogisticRegression(), order='random', random_state=i)"]}],".circleci\/config.yml":[{"add":["43","      - PYTHON_VERSION: \"2\"","44","      - NUMPY_VERSION: \"1.10\"","45","      - SCIPY_VERSION: \"0.16\"","46","      - MATPLOTLIB_VERSION: \"1.4\"","47","      - SCIKIT_IMAGE_VERSION: \"0.11\"","48","      - PANDAS_VERSION: \"0.17.1\""],"delete":["43","      - PYTHON_VERSION: 2","44","      - NUMPY_VERSION: 1.8.2","45","      # XXX: plot_gpc_xor.py fails with scipy 0.13.3","46","      - SCIPY_VERSION: 0.14","47","      - MATPLOTLIB_VERSION: 1.3","48","      - SCIKIT_IMAGE_VERSION: 0.9.3","49","      - PANDAS_VERSION: 0.13.1"]}],"README.rst":[{"add":["58","For running the examples Matplotlib >= 1.4 is required. A few examples","59","require scikit-image >= 0.11.3 and a few examples require pandas >= 0.17.1."],"delete":["58","For running the examples Matplotlib >= 1.3.1 is required. A few examples","59","require scikit-image >= 0.9.3 and a few examples require pandas >= 0.13.1."]}],"examples\/linear_model\/plot_logistic.py":[{"add":["25","# General a toy dataset:s it's just a straight line with some Gaussian noise:","35","","36","# Fit the classifier","37","clf = linear_model.LogisticRegression(C=1e5, solver='lbfgs')","49","","50","","67","plt.tight_layout()"],"delete":["25","# this is our test set, it's just a straight line with some","26","# Gaussian noise","36","# run the classifier","37","clf = linear_model.LogisticRegression(C=1e5)"]}],"examples\/ensemble\/plot_voting_probas.py":[{"add":["31","clf1 = LogisticRegression(solver='lbfgs', max_iter=1000, random_state=123)","81","plt.tight_layout()"],"delete":["31","clf1 = LogisticRegression(random_state=123)"]}],"examples\/linear_model\/plot_iris_logistic.py":[{"add":["9","first two dimensions (sepal length and width) of the `iris","10","<https:\/\/en.wikipedia.org\/wiki\/Iris_flower_data_set>`_ dataset. The datapoints","11","are colored according to their labels.","22","from sklearn.linear_model import LogisticRegression","23","from sklearn import datasets","30","logreg = LogisticRegression(C=1e5, solver='lbfgs', multi_class='multinomial')","39","h = .02  # step size in the mesh"],"delete":["9","`iris <https:\/\/en.wikipedia.org\/wiki\/Iris_flower_data_set>`_ dataset. The","10","datapoints are colored according to their labels.","15","","22","from sklearn import linear_model, datasets","29","h = .02  # step size in the mesh","30","","31","logreg = linear_model.LogisticRegression(C=1e5)"]}],"examples\/linear_model\/plot_logistic_path.py":[{"add":["2","==============================================","3","Regularization path of L1- Logistic Regression","4","==============================================","6","","7","Train l1-penalized logistic regression models on a binary classification","8","problem derived from the Iris dataset.","9","","10","The models are ordered from strongest regularized to least regularized. The 4","11","coefficients of the models are collected and plotted as a \"regularization","12","path\": on the left-hand side of the figure (strong regularizers), all the","13","coefficients are exactly 0. When regularization gets progressively looser,","14","coefficients can get non-zero values one after the other.","15","","16","Here we choose the SAGA solver because it can efficiently optimize for the","17","Logistic Regression loss with a non-smooth, sparsity inducing l1 penalty.","18","","19","Also note that we set a low value for the tolerance to make sure that the model","20","has converged before collecting the coefficients.","21","","22","We also use warm_start=True which means that the coefficients of the models are","23","reused to initialize the next model fit to speed-up the computation of the","24","full-path.","32","from time import time","47","X \/= X.max()  # Normalize X to speed-up convergence","52","cs = l1_min_c(X, y, loss='log') * np.logspace(0, 7, 16)","56","start = time()","57","clf = linear_model.LogisticRegression(penalty='l1', solver='saga',","58","                                      tol=1e-6, max_iter=int(1e6),","59","                                      warm_start=True)","65","print(\"This took %0.3fs\" % (time() - start))","68","plt.plot(np.log10(cs), coefs_, marker='o')"],"delete":["2","=================================","3","Path with L1- Logistic Regression","4","=================================","6","Computes path on IRIS dataset.","14","from datetime import datetime","29","X -= np.mean(X, 0)","34","cs = l1_min_c(X, y, loss='log') * np.logspace(0, 3)","38","start = datetime.now()","39","clf = linear_model.LogisticRegression(C=1.0, penalty='l1', tol=1e-6)","45","print(\"This took \", datetime.now() - start)","48","plt.plot(np.log10(cs), coefs_)"]}],"examples\/neural_networks\/plot_rbm_logistic_classification.py":[{"add":["42","from sklearn.base import clone","70","    def shift(x, w):","71","        return convolve(x.reshape((8, 8)), mode='constant', weights=w).ravel()","72","","79","","86","X_train, X_test, Y_train, Y_test = train_test_split(","87","    X, Y, test_size=0.2, random_state=0)","90","logistic = linear_model.LogisticRegression(solver='lbfgs', max_iter=10000,","91","                                           multi_class='multinomial')","94","rbm_features_classifier = Pipeline(","95","    steps=[('rbm', rbm), ('logistic', logistic)])","108","logistic.C = 6000","111","rbm_features_classifier.fit(X_train, Y_train)","113","# Training the Logistic regression classifier directly on the pixel","114","raw_pixel_classifier = clone(logistic)","115","raw_pixel_classifier.C = 100.","116","raw_pixel_classifier.fit(X_train, Y_train)","121","Y_pred = rbm_features_classifier.predict(X_test)","123","    metrics.classification_report(Y_test, Y_pred)))","125","Y_pred = raw_pixel_classifier.predict(X_test)","127","    metrics.classification_report(Y_test, Y_pred)))"],"delete":["69","    shift = lambda x, w: convolve(x.reshape((8, 8)), mode='constant',","70","                                  weights=w).ravel()","83","X_train, X_test, Y_train, Y_test = train_test_split(X, Y,","84","                                                    test_size=0.2,","85","                                                    random_state=0)","88","logistic = linear_model.LogisticRegression()","91","classifier = Pipeline(steps=[('rbm', rbm), ('logistic', logistic)])","104","logistic.C = 6000.0","107","classifier.fit(X_train, Y_train)","109","# Training Logistic regression","110","logistic_classifier = linear_model.LogisticRegression(C=100.0)","111","logistic_classifier.fit(X_train, Y_train)","116","print()","118","    metrics.classification_report(","119","        Y_test,","120","        classifier.predict(X_test))))","123","    metrics.classification_report(","124","        Y_test,","125","        logistic_classifier.predict(X_test))))"]}],"examples\/ensemble\/plot_feature_transformation.py":[{"add":["44","","49","X_train, X_train_lr, y_train, y_train_lr = train_test_split(","50","    X_train, y_train, test_size=0.5)","56","rt_lm = LogisticRegression(solver='lbfgs', max_iter=1000)","65","rf_lm = LogisticRegression(solver='lbfgs', max_iter=1000)","73","# Supervised transformation based on gradient boosted trees","76","grd_lm = LogisticRegression(solver='lbfgs', max_iter=1000)"],"delete":["48","X_train, X_train_lr, y_train, y_train_lr = train_test_split(X_train,","49","                                                            y_train,","50","                                                            test_size=0.5)","56","rt_lm = LogisticRegression()","65","rf_lm = LogisticRegression()","75","grd_lm = LogisticRegression()","84","","89",""]}],"examples\/linear_model\/plot_logistic_l1_l2_sparsity.py":[{"add":["39","for i, C in enumerate((1, 0.1, 0.01)):","41","    clf_l1_LR = LogisticRegression(C=C, penalty='l1', tol=0.01, solver='saga')","42","    clf_l2_LR = LogisticRegression(C=C, penalty='l2', tol=0.01, solver='saga')"],"delete":["39","for i, C in enumerate((100, 1, 0.01)):","41","    clf_l1_LR = LogisticRegression(C=C, penalty='l1', tol=0.01)","42","    clf_l2_LR = LogisticRegression(C=C, penalty='l2', tol=0.01)"]}],"examples\/compose\/plot_digits_pipe.py":[{"add":["24","import pandas as pd","26","from sklearn import datasets","27","from sklearn.decomposition import PCA","28","from sklearn.linear_model import SGDClassifier","33","# Define a pipeline to search for the best combination of PCA truncation","34","# and classifier regularization.","35","logistic = SGDClassifier(loss='log', penalty='l2', early_stopping=True,","36","                         max_iter=10000, tol=1e-5, random_state=0)","37","pca = PCA()","44","# Parameters of pipelines can be set using \u2018__\u2019 separated parameter names:","45","param_grid = {","46","    'pca__n_components': [5, 20, 30, 40, 50, 64],","47","    'logistic__alpha': np.logspace(-4, 4, 5),","48","}","49","search = GridSearchCV(pipe, param_grid, iid=False, cv=5,","50","                      return_train_score=False)","51","search.fit(X_digits, y_digits)","52","print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)","53","print(search.best_params_)","54","","58","fig, (ax0, ax1) = plt.subplots(nrows=2, sharex=True, figsize=(6, 6))","59","ax0.plot(pca.explained_variance_ratio_, linewidth=2)","60","ax0.set_ylabel('PCA explained variance')","62","ax0.axvline(search.best_estimator_.named_steps['pca'].n_components,","64","ax0.legend(prop=dict(size=12))","65","","66","# For each number of components, find the best classifier results","67","results = pd.DataFrame(search.cv_results_)","68","components_col = 'param_pca__n_components'","69","best_clfs = results.groupby(components_col).apply(","70","    lambda g: g.nlargest(1, 'mean_test_score'))","71","","72","best_clfs.plot(x=components_col, y='mean_test_score', yerr='std_test_score',","73","               legend=False, ax=ax1)","74","ax1.set_ylabel('Classification accuracy (val)')","75","ax1.set_xlabel('n_components')","76","","77","plt.tight_layout()"],"delete":["25","from sklearn import linear_model, decomposition, datasets","29","logistic = linear_model.LogisticRegression()","31","pca = decomposition.PCA()","41","plt.figure(1, figsize=(4, 3))","42","plt.clf()","43","plt.axes([.2, .2, .7, .7])","44","plt.plot(pca.explained_variance_, linewidth=2)","45","plt.axis('tight')","46","plt.xlabel('n_components')","47","plt.ylabel('explained_variance_')","49","# Prediction","50","n_components = [20, 40, 64]","51","Cs = np.logspace(-4, 4, 3)","52","","53","# Parameters of pipelines can be set using \u2018__\u2019 separated parameter names:","54","estimator = GridSearchCV(pipe,","55","                         dict(pca__n_components=n_components,","56","                              logistic__C=Cs), cv=5)","57","estimator.fit(X_digits, y_digits)","58","","59","plt.axvline(estimator.best_estimator_.named_steps['pca'].n_components,","61","plt.legend(prop=dict(size=12))"]}],"examples\/exercises\/plot_digits_classification_exercise.py":[{"add":["17","X_digits = digits.data \/ digits.data.max()","28","logistic = linear_model.LogisticRegression(solver='lbfgs', max_iter=1000,","29","                                           multi_class='multinomial')"],"delete":["17","X_digits = digits.data","28","logistic = linear_model.LogisticRegression()"]}],"examples\/calibration\/plot_compare_calibration.py":[{"add":["77","lr = LogisticRegression(solver='lbfgs')"],"delete":["77","lr = LogisticRegression()"]}],"examples\/compose\/plot_column_transformer_mixed_types.py":[{"add":["73","                      ('classifier', LogisticRegression(solver='lbfgs'))])"],"delete":["73","                      ('classifier', LogisticRegression())])"]}],"examples\/classification\/plot_classification_probability.py":[{"add":["5","Plot the classification probability for different classifiers. We use a 3 class","6","dataset, and we classify it with a Support Vector classifier, L1 and L2","7","penalized logistic regression with either a One-Vs-Rest or multinomial setting,","8","and Gaussian process classification.","10","Linear SVC is not a probabilistic classifier by default but it has a built-in","11","calibration option enabled in this example (`probability=True`).","12","","13","The logistic regression with One-Vs-Rest is not a multiclass classifier out of","14","the box. As a result it has more trouble in separating class 2 and 3 than the","15","other estimators.","25","from sklearn.metrics import accuracy_score","38","C = 10","41","# Create different classifiers.","42","classifiers = {","43","    'L1 logistic': LogisticRegression(C=C, penalty='l1',","44","                                      solver='saga',","45","                                      multi_class='multinomial',","46","                                      max_iter=10000),","47","    'L2 logistic (Multinomial)': LogisticRegression(C=C, penalty='l2',","48","                                                    solver='saga',","49","                                                    multi_class='multinomial',","50","                                                    max_iter=10000),","51","    'L2 logistic (OvR)': LogisticRegression(C=C, penalty='l2',","52","                                            solver='saga',","53","                                            multi_class='ovr',","54","                                            max_iter=10000),","55","    'Linear SVC': SVC(kernel='linear', C=C, probability=True,","56","                      random_state=0),","57","    'GPC': GaussianProcessClassifier(kernel)","58","}","74","    accuracy = accuracy_score(y, y_pred)","75","    print(\"Accuracy (train) for %s: %0.1f%% \" % (name, accuracy * 100))","77","    # View probabilities:"],"delete":["5","Plot the classification probability for different classifiers. We use a 3","6","class dataset, and we classify it with a Support Vector classifier, L1","7","and L2 penalized logistic regression with either a One-Vs-Rest or multinomial","8","setting, and Gaussian process classification.","10","The logistic regression is not a multiclass classifier out of the box. As","11","a result it can identify only the first class.","33","C = 1.0","36","# Create different classifiers. The logistic regression cannot do","37","# multiclass out of the box.","38","classifiers = {'L1 logistic': LogisticRegression(C=C, penalty='l1'),","39","               'L2 logistic (OvR)': LogisticRegression(C=C, penalty='l2'),","40","               'Linear SVC': SVC(kernel='linear', C=C, probability=True,","41","                                 random_state=0),","42","               'L2 logistic (Multinomial)': LogisticRegression(","43","                C=C, solver='lbfgs', multi_class='multinomial'),","44","               'GPC': GaussianProcessClassifier(kernel)","45","               }","61","    classif_rate = np.mean(y_pred.ravel() == y.ravel()) * 100","62","    print(\"classif_rate for %s : %f \" % (name, classif_rate))","64","    # View probabilities="]}]}},"11934e1838b87936da98de31350d88158a8640bb":{"changes":{"sklearn\/cluster\/tests\/test_birch.py":"MODIFY","sklearn\/cluster\/_birch.py":"MODIFY","doc\/whats_new\/v0.23.rst":"MODIFY"},"diff":{"sklearn\/cluster\/tests\/test_birch.py":[{"add":["161","","162","","163","def test_birch_n_clusters_long_int():","164","    # Check that birch supports n_clusters with np.int64 dtype, for instance","165","    # coming from np.arange. #16484","166","    X, _ = make_blobs(random_state=0)","167","    n_clusters = np.int64(5)","168","    Birch(n_clusters=n_clusters).fit(X)"],"delete":[]}],"sklearn\/cluster\/_birch.py":[{"add":["6","import numbers","620","        if isinstance(clusterer, numbers.Integral):"],"delete":["619","        if isinstance(clusterer, int):"]}],"doc\/whats_new\/v0.23.rst":[{"add":["62","- |Fix| Fixed a bug in :class:`cluster.Birch` where the `n_clusters` parameter","63","  could not have a `np.int64` type. :pr:`16484`","64","  by :user:`Jeremie du Boisberranger <jeremiedbb>`.","65",""],"delete":[]}]}},"572e43d149aaeb29a35f4dd706ca851d38970b2d":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/metrics\/ranking.py":"MODIFY","sklearn\/metrics\/tests\/test_ranking.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["441","- |Fix| Fixed a bug in :func:`metrics.label_ranking_average_precision_score` ","442","  where sample_weight wasn't taken into account for samples with degenerate","443","  labels.","444","  :issue:`13447` by :user:`Dan Ellis <dpwe>`.","445",""],"delete":[]}],"sklearn\/metrics\/ranking.py":[{"add":["729","            aux = 1.","730","        else:","731","            scores_i = y_score[i]","732","            rank = rankdata(scores_i, 'max')[relevant]","733","            L = rankdata(scores_i[relevant], 'max')","734","            aux = (L \/ rank).mean()"],"delete":["729","            out += 1.","730","            continue","732","        scores_i = y_score[i]","733","        rank = rankdata(scores_i, 'max')[relevant]","734","        L = rankdata(scores_i[relevant], 'max')","735","        aux = (L \/ rank).mean()"]}],"sklearn\/metrics\/tests\/test_ranking.py":[{"add":["954","def test_lrap_sample_weighting_zero_labels():","955","    # Degenerate sample labeling (e.g., zero labels for a sample) is a valid","956","    # special case for lrap (the sample is considered to achieve perfect","957","    # precision), but this case is not tested in test_common.","958","    # For these test samples, the APs are 0.5, 0.75, and 1.0 (default for zero","959","    # labels).","960","    y_true = np.array([[1, 0, 0, 0], [1, 0, 0, 1], [0, 0, 0, 0]],","961","                      dtype=np.bool)","962","    y_score = np.array([[0.3, 0.4, 0.2, 0.1], [0.1, 0.2, 0.3, 0.4],","963","                        [0.4, 0.3, 0.2, 0.1]])","964","    samplewise_lraps = np.array([0.5, 0.75, 1.0])","965","    sample_weight = np.array([1.0, 1.0, 0.0])","966","","967","    assert_almost_equal(","968","        label_ranking_average_precision_score(y_true, y_score,","969","                                              sample_weight=sample_weight),","970","        np.sum(sample_weight * samplewise_lraps) \/ np.sum(sample_weight))","971","","972",""],"delete":[]}]}},"661a8b42bd93a4b77a14d89aa7b96c2df158cf9d":{"changes":{"sklearn\/compose\/tests\/test_column_transformer.py":"MODIFY","sklearn\/compose\/_column_transformer.py":"MODIFY"},"diff":{"sklearn\/compose\/tests\/test_column_transformer.py":[{"add":["455","                                 n_jobs=3, remainder='drop',","456","                                 sparse_threshold=0.3)","461","    assert_equal(ct.sparse_threshold, 0.3)"],"delete":["455","                                 n_jobs=3, remainder='drop')"]}],"sklearn\/compose\/_column_transformer.py":[{"add":["691","    sparse_threshold : float, default = 0.3","692","        If the transformed output consists of a mix of sparse and dense data,","693","        it will be stacked as a sparse matrix if the density is lower than this","694","        value. Use ``sparse_threshold=0`` to always return dense.","695","        When the transformed output consists of all sparse or all dense data,","696","        the stacked result will be sparse or dense, respectively, and this","697","        keyword will be ignored.","698","","735","    sparse_threshold = kwargs.pop('sparse_threshold', 0.3)","741","                             remainder=remainder,","742","                             sparse_threshold=sparse_threshold)"],"delete":["732","                             remainder=remainder)"]}]}},"ae0dcddce8f8bc6b4a8af16353c0e6e7cb362bc1":{"changes":{"benchmarks\/bench_hist_gradient_boosting.py":"MODIFY"},"diff":{"benchmarks\/bench_hist_gradient_boosting.py":[{"add":["81","                    early_stopping=False,"],"delete":["81","                    n_iter_no_change=None,"]}]}},"a35b892522499bfe7a0e5fdfdfbd15752e63fbb0":{"changes":{"sklearn\/datasets\/_openml.py":"MODIFY","doc\/developers\/contributing.rst":"MODIFY"},"diff":{"sklearn\/datasets\/_openml.py":[{"add":["508","        def postprocess(frame):  # type:ignore","529","        def postprocess(X, y, nominal_attributes):  # type:ignore"],"delete":["508","        def postprocess(frame):","529","        def postprocess(X, y, nominal_attributes):"]}],"doc\/developers\/contributing.rst":[{"add":["433","  must not produce new errors in your pull request. Using `# type: ignore`","434","  annotation can be a workaround for a few cases that are not supported by","435","  mypy, in particular,"],"delete":["433","  must not produce new errors in your pull request. Using `# type: ignore` annotation can be a workaround for a few cases that are not supported by mypy, in particular,"]}]}},"9f015c8a14a67d248599dc376d33ec612dd9dbb9":{"changes":{"sklearn\/utils\/validation.py":"MODIFY"},"diff":{"sklearn\/utils\/validation.py":[{"add":["509","                dtypes_orig[i] = np.dtype(np.object)"],"delete":["509","                dtypes_orig[i] = np.object"]}]}},"e8a42aee5d5b3f2d25f3dddbc0c55c96da03e230":{"changes":{"build_tools\/azure\/install.sh":"MODIFY"},"diff":{"build_tools\/azure\/install.sh":[{"add":[],"delete":["100","    # TODO: Remove pin when https:\/\/github.com\/python-pillow\/Pillow\/issues\/4518 gets fixed","101","    python -m pip install \"pillow>=4.3.0,!=7.1.0,!=7.1.1\"","102",""]}]}},"fb76de72e7560aaa739872e94bee761777e54c0a":{"changes":{"examples\/impute\/plot_missing_values.py":"MODIFY","doc\/developers\/contributing.rst":"MODIFY"},"diff":{"examples\/impute\/plot_missing_values.py":[{"add":["8","In this example we will investigate different imputation techniques:","10","- imputation by the constant value 0","11","- imputation by the mean value of each feature combined with a missing-ness","12","  indicator auxiliary variable","13","- k nearest neighbor imputation","14","- iterative imputation","16","We will use two datasets: Diabetes dataset which consists of 10 feature","17","variables collected from diabetes patients with an aim to predict disease","18","progression and California Housing dataset for which the target is the median","19","house value for California districts.","20","","21","As neither of these datasets have missing values, we will remove some","22","values to create new versions with artificially missing data. The performance","23","of","24",":class:`~sklearn.ensemble.RandomForestRegressor` on the full original dataset","25","is then compared the performance on the altered datasets with the artificially","26","missing values imputed using different techniques.","27","","31","# Authors: Maria Telenczuk  <https:\/\/github.com\/maikia>","32","# License: BSD 3 clause","34","###############################################################################","35","# Download the data and make missing values sets","36","################################################","37","#","38","# First we download the two datasets. Diabetes dataset is shipped with","39","# scikit-learn. It has 442 entries, each with 10 features. California Housing","40","# dataset is much larger with 20640 entries and 8 features. It needs to be","41","# downloaded. We will only use the first 400 entries for the sake of speeding","42","# up the calculations but feel free to use the whole dataset.","43","#","44","","45","import numpy as np","46","","47","from sklearn.datasets import fetch_california_housing","49","","50","","51","rng = np.random.RandomState(42)","52","","53","X_diabetes, y_diabetes = load_diabetes(return_X_y=True)","54","X_california, y_california = fetch_california_housing(return_X_y=True)","55","X_california = X_california[:400]","56","y_california = y_california[:400]","57","","58","","59","def add_missing_values(X_full, y_full):","60","    n_samples, n_features = X_full.shape","61","","62","    # Add missing values in 75% of the lines","63","    missing_rate = 0.75","64","    n_missing_samples = int(n_samples * missing_rate)","65","","66","    missing_samples = np.zeros(n_samples, dtype=np.bool)","67","    missing_samples[: n_missing_samples] = True","68","","69","    rng.shuffle(missing_samples)","70","    missing_features = rng.randint(0, n_features, n_missing_samples)","71","    X_missing = X_full.copy()","72","    X_missing[missing_samples, missing_features] = np.nan","73","    y_missing = y_full.copy()","74","","75","    return X_missing, y_missing","76","","77","","78","X_miss_california, y_miss_california = add_missing_values(","79","    X_california, y_california)","80","","81","X_miss_diabetes, y_miss_diabetes = add_missing_values(","82","    X_diabetes, y_diabetes)","83","","84","","85","###############################################################################","86","# Impute the missing data and score","87","# #################################","88","# Now we will write a function which will score the results on the differently","89","# imputed data. Let's look at each imputer separately:","90","#","94","from sklearn.ensemble import RandomForestRegressor","95","","96","# To use the experimental IterativeImputer, we need to explicitly ask for it:","97","from sklearn.experimental import enable_iterative_imputer  # noqa","98","from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer","99","from sklearn.model_selection import cross_val_score","100","from sklearn.pipeline import make_pipeline","101","","102","","104","regressor = RandomForestRegressor(random_state=0)","105","","106","###############################################################################","107","# Missing information","108","# -------------------","109","# In addition to imputing the missing values, the imputers have an","110","# `add_indicator` parameter that marks the values that were missing, which","111","# might carry some information.","112","#","116","    estimator = make_pipeline(imputer, regressor)","128","","129","mses_california = np.zeros(5)","130","stds_california = np.zeros(5)","131","mses_diabetes = np.zeros(5)","132","stds_diabetes = np.zeros(5)","133","","134","###############################################################################","135","# Estimate the score","136","# ------------------","137","# First, we want to estimate the score on the original data:","138","#","139","","140","","141","def get_full_score(X_full, y_full):","142","    full_scores = cross_val_score(regressor, X_full, y_full,","143","                                  scoring='neg_mean_squared_error',","144","                                  cv=N_SPLITS)","145","    return full_scores.mean(), full_scores.std()","146","","147","","148","mses_california[0], stds_california[0] = get_full_score(X_california,","149","                                                        y_california)","150","mses_diabetes[0], stds_diabetes[0] = get_full_score(X_diabetes, y_diabetes)","151","","152","","153","###############################################################################","154","# Replace missing values by 0","155","# ---------------------------","156","#","157","# Now we will estimate the score on the data where the missing values are","158","# replaced by 0:","159","#","160","","161","","162","def get_impute_zero_score(X_missing, y_missing):","163","","164","    imputer = SimpleImputer(missing_values=np.nan, add_indicator=True,","165","                            strategy='constant', fill_value=0)","166","    zero_impute_scores = get_scores_for_imputer(imputer, X_missing, y_missing)","167","    return zero_impute_scores.mean(), zero_impute_scores.std()","168","","169","","170","mses_california[1], stds_california[1] = get_impute_zero_score(","171","    X_miss_california, y_miss_california)","172","mses_diabetes[1], stds_diabetes[1] = get_impute_zero_score(X_miss_diabetes,","173","                                                           y_miss_diabetes)","174","","175","","176","###############################################################################","177","# kNN-imputation of the missing values","178","# ------------------------------------","179","#","180","# :class:`sklearn.impute.KNNImputer` imputes missing values using the weighted","181","# or unweighted mean of the desired number of nearest neighbors.","182","","183","def get_impute_knn_score(X_missing, y_missing):","184","    imputer = KNNImputer(missing_values=np.nan, add_indicator=True)","185","    knn_impute_scores = get_scores_for_imputer(imputer, X_missing, y_missing)","186","    return knn_impute_scores.mean(), knn_impute_scores.std()","187","","188","","189","mses_california[2], stds_california[2] = get_impute_knn_score(","190","    X_miss_california, y_miss_california)","191","mses_diabetes[2], stds_diabetes[2] = get_impute_knn_score(X_miss_diabetes,","192","                                                          y_miss_diabetes)","193","","194","","195","###############################################################################","196","# Impute missing values with mean","197","# -------------------------------","198","#","199","","200","def get_impute_mean(X_missing, y_missing):","201","    imputer = SimpleImputer(missing_values=np.nan, strategy=\"mean\",","202","                            add_indicator=True)","203","    mean_impute_scores = get_scores_for_imputer(imputer, X_missing, y_missing)","204","    return mean_impute_scores.mean(), mean_impute_scores.std()","205","","206","","207","mses_california[3], stds_california[3] = get_impute_mean(X_miss_california,","208","                                                         y_miss_california)","209","mses_diabetes[3], stds_diabetes[3] = get_impute_mean(X_miss_diabetes,","210","                                                     y_miss_diabetes)","211","","212","","213","###############################################################################","214","# Iterative imputation of the missing values","215","# ------------------------------------------","216","#","217","# Another option is the :class:`sklearn.impute.IterativeImputer`. This uses","218","# round-robin linear regression, modeling each feature with missing values as a","219","# function of other features, in turn.","220","# The version implemented assumes Gaussian (output) variables. If your features","221","# are obviously non-normal, consider transforming them to look more normal","222","# to potentially improve performance.","223","#","224","","225","def get_impute_iterative(X_missing, y_missing):","226","    imputer = IterativeImputer(missing_values=np.nan, add_indicator=True,","227","                               random_state=0, n_nearest_features=5,","228","                               sample_posterior=True)","229","    iterative_impute_scores = get_scores_for_imputer(imputer,","230","                                                     X_missing,","231","                                                     y_missing)","232","    return iterative_impute_scores.mean(), iterative_impute_scores.std()","233","","234","","235","mses_california[4], stds_california[4] = get_impute_iterative(","236","    X_miss_california, y_miss_california)","237","mses_diabetes[4], stds_diabetes[4] = get_impute_iterative(X_miss_diabetes,","238","                                                          y_miss_diabetes)","239","","240","mses_diabetes = mses_diabetes * -1","241","mses_california = mses_california * -1","242","","243","###############################################################################","244","# Plot the results","245","# ################","246","#","247","# Finally we are going to visualize the score:","248","#","249","","250","import matplotlib.pyplot as plt","251","","252","","253","n_bars = len(mses_diabetes)","254","xval = np.arange(n_bars)","255","","273","# plot california dataset results","276","    ax2.barh(j, mses_california[j], xerr=stds_california[j],","279","ax2.set_title('Imputation Techniques with California Data')","286","","287","# You can also try different techniques. For instance, the median is a more","288","# robust estimator for data with high magnitude variables which could dominate","289","# results (otherwise known as a 'long tail')."],"delete":["7","The median is a more robust estimator for data with high magnitude variables","8","which could dominate results (otherwise known as a 'long tail').","10","With ``KNNImputer``, missing values can be imputed using the weighted","11","or unweighted mean of the desired number of nearest neighbors.","13","Another option is the :class:`sklearn.impute.IterativeImputer`. This uses","14","round-robin linear regression, treating every variable as an output in","15","turn. The version implemented assumes Gaussian (output) variables. If your","16","features are obviously non-Normal, consider transforming them to look more","17","Normal so as to potentially improve performance.","19","In addition of using an imputing method, we can also keep an indication of the","20","missing information using :func:`sklearn.impute.MissingIndicator` which might","21","carry some information.","25","import numpy as np","26","import matplotlib.pyplot as plt","28","# To use the experimental IterativeImputer, we need to explicitly ask for it:","29","from sklearn.experimental import enable_iterative_imputer  # noqa","31","from sklearn.datasets import load_boston","32","from sklearn.ensemble import RandomForestRegressor","33","from sklearn.pipeline import make_pipeline, make_union","34","from sklearn.impute import (","35","    SimpleImputer, KNNImputer, IterativeImputer, MissingIndicator)","36","from sklearn.model_selection import cross_val_score","41","REGRESSOR = RandomForestRegressor(random_state=0)","45","    estimator = make_pipeline(","46","        make_union(imputer, MissingIndicator(missing_values=0)),","47","        REGRESSOR)","54","def get_results(dataset):","55","    X_full, y_full = dataset.data, dataset.target","56","    n_samples = X_full.shape[0]","57","    n_features = X_full.shape[1]","58","","59","    # Estimate the score on the entire dataset, with no missing values","60","    full_scores = cross_val_score(REGRESSOR, X_full, y_full,","61","                                  scoring='neg_mean_squared_error',","62","                                  cv=N_SPLITS)","63","","64","    # Add missing values in 75% of the lines","65","    missing_rate = 0.75","66","    n_missing_samples = int(np.floor(n_samples * missing_rate))","67","    missing_samples = np.hstack((np.zeros(n_samples - n_missing_samples,","68","                                          dtype=np.bool),","69","                                 np.ones(n_missing_samples,","70","                                         dtype=np.bool)))","71","    rng.shuffle(missing_samples)","72","    missing_features = rng.randint(0, n_features, n_missing_samples)","73","    X_missing = X_full.copy()","74","    X_missing[np.where(missing_samples)[0], missing_features] = 0","75","    y_missing = y_full.copy()","76","","77","    # Estimate the score after replacing missing values by 0","78","    imputer = SimpleImputer(missing_values=0,","79","                            strategy='constant',","80","                            fill_value=0)","81","    zero_impute_scores = get_scores_for_imputer(imputer, X_missing, y_missing)","82","","83","    # Estimate the score after imputation (mean strategy) of the missing values","84","    imputer = SimpleImputer(missing_values=0, strategy=\"mean\")","85","    mean_impute_scores = get_scores_for_imputer(imputer, X_missing, y_missing)","86","","87","    # Estimate the score after kNN-imputation of the missing values","88","    imputer = KNNImputer(missing_values=0)","89","    knn_impute_scores = get_scores_for_imputer(imputer, X_missing, y_missing)","90","","91","    # Estimate the score after iterative imputation of the missing values","92","    imputer = IterativeImputer(missing_values=0,","93","                               random_state=0,","94","                               n_nearest_features=5,","95","                               sample_posterior=True)","96","    iterative_impute_scores = get_scores_for_imputer(imputer,","97","                                                     X_missing,","98","                                                     y_missing)","99","","100","    return ((full_scores.mean(), full_scores.std()),","101","            (zero_impute_scores.mean(), zero_impute_scores.std()),","102","            (mean_impute_scores.mean(), mean_impute_scores.std()),","103","            (knn_impute_scores.mean(), knn_impute_scores.std()),","104","            (iterative_impute_scores.mean(), iterative_impute_scores.std()))","105","","106","","107","results_diabetes = np.array(get_results(load_diabetes()))","108","mses_diabetes = results_diabetes[:, 0] * -1","109","stds_diabetes = results_diabetes[:, 1]","110","","111","results_boston = np.array(get_results(load_boston()))","112","mses_boston = results_boston[:, 0] * -1","113","stds_boston = results_boston[:, 1]","114","","115","n_bars = len(mses_diabetes)","116","xval = np.arange(n_bars)","117","","140","# plot boston results","143","    ax2.barh(j, mses_boston[j], xerr=stds_boston[j],","146","ax2.set_title('Imputation Techniques with Boston Data')"]}],"doc\/developers\/contributing.rst":[{"add":["436","","437","  - when importing C or Cython modules","438","  - on properties with decorators"],"delete":["436","   - when importing C or Cython modules","437","   - on properties with decorators"]}]}},"5431a1a9bfaf1927fade31492ac2b63423ad6384":{"changes":{"sklearn\/model_selection\/_split.py":"MODIFY","sklearn\/model_selection\/tests\/test_validation.py":"MODIFY"},"diff":{"sklearn\/model_selection\/_split.py":[{"add":["424","        if n_splits == 'warn':","495","        if n_splits == 'warn':","596","        if n_splits == 'warn':","750","        if n_splits == 'warn':","1941","    if cv is None or cv == 'warn':"],"delete":["424","        if n_splits is 'warn':","495","        if n_splits is 'warn':","596","        if n_splits is 'warn':","750","        if n_splits is 'warn':","1941","    if cv is None or cv is 'warn':"]}],"sklearn\/model_selection\/tests\/test_validation.py":[{"add":["283","@pytest.mark.filterwarnings('ignore:You should specify a value for')  # 0.22","284","def test_cross_validate_many_jobs():","285","    # regression test for #12154: cv='warn' with n_jobs>1 trigger a copy of","286","    # the parameters leading to a failure in check_cv due to cv is 'warn'","287","    # instead of cv == 'warn'.","288","    X, y = load_iris(return_X_y=True)","289","    clf = SVC(gamma='auto')","290","    grid = GridSearchCV(clf, param_grid={'C': [1, 10]})","291","    cross_validate(grid, X, y, n_jobs=2)","292","","293",""],"delete":[]}]}},"76ef8b0ef07f9c03b97d29a51e1543be7720e85a":{"changes":{"sklearn\/utils\/class_weight.py":"MODIFY","sklearn\/utils\/multiclass.py":"MODIFY","sklearn\/preprocessing\/_data.py":"MODIFY","sklearn\/neural_network\/_rbm.py":"MODIFY","sklearn\/feature_selection\/_univariate_selection.py":"MODIFY","sklearn\/utils\/__init__.py":"MODIFY","sklearn\/linear_model\/_stochastic_gradient.py":"MODIFY","sklearn\/feature_selection\/tests\/test_base.py":"MODIFY","sklearn\/utils\/validation.py":"MODIFY","sklearn\/manifold\/_spectral_embedding.py":"MODIFY","sklearn\/utils\/tests\/test_class_weight.py":"MODIFY","sklearn\/ensemble\/_forest.py":"MODIFY","sklearn\/utils\/tests\/test_validation.py":"MODIFY","sklearn\/linear_model\/_coordinate_descent.py":"MODIFY","sklearn\/neighbors\/_nca.py":"MODIFY"},"diff":{"sklearn\/utils\/class_weight.py":[{"add":["6","from .validation import _deprecate_positional_args","7","","73","@_deprecate_positional_args","74","def compute_sample_weight(class_weight, y, *, indices=None):"],"delete":["71","def compute_sample_weight(class_weight, y, indices=None):"]}],"sklearn\/utils\/multiclass.py":[{"add":["29","    return np.arange(","30","        check_array(y, accept_sparse=['csr', 'csc', 'coo']).shape[1]","31","    )","87","            len(set(check_array(y,","88","                                accept_sparse=['csr', 'csc', 'coo']).shape[1]"],"delete":["29","    return np.arange(check_array(y, ['csr', 'csc', 'coo']).shape[1])","85","            len(set(check_array(y, ['csr', 'csc', 'coo']).shape[1]"]}],"sklearn\/preprocessing\/_data.py":[{"add":["1709","    X = check_array(X, accept_sparse=sparse_format, copy=copy,"],"delete":["1709","    X = check_array(X, sparse_format, copy=copy,"]}],"sklearn\/neural_network\/_rbm.py":[{"add":["359","                                            n_batches, n_samples=n_samples))"],"delete":["359","                                            n_batches, n_samples))"]}],"sklearn\/feature_selection\/_univariate_selection.py":[{"add":["148","    X, y = check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'])","279","    X, y = check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'],","280","                     dtype=np.float64)"],"delete":["148","    X, y = check_X_y(X, y, ['csr', 'csc', 'coo'])","279","    X, y = check_X_y(X, y, ['csr', 'csc', 'coo'], dtype=np.float64)"]}],"sklearn\/utils\/__init__.py":[{"add":["31","                         check_symmetric, check_scalar,","32","                         _deprecate_positional_args)","317","    return _safe_indexing(X, indices, axis=axis)","320","def _safe_indexing(X, indices, *, axis=0):","687","@_deprecate_positional_args","688","def safe_sqr(X, *, copy=True):","727","@_deprecate_positional_args","728","def gen_batches(n, batch_size, *, min_batch_size=0):","777","@_deprecate_positional_args","778","def gen_even_slices(n, n_packs, *, n_samples=None):","963","@_deprecate_positional_args","964","def get_chunk_n_rows(row_bytes, *, max_n_rows=None, working_memory=None):"],"delete":["31","                         check_symmetric, check_scalar)","316","    return _safe_indexing(X, indices, axis)","319","def _safe_indexing(X, indices, axis=0):","686","def safe_sqr(X, copy=True):","725","def gen_batches(n, batch_size, min_batch_size=0):","774","def gen_even_slices(n, n_packs, n_samples=None):","959","def get_chunk_n_rows(row_bytes, max_n_rows=None,","960","                     working_memory=None):"]}],"sklearn\/linear_model\/_stochastic_gradient.py":[{"add":["489","        X, y = check_X_y(X, y, accept_sparse='csr', dtype=np.float64,","490","                         order=\"C\", accept_large_sparse=False)"],"delete":["489","        X, y = check_X_y(X, y, 'csr', dtype=np.float64, order=\"C\",","490","                         accept_large_sparse=False)"]}],"sklearn\/feature_selection\/tests\/test_base.py":[{"add":["17","        X = check_array(X, accept_sparse='csc')"],"delete":["17","        X = check_array(X, 'csc')"]}],"sklearn\/utils\/validation.py":[{"add":["38","def _deprecate_positional_args(f):","39","    \"\"\"Decorator for methods that issues warnings for positional arguments","40","","41","    Using the keyword-only argument syntax in pep 3102, arguments after the","42","    * will issue a warning when passed as a positional argument.","43","","44","    Parameters","45","    ----------","46","    f : function","47","        function to check arguments on","48","    \"\"\"","49","    sig = signature(f)","50","    kwonly_args = []","51","    all_args = []","52","","53","    for name, param in sig.parameters.items():","54","        if param.kind == Parameter.POSITIONAL_OR_KEYWORD:","55","            all_args.append(name)","56","        elif param.kind == Parameter.KEYWORD_ONLY:","57","            kwonly_args.append(name)","58","","59","    @wraps(f)","60","    def inner_f(*args, **kwargs):","61","        extra_args = len(args) - len(all_args)","62","        if extra_args > 0:","63","            # ignore first 'self' argument for instance methods","64","            args_msg = ['{}={}'.format(name, arg)","65","                        for name, arg in zip(kwonly_args[:extra_args],","66","                                             args[-extra_args:])]","67","            warnings.warn(\"Pass {} as keyword args. From version 0.25 \"","68","                          \"passing these as positional arguments will \"","69","                          \"result in an error\".format(\", \".join(args_msg)),","70","                          FutureWarning)","71","        kwargs.update({k: arg for k, arg in zip(sig.parameters, args)})","72","        return f(**kwargs)","73","    return inner_f","74","","75","","107","@_deprecate_positional_args","108","def assert_all_finite(X, *, allow_nan=False):","120","@_deprecate_positional_args","121","def as_float_array(X, *, copy=True, force_all_finite=True):","155","        return check_array(X, accept_sparse=['csr', 'csc', 'coo'],","156","                           dtype=np.float64, copy=copy,","157","                           force_all_finite=force_all_finite, ensure_2d=False)","391","@_deprecate_positional_args","392","def check_array(array, accept_sparse=False, *, accept_large_sparse=True,","663","@_deprecate_positional_args","664","def check_X_y(X, y, accept_sparse=False, *, accept_large_sparse=True,","776","        y = check_array(y, accept_sparse='csr', force_all_finite=True,","777","                        ensure_2d=False, dtype=None)","789","@_deprecate_positional_args","790","def column_or_1d(y, *, warn=False):","870","@_deprecate_positional_args","871","def check_symmetric(array, *, tol=1E-10, raise_warning=True,","927","@_deprecate_positional_args","928","def check_is_fitted(estimator, attributes=None, *, msg=None, all_or_any=all):","1021","def check_scalar(x, name, target_type, *, min_val=None, max_val=None):"],"delete":["69","def assert_all_finite(X, allow_nan=False):","81","def as_float_array(X, copy=True, force_all_finite=True):","115","        return check_array(X, ['csr', 'csc', 'coo'], dtype=np.float64,","116","                           copy=copy, force_all_finite=force_all_finite,","117","                           ensure_2d=False)","351","def check_array(array, accept_sparse=False, accept_large_sparse=True,","622","def check_X_y(X, y, accept_sparse=False, accept_large_sparse=True,","734","        y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,","735","                        dtype=None)","747","def column_or_1d(y, warn=False):","827","def check_symmetric(array, tol=1E-10, raise_warning=True,","883","def check_is_fitted(estimator, attributes=None, msg=None, all_or_any=all):","976","def check_scalar(x, name, target_type, min_val=None, max_val=None):","1270","def _deprecate_positional_args(f):","1271","    \"\"\"Decorator for methods that issues warnings for positional arguments","1272","","1273","    Using the keyword-only argument syntax in pep 3102, arguments after the","1274","    * will issue a warning when passed as a positional argument.","1275","","1276","    Parameters","1277","    ----------","1278","    f : function","1279","        function to check arguments on","1280","    \"\"\"","1281","    sig = signature(f)","1282","    kwonly_args = []","1283","    all_args = []","1284","","1285","    for name, param in sig.parameters.items():","1286","        if param.kind == Parameter.POSITIONAL_OR_KEYWORD:","1287","            all_args.append(name)","1288","        elif param.kind == Parameter.KEYWORD_ONLY:","1289","            kwonly_args.append(name)","1290","","1291","    @wraps(f)","1292","    def inner_f(*args, **kwargs):","1293","        extra_args = len(args) - len(all_args)","1294","        if extra_args > 0:","1295","            # ignore first 'self' argument for instance methods","1296","            args_msg = ['{}={}'.format(name, arg)","1297","                        for name, arg in zip(kwonly_args[:extra_args],","1298","                                             args[-extra_args:])]","1299","            warnings.warn(\"Pass {} as keyword args. From version 0.25 \"","1300","                          \"passing these as positional arguments will \"","1301","                          \"result in an error\".format(\", \".join(args_msg)),","1302","                          FutureWarning)","1303","        kwargs.update({k: arg for k, arg in zip(sig.parameters, args)})","1304","        return f(**kwargs)","1305","    return inner_f","1306","","1307",""]}],"sklearn\/manifold\/_spectral_embedding.py":[{"add":["303","        ml = smoothed_aggregation_solver(check_array(laplacian,","304","                                                     accept_sparse='csr'))"],"delete":["303","        ml = smoothed_aggregation_solver(check_array(laplacian, 'csr'))"]}],"sklearn\/utils\/tests\/test_class_weight.py":[{"add":["194","    sample_weight = compute_sample_weight(\"balanced\", y, indices=range(6))","199","    sample_weight = compute_sample_weight(\"balanced\", y, indices=range(6))","204","    sample_weight = compute_sample_weight(\"balanced\", y, indices=range(4))","210","    sample_weight = compute_sample_weight(\"balanced\", y,","211","                                          indices=[0, 1, 1, 2, 2, 3])","217","    sample_weight = compute_sample_weight(\"balanced\", y,","218","                                          indices=[0, 1, 1, 2, 2, 3])","223","    sample_weight = compute_sample_weight(\"balanced\", y, indices=range(6))","228","    sample_weight = compute_sample_weight(\"balanced\", y, indices=range(6))","241","        compute_sample_weight(\"ni\", y, indices=range(4))","245","        compute_sample_weight(\"ni\", y_, indices=range(4))","249","        compute_sample_weight({1: 2, 2: 1}, y, indices=range(4))"],"delete":["194","    sample_weight = compute_sample_weight(\"balanced\", y, range(6))","199","    sample_weight = compute_sample_weight(\"balanced\", y, range(6))","204","    sample_weight = compute_sample_weight(\"balanced\", y, range(4))","210","    sample_weight = compute_sample_weight(\"balanced\", y, [0, 1, 1, 2, 2, 3])","216","    sample_weight = compute_sample_weight(\"balanced\", y, [0, 1, 1, 2, 2, 3])","221","    sample_weight = compute_sample_weight(\"balanced\", y, range(6))","226","    sample_weight = compute_sample_weight(\"balanced\", y, range(6))","239","        compute_sample_weight(\"ni\", y, range(4))","243","        compute_sample_weight(\"ni\", y_, range(4))","247","        compute_sample_weight({1: 2, 2: 1}, y, range(4))"]}],"sklearn\/ensemble\/_forest.py":[{"add":["161","                curr_sample_weight *= compute_sample_weight('auto', y,","162","                                                            indices=indices)","164","            curr_sample_weight *= compute_sample_weight('balanced', y,","165","                                                        indices=indices)"],"delete":["161","                curr_sample_weight *= compute_sample_weight('auto', y, indices)","163","            curr_sample_weight *= compute_sample_weight('balanced', y, indices)"]}],"sklearn\/utils\/tests\/test_validation.py":[{"add":["65","    assert as_float_array(X, copy=False) is not X","914","        check_scalar(x, \"test_name\", target_type=target_type,","915","                     min_val=min_val, max_val=max_val)"],"delete":["65","    assert as_float_array(X, False) is not X","914","        check_scalar(x, \"test_name\", target_type, min_val, max_val)"]}],"sklearn\/linear_model\/_coordinate_descent.py":[{"add":["133","        X = check_array(X, accept_sparse='csc',","437","        X = check_array(X, accept_sparse='csc', dtype=[np.float64, np.float32],","439","        y = check_array(y, accept_sparse='csc', dtype=X.dtype.type,","440","                        order='F', copy=False, ensure_2d=False)","1097","    X_train = check_array(X_train, accept_sparse='csc', dtype=dtype,","1098","                          order=X_order)"],"delete":["133","        X = check_array(X, 'csc',","437","        X = check_array(X, 'csc', dtype=[np.float64, np.float32],","439","        y = check_array(y, 'csc', dtype=X.dtype.type, order='F', copy=False,","440","                        ensure_2d=False)","1097","    X_train = check_array(X_train, 'csc', dtype=dtype, order=X_order)"]}],"sklearn\/neighbors\/_nca.py":[{"add":["310","                self.n_components, 'n_components', numbers.Integral, min_val=1)","329","        check_scalar(self.max_iter, 'max_iter', numbers.Integral, min_val=1)","330","        check_scalar(self.tol, 'tol', numbers.Real, min_val=0.)","331","        check_scalar(self.verbose, 'verbose', numbers.Integral, min_val=0)"],"delete":["310","                self.n_components, 'n_components', numbers.Integral, 1)","329","        check_scalar(self.max_iter, 'max_iter', numbers.Integral, 1)","330","        check_scalar(self.tol, 'tol', numbers.Real, 0.)","331","        check_scalar(self.verbose, 'verbose', numbers.Integral, 0)"]}]}},"174f4aea6e544f2126ab2d920ec51d31d148d703":{"changes":{"sklearn\/linear_model\/tests\/test_omp.py":"MODIFY","doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/decomposition\/tests\/test_dict_learning.py":"MODIFY","sklearn\/linear_model\/omp.py":"MODIFY"},"diff":{"sklearn\/linear_model\/tests\/test_omp.py":[{"add":["106","def test_orthogonal_mp_gram_readonly():","107","    # Non-regression test for:","108","    # https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/5956","109","    idx, = gamma[:, 0].nonzero()","110","    G_readonly = G.copy()","111","    G_readonly.setflags(write=False)","112","    Xy_readonly = Xy.copy()","113","    Xy_readonly.setflags(write=False)","114","    gamma_gram = orthogonal_mp_gram(G_readonly, Xy_readonly[:, 0], 5,","115","                                    copy_Gram=False, copy_Xy=False)","116","    assert_array_equal(idx, np.flatnonzero(gamma_gram))","117","    assert_array_almost_equal(gamma[:, 0], gamma_gram, decimal=2)","118","","119",""],"delete":[]}],"doc\/whats_new\/v0.20.rst":[{"add":["508","- Fixed a bug in :class:`decomposition.SparseCoder` when running OMP sparse","509","  coding in parallel using readonly memory mapped datastructures. :issue:`5956`","510","  by :user:`Vighnesh Birodkar <vighneshbirodkar>` and","511","  :user:`Olivier Grisel <ogrisel>`.","512",""],"delete":[]}],"sklearn\/decomposition\/tests\/test_dict_learning.py":[{"add":["0","from __future__ import division","369","","370","","371","def test_sparse_coder_parallel_mmap():","372","    # Non-regression test for:","373","    # https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/5956","374","    # Test that SparseCoder does not error by passing reading only","375","    # arrays to child processes","376","","377","    rng = np.random.RandomState(777)","378","    n_components, n_features = 40, 64","379","    init_dict = rng.rand(n_components, n_features)","380","    # Ensure that `data` is >2M. Joblib memory maps arrays","381","    # if they are larger than 1MB. The 4 accounts for float32","382","    # data type","383","    n_samples = int(2e6) \/\/ (4 * n_features)","384","    data = np.random.rand(n_samples, n_features).astype(np.float32)","385","","386","    sc = SparseCoder(init_dict, transform_algorithm='omp', n_jobs=2)","387","    sc.fit_transform(data)"],"delete":[]}],"sklearn\/linear_model\/omp.py":[{"add":["193","    if copy_Xy or not Xy.flags.writeable:","493","    if copy_Xy or not Xy.flags.writeable:","494","        # Make the copy once instead of many times in _gram_omp itself.","495","        Xy = Xy.copy()","520","            copy_Gram=copy_Gram, copy_Xy=False,"],"delete":["193","    if copy_Xy:","517","            copy_Gram=copy_Gram, copy_Xy=copy_Xy,"]}]}},"62301aa81b72fd93a5fccc5db0b2a742b95aea7d":{"changes":{"sklearn\/decomposition\/dict_learning.py":"MODIFY","sklearn\/decomposition\/tests\/test_dict_learning.py":"MODIFY"},"diff":{"sklearn\/decomposition\/dict_learning.py":[{"add":["765","    dictionary = np.require(dictionary, requirements='W')","766",""],"delete":[]}],"sklearn\/decomposition\/tests\/test_dict_learning.py":[{"add":["267","def test_dict_learning_online_readonly_initialization():","268","    n_components = 12","269","    rng = np.random.RandomState(0)","270","    V = rng.randn(n_components, n_features)","271","    V.setflags(write=False)","272","    MiniBatchDictionaryLearning(n_components, n_iter=1, dict_init=V,","273","                                random_state=0, shuffle=False).fit(X)","274","","275",""],"delete":[]}]}},"0e4f85fe72932c5adeb7de8652a7b76482190341":{"changes":{".github\/labeler.yml":"ADD",".github\/workflows\/labeler.yml":"ADD"},"diff":{".github\/labeler.yml":[{"add":[],"delete":[]}],".github\/workflows\/labeler.yml":[{"add":[],"delete":[]}]}},"7d5a54b3291cf1af3e29bc5ed41a429610c9ed8a":{"changes":{"sklearn\/linear_model\/tests\/test_coordinate_descent.py":"MODIFY","sklearn\/linear_model\/_coordinate_descent.py":"MODIFY"},"diff":{"sklearn\/linear_model\/tests\/test_coordinate_descent.py":[{"add":["60","@pytest.mark.parametrize('l1_ratio', (-1, 2, None, 10, 'something_wrong'))","61","def test_l1_ratio_param_invalid(l1_ratio):","62","    # Check that correct error is raised when l1_ratio in ElasticNet","63","    # is outside the correct range","64","    X = np.array([[-1.], [0.], [1.]])","65","    Y = [-1, 0, 1]       # just a straight line","66","","67","    msg = \"l1_ratio must be between 0 and 1; got l1_ratio=\"","68","    clf = ElasticNet(alpha=0.1, l1_ratio=l1_ratio)","69","    with pytest.raises(ValueError, match=msg):","70","        clf.fit(X, Y)","71","","72",""],"delete":[]}],"sklearn\/linear_model\/_coordinate_descent.py":[{"add":["757","        if (not isinstance(self.l1_ratio, numbers.Number) or","758","                self.l1_ratio < 0 or self.l1_ratio > 1):","759","            raise ValueError(\"l1_ratio must be between 0 and 1; \"","760","                             f\"got l1_ratio={self.l1_ratio}\")","761",""],"delete":[]}]}},"553b5fb8f84ba05c8397f26dd079deece2b05029":{"changes":{"examples\/manifold\/plot_t_sne_perplexity.py":"MODIFY","examples\/manifold\/plot_compare_methods.py":"MODIFY"},"diff":{"examples\/manifold\/plot_t_sne_perplexity.py":[{"add":["2","t-SNE: The effect of various perplexity values on the shape"],"delete":["2"," t-SNE: The effect of various perplexity values on the shape"]}],"examples\/manifold\/plot_compare_methods.py":[{"add":["2","Comparison of Manifold Learning methods"],"delete":["2"," Comparison of Manifold Learning methods"]}]}},"86c5998dbb1951ce6ba52e42da88aeb7e6e9f0a4":{"changes":{"sklearn\/neighbors\/base.py":"MODIFY","sklearn\/svm\/base.py":"MODIFY","sklearn\/utils\/estimator_checks.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY","sklearn\/preprocessing\/data.py":"MODIFY"},"diff":{"sklearn\/neighbors\/base.py":[{"add":["214","        # Precomputed matrix X must be squared","215","        if self.metric == 'precomputed' and X.shape[0] != X.shape[1]:","216","            raise ValueError(\"Precomputed matrix must be a square matrix.\"","217","                             \" Input is a {}x{} matrix.\"","218","                             .format(X.shape[0], X.shape[1]))","219",""],"delete":[]}],"sklearn\/svm\/base.py":[{"add":["161","            raise ValueError(\"Precomputed matrix must be a square matrix.\"","162","                             \" Input is a {}x{} matrix.\"","163","                             .format(X.shape[0], X.shape[1]))"],"delete":["161","            raise ValueError(\"X.shape[0] should be equal to X.shape[1]\")"]}],"sklearn\/utils\/estimator_checks.py":[{"add":["98","    if _is_pairwise(estimator):","99","        # Check that pairwise estimator throws error on non-square input","100","        yield check_nonsquare_error","101","","880","    X = pairwise_estimator_convert_X(X, estimator_orig)","881","","969","    X = pairwise_estimator_convert_X(X, transformer)","985","    X = pairwise_estimator_convert_X(X, transformer)","1068","        if hasattr(X, 'shape') and \\","1069","           not _safe_tags(transformer, \"stateless\") and \\","1070","           X.ndim == 2 and X.shape[1] > 1:","1071","","1078","                transformer.transform(X[:, :-1])","1261","def check_nonsquare_error(name, estimator_orig):","1262","    \"\"\"Test that error is thrown when non-square data provided\"\"\"","1263","","1264","    X, y = make_blobs(n_samples=20, n_features=10)","1265","    estimator = clone(estimator_orig)","1266","","1267","    with assert_raises(ValueError, msg=\"The pairwise estimator {}\"","1268","                       \" does not raise an error on non-square data\"","1269","                       .format(name)):","1270","        estimator.fit(X, y)","1271","","1272","","1273","@ignore_warnings","1913","","1914","    X = rng.normal(size=(10, 4))","1915","    X = pairwise_estimator_convert_X(X, regressor_orig)"],"delete":["1060","        if hasattr(X, 'T') and not _safe_tags(transformer, \"stateless\"):","1067","                transformer.transform(X.T)","1888","    X = rng.normal(size=(10, 4))"]}],"doc\/whats_new\/v0.22.rst":[{"add":["215","- |Enhancement| SVM now throws more specific error when fit on non-square data","216","  and kernel = precomputed.  :class:`svm.BaseLibSVM`","217","  :pr:`14336` by :user:`Gregory Dexter <gdex1>`.","218","  ","227"," ","228","- |Fix| KernelCenterer now throws error when fit on non-square ","229","  class:`preprocessing.KernelCenterer`","230","  :pr:`14336` by :user:`Gregory Dexter <gdex1>`.","255",":mod:`sklearn.neighbors`","256",".............................","257","","258","- |Fix| KNearestRegressor now throws error when fit on non-square data and","259","  metric = precomputed.  :class:`neighbors.NeighborsBase`","260","  :pr:`14336` by :user:`Gregory Dexter <gdex1>`.  ","261","","271","  ","294","- |Fix| Added check_transformer_data_not_an_array to checks where missing","295","  |Fix| Added check that pairwise estimators raise error on non-square data","296","  :pr:`14336` by :user:`Gregory Dexter <gdex1>`.","297",""],"delete":["215","","224","","258",""]}],"sklearn\/preprocessing\/data.py":[{"add":["1988","","1990","","1991","        if K.shape[0] != K.shape[1]:","1992","            raise ValueError(\"Kernel matrix must be a square matrix.\"","1993","                             \" Input is a {}x{} matrix.\"","1994","                             .format(K.shape[0], K.shape[1]))","1995",""],"delete":[]}]}},"e93cde573960f737f84d07dbbc5f7bc7b9aa176b":{"changes":{"sklearn\/preprocessing\/_encoders.py":"MODIFY","sklearn\/preprocessing\/tests\/test_encoders.py":"MODIFY"},"diff":{"sklearn\/preprocessing\/_encoders.py":[{"add":["129","                le.classes_ = np.array(self._categories[i], dtype=X.dtype)"],"delete":["129","                le.classes_ = np.array(self._categories[i])"]}],"sklearn\/preprocessing\/tests\/test_encoders.py":[{"add":["406","@pytest.mark.parametrize(\"X, cat_exp, cat_dtype\", [","407","    ([['abc', 55], ['def', 55]], [['abc', 'def'], [55]], np.object_),","408","    (np.array([[1, 2], [3, 2]]), [[1, 3], [2]], np.integer),","409","    (np.array([['A', 'cat'], ['B', 'cat']], dtype=object),","410","     [['A', 'B'], ['cat']], np.object_),","411","    (np.array([['A', 'cat'], ['B', 'cat']]),","412","     [['A', 'B'], ['cat']], np.str_)","413","    ], ids=['mixed', 'numeric', 'object', 'string'])","414","def test_one_hot_encoder_categories(X, cat_exp, cat_dtype):","417","        enc = OneHotEncoder(categories='auto')","423","            assert np.issubdtype(res.dtype, cat_dtype)","426","@pytest.mark.parametrize(\"X, X2, cats, cat_dtype\", [","427","    (np.array([['a', 'b']], dtype=object).T,","428","     np.array([['a', 'd']], dtype=object).T,","429","     [['a', 'b', 'c']], np.object_),","430","    (np.array([[1, 2]], dtype='int64').T,","431","     np.array([[1, 4]], dtype='int64').T,","432","     [[1, 2, 3]], np.int64),","433","    (np.array([['a', 'b']], dtype=object).T,","434","     np.array([['a', 'd']], dtype=object).T,","435","     [np.array(['a', 'b', 'c'])], np.object_),","436","    ], ids=['object', 'numeric', 'object-string-cat'])","437","def test_one_hot_encoder_specified_categories(X, X2, cats, cat_dtype):","438","    enc = OneHotEncoder(categories=cats)","442","    assert list(enc.categories[0]) == list(cats[0])","443","    assert enc.categories_[0].tolist() == list(cats[0])","444","    # manually specified categories should have same dtype as","445","    # the data when coerced from lists","446","    assert enc.categories_[0].dtype == cat_dtype","447","","448","    # when specifying categories manually, unknown categories should already","449","    # raise when fitting","450","    enc = OneHotEncoder(categories=cats)","451","    assert_raises(ValueError, enc.fit, X2)","452","    enc = OneHotEncoder(categories=cats, handle_unknown='ignore')","453","    exp = np.array([[1., 0., 0.], [0., 0., 0.]])","454","    assert_array_equal(enc.fit(X2).transform(X2).toarray(), exp)","455","","456","","457","def test_one_hot_encoder_unsorted_categories():","458","    X = np.array([['a', 'b']], dtype=object).T","465","","466","def test_one_hot_encoder_specified_categories_mixed_columns():","474","    assert np.issubdtype(enc.categories_[0].dtype, np.object_)","476","    # integer categories but from object dtype data","477","    assert np.issubdtype(enc.categories_[1].dtype, np.object_)"],"delete":["406","def test_one_hot_encoder_categories():","407","    X = [['abc', 1, 55], ['def', 2, 55]]","408","","411","        enc = OneHotEncoder()","415","        cat_exp = [['abc', 'def'], [1, 2], [55]]","420","def test_one_hot_encoder_specified_categories():","421","    X = np.array([['a', 'b']], dtype=object).T","422","","423","    enc = OneHotEncoder(categories=[['a', 'b', 'c']])","427","    assert enc.categories[0] == ['a', 'b', 'c']","428","    assert enc.categories_[0].tolist() == ['a', 'b', 'c']","429","    assert np.issubdtype(enc.categories_[0].dtype, np.str_)","443","    assert np.issubdtype(enc.categories_[0].dtype, np.str_)","445","    assert np.issubdtype(enc.categories_[1].dtype, np.integer)","446","","447","    # when specifying categories manually, unknown categories should already","448","    # raise when fitting","449","    X = np.array([['a', 'b', 'c']]).T","450","    enc = OneHotEncoder(categories=[['a', 'b']])","451","    assert_raises(ValueError, enc.fit, X)","452","    enc = OneHotEncoder(categories=[['a', 'b']], handle_unknown='ignore')","453","    exp = np.array([[1., 0.], [0., 1.], [0., 0.]])","454","    assert_array_equal(enc.fit(X).transform(X).toarray(), exp)"]}]}}}