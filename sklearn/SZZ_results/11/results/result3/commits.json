{"60f887e65c7140f29ff9ebe73a0945884d1d685d":{"changes":{"sklearn\/ensemble\/tests\/test_iforest.py":"MODIFY"},"diff":{"sklearn\/ensemble\/tests\/test_iforest.py":[{"add":["8","import pytest","9","","108","    # note that assert_no_warnings does not apply since it enables a","109","    # PendingDeprecationWarning triggered by scipy.sparse's use of","110","    # np.matrix. See issue #11251.","111","    with pytest.warns(None) as record:","112","        IsolationForest(max_samples='auto').fit(X)","113","    user_warnings = [each for each in record","114","                     if issubclass(each.category, UserWarning)]","115","    assert len(user_warnings) == 0","116","    with pytest.warns(None) as record:","117","        IsolationForest(max_samples=np.int64(2)).fit(X)","118","    user_warnings = [each for each in record","119","                     if issubclass(each.category, UserWarning)]","120","    assert len(user_warnings) == 0","121",""],"delete":["17","from sklearn.utils.testing import assert_no_warnings","107","    assert_no_warnings(IsolationForest(max_samples='auto').fit, X)","108","    assert_no_warnings(IsolationForest(max_samples=np.int64(2)).fit, X)"]}]}},"2f5bb34b6d6093efe0ab1c10857ef07fd8aa503b":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/decomposition\/tests\/test_kernel_pca.py":"MODIFY","sklearn\/decomposition\/kernel_pca.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["95","- |Fix| Fixed a bug in :class:`decomposition.KernelPCA`, `fit().transform()`","96","  now produces the correct output (the same as `fit_transform()`) in case","97","  of non-removed zero eigenvalues (`remove_zero_eig=False`).","98","  `fit_inverse_transform` was also accelerated by using the same trick as","99","  `fit_transform` to compute the transform of `X`.","100","  :issue:`12143` by :user:`Sylvain Mari¨¦ <smarie>`","101",""],"delete":[]}],"sklearn\/decomposition\/tests\/test_kernel_pca.py":[{"add":["157","def test_leave_zero_eig():","158","    \"\"\"This test checks that fit().transform() returns the same result as","159","    fit_transform() in case of non-removed zero eigenvalue.","160","    Non-regression test for issue #12141 (PR #12143)\"\"\"","161","    X_fit = np.array([[1, 1], [0, 0]])","162","","163","    # Assert that even with all np warnings on, there is no div by zero warning","164","    with pytest.warns(None) as record:","165","        with np.errstate(all='warn'):","166","            k = KernelPCA(n_components=2, remove_zero_eig=False,","167","                          eigen_solver=\"dense\")","168","            # Fit, then transform","169","            A = k.fit(X_fit).transform(X_fit)","170","            # Do both at once","171","            B = k.fit_transform(X_fit)","172","            # Compare","173","            assert_array_almost_equal(np.abs(A), np.abs(B))","174","","175","    for w in record:","176","        # There might be warnings about the kernel being badly conditioned,","177","        # but there should not be warnings about division by zero.","178","        # (Numpy division by zero warning can have many message variants, but","179","        # at least we know that it is a RuntimeWarning so lets check only this)","180","        assert not issubclass(w.category, RuntimeWarning)","181","","182",""],"delete":[]}],"sklearn\/decomposition\/kernel_pca.py":[{"add":["222","        # remove eigenvectors with a zero eigenvalue (null space) if required","227","        # Maintenance note on Eigenvectors normalization","228","        # ----------------------------------------------","229","        # there is a link between","230","        # the eigenvectors of K=Phi(X)'Phi(X) and the ones of Phi(X)Phi(X)'","231","        # if v is an eigenvector of K","232","        #                      then Phi(X)v  is an eigenvector of Phi(X)Phi(X)'","233","        # if u is an eigenvector of Phi(X)Phi(X)'","234","        #                      then Phi(X)'u is an eigenvector of Phi(X)Phi(X)'","235","        #","236","        # At this stage our self.alphas_ (the v) have norm 1, we need to scale","237","        # them so that eigenvectors in kernel feature space (the u) have norm=1","238","        # instead","239","        #","240","        # We COULD scale them here:","241","        #       self.alphas_ = self.alphas_ \/ np.sqrt(self.lambdas_)","242","        #","243","        # But choose to perform that LATER when needed, in `fit()` and in","244","        # `transform()`.","245","","279","            # no need to use the kernel to transform X, use shortcut expression","280","            X_transformed = self.alphas_ * np.sqrt(self.lambdas_)","281","","302","        # no need to use the kernel to transform X, use shortcut expression","323","        # Compute centered gram matrix between X and training data X_fit_","325","","326","        # scale eigenvectors (properly account for null-space for dot product)","327","        non_zeros = np.flatnonzero(self.lambdas_)","328","        scaled_alphas = np.zeros_like(self.alphas_)","329","        scaled_alphas[:, non_zeros] = (self.alphas_[:, non_zeros]","330","                                       \/ np.sqrt(self.lambdas_[non_zeros]))","331","","332","        # Project with a scalar product between K and the scaled eigenvectors","333","        return np.dot(K, scaled_alphas)"],"delete":["222","        # remove eigenvectors with a zero eigenvalue","260","            sqrt_lambdas = np.diag(np.sqrt(self.lambdas_))","261","            X_transformed = np.dot(self.alphas_, sqrt_lambdas)","303","        return np.dot(K, self.alphas_ \/ np.sqrt(self.lambdas_))"]}]}},"819d8ef8b7532dac0f7cb00f5d3905eed9237b90":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/cluster\/dbscan_.py":"MODIFY","sklearn\/cluster\/tests\/test_dbscan.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["19","- :class:`cluster.DBSCAN` (bug fix)","50","- |Fix| Fixed a bug in :class:`cluster.DBSCAN` with precomputed sparse neighbors","51","  graph, which would add explicitly zeros on the diagonal even when already","52","  present. :issue:`12105` by `Tom Dupre la Tour`_.","53",""],"delete":["19","- please add class and reason here (see version 0.20 what's new)"]}],"doc\/whats_new\/v0.20.rst":[{"add":["49","- |Fix| Fixed a bug in :class:`cluster.DBSCAN` with precomputed sparse neighbors","50","  graph, which would add explicitly zeros on the diagonal even when already","51","  present. :issue:`12105` by `Tom Dupre la Tour`_.","52","","669","  average for multilabel data. :issue:`11679` by :user:`Alexander Pacha <apacha>`.","923","","1045","  ``missing_values='NaN'`` should now be"],"delete":["665","  average for multilabel data. :issue:`11679` by :user:`Alexander Pacha <apacha>`. ","919","  ","1041","  ``missing_values='NaN'``?should now be"]}],"sklearn\/cluster\/dbscan_.py":[{"add":["16","from ..utils.testing import ignore_warnings","145","","146","        # set the diagonal to explicit values, as a point is its own neighbor","147","        with ignore_warnings():","148","            X.setdiag(X.diagonal())  # XXX: modifies X's internals in-place","149","","152","        masked_indptr = np.concatenate(([0], np.cumsum(X_mask)))","153","        masked_indptr = masked_indptr[X.indptr[1:-1]]"],"delete":["146","        masked_indptr = np.concatenate(([0], np.cumsum(X_mask)))[X.indptr[1:]]","148","        # insert the diagonal: a point is its own neighbor, but 0 distance","149","        # means absence from sparse matrix data","150","        masked_indices = np.insert(masked_indices, masked_indptr,","151","                                   np.arange(X.shape[0]))","152","        masked_indptr = masked_indptr[:-1] + np.arange(1, X.shape[0])"]}],"sklearn\/cluster\/tests\/test_dbscan.py":[{"add":["83","@pytest.mark.parametrize('include_self', [False, True])","84","def test_dbscan_sparse_precomputed(include_self):","87","    X_ = X if include_self else None","88","    D_sparse = nn.radius_neighbors_graph(X=X_, mode='distance')","101","@pytest.mark.parametrize('use_sparse', [True, False])","102","@pytest.mark.parametrize('metric', ['precomputed', 'minkowski'])","103","def test_dbscan_input_not_modified(use_sparse, metric):","104","    # test that the input is not modified by dbscan","105","    X = np.random.RandomState(0).rand(10, 10)","106","    X = sparse.csr_matrix(X) if use_sparse else X","107","    X_copy = X.copy()","108","    dbscan(X, metric=metric)","109","","110","    if use_sparse:","111","        assert_array_equal(X.toarray(), X_copy.toarray())","112","    else:","113","        assert_array_equal(X, X_copy)","114","","115",""],"delete":["83","def test_dbscan_sparse_precomputed():","86","    D_sparse = nn.radius_neighbors_graph(mode='distance')"]}]}},"d6d1d63fa6b098c72953a6827aae475f611936ed":{"changes":{"sklearn\/svm\/tests\/test_svm.py":"MODIFY","sklearn\/svm\/src\/libsvm\/svm.cpp":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY"},"diff":{"sklearn\/svm\/tests\/test_svm.py":[{"add":["424","@pytest.mark.parametrize(\"estimator\", [svm.SVC(C=1e-2), svm.NuSVC()])","425","def test_svm_classifier_sided_sample_weight(estimator):","426","    # fit a linear SVM and check that giving more weight to opposed samples","427","    # in the space will flip the decision toward these samples.","428","    X = [[-2, 0], [-1, -1], [0, -2], [0, 2], [1, 1], [2, 0]]","429","    estimator.set_params(kernel='linear')","431","    # check that with unit weights, a sample is supposed to be predicted on","432","    # the boundary","433","    sample_weight = [1] * 6","434","    estimator.fit(X, Y, sample_weight=sample_weight)","435","    y_pred = estimator.decision_function([[-1., 1.]])","436","    assert y_pred == pytest.approx(0)","438","    # give more weights to opposed samples","439","    sample_weight = [10., .1, .1, .1, .1, 10]","440","    estimator.fit(X, Y, sample_weight=sample_weight)","441","    y_pred = estimator.decision_function([[-1., 1.]])","442","    assert y_pred < 0","443","","444","    sample_weight = [1., .1, 10., 10., .1, .1]","445","    estimator.fit(X, Y, sample_weight=sample_weight)","446","    y_pred = estimator.decision_function([[-1., 1.]])","447","    assert y_pred > 0","448","","449","","450","@pytest.mark.parametrize(","451","    \"estimator\",","452","    [svm.SVR(C=1e-2), svm.NuSVR(C=1e-2)]","453",")","454","def test_svm_regressor_sided_sample_weight(estimator):","455","    # similar test to test_svm_classifier_sided_sample_weight but for","456","    # SVM regressors","457","    X = [[-2, 0], [-1, -1], [0, -2], [0, 2], [1, 1], [2, 0]]","458","    estimator.set_params(kernel='linear')","459","","460","    # check that with unit weights, a sample is supposed to be predicted on","461","    # the boundary","462","    sample_weight = [1] * 6","463","    estimator.fit(X, Y, sample_weight=sample_weight)","464","    y_pred = estimator.predict([[-1., 1.]])","465","    assert y_pred == pytest.approx(1.5)","466","","467","    # give more weights to opposed samples","468","    sample_weight = [10., .1, .1, .1, .1, 10]","469","    estimator.fit(X, Y, sample_weight=sample_weight)","470","    y_pred = estimator.predict([[-1., 1.]])","471","    assert y_pred < 1.5","472","","473","    sample_weight = [1., .1, 10., 10., .1, .1]","474","    estimator.fit(X, Y, sample_weight=sample_weight)","475","    y_pred = estimator.predict([[-1., 1.]])","476","    assert y_pred > 1.5","477","","478","","479","def test_svm_equivalence_sample_weight_C():","486","    assert_allclose(dual_coef_no_weight, clf.dual_coef_)","487","","488","","489","@pytest.mark.parametrize(","490","    \"Estimator, err_msg\",","491","    [(svm.SVC,","492","      'Invalid input - all samples have zero or negative weights.'),","493","     (svm.NuSVC, '(negative dimensions are not allowed|nu is infeasible)'),","494","     (svm.SVR,","495","      'Invalid input - all samples have zero or negative weights.'),","496","     (svm.NuSVR,","497","      'Invalid input - all samples have zero or negative weights.'),","498","     (svm.OneClassSVM,","499","      'Invalid input - all samples have zero or negative weights.')","500","     ],","501","    ids=['SVC', 'NuSVC', 'SVR', 'NuSVR', 'OneClassSVM']","502",")","503","@pytest.mark.parametrize(","504","    \"sample_weight\",","505","    [[0] * len(Y), [-0.3] * len(Y)],","506","    ids=['weights-are-zero', 'weights-are-negative']","507",")","508","def test_negative_sample_weights_mask_all_samples(Estimator,","509","                                                  err_msg, sample_weight):","510","    est = Estimator(kernel='linear')","511","    with pytest.raises(ValueError, match=err_msg):","512","        est.fit(X, Y, sample_weight=sample_weight)","513","","514","","515","@pytest.mark.parametrize(","516","    \"Classifier, err_msg\",","517","    [(svm.SVC,","518","     'Invalid input - all samples with positive weights have the same label'),","519","     (svm.NuSVC, 'specified nu is infeasible')],","520","    ids=['SVC', 'NuSVC']","521",")","522","@pytest.mark.parametrize(","523","    \"sample_weight\",","524","    [[0, -0.5, 0, 1, 1, 1],","525","     [1, 1, 1, 0, -0.1, -0.3]],","526","    ids=['mask-label-1', 'mask-label-2']","527",")","528","def test_negative_weights_svc_leave_just_one_label(Classifier,","529","                                                   err_msg,","530","                                                   sample_weight):","531","    clf = Classifier(kernel='linear')","532","    with pytest.raises(ValueError, match=err_msg):","533","        clf.fit(X, Y, sample_weight=sample_weight)","534","","535","","536","@pytest.mark.parametrize(","537","    \"Classifier, model\",","538","    [(svm.SVC, {'when-left': [0.3998,  0.4], 'when-right': [0.4,  0.3999]}),","539","     (svm.NuSVC, {'when-left': [0.3333,  0.3333],","540","      'when-right': [0.3333, 0.3333]})],","541","    ids=['SVC', 'NuSVC']","542",")","543","@pytest.mark.parametrize(","544","    \"sample_weight, mask_side\",","545","    [([1, -0.5, 1, 1, 1, 1], 'when-left'),","546","     ([1, 1, 1, 0, 1, 1], 'when-right')],","547","    ids=['partial-mask-label-1', 'partial-mask-label-2']","548",")","549","def test_negative_weights_svc_leave_two_labels(Classifier, model,","550","                                               sample_weight, mask_side):","551","    clf = Classifier(kernel='linear')","552","    clf.fit(X, Y, sample_weight=sample_weight)","553","    assert_allclose(clf.coef_, [model[mask_side]], rtol=1e-3)","554","","555","","556","@pytest.mark.parametrize(","557","    \"Estimator\",","558","    [svm.SVC, svm.NuSVC, svm.NuSVR],","559","    ids=['SVC', 'NuSVC', 'NuSVR']","560",")","561","@pytest.mark.parametrize(","562","    \"sample_weight\",","563","    [[1, -0.5, 1, 1, 1, 1], [1, 1, 1, 0, 1, 1]],","564","    ids=['partial-mask-label-1', 'partial-mask-label-2']","565",")","566","def test_negative_weight_equal_coeffs(Estimator, sample_weight):","567","    # model generates equal coefficients","568","    est = Estimator(kernel='linear')","569","    est.fit(X, Y, sample_weight=sample_weight)","570","    coef = np.abs(est.coef_).ravel()","571","    assert coef[0] == pytest.approx(coef[1], rel=1e-3)"],"delete":["424","def test_sample_weights():","425","    # Test weights on individual samples","426","    # TODO: check on NuSVR, OneClass, etc.","427","    clf = svm.SVC()","428","    clf.fit(X, Y)","429","    assert_array_equal(clf.predict([X[2]]), [1.])","431","    sample_weight = [.1] * 3 + [10] * 3","432","    clf.fit(X, Y, sample_weight=sample_weight)","433","    assert_array_equal(clf.predict([X[2]]), [2.])","441","    assert_array_almost_equal(dual_coef_no_weight, clf.dual_coef_)"]}],"sklearn\/svm\/src\/libsvm\/svm.cpp":[{"add":["3103","\tif(svm_type == C_SVC ||","3104","\t   svm_type == EPSILON_SVR ||","3105","\t   svm_type == NU_SVR ||","3106","\t   svm_type == ONE_CLASS)","3107","\t{","3108","\t\tPREFIX(problem) newprob;","3109","\t\t\/\/ filter samples with negative and null weights ","3110","\t\tremove_zero_weight(&newprob, prob);","3111","","3112","\t\tchar* msg = NULL;","3113","\t\t\/\/ all samples were removed","3114","\t\tif(newprob.l == 0)","3115","\t\t\tmsg =  \"Invalid input - all samples have zero or negative weights.\";","3116","\t\telse if(prob->l != newprob.l && ","3117","\t\t        svm_type == C_SVC)","3118","\t\t{","3119","\t\t\tbool only_one_label = true;","3120","\t\t\tint first_label = newprob.y[0];","3121","\t\t\tfor(int i=1;i<newprob.l;i++)","3122","\t\t\t{","3123","\t\t\t\tif(newprob.y[i] != first_label)","3124","\t\t\t\t{","3125","\t\t\t\t\tonly_one_label = false;","3126","\t\t\t\t\tbreak;","3127","\t\t\t\t}","3128","\t\t\t}","3129","\t\t\tif(only_one_label == true)","3130","\t\t\t\tmsg = \"Invalid input - all samples with positive weights have the same label.\";","3131","\t\t}","3132","","3133","\t\tfree(newprob.x);","3134","\t\tfree(newprob.y);","3135","\t\tfree(newprob.W);","3136","\t\tif(msg != NULL)","3137","\t\t\treturn msg;","3138","\t}"],"delete":[]}],"doc\/whats_new\/v0.22.rst":[{"add":["518","- |Fix| :class:`svm.SVC`, :class:`svm.SVR`, :class:`svm.NuSVR` and","519","  :class:`svm.OneClassSVM` when received values negative or zero","520","  for parameter ``sample_weight`` in method fit(), generated an","521","  invalid model. This behavior occured only in some border scenarios.","522","  Now in these cases, fit() will fail with an Exception.","523","  :pr:`14286` by :user:`Alex Shacked <alexshacked>`.","524",""],"delete":[]}]}},"94c70ff235a19312063ef089ef587957f40db656":{"changes":{"sklearn\/preprocessing\/_encoders.py":"MODIFY"},"diff":{"sklearn\/preprocessing\/_encoders.py":[{"add":["423","            raise ValueError(\"OneHotEncoder in legacy mode cannot handle \"","424","                             \"categories encoded as negative integers. \"","425","                             \"Please set categories='auto' explicitly to \"","426","                             \"be able to use arbitrary integer values as \"","427","                             \"category identifiers.\")","510","            raise ValueError(\"OneHotEncoder in legacy mode cannot handle \"","511","                             \"categories encoded as negative integers. \"","512","                             \"Please set categories='auto' explicitly to \"","513","                             \"be able to use arbitrary integer values as \"","514","                             \"category identifiers.\")"],"delete":["423","            raise ValueError(\"X needs to contain only non-negative integers.\")","506","            raise ValueError(\"X needs to contain only non-negative integers.\")"]}]}},"06ac22d06f54353ea5d5bba244371474c7baf938":{"changes":{"sklearn\/utils\/metaestimators.py":"MODIFY","sklearn\/compose\/tests\/test_column_transformer.py":"MODIFY"},"diff":{"sklearn\/utils\/metaestimators.py":[{"add":["43","        items = getattr(self, attr)","44","        names = []","45","        if items:","46","            names, _ = zip(*items)"],"delete":["43","        names, _ = zip(*getattr(self, attr))"]}],"sklearn\/compose\/tests\/test_column_transformer.py":[{"add":["796","def test_column_transformer_no_estimators_set_params():","797","    ct = ColumnTransformer([]).set_params(n_jobs=2)","798","    assert ct.n_jobs == 2","799","","800",""],"delete":[]}]}},"b8c402722bb0a3dc307443e719ab7b5a037d6de5":{"changes":{"sklearn\/utils\/sparsefuncs_fast.pyx":"MODIFY","doc\/whats_new\/v0.23.rst":"MODIFY","sklearn\/utils\/tests\/test_sparsefuncs.py":"MODIFY","sklearn\/preprocessing\/tests\/test_data.py":"MODIFY"},"diff":{"sklearn\/utils\/sparsefuncs_fast.pyx":[{"add":["336","        if new_n[i] > 0:","337","            updated_n[i] = last_n[i] + new_n[i]","338","            last_over_new_n[i] = dtype(last_n[i]) \/ dtype(new_n[i])","339","            # Unnormalized stats","340","            last_mean[i] *= last_n[i]","341","            last_var[i] *= last_n[i]","342","            new_mean[i] *= new_n[i]","343","            new_var[i] *= new_n[i]","344","            # Update stats","345","            updated_var[i] = (","346","                last_var[i] + new_var[i] +","347","                last_over_new_n[i] \/ updated_n[i] *","348","                (last_mean[i] \/ last_over_new_n[i] - new_mean[i])**2","349","            )","350","            updated_mean[i] = (last_mean[i] + new_mean[i]) \/ updated_n[i]","351","            updated_var[i] \/= updated_n[i]","352","        else:","353","            updated_var[i] = last_var[i]","354","            updated_mean[i] = last_mean[i]","355","            updated_n[i] = last_n[i]"],"delete":["336","        updated_n[i] = last_n[i] + new_n[i]","337","        last_over_new_n[i] = last_n[i] \/ new_n[i]","338","","339","    # Unnormalized stats","340","    for i in range(n_features):","341","        last_mean[i] *= last_n[i]","342","        last_var[i] *= last_n[i]","343","        new_mean[i] *= new_n[i]","344","        new_var[i] *= new_n[i]","345","","346","    # Update stats","347","    for i in range(n_features):","348","        updated_var[i] = (last_var[i] + new_var[i] +","349","                          last_over_new_n[i] \/ updated_n[i] *","350","                          (last_mean[i] \/ last_over_new_n[i] - new_mean[i])**2)","351","        updated_mean[i] = (last_mean[i] + new_mean[i]) \/ updated_n[i]","352","        updated_var[i] \/= updated_n[i]"]}],"doc\/whats_new\/v0.23.rst":[{"add":["358","- |Fix| Fix a bug in :class:`preprocessing.StandardScaler` which was incorrectly","359","  computing statistics when calling `partial_fit` on sparse inputs.","360","  :pr:`16466` by :user:`Guillaume Lemaitre <glemaitre>`.","361",""],"delete":[]}],"sklearn\/utils\/tests\/test_sparsefuncs.py":[{"add":["153","@pytest.mark.parametrize(","154","    \"X1, X2\",","155","    [","156","        (sp.random(5, 2, density=0.8, format='csr', random_state=0),","157","         sp.random(13, 2, density=0.8, format='csr', random_state=0)),","158","        (sp.random(5, 2, density=0.8, format='csr', random_state=0),","159","         sp.hstack([sp.csr_matrix(np.full((13, 1), fill_value=np.nan)),","160","                    sp.random(13, 1, density=0.8, random_state=42)],","161","                   format=\"csr\"))","162","    ]","163",")","164","def test_incr_mean_variance_axis_equivalence_mean_variance(X1, X2):","165","    # non-regression test for:","166","    # https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/16448","167","    # check that computing the incremental mean and variance is equivalent to","168","    # computing the mean and variance on the stacked dataset.","169","    axis = 0","170","    last_mean, last_var = np.zeros(X1.shape[1]), np.zeros(X1.shape[1])","171","    last_n = np.zeros(X1.shape[1], dtype=np.int64)","172","    updated_mean, updated_var, updated_n = incr_mean_variance_axis(","173","        X1, axis, last_mean, last_var, last_n","174","    )","175","    updated_mean, updated_var, updated_n = incr_mean_variance_axis(","176","        X2, axis, updated_mean, updated_var, updated_n","177","    )","178","    X = sp.vstack([X1, X2])","179","    assert_allclose(updated_mean, np.nanmean(X.A, axis=axis))","180","    assert_allclose(updated_var, np.nanvar(X.A, axis=axis))","181","    assert_allclose(updated_n, np.count_nonzero(~np.isnan(X.A), axis=0))","182","","183","","184","def test_incr_mean_variance_no_new_n():","185","    # check the behaviour when we update the variance with an empty matrix","186","    axis = 0","187","    X1 = sp.random(5, 1, density=0.8, random_state=0).tocsr()","188","    X2 = sp.random(0, 1, density=0.8, random_state=0).tocsr()","189","    last_mean, last_var = np.zeros(X1.shape[1]), np.zeros(X1.shape[1])","190","    last_n = np.zeros(X1.shape[1], dtype=np.int64)","191","    last_mean, last_var, last_n = incr_mean_variance_axis(","192","        X1, axis, last_mean, last_var, last_n","193","    )","194","    # update statistic with a column which should ignored","195","    updated_mean, updated_var, updated_n = incr_mean_variance_axis(","196","        X2, axis, last_mean, last_var, last_n","197","    )","198","    assert_allclose(updated_mean, last_mean)","199","    assert_allclose(updated_var, last_var)","200","    assert_allclose(updated_n, last_n)","201","","202",""],"delete":[]}],"sklearn\/preprocessing\/tests\/test_data.py":[{"add":["2473","","2474","","2475","@pytest.mark.parametrize(","2476","    \"X_2\",","2477","    [sparse.random(10, 1, density=0.8, random_state=0),","2478","     sparse.csr_matrix(np.full((10, 1), fill_value=np.nan))]","2479",")","2480","def test_standard_scaler_sparse_partial_fit_finite_variance(X_2):","2481","    # non-regression test for:","2482","    # https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/16448","2483","    X_1 = sparse.random(5, 1, density=0.8)","2484","    scaler = StandardScaler(with_mean=False)","2485","    scaler.fit(X_1).partial_fit(X_2)","2486","    assert np.isfinite(scaler.var_[0])"],"delete":[]}]}},"f15ebb953b7b8971093962514187f447df4c716c":{"changes":{"sklearn\/compose\/tests\/test_column_transformer.py":"MODIFY","sklearn\/compose\/_column_transformer.py":"MODIFY"},"diff":{"sklearn\/compose\/tests\/test_column_transformer.py":[{"add":["20","from sklearn.exceptions import NotFittedError, DataConversionWarning","280","def test_column_transformer_list():","281","    X_list = [","282","        [1, float('nan'), 'a'],","283","        [0, 0, 'b']","284","    ]","285","    expected_result = np.array([","286","        [1, float('nan'), 1, 0],","287","        [-1, 0, 0, 1],","288","    ])","289","","290","    ct = ColumnTransformer([","291","        ('numerical', StandardScaler(), [0, 1]),","292","        ('categorical', OneHotEncoder(), [2]),","293","    ])","294","","295","    with pytest.warns(DataConversionWarning):","296","        # TODO: this warning is not very useful in this case, would be good","297","        # to get rid of it","298","        assert_array_equal(ct.fit_transform(X_list), expected_result)","299","        assert_array_equal(ct.fit(X_list).transform(X_list), expected_result)","300","","301",""],"delete":["20","from sklearn.exceptions import NotFittedError"]}],"sklearn\/compose\/_column_transformer.py":[{"add":["18","from ..pipeline import _fit_transform_one, _transform_one, _name_estimators","22","from ..utils.validation import check_array, check_is_fitted","437","        X = _check_X(X)","487","        X = _check_X(X)","514","def _check_X(X):","515","    \"\"\"Use check_array only on lists and other non-array-likes \/ sparse\"\"\"","516","    if hasattr(X, '__array__') or sparse.issparse(X):","517","        return X","518","    return check_array(X, force_all_finite='allow-nan', dtype=np.object)","519","","520",""],"delete":["18","from ..pipeline import (","19","    _fit_one_transformer, _fit_transform_one, _transform_one, _name_estimators)","23","from ..utils.validation import check_is_fitted"]}]}},"9cd13a1fa16708f94a0d821ac2865fa7d981cad8":{"changes":{"sklearn\/calibration.py":"MODIFY","sklearn\/multioutput.py":"MODIFY","sklearn\/model_selection\/tests\/test_search.py":"MODIFY","sklearn\/discriminant_analysis.py":"MODIFY","sklearn\/svm\/_classes.py":"MODIFY","sklearn\/linear_model\/_logistic.py":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_gradient_boosting.py":"MODIFY","sklearn\/ensemble\/tests\/test_stacking.py":"MODIFY","sklearn\/metrics\/_plot\/tests\/test_plot_precision_recall.py":"MODIFY"},"diff":{"sklearn\/calibration.py":[{"add":["34","class CalibratedClassifierCV(ClassifierMixin,","35","                             MetaEstimatorMixin,","36","                             BaseEstimator):"],"delete":["34","class CalibratedClassifierCV(BaseEstimator, ClassifierMixin,","35","                             MetaEstimatorMixin):"]}],"sklearn\/multioutput.py":[{"add":["62","class _MultiOutputEstimator(MetaEstimatorMixin,","63","                            BaseEstimator,"],"delete":["62","class _MultiOutputEstimator(BaseEstimator, MetaEstimatorMixin,"]}],"sklearn\/model_selection\/tests\/test_search.py":[{"add":["1823","    class TestEstimator(ClassifierMixin, BaseEstimator):"],"delete":["1823","    class TestEstimator(BaseEstimator, ClassifierMixin):"]}],"sklearn\/discriminant_analysis.py":[{"add":["130","class LinearDiscriminantAnalysis(LinearClassifierMixin,","131","                                 TransformerMixin,","132","                                 BaseEstimator):"],"delete":["130","class LinearDiscriminantAnalysis(BaseEstimator, LinearClassifierMixin,","131","                                 TransformerMixin):"]}],"sklearn\/svm\/_classes.py":[{"add":["12","class LinearSVC(LinearClassifierMixin,","13","                SparseCoefMixin,","14","                BaseEstimator):"],"delete":["12","class LinearSVC(BaseEstimator, LinearClassifierMixin,","13","                SparseCoefMixin):"]}],"sklearn\/linear_model\/_logistic.py":[{"add":["1010","class LogisticRegression(LinearClassifierMixin,","1011","                         SparseCoefMixin,","1012","                         BaseEstimator):","1501","class LogisticRegressionCV(LogisticRegression,","1502","                           LinearClassifierMixin,","1503","                           BaseEstimator):"],"delete":["1010","class LogisticRegression(BaseEstimator, LinearClassifierMixin,","1011","                         SparseCoefMixin):","1500","class LogisticRegressionCV(LogisticRegression, BaseEstimator,","1501","                           LinearClassifierMixin):"]}],"sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_gradient_boosting.py":[{"add":["394","    class MinMaxImputer(TransformerMixin, BaseEstimator):"],"delete":["394","    class MinMaxImputer(BaseEstimator, TransformerMixin):"]}],"sklearn\/ensemble\/tests\/test_stacking.py":[{"add":["264","class NoWeightRegressor(RegressorMixin, BaseEstimator):","273","class NoWeightClassifier(ClassifierMixin, BaseEstimator):"],"delete":["264","class NoWeightRegressor(BaseEstimator, RegressorMixin):","273","class NoWeightClassifier(BaseEstimator, ClassifierMixin):"]}],"sklearn\/metrics\/_plot\/tests\/test_plot_precision_recall.py":[{"add":["65","    class MyClassifier(ClassifierMixin, BaseEstimator):"],"delete":["65","    class MyClassifier(BaseEstimator, ClassifierMixin):"]}]}},"84bc8d341e5a0a0d0b20b8acda58a33d72a2c23a":{"changes":{"sklearn\/svm\/_base.py":"MODIFY","doc\/whats_new\/v0.23.rst":"MODIFY","sklearn\/svm\/tests\/test_svm.py":"MODIFY"},"diff":{"sklearn\/svm\/_base.py":[{"add":["16","from ..utils.validation import _num_samples","17","from ..utils.validation import _check_sample_weight, check_consistent_length","146","        if callable(self.kernel):","147","            check_consistent_length(X, y)","148","        else:","149","            X, y = check_X_y(X, y, dtype=np.float64,","150","                             order='C', accept_sparse='csr',","151","                             accept_large_sparse=False)","152","","161","        n_samples = _num_samples(X)","162","        if solver_type != 2 and n_samples != y.shape[0]:","165","                             (n_samples, y.shape[0]))","167","        if self.kernel == \"precomputed\" and n_samples != X.shape[1]:","172","        if sample_weight.shape[0] > 0 and sample_weight.shape[0] != n_samples:","179","        kernel = 'precomputed' if callable(self.kernel) else self.kernel","180","","181","        if kernel == 'precomputed':","182","            # unused but needs to be a float for cython code that ignores","183","            # it anyway","184","            self._gamma = 0.","185","        elif isinstance(self.gamma, str):","209","        self.shape_fit_ = X.shape if hasattr(X, \"shape\") else (n_samples, )","453","        if not callable(self.kernel):","454","            X = check_array(X, accept_sparse='csr', dtype=np.float64,","455","                            order=\"C\", accept_large_sparse=False)","456","","472","        elif not callable(self.kernel) and X.shape[1] != self.shape_fit_[1]:","475","                             (X.shape[1], self.shape_fit_[1]))","931","            raise ValueError(\"Intercept scaling is %r but needs to be greater \"","932","                             \"than 0. To disable fitting an intercept,\""],"delete":["16","from ..utils.validation import _check_sample_weight","145","        X, y = check_X_y(X, y, dtype=np.float64,","146","                         order='C', accept_sparse='csr',","147","                         accept_large_sparse=False)","156","        if solver_type != 2 and X.shape[0] != y.shape[0]:","159","                             (X.shape[0], y.shape[0]))","161","        if self.kernel == \"precomputed\" and X.shape[0] != X.shape[1]:","166","        if sample_weight.shape[0] > 0 and sample_weight.shape[0] != X.shape[0]:","173","        if isinstance(self.gamma, str):","189","        kernel = self.kernel","190","        if callable(kernel):","191","            kernel = 'precomputed'","192","","201","        self.shape_fit_ = X.shape","445","        X = check_array(X, accept_sparse='csr', dtype=np.float64, order=\"C\",","446","                        accept_large_sparse=False)","456","        n_samples, n_features = X.shape","463","        elif n_features != self.shape_fit_[1]:","466","                             (n_features, self.shape_fit_[1]))","922","            raise ValueError(\"Intercept scaling is %r but needs to be greater than 0.\"","923","                             \" To disable fitting an intercept,\""]}],"doc\/whats_new\/v0.23.rst":[{"add":["25","- list models here","212","- |Fix| Fix use of custom kernel not taking float entries such as string","213","  kernels in :class:`svm.SVC` and :class:`svm.SVR`. Note that custom kennels","214","  are now expected to validate their input where they previously received","215","  valid numeric arrays.","216","  :pr:`11296` by `Alexandre Gramfort`_ and  :user:`Georgi Peev <georgipeev>`.","217",""],"delete":["25","- models come here"]}],"sklearn\/svm\/tests\/test_svm.py":[{"add":["22","from sklearn.utils._testing import assert_raise_message","25","from sklearn.utils.validation import _num_samples","128","    clf.fit(np.array(X), Y)","545","    [(svm.SVC, {'when-left': [0.3998, 0.4], 'when-right': [0.4, 0.3999]}),","546","     (svm.NuSVC, {'when-left': [0.3333, 0.3333],","684","                             iris.target.astype(np.float64), 5,","685","                             kernel='linear',","686","                             random_seed=0)","983","    assert_warns(ConvergenceWarning, a.fit, np.array(X), Y)","1029","        assert_array_almost_equal(","1030","            svr.predict(X), np.dot(X, svr.coef_.ravel()) + svr.intercept_","1031","        )","1098","    ))","1252","","1253","","1254","@pytest.mark.parametrize(\"Estimator\", [svm.SVC, svm.SVR])","1255","def test_custom_kernel_not_array_input(Estimator):","1256","    \"\"\"Test using a custom kernel that is not fed with array-like for floats\"\"\"","1257","    data = [\"A A\", \"A\", \"B\", \"B B\", \"A B\"]","1258","    X = np.array([[2, 0], [1, 0], [0, 1], [0, 2], [1, 1]])  # count encoding","1259","    y = np.array([1, 1, 2, 2, 1])","1260","","1261","    def string_kernel(X1, X2):","1262","        assert isinstance(X1[0], str)","1263","        n_samples1 = _num_samples(X1)","1264","        n_samples2 = _num_samples(X2)","1265","        K = np.zeros((n_samples1, n_samples2))","1266","        for ii in range(n_samples1):","1267","            for jj in range(ii, n_samples2):","1268","                K[ii, jj] = X1[ii].count('A') * X2[jj].count('A')","1269","                K[ii, jj] += X1[ii].count('B') * X2[jj].count('B')","1270","                K[jj, ii] = K[ii, jj]","1271","        return K","1272","","1273","    K = string_kernel(data, data)","1274","    assert_array_equal(np.dot(X, X.T), K)","1275","","1276","    svc1 = Estimator(kernel=string_kernel).fit(data, y)","1277","    svc2 = Estimator(kernel='linear').fit(X, y)","1278","    svc3 = Estimator(kernel='precomputed').fit(K, y)","1279","","1280","    assert svc1.score(data, y) == svc3.score(K, y)","1281","    assert svc1.score(data, y) == svc2.score(X, y)","1282","    if hasattr(svc1, 'decision_function'):  # classifier","1283","        assert_allclose(svc1.decision_function(data),","1284","                        svc2.decision_function(X))","1285","        assert_allclose(svc1.decision_function(data),","1286","                        svc3.decision_function(K))","1287","        assert_array_equal(svc1.predict(data), svc2.predict(X))","1288","        assert_array_equal(svc1.predict(data), svc3.predict(K))","1289","    else:  # regressor","1290","        assert_allclose(svc1.predict(data), svc2.predict(X))","1291","        assert_allclose(svc1.predict(data), svc3.predict(K))"],"delete":["22","from sklearn.utils._testing import assert_warns_message, assert_raise_message","127","    clf.fit(X, Y)","544","    [(svm.SVC, {'when-left': [0.3998,  0.4], 'when-right': [0.4,  0.3999]}),","545","     (svm.NuSVC, {'when-left': [0.3333,  0.3333],","683","                                iris.target.astype(np.float64), 5,","684","                                kernel='linear',","685","                                random_seed=0)","982","    assert_warns(ConvergenceWarning, a.fit, X, Y)","1028","        assert_array_almost_equal(svr.predict(X),","1029","                                  np.dot(X, svr.coef_.ravel()) + svr.intercept_)","1096","        ))"]}]}},"0ad7481bb1636252e957efcf82fff6160f0bacfc":{"changes":{"doc\/whats_new\/v0.23.rst":"MODIFY","sklearn\/metrics\/_plot\/roc_curve.py":"MODIFY","sklearn\/metrics\/_plot\/tests\/test_plot_roc_curve.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.23.rst":[{"add":["260","- |Fix| Fixed a bug in :func:`metrics.plot_roc_curve` where","261","  the name of the estimator was passed in the :class:`metrics.RocCurveDisplay`","262","  instead of the parameter `name`. It results in a different plot when calling","263","  :meth:`metrics.RocCurveDisplay.plot` for the subsequent times.","264","  :pr:`16500` by :user:`Guillaume Lemaitre <glemaitre>`.","265",""],"delete":[]}],"sklearn\/metrics\/_plot\/roc_curve.py":[{"add":["167","    classification_error = (","168","        \"{} should be a binary classifier\".format(estimator.__class__.__name__)","169","    )","188","    name = estimator.__class__.__name__ if name is None else name","189","    viz = RocCurveDisplay(","190","        fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name=name","191","    )"],"delete":["167","    classification_error = (\"{} should be a binary classifer\".format(","168","        estimator.__class__.__name__))","187","    viz = RocCurveDisplay(fpr, tpr, roc_auc, estimator.__class__.__name__)"]}],"sklearn\/metrics\/_plot\/tests\/test_plot_roc_curve.py":[{"add":["38","    msg = \"DecisionTreeClassifier should be a binary classifier\"","133","    assert clf.__class__.__name__ in disp.line_.get_label()","135","","136","","137","def test_plot_roc_curve_estimator_name_multiple_calls(pyplot, data_binary):","138","    # non-regression test checking that the `name` used when calling","139","    # `plot_roc_curve` is used as well when calling `disp.plot()`","140","    X, y = data_binary","141","    clf_name = \"my hand-crafted name\"","142","    clf = LogisticRegression().fit(X, y)","143","    disp = plot_roc_curve(clf, X, y, name=clf_name)","144","    assert disp.estimator_name == clf_name","145","    pyplot.close(\"all\")","146","    disp.plot()","147","    assert clf_name in disp.line_.get_label()","148","    pyplot.close(\"all\")","149","    clf_name = \"another_name\"","150","    disp.plot(name=clf_name)","151","    assert clf_name in disp.line_.get_label()"],"delete":["38","    msg = \"DecisionTreeClassifier should be a binary classifer\""]}]}},"bf8eff3feaded1464e81dcc0c4b7f9a3975c014f":{"changes":{"sklearn\/cluster\/tests\/test_k_means.py":"MODIFY","sklearn\/cluster\/k_means_.py":"MODIFY"},"diff":{"sklearn\/cluster\/tests\/test_k_means.py":[{"add":["944","","945","","946","def test_minibatch_kmeans_partial_fit_int_data():","947","    # Issue GH #14314","948","    X = np.array([[-1], [1]], dtype=np.int)","949","    km = MiniBatchKMeans(n_clusters=2)","950","    km.partial_fit(X)","951","    assert km.cluster_centers_.dtype.kind == \"f\""],"delete":[]}],"sklearn\/cluster\/k_means_.py":[{"add":["1424","    array([[2. , 1. ],","1425","           [3.5, 4.5]])","1669","        X = check_array(X, accept_sparse=\"csr\", order=\"C\",","1670","                        dtype=[np.float64, np.float32])"],"delete":["1424","    array([[1, 1],","1425","           [3, 4]])","1669","        X = check_array(X, accept_sparse=\"csr\", order=\"C\")"]}]}},"0b818d3bcf77c94bef0e39802f41b8b486c9c304":{"changes":{"doc\/faq.rst":"MODIFY"},"diff":{"doc\/faq.rst":[{"add":["10","scikit-learn, but not scikit or SciKit nor sci-kit learn.","11","Also not scikits.learn or scikits-learn, which were previously used.","253","","263","","278","","279","How do I set a ``random_state`` for an entire execution?","280","----------------------------------------------------","281","","282","For testing and replicability, it is often important to have the entire execution","283","controlled by a single seed for the pseudo-random number generator used in","284","algorithms that have a randomized component. Scikit-learn does not use its own","285","global random state; whenever a RandomState instance or an integer random seed","286","is not provided as an argument, it relies on the numpy global random state,","287","which can be set using :func:`numpy.random.seed`.","288","For example, to set an execution's numpy global random state to 42, one could","289","execute the following in his or her script::","290","","291","    import numpy as np","292","    np.random.seed(42)","293","","294","However, a global random state is prone to modification by other code during","295","execution. Thus, the only way to ensure replicability is to pass ``RandomState``","296","instances everywhere and ensure that both estimators and cross-validation","297","splitters have their ``random_state`` parameter set."],"delete":["10","scikit-learn, but not scikit or SciKit nor sci-kit learn. Also not scikits.learn or scikits-learn, which where previously used.","203","","251","","261",""]}]}},"1fe00b58949a6b9bce45e9e15eb8b9c138bd6a2e":{"changes":{"sklearn\/utils\/tests\/test_pprint.py":"MODIFY","sklearn\/base.py":"MODIFY"},"diff":{"sklearn\/utils\/tests\/test_pprint.py":[{"add":["461","def test_bruteforce_ellipsis():","462","    # Check that the bruteforce ellipsis (used when the number of non-blank","463","    # characters exceeds N_CHAR_MAX) renders correctly.","465","    lr = LogisticRegression()","466","","467","    # test when the left and right side of the ellipsis aren't on the same","468","    # line.","469","    expected = \"\"\"","470","LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,","471","                   in...","472","                   multi_class='warn', n_jobs=None, penalty='l2',","473","                   random_state=None, solver='warn', tol=0.0001, verbose=0,","474","                   warm_start=False)\"\"\"","475","","476","    expected = expected[1:]  # remove first \\n","477","    assert expected == lr.__repr__(N_CHAR_MAX=150)","478","","479","    # test with very small N_CHAR_MAX","480","    # Note that N_CHAR_MAX is not strictly enforced, but it's normal: to avoid","481","    # weird reprs we still keep the whole line of the right part (after the","482","    # ellipsis).","483","    expected = \"\"\"","484","Lo...","485","                   warm_start=False)\"\"\"","486","","487","    expected = expected[1:]  # remove first \\n","488","    assert expected == lr.__repr__(N_CHAR_MAX=4)","489","","490","    # test with N_CHAR_MAX == number of non-blank characters: In this case we","491","    # don't want ellipsis","492","    full_repr = lr.__repr__(N_CHAR_MAX=float('inf'))","493","    n_nonblank = len(''.join(full_repr.split()))","494","    assert lr.__repr__(N_CHAR_MAX=n_nonblank) == full_repr","495","    assert '...' not in full_repr","496","","497","    # test with N_CHAR_MAX == number of non-blank characters - 10: the left and","498","    # right side of the ellispsis are on different lines. In this case we","499","    # want to expend the whole line of the right side","500","    expected = \"\"\"","501","LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,","502","                   intercept_scaling=1, l1_ratio=None, max_i...","503","                   multi_class='warn', n_jobs=None, penalty='l2',","504","                   random_state=None, solver='warn', tol=0.0001, verbose=0,","505","                   warm_start=False)\"\"\"","506","    expected = expected[1:]  # remove first \\n","507","    assert expected == lr.__repr__(N_CHAR_MAX=n_nonblank - 10)","508","","509","    # test with N_CHAR_MAX == number of non-blank characters - 10: the left and","510","    # right side of the ellispsis are on the same line. In this case we don't","511","    # want to expend the whole line of the right side, just add the ellispsis","512","    # between the 2 sides.","513","    expected = \"\"\"","514","LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,","515","                   intercept_scaling=1, l1_ratio=None, max_iter...,","516","                   multi_class='warn', n_jobs=None, penalty='l2',","517","                   random_state=None, solver='warn', tol=0.0001, verbose=0,","518","                   warm_start=False)\"\"\"","519","    expected = expected[1:]  # remove first \\n","520","    assert expected == lr.__repr__(N_CHAR_MAX=n_nonblank - 4)","521","","522","    # test with N_CHAR_MAX == number of non-blank characters - 2: the left and","523","    # right side of the ellispsis are on the same line, but adding the ellipsis","524","    # would actually make the repr longer. So we don't add the ellipsis.","525","    expected = \"\"\"","526","LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,","527","                   intercept_scaling=1, l1_ratio=None, max_iter=100,","528","                   multi_class='warn', n_jobs=None, penalty='l2',","529","                   random_state=None, solver='warn', tol=0.0001, verbose=0,","530","                   warm_start=False)\"\"\"","531","    expected = expected[1:]  # remove first \\n","532","    assert expected == lr.__repr__(N_CHAR_MAX=n_nonblank - 2)"],"delete":["461","def test_length_constraint():","462","    # When repr is still too long, use bruteforce ellipsis","463","    # repr is a very long line so we don't check for equality here, just that","464","    # ellipsis has been done. It's not the ellipsis from before because the","465","    # number of elements in the dict is only 1.","466","    vocabulary = {0: 'hello' * 1000}","467","    vectorizer = CountVectorizer(vocabulary=vocabulary)","468","    repr_ = vectorizer.__repr__()","469","    assert '...' in repr_"]}],"sklearn\/base.py":[{"add":["10","import re","236","    def __repr__(self, N_CHAR_MAX=700):","237","        # N_CHAR_MAX is the (approximate) maximum number of non-blank","238","        # characters to render. We pass it as an optional parameter to ease","239","        # the tests.","240","","252","        # Use bruteforce ellipsis when there are a lot of non-blank characters","253","        n_nonblank = len(''.join(repr_.split()))","254","        if n_nonblank > N_CHAR_MAX:","255","            lim = N_CHAR_MAX \/\/ 2  # apprx number of chars to keep on both ends","256","            regex = r'^(\\s*\\S){%d}' % lim","257","            # The regex '^(\\s*\\S){%d}' % n","258","            # matches from the start of the string until the nth non-blank","259","            # character:","260","            # - ^ matches the start of string","261","            # - (pattern){n} matches n repetitions of pattern","262","            # - \\s*\\S matches a non-blank char following zero or more blanks","263","            left_lim = re.match(regex, repr_).end()","264","            right_lim = re.match(regex, repr_[::-1]).end()","265","","266","            if '\\n' in repr_[left_lim:-right_lim]:","267","                # The left side and right side aren't on the same line.","268","                # To avoid weird cuts, e.g.:","269","                # categoric...ore',","270","                # we need to start the right side with an appropriate newline","271","                # character so that it renders properly as:","272","                # categoric...","273","                # handle_unknown='ignore',","274","                # so we add [^\\n]*\\n which matches until the next \\n","275","                regex += r'[^\\n]*\\n'","276","                right_lim = re.match(regex, repr_[::-1]).end()","277","","278","            ellipsis = '...'","279","            if left_lim + len(ellipsis) < len(repr_) - right_lim:","280","                # Only add ellipsis if it results in a shorter repr","281","                repr_ = repr_[:left_lim] + '...' + repr_[-right_lim:]","282",""],"delete":["235","    def __repr__(self):","238","        N_CHAR_MAX = 700  # number of non-whitespace or newline chars","248","        # Use bruteforce ellipsis if string is very long","249","        if len(''.join(repr_.split())) > N_CHAR_MAX:  # check non-blank chars","250","            lim = N_CHAR_MAX \/\/ 2","251","            repr_ = repr_[:lim] + '...' + repr_[-lim:]"]}]}},"eaf0a044fdc084ebeeb9bbfbcf42e6df2b1491bb":{"changes":{"sklearn\/svm\/src\/liblinear\/liblinear_helper.c":"MODIFY","sklearn\/svm\/src\/liblinear\/linear.h":"MODIFY","sklearn\/svm\/src\/newrand\/newrand.h":"ADD","doc\/whats_new\/v0.23.rst":"MODIFY","sklearn\/svm\/src\/libsvm\/svm.cpp":"MODIFY","sklearn\/svm\/setup.py":"MODIFY","sklearn\/svm\/src\/liblinear\/linear.cpp":"MODIFY","sklearn\/metrics\/tests\/test_ranking.py":"MODIFY","sklearn\/svm\/src\/libsvm\/LIBSVM_CHANGES":"MODIFY"},"diff":{"sklearn\/svm\/src\/liblinear\/liblinear_helper.c":[{"add":["184","    set_seed(seed);"],"delete":["184","    srand(seed);"]}],"sklearn\/svm\/src\/liblinear\/linear.h":[{"add":["51","void set_seed(unsigned seed);","52",""],"delete":[]}],"sklearn\/svm\/src\/newrand\/newrand.h":[{"add":[],"delete":[]}],"doc\/whats_new\/v0.23.rst":[{"add":["27","- Any model using the :func:`svm.libsvm` or the :func:`svm.liblinear` solver,","28","  including :class:`svm.LinearSVC`, :class:`svm.LinearSVR`,","29","  :class:`svm.NuSVC`, :class:`svm.NuSVR`, :class:`svm.OneClassSVM`,","30","  :class:`svm.SVC`, :class:`svm.SVR`, :class:`linear_model.LogisticRegression`.","31","  |Efficiency| |Fix|","32","","304","  or 'd'). :pr:`16159` by :user:`Rick Mackenbach <Rick-Mackenbach>` and","386","- |Fix| |Efficiency| Improved ``libsvm`` and ``liblinear`` random number","387","  generators used to randomly select coordinates in the coordinate descent","388","  algorithms. Platform-dependent C ``rand()`` was used, which is only able to","389","  generate numbers up to ``32767`` on windows platform (see this `blog","390","  post <https:\/\/codeforces.com\/blog\/entry\/61587>`) and also has poor","391","  randomization power as suggested by `this presentation","392","  <https:\/\/channel9.msdn.com\/Events\/GoingNative\/2013\/rand-Considered-Harmful>`.","393","  It was replaced with C++11 ``mt19937``, a Mersenne Twister that correctly","394","  generates 31bits\/63bits random numbers on all platforms. In addition, the","395","  crude \"modulo\" postprocessor used to get a random number in a bounded","396","  interval was replaced by the tweaked Lemire method as suggested by `this blog","397","  post <http:\/\/www.pcg-random.org\/posts\/bounded-rands.html>`.","398","  Any model using the :func:`svm.libsvm` or the :func:`svm.liblinear` solver,","399","  including :class:`svm.LinearSVC`, :class:`svm.LinearSVR`,","400","  :class:`svm.NuSVC`, :class:`svm.NuSVR`, :class:`svm.OneClassSVM`,","401","  :class:`svm.SVC`, :class:`svm.SVR`, :class:`linear_model.LogisticRegression`,","402","  is affected. In particular users can expect a better convergence when the","403","  number of samples (LibSVM) or the number of features (LibLinear) is large.","404","  :pr:`13511` by :user:`Sylvain Mari¨¦ <smarie>`.","405",""],"delete":["298","  or 'd'). :pr:`16159` by :user:`Rick Mackenbach <Rick-Mackenbach>` and "]}],"sklearn\/svm\/src\/libsvm\/svm.cpp":[{"add":["50","   Modified 2020:","51","","52","   - Improved random number generator by using a mersenne twister + tweaked","53","     lemire postprocessor. This fixed a convergence issue on windows targets.","54","     Sylvain Marie,","55","     see <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/13511#issuecomment-481729756>","56","","66","#include <climits>","67","#include <random>","69","#include \"..\/newrand\/newrand.h\"","2105","\t\tint j = i+bounded_rand_int(prob->l-i);","2360","        set_seed(param->random_seed);","2640","        set_seed(param->random_seed);","2662","\t\t\t\tint j = i+bounded_rand_int(count[c]-i);","2699","\t\t\tint j = i+bounded_rand_int(l-i);"],"delete":["2095","\t\tint j = i+rand()%(prob->l-i);","2350","        srand(param->random_seed);","2630","        srand(param->random_seed);","2652","\t\t\t\tint j = i+rand()%(count[c]-i);","2689","\t\t\tint j = i+rand()%(l-i);"]}],"sklearn\/svm\/setup.py":[{"add":["18","                                join('src', 'libsvm', 'svm.h'),","19","                                join('src', 'newrand', 'newrand.h')],","23","                       # Use C++11 to use the random number generator fix","24","                       extra_compiler_args=['-std=c++11'],","31","                      join('src', 'libsvm', 'svm.h'),","32","                      join('src', 'newrand', 'newrand.h')]","37","                                       join('src', 'libsvm'),","38","                                       join('src', 'newrand')],","48","    # precompile liblinear to use C++11 flag","49","    config.add_library('liblinear-skl',","50","                       sources=[join('src', 'liblinear', 'linear.cpp'),","51","                                join('src', 'liblinear', 'tron.cpp')],","52","                       depends=[join('src', 'liblinear', 'linear.h'),","53","                                join('src', 'liblinear', 'tron.h'),","54","                                join('src', 'newrand', 'newrand.h')],","55","                       # Force C++ linking in case gcc is picked up instead","56","                       # of g++ under windows with some versions of MinGW","57","                       extra_link_args=['-lstdc++'],","58","                       # Use C++11 to use the random number generator fix","59","                       extra_compiler_args=['-std=c++11'],","60","                       )","62","    liblinear_sources = ['_liblinear.pyx']","64","                         join('src', 'newrand', 'newrand.h'),","69","                         libraries=['liblinear-skl'] + libraries,","71","                                       join('.', 'src', 'newrand'),","85","                                       join(\"src\", \"libsvm\"),","86","                                       join(\"src\", \"newrand\")],","88","                                  join('src', 'newrand', 'newrand.h'),"],"delete":["18","                                join('src', 'libsvm', 'svm.h')],","28","                      join('src', 'libsvm', 'svm.h')]","33","                                       join('src', 'libsvm')],","43","    liblinear_sources = ['_liblinear.pyx',","44","                         join('src', 'liblinear', '*.cpp')]","51","                         libraries=libraries,","66","                                       join(\"src\", \"libsvm\")],"]}],"sklearn\/svm\/src\/liblinear\/linear.cpp":[{"add":["0","\/*","4","","24","","25","   Modified 2020:","26","   - Improved random number generator by using a mersenne twister + tweaked","27","     lemire postprocessor. This fixed a convergence issue on windows targets.","28","     Sylvain Marie","29","     See <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/13511#issuecomment-481729756>","30","","41","#include <climits>","42","#include <random>","43","#include \"..\/newrand\/newrand.h\"","44","","469","\/\/ A coordinate descent algorithm for","474","\/\/","477","\/\/  C^m_i = C if m  = y_i,","478","\/\/  C^m_i = 0 if m != y_i,","479","\/\/  and w_m(\\alpha) = \\sum_i \\alpha^m_i x_i","481","\/\/ Given:","592","\t\/\/ Initial alpha can be set here. Note that","628","\t\t\tint j = i+bounded_rand_int(active_size-i);","788","\/\/ A coordinate descent algorithm for","793","\/\/","795","\/\/  D is a diagonal matrix","806","\/\/ Given:","811","\/\/","901","\t\t\tint j = i+bounded_rand_int(active_size-i);","1022","\/\/ A coordinate descent algorithm for","1027","\/\/","1029","\/\/  D is a diagonal matrix","1038","\/\/ Given:","1044","\/\/ See Algorithm 4 of Ho and Lin, 2012","1120","\t\t\tint j = i+bounded_rand_int(active_size-i);","1266","\/\/ A coordinate descent algorithm for","1271","\/\/","1272","\/\/  where Qij = yi yj xi^T xj and","1276","\/\/ Given:","1346","\t\t\tint j = i+bounded_rand_int(l-i);","1534","\t\t\tint i = j+bounded_rand_int(active_size-j);","1916","\t\t\t\tint i = j+bounded_rand_int(QP_active_size-j);","2247","","2254","                        j++;","2607","\t\tint j = i+bounded_rand_int(l-i);"],"delete":["0","\/* ","4","   ","458","\/\/ A coordinate descent algorithm for ","463","\/\/ ","466","\/\/  C^m_i = C if m  = y_i, ","467","\/\/  C^m_i = 0 if m != y_i, ","468","\/\/  and w_m(\\alpha) = \\sum_i \\alpha^m_i x_i ","470","\/\/ Given: ","581","\t\/\/ Initial alpha can be set here. Note that ","617","\t\t\tint j = i+rand()%(active_size-i);","777","\/\/ A coordinate descent algorithm for ","782","\/\/ ","784","\/\/  D is a diagonal matrix ","795","\/\/ Given: ","800","\/\/ ","890","\t\t\tint j = i+rand()%(active_size-i);","1011","\/\/ A coordinate descent algorithm for ","1016","\/\/ ","1018","\/\/  D is a diagonal matrix ","1027","\/\/ Given: ","1033","\/\/ See Algorithm 4 of Ho and Lin, 2012   ","1109","\t\t\tint j = i+rand()%(active_size-i);","1255","\/\/ A coordinate descent algorithm for ","1260","\/\/ ","1261","\/\/  where Qij = yi yj xi^T xj and ","1265","\/\/ Given: ","1335","\t\t\tint j = i+rand()%(l-i);","1523","\t\t\tint i = j+rand()%(active_size-j);","1905","\t\t\t\tint i = j+rand()%(QP_active_size-j);","2236","        ","2243","                        j++;      ","2596","\t\tint j = i+rand()%(l-i);","3059",""]}],"sklearn\/metrics\/tests\/test_ranking.py":[{"add":["739","    # `_average_precision` is not very precise in case of 0.5 ties: be tolerant","741","                        precision_recall_auc, decimal=2)"],"delete":["740","                        precision_recall_auc, decimal=3)"]}],"sklearn\/svm\/src\/libsvm\/LIBSVM_CHANGES":[{"add":["6","  * Improved random number generator (fix on windows, enhancement on other","7","    platforms). See <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/13511#issuecomment-481729756>"],"delete":[]}]}},"5fc5c6e62e163a20b890b64cb1efa8ed151bbc18":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/feature_extraction\/text.py":"MODIFY","sklearn\/feature_extraction\/tests\/test_text.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["53",":mod:`sklearn.feature_extraction.text`","54","......................................","55","","56","- |Fix| Fixed a bug in :class:`feature_extraction.text.CountVectorizer` which ","57","  would result in the sparse feature matrix having conflicting `indptr` and","58","  `indices` precisions under very large vocabularies. :issue:`11295` by","59","  :user:`Gabriel Vacaliuc <gvacaliuc>`.","60",""],"delete":[]}],"sklearn\/feature_extraction\/text.py":[{"add":["33","from ..utils import _IS_32BIT","874","        map_index = np.empty(len(sorted_features), dtype=X.indices.dtype)","964","            if _IS_32BIT:","967","                                  'which is unsupported with 32 bit Python.')","968","                                 .format(indptr[-1]))","969","            indices_dtype = np.int64"],"delete":["873","        map_index = np.empty(len(sorted_features), dtype=np.int32)","963","            if sp_version >= (0, 14):","964","                indices_dtype = np.int64","965","            else:","968","                                  ' which is unsupported with scipy {}. '","969","                                  'Please upgrade to scipy >=0.14')","970","                                 .format(indptr[-1], '.'.join(sp_version)))"]}],"sklearn\/feature_extraction\/tests\/test_text.py":[{"add":["38","                                   fails_if_pypy, assert_allclose_dense_sparse,","39","                                   skip_if_32bit)","1147","@skip_if_32bit","1148","def test_countvectorizer_sort_features_64bit_sparse_indices():","1149","    \"\"\"","1150","    Check that CountVectorizer._sort_features preserves the dtype of its sparse","1151","    feature matrix.","1152","","1153","    This test is skipped on 32bit platforms, see:","1154","        https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/11295","1155","    for more details.","1156","    \"\"\"","1157","","1158","    X = sparse.csr_matrix((5, 5), dtype=np.int64)","1159","","1160","    # force indices and indptr to int64.","1161","    INDICES_DTYPE = np.int64","1162","    X.indices = X.indices.astype(INDICES_DTYPE)","1163","    X.indptr = X.indptr.astype(INDICES_DTYPE)","1164","","1165","    vocabulary = {","1166","            \"scikit-learn\": 0,","1167","            \"is\": 1,","1168","            \"great!\": 2","1169","            }","1170","","1171","    Xs = CountVectorizer()._sort_features(X, vocabulary)","1172","","1173","    assert INDICES_DTYPE == Xs.indices.dtype","1174","","1175",""],"delete":["38","                                   fails_if_pypy, assert_allclose_dense_sparse)"]}]}},"986a49bbe018aa8060f53c146fc06f278f80b7a6":{"changes":{"sklearn\/model_selection\/_validation.py":"MODIFY","sklearn\/cross_validation.py":"MODIFY","sklearn\/model_selection\/tests\/test_validation.py":"MODIFY"},"diff":{"sklearn\/model_selection\/_validation.py":[{"add":["624","        X_train, y_train = _safe_split(estimator, X, y, train)","625","        X_test, y_test = _safe_split(estimator, X, y, test, train)","626","        estimator.fit(X_train, y_train)","627","        avg_score.append(scorer(estimator, X_test, y_test))","640","    return safe_indexing(y, indices)"],"delete":["624","        estimator.fit(X[train], y[train])","625","        avg_score.append(scorer(estimator, X[test], y[test]))","638","    return y[indices]"]}],"sklearn\/cross_validation.py":[{"add":["1758","        X_train, y_train = _safe_split(estimator, X, y, train)","1759","        X_test, y_test = _safe_split(estimator, X, y, test, train)","1760","        estimator.fit(X_train, y_train)","1761","        avg_score.append(scorer(estimator, X_test, y_test))","1774","    return safe_indexing(y, ind)"],"delete":["1758","        estimator.fit(X[train], y[train])","1759","        avg_score.append(scorer(estimator, X[test], y[test]))","1772","    return y[ind]"]}],"sklearn\/model_selection\/tests\/test_validation.py":[{"add":["968","","969","","970","def test_permutation_test_score_pandas():","971","    # check permutation_test_score doesn't destroy pandas dataframe","972","    types = [(MockDataFrame, MockDataFrame)]","973","    try:","974","        from pandas import Series, DataFrame","975","        types.append((Series, DataFrame))","976","    except ImportError:","977","        pass","978","    for TargetType, InputFeatureType in types:","979","        # X dataframe, y series","980","        iris = load_iris()","981","        X, y = iris.data, iris.target","982","        X_df, y_ser = InputFeatureType(X), TargetType(y)","983","        check_df = lambda x: isinstance(x, InputFeatureType)","984","        check_series = lambda x: isinstance(x, TargetType)","985","        clf = CheckingClassifier(check_X=check_df, check_y=check_series)","986","        permutation_test_score(clf, X_df, y_ser)"],"delete":[]}]}},"bbb1e3b6e9af85d04a01465abde7620e2ac5a77b":{"changes":{"doc\/whats_new\/v0.23.rst":"MODIFY","sklearn\/metrics\/_plot\/precision_recall_curve.py":"MODIFY","sklearn\/metrics\/_plot\/tests\/test_plot_precision_recall.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.23.rst":[{"add":["254","- |Fix| Fixed a bug in :func:`metrics.plot_precision_recall_curve` where the","255","  name of the estimator was passed in the","256","  :class:`metrics.PrecisionRecallDisplay` instead of the parameter `name`. It","257","  results in a different plot when calling","258","  :meth:`metrics.PrecisionRecallDisplay.plot` for the subsequent times.","259","  :pr:`#16505` by :user:`Guillaume Lemaitre <glemaitre>`.","260",""],"delete":[]}],"sklearn\/metrics\/_plot\/precision_recall_curve.py":[{"add":["143","    classification_error = (\"{} should be a binary classifier\".format(","165","    name = name if name is not None else estimator.__class__.__name__","166","    viz = PrecisionRecallDisplay(","167","        precision=precision, recall=recall,","168","        average_precision=average_precision, estimator_name=name","169","    )"],"delete":["143","    classification_error = (\"{} should be a binary classifer\".format(","165","    viz = PrecisionRecallDisplay(precision, recall, average_precision,","166","                                 estimator.__class__.__name__)"]}],"sklearn\/metrics\/_plot\/tests\/test_plot_precision_recall.py":[{"add":["39","    msg = \"DecisionTreeClassifier should be a binary classifier\"","44","    msg = \"DecisionTreeRegressor should be a binary classifier\"","155","","156","","157","def test_plot_precision_recall_curve_estimator_name_multiple_calls(pyplot):","158","    # non-regression test checking that the `name` used when calling","159","    # `plot_roc_curve` is used as well when calling `disp.plot()`","160","    X, y = make_classification(n_classes=2, n_samples=50, random_state=0)","161","    clf_name = \"my hand-crafted name\"","162","    clf = LogisticRegression().fit(X, y)","163","    disp = plot_precision_recall_curve(clf, X, y, name=clf_name)","164","    assert disp.estimator_name == clf_name","165","    pyplot.close(\"all\")","166","    disp.plot()","167","    assert clf_name in disp.line_.get_label()","168","    pyplot.close(\"all\")","169","    clf_name = \"another_name\"","170","    disp.plot(name=clf_name)","171","    assert clf_name in disp.line_.get_label()"],"delete":["39","    msg = \"DecisionTreeClassifier should be a binary classifer\"","44","    msg = \"DecisionTreeRegressor should be a binary classifer\""]}]}},"b55d12e60369300b97501296e87e13cd15a0e5b7":{"changes":{"sklearn\/ensemble\/tests\/test_voting.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY","sklearn\/ensemble\/voting.py":"MODIFY"},"diff":{"sklearn\/ensemble\/tests\/test_voting.py":[{"add":["70","    with pytest.raises(AttributeError, match=msg):","71","        eclf.predict_proba","72","","73","    assert not hasattr(eclf, \"predict_proba\")","74","    eclf.fit(X, y)","75","    assert not hasattr(eclf, \"predict_proba\")"],"delete":["70","    assert_raise_message(AttributeError, msg, eclf.predict_proba, X)"]}],"doc\/whats_new\/v0.22.rst":[{"add":["71","- |Feature| :class:`ensemble.HistGradientBoostingClassifier` and","72","  :class:`ensemble.HistGradientBoostingRegressor` have an additional","73","  parameter called `warm_start` that enables warm starting. :pr:`14012` by","74","  :user:`Johann Faouzi <johannfaouzi>`.","75","","81","- |Fix| :func:`ensemble.VotingClassifier.predict_proba` will no longer be","82","  present when `voting='hard'`. :pr:`14287` by `Thomas Fan`_."],"delete":["76","- |Feature| :class:`ensemble.HistGradientBoostingClassifier` and","77","  :class:`ensemble.HistGradientBoostingRegressor` have an additional","78","  parameter called `warm_start` that enables warm starting. :pr:`14012` by","79","  :user:`Johann Faouzi <johannfaouzi>`."]}],"sklearn\/ensemble\/voting.py":[{"add":["334","        if self.voting == 'hard':","335","            raise AttributeError(\"predict_proba is not available when\"","336","                                 \" voting=%r\" % self.voting)"],"delete":["315","        if self.voting == 'hard':","316","            raise AttributeError(\"predict_proba is not available when\"","317","                                 \" voting=%r\" % self.voting)"]}]}},"d6b368e81892115d4be0697dde0666dca38ee89a":{"changes":{"sklearn\/ensemble\/gradient_boosting.py":"MODIFY","sklearn\/ensemble\/tests\/test_gradient_boosting.py":"MODIFY"},"diff":{"sklearn\/ensemble\/gradient_boosting.py":[{"add":["1480","                # XXX clean this once we have a support_sample_weight tag","1481","                if sample_weight_is_none:","1482","                    self.init_.fit(X, y)","1483","                else:","1484","                    msg = (\"The initial estimator {} does not support sample \"","1485","                           \"weights.\".format(self.init_.__class__.__name__))","1486","                    try:","1487","                        self.init_.fit(X, y, sample_weight=sample_weight)","1488","                    except TypeError:  # regular estimator without SW support","1489","                        raise ValueError(msg)","1490","                    except ValueError as e:","1491","                        if 'not enough values to unpack' in str(e):  # pipeline","1492","                            raise ValueError(msg) from e","1493","                        else:  # regular estimator whose input checking failed","1494","                            raise"],"delete":["1480","                try:","1481","                    self.init_.fit(X, y, sample_weight=sample_weight)","1482","                except TypeError:","1483","                    if sample_weight_is_none:","1484","                        self.init_.fit(X, y)","1485","                    else:","1486","                        raise ValueError(","1487","                            \"The initial estimator {} does not support sample \"","1488","                            \"weights.\".format(self.init_.__class__.__name__))","1493",""]}],"sklearn\/ensemble\/tests\/test_gradient_boosting.py":[{"add":["41","from sklearn.pipeline import make_pipeline","42","from sklearn.linear_model import LinearRegression","43","from sklearn.svm import NuSVR","1383","def test_gradient_boosting_with_init_pipeline():","1384","    # Check that the init estimator can be a pipeline (see issue #13466)","1385","","1386","    X, y = make_regression(random_state=0)","1387","    init = make_pipeline(LinearRegression())","1388","    gb = GradientBoostingRegressor(init=init)","1389","    gb.fit(X, y)  # pipeline without sample_weight works fine","1390","","1391","    with pytest.raises(","1392","            ValueError,","1393","            match='The initial estimator Pipeline does not support sample '","1394","                  'weights'):","1395","        gb.fit(X, y, sample_weight=np.ones(X.shape[0]))","1396","","1397","    # Passing sample_weight to a pipeline raises a ValueError. This test makes","1398","    # sure we make the distinction between ValueError raised by a pipeline that","1399","    # was passed sample_weight, and a ValueError raised by a regular estimator","1400","    # whose input checking failed.","1401","    with pytest.raises(","1402","            ValueError,","1403","            match='nu <= 0 or nu > 1'):","1404","        # Note that NuSVR properly supports sample_weight","1405","        init = NuSVR(gamma='auto', nu=1.5)","1406","        gb = GradientBoostingRegressor(init=init)","1407","        gb.fit(X, y, sample_weight=np.ones(X.shape[0]))","1408","","1409",""],"delete":[]}]}},"5a9ce9fe3fe6cdf2574f0142e3f38698155f707a":{"changes":{".travis.yml":"MODIFY","build_tools\/travis\/install.sh":"MODIFY","doc\/developers\/contributing.rst":"MODIFY","sklearn\/preprocessing\/tests\/test_common.py":"MODIFY"},"diff":{".travis.yml":[{"add":["58","       env: DISTRIB=\"scipy-dev\"","59","       if: type = cron OR commit_message ~ \/\\[scipy-dev\\]\/"],"delete":["58","       env: DISTRIB=\"scipy-dev-wheels\"","59","       if: type = cron"]}],"build_tools\/travis\/install.sh":[{"add":["80","elif [[ \"$DISTRIB\" == \"scipy-dev\" ]]; then"],"delete":["80","elif [[ \"$DISTRIB\" == \"scipy-dev-wheels\" ]]; then"]}],"doc\/developers\/contributing.rst":[{"add":["402","     [scipy-dev]            Add a Travis build with our dependencies (numpy, scipy, etc ...) development builds"],"delete":[]}],"sklearn\/preprocessing\/tests\/test_common.py":[{"add":["0","import warnings","1","","114","                warnings.simplefilter('ignore', PendingDeprecationWarning)","119","                warnings.simplefilter('ignore', PendingDeprecationWarning)"],"delete":[]}]}},"b3e122a0b5d7d2bb7770c4ab30557b05cb550f46":{"changes":{"sklearn\/ensemble\/voting_classifier.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/ensemble\/tests\/test_voting_classifier.py":"MODIFY"},"diff":{"sklearn\/ensemble\/voting_classifier.py":[{"add":["21","from ..externals.joblib import Parallel, delayed","22","from ..utils.validation import has_fit_parameter, check_is_fitted","23","","24","","25","def _parallel_fit_estimator(estimator, X, y, sample_weight):","26","    \"\"\"Private function used to fit an estimator within a job.\"\"\"","27","    if sample_weight is not None:","28","        estimator.fit(X, y, sample_weight)","29","    else:","30","        estimator.fit(X, y)","31","    return estimator","59","    n_jobs : int, optional (default=1)","60","        The number of jobs to run in parallel for ``fit``.","61","        If -1, then the number of jobs is set to the number of cores.","62","","102","    def __init__(self, estimators, voting='hard', weights=None, n_jobs=1):","107","        self.n_jobs = n_jobs","109","    def fit(self, X, y, sample_weight=None):","121","        sample_weight : array-like, shape = [n_samples] or None","122","            Sample weights. If None, then samples are equally weighted.","123","            Note that this is supported only if all underlying estimators","124","            support sample weights.","125","","148","        if sample_weight is not None:","149","            for name, step in self.estimators:","150","                if not has_fit_parameter(step, 'sample_weight'):","151","                    raise ValueError('Underlying estimator \\'%s\\' does not support'","152","                                     ' sample weights.' % name)","153","","159","        transformed_y = self.le_.transform(y)","160","","161","        self.estimators_ = Parallel(n_jobs=self.n_jobs)(","162","                delayed(_parallel_fit_estimator)(clone(clf), X, transformed_y,","163","                    sample_weight)","164","                    for _, clf in self.estimators)","244","          array-like = [n_samples, n_classifiers]"],"delete":["21","from ..utils.validation import check_is_fitted","88","    def __init__(self, estimators, voting='hard', weights=None):","89","","95","    def fit(self, X, y):","134","        for name, clf in self.estimators:","135","            fitted_clf = clone(clf).fit(X, self.le_.transform(y))","136","            self.estimators_.append(fitted_clf)","216","          array-like = [n_classifiers, n_samples]"]}],"doc\/whats_new.rst":[{"add":["283","   - Added ``n_jobs`` and ``sample_weights`` parameters for :class:`VotingClassifier`","284","     to fit underlying estimators in parallel.","285","     (`#5805 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/5805>`_)","286","     By `Ibraim Ganiev`_.","287",""],"delete":[]}],"sklearn\/ensemble\/tests\/test_voting_classifier.py":[{"add":["0","\"\"\"Testing for the VotingClassifier\"\"\"","3","from sklearn.utils.testing import assert_almost_equal, assert_array_equal","17","from sklearn.neighbors import KNeighborsClassifier","210","","211","","212","def test_parallel_predict():","213","    \"\"\"Check parallel backend of VotingClassifier on toy dataset.\"\"\"","214","    clf1 = LogisticRegression(random_state=123)","215","    clf2 = RandomForestClassifier(random_state=123)","216","    clf3 = GaussianNB()","217","    X = np.array([[-1.1, -1.5], [-1.2, -1.4], [-3.4, -2.2], [1.1, 1.2]])","218","    y = np.array([1, 1, 2, 2])","219","","220","    eclf1 = VotingClassifier(estimators=[","221","        ('lr', clf1), ('rf', clf2), ('gnb', clf3)],","222","        voting='soft',","223","        n_jobs=1).fit(X, y)","224","    eclf2 = VotingClassifier(estimators=[","225","        ('lr', clf1), ('rf', clf2), ('gnb', clf3)],","226","        voting='soft',","227","        n_jobs=2).fit(X, y)","228","","229","    assert_array_equal(eclf1.predict(X), eclf2.predict(X))","230","    assert_array_equal(eclf1.predict_proba(X), eclf2.predict_proba(X))","231","","232","","233","def test_sample_weight():","234","    \"\"\"Tests sample_weight parameter of VotingClassifier\"\"\"","235","    clf1 = LogisticRegression(random_state=123)","236","    clf2 = RandomForestClassifier(random_state=123)","237","    clf3 = SVC(probability=True, random_state=123)","238","    eclf1 = VotingClassifier(estimators=[","239","        ('lr', clf1), ('rf', clf2), ('svc', clf3)],","240","        voting='soft').fit(X, y, sample_weight=np.ones((len(y),)))","241","    eclf2 = VotingClassifier(estimators=[","242","        ('lr', clf1), ('rf', clf2), ('svc', clf3)],","243","        voting='soft').fit(X, y)","244","    assert_array_equal(eclf1.predict(X), eclf2.predict(X))","245","    assert_array_equal(eclf1.predict_proba(X), eclf2.predict_proba(X))","246","","247","    sample_weight = np.random.RandomState(123).uniform(size=(len(y),))","248","    eclf3 = VotingClassifier(estimators=[('lr', clf1)], voting='soft')","249","    eclf3.fit(X, y, sample_weight)","250","    clf1.fit(X, y, sample_weight)","251","    assert_array_equal(eclf3.predict(X), clf1.predict(X))","252","    assert_array_equal(eclf3.predict_proba(X), clf1.predict_proba(X))","253","","254","    clf4 = KNeighborsClassifier()","255","    eclf3 = VotingClassifier(estimators=[","256","        ('lr', clf1), ('svc', clf3), ('knn', clf4)],","257","        voting='soft')","258","    msg = ('Underlying estimator \\'knn\\' does not support sample weights.')","259","    assert_raise_message(ValueError, msg, eclf3.fit, X, y, sample_weight)"],"delete":["0","\"\"\"Testing for the boost module (sklearn.ensemble.boost).\"\"\"","3","from sklearn.utils.testing import assert_almost_equal"]}]}},"6d4ae1b61ea8927efe39e27752e15a3cf2263e33":{"changes":{"sklearn\/semi_supervised\/tests\/test_label_propagation.py":"MODIFY","sklearn\/semi_supervised\/label_propagation.py":"MODIFY"},"diff":{"sklearn\/semi_supervised\/tests\/test_label_propagation.py":[{"add":["11","from sklearn.exceptions import ConvergenceWarning","73","    lp_default_y = lp_default.fit(X, y).transduction_","111","                                             gamma=0.1)","112","    clf.fit(X, y)","149","","150","","151","def test_convergence_warning():","152","    # This is a non-regression test for #5774","153","    X = np.array([[1., 0.], [0., 1.], [1., 2.5]])","154","    y = np.array([0, 1, -1])","155","    mdl = label_propagation.LabelSpreading(kernel='rbf', max_iter=1)","156","    assert_warns(ConvergenceWarning, mdl.fit, X, y)","157","    assert_equal(mdl.n_iter_, mdl.max_iter)","158","","159","    mdl = label_propagation.LabelPropagation(kernel='rbf', max_iter=1)","160","    assert_warns(ConvergenceWarning, mdl.fit, X, y)","161","    assert_equal(mdl.n_iter_, mdl.max_iter)","162","","163","    mdl = label_propagation.LabelSpreading(kernel='rbf', max_iter=500)","164","    assert_no_warnings(mdl.fit, X, y)","165","","166","    mdl = label_propagation.LabelPropagation(kernel='rbf', max_iter=500)","167","    assert_no_warnings(mdl.fit, X, y)"],"delete":["72","    lp_default_y = assert_no_warnings(lp_default.fit, X, y).transduction_","110","                                             gamma=0.1).fit(X, y)"]}],"sklearn\/semi_supervised\/label_propagation.py":[{"add":["36",">>> rng = np.random.RandomState(42)","37",">>> random_unlabeled_points = rng.rand(len(iris.target)) < 0.3","55","#          Utkarsh Upadhyay <mail@musicallyut.in>","70","from ..exceptions import ConvergenceWarning","94","    max_iter : integer","264","","265","        for self.n_iter_ in range(self.max_iter):","266","            if np.abs(self.label_distributions_ - l_previous).sum() < self.tol:","267","                break","268","","284","        else:","285","            warnings.warn(","286","                'max_iter=%d was reached without convergence.' % self.max_iter,","287","                category=ConvergenceWarning","288","            )","289","            self.n_iter_ += 1","327","    max_iter : integer","361","    >>> rng = np.random.RandomState(42)","362","    >>> random_unlabeled_points = rng.rand(len(iris.target)) < 0.3","444","    max_iter : integer","478","    >>> rng = np.random.RandomState(42)","479","    >>> random_unlabeled_points = rng.rand(len(iris.target)) < 0.3"],"delete":["36",">>> random_unlabeled_points = np.where(np.random.randint(0, 2,","37","...        size=len(iris.target)))","69","","70","","71","# Helper functions","72","","73","def _not_converged(y_truth, y_prediction, tol=1e-3):","74","    \"\"\"basic convergence check\"\"\"","75","    return np.abs(y_truth - y_prediction).sum() > tol","99","    max_iter : float","266","        remaining_iter = self.max_iter","270","        while (_not_converged(self.label_distributions_, l_previous, self.tol)","271","               and remaining_iter > 1):","287","            remaining_iter -= 1","296","        self.n_iter_ = self.max_iter - remaining_iter","326","    max_iter : float","360","    >>> random_unlabeled_points = np.where(np.random.randint(0, 2,","361","    ...    size=len(iris.target)))","443","    max_iter : float","477","    >>> random_unlabeled_points = np.where(np.random.randint(0, 2,","478","    ...    size=len(iris.target)))"]}]}}}