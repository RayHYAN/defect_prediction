{"eb1f5f29df4bcb64fa3a96e3018aefcbe99dffab":{"changes":{"sklearn\/ensemble\/_hist_gradient_boosting\/tests\/__init__.py":"ADD","MANIFEST.in":"MODIFY"},"diff":{"sklearn\/ensemble\/_hist_gradient_boosting\/tests\/__init__.py":[{"add":[],"delete":[]}],"MANIFEST.in":[{"add":["3","recursive-include sklearn *.c *.h *.pyx *.pxd *.pxi *.tp"],"delete":["3","recursive-include sklearn *.c *.h *.pyx *.pxd *.pxi"]}]}},"36c3c2bf8de66d236ffc78de43145140580c0c2d":{"changes":{"sklearn\/datasets\/tests\/test_common.py":"MODIFY"},"diff":{"sklearn\/datasets\/tests\/test_common.py":[{"add":["96","            if name in markers_fetch:","97","                marks.append(markers_fetch[name])"],"delete":["96","            marks.append(markers_fetch.get(name, pytest.mark.basic))"]}]}},"70e1558607b4b0bdeb94f5af4aae115e4108e202":{"changes":{"doc\/modules\/manifold.rst":"MODIFY","doc\/modules\/multiclass.rst":"MODIFY","doc\/modules\/compose.rst":"MODIFY","doc\/modules\/feature_selection.rst":"MODIFY","doc\/modules\/preprocessing.rst":"MODIFY","doc\/faq.rst":"MODIFY","doc\/related_projects.rst":"MODIFY","doc\/modules\/metrics.rst":"MODIFY","doc\/developers\/contributing.rst":"MODIFY","doc\/modules\/random_projection.rst":"MODIFY","doc\/modules\/feature_extraction.rst":"MODIFY","doc\/developers\/utilities.rst":"MODIFY","doc\/whats_new\/older_versions.rst":"MODIFY","doc\/modules\/neighbors.rst":"MODIFY","doc\/modules\/svm.rst":"MODIFY","doc\/modules\/density.rst":"MODIFY","doc\/computing\/computational_performance.rst":"MODIFY"},"diff":{"doc\/modules\/manifold.rst":[{"add":["118","   :class:`~sklearn.neighbors.BallTree` for efficient neighbor search.","581","  projection for instance using :class:`~sklearn.decomposition.TruncatedSVD`"],"delete":["118","   :class:`sklearn.neighbors.BallTree` for efficient neighbor search.","581","  projection for instance using :class:`sklearn.decomposition.TruncatedSVD`"]}],"doc\/modules\/multiclass.rst":[{"add":["62","  :class:`~sklearn.multioutput.MultiOutputClassifier`. This approach treats","179",".. currentmodule:: sklearn","180","","183","  - :class:`naive_bayes.BernoulliNB`","184","  - :class:`tree.DecisionTreeClassifier`","185","  - :class:`tree.ExtraTreeClassifier`","186","  - :class:`ensemble.ExtraTreesClassifier`","187","  - :class:`naive_bayes.GaussianNB`","188","  - :class:`neighbors.KNeighborsClassifier`","189","  - :class:`semi_supervised.LabelPropagation`","190","  - :class:`semi_supervised.LabelSpreading`","191","  - :class:`discriminant_analysis.LinearDiscriminantAnalysis`","192","  - :class:`svm.LinearSVC` (setting multi_class=\"crammer_singer\")","193","  - :class:`linear_model.LogisticRegression` (setting multi_class=\"multinomial\")","194","  - :class:`linear_model.LogisticRegressionCV` (setting multi_class=\"multinomial\")","195","  - :class:`neural_network.MLPClassifier`","196","  - :class:`neighbors.NearestCentroid`","197","  - :class:`discriminant_analysis.QuadraticDiscriminantAnalysis`","198","  - :class:`neighbors.RadiusNeighborsClassifier`","199","  - :class:`ensemble.RandomForestClassifier`","200","  - :class:`linear_model.RidgeClassifier`","201","  - :class:`linear_model.RidgeClassifierCV`","206","  - :class:`svm.NuSVC`","207","  - :class:`svm.SVC`.","208","  - :class:`gaussian_process.GaussianProcessClassifier` (setting multi_class = \"one_vs_one\")","213","  - :class:`ensemble.GradientBoostingClassifier`","214","  - :class:`gaussian_process.GaussianProcessClassifier` (setting multi_class = \"one_vs_rest\")","215","  - :class:`svm.LinearSVC` (setting multi_class=\"ovr\")","216","  - :class:`linear_model.LogisticRegression` (setting multi_class=\"ovr\")","217","  - :class:`linear_model.LogisticRegressionCV` (setting multi_class=\"ovr\")","218","  - :class:`linear_model.SGDClassifier`","219","  - :class:`linear_model.Perceptron`","220","  - :class:`linear_model.PassiveAggressiveClassifier`","225","  - :class:`tree.DecisionTreeClassifier`","226","  - :class:`tree.ExtraTreeClassifier`","227","  - :class:`ensemble.ExtraTreesClassifier`","228","  - :class:`neighbors.KNeighborsClassifier`","229","  - :class:`neural_network.MLPClassifier`","230","  - :class:`neighbors.RadiusNeighborsClassifier`","231","  - :class:`ensemble.RandomForestClassifier`","232","  - :class:`linear_model.RidgeClassifierCV`","237","  - :class:`tree.DecisionTreeClassifier`","238","  - :class:`tree.ExtraTreeClassifier`","239","  - :class:`ensemble.ExtraTreesClassifier`","240","  - :class:`neighbors.KNeighborsClassifier`","241","  - :class:`neighbors.RadiusNeighborsClassifier`","242","  - :class:`ensemble.RandomForestClassifier`","261","The :class:`~preprocessing.MultiLabelBinarizer` transformer can be used","262","to convert between a collection of collections of labels and the indicator format."],"delete":["62","  :class:`sklearn.multioutput.MultiOutputClassifier`. This approach treats","181","  - :class:`sklearn.naive_bayes.BernoulliNB`","182","  - :class:`sklearn.tree.DecisionTreeClassifier`","183","  - :class:`sklearn.tree.ExtraTreeClassifier`","184","  - :class:`sklearn.ensemble.ExtraTreesClassifier`","185","  - :class:`sklearn.naive_bayes.GaussianNB`","186","  - :class:`sklearn.neighbors.KNeighborsClassifier`","187","  - :class:`sklearn.semi_supervised.LabelPropagation`","188","  - :class:`sklearn.semi_supervised.LabelSpreading`","189","  - :class:`sklearn.discriminant_analysis.LinearDiscriminantAnalysis`","190","  - :class:`sklearn.svm.LinearSVC` (setting multi_class=\"crammer_singer\")","191","  - :class:`sklearn.linear_model.LogisticRegression` (setting multi_class=\"multinomial\")","192","  - :class:`sklearn.linear_model.LogisticRegressionCV` (setting multi_class=\"multinomial\")","193","  - :class:`sklearn.neural_network.MLPClassifier`","194","  - :class:`sklearn.neighbors.NearestCentroid`","195","  - :class:`sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis`","196","  - :class:`sklearn.neighbors.RadiusNeighborsClassifier`","197","  - :class:`sklearn.ensemble.RandomForestClassifier`","198","  - :class:`sklearn.linear_model.RidgeClassifier`","199","  - :class:`sklearn.linear_model.RidgeClassifierCV`","204","  - :class:`sklearn.svm.NuSVC`","205","  - :class:`sklearn.svm.SVC`.","206","  - :class:`sklearn.gaussian_process.GaussianProcessClassifier` (setting multi_class = \"one_vs_one\")","211","  - :class:`sklearn.ensemble.GradientBoostingClassifier`","212","  - :class:`sklearn.gaussian_process.GaussianProcessClassifier` (setting multi_class = \"one_vs_rest\")","213","  - :class:`sklearn.svm.LinearSVC` (setting multi_class=\"ovr\")","214","  - :class:`sklearn.linear_model.LogisticRegression` (setting multi_class=\"ovr\")","215","  - :class:`sklearn.linear_model.LogisticRegressionCV` (setting multi_class=\"ovr\")","216","  - :class:`sklearn.linear_model.SGDClassifier`","217","  - :class:`sklearn.linear_model.Perceptron`","218","  - :class:`sklearn.linear_model.PassiveAggressiveClassifier`","223","  - :class:`sklearn.tree.DecisionTreeClassifier`","224","  - :class:`sklearn.tree.ExtraTreeClassifier`","225","  - :class:`sklearn.ensemble.ExtraTreesClassifier`","226","  - :class:`sklearn.neighbors.KNeighborsClassifier`","227","  - :class:`sklearn.neural_network.MLPClassifier`","228","  - :class:`sklearn.neighbors.RadiusNeighborsClassifier`","229","  - :class:`sklearn.ensemble.RandomForestClassifier`","230","  - :class:`sklearn.linear_model.RidgeClassifierCV`","235","  - :class:`sklearn.tree.DecisionTreeClassifier`","236","  - :class:`sklearn.tree.ExtraTreeClassifier`","237","  - :class:`sklearn.ensemble.ExtraTreesClassifier`","238","  - :class:`sklearn.neighbors.KNeighborsClassifier`","239","  - :class:`sklearn.neighbors.RadiusNeighborsClassifier`","240","  - :class:`sklearn.ensemble.RandomForestClassifier`","259","The :class:`MultiLabelBinarizer <sklearn.preprocessing.MultiLabelBinarizer>`","260","transformer can be used to convert between a collection of collections of","261","labels and the indicator format."]}],"doc\/modules\/compose.rst":[{"add":["332","see the related class :class:`~sklearn.compose.ColumnTransformer`","418","variable using :class:`~sklearn.preprocessing.OneHotEncoder` but apply a","419",":class:`~sklearn.feature_extraction.text.CountVectorizer` to the ``'title'`` column.","453","However, :class:`~sklearn.preprocessing.OneHotEncoder`","537","`display` option in :func:`~sklearn.set_config`::"],"delete":["332","see the related class :class:`sklearn.compose.ColumnTransformer`","418","variable using :class:`preprocessing.OneHotEncoder","419","<sklearn.preprocessing.OneHotEncoder>` but apply a","420",":class:`feature_extraction.text.CountVectorizer","421","<sklearn.feature_extraction.text.CountVectorizer>` to the ``'title'`` column.","455","However, :class:`preprocessing.OneHotEncoder <sklearn.preprocessing.OneHotEncoder>`","539","`display` option in :func:`sklearn.set_config`::"]}],"doc\/modules\/feature_selection.rst":[{"add":["176","they can be used along with :class:`~feature_selection.SelectFromModel`","178","for this purpose are the :class:`~linear_model.Lasso` for regression, and","179","of :class:`~linear_model.LogisticRegression` and :class:`~svm.LinearSVC`","237","features (when coupled with the :class:`~feature_selection.SelectFromModel`","269","to use a :class:`~pipeline.Pipeline`::","277","In this snippet we make use of a :class:`~svm.LinearSVC`","278","coupled with :class:`~feature_selection.SelectFromModel`","280","Then, a :class:`~ensemble.RandomForestClassifier` is trained on the","284","See the :class:`~pipeline.Pipeline` examples for more details."],"delete":["176","they can be used along with :class:`feature_selection.SelectFromModel`","178","for this purpose are the :class:`linear_model.Lasso` for regression, and","179","of :class:`linear_model.LogisticRegression` and :class:`svm.LinearSVC`","237","features (when coupled with the :class:`sklearn.feature_selection.SelectFromModel`","269","to use a :class:`sklearn.pipeline.Pipeline`::","277","In this snippet we make use of a :class:`sklearn.svm.LinearSVC`","278","coupled with :class:`sklearn.feature_selection.SelectFromModel`","280","Then, a :class:`sklearn.ensemble.RandomForestClassifier` is trained on the","284","See the :class:`sklearn.pipeline.Pipeline` examples for more details."]}],"doc\/modules\/preprocessing.rst":[{"add":["78",":class:`~sklearn.pipeline.Pipeline`::","237","  To address this issue you can use :class:`~sklearn.decomposition.PCA` with","435",":class:`~sklearn.pipeline.Pipeline`::","639","in a :class:`~sklearn.pipeline.Pipeline`.","667","this is the case for the :class:`~sklearn.neural_network.BernoulliRBM`.","676",":class:`~sklearn.pipeline.Pipeline`. The ``fit`` method does nothing","761","Note that polynomial features are used implicitly in `kernel methods <https:\/\/en.wikipedia.org\/wiki\/Kernel_method>`_ (e.g., :class:`~sklearn.svm.SVC`, :class:`~sklearn.decomposition.KernelPCA`) when using polynomial :ref:`svm_kernels`."],"delete":["78",":class:`sklearn.pipeline.Pipeline`::","237","  To address this issue you can use :class:`sklearn.decomposition.PCA` with","435",":class:`sklearn.pipeline.Pipeline`::","639","in a :class:`sklearn.pipeline.Pipeline`.","667","this is the case for the :class:`sklearn.neural_network.BernoulliRBM`.","676",":class:`sklearn.pipeline.Pipeline`. The ``fit`` method does nothing","761","Note that polynomial features are used implicitly in `kernel methods <https:\/\/en.wikipedia.org\/wiki\/Kernel_method>`_ (e.g., :class:`sklearn.svm.SVC`, :class:`sklearn.decomposition.KernelPCA`) when using polynomial :ref:`svm_kernels`."]}],"doc\/faq.rst":[{"add":["329","in :mod:`sklearn.neural_network`. We will only accept bug fixes for this module.","402",":class:`~compose.TransformedTargetRegressor`,"],"delete":["329","in `sklearn.neural_network`. We will only accept bug fixes for this module.","402",":class:`sklearn.compose.TransformedTargetRegressor`,"]}],"doc\/related_projects.rst":[{"add":["202","  :class:`~sklearn.decomposition.LatentDirichletAllocation` implementation uses"],"delete":["202","  :class:`sklearn.decomposition.LatentDirichletAllocation` implementation uses"]}],"doc\/modules\/metrics.rst":[{"add":["188",":class:`~sklearn.svm.SVC` with ``kernel=\"precomputed\"``::"],"delete":["188",":class:`sklearn.svm.SVC` with ``kernel=\"precomputed\"``::"]}],"doc\/developers\/contributing.rst":[{"add":["6",".. currentmodule:: sklearn","7","","1169","  :class:`~linear_model.LinearRegression`, what you're looking for","1176","  at least from :class:`~base.BaseEstimator`, and","1177","  from a ``Mixin`` class (e.g. :class:`~base.ClassifierMixin`) that enables default","1178","  behaviour depending on the nature of the estimator (classifier, regressor,","1179","  transformer, etc.)."],"delete":["1167","  :class:`sklearn.linear_model.LinearRegression`, what you're looking for","1174","  at least from :class:`BaseEstimator <sklearn.base.BaseEstimator>`, and","1175","  from a ``Mixin`` class (e.g. :class:`ClassifierMixin","1176","  <sklearn.base.ClassifierMixin>`) that enables default behaviour depending","1177","  on the nature of the estimator (classifier, regressor, transformer, etc.)."]}],"doc\/modules\/random_projection.rst":[{"add":["54",":func:`johnson_lindenstrauss_min_dim` estimates","92","The :class:`GaussianRandomProjection` reduces the","113","The :class:`SparseRandomProjection` reduces the"],"delete":["54",":func:`sklearn.random_projection.johnson_lindenstrauss_min_dim` estimates","92","The :class:`sklearn.random_projection.GaussianRandomProjection` reduces the","113","The :class:`sklearn.random_projection.SparseRandomProjection` reduces the"]}],"doc\/modules\/feature_extraction.rst":[{"add":["104",":class:`~text.TfidfTransformer` for normalization)::","152",":class:`~sklearn.naive_bayes.MultinomialNB` or","153",":class:`~sklearn.feature_selection.chi2`","171","but unlike :class:`~text.CountVectorizer`,","823",":class:`~sklearn.feature_extraction.FeatureHasher` class and the text"],"delete":["104",":class:`text.TfidfTransformer` for normalization)::","152",":class:`sklearn.naive_bayes.MultinomialNB` or","153",":class:`sklearn.feature_selection.chi2`","171","but unlike :class:`text.CountVectorizer`,","823",":class:`sklearn.feature_extraction.FeatureHasher` class and the text"]}],"doc\/developers\/utilities.rst":[{"add":["100","  (used in :func:`~sklearn.linear_model.lars_path`)  Remove an","123","  Used in :func:`~sklearn.cluster.k_means`.","143","  :class:`~sklearn.cluster.KMeans`.","148","  :class:`~sklearn.preprocessing.Normalizer`.","153","  :class:`~sklearn.preprocessing.StandardScaler`.","168","  (used in :class:`~sklearn.manifold.Isomap`)","195","  to ``n``.  Used in :func:`~sklearn.decomposition.dict_learning` and","196","  :func:`~sklearn.cluster.k_means`.","233","- :class:`~sklearn.exceptions.ConvergenceWarning`: Custom warning to catch"],"delete":["100","  (used in :func:`sklearn.linear_model.lars_path`)  Remove an","123","  Used in :func:`sklearn.cluster.k_means`.","143","  :class:`sklearn.cluster.KMeans`.","148","  :class:`sklearn.preprocessing.Normalizer`.","153","  :class:`sklearn.preprocessing.StandardScaler`.","168","  (used in :class:`sklearn.manifold.Isomap`)","195","  to ``n``.  Used in :func:`sklearn.decomposition.dict_learning` and","196","  :func:`sklearn.cluster.k_means`.","233","- :class:`sklearn.exceptions.ConvergenceWarning`: Custom warning to catch"]}],"doc\/whats_new\/older_versions.rst":[{"add":["64","- :class:`~ensemble.GradientBoostingRegressor` and","65","  :class:`~ensemble.GradientBoostingClassifier` now support feature subsampling","69","  :class:`~ensemble.GradientBoostingRegressor`, by `Peter Prettenhofer`_.","75","- Added :class:`~preprocessing.LabelEncoder`, a simple utility class to","116","- In :class:`~feature_extraction.text.CountVectorizer`, added an option to","123","- Fixes in :class:`~decomposition.ProbabilisticPCA` score function by Wei Li.","138","- In :class:`hmm` objects, like :class:`~hmm.GaussianHMM`,","139","  :class:`~hmm.MultinomialHMM`, etc., all parameters must be passed to the","154","- In :class:`~feature_extraction.text.CountVectorizer` the parameters","158","- In :class:`~feature_extraction.text.CountVectorizer`, words that appear","169","- Grid of alphas used for fitting :class:`~linear_model.LassoCV` and","170","  :class:`~linear_model.ElasticNetCV` is now stored","178","- :class:`~ensemble.GradientBoostingClassifier` now supports","179","  :meth:`~ensemble.GradientBoostingClassifier.staged_predict_proba`, and","180","  :meth:`~ensemble.GradientBoostingClassifier.staged_predict`.","182","- :class:`~svm.sparse.SVC` and other sparse SVM classes are now deprecated.","187","  input data, in particular :class:`~cluster.SpectralClustering` and","188","  :class:`~cluster.AffinityPropagation` which previously expected affinity matrices.","264","  (:class:`~feature_extraction.DictVectorizer`) by `Lars Buitinck`_.","268","  :func:`~metrics.precision_score`, :func:`metrics.recall_score` and","269","  :func:`~metrics.f1_score` by `Satrajit Ghosh`_.","284","- Added :class:`~sklearn.cross_validation.StratifiedShuffleSplit`, which is","285","  a :class:`~sklearn.cross_validation.ShuffleSplit` with balanced splits,","288","- :class:`~sklearn.neighbors.NearestCentroid` classifier added, along with a","304","  :class:`~linear_model.LogisticRegression` merged by `Lars Buitinck`_.","320","- :class:`~svm.SVC` members ``coef_`` and ``intercept_`` changed sign for","326","  :class:`~linear_model.RidgeCV`, by Reuben Fletcher-Costin.","342","- :class:`~covariance.EllipticEnvelop` is now deprecated - Please use :class:`~covariance.EllipticEnvelope`","352","- In :class:`~mixture.GMM`, :class:`~mixture.DPGMM` and :class:`~mixture.VBGMM`,","372","- In :class:`~svm.LinearSVC`, the meaning of the ``multi_class`` parameter","377","- Class :class:`~feature_selection.text.Vectorizer` is deprecated and","378","  replaced by :class:`~feature_selection.text.TfidfVectorizer`.","383","  to :class:`~feature_selection.text.TfidfVectorizer` and","384","  :class:`~feature_selection.text.CountVectorizer`, in particular the","403","- Class :class:`~feature_selection.text.TfidfVectorizer` now derives directly","404","  from :class:`~feature_selection.text.CountVectorizer` to make grid","510","  :func:`~sklearn.metrics.silhouette_score` by Robert Layton.","521","  :func:`~sklearn.metrics.adjusted_mutual_info_score` by Robert Layton.","550","  (:func:`~sklearn.datasets.fetch_20newsgroups_vectorized`) by","558","- Make :func:`~sklearn.preprocessing.scale` and","559","  :class:`~sklearn.preprocessing.Scaler` work on sparse matrices by","568","- :class:`~sklearn.cross_validation.ShuffleSplit` can subsample the train","587","- The SVMlight dataset loader :func:`~sklearn.datasets.load_svmlight_file` no","613","  :func:`~sklearn.decomposition.sparse_encode`, and the shapes of the arrays","618","  files generated using :func:`~sklearn.datasets.dump_svmlight_file` should be","624","- :func:`~sklearn.utils.extmath.fast_svd` has been renamed","625","  :func:`~sklearn.utils.extmath.randomized_svd` and the default","772","- Implementation of :class:`~linear_model.LassoLarsCV`","774","  :class:`~linear_model.LassoLarsIC` (BIC\/AIC model","808","  from :class:`~base.BaseEstimator`.","950","- :class:`~decomposition.PCA` is now usable from the Pipeline object by `Olivier Grisel`_.","961","  :class:`~discriminant_analysis.LinearDiscriminantAnalysis` By `Mathieu Blondel`_.","1032","  :class:`~linear_model.RidgeCV` [`Mathieu Blondel`_]","1044","- Performance improvements for :class:`~cluster.KMeans` [`Gael","1049","- Refactoring of :class:`~neighbors.NeighborsClassifier` and","1057","- Documentation improvements: Added :class:`~pca.RandomizedPCA` and","1058","  :class:`~linear_model.LogisticRegression` to the class","1065","  dense and sparse variants, like :class:`~svm.LinearSVC` or","1066","  :class:`~linear_model.LogisticRegression` [`Fabian Pedregosa`_].","1070","  :class:`~pca.RandomizedPCA` [`James Bergstra`_].","1074","- Allow input sequences of different lengths in :class:`~hmm.GaussianHMM`","1137","  multi_class in :class:`~svm.LinearSVC`)","1143","  (:class:`~grid_search.GridSearchCV`) as in modules","1163","  :class:`~linear_model.LogisticRegression` model.","1220","  ``svm`` and ``linear_model`` (see :class:`~svm.sparse.SVC`,","1221","  :class:`~svm.sparse.SVR`, :class:`~svm.sparse.LinearSVC`,","1222","  :class:`~linear_model.sparse.Lasso`, :class:`~linear_model.sparse.ElasticNet`)","1224","- New :class:`~pipeline.Pipeline` object to compose different estimators.","1230","  linear_model module (:class:`~linear_model.LassoCV`, :class:`~linear_model.ElasticNetCV`,","1235","  :class:`~linear_model.lars_path`, :class:`~linear_model.Lars` and","1236","  :class:`~linear_model.LassoLars`.","1239","  :class:`~hmm.GaussianHMM`, :class:`~hmm.MultinomialHMM`,","1240","  :class:`~hmm.GMMHMM`)"],"delete":["64","- :class:`ensemble.GradientBoostingRegressor` and","65","  :class:`ensemble.GradientBoostingClassifier` now support feature subsampling","69","  :class:`ensemble.GradientBoostingRegressor`, by `Peter Prettenhofer`_.","75","- Added :class:`preprocessing.LabelEncoder`, a simple utility class to","116","- In :class:`feature_extraction.text.CountVectorizer`, added an option to","123","- Fixes in :class:`decomposition.ProbabilisticPCA` score function by Wei Li.","138","- In :class:`hmm` objects, like :class:`hmm.GaussianHMM`,","139","  :class:`hmm.MultinomialHMM`, etc., all parameters must be passed to the","154","- In :class:`feature_extraction.text.CountVectorizer` the parameters","158","- In :class:`feature_extraction.text.CountVectorizer`, words that appear","169","- Grid of alphas used for fitting :class:`linear_model.LassoCV` and","170","  :class:`linear_model.ElasticNetCV` is now stored","178","- :class:`ensemble.GradientBoostingClassifier` now supports","179","  :meth:`ensemble.GradientBoostingClassifier.staged_predict_proba`, and","180","  :meth:`ensemble.GradientBoostingClassifier.staged_predict`.","182","- :class:`svm.sparse.SVC` and other sparse SVM classes are now deprecated.","187","  input data, in particular :class:`cluster.SpectralClustering` and","188","  :class:`cluster.AffinityPropagation` which previously expected affinity matrices.","264","  (:class:`feature_extraction.DictVectorizer`) by `Lars Buitinck`_.","268","  :func:`metrics.precision_score`, :func:`metrics.recall_score` and","269","  :func:`metrics.f1_score` by `Satrajit Ghosh`_.","284","- Added :class:`sklearn.cross_validation.StratifiedShuffleSplit`, which is","285","  a :class:`sklearn.cross_validation.ShuffleSplit` with balanced splits,","288","- :class:`sklearn.neighbors.NearestCentroid` classifier added, along with a","304","  :class:`linear_model.LogisticRegression` merged by `Lars Buitinck`_.","320","- :class:`svm.SVC` members ``coef_`` and ``intercept_`` changed sign for","326","  :class:`linear_model.RidgeCV`, by Reuben Fletcher-Costin.","342","- :class:`covariance.EllipticEnvelop` is now deprecated - Please use :class:`covariance.EllipticEnvelope`","352","- In :class:`mixture.GMM`, :class:`mixture.DPGMM` and :class:`mixture.VBGMM`,","372","- In :class:`svm.LinearSVC`, the meaning of the ``multi_class`` parameter","377","- Class :class:`feature_selection.text.Vectorizer` is deprecated and","378","  replaced by :class:`feature_selection.text.TfidfVectorizer`.","383","  to :class:`feature_selection.text.TfidfVectorizer` and","384","  :class:`feature_selection.text.CountVectorizer`, in particular the","403","- Class :class:`feature_selection.text.TfidfVectorizer` now derives directly","404","  from :class:`feature_selection.text.CountVectorizer` to make grid","510","  :func:`sklearn.metrics.silhouette_score` by Robert Layton.","521","  :func:`sklearn.metrics.adjusted_mutual_info_score` by Robert Layton.","550","  (:func:`sklearn.datasets.fetch_20newsgroups_vectorized`) by","558","- Make :func:`sklearn.preprocessing.scale` and","559","  :class:`sklearn.preprocessing.Scaler` work on sparse matrices by","568","- :class:`sklearn.cross_validation.ShuffleSplit` can subsample the train","587","- The SVMlight dataset loader :func:`sklearn.datasets.load_svmlight_file` no","613","  :func:`sklearn.decomposition.sparse_encode`, and the shapes of the arrays","618","  files generated using :func:`sklearn.datasets.dump_svmlight_file` should be","624","- :func:`sklearn.utils.extmath.fast_svd` has been renamed","625","  :func:`sklearn.utils.extmath.randomized_svd` and the default","772","- Implementation of :class:`linear_model.LassoLarsCV`","774","  :class:`linear_model.LassoLarsIC` (BIC\/AIC model","808","  from :class:`base.BaseEstimator`.","950","- :class:`decomposition.PCA` is now usable from the Pipeline object by `Olivier Grisel`_.","961","  :class:`discriminant_analysis.LinearDiscriminantAnalysis` By `Mathieu Blondel`_.","1032","  :class:`linear_model.RidgeCV` [`Mathieu Blondel`_]","1044","- Performance improvements for :class:`cluster.KMeans` [`Gael","1049","- Refactoring of :class:`neighbors.NeighborsClassifier` and","1057","- Documentation improvements: Added :class:`pca.RandomizedPCA` and","1058","  :class:`linear_model.LogisticRegression` to the class","1065","  dense and sparse variants, like :class:`svm.LinearSVC` or","1066","  :class:`linear_model.LogisticRegression` [`Fabian Pedregosa`_].","1070","  :class:`pca.RandomizedPCA` [`James Bergstra`_].","1074","- Allow input sequences of different lengths in :class:`hmm.GaussianHMM`","1137","  multi_class in :class:`svm.LinearSVC`)","1143","  (:class:`grid_search.GridSearchCV`) as in modules","1163","  :class:`linear_model.LogisticRegression` model.","1220","  ``svm`` and ``linear_model`` (see :class:`svm.sparse.SVC`,","1221","  :class:`svm.sparse.SVR`, :class:`svm.sparse.LinearSVC`,","1222","  :class:`linear_model.sparse.Lasso`, :class:`linear_model.sparse.ElasticNet`)","1224","- New :class:`pipeline.Pipeline` object to compose different estimators.","1230","  linear_model module (:class:`linear_model.LassoCV`, :class:`linear_model.ElasticNetCV`,","1235","  :class:`linear_model.lars_path`, :class:`linear_model.Lars` and","1236","  :class:`linear_model.LassoLars`.","1239","  :class:`hmm.GaussianHMM`, :class:`hmm.MultinomialHMM`,","1240","  :class:`hmm.GMMHMM`)"]}],"doc\/modules\/neighbors.rst":[{"add":["112","unsupervised learning: in particular, see :class:`~sklearn.manifold.Isomap`,","113",":class:`~sklearn.manifold.LocallyLinearEmbedding`, and","114",":class:`~sklearn.cluster.SpectralClustering`.","341","and are computed using the class :class:`BallTree`.","473","similar to the label updating phase of the :class:`~sklearn.cluster.KMeans` algorithm.","477","assumed. See Linear Discriminant Analysis (:class:`~sklearn.discriminant_analysis.LinearDiscriminantAnalysis`)","478","and Quadratic Discriminant Analysis (:class:`~sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis`)","708","(:class:`~sklearn.decomposition.PCA`), Linear Discriminant Analysis","709","(:class:`~sklearn.discriminant_analysis.LinearDiscriminantAnalysis`) and"],"delete":["112","unsupervised learning: in particular, see :class:`sklearn.manifold.Isomap`,","113",":class:`sklearn.manifold.LocallyLinearEmbedding`, and","114",":class:`sklearn.cluster.SpectralClustering`.","341","and are computed using the class :class:`sklearn.neighbors.BallTree`.","473","similar to the label updating phase of the :class:`sklearn.cluster.KMeans` algorithm.","477","assumed. See Linear Discriminant Analysis (:class:`sklearn.discriminant_analysis.LinearDiscriminantAnalysis`)","478","and Quadratic Discriminant Analysis (:class:`sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis`)","708","(:class:`sklearn.decomposition.PCA`), Linear Discriminant Analysis","709","(:class:`sklearn.discriminant_analysis.LinearDiscriminantAnalysis`) and"]}],"doc\/modules\/svm.rst":[{"add":["502","is advised to use :class:`~sklearn.model_selection.GridSearchCV` with","669","    estimator used is :class:`~sklearn.linear_model.Ridge` regression,"],"delete":["502","is advised to use :class:`sklearn.model_selection.GridSearchCV` with ","669","    estimator used is :class:`sklearn.linear_model.Ridge <ridge>` regression,"]}],"doc\/modules\/density.rst":[{"add":["10","Gaussian Mixtures (:class:`~sklearn.mixture.GaussianMixture`), and","12","(:class:`~sklearn.neighbors.KernelDensity`).","60",":class:`~sklearn.neighbors.KernelDensity` estimator, which uses the","102",":class:`~sklearn.neighbors.KernelDensity` implements several common kernel","138","metrics (see :class:`~sklearn.neighbors.DistanceMetric` for a list of available metrics), though"],"delete":["10","Gaussian Mixtures (:class:`sklearn.mixture.GaussianMixture`), and","12","(:class:`sklearn.neighbors.KernelDensity`).","60",":class:`sklearn.neighbors.KernelDensity` estimator, which uses the","102",":class:`sklearn.neighbors.KernelDensity` implements several common kernel","138","metrics (see :class:`sklearn.neighbors.DistanceMetric` for a list of available metrics), though"]}],"doc\/computing\/computational_performance.rst":[{"add":["6",".. currentmodule:: sklearn","7","","84","scikit-learn, or configure it in Python with :func:`set_config`.","93",":func:`~utils.assert_all_finite` within the context.","165",":class:`~linear_model.SGDClassifier` with the","187",":class:`~svm.NuSVR` was used to influence the number of","200",":class:`~ensemble.GradientBoostingRegressor`.","307","working memory (defaulting to 1GB) using :func:`set_config` or","316",":func:`~metrics.pairwise_distances_chunked`, which facilitates computing"],"delete":["82","scikit-learn, or configure it in Python with :func:`sklearn.set_config`.","91",":func:`sklearn.utils.assert_all_finite` within the context.","163",":class:`sklearn.linear_model.SGDClassifier` with the","185",":class:`sklearn.svm.NuSVR` was used to influence the number of","198",":class:`sklearn.ensemble.gradient_boosting.GradientBoostingRegressor`.","305","working memory (defaulting to 1GB) using :func:`sklearn.set_config` or","314",":func:`metric.pairwise_distances_chunked`, which facilitates computing"]}]}},"f1acf834685f8bcd1bcdd903e9c40b7515fe0a67":{"changes":{"sklearn\/utils\/_mocking.py":"MODIFY","doc\/whats_new\/v0.23.rst":"MODIFY","sklearn\/ensemble\/_stacking.py":"MODIFY","sklearn\/ensemble\/tests\/test_stacking.py":"MODIFY"},"diff":{"sklearn\/utils\/_mocking.py":[{"add":["97","        self.n_features_in_ = len(X)"],"delete":[]}],"doc\/whats_new\/v0.23.rst":[{"add":["184","- |Fix| Fixed a bug in :class:`ensemble.StackingClassifier` and","185","  :class:`ensemble.StackingRegressor` where the `sample_weight`","186","  argument was not being passed to `cross_val_predict` when","187","  evaluating the base estimators on cross-validation folds","188","  to obtain the input to the meta estimator.","189","  :pr:`16539` by :user:`Bill DeRose <wderose>`.","190",""],"delete":[]}],"sklearn\/ensemble\/_stacking.py":[{"add":["124","            .. versionchanged:: 0.23","125","               when not None, `sample_weight` is passed to all underlying","126","               estimators","127","","172","        fit_params = ({\"sample_weight\": sample_weight}","173","                      if sample_weight is not None","174","                      else None)","178","                                       fit_params=fit_params,","192","        _fit_single_estimator(self.final_estimator_, X_meta, y,","193","                              sample_weight=sample_weight)"],"delete":["168","","185","        if sample_weight is not None:","186","            try:","187","                self.final_estimator_.fit(","188","                    X_meta, y, sample_weight=sample_weight","189","                )","190","            except TypeError as exc:","191","                if \"unexpected keyword argument 'sample_weight'\" in str(exc):","192","                    raise TypeError(","193","                        \"Underlying estimator {} does not support sample \"","194","                        \"weights.\"","195","                        .format(self.final_estimator_.__class__.__name__)","196","                    ) from exc","197","                raise","198","        else:","199","            self.final_estimator_.fit(X_meta, y)"]}],"sklearn\/ensemble\/tests\/test_stacking.py":[{"add":["40","from sklearn.utils._mocking import CheckingClassifier","442","def test_stacking_classifier_sample_weight_fit_param():","443","    # check sample_weight is passed to all invocations of fit","444","    stacker = StackingClassifier(","445","        estimators=[","446","            ('lr', CheckingClassifier(expected_fit_params=['sample_weight']))","447","        ],","448","        final_estimator=CheckingClassifier(","449","            expected_fit_params=['sample_weight']","450","        )","451","    )","452","    stacker.fit(X_iris, y_iris, sample_weight=np.ones(X_iris.shape[0]))","453","","454",""],"delete":[]}]}},"93e09aaae68ec2fc2d7b78818364ca868442e61e":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/cluster\/hierarchical.py":"MODIFY","sklearn\/cluster\/tests\/test_hierarchical.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["78","- |API| The ``n_components_`` attribute in :class:`cluster.AgglomerativeClustering`","79","  and :class:`cluster.FeatureAgglomeration` has been renamed to","80","  ``n_connected_components_``.","81","  :issue:`13427` by :user:`Stephane Couvreur <scouvreur>`.","82",""],"delete":[]}],"sklearn\/cluster\/hierarchical.py":[{"add":["25","from ..utils import deprecated","56","    n_connected_components, labels = connected_components(connectivity)","58","    if n_connected_components > 1:","61","                      \"stopping the tree early.\" % n_connected_components,","64","        for i in range(n_connected_components):","77","    return connectivity, n_connected_components","81","                         n_connected_components, return_distance):","127","        return children_, n_connected_components, n_samples, parent, distances","128","    return children_, n_connected_components, n_samples, parent","179","    n_connected_components : int","241","    connectivity, n_connected_components = _fix_connectivity(","242","                                                X, connectivity,","243","                                                affinity='euclidean')","336","        return children, n_connected_components, n_leaves, parent, distances","338","        return children, n_connected_components, n_leaves, parent","399","    n_connected_components : int","470","    connectivity, n_connected_components = _fix_connectivity(","471","                                                X, connectivity,","472","                                                affinity=affinity)","500","                                    n_clusters, n_connected_components,","501","                                    return_distance)","571","        return children, n_connected_components, n_leaves, parent, distances","572","    return children, n_connected_components, n_leaves, parent","721","    n_connected_components_ : int","760","    @property","761","    @deprecated(\"The ``n_components_`` attribute was deprecated \"","762","                \"in favor of ``n_connected_components_`` in 0.21 \"","763","                \"and will be removed in 0.23.\")","764","    def n_components_(self):","765","        return self.n_connected_components_","766","","830","        (self.children_, self.n_connected_components_, self.n_leaves_,","831","            parents) = memory.cache(tree_builder)(X, connectivity,","832","                                                  n_clusters=n_clusters,","833","                                                  **kwargs)","834","","914","    n_connected_components_ : int"],"delete":["29","","56","    n_components, labels = connected_components(connectivity)","58","    if n_components > 1:","61","                      \"stopping the tree early.\" % n_components,","64","        for i in range(n_components):","77","    return connectivity, n_components","81","                         n_components, return_distance):","127","        return children_, n_components, n_samples, parent, distances","128","    return children_, n_components, n_samples, parent","179","    n_components : int","241","    connectivity, n_components = _fix_connectivity(X, connectivity,","242","                                                   affinity='euclidean')","335","        return children, n_components, n_leaves, parent, distances","337","        return children, n_components, n_leaves, parent","398","    n_components : int","469","    connectivity, n_components = _fix_connectivity(X, connectivity,","470","                                                   affinity=affinity)","471","","499","                                    n_clusters, n_components, return_distance)","569","        return children, n_components, n_leaves, parent, distances","570","    return children, n_components, n_leaves, parent","719","    n_components_ : int","821","        self.children_, self.n_components_, self.n_leaves_, parents = \\","822","            memory.cache(tree_builder)(X, connectivity,","823","                                       n_clusters=n_clusters,","824","                                       **kwargs)","904","    n_components_ : int"]}],"sklearn\/cluster\/tests\/test_hierarchical.py":[{"add":["600","","601","","602","def test_n_components_deprecation():","603","    # Test that a Deprecation warning is thrown when n_components_","604","    # attribute is accessed","605","","606","    X = np.array([[1, 2], [1, 4], [1, 0], [4, 2]])","607","    agc = AgglomerativeClustering().fit(X)","608","","609","    match = (\"``n_components_`` attribute was deprecated \"","610","             \"in favor of ``n_connected_components_``\")","611","    with pytest.warns(DeprecationWarning, match=match):","612","        n = agc.n_components_","613","    assert n == agc.n_connected_components_"],"delete":[]}]}},"4f3c60c82e1540fb2f384d25952e3d25e81b73ab":{"changes":{"sklearn\/ensemble\/iforest.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/ensemble\/tests\/test_iforest.py":"MODIFY"},"diff":{"sklearn\/ensemble\/iforest.py":[{"add":["250","        X = check_array(X, accept_sparse='csr')","256","        if self._max_features == X.shape[1]:","257","            subsample_features = False","258","        else:","259","            subsample_features = True","260","","261","        for i, (tree, features) in enumerate(zip(self.estimators_,","262","                                                 self.estimators_features_)):","263","            if subsample_features:","264","                X_subset = X[:, features]","265","            else:","266","                X_subset = X","267","            leaves_index = tree.apply(X_subset)","268","            node_indicator = tree.decision_path(X_subset)","270","            depths[:, i] = np.ravel(node_indicator.sum(axis=1))","271","            depths[:, i] -= 1"],"delete":["250","        X = self.estimators_[0]._validate_X_predict(X, check_input=True)","256","        for i, tree in enumerate(self.estimators_):","257","            leaves_index = tree.apply(X)","258","            node_indicator = tree.decision_path(X)","260","            depths[:, i] = np.asarray(node_indicator.sum(axis=1)).reshape(-1) - 1"]}],"doc\/whats_new.rst":[{"add":["138","  ","139","   - Fixed a bug where :class:`sklearn.ensemble.IsolationForest` fails when ","140","     ``max_features`` is less than 1.","141","     :issue:`5732` by :user:`Ishank Gulati <IshankGulati>`."],"delete":[]}],"sklearn\/ensemble\/tests\/test_iforest.py":[{"add":["202","","203","","204","def test_iforest_subsampled_features():","205","    # It tests non-regression for #5732 which failed at predict.","206","    rng = check_random_state(0)","207","    X_train, X_test, y_train, y_test = train_test_split(boston.data[:50],","208","                                                        boston.target[:50],","209","                                                        random_state=rng)","210","    clf = IsolationForest(max_features=0.8)","211","    clf.fit(X_train, y_train)","212","    clf.predict(X_test)"],"delete":[]}]}},"81601fb4b8ca43b89e867ec038d9ba5a48d01a49":{"changes":{"examples\/compose\/plot_column_transformer.py":"MODIFY"},"diff":{"examples\/compose\/plot_column_transformer.py":[{"add":["91","    # Use ColumnTransformer to combine the features from subject and body"],"delete":["91","    # Use C toolumnTransformer to combine the features from subject and body"]}]}},"01bc8b18acf4123f3213786dccfef95d8ad1d24c":{"changes":{"sklearn\/utils\/_random.pyx":"MODIFY","build_tools\/circle\/build_doc.sh":"MODIFY","sklearn\/linear_model\/tests\/test_logistic.py":"MODIFY","sklearn\/tree\/_utils.pyx":"MODIFY","sklearn\/tests\/test_multioutput.py":"MODIFY","sklearn\/tree\/_utils.pxd":"MODIFY","sklearn\/utils\/_random.pxd":"MODIFY","sklearn\/utils\/seq_dataset.pyx.tp":"MODIFY","sklearn\/utils\/tests\/test_seq_dataset.py":"MODIFY","sklearn\/linear_model\/stochastic_gradient.py":"MODIFY","doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/linear_model\/perceptron.py":"MODIFY","sklearn\/kernel_approximation.py":"MODIFY","sklearn\/utils\/tests\/test_random.py":"MODIFY","doc\/tutorial\/text_analytics\/working_with_text_data.rst":"MODIFY","sklearn\/linear_model\/tests\/test_sgd.py":"MODIFY","sklearn\/linear_model\/cd_fast.pyx":"MODIFY","sklearn\/linear_model\/tests\/test_passive_aggressive.py":"MODIFY","sklearn\/linear_model\/passive_aggressive.py":"MODIFY"},"diff":{"sklearn\/utils\/_random.pyx":[{"add":["15","    * Fast rand_r alternative based on xor shifts","25","cdef UINT32_t DEFAULT_SEED = 1","26","","314","","315","","316","def _our_rand_r_py(seed):","317","    \"\"\"Python utils to test the our_rand_r function\"\"\"","318","    cdef UINT32_t my_seed = seed","319","    return our_rand_r(&my_seed)"],"delete":["15",""]}],"build_tools\/circle\/build_doc.sh":[{"add":["126","pip install \"sphinx-gallery>=0.2,<0.3\""],"delete":["126","pip install sphinx-gallery"]}],"sklearn\/linear_model\/tests\/test_logistic.py":[{"add":["1673","                               random_state=1)","1677","        penalty='elasticnet', random_state=1, fit_intercept=False, tol=-np.inf,","1680","        penalty='elasticnet', random_state=1, fit_intercept=False, tol=1e-5,"],"delete":["1673","                               random_state=0)","1677","        penalty='elasticnet', random_state=0, fit_intercept=False, tol=-np.inf,","1680","        penalty='elasticnet', random_state=0, fit_intercept=False, tol=1e-5,"]}],"sklearn\/tree\/_utils.pyx":[{"add":["23","from sklearn.utils cimport _random","24","","67","    return low + _random.our_rand_r(random_state) % (high - low)","73","    return ((high - low) * <double> _random.our_rand_r(random_state) \/"],"delete":["55","# rand_r replacement using a 32bit XorShift generator","56","# See https:\/\/www.jstatsoft.org\/v08\/i14\/paper for details","57","cdef inline UINT32_t our_rand_r(UINT32_t* seed) nogil:","58","    seed[0] ^= <UINT32_t>(seed[0] << 13)","59","    seed[0] ^= <UINT32_t>(seed[0] >> 17)","60","    seed[0] ^= <UINT32_t>(seed[0] << 5)","61","","62","    return seed[0] % (<UINT32_t>RAND_R_MAX + 1)","63","","64","","75","    return low + our_rand_r(random_state) % (high - low)","81","    return ((high - low) * <double> our_rand_r(random_state) \/"]}],"sklearn\/tests\/test_multioutput.py":[{"add":["334","    sgd_linear_clf = SGDClassifier(random_state=1, max_iter=20)","341","    sgd_linear_clf = SGDClassifier(random_state=1, max_iter=20)"],"delete":["334","    sgd_linear_clf = SGDClassifier(random_state=1, max_iter=5)","341","    sgd_linear_clf = SGDClassifier(random_state=1, max_iter=5)"]}],"sklearn\/tree\/_utils.pxd":[{"add":["25",""],"delete":[]}],"sklearn\/utils\/_random.pxd":[{"add":["9","ctypedef np.npy_uint32 UINT32_t","11","cdef inline UINT32_t DEFAULT_SEED = 1","12","","13","cdef enum:","14","    # Max value for our rand_r replacement (near the bottom).","15","    # We don't use RAND_MAX because it's different across platforms and","16","    # particularly tiny on Windows\/MSVC.","17","    RAND_R_MAX = 0x7FFFFFFF","24","# rand_r replacement using a 32bit XorShift generator","25","# See http:\/\/www.jstatsoft.org\/v08\/i14\/paper for details","26","cdef inline UINT32_t our_rand_r(UINT32_t* seed) nogil:","27","    \"\"\"Generate a pseudo-random np.uint32 from a np.uint32 seed\"\"\"","28","    # seed shouldn't ever be 0.","29","    if (seed[0] == 0): seed[0] = DEFAULT_SEED","30","","31","    seed[0] ^= <UINT32_t>(seed[0] << 13)","32","    seed[0] ^= <UINT32_t>(seed[0] >> 17)","33","    seed[0] ^= <UINT32_t>(seed[0] << 5)","34","","35","    # Note: we must be careful with the final line cast to np.uint32 so that","36","    # the function behaves consistently across platforms.","37","    #","38","    # The following cast might yield different results on different platforms:","39","    # wrong_cast = <UINT32_t> RAND_R_MAX + 1","40","    #","41","    # We can use:","42","    # good_cast = <UINT32_t>(RAND_R_MAX + 1)","43","    # or:","44","    # cdef np.uint32_t another_good_cast = <UINT32_t>RAND_R_MAX + 1","45","    return seed[0] % <UINT32_t>(RAND_R_MAX + 1)"],"delete":[]}],"sklearn\/utils\/seq_dataset.pyx.tp":[{"add":["48","from sklearn.utils cimport _random","158","            j = i + _random.our_rand_r(&seed) % (n - i)","172","        cdef int current_index = _random.our_rand_r(&self.seed) % n"],"delete":["157","            j = i + our_rand_r(&seed) % (n - i)","171","        cdef int current_index = our_rand_r(&self.seed) % n","359","","360","cdef enum:","361","    RAND_R_MAX = 0x7FFFFFFF","362","","363","","364","# rand_r replacement using a 32bit XorShift generator","365","# See https:\/\/www.jstatsoft.org\/v08\/i14\/paper for details","366","# XXX copied over from sklearn\/tree\/_tree.pyx, should refactor","367","cdef inline np.uint32_t our_rand_r(np.uint32_t* seed) nogil:","368","    seed[0] ^= <np.uint32_t>(seed[0] << 13)","369","    seed[0] ^= <np.uint32_t>(seed[0] >> 17)","370","    seed[0] ^= <np.uint32_t>(seed[0] << 5)","371","","372","    return seed[0] % (<np.uint32_t>RAND_R_MAX + 1)"]}],"sklearn\/utils\/tests\/test_seq_dataset.py":[{"add":["6","import pytest","8","from numpy.testing import assert_array_equal","9","from sklearn.utils.seq_dataset import (","10","    ArrayDataset32, ArrayDataset64, CSRDataset32, CSRDataset64)","13","from sklearn.utils.testing import assert_allclose","96","    for i in [132, 50, 9, 18, 58]:","99","        assert idx1 == i","100","        assert idx2 == i","106","    idx_next = [63, 91, 148, 87, 29]","107","    idx_shuffle = [137, 125, 56, 121, 127]","108","    for i, j in zip(idx_next, idx_shuffle):","111","        assert idx1 == i","112","        assert idx2 == i","116","        assert idx1 == j","117","        assert idx2 == j"],"delete":["5","import pytest","7","from numpy.testing import assert_array_equal","8","from sklearn.utils.testing import assert_allclose","10","","11","from sklearn.utils.seq_dataset import ArrayDataset64","12","from sklearn.utils.seq_dataset import ArrayDataset32","13","from sklearn.utils.seq_dataset import CSRDataset64","14","from sklearn.utils.seq_dataset import CSRDataset32","99","    for i in range(5):","102","        assert idx1 == idx2","108","    for i in range(5):","111","        assert idx1 == idx2","115","        assert idx1 == idx2"]}],"sklearn\/linear_model\/stochastic_gradient.py":[{"add":["44","MAX_INT = np.iinfo(np.int32).max","45","","326","               pos_weight, neg_weight, sample_weight, validation_mask=None,","327","               random_state=None):","371","","372","    random_state : int, RandomState instance or None, optional (default=None)","373","        If int, random_state is the seed used by the random number generator;","374","        If RandomState instance, random_state is the random number generator;","375","        If None, the random number generator is the RandomState instance used","376","        by `np.random`.","383","","384","    random_state = check_random_state(random_state)","385","    dataset, intercept_decay = make_dataset(","386","        X, y_i, sample_weight, random_state=random_state)","399","    seed = random_state.randint(MAX_INT)","570","                                              sample_weight,","571","                                              random_state=self.random_state)","602","        # Pick the random seed for each job outside of fit_binary to avoid","603","        # sharing the estimator random state between threads which could lead","604","        # to non-deterministic behavior","605","        random_state = check_random_state(self.random_state)","606","        seeds = random_state.randint(MAX_INT, size=len(self.classes_))","612","                                validation_mask=validation_mask,","613","                                random_state=seed)","614","            for i, seed in enumerate(seeds))"],"delete":["324","               pos_weight, neg_weight, sample_weight, validation_mask=None):","374","    dataset, intercept_decay = make_dataset(X, y_i, sample_weight)","385","    # XXX should have random_state_!","386","    random_state = check_random_state(est.random_state)","389","    seed = random_state.randint(0, np.iinfo(np.int32).max)","560","                                              sample_weight)","596","                                validation_mask=validation_mask)","597","            for i in range(len(self.classes_)))"]}],"doc\/whats_new\/v0.21.rst":[{"add":["32","- :class:`linear_model.SGDClassifier` and any derived classifiers. |Fix|","33","- Any model using the :func:`linear_model.sag.sag_solver` function with a `0`","34","  seed, including :class:`linear_model.LogisticRegression`,","35","  :class:`linear_model.LogisticRegressionCV`, :class:`linear_model.Ridge`,","36","  and :class:`linear_model.RidgeCV` with 'sag' solver. |Fix|","311","- |Fix| Fixed a bug in","312","  :class:`linear_model.stochastic_gradient.BaseSGDClassifier` that was not","313","  deterministic when trained in a multi-class setting on several threads.","314","  :issue:`13422` by :user:`Clment Doumouro <ClemDoum>`.","315","","562","- |Efficiency| Memory copies are avoided when casting arrays to a different","563","  dtype in multiple estimators. :issue:`11973` by :user:`Roman Yurchak","564","  <rth>`.","565","- |Fix| Fixed a bug in the implementation of the :func:`our_rand_r`","566","  helper function that was not behaving consistently across platforms.","567","  :issue:`13422` by :user:`Madhura Parikh <jdnc>` and","568","  :user:`Clment Doumouro <ClemDoum>`."],"delete":["546","- |Efficiency| Memory copies are avoided when casting arrays to a different","547","  dtype in multiple estimators. :issue:`11973` by :user:`Roman Yurchak","548","  <rth>`."]}],"sklearn\/linear_model\/perceptron.py":[{"add":["130","    0.939..."],"delete":["130","    0.946..."]}],"sklearn\/kernel_approximation.py":[{"add":["293","    0.9499..."],"delete":["293","    0.9543..."]}],"sklearn\/utils\/tests\/test_random.py":[{"add":["4","from sklearn.utils.fixes import comb","5","from sklearn.utils.random import random_choice_csc, sample_without_replacement","6","from sklearn.utils._random import _our_rand_r_py","7","from sklearn.utils.testing import (assert_equal, assert_raises)","177","","178","","179","def test_our_rand_r():","180","    assert 131541053 == _our_rand_r_py(1273642419)","181","    assert 270369 == _our_rand_r_py(0)"],"delete":["0","","4","from sklearn.utils.random import sample_without_replacement","5","from sklearn.utils.random import random_choice_csc","6","from sklearn.utils.fixes import comb","8","from sklearn.utils.testing import (","9","    assert_raises,","10","    assert_equal)"]}],"doc\/tutorial\/text_analytics\/working_with_text_data.rst":[{"add":["369","  0.9101...","380","             alt.atheism       0.95      0.80      0.87       319","381","           comp.graphics       0.87      0.98      0.92       389","382","                 sci.med       0.94      0.89      0.91       396","386","               macro avg       0.91      0.91      0.91      1502","387","            weighted avg       0.91      0.91      0.91      1502","391","  array([[256,  11,  16,  36],","392","         [  4, 380,   3,   2],","393","         [  5,  35, 353,   3],","394","         [  5,  11,   4, 378]])","472","  vect__ngram_range: (1, 1)"],"delete":["369","  0.9127...","380","             alt.atheism       0.95      0.81      0.87       319","381","           comp.graphics       0.88      0.97      0.92       389","382","                 sci.med       0.94      0.90      0.92       396","386","               macro avg       0.92      0.91      0.91      1502","387","            weighted avg       0.92      0.91      0.91      1502","391","  array([[258,  11,  15,  35],","392","         [  4, 379,   3,   3],","393","         [  5,  33, 355,   3],","394","         [  5,  10,   4, 379]])","395","","473","  vect__ngram_range: (1, 2)"]}],"sklearn\/linear_model\/tests\/test_sgd.py":[{"add":["1034","    random_state = np.random.RandomState(1)","1036","                eta0=0.1, loss=\"epsilon_insensitive\",","1037","                random_state=random_state)","1042","                eta0=0.1, loss=\"squared_epsilon_insensitive\",","1043","                random_state=random_state)","1047","    clf = klass(alpha=0.01, loss=\"huber\", random_state=random_state)","1052","                loss=\"squared_loss\", random_state=random_state)"],"delete":["1035","                eta0=0.1, loss=\"epsilon_insensitive\")","1040","                eta0=0.1, loss=\"squared_epsilon_insensitive\")","1044","    clf = klass(alpha=0.01, loss=\"huber\")","1049","                loss=\"squared_loss\")"]}],"sklearn\/linear_model\/cd_fast.pyx":[{"add":["26","from sklearn.utils cimport _random ","27","","44","    return _random.our_rand_r(random_state) % end"],"delete":["40","cdef inline UINT32_t our_rand_r(UINT32_t* seed) nogil:","41","    seed[0] ^= <UINT32_t>(seed[0] << 13)","42","    seed[0] ^= <UINT32_t>(seed[0] >> 17)","43","    seed[0] ^= <UINT32_t>(seed[0] << 5)","44","","45","    return seed[0] % (<UINT32_t>RAND_R_MAX + 1)","46","","47","","50","    return our_rand_r(random_state) % end"]}],"sklearn\/linear_model\/tests\/test_passive_aggressive.py":[{"add":["78","                    random_state=1, average=average, tol=None)"],"delete":["78","                    random_state=0, average=average, tol=None)"]}],"sklearn\/linear_model\/passive_aggressive.py":[{"add":["141","    [[0.26642044 0.45070924 0.67251877 0.64185414]]","143","    [1.84127814]"],"delete":["141","    [[-0.6543424   1.54603022  1.35361642  0.22199435]]","143","    [0.63310933]"]}]}},"c7ce6757d35d50c59b9e3d59ece755e95dd0e602":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/metrics\/pairwise.py":"MODIFY","sklearn\/metrics\/tests\/test_pairwise.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["4",".. _changes_0_21_1:","5","","6","Version 0.21.1","7","==============","8","","9","**May 2019**","10","","11","","12","This is a bug-fix release with some minor documentation improvements and","13","enhancements to features released in 0.21.0.","14","","15","Changelog","16","---------","17","","18",":mod:`sklearn.metrics`","19","......................","20","","21","- |Fix| Fixed a bug in :class:`metrics.pairwise_distances` where it would raise","22","  ``AttributeError`` for boolean metrics when ``X`` had a boolean dtype and","23","  ``Y == None``.","24","  :issue:`13864` by :user:`Paresh Mathur <rick2047>`.","25","","26",""],"delete":[]}],"sklearn\/metrics\/pairwise.py":[{"add":["308","        10 * 2 ** 17)","317","    batch_size = (-tmp + np.sqrt(tmp ** 2 + 4 * maxmem)) \/ 2","902","    np.tanh(K, K)  # compute tanh in-place","935","    np.exp(K, K)  # exponentiate K in-place","969","    np.exp(K, K)  # exponentiate K in-place","1547","        if (dtype == bool and","1548","                (X.dtype != bool or (Y is not None and Y.dtype != bool))):"],"delete":["308","        10 * 2**17)","317","    batch_size = (-tmp + np.sqrt(tmp**2 + 4 * maxmem)) \/ 2","902","    np.tanh(K, K)   # compute tanh in-place","935","    np.exp(K, K)    # exponentiate K in-place","969","    np.exp(K, K)    # exponentiate K in-place","1547","        if dtype == bool and (X.dtype != bool or Y.dtype != bool):","1578",""]}],"sklearn\/metrics\/tests\/test_pairwise.py":[{"add":["175","    # Check that the warning is raised if X is boolean by Y is not boolean:","176","    with pytest.warns(DataConversionWarning, match=msg):","177","        pairwise_distances(X.astype(bool), Y=Y, metric=metric)","178","","179","    # Check that no warning is raised if X is already boolean and Y is None:","180","    with pytest.warns(None) as records:","181","        pairwise_distances(X.astype(bool), metric=metric)","182","    assert len(records) == 0","183",""],"delete":[]}]}},"1add39ff110bab171df998d978cc88bab0232e54":{"changes":{"sklearn\/feature_selection\/univariate_selection.py":"MODIFY","sklearn\/feature_selection\/tests\/test_feature_select.py":"MODIFY"},"diff":{"sklearn\/feature_selection\/univariate_selection.py":[{"add":["498","        m = float(len(self.pvalues_))","499","        selected = sv[sv < alpha \/ m * np.arange(len(self.pvalues_))]"],"delete":["498","        selected = sv[sv < alpha * np.arange(len(self.pvalues_))]"]}],"sklearn\/feature_selection\/tests\/test_feature_select.py":[{"add":["297","    alpha = 0.01","298","    univariate_filter = SelectFdr(f_classif, alpha=alpha)","301","        f_classif, mode='fdr', param=alpha).fit(X, y).transform(X)","308","    num_false_positives = np.sum(support[5:] == 1)","309","    num_true_positives = np.sum(support[:5] == 1)","310","","311","    false_discovery_rate = float(num_false_positives) \/ \\","312","        (num_true_positives + num_false_positives)","313","    # We have that","314","    #  FDR = E(num_false_positives \/ (true_positives + false_positives))","315","    #      <= alpha","316","    assert(false_discovery_rate < alpha)","317","","323","    with the fwe heuristic","457","    with the fdr heuristic.","459","    Tests that the scaling factors","460","    \"\"\"","461","    def scale_invariance(n_samples, n_features, n_informative, alpha):","462","        X, y = make_regression(n_samples=n_samples, n_features=n_features,","463","                               n_informative=n_informative, shuffle=False,","464","                               random_state=0)","465","","466","        univariate_filter = SelectFdr(f_regression, alpha=alpha)","467","        X_r = univariate_filter.fit(X, y).transform(X)","468","        X_r2 = GenericUnivariateSelect(","469","            f_regression, mode='fdr', param=alpha).fit(X, y).transform(X)","470","        assert_array_equal(X_r, X_r2)","471","        support = univariate_filter.get_support()","472","        num_false_positives = np.sum(support[n_informative:] == 1)","473","        num_true_positives = np.sum(support[:n_informative] == 1)","474","","475","        false_discovery_rate = float(num_false_positives) \/ \\","476","            (num_true_positives + num_false_positives)","477","        # We have that","478","        #  FDR = E(num_false_positives \/ (true_positives + false_positives))","479","        #      <= alpha","480","        assert(false_discovery_rate < alpha)","481","","482","    feature_scaling_factors = (5, 10, 20)","483","    for scale_factor in feature_scaling_factors:","484","        scale_invariance(2000, 5 * scale_factor, 1 * scale_factor, 0.05)"],"delete":["297","","298","    univariate_filter = SelectFdr(f_classif, alpha=0.0001)","301","        f_classif, mode='fdr', param=0.0001).fit(X, y).transform(X)","313","    with the fpr heuristic","447","    with the fdr heuristic","448","    \"\"\"","449","    X, y = make_regression(n_samples=200, n_features=20,","450","                           n_informative=5, shuffle=False, random_state=0)","452","    univariate_filter = SelectFdr(f_regression, alpha=0.01)","453","    X_r = univariate_filter.fit(X, y).transform(X)","454","    X_r2 = GenericUnivariateSelect(","455","        f_regression, mode='fdr', param=0.01).fit(X, y).transform(X)","456","    assert_array_equal(X_r, X_r2)","457","    support = univariate_filter.get_support()","458","    gtruth = np.zeros(20)","459","    gtruth[:5] = 1","460","    assert_array_equal(support, gtruth)"]}]}},"3f8743f47b61a269e8bfff2322cb544170976574":{"changes":{"sklearn\/__init__.py":"MODIFY","sklearn\/neighbors\/tests\/test_neighbors.py":"MODIFY","sklearn\/setup.py":"MODIFY","sklearn\/tests\/test_multiclass.py":"MODIFY","sklearn\/utils\/testing.py":"MODIFY","sklearn\/covariance\/graph_lasso_.py":"MODIFY","sklearn\/learning_curve.py":"MODIFY","sklearn\/tests\/test_naive_bayes.py":"MODIFY","sklearn\/tests\/test_metaestimators.py":"MODIFY","sklearn\/tests\/test_base.py":"MODIFY","sklearn\/model_selection\/_search.py":"ADD","sklearn\/cross_validation.py":"MODIFY","sklearn\/tree\/tree.py":"MODIFY","sklearn\/linear_model\/logistic.py":"MODIFY","sklearn\/ensemble\/tests\/test_forest.py":"MODIFY","sklearn\/ensemble\/tests\/test_voting_classifier.py":"MODIFY","sklearn\/linear_model\/omp.py":"MODIFY","sklearn\/ensemble\/tests\/test_weight_boosting.py":"MODIFY","sklearn\/metrics\/tests\/test_score_objects.py":"MODIFY","sklearn\/model_selection\/_split.py":"ADD","sklearn\/preprocessing\/tests\/test_imputation.py":"MODIFY","sklearn\/linear_model\/tests\/test_least_angle.py":"MODIFY","sklearn\/tests\/test_grid_search.py":"MODIFY","sklearn\/model_selection\/tests\/test_validation.py":"ADD","doc\/whats_new.rst":"MODIFY","sklearn\/tests\/test_learning_curve.py":"MODIFY","sklearn\/grid_search.py":"MODIFY","sklearn\/exceptions.py":"MODIFY","sklearn\/linear_model\/tests\/test_logistic.py":"MODIFY","sklearn\/model_selection\/tests\/test_search.py":"ADD","sklearn\/linear_model\/coordinate_descent.py":"MODIFY","sklearn\/model_selection\/tests\/test_split.py":"ADD","sklearn\/linear_model\/tests\/test_ridge.py":"MODIFY","sklearn\/feature_selection\/tests\/test_rfe.py":"MODIFY","sklearn\/neighbors\/tests\/test_kde.py":"MODIFY","sklearn\/feature_extraction\/tests\/test_text.py":"MODIFY","sklearn\/metrics\/scorer.py":"MODIFY","sklearn\/calibration.py":"MODIFY","sklearn\/tests\/test_cross_validation.py":"MODIFY","sklearn\/svm\/tests\/test_svm.py":"MODIFY","sklearn\/linear_model\/ridge.py":"MODIFY","sklearn\/feature_selection\/rfe.py":"MODIFY","sklearn\/decomposition\/tests\/test_kernel_pca.py":"MODIFY","sklearn\/utils\/estimator_checks.py":"MODIFY","sklearn\/cluster\/tests\/test_bicluster.py":"MODIFY","sklearn\/model_selection\/__init__.py":"ADD","sklearn\/ensemble\/tests\/test_bagging.py":"MODIFY","sklearn\/model_selection\/_validation.py":"ADD","sklearn\/linear_model\/least_angle.py":"MODIFY"},"diff":{"sklearn\/__init__.py":[{"add":["64","               'lda', 'learning_curve', 'linear_model', 'manifold', 'metrics',","65","               'mixture', 'model_selection', 'multiclass',"],"delete":["64","               'lda', 'learning_curve',","65","               'linear_model', 'manifold', 'metrics', 'mixture', 'multiclass',"]}],"sklearn\/neighbors\/tests\/test_neighbors.py":[{"add":["8","from sklearn.model_selection import train_test_split","9","from sklearn.model_selection import cross_val_score"],"delete":["8","from sklearn.cross_validation import train_test_split, cross_val_score"]}],"sklearn\/setup.py":[{"add":["51","    config.add_subpackage('model_selection')","52","    config.add_subpackage('model_selection\/tests')"],"delete":[]}],"sklearn\/tests\/test_multiclass.py":[{"add":["23","from sklearn.model_selection import GridSearchCV"],"delete":["23","from sklearn.grid_search import GridSearchCV"]}],"sklearn\/utils\/testing.py":[{"add":["22","from operator import itemgetter","607","        if (\".tests.\" in modname):","651","    # itemgetter is used to ensure the sort does not extend to the 2nd item of","652","    # the tuple","653","    return sorted(set(estimators), key=itemgetter(0))"],"delete":["606","        if \".tests.\" in modname:","650","    return sorted(set(estimators))"]}],"sklearn\/covariance\/graph_lasso_.py":[{"add":["23","from ..model_selection import check_cv, cross_val_score","582","        cv = check_cv(self.cv, y, classifier=False)","614","                )(delayed(graph_lasso_path)(X[train], alphas=alphas,","615","                                            X_test=X[test], mode=self.mode,","616","                                            tol=self.tol,","617","                                            enet_tol=self.enet_tol,","618","                                            max_iter=int(.1 * self.max_iter),","619","                                            verbose=inner_verbose)","620","                  for train, test in cv.split(X, y))"],"delete":["23","from ..cross_validation import check_cv, cross_val_score","582","        cv = check_cv(self.cv, X, y, classifier=False)","614","                )(","615","                    delayed(graph_lasso_path)(","616","                        X[train], alphas=alphas,","617","                        X_test=X[test], mode=self.mode,","618","                        tol=self.tol, enet_tol=self.enet_tol,","619","                        max_iter=int(.1 * self.max_iter),","620","                        verbose=inner_verbose)","621","                    for train, test in cv)"]}],"sklearn\/learning_curve.py":[{"add":["19","warnings.warn(\"This module has been deprecated in favor of the \"","20","              \"model_selection module into which all the functions are moved.\"","21","              \" This module will be removed in 0.19\",","22","              DeprecationWarning)","23","","24",""],"delete":[]}],"sklearn\/tests\/test_naive_bayes.py":[{"add":["6","","7","from sklearn.model_selection import train_test_split","8","from sklearn.model_selection import cross_val_score"],"delete":["6","from sklearn.cross_validation import cross_val_score, train_test_split"]}],"sklearn\/tests\/test_metaestimators.py":[{"add":["11","from sklearn.model_selection import GridSearchCV, RandomizedSearchCV"],"delete":["11","from sklearn.grid_search import GridSearchCV, RandomizedSearchCV"]}],"sklearn\/tests\/test_base.py":[{"add":["16","from sklearn.model_selection import GridSearchCV"],"delete":["16","from sklearn.grid_search import GridSearchCV"]}],"sklearn\/model_selection\/_search.py":[{"add":[],"delete":[]}],"sklearn\/cross_validation.py":[{"add":["36","","37","warnings.warn(\"This module has been deprecated in favor of the \"","38","              \"model_selection module into which all the refactored classes \"","39","              \"and functions are moved. Also note that the interface of the \"","40","              \"new CV iterators are different from that of this module. \"","41","              \"This module will be removed in 0.19.\", DeprecationWarning)","42","","43","","314","    StratifiedKFold take label information into account to avoid building"],"delete":["306","    StratifiedKFold: take label information into account to avoid building"]}],"sklearn\/tree\/tree.py":[{"add":["636","    >>> from sklearn.model_selection import cross_val_score","856","    >>> from sklearn.model_selection import cross_val_score"],"delete":["636","    >>> from sklearn.cross_validation import cross_val_score","856","    >>> from sklearn.cross_validation import cross_val_score"]}],"sklearn\/linear_model\/logistic.py":[{"add":["0","","35","from ..model_selection import check_cv","1312","        See the module :mod:`sklearn.model_selection` module for the","1509","        cv = check_cv(self.cv, y, classifier=True)","1510","        folds = list(cv.split(X, y))"],"delete":["34","from ..cross_validation import check_cv","1311","        See the module :mod:`sklearn.cross_validation` module for the","1508","        cv = check_cv(self.cv, X, y, classifier=True)","1509","        folds = list(cv)"]}],"sklearn\/ensemble\/tests\/test_forest.py":[{"add":["40","from sklearn.model_selection import GridSearchCV"],"delete":["40","from sklearn.grid_search import GridSearchCV"]}],"sklearn\/ensemble\/tests\/test_voting_classifier.py":[{"add":["9","from sklearn.model_selection import GridSearchCV","11","from sklearn.model_selection import cross_val_score","29","    scores = cross_val_score(eclf, X, y, cv=5, scoring='accuracy')","53","    scores = cross_val_score(eclf, X, y, cv=5, scoring='accuracy')"],"delete":["9","from sklearn.grid_search import GridSearchCV","11","from sklearn import cross_validation","29","    scores = cross_validation.cross_val_score(eclf,","30","                                              X,","31","                                              y,","32","                                              cv=5,","33","                                              scoring='accuracy')","57","    scores = cross_validation.cross_val_score(eclf,","58","                                              X,","59","                                              y,","60","                                              cv=5,","61","                                              scoring='accuracy')"]}],"sklearn\/linear_model\/omp.py":[{"add":["17","from ..model_selection import check_cv","837","        cv = check_cv(self.cv, classifier=False)","845","            for train, test in cv.split(X))"],"delete":["17","from ..cross_validation import check_cv","837","        cv = check_cv(self.cv, X, y, classifier=False)","845","            for train, test in cv)"]}],"sklearn\/ensemble\/tests\/test_weight_boosting.py":[{"add":["9","from sklearn.model_selection import train_test_split","10","from sklearn.model_selection import GridSearchCV"],"delete":["9","from sklearn.cross_validation import train_test_split","10","from sklearn.grid_search import GridSearchCV"]}],"sklearn\/metrics\/tests\/test_score_objects.py":[{"add":["29","from sklearn.model_selection import train_test_split, cross_val_score","30","from sklearn.model_selection import GridSearchCV"],"delete":["29","from sklearn.cross_validation import train_test_split, cross_val_score","30","from sklearn.grid_search import GridSearchCV"]}],"sklearn\/model_selection\/_split.py":[{"add":[],"delete":[]}],"sklearn\/preprocessing\/tests\/test_imputation.py":[{"add":["12","from sklearn.model_selection import GridSearchCV","271","    gs = GridSearchCV(pipeline, parameters)"],"delete":["12","from sklearn import grid_search","271","    gs = grid_search.GridSearchCV(pipeline, parameters)"]}],"sklearn\/linear_model\/tests\/test_least_angle.py":[{"add":["5","from sklearn.model_selection import train_test_split"],"delete":["5","from sklearn.cross_validation import train_test_split"]}],"sklearn\/tests\/test_grid_search.py":[{"add":["10","import warnings","44","","45","from sklearn.exceptions import ChangedBehaviorWarning","47","","48","with warnings.catch_warnings():","49","    warnings.simplefilter('ignore')","50","    from sklearn.grid_search import (GridSearchCV, RandomizedSearchCV,","51","                                     ParameterGrid, ParameterSampler)","52","    from sklearn.cross_validation import KFold, StratifiedKFold","53",""],"delete":["35","from sklearn.grid_search import GridSearchCV, RandomizedSearchCV","36","from sklearn.grid_search import ParameterGrid, ParameterSampler","37","from sklearn.exceptions import ChangedBehaviorWarning","46","from sklearn.cross_validation import KFold, StratifiedKFold"]}],"sklearn\/model_selection\/tests\/test_validation.py":[{"add":[],"delete":[]}],"doc\/whats_new.rst":[{"add":["27","   - The cross-validation iterators are now modified as cross-validation splitters","28","     which expose a ``split`` method that takes in the data and yields a generator","29","     for the different splits. This change makes it possible to do nested cross-validation","30","     with ease. (`#4294 https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/4294>`_) by `Raghav R V`_.","31","","32","   - The :mod:`cross_validation`, :mod:`grid_search` and :mod:`learning_curve`","33","     have been deprecated and the classes and functions have been reorganized into","34","     the :mod:`model_selection` module. (`#4294 https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/4294>`_) by `Raghav R V`_.","35","","36",""],"delete":[]}],"sklearn\/tests\/test_learning_curve.py":[{"add":["15","","16","with warnings.catch_warnings():","17","    warnings.simplefilter('ignore')","18","    from sklearn.learning_curve import learning_curve, validation_curve","19","    from sklearn.cross_validation import KFold","20",""],"delete":["9","from sklearn.learning_curve import learning_curve, validation_curve","16","from sklearn.cross_validation import KFold"]}],"sklearn\/grid_search.py":[{"add":["39","warnings.warn(\"This module has been deprecated in favor of the \"","40","              \"model_selection module into which all the refactored classes \"","41","              \"and functions are moved. This module will be removed in 0.19.\",","42","              DeprecationWarning)","43","","44",""],"delete":[]}],"sklearn\/exceptions.py":[{"add":["87","    >>> from sklearn.model_selection import GridSearchCV"],"delete":["87","    >>> from sklearn.grid_search import GridSearchCV"]}],"sklearn\/linear_model\/tests\/test_logistic.py":[{"add":["26","from sklearn.model_selection import StratifiedKFold","456","    # The cv indices from stratified kfold (where stratification is done based","457","    # on the fine-grained iris classes, i.e, before the classes 0 and 1 are","458","    # conflated) is used for both clf and clf1","459","    cv = StratifiedKFold(3)","460","    precomputed_folds = list(cv.split(train, target))","461","","462","    # Train clf on the original dataset where classes 0 and 1 are separated","463","    clf = LogisticRegressionCV(cv=precomputed_folds)","466","    # Conflate classes 0 and 1 and train clf1 on this modifed dataset","467","    clf1 = LogisticRegressionCV(cv=precomputed_folds)","472","    # Ensure that what OvR learns for class2 is same regardless of whether","473","    # classes 0 and 1 are separated or not"],"delete":["26","from sklearn.cross_validation import StratifiedKFold","456","    # Use pre-defined fold as folds generated for different y","457","    cv = StratifiedKFold(target, 3)","458","    clf = LogisticRegressionCV(cv=cv)","461","    clf1 = LogisticRegressionCV(cv=cv)"]}],"sklearn\/model_selection\/tests\/test_search.py":[{"add":[],"delete":[]}],"sklearn\/linear_model\/coordinate_descent.py":[{"add":["19","from ..model_selection import check_cv","1122","        cv = check_cv(self.cv)","1125","        folds = list(cv.split(X))"],"delete":["19","from ..cross_validation import check_cv","1122","        cv = check_cv(self.cv, X)","1125","        folds = list(cv)"]}],"sklearn\/model_selection\/tests\/test_split.py":[{"add":[],"delete":[]}],"sklearn\/linear_model\/tests\/test_ridge.py":[{"add":["31","from sklearn.model_selection import GridSearchCV","32","from sklearn.model_selection import KFold","366","    cv = KFold(5)","405","    cv = KFold(5)","569","        cv = KFold(5)"],"delete":["31","from sklearn.grid_search import GridSearchCV","32","","33","from sklearn.cross_validation import KFold","360","    n_samples = X_diabetes.shape[0]","361","","369","    cv = KFold(n_samples, 5)","408","    n_samples = X_iris.shape[0]","409","    cv = KFold(n_samples, 5)","573","        cv = KFold(n_samples, 5)"]}],"sklearn\/feature_selection\/tests\/test_rfe.py":[{"add":["14","from sklearn.model_selection import cross_val_score"],"delete":["14","from sklearn.cross_validation import cross_val_score"]}],"sklearn\/neighbors\/tests\/test_kde.py":[{"add":["7","from sklearn.model_selection import GridSearchCV"],"delete":["7","from sklearn.grid_search import GridSearchCV"]}],"sklearn\/feature_extraction\/tests\/test_text.py":[{"add":["14","from sklearn.model_selection import train_test_split","15","from sklearn.model_selection import cross_val_score","16","from sklearn.model_selection import GridSearchCV"],"delete":["14","from sklearn.cross_validation import train_test_split","15","from sklearn.cross_validation import cross_val_score","16","from sklearn.grid_search import GridSearchCV"]}],"sklearn\/metrics\/scorer.py":[{"add":["6",":class:`sklearn.model_selection.GridSearchCV` or","7",":func:`sklearn.model_selection.cross_val_score` as the ``scoring``","8","parameter, to specify how a model should be evaluated.","296","    >>> from sklearn.model_selection import GridSearchCV"],"delete":["6",":class:`sklearn.grid_search.GridSearchCV` or","7",":func:`sklearn.cross_validation.cross_val_score` as the ``scoring`` parameter,","8","to specify how a model should be evaluated.","296","    >>> from sklearn.grid_search import GridSearchCV"]}],"sklearn\/calibration.py":[{"add":["24","from .model_selection import check_cv","154","            cv = check_cv(self.cv, y, classifier=True)","165","            for train, test in cv.split(X, y):"],"delete":["24","from .cross_validation import check_cv","154","            cv = check_cv(self.cv, X, y, classifier=True)","165","            for train, test in cv:"]}],"sklearn\/tests\/test_cross_validation.py":[{"add":["24","with warnings.catch_warnings():","25","    warnings.simplefilter('ignore')","26","    from sklearn import cross_validation as cval","27",""],"delete":["24","from sklearn import cross_validation as cval"]}],"sklearn\/svm\/tests\/test_svm.py":[{"add":["15","from sklearn.model_selection import train_test_split"],"delete":["15","from sklearn.cross_validation import train_test_split"]}],"sklearn\/linear_model\/ridge.py":[{"add":["30","from ..model_selection import GridSearchCV"],"delete":["30","from ..grid_search import GridSearchCV"]}],"sklearn\/feature_selection\/rfe.py":[{"add":["16","from ..model_selection import check_cv","17","from ..model_selection._validation import _safe_split, _score","375","        cv = check_cv(self.cv, y, is_classifier(self.estimator))","384","        for n, (train, test) in enumerate(cv.split(X, y)):","416","        # Fixing a normalization error, n is equal to get_n_splits(X, y) - 1","417","        # here, the scores are normalized by get_n_splits(X, y)","418","        self.grid_scores_ = scores \/ cv.get_n_splits(X, y)"],"delete":["16","from ..cross_validation import check_cv","17","from ..cross_validation import _safe_split, _score","375","        cv = check_cv(self.cv, X, y, is_classifier(self.estimator))","384","        for n, (train, test) in enumerate(cv):","416","        # Fixing a normalization error, n is equal to len(cv) - 1","417","        # here, the scores are normalized by len(cv)","418","        self.grid_scores_ = scores \/ len(cv)"]}],"sklearn\/decomposition\/tests\/test_kernel_pca.py":[{"add":["11","from sklearn.model_selection import GridSearchCV"],"delete":["11","from sklearn.grid_search import GridSearchCV"]}],"sklearn\/utils\/estimator_checks.py":[{"add":["45","from sklearn.model_selection import train_test_split"],"delete":["45","from sklearn.cross_validation import train_test_split"]}],"sklearn\/cluster\/tests\/test_bicluster.py":[{"add":["5","from sklearn.model_selection import ParameterGrid"],"delete":["5","from sklearn.grid_search import ParameterGrid"]}],"sklearn\/model_selection\/__init__.py":[{"add":[],"delete":[]}],"sklearn\/ensemble\/tests\/test_bagging.py":[{"add":["23","from sklearn.model_selection import GridSearchCV, ParameterGrid","31","from sklearn.model_selection import train_test_split"],"delete":["23","from sklearn.grid_search import GridSearchCV, ParameterGrid","31","from sklearn.cross_validation import train_test_split"]}],"sklearn\/model_selection\/_validation.py":[{"add":[],"delete":[]}],"sklearn\/linear_model\/least_angle.py":[{"add":["24","from ..model_selection import check_cv","1081","        cv = check_cv(self.cv, classifier=False)","1091","            for train, test in cv.split(X, y))"],"delete":["24","from ..cross_validation import check_cv","1081","        cv = check_cv(self.cv, X, y, classifier=False)","1091","            for train, test in cv)"]}]}},"ab399a675ab54b27ed16d79f0bc7ce6f5240346e":{"changes":{"doc\/developers\/contributing.rst":"MODIFY"},"diff":{"doc\/developers\/contributing.rst":[{"add":["215",".. note::","216","","217","  In the above setup, your ``origin`` remote repository points to","218","  ``YourLogin\/scikit-learn.git``. If you wish to fetch\/merge from the main","219","  repository instead of your forked one, you will need to add another remote","220","  to use instead of ``origin``. If we choose the name ``upstream`` for it, the","221","  command will be::","222","","223","        $ git remote add upstream https:\/\/github.com\/scikit-learn\/scikit-learn.git","224","","225","  And in order to fetch the new remote and base your work on the latest changes","226","  of it you can::","227","","228","        $ git fetch upstream","229","        $ git checkout -b my-feature upstream\/master","230","","260","to merge ``master``. For that, you first need to fetch the ``upstream``, and","261","then merge its ``master`` into your branch::","263","  $ git fetch upstream","264","  $ git merge upstream\/master"],"delete":["237",".. note::","238","","239","  In the above setup, your ``origin`` remote repository points to","240","  YourLogin\/scikit-learn.git. If you wish to fetch\/merge from the main","241","  repository instead of your forked one, you will need to add another remote","242","  to use instead of ``origin``. If we choose the name ``upstream`` for it, the","243","  command will be::","244","","245","        $ git remote add upstream https:\/\/github.com\/scikit-learn\/scikit-learn.git","253","to merge ``master``. The command will be::","255","  $ git merge master","256","","257","with ``master`` being synchronized with the ``upstream``."]}]}},"5925fb9e4fb372f254b5401c6011215132fa61ee":{"changes":{"sklearn\/ensemble\/tests\/test_voting.py":"MODIFY","sklearn\/utils\/estimator_checks.py":"MODIFY","sklearn\/tests\/test_common.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY","sklearn\/ensemble\/voting.py":"MODIFY"},"diff":{"sklearn\/ensemble\/tests\/test_voting.py":[{"add":["8","from sklearn.utils.estimator_checks import check_estimator","9","from sklearn.utils.estimator_checks import check_no_attributes_set_in_init","17","from sklearn.tree import DecisionTreeClassifier","18","from sklearn.tree import DecisionTreeRegressor","514","","515","","516","@pytest.mark.parametrize(","517","    \"estimator\",","518","    [VotingRegressor(","519","        estimators=[('lr', LinearRegression()),","520","                    ('tree', DecisionTreeRegressor(random_state=0))]),","521","     VotingClassifier(","522","         estimators=[('lr', LogisticRegression(random_state=0)),","523","                     ('tree', DecisionTreeClassifier(random_state=0))])],","524","    ids=['VotingRegressor', 'VotingClassifier']","525",")","526","def test_check_estimators_voting_estimator(estimator):","527","    # FIXME: to be removed when meta-estimators can be specified themselves","528","    # their testing parameters (for required parameters).","529","    check_estimator(estimator)","530","    check_no_attributes_set_in_init(estimator.__class__.__name__, estimator)"],"delete":[]}],"sklearn\/utils\/estimator_checks.py":[{"add":[],"delete":["32",""]}],"sklearn\/tests\/test_common.py":[{"add":["25","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"],"delete":["27","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"]}],"doc\/whats_new\/v0.22.rst":[{"add":["117","- |Fix| Run by default","118","  :func:`utils.estimator_checks.check_estimator` on both","119","  :class:`ensemble.VotingClassifier` and :class:`ensemble.VotingRegressor`. It","120","  leads to solve issues regarding shape consistency during `predict` which was","121","  failing when the underlying estimators were not outputting consistent array","122","  dimensions. Note that it should be replaced by refactoring the common tests","123","  in the future.","124","  :pr:`14305` by :user:`Guillaume Lemaitre <glemaitre>`.","125",""],"delete":[]}],"sklearn\/ensemble\/voting.py":[{"add":["17","import numpy as np","18","","26","from ..utils import Bunch","29","from ..utils.multiclass import check_classification_targets","30","from ..utils.validation import column_or_1d","72","        return np.asarray([est.predict(X) for est in self.estimators_]).T","269","        check_classification_targets(y)","460","        y = column_or_1d(y, warn=True)"],"delete":["15","import numpy as np","27","from ..utils import Bunch","69","        return np.asarray([clf.predict(X) for clf in self.estimators_]).T"]}]}},"08b82f8f46cca860d7d4afc47d803765b9a0f48c":{"changes":{"sklearn\/ensemble\/tests\/test_forest.py":"MODIFY"},"diff":{"sklearn\/ensemble\/tests\/test_forest.py":[{"add":["127","        assert score > 0.95, (\"Failed with max_features=None, \"","128","                              \"criterion %s and score = %f\" % (c, score))","134","        assert score > 0.95, (\"Failed with max_features=6, \"","135","                              \"criterion %s and score = %f\" % (c, score))","141","        assert score > 0.95, (\"Failed with max_features=None, \"","142","                              \"criterion %s and score = %f\" % (c, score))","148","        assert score > 0.95, (\"Failed with max_features=6, \"","149","                              \"criterion %s and score = %f\" % (c, score))"],"delete":["127","        assert score < 3, (\"Failed with max_features=None, \"","128","                           \"criterion %s and score = %f\" % (c, score))","134","        assert score < 3, (\"Failed with max_features=None, \"","135","                           \"criterion %s and score = %f\" % (c, score))","141","        assert score < 3, (\"Failed with max_features=None, \"","142","                           \"criterion %s and score = %f\" % (c, score))","148","        assert score < 3, (\"Failed with max_features=None, \"","149","                           \"criterion %s and score = %f\" % (c, score))"]}]}},"9b42b0cc7d5cf6978805619bc2433e3888c38d0c":{"changes":{"sklearn\/utils\/estimator_checks.py":"MODIFY","doc\/whats_new\/v0.24.rst":"MODIFY","sklearn\/utils\/tests\/test_estimator_checks.py":"MODIFY"},"diff":{"sklearn\/utils\/estimator_checks.py":[{"add":["703","    y = (4 * rng.rand(40)).astype(int)","708","    tags = estimator_orig._get_tags()","806","        y = np.arange(n_samples) % 3","897","    y = (X[:, 0] * 4).astype(int)","1360","    y = np.arange(n_samples) % 3","1563","    y = _enforce_estimator_tags_y(estimator_orig, y)","2030","    X, y = make_blobs(random_state=0, n_samples=21)","2072","    y = np.arange(n_samples) % 3","2390","    X, y = make_blobs(random_state=0, n_samples=21)","2461","    y = np.array([1, 1, 1, 2, 2, 2, 3, 3, 3])","2462","    y = _enforce_estimator_tags_y(estimator_orig, y)","2486","    y = np.array([1, 1, 1, 2, 2, 2, 1, 1, 1, 2, 2, 2])","2622","    # Estimators with a `binary_only` tag only accept up to two unique y values","2623","    if estimator._get_tags()[\"binary_only\"] and y.size > 0:","2624","        y = np.where(y == y.flat[0], y, y.flat[0] + 1)"],"delete":["703","    tags = estimator_orig._get_tags()","704","    if tags['binary_only']:","705","        y = (2 * rng.rand(40)).astype(int)","706","    else:","707","        y = (4 * rng.rand(40)).astype(int)","809","        if estimator._get_tags()['binary_only']:","810","            y = np.arange(n_samples) % 2","811","        else:","812","            y = np.arange(n_samples) % 3","903","    if tags['binary_only']:","904","        y = (X[:, 0] * 2).astype(int)","905","    else:","906","        y = (X[:, 0] * 4).astype(int)","1000","    if estimator._get_tags()['binary_only']:","1001","        y[y == 2] = 1","1052","    if tags['binary_only']:","1053","        y[y == 2] = 1","1102","    if estimator_orig._get_tags()['binary_only']:","1103","        y[y == 2] = 1","1375","    if estimator_orig._get_tags()['binary_only']:","1376","        y = np.arange(n_samples) % 2","1377","    else:","1378","        y = np.arange(n_samples) % 3","1408","    if estimator_orig._get_tags()['binary_only']:","1409","        y[y == 2] = 1","2049","    if estimator_orig._get_tags()['binary_only']:","2050","        n_centers = 2","2051","    else:","2052","        n_centers = 3","2053","    X, y = make_blobs(random_state=0, n_samples=21, centers=n_centers)","2095","    if tags['binary_only']:","2096","        y = np.arange(n_samples) % 2","2097","    else:","2098","        y = np.arange(n_samples) % 3","2416","    if estimator_orig._get_tags()['binary_only']:","2417","        n_centers = 2","2418","    else:","2419","        n_centers = 3","2420","    X, y = make_blobs(random_state=0, n_samples=21, centers=n_centers)","2491","    y = [1, 1, 1, 2, 2, 2, 3, 3, 3]","2515","    y = [1, 1, 1, 2, 2, 2, 1, 1, 1, 2, 2, 2]"]}],"doc\/whats_new\/v0.24.rst":[{"add":["296",":mod:`sklearn.utils`","297",".........................","298","","299","- |Fix| Fix :func:`utils.estimator_checks.check_estimator` so that all test","300","  cases support the `binary_only` estimator tag.","301","  :pr:`17812` by :user:`Bruno Charron <brcharron>`.","302",""],"delete":[]}],"sklearn\/utils\/tests\/test_estimator_checks.py":[{"add":["308","class UntaggedBinaryClassifier(SGDClassifier):","310","    def fit(self, X, y, coef_init=None, intercept_init=None,","311","            sample_weight=None):","312","        super().fit(X, y, coef_init, intercept_init, sample_weight)","313","        if len(self.classes_) > 2:","314","            raise ValueError('Only 2 classes are supported')","315","        return self","316","","317","    def partial_fit(self, X, y, classes=None, sample_weight=None):","318","        super().partial_fit(X=X, y=y, classes=classes,","319","                            sample_weight=sample_weight)","320","        if len(self.classes_) > 2:"],"delete":["36","from sklearn.tree import DecisionTreeClassifier","309","class UntaggedBinaryClassifier(DecisionTreeClassifier):","311","    def fit(self, X, y, sample_weight=None):","312","        super().fit(X, y, sample_weight)","313","        if np.all(self.n_classes_ > 2):"]}]}},"2a1e9686eeb203f5fddf44fd06414db8ab6a554a":{"changes":{"setup.py":"MODIFY"},"diff":{"setup.py":[{"add":["16","    # Python 2 compat: just to be able to declare that Python >=3.5 is needed.","17","    import __builtin__ as builtins","19","# This is a bit (!) hackish: we are setting a global variable so that the","20","# main sklearn __init__ can detect if it is being loaded by the setup","21","# routine, to avoid attempting to load components that aren't built yet:","22","# the numpy distutils extensions that are used by scikit-learn to","23","# recursively build the compiled extensions in sub-packages is based on the","24","# Python import machinery.","25","builtins.__SKLEARN_SETUP__ = True","26","","115","cmdclass = {'clean': CleanCommand}","116","","120","try:","121","    from numpy.distutils.command.build_ext import build_ext  # noqa","123","    class build_ext_subclass(build_ext):","124","        def build_extensions(self):","125","            from sklearn._build_utils.openmp_helpers import get_openmp_flag","127","            if not os.getenv('SKLEARN_NO_OPENMP'):","128","                openmp_flag = get_openmp_flag(self.compiler)","130","                for e in self.extensions:","131","                    e.extra_compile_args += openmp_flag","132","                    e.extra_link_args += openmp_flag","134","            build_ext.build_extensions(self)","136","    cmdclass['build_ext'] = build_ext_subclass","138","except ImportError:","139","    # Numpy should not be a dependency just to be able to introspect","140","    # that python 3.5 is required.","141","    pass","230","                    python_requires=\">=3.5\",","256","        if sys.version_info < (3, 5):","257","            raise RuntimeError(","258","                \"Scikit-learn requires Python 3.5 or later. The current\"","259","                \" Python version is %s installed in %s.\"","260","                % (platform.python_version(), sys.executable))","261",""],"delete":["15","    # This is a bit (!) hackish: we are setting a global variable so that the","16","    # main sklearn __init__ can detect if it is being loaded by the setup","17","    # routine, to avoid attempting to load components that aren't built yet:","18","    # the numpy distutils extensions that are used by scikit-learn to","19","    # recursively build the compiled extensions in sub-packages is based on the","20","    # Python import machinery.","21","    builtins.__SKLEARN_SETUP__ = True","23","    # Python 2 is not support but we will raise an explicit error message next.","24","    pass","26","if sys.version_info < (3, 5):","27","    raise RuntimeError(\"Scikit-learn requires Python 3.5 or later. The current\"","28","                       \" Python version is %s installed in %s.\"","29","                       % (platform.python_version(), sys.executable))","121","from numpy.distutils.command.build_ext import build_ext  # noqa","124","class build_ext_subclass(build_ext):","125","    def build_extensions(self):","126","        from sklearn._build_utils.openmp_helpers import get_openmp_flag","128","        if not os.getenv('SKLEARN_NO_OPENMP'):","129","            openmp_flag = get_openmp_flag(self.compiler)","131","            for e in self.extensions:","132","                e.extra_compile_args += openmp_flag","133","                e.extra_link_args += openmp_flag","135","        build_ext.build_extensions(self)","137","","138","cmdclass = {'clean': CleanCommand, 'build_ext': build_ext_subclass}"]}]}},"31ee1a8e6ed9b78408f7b807b5a0cab7a4fd35e5":{"changes":{"sklearn\/linear_model\/tests\/test_logistic.py":"MODIFY","sklearn\/linear_model\/logistic.py":"MODIFY","doc\/whats_new.rst":"MODIFY"},"diff":{"sklearn\/linear_model\/tests\/test_logistic.py":[{"add":["29","from sklearn.preprocessing import LabelEncoder","401","def test_multinomial_logistic_regression_string_inputs():","402","    # Test with string labels for LogisticRegression(CV)","403","    n_samples, n_features, n_classes = 50, 5, 3","404","    X_ref, y = make_classification(n_samples=n_samples, n_features=n_features,","405","                                   n_classes=n_classes, n_informative=3)","406","    y_str = LabelEncoder().fit(['bar', 'baz', 'foo']).inverse_transform(y)","407","    # For numerical labels, let y values be taken from set (-1, 0, 1)","408","    y = np.array(y) - 1","409","    # Test for string labels","410","    lr = LogisticRegression(solver='lbfgs', multi_class='multinomial')","411","    lr_cv = LogisticRegressionCV(solver='lbfgs', multi_class='multinomial')","412","    lr_str = LogisticRegression(solver='lbfgs', multi_class='multinomial')","413","    lr_cv_str = LogisticRegressionCV(solver='lbfgs', multi_class='multinomial')","414","","415","    lr.fit(X_ref, y)","416","    lr_cv.fit(X_ref, y)","417","    lr_str.fit(X_ref, y_str)","418","    lr_cv_str.fit(X_ref, y_str)","419","","420","    assert_array_almost_equal(lr.coef_, lr_str.coef_)","421","    assert_equal(sorted(lr_str.classes_), ['bar', 'baz', 'foo'])","422","    assert_array_almost_equal(lr_cv.coef_, lr_cv_str.coef_)","423","    assert_equal(sorted(lr_str.classes_), ['bar', 'baz', 'foo'])","424","    assert_equal(sorted(lr_cv_str.classes_), ['bar', 'baz', 'foo'])","425","","426","    # The predictions should be in original labels","427","    assert_equal(sorted(np.unique(lr_str.predict(X_ref))),","428","                 ['bar', 'baz', 'foo'])","429","    assert_equal(sorted(np.unique(lr_cv_str.predict(X_ref))),","430","                 ['bar', 'baz', 'foo'])","431","","432","    # Make sure class weights can be given with string labels","433","    lr_cv_str = LogisticRegression(","434","        solver='lbfgs', class_weight={'bar': 1, 'baz': 2, 'foo': 0},","435","        multi_class='multinomial').fit(X_ref, y_str)","436","    assert_equal(sorted(np.unique(lr_cv_str.predict(X_ref))), ['bar', 'baz'])","437","","438",""],"delete":[]}],"sklearn\/linear_model\/logistic.py":[{"add":["1558","        check_classification_targets(y)","1559","","1560","        class_weight = self.class_weight","1561","        if class_weight and not(isinstance(class_weight, dict) or","1562","                                class_weight in ['balanced', 'auto']):","1563","            # 'auto' is deprecated and will be removed in 0.19","1564","            raise ValueError(\"class_weight provided should be a \"","1565","                             \"dict or 'balanced'\")","1566","","1567","        # Encode for string labels","1568","        label_encoder = LabelEncoder().fit(y)","1569","        y = label_encoder.transform(y)","1570","        if isinstance(class_weight, dict):","1571","            class_weight = dict((label_encoder.transform([cls])[0], v)","1572","                                for cls, v in class_weight.items())","1573","","1574","        # The original class labels","1575","        classes = self.classes_ = label_encoder.classes_","1576","        encoded_labels = label_encoder.transform(label_encoder.classes_)","1587","        # Use the label encoded classes","1588","        n_classes = len(encoded_labels)","1593","                             \" class: %r\" % classes[0])","1594","","1599","            encoded_labels = encoded_labels[1:]","1600","            classes = classes[1:]","1605","            iter_encoded_labels = iter_classes = [None]","1606","        else:","1607","            iter_encoded_labels = encoded_labels","1608","            iter_classes = classes","1611","        if class_weight in (\"auto\", \"balanced\"):","1612","            class_weight = compute_class_weight(class_weight,","1613","                                                np.arange(len(self.classes_)),","1614","                                                y)","1615","            class_weight = dict(enumerate(class_weight))","1635","            for label in iter_encoded_labels","1666","        self.coefs_paths_ = dict(zip(classes, coefs_paths))","1668","        self.scores_ = dict(zip(classes, scores))","1679","        for index, (cls, encoded_label) in enumerate(","1680","                zip(iter_classes, iter_encoded_labels)):","1681","","1683","                # The scores_ \/ coefs_paths_ dict have unencoded class","1684","                # labels as their keys","1685","                scores = self.scores_[cls]","1686","                coefs_paths = self.coefs_paths_[cls]","1699","                # Note that y is label encoded and hence pos_class must be","1700","                # the encoded label \/ None (for 'multinomial')","1702","                    X, y, pos_class=encoded_label, Cs=[C_], solver=self.solver,"],"delete":["0","","30","from ..exceptions import DataConversionWarning","927","    # To deal with object dtypes, we need to convert into an array of floats.","928","    y_test = check_array(y_test, dtype=np.float64, ensure_2d=False)","929","","1569","        check_classification_targets(y)","1570","","1571","        if y.ndim == 2 and y.shape[1] == 1:","1572","            warnings.warn(","1573","                \"A column-vector y was passed when a 1d array was\"","1574","                \" expected. Please change the shape of y to \"","1575","                \"(n_samples, ), for example using ravel().\",","1576","                DataConversionWarning)","1577","            y = np.ravel(y)","1578","","1579","        check_consistent_length(X, y)","1580","","1585","        self._enc = LabelEncoder()","1586","        self._enc.fit(y)","1587","","1588","        labels = self.classes_ = np.unique(y)","1589","        n_classes = len(labels)","1594","                             \" class: %r\" % self.classes_[0])","1599","            labels = labels[1:]","1603","        iter_labels = labels","1605","            iter_labels = [None]","1606","","1607","        if self.class_weight and not(isinstance(self.class_weight, dict) or","1608","                                     self.class_weight in","1609","                                     ['balanced', 'auto']):","1610","            # 'auto' is deprecated and will be removed in 0.19","1611","            raise ValueError(\"class_weight provided should be a \"","1612","                             \"dict or 'balanced'\")","1615","        if self.class_weight in (\"auto\", \"balanced\"):","1616","            classes = np.unique(y)","1617","            class_weight = compute_class_weight(self.class_weight, classes, y)","1618","            class_weight = dict(zip(classes, class_weight))","1619","        else:","1620","            class_weight = self.class_weight","1640","            for label in iter_labels","1671","        self.coefs_paths_ = dict(zip(labels, coefs_paths))","1673","        self.scores_ = dict(zip(labels, scores))","1684","        for index, label in enumerate(iter_labels):","1686","                scores = self.scores_[label]","1687","                coefs_paths = self.coefs_paths_[label]","1701","                    X, y, pos_class=label, Cs=[C_], solver=self.solver,"]}],"doc\/whats_new.rst":[{"add":["99","   - :class:`sklearn.linear_model.LogisticRegressionCV` now correctly handles","100","     string labels. :issue:`5874` by `Raghav RV`_.","101","","102",""],"delete":[]}]}},"c303ed8ef27c278633d1fa4d869b51dfa2418ca6":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/mixture\/tests\/test_gaussian_mixture.py":"MODIFY","sklearn\/mixture\/base.py":"MODIFY","sklearn\/mixture\/tests\/test_bayesian_mixture.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["707","- Added function :func:`fit_predict` to :class:`mixture.GaussianMixture` and","708","  :class:`mixture.GaussianMixture`, which is essentially equivalent to calling","709","  :func:`fit` and :func:`predict`. :issue:`10336` by","710","  :user:`Shu Haoran <haoranShu>` and :user:`Andrew Peng <Andrew-peng>`.","711",""],"delete":[]}],"sklearn\/mixture\/tests\/test_gaussian_mixture.py":[{"add":["5","import copy","572","def test_gaussian_mixture_fit_predict():","573","    rng = np.random.RandomState(0)","574","    rand_data = RandomData(rng)","575","    for covar_type in COVARIANCE_TYPE:","576","        X = rand_data.X[covar_type]","577","        Y = rand_data.Y","578","        g = GaussianMixture(n_components=rand_data.n_components,","579","                            random_state=rng, weights_init=rand_data.weights,","580","                            means_init=rand_data.means,","581","                            precisions_init=rand_data.precisions[covar_type],","582","                            covariance_type=covar_type)","583","","584","        # check if fit_predict(X) is equivalent to fit(X).predict(X)","585","        f = copy.deepcopy(g)","586","        Y_pred1 = f.fit(X).predict(X)","587","        Y_pred2 = g.fit_predict(X)","588","        assert_array_equal(Y_pred1, Y_pred2)","589","        assert_greater(adjusted_rand_score(Y, Y_pred2), .95)","590","","591",""],"delete":[]}],"sklearn\/mixture\/base.py":[{"add":["174","        The method fits the model `n_init` times and set the parameters with","190","        self.fit_predict(X, y)","191","        return self","192","","193","    def fit_predict(self, X, y=None):","194","        \"\"\"Estimate model parameters using X and predict the labels for X.","195","","196","        The method fits the model n_init times and sets the parameters with","197","        which the model has the largest likelihood or lower bound. Within each","198","        trial, the method iterates between E-step and M-step for `max_iter`","199","        times until the change of likelihood or lower bound is less than","200","        `tol`, otherwise, a `ConvergenceWarning` is raised. After fitting, it","201","        predicts the most probable label for the input data points.","202","","203","        .. versionadded:: 0.20","204","","205","        Parameters","206","        ----------","207","        X : array-like, shape (n_samples, n_features)","208","            List of n_features-dimensional data points. Each row","209","            corresponds to a single data point.","210","","211","        Returns","212","        -------","213","        labels : array, shape (n_samples,)","214","            Component labels.","215","        \"\"\"","268","        return log_resp.argmax(axis=1)"],"delete":["174","        The method fit the model `n_init` times and set the parameters with","242","        return self"]}],"sklearn\/mixture\/tests\/test_bayesian_mixture.py":[{"add":["3","import copy","10","from sklearn.utils.testing import assert_array_equal","11","","12","from sklearn.metrics.cluster import adjusted_rand_score","20","from sklearn.exceptions import ConvergenceWarning, NotFittedError","425","","426","","427","def test_bayesian_mixture_fit_predict():","428","    rng = np.random.RandomState(0)","429","    rand_data = RandomData(rng, scale=7)","430","    n_components = 2 * rand_data.n_components","431","","432","    for covar_type in COVARIANCE_TYPE:","433","        bgmm1 = BayesianGaussianMixture(n_components=n_components,","434","                                        max_iter=100, random_state=rng,","435","                                        tol=1e-3, reg_covar=0)","436","        bgmm1.covariance_type = covar_type","437","        bgmm2 = copy.deepcopy(bgmm1)","438","        X = rand_data.X[covar_type]","439","","440","        Y_pred1 = bgmm1.fit(X).predict(X)","441","        Y_pred2 = bgmm2.fit_predict(X)","442","        assert_array_equal(Y_pred1, Y_pred2)","443","","444","","445","def test_bayesian_mixture_predict_predict_proba():","446","    # this is the same test as test_gaussian_mixture_predict_predict_proba()","447","    rng = np.random.RandomState(0)","448","    rand_data = RandomData(rng)","449","    for prior_type in PRIOR_TYPE:","450","        for covar_type in COVARIANCE_TYPE:","451","            X = rand_data.X[covar_type]","452","            Y = rand_data.Y","453","            bgmm = BayesianGaussianMixture(","454","                n_components=rand_data.n_components,","455","                random_state=rng,","456","                weight_concentration_prior_type=prior_type,","457","                covariance_type=covar_type)","458","","459","            # Check a warning message arrive if we don't do fit","460","            assert_raise_message(NotFittedError,","461","                                 \"This BayesianGaussianMixture instance\"","462","                                 \" is not fitted yet. Call 'fit' with \"","463","                                 \"appropriate arguments before using \"","464","                                 \"this method.\", bgmm.predict, X)","465","","466","            bgmm.fit(X)","467","            Y_pred = bgmm.predict(X)","468","            Y_pred_proba = bgmm.predict_proba(X).argmax(axis=1)","469","            assert_array_equal(Y_pred, Y_pred_proba)","470","            assert_greater_equal(adjusted_rand_score(Y, Y_pred), .95)"],"delete":["16","from sklearn.exceptions import ConvergenceWarning"]}]}},"bb1299b534887964cde73b38d28aa7bdeec778a4":{"changes":{"sklearn\/linear_model\/tests\/test_least_angle.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/linear_model\/randomized_l1.py":"MODIFY","sklearn\/linear_model\/tests\/test_randomized_l1.py":"MODIFY","sklearn\/linear_model\/least_angle.py":"MODIFY"},"diff":{"sklearn\/linear_model\/tests\/test_least_angle.py":[{"add":["174","def test_lars_precompute():","175","    # Check for different values of precompute","176","    X, y = diabetes.data, diabetes.target","177","    G = np.dot(X.T, X)","178","    for classifier in [linear_model.Lars, linear_model.LarsCV,","179","                       linear_model.LassoLarsIC]:","180","        clf = classifier(precompute=G)","181","        output_1 = ignore_warnings(clf.fit)(X, y).coef_","182","        for precompute in [True, False, 'auto', None]:","183","            clf = classifier(precompute=precompute)","184","            output_2 = clf.fit(X, y).coef_","185","            assert_array_almost_equal(output_1, output_2, decimal=8)","186","","187",""],"delete":[]}],"doc\/whats_new.rst":[{"add":["380","   - Fixed a bug in :class:`linear_model.RandomizedLasso`,","381","     :class:`linear_model.Lars`, :class:`linear_model.LarsLasso`,","382","     :class:`linear_model.LarsCV` and :class:`linear_model.LarsLassoCV`,","383","     where the parameter ``precompute`` were not used consistently accross","384","     classes, and some values proposed in the docstring could raise errors.","385","     :issue:`5359` by `Tom Dupre la Tour`_.","386",""],"delete":[]}],"sklearn\/linear_model\/randomized_l1.py":[{"add":["159","","233","    precompute : True | False | 'auto' | array-like","234","        Whether to use a precomputed Gram matrix to speed up calculations.","235","        If set to 'auto' let us decide.","236","        The Gram matrix can also be passed as argument, but it will be used","237","        only for the selection of parameter alpha, if alpha is 'aic' or 'bic'.","346","","347","        precompute = self.precompute","348","        # A precomputed Gram array is useless, since _randomized_lasso","349","        # change X a each iteration","350","        if hasattr(precompute, '__array__'):","351","            precompute = 'auto'","352","        assert precompute in (True, False, None, 'auto')","355","                                       precompute=precompute)"],"delete":["232","    precompute : True | False | 'auto'","233","        Whether to use a precomputed Gram matrix to speed up","234","        calculations. If set to 'auto' let us decide. The Gram","235","        matrix can also be passed as argument.","336","        assert self.precompute in (True, False, None, 'auto')","347","                                       precompute=self.precompute)"]}],"sklearn\/linear_model\/tests\/test_randomized_l1.py":[{"add":["61","    n_resampling = 20","65","                          scaling=scaling, n_resampling=n_resampling,","72","                          scaling=scaling, n_resampling=n_resampling,","96","                          scaling=scaling, n_resampling=100)","107","def test_randomized_lasso_precompute():","108","    # Check randomized lasso for different values of precompute","109","    n_resampling = 20","110","    alpha = 1","111","    random_state = 42","112","","113","    G = np.dot(X.T, X)","114","","115","    clf = RandomizedLasso(alpha=alpha, random_state=random_state,","116","                          precompute=G, n_resampling=n_resampling)","117","    feature_scores_1 = clf.fit(X, y).scores_","118","","119","    for precompute in [True, False, None, 'auto']:","120","        clf = RandomizedLasso(alpha=alpha, random_state=random_state,","121","                              precompute=precompute, n_resampling=n_resampling)","122","        feature_scores_2 = clf.fit(X, y).scores_","123","        assert_array_equal(feature_scores_1, feature_scores_2)","124","","125",""],"delete":["64","                          scaling=scaling,","71","                          scaling=scaling,","95","                          scaling=scaling)"]}],"sklearn\/linear_model\/least_angle.py":[{"add":["172","    if Gram is None or Gram is False:","173","        Gram = None","179","","180","    elif isinstance(Gram, string_types) and Gram == 'auto' or Gram is True:","181","        if Gram is True or X.shape[0] > X.shape[1]:","183","        else:","184","            Gram = None","603","    def _get_gram(self, precompute, X, y):","604","        if (not hasattr(precompute, '__array__')) and (","605","                (precompute is True) or","606","                (precompute == 'auto' and X.shape[0] > X.shape[1]) or","607","                (precompute == 'auto' and y.shape[1] > 1)):","608","            precompute = np.dot(X.T, X)","609","","610","        return precompute","626","        Gram = self._get_gram(self.precompute, X, y)","996","    precompute : True | False | 'auto'","998","        calculations. If set to ``'auto'`` let us decide. The Gram matrix","999","        cannot be passed as argument since we will use only subsets of X.","1104","        # As we use cross-validation, the Gram matrix is not precomputed here","1105","        Gram = self.precompute","1106","        if hasattr(Gram, '__array__'):","1107","            warnings.warn(\"Parameter 'precompute' cannot be an array in \"","1108","                          \"%s. Automatically switch to 'auto' instead.\"","1109","                          % self.__class__.__name__)","1110","            Gram = 'auto'","1214","    precompute : True | False | 'auto'","1216","        calculations. If set to ``'auto'`` let us decide. The Gram matrix","1217","        cannot be passed as argument since we will use only subsets of X.","1473","        Gram = self.precompute"],"delete":["172","    if Gram is None:","178","    elif isinstance(Gram, string_types) and Gram == 'auto':","179","        Gram = None","180","        if X.shape[0] > X.shape[1]:","600","    def _get_gram(self):","601","        # precompute if n_samples > n_features","602","        precompute = self.precompute","603","        if hasattr(precompute, '__array__'):","604","            Gram = precompute","605","        elif precompute == 'auto':","606","            Gram = 'auto'","607","        else:","608","            Gram = None","609","        return Gram","625","        precompute = self.precompute","626","        if not hasattr(precompute, '__array__') and (","627","                precompute is True or","628","                (precompute == 'auto' and X.shape[0] > X.shape[1]) or","629","                (precompute == 'auto' and y.shape[1] > 1)):","630","            Gram = np.dot(X.T, X)","631","        else:","632","            Gram = self._get_gram()","1002","    precompute : True | False | 'auto' | array-like","1004","        calculations. If set to ``'auto'`` let us decide. The Gram","1005","        matrix can also be passed as argument.","1110","        Gram = 'auto' if self.precompute else None","1214","    precompute : True | False | 'auto' | array-like","1216","        calculations. If set to ``'auto'`` let us decide. The Gram","1217","        matrix can also be passed as argument.","1473","        Gram = self._get_gram()"]}]}},"0eda10a598b3ae988f7d8c116089045474248d62":{"changes":{"doc\/modules\/linear_model.rst":"MODIFY","doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/linear_model\/tests\/test_bayes.py":"MODIFY","sklearn\/linear_model\/bayes.py":"MODIFY"},"diff":{"doc\/modules\/linear_model.rst":[{"add":["579","where :math:`\\alpha` is again treated as a random variable that is to be","580","estimated from the data.","616","conjugate prior for the precision of the Gaussian. The resulting model is","617","called *Bayesian Ridge Regression*, and is similar to the classical","618",":class:`Ridge`.","620","The parameters :math:`w`, :math:`\\alpha` and :math:`\\lambda` are estimated","621","jointly during the fit of the model, the regularization parameters","622",":math:`\\alpha` and :math:`\\lambda` being estimated by maximizing the","623","*log marginal likelihood*. The scikit-learn implementation","624","is based on the algorithm described in Appendix A of (Tipping, 2001)","625","where the update of the parameters :math:`\\alpha` and :math:`\\lambda` is done","626","as suggested in (MacKay, 1992).","628","The remaining hyperparameters are the parameters :math:`\\alpha_1`,","629",":math:`\\alpha_2`, :math:`\\lambda_1` and :math:`\\lambda_2` of the gamma priors","630","over :math:`\\alpha` and :math:`\\lambda`. These are usually chosen to be","631","*non-informative*. By default :math:`\\alpha_1 = \\alpha_2 =  \\lambda_1 = \\lambda_2 = 10^{-6}`.","670",".. topic:: References:","672","    * Section 3.3 in Christopher M. Bishop: Pattern Recognition and Machine Learning, 2006","674","    * David J. C. MacKay, `Bayesian Interpolation <http:\/\/citeseerx.ist.psu.edu\/viewdoc\/download?doi=10.1.1.27.9072&rep=rep1&type=pdf>`_, 1992.","675","","676","    * Michael E. Tipping, `Sparse Bayesian Learning and the Relevance Vector Machine <http:\/\/www.jmlr.org\/papers\/volume1\/tipping01a\/tipping01a.pdf>`_, 2001."],"delete":["579","Alpha is again treated as a random variable that is to be estimated from the","580","data.","616","conjugate prior for the precision of the Gaussian.","618","The resulting model is called *Bayesian Ridge Regression*, and is similar to the","619","classical :class:`Ridge`.  The parameters :math:`w`, :math:`\\alpha` and","620",":math:`\\lambda` are estimated jointly during the fit of the model.  The","621","remaining hyperparameters are the parameters of the gamma priors over","622",":math:`\\alpha` and :math:`\\lambda`.  These are usually chosen to be","623","*non-informative*.  The parameters are estimated by maximizing the *marginal","624","log likelihood*.","626","By default :math:`\\alpha_1 = \\alpha_2 =  \\lambda_1 = \\lambda_2 = 10^{-6}`.","665",".. topic:: References","667","  * More details can be found in the article `Bayesian Interpolation","668","    <http:\/\/citeseerx.ist.psu.edu\/viewdoc\/download?doi=10.1.1.27.9072&rep=rep1&type=pdf>`_","669","    by MacKay, David J. C."]}],"doc\/whats_new\/v0.21.rst":[{"add":["19","- :class:`linear_model.BayesianRidge` |Fix|","236",":mod:`sklearn.linear_model`","237","...........................","238","","239","- |Fix| Fixed the posterior mean, posterior covariance and returned","240","  regularization parameters in :class:`linear_model.BayesianRidge`. The","241","  posterior mean and the posterior covariance were not the ones computed","242","  with the last update of the regularization parameters and the returned","243","  regularization parameters were not the final ones. Also fixed the formula of","244","  the log marginal likelihood used to compute the score when","245","  `compute_score=True`. :issue:`12174` by","246","  :user:`Albert Thomas <albertcthomas>`.","247",""],"delete":["19","..","20","    please add class and reason here (see version 0.20 what's new)","21","","29",""]}],"sklearn\/linear_model\/tests\/test_bayes.py":[{"add":["5","from math import log","7","import numpy as np","8","from scipy.linalg import pinvh","9","","14","from sklearn.utils.testing import assert_raise_message","19","from sklearn.utils.extmath import fast_logdet","20","","21","diabetes = datasets.load_diabetes()","24","def test_n_iter():","25","    \"\"\"Check value of n_iter.\"\"\"","26","    X = np.array([[1], [2], [6], [8], [10]])","27","    y = np.array([1, 2, 6, 8, 10])","28","    clf = BayesianRidge(n_iter=0)","29","    msg = \"n_iter should be greater than or equal to 1.\"","30","    assert_raise_message(ValueError, msg, clf.fit, X, y)","31","","32","","33","def test_bayesian_ridge_scores():","34","    \"\"\"Check scores attribute shape\"\"\"","40","    assert clf.scores_.shape == (clf.n_iter_ + 1,)","41","","42","","43","def test_bayesian_ridge_score_values():","44","    \"\"\"Check value of score on toy example.","45","","46","    Compute log marginal likelihood with equation (36) in Sparse Bayesian","47","    Learning and the Relevance Vector Machine (Tipping, 2001):","48","","49","    - 0.5 * (log |Id\/alpha + X.X^T\/lambda| +","50","             y^T.(Id\/alpha + X.X^T\/lambda).y + n * log(2 * pi))","51","    + lambda_1 * log(lambda) - lambda_2 * lambda","52","    + alpha_1 * log(alpha) - alpha_2 * alpha","53","","54","    and check equality with the score computed during training.","55","    \"\"\"","56","","57","    X, y = diabetes.data, diabetes.target","58","    n_samples = X.shape[0]","59","    # check with initial values of alpha and lambda (see code for the values)","60","    eps = np.finfo(np.float64).eps","61","    alpha_ = 1. \/ (np.var(y) + eps)","62","    lambda_ = 1.","63","","64","    # value of the parameters of the Gamma hyperpriors","65","    alpha_1 = 0.1","66","    alpha_2 = 0.1","67","    lambda_1 = 0.1","68","    lambda_2 = 0.1","69","","70","    # compute score using formula of docstring","71","    score = lambda_1 * log(lambda_) - lambda_2 * lambda_","72","    score += alpha_1 * log(alpha_) - alpha_2 * alpha_","73","    M = 1. \/ alpha_ * np.eye(n_samples) + 1. \/ lambda_ * np.dot(X, X.T)","74","    M_inv = pinvh(M)","75","    score += - 0.5 * (fast_logdet(M) + np.dot(y.T, np.dot(M_inv, y)) +","76","                      n_samples * log(2 * np.pi))","77","","78","    # compute score with BayesianRidge","79","    clf = BayesianRidge(alpha_1=alpha_1, alpha_2=alpha_2,","80","                        lambda_1=lambda_1, lambda_2=lambda_2,","81","                        n_iter=1, fit_intercept=False, compute_score=True)","83","","84","    assert_almost_equal(clf.scores_[0], score, decimal=9)"],"delete":["5","import numpy as np","7","from sklearn.utils.testing import assert_array_equal","12","from sklearn.utils.testing import SkipTest","19","def test_bayesian_on_diabetes():","20","    # Test BayesianRidge on diabetes","21","    raise SkipTest(\"test_bayesian_on_diabetes is broken\")","22","    diabetes = datasets.load_diabetes()","26","","27","    # Test with more samples than features","29","    # Test that scores are increasing at each iteration","30","    assert_array_equal(np.diff(clf.scores_) > 0, True)","32","    # Test with more features than samples","33","    X = X[:5, :]","34","    y = y[:5]","36","    # Test that scores are increasing at each iteration","37","    assert_array_equal(np.diff(clf.scores_) > 0, True)"]}],"sklearn\/linear_model\/bayes.py":[{"add":["23","    \"\"\"Bayesian ridge regression.","25","    Fit a Bayesian ridge model. See the Notes section for details on this","26","    implementation and the optimization of the regularization parameters","34","        Maximum number of iterations.  Default is 300. Should be greater than","35","        or equal to 1.","59","        If True, compute the log marginal likelihood at each iteration of the","60","        optimization. Default is False.","63","        Whether to calculate the intercept for this model. If set","85","    coef_ : array, shape = (n_features,)","86","        Coefficients of the regression model (mean of distribution).","89","       Estimated precision of the noise.","92","       Estimated precision of the weights.","95","        Estimated variance-covariance matrix of the weights.","97","    scores_ : array, shape = (n_iter_ + 1,)","98","        If computed_score is True, value of the log marginal likelihood (to be","99","        maximized) at each iteration of the optimization. The array starts","100","        with the value of the log marginal likelihood obtained for the initial","101","        values of alpha and lambda and ends with the value obtained for the","102","        estimated alpha and lambda.","103","","104","    n_iter_ : int","105","        The actual number of iterations to reach the stopping criterion.","121","    There exist several strategies to perform Bayesian ridge regression. This","122","    implementation is based on the algorithm described in Appendix A of","123","    (Tipping, 2001) where updates of the regularization parameters are done as","124","    suggested in (MacKay, 1992). Note that according to A New","125","    View of Automatic Relevance Determination (Wipf and Nagarajan, 2008) these","126","    update rules do not guarantee that the marginal likelihood is increasing","127","    between two consecutive iterations of the optimization.","134","    M. E. Tipping, Sparse Bayesian Learning and the Relevance Vector Machine,","135","    Journal of Machine Learning Research, Vol. 1, 2001.","174","","175","        if self.n_iter < 1:","176","            raise ValueError('n_iter should be greater than or equal to 1.'","177","                             ' Got {!r}.'.format(self.n_iter))","178","","215","            # update posterior mean coef_ based on alpha_ and lambda_ and","216","            # compute corresponding rmse","217","            coef_, rmse_ = self._update_coef_(X, y, n_samples, n_features,","218","                                              XT_y, U, Vh, eigen_vals_,","219","                                              alpha_, lambda_)","220","            if self.compute_score:","221","                # compute the log marginal likelihood","222","                s = self._log_marginal_likelihood(n_samples, n_features,","223","                                                  eigen_vals_,","224","                                                  alpha_, lambda_,","225","                                                  coef_, rmse_)","226","                self.scores_.append(s)","228","            # Update alpha and lambda according to (MacKay, 1992)","229","            gamma_ = np.sum((alpha_ * eigen_vals_) \/","230","                            (lambda_ + alpha_ * eigen_vals_))","243","        self.n_iter_ = iter_ + 1","244","","245","        # return regularization parameters and corresponding posterior mean,","246","        # log marginal likelihood and posterior covariance","247","        self.alpha_ = alpha_","248","        self.lambda_ = lambda_","249","        self.coef_, rmse_ = self._update_coef_(X, y, n_samples, n_features,","250","                                               XT_y, U, Vh, eigen_vals_,","251","                                               alpha_, lambda_)","252","        if self.compute_score:","253","            # compute the log marginal likelihood","254","            s = self._log_marginal_likelihood(n_samples, n_features,","255","                                              eigen_vals_,","256","                                              alpha_, lambda_,","257","                                              coef_, rmse_)","258","            self.scores_.append(s)","259","            self.scores_ = np.array(self.scores_)","260","","261","        # posterior covariance is given by 1\/alpha_ * scaled_sigma_","262","        scaled_sigma_ = np.dot(Vh.T,","263","                               Vh \/ (eigen_vals_ +","264","                                     lambda_ \/ alpha_)[:, np.newaxis])","265","        self.sigma_ = (1. \/ alpha_) * scaled_sigma_","268","","303","    def _update_coef_(self, X, y, n_samples, n_features, XT_y, U, Vh,","304","                      eigen_vals_, alpha_, lambda_):","305","        \"\"\"Update posterior mean and compute corresponding rmse.","306","","307","        Posterior mean is given by coef_ = scaled_sigma_ * X.T * y where","308","        scaled_sigma_ = (lambda_\/alpha_ * np.eye(n_features)","309","                         + np.dot(X.T, X))^-1","310","        \"\"\"","311","","312","        if n_samples > n_features:","313","            coef_ = np.dot(Vh.T,","314","                           Vh \/ (eigen_vals_ +","315","                                 lambda_ \/ alpha_)[:, np.newaxis])","316","            coef_ = np.dot(coef_, XT_y)","317","        else:","318","            coef_ = np.dot(X.T, np.dot(","319","                U \/ (eigen_vals_ + lambda_ \/ alpha_)[None, :], U.T))","320","            coef_ = np.dot(coef_, y)","321","","322","        rmse_ = np.sum((y - np.dot(X, coef_)) ** 2)","323","","324","        return coef_, rmse_","325","","326","    def _log_marginal_likelihood(self, n_samples, n_features, eigen_vals,","327","                                 alpha_, lambda_, coef, rmse):","328","        \"\"\"Log marginal likelihood.\"\"\"","329","        alpha_1 = self.alpha_1","330","        alpha_2 = self.alpha_2","331","        lambda_1 = self.lambda_1","332","        lambda_2 = self.lambda_2","333","","334","        # compute the log of the determinant of the posterior covariance.","335","        # posterior covariance is given by","336","        # sigma = (lambda_ * np.eye(n_features) + alpha_ * np.dot(X.T, X))^-1","337","        if n_samples > n_features:","338","            logdet_sigma = - np.sum(np.log(lambda_ + alpha_ * eigen_vals))","339","        else:","340","            logdet_sigma = np.full(n_features, lambda_,","341","                                   dtype=np.array(lambda_).dtype)","342","            logdet_sigma[:n_samples] += alpha_ * eigen_vals","343","            logdet_sigma = - np.sum(np.log(logdet_sigma))","344","","345","        score = lambda_1 * log(lambda_) - lambda_2 * lambda_","346","        score += alpha_1 * log(alpha_) - alpha_2 * alpha_","347","        score += 0.5 * (n_features * log(lambda_) +","348","                        n_samples * log(alpha_) -","349","                        alpha_ * rmse -","350","                        lambda_ * np.sum(coef ** 2) +","351","                        logdet_sigma -","352","                        n_samples * log(2 * np.pi))","353","","354","        return score","355",""],"delete":["23","    \"\"\"Bayesian ridge regression","25","    Fit a Bayesian ridge model and optimize the regularization parameters","33","        Maximum number of iterations.  Default is 300.","57","        If True, compute the objective function at each step of the model.","58","        Default is False","61","        whether to calculate the intercept for this model. If set","83","    coef_ : array, shape = (n_features)","84","        Coefficients of the regression model (mean of distribution)","87","       estimated precision of the noise.","90","       estimated precision of the weights.","93","        estimated variance-covariance matrix of the weights","95","    scores_ : float","96","        if computed, value of the objective function (to be maximized)","112","    For an example, see :ref:`examples\/linear_model\/plot_bayesian_ridge.py","113","    <sphx_glr_auto_examples_linear_model_plot_bayesian_ridge.py>`.","120","    R. Salakhutdinov, Lecture notes on Statistical Machine Learning,","121","    http:\/\/www.utstat.toronto.edu\/~rsalakhu\/sta4273\/notes\/Lecture2.pdf#page=15","122","    Their beta is our ``self.alpha_``","123","    Their alpha is our ``self.lambda_``","198","            # Compute mu and sigma","199","            # sigma_ = lambda_ \/ alpha_ * np.eye(n_features) + np.dot(X.T, X)","200","            # coef_ = sigma_^-1 * XT * y","201","            if n_samples > n_features:","202","                coef_ = np.dot(Vh.T,","203","                               Vh \/ (eigen_vals_ +","204","                                     lambda_ \/ alpha_)[:, np.newaxis])","205","                coef_ = np.dot(coef_, XT_y)","206","                if self.compute_score:","207","                    logdet_sigma_ = - np.sum(","208","                        np.log(lambda_ + alpha_ * eigen_vals_))","209","            else:","210","                coef_ = np.dot(X.T, np.dot(","211","                    U \/ (eigen_vals_ + lambda_ \/ alpha_)[None, :], U.T))","212","                coef_ = np.dot(coef_, y)","213","                if self.compute_score:","214","                    logdet_sigma_ = np.full(n_features, lambda_,","215","                                            dtype=np.array(lambda_).dtype)","216","                    logdet_sigma_[:n_samples] += alpha_ * eigen_vals_","217","                    logdet_sigma_ = - np.sum(np.log(logdet_sigma_))","219","            # Preserve the alpha and lambda values that were used to","220","            # calculate the final coefficients","221","            self.alpha_ = alpha_","222","            self.lambda_ = lambda_","223","","224","            # Update alpha and lambda","225","            rmse_ = np.sum((y - np.dot(X, coef_)) ** 2)","226","            gamma_ = (np.sum((alpha_ * eigen_vals_) \/","227","                      (lambda_ + alpha_ * eigen_vals_)))","233","            # Compute the objective function","234","            if self.compute_score:","235","                s = lambda_1 * log(lambda_) - lambda_2 * lambda_","236","                s += alpha_1 * log(alpha_) - alpha_2 * alpha_","237","                s += 0.5 * (n_features * log(lambda_) +","238","                            n_samples * log(alpha_) -","239","                            alpha_ * rmse_ -","240","                            (lambda_ * np.sum(coef_ ** 2)) -","241","                            logdet_sigma_ -","242","                            n_samples * log(2 * np.pi))","243","                self.scores_.append(s)","244","","252","        self.coef_ = coef_","253","        sigma_ = np.dot(Vh.T,","254","                        Vh \/ (eigen_vals_ + lambda_ \/ alpha_)[:, np.newaxis])","255","        self.sigma_ = (1. \/ alpha_) * sigma_"]}]}},"3b35104c93cb53f67fb5f52ae2fece76ef7144da":{"changes":{"sklearn\/utils\/seq_dataset.pxd.tp":"MODIFY","sklearn\/svm\/libsvm.pyx":"MODIFY","sklearn\/manifold\/_barnes_hut_tsne.pyx":"MODIFY","sklearn\/utils\/seq_dataset.pyx.tp":"MODIFY"},"diff":{"sklearn\/utils\/seq_dataset.pxd.tp":[{"add":["0","# cython: language_level=3"],"delete":["23",""]}],"sklearn\/svm\/libsvm.pyx":[{"add":[],"delete":["36","from . cimport libsvm"]}],"sklearn\/manifold\/_barnes_hut_tsne.pyx":[{"add":["18","from sklearn.neighbors.quad_tree cimport _QuadTree","51","                            _QuadTree qt,","165","                                    _QuadTree qt,","263","    cdef _QuadTree qt = _QuadTree(pos_output.shape[1], verbose)"],"delete":["18","from sklearn.neighbors import quad_tree","19","from sklearn.neighbors cimport quad_tree","52","                            quad_tree._QuadTree qt,","166","                                    quad_tree._QuadTree qt,","264","    cdef quad_tree._QuadTree qt = quad_tree._QuadTree(pos_output.shape[1],","265","                                                      verbose)"]}],"sklearn\/utils\/seq_dataset.pyx.tp":[{"add":["0","# cython: language_level=3","1","# cython: cdivision=True","2","# cython: boundscheck=False","3","# cython: wraparound=False"],"delete":["28","","38","# cython: cdivision=True","39","# cython: boundscheck=False","40","# cython: wraparound=False","41",""]}]}},"8442eead9ed3fef8ce03db7f2cccec3078dd685f":{"changes":{"doc\/modules\/multiclass.rst":"MODIFY"},"diff":{"doc\/modules\/multiclass.rst":[{"add":["38","    several joint classification tasks. This is both a generalization","40","    problems is restricted to binary classification, ","41","    as well as a generalization of the multi-class classification task. ","42","    *The output format is a 2d numpy array or sparse matrix.*","45","    For instance, a sample could be assigned \"pear\" for an output variable that","46","    takes possible values in a finite set of species such as \"pear\", \"apple\"; ","47","    and \"blue\" or \"green\" for a second output variable that takes possible values","48","    in a finite set of colors such as \"green\", \"red\", \"blue\", \"yellow\"...","51","    multiclass or multi-task classification tasks,","52","    support the multi-label classification task as a special case.","65","if you're using one of these, unless you want custom multiclass behavior:","121","interpretability. Since each class is represented by one and only one classifier, ","122","it is possible to gain knowledge about the class by inspecting its","214","one-vs-one. With these strategies, each class is represented in a Euclidean"],"delete":["38","    several joint classification tasks. This is a generalization","40","    problem is restricted to binary classification, and of the multi-class","41","    classification task. *The output format is a 2d numpy array or sparse","42","    matrix.*","45","    For instance a sample could be assigned \"pear\" for an output variable that","46","    takes possible values in a finite set of species such as \"pear\", \"apple\",","47","    \"orange\" and \"green\" for a second output variable that takes possible values","48","    in a finite set of colors such as \"green\", \"red\", \"orange\", \"yellow\"...","51","    multiclass or multi-task classification task","52","    supports the multi-label classification task as a special case.","65","if you're using one of these unless you want custom multiclass behavior:","121","interpretability. Since each class is represented by one and one classifier","122","only, it is possible to gain knowledge about the class by inspecting its","214","one-vs-one. With these strategies, each class is represented in a euclidean"]}]}},"88b49e5caf01ee8e6d803f8daca2bf1666219b0b":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/neighbors\/tests\/test_neighbors.py":"MODIFY","sklearn\/neighbors\/base.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["25","- |Fix| force the parallelism backend to :code:`threading` for","26","  :class:`neighbors.KDTree` and :class:`neighbors.BallTree` in Python 2.7 to","27","  avoid pickling errors caused by the serialization of their methods.","28","  :issue:`12171` by :user:`Thomas Moreau <tomMoral>`","29",""],"delete":[]}],"sklearn\/neighbors\/tests\/test_neighbors.py":[{"add":["29","from sklearn.externals.joblib import parallel_backend","30","","1320","@pytest.mark.parametrize('backend', ['loky', 'multiprocessing', 'threading'])","1321","@pytest.mark.parametrize('algorithm', ALGORITHMS)","1322","def test_knn_forcing_backend(backend, algorithm):","1323","    # Non-regression test which ensure the knn methods are properly working","1324","    # even when forcing the global joblib backend.","1325","    with parallel_backend(backend):","1326","        X, y = datasets.make_classification(n_samples=30, n_features=5,","1327","                                            n_redundant=0, random_state=0)","1328","        X_train, X_test, y_train, y_test = train_test_split(X, y)","1329","","1330","        clf = neighbors.KNeighborsClassifier(n_neighbors=3,","1331","                                             algorithm=algorithm,","1332","                                             n_jobs=3)","1333","        clf.fit(X_train, y_train)","1334","        clf.predict(X_test)","1335","        clf.kneighbors(X_test)","1336","        clf.kneighbors_graph(X_test, mode='distance').toarray()","1337","","1338",""],"delete":[]}],"sklearn\/neighbors\/base.py":[{"add":["11","import sys","432","            if (sys.version_info < (3,) or","433","                    LooseVersion(joblib_version) < LooseVersion('0.12')):"],"delete":["431","            if LooseVersion(joblib_version) < LooseVersion('0.12'):"]}]}}}