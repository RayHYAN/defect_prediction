{"d4e082679d970e6b30dfe9526d9b18c745f88a81":{"changes":{"sklearn\/metrics\/cluster\/_supervised.py":"MODIFY","sklearn\/metrics\/cluster\/tests\/test_common.py":"MODIFY"},"diff":{"sklearn\/metrics\/cluster\/_supervised.py":[{"add":["45","        labels_true, ensure_2d=False, ensure_min_samples=0, dtype=None,","48","        labels_pred, ensure_2d=False, ensure_min_samples=0, dtype=None,"],"delete":["45","        labels_true, ensure_2d=False, ensure_min_samples=0","48","        labels_pred, ensure_2d=False, ensure_min_samples=0"]}],"sklearn\/metrics\/cluster\/tests\/test_common.py":[{"add":["163","        yield [str(x) + \"-a\" for x in y.tolist()], 'list of strs'","164","        yield (np.array([str(x) + \"-a\" for x in y.tolist()], dtype=object),","165","               'array of strs')"],"delete":["163","        yield [str(x) for x in y.tolist()], 'list of strs'"]}]}},"2885a06c94f2938ba722d3dee02e2b9d772b7104":{"changes":{"sklearn\/inspection\/tests\/test_plot_partial_dependence.py":"MODIFY","sklearn\/inspection\/_partial_dependence.py":"MODIFY"},"diff":{"sklearn\/inspection\/tests\/test_plot_partial_dependence.py":[{"add":["231","    # The first call to plot_partial_dependence will create two new axes to","232","    # place in the space of the passed in axes, which results in a total of","233","    # three axes in the figure.","234","    # Currently the API does not allow for the second call to","235","    # plot_partial_dependence to use the same axes again, because it will","236","    # create two new axes in the space resulting in five axes. To get the","237","    # expected behavior one needs to pass the generated axes into the second","238","    # call:","239","    # disp1 = plot_partial_dependence(...)","240","    # disp2 = plot_partial_dependence(..., ax=disp1.axes_)"],"delete":["231","    # The first call to `plot_*` will plot the axes"]}],"sklearn\/inspection\/_partial_dependence.py":[{"add":["861","            # If ax was set off, it has most likely been set to off","863","            if not ax.axison:"],"delete":["861","            # If ax has visible==False, it has most likely been set to False","863","            if not ax.get_visible():","869","            ax.set_visible(False)"]}]}},"865069c234f0f7ea23e50c47b171b186a684e2a7":{"changes":{"sklearn\/base.py":"MODIFY","sklearn\/cluster\/_birch.py":"MODIFY"},"diff":{"sklearn\/base.py":[{"add":["56","    estimator : {list, tuple, set} of estimator objects or estimator object","57","        The estimator or group of estimators to be cloned.","627","        shape : tuple (int, int)","671","        X : {array-like, sparse matrix, dataframe} of shape \\","672","                (n_samples, n_features)","727","        X : {array-like, sparse matrix, dataframe} of shape \\","728","            (n_samples, n_features)"],"delete":["56","    estimator : estimator object, or list, tuple or set of objects","57","        The estimator or group of estimators to be cloned","627","        shape : (int, int)","671","        X : ndarray of shape (n_samples, n_features)","672","            Training set.","727","        X : ndarray of shape (n_samples, n_features)","728","            Input data."]}],"sklearn\/cluster\/_birch.py":[{"add":["114","    subclusters_ : list","115","        List of subclusters for a particular CFNode.","118","        Useful only if is_leaf is True.","124","    init_centroids_ : ndarray of shape (branching_factor + 1, n_features)","125","        Manipulate ``init_centroids_`` throughout rather than centroids_ since","128","    init_sq_norm_ : ndarray of shape (branching_factor + 1,)","131","    centroids_ : ndarray of shape (branching_factor + 1, n_features)","132","        View of ``init_centroids_``.","134","    squared_norm_ : ndarray of shape (branching_factor + 1,)","135","        View of ``init_sq_norm_``.","251","    linear_sum : ndarray of shape (n_features,), default=None","267","    centroid_ : ndarray of shape (branching_factor + 1, n_features)","275","    sq_norm_ : ndarray of shape (branching_factor + 1,)","382","    subcluster_centers_ : ndarray","385","    subcluster_labels_ : ndarray","389","    labels_ : ndarray of shape (n_samples,)","450","        X : {array-like, sparse matrix} of shape (n_samples, n_features)","525","        leaves : list of shape (n_leaves,)","541","        X : {array-like, sparse matrix} of shape (n_samples, n_features), \\","542","            default=None","580","        X : {array-like, sparse matrix} of shape (n_samples, n_features)","585","        labels : ndarray of shape(n_samples,)","606","        X : {array-like, sparse matrix} of shape (n_samples, n_features)","611","        X_trans : {array-like, sparse matrix} of shape (n_samples, n_clusters)"],"delete":["114","    subclusters_ : array-like","115","        list of subclusters for a particular CFNode.","118","        prev_leaf. Useful only if is_leaf is True.","124","    init_centroids_ : ndarray, shape (branching_factor + 1, n_features)","125","        manipulate ``init_centroids_`` throughout rather than centroids_ since","128","    init_sq_norm_ : ndarray, shape (branching_factor + 1,)","131","    centroids_ : ndarray","132","        view of ``init_centroids_``.","134","    squared_norm_ : ndarray","135","        view of ``init_sq_norm_``.","251","    linear_sum : ndarray, shape (n_features,), optional","267","    centroid_ : ndarray","275","    sq_norm_ : ndarray","382","    subcluster_centers_ : ndarray,","385","    subcluster_labels_ : ndarray,","389","    labels_ : ndarray, shape (n_samples,)","450","        X : {array-like, sparse matrix}, shape (n_samples, n_features)","525","        leaves : array-like","541","        X : {array-like, sparse matrix}, shape (n_samples, n_features), None","579","        X : {array-like, sparse matrix}, shape (n_samples, n_features)","584","        labels : ndarray, shape(n_samples)","605","        X : {array-like, sparse matrix}, shape (n_samples, n_features)","610","        X_trans : {array-like, sparse matrix}, shape (n_samples, n_clusters)"]}]}},"97958c174f8b95f7e86d6a0280847be61bfc8e04":{"changes":{"sklearn\/ensemble\/_hist_gradient_boosting\/gradient_boosting.py":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_warm_start.py":"MODIFY"},"diff":{"sklearn\/ensemble\/_hist_gradient_boosting\/gradient_boosting.py":[{"add":["272","            if self.do_early_stopping_ and self._use_validation_data:","273","                raw_predictions_val = self._raw_predict(X_binned_val)"],"delete":[]}],"sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_warm_start.py":[{"add":["95","@pytest.mark.parametrize('scoring', (None, 'loss'))","96","def test_warm_start_early_stopping(GradientBoosting, X, y, scoring):","103","        random_state=42, warm_start=True, tol=1e-3, scoring=scoring,"],"delete":["95","def test_warm_start_early_stopping(GradientBoosting, X, y):","102","        random_state=42, warm_start=True, tol=1e-3"]}]}},"556eb9246d1290ea248f2e3b3581dc53e75b8061":{"changes":{"sklearn\/inspection\/tests\/test_plot_partial_dependence.py":"MODIFY","sklearn\/inspection\/_partial_dependence.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY"},"diff":{"sklearn\/inspection\/tests\/test_plot_partial_dependence.py":[{"add":["224","@pytest.mark.parametrize(\"nrows, ncols\", [(2, 2), (3, 1)])","226","                                                    boston, nrows, ncols):","227","    grid_resolution = 5","228","    fig, axes = pyplot.subplots(nrows, ncols)","229","    axes_formats = [list(axes.ravel()), tuple(axes.ravel()), axes]","231","    msg = \"Expected ax to have 2 axes, got {}\".format(nrows * ncols)","234","                                   ['CRIM', 'ZN'],","238","    for ax_format in axes_formats:","239","        with pytest.raises(ValueError, match=msg):","240","            plot_partial_dependence(clf_boston, boston.data,","241","                                    ['CRIM', 'ZN'],","242","                                    grid_resolution=grid_resolution,","243","                                    feature_names=boston.feature_names,","244","                                    ax=ax_format)","245","","246","        # with axes object","247","        with pytest.raises(ValueError, match=msg):","248","            disp.plot(ax=ax_format)"],"delete":["225","                                                    boston):","226","    grid_resolution = 25","227","    fig, (ax1, ax2, ax3) = pyplot.subplots(1, 3)","229","    msg = r\"Expected len\\(ax\\) == len\\(features\\), got len\\(ax\\) = 3\"","230","    with pytest.raises(ValueError, match=msg):","231","        plot_partial_dependence(clf_boston, boston.data,","232","                                ['CRIM', ('CRIM', 'ZN')],","233","                                grid_resolution=grid_resolution,","234","                                feature_names=boston.feature_names,","235","                                ax=[ax1, ax2, ax3])","238","                                   ['CRIM', ('CRIM', 'ZN')],","242","    with pytest.raises(ValueError, match=msg):","243","        disp.plot(ax=[ax1, ax2, ax3])"]}],"sklearn\/inspection\/_partial_dependence.py":[{"add":["657","    # Early exit if the axes does not have the correct number of axes","658","    if ax is not None and not isinstance(ax, plt.Axes):","659","        axes = np.asarray(ax, dtype=object)","660","        if axes.size != len(features):","661","            raise ValueError(\"Expected ax to have {} axes, got {}\".format(","662","                             len(features), axes.size))","890","            ax = np.asarray(ax, dtype=object)","891","            if ax.size != n_features:","892","                raise ValueError(\"Expected ax to have {} axes, got {}\"","893","                                 .format(n_features, ax.size))"],"delete":["657","    if isinstance(ax, list):","658","        if len(ax) != len(features):","659","            raise ValueError(\"Expected len(ax) == len(features), \"","660","                             \"got len(ax) = {}\".format(len(ax)))","888","            ax = check_array(ax, dtype=object, ensure_2d=False)","895","            if ax.ndim == 1 and ax.shape[0] != n_features:","896","                raise ValueError(\"Expected len(ax) == len(features), \"","897","                                 \"got len(ax) = {}\".format(len(ax)))"]}],"doc\/whats_new\/v0.22.rst":[{"add":["30",":mod:`sklearn.inspection`","31",".........................","32","","33","- |Fix| :func:`inspection.plot_partial_dependence` and ","34","  :meth:`inspection.PartialDependenceDisplay.plot` now consistently checks","35","  the number of axes passed in. :pr:`15760` by `Thomas Fan`_.","36",""],"delete":[]}]}},"0396c886066c9a0dc2a49dc8ac97037b4e73d947":{"changes":{"doc\/whats_new\/v0.23.rst":"MODIFY","sklearn\/metrics\/tests\/test_regression.py":"MODIFY","sklearn\/metrics\/_regression.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.23.rst":[{"add":["155",":mod:`sklearn.metrics`","156","......................","157","","158","- |Fix| Fixed a bug in :func:`metrics.mean_squared_error` to not ignore","159","  argument `squared` when argument `multioutput='raw_values'`. ","160","  :pr:`16323` by :user:`Rushabh Vasani <rushabh-v>`","161",""],"delete":[]}],"sklearn\/metrics\/tests\/test_regression.py":[{"add":["58","def test_mean_squared_error_multioutput_raw_value_squared():","59","    # non-regression test for","60","    # https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/16323","61","    mse1 = mean_squared_error(","62","        [[1]], [[10]], multioutput=\"raw_values\", squared=True","63","    )","64","    mse2 = mean_squared_error(","65","        [[1]], [[10]], multioutput=\"raw_values\", squared=False","66","    )","67","    assert np.sqrt(mse1) == pytest.approx(mse2)","68","","69",""],"delete":[]}],"sklearn\/metrics\/_regression.py":[{"add":["257","            return output_errors if squared else np.sqrt(output_errors)"],"delete":["257","            return output_errors"]}]}},"ea3181879c43f7423c32007111285d1f725a578b":{"changes":{"doc\/whats_new\/v0.23.rst":"MODIFY","sklearn\/decomposition\/_pca.py":"MODIFY","sklearn\/decomposition\/tests\/test_pca.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.23.rst":[{"add":["135","- |Fix| :func:`decomposition._pca._assess_dimension` now correctly handles small","136","   eigenvalues. :pr: `4441` by :user:`Lisa Schwetlick <lschwetlick>`, and","137","   :user:`Gelavizh Ahmadi <gelavizh1>` and","138","   :user:`Marija Vlajic Wheeler <marijavlajic>`."],"delete":[]}],"sklearn\/decomposition\/_pca.py":[{"add":["29","def _assess_dimension(spectrum, rank, n_samples, n_features):","60","    spectrum_threshold = np.finfo(type(spectrum[0])).eps","61","","71","        # TODO: this line is never executed because _infer_dimension's","72","        # for loop is off by one","77","        if spectrum_threshold > v:","78","            return -np.inf","88","        if spectrum_[i] < spectrum_threshold:","89","            # TODO: this line is never executed","90","            # (off by one in _infer_dimension)","91","            # this break only happens when rank == n_features and","92","            # spectrum_[i] < spectrum_threshold, otherwise the early return","93","            # above catches this case.","94","            break","104","def _infer_dimension(spectrum, n_samples, n_features):","112","        ll[rank] = _assess_dimension(spectrum, rank, n_samples, n_features)","473","                _infer_dimension(explained_variance_, n_samples, n_features)"],"delete":["29","def _assess_dimension_(spectrum, rank, n_samples, n_features):","91","def _infer_dimension_(spectrum, n_samples, n_features):","99","        ll[rank] = _assess_dimension_(spectrum, rank, n_samples, n_features)","460","                _infer_dimension_(explained_variance_, n_samples, n_features)"]}],"sklearn\/decomposition\/tests\/test_pca.py":[{"add":["10","from sklearn.decomposition._pca import _assess_dimension","11","from sklearn.decomposition._pca import _infer_dimension","335","    ll = np.array([_assess_dimension(spect, k, n, p) for k in range(p)])","350","    assert _infer_dimension(spect, n, p) > 1","363","    assert _infer_dimension(spect, n, p) > 2","570","","571","","572","def test_infer_dim_bad_spec():","573","    # Test a spectrum that drops to near zero for PR #16224","574","    spectrum = np.array([1, 1e-30, 1e-30, 1e-30])","575","    n_samples = 10","576","    n_features = 5","577","    ret = _infer_dimension(spectrum, n_samples, n_features)","578","    assert ret == 0","579","","580","","581","def test_assess_dimension_error_rank_greater_than_features():","582","    # Test error when tested rank is greater than the number of features","583","    # for PR #16224","584","    spectrum = np.array([1, 1e-30, 1e-30, 1e-30])","585","    n_samples = 10","586","    n_features = 4","587","    rank = 5","588","    with pytest.raises(ValueError, match=\"The tested rank cannot exceed \"","589","                                         \"the rank of the dataset\"):","590","        _assess_dimension(spectrum, rank, n_samples, n_features)","591","","592","","593","def test_assess_dimension_small_eigenvalues():","594","    # Test tiny eigenvalues appropriately when using 'mle'","595","    # for  PR #16224","596","    spectrum = np.array([1, 1e-30, 1e-30, 1e-30])","597","    n_samples = 10","598","    n_features = 5","599","    rank = 3","600","    ret = _assess_dimension(spectrum, rank, n_samples, n_features)","601","    assert ret == -np.inf","602","","603","","604","def test_infer_dim_mle():","605","    # Test small eigenvalues when 'mle' with pathological 'X' dataset","606","    # for PR #16224","607","    X, _ = datasets.make_classification(n_informative=1, n_repeated=18,","608","                                        n_redundant=1, n_clusters_per_class=1,","609","                                        random_state=42)","610","    pca = PCA(n_components='mle').fit(X)","611","    assert pca.n_components_ == 0","612","","613","","614","def test_fit_mle_too_few_samples():","615","    # Tests that an error is raised when the number of samples is smaller","616","    # than the number of features during an mle fit for PR #16224","617","    X, _ = datasets.make_classification(n_samples=20, n_features=21,","618","                                        random_state=42)","619","","620","    pca = PCA(n_components='mle', svd_solver='full')","621","    with pytest.raises(ValueError, match=\"n_components='mle' is only \"","622","                                         \"supported if \"","623","                                         \"n_samples >= n_features\"):","624","        pca.fit(X)"],"delete":["10","from sklearn.decomposition._pca import _assess_dimension_","11","from sklearn.decomposition._pca import _infer_dimension_","335","    ll = np.array([_assess_dimension_(spect, k, n, p) for k in range(p)])","350","    assert _infer_dimension_(spect, n, p) > 1","363","    assert _infer_dimension_(spect, n, p) > 2"]}]}},"beb7afcef408833a31d46d74ddc029b1780f3d30":{"changes":{"sklearn\/cluster\/_dbscan.py":"MODIFY"},"diff":{"sklearn\/cluster\/_dbscan.py":[{"add":["16","from ..utils import check_array","17","from ..utils.validation import _check_sample_weight","315","            sample_weight = _check_sample_weight(sample_weight, X)"],"delete":["16","from ..utils import check_array, check_consistent_length","314","            sample_weight = np.asarray(sample_weight)","315","            check_consistent_length(X, sample_weight)"]}]}},"a68ba97309512f691feb25e6a07af7561fc56e3b":{"changes":{"sklearn\/inspection\/tests\/test_permutation_importance.py":"MODIFY","sklearn\/inspection\/_permutation_importance.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY"},"diff":{"sklearn\/inspection\/tests\/test_permutation_importance.py":[{"add":["8","from sklearn.datasets import make_classification","10","from sklearn.dummy import DummyClassifier","18","from sklearn.preprocessing import KBinsDiscretizer","22","from sklearn.utils import parallel_backend","23","from sklearn.utils._testing import _convert_container","24","","158","","159","","160","def test_permutation_importance_equivalence_sequential_parallel():","161","    # regression test to make sure that sequential and parallel calls will","162","    # output the same results.","163","    X, y = make_regression(n_samples=500, n_features=10, random_state=0)","164","    lr = LinearRegression().fit(X, y)","165","","166","    importance_sequential = permutation_importance(","167","        lr, X, y, n_repeats=5, random_state=0, n_jobs=1","168","    )","169","","170","    # First check that the problem is structured enough and that the model is","171","    # complex enough to not yield trivial, constant importances:","172","    imp_min = importance_sequential['importances'].min()","173","    imp_max = importance_sequential['importances'].max()","174","    assert imp_max - imp_min > 0.3","175","","176","    # The actually check that parallelism does not impact the results","177","    # either with shared memory (threading) or without isolated memory","178","    # via process-based parallelism using the default backend","179","    # ('loky' or 'multiprocessing') depending on the joblib version:","180","","181","    # process-based parallelism (by default):","182","    importance_processes = permutation_importance(","183","        lr, X, y, n_repeats=5, random_state=0, n_jobs=2)","184","    assert_allclose(","185","        importance_processes['importances'],","186","        importance_sequential['importances']","187","    )","188","","189","    # thread-based parallelism:","190","    with parallel_backend(\"threading\"):","191","        importance_threading = permutation_importance(","192","            lr, X, y, n_repeats=5, random_state=0, n_jobs=2","193","        )","194","    assert_allclose(","195","        importance_threading['importances'],","196","        importance_sequential['importances']","197","    )","198","","199","","200","@pytest.mark.parametrize(\"n_jobs\", [None, 1, 2])","201","def test_permutation_importance_equivalence_array_dataframe(n_jobs):","202","    # This test checks that the column shuffling logic has the same behavior","203","    # both a dataframe and a simple numpy array.","204","    pd = pytest.importorskip('pandas')","205","","206","    # regression test to make sure that sequential and parallel calls will","207","    # output the same results.","208","    X, y = make_regression(n_samples=100, n_features=5, random_state=0)","209","    X_df = pd.DataFrame(X)","210","","211","    # Add a categorical feature that is statistically linked to y:","212","    binner = KBinsDiscretizer(n_bins=3, encode=\"ordinal\")","213","    cat_column = binner.fit_transform(y.reshape(-1, 1))","214","","215","    # Concatenate the extra column to the numpy array: integers will be","216","    # cast to float values","217","    X = np.hstack([X, cat_column])","218","    assert X.dtype.kind == \"f\"","219","","220","    # Insert extra column as a non-numpy-native dtype (while keeping backward","221","    # compat for old pandas versions):","222","    if hasattr(pd, \"Categorical\"):","223","        cat_column = pd.Categorical(cat_column.ravel())","224","    else:","225","        cat_column = cat_column.ravel()","226","    new_col_idx = len(X_df.columns)","227","    X_df[new_col_idx] = cat_column","228","    assert X_df[new_col_idx].dtype == cat_column.dtype","229","","230","    # Stich an aribtrary index to the dataframe:","231","    X_df.index = np.arange(len(X_df)).astype(str)","232","","233","    rf = RandomForestRegressor(n_estimators=5, max_depth=3, random_state=0)","234","    rf.fit(X, y)","235","","236","    n_repeats = 3","237","    importance_array = permutation_importance(","238","        rf, X, y, n_repeats=n_repeats, random_state=0, n_jobs=n_jobs","239","    )","240","","241","    # First check that the problem is structured enough and that the model is","242","    # complex enough to not yield trivial, constant importances:","243","    imp_min = importance_array['importances'].min()","244","    imp_max = importance_array['importances'].max()","245","    assert imp_max - imp_min > 0.3","246","","247","    # Now check that importances computed on dataframe matche the values","248","    # of those computed on the array with the same data.","249","    importance_dataframe = permutation_importance(","250","        rf, X_df, y, n_repeats=n_repeats, random_state=0, n_jobs=n_jobs","251","    )","252","    assert_allclose(","253","        importance_array['importances'],","254","        importance_dataframe['importances']","255","    )","256","","257","","258","@pytest.mark.parametrize(\"input_type\", [\"array\", \"dataframe\"])","259","def test_permutation_importance_large_memmaped_data(input_type):","260","    # Smoke, non-regression test for:","261","    # https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/15810","262","    n_samples, n_features = int(5e4), 4","263","    X, y = make_classification(n_samples=n_samples, n_features=n_features,","264","                               random_state=0)","265","    assert X.nbytes > 1e6  # trigger joblib memmaping","266","","267","    X = _convert_container(X, input_type)","268","    clf = DummyClassifier(strategy='prior').fit(X, y)","269","","270","    # Actual smoke test: should not raise any error:","271","    n_repeats = 5","272","    r = permutation_importance(clf, X, y, n_repeats=n_repeats, n_jobs=2)","273","","274","    # Auxiliary check: DummyClassifier is feature independent:","275","    # permutating feature should not change the predictions","276","    expected_importances = np.zeros((n_features, n_repeats))","277","    assert_allclose(expected_importances, r.importances)"],"delete":[]}],"sklearn\/inspection\/_permutation_importance.py":[{"add":["6","from ..utils import Bunch","14","    random_state = check_random_state(random_state)","16","    # Work on a copy of X to to ensure thread-safety in case of threading based","17","    # parallelism. Furthermore, making a copy is also useful when the joblib","18","    # backend is 'loky' (default) or the old 'multiprocessing': in those cases,","19","    # if X is large it will be automatically be backed by a readonly memory map","20","    # (memmap). X.copy() on the other hand is always guaranteed to return a","21","    # writable data-structure whose columns can be shuffled inplace.","22","    X_permuted = X.copy()","24","    shuffling_idx = np.arange(X.shape[0])","26","        random_state.shuffle(shuffling_idx)","27","        if hasattr(X_permuted, \"iloc\"):","28","            col = X_permuted.iloc[shuffling_idx, col_idx]","29","            col.index = X_permuted.index","30","            X_permuted.iloc[:, col_idx] = col","31","        else:","32","            X_permuted[:, col_idx] = X_permuted[shuffling_idx, col_idx]","33","        feature_score = scorer(estimator, X_permuted, y)","101","    if not hasattr(X, \"iloc\"):","102","        X = check_array(X, force_all_finite='allow-nan', dtype=None)","104","    # Precompute random seed from the random state to be used","105","    # to get a fresh independent RandomState instance for each","106","    # parallel call to _calculate_permutation_scores, irrespective of","107","    # the fact that variables are shared or not depending on the active","108","    # joblib backend (sequential, thread-based or process-based).","110","    random_seed = random_state.randint(np.iinfo(np.int32).max + 1)","112","    scorer = check_scoring(estimator, scoring=scoring)","116","        estimator, X, y, col_idx, random_seed, n_repeats, scorer"],"delete":["8","from ..utils import Bunch","9","","10","","11","def _safe_column_setting(X, col_idx, values):","12","    \"\"\"Set column on X using `col_idx`\"\"\"","13","    if hasattr(X, \"iloc\"):","14","        X.iloc[:, col_idx] = values","15","    else:","16","        X[:, col_idx] = values","17","","18","","19","def _safe_column_indexing(X, col_idx):","20","    \"\"\"Return column from X using `col_idx`\"\"\"","21","    if hasattr(X, \"iloc\"):","22","        return X.iloc[:, col_idx].values","23","    else:","24","        return X[:, col_idx]","30","    original_feature = _safe_column_indexing(X, col_idx).copy()","31","    temp = original_feature.copy()","35","        random_state.shuffle(temp)","36","        _safe_column_setting(X, col_idx, temp)","37","        feature_score = scorer(estimator, X, y)","40","    _safe_column_setting(X, col_idx, original_feature)","106","    if hasattr(X, \"iloc\"):","107","        X = X.copy()  # Dataframe","108","    else:","109","        X = check_array(X, force_all_finite='allow-nan', dtype=np.object,","110","                        copy=True)","113","    scorer = check_scoring(estimator, scoring=scoring)","116","    scores = np.zeros((X.shape[1], n_repeats))","119","        estimator, X, y, col_idx, random_state, n_repeats, scorer"]}],"doc\/whats_new\/v0.22.rst":[{"add":["17","","24"," ","25",":mod:`sklearn.inspection`","26",".........................","27","","28","- |Fix| :func:`inspection.permutation_importance` will return the same","29","  `importances` when a `random_state` is given for both `n_jobs=1` or","30","  `n_jobs>1` both with shared memory backends (thread-safety) and","31","  isolated memory, process-based backends.","32","  Also avoid casting the data as object dtype and avoid read-only error","33","  on large dataframes with `n_jobs>1` as reported in :issue:`15810`.","34","  Follow-up of :pr:`15898` by :user:`Shivam Gargsya <shivamgargsya>`.","35","  :pr:`15933` by :user:`Guillaume Lemaitre <glemaitre>` and `Olivier Grisel`_."],"delete":[]}]}},"dfad061da209869716d4113987db6a655f5c5c3b":{"changes":{"sklearn\/metrics\/_classification.py":"MODIFY","sklearn\/metrics\/tests\/test_classification.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY"},"diff":{"sklearn\/metrics\/_classification.py":[{"add":["1966","            average=average, sample_weight=sample_weight,","1967","            zero_division=zero_division)"],"delete":["1966","            average=average, sample_weight=sample_weight)"]}],"sklearn\/metrics\/tests\/test_classification.py":[{"add":["155","@pytest.mark.parametrize('zero_division', [\"warn\", 0, 1])","156","def test_classification_report_zero_division_warning(zero_division):","157","    y_true, y_pred = [\"a\", \"b\", \"c\"], [\"a\", \"b\", \"d\"]","158","    with warnings.catch_warnings(record=True) as record:","159","        classification_report(","160","            y_true, y_pred, zero_division=zero_division, output_dict=True)","161","        if zero_division == \"warn\":","162","            assert len(record) > 1","163","            for item in record:","164","                msg = (\"Use `zero_division` parameter to control this \"","165","                       \"behavior.\")","166","                assert msg in str(item.message)","167","        else:","168","            assert not record","169","","170",""],"delete":["22","from sklearn.utils._testing import assert_warns"]}],"doc\/whats_new\/v0.22.rst":[{"add":["48","- |Fix| :func:`metrics.classification_report` does no longer ignore the","49","  value of the ``zero_division`` keyword argument. :pr:`15879`","50","  by :user:`Bibhash Chandra Mitra <Bibyutatsu>`.","51",""],"delete":[]}]}},"1c42e79d420cc03de5e0c3b625753c6084e25a3f":{"changes":{"sklearn\/linear_model\/_ransac.py":"MODIFY","doc\/whats_new\/v0.23.rst":"MODIFY","sklearn\/linear_model\/tests\/test_ransac.py":"MODIFY"},"diff":{"sklearn\/linear_model\/_ransac.py":[{"add":["330","        inlier_best_idxs_subset = None","407","            inlier_best_idxs_subset = inlier_idxs_subset","445","        if sample_weight is None:","446","            base_estimator.fit(X_inlier_best, y_inlier_best)","447","        else:","448","            base_estimator.fit(","449","                X_inlier_best,","450","                y_inlier_best,","451","                sample_weight=sample_weight[inlier_best_idxs_subset])"],"delete":["443","        base_estimator.fit(X_inlier_best, y_inlier_best)"]}],"doc\/whats_new\/v0.23.rst":[{"add":["60",":mod:`sklearn.linear_model`","61","...........................","62","","63","- |Fix| Fixed a bug where if a `sample_weight` parameter was passed to the fit","64","  method of :class:`linear_model.RANSACRegressor`, it would not be passed to","65","  the wrapped `base_estimator` during the fitting of the final model.","66","  :pr:`15573` by :user:`Jeremy Alexandre <J-A16>`.","67",""],"delete":[]}],"sklearn\/linear_model\/tests\/test_ransac.py":[{"add":["12","from sklearn.utils._testing import assert_allclose","13","from sklearn.datasets import make_regression","498","","499","","500","def test_ransac_final_model_fit_sample_weight():","501","    X, y = make_regression(n_samples=1000, random_state=10)","502","    rng = check_random_state(42)","503","    sample_weight = rng.randint(1, 4, size=y.shape[0])","504","    sample_weight = sample_weight \/ sample_weight.sum()","505","    ransac = RANSACRegressor(base_estimator=LinearRegression(), random_state=0)","506","    ransac.fit(X, y, sample_weight=sample_weight)","507","","508","    final_model = LinearRegression()","509","    mask_samples = ransac.inlier_mask_","510","    final_model.fit(","511","        X[mask_samples], y[mask_samples],","512","        sample_weight=sample_weight[mask_samples]","513","    )","514","","515","    assert_allclose(ransac.estimator_.coef_, final_model.coef_)"],"delete":[]}]}},"2151b79a916e37a7f416cce6ba512ad464259bb9":{"changes":{"sklearn\/svm\/src\/liblinear\/linear.cpp":"MODIFY"},"diff":{"sklearn\/svm\/src\/liblinear\/linear.cpp":[{"add":["517","\tdelete[] C;"],"delete":[]}]}},"078e3ef296f29b6a601b075ebd43c2792e8c620f":{"changes":{"build_tools\/azure\/install.cmd":"MODIFY"},"diff":{"build_tools\/azure\/install.cmd":[{"add":["27","    pip install coverage==4.5.3 codecov pytest-cov"],"delete":["27","    pip install coverage codecov pytest-cov"]}]}},"eb3f5dfe562b15e507c1f8e3ab16848ec2ab6f84":{"changes":{"doc\/conf.py":"MODIFY"},"diff":{"doc\/conf.py":[{"add":["299","    'show_memory': False,"],"delete":["299","    'show_memory': True,"]}]}},"cc2fbeddfa7f462cad230edcbffaf14e0fa5f965":{"changes":{"sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_grower.py":"MODIFY","doc\/whats_new\/v0.23.rst":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/gradient_boosting.py":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_gradient_boosting.py":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/grower.py":"MODIFY"},"diff":{"sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_grower.py":[{"add":["259","def assert_is_stump(grower):","260","    # To assert that stumps are created when max_depth=1","261","    for leaf in (grower.root.left_child, grower.root.right_child):","262","        assert leaf.left_child is None","263","        assert leaf.right_child is None","264","","265","","266","@pytest.mark.parametrize('max_depth', [1, 2, 3])","288","    if max_depth == 1:","289","        assert_is_stump(grower)","290",""],"delete":["259","@pytest.mark.parametrize('max_depth', [2, 3])"]}],"doc\/whats_new\/v0.23.rst":[{"add":["92","- |Fix|  Changed the convention for `max_depth` parameter of","93","  :class:`ensemble.HistGradientBoostingClassifier` and","94","  :class:`ensemble.HistGradientBoostingRegressor`. The depth now corresponds to","95","  the number of edges to go from the root to the deepest leaf.","96","  Stumps (trees with one split) are now allowed.","97","  :pr: `16182` by :user:`Santhosh B <santhoshbala18>`","98",""],"delete":[]}],"sklearn\/ensemble\/_hist_gradient_boosting\/gradient_boosting.py":[{"add":["691","        edges to go from the root to the deepest leaf.","692","        Depth isn't constrained by default.","874","        edges to go from the root to the deepest leaf.","875","        Depth isn't constrained by default."],"delete":["691","        nodes to go from the root to the deepest leaf. Must be strictly greater","692","        than 1. Depth isn't constrained by default.","874","        nodes to go from the root to the deepest leaf. Must be strictly greater","875","        than 1. Depth isn't constrained by default."]}],"sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_gradient_boosting.py":[{"add":["33","     ({'max_depth': 0}, 'max_depth=0 should not be smaller than 1'),"],"delete":["33","     ({'max_depth': 0}, 'max_depth=0 should not be smaller than 2'),","34","     ({'max_depth': 1}, 'max_depth=1 should not be smaller than 2'),"]}],"sklearn\/ensemble\/_hist_gradient_boosting\/grower.py":[{"add":["137","        edges to go from the root to the deepest leaf.","138","        Depth isn't constrained by default.","233","        if max_depth is not None and max_depth < 1:","235","                             ' smaller than 1'.format(max_depth))"],"delete":["137","        nodes to go from the root to the deepest leaf.","232","        if max_depth is not None and max_depth <= 1:","234","                             ' smaller than 2'.format(max_depth))"]}]}},"82845abac64a2de9ef851d9838ab857d74dea8fd":{"changes":{"sklearn\/_build_utils\/__init__.py":"MODIFY"},"diff":{"sklearn\/_build_utils\/__init__.py":[{"add":["63","    # Lazy import because cython is not a runtime dependency.","64","    from Cython import Tempita"],"delete":["63","    is_release = os.path.exists(os.path.join(top_path, 'PKG-INFO'))","64","    # Files are already cythonized, nothing to do.","65","    if is_release:","66","        return","67","","68","    # Lazy import because cython is not a dependency when building from","69","    # source distribution.","70","    from Cython import Tempita # noqa"]}]}},"ac7081c510fc2c27b1bef002dfefc9f3854e2c9a":{"changes":{"sklearn\/metrics\/_plot\/roc_curve.py":"MODIFY","sklearn\/metrics\/_plot\/tests\/test_plot_roc_curve.py":"MODIFY","sklearn\/metrics\/_plot\/precision_recall_curve.py":"MODIFY","sklearn\/metrics\/_plot\/tests\/test_plot_precision_recall.py":"MODIFY"},"diff":{"sklearn\/metrics\/_plot\/roc_curve.py":[{"add":["169","    if not is_classifier(estimator):","177","        if y_pred.shape[1] != 2:","178","            raise ValueError(classification_error)","179","        else:","180","            y_pred = y_pred[:, 1]","182","    pos_label = estimator.classes_[1]"],"delete":["6","from ...utils.validation import check_is_fitted","167","    check_is_fitted(estimator)","171","","172","    if is_classifier(estimator):","173","        if len(estimator.classes_) != 2:","174","            raise ValueError(classification_error)","175","        pos_label = estimator.classes_[1]","176","    else:","181","","185","        y_pred = y_pred[:, 1]"]}],"sklearn\/metrics\/_plot\/tests\/test_plot_roc_curve.py":[{"add":["10","from sklearn.exceptions import NotFittedError","11","from sklearn.pipeline import make_pipeline","12","from sklearn.preprocessing import StandardScaler","13","from sklearn.compose import make_column_transformer","114","","115","","116","@pytest.mark.parametrize(","117","    \"clf\", [LogisticRegression(),","118","            make_pipeline(StandardScaler(), LogisticRegression()),","119","            make_pipeline(make_column_transformer((StandardScaler(), [0, 1])),","120","                          LogisticRegression())])","121","def test_roc_curve_not_fitted_errors(pyplot, data_binary, clf):","122","    X, y = data_binary","123","    with pytest.raises(NotFittedError):","124","        plot_roc_curve(clf, X, y)","125","    clf.fit(X, y)","126","    disp = plot_roc_curve(clf, X, y)","127","    assert disp.estimator_name == clf.__class__.__name__"],"delete":[]}],"sklearn\/metrics\/_plot\/precision_recall_curve.py":[{"add":["143","    classification_error = (\"{} should be a binary classifer\".format(","145","    if not is_classifier(estimator):","146","        raise ValueError(classification_error)","153","        if y_pred.shape[1] != 2:","154","            raise ValueError(classification_error)","155","        else:","156","            y_pred = y_pred[:, 1]","158","    pos_label = estimator.classes_[1]"],"delete":["6","from ...utils.validation import check_is_fitted","143","    check_is_fitted(estimator)","145","    classificaiton_error = (\"{} should be a binary classifer\".format(","147","    if is_classifier(estimator):","148","        if len(estimator.classes_) != 2:","149","            raise ValueError(classificaiton_error)","150","        pos_label = estimator.classes_[1]","151","    else:","152","        raise ValueError(classificaiton_error)","159","        y_pred = y_pred[:, 1]"]}],"sklearn\/metrics\/_plot\/tests\/test_plot_precision_recall.py":[{"add":["12","from sklearn.pipeline import make_pipeline","13","from sklearn.preprocessing import StandardScaler","14","from sklearn.compose import make_column_transformer","115","","116","","117","@pytest.mark.parametrize(","118","    \"clf\", [make_pipeline(StandardScaler(), LogisticRegression()),","119","            make_pipeline(make_column_transformer((StandardScaler(), [0, 1])),","120","                          LogisticRegression())])","121","def test_precision_recall_curve_pipeline(pyplot, clf):","122","    X, y = make_classification(n_classes=2, n_samples=50, random_state=0)","123","    with pytest.raises(NotFittedError):","124","        plot_precision_recall_curve(clf, X, y)","125","    clf.fit(X, y)","126","    disp = plot_precision_recall_curve(clf, X, y)","127","    assert disp.estimator_name == clf.__class__.__name__"],"delete":[]}]}},"3ef8357cc01721bfe817a702acda5315ce0d716a":{"changes":{"sklearn\/model_selection\/tests\/test_search.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY","sklearn\/model_selection\/_search.py":"MODIFY"},"diff":{"sklearn\/model_selection\/tests\/test_search.py":[{"add":["58","from sklearn.neighbors import KNeighborsClassifier","64","from sklearn.metrics.pairwise import euclidean_distances","1802","","1803","","1804","def test_search_cv__pairwise_property_delegated_to_base_estimator():","1805","    \"\"\"","1806","    Test implementation of BaseSearchCV has the _pairwise property","1807","    which matches the _pairwise property of its estimator.","1808","    This test make sure _pairwise is delegated to the base estimator.","1809","","1810","    Non-regression test for issue #13920.","1811","    \"\"\"","1812","    est = BaseEstimator()","1813","    attr_message = \"BaseSearchCV _pairwise property must match estimator\"","1814","","1815","    for _pairwise_setting in [True, False]:","1816","        setattr(est, '_pairwise', _pairwise_setting)","1817","        cv = GridSearchCV(est, {'n_neighbors': [10]})","1818","        assert _pairwise_setting == cv._pairwise, attr_message","1819","","1820","","1821","def test_search_cv__pairwise_property_equivalence_of_precomputed():","1822","    \"\"\"","1823","    Test implementation of BaseSearchCV has the _pairwise property","1824","    which matches the _pairwise property of its estimator.","1825","    This test ensures the equivalence of 'precomputed'.","1826","","1827","    Non-regression test for issue #13920.","1828","    \"\"\"","1829","    n_samples = 50","1830","    n_splits = 2","1831","    X, y = make_classification(n_samples=n_samples, random_state=0)","1832","    grid_params = {'n_neighbors': [10]}","1833","","1834","    # defaults to euclidean metric (minkowski p = 2)","1835","    clf = KNeighborsClassifier()","1836","    cv = GridSearchCV(clf, grid_params, cv=n_splits)","1837","    cv.fit(X, y)","1838","    preds_original = cv.predict(X)","1839","","1840","    # precompute euclidean metric to validate _pairwise is working","1841","    X_precomputed = euclidean_distances(X)","1842","    clf = KNeighborsClassifier(metric='precomputed')","1843","    cv = GridSearchCV(clf, grid_params, cv=n_splits)","1844","    cv.fit(X_precomputed, y)","1845","    preds_precomputed = cv.predict(X_precomputed)","1846","","1847","    attr_message = \"GridSearchCV not identical with precomputed metric\"","1848","    assert (preds_original == preds_precomputed).all(), attr_message"],"delete":[]}],"doc\/whats_new\/v0.22.rst":[{"add":["742",":mod:`sklearn.model_selection`","743","..................","744","","745","- |Fix| :class:`model_selection.GridSearchCV` and","746","  `model_selection.RandomizedSearchCV` now supports the","747","  :term:`_pairwise` property, which prevents an error during cross-validation","748","  for estimators with pairwise inputs (such as","749","  :class:`neighbors.KNeighborsClassifier` when :term:`metric` is set to","750","  'precomputed').","751","  :pr:`13925` by :user:`Isaac S. Robson <isrobson>` and :pr:`15524` by","752","  :user:`Xun Tang <xun-tang>`.","753","","754",""],"delete":[]}],"sklearn\/model_selection\/_search.py":[{"add":["416","    @property","417","    def _pairwise(self):","418","        # allows cross-validation to see 'precomputed' metrics","419","        return getattr(self.estimator, '_pairwise', False)","420",""],"delete":[]}]}},"7c47337f7b15a5368c922ed1781a267bf66c7367":{"changes":{"sklearn\/ensemble\/_voting.py":"MODIFY","sklearn\/ensemble\/tests\/test_voting.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY"},"diff":{"sklearn\/ensemble\/_voting.py":[{"add":["73","","74","        # Uses None or 'drop' as placeholder for dropped estimators","75","        est_iter = iter(self.estimators_)","76","        for name, est in self.estimators:","77","            current_est = est if est in (None, 'drop') else next(est_iter)","78","            self.named_estimators_[name] = current_est","79",""],"delete":["73","        for k, e in zip(self.estimators, self.estimators_):","74","            self.named_estimators_[k[0]] = e"]}],"sklearn\/ensemble\/tests\/test_voting.py":[{"add":["562","","563","","564","# TODO: Remove drop parametrize in 0.24 when None is removed in Voting*","565","@pytest.mark.parametrize(","566","    \"Voter, BaseEstimator\",","567","    [(VotingClassifier, DecisionTreeClassifier),","568","     (VotingRegressor, DecisionTreeRegressor)]","569",")","570","@pytest.mark.parametrize(\"drop\", [None, 'drop'])","571","def test_correct_named_estimator_with_drop(Voter, BaseEstimator, drop):","572","    est = Voter(estimators=[('lr', drop),","573","                            ('tree', BaseEstimator(random_state=0))])","574","","575","    with pytest.warns(None) as rec:","576","        est.fit(X, y)","577","    assert rec if drop is None else not rec","578","","579","    assert est.named_estimators_['lr'] == drop"],"delete":[]}],"doc\/whats_new\/v0.22.rst":[{"add":["718",":mod:`sklearn.voting`","719",".....................","720","","721","- |Fix| The `named_estimators_` attribute in :class:`voting.VotingClassifier`","722","  and :class:`voting.VotingRegressor` now correctly maps to dropped estimators.","723","  Previously, the `named_estimators_` mapping was incorrect whenever one of the","724","  estimators was dropped. :pr:`15375` by `Thomas Fan`_.","725",""],"delete":[]}]}},"a5542e9490c63e44e5611583b5e8764149b230c8":{"changes":{"sklearn\/datasets\/__init__.py":"MODIFY","\/dev\/null":"DELETE","sklearn\/datasets\/_svmlight_format_io.py":"ADD","sklearn\/tests\/test_docstring_parameters.py":"MODIFY","sklearn\/_build_utils\/deprecated_modules.py":"MODIFY","sklearn\/datasets\/_rcv1.py":"MODIFY","sklearn\/tests\/test_common.py":"MODIFY","sklearn\/utils\/_testing.py":"MODIFY"},"diff":{"sklearn\/datasets\/__init__.py":[{"add":["44","from ._svmlight_format_io import load_svmlight_file","45","from ._svmlight_format_io import load_svmlight_files","46","from ._svmlight_format_io import dump_svmlight_file"],"delete":["44","from ._svmlight_format import load_svmlight_file","45","from ._svmlight_format import load_svmlight_files","46","from ._svmlight_format import dump_svmlight_file"]}],"\/dev\/null":[{"add":[],"delete":[]}],"sklearn\/datasets\/_svmlight_format_io.py":[{"add":[],"delete":[]}],"sklearn\/tests\/test_docstring_parameters.py":[{"add":["142","        if IS_PYPY and ('_svmlight_format_io' in modname or"],"delete":["142","        if IS_PYPY and ('_svmlight_format' in modname or"]}],"sklearn\/_build_utils\/deprecated_modules.py":[{"add":["135","    ('_svmlight_format_io', 'sklearn.datasets.svmlight_format',"],"delete":["135","    ('_svmlight_format', 'sklearn.datasets.svmlight_format',"]}],"sklearn\/datasets\/_rcv1.py":[{"add":["25","from ._svmlight_format_io import load_svmlight_files"],"delete":["25","from ._svmlight_format import load_svmlight_files"]}],"sklearn\/tests\/test_common.py":[{"add":["181","        if IS_PYPY and ('_svmlight_format_io' in modname or"],"delete":["181","        if IS_PYPY and ('_svmlight_format' in modname or"]}],"sklearn\/utils\/_testing.py":[{"add":["468","        if IS_PYPY and ('_svmlight_format_io' in modname or"],"delete":["468","        if IS_PYPY and ('_svmlight_format' in modname or"]}]}},"e3e91378114742b3ea53ae8ba1f1e0fff6f31909":{"changes":{"doc\/modules\/svm.rst":"MODIFY","sklearn\/svm\/_base.py":"MODIFY","sklearn\/svm\/_classes.py":"MODIFY"},"diff":{"doc\/modules\/svm.rst":[{"add":["6",".. TODO: Describe tol parameter","7",".. TODO: Describe max_iter parameter","8","","54","capable of performing binary and multi-class classification on a dataset.","65","other hand, :class:`LinearSVC` is another (faster) implementation of Support","67",":class:`LinearSVC` does not accept parameter ``kernel``, as this is","68","assumed to be linear. It also lacks some of the attributes of","72",":class:`LinearSVC` take as input two arrays: an array `X` of shape","73","`(n_samples, n_features)` holding the training samples, and an array `y` of","74","class labels (strings or integers), of shape `(n_samples)`::","89","SVMs decision function (detailed in the :ref:`svm_mathematical_formulation`)","90","depends on some subset of the training data, called the support vectors. Some","91","properties of these support vectors can be found in attributes","92","``support_vectors_``, ``support_`` and ``n_support``::","105",".. topic:: Examples:","106","","107"," * :ref:`sphx_glr_auto_examples_svm_plot_separating_hyperplane.py`,","108"," * :ref:`sphx_glr_auto_examples_svm_plot_svm_nonlinear.py`","109"," * :ref:`sphx_glr_auto_examples_svm_plot_svm_anova.py`,","110","","116",":class:`SVC` and :class:`NuSVC` implement the \"one-versus-one\"","117","approach for multi-class classification. In total,","118","``n_classes * (n_classes - 1) \/ 2``","121","``decision_function_shape`` option allows to monotonically transform the","122","results of the \"one-versus-one\" classifiers to a \"one-vs-rest\" decision","123","function of shape ``(n_samples, n_classes)``.","139","multi-class strategy, thus training `n_classes` models.","152","strategy, the so-called multi-class SVM formulated by Crammer and Singer","153","[#8]_, by using the option ``multi_class='crammer_singer'``. In practice,","154","one-vs-rest classification is usually preferred, since the results are mostly","155","similar, but the runtime is significantly less.","158","have the shape ``(n_classes, n_features)`` and ``(n_classes,)`` respectively.","159","Each row of the coefficients corresponds to one of the ``n_classes``","163","In the case of \"one-vs-one\" :class:`SVC` and :class:`NuSVC`, the layout of","164","the attributes is a little more involved. In the case of a linear","165","kernel, the attributes ``coef_`` and ``intercept_`` have the shape","166","``(n_classes * (n_classes - 1) \/ 2, n_features)`` and ``(n_classes *","167","(n_classes - 1) \/ 2)`` respectively. This is similar to the layout for","168",":class:`LinearSVC` described above, with each row now corresponding","173","The shape of ``dual_coef_`` is ``(n_classes-1, n_SV)`` with","176","of the ``n_classes * (n_classes - 1) \/ 2`` \"one-vs-one\" classifiers.","177","Each of the support vectors is used in ``n_classes - 1`` classifiers.","178","The ``n_classes - 1`` entries in each row correspond to the dual coefficients","181","This might be clearer with an example: consider a three class problem with","182","class 0 having three support vectors","206",".. topic:: Examples:","207","","208"," * :ref:`sphx_glr_auto_examples_svm_plot_iris_svc.py`,","220","calibrated using Platt scaling [#1]_: logistic regression on the SVM's scores,","222","In the multiclass case, this is extended as per [#2]_.","224",".. note::","225","","226","  The same probability calibration procedure is available for all estimators","227","  via the :class:`~sklearn.calibration.CalibratedClassifierCV` (see","228","  :ref:`calibration`). In the case of :class:`SVC` and :class:`NuSVC`, this","229","  procedure is builtin in `libsvm`_ which is used under the hood, so it does","230","  not rely on scikit-learn's","231","  :class:`~sklearn.calibration.CalibratedClassifierCV`.","232","","233","The cross-validation involved in Platt scaling","235","In addition, the probability estimates may be inconsistent with the scores:","236","","237","- the \"argmax\" of the scores may not be the argmax of the probabilities","238","- in binary classification, a sample may be labeled by ``predict`` as","239","  belonging to the positive class even if the output of `predict_proba` is","240","  less than 0.5; and similarly, it could be labeled as negative even if the","241","  output of `predict_proba` is more than 0.5.","242","","253","that it comes with a computational cost. See","254",":ref:`sphx_glr_auto_examples_svm_plot_svm_tie_breaking.py` for an example on","255","tie breaking.","261","classes or certain individual samples, the parameters ``class_weight`` and","264",":class:`SVC` (but not :class:`NuSVC`) implements the parameter","268","The figure below illustrates the decision boundary of an unbalanced problem,","269","with and without weight correction.","279","individual samples in the `fit` method through the ``sample_weight`` parameter.","280","Similar to ``class_weight``, this sets the parameter ``C`` for the i-th","281","example to ``C * sample_weight[i]``, which will encourage the classifier to","282","get these samples right. The figure below illustrates the effect of sample","283","weighting on the decision boundary. The size of the circles is proportional","284","to the sample weights:","310","because the cost function ignores samples whose prediction is close to their","311","target.","316","the linear kernel, while :class:`NuSVR` implements a slightly different","355","solver used by the `libsvm`_-based implementation scales between","362","For the linear case, the algorithm used in","374","    contiguous and double precision, it will be copied before calling the","380","    array will be copied and converted to the `liblinear`_ internal sparse data","383","    copying a dense numpy C-contiguous double precision array as input, we","395","","397","    choice.  If you have a lot of noisy observations you should decrease it:","398","    decreasing C corresponds to more regularization.","403","    sometimes up to 10 times longer, as shown in [#3]_.","409","    applied to the test vector to obtain meaningful results. This can be done","410","    easily by using a :class:`~sklearn.pipeline.Pipeline`::","411","","412","        >>> from sklearn.pipeline import make_pipeline","413","        >>> from sklearn.preprocessing import StandardScaler","414","        >>> from sklearn.svm import SVC","415","","416","        >>> clf = make_pipeline(StandardScaler(), SVC())","417","    ","418","    See section :ref:`preprocessing` for more details on scaling and","419","    normalization.","420","  ","421","  .. _shrinking_svm:","422","","423","  * Regarding the `shrinking` parameter, quoting [#4]_: *We found that if the","424","    number of iterations is large, then shrinking can shorten the training","425","    time. However, if we loosely solve the optimization problem (e.g., by","426","    using a large stopping tolerance), the code without using shrinking may","427","    be much faster*","432","  * In :class:`SVC`, if the data is unbalanced (e.g. many","448","    descent (i.e when ``dual`` is set to ``True``). It is thus not uncommon","450","    happens, try with a smaller `tol` parameter. This randomness can also be","458","    Increasing ``C`` yields a more complex model (more features are selected).","473","    :math:`d` is specified by parameter ``degree``, :math:`r` by ``coef0``.","476","    specified by parameter ``gamma``, must be greater than 0.","481","Different kernels are specified by the `kernel` parameter::","490","Parameters of the RBF Kernel","491","----------------------------","492","","493","When training an SVM with the *Radial Basis Function* (RBF) kernel, two","494","parameters must be considered: ``C`` and ``gamma``.  The parameter ``C``,","495","common to all SVM kernels, trades off misclassification of training examples","496","against simplicity of the decision surface. A low ``C`` makes the decision","497","surface smooth, while a high ``C`` aims at classifying all training examples","498","correctly.  ``gamma`` defines how much influence a single training example has.","499","The larger ``gamma`` is, the closer other examples must be to be affected.","500","","501","Proper choice of ``C`` and ``gamma`` is critical to the SVM's performance.  One","502","is advised to use :class:`sklearn.model_selection.GridSearchCV` with ","503","``C`` and ``gamma`` spaced exponentially far apart to choose good values.","504","","505",".. topic:: Examples:","506","","507"," * :ref:`sphx_glr_auto_examples_svm_plot_rbf_parameters.py`","508"," * :ref:`sphx_glr_auto_examples_svm_plot_svm_nonlinear.py`","509","","531","You can use your own defined kernels by passing a function to the","532","``kernel`` parameter.","555","You can pass pre-computed kernels by using the ``kernel='precomputed'``","556","option. You should then pass Gram matrix instead of X to the `fit` and","557","`predict` methods. The kernel values between *all* training vectors and the","558","test vectors must be provided:","561","    >>> from sklearn.datasets import make_classification","562","    >>> from sklearn.model_selection import train_test_split ","564","    >>> X, y = make_classification(n_samples=10, random_state=0)","565","    >>> X_train , X_test , y_train, y_test = train_test_split(X, y, random_state=0)","568","    >>> gram_train = np.dot(X_train, X_train.T)","569","    >>> clf.fit(gram_train, y_train)","572","    >>> gram_test = np.dot(X_test, X_train.T)","573","    >>> clf.predict(gram_test)","574","    array([0, 1, 0])","582","A support vector machine constructs a hyper-plane or set of hyper-planes in a","583","high or infinite dimensional space, which can be used for","588","generalization error of the classifier. The figure below shows the decision","589","function for a linearly separable problem, with three samples on the","590","margin boundaries, called \"support vectors\":","596","In general, when the problem isn't linearly separable, the support vectors","597","are the samples *within* the margin boundaries.","598","","599","We recommend [#5]_ and [#6]_ as good references for the theory and","600","practicalities of SVMs.","601","","606","vector :math:`y \\in \\{1, -1\\}^n`, our goal is to find :math:`w \\in","607","\\mathbb{R}^p` and :math:`b \\in \\mathbb{R}` such that the prediction given by","608",":math:`\\text{sign} (w^T\\phi(x) + b)` is correct for most samples.","610","SVC solves the following primal problem:","619","Intuitively, we're trying to maximize the margin (by minimizing","620",":math:`||w||^2 = w^Tw`), while incurring a penalty when a sample is","621","misclassified or within the margin boundary. Ideally, the value :math:`y_i","622","(w^T \\phi (x_i) + b)` would be :math:`\\geq 1` for all samples, which","623","indicates a perfect prediction. But problems are usually not always perfectly","624","separable with a hyperplane, so we allow some samples to be at a distance :math:`\\zeta_i` from","625","their correct margin boundary. The penalty term `C` controls the strengh of","626","this penalty, and as a result, acts as an inverse regularization parameter","627","(see note below).","628","","629","The dual problem to the primal is","639","where :math:`e` is the vector of all ones,","640","and :math:`Q` is an :math:`n` by :math:`n` positive semidefinite matrix,","642","is the kernel. The terms :math:`\\alpha_i` are called the dual coefficients,","643","and they are upper-bounded by :math:`C`.","644","This dual representation highlights the fact that training vectors are","645","implicitly mapped into a higher (maybe infinite)","646","dimensional space by the function :math:`\\phi`: see `kernel trick","647","<https:\/\/en.wikipedia.org\/wiki\/Kernel_method>`_.","649","Once the optimization problem is solved, the output of","650",":term:`decision_function` for a given sample :math:`x` becomes:","652",".. math:: \\sum_{i\\in SV} y_i \\alpha_i K(x_i, x) + b,","654","and the predicted class correspond to its sign. We only need to sum over the","655","support vectors (i.e. the samples that lie within the margin) because the","656","dual coefficients :math:`\\alpha_i` are zero for the other samples.","657","","658","These parameters can be accessed through the attributes ``dual_coef_``","659","which holds the product :math:`y_i \\alpha_i`, ``support_vectors_`` which","660","holds the support vectors, and ``intercept_`` which holds the independent","661","term :math:`b`","672","LinearSVC","673","---------","675","The primal problem can be equivalently formulated as","677",".. math::","679","    \\min_ {w, b} \\frac{1}{2} w^T w + C \\sum_{i=1}\\max(0, y_i (w^T \\phi(x_i) + b)),","681","where we make use of the `hinge loss","682","<https:\/\/en.wikipedia.org\/wiki\/Hinge_loss>`_. This is the form that is","683","directly optimized by :class:`LinearSVC`, but unlike the dual form, this one","684","does not involve inner products between samples, so the famous kernel trick","685","cannot be applied. This is why only the linear kernel is supported by","686",":class:`LinearSVC` (:math:`\\phi` is the identity function).","688",".. _nu_svc:","693","The :math:`\\nu`-SVC formulation [#7]_ is a reparameterization of the","694",":math:`C`-SVC and therefore mathematically equivalent.","696","We introduce a new parameter :math:`\\nu` (instead of :math:`C`) which","697","controls the number of support vectors and *margin errors*:","698",":math:`\\nu \\in (0, 1]` is an upper bound on the fraction of margin errors and","699","a lower bound of the fraction of support vectors. A margin error corresponds","700","to a sample that lies on the wrong side of its margin boundary: it is either","701","misclassified, or it is correctly classified but does not lie beyond the","702","margin.","722","Here, we are penalizing samples whose prediction is at least :math:`\\varepsilon`","723","away from their true target. These samples penalize the objective by","724",":math:`\\zeta_i` or :math:`\\zeta_i^*`, depending on whether their predictions","725","lie above or below the :math:`\\varepsilon` tube.","726","","727","The dual problem is","737","where :math:`e` is the vector of all ones,","743","The prediction is:","745",".. math:: \\sum_{i \\in SV}(\\alpha_i - \\alpha_i^*) K(x_i, x) + b","747","These parameters can be accessed through the attributes ``dual_coef_``","750","term :math:`b`","752","LinearSVR","753","---------","755","The primal problem can be equivalently formulated as","757",".. math::","758","","759","    \\min_ {w, b} \\frac{1}{2} w^T w + C \\sum_{i=1}\\max(0, |y_i - (w^T \\phi(x_i) + b)| - \\varepsilon),","760","","761","where we make use of the epsilon-insensitive loss, i.e. errors of less than","762",":math:`\\varepsilon` are ignored. This is the form that is directly optimized","763","by :class:`LinearSVR`.","770","Internally, we use `libsvm`_ [#4]_ and `liblinear`_ [#3]_ to handle all","772","For a description of the implementation and details of the algorithms","773","used, please refer to their respective papers.","774","","781","   .. [#1] Platt `\"Probabilistic outputs for SVMs and comparisons to","782","      regularized likelihood methods\"","783","      <https:\/\/www.cs.colorado.edu\/~mozer\/Teaching\/syllabi\/6622\/papers\/Platt1999.pdf>`_.","785","   .. [#2] Wu, Lin and Weng, `\"Probability estimates for multi-class","786","      classification by pairwise coupling\"","787","      <https:\/\/www.csie.ntu.edu.tw\/~cjlin\/papers\/svmprob\/svmprob.pdf>`_, JMLR","788","      5:975-1005, 2004.","789"," ","790","   .. [#3] Fan, Rong-En, et al.,","791","      `\"LIBLINEAR: A library for large linear classification.\"","792","      <https:\/\/www.csie.ntu.edu.tw\/~cjlin\/papers\/liblinear.pdf>`_,","793","      Journal of machine learning research 9.Aug (2008): 1871-1874.","794","","795","   .. [#4] Chang and Lin, `LIBSVM: A Library for Support Vector Machines","798","   .. [#5] Bishop, `Pattern recognition and machine learning","799","      <https:\/\/www.microsoft.com\/en-us\/research\/uploads\/prod\/2006\/01\/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf>`_,","800","      chapter 7 Sparse Kernel Machines","802","   .. [#6] `\"A Tutorial on Support Vector Regression\"","803","      <http:\/\/citeseerx.ist.psu.edu\/viewdoc\/summary?doi=10.1.1.114.4288>`_,","804","      Alex J. Smola, Bernhard Sch?lkopf - Statistics and Computing archive","805","      Volume 14 Issue 3, August 2004, p. 199-222.","807","   .. [#7] Sch?lkopf et. al `New Support Vector Algorithms","808","      <https:\/\/www.stat.purdue.edu\/~yuzhu\/stat598m3\/Papers\/NewSVM.pdf>`_","809","    ","810","   .. [#8] Crammer and Singer `On the Algorithmic Implementation ofMulticlass","811","      Kernel-based Vector Machines","812","      <http:\/\/jmlr.csail.mit.edu\/papers\/volume2\/crammer01a\/crammer01a.pdf>`_,","813","      JMLR 2001."],"delete":["51","capable of performing multi-class classification on a dataset.","62","other hand, :class:`LinearSVC` is another implementation of Support","64",":class:`LinearSVC` does not accept keyword ``kernel``, as this is","65","assumed to be linear. It also lacks some of the members of","69",":class:`LinearSVC` take as input two arrays: an array X of size ``[n_samples,","70","n_features]`` holding the training samples, and an array y of class labels","71","(strings or integers), size ``[n_samples]``::","86","SVMs decision function depends on some subset of the training data,","87","called the support vectors. Some properties of these support vectors","88","can be found in members ``support_vectors_``, ``support_`` and","89","``n_support``::","107",":class:`SVC` and :class:`NuSVC` implement the \"one-against-one\"","108","approach (Knerr et al., 1990) for multi- class classification. If","109","``n_class`` is the number of classes, then ``n_class * (n_class - 1) \/ 2``","112","``decision_function_shape`` option allows to monotically transform the results of the","113","\"one-against-one\" classifiers to a decision function of shape ``(n_samples,","114","n_classes)``.","130","multi-class strategy, thus training n_class models. If there are only","131","two classes, only one model is trained::","144","strategy, the so-called multi-class SVM formulated by Crammer and Singer, by","145","using the option ``multi_class='crammer_singer'``. This method is consistent,","146","which is not true for one-vs-rest classification.","147","In practice, one-vs-rest classification is usually preferred, since the results","148","are mostly similar, but the runtime is significantly less.","151","have the shape ``[n_class, n_features]`` and ``[n_class]`` respectively.","152","Each row of the coefficients corresponds to one of the ``n_class`` many","156","In the case of \"one-vs-one\" :class:`SVC`, the layout of the attributes","157","is a little more involved. In the case of having a linear kernel, the","158","attributes ``coef_`` and ``intercept_`` have the shape","159","``[n_class * (n_class - 1) \/ 2, n_features]`` and","160","``[n_class * (n_class - 1) \/ 2]`` respectively. This is similar to the","161","layout for :class:`LinearSVC` described above, with each row now corresponding","166","The shape of ``dual_coef_`` is ``[n_class-1, n_SV]`` with","169","of the ``n_class * (n_class - 1) \/ 2`` \"one-vs-one\" classifiers.","170","Each of the support vectors is used in ``n_class - 1`` classifiers.","171","The ``n_class - 1`` entries in each row correspond to the dual coefficients","174","This might be made more clear by an example:","175","","176","Consider a three class problem with class 0 having three support vectors","211","calibrated using Platt scaling: logistic regression on the SVM's scores,","213","In the multiclass case, this is extended as per Wu et al. (2004).","215","Needless to say, the cross-validation involved in Platt scaling","217","In addition, the probability estimates may be inconsistent with the scores,","218","in the sense that the \"argmax\" of the scores","219","may not be the argmax of the probabilities.","220","(E.g., in binary classification,","221","a sample may be labeled by ``predict`` as belonging to a class","222","that has probability <? according to ``predict_proba``.)","233","that it comes with a computational cost.","234","","235",".. figure:: ..\/auto_examples\/svm\/images\/sphx_glr_plot_svm_tie_breaking_001.png","236","   :target: ..\/auto_examples\/svm\/plot_svm_tie_breaking.html","237","   :align: center","238","","239",".. topic:: References:","240","","241"," * Wu, Lin and Weng,","242","   `\"Probability estimates for multi-class classification by pairwise coupling\"","243","   <https:\/\/www.csie.ntu.edu.tw\/~cjlin\/papers\/svmprob\/svmprob.pdf>`_,","244","   JMLR 5:975-1005, 2004.","245"," ","246"," ","247"," * Platt","248","   `\"Probabilistic outputs for SVMs and comparisons to regularized likelihood methods\"","249","   <https:\/\/www.cs.colorado.edu\/~mozer\/Teaching\/syllabi\/6622\/papers\/Platt1999.pdf>`_.","255","classes or certain individual samples keywords ``class_weight`` and","258",":class:`SVC` (but not :class:`NuSVC`) implement a keyword","271","individual samples in method ``fit`` through keyword ``sample_weight``. Similar","272","to ``class_weight``, these set the parameter ``C`` for the i-th example to","273","``C * sample_weight[i]``.","274","","281","","284"," * :ref:`sphx_glr_auto_examples_svm_plot_iris_svc.py`,","285"," * :ref:`sphx_glr_auto_examples_svm_plot_separating_hyperplane.py`,","287"," * :ref:`sphx_glr_auto_examples_svm_plot_svm_anova.py`,","288"," * :ref:`sphx_glr_auto_examples_svm_plot_svm_nonlinear.py`","305","because the cost function for building the model ignores any training","306","data close to the model prediction.","311","linear kernels, while :class:`NuSVR` implements a slightly different","333","","334","","352","solver used by this `libsvm`_-based implementation scales between","359","Also note that for the linear case, the algorithm used in","371","    contiguous, and double precision, it will be copied before calling the","377","    array will be copied and converted to the liblinear internal sparse data","380","    copying a dense numpy C-contiguous double precision array as input we","393","    choice.  If you have a lot of noisy observations you should decrease it.","394","    It corresponds to regularize more the estimation.","399","    sometimes up to 10 times longer, as shown by Fan et al. (2008)","405","    applied to the test vector to obtain meaningful results. See section","406","    :ref:`preprocessing` for more details on scaling and normalization.","411","  * In :class:`SVC`, if data for classification are unbalanced (e.g. many","427","    descent (i.e when ``dual`` is set to ``True``). It is thus not uncommon,","429","    happens, try with a smaller tol parameter. This randomness can also be","437","    Increasing ``C`` yields a more complex model (more feature are selected).","442",".. topic:: References:","443","","444"," * Fan, Rong-En, et al.,","445","   `\"LIBLINEAR: A library for large linear classification.\"","446","   <https:\/\/www.csie.ntu.edu.tw\/~cjlin\/papers\/liblinear.pdf>`_,","447","   Journal of machine learning research 9.Aug (2008): 1871-1874.","448","","459","    :math:`d` is specified by keyword ``degree``, :math:`r` by ``coef0``.","462","    specified by keyword ``gamma``, must be greater than 0.","467","Different kernels are specified by keyword kernel at initialization::","497","You can also use your own defined kernels by passing a function to the","498","keyword ``kernel`` in the constructor.","521","Set ``kernel='precomputed'`` and pass the Gram matrix instead of X in the fit","522","method. At the moment, the kernel values between *all* training vectors and the","523","test vectors must be provided.","527","    >>> X = np.array([[0, 0], [1, 1]])","528","    >>> y = [0, 1]","531","    >>> gram = np.dot(X, X.T)","532","    >>> clf.fit(gram, y)","535","    >>> clf.predict(gram)","536","    array([0, 1])","538","Parameters of the RBF Kernel","539","~~~~~~~~~~~~~~~~~~~~~~~~~~~~","540","","541","When training an SVM with the *Radial Basis Function* (RBF) kernel, two","542","parameters must be considered: ``C`` and ``gamma``.  The parameter ``C``,","543","common to all SVM kernels, trades off misclassification of training examples","544","against simplicity of the decision surface. A low ``C`` makes the decision","545","surface smooth, while a high ``C`` aims at classifying all training examples","546","correctly.  ``gamma`` defines how much influence a single training example has.","547","The larger ``gamma`` is, the closer other examples must be to be affected.","548","","549","Proper choice of ``C`` and ``gamma`` is critical to the SVM's performance.  One","550","is advised to use :class:`sklearn.model_selection.GridSearchCV` with ","551","``C`` and ``gamma`` spaced exponentially far apart to choose good values.","552","","553",".. topic:: Examples:","554","","555"," * :ref:`sphx_glr_auto_examples_svm_plot_rbf_parameters.py`","562","A support vector machine constructs a hyper-plane or set of hyper-planes","563","in a high or infinite dimensional space, which can be used for","568","generalization error of the classifier.","569","","579","vector :math:`y \\in \\{1, -1\\}^n`, SVC solves the following primal problem:","586","","587","","591","Its dual is","601","where :math:`e` is the vector of all ones, :math:`C > 0` is the upper bound,","602",":math:`Q` is an :math:`n` by :math:`n` positive semidefinite matrix,","604","is the kernel. Here training vectors are implicitly mapped into a higher","605","(maybe infinite) dimensional space by the function :math:`\\phi`.","608","The decision function is:","610",".. math:: \\operatorname{sgn}(\\sum_{i=1}^n y_i \\alpha_i K(x_i, x) + \\rho)","621",".. TODO multiclass case ?\/","623","This parameters can be accessed through the members ``dual_coef_``","624","which holds the product :math:`y_i \\alpha_i`, ``support_vectors_`` which","625","holds the support vectors, and ``intercept_`` which holds the independent","626","term :math:`\\rho` :","628",".. topic:: References:","630"," * `\"Automatic Capacity Tuning of Very Large VC-dimension Classifiers\"","631","   <http:\/\/citeseerx.ist.psu.edu\/viewdoc\/summary?doi=10.1.1.17.7215>`_,","632","   I. Guyon, B. Boser, V. Vapnik - Advances in neural information","633","   processing 1993.","636"," * `\"Support-vector networks\"","637","   <https:\/\/link.springer.com\/article\/10.1007%2FBF00994018>`_,","638","   C. Cortes, V. Vapnik - Machine Learning, 20, 273-297 (1995).","639","","640","","645","We introduce a new parameter :math:`\\nu` which controls the number of","646","support vectors and training errors. The parameter :math:`\\nu \\in (0,","647","1]` is an upper bound on the fraction of training errors and a lower","648","bound of the fraction of support vectors.","650","It can be shown that the :math:`\\nu`-SVC formulation is a reparameterization","651","of the :math:`C`-SVC and therefore mathematically equivalent.","671","Its dual is","681","where :math:`e` is the vector of all ones, :math:`C > 0` is the upper bound,","687","The decision function is:","689",".. math:: \\sum_{i=1}^n (\\alpha_i - \\alpha_i^*) K(x_i, x) + \\rho","691","These parameters can be accessed through the members ``dual_coef_``","694","term :math:`\\rho`","696",".. topic:: References:","698"," * `\"A Tutorial on Support Vector Regression\"","699","   <http:\/\/citeseerx.ist.psu.edu\/viewdoc\/summary?doi=10.1.1.114.4288>`_,","700","   Alex J. Smola, Bernhard Sch?lkopf - Statistics and Computing archive","701","   Volume 14 Issue 3, August 2004, p. 199-222. ","709","Internally, we use `libsvm`_ and `liblinear`_ to handle all","717","  For a description of the implementation and details of the algorithms","718","  used, please refer to","720","    - `LIBSVM: A Library for Support Vector Machines","723","    - `LIBLINEAR -- A Library for Large Linear Classification","724","      <https:\/\/www.csie.ntu.edu.tw\/~cjlin\/liblinear\/>`_."]}],"sklearn\/svm\/_base.py":[{"add":["114","        X : {array-like, sparse matrix} of shape (n_samples, n_features) \\","115","                or (n_samples, n_samples)"],"delete":["114","        X : {array-like, sparse matrix} of shape (n_samples, n_features)"]}],"sklearn\/svm\/_classes.py":[{"add":["161","    >>> from sklearn.pipeline import make_pipeline","162","    >>> from sklearn.preprocessing import StandardScaler","165","    >>> clf = make_pipeline(StandardScaler(),","166","    ...                     LinearSVC(random_state=0, tol=1e-5))","168","    Pipeline(steps=[('standardscaler', StandardScaler()),","169","                    ('linearsvc', LinearSVC(random_state=0, tol=1e-05))])","170","","171","    >>> print(clf.named_steps['linearsvc'].coef_)","172","    [[0.141...   0.526... 0.679... 0.493...]]","173","","174","    >>> print(clf.named_steps['linearsvc'].intercept_)","175","    [0.1693...]","330","    >>> from sklearn.pipeline import make_pipeline","331","    >>> from sklearn.preprocessing import StandardScaler","334","    >>> regr = make_pipeline(StandardScaler(),","335","    ...                      LinearSVR(random_state=0, tol=1e-5))","337","    Pipeline(steps=[('standardscaler', StandardScaler()),","338","                    ('linearsvr', LinearSVR(random_state=0, tol=1e-05))])","339","","340","    >>> print(regr.named_steps['linearsvr'].coef_)","341","    [18.582... 27.023... 44.357... 64.522...]","342","    >>> print(regr.named_steps['linearsvr'].intercept_)","343","    [-4...]","345","    [-2.384...]","346","","475","        See the :ref:`User Guide <shrinking_svm>`.","510","        ('ovo') is always used as multi-class strategy. The parameter is","511","        ignored for binary classification.","533","        probability estimates. Ignored when `probability` is False.","549","        Dual coefficients of the support vector in the decision","550","        function (see :ref:`sgd_mathematical_formulation`), multiplied by","551","        their targets.","554","        non-trivial. See the :ref:`multi-class section of the User Guide","555","        <svm_multi_class>` for details.","594","    >>> from sklearn.pipeline import make_pipeline","595","    >>> from sklearn.preprocessing import StandardScaler","599","    >>> clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))","601","    Pipeline(steps=[('standardscaler', StandardScaler()),","602","                    ('svc', SVC(gamma='auto'))])","603","","659","        An upper bound on the fraction of margin errors (see :ref:`User Guide","660","        <nu_svc>`) and a lower bound of the fraction of support vectors.","661","        Should be in the interval (0, 1].","690","        See the :ref:`User Guide <shrinking_svm>`.","723","        (n_samples, n_classes * (n_classes - 1) \/ 2). However, one-vs-one","724","        ('ovo') is always used as multi-class strategy. The parameter is","725","        ignored for binary classification.","747","        probability estimates. Ignored when `probability` is False.","763","        Dual coefficients of the support vector in the decision","764","        function (see :ref:`sgd_mathematical_formulation`), multiplied by","765","        their targets.","768","        non-trivial. See the :ref:`multi-class section of the User Guide","769","        <svm_multi_class>` for details.","810","    >>> from sklearn.pipeline import make_pipeline","811","    >>> from sklearn.preprocessing import StandardScaler","813","    >>> clf = make_pipeline(StandardScaler(), NuSVC())","815","    Pipeline(steps=[('standardscaler', StandardScaler()), ('nusvc', NuSVC())])","922","        See the :ref:`User Guide <shrinking_svm>`.","962","    >>> from sklearn.pipeline import make_pipeline","963","    >>> from sklearn.preprocessing import StandardScaler","969","    >>> regr = make_pipeline(StandardScaler(), SVR(C=1.0, epsilon=0.2))","971","    Pipeline(steps=[('standardscaler', StandardScaler()),","972","                    ('svr', SVR(epsilon=0.2))])","973","","1069","        See the :ref:`User Guide <shrinking_svm>`.","1109","    >>> from sklearn.pipeline import make_pipeline","1110","    >>> from sklearn.preprocessing import StandardScaler","1116","    >>> regr = make_pipeline(StandardScaler(), NuSVR(C=1.0, nu=0.1))","1118","    Pipeline(steps=[('standardscaler', StandardScaler()),","1119","                    ('nusvr', NuSVR(nu=0.1))])","1197","        See the :ref:`User Guide <shrinking_svm>`."],"delete":["163","    >>> clf = LinearSVC(random_state=0, tol=1e-5)","165","    LinearSVC(random_state=0, tol=1e-05)","166","    >>> print(clf.coef_)","167","    [[0.085... 0.394... 0.498... 0.375...]]","168","    >>> print(clf.intercept_)","169","    [0.284...]","326","    >>> regr = LinearSVR(random_state=0, tol=1e-5)","328","    LinearSVR(random_state=0, tol=1e-05)","329","    >>> print(regr.coef_)","330","    [16.35... 26.91... 42.30... 60.47...]","331","    >>> print(regr.intercept_)","332","    [-4.29...]","334","    [-4.29...]","497","        ('ovo') is always used as multi-class strategy.","519","        probability estimates.","535","        Coefficients of the support vector in the decision function.","538","        non-trivial. See the section about multi-class classification in the","539","        SVM section of the User Guide for details.","581","    >>> clf = SVC(gamma='auto')","583","    SVC(gamma='auto')","639","        An upper bound on the fraction of training errors and a lower","640","        bound of the fraction of support vectors. Should be in the","641","        interval (0, 1].","702","        (n_samples, n_classes * (n_classes - 1) \/ 2).","724","        probability estimates.","740","        Coefficients of the support vector in the decision function.","743","        non-trivial. See the section about multi-class classification in","744","        the SVM section of the User Guide for details.","786","    >>> clf = NuSVC()","788","    NuSVC()","939","    >>> regr = SVR(C=1.0, epsilon=0.2)","941","    SVR(epsilon=0.2)","1081","    >>> regr = NuSVR(C=1.0, nu=0.1)","1083","    NuSVR(nu=0.1)"]}]}},"01c9ef3da3986f57db07c74565685f4b10eff10c":{"changes":{"sklearn\/metrics\/cluster\/_supervised.py":"MODIFY","doc\/whats_new\/v0.23.rst":"MODIFY","sklearn\/metrics\/cluster\/tests\/test_supervised.py":"MODIFY"},"diff":{"sklearn\/metrics\/cluster\/_supervised.py":[{"add":["647","    return np.clip(mi.sum(), 0.0, None)"],"delete":["647","    return mi.sum()"]}],"doc\/whats_new\/v0.23.rst":[{"add":["168","- |Fix| Fixed a bug in :func:`metrics.mutual_info_score` where negative","169","  scores could be returned. :pr:`16362` by `Thomas Fan`_.","170",""],"delete":[]}],"sklearn\/metrics\/cluster\/tests\/test_supervised.py":[{"add":["339","","340","","341","@pytest.mark.parametrize('labels_true, labels_pred', [","342","    (['a'] * 6, [1, 1, 0, 0, 1, 1]),","343","    ([1] * 6, [1, 1, 0, 0, 1, 1]),","344","    ([1, 1, 0, 0, 1, 1], ['a'] * 6),","345","    ([1, 1, 0, 0, 1, 1], [1] * 6),","346","])","347","def test_mutual_info_score_positive_constant_label(labels_true, labels_pred):","348","    # non-regression test for #16355","349","    assert mutual_info_score(labels_true, labels_pred) >= 0"],"delete":[]}]}},"1382831f4158f358701a00c6ad216d7814c5716f":{"changes":{"doc\/developers\/maintainer.rst":"MODIFY"},"diff":{"doc\/developers\/maintainer.rst":[{"add":["3","","4","Releasing","5","---------","6","","7","This section is about preparing a major release, incrementing the minor","8","version, or a bug fix release incrementing the patch version. Our convention is","9","that we release one or more release candidates (0.RRrcN) before releasing the","10","final distributions. We follow the `PEP101","11","<https:\/\/www.python.org\/dev\/peps\/pep-0101\/>`_ to indicate release candidates,","12","post, and minor releases.","13","","15","................","21","   and commit. This is only needed if the authors have changed since the last","22","   release. This step is sometimes done independent of the release. This","23","   updates the maintainer list and is not the contributor list for the release.","32","     sections. It's not perfect, and requires manual checking of the changes.","33","     If the whats new list is well curated, it may not be necessary.","38","4. Make sure the deprecations, FIXME and TODOs tagged for the release have","39","   been taken care of.","41","**Permissions**","43","The release manager requires a set of permissions on top of the usual","44","permissions given to maintainers, which includes:","46","- *maintainer* role on ``scikit-learn`` projects on ``pypi.org`` and","47","  ``test.pypi.org``, separately.","48","- become a member of the *scikit-learn* team on conda-forge by editing the ","49","  ``recipe\/meta.yaml`` file on ","50","  ``https:\/\/github.com\/conda-forge\/scikit-learn-feedstock``","51","- *maintainer* on ``https:\/\/github.com\/MacPython\/scikit-learn-wheels``","52","","53","","54",".. _preparing_a_release_pr:","55","","56","Preparing a release PR","57","......................","58","","59","Releasing the first RC of e.g. version `0.99` involves creating the release","60","branch `0.99.X` directly on the main repo, where `X` really is the letter X,","61","**not a placeholder**. This is considered the *feature freeze*. The","62","development for the major and minor releases of 0.99 should","63","**also** happen under `0.99.X`. Each release (rc, major, or minor) is a tag","64","under that branch.","65","","66","In terms of including changes, the first RC ideally counts as a *feature","67","freeze*. Each coming release candidate and the final release afterwards will","68","include minor documentation changes and bug fixes. Any major enhancement or","69","feature should be excluded.","70","","71","The minor releases should include bug fixes and some relevant documentation","72","changes only. Any PR resulting in a behavior change which is not a bug fix","73","should be excluded.","74","","75","First, create a branch, **on your own fork** (to release e.g. `0.999.3`)::","76","","77","    $ # assuming master and upstream\/master are the same","80","Then, create a PR **to the** `scikit-learn\/0.999.X` **branch** (not to","81","master!) with all the desired changes::","82","","83","\t$ git rebase -i upstream\/0.999.2","84","","85","It's nice to have a copy of the ``git rebase -i`` log in the PR to help others","86","understand what's included.","89","................","91","0. Create the release branch on the main repo, if it does not exist. This is","92","   done only once, as the major and minor releases happen on the same branch::","93","","94","     $ git checkout -b 0.99.X","95","","96","   Again, `X` is literal here, and `99` is replaced by the release number.","97","   The branches are called ``0.19.X``, ``0.20.X``, etc.","98","","99","1. Update docs. Note that this is for the final release, not necessarily for","100","   the RC releases. These changes should be made in master and cherry-picked","101","   into the release branch, only before the final release.","108","   - Update the release date in ``whats_new.rst``","110","   - Edit the doc\/templates\/index.html to change the 'News' entry of the front","111","     page.","114","   `sklearn\/__init__.py`, the ``__version__`` variable by removing ``dev*``","115","   only when ready to release. On master, increment the version in the same","116","   place (when branching for release). This means while we're in the release","117","   candidate period, the latest stable is two versions behind the master","118","   branch, instead of one.","120","3. At this point all relevant PRs should have been merged into the `0.99.X`","121","   branch. Create the source tarball:","131","   - You can also test a binary dist build using::","132","","133","       $ python setup.py bdist_wheel","134","","135","   - You can test if PyPi is going to accept the package using::","136","","137","       $ twine check dist\/*","138","","139","   You can run ``twine check`` after step 5 (fetching artifacts) as well.","140","","145","4. Proceed with caution. Ideally, tags should be created when you're almost","146","   certain that the release is ready, since adding a tag to the main repo can","147","   trigger certain automated processes. You can test upload the ``sdist`` to","148","   ``test.pypi.org``, and test the next step by setting ``BUILD_COMMIT`` to the","149","   branch name (``0.99.X`` for instance) in a PR to the wheel building repo.","150","   Once all works, you can proceed with tagging. Create the tag and push it (if","151","   it's an RC, it can be ``0.xxrc1`` for instance)::","152","","153","    $ git tag -a 0.99  # in the 0.99.X branch","154","","155","    $ git push git@github.com:scikit-learn\/scikit-learn.git 0.99","156","","166","       $ rm -r dist # only if there's anything other than the sdist tar.gz there","171","   along with the source tarball (\"scikit-learn-RRR.tar.gz\").","176","   Before uploading to pypi, you can test upload to test.pypi.org::","177","","178","       $ twine upload --verbose --repository-url https:\/\/test.pypi.org\/legacy\/ dist\/*","179","","195","       $ sed -i \"s\/latestStable = '.*\/latestStable = '0.999';\/\" versionwarning.js","196","       $ git add stable\/ versionwarning.js","197","       $ git commit -m \"Update stable to point to 0.999\"","204","    * [ ] update dependencies and release tag at","205","      https:\/\/github.com\/MacPython\/scikit-learn-wheels","208","    * [ ] confirm bot detected at","209","      https:\/\/github.com\/conda-forge\/scikit-learn-feedstock and wait for merge","211","    * [ ] fix the binder release version in ``.binder\/requirement.txt`` (see","212","      #15847)","213","    * [ ] announce on mailing list and on twitter"],"delete":["4","----------------","10","   and commit.","19","     sections.","24","Preparing a bug-fix-release","25","...........................","27","Since any commits to a released branch (e.g. 0.999.X) will automatically update","28","the web site documentation, it is best to develop a bug-fix release with a pull","29","request in which 0.999.X is the base. It also allows you to keep track of any","30","tasks towards release with a TO DO list.","32","Most development of the bug fix release, and its documentation, should","33","happen in master to avoid asynchrony. To select commits from master for use in","34","the bug fix (version 0.999.3), you can use::","37","    $ git rebase -i 0.999.X","39","Then pick the commits for release and resolve any issues, and create a pull","40","request with 0.999.X as base. Add a commit updating ``sklearn.__version__``.","41","Additional commits can be cherry-picked into the ``release-0.999.3`` branch","42","while preparing the release.","45","----------------","47","1. Update docs:","54","   - Update the release date in whats_new.rst","56","   - Edit the doc\/index.rst to change the 'News' entry of the front page.","57","","58","   - Note that these changes should be made in master and cherry-picked into","59","     the release branch.","62","   sklearn\/__init__.py, the ``__version__`` variable by removing ``dev*`` only","63","   when ready to release.","64","   On master, increment the version in the same place (when branching for","65","   release).","67","3. Create the tag and push it::","68","","69","    $ git tag -a 0.999","70","","71","    $ git push git@github.com:scikit-learn\/scikit-learn.git --tags","72","","73","4. Create the source tarball:","96","       $ rm -r dist","101","   along with the source tarball (\"scikit-learn-XXX.tar.gz\").","121","       $ sed -i \"s\/latestStable = '.*\/latestStable = '0.999';\" versionwarning.js","122","       $ git commit -m \"Update stable to point to 0.999\" stable","129","    * [ ] update dependencies and release tag at https:\/\/github.com\/MacPython\/scikit-learn-wheels","132","    * [ ] confirm bot detected at https:\/\/github.com\/conda-forge\/scikit-learn-feedstock and wait for merge","134","    * [ ] announce on mailing list","135","    * [ ] (regenerate Dash docs: https:\/\/github.com\/Kapeli\/Dash-User-Contributions\/tree\/master\/docsets\/Scikit)"]}]}},"a2ccc330bfabe8a878c5887066204fe3d366562f":{"changes":{"sklearn\/linear_model\/_logistic.py":"MODIFY","sklearn\/utils\/_testing.py":"MODIFY"},"diff":{"sklearn\/linear_model\/_logistic.py":[{"add":["731","            def func(x, *args): return _multinomial_loss_grad(x, *args)[0:2]","733","            def func(x, *args): return _multinomial_loss(x, *args)[0]","734","            def grad(x, *args): return _multinomial_loss_grad(x, *args)[1]","743","            def grad(x, *args): return _logistic_loss_and_grad(x, *args)[1]","1308","                raise ValueError(\"l1_ratio must be between 0 and 1;\"","1309","                                 \" got (l1_ratio=%r)\" % self.l1_ratio)"],"delete":["731","            func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]","733","            func = lambda x, *args: _multinomial_loss(x, *args)[0]","734","            grad = lambda x, *args: _multinomial_loss_grad(x, *args)[1]","743","            grad = lambda x, *args: _logistic_loss_and_grad(x, *args)[1]","1308","                        raise ValueError(\"l1_ratio must be between 0 and 1;\"","1309","                                         \" got (l1_ratio=%r)\" % self.l1_ratio)"]}],"sklearn\/utils\/_testing.py":[{"add":["185","                def check_in_message(msg): return message in msg"],"delete":["185","                check_in_message = lambda msg: message in msg"]}]}},"d7c375869ada53040f035f6fb0eb3b8d2d5dfff2":{"changes":{"sklearn\/neural_network\/_base.py":"MODIFY","doc\/whats_new\/v0.23.rst":"MODIFY","sklearn\/neural_network\/tests\/test_base.py":"ADD"},"diff":{"sklearn\/neural_network\/_base.py":[{"add":["214","    eps = np.finfo(y_prob.dtype).eps","215","    y_prob = np.clip(y_prob, eps, 1 - eps)","236","    y_prob : array-like of float, shape = (n_samples, 1)","245","    eps = np.finfo(y_prob.dtype).eps","246","    y_prob = np.clip(y_prob, eps, 1 - eps)"],"delete":["234","    y_prob : array-like of float, shape = (n_samples, n_classes)"]}],"doc\/whats_new\/v0.23.rst":[{"add":["147",":mod:`sklearn.neural_network`","148",".............................","149","","150","- |Fix| Increases the numerical stability of the logistic loss function in","151","  :class:`neural_network.MLPClassifier` by clipping the probabilities.","152","  :pr:`16117` by `Thomas Fan`_.","153",""],"delete":[]}],"sklearn\/neural_network\/tests\/test_base.py":[{"add":[],"delete":[]}]}},"1b1c869ef3d5443d2104a1ea444fa41b86d6e3a7":{"changes":{"sklearn\/utils\/validation.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY","sklearn\/utils\/tests\/test_validation.py":"MODIFY"},"diff":{"sklearn\/utils\/validation.py":[{"add":["456","        dtypes_orig = list(array.dtypes)","457","        # pandas boolean dtype __array__ interface coerces bools to objects","458","        for i, dtype_iter in enumerate(dtypes_orig):","459","            if dtype_iter.kind == 'b':","460","                dtypes_orig[i] = np.object","461","","463","            dtype_orig = np.result_type(*dtypes_orig)"],"delete":["456","        dtypes_orig = np.array(array.dtypes)","458","            dtype_orig = np.result_type(*array.dtypes)"]}],"doc\/whats_new\/v0.22.rst":[{"add":["4",".. _changes_0_22_1:","5","","6","Version 0.22.1","7","==============","8","","9","**In Development**","10","","11","This is a bug-fix release to primarily resolve some packaging issues in version","12","0.22.0. It also includes minor documentation improvements and some bug fixes.","13","","14","Changelog","15","---------","16","","17",":mod:`sklearn.utils`","18","....................","19","","20","- |Fix| :func:`utils.check_array` now correctly converts pandas DataFrame with","21","  boolean columns to floats. :pr:`15797` by `Thomas Fan`_.","22",""],"delete":[]}],"sklearn\/utils\/tests\/test_validation.py":[{"add":["828","def test_check_dataframe_mixed_float_dtypes():","829","    # pandas dataframe will coerce a boolean into a object, this is a mismatch","830","    # with np.result_type which will return a float","831","    # check_array needs to explicitly check for bool dtype in a dataframe for","832","    # this situation","833","    # https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/15787","834","","835","    pd = importorskip(\"pandas\")","836","    df = pd.DataFrame({","837","        'int': [1, 2, 3],","838","        'float': [0, 0.1, 2.1],","839","        'bool': [True, False, True]}, columns=['int', 'float', 'bool'])","840","","841","    array = check_array(df, dtype=(np.float64, np.float32, np.float16))","842","    expected_array = np.array(","843","        [[1.0, 0.0, 1.0],","844","         [2.0, 0.1, 0.0],","845","         [3.0, 2.1, 1.0]], dtype=np.float)","846","    assert_allclose_dense_sparse(array, expected_array)","847","","848",""],"delete":[]}]}},"641b8631884e2cb19a27fd30440a41a1437e4e40":{"changes":{"build_tools\/circle\/build_doc.sh":"MODIFY"},"diff":{"build_tools\/circle\/build_doc.sh":[{"add":["81","        scripts_names=\"$(echo \"$auto_example_files\" | sed 's\/sphx_glr_\/\/' | sed -E 's\/_([[:digit:]][[:digit:]][[:digit:]]|thumb).png\/.py\/')\""],"delete":["81","        scripts_names=\"$(echo \"$auto_example_files\" | sed 's\/sphx_glr_\/\/' | sed -e 's\/_([[:digit:]][[:digit:]][[:digit:]]|thumb).png\/.py\/')\""]}]}},"6464e15f61d2fdb904f8595f1971e4b71ddec3fb":{"changes":{"sklearn\/gaussian_process\/kernels.py":"MODIFY"},"diff":{"sklearn\/gaussian_process\/kernels.py":[{"add":["1098","    constant_value_bounds : pair of floats >= 0 or \"fixed\", default=(1e-5, 1e5)","1099","        The lower and upper bound on `constant_value`.","1100","        If set to \"fixed\", `constant_value` cannot be changed during","1101","        hyperparameter tuning.","1217","    noise_level_bounds : pair of floats >= 0 or \"fixed\", default=(1e-5, 1e5)","1218","        The lower and upper bound on 'noise_level'.","1219","        If set to \"fixed\", 'noise_level' cannot be changed during","1220","        hyperparameter tuning.","1346","    length_scale_bounds : pair of floats >= 0 or \"fixed\", default=(1e-5, 1e5)","1347","        The lower and upper bound on 'length_scale'.","1348","        If set to \"fixed\", 'length_scale' cannot be changed during","1349","        hyperparameter tuning.","1504","    length_scale_bounds : pair of floats >= 0 or \"fixed\", default=(1e-5, 1e5)","1505","        The lower and upper bound on 'length_scale'.","1506","        If set to \"fixed\", 'length_scale' cannot be changed during","1507","        hyperparameter tuning.","1689","    length_scale_bounds : pair of floats >= 0 or \"fixed\", default=(1e-5, 1e5)","1690","        The lower and upper bound on 'length_scale'.","1691","        If set to \"fixed\", 'length_scale' cannot be changed during","1692","        hyperparameter tuning.","1694","    alpha_bounds : pair of floats >= 0 or \"fixed\", default=(1e-5, 1e5)","1695","        The lower and upper bound on 'alpha'.","1696","        If set to \"fixed\", 'alpha' cannot be changed during","1697","        hyperparameter tuning.","1832","    length_scale : float > 0, default=1.0","1833","        The length scale of the kernel.","1835","    periodicity : float > 0, default=1.0","1836","        The periodicity of the kernel.","1838","    length_scale_bounds : pair of floats >= 0 or \"fixed\", default=(1e-5, 1e5)","1839","        The lower and upper bound on 'length_scale'.","1840","        If set to \"fixed\", 'length_scale' cannot be changed during","1841","        hyperparameter tuning.","1842","","1843","    periodicity_bounds : pair of floats >= 0 or \"fixed\", default=(1e-5, 1e5)","1844","        The lower and upper bound on 'periodicity'.","1845","        If set to \"fixed\", 'periodicity' cannot be changed during","1846","        hyperparameter tuning.","1980","    sigma_0_bounds : pair of floats >= 0 or \"fixed\", default=(1e-5, 1e5)","1981","        The lower and upper bound on 'sigma_0'.","1982","        If set to \"fixed\", 'sigma_0' cannot be changed during","1983","        hyperparameter tuning.","2121","    gamma_bounds : pair of floats >= 0 or \"fixed\", default=(1e-5, 1e5)","2122","        The lower and upper bound on 'gamma'.","2123","        If set to \"fixed\", 'gamma' cannot be changed during","2124","        hyperparameter tuning."],"delete":["1098","    constant_value_bounds : pair of floats >= 0, default=(1e-5, 1e5)","1099","        The lower and upper bound on constant_value","1100","","1216","    noise_level_bounds : pair of floats >= 0, default=(1e-5, 1e5)","1217","        The lower and upper bound on noise_level","1218","","1344","    length_scale_bounds : pair of floats >= 0, default=(1e-5, 1e5)","1345","        The lower and upper bound on length_scale","1500","    length_scale_bounds : pair of floats >= 0, default=(1e-5, 1e5)","1501","        The lower and upper bound on length_scale","1683","    length_scale_bounds : pair of floats >= 0, default=(1e-5, 1e5)","1684","        The lower and upper bound on length_scale","1686","    alpha_bounds : pair of floats >= 0, default=(1e-5, 1e5)","1687","        The lower and upper bound on alpha","1821","    length_scale : float, default=1.0","1822","        The length scale of the kernel. It should be strictly positive.","1824","    periodicity : float, default=1.0","1825","        The periodicity of the kernel. It should be strictly positive.","1827","    length_scale_bounds : pair of floats >= 0, default=(1e-5, 1e5)","1828","        The lower and upper bound on length scale.","1830","    periodicity_bounds : pair of floats >= 0, default=(1e-5, 1e5)","1831","        The lower and upper bound on periodicity.","1965","    sigma_0_bounds : pair of floats >= 0, default=(1e-5, 1e5)","1966","        The lower and upper bound on l.","2104","    gamma_bounds : pair of floats, default=(1e-5, 1e5)","2105","        The lower and upper bound on gamma. They should be positive."]}]}},"9c62eee695cdcd75c6fd23d02334b8d0c241ddde":{"changes":{"sklearn\/tree\/tests\/test_export.py":"MODIFY","doc\/whats_new\/v0.23.rst":"MODIFY","sklearn\/tree\/_export.py":"MODIFY"},"diff":{"sklearn\/tree\/tests\/test_export.py":[{"add":["450","","451","","452","# FIXME: to be removed in 0.25","453","def test_plot_tree_rotate_deprecation(pyplot):","454","    tree = DecisionTreeClassifier()","455","    tree.fit(X, y)","456","    # test that a warning is raised when rotate is used.","457","    match = (\"'rotate' has no effect and is deprecated in 0.23. \"","458","             \"It will be removed in 0.25.\")","459","    with pytest.warns(FutureWarning, match=match):","460","        plot_tree(tree, rotate=True)"],"delete":[]}],"doc\/whats_new\/v0.23.rst":[{"add":["58","","59",":mod:`sklearn.tree`","60","...................","61","","62","- |Fix| :func:`tree.plot_tree` `rotate` parameter was unused and has been","63","  deprecated.","64","  :pr:`15806` by :user:`Chiara Marmo <cmarmo>`.","65",""],"delete":[]}],"sklearn\/tree\/_export.py":[{"add":["26","import warnings","81","              proportion=False, rotate='deprecated', rounded=False,","134","        This parameter has no effect on the matplotlib tree visualisation and","135","        it is kept here for backward compatibility.","136","","137","        .. deprecated:: 0.23","138","           ``rotate`` is deprecated in 0.23 and will be removed in 0.25.","139","","175","","176","    if rotate != 'deprecated':","177","        warnings.warn((\"'rotate' has no effect and is deprecated in 0.23. \"","178","                       \"It will be removed in 0.25.\"),","179","                      FutureWarning)","180",""],"delete":["80","              proportion=False, rotate=False, rounded=False,","133","        When set to ``True``, orient tree left to right rather than top-down."]}]}},"ffc7a473fd978e5b2f6f7a2ea65f4a66001f6f2a":{"changes":{"sklearn\/dummy.py":"MODIFY"},"diff":{"sklearn\/dummy.py":[{"add":["15","from .utils.validation import check_is_fitted, _check_sample_weight","147","        if sample_weight is not None:","148","            sample_weight = _check_sample_weight(sample_weight, X)","149","","480","","482","            sample_weight = _check_sample_weight(sample_weight, X)"],"delete":["15","from .utils.validation import check_is_fitted","478","            sample_weight = np.asarray(sample_weight)"]}]}},"26d392aa4ce79d7bbc4aa09eafbbac0620c95fa7":{"changes":{"sklearn\/manifold\/tests\/test_mds.py":"MODIFY"},"diff":{"sklearn\/manifold\/tests\/test_mds.py":[{"add":["4","from sklearn.manifold import _mds as mds"],"delete":["4","from sklearn.manifold import mds"]}]}},"0831dfbe9595e54587f92dcafd30d030dcfcc631":{"changes":{"doc\/whats_new\/v0.22.rst":"MODIFY","sklearn\/cross_decomposition\/_pls.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.22.rst":[{"add":["103","- :class:`cross_decomposition.CCA` when using scipy >= 1.3 |Fix|","212","- |Fix| :class:`cross_decomposition.CCA` now produces the same results with ","213","  scipy 1.3 and previous scipy versions. :pr:`15661` by `Thomas Fan`_.","214",""],"delete":[]}],"sklearn\/cross_decomposition\/_pls.py":[{"add":["42","","43","    if mode == \"B\":","44","        # Uses condition from scipy<1.3 in pinv2 which was changed in","45","        # https:\/\/github.com\/scipy\/scipy\/pull\/10067. In scipy 1.3, the","46","        # condition was changed to depend on the largest singular value","47","        X_t = X.dtype.char.lower()","48","        Y_t = Y.dtype.char.lower()","49","        factor = {'f': 1E3, 'd': 1E6}","50","","51","        cond_X = factor[X_t] * eps","52","        cond_Y = factor[Y_t] * eps","53","","61","                X_pinv = pinv2(X, check_finite=False, cond=cond_X)","78","                # compute once pinv(Y)","79","                Y_pinv = pinv2(Y, check_finite=False, cond=cond_Y)"],"delete":["49","                X_pinv = pinv2(X, check_finite=False)","66","                Y_pinv = pinv2(Y, check_finite=False)  # compute once pinv(Y)"]}]}},"91261c2013795a319f3075661827f631e2f883cb":{"changes":{"sklearn\/cluster\/tests\/test_k_means.py":"MODIFY","doc\/whats_new\/v0.23.rst":"MODIFY","sklearn\/cluster\/_k_means_elkan.pyx":"MODIFY"},"diff":{"sklearn\/cluster\/tests\/test_k_means.py":[{"add":["71","@pytest.mark.parametrize('tol', [0, 1e-2, 1e-4, 1e-8])"],"delete":["71","@pytest.mark.parametrize('tol', [1e-2, 1e-4, 1e-8])"]}],"doc\/whats_new\/v0.23.rst":[{"add":["52","- |Fix| :class:`cluster.KMeans` with ``algorithm=\"elkan\"`` now converges with","53","  ``tol=0`` as with the default ``algorithm=\"full\"``. :pr:`16075` by","54","  :user:`Erich Schubert <kno10>`."],"delete":[]}],"sklearn\/cluster\/_k_means_elkan.pyx":[{"add":["247","        if center_shift_total <= tol:"],"delete":["247","        if center_shift_total < tol:"]}]}},"c809b8d1d083041c260d92ba8a68398801047a80":{"changes":{"sklearn\/cluster\/_affinity_propagation.py":"MODIFY","sklearn\/cluster\/tests\/test_affinity_propagation.py":"MODIFY"},"diff":{"sklearn\/cluster\/_affinity_propagation.py":[{"add":["196","                never_converged = False","201","        never_converged = True","208","    if K > 0 and not never_converged:","412","        X = check_array(X)"],"delete":["206","    if K > 0:"]}],"sklearn\/cluster\/tests\/test_affinity_propagation.py":[{"add":["154","def test_affinity_propagation_non_convergence_regressiontest():","155","    X = np.array([[1, 0, 0, 0, 0, 0],","156","                  [0, 1, 1, 1, 0, 0],","157","                  [0, 0, 1, 0, 0, 1]])","158","    af = AffinityPropagation(affinity='euclidean', max_iter=2).fit(X)","159","    assert_array_equal(np.array([-1, -1, -1]), af.labels_)","160","","161",""],"delete":[]}]}},"e05b9e146006236aa41349105e8a01ac9535fba7":{"changes":{".github\/ISSUE_TEMPLATE\/bug_report.md":"MODIFY"},"diff":{".github\/ISSUE_TEMPLATE\/bug_report.md":[{"add":["9","<!--","12","-->"],"delete":[]}]}},"5e2d74bc5f61b382758c0403c577253539e77156":{"changes":{"sklearn\/model_selection\/_validation.py":"MODIFY","sklearn\/multioutput.py":"MODIFY","sklearn\/neighbors\/_nearest_centroid.py":"MODIFY","sklearn\/decomposition\/_lda.py":"MODIFY","sklearn\/feature_extraction\/_hash.py":"MODIFY","sklearn\/multiclass.py":"MODIFY","sklearn\/model_selection\/_search.py":"MODIFY"},"diff":{"sklearn\/model_selection\/_validation.py":[{"add":["136","        .. versionadded:: 0.19","137","","138","        .. versionchanged:: 0.21","139","            Default value was changed from ``True`` to ``False``","140",""],"delete":[]}],"sklearn\/multioutput.py":[{"add":["364","","365","            .. versionchanged:: 0.19","366","                This function now returns a list of arrays where the length of","367","                the list is ``n_outputs``, and each array is (``n_samples``,","368","                ``n_classes``) for that particular output."],"delete":[]}],"sklearn\/neighbors\/_nearest_centroid.py":[{"add":["43","        .. versionchanged:: 0.19","44","            ``metric='precomputed'`` was deprecated and now raises an error","45",""],"delete":[]}],"sklearn\/decomposition\/_lda.py":[{"add":["145","        .. versionchanged:: 0.19","146","            ``n_topics `` was renamed to ``n_components``","147",""],"delete":[]}],"sklearn\/feature_extraction\/_hash.py":[{"add":["71","    .. versionchanged:: 0.19","72","        ``alternate_sign`` replaces the now deprecated ``non_negative``","73","        parameter.","74",""],"delete":[]}],"sklearn\/multiclass.py":[{"add":["392","","393","            .. versionchanged:: 0.19","394","                output shape changed to ``(n_samples,)`` to conform to","395","                scikit-learn conventions for binary classification.","649","","650","            .. versionchanged:: 0.19","651","                output shape changed to ``(n_samples,)`` to conform to","652","                scikit-learn conventions for binary classification."],"delete":[]}],"sklearn\/model_selection\/_search.py":[{"add":["1004","        .. versionadded:: 0.19","1005","","1006","        .. versionchanged:: 0.21","1007","            Default value was changed from ``True`` to ``False``","1008","","1345","        .. versionadded:: 0.19","1346","","1347","        .. versionchanged:: 0.21","1348","            Default value was changed from ``True`` to ``False``","1349",""],"delete":[]}]}},"a203b9e1c6e0ec4f09aaddb4af5010592ea266a3":{"changes":{"sklearn\/utils\/estimator_checks.py":"MODIFY","sklearn\/tests\/test_common.py":"MODIFY"},"diff":{"sklearn\/utils\/estimator_checks.py":[{"add":["362","    if isinstance(estimator, type):","363","        # try to construct estimator to get tags, if it is unable to then","364","        # return the estimator class","365","        try:","366","            xfail_checks = _safe_tags(_construct_instance(estimator),","367","                                      '_xfail_test')","368","        except Exception:","369","            return estimator, check","370","    else:","371","        xfail_checks = _safe_tags(estimator, '_xfail_test')"],"delete":["363","    xfail_checks = _safe_tags(estimator, '_xfail_test')"]}],"sklearn\/tests\/test_common.py":[{"add":["33","    _mark_xfail_checks,","50","def test_estimator_cls_parameterize_with_checks():","51","    # Non-regression test for #16707 to ensure that parametrize_with_checks","52","    # works with estimator classes","53","    param_checks = parametrize_with_checks([LogisticRegression])","54","    # Using the generator does not raise","55","    list(param_checks.args[1])","56","","57","","58","def test_mark_xfail_checks_with_unconsructable_estimator():","59","    class MyEstimator:","60","        def __init__(self):","61","            raise ValueError(\"This is bad\")","62","","63","    estimator, check = _mark_xfail_checks(MyEstimator, 42, None)","64","    assert estimator == MyEstimator","65","    assert check == 42","66","","67",""],"delete":[]}]}},"a07974b3c05f5f5b957aea99fe8c199e29485f5b":{"changes":{"doc\/whats_new\/v0.23.rst":"MODIFY","sklearn\/linear_model\/tests\/test_ridge.py":"MODIFY","sklearn\/linear_model\/_ridge.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.23.rst":[{"add":["83","- |Fix| add `best_score_` attribute to :class:`linear_model.RidgeCV` and","84","  :class:`linear_model.RidgeClassifierCV`.","85","  :pr:`15653` by :user:`Jr?me Docks <jeromedockes>`.","86",""],"delete":[]}],"sklearn\/linear_model\/tests\/test_ridge.py":[{"add":["666","    [(RidgeCV(store_cv_values=False), make_regression),","667","     (RidgeClassifierCV(store_cv_values=False), make_classification)]","676","@pytest.mark.parametrize(","677","    \"ridge, make_dataset\",","678","    [(RidgeCV(), make_regression),","679","     (RidgeClassifierCV(), make_classification)]","680",")","681","@pytest.mark.parametrize(\"cv\", [None, 3])","682","def test_ridge_best_score(ridge, make_dataset, cv):","683","    # check that the best_score_ is store","684","    X, y = make_dataset(n_samples=6, random_state=42)","685","    ridge.set_params(store_cv_values=False, cv=cv)","686","    ridge.fit(X, y)","687","    assert hasattr(ridge, \"best_score_\")","688","    assert isinstance(ridge.best_score_, float)","689","","690",""],"delete":["666","    [(RidgeCV(), make_regression),","667","     (RidgeClassifierCV(), make_classification)]","672","    ridge.set_params(store_cv_values=False)"]}],"sklearn\/linear_model\/_ridge.py":[{"add":["1578","            self.best_score_ = estimator.best_score_","1594","            self.best_score_ = gs.best_score_","1697","    best_score_ : float","1698","        Mean cross-validated score of the estimator with the best alpha found.","1699","","1802","        Estimated regularization parameter.","1803","","1804","    best_score_ : float","1805","        Mean cross-validated score of the estimator with the best alpha found."],"delete":["1797","        Estimated regularization parameter"]}]}},"0d3de54c688a19ebe147ca7c6d78fa717de77a71":{"changes":{"sklearn\/decomposition\/__init__.py":"MODIFY","sklearn\/utils\/tests\/test_deprecated_utils.py":"MODIFY","sklearn\/inspection\/__init__.py":"MODIFY","sklearn\/tests\/test_import_deprecations.py":"MODIFY"},"diff":{"sklearn\/decomposition\/__init__.py":[{"add":["6","# TODO: remove me in 0.24 (as well as the noqa markers) and","7","# import the dict_learning func directly from the ._dict_learning","8","# module instead.","9","# Pre-cache the import of the deprecated module so that import","10","# sklearn.decomposition.dict_learning returns the function as in","11","# 0.21, instead of the module.","12","# https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/15842","13","import warnings","14","with warnings.catch_warnings():","15","    warnings.simplefilter(\"ignore\", category=FutureWarning)","16","    from .dict_learning import dict_learning","17","","18","","19","from ._nmf import NMF, non_negative_factorization  # noqa","20","from ._pca import PCA  # noqa","21","from ._incremental_pca import IncrementalPCA  # noqa","22","from ._kernel_pca import KernelPCA  # noqa","23","from ._sparse_pca import SparsePCA, MiniBatchSparsePCA  # noqa","24","from ._truncated_svd import TruncatedSVD  # noqa","25","from ._fastica import FastICA, fastica  # noqa","26","from ._dict_learning import (dict_learning_online,","28","                             MiniBatchDictionaryLearning, SparseCoder)  # noqa","29","from ._factor_analysis import FactorAnalysis  # noqa","30","from ..utils.extmath import randomized_svd  # noqa","31","from ._lda import LatentDirichletAllocation  # noqa","32",""],"delete":["6","from ._nmf import NMF, non_negative_factorization","7","from ._pca import PCA","8","from ._incremental_pca import IncrementalPCA","9","from ._kernel_pca import KernelPCA","10","from ._sparse_pca import SparsePCA, MiniBatchSparsePCA","11","from ._truncated_svd import TruncatedSVD","12","from ._fastica import FastICA, fastica","13","from ._dict_learning import (dict_learning, dict_learning_online,","15","                             MiniBatchDictionaryLearning, SparseCoder)","16","from ._factor_analysis import FactorAnalysis","17","from ..utils.extmath import randomized_svd","18","from ._lda import LatentDirichletAllocation"]}],"sklearn\/utils\/tests\/test_deprecated_utils.py":[{"add":["1","import types","3","import warnings","6","from sklearn.utils import all_estimators","99","","100","","101","# TODO: remove in 0.24","102","def test_partial_dependence_no_shadowing():","103","    # Non-regression test for:","104","    # https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/15842","105","    with warnings.catch_warnings():","106","        warnings.simplefilter(\"ignore\", category=FutureWarning)","107","        from sklearn.inspection.partial_dependence import partial_dependence as _  # noqa","108","","109","        # Calling all_estimators() also triggers a recursive import of all","110","        # submodules, including deprecated ones.","111","        all_estimators()","112","","113","    from sklearn.inspection import partial_dependence","114","    assert isinstance(partial_dependence, types.FunctionType)","115","","116","","117","# TODO: remove in 0.24","118","def test_dict_learning_no_shadowing():","119","    # Non-regression test for:","120","    # https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/15842","121","    with warnings.catch_warnings():","122","        warnings.simplefilter(\"ignore\", category=FutureWarning)","123","        from sklearn.decomposition.dict_learning import dict_learning as _  # noqa","124","","125","        # Calling all_estimators() also triggers a recursive import of all","126","        # submodules, including deprecated ones.","127","        all_estimators()","128","","129","    from sklearn.decomposition import dict_learning","130","    assert isinstance(dict_learning, types.FunctionType)"],"delete":[]}],"sklearn\/inspection\/__init__.py":[{"add":["1","","2","# TODO: remove me in 0.24 (as well as the noqa markers) and","3","# import the partial_dependence func directly from the","4","# ._partial_dependence module instead.","5","# Pre-cache the import of the deprecated module so that import","6","# sklearn.inspection.partial_dependence returns the function as in","7","# 0.21, instead of the module","8","# https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/15842","9","import warnings","10","with warnings.catch_warnings():","11","    warnings.simplefilter(\"ignore\", category=FutureWarning)","12","    from .partial_dependence import partial_dependence","13","","14","from ._partial_dependence import plot_partial_dependence  # noqa","15","from ._partial_dependence import PartialDependenceDisplay  # noqa","16","from ._permutation_importance import permutation_importance  # noqa","17",""],"delete":["1","from ._partial_dependence import partial_dependence","2","from ._partial_dependence import plot_partial_dependence","3","from ._partial_dependence import PartialDependenceDisplay","4","from ._permutation_importance import permutation_importance"]}],"sklearn\/tests\/test_import_deprecations.py":[{"add":["26","    # Special case for:","27","    # https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/15842","28","    if deprecated_path in (\"sklearn.decomposition.dict_learning\",","29","                           \"sklearn.inspection.partial_dependence\"):","30","        pytest.skip(\"No warning can be raised for \" + deprecated_path)","31",""],"delete":[]}]}},"6d2b7bc6da26249cfcc99a45ee78a76b459fc3da":{"changes":{"sklearn\/datasets\/descr\/linnerud.rst":"MODIFY"},"diff":{"sklearn\/datasets\/descr\/linnerud.rst":[{"add":["13","- *physiological* - CSV containing 20 observations on 3 physiological variables:","16","- *exercise* - CSV containing 20 observations on 3 exercise variables:"],"delete":["13","- *physiological* - CSV containing 20 observations on 3 exercise variables:","16","- *exercise* - CSV containing 20 observations on 3 physiological variables:"]}]}},"406184ef9ca0d526780b6a250eb7fa92966e52e5":{"changes":{"sklearn\/isotonic.py":"MODIFY","doc\/modules\/isotonic.rst":"MODIFY"},"diff":{"sklearn\/isotonic.py":[{"add":["79","    \"\"\"Solve the isotonic regression model.","92","    y_min : float, default=None","93","        Lower bound on the lowest predicted value (the minimum value may","94","        still be higher). If not set, defaults to -inf.","96","    y_max : float, default=None","97","        Upper bound on the highest predicted value (the maximum may still be","98","        lower). If not set, defaults to +inf.","141","        Lower bound on the lowest predicted value (the minimum value may","142","        still be higher). If not set, defaults to -inf.","145","        Upper bound on the highest predicted value (the maximum may still be","146","        lower). If not set, defaults to +inf.","148","    increasing : bool or 'auto', default=True","149","        Determines whether the predictions should be constrained to increase","150","        or decrease with `X`. 'auto' will decide based on the Spearman","151","        correlation estimate's sign.","154","        The ``out_of_bounds`` parameter handles how `X` values outside of the","155","        training domain are handled.  When set to \"nan\", predictions","156","        will be NaN.  When set to \"clip\", predictions will be","158","        When set to \"raise\" a `ValueError` is raised.","330","        y_pred : ndarray of shape (n_samples,)","370","        y_pred : ndarray of shape (n_samples,)"],"delete":["79","    \"\"\"Solve the isotonic regression model::","80","","81","        min sum w[i] (y[i] - y_[i]) ** 2","82","","83","        subject to y_min = y_[1] <= y_[2] ... <= y_[n] = y_max","84","","85","    where:","86","        - y[i] are inputs (real numbers)","87","        - y_[i] are fitted","88","        - w[i] are optional strictly positive weights (default to 1.0)","101","    y_min : optional, default: None","102","        If not None, set the lowest value of the fit to y_min.","104","    y_max : optional, default: None","105","        If not None, set the highest value of the fit to y_max.","141","    The isotonic regression optimization problem is defined by::","142","","143","        min sum w_i (y[i] - y_[i]) ** 2","144","","145","        subject to y_[i] <= y_[j] whenever X[i] <= X[j]","146","        and min(y_) = y_min, max(y_) = y_max","147","","148","    where:","149","        - ``y[i]`` are inputs (real numbers)","150","        - ``y_[i]`` are fitted","151","        - ``X`` specifies the order.","152","          If ``X`` is non-decreasing then ``y_`` is non-decreasing.","153","        - ``w[i]`` are optional strictly positive weights (default to 1.0)","154","","162","        If not None, set the lowest value of the fit to y_min.","165","        If not None, set the highest value of the fit to y_max.","167","    increasing : bool or string, default=True","168","        If boolean, whether or not to fit the isotonic regression with y","169","        increasing or decreasing.","170","","171","        The string value \"auto\" determines whether y should","172","        increase or decrease based on the Spearman correlation estimate's","173","        sign.","176","        The ``out_of_bounds`` parameter handles how x-values outside of the","177","        training domain are handled.  When set to \"nan\", predicted y-values","178","        will be NaN.  When set to \"clip\", predicted y-values will be","180","        When set to \"raise\", allow ``interp1d`` to throw ValueError.","352","        T_ : array, shape=(n_samples,)","392","        T_ : array, shape=(n_samples,)"]}],"doc\/modules\/isotonic.rst":[{"add":["8","The class :class:`IsotonicRegression` fits a non-decreasing real function to","9","1-dimensional data. It solves the following problem:","13","  subject to :math:`\\hat{y}_i \\le \\hat{y}_j` whenever :math:`X_i \\le X_j`,","15","where the weights :math:`w_i` are strictly positive, and both `X` and `y` are","16","arbitrary real quantities.","17","","18","The `increasing` parameter changes the constraint to","19",":math:`\\hat{y}_i \\ge \\hat{y}_j` whenever :math:`X_i \\le X_j`. Setting it to","20","'auto' will automatically choose the constraint based on `Spearman's rank","21","correlation coefficient","22","<https:\/\/en.wikipedia.org\/wiki\/Spearman%27s_rank_correlation_coefficient>`_.","23","","24",":class:`IsotonicRegression` produces a series of predictions","25",":math:`\\hat{y}_i` for the training data which are the closest to the targets","26",":math:`y` in terms of mean squared error. These predictions are interpolated","27","for predicting to unseen data. The predictions of :class:`IsotonicRegression`","28","thus form a function that is piecewise linear:"],"delete":["8","The class :class:`IsotonicRegression` fits a non-decreasing function to data.","9","It solves the following problem:","13","  subject to :math:`\\hat{y}_{min} = \\hat{y}_1 \\le \\hat{y}_2 ... \\le \\hat{y}_n = \\hat{y}_{max}`","15","where each :math:`w_i` is strictly positive and each :math:`y_i` is an","16","arbitrary real number. It yields the vector which is composed of non-decreasing","17","elements the closest in terms of mean squared error. In practice this list","18","of elements forms a function that is piecewise linear."]}]}},"ccac637855411169a256eb095302cff446b342be":{"changes":{"sklearn\/metrics\/pairwise.py":"MODIFY","sklearn\/metrics\/tests\/test_pairwise.py":"MODIFY"},"diff":{"sklearn\/metrics\/pairwise.py":[{"add":["408","    np.clip(distances, 0, None, out=distances)","409",""],"delete":[]}],"sklearn\/metrics\/tests\/test_pairwise.py":[{"add":["873","@pytest.mark.parametrize(\"missing_value\", [np.nan, -1])","874","def test_nan_euclidean_distances_one_feature_match_positive(missing_value):","875","    # First feature is the only feature that is non-nan and in both","876","    # samples. The result of `nan_euclidean_distances` with squared=True","877","    # should be non-negative. The non-squared version should all be close to 0.","878","    X = np.array([[-122.27, 648., missing_value, 37.85],","879","                  [-122.27, missing_value, 2.34701493, missing_value]])","880","","881","    dist_squared = nan_euclidean_distances(X, missing_values=missing_value,","882","                                           squared=True)","883","    assert np.all(dist_squared >= 0)","884","","885","    dist = nan_euclidean_distances(X, missing_values=missing_value,","886","                                   squared=False)","887","    assert_allclose(dist, 0.0)","888","","889",""],"delete":[]}]}},"4f97facc3a992c6e2459c3da86c9d69b0688d5ab":{"changes":{"doc\/whats_new\/_contributors.rst":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY"},"diff":{"doc\/whats_new\/_contributors.rst":[{"add":["177","","178",".. _Guillaume Lemaitre: https:\/\/github.com\/glemaitre"],"delete":[]}],"doc\/whats_new\/v0.22.rst":[{"add":["184","  :pr:`14510` by `Guillaume Lemaitre`_.","199","- |Feature| :class:`cross_decomposition.PLSCanonical` and","200","  :class:`cross_decomposition.PLSRegression` have a new function","201","  ``inverse_transform`` to transform data to the original space.","202","  :pr:`15304` by :user:`Jaime Ferrando Huertas <jiwidi>`.","203","","216","- |Fix| :class:`cross_decomposition.CCA` now produces the same results with","236","- |Enhancement| The parameter `normalize` was added to","237","   :func:`datasets.fetch_20newsgroups_vectorized`.","238","   :pr:`14740` by :user:`Stphan Tulkens <stephantul>`","239","","247","- |Efficiency| :class:`decomposition.NMF(solver='mu')` fitted on sparse input","248","  matrices now uses batching to avoid briefly allocating an array with size","249","  (#non-zero elements, n_components). :pr:`15257` by `Mart Willocx <Maocx>`_.","250","","275","- |Fix| :class:`dummy.DummyClassifier` now handles checking the existence","276","  of the provided constant in multiouput cases.","277","  :pr:`14908` by :user:`Martina G. Vilas <martinagvilas>`.","278","","307","  - |Feature| :func:`inspection.partial_dependence` and","308","    :func:`inspection.plot_partial_dependence` now support the fast 'recursion'","309","    method for both estimators. :pr:`13769` by `Nicolas Hug`_.","319","    :pr:`14710` by `Guillaume Lemaitre`_.","326","- |Enhancement| Addition of ``max_samples`` argument allows limiting","327","  size of bootstrap samples to be less than size of dataset. Added to","328","  :class:`ensemble.ForestClassifier`,","329","  :class:`ensemble.ForestRegressor`,","330","  :class:`ensemble.RandomForestClassifier`,","331","  :class:`ensemble.RandomForestRegressor`,","332","  :class:`ensemble.ExtraTreesClassifier`,","333","  :class:`ensemble.ExtraTreesRegressor`,","334","  :class:`ensemble.RandomTreesEmbedding`. :pr:`14682` by","335","  :user:`Matt Hancock <notmatthancock>` and","336","  :pr:`5963` by :user:`Pablo Duboue <DrDub>`.","337","","353","  :pr:`14305` by `Guillaume Lemaitre`_.","358","  :pr:`14114` by `Guillaume Lemaitre`_.","359","","360","- |Fix| Stacking and Voting estimators now ensure that their underlying","361","  estimators are either all classifiers or all regressors.","362","  :class:`ensemble.StackingClassifier`, :class:`ensemble.StackingRegressor`,","363","  and :class:`ensemble.VotingClassifier` and :class:`VotingRegressor`","364","  now raise consistent error messages.","365","  :pr:`15084` by `Guillaume Lemaitre`_.","366","","367","- |Fix| :class:`ensemble.AdaBoostRegressor` where the loss should be normalized","368","  by the max of the samples with non-null weights only.","369","  :pr:`14294` by `Guillaume Lemaitre`_.","400","- |API| Deprecated unused `copy` param for","401","  :meth:`feature_extraction.text.TfidfVectorizer.transform` it will be","402","  removed in v0.24. :pr:`14520` by","403","  :user:`Guillem G. Subies <guillemgsubies>`.","404","","440","- |API| From version 0.24 :meth:`gaussian_process.kernels.Kernel.get_params` will raise an","441","  ``AttributeError`` rather than return ``None`` for parameters that are in the","442","  estimator's constructor but not stored as attributes on the instance.","443","  :pr:`14464` by `Joel Nothman`_.","444","","450","  `Thomas Fan`_ and :pr:`15010` by `Guillaume Lemaitre`_.","460","- |Fix| :class:`impute.IterativeImputer` now works when there is only one feature.","461","  By :user:`Sergey Feldman <sergeyf>`.","462","","489","  :pr:`14028` and :pr:`15429` by `Guillaume Lemaitre`_.","501","- |Efficiency| The 'liblinear' logistic regression solver is now faster and","502","  requires less memory.","503","  :pr:`14108`, :pr:`14170`, :pr:`14296` by :user:`Alex Henrie <alexhenrie>`.","504","","517","  :pr:`14458` by `Guillaume Lemaitre`_.","536","  :pr:`15038` by `Guillaume Lemaitre`_.","566","- |API| Deprecate ``training_data_`` unused attribute in","567","  :class:`manifold.Isomap`. :issue:`10482` by `Tom Dupre la Tour`_.","568","","572","- |MajorFeature| :func:`metrics.plot_roc_curve` has been added to plot roc","573","  curves. This function introduces the visualization API described in","574","  the :ref:`User Guide <visualizations>`. :pr:`14357` by `Thomas Fan`_.","575","","613","- |Efficiency| Improved performance of","614","  :func:`metrics.pairwise.manhattan_distances` in the case of sparse matrices.","615","  :pr:`15049` by `Paolo Toccaceli <ptocca>`.","616","","646","- |Fix| Raise a ValueError in :func:`metrics.silhouette_score` when a","647","  precomputed distance matrix contains non-zero diagonal entries.","648","  :pr:`12258` by :user:`Stephen Tierney <sjtrny>`.","649","","650","- |API| ``scoring=\"neg_brier_score\"`` should be used instead of","651","  ``scoring=\"brier_score_loss\"`` which is now deprecated.","652","  :pr:`14898` by :user:`Stefan Matcovici <stefan-matcovici>`.","653","","674","- |Fix| The `cv_results_` attribute of :class:`model_selection.GridSearchCV`","675","  and :class:`model_selection.RandomizedSearchCV` now only contains unfitted","676","  estimators. This potentially saves a lot of memory since the state of the","677","  estimators isn't stored. :pr:`#15096` by `Andreas Mller`_.","678","","750","- |Fix| The `fit` in :class:`~pipeline.FeatureUnion` now accepts `fit_params`","751","  to pass to the underlying transformers. :pr:`15119` by `Adrin Jalali`_.","752","","813","  :pr:`15038` by `Guillaume Lemaitre`_.","853","- |Feature| A new random variable, :class:`utils.fixes.loguniform` implements a","864","  by `Guillaume Lemaitre`_.","886","  - ``utils.choose_check_classifiers_labels``","887","  - ``utils.enforce_estimator_tags_y``","888","  - ``utils.optimize.newton_cg``","889","  - ``utils.random.random_choice_csc``","890","  - ``utils.safe_indexing``","891","  - ``utils.mocking``","892","  - ``utils.fast_dict``","893","  - ``utils.seq_dataset``","894","  - ``utils.weight_vector``","895","  - ``utils.fixes.parallel_helper`` (removed)","896","  - All of ``utils.testing`` except for ``all_estimators`` which is now in","897","    ``utils``.","909","- |Fix| Port `lobpcg` from SciPy which implement some bug fixes but only","910","  available in 1.3+.","911","  :pr:`13609` and :pr:`14971` by `Guillaume Lemaitre`_.","912","","959","  to be overridable only once. :pr:`14884` by `Andreas Mller`_."],"delete":["184","  :pr:`14510` by :user:`Guillaume Lemaitre <glemaitre>`.","211","- |Feature| :class:`cross_decomposition.PLSCanonical` and","212","  :class:`cross_decomposition.PLSRegression` have a new function","213","  ``inverse_transform`` to transform data to the original space`.","214","  :pr:`15304` by :user:`Jaime Ferrando Huertas <jiwidi>`.","215","","216","- |Fix| :class:`cross_decomposition.CCA` now produces the same results with ","240"," - |Enhancement| The parameter `normalize` was added to","241","   :func:`datasets.fetch_20newsgroups_vectorized`.","242","   :pr:`14740` by :user:`Stphan Tulkens <stephantul>`","243","","268","- |Efficiency| :class:`decomposition.NMF(solver='mu')` fitted on sparse input","269","  matrices now uses batching to avoid briefly allocating an array with size","270","  (#non-zero elements, n_components). :pr:`15257` by `Mart Willocx <Maocx>`_.","271","","280","- |Fix| :class:`dummy.DummyClassifier` now handles checking the existence","281","  of the provided constant in multiouput cases.","282","  :pr:`14908` by :user:`Martina G. Vilas <martinagvilas>`.","283","","311","  - |Feature| :func:`inspection.partial_dependence` and","312","    :func:`inspection.plot_partial_dependence` now support the fast 'recursion'","313","    method for both estimators. :pr:`13769` by `Nicolas Hug`_.","319","    :pr:`14710` by :user:`Guillaume Lemaitre <glemaitre>`.","341","  :pr:`14305` by :user:`Guillaume Lemaitre <glemaitre>`.","346","  :pr:`14114` by :user:`Guillaume Lemaitre <glemaitre>`.","355","- |Enhancement| Addition of ``max_samples`` argument allows limiting","356","  size of bootstrap samples to be less than size of dataset. Added to","357","  :class:`ensemble.ForestClassifier`,","358","  :class:`ensemble.ForestRegressor`,","359","  :class:`ensemble.RandomForestClassifier`,","360","  :class:`ensemble.RandomForestRegressor`,","361","  :class:`ensemble.ExtraTreesClassifier`,","362","  :class:`ensemble.ExtraTreesRegressor`,","363","  :class:`ensemble.RandomTreesEmbedding`. :pr:`14682` by","364","  :user:`Matt Hancock <notmatthancock>` and","365","  :pr:`5963` by :user:`Pablo Duboue <DrDub>`.","366","","367","- |Fix| Stacking and Voting estimators now ensure that their underlying","368","  estimators are either all classifiers or all regressors.","369","  :class:`ensemble.StackingClassifier`, :class:`ensemble.StackingRegressor`,","370","  and :class:`ensemble.VotingClassifier` and :class:`VotingRegressor`","371","  now raise consistent error messages.","372","  :pr:`15084` by :user:`Guillaume Lemaitre <glemaitre>`.","373","","374","- |Fix| :class:`ensemble.AdaBoostRegressor` where the loss should be normalized","375","  by the max of the samples with non-null weights only.","376","  :pr:`14294` by :user:`Guillaume Lemaitre <glemaitre>`.","377","","392","- |API| Deprecated unused `copy` param for","393","  :meth:`feature_extraction.text.TfidfVectorizer.transform` it will be","394","  removed in v0.24. :pr:`14520` by","395","  :user:`Guillem G. Subies <guillemgsubies>`.","396","","431","- |API| From version 0.24 :meth:`gaussian_process.kernels.Kernel.get_params` will raise an","432","  ``AttributeError`` rather than return ``None`` for parameters that are in the","433","  estimator's constructor but not stored as attributes on the instance.","434","  :pr:`14464` by `Joel Nothman`_.","435","","450","  `Thomas Fan`_.","451","","452","- |Enhancement| Adds parameter `add_indicator` to :class:`impute.KNNImputer`","453","  to get indicator of missing data.","454","  :pr:`15010` by :user:`Guillaume Lemaitre <glemaitre>`.","461","- |Fix| :class:`impute.IterativeImputer` now works when there is only one feature.","462","  By :user:`Sergey Feldman <sergeyf>`.","463","","493","  :pr:`14028` and :pr:`15429` by :user:`Guillaume Lemaitre <glemaitre>`.","510","- |Efficiency| The 'liblinear' logistic regression solver is now faster and","511","  requires less memory.","512","  :pr:`14108`, :pr:`14170`, :pr:`14296` by :user:`Alex Henrie <alexhenrie>`.","513","","521","  :pr:`14458` by :user:`Guillaume Lemaitre <glemaitre>`.","540","  :pr:`15038` by :user:`Guillaume Lemaitre <glemaitre>`.","559","- |API| Deprecate ``training_data_`` unused attribute in","560","  :class:`manifold.Isomap`. :issue:`10482` by `Tom Dupre la Tour`_.","561","","583","- |MajorFeature| :func:`metrics.plot_roc_curve` has been added to plot roc","584","  curves. This function introduces the visualization API described in","585","  the :ref:`User Guide <visualizations>`. :pr:`14357` by `Thomas Fan`_.","586","","628","- |Fix| Raise a ValueError in :func:`metrics.silhouette_score` when a","629","  precomputed distance matrix contains non-zero diagonal entries.","630","  :pr:`12258` by :user:`Stephen Tierney <sjtrny>`.","631","","637","- |API| ``scoring=\"neg_brier_score\"`` should be used instead of","638","  ``scoring=\"brier_score_loss\"`` which is now deprecated.","639","  :pr:`14898` by :user:`Stefan Matcovici <stefan-matcovici>`.","640","","641","- |Efficiency| Improved performance of","642","  :func:`metrics.pairwise.manhattan_distances` in the case of sparse matrices.","643","  :pr:`15049` by `Paolo Toccaceli <ptocca>`.","644","","683","- |Fix| The `cv_results_` attribute of :class:`model_selection.GridSearchCV`","684","  and :class:`model_selection.RandomizedSearchCV` now only contains unfitted","685","  estimators. This potentially saves a lot of memory since the state of the","686","  estimators isn't stored. :pr:`#15096` by :user:`Andreas Mller <amueller>`.","687","","758","- |Fix| The `fit` in :class:`~pipeline.FeatureUnion` now accepts `fit_params`","759","  to pass to the underlying transformers. :pr:`15119` by `Adrin Jalali`_.","760","","789","","818","  :pr:`15038` by :user:`Guillaume Lemaitre <glemaitre>`.","858","- |API| The following utils have been deprecated and are now private:","859","","860","  - ``utils.choose_check_classifiers_labels``","861","  - ``utils.enforce_estimator_tags_y``","862","  - ``utils.optimize.newton_cg``","863","  - ``utils.random.random_choice_csc``","864","  - ``utils.safe_indexing``","865","  - ``utils.mocking``","866","  - ``utils.fast_dict``","867","  - ``utils.seq_dataset``","868","  - ``utils.weight_vector``","869","  - ``utils.fixes.parallel_helper`` (removed)","870","  - All of ``utils.testing`` except for ``all_estimators`` which is now in","871","    ``utils``.","872","","873","- A new random variable, :class:`utils.fixes.loguniform` implements a","884","  by :user:`Guillaume Lemaitre <glemaitre>`.","914","","928","- |Fix| Port `lobpcg` from SciPy which implement some bug fixes but only","929","  available in 1.3+.","930","  :pr:`13609` and :pr:`14971` by :user:`Guillaume Lemaitre <glemaitre>`.","931","","968","  to be overridable only once. :pr:`14884` by :user:`Andreas Mller","969","  <amueller>`."]}]}},"663d052d3c7da2630357a92c3f5c59128b51480e":{"changes":{"sklearn\/ensemble\/_forest.py":"MODIFY"},"diff":{"sklearn\/ensemble\/_forest.py":[{"add":["964","        Controls both the randomness of the bootstrapping of the samples used","965","        when building trees (if ``bootstrap=True``) and the sampling of the","966","        features to consider when looking for the best split at each node","967","        (if ``max_features < n_features``).","968","        See :term:`Glossary <random_state>` for details.","1281","        Controls both the randomness of the bootstrapping of the samples used","1282","        when building trees (if ``bootstrap=True``) and the sampling of the","1283","        features to consider when looking for the best split at each node","1284","        (if ``max_features < n_features``).","1285","        See :term:`Glossary <random_state>` for details.","1544","        whole dataset is used to build each tree.","1558","        Controls 3 sources of randomness:","1559","","1560","        - the bootstrapping of the samples used when building trees","1561","          (if ``bootstrap=True``)","1562","        - the sampling of the features to consider when looking for the best","1563","          split at each node (if ``max_features < n_features``)","1564","        - the draw of the splits for each of the `max_features`","1565","        See :term:`Glossary <random_state>` for details.","1853","        whole dataset is used to build each tree.","1866","        Controls 3 sources of randomness:","1867","","1868","        - the bootstrapping of the samples used when building trees","1869","          (if ``bootstrap=True``)","1870","        - the sampling of the features to consider when looking for the best","1871","          split at each node (if ``max_features < n_features``)","1872","        - the draw of the splits for each of the `max_features`","1873","        See :term:`Glossary <random_state>` for details.","2100","        Controls the generation of the random `y` used to fit the trees","2101","        and the draw of the splits for each feature at the trees' nodes.","2102","        See :term:`Glossary <random_state>` for details."],"delete":["964","        If int, random_state is the seed used by the random number generator;","965","        If RandomState instance, random_state is the random number generator;","966","        If None, the random number generator is the RandomState instance used","967","        by `np.random`.","1280","        If int, random_state is the seed used by the random number generator;","1281","        If RandomState instance, random_state is the random number generator;","1282","        If None, the random number generator is the RandomState instance used","1283","        by `np.random`.","1542","        whole datset is used to build each tree.","1556","        If int, random_state is the seed used by the random number generator;","1557","        If RandomState instance, random_state is the random number generator;","1558","        If None, the random number generator is the RandomState instance used","1559","        by `np.random`.","1847","        whole datset is used to build each tree.","1860","        If int, random_state is the seed used by the random number generator;","1861","        If RandomState instance, random_state is the random number generator;","1862","        If None, the random number generator is the RandomState instance used","1863","        by `np.random`.","2090","        If int, random_state is the seed used by the random number generator;","2091","        If RandomState instance, random_state is the random number generator;","2092","        If None, the random number generator is the RandomState instance used","2093","        by `np.random`."]}]}},"cdbda0eddba48fc6b6b1822ec272cccc656a0329":{"changes":{"sklearn\/ensemble\/_hist_gradient_boosting\/gradient_boosting.py":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_gradient_boosting.py":"MODIFY"},"diff":{"sklearn\/ensemble\/_hist_gradient_boosting\/gradient_boosting.py":[{"add":["306","            else:","307","                raw_predictions_val = None"],"delete":[]}],"sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_gradient_boosting.py":[{"add":["647","","648","","649","def test_early_stopping_on_test_set_with_warm_start():","650","    # Non regression test for #16661 where second fit fails with","651","    # warm_start=True, early_stopping is on, and no validation set","652","    X, y = make_classification(random_state=0)","653","    gb = HistGradientBoostingClassifier(","654","        max_iter=1, scoring='loss', warm_start=True, early_stopping=True,","655","        n_iter_no_change=1, validation_fraction=None)","656","","657","    gb.fit(X, y)","658","    # does not raise on second call","659","    gb.set_params(max_iter=2)","660","    gb.fit(X, y)"],"delete":[]}]}},"a6c07f2b0862885850d455a3a8b995d57f7d5648":{"changes":{"doc\/conf.py":"MODIFY"},"diff":{"doc\/conf.py":[{"add":["53","    mathjax_path = ''"],"delete":[]}]}},"da6065b088e03a09964a51f14ff8b069a3873cc7":{"changes":{"sklearn\/cluster\/_kmeans.py":"MODIFY"},"diff":{"sklearn\/cluster\/_kmeans.py":[{"add":["238","        Relative tolerance with regards to Frobenius norm of the difference","239","        in the cluster centers of two consecutive iterations to declare","240","        convergence.","685","        Relative tolerance with regards to Frobenius norm of the difference","686","        in the cluster centers of two consecutive iterations to declare","687","        convergence."],"delete":["238","        The relative increment in the results before declaring convergence.","683","        Relative tolerance with regards to inertia to declare convergence."]}]}},"75d3f29e724c9a2db7bcb5afb2d7e8c08ddbd80b":{"changes":{"sklearn\/utils\/fixes.py":"MODIFY","sklearn\/decomposition\/_lda.py":"MODIFY","sklearn\/naive_bayes.py":"MODIFY","sklearn\/utils\/tests\/test_fixes.py":"MODIFY","sklearn\/impute\/_iterative.py":"MODIFY","sklearn\/model_selection\/tests\/test_split.py":"MODIFY","sklearn\/preprocessing\/_encoders.py":"MODIFY","sklearn\/ensemble\/tests\/test_forest.py":"MODIFY","sklearn\/ensemble\/_gb_losses.py":"MODIFY","sklearn\/linear_model\/_logistic.py":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/loss.py":"MODIFY","sklearn\/model_selection\/_split.py":"MODIFY","sklearn\/metrics\/cluster\/_supervised.py":"MODIFY","sklearn\/mixture\/_base.py":"MODIFY","sklearn\/utils\/tests\/test_random.py":"MODIFY","sklearn\/linear_model\/tests\/test_sag.py":"MODIFY","sklearn\/model_selection\/_search.py":"MODIFY"},"diff":{"sklearn\/utils\/fixes.py":[{"add":[],"delete":["36","try:  # SciPy >= 0.19","37","    from scipy.special import comb, logsumexp","38","except ImportError:","39","    from scipy.misc import comb, logsumexp  # noqa","40","","55","if sp_version >= (0, 19):","56","    def _argmax(arr_or_spmatrix, axis=None):","57","        return arr_or_spmatrix.argmax(axis=axis)","58","else:","59","    # Backport of argmax functionality from scipy 0.19.1, can be removed","60","    # once support for scipy 0.18 and below is dropped","61","","62","    def _find_missing_index(ind, n):","63","        for k, a in enumerate(ind):","64","            if k != a:","65","                return k","66","","67","        k += 1","68","        if k < n:","69","            return k","70","        else:","71","            return -1","72","","73","    def _arg_min_or_max_axis(self, axis, op, compare):","74","        if self.shape[axis] == 0:","75","            raise ValueError(\"Can't apply the operation along a zero-sized \"","76","                             \"dimension.\")","77","","78","        if axis < 0:","79","            axis += 2","80","","81","        zero = self.dtype.type(0)","82","","83","        mat = self.tocsc() if axis == 0 else self.tocsr()","84","        mat.sum_duplicates()","85","","86","        ret_size, line_size = mat._swap(mat.shape)","87","        ret = np.zeros(ret_size, dtype=int)","88","","89","        nz_lines, = np.nonzero(np.diff(mat.indptr))","90","        for i in nz_lines:","91","            p, q = mat.indptr[i:i + 2]","92","            data = mat.data[p:q]","93","            indices = mat.indices[p:q]","94","            am = op(data)","95","            m = data[am]","96","            if compare(m, zero) or q - p == line_size:","97","                ret[i] = indices[am]","98","            else:","99","                zero_ind = _find_missing_index(indices, line_size)","100","                if m == zero:","101","                    ret[i] = min(am, zero_ind)","102","                else:","103","                    ret[i] = zero_ind","104","","105","        if axis == 1:","106","            ret = ret.reshape(-1, 1)","107","","108","        return np.asmatrix(ret)","109","","110","    def _arg_min_or_max(self, axis, out, op, compare):","111","        if out is not None:","112","            raise ValueError(\"Sparse matrices do not support \"","113","                             \"an 'out' parameter.\")","114","","115","        # validateaxis(axis)","116","","117","        if axis is None:","118","            if 0 in self.shape:","119","                raise ValueError(\"Can't apply the operation to \"","120","                                 \"an empty matrix.\")","121","","122","            if self.nnz == 0:","123","                return 0","124","            else:","125","                zero = self.dtype.type(0)","126","                mat = self.tocoo()","127","                mat.sum_duplicates()","128","                am = op(mat.data)","129","                m = mat.data[am]","130","","131","                if compare(m, zero):","132","                    return mat.row[am] * mat.shape[1] + mat.col[am]","133","                else:","134","                    size = np.product(mat.shape)","135","                    if size == mat.nnz:","136","                        return am","137","                    else:","138","                        ind = mat.row * mat.shape[1] + mat.col","139","                        zero_ind = _find_missing_index(ind, size)","140","                        if m == zero:","141","                            return min(zero_ind, am)","142","                        else:","143","                            return zero_ind","144","","145","        return _arg_min_or_max_axis(self, axis, op, compare)","146","","147","    def _sparse_argmax(self, axis=None, out=None):","148","        return _arg_min_or_max(self, axis, out, np.argmax, np.greater)","149","","150","    def _argmax(arr_or_matrix, axis=None):","151","        if sp.issparse(arr_or_matrix):","152","            return _sparse_argmax(arr_or_matrix, axis=axis)","153","        else:","154","            return arr_or_matrix.argmax(axis=axis)","155","","156","","157","if np_version < (1, 12):","158","    class MaskedArray(np.ma.MaskedArray):","159","        # Before numpy 1.12, np.ma.MaskedArray object is not picklable","160","        # This fix is needed to make our model_selection.GridSearchCV","161","        # picklable as the ``cv_results_`` param uses MaskedArray","162","        def __getstate__(self):","163","            \"\"\"Return the internal state of the masked array, for pickling","164","            purposes.","165","","166","            \"\"\"","167","            cf = 'CF'[self.flags.fnc]","168","            data_state = super(np.ma.MaskedArray, self).__reduce__()[2]","169","            return data_state + (np.ma.getmaskarray(self).tostring(cf),","170","                                 self._fill_value)","171","else:","172","    from numpy.ma import MaskedArray    # noqa","173",""]}],"sklearn\/decomposition\/_lda.py":[{"add":["15","from scipy.special import gammaln, logsumexp"],"delete":["15","from scipy.special import gammaln","20","from ..utils.fixes import logsumexp"]}],"sklearn\/naive_bayes.py":[{"add":["23","from scipy.special import logsumexp"],"delete":["30","from .utils.fixes import logsumexp"]}],"sklearn\/utils\/tests\/test_fixes.py":[{"add":[],"delete":["6","import pickle","14","from sklearn.utils.fixes import MaskedArray","20","def test_masked_array_obj_dtype_pickleable():","21","    marr = MaskedArray([1, None, 'a'], dtype=object)","22","","23","    for mask in (True, False, [0, 1, 0]):","24","        marr.mask = mask","25","        marr_pickled = pickle.loads(pickle.dumps(marr))","26","        assert_array_equal(marr.data, marr_pickled.data)","27","        assert_array_equal(marr.mask, marr_pickled.mask)","28","","29",""]}],"sklearn\/impute\/_iterative.py":[{"add":["329","            truncated_normal = stats.truncnorm(a=a, b=b,","330","                                               loc=mus, scale=sigmas)","331","            imputed_values[inrange_mask] = truncated_normal.rvs(","332","                random_state=self.random_state_)"],"delete":["2","from distutils.version import LooseVersion","6","import scipy","331","            if scipy.__version__ < LooseVersion('0.18'):","332","                # bug with vector-valued `a` in old scipy","333","                imputed_values[inrange_mask] = [","334","                    stats.truncnorm(a=a_, b=b_,","335","                                    loc=loc_, scale=scale_).rvs(","336","                                        random_state=self.random_state_)","337","                    for a_, b_, loc_, scale_","338","                    in zip(a, b, mus, sigmas)]","339","            else:","340","                truncated_normal = stats.truncnorm(a=a, b=b,","341","                                                   loc=mus, scale=sigmas)","342","                imputed_values[inrange_mask] = truncated_normal.rvs(","343","                    random_state=self.random_state_)"]}],"sklearn\/model_selection\/tests\/test_split.py":[{"add":["6","from scipy.special import comb"],"delete":["48","from sklearn.utils.fixes import comb","49",""]}],"sklearn\/preprocessing\/_encoders.py":[{"add":["528","            labels = np.asarray(sub.argmax(axis=1)).flatten()"],"delete":["9","from ..utils.fixes import _argmax","529","            labels = np.asarray(_argmax(sub, axis=1)).flatten()"]}],"sklearn\/ensemble\/tests\/test_forest.py":[{"add":["22","from scipy.special import comb"],"delete":["49","from sklearn.utils.fixes import comb"]}],"sklearn\/ensemble\/_gb_losses.py":[{"add":["8","from scipy.special import expit, logsumexp"],"delete":["8","from scipy.special import expit","11","from ..utils.fixes import logsumexp"]}],"sklearn\/linear_model\/_logistic.py":[{"add":["17","from scipy.special import expit, logsumexp"],"delete":["17","from scipy.special import expit","29","from ..utils.fixes import logsumexp"]}],"sklearn\/ensemble\/_hist_gradient_boosting\/loss.py":[{"add":["11","from scipy.special import expit, logsumexp"],"delete":["11","from scipy.special import expit","12","try:  # logsumexp was moved from mist to special in 0.19","13","    from scipy.special import logsumexp","14","except ImportError:","15","    from scipy.misc import logsumexp"]}],"sklearn\/model_selection\/_split.py":[{"add":["20","from scipy.special import comb"],"delete":["26","from ..utils.fixes import comb"]}],"sklearn\/metrics\/cluster\/_supervised.py":[{"add":["21","from scipy.special import comb","25","from ...utils.fixes import _astype_copy_false"],"delete":["24","from ...utils.fixes import comb, _astype_copy_false"]}],"sklearn\/mixture\/_base.py":[{"add":["11","from scipy.special import logsumexp"],"delete":["18","from ..utils.fixes import logsumexp"]}],"sklearn\/utils\/tests\/test_random.py":[{"add":["3","from scipy.special import comb"],"delete":["5","from sklearn.utils.fixes import comb"]}],"sklearn\/linear_model\/tests\/test_sag.py":[{"add":["9","from scipy.special import logsumexp"],"delete":["16","from sklearn.utils.fixes import logsumexp"]}],"sklearn\/model_selection\/_search.py":[{"add":["23","from numpy.ma import MaskedArray"],"delete":["33","from ..utils.fixes import MaskedArray"]}]}},"b6bbf58f0c0b6cc7be219acfe8eec8626bd04842":{"changes":{"doc\/modules\/ensemble.rst":"MODIFY"},"diff":{"doc\/modules\/ensemble.rst":[{"add":["491","The usage and the parameters of :class:`GradientBoostingClassifier` and","492",":class:`GradientBoostingRegressor` are described below. The 2 most important","493","parameters of these estimators are `n_estimators` and `learning_rate`.","494","","515","The number of weak learners (i.e. regression trees) is controlled by the","516","parameter ``n_estimators``; :ref:`The size of each tree","517","<gradient_boosting_tree_size>` can be controlled either by setting the tree","518","depth via ``max_depth`` or by setting the number of leaf nodes via","519","``max_leaf_nodes``. The ``learning_rate`` is a hyper-parameter in the range","520","(0.0, 1.0] that controls overfitting via :ref:`shrinkage","521","<gradient_boosting_shrinkage>` .","627","We first present GBRT for regression, and then detail the classification","628","case.","629","","630","Regression","631","^^^^^^^^^^","632","","633","GBRT regressors are additive models whose prediction :math:`y_i` for a","634","given input :math:`x_i` is of the following form:","638","    \\hat{y_i} = F_M(x_i) = \\sum_{m=1}^{M} h_m(x_i)","640","where the :math:`h_m` are estimators called *weak learners* in the context","641","of boosting. Gradient Tree Boosting uses :ref:`decision tree regressors","642","<tree>` of fixed size as weak learners. The constant M corresponds to the","643","`n_estimators` parameter.","645","Similar to other boosting algorithms, a GBRT is built in a greedy fashion:","649","    F_m(x) = F_{m-1}(x) + h_m(x),","651","where the newly added tree :math:`h_m` is fitted in order to minimize a sum","652","of losses :math:`L_m`, given the previous ensemble :math:`F_{m-1}`:","656","    h_m =  \\arg\\min_{h} L_m = \\arg\\min_{h} \\sum_{i=1}^{n}","657","    l(y_i, F_{m-1}(x_i) + h(x_i)),","659","where :math:`l(y_i, F(x_i))` is defined by the `loss` parameter, detailed","660","in the next section.","662","By default, the initial model :math:`F_{0}` is chosen as the constant that","663","minimizes the loss: for a least-squares loss, this is the empirical mean of","664","the target values. The initial model can also be specified via the ``init``","665","argument.","667","Using a first-order Taylor approximation, the value of :math:`l` can be","668","approximated as follows:","672","    l(y_i, F_{m-1}(x_i) + h_m(x_i)) \\approx","673","    l(y_i, F_{m-1}(x_i))","674","    + h_m(x_i)","675","    \\left[ \\frac{\\partial l(y_i, F(x_i))}{\\partial F(x_i)} \\right]_{F=F_{m - 1}}.","677",".. note::","678","","679","  Briefly, a first-order Taylor approximation says that","680","  :math:`l(z) \\approx l(a) + (z - a) \\frac{\\partial l(a)}{\\partial a}`.","681","  Here, :math:`z` corresponds to :math:`F_{m - 1}(x_i) + h_m(x_i)`, and","682","  :math:`a` corresponds to :math:`F_{m-1}(x_i)`","683","","684","The quantity :math:`\\left[ \\frac{\\partial l(y_i, F(x_i))}{\\partial F(x_i)}","685","\\right]_{F=F_{m - 1}}` is the derivative of the loss with respect to its","686","second parameter, evaluated at :math:`F_{m-1}(x)`. It is easy to compute for","687","any given :math:`F_{m - 1}(x_i)` in a closed form since the loss is","688","differentiable. We will denote it by :math:`g_i`.","689","","690","Removing the constant terms, we have:","694","    h_m \\approx \\arg\\min_{h} \\sum_{i=1}^{n} h(x_i) g_i","696","This is minimized if :math:`h(x_i)` is fitted to predict a value that is","697","proportional to the negative gradient :math:`-g_i`. Therefore, at each","698","iteration, **the estimator** :math:`h_m` **is fitted to predict the negative","699","gradients of the samples**. The gradients are updated at each iteration.","700","This can be considered as some kind of gradient descent in a functional","701","space.","702","","703",".. note::","704","","705","  For some losses, e.g. the least absolute deviation (LAD) where the gradients","706","  are :math:`\\pm 1`, the values predicted by a fitted :math:`h_m` are not","707","  accurate enough: the tree can only output integer values. As a result, the","708","  leaves values of the tree :math:`h_m` are modified once the tree is","709","  fitted, such that the leaves values minimize the loss :math:`L_m`. The","710","  update is loss-dependent: for the LAD loss, the value of a leaf is updated","711","  to the median of the samples in that leaf.","712","","713","Classification","714","^^^^^^^^^^^^^^","715","","716","Gradient boosting for classification is very similar to the regression case.","717","However, the sum of the trees :math:`F_M(x_i) = \\sum_m h_m(x_i)` is not","718","homogeneous to a prediction: it cannot be a class, since the trees predict","719","continuous values.","720","","721","The mapping from the value :math:`F_M(x_i)` to a class or a probability is","722","loss-dependent. For the deviance (or log-loss), the probability that","723",":math:`x_i` belongs to the positive class is modeled as :math:`p(y_i = 1 |","724","x_i) = \\sigma(F_M(x_i))` where :math:`\\sigma` is the sigmoid function.","725","","726","For multiclass classification, K trees (for K classes) are built at each of","727","the :math:`M` iterations. The probability that :math:`x_i` belongs to class","728","k is modeled as a softmax of the :math:`F_{M,k}(x_i)` values.","729","","730","Note that even for a classification task, the :math:`h_m` sub-estimator is","731","still a regressor, not a classifier. This is because the sub-estimators are","732","trained to predict (negative) *gradients*, which are always continuous","733","quantities.","738","--------------","780","Shrinkage via learning rate","781","---------------------------","784","the contribution of each weak learner by a constant factor :math:`\\nu`:","788","    F_m(x) = F_{m-1}(x) + \\nu h_m(x)","805","-----------","849","Interpretation with feature importance","850","--------------------------------------","886","Note that this computation of feature importance is based on entropy, and it","887","is distinct from :func:`sklearn.inspection.permutation_importance` which is","888","based on permutation of the features.","889","","1152","-----","1269","-----"],"delete":["511","The number of weak learners (i.e. regression trees) is controlled by the parameter ``n_estimators``; :ref:`The size of each tree <gradient_boosting_tree_size>` can be controlled either by setting the tree depth via ``max_depth`` or by setting the number of leaf nodes via ``max_leaf_nodes``. The ``learning_rate`` is a hyper-parameter in the range (0.0, 1.0] that controls overfitting via :ref:`shrinkage <gradient_boosting_shrinkage>` .","617","GBRT considers additive models of the following form:","621","    F(x) = \\sum_{m=1}^{M} \\gamma_m h_m(x)","623","where :math:`h_m(x)` are the basis functions which are usually called","624","*weak learners* in the context of boosting. Gradient Tree Boosting","625","uses :ref:`decision trees <tree>` of fixed size as weak","626","learners. Decision trees have a number of abilities that make them","627","valuable for boosting, namely the ability to handle data of mixed type","628","and the ability to model complex functions.","630","Similar to other boosting algorithms, GBRT builds the additive model in","631","a greedy fashion:","635","    F_m(x) = F_{m-1}(x) + \\gamma_m h_m(x),","637","where the newly added tree :math:`h_m` tries to minimize the loss :math:`L`,","638","given the previous ensemble :math:`F_{m-1}`:","642","    h_m =  \\arg\\min_{h} \\sum_{i=1}^{n} L(y_i,","643","    F_{m-1}(x_i) + h(x_i)).","645","The initial model :math:`F_{0}` is problem specific, for least-squares","646","regression one usually chooses the mean of the target values.","648",".. note:: The initial model can also be specified via the ``init``","649","          argument. The passed object has to implement ``fit`` and ``predict``.","651","Gradient Boosting attempts to solve this minimization problem","652","numerically via steepest descent: The steepest descent direction is","653","the negative gradient of the loss function evaluated at the current","654","model :math:`F_{m-1}` which can be calculated for any differentiable","655","loss function:","659","    F_m(x) = F_{m-1}(x) - \\gamma_m \\sum_{i=1}^{n} \\nabla_F L(y_i,","660","    F_{m-1}(x_i))","662","Where the step length :math:`\\gamma_m` is chosen using line search:","666","    \\gamma_m = \\arg\\min_{\\gamma} \\sum_{i=1}^{n} L(y_i, F_{m-1}(x_i)","667","    - \\gamma \\frac{\\partial L(y_i, F_{m-1}(x_i))}{\\partial F_{m-1}(x_i)})","669","The algorithms for regression and classification","670","only differ in the concrete loss function used.","675","...............","715","Regularization","716","----------------","717","","720","Shrinkage","721","..........","724","the contribution of each weak learner by a factor :math:`\\nu`:","728","    F_m(x) = F_{m-1}(x) + \\nu \\gamma_m h_m(x)","745","............","789","Interpretation","790","--------------","799","Feature importance","800","..................","801","","1091",".....","1208","....."]}]}},"dbc35934a6ebf7d0bbfdfc80e8bf8a9fabc1ba4b":{"changes":{"sklearn\/model_selection\/_split.py":"MODIFY","sklearn\/metrics\/cluster\/_supervised.py":"MODIFY","sklearn\/multioutput.py":"MODIFY","sklearn\/metrics\/_classification.py":"MODIFY","sklearn\/cluster\/_kmeans.py":"MODIFY","sklearn\/feature_selection\/_rfe.py":"MODIFY","sklearn\/ensemble\/_voting.py":"MODIFY","sklearn\/linear_model\/_ransac.py":"MODIFY","sklearn\/svm\/_classes.py":"MODIFY","sklearn\/feature_selection\/_univariate_selection.py":"MODIFY","sklearn\/preprocessing\/_function_transformer.py":"MODIFY"},"diff":{"sklearn\/model_selection\/_split.py":[{"add":["741","    .. versionadded:: 0.18","742",""],"delete":[]}],"sklearn\/metrics\/cluster\/_supervised.py":[{"add":["883","    .. versionadded:: 0.18","884",""],"delete":[]}],"sklearn\/multioutput.py":[{"add":["217","    .. versionadded:: 0.18","218",""],"delete":[]}],"sklearn\/metrics\/_classification.py":[{"add":["229","        .. versionadded:: 0.18","230","","793","        .. versionadded:: 0.18","794","","2162",""],"delete":[]}],"sklearn\/cluster\/_kmeans.py":[{"add":["836","        .. versionchanged:: 0.18","837","            Added Elkan algorithm","838",""],"delete":[]}],"sklearn\/feature_selection\/_rfe.py":[{"add":["412","        .. versionadded:: 0.18","413",""],"delete":[]}],"sklearn\/ensemble\/_voting.py":[{"add":["143","        .. versionadded:: 0.18","144","","236","            .. versionadded:: 0.18","237",""],"delete":[]}],"sklearn\/linear_model\/_ransac.py":[{"add":["152","        .. versionadded:: 0.18","153","","243","            .. versionadded:: 0.18","244",""],"delete":[]}],"sklearn\/svm\/_classes.py":[{"add":["215","            .. versionadded:: 0.18","216","","402","            .. versionadded:: 0.18","403",""],"delete":[]}],"sklearn\/feature_selection\/_univariate_selection.py":[{"add":["386","        .. versionadded:: 0.18","387","","471","        .. versionadded:: 0.18","472",""],"delete":[]}],"sklearn\/preprocessing\/_function_transformer.py":[{"add":["68","        .. versionadded:: 0.18","69","","73","        .. versionadded:: 0.18","74",""],"delete":[]}]}},"856d273b9dd100cf478f86579333a2b2d78c2d76":{"changes":{"doc\/modules\/classes.rst":"MODIFY","sklearn\/feature_selection\/__init__.py":"MODIFY"},"diff":{"doc\/modules\/classes.rst":[{"add":["35","   feature_selection.SelectorMixin"],"delete":[]}],"sklearn\/feature_selection\/__init__.py":[{"add":["26","from ._base import SelectorMixin","27","","44","           'mutual_info_regression',","45","           'SelectorMixin']"],"delete":["42","           'mutual_info_regression']"]}]}},"cac167244d9e8dc57087f63d55dadfc1bc0ce09c":{"changes":{"doc\/developers\/advanced_installation.rst":"MODIFY","sklearn\/_build_utils\/openmp_helpers.py":"MODIFY",".travis.yml":"MODIFY","build_tools\/travis\/install.sh":"MODIFY","build_tools\/travis\/test_script.sh":"MODIFY","build_tools\/travis\/test_docs.sh":"MODIFY","sklearn\/_build_utils\/pre_build_helpers.py":"MODIFY"},"diff":{"doc\/developers\/advanced_installation.rst":[{"add":["390","","391","Alternative compilers","392","=====================","393","","394","The command::","395","","396","    pip install --verbose --editable .","397","","398","will build scikit-learn using your default C\/C++ compiler. If you want to build","399","scikit-learn with another compiler handled by ``distutils`` or by","400","``numpy.distutils``, use the following command::","401","","402","    python setup.py build_ext --compiler=<compiler> -i build_clib --compiler=<compiler>","403","","404","To see the list of available compilers run::","405","","406","    python setup.py build_ext --help-compiler","407","","408","If your compiler is not listed here, you can specify it via the ``CC`` and","409","``LDSHARED`` environment variables (does not work on windows)::","410","","411","    CC=<compiler> LDSHARED=\"<compiler> -shared\" python setup.py build_ext -i","412","","413","Building with Intel C Compiler (ICC) using oneAPI on Linux","414","----------------------------------------------------------","415","","416","Intel provides access to all of its oneAPI toolkits and packages through a","417","public APT repository. First you need to get and install the public key of this","418","repository::","419","","420","    wget https:\/\/apt.repos.intel.com\/intel-gpg-keys\/GPG-PUB-KEY-INTEL-SW-PRODUCTS-2023.PUB","421","    sudo apt-key add GPG-PUB-KEY-INTEL-SW-PRODUCTS-2023.PUB","422","    rm GPG-PUB-KEY-INTEL-SW-PRODUCTS-2023.PUB","423","","424","Then, add the oneAPI repository to your APT repositories::","425","","426","    sudo add-apt-repository \"deb https:\/\/apt.repos.intel.com\/oneapi all main\"","427","    sudo apt-get update","428","","429","Install ICC, packaged under the name ``intel-oneapi-icc``::","430","","431","    sudo apt-get install intel-oneapi-icc","432","","433","Before using ICC, you need to set up environment variables::","434","","435","    source \/opt\/intel\/inteloneapi\/setvars.sh","436","","437","Finally, you can build scikit-learn. For example on Linux x86_64::","438","","439","    python setup.py build_ext --compiler=intelem -i build_clib --compiler=intelem"],"delete":[]}],"sklearn\/_build_utils\/openmp_helpers.py":[{"add":["3","# can be found at: https:\/\/github.com\/astropy\/extension-helpers\/blob\/master\/extension_helpers\/_openmp_helpers.py  # noqa","27","    elif sys.platform in (\"darwin\", \"linux\") and \"icc\" in compiler:","28","        return ['-qopenmp']"],"delete":["3","# can be found at: https:\/\/github.com\/astropy\/astropy-helpers\/blob\/master\/astropy_helpers\/openmp_helpers.py  # noqa","27","    elif sys.platform == \"darwin\" and ('icc' in compiler or 'icl' in compiler):","28","        return ['-openmp']"]}],".travis.yml":[{"add":["24","    - python: 3.7","25","      env: CHECK_WARNINGS=\"true\"","26","      if: type = cron OR commit_message =~ \/\\[scipy-dev\\]\/","27","    ","28","    # As above but build scikit-learn with Intel C compiler (ICC).","29","    - python: 3.7","30","      env:","31","        - CHECK_WARNING=\"true\"","32","        - BUILD_WITH_ICC=\"true\"","33","      if: type = cron OR commit_message =~ \/\\[icc-build\\]\/"],"delete":["24","    -  python: 3.7","25","       env: CHECK_WARNINGS=\"true\"","26","       if: type = cron OR commit_message =~ \/\\[scipy-dev\\]\/"]}],"build_tools\/travis\/install.sh":[{"add":["68","if [[ \"$BUILD_WITH_ICC\" == \"true\" ]]; then","69","    wget https:\/\/apt.repos.intel.com\/intel-gpg-keys\/GPG-PUB-KEY-INTEL-SW-PRODUCTS-2023.PUB","70","    sudo apt-key add GPG-PUB-KEY-INTEL-SW-PRODUCTS-2023.PUB","71","    rm GPG-PUB-KEY-INTEL-SW-PRODUCTS-2023.PUB","72","    sudo add-apt-repository \"deb https:\/\/apt.repos.intel.com\/oneapi all main\"","73","    sudo apt-get update","74","    sudo apt-get install intel-oneapi-icc","75","    source \/opt\/intel\/inteloneapi\/setvars.sh","76","","77","    # The build_clib command is implicitly used to build libsvm-skl. To compile","78","    # with a different compiler we also need to specify the compiler for this","79","    # command.","80","    python setup.py build_ext --compiler=intelem -i -j 3 build_clib --compiler=intelem","81","else","82","    # Use setup.py instead of `pip install -e .` to be able to pass the -j flag","83","    # to speed-up the building multicore CI machines.","84","    python setup.py build_ext --inplace -j 3","85","fi","86",""],"delete":[]}],"build_tools\/travis\/test_script.sh":[{"add":["22","if [[ \"$BUILD_WITH_ICC\" == \"true\" ]]; then","23","    # the tools in the oneAPI toolkits are configured via environment variables","24","    # which are also required at runtime.","25","    source \/opt\/intel\/inteloneapi\/setvars.sh","26","fi","27",""],"delete":[]}],"build_tools\/travis\/test_docs.sh":[{"add":["5","if [[ \"$BUILD_WITH_ICC\" == \"true\" ]]; then","6","    # the tools in the oneAPI toolkits are configured via environment variables","7","    # which are also required at runtime.","8","    source \/opt\/intel\/inteloneapi\/setvars.sh","9","fi","10",""],"delete":[]}],"sklearn\/_build_utils\/pre_build_helpers.py":[{"add":["9","from distutils.dist import Distribution","12","from numpy.distutils.command.config_compiler import config_cc","13","","14","","15","def _get_compiler():","16","    \"\"\"Get a compiler equivalent to the one that will be used to build sklearn","17","","18","    Handles compiler specified as follows:","19","        - python setup.py build_ext --compiler=<compiler>","20","        - CC=<compiler> python setup.py build_ext","21","    \"\"\"","22","    dist = Distribution({'script_name': os.path.basename(sys.argv[0]),","23","                         'script_args': sys.argv[1:],","24","                         'cmdclass': {'config_cc': config_cc}})","25","    dist.parse_config_files()","26","    dist.parse_command_line()","27","","28","    cmd_opts = dist.command_options.get('build_ext')","29","    if cmd_opts is not None and 'compiler' in cmd_opts:","30","        compiler = cmd_opts['compiler'][1]","31","    else:","32","        compiler = None","33","","34","    ccompiler = new_compiler(compiler=compiler)","35","    customize_compiler(ccompiler)","36","","37","    return ccompiler","42","    ccompiler = _get_compiler()"],"delete":["15","    ccompiler = new_compiler()","16","    customize_compiler(ccompiler)"]}]}},"6419f65ec0183c01316f20c797ffcdc868ab141e":{"changes":{"sklearn\/ensemble\/_forest.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY"},"diff":{"sklearn\/ensemble\/_forest.py":[{"add":["2145","                 warm_start=False):","2160","            max_samples=None)"],"delete":["2114","    max_samples : int or float, default=None","2115","        If bootstrap is True, the number of samples to draw from X","2116","        to train each base estimator.","2117","","2118","        - If None (default), then draw `X.shape[0]` samples.","2119","        - If int, then draw `max_samples` samples.","2120","        - If float, then draw `max_samples * X.shape[0]` samples. Thus,","2121","          `max_samples` should be in the interval `(0, 1)`.","2122","","2123","        .. versionadded:: 0.22","2124","","2156","                 warm_start=False,","2157","                 max_samples=None):","2172","            max_samples=max_samples)"]}],"doc\/whats_new\/v0.22.rst":[{"add":["331","  :class:`ensemble.ExtraTreesRegressor`. :pr:`14682` by"],"delete":["328","  :class:`ensemble.ForestClassifier`,","329","  :class:`ensemble.ForestRegressor`,","333","  :class:`ensemble.ExtraTreesRegressor`,","334","  :class:`ensemble.RandomTreesEmbedding`. :pr:`14682` by"]}]}},"d6d624db5d6fea6f1c29952f7de63d1d1d6ae345":{"changes":{"sklearn\/decomposition\/_dict_learning.py":"MODIFY","sklearn\/decomposition\/_factor_analysis.py":"MODIFY","sklearn\/decomposition\/tests\/test_nmf.py":"MODIFY","sklearn\/decomposition\/_lda.py":"MODIFY","sklearn\/decomposition\/_kernel_pca.py":"MODIFY","sklearn\/decomposition\/_pca.py":"MODIFY","sklearn\/decomposition\/_sparse_pca.py":"MODIFY","sklearn\/decomposition\/_fastica.py":"MODIFY","sklearn\/decomposition\/_truncated_svd.py":"MODIFY","sklearn\/decomposition\/_incremental_pca.py":"MODIFY","sklearn\/decomposition\/_nmf.py":"MODIFY"},"diff":{"sklearn\/decomposition\/_dict_learning.py":[{"add":["19","from ..utils.validation import check_is_fitted, _deprecate_positional_args","1015","    @_deprecate_positional_args","1016","    def __init__(self, dictionary, *, transform_algorithm='omp',","1186","    @_deprecate_positional_args","1187","    def __init__(self, n_components=None, *, alpha=1, max_iter=1000, tol=1e-8,","1392","    @_deprecate_positional_args","1393","    def __init__(self, n_components=None, *, alpha=1, n_iter=1000,"],"delete":["19","from ..utils.validation import check_is_fitted","1015","    def __init__(self, dictionary, transform_algorithm='omp',","1185","    def __init__(self, n_components=None, alpha=1, max_iter=1000, tol=1e-8,","1390","    def __init__(self, n_components=None, alpha=1, n_iter=1000,"]}],"sklearn\/decomposition\/_factor_analysis.py":[{"add":["30","from ..utils.validation import check_is_fitted, _deprecate_positional_args","140","    @_deprecate_positional_args","141","    def __init__(self, n_components=None, *, tol=1e-2, copy=True,","142","                 max_iter=1000,"],"delete":["30","from ..utils.validation import check_is_fitted","140","    def __init__(self, n_components=None, tol=1e-2, copy=True, max_iter=1000,"]}],"sklearn\/decomposition\/tests\/test_nmf.py":[{"add":["65","        assert_raise_message(ValueError, msg, NMF(3, init=init).fit, A)"],"delete":["65","        assert_raise_message(ValueError, msg, NMF(3, init).fit, A)"]}],"sklearn\/decomposition\/_lda.py":[{"add":["22","from ..utils.validation import _deprecate_positional_args","283","    @_deprecate_positional_args","284","    def __init__(self, n_components=10, *, doc_topic_prior=None,"],"delete":["282","","283","    def __init__(self, n_components=10, doc_topic_prior=None,"]}],"sklearn\/decomposition\/_kernel_pca.py":[{"add":["16","from ..utils.validation import _deprecate_positional_args","141","    @_deprecate_positional_args","142","    def __init__(self, n_components=None, *, kernel=\"linear\","],"delete":["140","","141","    def __init__(self, n_components=None, kernel=\"linear\","]}],"sklearn\/decomposition\/_pca.py":[{"add":["27","from ..utils.validation import _deprecate_positional_args","327","    @_deprecate_positional_args"],"delete":["326",""]}],"sklearn\/decomposition\/_sparse_pca.py":[{"add":["10","from ..utils.validation import _deprecate_positional_args","134","    @_deprecate_positional_args","135","    def __init__(self, n_components=None, *, alpha=1, ridge_alpha=0.01,","344","    @_deprecate_positional_args","345","    def __init__(self, n_components=None, *, alpha=1, ridge_alpha=0.01,"],"delete":["133","    def __init__(self, n_components=None, alpha=1, ridge_alpha=0.01,","342","    def __init__(self, n_components=None, alpha=1, ridge_alpha=0.01,"]}],"sklearn\/decomposition\/_fastica.py":[{"add":["22","from ..utils.validation import _deprecate_positional_args","393","    @_deprecate_positional_args","394","    def __init__(self, n_components=None, *, algorithm='parallel', whiten=True,"],"delete":["392","    def __init__(self, n_components=None, algorithm='parallel', whiten=True,"]}],"sklearn\/decomposition\/_truncated_svd.py":[{"add":["16","from ..utils.validation import _deprecate_positional_args","17","","120","    @_deprecate_positional_args","121","    def __init__(self, n_components=2, *, algorithm=\"randomized\", n_iter=5,"],"delete":["118","    def __init__(self, n_components=2, algorithm=\"randomized\", n_iter=5,"]}],"sklearn\/decomposition\/_incremental_pca.py":[{"add":["12","from ..utils.validation import _deprecate_positional_args","166","    @_deprecate_positional_args","167","    def __init__(self, n_components=None, *, whiten=False, copy=True,"],"delete":["165","","166","    def __init__(self, n_components=None, whiten=False, copy=True,"]}],"sklearn\/decomposition\/_nmf.py":[{"add":["21","from ..utils.validation import _deprecate_positional_args","1235","    @_deprecate_positional_args","1236","    def __init__(self, n_components=None, *, init=None, solver='cd',"],"delete":["1234","","1235","    def __init__(self, n_components=None, init=None, solver='cd',"]}]}},"a83b8e0d486acb6d0958602dc3b8b48de151e44a":{"changes":{"examples\/inspection\/plot_permutation_importance_multicollinear.py":"MODIFY"},"diff":{"examples\/inspection\/plot_permutation_importance_multicollinear.py":[{"add":["62","ax1.set_yticklabels(data.feature_names[tree_importance_sorted_idx])","66","            labels=data.feature_names[perm_sorted_idx])"],"delete":["62","ax1.set_yticklabels(data.feature_names)","66","            labels=data.feature_names)"]}]}},"c4733f4895c1becdf587b38970f6f7066656e3f9":{"changes":{"doc\/developers\/advanced_installation.rst":"MODIFY","sklearn\/decomposition\/_dict_learning.py":"MODIFY","doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_gradient_boosting.py":"MODIFY","doc\/modules\/computing.rst":"MODIFY","sklearn\/metrics\/tests\/test_score_objects.py":"MODIFY","doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/metrics\/tests\/test_common.py":"MODIFY","doc\/modules\/neighbors.rst":"MODIFY","sklearn\/externals\/_arff.py":"MODIFY","examples\/inspection\/plot_partial_dependence.py":"MODIFY","doc\/modules\/model_evaluation.rst":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/binning.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY","sklearn\/ensemble\/tests\/test_gradient_boosting.py":"MODIFY","build_tools\/azure\/install.sh":"MODIFY","sklearn\/metrics\/_regression.py":"MODIFY","sklearn\/model_selection\/_search.py":"MODIFY"},"diff":{"doc\/developers\/advanced_installation.rst":[{"add":["376","For the upcoming FreeBSD 12.1 and 11.3 versions, OpenMP will be included in"],"delete":["376","For the upcomming FreeBSD 12.1 and 11.3 versions, OpenMP will be included in"]}],"sklearn\/decomposition\/_dict_learning.py":[{"add":["706","        avoid losing the history of the evolution.","1353","        Keeping them is useful in online settings, to avoid losing the"],"delete":["706","        avoid loosing the history of the evolution.","1353","        Keeping them is useful in online settings, to avoid loosing the"]}],"doc\/whats_new\/v0.20.rst":[{"add":["711","  centered with the train mean respectively during the fit phase and the"],"delete":["711","  centered with the train mean repsectively during the fit phase and the"]}],"sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_gradient_boosting.py":[{"add":["415","    # cannot check the predictions for binned values here."],"delete":["415","    # cannot check the predicitons for binned values here."]}],"doc\/modules\/computing.rst":[{"add":["531","(threads or processes) that are spawned in parallel can be controlled via the","668","    the optimal size of temporary arrays used by some algorithms."],"delete":["531","(threads or processes) that are spawned in parallel can be controled via the","668","    the optimal size of temporary arrays used by some algoritms."]}],"sklearn\/metrics\/tests\/test_score_objects.py":[{"add":["651","    # scoring dictionary returned is the same as calling each scorer separately","666","    separate_scores = {","672","        assert_allclose(value, separate_scores[score_name])"],"delete":["651","    # scoring dictionary returned is the same as calling each scorer seperately","666","    seperate_scores = {","672","        assert_allclose(value, seperate_scores[score_name])"]}],"doc\/whats_new\/v0.21.rst":[{"add":["297","  algorithm related to :class:`cluster.DBSCAN`, that has hyperparameters easier"],"delete":["297","  algoritm related to :class:`cluster.DBSCAN`, that has hyperparameters easier"]}],"sklearn\/metrics\/tests\/test_common.py":[{"add":["117","    # . Naming it with an unnormalized_ prefix is necessary for this module to"],"delete":["117","    # . Naming it with an unnormalized_ prefix is neccessary for this module to"]}],"doc\/modules\/neighbors.rst":[{"add":["583","  To maximise compatibility with all estimators, a safe choice is to always"],"delete":["583","  To maximise compatiblity with all estimators, a safe choice is to always"]}],"sklearn\/externals\/_arff.py":[{"add":["100","``STRING``. For nominal attributes, the ``attribute_type`` must be a list of"],"delete":["100","``STRING``. For nominal attributes, the ``atribute_type`` must be a list of"]}],"examples\/inspection\/plot_partial_dependence.py":[{"add":["16","The plots show four 1-way and two 1-way partial dependence plots (omitted for"],"delete":["16","The plots show four 1-way and two 1-way partial dependence plots (ommitted for"]}],"doc\/modules\/model_evaluation.rst":[{"add":["1722","the prediction :math:`\\hat{y}`, which induces the ranking function :math:`f`, the"],"delete":["1722","the prediction :math:`\\hat{y}`, which induces the ranking funtion :math:`f`, the"]}],"sklearn\/ensemble\/_hist_gradient_boosting\/binning.py":[{"add":["34","        randomly chosen to compute the quantiles. If ``None``, the whole data","109","        randomly chosen to compute the quantiles. If ``None``, the whole data","128","        constant across all features. This corresponds to the last bin, and"],"delete":["34","        randomly choosen to compute the quantiles. If ``None``, the whole data","109","        randomly choosen to compute the quantiles. If ``None``, the whole data","128","        constant accross all features. This corresponds to the last bin, and"]}],"doc\/whats_new\/v0.22.rst":[{"add":["801","  invalid model. This behavior occurred only in some border scenarios."],"delete":["801","  invalid model. This behavior occured only in some border scenarios."]}],"sklearn\/ensemble\/tests\/test_gradient_boosting.py":[{"add":["1313","    # initial estimator does not support sample weight"],"delete":["1313","    # inital estimator does not support sample weight"]}],"build_tools\/azure\/install.sh":[{"add":["13","    # The two version numbers are separated with a new line is piped to sort"],"delete":["13","    # The two version numbers are seperated with a new line is piped to sort"]}],"sklearn\/metrics\/_regression.py":[{"add":["719","        # 'Extreme stable', y_true any real number, y_pred > 0"],"delete":["719","        # 'Extreme stable', y_true any realy number, y_pred > 0"]}],"sklearn\/model_selection\/_search.py":[{"add":["950","        attribute will not be available.","1280","        attribute will not be available."],"delete":["950","        attribute will not be availble.","1280","        attribute will not be availble."]}]}},"4c29be44facbdaef188f84bdc8bf1190b2eebe07":{"changes":{"doc\/whats_new\/v0.23.rst":"MODIFY","sklearn\/gaussian_process\/tests\/test_gpr.py":"MODIFY","sklearn\/gaussian_process\/_gpr.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.23.rst":[{"add":["198","  :pr:`15503` by :user:`Sam Dixon` <sam-dixon>.","199","","200","- |Fix| Fixed bug in :class:`gaussian_process.GaussianProcessRegressor` that","201","  caused predicted standard deviations to only be between 0 and 1 when","202","  WhiteKernel is not used. :pr:`15782`","203","  by :user:`plgreenLIRU`.","399","- |Fix| :class:`cluster.AgglomerativeClustering` add specific error when","400","  distance matrix is not square and `affinity=precomputed`."],"delete":["198","  :pr:`15503` by :user:`Sam Dixon <sam-dixon>`.","220","","395","- |Fix| :class:`cluster.AgglomerativeClustering` add specific error when ","396","  distance matrix is not square and `affinity=precomputed`. "]}],"sklearn\/gaussian_process\/tests\/test_gpr.py":[{"add":["3","# Modified by: Pete Green <p.l.green@liverpool.ac.uk>","22","            assert_array_almost_equal, assert_array_equal,","23","            assert_allclose)","236","    \"\"\"","237","    Test normalization of the target values in GP","239","    Fitting non-normalizing GP on normalized y and fitting normalizing GP","240","    on unnormalized y should yield identical results. Note that, here,","241","    'normalized y' refers to y that has been made zero mean and unit","242","    variance.","243","","244","    \"\"\"","245","","246","    y_mean = np.mean(y)","247","    y_std = np.std(y)","248","    y_norm = (y - y_mean) \/ y_std","253","","260","    y_pred = y_pred * y_std + y_mean","261","    y_pred_std = y_pred_std * y_std","268","    y_cov = y_cov * y_std**2","270","","274","def test_large_variance_y():","275","    \"\"\"","276","    Here we test that, when noramlize_y=True, our GP can produce a","277","    sensible fit to training data whose variance is significantly","278","    larger than unity. This test was made in response to issue #15612.","279","","280","    GP predictions are verified against predictions that were made","281","    using GPy which, here, is treated as the 'gold standard'. Note that we","282","    only investigate the RBF kernel here, as that is what was used in the","283","    GPy implementation.","284","","285","    The following code can be used to recreate the GPy data:","286","","287","    --------------------------------------------------------------------------","288","    import GPy","289","","290","    kernel_gpy = GPy.kern.RBF(input_dim=1, lengthscale=1.)","291","    gpy = GPy.models.GPRegression(X, np.vstack(y_large), kernel_gpy)","292","    gpy.optimize()","293","    y_pred_gpy, y_var_gpy = gpy.predict(X2)","294","    y_pred_std_gpy = np.sqrt(y_var_gpy)","295","    --------------------------------------------------------------------------","296","    \"\"\"","297","","298","    # Here we utilise a larger variance version of the training data","299","    y_large = 10 * y","300","","301","    # Standard GP with normalize_y=True","302","    RBF_params = {'length_scale': 1.0}","303","    kernel = RBF(**RBF_params)","304","    gpr = GaussianProcessRegressor(kernel=kernel, normalize_y=True)","305","    gpr.fit(X, y_large)","306","    y_pred, y_pred_std = gpr.predict(X2, return_std=True)","307","","308","    # 'Gold standard' mean predictions from GPy","309","    y_pred_gpy = np.array([15.16918303,","310","                           -27.98707845,","311","                           -39.31636019,","312","                           14.52605515,","313","                           69.18503589])","314","","315","    # 'Gold standard' std predictions from GPy","316","    y_pred_std_gpy = np.array([7.78860962,","317","                               3.83179178,","318","                               0.63149951,","319","                               0.52745188,","320","                               0.86170042])","321","","322","    # Based on numerical experiments, it's reasonable to expect our","323","    # GP's mean predictions to get within 7% of predictions of those","324","    # made by GPy.","325","    assert_allclose(y_pred, y_pred_gpy, rtol=0.07, atol=0)","326","","327","    # Based on numerical experiments, it's reasonable to expect our","328","    # GP's std predictions to get within 15% of predictions of those","329","    # made by GPy.","330","    assert_allclose(y_pred_std, y_pred_std_gpy, rtol=0.15, atol=0)","331","","332",""],"delete":["21","            assert_array_almost_equal, assert_array_equal)","234","    # Test normalization of the target values in GP","236","    # Fitting non-normalizing GP on normalized y and fitting normalizing GP","237","    # on unnormalized y should yield identical results","238","    y_mean = y.mean(0)","239","    y_norm = y - y_mean","250","    y_pred = y_mean + y_pred"]}],"sklearn\/gaussian_process\/_gpr.py":[{"add":["3","# Modified by: Pete Green <p.l.green@liverpool.ac.uk>","94","    normalize_y : boolean, optional (default: False)","95","        Whether the target values y are normalized, the mean and variance of","96","        the target values are set equal to 0 and 1 respectively. This is","97","        recommended for cases where zero-mean, unit-variance priors are used.","98","        Note that, in this implementation, the normalisation is reversed","99","        before the GP predictions are reported.","100","","101","        .. versionchanged:: 0.23","195","            self._y_train_std = np.std(y, axis=0)","196","","197","            # Remove mean and make unit variance","198","            y = (y - self._y_train_mean) \/ self._y_train_std","199","","202","            self._y_train_std = 1","337","","338","            # undo normalisation","339","            y_mean = self._y_train_std * y_mean + self._y_train_mean","340","","344","","345","                # undo normalisation","346","                y_cov = y_cov * self._y_train_std**2","347","","370","","371","                # undo normalisation","372","                y_var = y_var * self._y_train_std**2","373",""],"delete":["3","#","94","    normalize_y : bool, default=False","95","        Whether the target values y are normalized, i.e., the mean of the","96","        observed target values become zero. This parameter should be set to","97","        True if the target values' mean is expected to differ considerable from","98","        zero. When enabled, the normalization effectively modifies the GP's","99","        prior based on the data, which contradicts the likelihood principle;","100","        normalization is thus disabled per default.","194","            # demean y","195","            y = y - self._y_train_mean","332","            y_mean = self._y_train_mean + y_mean  # undo normal."]}]}},"c96e0958da46ebef482a4084cdda3285d5f5ad23":{"changes":{"sklearn\/impute\/tests\/test_impute.py":"MODIFY","sklearn\/impute\/_iterative.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY"},"diff":{"sklearn\/impute\/tests\/test_impute.py":[{"add":["28","from sklearn.exceptions import ConvergenceWarning","866","                               imputation_order='descending',","1275","","1276","","1277","@pytest.mark.parametrize(","1278","    \"order, idx_order\",","1279","    [","1280","        (\"ascending\", [3, 4, 2, 0, 1]),","1281","        (\"descending\", [1, 0, 2, 4, 3])","1282","    ]","1283",")","1284","def test_imputation_order(order, idx_order):","1285","    # regression test for #15393","1286","    rng = np.random.RandomState(42)","1287","    X = rng.rand(100, 5)","1288","    X[:50, 1] = np.nan","1289","    X[:30, 0] = np.nan","1290","    X[:20, 2] = np.nan","1291","    X[:10, 4] = np.nan","1292","","1293","    with pytest.warns(ConvergenceWarning):","1294","        trs = IterativeImputer(max_iter=1,","1295","                               imputation_order=order,","1296","                               random_state=0).fit(X)","1297","        idx = [x.feat_idx for x in trs.imputation_sequence_]","1298","        assert idx == idx_order"],"delete":[]}],"sklearn\/impute\/_iterative.py":[{"add":["422","                                     kind='mergesort')[n:]","426","                                     kind='mergesort')[n:][::-1]"],"delete":["422","                                     kind='mergesort')[n:][::-1]","426","                                     kind='mergesort')[n:]"]}],"doc\/whats_new\/v0.22.rst":[{"add":["429","- |Fix| Fixed a bug in :class:`impute.IterativeImputer` where features where","430","  imputed in the reverse desired order with ``imputation_order`` either","431","  ``\"ascending\"`` or ``\"descending\"``. :pr:`15393` by","432","  :user:`Venkatachalam N <venkyyuvy>`.","433",""],"delete":[]}]}},"4b7afeeabd430073168649045fac33c8e80a4f50":{"changes":{"sklearn\/cluster\/tests\/test_hierarchical.py":"MODIFY"},"diff":{"sklearn\/cluster\/tests\/test_hierarchical.py":[{"add":["16","from sklearn.metrics.cluster import adjusted_rand_score"],"delete":["16","from sklearn.metrics.cluster.supervised import adjusted_rand_score"]}]}},"0c0b6dad69eed6bf42e98ef5c865718ec49bb932":{"changes":{"sklearn\/preprocessing\/_data.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY","sklearn\/preprocessing\/tests\/test_data.py":"MODIFY"},"diff":{"sklearn\/preprocessing\/_data.py":[{"add":["2264","        # Due to floating-point precision error in `np.nanpercentile`,","2265","        # make sure that quantiles are monotonically increasing.","2266","        # Upstream issue in numpy:","2267","        # https:\/\/github.com\/numpy\/numpy\/issues\/14685","2268","        self.quantiles_ = np.maximum.accumulate(self.quantiles_)","2312","        # due to floating-point precision error in `np.nanpercentile`,","2313","        # make sure the quantiles are monotonically increasing","2314","        # Upstream issue in numpy:","2315","        # https:\/\/github.com\/numpy\/numpy\/issues\/14685","2316","        self.quantiles_ = np.maximum.accumulate(self.quantiles_)"],"delete":[]}],"doc\/whats_new\/v0.22.rst":[{"add":["819","- |Fix| :class:`preprocessing.QuantileTransformer` now guarantees the ","820","  `quantiles_` attribute to be completely sorted in non-decreasing manner.","821","  :pr:`15751` by :user:`Tirth Patel <tirthasheshpatel>`.","822",""],"delete":[]}],"sklearn\/preprocessing\/tests\/test_data.py":[{"add":["27","from sklearn.utils._testing import _convert_container","1535","@pytest.mark.parametrize(\"array_type\", ['array', 'sparse'])","1536","def test_quantile_transformer_sorted_quantiles(array_type):","1537","    # Non-regression test for:","1538","    # https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/15733","1539","    # Taken from upstream bug report:","1540","    # https:\/\/github.com\/numpy\/numpy\/issues\/14685","1541","    X = np.array([0, 1, 1, 2, 2, 3, 3, 4, 5, 5, 1, 1, 9, 9, 9, 8, 8, 7] * 10)","1542","    X = 0.1 * X.reshape(-1, 1)","1543","    X = _convert_container(X, array_type)","1544","","1545","    n_quantiles = 100","1546","    qt = QuantileTransformer(n_quantiles=n_quantiles).fit(X)","1547","","1548","    # Check that the estimated quantile threasholds are monotically","1549","    # increasing:","1550","    quantiles = qt.quantiles_[:, 0]","1551","    assert len(quantiles) == 100","1552","    assert all(np.diff(quantiles) >= 0)","1553","","1554",""],"delete":[]}]}},"94f877b55efb55b4cccf8fe03f8d299abca3eb7a":{"changes":{"sklearn\/metrics\/_plot\/confusion_matrix.py":"MODIFY","doc\/whats_new\/v0.23.rst":"MODIFY","sklearn\/metrics\/_plot\/tests\/test_plot_confusion_matrix.py":"MODIFY"},"diff":{"sklearn\/metrics\/_plot\/confusion_matrix.py":[{"add":["63","            the format specification is 'd' or '.2g' whichever is shorter.","92","","95","","96","                if values_format is None:","97","                    text_cm = format(cm[i, j], '.2g')","98","                    if cm.dtype.kind != 'f':","99","                        text_d = format(cm[i, j], 'd')","100","                        if len(text_d) < len(text_cm):","101","                            text_cm = text_d","102","                else:","103","                    text_cm = format(cm[i, j], values_format)","104","","105","                self.text_[i, j] = ax.text(","106","                    j, i, text_cm,","107","                    ha=\"center\", va=\"center\",","108","                    color=color)","174","        the format specification is 'd' or '.2g' whichever is shorter."],"delete":["63","            the format specification is '.2g'.","85","","90","            if values_format is None:","91","                values_format = '.2g'","97","                self.text_[i, j] = ax.text(j, i,","98","                                           format(cm[i, j], values_format),","99","                                           ha=\"center\", va=\"center\",","100","                                           color=color)","166","        the format specification is '.2g'."]}],"doc\/whats_new\/v0.23.rst":[{"add":["264","- |API| Changed the formatting of values in","265","  :meth:`metrics.ConfusionMatrixDisplay.plot` and","266","  :func:`metrics.plot_confusion_matrix` to pick the shorter format (either '2g'","267","  or 'd'). :pr:`16159` by :user:`Rick Mackenbach <Rick-Mackenbach>` and ","268","  `Thomas Fan`_.","269",""],"delete":[]}],"sklearn\/metrics\/_plot\/tests\/test_plot_confusion_matrix.py":[{"add":["23","","265","","266","","267","def test_confusion_matrix_standard_format(pyplot):","268","    cm = np.array([[10000000, 0], [123456, 12345678]])","269","    plotted_text = ConfusionMatrixDisplay(cm, [False, True]).plot().text_","270","    # Values should be shown as whole numbers 'd',","271","    # except the first number which should be shown as 1e+07 (longer length)","272","    # and the last number will be showns as 1.2e+07 (longer length)","273","    test = [t.get_text() for t in plotted_text.ravel()]","274","    assert test == ['1e+07', '0', '123456', '1.2e+07']","275","","276","    cm = np.array([[0.1, 10], [100, 0.525]])","277","    plotted_text = ConfusionMatrixDisplay(cm, [False, True]).plot().text_","278","    # Values should now formatted as '.2g', since there's a float in","279","    # Values are have two dec places max, (e.g 100 becomes 1e+02)","280","    test = [t.get_text() for t in plotted_text.ravel()]","281","    assert test == ['0.1', '10', '1e+02', '0.53']"],"delete":[]}]}},"981fa7b8f1a6e273e05df6b173eee311f6d11a26":{"changes":{"sklearn\/multioutput.py":"MODIFY","sklearn\/tests\/test_multioutput.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY"},"diff":{"sklearn\/multioutput.py":[{"add":["362","    @property","363","    def predict_proba(self):","385","            raise AttributeError(\"The base estimator should \"","386","                                 \"implement predict_proba method\")","387","        return self._predict_proba","389","    def _predict_proba(self, X):"],"delete":["362","    def predict_proba(self, X):","384","            raise ValueError(\"The base estimator should implement \"","385","                             \"predict_proba method\")"]}],"sklearn\/tests\/test_multioutput.py":[{"add":["177","# check multioutput has predict_proba","178","def test_hasattr_multi_output_predict_proba():","179","    # default SGDClassifier has loss='hinge'","180","    # which does not expose a predict_proba method","181","    sgd_linear_clf = SGDClassifier(random_state=1, max_iter=5)","182","    multi_target_linear = MultiOutputClassifier(sgd_linear_clf)","183","    multi_target_linear.fit(X, y)","184","    assert not hasattr(multi_target_linear, \"predict_proba\")","185","","186","    # case where predict_proba attribute exists","187","    sgd_linear_clf = SGDClassifier(loss='log', random_state=1, max_iter=5)","188","    multi_target_linear = MultiOutputClassifier(sgd_linear_clf)","189","    multi_target_linear.fit(X, y)","190","    assert hasattr(multi_target_linear, \"predict_proba\")","191","","192","","217","    with pytest.raises(AttributeError, match=err_msg):","396","    with pytest.raises(NotFittedError):","397","        moc.predict_proba"],"delete":["201","    with pytest.raises(ValueError, match=err_msg):","380","    assert_raises(NotFittedError, moc.predict_proba, y)"]}],"doc\/whats_new\/v0.22.rst":[{"add":["659","- |Fix| :class:`multioutput.MultiOutputClassifier` now has `predict_proba`","660","  as property and can be checked with `hasattr`.","661","  :issue:`15488` :pr:`15490` by :user:`Rebekah Kim <rebekahkim>`","662",""],"delete":[]}]}},"f4da713c44d9529da2d82d76e0c0c6ef7e0e4795":{"changes":{"sklearn\/metrics\/_plot\/precision_recall_curve.py":"MODIFY","sklearn\/metrics\/_plot\/tests\/test_plot_precision_recall.py":"MODIFY"},"diff":{"sklearn\/metrics\/_plot\/precision_recall_curve.py":[{"add":["163","                                                pos_label=pos_label,"],"delete":[]}],"sklearn\/metrics\/_plot\/tests\/test_plot_precision_recall.py":[{"add":["9","from sklearn.datasets import load_breast_cancer","135","","136","","137","def test_precision_recall_curve_string_labels(pyplot):","138","    # regression test #15738","139","    cancer = load_breast_cancer()","140","    X = cancer.data","141","    y = cancer.target_names[cancer.target]","142","","143","    lr = make_pipeline(StandardScaler(), LogisticRegression())","144","    lr.fit(X, y)","145","    for klass in cancer.target_names:","146","        assert klass in lr.classes_","147","    disp = plot_precision_recall_curve(lr, X, y)","148","","149","    y_pred = lr.predict_proba(X)[:, 1]","150","    avg_prec = average_precision_score(y, y_pred,","151","                                       pos_label=lr.classes_[1])","152","","153","    assert disp.average_precision == pytest.approx(avg_prec)","154","    assert disp.estimator_name == lr.__class__.__name__"],"delete":[]}]}},"034c021fb274076982cb342cf1d9aaad72f0873e":{"changes":{"sklearn\/multioutput.py":"MODIFY"},"diff":{"sklearn\/multioutput.py":[{"add":["550","        chain order.","711","        chain order."],"delete":["550","        chain orders.","711","        chain orders."]}]}},"b9692a6becca64daeac28c6156e847f2989ce45e":{"changes":{"sklearn\/decomposition\/_kernel_pca.py":"MODIFY","doc\/whats_new\/v0.23.rst":"MODIFY"},"diff":{"sklearn\/decomposition\/_kernel_pca.py":[{"add":["218","                                   np.zeros_like(self.alphas_).T)"],"delete":["218","                                   np.empty_like(self.alphas_).T)"]}],"doc\/whats_new\/v0.23.rst":[{"add":["148","- |Fix| Fixed bug that was causing :class:`decomposition.KernelPCA` to sometimes","149","  raise `invalid value encountered in multiply` during `fit`.","150","  :pr:`16718` by :user:`Gui Miotto <gui-miotto>`.","151",""],"delete":[]}]}},"53082e92af18d533061f62d457e781c3c3617a4d":{"changes":{"doc\/conf.py":"MODIFY"},"diff":{"doc\/conf.py":[{"add":["340","","341","    # searchindex only exist when generating html","342","    if app.builder.name != 'html':","343","        return","344",""],"delete":[]}]}},"4ce39dbc699cd9cd33ab2b67ca029698713aec3e":{"changes":{"sklearn\/tests\/test_common.py":"MODIFY","sklearn\/utils\/__init__.py":"MODIFY","sklearn\/utils\/tests\/test_estimator_checks.py":"MODIFY"},"diff":{"sklearn\/tests\/test_common.py":[{"add":[],"delete":["80","        if name.startswith(\"_\"):","81","            continue"]}],"sklearn\/utils\/__init__.py":[{"add":["5","from importlib import import_module","15","from pathlib import Path","1162","    modules_to_ignore = {\"tests\", \"externals\", \"setup\", \"conftest\"}","1163","    root = str(Path(__file__).parent.parent)  # sklearn package","1164","    # Ignore deprecation warnings triggered at import time and from walking","1165","    # packages","1166","    with ignore_warnings(category=FutureWarning):","1167","        for importer, modname, ispkg in pkgutil.walk_packages(","1168","                path=[root], prefix='sklearn.'):","1169","            mod_parts = modname.split(\".\")","1170","            if (any(part in modules_to_ignore for part in mod_parts)","1171","                    or '._' in modname):","1172","                continue","1173","            module = import_module(modname)","1174","            classes = inspect.getmembers(module, inspect.isclass)","1175","            classes = [(name, est_cls) for name, est_cls in classes","1176","                       if not name.startswith(\"_\")]","1177","","1178","            # TODO: Remove when FeatureHasher is implemented in PYPY","1179","            # Skips FeatureHasher for PYPY","1180","            if IS_PYPY and 'feature_extraction' in modname:","1181","                classes = [(name, est_cls) for name, est_cls in classes","1182","                           if name == \"FeatureHasher\"]","1183","","1184","            all_classes.extend(classes)"],"delete":["1133","    import sklearn","1161","    # get parent folder","1162","    path = sklearn.__path__","1163","    for importer, modname, ispkg in pkgutil.walk_packages(","1164","            path=path, prefix='sklearn.', onerror=lambda x: None):","1165","        if \".tests.\" in modname or \"externals\" in modname:","1166","            continue","1167","        if IS_PYPY and ('_svmlight_format' in modname or","1168","                        'feature_extraction._hashing' in modname):","1169","            continue","1170","        # Ignore deprecation warnings triggered at import time.","1171","        with ignore_warnings(category=FutureWarning):","1172","            module = __import__(modname, fromlist=\"dummy\")","1173","        classes = inspect.getmembers(module, inspect.isclass)","1174","        all_classes.extend(classes)"]}],"sklearn\/utils\/tests\/test_estimator_checks.py":[{"add":["36","from sklearn.utils import all_estimators","575","def test_all_estimators_all_public():","576","    # all_estimator should not fail when pytest is not installed and return","577","    # only public estimators","578","    estimators = all_estimators()","579","    for est in estimators:","580","        assert not est.__class__.__name__.startswith(\"_\")","581","","582",""],"delete":[]}]}},"5a4340834d23c4bdcd813ccda24a690ae174c168":{"changes":{"sklearn\/__init__.py":"MODIFY","sklearn\/tests\/test_docstring_parameters.py":"MODIFY","sklearn\/utils\/fixes.py":"MODIFY","sklearn\/experimental\/enable_iterative_imputer.py":"MODIFY","sklearn\/covariance\/_graph_lasso.py":"MODIFY","sklearn\/svm\/_classes.py":"MODIFY","doc\/developers\/contributing.rst":"MODIFY","sklearn\/manifold\/_t_sne.py":"MODIFY","sklearn\/externals\/_arff.py":"MODIFY","sklearn\/svm\/_base.py":"MODIFY","sklearn\/impute\/__init__.py":"MODIFY","sklearn\/svm\/tests\/test_svm.py":"MODIFY","sklearn\/manifold\/_isomap.py":"MODIFY","sklearn\/linear_model\/_least_angle.py":"MODIFY","sklearn\/linear_model\/_coordinate_descent.py":"MODIFY","azure-pipelines.yml":"MODIFY","sklearn\/_build_utils\/deprecated_modules.py":"MODIFY","sklearn\/manifold\/tests\/test_t_sne.py":"MODIFY","sklearn\/ensemble\/tests\/test_forest.py":"MODIFY","sklearn\/ensemble\/__init__.py":"MODIFY","sklearn\/linear_model\/_stochastic_gradient.py":"MODIFY","sklearn\/model_selection\/_split.py":"MODIFY","sklearn\/utils\/metaestimators.py":"MODIFY","doc\/developers\/maintainer.rst":"MODIFY","sklearn\/utils\/_pprint.py":"MODIFY","sklearn\/tests\/test_random_projection.py":"MODIFY","sklearn\/experimental\/enable_hist_gradient_boosting.py":"MODIFY","sklearn\/ensemble\/_base.py":"MODIFY","sklearn\/dummy.py":"MODIFY"},"diff":{"sklearn\/__init__.py":[{"add":["62","    # mypy error: Cannot determine type of '__SKLEARN_SETUP__'","63","    __SKLEARN_SETUP__  # type: ignore"],"delete":["62","    __SKLEARN_SETUP__"]}],"sklearn\/tests\/test_docstring_parameters.py":[{"add":["34","        pckg[1] for pckg in walk_packages(","35","            prefix='sklearn.',","36","            # mypy error: Module has no attribute \"__path__\"","37","            path=sklearn.__path__)  # type: ignore  # mypy issue #1422"],"delete":["34","        pckg[1] for pckg in walk_packages(prefix='sklearn.',","35","                                          path=sklearn.__path__)"]}],"sklearn\/utils\/fixes.py":[{"add":["41","    # mypy error: Name 'lobpcg' already defined (possibly by an import)","42","    from ..externals._lobpcg import lobpcg  # type: ignore  # noqa","49","    # mypy error: Name 'pinvh' already defined (possibly by an import)","50","    from scipy.linalg import pinvh  # type: ignore  # noqa"],"delete":["41","    from ..externals._lobpcg import lobpcg  # noqa","48","    from scipy.linalg import pinvh # noqa"]}],"sklearn\/experimental\/enable_iterative_imputer.py":[{"add":["17","# use settattr to avoid mypy errors when monkeypatching","18","setattr(impute, 'IterativeImputer', IterativeImputer)"],"delete":["17","impute.IterativeImputer = IterativeImputer"]}],"sklearn\/covariance\/_graph_lasso.py":[{"add":["22","# mypy error: Module 'sklearn.linear_model' has no attribute '_cd_fast'","23","from ..linear_model import _cd_fast as cd_fast  # type: ignore"],"delete":["22","from ..linear_model import _cd_fast as cd_fast"]}],"sklearn\/svm\/_classes.py":[{"add":["972","    # mypy error: Decorated property not supported","973","    @deprecated(  # type: ignore","980","    # mypy error: Decorated property not supported","981","    @deprecated(  # type: ignore","1308","    # mypy error: Decorated property not supported","1309","    @deprecated(  # type: ignore","1316","    # mypy error: Decorated property not supported","1317","    @deprecated(  # type: ignore"],"delete":["972","    @deprecated(","979","    @deprecated(","1306","    @deprecated(","1313","    @deprecated("]}],"doc\/developers\/contributing.rst":[{"add":["217","       $ pip install cython pytest pytest-cov flake8 mypy","370","","416","* A moderate use of type annotations is encouraged but is not mandatory. See","417","  [mypy quickstart](https:\/\/mypy.readthedocs.io\/en\/latest\/getting_started.html)","418","  for an introduction, as well as [pandas contributing documentation](","419","  https:\/\/pandas.pydata.org\/pandas-docs\/stable\/development\/contributing.html#type-hints)","420","  for style guidelines. Whether you add type annotation or not::","421","","422","    mypy --ignore-missing-import sklearn","423","","424","  must not produce new errors in your pull request. Using `# type: ignore` annotation can be a workaround for a few cases that are not supported by mypy, in particular,","425","   - when importing C or Cython modules","426","   - on properties with decorators","427",""],"delete":["217","       $ pip install cython pytest pytest-cov flake8"]}],"sklearn\/manifold\/_t_sne.py":[{"add":["24","# mypy error: Module 'sklearn.manifold' has no attribute '_barnes_hut_tsne'","25","from . import _barnes_hut_tsne  # type: ignore"],"delete":["24","from . import _barnes_hut_tsne"]}],"sklearn\/externals\/_arff.py":[{"add":["150","from typing import Optional","151","","322","    message : Optional[str] = None"],"delete":["320","    message = None"]}],"sklearn\/svm\/_base.py":[{"add":["5","# mypy error: error: Module 'sklearn.svm' has no attribute '_libsvm'","6","# (and same for other imports)","7","from . import _libsvm as libsvm  # type: ignore","8","from .import _liblinear as liblinear  # type: ignore","9","from . import _libsvm_sparse as libsvm_sparse  # type: ignore"],"delete":["5","from . import _libsvm as libsvm","6","from .import _liblinear as liblinear","7","from . import _libsvm_sparse as libsvm_sparse"]}],"sklearn\/impute\/__init__.py":[{"add":["1","import typing","6","if typing.TYPE_CHECKING:","7","    # Avoid errors in type checkers (e.g. mypy) for experimental estimators.","8","    # TODO: remove this check once the estimator is no longer experimental.","9","    from ._iterative import IterativeImputer  # noqa","10",""],"delete":[]}],"sklearn\/svm\/tests\/test_svm.py":[{"add":["30","# mypy error: Module 'sklearn.svm' has no attribute '_libsvm'","31","from sklearn.svm import _libsvm  # type: ignore"],"delete":["30","from sklearn.svm import _libsvm"]}],"sklearn\/manifold\/_isomap.py":[{"add":["169","    # mypy error: Decorated property not supported","170","    @deprecated(  # type: ignore","171","        \"Attribute `training_data_` was deprecated in version 0.22 and\"","172","        \" will be removed in 0.24.\"","173","    )"],"delete":["169","    @deprecated(\"Attribute `training_data_` was deprecated in version 0.22 and\"","170","                \" will be removed in 0.24.\")"]}],"sklearn\/linear_model\/_least_angle.py":[{"add":["21","# mypy error: Module 'sklearn.utils' has no attribute 'arrayfuncs'","22","from ..utils import arrayfuncs, as_float_array  # type: ignore"],"delete":["21","from ..utils import arrayfuncs, as_float_array"]}],"sklearn\/linear_model\/_coordinate_descent.py":[{"add":["27","# mypy error: Module 'sklearn.linear_model' has no attribute '_cd_fast'","28","from . import _cd_fast as cd_fast  # type: ignore"],"delete":["27","from . import _cd_fast as cd_fast"]}],"azure-pipelines.yml":[{"add":["19","    - bash: |","20","        conda create --name flake8_env --yes python=3.8","21","        conda activate flake8_env","22","        pip install flake8 mypy==0.770","30","          conda activate flake8_env","35","        if [[ $BUILD_SOURCEVERSIONMESSAGE =~ \\[lint\\ skip\\] ]]; then","36","          # skip linting","37","          echo \"Skipping linting\"","38","          exit 0","39","        else","40","          conda activate flake8_env","41","          mypy sklearn\/ --ignore-missing-imports","42","        fi","43","      displayName: Run mypy","44","    - bash: |"],"delete":["19","    - bash: conda create --name flake8_env --yes flake8","27","          source activate flake8_env"]}],"sklearn\/_build_utils\/deprecated_modules.py":[{"add":["273","# mypy error: Module X has no attribute y (typically for C extensions)","274","from . import {new_module_name}  # type: ignore"],"delete":["273","from . import {new_module_name}"]}],"sklearn\/manifold\/tests\/test_t_sne.py":[{"add":["23","# mypy error: Module 'sklearn.manifold' has no attribute '_barnes_hut_tsne'","24","from sklearn.manifold import _barnes_hut_tsne  # type: ignore"],"delete":["23","from sklearn.manifold import _barnes_hut_tsne"]}],"sklearn\/ensemble\/tests\/test_forest.py":[{"add":["17","from typing import Dict, Any","103","FOREST_ESTIMATORS: Dict[str, Any] = dict()","108","FOREST_CLASSIFIERS_REGRESSORS: Dict[str, Any] = FOREST_CLASSIFIERS.copy()","1262","# mypy error: Variable \"DEFAULT_JOBLIB_BACKEND\" is not valid type","1263","class MyBackend(DEFAULT_JOBLIB_BACKEND):  # type: ignore"],"delete":["102","FOREST_ESTIMATORS = dict()","107","FOREST_CLASSIFIERS_REGRESSORS = FOREST_CLASSIFIERS.copy()","1261","class MyBackend(DEFAULT_JOBLIB_BACKEND):"]}],"sklearn\/ensemble\/__init__.py":[{"add":["4","import typing","24","if typing.TYPE_CHECKING:","25","    # Avoid errors in type checkers (e.g. mypy) for experimental estimators.","26","    # TODO: remove this check once the estimator is no longer experimental.","27","    from ._hist_gradient_boosting.gradient_boosting import (  # noqa","28","        HistGradientBoostingRegressor, HistGradientBoostingClassifier","29","    )"],"delete":[]}],"sklearn\/linear_model\/_stochastic_gradient.py":[{"add":["289","    # mypy error: Decorated property not supported","290","    @deprecated(\"Attribute standard_coef_ was deprecated \"  # type: ignore","296","    # mypy error: Decorated property not supported","297","    @deprecated(  # type: ignore","298","        \"Attribute standard_intercept_ was deprecated \"","299","        \"in version 0.23 and will be removed in 0.25.\"","300","    )","305","    # mypy error: Decorated property not supported","306","    @deprecated(\"Attribute average_coef_ was deprecated \"  # type: ignore","312","    # mypy error: Decorated property not supported","313","    @deprecated(\"Attribute average_intercept_ was deprecated \"  # type: ignore"],"delete":["289","    @deprecated(\"Attribute standard_coef_ was deprecated \"","295","    @deprecated(\"Attribute standard_intercept_ was deprecated \"","296","                \"in version 0.23 and will be removed in 0.25.\")","301","    @deprecated(\"Attribute average_coef_ was deprecated \"","307","    @deprecated(\"Attribute average_intercept_ was deprecated \""]}],"sklearn\/model_selection\/_split.py":[{"add":["2146","# Use setattr to avoid mypy errors when monkeypatching.","2147","setattr(train_test_split, '__test__', False)"],"delete":["2146","train_test_split.__test__ = False"]}],"sklearn\/utils\/metaestimators.py":[{"add":["4","from typing import List, Any","20","    steps: List[Any]","21",""],"delete":[]}],"doc\/developers\/maintainer.rst":[{"add":["291","To avoid type checker (e.g. mypy) errors a direct import of experimenal","292","estimators should be done in the parent module, protected by the","293","``if typing.TYPE_CHECKING`` check. See `sklearn\/ensemble\/__init__.py","294","<https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/master\/sklearn\/ensemble\/__init__.py>`_,","295","or `sklearn\/impute\/__init__.py","296","<https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/master\/sklearn\/impute\/__init__.py>`_","297","for an example.","298",""],"delete":[]}],"sklearn\/utils\/_pprint.py":[{"add":["326","    # mypy error: \"Type[PrettyPrinter]\" has no attribute \"_dispatch\"","327","    _dispatch = pprint.PrettyPrinter._dispatch.copy()  # type: ignore"],"delete":["326","    _dispatch = pprint.PrettyPrinter._dispatch.copy()"]}],"sklearn\/tests\/test_random_projection.py":[{"add":["2","from typing import List, Any","26","all_sparse_random_matrix: List[Any] = [_sparse_random_matrix]","27","all_dense_random_matrix: List[Any] = [_gaussian_random_matrix]","30","all_SparseRandomProjection: List[Any] = [SparseRandomProjection]","31","all_DenseRandomProjection: List[Any] = [GaussianRandomProjection]"],"delete":["25","all_sparse_random_matrix = [_sparse_random_matrix]","26","all_dense_random_matrix = [_gaussian_random_matrix]","29","all_SparseRandomProjection = [SparseRandomProjection]","30","all_DenseRandomProjection = [GaussianRandomProjection]"]}],"sklearn\/experimental\/enable_hist_gradient_boosting.py":[{"add":["28","# use settattr to avoid mypy errors when monkeypatching","29","setattr(ensemble, \"HistGradientBoostingClassifier\",","30","        HistGradientBoostingClassifier)","31","setattr(ensemble, \"HistGradientBoostingRegressor\",","32","        HistGradientBoostingRegressor)","33",""],"delete":["28","ensemble.HistGradientBoostingClassifier = HistGradientBoostingClassifier","29","ensemble.HistGradientBoostingRegressor = HistGradientBoostingRegressor"]}],"sklearn\/ensemble\/_base.py":[{"add":["8","from typing import List","109","    _required_parameters: List[str] = []"],"delete":["108","    _required_parameters = []"]}],"sklearn\/dummy.py":[{"add":["397","    # mypy error: Decorated property not supported","398","    @deprecated(  # type: ignore","625","    # mypy error: Decorated property not supported","626","    @deprecated(  # type: ignore"],"delete":["397","    @deprecated(","624","    @deprecated("]}]}},"3109add033757aa8c69cfa2167316ca236b15489":{"changes":{"sklearn\/inspection\/_partial_dependence.py":"MODIFY"},"diff":{"sklearn\/inspection\/_partial_dependence.py":[{"add":["593","    if (is_classifier(estimator) and","594","            hasattr(estimator, 'classes_') and","595","            np.size(estimator.classes_) > 2):"],"delete":["27","from ..tree._tree import DTYPE","594","    if hasattr(estimator, 'classes_') and np.size(estimator.classes_) > 2:"]}]}},"728108383e6655e8f2f86c1c941ff365c7cf937e":{"changes":{"doc\/modules\/classes.rst":"MODIFY"},"diff":{"doc\/modules\/classes.rst":[{"add":[],"delete":["1539","   utils.arrayfuncs.cholesky_delete"]}]}},"862a726ca98c4bc30019cd88a1e802bbc49cbfe7":{"changes":{"examples\/inspection\/plot_linear_model_coefficient_interpretation.py":"MODIFY"},"diff":{"examples\/inspection\/plot_linear_model_coefficient_interpretation.py":[{"add":["218","# The AGE coefficient is expressed in \"dollars\/hour per living years\" while the","219","# EDUCATION one is expressed in \"dollars\/hour per years of education\". This","220","# representation of the coefficients has the advantage of making clear the","221","# practical predictions of the model: an increase of :math:`1` year in AGE","222","# means a decrease of :math:`0.030867` dollars\/hour, while an increase of","223","# :math:`1` year in EDUCATION means an increase of :math:`0.054699`","224","# dollars\/hour. On the other hand, categorical variables (as UNION or SEX) are","225","# adimensional numbers taking either the value 0 or 1. Their coefficients","226","# are expressed in dollars\/hour. Then, we cannot compare the magnitude of","227","# different coefficients since the features have different natural scales, and","228","# hence value ranges, because of their different unit of measure. This is more","229","# evident if we plot the coefficients."],"delete":["218","# The AGE coefficient is expressed in","219","# :math:`$\/hours\/(living\\ years)` while the EDUCATION one is expressed","220","# in :math:`$\/hours\/(years\\ of\\ education)`.","221","# This representation of the coefficients has the advantage of making clear","222","# the practical predictions of the model:","223","# an increase of :math:`1` year in AGE means a decrease of :math:`0.030867$`,","224","# while an increase of :math:`1` year in EDUCATION means an increase of","225","# :math:`0.054699$`.","226","# On the other hand, categorical variables (as UNION or SEX) are adimensional","227","# numbers taking the value either of 0 or 1. Their coefficients are expressed","228","# in :math:`$\/hours`. Then, we cannot compare the magnitude of different","229","# coefficients since the features have different natural scales, and hence","230","# value ranges, because of their different unit of measure.","231","# This is more evident if we plot the coefficients."]}]}},"1ba06518c5c4c7c1865110a8f34c4da64d8e478f":{"changes":{"sklearn\/datasets\/_openml.py":"MODIFY","sklearn\/datasets\/_svmlight_format_io.py":"MODIFY","sklearn\/datasets\/_rcv1.py":"MODIFY","sklearn\/datasets\/_base.py":"MODIFY","sklearn\/datasets\/_california_housing.py":"MODIFY","sklearn\/datasets\/_species_distributions.py":"MODIFY","sklearn\/datasets\/_covtype.py":"MODIFY","sklearn\/datasets\/_lfw.py":"MODIFY","sklearn\/linear_model\/tests\/test_omp.py":"MODIFY","sklearn\/datasets\/_samples_generator.py":"MODIFY","sklearn\/datasets\/_olivetti_faces.py":"MODIFY","sklearn\/datasets\/_kddcup99.py":"MODIFY","sklearn\/datasets\/_twenty_newsgroups.py":"MODIFY","sklearn\/datasets\/tests\/test_base.py":"MODIFY"},"diff":{"sklearn\/datasets\/_openml.py":[{"add":["25","from ..utils.validation import _deprecate_positional_args","611","@_deprecate_positional_args","612","def fetch_openml(name=None, *, version='active', data_id=None, data_home=None,"],"delete":["610","def fetch_openml(name=None, version='active', data_id=None, data_home=None,"]}],"sklearn\/datasets\/_svmlight_format_io.py":[{"add":["27","from ..utils.validation import _deprecate_positional_args","40","@_deprecate_positional_args","41","def load_svmlight_file(f, *, n_features=None, dtype=np.float64,","155","    return tuple(load_svmlight_files([f], n_features=n_features,","156","                                     dtype=dtype,","157","                                     multilabel=multilabel,","158","                                     zero_based=zero_based,","159","                                     query_id=query_id,","160","                                     offset=offset,","161","                                     length=length))","205","@_deprecate_positional_args","206","def load_svmlight_files(files, *, n_features=None, dtype=np.float64,","390","@_deprecate_positional_args","391","def dump_svmlight_file(X, y, f, *, zero_based=True, comment=None,","392","                       query_id=None,"],"delete":["39","def load_svmlight_file(f, n_features=None, dtype=np.float64,","153","    return tuple(load_svmlight_files([f], n_features, dtype, multilabel,","154","                                     zero_based, query_id, offset, length))","198","def load_svmlight_files(files, n_features=None, dtype=np.float64,","382","def dump_svmlight_file(X, y, f,  zero_based=True, comment=None, query_id=None,"]}],"sklearn\/datasets\/_rcv1.py":[{"add":["27","from ..utils.validation import _deprecate_positional_args","78","@_deprecate_positional_args","79","def fetch_rcv1(*, data_home=None, subset='all', download_if_missing=True,"],"delete":["77","def fetch_rcv1(data_home=None, subset='all', download_if_missing=True,"]}],"sklearn\/datasets\/_base.py":[{"add":["19","from ..utils.validation import _deprecate_positional_args","83","@_deprecate_positional_args","84","def load_files(container_path, *, description=None, categories=None,","271","@_deprecate_positional_args","272","def load_wine(*, return_X_y=False, as_frame=False):","386","@_deprecate_positional_args","387","def load_iris(*, return_X_y=False, as_frame=False):","501","@_deprecate_positional_args","502","def load_breast_cancer(*, return_X_y=False, as_frame=False):","626","@_deprecate_positional_args","627","def load_digits(*, n_class=10, return_X_y=False, as_frame=False):","750","@_deprecate_positional_args","751","def load_diabetes(*, return_X_y=False, as_frame=False):","843","@_deprecate_positional_args","844","def load_linnerud(*, return_X_y=False, as_frame=False):","947","@_deprecate_positional_args","948","def load_boston(*, return_X_y=False):"],"delete":["82","def load_files(container_path, description=None, categories=None,","269","def load_wine(return_X_y=False, as_frame=False):","383","def load_iris(return_X_y=False, as_frame=False):","497","def load_breast_cancer(return_X_y=False, as_frame=False):","621","def load_digits(n_class=10, return_X_y=False, as_frame=False):","744","def load_diabetes(return_X_y=False, as_frame=False):","836","def load_linnerud(return_X_y=False, as_frame=False):","939","def load_boston(return_X_y=False):"]}],"sklearn\/datasets\/_california_housing.py":[{"add":["38","from ..utils.validation import _deprecate_positional_args","39","","52","@_deprecate_positional_args","53","def fetch_california_housing(*, data_home=None, download_if_missing=True,"],"delete":["50","def fetch_california_housing(data_home=None, download_if_missing=True,"]}],"sklearn\/datasets\/_species_distributions.py":[{"add":["52","from ..utils.validation import _deprecate_positional_args","140","@_deprecate_positional_args","141","def fetch_species_distributions(*, data_home=None,"],"delete":["139","def fetch_species_distributions(data_home=None,"]}],"sklearn\/datasets\/_covtype.py":[{"add":["30","from ..utils.validation import _deprecate_positional_args","31","","44","@_deprecate_positional_args","45","def fetch_covtype(*, data_home=None, download_if_missing=True,"],"delete":["42","def fetch_covtype(data_home=None, download_if_missing=True,"]}],"sklearn\/datasets\/_lfw.py":[{"add":["22","from ..utils.validation import _deprecate_positional_args","218","@_deprecate_positional_args","219","def fetch_lfw_people(*, data_home=None, funneled=True, resize=0.5,","389","@_deprecate_positional_args","390","def fetch_lfw_pairs(*, subset='train', data_home=None, funneled=True,","391","                    resize=0.5,"],"delete":["217","def fetch_lfw_people(data_home=None, funneled=True, resize=0.5,","387","def fetch_lfw_pairs(subset='train', data_home=None, funneled=True, resize=0.5,"]}],"sklearn\/linear_model\/tests\/test_omp.py":[{"add":["20","y, X, gamma = make_sparse_coded_signal(n_samples=n_targets,","21","                                       n_components=n_features,","22","                                       n_features=n_samples,","23","                                       n_nonzero_coefs=n_nonzero_coefs,","24","                                       random_state=0)"],"delete":["20","y, X, gamma = make_sparse_coded_signal(n_targets, n_features, n_samples,","21","                                       n_nonzero_coefs, random_state=0)"]}],"sklearn\/datasets\/_samples_generator.py":[{"add":["20","from ..utils.validation import _deprecate_positional_args","36","@_deprecate_positional_args","37","def make_classification(n_samples=100, n_features=20, *, n_informative=2,","265","@_deprecate_positional_args","266","def make_multilabel_classification(n_samples=100, n_features=20, *,","267","                                   n_classes=5,","428","@_deprecate_positional_args","429","def make_hastie_10_2(n_samples=12000, *, random_state=None):","477","@_deprecate_positional_args","478","def make_regression(n_samples=100, n_features=100, *, n_informative=10,","600","@_deprecate_positional_args","601","def make_circles(n_samples=100, *, shuffle=True, noise=None, random_state=None,","677","@_deprecate_positional_args","678","def make_moons(n_samples=100, *, shuffle=True, noise=None, random_state=None):","741","@_deprecate_positional_args","742","def make_blobs(n_samples=100, n_features=2, *, centers=None, cluster_std=1.0,","900","@_deprecate_positional_args","901","def make_friedman1(n_samples=100, n_features=10, *, noise=0.0,","902","                   random_state=None):","964","@_deprecate_positional_args","965","def make_friedman2(n_samples=100, *, noise=0.0, random_state=None):","1030","@_deprecate_positional_args","1031","def make_friedman3(n_samples=100, *, noise=0.0, random_state=None):","1095","@_deprecate_positional_args","1096","def make_low_rank_matrix(n_samples=100, n_features=100, *, effective_rank=10,","1165","@_deprecate_positional_args","1166","def make_sparse_coded_signal(n_samples, *, n_components, n_features,","1228","@_deprecate_positional_args","1229","def make_sparse_uncorrelated(n_samples=100, n_features=10, *,","1230","                             random_state=None):","1281","@_deprecate_positional_args","1282","def make_spd_matrix(n_dim, *, random_state=None):","1315","@_deprecate_positional_args","1316","def make_sparse_spd_matrix(dim=1, *, alpha=0.95, norm_diag=False,","1390","@_deprecate_positional_args","1391","def make_swiss_roll(n_samples=100, *, noise=0.0, random_state=None):","1443","@_deprecate_positional_args","1444","def make_s_curve(n_samples=100, *, noise=0.0, random_state=None):","1486","@_deprecate_positional_args","1487","def make_gaussian_quantiles(*, mean=None, cov=1., n_samples=100,","1582","@_deprecate_positional_args","1583","def make_biclusters(shape, n_clusters, *, noise=0.0, minval=10,","1674","@_deprecate_positional_args","1675","def make_checkerboard(shape, n_clusters, *, noise=0.0, minval=10,"],"delete":["35","def make_classification(n_samples=100, n_features=20, n_informative=2,","263","def make_multilabel_classification(n_samples=100, n_features=20, n_classes=5,","424","def make_hastie_10_2(n_samples=12000, random_state=None):","472","def make_regression(n_samples=100, n_features=100, n_informative=10,","594","def make_circles(n_samples=100, shuffle=True, noise=None, random_state=None,","670","def make_moons(n_samples=100, shuffle=True, noise=None, random_state=None):","733","def make_blobs(n_samples=100, n_features=2, centers=None, cluster_std=1.0,","891","def make_friedman1(n_samples=100, n_features=10, noise=0.0, random_state=None):","953","def make_friedman2(n_samples=100, noise=0.0, random_state=None):","1018","def make_friedman3(n_samples=100, noise=0.0, random_state=None):","1082","def make_low_rank_matrix(n_samples=100, n_features=100, effective_rank=10,","1151","def make_sparse_coded_signal(n_samples, n_components, n_features,","1213","def make_sparse_uncorrelated(n_samples=100, n_features=10, random_state=None):","1264","def make_spd_matrix(n_dim, random_state=None):","1297","def make_sparse_spd_matrix(dim=1, alpha=0.95, norm_diag=False,","1371","def make_swiss_roll(n_samples=100, noise=0.0, random_state=None):","1423","def make_s_curve(n_samples=100, noise=0.0, random_state=None):","1465","def make_gaussian_quantiles(mean=None, cov=1., n_samples=100,","1560","def make_biclusters(shape, n_clusters, noise=0.0, minval=10,","1651","def make_checkerboard(shape, n_clusters, noise=0.0, minval=10,"]}],"sklearn\/datasets\/_olivetti_faces.py":[{"add":["27","from ..utils.validation import _deprecate_positional_args","38","@_deprecate_positional_args","39","def fetch_olivetti_faces(*, data_home=None, shuffle=False, random_state=0,"],"delete":["37","def fetch_olivetti_faces(data_home=None, shuffle=False, random_state=0,"]}],"sklearn\/datasets\/_kddcup99.py":[{"add":["25","from ..utils.validation import _deprecate_positional_args","26","","47","@_deprecate_positional_args","48","def fetch_kddcup99(*, subset=None, data_home=None, shuffle=False,"],"delete":["45","def fetch_kddcup99(subset=None, data_home=None, shuffle=False,"]}],"sklearn\/datasets\/_twenty_newsgroups.py":[{"add":["47","from ..utils.validation import _deprecate_positional_args","149","@_deprecate_positional_args","150","def fetch_20newsgroups(*, data_home=None, subset='train', categories=None,","326","@_deprecate_positional_args","327","def fetch_20newsgroups_vectorized(*, subset=\"train\", remove=(), data_home=None,"],"delete":["148","def fetch_20newsgroups(data_home=None, subset='train', categories=None,","324","def fetch_20newsgroups_vectorized(subset=\"train\", remove=(), data_home=None,"]}],"sklearn\/datasets\/tests\/test_base.py":[{"add":["154","    digits = load_digits(n_class=9)"],"delete":["154","    digits = load_digits(9)"]}]}},"45bc7d98b883d3c6163344996362125497a1476b":{"changes":{"examples\/inspection\/plot_linear_model_coefficient_interpretation.py":"MODIFY"},"diff":{"examples\/inspection\/plot_linear_model_coefficient_interpretation.py":[{"add":["65","# Wages are described as floating-point number in dollars per hour."],"delete":["65","# Wages are described as floating-point number in :math:`k$`"]}]}},"46001e7cfb163b73a83d5adf73fb454e8a3c64b6":{"changes":{"azure-pipelines.yml":"MODIFY","sklearn\/model_selection\/_validation.py":"MODIFY","sklearn\/model_selection\/tests\/test_search.py":"MODIFY","sklearn\/utils\/validation.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY","build_tools\/azure\/install.sh":"MODIFY","sklearn\/utils\/tests\/test_validation.py":"MODIFY","sklearn\/model_selection\/_search.py":"MODIFY"},"diff":{"azure-pipelines.yml":[{"add":["73","      # It runs tests requiring lightgbm, pandas and PyAMG."],"delete":["73","      # It runs tests requiring pandas and PyAMG.","76","        # FIXME: pinned until SciPy wheels are available for Python 3.8"]}],"sklearn\/model_selection\/_validation.py":[{"add":["25","from ..utils.validation import _check_fit_params","492","    fit_params = _check_fit_params(X, fit_params, train)","832","    fit_params = _check_fit_params(X, fit_params, train)"],"delete":["491","    fit_params = {k: _index_param_value(X, v, train)","492","                  for k, v in fit_params.items()}","832","    fit_params = {k: _index_param_value(X, v, train)","833","                  for k, v in fit_params.items()}","943","def _index_param_value(X, v, indices):","944","    \"\"\"Private helper function for parameter value indexing.\"\"\"","945","    if not _is_arraylike(v) or _num_samples(v) != _num_samples(X):","946","        # pass through: skip indexing","947","        return v","948","    if sp.issparse(v):","949","        v = v.tocsr()","950","    return _safe_indexing(v, indices)","951","","952",""]}],"sklearn\/model_selection\/tests\/test_search.py":[{"add":["29","from sklearn.base import BaseEstimator, ClassifierMixin","38","from sklearn.model_selection import train_test_split","221","@pytest.mark.parametrize(\"SearchCV\", [GridSearchCV, RandomizedSearchCV])","222","def test_SearchCV_with_fit_params(SearchCV):","226","    searcher = SearchCV(","227","        clf, {'foo_param': [1, 2, 3]}, cv=2, error_score=\"raise\"","228","    )","232","    err_msg = r\"Expected fit parameter\\(s\\) \\['eggs'\\] not seen.\"","233","    with pytest.raises(AssertionError, match=err_msg):","234","        searcher.fit(X, y, spam=np.ones(10))","235","","236","    err_msg = \"Fit parameter spam has length 1; expected\"","237","    with pytest.raises(AssertionError, match=err_msg):","238","        searcher.fit(X, y, spam=np.ones(1), eggs=np.zeros(10))","1841","","1842","","1843","@pytest.mark.parametrize(","1844","    \"SearchCV, param_search\",","1845","    [(GridSearchCV, {'a': [0.1, 0.01]}),","1846","     (RandomizedSearchCV, {'a': uniform(1, 3)})]","1847",")","1848","def test_scalar_fit_param(SearchCV, param_search):","1849","    # unofficially sanctioned tolerance for scalar values in fit_params","1850","    # non-regression test for:","1851","    # https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/15805","1852","    class TestEstimator(BaseEstimator, ClassifierMixin):","1853","        def __init__(self, a=None):","1854","            self.a = a","1855","","1856","        def fit(self, X, y, r=None):","1857","            self.r_ = r","1858","","1859","        def predict(self, X):","1860","            return np.zeros(shape=(len(X)))","1861","","1862","    model = SearchCV(TestEstimator(), param_search)","1863","    X, y = make_classification(random_state=42)","1864","    model.fit(X, y, r=42)","1865","    assert model.best_estimator_.r_ == 42","1866","","1867","","1868","@pytest.mark.parametrize(","1869","    \"SearchCV, param_search\",","1870","    [(GridSearchCV, {'alpha': [0.1, 0.01]}),","1871","     (RandomizedSearchCV, {'alpha': uniform(0.01, 0.1)})]","1872",")","1873","def test_scalar_fit_param_compat(SearchCV, param_search):","1874","    # check support for scalar values in fit_params, for instance in LightGBM","1875","    # that do not exactly respect the scikit-learn API contract but that we do","1876","    # not want to break without an explicit deprecation cycle and API","1877","    # recommendations for implementing early stopping with a user provided","1878","    # validation set. non-regression test for:","1879","    # https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/15805","1880","    X_train, X_valid, y_train, y_valid = train_test_split(","1881","        *make_classification(random_state=42), random_state=42","1882","    )","1883","","1884","    class _FitParamClassifier(SGDClassifier):","1885","","1886","        def fit(self, X, y, sample_weight=None, tuple_of_arrays=None,","1887","                scalar_param=None, callable_param=None):","1888","            super().fit(X, y, sample_weight=sample_weight)","1889","            assert scalar_param > 0","1890","            assert callable(callable_param)","1891","","1892","            # The tuple of arrays should be preserved as tuple.","1893","            assert isinstance(tuple_of_arrays, tuple)","1894","            assert tuple_of_arrays[0].ndim == 2","1895","            assert tuple_of_arrays[1].ndim == 1","1896","            return self","1897","","1898","    def _fit_param_callable():","1899","        pass","1900","","1901","    model = SearchCV(","1902","        _FitParamClassifier(), param_search","1903","    )","1904","","1905","    # NOTE: `fit_params` should be data dependent (e.g. `sample_weight`) which","1906","    # is not the case for the following parameters. But this abuse is common in","1907","    # popular third-party libraries and we should tolerate this behavior for","1908","    # now and be careful not to break support for those without following","1909","    # proper deprecation cycle.","1910","    fit_params = {","1911","        'tuple_of_arrays': (X_valid, y_valid),","1912","        'callable_param': _fit_param_callable,","1913","        'scalar_param': 42,","1914","    }","1915","    model.fit(X_train, y_train, **fit_params)"],"delete":["29","from sklearn.base import BaseEstimator","220","def check_hyperparameter_searcher_with_fit_params(klass, **klass_kwargs):","224","    searcher = klass(clf, {'foo_param': [1, 2, 3]}, cv=2, **klass_kwargs)","228","    assert_raise_message(AssertionError,","229","                         \"Expected fit parameter(s) ['eggs'] not seen.\",","230","                         searcher.fit, X, y, spam=np.ones(10))","231","    assert_raise_message(","232","        ValueError,","233","        \"Found input variables with inconsistent numbers of samples: [\",","234","        searcher.fit, X, y, spam=np.ones(1),","235","        eggs=np.zeros(10))","239","def test_grid_search_with_fit_params():","240","    check_hyperparameter_searcher_with_fit_params(GridSearchCV,","241","                                                  error_score='raise')","242","","243","","244","def test_random_search_with_fit_params():","245","    check_hyperparameter_searcher_with_fit_params(RandomizedSearchCV, n_iter=1,","246","                                                  error_score='raise')","247","","248",""]}],"sklearn\/utils\/validation.py":[{"add":["214","def _make_indexable(iterable):","215","    \"\"\"Ensure iterable supports indexing or convert to an indexable variant.","216","","217","    Convert sparse matrices to csr and other non-indexable iterable to arrays.","218","    Let `None` and indexable objects (e.g. pandas dataframes) pass unchanged.","219","","220","    Parameters","221","    ----------","222","    iterable : {list, dataframe, array, sparse} or None","223","        Object to be converted to an indexable iterable.","224","    \"\"\"","225","    if sp.issparse(iterable):","226","        return iterable.tocsr()","227","    elif hasattr(iterable, \"__getitem__\") or hasattr(iterable, \"iloc\"):","228","        return iterable","229","    elif iterable is None:","230","        return iterable","231","    return np.array(iterable)","232","","233","","246","    result = [_make_indexable(X) for X in iterables]","1290","","1291","","1292","def _check_fit_params(X, fit_params, indices=None):","1293","    \"\"\"Check and validate the parameters passed during `fit`.","1294","","1295","    Parameters","1296","    ----------","1297","    X : array-like of shape (n_samples, n_features)","1298","        Data array.","1299","","1300","    fit_params : dict","1301","        Dictionary containing the parameters passed at fit.","1302","","1303","    indices : array-like of shape (n_samples,), default=None","1304","        Indices to be selected if the parameter has the same size as `X`.","1305","","1306","    Returns","1307","    -------","1308","    fit_params_validated : dict","1309","        Validated parameters. We ensure that the values support indexing.","1310","    \"\"\"","1311","    from . import _safe_indexing","1312","    fit_params_validated = {}","1313","    for param_key, param_value in fit_params.items():","1314","        if (not _is_arraylike(param_value) or","1315","                _num_samples(param_value) != _num_samples(X)):","1316","            # Non-indexable pass-through (for now for backward-compatibility).","1317","            # https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/15805","1318","            fit_params_validated[param_key] = param_value","1319","        else:","1320","            # Any other fit_params should support indexing","1321","            # (e.g. for cross-validation).","1322","            fit_params_validated[param_key] = _make_indexable(param_value)","1323","            fit_params_validated[param_key] = _safe_indexing(","1324","                fit_params_validated[param_key], indices","1325","            )","1326","","1327","    return fit_params_validated"],"delete":["226","    result = []","227","    for X in iterables:","228","        if sp.issparse(X):","229","            result.append(X.tocsr())","230","        elif hasattr(X, \"__getitem__\") or hasattr(X, \"iloc\"):","231","            result.append(X)","232","        elif X is None:","233","            result.append(X)","234","        else:","235","            result.append(np.array(X))"]}],"doc\/whats_new\/v0.22.rst":[{"add":["68",":mod:`sklearn.model_selection`","69","..............................","70","","71","- |Fix| :class:`model_selection.GridSearchCV` and","72","  :class:`model_selection.RandomizedSearchCV` accept scalar values provided in","73","  `fit_params`. Change in 0.22 was breaking backward compatibility.","74","  :pr:`15863` by :user:`Adrin Jalali <adrinjalali>` and","75","  :user:`Guillaume Lemaitre <glemaitre>`.","76",""],"delete":[]}],"build_tools\/azure\/install.sh":[{"add":["94","    # do not install dependencies for lightgbm since it requires scikit-learn","95","    python -m pip install lightgbm --no-deps"],"delete":[]}],"sklearn\/utils\/tests\/test_validation.py":[{"add":["34","from sklearn.utils import _safe_indexing","49","from sklearn.utils.validation import _check_fit_params","1102","","1103","","1104","@pytest.mark.parametrize(\"indices\", [None, [1, 3]])","1105","def test_check_fit_params(indices):","1106","    X = np.random.randn(4, 2)","1107","    fit_params = {","1108","        'list': [1, 2, 3, 4],","1109","        'array': np.array([1, 2, 3, 4]),","1110","        'sparse-col': sp.csc_matrix([1, 2, 3, 4]).T,","1111","        'sparse-row': sp.csc_matrix([1, 2, 3, 4]),","1112","        'scalar-int': 1,","1113","        'scalar-str': 'xxx',","1114","        'None': None,","1115","    }","1116","    result = _check_fit_params(X, fit_params, indices)","1117","    indices_ = indices if indices is not None else list(range(X.shape[0]))","1118","","1119","    for key in ['sparse-row', 'scalar-int', 'scalar-str', 'None']:","1120","        assert result[key] is fit_params[key]","1121","","1122","    assert result['list'] == _safe_indexing(fit_params['list'], indices_)","1123","    assert_array_equal(","1124","        result['array'], _safe_indexing(fit_params['array'], indices_)","1125","    )","1126","    assert_allclose_dense_sparse(","1127","        result['sparse-col'],","1128","        _safe_indexing(fit_params['sparse-col'], indices_)","1129","    )"],"delete":[]}],"sklearn\/model_selection\/_search.py":[{"add":["35","from ..utils.validation import indexable, check_is_fitted, _check_fit_params","650","        fit_params = _check_fit_params(X, fit_params)"],"delete":["35","from ..utils.validation import indexable, check_is_fitted","650","        # make sure fit_params are sliceable","651","        fit_params_values = indexable(*fit_params.values())","652","        fit_params = dict(zip(fit_params.keys(), fit_params_values))"]}]}},"06c71ec8b9c5984df5778a5694c4b17b44ed72ea":{"changes":{"doc\/whats_new\/v0.23.rst":"MODIFY","sklearn\/cluster\/_agglomerative.py":"MODIFY","sklearn\/cluster\/tests\/test_hierarchical.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.23.rst":[{"add":["297","","298",":mod:`sklearn.cluster`","299","......................","300","","301","- |Fix| :class:`cluster.AgglomerativeClustering` add specific error when ","302","  distance matrix is not square and `affinity=precomputed`. ","303","  :pr:`16257` by :user:`Simona Maggio <simonamaggio>`."],"delete":[]}],"sklearn\/cluster\/_agglomerative.py":[{"add":["455","            # by sklearn.metrics.pairwise_distances.","456","            if X.shape[0] != X.shape[1]:","457","                raise ValueError(","458","                    'Distance matrix should be square, '","459","                    'Got matrix of shape {X.shape}'","460","                )"],"delete":["455","            # by pdist: it is a flat array containing the upper triangular of","456","            # the distance matrix."]}],"sklearn\/cluster\/tests\/test_hierarchical.py":[{"add":["753","","754","","755","def test_invalid_shape_precomputed_dist_matrix():","756","    # Check that an error is raised when affinity='precomputed'","757","    # and a non square matrix is passed (PR #16257).","758","    rng = np.random.RandomState(0)","759","    X = rng.rand(5, 3)","760","    with pytest.raises(ValueError, match=\"Distance matrix should be square, \"):","761","        AgglomerativeClustering(affinity='precomputed',","762","                                linkage='complete').fit(X)"],"delete":[]}]}},"d163d5ad9433d11b36fb3ce580d97c9462087a40":{"changes":{"sklearn\/semi_supervised\/tests\/test_label_propagation.py":"MODIFY","sklearn\/semi_supervised\/_label_propagation.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY"},"diff":{"sklearn\/semi_supervised\/tests\/test_label_propagation.py":[{"add":["5","from scipy.sparse import issparse","10","from sklearn.model_selection import train_test_split","11","from sklearn.neighbors import NearestNeighbors","157","","158","","159","def test_predict_sparse_callable_kernel():","160","    # This is a non-regression test for #15866","161","","162","    # Custom sparse kernel (top-K RBF)","163","    def topk_rbf(X, Y=None, n_neighbors=10, gamma=1e-5):","164","        nn = NearestNeighbors(n_neighbors=10, metric='euclidean', n_jobs=-1)","165","        nn.fit(X)","166","        W = -1 * nn.kneighbors_graph(Y, mode='distance').power(2) * gamma","167","        np.exp(W.data, out=W.data)","168","        assert issparse(W)","169","        return W.T","170","","171","    n_classes = 4","172","    n_samples = 500","173","    n_test = 10","174","    X, y = make_classification(n_classes=n_classes,","175","                               n_samples=n_samples,","176","                               n_features=20,","177","                               n_informative=20,","178","                               n_redundant=0,","179","                               n_repeated=0,","180","                               random_state=0)","181","","182","    X_train, X_test, y_train, y_test = train_test_split(X, y,","183","                                                        test_size=n_test,","184","                                                        random_state=0)","185","","186","    model = label_propagation.LabelSpreading(kernel=topk_rbf)","187","    model.fit(X_train, y_train)","188","    assert model.score(X_test, y_test) >= 0.9","189","","190","    model = label_propagation.LabelPropagation(kernel=topk_rbf)","191","    model.fit(X_train, y_train)","192","    assert model.score(X_test, y_test) >= 0.9"],"delete":[]}],"sklearn\/semi_supervised\/_label_propagation.py":[{"add":["197","            probabilities = safe_sparse_dot(","198","                    weight_matrices, self.label_distributions_)"],"delete":["197","            probabilities = np.dot(weight_matrices, self.label_distributions_)"]}],"doc\/whats_new\/v0.22.rst":[{"add":["55","  ","56",":mod:`sklearn.semi_supervised`","57","..............................","58","","59","- |Fix| :class:`semi_supervised.LabelPropagation` and","60","  :class:`semi_supervised.LabelSpreading` now allow callable kernel function to","61","  return sparse weight matrix.","62","  :pr:`15868` by :user:`Niklas Smedemark-Margulies <nik-sm>`."],"delete":[]}]}},"a6a20f29d7baf51960172aa64b981ffb240d3ebd":{"changes":{"sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_loss.py":"MODIFY"},"diff":{"sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_loss.py":[{"add":["4","from scipy.special import logit","52","    # The argmin of binary_crossentropy for y_true=0 and y_true=1 is resp. -inf","53","    # and +inf due to logit, cf. \"complete separation\". Therefore, we use","54","    # 0 < y_true < 1.","55","    ('binary_crossentropy', 0.3, 0.1),","56","    ('binary_crossentropy', -12, 0.2),","57","    ('binary_crossentropy', 30, 0.9),","66","    # Check that gradients are zero when the loss is minimized on a single","67","    # value\/sample using Halley's method with the first and second order","68","    # derivatives computed by the Loss instance.","69","    # Note that methods of Loss instances operate on arrays while the newton","70","    # root finder expects a scalar or a one-element array for this purpose.","77","    def func(x: np.ndarray) -> np.ndarray:","78","        if isinstance(loss, _LOSSES['binary_crossentropy']):","79","            # Subtract a constant term such that the binary cross entropy","80","            # has its minimum at zero, which is needed for the newton method.","81","            actual_min = loss.pointwise_loss(y_true, logit(y_true))","82","            return loss.pointwise_loss(y_true, x) - actual_min","83","        else:","84","            return loss.pointwise_loss(y_true, x)","86","    def fprime(x: np.ndarray) -> np.ndarray:","89","    def fprime2(x: np.ndarray) -> np.ndarray:","94","","95","    # Need to ravel arrays because assert_allclose requires matching dimensions","96","    y_true = y_true.ravel()","97","    optimum = optimum.ravel()","98","    assert_allclose(loss.inverse_link_function(optimum), y_true)","99","    assert_allclose(func(optimum), 0, atol=1e-14)","100","    assert_allclose(get_gradients(y_true, optimum), 0, atol=1e-7)"],"delete":["51","    # I don't understand why but y_true == 0 fails :\/","52","    # ('binary_crossentropy', 0.3, 0),","53","    ('binary_crossentropy', -12, 1),","54","    ('binary_crossentropy', 30, 1),","63","    # Check that gradients are zero when the loss is minimized on 1D array","64","    # using Halley's method with the first and second order derivatives","65","    # computed by the Loss instance.","72","    def func(x):","73","        return loss.pointwise_loss(y_true, x)","75","    def fprime(x):","78","    def fprime2(x):","83","    assert np.allclose(loss.inverse_link_function(optimum), y_true)","84","    assert np.allclose(loss.pointwise_loss(y_true, optimum), 0)","85","    assert np.allclose(get_gradients(y_true, optimum), 0, atol=1e-7)"]}]}},"913da3f286649af6378204d824827bfc4681fe20":{"changes":{"benchmarks\/bench_hist_gradient_boosting_higgsboson.py":"MODIFY"},"diff":{"benchmarks\/bench_hist_gradient_boosting_higgsboson.py":[{"add":["77","                                     early_stopping=False,"],"delete":["77","                                     n_iter_no_change=None,"]}]}},"bcd399f4cbe6ad13a5dd1e82c128cebb8fcf9eb4":{"changes":{"sklearn\/model_selection\/_validation.py":"MODIFY","sklearn\/linear_model\/tests\/test_logistic.py":"MODIFY","sklearn\/feature_selection\/rfe.py":"MODIFY","sklearn\/model_selection\/tests\/test_validation.py":"MODIFY","doc\/modules\/cross_validation.rst":"MODIFY","sklearn\/feature_selection\/mutual_info_.py":"MODIFY"},"diff":{"sklearn\/model_selection\/_validation.py":[{"add":["179","    >>> from sklearn.metrics import make_scorer"],"delete":["179","    >>> from sklearn.metrics.scorer import make_scorer"]}],"sklearn\/linear_model\/tests\/test_logistic.py":[{"add":["11","from sklearn.metrics import get_scorer"],"delete":["11","from sklearn.metrics.scorer import get_scorer"]}],"sklearn\/feature_selection\/rfe.py":[{"add":["21","from ..metrics import check_scoring"],"delete":["21","from ..metrics.scorer import check_scoring"]}],"sklearn\/model_selection\/tests\/test_validation.py":[{"add":["54","from sklearn.metrics import check_scoring"],"delete":["54","from sklearn.metrics.scorer import check_scoring"]}],"doc\/modules\/cross_validation.rst":[{"add":["243","    >>> from sklearn.metrics import make_scorer"],"delete":["243","    >>> from sklearn.metrics.scorer import make_scorer"]}],"sklearn\/feature_selection\/mutual_info_.py":[{"add":["7","from ..metrics.cluster import mutual_info_score"],"delete":["7","from ..metrics.cluster.supervised import mutual_info_score"]}]}},"86f47da52954cb68b5eb17ffcc909194e31bbaee":{"changes":{"examples\/neighbors\/approximate_nearest_neighbors.py":"MODIFY"},"diff":{"examples\/neighbors\/approximate_nearest_neighbors.py":[{"add":["211","    mnist = fetch_openml(\"mnist_784\")","212","    X, y = shuffle(mnist.data, mnist.target, random_state=2)","213","    return X[:n_samples] \/ 255, y[:n_samples]","280","                axes[i_ax].scatter(Xt[:, 0], Xt[:, 1], c=y.astype(np.int32),","281","                                   alpha=0.2, cmap=plt.cm.viridis)"],"delete":["211","    mnist = fetch_openml(data_id=41063)","212","    X, y = shuffle(mnist.data, mnist.target, random_state=42)","213","    return X[:n_samples], y[:n_samples]","280","                axes[i_ax].scatter(Xt[:, 0], Xt[:, 1], c=y, alpha=0.2,","281","                                   cmap=plt.cm.viridis)"]}]}},"0db80b90c7f46d8c0a0a377030a110e2d1be7937":{"changes":{"doc\/whats_new\/v0.23.rst":"MODIFY","sklearn\/tree\/_tree.pyx":"MODIFY","sklearn\/tree\/tests\/test_tree.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.23.rst":[{"add":["219","- |Fix| Fix support of read-only float32 array input in ``predict``,","220","  ``decision_path`` and ``predict_proba`` methods of","221","  :class:`tree.DecisionTreeClassifier`, :class:`tree.ExtraTreeClassifier` and","222","  :class:`ensemble.GradientBoostingClassifier` as well as ``predict`` method of","223","  :class:`tree.DecisionTreeRegressor`, :class:`tree.ExtraTreeRegressor`, and","224","  :class:`ensemble.GradientBoostingRegressor`.","225","  :pr:`16331` by :user:`Alexandre Batisse <batalex>`.","226",""],"delete":[]}],"sklearn\/tree\/_tree.pyx":[{"add":["794","        cdef const DTYPE_t[:, :] X_ndarray = X","914","        cdef const DTYPE_t[:, :] X_ndarray = X"],"delete":["794","        cdef DTYPE_t[:, :] X_ndarray = X","914","        cdef DTYPE_t[:, :] X_ndarray = X"]}],"sklearn\/tree\/tests\/test_tree.py":[{"add":["25","from sklearn.utils._testing import create_memmap_backed_data","1957","","1958","","1959","def check_apply_path_readonly(name):","1960","    X_readonly = create_memmap_backed_data(X_small.astype(tree._tree.DTYPE,","1961","                                                          copy=False))","1962","    y_readonly = create_memmap_backed_data(np.array(y_small,","1963","                                                    dtype=tree._tree.DTYPE))","1964","    est = ALL_TREES[name]()","1965","    est.fit(X_readonly, y_readonly)","1966","    assert_array_equal(est.predict(X_readonly),","1967","                       est.predict(X_small))","1968","    assert_array_equal(est.decision_path(X_readonly).todense(),","1969","                       est.decision_path(X_small).todense())","1970","","1971","","1972","@pytest.mark.parametrize(\"name\", ALL_TREES)","1973","def test_apply_path_readonly_all_trees(name):","1974","    check_apply_path_readonly(name)"],"delete":["1825","def test_decision_tree_memmap():","1826","    # check that decision trees supports read-only buffer (#13626)","1827","    X = np.random.RandomState(0).random_sample((10, 2)).astype(np.float32)","1828","    y = np.zeros(10)","1829","","1830","    with TempMemmap((X, y)) as (X_read_only, y_read_only):","1831","        DecisionTreeClassifier().fit(X_read_only, y_read_only)","1832","","1833",""]}]}},"7366a5a4cc2a94eca2911a90c3abfe026cd53f64":{"changes":{"sklearn\/metrics\/_classification.py":"MODIFY","sklearn\/metrics\/tests\/test_classification.py":"MODIFY","doc\/whats_new\/v0.23.rst":"MODIFY","sklearn\/metrics\/_plot\/tests\/test_plot_confusion_matrix.py":"MODIFY"},"diff":{"sklearn\/metrics\/_classification.py":[{"add":["278","        n_labels = labels.size","279","        if n_labels == 0:","280","            raise ValueError(\"'labels' should contains at least one label.\")","281","        elif y_true.size == 0:","282","            return np.zeros((n_labels, n_labels), dtype=np.int)","283","        elif np.all([l not in y_true for l in labels]):"],"delete":["278","        if np.all([l not in y_true for l in labels]):"]}],"sklearn\/metrics\/tests\/test_classification.py":[{"add":["910","","911","@pytest.mark.parametrize(","912","    \"labels, err_msg\",","913","    [([], \"'labels' should contains at least one label.\"),","914","     ([3, 4], \"At least one label specified must be in y_true\")],","915","    ids=[\"empty list\", \"unknown labels\"]","916",")","917","def test_confusion_matrix_error(labels, err_msg):","918","    y_true, y_pred, _ = make_prediction(binary=False)","919","    with pytest.raises(ValueError, match=err_msg):","920","        confusion_matrix(y_true, y_pred, labels=labels)","921","","922","","923","@pytest.mark.parametrize(","924","    'labels', (None, [0, 1], [0, 1, 2]),","925","    ids=['None', 'binary', 'multiclass']","926",")","927","def test_confusion_matrix_on_zero_length_input(labels):","928","    expected_n_classes = len(labels) if labels else 0","929","    expected = np.zeros((expected_n_classes, expected_n_classes), dtype=np.int)","930","    cm = confusion_matrix([], [], labels=labels)","931","    assert_array_equal(cm, expected)"],"delete":["910","    # check for exception when none of the specified labels are in y_true","911","    with pytest.raises(ValueError):","912","        confusion_matrix(y_true, y_pred,","913","                         labels=[extra_label, extra_label + 1])"]}],"doc\/whats_new\/v0.23.rst":[{"add":["254","- |Fix| Fixed a bug in :func:`metrics.confusion_matrix` that would raise","255","  an error when `y_true` and `y_pred` were length zero and `labels` was","256","  not `None`. In addition, we raise an error when an empty list is given to","257","  the `labels` parameter.","258","  :pr:`16442` by `Kyle Parsons <parsons-kyle-89>`.","259",""],"delete":[]}],"sklearn\/metrics\/_plot\/tests\/test_plot_confusion_matrix.py":[{"add":[],"delete":["228","","229",""]}]}},"98b3c7c8719e2158a61891dd78e7982929c1cea0":{"changes":{"doc\/whats_new\/v0.23.rst":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_gradient_boosting.py":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/grower.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.23.rst":[{"add":["114","- |API| Fixed a bug in :class:`ensemble.HistGradientBoostingClassifier` and","115","  :class:`ensemble.HistGradientBoostingRegrerssor` that would not respect the","116","  `max_leaf_nodes` parameter if the criteria was reached at the same time as","117","  the `max_depth` criteria. :pr:`16183` by `Nicolas Hug`_.","118",""],"delete":[]}],"sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_gradient_boosting.py":[{"add":["447","","448","","449","def test_max_depth_max_leaf_nodes():","450","    # Non regression test for","451","    # https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/16179","452","    # there was a bug when the max_depth and the max_leaf_nodes criteria were","453","    # met at the same time, which would lead to max_leaf_nodes not being","454","    # respected.","455","    X, y = make_classification(random_state=0)","456","    est = HistGradientBoostingClassifier(max_depth=2, max_leaf_nodes=3,","457","                                         max_iter=1).fit(X, y)","458","    tree = est._predictors[0][0]","459","    assert tree.get_max_depth() == 2","460","    assert tree.get_n_leaf_nodes() == 3  # would be 4 prior to bug fix"],"delete":[]}],"sklearn\/ensemble\/_hist_gradient_boosting\/grower.py":[{"add":["364","        if self.max_depth is not None and depth == self.max_depth:","365","            self._finalize_leaf(left_child_node)","366","            self._finalize_leaf(right_child_node)","367","            return left_child_node, right_child_node","368",""],"delete":["357","        if self.max_depth is not None and depth == self.max_depth:","358","            self._finalize_leaf(left_child_node)","359","            self._finalize_leaf(right_child_node)","360","            return left_child_node, right_child_node","361",""]}]}},"29932e690e00fa0c610a8fba2dfb0abc1d66cbac":{"changes":{"sklearn\/neighbors\/_base.py":"MODIFY"},"diff":{"sklearn\/neighbors\/_base.py":[{"add":["278","        neigh_dist = np.array(np.split(data, indptr[1:-1]), dtype=object)","279","    neigh_ind = np.array(np.split(indices, indptr[1:-1]), dtype=object)"],"delete":["278","        neigh_dist = np.array(np.split(data, indptr[1:-1]))","279","    neigh_ind = np.array(np.split(indices, indptr[1:-1]))"]}]}},"c311efd501538ceccec0d3def70f601331e090ca":{"changes":{"sklearn\/utils\/tests\/test_utils.py":"MODIFY","sklearn\/utils\/tests\/test_testing.py":"MODIFY","sklearn\/inspection\/tests\/test_plot_partial_dependence.py":"MODIFY","sklearn\/inspection\/_partial_dependence.py":"MODIFY","sklearn\/utils\/_testing.py":"MODIFY"},"diff":{"sklearn\/utils\/tests\/test_utils.py":[{"add":["11","                                    assert_array_equal,","12","                                    assert_allclose_dense_sparse,","13","                                    assert_raises_regex,","14","                                    assert_warns_message,","15","                                    assert_no_warnings,","16","                                    _convert_container)"],"delete":["11","                                   assert_array_equal,","12","                                   assert_allclose_dense_sparse,","13","                                   assert_raises_regex,","14","                                   assert_warns_message, assert_no_warnings)","238","def _convert_container(container, constructor_name, columns_name=None):","239","    if constructor_name == 'list':","240","        return list(container)","241","    elif constructor_name == 'tuple':","242","        return tuple(container)","243","    elif constructor_name == 'array':","244","        return np.asarray(container)","245","    elif constructor_name == 'sparse':","246","        return sp.csr_matrix(container)","247","    elif constructor_name == 'dataframe':","248","        pd = pytest.importorskip('pandas')","249","        return pd.DataFrame(container, columns=columns_name)","250","    elif constructor_name == 'series':","251","        pd = pytest.importorskip('pandas')","252","        return pd.Series(container)","253","    elif constructor_name == 'slice':","254","        return slice(container[0], container[1])","255","","256",""]}],"sklearn\/utils\/tests\/test_testing.py":[{"add":["34","    _delete_folder,","35","    _convert_container)","677","","678","","679","@pytest.mark.parametrize(","680","    \"constructor_name, container_type\",","681","    [('list', list),","682","     ('tuple', tuple),","683","     ('array', np.ndarray),","684","     ('sparse', sparse.csr_matrix),","685","     ('dataframe', pytest.importorskip('pandas').DataFrame),","686","     ('series', pytest.importorskip('pandas').Series),","687","     ('index', pytest.importorskip('pandas').Index),","688","     ('slice', slice)]","689",")","690","def test_convert_container(constructor_name, container_type):","691","    container = [0, 1]","692","    assert isinstance(_convert_container(container, constructor_name),","693","                      container_type)"],"delete":["34","    _delete_folder)"]}],"sklearn\/inspection\/tests\/test_plot_partial_dependence.py":[{"add":["12","from sklearn.utils._testing import _convert_container","13","","90","    \"input_type, feature_names_type\",","91","    [('dataframe', None),","92","     ('dataframe', 'list'), ('list', 'list'), ('array', 'list'),","93","     ('dataframe', 'array'), ('list', 'array'), ('array', 'array'),","94","     ('dataframe', 'series'), ('list', 'series'), ('array', 'series'),","95","     ('dataframe', 'index'), ('list', 'index'), ('array', 'index')]","98","                                              input_type, feature_names_type):","106","","107","    if feature_names_type is None:","108","        feature_names = None","109","    else:","110","        feature_names = _convert_container(boston.feature_names,","111","                                           feature_names_type)"],"delete":["88","    \"input_type, use_feature_names\",","89","    [('dataframe', False), ('dataframe', True),","90","     ('list', True), ('array', True)]","93","                                              input_type, use_feature_names):","101","    feature_names = boston.feature_names if use_feature_names else None"]}],"sklearn\/inspection\/_partial_dependence.py":[{"add":["625","    elif hasattr(feature_names, \"tolist\"):","626","        # convert numpy array or pandas index to a list"],"delete":["625","    elif isinstance(feature_names, np.ndarray):"]}],"sklearn\/utils\/_testing.py":[{"add":["914","","915","","916","def _convert_container(container, constructor_name, columns_name=None):","917","    if constructor_name == 'list':","918","        return list(container)","919","    elif constructor_name == 'tuple':","920","        return tuple(container)","921","    elif constructor_name == 'array':","922","        return np.asarray(container)","923","    elif constructor_name == 'sparse':","924","        return sp.sparse.csr_matrix(container)","925","    elif constructor_name == 'dataframe':","926","        pd = pytest.importorskip('pandas')","927","        return pd.DataFrame(container, columns=columns_name)","928","    elif constructor_name == 'series':","929","        pd = pytest.importorskip('pandas')","930","        return pd.Series(container)","931","    elif constructor_name == 'index':","932","        pd = pytest.importorskip('pandas')","933","        return pd.Index(container)","934","    elif constructor_name == 'slice':","935","        return slice(container[0], container[1])"],"delete":[]}]}},"c4ea377198f4289af16d33f60b74c6258158bf9f":{"changes":{"sklearn\/preprocessing\/tests\/test_label.py":"MODIFY","sklearn\/utils\/tests\/test_utils.py":"MODIFY","sklearn\/neighbors\/tests\/test_neighbors.py":"MODIFY","sklearn\/neighbors\/_base.py":"MODIFY","doc\/whats_new\/v0.23.rst":"MODIFY","sklearn\/utils\/__init__.py":"MODIFY"},"diff":{"sklearn\/preprocessing\/tests\/test_label.py":[{"add":["16","from sklearn.utils import _to_object_array","436","    tuple_classes = _to_object_array([(1,), (2,), (3,)])"],"delete":["435","    tuple_classes = np.empty(3, dtype=object)","436","    tuple_classes[:] = [(1,), (2,), (3,)]"]}],"sklearn\/utils\/tests\/test_utils.py":[{"add":["30","from sklearn.utils import _to_object_array","649","","650","","651","@pytest.mark.parametrize(","652","    \"sequence\",","653","    [[np.array(1), np.array(2)], [[1, 2], [3, 4]]]","654",")","655","def test_to_object_array(sequence):","656","    out = _to_object_array(sequence)","657","    assert isinstance(out, np.ndarray)","658","    assert out.dtype.kind == 'O'","659","    assert out.ndim == 1"],"delete":[]}],"sklearn\/neighbors\/tests\/test_neighbors.py":[{"add":["651","def test_radius_neighbors_returns_array_of_objects():","652","    # check that we can pass precomputed distances to","653","    # NearestNeighbors.radius_neighbors()","654","    # non-regression test for","655","    # https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/16036","656","    X = csr_matrix(np.ones((4, 4)))","657","    X.setdiag([0, 0, 0, 0])","658","","659","    nbrs = neighbors.NearestNeighbors(radius=0.5, algorithm='auto',","660","                                      leaf_size=30,","661","                                      metric='precomputed').fit(X)","662","    neigh_dist, neigh_ind = nbrs.radius_neighbors(X, return_distance=True)","663","","664","    expected_dist = np.empty(X.shape[0], dtype=object)","665","    expected_dist[:] = [np.array([0]), np.array([0]), np.array([0]),","666","                        np.array([0])]","667","    expected_ind = np.empty(X.shape[0], dtype=object)","668","    expected_ind[:] = [np.array([0]), np.array([1]), np.array([2]),","669","                       np.array([3])]","670","","671","    assert_array_equal(neigh_dist, expected_dist)","672","    assert_array_equal(neigh_ind, expected_ind)","673","","674",""],"delete":[]}],"sklearn\/neighbors\/_base.py":[{"add":["26","from ..utils import _to_object_array","279","        neigh_dist = _to_object_array(np.split(data, indptr[1:-1]))","280","    neigh_ind = _to_object_array(np.split(indices, indptr[1:-1]))","943","                neigh_dist = _to_object_array(neigh_dist_list)","944","                neigh_ind = _to_object_array(neigh_ind_list)","948","                results = _to_object_array(neigh_ind_list)"],"delete":["278","        neigh_dist = np.array(np.split(data, indptr[1:-1]), dtype=object)","279","    neigh_ind = np.array(np.split(indices, indptr[1:-1]), dtype=object)","942","                # See https:\/\/github.com\/numpy\/numpy\/issues\/5456","943","                # to understand why this is initialized this way.","944","                neigh_dist = np.empty(len(neigh_dist_list), dtype='object')","945","                neigh_dist[:] = neigh_dist_list","946","                neigh_ind = np.empty(len(neigh_ind_list), dtype='object')","947","                neigh_ind[:] = neigh_ind_list","951","                results = np.empty(len(neigh_ind_list), dtype='object')","952","                results[:] = neigh_ind_list"]}],"doc\/whats_new\/v0.23.rst":[{"add":["138",":mod:`sklearn.neighbors`","139","..............................","140","","141","- |Fix| Fix a bug which converted a list of arrays into a 2-D object ","142","  array instead of a 1-D array containing NumPy arrays. This bug","143","  was affecting :meth:`neighbors.NearestNeighbors.radius_neighbors`.","144","  :pr:`16076` by :user:`Guillaume Lemaitre <glemaitre>` and  ","145","  :user:`Alex Shacked <alexshacked>`.","146",""],"delete":[]}],"sklearn\/utils\/__init__.py":[{"add":["821","def _to_object_array(sequence):","822","    \"\"\"Convert sequence to a 1-D NumPy array of object dtype.","823","","824","    numpy.array constructor has a similar use but it's output","825","    is ambiguous. It can be 1-D NumPy array of object dtype if","826","    the input is a ragged array, but if the input is a list of","827","    equal length arrays, then the output is a 2D numpy.array.","828","    _to_object_array solves this ambiguity by guarantying that","829","    the output is a 1-D NumPy array of objects for any input.","830","","831","    Parameters","832","    ----------","833","    sequence : array-like of shape (n_elements,)","834","        The sequence to be converted.","835","","836","    Returns","837","    -------","838","    out : ndarray of shape (n_elements,), dtype=object","839","        The converted sequence into a 1-D NumPy array of object dtype.","840","","841","    Examples","842","    --------","843","    >>> import numpy as np","844","    >>> from sklearn.utils import _to_object_array","845","    >>> _to_object_array([np.array([0]), np.array([1])])","846","    array([array([0]), array([1])], dtype=object)","847","    >>> _to_object_array([np.array([0]), np.array([1, 2])])","848","    array([array([0]), array([1, 2])], dtype=object)","849","    >>> np.array([np.array([0]), np.array([1])])","850","    array([[0],","851","       [1]])","852","    >>> np.array([np.array([0]), np.array([1, 2])])","853","    array([array([0]), array([1, 2])], dtype=object)","854","    \"\"\"","855","    out = np.empty(len(sequence), dtype=object)","856","    out[:] = sequence","857","    return out","858","","859",""],"delete":[]}]}},"3e4ad0567de5401ec42cc49af68cd07b83f0d716":{"changes":{"doc\/templates\/index.html":"MODIFY"},"diff":{"doc\/templates\/index.html":[{"add":["10","        <a class=\"btn sk-landing-btn mb-1\" href=\"whats_new\/v{{ version }}.html\" role=\"button\">What's New in {{ version }}<\/a>"],"delete":["10","        <a class=\"btn sk-landing-btn mb-1\" href=\"whats_new.html\" role=\"button\">Whats New in {{ version }}<\/a>"]}]}},"13134a884b92f8c601162ce4f125c2fc17c6ed4a":{"changes":{"sklearn\/_build_utils\/deprecated_modules.py":"MODIFY","sklearn\/cluster\/_spectral.py":"MODIFY","sklearn\/feature_extraction\/text.py":"MODIFY","sklearn\/decomposition\/_lda.py":"ADD","sklearn\/cluster\/tests\/test_k_means.py":"MODIFY","sklearn\/feature_extraction\/_hash.py":"ADD","sklearn\/decomposition\/tests\/test_online_lda.py":"MODIFY","sklearn\/feature_extraction\/__init__.py":"MODIFY","\/dev\/null":"DELETE","sklearn\/decomposition\/__init__.py":"MODIFY","sklearn\/cluster\/__init__.py":"MODIFY","sklearn\/cluster\/_kmeans.py":"ADD","sklearn\/cluster\/_agglomerative.py":"ADD","sklearn\/cluster\/tests\/test_hierarchical.py":"MODIFY"},"diff":{"sklearn\/_build_utils\/deprecated_modules.py":[{"add":["49","    ('_agglomerative', 'sklearn.cluster.hierarchical', 'sklearn.cluster',","51","    ('_kmeans', 'sklearn.cluster.k_means_', 'sklearn.cluster', 'KMeans'),","103","    ('_lda', 'sklearn.decomposition.online_lda',","142","    ('_hash', 'sklearn.feature_extraction.hashing',"],"delete":["49","    ('_hierarchical', 'sklearn.cluster.hierarchical', 'sklearn.cluster',","51","    ('_k_means', 'sklearn.cluster.k_means_', 'sklearn.cluster', 'KMeans'),","103","    ('_online_lda', 'sklearn.decomposition.online_lda',","142","    ('_hashing', 'sklearn.feature_extraction.hashing',"]}],"sklearn\/cluster\/_spectral.py":[{"add":["17","from ._kmeans import k_means"],"delete":["17","from ._k_means import k_means"]}],"sklearn\/feature_extraction\/text.py":[{"add":["29","from ._hash import FeatureHasher"],"delete":["29","from ._hashing import FeatureHasher"]}],"sklearn\/decomposition\/_lda.py":[{"add":[],"delete":[]}],"sklearn\/cluster\/tests\/test_k_means.py":[{"add":["24","from sklearn.cluster._kmeans import _labels_inertia","25","from sklearn.cluster._kmeans import _mini_batch_step","736","    from sklearn.cluster._kmeans import _init_centroids","923","    from sklearn.cluster._kmeans import _check_normalize_sample_weight"],"delete":["24","from sklearn.cluster._k_means import _labels_inertia","25","from sklearn.cluster._k_means import _mini_batch_step","736","    from sklearn.cluster._k_means import _init_centroids","923","    from sklearn.cluster._k_means import _check_normalize_sample_weight"]}],"sklearn\/feature_extraction\/_hash.py":[{"add":[],"delete":[]}],"sklearn\/decomposition\/tests\/test_online_lda.py":[{"add":["10","from sklearn.decomposition._lda import (_dirichlet_expectation_1d,","11","                                        _dirichlet_expectation_2d)"],"delete":["10","from sklearn.decomposition._online_lda import (_dirichlet_expectation_1d,","11","                                               _dirichlet_expectation_2d)"]}],"sklearn\/feature_extraction\/__init__.py":[{"add":["7","from ._hash import FeatureHasher"],"delete":["7","from ._hashing import FeatureHasher"]}],"\/dev\/null":[{"add":[],"delete":[]}],"sklearn\/decomposition\/__init__.py":[{"add":["18","from ._lda import LatentDirichletAllocation"],"delete":["18","from ._online_lda import LatentDirichletAllocation"]}],"sklearn\/cluster\/__init__.py":[{"add":["9","from ._agglomerative import (ward_tree, AgglomerativeClustering,","10","                             linkage_tree, FeatureAgglomeration)","11","from ._kmeans import k_means, KMeans, MiniBatchKMeans"],"delete":["9","from ._hierarchical import (ward_tree, AgglomerativeClustering, linkage_tree,","10","                            FeatureAgglomeration)","11","from ._k_means import k_means, KMeans, MiniBatchKMeans"]}],"sklearn\/cluster\/_kmeans.py":[{"add":[],"delete":[]}],"sklearn\/cluster\/_agglomerative.py":[{"add":[],"delete":[]}],"sklearn\/cluster\/tests\/test_hierarchical.py":[{"add":["24","from sklearn.cluster._agglomerative import (_hc_cut, _TREE_BUILDERS,","25","                                            linkage_tree,","26","                                            _fix_connectivity)"],"delete":["24","from sklearn.cluster._hierarchical import (_hc_cut, _TREE_BUILDERS,","25","                                           linkage_tree, _fix_connectivity)"]}]}},"77aec1fb8dbacbcdd1b863cdaca27571e301fa29":{"changes":{"sklearn\/feature_extraction\/_dict_vectorizer.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY"},"diff":{"sklearn\/feature_extraction\/_dict_vectorizer.py":[{"add":["156","        indptr = [0]"],"delete":["156","        indptr = array(\"i\", [0])","184","        indptr = np.frombuffer(indptr, dtype=np.intc)"]}],"doc\/whats_new\/v0.22.rst":[{"add":["310","- |Fix| Fixed a bug that caused :class:`feature_extraction.DictVectorizer` to raise","311","  an `OverflowError` during the `transform` operation when producing a `scipy.sparse`","312","  matrix on large input data. :pr:`15463` by :user:`Norvan Sahiner <norvan>`.","313",""],"delete":[]}]}},"535ef5516bce75c6a51127da95dcb577af1fe35e":{"changes":{"sklearn\/decomposition\/tests\/test_kernel_pca.py":"MODIFY","sklearn\/decomposition\/_kernel_pca.py":"MODIFY","doc\/whats_new\/v0.23.rst":"MODIFY"},"diff":{"sklearn\/decomposition\/tests\/test_kernel_pca.py":[{"add":["9","from sklearn.datasets import make_blobs","285","","286","","287","@pytest.mark.parametrize(\"kernel\",","288","                         [\"linear\", \"poly\", \"rbf\", \"sigmoid\", \"cosine\"])","289","def test_kernel_pca_inverse_transform(kernel):","290","    X, *_ = make_blobs(n_samples=100, n_features=4, centers=[[1, 1, 1, 1]],","291","                       random_state=0)","292","","293","    kp = KernelPCA(n_components=2, kernel=kernel, fit_inverse_transform=True)","294","    X_trans = kp.fit_transform(X)","295","    X_inv = kp.inverse_transform(X_trans)","296","    assert_allclose(X, X_inv)"],"delete":[]}],"sklearn\/decomposition\/_kernel_pca.py":[{"add":["360","        n_samples = self.X_transformed_fit_.shape[0]","361","        K.flat[::n_samples + 1] += self.alpha"],"delete":["360",""]}],"doc\/whats_new\/v0.23.rst":[{"add":["144","- |Fix| :class:`decomposition.KernelPCA` method ``inverse_transform`` now","145","  applies the correct inverse transform to the transformed data. :pr:`16655`","146","  by :user:`Lewis Ball <lrjball>`.","147",""],"delete":[]}]}},"6b646da89c516a14adf36bad2994b5782df67050":{"changes":{"sklearn\/kernel_approximation.py":"MODIFY"},"diff":{"sklearn\/kernel_approximation.py":[{"add":["48","    random_offset_ : ndarray of shape (n_components,), dtype=float64","52","    random_weights_ : ndarray of shape (n_features, n_components),\\","53","        dtype=float64"],"delete":["48","    random_offset_: ndarray of shape (n_components,), dtype=float64","52","    random_weights_: ndarray of shape (n_features, n_components), dtype=float64"]}]}},"073de7c6e56eb7fc4f1a4fac5f66ef94fea58d78":{"changes":{"sklearn\/naive_bayes.py":"MODIFY","doc\/whats_new\/v0.23.rst":"MODIFY","sklearn\/tests\/test_naive_bayes.py":"MODIFY"},"diff":{"sklearn\/naive_bayes.py":[{"add":["1207","                             % (self.n_features_, X.shape[1]))"],"delete":["1207","                             .format(self.n_features_, X.shape[1]))"]}],"doc\/whats_new\/v0.23.rst":[{"add":["130",":mod:`sklearn.naive_bayes`","131",".............................","132","","133","- |Fix| A correctly formatted error message is shown in","134","  :class:`naive_bayes.CategoricalNB` when the number of features in the input","135","  differs between `predict` and `fit`.","136","  :pr:`16090` by :user:`Madhura Jayaratne <madhuracj>`.","137",""],"delete":[]}],"sklearn\/tests\/test_naive_bayes.py":[{"add":["669","    # Check error is raised for incorrect X","670","    X = np.array([[1, 4, 1], [2, 5, 6]])","671","    msg = \"Expected input with 2 features, got 3 instead\"","672","    assert_raise_message(ValueError, msg, clf.predict, X)","673",""],"delete":[]}]}},"b4678ca8aa0687575e1e1d2809355aa6c6975b16":{"changes":{"sklearn\/metrics\/pairwise.py":"MODIFY","sklearn\/metrics\/tests\/test_pairwise.py":"MODIFY","doc\/whats_new\/v0.24.rst":"MODIFY"},"diff":{"sklearn\/metrics\/pairwise.py":[{"add":["31","from ..utils.fixes import sp_version, parse_version","1448","        # There is a bug in scipy < 1.5 that will cause a crash if","1449","        # X.dtype != np.double (float64). See PR #15730","1450","        dtype = np.float64 if sp_version < parse_version('1.5') else None","1452","            V = np.var(X, axis=0, ddof=1, dtype=dtype)","1457","            V = np.var(np.vstack([X, Y]), axis=0, ddof=1, dtype=dtype)"],"delete":["1448","            V = np.var(X, axis=0, ddof=1)","1453","            V = np.var(np.vstack([X, Y]), axis=0, ddof=1)"]}],"sklearn\/metrics\/tests\/test_pairwise.py":[{"add":["683","    # and fails due to rounding errors.","704","    # and fails due to rounding errors.","731","    # and fails due to rounding errors.","751","    # and fails due to rounding errors.","1296","","1297","","1298","@pytest.mark.parametrize(","1299","        'metric', [","1300","            'braycurtis', 'canberra', 'chebyshev',","1301","            'correlation', 'hamming', 'mahalanobis', 'minkowski', 'seuclidean',","1302","            'sqeuclidean', 'cityblock', 'cosine', 'euclidean'])","1303","@pytest.mark.parametrize(","1304","        \"dtype\",","1305","        [np.float32, np.float64])","1306","@pytest.mark.parametrize(\"y_is_x\", [True, False], ids=[\"Y is X\", \"Y is not X\"])","1307","def test_numeric_pairwise_distances_datatypes(metric, dtype, y_is_x):","1308","    # Check that pairwise distances gives the same result as pdist and cdist","1309","    # regardless of input datatype when using any scipy metric for comparing","1310","    # numeric vectors","1311","    #","1312","    # This test is necessary because pairwise_distances used to throw an","1313","    # error when using metric='seuclidean' and the input data was not","1314","    # of type np.float64 (#15730)","1315","","1316","    rng = np.random.RandomState(0)","1317","","1318","    X = rng.random_sample((5, 4)).astype(dtype)","1319","","1320","    params = {}","1321","    if y_is_x:","1322","        Y = X","1323","        expected_dist = squareform(pdist(X, metric=metric))","1324","    else:","1325","        Y = rng.random_sample((5, 4)).astype(dtype)","1326","        expected_dist = cdist(X, Y, metric=metric)","1327","        # precompute parameters for seuclidean & mahalanobis when x is not y","1328","        if metric == 'seuclidean':","1329","            params = {'V': np.var(np.vstack([X, Y]),","1330","                                  axis=0, ddof=1, dtype=np.float64)}","1331","        elif metric == 'mahalanobis':","1332","            params = {'VI': np.linalg.inv(np.cov(np.vstack([X, Y]).T)).T}","1333","","1334","    dist = pairwise_distances(X, Y, metric=metric, **params)","1335","","1336","    # the default rtol=1e-7 is too close to the float32 precision","1337","    # and fails due to rounding errors","1338","    rtol = 1e-5 if dtype is np.float32 else 1e-7","1339","    assert_allclose(dist, expected_dist, rtol=rtol)"],"delete":["683","    # and fails due too rounding errors.","704","    # and fails due too rounding errors.","731","    # and fails due too rounding errors.","751","    # and fails due too rounding errors."]}],"doc\/whats_new\/v0.24.rst":[{"add":["229","- |Fix| Fixed a bug where :func:`metrics.pairwise_distances` would raise an error if","230","  ``metric='seuclidean'`` and ``X`` is not type ``np.float64``.","231","  :pr:`15730` by :user:`Forrest Koch <ForrestCKoch>`.","232",""],"delete":[]}]}},"baa4f0780801a71a92a18b33008207f12fedba0b":{"changes":{"sklearn\/ensemble\/tests\/test_bagging.py":"MODIFY","sklearn\/tree\/_classes.py":"MODIFY","sklearn\/ensemble\/_bagging.py":"MODIFY","doc\/whats_new\/v0.23.rst":"MODIFY"},"diff":{"sklearn\/ensemble\/tests\/test_bagging.py":[{"add":["880","","881","","882","def test_bagging_get_estimators_indices():","883","    # Check that Bagging estimator can generate sample indices properly","884","    # Non-regression test for:","885","    # https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/16436","886","","887","    rng = np.random.RandomState(0)","888","    X = rng.randn(13, 4)","889","    y = np.arange(13)","890","","891","    class MyEstimator(DecisionTreeRegressor):","892","        \"\"\"An estimator which stores y indices information at fit.\"\"\"","893","        def fit(self, X, y):","894","            self._sample_indices = y","895","","896","    clf = BaggingRegressor(base_estimator=MyEstimator(),","897","                           n_estimators=1, random_state=0)","898","    clf.fit(X, y)","899","","900","    assert_array_equal(clf.estimators_[0]._sample_indices,","901","                       clf.estimators_samples_[0])"],"delete":[]}],"sklearn\/tree\/_classes.py":[{"add":["1670","    0.7447..."],"delete":["1670","    0.7788..."]}],"sklearn\/ensemble\/_bagging.py":[{"add":["84","        random_state = seeds[i]","408","                seed, self.bootstrap_features, self.bootstrap,"],"delete":["84","        random_state = np.random.RandomState(seeds[i])","407","            random_state = np.random.RandomState(seed)","409","                random_state, self.bootstrap_features, self.bootstrap,"]}],"doc\/whats_new\/v0.23.rst":[{"add":["25","- :class:`ensemble.BaggingClassifier`, :class:`ensemble.BaggingRegressor`,","26","  and :class:`ensemble.IsolationForest`. |Fix|","143","- |Fix| Fixed a bug in :class:`ensemble.BaggingClassifier`,","144","  :class:`ensemble.BaggingRegressor` and :class:`ensemble.IsolationForest`","145","  where the attribute `estimators_samples_` did not generate the proper indices","146","  used during `fit`.","147","  :pr:`16437` by :user:`Jin-Hwan CHO <chofchof>`.","148",""],"delete":["25","- list models here"]}]}},"54a09dc864620c4e351aa0c08cb0425a4006052d":{"changes":{"sklearn\/ensemble\/_forest.py":"MODIFY"},"diff":{"sklearn\/ensemble\/_forest.py":[{"add":["955","        whole dataset is used to build each tree.","1276","        whole dataset is used to build each tree."],"delete":["955","        whole datset is used to build each tree.","1276","        whole datset is used to build each tree."]}]}},"4953ec36a41694d38f8d92fd48c7049cdd82e0ed":{"changes":{"\/dev\/null":"DELETE","sklearn\/linear_model\/tests\/test_least_angle.py":"MODIFY","sklearn\/multioutput.py":"MODIFY","sklearn\/svm\/_classes.py":"MODIFY","sklearn\/svm\/tests\/test_svm.py":"MODIFY","sklearn\/linear_model\/_least_angle.py":"MODIFY"},"diff":{"\/dev\/null":[{"add":[],"delete":[]}],"sklearn\/linear_model\/tests\/test_least_angle.py":[{"add":["17","from sklearn.linear_model._least_angle import _lars_path_residues","18","from sklearn.linear_model import LassoLarsIC, lars_path","733","","734","","735","def test_X_none_gram_not_none():","736","    with pytest.raises(ValueError,","737","                       match=\"X cannot be None if Gram is not None\"):","738","        lars_path(X=None, y=[1], Gram='not None')"],"delete":["17","from sklearn.linear_model._least_angle import _lars_path_residues, LassoLarsIC"]}],"sklearn\/multioutput.py":[{"add":[],"delete":["267","    # XXX Remove this method in 0.23","268","    def score(self, X, y, sample_weight=None):","269","        \"\"\"Returns the coefficient of determination R^2 of the prediction.","270","","271","        The coefficient R^2 is defined as (1 - u\/v), where u is the residual","272","        sum of squares ((y_true - y_pred) ** 2).sum() and v is the regression","273","        sum of squares ((y_true - y_true.mean()) ** 2).sum().","274","        Best possible score is 1.0 and it can be negative (because the","275","        model can be arbitrarily worse). A constant model that always","276","        predicts the expected value of y, disregarding the input features,","277","        would get a R^2 score of 0.0.","278","","279","        Notes","280","        -----","281","        R^2 is calculated by weighting all the targets equally using","282","        `multioutput='uniform_average'`.","283","","284","        Parameters","285","        ----------","286","        X : array-like, shape (n_samples, n_features)","287","            Test samples.","288","","289","        y : array-like, shape (n_samples) or (n_samples, n_outputs)","290","            True values for X.","291","","292","        sample_weight : array-like, shape [n_samples], optional","293","            Sample weights.","294","","295","        Returns","296","        -------","297","        score : float","298","            R^2 of self.predict(X) wrt. y.","299","        \"\"\"","300","        # XXX remove in 0.19 when r2_score default for multioutput changes","301","        from .metrics import r2_score","302","        return r2_score(y, self.predict(X), sample_weight=sample_weight,","303","                        multioutput='uniform_average')","304",""]}],"sklearn\/svm\/_classes.py":[{"add":[],"delete":["216","        # FIXME Remove l1\/l2 support in 0.23 ----------------------------------","217","        msg = (\"loss='%s' has been deprecated in favor of \"","218","               \"loss='%s' as of 0.16. Backward compatibility\"","219","               \" for the loss='%s' will be removed in %s\")","220","","221","        if self.loss in ('l1', 'l2'):","222","            old_loss = self.loss","223","            self.loss = {'l1': 'hinge', 'l2': 'squared_hinge'}.get(self.loss)","224","            warnings.warn(msg % (old_loss, self.loss, old_loss, '0.23'),","225","                          FutureWarning)","226","        # ---------------------------------------------------------------------","227","","408","        # FIXME Remove l1\/l2 support in 0.23 ----------------------------------","409","        msg = (\"loss='%s' has been deprecated in favor of \"","410","               \"loss='%s' as of 0.16. Backward compatibility\"","411","               \" for the loss='%s' will be removed in %s\")","412","","413","        if self.loss in ('l1', 'l2'):","414","            old_loss = self.loss","415","            self.loss = {'l1': 'epsilon_insensitive',","416","                         'l2': 'squared_epsilon_insensitive'","417","                         }.get(self.loss)","418","            warnings.warn(msg % (old_loss, self.loss, old_loss, '0.23'),","419","                          FutureWarning)","420","        # ---------------------------------------------------------------------","421",""]}],"sklearn\/svm\/tests\/test_svm.py":[{"add":[],"delete":["738","# FIXME remove in 0.23","739","def test_linearsvx_loss_penalty_deprecations():","740","    X, y = [[0.0], [1.0]], [0, 1]","741","","742","    msg = (\"loss='%s' has been deprecated in favor of \"","743","           \"loss='%s' as of 0.16. Backward compatibility\"","744","           \" for the %s will be removed in %s\")","745","","746","    # LinearSVC","747","    # loss l1 --> hinge","748","    assert_warns_message(FutureWarning,","749","                         msg % (\"l1\", \"hinge\", \"loss='l1'\", \"0.23\"),","750","                         svm.LinearSVC(loss=\"l1\").fit, X, y)","751","","752","    # loss l2 --> squared_hinge","753","    assert_warns_message(FutureWarning,","754","                         msg % (\"l2\", \"squared_hinge\", \"loss='l2'\", \"0.23\"),","755","                         svm.LinearSVC(loss=\"l2\").fit, X, y)","756","","757","    # LinearSVR","758","    # loss l1 --> epsilon_insensitive","759","    assert_warns_message(FutureWarning,","760","                         msg % (\"l1\", \"epsilon_insensitive\", \"loss='l1'\",","761","                                \"0.23\"),","762","                         svm.LinearSVR(loss=\"l1\").fit, X, y)","763","","764","    # loss l2 --> squared_epsilon_insensitive","765","    assert_warns_message(FutureWarning,","766","                         msg % (\"l2\", \"squared_epsilon_insensitive\",","767","                                \"loss='l2'\", \"0.23\"),","768","                         svm.LinearSVR(loss=\"l2\").fit, X, y)","769","","770",""]}],"sklearn\/linear_model\/_least_angle.py":[{"add":["146","        raise ValueError(","147","            'X cannot be None if Gram is not None'","148","            'Use lars_path_gram to avoid passing X and y.'","149","        )"],"delete":["49","        .. deprecated:: 0.21","50","","51","           The use of ``X`` is ``None`` in combination with ``Gram`` is not","52","           ``None`` will be removed in v0.23. Use :func:`lars_path_gram`","53","           instead.","54","","69","        .. deprecated:: 0.21","70","","71","           The use of ``X`` is ``None`` in combination with ``Gram`` is not","72","           None will be removed in v0.23. Use :func:`lars_path_gram` instead.","73","","157","        warnings.warn('Use lars_path_gram to avoid passing X and y. '","158","                      'The current option will be removed in v0.23.',","159","                      FutureWarning)"]}]}},"b189bf60708af22dde82a00aca7b5a54290b666d":{"changes":{"sklearn\/preprocessing\/_data.py":"MODIFY","doc\/whats_new\/v0.23.rst":"MODIFY","sklearn\/preprocessing\/tests\/test_data.py":"MODIFY"},"diff":{"sklearn\/preprocessing\/_data.py":[{"add":["1720","            mins, maxes = min_max_axis(X, 1)","1721","            norms = np.maximum(abs(mins), maxes)","1731","            norms = np.max(abs(X), axis=1)","1749","    that its norm (l1, l2 or inf) equals one.","1766","        The norm to use to normalize each non zero sample. If norm='max'","1767","        is used, values will be rescaled by the maximum of the absolute","1768","        values."],"delete":["1720","            _, norms = min_max_axis(X, 1)","1730","            norms = np.max(X, axis=1)","1748","    that its norm (l1 or l2) equals one.","1765","        The norm to use to normalize each non zero sample."]}],"doc\/whats_new\/v0.23.rst":[{"add":["359","- |Fix| Fix a bug in :class:`preprocessing.Normalizer` with norm='max',","360","  which was not taking the absolute value of the maximum values before","361","  normalizing the vectors. :pr:`16632` by","362","  :user:`Maura Pintor <Maupin1991>` and :user:`Battista Biggio <bbiggio>`.","363",""],"delete":[]}],"sklearn\/preprocessing\/tests\/test_data.py":[{"add":["1949","            row_maxs = abs(X_norm).max(axis=1)","1968","def test_normalizer_max_sign():","1969","    # check that we normalize by a positive number even for negative data","1970","    rng = np.random.RandomState(0)","1971","    X_dense = rng.randn(4, 5)","1972","    # set the row number 3 to zero","1973","    X_dense[3, :] = 0.0","1974","    # check for mixed data where the value with","1975","    # largest magnitude is negative","1976","    X_dense[2, abs(X_dense[2, :]).argmax()] *= -1","1977","    X_all_neg = -np.abs(X_dense)","1978","    X_all_neg_sparse = sparse.csr_matrix(X_all_neg)","1979","","1980","    for X in (X_dense, X_all_neg, X_all_neg_sparse):","1981","        normalizer = Normalizer(norm='max')","1982","        X_norm = normalizer.transform(X)","1983","        assert X_norm is not X","1984","        X_norm = toarray(X_norm)","1985","        assert_array_equal(","1986","            np.sign(X_norm), np.sign(toarray(X)))","1987","","1988",""],"delete":["1949","            row_maxs = X_norm.max(axis=1)"]}]}},"fd12d5684ad224ad7760374b1dcca2821c644feb":{"changes":{"doc\/whats_new\/v0.23.rst":"MODIFY","sklearn\/decomposition\/_pca.py":"MODIFY","sklearn\/decomposition\/tests\/test_pca.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.23.rst":[{"add":["75",":mod:`sklearn.decomposition`","76","............................","77","","78","- |Fix| :class:`decomposition.PCA` with a float `n_components` parameter, will","79","   exclusively choose the components that explain the variance greater than","80","   `n_components`. :pr:`15669` by :user:`Krishna Chaitanya <krishnachaitanya9>`","81","","83","......................."],"delete":["76","................................."]}],"sklearn\/decomposition\/_pca.py":[{"add":["464","            # side='right' ensures that number of features selected","465","            # their variance is always greater than n_components float","466","            # passed. More discussion in issue: #15669","468","            n_components = np.searchsorted(ratio_cumsum, n_components,","469","                                           side='right') + 1"],"delete":["465","            n_components = np.searchsorted(ratio_cumsum, n_components) + 1","466",""]}],"sklearn\/decomposition\/tests\/test_pca.py":[{"add":["9","from sklearn.datasets import load_iris","558","","559","","560","def test_pca_n_components_mostly_explained_variance_ratio():","561","    # when n_components is the second highest cumulative sum of the","562","    # explained_variance_ratio_, then n_components_ should equal the","563","    # number of features in the dataset #15669","564","    X, y = load_iris(return_X_y=True)","565","    pca1 = PCA().fit(X, y)","566","","567","    n_components = pca1.explained_variance_ratio_.cumsum()[-2]","568","    pca2 = PCA(n_components=n_components).fit(X, y)","569","    assert pca2.n_components_ == X.shape[1]"],"delete":[]}]}},"32e9911c9987bc810cf4b326de5995e2c50aefc4":{"changes":{"doc\/modules\/classes.rst":"MODIFY","sklearn\/semi_supervised\/_label_propagation.py":"MODIFY"},"diff":{"doc\/modules\/classes.rst":[{"add":[],"delete":["1482","Low-level methods","1483","-----------------","1484","","1485",".. autosummary::","1486","   :toctree: generated","1487","   :template: function.rst","1488","","1489","   svm.libsvm.cross_validation","1490","   svm.libsvm.decision_function","1491","   svm.libsvm.fit","1492","   svm.libsvm.predict","1493","   svm.libsvm.predict_proba","1494","","1495",""]}],"sklearn\/semi_supervised\/_label_propagation.py":[{"add":["66","from ..neighbors import NearestNeighbors"],"delete":["66","from ..neighbors.unsupervised import NearestNeighbors"]}]}},"3743a55aed5c9ee18c8b94b95dfc4c41d0ae99f5":{"changes":{"doc\/whats_new\/v0.23.rst":"MODIFY","sklearn\/utils\/optimize.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.23.rst":[{"add":["143","- |Fix| :class:`linear_model.LogisticRegression` will now avoid an unnecessary","144","  iteration when `solver='newton-cg'` by checking for inferior or equal instead","145","  of strictly inferior for maximum of `absgrad` and `tol` in `utils.optimize._newton_cg`.","146","  :pr:`16266` by :user:`Rushabh Vasani <rushabh-v>`.","147","","210",""],"delete":[]}],"sklearn\/utils\/optimize.py":[{"add":["184","        if np.max(absgrad) <= tol:"],"delete":["184","        if np.max(absgrad) < tol:"]}]}}}