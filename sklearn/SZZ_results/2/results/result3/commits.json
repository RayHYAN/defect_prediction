{"a2ccc330bfabe8a878c5887066204fe3d366562f":{"changes":{"sklearn\/linear_model\/_logistic.py":"MODIFY","sklearn\/utils\/_testing.py":"MODIFY"},"diff":{"sklearn\/linear_model\/_logistic.py":[{"add":["731","            def func(x, *args): return _multinomial_loss_grad(x, *args)[0:2]","733","            def func(x, *args): return _multinomial_loss(x, *args)[0]","734","            def grad(x, *args): return _multinomial_loss_grad(x, *args)[1]","743","            def grad(x, *args): return _logistic_loss_and_grad(x, *args)[1]","1308","                raise ValueError(\"l1_ratio must be between 0 and 1;\"","1309","                                 \" got (l1_ratio=%r)\" % self.l1_ratio)"],"delete":["731","            func = lambda x, *args: _multinomial_loss_grad(x, *args)[0:2]","733","            func = lambda x, *args: _multinomial_loss(x, *args)[0]","734","            grad = lambda x, *args: _multinomial_loss_grad(x, *args)[1]","743","            grad = lambda x, *args: _logistic_loss_and_grad(x, *args)[1]","1308","                        raise ValueError(\"l1_ratio must be between 0 and 1;\"","1309","                                         \" got (l1_ratio=%r)\" % self.l1_ratio)"]}],"sklearn\/utils\/_testing.py":[{"add":["185","                def check_in_message(msg): return message in msg"],"delete":["185","                check_in_message = lambda msg: message in msg"]}]}},"2885a06c94f2938ba722d3dee02e2b9d772b7104":{"changes":{"sklearn\/inspection\/tests\/test_plot_partial_dependence.py":"MODIFY","sklearn\/inspection\/_partial_dependence.py":"MODIFY"},"diff":{"sklearn\/inspection\/tests\/test_plot_partial_dependence.py":[{"add":["231","    # The first call to plot_partial_dependence will create two new axes to","232","    # place in the space of the passed in axes, which results in a total of","233","    # three axes in the figure.","234","    # Currently the API does not allow for the second call to","235","    # plot_partial_dependence to use the same axes again, because it will","236","    # create two new axes in the space resulting in five axes. To get the","237","    # expected behavior one needs to pass the generated axes into the second","238","    # call:","239","    # disp1 = plot_partial_dependence(...)","240","    # disp2 = plot_partial_dependence(..., ax=disp1.axes_)"],"delete":["231","    # The first call to `plot_*` will plot the axes"]}],"sklearn\/inspection\/_partial_dependence.py":[{"add":["861","            # If ax was set off, it has most likely been set to off","863","            if not ax.axison:"],"delete":["861","            # If ax has visible==False, it has most likely been set to False","863","            if not ax.get_visible():","869","            ax.set_visible(False)"]}]}},"856d273b9dd100cf478f86579333a2b2d78c2d76":{"changes":{"doc\/modules\/classes.rst":"MODIFY","sklearn\/feature_selection\/__init__.py":"MODIFY"},"diff":{"doc\/modules\/classes.rst":[{"add":["35","   feature_selection.SelectorMixin"],"delete":[]}],"sklearn\/feature_selection\/__init__.py":[{"add":["26","from ._base import SelectorMixin","27","","44","           'mutual_info_regression',","45","           'SelectorMixin']"],"delete":["42","           'mutual_info_regression']"]}]}},"6419f65ec0183c01316f20c797ffcdc868ab141e":{"changes":{"sklearn\/ensemble\/_forest.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY"},"diff":{"sklearn\/ensemble\/_forest.py":[{"add":["2145","                 warm_start=False):","2160","            max_samples=None)"],"delete":["2114","    max_samples : int or float, default=None","2115","        If bootstrap is True, the number of samples to draw from X","2116","        to train each base estimator.","2117","","2118","        - If None (default), then draw `X.shape[0]` samples.","2119","        - If int, then draw `max_samples` samples.","2120","        - If float, then draw `max_samples * X.shape[0]` samples. Thus,","2121","          `max_samples` should be in the interval `(0, 1)`.","2122","","2123","        .. versionadded:: 0.22","2124","","2156","                 warm_start=False,","2157","                 max_samples=None):","2172","            max_samples=max_samples)"]}],"doc\/whats_new\/v0.22.rst":[{"add":["331","  :class:`ensemble.ExtraTreesRegressor`. :pr:`14682` by"],"delete":["328","  :class:`ensemble.ForestClassifier`,","329","  :class:`ensemble.ForestRegressor`,","333","  :class:`ensemble.ExtraTreesRegressor`,","334","  :class:`ensemble.RandomTreesEmbedding`. :pr:`14682` by"]}]}},"641b8631884e2cb19a27fd30440a41a1437e4e40":{"changes":{"build_tools\/circle\/build_doc.sh":"MODIFY"},"diff":{"build_tools\/circle\/build_doc.sh":[{"add":["81","        scripts_names=\"$(echo \"$auto_example_files\" | sed 's\/sphx_glr_\/\/' | sed -E 's\/_([[:digit:]][[:digit:]][[:digit:]]|thumb).png\/.py\/')\""],"delete":["81","        scripts_names=\"$(echo \"$auto_example_files\" | sed 's\/sphx_glr_\/\/' | sed -e 's\/_([[:digit:]][[:digit:]][[:digit:]]|thumb).png\/.py\/')\""]}]}},"c4733f4895c1becdf587b38970f6f7066656e3f9":{"changes":{"doc\/developers\/advanced_installation.rst":"MODIFY","sklearn\/decomposition\/_dict_learning.py":"MODIFY","doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_gradient_boosting.py":"MODIFY","doc\/modules\/computing.rst":"MODIFY","sklearn\/metrics\/tests\/test_score_objects.py":"MODIFY","doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/metrics\/tests\/test_common.py":"MODIFY","doc\/modules\/neighbors.rst":"MODIFY","sklearn\/externals\/_arff.py":"MODIFY","examples\/inspection\/plot_partial_dependence.py":"MODIFY","doc\/modules\/model_evaluation.rst":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/binning.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY","sklearn\/ensemble\/tests\/test_gradient_boosting.py":"MODIFY","build_tools\/azure\/install.sh":"MODIFY","sklearn\/metrics\/_regression.py":"MODIFY","sklearn\/model_selection\/_search.py":"MODIFY"},"diff":{"doc\/developers\/advanced_installation.rst":[{"add":["376","For the upcoming FreeBSD 12.1 and 11.3 versions, OpenMP will be included in"],"delete":["376","For the upcomming FreeBSD 12.1 and 11.3 versions, OpenMP will be included in"]}],"sklearn\/decomposition\/_dict_learning.py":[{"add":["706","        avoid losing the history of the evolution.","1353","        Keeping them is useful in online settings, to avoid losing the"],"delete":["706","        avoid loosing the history of the evolution.","1353","        Keeping them is useful in online settings, to avoid loosing the"]}],"doc\/whats_new\/v0.20.rst":[{"add":["711","  centered with the train mean respectively during the fit phase and the"],"delete":["711","  centered with the train mean repsectively during the fit phase and the"]}],"sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_gradient_boosting.py":[{"add":["415","    # cannot check the predictions for binned values here."],"delete":["415","    # cannot check the predicitons for binned values here."]}],"doc\/modules\/computing.rst":[{"add":["531","(threads or processes) that are spawned in parallel can be controlled via the","668","    the optimal size of temporary arrays used by some algorithms."],"delete":["531","(threads or processes) that are spawned in parallel can be controled via the","668","    the optimal size of temporary arrays used by some algoritms."]}],"sklearn\/metrics\/tests\/test_score_objects.py":[{"add":["651","    # scoring dictionary returned is the same as calling each scorer separately","666","    separate_scores = {","672","        assert_allclose(value, separate_scores[score_name])"],"delete":["651","    # scoring dictionary returned is the same as calling each scorer seperately","666","    seperate_scores = {","672","        assert_allclose(value, seperate_scores[score_name])"]}],"doc\/whats_new\/v0.21.rst":[{"add":["297","  algorithm related to :class:`cluster.DBSCAN`, that has hyperparameters easier"],"delete":["297","  algoritm related to :class:`cluster.DBSCAN`, that has hyperparameters easier"]}],"sklearn\/metrics\/tests\/test_common.py":[{"add":["117","    # . Naming it with an unnormalized_ prefix is necessary for this module to"],"delete":["117","    # . Naming it with an unnormalized_ prefix is neccessary for this module to"]}],"doc\/modules\/neighbors.rst":[{"add":["583","  To maximise compatibility with all estimators, a safe choice is to always"],"delete":["583","  To maximise compatiblity with all estimators, a safe choice is to always"]}],"sklearn\/externals\/_arff.py":[{"add":["100","``STRING``. For nominal attributes, the ``attribute_type`` must be a list of"],"delete":["100","``STRING``. For nominal attributes, the ``atribute_type`` must be a list of"]}],"examples\/inspection\/plot_partial_dependence.py":[{"add":["16","The plots show four 1-way and two 1-way partial dependence plots (omitted for"],"delete":["16","The plots show four 1-way and two 1-way partial dependence plots (ommitted for"]}],"doc\/modules\/model_evaluation.rst":[{"add":["1722","the prediction :math:`\\hat{y}`, which induces the ranking function :math:`f`, the"],"delete":["1722","the prediction :math:`\\hat{y}`, which induces the ranking funtion :math:`f`, the"]}],"sklearn\/ensemble\/_hist_gradient_boosting\/binning.py":[{"add":["34","        randomly chosen to compute the quantiles. If ``None``, the whole data","109","        randomly chosen to compute the quantiles. If ``None``, the whole data","128","        constant across all features. This corresponds to the last bin, and"],"delete":["34","        randomly choosen to compute the quantiles. If ``None``, the whole data","109","        randomly choosen to compute the quantiles. If ``None``, the whole data","128","        constant accross all features. This corresponds to the last bin, and"]}],"doc\/whats_new\/v0.22.rst":[{"add":["801","  invalid model. This behavior occurred only in some border scenarios."],"delete":["801","  invalid model. This behavior occured only in some border scenarios."]}],"sklearn\/ensemble\/tests\/test_gradient_boosting.py":[{"add":["1313","    # initial estimator does not support sample weight"],"delete":["1313","    # inital estimator does not support sample weight"]}],"build_tools\/azure\/install.sh":[{"add":["13","    # The two version numbers are separated with a new line is piped to sort"],"delete":["13","    # The two version numbers are seperated with a new line is piped to sort"]}],"sklearn\/metrics\/_regression.py":[{"add":["719","        # 'Extreme stable', y_true any real number, y_pred > 0"],"delete":["719","        # 'Extreme stable', y_true any realy number, y_pred > 0"]}],"sklearn\/model_selection\/_search.py":[{"add":["950","        attribute will not be available.","1280","        attribute will not be available."],"delete":["950","        attribute will not be availble.","1280","        attribute will not be availble."]}]}},"26d392aa4ce79d7bbc4aa09eafbbac0620c95fa7":{"changes":{"sklearn\/manifold\/tests\/test_mds.py":"MODIFY"},"diff":{"sklearn\/manifold\/tests\/test_mds.py":[{"add":["4","from sklearn.manifold import _mds as mds"],"delete":["4","from sklearn.manifold import mds"]}]}},"dfad061da209869716d4113987db6a655f5c5c3b":{"changes":{"sklearn\/metrics\/_classification.py":"MODIFY","sklearn\/metrics\/tests\/test_classification.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY"},"diff":{"sklearn\/metrics\/_classification.py":[{"add":["1966","            average=average, sample_weight=sample_weight,","1967","            zero_division=zero_division)"],"delete":["1966","            average=average, sample_weight=sample_weight)"]}],"sklearn\/metrics\/tests\/test_classification.py":[{"add":["155","@pytest.mark.parametrize('zero_division', [\"warn\", 0, 1])","156","def test_classification_report_zero_division_warning(zero_division):","157","    y_true, y_pred = [\"a\", \"b\", \"c\"], [\"a\", \"b\", \"d\"]","158","    with warnings.catch_warnings(record=True) as record:","159","        classification_report(","160","            y_true, y_pred, zero_division=zero_division, output_dict=True)","161","        if zero_division == \"warn\":","162","            assert len(record) > 1","163","            for item in record:","164","                msg = (\"Use `zero_division` parameter to control this \"","165","                       \"behavior.\")","166","                assert msg in str(item.message)","167","        else:","168","            assert not record","169","","170",""],"delete":["22","from sklearn.utils._testing import assert_warns"]}],"doc\/whats_new\/v0.22.rst":[{"add":["48","- |Fix| :func:`metrics.classification_report` does no longer ignore the","49","  value of the ``zero_division`` keyword argument. :pr:`15879`","50","  by :user:`Bibhash Chandra Mitra <Bibyutatsu>`.","51",""],"delete":[]}]}},"2151b79a916e37a7f416cce6ba512ad464259bb9":{"changes":{"sklearn\/svm\/src\/liblinear\/linear.cpp":"MODIFY"},"diff":{"sklearn\/svm\/src\/liblinear\/linear.cpp":[{"add":["517","\tdelete[] C;"],"delete":[]}]}},"f4da713c44d9529da2d82d76e0c0c6ef7e0e4795":{"changes":{"sklearn\/metrics\/_plot\/precision_recall_curve.py":"MODIFY","sklearn\/metrics\/_plot\/tests\/test_plot_precision_recall.py":"MODIFY"},"diff":{"sklearn\/metrics\/_plot\/precision_recall_curve.py":[{"add":["163","                                                pos_label=pos_label,"],"delete":[]}],"sklearn\/metrics\/_plot\/tests\/test_plot_precision_recall.py":[{"add":["9","from sklearn.datasets import load_breast_cancer","135","","136","","137","def test_precision_recall_curve_string_labels(pyplot):","138","    # regression test #15738","139","    cancer = load_breast_cancer()","140","    X = cancer.data","141","    y = cancer.target_names[cancer.target]","142","","143","    lr = make_pipeline(StandardScaler(), LogisticRegression())","144","    lr.fit(X, y)","145","    for klass in cancer.target_names:","146","        assert klass in lr.classes_","147","    disp = plot_precision_recall_curve(lr, X, y)","148","","149","    y_pred = lr.predict_proba(X)[:, 1]","150","    avg_prec = average_precision_score(y, y_pred,","151","                                       pos_label=lr.classes_[1])","152","","153","    assert disp.average_precision == pytest.approx(avg_prec)","154","    assert disp.estimator_name == lr.__class__.__name__"],"delete":[]}]}},"034c021fb274076982cb342cf1d9aaad72f0873e":{"changes":{"sklearn\/multioutput.py":"MODIFY"},"diff":{"sklearn\/multioutput.py":[{"add":["550","        chain order.","711","        chain order."],"delete":["550","        chain orders.","711","        chain orders."]}]}},"a203b9e1c6e0ec4f09aaddb4af5010592ea266a3":{"changes":{"sklearn\/utils\/estimator_checks.py":"MODIFY","sklearn\/tests\/test_common.py":"MODIFY"},"diff":{"sklearn\/utils\/estimator_checks.py":[{"add":["362","    if isinstance(estimator, type):","363","        # try to construct estimator to get tags, if it is unable to then","364","        # return the estimator class","365","        try:","366","            xfail_checks = _safe_tags(_construct_instance(estimator),","367","                                      '_xfail_test')","368","        except Exception:","369","            return estimator, check","370","    else:","371","        xfail_checks = _safe_tags(estimator, '_xfail_test')"],"delete":["363","    xfail_checks = _safe_tags(estimator, '_xfail_test')"]}],"sklearn\/tests\/test_common.py":[{"add":["33","    _mark_xfail_checks,","50","def test_estimator_cls_parameterize_with_checks():","51","    # Non-regression test for #16707 to ensure that parametrize_with_checks","52","    # works with estimator classes","53","    param_checks = parametrize_with_checks([LogisticRegression])","54","    # Using the generator does not raise","55","    list(param_checks.args[1])","56","","57","","58","def test_mark_xfail_checks_with_unconsructable_estimator():","59","    class MyEstimator:","60","        def __init__(self):","61","            raise ValueError(\"This is bad\")","62","","63","    estimator, check = _mark_xfail_checks(MyEstimator, 42, None)","64","    assert estimator == MyEstimator","65","    assert check == 42","66","","67",""],"delete":[]}]}},"0d3de54c688a19ebe147ca7c6d78fa717de77a71":{"changes":{"sklearn\/decomposition\/__init__.py":"MODIFY","sklearn\/utils\/tests\/test_deprecated_utils.py":"MODIFY","sklearn\/inspection\/__init__.py":"MODIFY","sklearn\/tests\/test_import_deprecations.py":"MODIFY"},"diff":{"sklearn\/decomposition\/__init__.py":[{"add":["6","# TODO: remove me in 0.24 (as well as the noqa markers) and","7","# import the dict_learning func directly from the ._dict_learning","8","# module instead.","9","# Pre-cache the import of the deprecated module so that import","10","# sklearn.decomposition.dict_learning returns the function as in","11","# 0.21, instead of the module.","12","# https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/15842","13","import warnings","14","with warnings.catch_warnings():","15","    warnings.simplefilter(\"ignore\", category=FutureWarning)","16","    from .dict_learning import dict_learning","17","","18","","19","from ._nmf import NMF, non_negative_factorization  # noqa","20","from ._pca import PCA  # noqa","21","from ._incremental_pca import IncrementalPCA  # noqa","22","from ._kernel_pca import KernelPCA  # noqa","23","from ._sparse_pca import SparsePCA, MiniBatchSparsePCA  # noqa","24","from ._truncated_svd import TruncatedSVD  # noqa","25","from ._fastica import FastICA, fastica  # noqa","26","from ._dict_learning import (dict_learning_online,","28","                             MiniBatchDictionaryLearning, SparseCoder)  # noqa","29","from ._factor_analysis import FactorAnalysis  # noqa","30","from ..utils.extmath import randomized_svd  # noqa","31","from ._lda import LatentDirichletAllocation  # noqa","32",""],"delete":["6","from ._nmf import NMF, non_negative_factorization","7","from ._pca import PCA","8","from ._incremental_pca import IncrementalPCA","9","from ._kernel_pca import KernelPCA","10","from ._sparse_pca import SparsePCA, MiniBatchSparsePCA","11","from ._truncated_svd import TruncatedSVD","12","from ._fastica import FastICA, fastica","13","from ._dict_learning import (dict_learning, dict_learning_online,","15","                             MiniBatchDictionaryLearning, SparseCoder)","16","from ._factor_analysis import FactorAnalysis","17","from ..utils.extmath import randomized_svd","18","from ._lda import LatentDirichletAllocation"]}],"sklearn\/utils\/tests\/test_deprecated_utils.py":[{"add":["1","import types","3","import warnings","6","from sklearn.utils import all_estimators","99","","100","","101","# TODO: remove in 0.24","102","def test_partial_dependence_no_shadowing():","103","    # Non-regression test for:","104","    # https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/15842","105","    with warnings.catch_warnings():","106","        warnings.simplefilter(\"ignore\", category=FutureWarning)","107","        from sklearn.inspection.partial_dependence import partial_dependence as _  # noqa","108","","109","        # Calling all_estimators() also triggers a recursive import of all","110","        # submodules, including deprecated ones.","111","        all_estimators()","112","","113","    from sklearn.inspection import partial_dependence","114","    assert isinstance(partial_dependence, types.FunctionType)","115","","116","","117","# TODO: remove in 0.24","118","def test_dict_learning_no_shadowing():","119","    # Non-regression test for:","120","    # https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/15842","121","    with warnings.catch_warnings():","122","        warnings.simplefilter(\"ignore\", category=FutureWarning)","123","        from sklearn.decomposition.dict_learning import dict_learning as _  # noqa","124","","125","        # Calling all_estimators() also triggers a recursive import of all","126","        # submodules, including deprecated ones.","127","        all_estimators()","128","","129","    from sklearn.decomposition import dict_learning","130","    assert isinstance(dict_learning, types.FunctionType)"],"delete":[]}],"sklearn\/inspection\/__init__.py":[{"add":["1","","2","# TODO: remove me in 0.24 (as well as the noqa markers) and","3","# import the partial_dependence func directly from the","4","# ._partial_dependence module instead.","5","# Pre-cache the import of the deprecated module so that import","6","# sklearn.inspection.partial_dependence returns the function as in","7","# 0.21, instead of the module","8","# https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/15842","9","import warnings","10","with warnings.catch_warnings():","11","    warnings.simplefilter(\"ignore\", category=FutureWarning)","12","    from .partial_dependence import partial_dependence","13","","14","from ._partial_dependence import plot_partial_dependence  # noqa","15","from ._partial_dependence import PartialDependenceDisplay  # noqa","16","from ._permutation_importance import permutation_importance  # noqa","17",""],"delete":["1","from ._partial_dependence import partial_dependence","2","from ._partial_dependence import plot_partial_dependence","3","from ._partial_dependence import PartialDependenceDisplay","4","from ._permutation_importance import permutation_importance"]}],"sklearn\/tests\/test_import_deprecations.py":[{"add":["26","    # Special case for:","27","    # https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/15842","28","    if deprecated_path in (\"sklearn.decomposition.dict_learning\",","29","                           \"sklearn.inspection.partial_dependence\"):","30","        pytest.skip(\"No warning can be raised for \" + deprecated_path)","31",""],"delete":[]}]}},"535ef5516bce75c6a51127da95dcb577af1fe35e":{"changes":{"sklearn\/decomposition\/tests\/test_kernel_pca.py":"MODIFY","sklearn\/decomposition\/_kernel_pca.py":"MODIFY","doc\/whats_new\/v0.23.rst":"MODIFY"},"diff":{"sklearn\/decomposition\/tests\/test_kernel_pca.py":[{"add":["9","from sklearn.datasets import make_blobs","285","","286","","287","@pytest.mark.parametrize(\"kernel\",","288","                         [\"linear\", \"poly\", \"rbf\", \"sigmoid\", \"cosine\"])","289","def test_kernel_pca_inverse_transform(kernel):","290","    X, *_ = make_blobs(n_samples=100, n_features=4, centers=[[1, 1, 1, 1]],","291","                       random_state=0)","292","","293","    kp = KernelPCA(n_components=2, kernel=kernel, fit_inverse_transform=True)","294","    X_trans = kp.fit_transform(X)","295","    X_inv = kp.inverse_transform(X_trans)","296","    assert_allclose(X, X_inv)"],"delete":[]}],"sklearn\/decomposition\/_kernel_pca.py":[{"add":["360","        n_samples = self.X_transformed_fit_.shape[0]","361","        K.flat[::n_samples + 1] += self.alpha"],"delete":["360",""]}],"doc\/whats_new\/v0.23.rst":[{"add":["144","- |Fix| :class:`decomposition.KernelPCA` method ``inverse_transform`` now","145","  applies the correct inverse transform to the transformed data. :pr:`16655`","146","  by :user:`Lewis Ball <lrjball>`.","147",""],"delete":[]}]}},"073de7c6e56eb7fc4f1a4fac5f66ef94fea58d78":{"changes":{"sklearn\/naive_bayes.py":"MODIFY","doc\/whats_new\/v0.23.rst":"MODIFY","sklearn\/tests\/test_naive_bayes.py":"MODIFY"},"diff":{"sklearn\/naive_bayes.py":[{"add":["1207","                             % (self.n_features_, X.shape[1]))"],"delete":["1207","                             .format(self.n_features_, X.shape[1]))"]}],"doc\/whats_new\/v0.23.rst":[{"add":["130",":mod:`sklearn.naive_bayes`","131",".............................","132","","133","- |Fix| A correctly formatted error message is shown in","134","  :class:`naive_bayes.CategoricalNB` when the number of features in the input","135","  differs between `predict` and `fit`.","136","  :pr:`16090` by :user:`Madhura Jayaratne <madhuracj>`.","137",""],"delete":[]}],"sklearn\/tests\/test_naive_bayes.py":[{"add":["669","    # Check error is raised for incorrect X","670","    X = np.array([[1, 4, 1], [2, 5, 6]])","671","    msg = \"Expected input with 2 features, got 3 instead\"","672","    assert_raise_message(ValueError, msg, clf.predict, X)","673",""],"delete":[]}]}},"82845abac64a2de9ef851d9838ab857d74dea8fd":{"changes":{"sklearn\/_build_utils\/__init__.py":"MODIFY"},"diff":{"sklearn\/_build_utils\/__init__.py":[{"add":["63","    # Lazy import because cython is not a runtime dependency.","64","    from Cython import Tempita"],"delete":["63","    is_release = os.path.exists(os.path.join(top_path, 'PKG-INFO'))","64","    # Files are already cythonized, nothing to do.","65","    if is_release:","66","        return","67","","68","    # Lazy import because cython is not a dependency when building from","69","    # source distribution.","70","    from Cython import Tempita # noqa"]}]}},"b4678ca8aa0687575e1e1d2809355aa6c6975b16":{"changes":{"sklearn\/metrics\/pairwise.py":"MODIFY","sklearn\/metrics\/tests\/test_pairwise.py":"MODIFY","doc\/whats_new\/v0.24.rst":"MODIFY"},"diff":{"sklearn\/metrics\/pairwise.py":[{"add":["31","from ..utils.fixes import sp_version, parse_version","1448","        # There is a bug in scipy < 1.5 that will cause a crash if","1449","        # X.dtype != np.double (float64). See PR #15730","1450","        dtype = np.float64 if sp_version < parse_version('1.5') else None","1452","            V = np.var(X, axis=0, ddof=1, dtype=dtype)","1457","            V = np.var(np.vstack([X, Y]), axis=0, ddof=1, dtype=dtype)"],"delete":["1448","            V = np.var(X, axis=0, ddof=1)","1453","            V = np.var(np.vstack([X, Y]), axis=0, ddof=1)"]}],"sklearn\/metrics\/tests\/test_pairwise.py":[{"add":["683","    # and fails due to rounding errors.","704","    # and fails due to rounding errors.","731","    # and fails due to rounding errors.","751","    # and fails due to rounding errors.","1296","","1297","","1298","@pytest.mark.parametrize(","1299","        'metric', [","1300","            'braycurtis', 'canberra', 'chebyshev',","1301","            'correlation', 'hamming', 'mahalanobis', 'minkowski', 'seuclidean',","1302","            'sqeuclidean', 'cityblock', 'cosine', 'euclidean'])","1303","@pytest.mark.parametrize(","1304","        \"dtype\",","1305","        [np.float32, np.float64])","1306","@pytest.mark.parametrize(\"y_is_x\", [True, False], ids=[\"Y is X\", \"Y is not X\"])","1307","def test_numeric_pairwise_distances_datatypes(metric, dtype, y_is_x):","1308","    # Check that pairwise distances gives the same result as pdist and cdist","1309","    # regardless of input datatype when using any scipy metric for comparing","1310","    # numeric vectors","1311","    #","1312","    # This test is necessary because pairwise_distances used to throw an","1313","    # error when using metric='seuclidean' and the input data was not","1314","    # of type np.float64 (#15730)","1315","","1316","    rng = np.random.RandomState(0)","1317","","1318","    X = rng.random_sample((5, 4)).astype(dtype)","1319","","1320","    params = {}","1321","    if y_is_x:","1322","        Y = X","1323","        expected_dist = squareform(pdist(X, metric=metric))","1324","    else:","1325","        Y = rng.random_sample((5, 4)).astype(dtype)","1326","        expected_dist = cdist(X, Y, metric=metric)","1327","        # precompute parameters for seuclidean & mahalanobis when x is not y","1328","        if metric == 'seuclidean':","1329","            params = {'V': np.var(np.vstack([X, Y]),","1330","                                  axis=0, ddof=1, dtype=np.float64)}","1331","        elif metric == 'mahalanobis':","1332","            params = {'VI': np.linalg.inv(np.cov(np.vstack([X, Y]).T)).T}","1333","","1334","    dist = pairwise_distances(X, Y, metric=metric, **params)","1335","","1336","    # the default rtol=1e-7 is too close to the float32 precision","1337","    # and fails due to rounding errors","1338","    rtol = 1e-5 if dtype is np.float32 else 1e-7","1339","    assert_allclose(dist, expected_dist, rtol=rtol)"],"delete":["683","    # and fails due too rounding errors.","704","    # and fails due too rounding errors.","731","    # and fails due too rounding errors.","751","    # and fails due too rounding errors."]}],"doc\/whats_new\/v0.24.rst":[{"add":["229","- |Fix| Fixed a bug where :func:`metrics.pairwise_distances` would raise an error if","230","  ``metric='seuclidean'`` and ``X`` is not type ``np.float64``.","231","  :pr:`15730` by :user:`Forrest Koch <ForrestCKoch>`.","232",""],"delete":[]}]}},"ccac637855411169a256eb095302cff446b342be":{"changes":{"sklearn\/metrics\/pairwise.py":"MODIFY","sklearn\/metrics\/tests\/test_pairwise.py":"MODIFY"},"diff":{"sklearn\/metrics\/pairwise.py":[{"add":["408","    np.clip(distances, 0, None, out=distances)","409",""],"delete":[]}],"sklearn\/metrics\/tests\/test_pairwise.py":[{"add":["873","@pytest.mark.parametrize(\"missing_value\", [np.nan, -1])","874","def test_nan_euclidean_distances_one_feature_match_positive(missing_value):","875","    # First feature is the only feature that is non-nan and in both","876","    # samples. The result of `nan_euclidean_distances` with squared=True","877","    # should be non-negative. The non-squared version should all be close to 0.","878","    X = np.array([[-122.27, 648., missing_value, 37.85],","879","                  [-122.27, missing_value, 2.34701493, missing_value]])","880","","881","    dist_squared = nan_euclidean_distances(X, missing_values=missing_value,","882","                                           squared=True)","883","    assert np.all(dist_squared >= 0)","884","","885","    dist = nan_euclidean_distances(X, missing_values=missing_value,","886","                                   squared=False)","887","    assert_allclose(dist, 0.0)","888","","889",""],"delete":[]}]}},"663d052d3c7da2630357a92c3f5c59128b51480e":{"changes":{"sklearn\/ensemble\/_forest.py":"MODIFY"},"diff":{"sklearn\/ensemble\/_forest.py":[{"add":["964","        Controls both the randomness of the bootstrapping of the samples used","965","        when building trees (if ``bootstrap=True``) and the sampling of the","966","        features to consider when looking for the best split at each node","967","        (if ``max_features < n_features``).","968","        See :term:`Glossary <random_state>` for details.","1281","        Controls both the randomness of the bootstrapping of the samples used","1282","        when building trees (if ``bootstrap=True``) and the sampling of the","1283","        features to consider when looking for the best split at each node","1284","        (if ``max_features < n_features``).","1285","        See :term:`Glossary <random_state>` for details.","1544","        whole dataset is used to build each tree.","1558","        Controls 3 sources of randomness:","1559","","1560","        - the bootstrapping of the samples used when building trees","1561","          (if ``bootstrap=True``)","1562","        - the sampling of the features to consider when looking for the best","1563","          split at each node (if ``max_features < n_features``)","1564","        - the draw of the splits for each of the `max_features`","1565","        See :term:`Glossary <random_state>` for details.","1853","        whole dataset is used to build each tree.","1866","        Controls 3 sources of randomness:","1867","","1868","        - the bootstrapping of the samples used when building trees","1869","          (if ``bootstrap=True``)","1870","        - the sampling of the features to consider when looking for the best","1871","          split at each node (if ``max_features < n_features``)","1872","        - the draw of the splits for each of the `max_features`","1873","        See :term:`Glossary <random_state>` for details.","2100","        Controls the generation of the random `y` used to fit the trees","2101","        and the draw of the splits for each feature at the trees' nodes.","2102","        See :term:`Glossary <random_state>` for details."],"delete":["964","        If int, random_state is the seed used by the random number generator;","965","        If RandomState instance, random_state is the random number generator;","966","        If None, the random number generator is the RandomState instance used","967","        by `np.random`.","1280","        If int, random_state is the seed used by the random number generator;","1281","        If RandomState instance, random_state is the random number generator;","1282","        If None, the random number generator is the RandomState instance used","1283","        by `np.random`.","1542","        whole datset is used to build each tree.","1556","        If int, random_state is the seed used by the random number generator;","1557","        If RandomState instance, random_state is the random number generator;","1558","        If None, the random number generator is the RandomState instance used","1559","        by `np.random`.","1847","        whole datset is used to build each tree.","1860","        If int, random_state is the seed used by the random number generator;","1861","        If RandomState instance, random_state is the random number generator;","1862","        If None, the random number generator is the RandomState instance used","1863","        by `np.random`.","2090","        If int, random_state is the seed used by the random number generator;","2091","        If RandomState instance, random_state is the random number generator;","2092","        If None, the random number generator is the RandomState instance used","2093","        by `np.random`."]}]}},"ac7081c510fc2c27b1bef002dfefc9f3854e2c9a":{"changes":{"sklearn\/metrics\/_plot\/roc_curve.py":"MODIFY","sklearn\/metrics\/_plot\/tests\/test_plot_roc_curve.py":"MODIFY","sklearn\/metrics\/_plot\/precision_recall_curve.py":"MODIFY","sklearn\/metrics\/_plot\/tests\/test_plot_precision_recall.py":"MODIFY"},"diff":{"sklearn\/metrics\/_plot\/roc_curve.py":[{"add":["169","    if not is_classifier(estimator):","177","        if y_pred.shape[1] != 2:","178","            raise ValueError(classification_error)","179","        else:","180","            y_pred = y_pred[:, 1]","182","    pos_label = estimator.classes_[1]"],"delete":["6","from ...utils.validation import check_is_fitted","167","    check_is_fitted(estimator)","171","","172","    if is_classifier(estimator):","173","        if len(estimator.classes_) != 2:","174","            raise ValueError(classification_error)","175","        pos_label = estimator.classes_[1]","176","    else:","181","","185","        y_pred = y_pred[:, 1]"]}],"sklearn\/metrics\/_plot\/tests\/test_plot_roc_curve.py":[{"add":["10","from sklearn.exceptions import NotFittedError","11","from sklearn.pipeline import make_pipeline","12","from sklearn.preprocessing import StandardScaler","13","from sklearn.compose import make_column_transformer","114","","115","","116","@pytest.mark.parametrize(","117","    \"clf\", [LogisticRegression(),","118","            make_pipeline(StandardScaler(), LogisticRegression()),","119","            make_pipeline(make_column_transformer((StandardScaler(), [0, 1])),","120","                          LogisticRegression())])","121","def test_roc_curve_not_fitted_errors(pyplot, data_binary, clf):","122","    X, y = data_binary","123","    with pytest.raises(NotFittedError):","124","        plot_roc_curve(clf, X, y)","125","    clf.fit(X, y)","126","    disp = plot_roc_curve(clf, X, y)","127","    assert disp.estimator_name == clf.__class__.__name__"],"delete":[]}],"sklearn\/metrics\/_plot\/precision_recall_curve.py":[{"add":["143","    classification_error = (\"{} should be a binary classifer\".format(","145","    if not is_classifier(estimator):","146","        raise ValueError(classification_error)","153","        if y_pred.shape[1] != 2:","154","            raise ValueError(classification_error)","155","        else:","156","            y_pred = y_pred[:, 1]","158","    pos_label = estimator.classes_[1]"],"delete":["6","from ...utils.validation import check_is_fitted","143","    check_is_fitted(estimator)","145","    classificaiton_error = (\"{} should be a binary classifer\".format(","147","    if is_classifier(estimator):","148","        if len(estimator.classes_) != 2:","149","            raise ValueError(classificaiton_error)","150","        pos_label = estimator.classes_[1]","151","    else:","152","        raise ValueError(classificaiton_error)","159","        y_pred = y_pred[:, 1]"]}],"sklearn\/metrics\/_plot\/tests\/test_plot_precision_recall.py":[{"add":["12","from sklearn.pipeline import make_pipeline","13","from sklearn.preprocessing import StandardScaler","14","from sklearn.compose import make_column_transformer","115","","116","","117","@pytest.mark.parametrize(","118","    \"clf\", [make_pipeline(StandardScaler(), LogisticRegression()),","119","            make_pipeline(make_column_transformer((StandardScaler(), [0, 1])),","120","                          LogisticRegression())])","121","def test_precision_recall_curve_pipeline(pyplot, clf):","122","    X, y = make_classification(n_classes=2, n_samples=50, random_state=0)","123","    with pytest.raises(NotFittedError):","124","        plot_precision_recall_curve(clf, X, y)","125","    clf.fit(X, y)","126","    disp = plot_precision_recall_curve(clf, X, y)","127","    assert disp.estimator_name == clf.__class__.__name__"],"delete":[]}]}},"3109add033757aa8c69cfa2167316ca236b15489":{"changes":{"sklearn\/inspection\/_partial_dependence.py":"MODIFY"},"diff":{"sklearn\/inspection\/_partial_dependence.py":[{"add":["593","    if (is_classifier(estimator) and","594","            hasattr(estimator, 'classes_') and","595","            np.size(estimator.classes_) > 2):"],"delete":["27","from ..tree._tree import DTYPE","594","    if hasattr(estimator, 'classes_') and np.size(estimator.classes_) > 2:"]}]}},"cdbda0eddba48fc6b6b1822ec272cccc656a0329":{"changes":{"sklearn\/ensemble\/_hist_gradient_boosting\/gradient_boosting.py":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_gradient_boosting.py":"MODIFY"},"diff":{"sklearn\/ensemble\/_hist_gradient_boosting\/gradient_boosting.py":[{"add":["306","            else:","307","                raw_predictions_val = None"],"delete":[]}],"sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_gradient_boosting.py":[{"add":["647","","648","","649","def test_early_stopping_on_test_set_with_warm_start():","650","    # Non regression test for #16661 where second fit fails with","651","    # warm_start=True, early_stopping is on, and no validation set","652","    X, y = make_classification(random_state=0)","653","    gb = HistGradientBoostingClassifier(","654","        max_iter=1, scoring='loss', warm_start=True, early_stopping=True,","655","        n_iter_no_change=1, validation_fraction=None)","656","","657","    gb.fit(X, y)","658","    # does not raise on second call","659","    gb.set_params(max_iter=2)","660","    gb.fit(X, y)"],"delete":[]}]}},"75d3f29e724c9a2db7bcb5afb2d7e8c08ddbd80b":{"changes":{"sklearn\/utils\/fixes.py":"MODIFY","sklearn\/decomposition\/_lda.py":"MODIFY","sklearn\/naive_bayes.py":"MODIFY","sklearn\/utils\/tests\/test_fixes.py":"MODIFY","sklearn\/impute\/_iterative.py":"MODIFY","sklearn\/model_selection\/tests\/test_split.py":"MODIFY","sklearn\/preprocessing\/_encoders.py":"MODIFY","sklearn\/ensemble\/tests\/test_forest.py":"MODIFY","sklearn\/ensemble\/_gb_losses.py":"MODIFY","sklearn\/linear_model\/_logistic.py":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/loss.py":"MODIFY","sklearn\/model_selection\/_split.py":"MODIFY","sklearn\/metrics\/cluster\/_supervised.py":"MODIFY","sklearn\/mixture\/_base.py":"MODIFY","sklearn\/utils\/tests\/test_random.py":"MODIFY","sklearn\/linear_model\/tests\/test_sag.py":"MODIFY","sklearn\/model_selection\/_search.py":"MODIFY"},"diff":{"sklearn\/utils\/fixes.py":[{"add":[],"delete":["36","try:  # SciPy >= 0.19","37","    from scipy.special import comb, logsumexp","38","except ImportError:","39","    from scipy.misc import comb, logsumexp  # noqa","40","","55","if sp_version >= (0, 19):","56","    def _argmax(arr_or_spmatrix, axis=None):","57","        return arr_or_spmatrix.argmax(axis=axis)","58","else:","59","    # Backport of argmax functionality from scipy 0.19.1, can be removed","60","    # once support for scipy 0.18 and below is dropped","61","","62","    def _find_missing_index(ind, n):","63","        for k, a in enumerate(ind):","64","            if k != a:","65","                return k","66","","67","        k += 1","68","        if k < n:","69","            return k","70","        else:","71","            return -1","72","","73","    def _arg_min_or_max_axis(self, axis, op, compare):","74","        if self.shape[axis] == 0:","75","            raise ValueError(\"Can't apply the operation along a zero-sized \"","76","                             \"dimension.\")","77","","78","        if axis < 0:","79","            axis += 2","80","","81","        zero = self.dtype.type(0)","82","","83","        mat = self.tocsc() if axis == 0 else self.tocsr()","84","        mat.sum_duplicates()","85","","86","        ret_size, line_size = mat._swap(mat.shape)","87","        ret = np.zeros(ret_size, dtype=int)","88","","89","        nz_lines, = np.nonzero(np.diff(mat.indptr))","90","        for i in nz_lines:","91","            p, q = mat.indptr[i:i + 2]","92","            data = mat.data[p:q]","93","            indices = mat.indices[p:q]","94","            am = op(data)","95","            m = data[am]","96","            if compare(m, zero) or q - p == line_size:","97","                ret[i] = indices[am]","98","            else:","99","                zero_ind = _find_missing_index(indices, line_size)","100","                if m == zero:","101","                    ret[i] = min(am, zero_ind)","102","                else:","103","                    ret[i] = zero_ind","104","","105","        if axis == 1:","106","            ret = ret.reshape(-1, 1)","107","","108","        return np.asmatrix(ret)","109","","110","    def _arg_min_or_max(self, axis, out, op, compare):","111","        if out is not None:","112","            raise ValueError(\"Sparse matrices do not support \"","113","                             \"an 'out' parameter.\")","114","","115","        # validateaxis(axis)","116","","117","        if axis is None:","118","            if 0 in self.shape:","119","                raise ValueError(\"Can't apply the operation to \"","120","                                 \"an empty matrix.\")","121","","122","            if self.nnz == 0:","123","                return 0","124","            else:","125","                zero = self.dtype.type(0)","126","                mat = self.tocoo()","127","                mat.sum_duplicates()","128","                am = op(mat.data)","129","                m = mat.data[am]","130","","131","                if compare(m, zero):","132","                    return mat.row[am] * mat.shape[1] + mat.col[am]","133","                else:","134","                    size = np.product(mat.shape)","135","                    if size == mat.nnz:","136","                        return am","137","                    else:","138","                        ind = mat.row * mat.shape[1] + mat.col","139","                        zero_ind = _find_missing_index(ind, size)","140","                        if m == zero:","141","                            return min(zero_ind, am)","142","                        else:","143","                            return zero_ind","144","","145","        return _arg_min_or_max_axis(self, axis, op, compare)","146","","147","    def _sparse_argmax(self, axis=None, out=None):","148","        return _arg_min_or_max(self, axis, out, np.argmax, np.greater)","149","","150","    def _argmax(arr_or_matrix, axis=None):","151","        if sp.issparse(arr_or_matrix):","152","            return _sparse_argmax(arr_or_matrix, axis=axis)","153","        else:","154","            return arr_or_matrix.argmax(axis=axis)","155","","156","","157","if np_version < (1, 12):","158","    class MaskedArray(np.ma.MaskedArray):","159","        # Before numpy 1.12, np.ma.MaskedArray object is not picklable","160","        # This fix is needed to make our model_selection.GridSearchCV","161","        # picklable as the ``cv_results_`` param uses MaskedArray","162","        def __getstate__(self):","163","            \"\"\"Return the internal state of the masked array, for pickling","164","            purposes.","165","","166","            \"\"\"","167","            cf = 'CF'[self.flags.fnc]","168","            data_state = super(np.ma.MaskedArray, self).__reduce__()[2]","169","            return data_state + (np.ma.getmaskarray(self).tostring(cf),","170","                                 self._fill_value)","171","else:","172","    from numpy.ma import MaskedArray    # noqa","173",""]}],"sklearn\/decomposition\/_lda.py":[{"add":["15","from scipy.special import gammaln, logsumexp"],"delete":["15","from scipy.special import gammaln","20","from ..utils.fixes import logsumexp"]}],"sklearn\/naive_bayes.py":[{"add":["23","from scipy.special import logsumexp"],"delete":["30","from .utils.fixes import logsumexp"]}],"sklearn\/utils\/tests\/test_fixes.py":[{"add":[],"delete":["6","import pickle","14","from sklearn.utils.fixes import MaskedArray","20","def test_masked_array_obj_dtype_pickleable():","21","    marr = MaskedArray([1, None, 'a'], dtype=object)","22","","23","    for mask in (True, False, [0, 1, 0]):","24","        marr.mask = mask","25","        marr_pickled = pickle.loads(pickle.dumps(marr))","26","        assert_array_equal(marr.data, marr_pickled.data)","27","        assert_array_equal(marr.mask, marr_pickled.mask)","28","","29",""]}],"sklearn\/impute\/_iterative.py":[{"add":["329","            truncated_normal = stats.truncnorm(a=a, b=b,","330","                                               loc=mus, scale=sigmas)","331","            imputed_values[inrange_mask] = truncated_normal.rvs(","332","                random_state=self.random_state_)"],"delete":["2","from distutils.version import LooseVersion","6","import scipy","331","            if scipy.__version__ < LooseVersion('0.18'):","332","                # bug with vector-valued `a` in old scipy","333","                imputed_values[inrange_mask] = [","334","                    stats.truncnorm(a=a_, b=b_,","335","                                    loc=loc_, scale=scale_).rvs(","336","                                        random_state=self.random_state_)","337","                    for a_, b_, loc_, scale_","338","                    in zip(a, b, mus, sigmas)]","339","            else:","340","                truncated_normal = stats.truncnorm(a=a, b=b,","341","                                                   loc=mus, scale=sigmas)","342","                imputed_values[inrange_mask] = truncated_normal.rvs(","343","                    random_state=self.random_state_)"]}],"sklearn\/model_selection\/tests\/test_split.py":[{"add":["6","from scipy.special import comb"],"delete":["48","from sklearn.utils.fixes import comb","49",""]}],"sklearn\/preprocessing\/_encoders.py":[{"add":["528","            labels = np.asarray(sub.argmax(axis=1)).flatten()"],"delete":["9","from ..utils.fixes import _argmax","529","            labels = np.asarray(_argmax(sub, axis=1)).flatten()"]}],"sklearn\/ensemble\/tests\/test_forest.py":[{"add":["22","from scipy.special import comb"],"delete":["49","from sklearn.utils.fixes import comb"]}],"sklearn\/ensemble\/_gb_losses.py":[{"add":["8","from scipy.special import expit, logsumexp"],"delete":["8","from scipy.special import expit","11","from ..utils.fixes import logsumexp"]}],"sklearn\/linear_model\/_logistic.py":[{"add":["17","from scipy.special import expit, logsumexp"],"delete":["17","from scipy.special import expit","29","from ..utils.fixes import logsumexp"]}],"sklearn\/ensemble\/_hist_gradient_boosting\/loss.py":[{"add":["11","from scipy.special import expit, logsumexp"],"delete":["11","from scipy.special import expit","12","try:  # logsumexp was moved from mist to special in 0.19","13","    from scipy.special import logsumexp","14","except ImportError:","15","    from scipy.misc import logsumexp"]}],"sklearn\/model_selection\/_split.py":[{"add":["20","from scipy.special import comb"],"delete":["26","from ..utils.fixes import comb"]}],"sklearn\/metrics\/cluster\/_supervised.py":[{"add":["21","from scipy.special import comb","25","from ...utils.fixes import _astype_copy_false"],"delete":["24","from ...utils.fixes import comb, _astype_copy_false"]}],"sklearn\/mixture\/_base.py":[{"add":["11","from scipy.special import logsumexp"],"delete":["18","from ..utils.fixes import logsumexp"]}],"sklearn\/utils\/tests\/test_random.py":[{"add":["3","from scipy.special import comb"],"delete":["5","from sklearn.utils.fixes import comb"]}],"sklearn\/linear_model\/tests\/test_sag.py":[{"add":["9","from scipy.special import logsumexp"],"delete":["16","from sklearn.utils.fixes import logsumexp"]}],"sklearn\/model_selection\/_search.py":[{"add":["23","from numpy.ma import MaskedArray"],"delete":["33","from ..utils.fixes import MaskedArray"]}]}},"06c71ec8b9c5984df5778a5694c4b17b44ed72ea":{"changes":{"doc\/whats_new\/v0.23.rst":"MODIFY","sklearn\/cluster\/_agglomerative.py":"MODIFY","sklearn\/cluster\/tests\/test_hierarchical.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.23.rst":[{"add":["297","","298",":mod:`sklearn.cluster`","299","......................","300","","301","- |Fix| :class:`cluster.AgglomerativeClustering` add specific error when ","302","  distance matrix is not square and `affinity=precomputed`. ","303","  :pr:`16257` by :user:`Simona Maggio <simonamaggio>`."],"delete":[]}],"sklearn\/cluster\/_agglomerative.py":[{"add":["455","            # by sklearn.metrics.pairwise_distances.","456","            if X.shape[0] != X.shape[1]:","457","                raise ValueError(","458","                    'Distance matrix should be square, '","459","                    'Got matrix of shape {X.shape}'","460","                )"],"delete":["455","            # by pdist: it is a flat array containing the upper triangular of","456","            # the distance matrix."]}],"sklearn\/cluster\/tests\/test_hierarchical.py":[{"add":["753","","754","","755","def test_invalid_shape_precomputed_dist_matrix():","756","    # Check that an error is raised when affinity='precomputed'","757","    # and a non square matrix is passed (PR #16257).","758","    rng = np.random.RandomState(0)","759","    X = rng.rand(5, 3)","760","    with pytest.raises(ValueError, match=\"Distance matrix should be square, \"):","761","        AgglomerativeClustering(affinity='precomputed',","762","                                linkage='complete').fit(X)"],"delete":[]}]}},"3743a55aed5c9ee18c8b94b95dfc4c41d0ae99f5":{"changes":{"doc\/whats_new\/v0.23.rst":"MODIFY","sklearn\/utils\/optimize.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.23.rst":[{"add":["143","- |Fix| :class:`linear_model.LogisticRegression` will now avoid an unnecessary","144","  iteration when `solver='newton-cg'` by checking for inferior or equal instead","145","  of strictly inferior for maximum of `absgrad` and `tol` in `utils.optimize._newton_cg`.","146","  :pr:`16266` by :user:`Rushabh Vasani <rushabh-v>`.","147","","210",""],"delete":[]}],"sklearn\/utils\/optimize.py":[{"add":["184","        if np.max(absgrad) <= tol:"],"delete":["184","        if np.max(absgrad) < tol:"]}]}}}