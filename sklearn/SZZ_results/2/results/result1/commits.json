{"dbc35934a6ebf7d0bbfdfc80e8bf8a9fabc1ba4b":{"changes":{"sklearn\/model_selection\/_split.py":"MODIFY","sklearn\/metrics\/cluster\/_supervised.py":"MODIFY","sklearn\/multioutput.py":"MODIFY","sklearn\/metrics\/_classification.py":"MODIFY","sklearn\/cluster\/_kmeans.py":"MODIFY","sklearn\/feature_selection\/_rfe.py":"MODIFY","sklearn\/ensemble\/_voting.py":"MODIFY","sklearn\/linear_model\/_ransac.py":"MODIFY","sklearn\/svm\/_classes.py":"MODIFY","sklearn\/feature_selection\/_univariate_selection.py":"MODIFY","sklearn\/preprocessing\/_function_transformer.py":"MODIFY"},"diff":{"sklearn\/model_selection\/_split.py":[{"add":["741","    .. versionadded:: 0.18","742",""],"delete":[]}],"sklearn\/metrics\/cluster\/_supervised.py":[{"add":["883","    .. versionadded:: 0.18","884",""],"delete":[]}],"sklearn\/multioutput.py":[{"add":["217","    .. versionadded:: 0.18","218",""],"delete":[]}],"sklearn\/metrics\/_classification.py":[{"add":["229","        .. versionadded:: 0.18","230","","793","        .. versionadded:: 0.18","794","","2162",""],"delete":[]}],"sklearn\/cluster\/_kmeans.py":[{"add":["836","        .. versionchanged:: 0.18","837","            Added Elkan algorithm","838",""],"delete":[]}],"sklearn\/feature_selection\/_rfe.py":[{"add":["412","        .. versionadded:: 0.18","413",""],"delete":[]}],"sklearn\/ensemble\/_voting.py":[{"add":["143","        .. versionadded:: 0.18","144","","236","            .. versionadded:: 0.18","237",""],"delete":[]}],"sklearn\/linear_model\/_ransac.py":[{"add":["152","        .. versionadded:: 0.18","153","","243","            .. versionadded:: 0.18","244",""],"delete":[]}],"sklearn\/svm\/_classes.py":[{"add":["215","            .. versionadded:: 0.18","216","","402","            .. versionadded:: 0.18","403",""],"delete":[]}],"sklearn\/feature_selection\/_univariate_selection.py":[{"add":["386","        .. versionadded:: 0.18","387","","471","        .. versionadded:: 0.18","472",""],"delete":[]}],"sklearn\/preprocessing\/_function_transformer.py":[{"add":["68","        .. versionadded:: 0.18","69","","73","        .. versionadded:: 0.18","74",""],"delete":[]}]}},"d4e082679d970e6b30dfe9526d9b18c745f88a81":{"changes":{"sklearn\/metrics\/cluster\/_supervised.py":"MODIFY","sklearn\/metrics\/cluster\/tests\/test_common.py":"MODIFY"},"diff":{"sklearn\/metrics\/cluster\/_supervised.py":[{"add":["45","        labels_true, ensure_2d=False, ensure_min_samples=0, dtype=None,","48","        labels_pred, ensure_2d=False, ensure_min_samples=0, dtype=None,"],"delete":["45","        labels_true, ensure_2d=False, ensure_min_samples=0","48","        labels_pred, ensure_2d=False, ensure_min_samples=0"]}],"sklearn\/metrics\/cluster\/tests\/test_common.py":[{"add":["163","        yield [str(x) + \"-a\" for x in y.tolist()], 'list of strs'","164","        yield (np.array([str(x) + \"-a\" for x in y.tolist()], dtype=object),","165","               'array of strs')"],"delete":["163","        yield [str(x) for x in y.tolist()], 'list of strs'"]}]}},"913da3f286649af6378204d824827bfc4681fe20":{"changes":{"benchmarks\/bench_hist_gradient_boosting_higgsboson.py":"MODIFY"},"diff":{"benchmarks\/bench_hist_gradient_boosting_higgsboson.py":[{"add":["77","                                     early_stopping=False,"],"delete":["77","                                     n_iter_no_change=None,"]}]}},"556eb9246d1290ea248f2e3b3581dc53e75b8061":{"changes":{"sklearn\/inspection\/tests\/test_plot_partial_dependence.py":"MODIFY","sklearn\/inspection\/_partial_dependence.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY"},"diff":{"sklearn\/inspection\/tests\/test_plot_partial_dependence.py":[{"add":["224","@pytest.mark.parametrize(\"nrows, ncols\", [(2, 2), (3, 1)])","226","                                                    boston, nrows, ncols):","227","    grid_resolution = 5","228","    fig, axes = pyplot.subplots(nrows, ncols)","229","    axes_formats = [list(axes.ravel()), tuple(axes.ravel()), axes]","231","    msg = \"Expected ax to have 2 axes, got {}\".format(nrows * ncols)","234","                                   ['CRIM', 'ZN'],","238","    for ax_format in axes_formats:","239","        with pytest.raises(ValueError, match=msg):","240","            plot_partial_dependence(clf_boston, boston.data,","241","                                    ['CRIM', 'ZN'],","242","                                    grid_resolution=grid_resolution,","243","                                    feature_names=boston.feature_names,","244","                                    ax=ax_format)","245","","246","        # with axes object","247","        with pytest.raises(ValueError, match=msg):","248","            disp.plot(ax=ax_format)"],"delete":["225","                                                    boston):","226","    grid_resolution = 25","227","    fig, (ax1, ax2, ax3) = pyplot.subplots(1, 3)","229","    msg = r\"Expected len\\(ax\\) == len\\(features\\), got len\\(ax\\) = 3\"","230","    with pytest.raises(ValueError, match=msg):","231","        plot_partial_dependence(clf_boston, boston.data,","232","                                ['CRIM', ('CRIM', 'ZN')],","233","                                grid_resolution=grid_resolution,","234","                                feature_names=boston.feature_names,","235","                                ax=[ax1, ax2, ax3])","238","                                   ['CRIM', ('CRIM', 'ZN')],","242","    with pytest.raises(ValueError, match=msg):","243","        disp.plot(ax=[ax1, ax2, ax3])"]}],"sklearn\/inspection\/_partial_dependence.py":[{"add":["657","    # Early exit if the axes does not have the correct number of axes","658","    if ax is not None and not isinstance(ax, plt.Axes):","659","        axes = np.asarray(ax, dtype=object)","660","        if axes.size != len(features):","661","            raise ValueError(\"Expected ax to have {} axes, got {}\".format(","662","                             len(features), axes.size))","890","            ax = np.asarray(ax, dtype=object)","891","            if ax.size != n_features:","892","                raise ValueError(\"Expected ax to have {} axes, got {}\"","893","                                 .format(n_features, ax.size))"],"delete":["657","    if isinstance(ax, list):","658","        if len(ax) != len(features):","659","            raise ValueError(\"Expected len(ax) == len(features), \"","660","                             \"got len(ax) = {}\".format(len(ax)))","888","            ax = check_array(ax, dtype=object, ensure_2d=False)","895","            if ax.ndim == 1 and ax.shape[0] != n_features:","896","                raise ValueError(\"Expected len(ax) == len(features), \"","897","                                 \"got len(ax) = {}\".format(len(ax)))"]}],"doc\/whats_new\/v0.22.rst":[{"add":["30",":mod:`sklearn.inspection`","31",".........................","32","","33","- |Fix| :func:`inspection.plot_partial_dependence` and ","34","  :meth:`inspection.PartialDependenceDisplay.plot` now consistently checks","35","  the number of axes passed in. :pr:`15760` by `Thomas Fan`_.","36",""],"delete":[]}]}},"86f47da52954cb68b5eb17ffcc909194e31bbaee":{"changes":{"examples\/neighbors\/approximate_nearest_neighbors.py":"MODIFY"},"diff":{"examples\/neighbors\/approximate_nearest_neighbors.py":[{"add":["211","    mnist = fetch_openml(\"mnist_784\")","212","    X, y = shuffle(mnist.data, mnist.target, random_state=2)","213","    return X[:n_samples] \/ 255, y[:n_samples]","280","                axes[i_ax].scatter(Xt[:, 0], Xt[:, 1], c=y.astype(np.int32),","281","                                   alpha=0.2, cmap=plt.cm.viridis)"],"delete":["211","    mnist = fetch_openml(data_id=41063)","212","    X, y = shuffle(mnist.data, mnist.target, random_state=42)","213","    return X[:n_samples], y[:n_samples]","280","                axes[i_ax].scatter(Xt[:, 0], Xt[:, 1], c=y, alpha=0.2,","281","                                   cmap=plt.cm.viridis)"]}]}},"0db80b90c7f46d8c0a0a377030a110e2d1be7937":{"changes":{"doc\/whats_new\/v0.23.rst":"MODIFY","sklearn\/tree\/_tree.pyx":"MODIFY","sklearn\/tree\/tests\/test_tree.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.23.rst":[{"add":["219","- |Fix| Fix support of read-only float32 array input in ``predict``,","220","  ``decision_path`` and ``predict_proba`` methods of","221","  :class:`tree.DecisionTreeClassifier`, :class:`tree.ExtraTreeClassifier` and","222","  :class:`ensemble.GradientBoostingClassifier` as well as ``predict`` method of","223","  :class:`tree.DecisionTreeRegressor`, :class:`tree.ExtraTreeRegressor`, and","224","  :class:`ensemble.GradientBoostingRegressor`.","225","  :pr:`16331` by :user:`Alexandre Batisse <batalex>`.","226",""],"delete":[]}],"sklearn\/tree\/_tree.pyx":[{"add":["794","        cdef const DTYPE_t[:, :] X_ndarray = X","914","        cdef const DTYPE_t[:, :] X_ndarray = X"],"delete":["794","        cdef DTYPE_t[:, :] X_ndarray = X","914","        cdef DTYPE_t[:, :] X_ndarray = X"]}],"sklearn\/tree\/tests\/test_tree.py":[{"add":["25","from sklearn.utils._testing import create_memmap_backed_data","1957","","1958","","1959","def check_apply_path_readonly(name):","1960","    X_readonly = create_memmap_backed_data(X_small.astype(tree._tree.DTYPE,","1961","                                                          copy=False))","1962","    y_readonly = create_memmap_backed_data(np.array(y_small,","1963","                                                    dtype=tree._tree.DTYPE))","1964","    est = ALL_TREES[name]()","1965","    est.fit(X_readonly, y_readonly)","1966","    assert_array_equal(est.predict(X_readonly),","1967","                       est.predict(X_small))","1968","    assert_array_equal(est.decision_path(X_readonly).todense(),","1969","                       est.decision_path(X_small).todense())","1970","","1971","","1972","@pytest.mark.parametrize(\"name\", ALL_TREES)","1973","def test_apply_path_readonly_all_trees(name):","1974","    check_apply_path_readonly(name)"],"delete":["1825","def test_decision_tree_memmap():","1826","    # check that decision trees supports read-only buffer (#13626)","1827","    X = np.random.RandomState(0).random_sample((10, 2)).astype(np.float32)","1828","    y = np.zeros(10)","1829","","1830","    with TempMemmap((X, y)) as (X_read_only, y_read_only):","1831","        DecisionTreeClassifier().fit(X_read_only, y_read_only)","1832","","1833",""]}]}},"ea3181879c43f7423c32007111285d1f725a578b":{"changes":{"doc\/whats_new\/v0.23.rst":"MODIFY","sklearn\/decomposition\/_pca.py":"MODIFY","sklearn\/decomposition\/tests\/test_pca.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.23.rst":[{"add":["135","- |Fix| :func:`decomposition._pca._assess_dimension` now correctly handles small","136","   eigenvalues. :pr: `4441` by :user:`Lisa Schwetlick <lschwetlick>`, and","137","   :user:`Gelavizh Ahmadi <gelavizh1>` and","138","   :user:`Marija Vlajic Wheeler <marijavlajic>`."],"delete":[]}],"sklearn\/decomposition\/_pca.py":[{"add":["29","def _assess_dimension(spectrum, rank, n_samples, n_features):","60","    spectrum_threshold = np.finfo(type(spectrum[0])).eps","61","","71","        # TODO: this line is never executed because _infer_dimension's","72","        # for loop is off by one","77","        if spectrum_threshold > v:","78","            return -np.inf","88","        if spectrum_[i] < spectrum_threshold:","89","            # TODO: this line is never executed","90","            # (off by one in _infer_dimension)","91","            # this break only happens when rank == n_features and","92","            # spectrum_[i] < spectrum_threshold, otherwise the early return","93","            # above catches this case.","94","            break","104","def _infer_dimension(spectrum, n_samples, n_features):","112","        ll[rank] = _assess_dimension(spectrum, rank, n_samples, n_features)","473","                _infer_dimension(explained_variance_, n_samples, n_features)"],"delete":["29","def _assess_dimension_(spectrum, rank, n_samples, n_features):","91","def _infer_dimension_(spectrum, n_samples, n_features):","99","        ll[rank] = _assess_dimension_(spectrum, rank, n_samples, n_features)","460","                _infer_dimension_(explained_variance_, n_samples, n_features)"]}],"sklearn\/decomposition\/tests\/test_pca.py":[{"add":["10","from sklearn.decomposition._pca import _assess_dimension","11","from sklearn.decomposition._pca import _infer_dimension","335","    ll = np.array([_assess_dimension(spect, k, n, p) for k in range(p)])","350","    assert _infer_dimension(spect, n, p) > 1","363","    assert _infer_dimension(spect, n, p) > 2","570","","571","","572","def test_infer_dim_bad_spec():","573","    # Test a spectrum that drops to near zero for PR #16224","574","    spectrum = np.array([1, 1e-30, 1e-30, 1e-30])","575","    n_samples = 10","576","    n_features = 5","577","    ret = _infer_dimension(spectrum, n_samples, n_features)","578","    assert ret == 0","579","","580","","581","def test_assess_dimension_error_rank_greater_than_features():","582","    # Test error when tested rank is greater than the number of features","583","    # for PR #16224","584","    spectrum = np.array([1, 1e-30, 1e-30, 1e-30])","585","    n_samples = 10","586","    n_features = 4","587","    rank = 5","588","    with pytest.raises(ValueError, match=\"The tested rank cannot exceed \"","589","                                         \"the rank of the dataset\"):","590","        _assess_dimension(spectrum, rank, n_samples, n_features)","591","","592","","593","def test_assess_dimension_small_eigenvalues():","594","    # Test tiny eigenvalues appropriately when using 'mle'","595","    # for  PR #16224","596","    spectrum = np.array([1, 1e-30, 1e-30, 1e-30])","597","    n_samples = 10","598","    n_features = 5","599","    rank = 3","600","    ret = _assess_dimension(spectrum, rank, n_samples, n_features)","601","    assert ret == -np.inf","602","","603","","604","def test_infer_dim_mle():","605","    # Test small eigenvalues when 'mle' with pathological 'X' dataset","606","    # for PR #16224","607","    X, _ = datasets.make_classification(n_informative=1, n_repeated=18,","608","                                        n_redundant=1, n_clusters_per_class=1,","609","                                        random_state=42)","610","    pca = PCA(n_components='mle').fit(X)","611","    assert pca.n_components_ == 0","612","","613","","614","def test_fit_mle_too_few_samples():","615","    # Tests that an error is raised when the number of samples is smaller","616","    # than the number of features during an mle fit for PR #16224","617","    X, _ = datasets.make_classification(n_samples=20, n_features=21,","618","                                        random_state=42)","619","","620","    pca = PCA(n_components='mle', svd_solver='full')","621","    with pytest.raises(ValueError, match=\"n_components='mle' is only \"","622","                                         \"supported if \"","623","                                         \"n_samples >= n_features\"):","624","        pca.fit(X)"],"delete":["10","from sklearn.decomposition._pca import _assess_dimension_","11","from sklearn.decomposition._pca import _infer_dimension_","335","    ll = np.array([_assess_dimension_(spect, k, n, p) for k in range(p)])","350","    assert _infer_dimension_(spect, n, p) > 1","363","    assert _infer_dimension_(spect, n, p) > 2"]}]}},"9c62eee695cdcd75c6fd23d02334b8d0c241ddde":{"changes":{"sklearn\/tree\/tests\/test_export.py":"MODIFY","doc\/whats_new\/v0.23.rst":"MODIFY","sklearn\/tree\/_export.py":"MODIFY"},"diff":{"sklearn\/tree\/tests\/test_export.py":[{"add":["450","","451","","452","# FIXME: to be removed in 0.25","453","def test_plot_tree_rotate_deprecation(pyplot):","454","    tree = DecisionTreeClassifier()","455","    tree.fit(X, y)","456","    # test that a warning is raised when rotate is used.","457","    match = (\"'rotate' has no effect and is deprecated in 0.23. \"","458","             \"It will be removed in 0.25.\")","459","    with pytest.warns(FutureWarning, match=match):","460","        plot_tree(tree, rotate=True)"],"delete":[]}],"doc\/whats_new\/v0.23.rst":[{"add":["58","","59",":mod:`sklearn.tree`","60","...................","61","","62","- |Fix| :func:`tree.plot_tree` `rotate` parameter was unused and has been","63","  deprecated.","64","  :pr:`15806` by :user:`Chiara Marmo <cmarmo>`.","65",""],"delete":[]}],"sklearn\/tree\/_export.py":[{"add":["26","import warnings","81","              proportion=False, rotate='deprecated', rounded=False,","134","        This parameter has no effect on the matplotlib tree visualisation and","135","        it is kept here for backward compatibility.","136","","137","        .. deprecated:: 0.23","138","           ``rotate`` is deprecated in 0.23 and will be removed in 0.25.","139","","175","","176","    if rotate != 'deprecated':","177","        warnings.warn((\"'rotate' has no effect and is deprecated in 0.23. \"","178","                       \"It will be removed in 0.25.\"),","179","                      FutureWarning)","180",""],"delete":["80","              proportion=False, rotate=False, rounded=False,","133","        When set to ``True``, orient tree left to right rather than top-down."]}]}},"98b3c7c8719e2158a61891dd78e7982929c1cea0":{"changes":{"doc\/whats_new\/v0.23.rst":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_gradient_boosting.py":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/grower.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.23.rst":[{"add":["114","- |API| Fixed a bug in :class:`ensemble.HistGradientBoostingClassifier` and","115","  :class:`ensemble.HistGradientBoostingRegrerssor` that would not respect the","116","  `max_leaf_nodes` parameter if the criteria was reached at the same time as","117","  the `max_depth` criteria. :pr:`16183` by `Nicolas Hug`_.","118",""],"delete":[]}],"sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_gradient_boosting.py":[{"add":["447","","448","","449","def test_max_depth_max_leaf_nodes():","450","    # Non regression test for","451","    # https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/16179","452","    # there was a bug when the max_depth and the max_leaf_nodes criteria were","453","    # met at the same time, which would lead to max_leaf_nodes not being","454","    # respected.","455","    X, y = make_classification(random_state=0)","456","    est = HistGradientBoostingClassifier(max_depth=2, max_leaf_nodes=3,","457","                                         max_iter=1).fit(X, y)","458","    tree = est._predictors[0][0]","459","    assert tree.get_max_depth() == 2","460","    assert tree.get_n_leaf_nodes() == 3  # would be 4 prior to bug fix"],"delete":[]}],"sklearn\/ensemble\/_hist_gradient_boosting\/grower.py":[{"add":["364","        if self.max_depth is not None and depth == self.max_depth:","365","            self._finalize_leaf(left_child_node)","366","            self._finalize_leaf(right_child_node)","367","            return left_child_node, right_child_node","368",""],"delete":["357","        if self.max_depth is not None and depth == self.max_depth:","358","            self._finalize_leaf(left_child_node)","359","            self._finalize_leaf(right_child_node)","360","            return left_child_node, right_child_node","361",""]}]}},"a68ba97309512f691feb25e6a07af7561fc56e3b":{"changes":{"sklearn\/inspection\/tests\/test_permutation_importance.py":"MODIFY","sklearn\/inspection\/_permutation_importance.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY"},"diff":{"sklearn\/inspection\/tests\/test_permutation_importance.py":[{"add":["8","from sklearn.datasets import make_classification","10","from sklearn.dummy import DummyClassifier","18","from sklearn.preprocessing import KBinsDiscretizer","22","from sklearn.utils import parallel_backend","23","from sklearn.utils._testing import _convert_container","24","","158","","159","","160","def test_permutation_importance_equivalence_sequential_parallel():","161","    # regression test to make sure that sequential and parallel calls will","162","    # output the same results.","163","    X, y = make_regression(n_samples=500, n_features=10, random_state=0)","164","    lr = LinearRegression().fit(X, y)","165","","166","    importance_sequential = permutation_importance(","167","        lr, X, y, n_repeats=5, random_state=0, n_jobs=1","168","    )","169","","170","    # First check that the problem is structured enough and that the model is","171","    # complex enough to not yield trivial, constant importances:","172","    imp_min = importance_sequential['importances'].min()","173","    imp_max = importance_sequential['importances'].max()","174","    assert imp_max - imp_min > 0.3","175","","176","    # The actually check that parallelism does not impact the results","177","    # either with shared memory (threading) or without isolated memory","178","    # via process-based parallelism using the default backend","179","    # ('loky' or 'multiprocessing') depending on the joblib version:","180","","181","    # process-based parallelism (by default):","182","    importance_processes = permutation_importance(","183","        lr, X, y, n_repeats=5, random_state=0, n_jobs=2)","184","    assert_allclose(","185","        importance_processes['importances'],","186","        importance_sequential['importances']","187","    )","188","","189","    # thread-based parallelism:","190","    with parallel_backend(\"threading\"):","191","        importance_threading = permutation_importance(","192","            lr, X, y, n_repeats=5, random_state=0, n_jobs=2","193","        )","194","    assert_allclose(","195","        importance_threading['importances'],","196","        importance_sequential['importances']","197","    )","198","","199","","200","@pytest.mark.parametrize(\"n_jobs\", [None, 1, 2])","201","def test_permutation_importance_equivalence_array_dataframe(n_jobs):","202","    # This test checks that the column shuffling logic has the same behavior","203","    # both a dataframe and a simple numpy array.","204","    pd = pytest.importorskip('pandas')","205","","206","    # regression test to make sure that sequential and parallel calls will","207","    # output the same results.","208","    X, y = make_regression(n_samples=100, n_features=5, random_state=0)","209","    X_df = pd.DataFrame(X)","210","","211","    # Add a categorical feature that is statistically linked to y:","212","    binner = KBinsDiscretizer(n_bins=3, encode=\"ordinal\")","213","    cat_column = binner.fit_transform(y.reshape(-1, 1))","214","","215","    # Concatenate the extra column to the numpy array: integers will be","216","    # cast to float values","217","    X = np.hstack([X, cat_column])","218","    assert X.dtype.kind == \"f\"","219","","220","    # Insert extra column as a non-numpy-native dtype (while keeping backward","221","    # compat for old pandas versions):","222","    if hasattr(pd, \"Categorical\"):","223","        cat_column = pd.Categorical(cat_column.ravel())","224","    else:","225","        cat_column = cat_column.ravel()","226","    new_col_idx = len(X_df.columns)","227","    X_df[new_col_idx] = cat_column","228","    assert X_df[new_col_idx].dtype == cat_column.dtype","229","","230","    # Stich an aribtrary index to the dataframe:","231","    X_df.index = np.arange(len(X_df)).astype(str)","232","","233","    rf = RandomForestRegressor(n_estimators=5, max_depth=3, random_state=0)","234","    rf.fit(X, y)","235","","236","    n_repeats = 3","237","    importance_array = permutation_importance(","238","        rf, X, y, n_repeats=n_repeats, random_state=0, n_jobs=n_jobs","239","    )","240","","241","    # First check that the problem is structured enough and that the model is","242","    # complex enough to not yield trivial, constant importances:","243","    imp_min = importance_array['importances'].min()","244","    imp_max = importance_array['importances'].max()","245","    assert imp_max - imp_min > 0.3","246","","247","    # Now check that importances computed on dataframe matche the values","248","    # of those computed on the array with the same data.","249","    importance_dataframe = permutation_importance(","250","        rf, X_df, y, n_repeats=n_repeats, random_state=0, n_jobs=n_jobs","251","    )","252","    assert_allclose(","253","        importance_array['importances'],","254","        importance_dataframe['importances']","255","    )","256","","257","","258","@pytest.mark.parametrize(\"input_type\", [\"array\", \"dataframe\"])","259","def test_permutation_importance_large_memmaped_data(input_type):","260","    # Smoke, non-regression test for:","261","    # https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/15810","262","    n_samples, n_features = int(5e4), 4","263","    X, y = make_classification(n_samples=n_samples, n_features=n_features,","264","                               random_state=0)","265","    assert X.nbytes > 1e6  # trigger joblib memmaping","266","","267","    X = _convert_container(X, input_type)","268","    clf = DummyClassifier(strategy='prior').fit(X, y)","269","","270","    # Actual smoke test: should not raise any error:","271","    n_repeats = 5","272","    r = permutation_importance(clf, X, y, n_repeats=n_repeats, n_jobs=2)","273","","274","    # Auxiliary check: DummyClassifier is feature independent:","275","    # permutating feature should not change the predictions","276","    expected_importances = np.zeros((n_features, n_repeats))","277","    assert_allclose(expected_importances, r.importances)"],"delete":[]}],"sklearn\/inspection\/_permutation_importance.py":[{"add":["6","from ..utils import Bunch","14","    random_state = check_random_state(random_state)","16","    # Work on a copy of X to to ensure thread-safety in case of threading based","17","    # parallelism. Furthermore, making a copy is also useful when the joblib","18","    # backend is 'loky' (default) or the old 'multiprocessing': in those cases,","19","    # if X is large it will be automatically be backed by a readonly memory map","20","    # (memmap). X.copy() on the other hand is always guaranteed to return a","21","    # writable data-structure whose columns can be shuffled inplace.","22","    X_permuted = X.copy()","24","    shuffling_idx = np.arange(X.shape[0])","26","        random_state.shuffle(shuffling_idx)","27","        if hasattr(X_permuted, \"iloc\"):","28","            col = X_permuted.iloc[shuffling_idx, col_idx]","29","            col.index = X_permuted.index","30","            X_permuted.iloc[:, col_idx] = col","31","        else:","32","            X_permuted[:, col_idx] = X_permuted[shuffling_idx, col_idx]","33","        feature_score = scorer(estimator, X_permuted, y)","101","    if not hasattr(X, \"iloc\"):","102","        X = check_array(X, force_all_finite='allow-nan', dtype=None)","104","    # Precompute random seed from the random state to be used","105","    # to get a fresh independent RandomState instance for each","106","    # parallel call to _calculate_permutation_scores, irrespective of","107","    # the fact that variables are shared or not depending on the active","108","    # joblib backend (sequential, thread-based or process-based).","110","    random_seed = random_state.randint(np.iinfo(np.int32).max + 1)","112","    scorer = check_scoring(estimator, scoring=scoring)","116","        estimator, X, y, col_idx, random_seed, n_repeats, scorer"],"delete":["8","from ..utils import Bunch","9","","10","","11","def _safe_column_setting(X, col_idx, values):","12","    \"\"\"Set column on X using `col_idx`\"\"\"","13","    if hasattr(X, \"iloc\"):","14","        X.iloc[:, col_idx] = values","15","    else:","16","        X[:, col_idx] = values","17","","18","","19","def _safe_column_indexing(X, col_idx):","20","    \"\"\"Return column from X using `col_idx`\"\"\"","21","    if hasattr(X, \"iloc\"):","22","        return X.iloc[:, col_idx].values","23","    else:","24","        return X[:, col_idx]","30","    original_feature = _safe_column_indexing(X, col_idx).copy()","31","    temp = original_feature.copy()","35","        random_state.shuffle(temp)","36","        _safe_column_setting(X, col_idx, temp)","37","        feature_score = scorer(estimator, X, y)","40","    _safe_column_setting(X, col_idx, original_feature)","106","    if hasattr(X, \"iloc\"):","107","        X = X.copy()  # Dataframe","108","    else:","109","        X = check_array(X, force_all_finite='allow-nan', dtype=np.object,","110","                        copy=True)","113","    scorer = check_scoring(estimator, scoring=scoring)","116","    scores = np.zeros((X.shape[1], n_repeats))","119","        estimator, X, y, col_idx, random_state, n_repeats, scorer"]}],"doc\/whats_new\/v0.22.rst":[{"add":["17","","24"," ","25",":mod:`sklearn.inspection`","26",".........................","27","","28","- |Fix| :func:`inspection.permutation_importance` will return the same","29","  `importances` when a `random_state` is given for both `n_jobs=1` or","30","  `n_jobs>1` both with shared memory backends (thread-safety) and","31","  isolated memory, process-based backends.","32","  Also avoid casting the data as object dtype and avoid read-only error","33","  on large dataframes with `n_jobs>1` as reported in :issue:`15810`.","34","  Follow-up of :pr:`15898` by :user:`Shivam Gargsya <shivamgargsya>`.","35","  :pr:`15933` by :user:`Guillaume Lemaitre <glemaitre>` and `Olivier Grisel`_."],"delete":[]}]}},"1c42e79d420cc03de5e0c3b625753c6084e25a3f":{"changes":{"sklearn\/linear_model\/_ransac.py":"MODIFY","doc\/whats_new\/v0.23.rst":"MODIFY","sklearn\/linear_model\/tests\/test_ransac.py":"MODIFY"},"diff":{"sklearn\/linear_model\/_ransac.py":[{"add":["330","        inlier_best_idxs_subset = None","407","            inlier_best_idxs_subset = inlier_idxs_subset","445","        if sample_weight is None:","446","            base_estimator.fit(X_inlier_best, y_inlier_best)","447","        else:","448","            base_estimator.fit(","449","                X_inlier_best,","450","                y_inlier_best,","451","                sample_weight=sample_weight[inlier_best_idxs_subset])"],"delete":["443","        base_estimator.fit(X_inlier_best, y_inlier_best)"]}],"doc\/whats_new\/v0.23.rst":[{"add":["60",":mod:`sklearn.linear_model`","61","...........................","62","","63","- |Fix| Fixed a bug where if a `sample_weight` parameter was passed to the fit","64","  method of :class:`linear_model.RANSACRegressor`, it would not be passed to","65","  the wrapped `base_estimator` during the fitting of the final model.","66","  :pr:`15573` by :user:`Jeremy Alexandre <J-A16>`.","67",""],"delete":[]}],"sklearn\/linear_model\/tests\/test_ransac.py":[{"add":["12","from sklearn.utils._testing import assert_allclose","13","from sklearn.datasets import make_regression","498","","499","","500","def test_ransac_final_model_fit_sample_weight():","501","    X, y = make_regression(n_samples=1000, random_state=10)","502","    rng = check_random_state(42)","503","    sample_weight = rng.randint(1, 4, size=y.shape[0])","504","    sample_weight = sample_weight \/ sample_weight.sum()","505","    ransac = RANSACRegressor(base_estimator=LinearRegression(), random_state=0)","506","    ransac.fit(X, y, sample_weight=sample_weight)","507","","508","    final_model = LinearRegression()","509","    mask_samples = ransac.inlier_mask_","510","    final_model.fit(","511","        X[mask_samples], y[mask_samples],","512","        sample_weight=sample_weight[mask_samples]","513","    )","514","","515","    assert_allclose(ransac.estimator_.coef_, final_model.coef_)"],"delete":[]}]}},"4b7afeeabd430073168649045fac33c8e80a4f50":{"changes":{"sklearn\/cluster\/tests\/test_hierarchical.py":"MODIFY"},"diff":{"sklearn\/cluster\/tests\/test_hierarchical.py":[{"add":["16","from sklearn.metrics.cluster import adjusted_rand_score"],"delete":["16","from sklearn.metrics.cluster.supervised import adjusted_rand_score"]}]}},"981fa7b8f1a6e273e05df6b173eee311f6d11a26":{"changes":{"sklearn\/multioutput.py":"MODIFY","sklearn\/tests\/test_multioutput.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY"},"diff":{"sklearn\/multioutput.py":[{"add":["362","    @property","363","    def predict_proba(self):","385","            raise AttributeError(\"The base estimator should \"","386","                                 \"implement predict_proba method\")","387","        return self._predict_proba","389","    def _predict_proba(self, X):"],"delete":["362","    def predict_proba(self, X):","384","            raise ValueError(\"The base estimator should implement \"","385","                             \"predict_proba method\")"]}],"sklearn\/tests\/test_multioutput.py":[{"add":["177","# check multioutput has predict_proba","178","def test_hasattr_multi_output_predict_proba():","179","    # default SGDClassifier has loss='hinge'","180","    # which does not expose a predict_proba method","181","    sgd_linear_clf = SGDClassifier(random_state=1, max_iter=5)","182","    multi_target_linear = MultiOutputClassifier(sgd_linear_clf)","183","    multi_target_linear.fit(X, y)","184","    assert not hasattr(multi_target_linear, \"predict_proba\")","185","","186","    # case where predict_proba attribute exists","187","    sgd_linear_clf = SGDClassifier(loss='log', random_state=1, max_iter=5)","188","    multi_target_linear = MultiOutputClassifier(sgd_linear_clf)","189","    multi_target_linear.fit(X, y)","190","    assert hasattr(multi_target_linear, \"predict_proba\")","191","","192","","217","    with pytest.raises(AttributeError, match=err_msg):","396","    with pytest.raises(NotFittedError):","397","        moc.predict_proba"],"delete":["201","    with pytest.raises(ValueError, match=err_msg):","380","    assert_raises(NotFittedError, moc.predict_proba, y)"]}],"doc\/whats_new\/v0.22.rst":[{"add":["659","- |Fix| :class:`multioutput.MultiOutputClassifier` now has `predict_proba`","660","  as property and can be checked with `hasattr`.","661","  :issue:`15488` :pr:`15490` by :user:`Rebekah Kim <rebekahkim>`","662",""],"delete":[]}]}},"eb3f5dfe562b15e507c1f8e3ab16848ec2ab6f84":{"changes":{"doc\/conf.py":"MODIFY"},"diff":{"doc\/conf.py":[{"add":["299","    'show_memory': False,"],"delete":["299","    'show_memory': True,"]}]}},"13134a884b92f8c601162ce4f125c2fc17c6ed4a":{"changes":{"sklearn\/_build_utils\/deprecated_modules.py":"MODIFY","sklearn\/cluster\/_spectral.py":"MODIFY","sklearn\/feature_extraction\/text.py":"MODIFY","sklearn\/decomposition\/_lda.py":"ADD","sklearn\/cluster\/tests\/test_k_means.py":"MODIFY","sklearn\/feature_extraction\/_hash.py":"ADD","sklearn\/decomposition\/tests\/test_online_lda.py":"MODIFY","sklearn\/feature_extraction\/__init__.py":"MODIFY","\/dev\/null":"DELETE","sklearn\/decomposition\/__init__.py":"MODIFY","sklearn\/cluster\/__init__.py":"MODIFY","sklearn\/cluster\/_kmeans.py":"ADD","sklearn\/cluster\/_agglomerative.py":"ADD","sklearn\/cluster\/tests\/test_hierarchical.py":"MODIFY"},"diff":{"sklearn\/_build_utils\/deprecated_modules.py":[{"add":["49","    ('_agglomerative', 'sklearn.cluster.hierarchical', 'sklearn.cluster',","51","    ('_kmeans', 'sklearn.cluster.k_means_', 'sklearn.cluster', 'KMeans'),","103","    ('_lda', 'sklearn.decomposition.online_lda',","142","    ('_hash', 'sklearn.feature_extraction.hashing',"],"delete":["49","    ('_hierarchical', 'sklearn.cluster.hierarchical', 'sklearn.cluster',","51","    ('_k_means', 'sklearn.cluster.k_means_', 'sklearn.cluster', 'KMeans'),","103","    ('_online_lda', 'sklearn.decomposition.online_lda',","142","    ('_hashing', 'sklearn.feature_extraction.hashing',"]}],"sklearn\/cluster\/_spectral.py":[{"add":["17","from ._kmeans import k_means"],"delete":["17","from ._k_means import k_means"]}],"sklearn\/feature_extraction\/text.py":[{"add":["29","from ._hash import FeatureHasher"],"delete":["29","from ._hashing import FeatureHasher"]}],"sklearn\/decomposition\/_lda.py":[{"add":[],"delete":[]}],"sklearn\/cluster\/tests\/test_k_means.py":[{"add":["24","from sklearn.cluster._kmeans import _labels_inertia","25","from sklearn.cluster._kmeans import _mini_batch_step","736","    from sklearn.cluster._kmeans import _init_centroids","923","    from sklearn.cluster._kmeans import _check_normalize_sample_weight"],"delete":["24","from sklearn.cluster._k_means import _labels_inertia","25","from sklearn.cluster._k_means import _mini_batch_step","736","    from sklearn.cluster._k_means import _init_centroids","923","    from sklearn.cluster._k_means import _check_normalize_sample_weight"]}],"sklearn\/feature_extraction\/_hash.py":[{"add":[],"delete":[]}],"sklearn\/decomposition\/tests\/test_online_lda.py":[{"add":["10","from sklearn.decomposition._lda import (_dirichlet_expectation_1d,","11","                                        _dirichlet_expectation_2d)"],"delete":["10","from sklearn.decomposition._online_lda import (_dirichlet_expectation_1d,","11","                                               _dirichlet_expectation_2d)"]}],"sklearn\/feature_extraction\/__init__.py":[{"add":["7","from ._hash import FeatureHasher"],"delete":["7","from ._hashing import FeatureHasher"]}],"\/dev\/null":[{"add":[],"delete":[]}],"sklearn\/decomposition\/__init__.py":[{"add":["18","from ._lda import LatentDirichletAllocation"],"delete":["18","from ._online_lda import LatentDirichletAllocation"]}],"sklearn\/cluster\/__init__.py":[{"add":["9","from ._agglomerative import (ward_tree, AgglomerativeClustering,","10","                             linkage_tree, FeatureAgglomeration)","11","from ._kmeans import k_means, KMeans, MiniBatchKMeans"],"delete":["9","from ._hierarchical import (ward_tree, AgglomerativeClustering, linkage_tree,","10","                            FeatureAgglomeration)","11","from ._k_means import k_means, KMeans, MiniBatchKMeans"]}],"sklearn\/cluster\/_kmeans.py":[{"add":[],"delete":[]}],"sklearn\/cluster\/_agglomerative.py":[{"add":[],"delete":[]}],"sklearn\/cluster\/tests\/test_hierarchical.py":[{"add":["24","from sklearn.cluster._agglomerative import (_hc_cut, _TREE_BUILDERS,","25","                                            linkage_tree,","26","                                            _fix_connectivity)"],"delete":["24","from sklearn.cluster._hierarchical import (_hc_cut, _TREE_BUILDERS,","25","                                           linkage_tree, _fix_connectivity)"]}]}},"a07974b3c05f5f5b957aea99fe8c199e29485f5b":{"changes":{"doc\/whats_new\/v0.23.rst":"MODIFY","sklearn\/linear_model\/tests\/test_ridge.py":"MODIFY","sklearn\/linear_model\/_ridge.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.23.rst":[{"add":["83","- |Fix| add `best_score_` attribute to :class:`linear_model.RidgeCV` and","84","  :class:`linear_model.RidgeClassifierCV`.","85","  :pr:`15653` by :user:`J¨¦r?me Dock¨¨s <jeromedockes>`.","86",""],"delete":[]}],"sklearn\/linear_model\/tests\/test_ridge.py":[{"add":["666","    [(RidgeCV(store_cv_values=False), make_regression),","667","     (RidgeClassifierCV(store_cv_values=False), make_classification)]","676","@pytest.mark.parametrize(","677","    \"ridge, make_dataset\",","678","    [(RidgeCV(), make_regression),","679","     (RidgeClassifierCV(), make_classification)]","680",")","681","@pytest.mark.parametrize(\"cv\", [None, 3])","682","def test_ridge_best_score(ridge, make_dataset, cv):","683","    # check that the best_score_ is store","684","    X, y = make_dataset(n_samples=6, random_state=42)","685","    ridge.set_params(store_cv_values=False, cv=cv)","686","    ridge.fit(X, y)","687","    assert hasattr(ridge, \"best_score_\")","688","    assert isinstance(ridge.best_score_, float)","689","","690",""],"delete":["666","    [(RidgeCV(), make_regression),","667","     (RidgeClassifierCV(), make_classification)]","672","    ridge.set_params(store_cv_values=False)"]}],"sklearn\/linear_model\/_ridge.py":[{"add":["1578","            self.best_score_ = estimator.best_score_","1594","            self.best_score_ = gs.best_score_","1697","    best_score_ : float","1698","        Mean cross-validated score of the estimator with the best alpha found.","1699","","1802","        Estimated regularization parameter.","1803","","1804","    best_score_ : float","1805","        Mean cross-validated score of the estimator with the best alpha found."],"delete":["1797","        Estimated regularization parameter"]}]}},"cc2fbeddfa7f462cad230edcbffaf14e0fa5f965":{"changes":{"sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_grower.py":"MODIFY","doc\/whats_new\/v0.23.rst":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/gradient_boosting.py":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_gradient_boosting.py":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/grower.py":"MODIFY"},"diff":{"sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_grower.py":[{"add":["259","def assert_is_stump(grower):","260","    # To assert that stumps are created when max_depth=1","261","    for leaf in (grower.root.left_child, grower.root.right_child):","262","        assert leaf.left_child is None","263","        assert leaf.right_child is None","264","","265","","266","@pytest.mark.parametrize('max_depth', [1, 2, 3])","288","    if max_depth == 1:","289","        assert_is_stump(grower)","290",""],"delete":["259","@pytest.mark.parametrize('max_depth', [2, 3])"]}],"doc\/whats_new\/v0.23.rst":[{"add":["92","- |Fix|  Changed the convention for `max_depth` parameter of","93","  :class:`ensemble.HistGradientBoostingClassifier` and","94","  :class:`ensemble.HistGradientBoostingRegressor`. The depth now corresponds to","95","  the number of edges to go from the root to the deepest leaf.","96","  Stumps (trees with one split) are now allowed.","97","  :pr: `16182` by :user:`Santhosh B <santhoshbala18>`","98",""],"delete":[]}],"sklearn\/ensemble\/_hist_gradient_boosting\/gradient_boosting.py":[{"add":["691","        edges to go from the root to the deepest leaf.","692","        Depth isn't constrained by default.","874","        edges to go from the root to the deepest leaf.","875","        Depth isn't constrained by default."],"delete":["691","        nodes to go from the root to the deepest leaf. Must be strictly greater","692","        than 1. Depth isn't constrained by default.","874","        nodes to go from the root to the deepest leaf. Must be strictly greater","875","        than 1. Depth isn't constrained by default."]}],"sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_gradient_boosting.py":[{"add":["33","     ({'max_depth': 0}, 'max_depth=0 should not be smaller than 1'),"],"delete":["33","     ({'max_depth': 0}, 'max_depth=0 should not be smaller than 2'),","34","     ({'max_depth': 1}, 'max_depth=1 should not be smaller than 2'),"]}],"sklearn\/ensemble\/_hist_gradient_boosting\/grower.py":[{"add":["137","        edges to go from the root to the deepest leaf.","138","        Depth isn't constrained by default.","233","        if max_depth is not None and max_depth < 1:","235","                             ' smaller than 1'.format(max_depth))"],"delete":["137","        nodes to go from the root to the deepest leaf.","232","        if max_depth is not None and max_depth <= 1:","234","                             ' smaller than 2'.format(max_depth))"]}]}},"3ef8357cc01721bfe817a702acda5315ce0d716a":{"changes":{"sklearn\/model_selection\/tests\/test_search.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY","sklearn\/model_selection\/_search.py":"MODIFY"},"diff":{"sklearn\/model_selection\/tests\/test_search.py":[{"add":["58","from sklearn.neighbors import KNeighborsClassifier","64","from sklearn.metrics.pairwise import euclidean_distances","1802","","1803","","1804","def test_search_cv__pairwise_property_delegated_to_base_estimator():","1805","    \"\"\"","1806","    Test implementation of BaseSearchCV has the _pairwise property","1807","    which matches the _pairwise property of its estimator.","1808","    This test make sure _pairwise is delegated to the base estimator.","1809","","1810","    Non-regression test for issue #13920.","1811","    \"\"\"","1812","    est = BaseEstimator()","1813","    attr_message = \"BaseSearchCV _pairwise property must match estimator\"","1814","","1815","    for _pairwise_setting in [True, False]:","1816","        setattr(est, '_pairwise', _pairwise_setting)","1817","        cv = GridSearchCV(est, {'n_neighbors': [10]})","1818","        assert _pairwise_setting == cv._pairwise, attr_message","1819","","1820","","1821","def test_search_cv__pairwise_property_equivalence_of_precomputed():","1822","    \"\"\"","1823","    Test implementation of BaseSearchCV has the _pairwise property","1824","    which matches the _pairwise property of its estimator.","1825","    This test ensures the equivalence of 'precomputed'.","1826","","1827","    Non-regression test for issue #13920.","1828","    \"\"\"","1829","    n_samples = 50","1830","    n_splits = 2","1831","    X, y = make_classification(n_samples=n_samples, random_state=0)","1832","    grid_params = {'n_neighbors': [10]}","1833","","1834","    # defaults to euclidean metric (minkowski p = 2)","1835","    clf = KNeighborsClassifier()","1836","    cv = GridSearchCV(clf, grid_params, cv=n_splits)","1837","    cv.fit(X, y)","1838","    preds_original = cv.predict(X)","1839","","1840","    # precompute euclidean metric to validate _pairwise is working","1841","    X_precomputed = euclidean_distances(X)","1842","    clf = KNeighborsClassifier(metric='precomputed')","1843","    cv = GridSearchCV(clf, grid_params, cv=n_splits)","1844","    cv.fit(X_precomputed, y)","1845","    preds_precomputed = cv.predict(X_precomputed)","1846","","1847","    attr_message = \"GridSearchCV not identical with precomputed metric\"","1848","    assert (preds_original == preds_precomputed).all(), attr_message"],"delete":[]}],"doc\/whats_new\/v0.22.rst":[{"add":["742",":mod:`sklearn.model_selection`","743","..................","744","","745","- |Fix| :class:`model_selection.GridSearchCV` and","746","  `model_selection.RandomizedSearchCV` now supports the","747","  :term:`_pairwise` property, which prevents an error during cross-validation","748","  for estimators with pairwise inputs (such as","749","  :class:`neighbors.KNeighborsClassifier` when :term:`metric` is set to","750","  'precomputed').","751","  :pr:`13925` by :user:`Isaac S. Robson <isrobson>` and :pr:`15524` by","752","  :user:`Xun Tang <xun-tang>`.","753","","754",""],"delete":[]}],"sklearn\/model_selection\/_search.py":[{"add":["416","    @property","417","    def _pairwise(self):","418","        # allows cross-validation to see 'precomputed' metrics","419","        return getattr(self.estimator, '_pairwise', False)","420",""],"delete":[]}]}},"7c47337f7b15a5368c922ed1781a267bf66c7367":{"changes":{"sklearn\/ensemble\/_voting.py":"MODIFY","sklearn\/ensemble\/tests\/test_voting.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY"},"diff":{"sklearn\/ensemble\/_voting.py":[{"add":["73","","74","        # Uses None or 'drop' as placeholder for dropped estimators","75","        est_iter = iter(self.estimators_)","76","        for name, est in self.estimators:","77","            current_est = est if est in (None, 'drop') else next(est_iter)","78","            self.named_estimators_[name] = current_est","79",""],"delete":["73","        for k, e in zip(self.estimators, self.estimators_):","74","            self.named_estimators_[k[0]] = e"]}],"sklearn\/ensemble\/tests\/test_voting.py":[{"add":["562","","563","","564","# TODO: Remove drop parametrize in 0.24 when None is removed in Voting*","565","@pytest.mark.parametrize(","566","    \"Voter, BaseEstimator\",","567","    [(VotingClassifier, DecisionTreeClassifier),","568","     (VotingRegressor, DecisionTreeRegressor)]","569",")","570","@pytest.mark.parametrize(\"drop\", [None, 'drop'])","571","def test_correct_named_estimator_with_drop(Voter, BaseEstimator, drop):","572","    est = Voter(estimators=[('lr', drop),","573","                            ('tree', BaseEstimator(random_state=0))])","574","","575","    with pytest.warns(None) as rec:","576","        est.fit(X, y)","577","    assert rec if drop is None else not rec","578","","579","    assert est.named_estimators_['lr'] == drop"],"delete":[]}],"doc\/whats_new\/v0.22.rst":[{"add":["718",":mod:`sklearn.voting`","719",".....................","720","","721","- |Fix| The `named_estimators_` attribute in :class:`voting.VotingClassifier`","722","  and :class:`voting.VotingRegressor` now correctly maps to dropped estimators.","723","  Previously, the `named_estimators_` mapping was incorrect whenever one of the","724","  estimators was dropped. :pr:`15375` by `Thomas Fan`_.","725",""],"delete":[]}]}},"a6c07f2b0862885850d455a3a8b995d57f7d5648":{"changes":{"doc\/conf.py":"MODIFY"},"diff":{"doc\/conf.py":[{"add":["53","    mathjax_path = ''"],"delete":[]}]}},"a5542e9490c63e44e5611583b5e8764149b230c8":{"changes":{"sklearn\/datasets\/__init__.py":"MODIFY","\/dev\/null":"DELETE","sklearn\/datasets\/_svmlight_format_io.py":"ADD","sklearn\/tests\/test_docstring_parameters.py":"MODIFY","sklearn\/_build_utils\/deprecated_modules.py":"MODIFY","sklearn\/datasets\/_rcv1.py":"MODIFY","sklearn\/tests\/test_common.py":"MODIFY","sklearn\/utils\/_testing.py":"MODIFY"},"diff":{"sklearn\/datasets\/__init__.py":[{"add":["44","from ._svmlight_format_io import load_svmlight_file","45","from ._svmlight_format_io import load_svmlight_files","46","from ._svmlight_format_io import dump_svmlight_file"],"delete":["44","from ._svmlight_format import load_svmlight_file","45","from ._svmlight_format import load_svmlight_files","46","from ._svmlight_format import dump_svmlight_file"]}],"\/dev\/null":[{"add":[],"delete":[]}],"sklearn\/datasets\/_svmlight_format_io.py":[{"add":[],"delete":[]}],"sklearn\/tests\/test_docstring_parameters.py":[{"add":["142","        if IS_PYPY and ('_svmlight_format_io' in modname or"],"delete":["142","        if IS_PYPY and ('_svmlight_format' in modname or"]}],"sklearn\/_build_utils\/deprecated_modules.py":[{"add":["135","    ('_svmlight_format_io', 'sklearn.datasets.svmlight_format',"],"delete":["135","    ('_svmlight_format', 'sklearn.datasets.svmlight_format',"]}],"sklearn\/datasets\/_rcv1.py":[{"add":["25","from ._svmlight_format_io import load_svmlight_files"],"delete":["25","from ._svmlight_format import load_svmlight_files"]}],"sklearn\/tests\/test_common.py":[{"add":["181","        if IS_PYPY and ('_svmlight_format_io' in modname or"],"delete":["181","        if IS_PYPY and ('_svmlight_format' in modname or"]}],"sklearn\/utils\/_testing.py":[{"add":["468","        if IS_PYPY and ('_svmlight_format_io' in modname or"],"delete":["468","        if IS_PYPY and ('_svmlight_format' in modname or"]}]}},"4953ec36a41694d38f8d92fd48c7049cdd82e0ed":{"changes":{"\/dev\/null":"DELETE","sklearn\/linear_model\/tests\/test_least_angle.py":"MODIFY","sklearn\/multioutput.py":"MODIFY","sklearn\/svm\/_classes.py":"MODIFY","sklearn\/svm\/tests\/test_svm.py":"MODIFY","sklearn\/linear_model\/_least_angle.py":"MODIFY"},"diff":{"\/dev\/null":[{"add":[],"delete":[]}],"sklearn\/linear_model\/tests\/test_least_angle.py":[{"add":["17","from sklearn.linear_model._least_angle import _lars_path_residues","18","from sklearn.linear_model import LassoLarsIC, lars_path","733","","734","","735","def test_X_none_gram_not_none():","736","    with pytest.raises(ValueError,","737","                       match=\"X cannot be None if Gram is not None\"):","738","        lars_path(X=None, y=[1], Gram='not None')"],"delete":["17","from sklearn.linear_model._least_angle import _lars_path_residues, LassoLarsIC"]}],"sklearn\/multioutput.py":[{"add":[],"delete":["267","    # XXX Remove this method in 0.23","268","    def score(self, X, y, sample_weight=None):","269","        \"\"\"Returns the coefficient of determination R^2 of the prediction.","270","","271","        The coefficient R^2 is defined as (1 - u\/v), where u is the residual","272","        sum of squares ((y_true - y_pred) ** 2).sum() and v is the regression","273","        sum of squares ((y_true - y_true.mean()) ** 2).sum().","274","        Best possible score is 1.0 and it can be negative (because the","275","        model can be arbitrarily worse). A constant model that always","276","        predicts the expected value of y, disregarding the input features,","277","        would get a R^2 score of 0.0.","278","","279","        Notes","280","        -----","281","        R^2 is calculated by weighting all the targets equally using","282","        `multioutput='uniform_average'`.","283","","284","        Parameters","285","        ----------","286","        X : array-like, shape (n_samples, n_features)","287","            Test samples.","288","","289","        y : array-like, shape (n_samples) or (n_samples, n_outputs)","290","            True values for X.","291","","292","        sample_weight : array-like, shape [n_samples], optional","293","            Sample weights.","294","","295","        Returns","296","        -------","297","        score : float","298","            R^2 of self.predict(X) wrt. y.","299","        \"\"\"","300","        # XXX remove in 0.19 when r2_score default for multioutput changes","301","        from .metrics import r2_score","302","        return r2_score(y, self.predict(X), sample_weight=sample_weight,","303","                        multioutput='uniform_average')","304",""]}],"sklearn\/svm\/_classes.py":[{"add":[],"delete":["216","        # FIXME Remove l1\/l2 support in 0.23 ----------------------------------","217","        msg = (\"loss='%s' has been deprecated in favor of \"","218","               \"loss='%s' as of 0.16. Backward compatibility\"","219","               \" for the loss='%s' will be removed in %s\")","220","","221","        if self.loss in ('l1', 'l2'):","222","            old_loss = self.loss","223","            self.loss = {'l1': 'hinge', 'l2': 'squared_hinge'}.get(self.loss)","224","            warnings.warn(msg % (old_loss, self.loss, old_loss, '0.23'),","225","                          FutureWarning)","226","        # ---------------------------------------------------------------------","227","","408","        # FIXME Remove l1\/l2 support in 0.23 ----------------------------------","409","        msg = (\"loss='%s' has been deprecated in favor of \"","410","               \"loss='%s' as of 0.16. Backward compatibility\"","411","               \" for the loss='%s' will be removed in %s\")","412","","413","        if self.loss in ('l1', 'l2'):","414","            old_loss = self.loss","415","            self.loss = {'l1': 'epsilon_insensitive',","416","                         'l2': 'squared_epsilon_insensitive'","417","                         }.get(self.loss)","418","            warnings.warn(msg % (old_loss, self.loss, old_loss, '0.23'),","419","                          FutureWarning)","420","        # ---------------------------------------------------------------------","421",""]}],"sklearn\/svm\/tests\/test_svm.py":[{"add":[],"delete":["738","# FIXME remove in 0.23","739","def test_linearsvx_loss_penalty_deprecations():","740","    X, y = [[0.0], [1.0]], [0, 1]","741","","742","    msg = (\"loss='%s' has been deprecated in favor of \"","743","           \"loss='%s' as of 0.16. Backward compatibility\"","744","           \" for the %s will be removed in %s\")","745","","746","    # LinearSVC","747","    # loss l1 --> hinge","748","    assert_warns_message(FutureWarning,","749","                         msg % (\"l1\", \"hinge\", \"loss='l1'\", \"0.23\"),","750","                         svm.LinearSVC(loss=\"l1\").fit, X, y)","751","","752","    # loss l2 --> squared_hinge","753","    assert_warns_message(FutureWarning,","754","                         msg % (\"l2\", \"squared_hinge\", \"loss='l2'\", \"0.23\"),","755","                         svm.LinearSVC(loss=\"l2\").fit, X, y)","756","","757","    # LinearSVR","758","    # loss l1 --> epsilon_insensitive","759","    assert_warns_message(FutureWarning,","760","                         msg % (\"l1\", \"epsilon_insensitive\", \"loss='l1'\",","761","                                \"0.23\"),","762","                         svm.LinearSVR(loss=\"l1\").fit, X, y)","763","","764","    # loss l2 --> squared_epsilon_insensitive","765","    assert_warns_message(FutureWarning,","766","                         msg % (\"l2\", \"squared_epsilon_insensitive\",","767","                                \"loss='l2'\", \"0.23\"),","768","                         svm.LinearSVR(loss=\"l2\").fit, X, y)","769","","770",""]}],"sklearn\/linear_model\/_least_angle.py":[{"add":["146","        raise ValueError(","147","            'X cannot be None if Gram is not None'","148","            'Use lars_path_gram to avoid passing X and y.'","149","        )"],"delete":["49","        .. deprecated:: 0.21","50","","51","           The use of ``X`` is ``None`` in combination with ``Gram`` is not","52","           ``None`` will be removed in v0.23. Use :func:`lars_path_gram`","53","           instead.","54","","69","        .. deprecated:: 0.21","70","","71","           The use of ``X`` is ``None`` in combination with ``Gram`` is not","72","           None will be removed in v0.23. Use :func:`lars_path_gram` instead.","73","","157","        warnings.warn('Use lars_path_gram to avoid passing X and y. '","158","                      'The current option will be removed in v0.23.',","159","                      FutureWarning)"]}]}},"728108383e6655e8f2f86c1c941ff365c7cf937e":{"changes":{"doc\/modules\/classes.rst":"MODIFY"},"diff":{"doc\/modules\/classes.rst":[{"add":[],"delete":["1539","   utils.arrayfuncs.cholesky_delete"]}]}},"862a726ca98c4bc30019cd88a1e802bbc49cbfe7":{"changes":{"examples\/inspection\/plot_linear_model_coefficient_interpretation.py":"MODIFY"},"diff":{"examples\/inspection\/plot_linear_model_coefficient_interpretation.py":[{"add":["218","# The AGE coefficient is expressed in \"dollars\/hour per living years\" while the","219","# EDUCATION one is expressed in \"dollars\/hour per years of education\". This","220","# representation of the coefficients has the advantage of making clear the","221","# practical predictions of the model: an increase of :math:`1` year in AGE","222","# means a decrease of :math:`0.030867` dollars\/hour, while an increase of","223","# :math:`1` year in EDUCATION means an increase of :math:`0.054699`","224","# dollars\/hour. On the other hand, categorical variables (as UNION or SEX) are","225","# adimensional numbers taking either the value 0 or 1. Their coefficients","226","# are expressed in dollars\/hour. Then, we cannot compare the magnitude of","227","# different coefficients since the features have different natural scales, and","228","# hence value ranges, because of their different unit of measure. This is more","229","# evident if we plot the coefficients."],"delete":["218","# The AGE coefficient is expressed in","219","# :math:`$\/hours\/(living\\ years)` while the EDUCATION one is expressed","220","# in :math:`$\/hours\/(years\\ of\\ education)`.","221","# This representation of the coefficients has the advantage of making clear","222","# the practical predictions of the model:","223","# an increase of :math:`1` year in AGE means a decrease of :math:`0.030867$`,","224","# while an increase of :math:`1` year in EDUCATION means an increase of","225","# :math:`0.054699$`.","226","# On the other hand, categorical variables (as UNION or SEX) are adimensional","227","# numbers taking the value either of 0 or 1. Their coefficients are expressed","228","# in :math:`$\/hours`. Then, we cannot compare the magnitude of different","229","# coefficients since the features have different natural scales, and hence","230","# value ranges, because of their different unit of measure.","231","# This is more evident if we plot the coefficients."]}]}},"b6bbf58f0c0b6cc7be219acfe8eec8626bd04842":{"changes":{"doc\/modules\/ensemble.rst":"MODIFY"},"diff":{"doc\/modules\/ensemble.rst":[{"add":["491","The usage and the parameters of :class:`GradientBoostingClassifier` and","492",":class:`GradientBoostingRegressor` are described below. The 2 most important","493","parameters of these estimators are `n_estimators` and `learning_rate`.","494","","515","The number of weak learners (i.e. regression trees) is controlled by the","516","parameter ``n_estimators``; :ref:`The size of each tree","517","<gradient_boosting_tree_size>` can be controlled either by setting the tree","518","depth via ``max_depth`` or by setting the number of leaf nodes via","519","``max_leaf_nodes``. The ``learning_rate`` is a hyper-parameter in the range","520","(0.0, 1.0] that controls overfitting via :ref:`shrinkage","521","<gradient_boosting_shrinkage>` .","627","We first present GBRT for regression, and then detail the classification","628","case.","629","","630","Regression","631","^^^^^^^^^^","632","","633","GBRT regressors are additive models whose prediction :math:`y_i` for a","634","given input :math:`x_i` is of the following form:","638","    \\hat{y_i} = F_M(x_i) = \\sum_{m=1}^{M} h_m(x_i)","640","where the :math:`h_m` are estimators called *weak learners* in the context","641","of boosting. Gradient Tree Boosting uses :ref:`decision tree regressors","642","<tree>` of fixed size as weak learners. The constant M corresponds to the","643","`n_estimators` parameter.","645","Similar to other boosting algorithms, a GBRT is built in a greedy fashion:","649","    F_m(x) = F_{m-1}(x) + h_m(x),","651","where the newly added tree :math:`h_m` is fitted in order to minimize a sum","652","of losses :math:`L_m`, given the previous ensemble :math:`F_{m-1}`:","656","    h_m =  \\arg\\min_{h} L_m = \\arg\\min_{h} \\sum_{i=1}^{n}","657","    l(y_i, F_{m-1}(x_i) + h(x_i)),","659","where :math:`l(y_i, F(x_i))` is defined by the `loss` parameter, detailed","660","in the next section.","662","By default, the initial model :math:`F_{0}` is chosen as the constant that","663","minimizes the loss: for a least-squares loss, this is the empirical mean of","664","the target values. The initial model can also be specified via the ``init``","665","argument.","667","Using a first-order Taylor approximation, the value of :math:`l` can be","668","approximated as follows:","672","    l(y_i, F_{m-1}(x_i) + h_m(x_i)) \\approx","673","    l(y_i, F_{m-1}(x_i))","674","    + h_m(x_i)","675","    \\left[ \\frac{\\partial l(y_i, F(x_i))}{\\partial F(x_i)} \\right]_{F=F_{m - 1}}.","677",".. note::","678","","679","  Briefly, a first-order Taylor approximation says that","680","  :math:`l(z) \\approx l(a) + (z - a) \\frac{\\partial l(a)}{\\partial a}`.","681","  Here, :math:`z` corresponds to :math:`F_{m - 1}(x_i) + h_m(x_i)`, and","682","  :math:`a` corresponds to :math:`F_{m-1}(x_i)`","683","","684","The quantity :math:`\\left[ \\frac{\\partial l(y_i, F(x_i))}{\\partial F(x_i)}","685","\\right]_{F=F_{m - 1}}` is the derivative of the loss with respect to its","686","second parameter, evaluated at :math:`F_{m-1}(x)`. It is easy to compute for","687","any given :math:`F_{m - 1}(x_i)` in a closed form since the loss is","688","differentiable. We will denote it by :math:`g_i`.","689","","690","Removing the constant terms, we have:","694","    h_m \\approx \\arg\\min_{h} \\sum_{i=1}^{n} h(x_i) g_i","696","This is minimized if :math:`h(x_i)` is fitted to predict a value that is","697","proportional to the negative gradient :math:`-g_i`. Therefore, at each","698","iteration, **the estimator** :math:`h_m` **is fitted to predict the negative","699","gradients of the samples**. The gradients are updated at each iteration.","700","This can be considered as some kind of gradient descent in a functional","701","space.","702","","703",".. note::","704","","705","  For some losses, e.g. the least absolute deviation (LAD) where the gradients","706","  are :math:`\\pm 1`, the values predicted by a fitted :math:`h_m` are not","707","  accurate enough: the tree can only output integer values. As a result, the","708","  leaves values of the tree :math:`h_m` are modified once the tree is","709","  fitted, such that the leaves values minimize the loss :math:`L_m`. The","710","  update is loss-dependent: for the LAD loss, the value of a leaf is updated","711","  to the median of the samples in that leaf.","712","","713","Classification","714","^^^^^^^^^^^^^^","715","","716","Gradient boosting for classification is very similar to the regression case.","717","However, the sum of the trees :math:`F_M(x_i) = \\sum_m h_m(x_i)` is not","718","homogeneous to a prediction: it cannot be a class, since the trees predict","719","continuous values.","720","","721","The mapping from the value :math:`F_M(x_i)` to a class or a probability is","722","loss-dependent. For the deviance (or log-loss), the probability that","723",":math:`x_i` belongs to the positive class is modeled as :math:`p(y_i = 1 |","724","x_i) = \\sigma(F_M(x_i))` where :math:`\\sigma` is the sigmoid function.","725","","726","For multiclass classification, K trees (for K classes) are built at each of","727","the :math:`M` iterations. The probability that :math:`x_i` belongs to class","728","k is modeled as a softmax of the :math:`F_{M,k}(x_i)` values.","729","","730","Note that even for a classification task, the :math:`h_m` sub-estimator is","731","still a regressor, not a classifier. This is because the sub-estimators are","732","trained to predict (negative) *gradients*, which are always continuous","733","quantities.","738","--------------","780","Shrinkage via learning rate","781","---------------------------","784","the contribution of each weak learner by a constant factor :math:`\\nu`:","788","    F_m(x) = F_{m-1}(x) + \\nu h_m(x)","805","-----------","849","Interpretation with feature importance","850","--------------------------------------","886","Note that this computation of feature importance is based on entropy, and it","887","is distinct from :func:`sklearn.inspection.permutation_importance` which is","888","based on permutation of the features.","889","","1152","-----","1269","-----"],"delete":["511","The number of weak learners (i.e. regression trees) is controlled by the parameter ``n_estimators``; :ref:`The size of each tree <gradient_boosting_tree_size>` can be controlled either by setting the tree depth via ``max_depth`` or by setting the number of leaf nodes via ``max_leaf_nodes``. The ``learning_rate`` is a hyper-parameter in the range (0.0, 1.0] that controls overfitting via :ref:`shrinkage <gradient_boosting_shrinkage>` .","617","GBRT considers additive models of the following form:","621","    F(x) = \\sum_{m=1}^{M} \\gamma_m h_m(x)","623","where :math:`h_m(x)` are the basis functions which are usually called","624","*weak learners* in the context of boosting. Gradient Tree Boosting","625","uses :ref:`decision trees <tree>` of fixed size as weak","626","learners. Decision trees have a number of abilities that make them","627","valuable for boosting, namely the ability to handle data of mixed type","628","and the ability to model complex functions.","630","Similar to other boosting algorithms, GBRT builds the additive model in","631","a greedy fashion:","635","    F_m(x) = F_{m-1}(x) + \\gamma_m h_m(x),","637","where the newly added tree :math:`h_m` tries to minimize the loss :math:`L`,","638","given the previous ensemble :math:`F_{m-1}`:","642","    h_m =  \\arg\\min_{h} \\sum_{i=1}^{n} L(y_i,","643","    F_{m-1}(x_i) + h(x_i)).","645","The initial model :math:`F_{0}` is problem specific, for least-squares","646","regression one usually chooses the mean of the target values.","648",".. note:: The initial model can also be specified via the ``init``","649","          argument. The passed object has to implement ``fit`` and ``predict``.","651","Gradient Boosting attempts to solve this minimization problem","652","numerically via steepest descent: The steepest descent direction is","653","the negative gradient of the loss function evaluated at the current","654","model :math:`F_{m-1}` which can be calculated for any differentiable","655","loss function:","659","    F_m(x) = F_{m-1}(x) - \\gamma_m \\sum_{i=1}^{n} \\nabla_F L(y_i,","660","    F_{m-1}(x_i))","662","Where the step length :math:`\\gamma_m` is chosen using line search:","666","    \\gamma_m = \\arg\\min_{\\gamma} \\sum_{i=1}^{n} L(y_i, F_{m-1}(x_i)","667","    - \\gamma \\frac{\\partial L(y_i, F_{m-1}(x_i))}{\\partial F_{m-1}(x_i)})","669","The algorithms for regression and classification","670","only differ in the concrete loss function used.","675","...............","715","Regularization","716","----------------","717","","720","Shrinkage","721","..........","724","the contribution of each weak learner by a factor :math:`\\nu`:","728","    F_m(x) = F_{m-1}(x) + \\nu \\gamma_m h_m(x)","745","............","789","Interpretation","790","--------------","799","Feature importance","800","..................","801","","1091",".....","1208","....."]}]}}}