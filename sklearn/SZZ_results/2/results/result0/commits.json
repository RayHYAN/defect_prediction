{"a6a20f29d7baf51960172aa64b981ffb240d3ebd":{"changes":{"sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_loss.py":"MODIFY"},"diff":{"sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_loss.py":[{"add":["4","from scipy.special import logit","52","    # The argmin of binary_crossentropy for y_true=0 and y_true=1 is resp. -inf","53","    # and +inf due to logit, cf. \"complete separation\". Therefore, we use","54","    # 0 < y_true < 1.","55","    ('binary_crossentropy', 0.3, 0.1),","56","    ('binary_crossentropy', -12, 0.2),","57","    ('binary_crossentropy', 30, 0.9),","66","    # Check that gradients are zero when the loss is minimized on a single","67","    # value\/sample using Halley's method with the first and second order","68","    # derivatives computed by the Loss instance.","69","    # Note that methods of Loss instances operate on arrays while the newton","70","    # root finder expects a scalar or a one-element array for this purpose.","77","    def func(x: np.ndarray) -> np.ndarray:","78","        if isinstance(loss, _LOSSES['binary_crossentropy']):","79","            # Subtract a constant term such that the binary cross entropy","80","            # has its minimum at zero, which is needed for the newton method.","81","            actual_min = loss.pointwise_loss(y_true, logit(y_true))","82","            return loss.pointwise_loss(y_true, x) - actual_min","83","        else:","84","            return loss.pointwise_loss(y_true, x)","86","    def fprime(x: np.ndarray) -> np.ndarray:","89","    def fprime2(x: np.ndarray) -> np.ndarray:","94","","95","    # Need to ravel arrays because assert_allclose requires matching dimensions","96","    y_true = y_true.ravel()","97","    optimum = optimum.ravel()","98","    assert_allclose(loss.inverse_link_function(optimum), y_true)","99","    assert_allclose(func(optimum), 0, atol=1e-14)","100","    assert_allclose(get_gradients(y_true, optimum), 0, atol=1e-7)"],"delete":["51","    # I don't understand why but y_true == 0 fails :\/","52","    # ('binary_crossentropy', 0.3, 0),","53","    ('binary_crossentropy', -12, 1),","54","    ('binary_crossentropy', 30, 1),","63","    # Check that gradients are zero when the loss is minimized on 1D array","64","    # using Halley's method with the first and second order derivatives","65","    # computed by the Loss instance.","72","    def func(x):","73","        return loss.pointwise_loss(y_true, x)","75","    def fprime(x):","78","    def fprime2(x):","83","    assert np.allclose(loss.inverse_link_function(optimum), y_true)","84","    assert np.allclose(loss.pointwise_loss(y_true, optimum), 0)","85","    assert np.allclose(get_gradients(y_true, optimum), 0, atol=1e-7)"]}]}},"cac167244d9e8dc57087f63d55dadfc1bc0ce09c":{"changes":{"doc\/developers\/advanced_installation.rst":"MODIFY","sklearn\/_build_utils\/openmp_helpers.py":"MODIFY",".travis.yml":"MODIFY","build_tools\/travis\/install.sh":"MODIFY","build_tools\/travis\/test_script.sh":"MODIFY","build_tools\/travis\/test_docs.sh":"MODIFY","sklearn\/_build_utils\/pre_build_helpers.py":"MODIFY"},"diff":{"doc\/developers\/advanced_installation.rst":[{"add":["390","","391","Alternative compilers","392","=====================","393","","394","The command::","395","","396","    pip install --verbose --editable .","397","","398","will build scikit-learn using your default C\/C++ compiler. If you want to build","399","scikit-learn with another compiler handled by ``distutils`` or by","400","``numpy.distutils``, use the following command::","401","","402","    python setup.py build_ext --compiler=<compiler> -i build_clib --compiler=<compiler>","403","","404","To see the list of available compilers run::","405","","406","    python setup.py build_ext --help-compiler","407","","408","If your compiler is not listed here, you can specify it via the ``CC`` and","409","``LDSHARED`` environment variables (does not work on windows)::","410","","411","    CC=<compiler> LDSHARED=\"<compiler> -shared\" python setup.py build_ext -i","412","","413","Building with Intel C Compiler (ICC) using oneAPI on Linux","414","----------------------------------------------------------","415","","416","Intel provides access to all of its oneAPI toolkits and packages through a","417","public APT repository. First you need to get and install the public key of this","418","repository::","419","","420","    wget https:\/\/apt.repos.intel.com\/intel-gpg-keys\/GPG-PUB-KEY-INTEL-SW-PRODUCTS-2023.PUB","421","    sudo apt-key add GPG-PUB-KEY-INTEL-SW-PRODUCTS-2023.PUB","422","    rm GPG-PUB-KEY-INTEL-SW-PRODUCTS-2023.PUB","423","","424","Then, add the oneAPI repository to your APT repositories::","425","","426","    sudo add-apt-repository \"deb https:\/\/apt.repos.intel.com\/oneapi all main\"","427","    sudo apt-get update","428","","429","Install ICC, packaged under the name ``intel-oneapi-icc``::","430","","431","    sudo apt-get install intel-oneapi-icc","432","","433","Before using ICC, you need to set up environment variables::","434","","435","    source \/opt\/intel\/inteloneapi\/setvars.sh","436","","437","Finally, you can build scikit-learn. For example on Linux x86_64::","438","","439","    python setup.py build_ext --compiler=intelem -i build_clib --compiler=intelem"],"delete":[]}],"sklearn\/_build_utils\/openmp_helpers.py":[{"add":["3","# can be found at: https:\/\/github.com\/astropy\/extension-helpers\/blob\/master\/extension_helpers\/_openmp_helpers.py  # noqa","27","    elif sys.platform in (\"darwin\", \"linux\") and \"icc\" in compiler:","28","        return ['-qopenmp']"],"delete":["3","# can be found at: https:\/\/github.com\/astropy\/astropy-helpers\/blob\/master\/astropy_helpers\/openmp_helpers.py  # noqa","27","    elif sys.platform == \"darwin\" and ('icc' in compiler or 'icl' in compiler):","28","        return ['-openmp']"]}],".travis.yml":[{"add":["24","    - python: 3.7","25","      env: CHECK_WARNINGS=\"true\"","26","      if: type = cron OR commit_message =~ \/\\[scipy-dev\\]\/","27","    ","28","    # As above but build scikit-learn with Intel C compiler (ICC).","29","    - python: 3.7","30","      env:","31","        - CHECK_WARNING=\"true\"","32","        - BUILD_WITH_ICC=\"true\"","33","      if: type = cron OR commit_message =~ \/\\[icc-build\\]\/"],"delete":["24","    -  python: 3.7","25","       env: CHECK_WARNINGS=\"true\"","26","       if: type = cron OR commit_message =~ \/\\[scipy-dev\\]\/"]}],"build_tools\/travis\/install.sh":[{"add":["68","if [[ \"$BUILD_WITH_ICC\" == \"true\" ]]; then","69","    wget https:\/\/apt.repos.intel.com\/intel-gpg-keys\/GPG-PUB-KEY-INTEL-SW-PRODUCTS-2023.PUB","70","    sudo apt-key add GPG-PUB-KEY-INTEL-SW-PRODUCTS-2023.PUB","71","    rm GPG-PUB-KEY-INTEL-SW-PRODUCTS-2023.PUB","72","    sudo add-apt-repository \"deb https:\/\/apt.repos.intel.com\/oneapi all main\"","73","    sudo apt-get update","74","    sudo apt-get install intel-oneapi-icc","75","    source \/opt\/intel\/inteloneapi\/setvars.sh","76","","77","    # The build_clib command is implicitly used to build libsvm-skl. To compile","78","    # with a different compiler we also need to specify the compiler for this","79","    # command.","80","    python setup.py build_ext --compiler=intelem -i -j 3 build_clib --compiler=intelem","81","else","82","    # Use setup.py instead of `pip install -e .` to be able to pass the -j flag","83","    # to speed-up the building multicore CI machines.","84","    python setup.py build_ext --inplace -j 3","85","fi","86",""],"delete":[]}],"build_tools\/travis\/test_script.sh":[{"add":["22","if [[ \"$BUILD_WITH_ICC\" == \"true\" ]]; then","23","    # the tools in the oneAPI toolkits are configured via environment variables","24","    # which are also required at runtime.","25","    source \/opt\/intel\/inteloneapi\/setvars.sh","26","fi","27",""],"delete":[]}],"build_tools\/travis\/test_docs.sh":[{"add":["5","if [[ \"$BUILD_WITH_ICC\" == \"true\" ]]; then","6","    # the tools in the oneAPI toolkits are configured via environment variables","7","    # which are also required at runtime.","8","    source \/opt\/intel\/inteloneapi\/setvars.sh","9","fi","10",""],"delete":[]}],"sklearn\/_build_utils\/pre_build_helpers.py":[{"add":["9","from distutils.dist import Distribution","12","from numpy.distutils.command.config_compiler import config_cc","13","","14","","15","def _get_compiler():","16","    \"\"\"Get a compiler equivalent to the one that will be used to build sklearn","17","","18","    Handles compiler specified as follows:","19","        - python setup.py build_ext --compiler=<compiler>","20","        - CC=<compiler> python setup.py build_ext","21","    \"\"\"","22","    dist = Distribution({'script_name': os.path.basename(sys.argv[0]),","23","                         'script_args': sys.argv[1:],","24","                         'cmdclass': {'config_cc': config_cc}})","25","    dist.parse_config_files()","26","    dist.parse_command_line()","27","","28","    cmd_opts = dist.command_options.get('build_ext')","29","    if cmd_opts is not None and 'compiler' in cmd_opts:","30","        compiler = cmd_opts['compiler'][1]","31","    else:","32","        compiler = None","33","","34","    ccompiler = new_compiler(compiler=compiler)","35","    customize_compiler(ccompiler)","36","","37","    return ccompiler","42","    ccompiler = _get_compiler()"],"delete":["15","    ccompiler = new_compiler()","16","    customize_compiler(ccompiler)"]}]}},"d7c375869ada53040f035f6fb0eb3b8d2d5dfff2":{"changes":{"sklearn\/neural_network\/_base.py":"MODIFY","doc\/whats_new\/v0.23.rst":"MODIFY","sklearn\/neural_network\/tests\/test_base.py":"ADD"},"diff":{"sklearn\/neural_network\/_base.py":[{"add":["214","    eps = np.finfo(y_prob.dtype).eps","215","    y_prob = np.clip(y_prob, eps, 1 - eps)","236","    y_prob : array-like of float, shape = (n_samples, 1)","245","    eps = np.finfo(y_prob.dtype).eps","246","    y_prob = np.clip(y_prob, eps, 1 - eps)"],"delete":["234","    y_prob : array-like of float, shape = (n_samples, n_classes)"]}],"doc\/whats_new\/v0.23.rst":[{"add":["147",":mod:`sklearn.neural_network`","148",".............................","149","","150","- |Fix| Increases the numerical stability of the logistic loss function in","151","  :class:`neural_network.MLPClassifier` by clipping the probabilities.","152","  :pr:`16117` by `Thomas Fan`_.","153",""],"delete":[]}],"sklearn\/neural_network\/tests\/test_base.py":[{"add":[],"delete":[]}]}},"865069c234f0f7ea23e50c47b171b186a684e2a7":{"changes":{"sklearn\/base.py":"MODIFY","sklearn\/cluster\/_birch.py":"MODIFY"},"diff":{"sklearn\/base.py":[{"add":["56","    estimator : {list, tuple, set} of estimator objects or estimator object","57","        The estimator or group of estimators to be cloned.","627","        shape : tuple (int, int)","671","        X : {array-like, sparse matrix, dataframe} of shape \\","672","                (n_samples, n_features)","727","        X : {array-like, sparse matrix, dataframe} of shape \\","728","            (n_samples, n_features)"],"delete":["56","    estimator : estimator object, or list, tuple or set of objects","57","        The estimator or group of estimators to be cloned","627","        shape : (int, int)","671","        X : ndarray of shape (n_samples, n_features)","672","            Training set.","727","        X : ndarray of shape (n_samples, n_features)","728","            Input data."]}],"sklearn\/cluster\/_birch.py":[{"add":["114","    subclusters_ : list","115","        List of subclusters for a particular CFNode.","118","        Useful only if is_leaf is True.","124","    init_centroids_ : ndarray of shape (branching_factor + 1, n_features)","125","        Manipulate ``init_centroids_`` throughout rather than centroids_ since","128","    init_sq_norm_ : ndarray of shape (branching_factor + 1,)","131","    centroids_ : ndarray of shape (branching_factor + 1, n_features)","132","        View of ``init_centroids_``.","134","    squared_norm_ : ndarray of shape (branching_factor + 1,)","135","        View of ``init_sq_norm_``.","251","    linear_sum : ndarray of shape (n_features,), default=None","267","    centroid_ : ndarray of shape (branching_factor + 1, n_features)","275","    sq_norm_ : ndarray of shape (branching_factor + 1,)","382","    subcluster_centers_ : ndarray","385","    subcluster_labels_ : ndarray","389","    labels_ : ndarray of shape (n_samples,)","450","        X : {array-like, sparse matrix} of shape (n_samples, n_features)","525","        leaves : list of shape (n_leaves,)","541","        X : {array-like, sparse matrix} of shape (n_samples, n_features), \\","542","            default=None","580","        X : {array-like, sparse matrix} of shape (n_samples, n_features)","585","        labels : ndarray of shape(n_samples,)","606","        X : {array-like, sparse matrix} of shape (n_samples, n_features)","611","        X_trans : {array-like, sparse matrix} of shape (n_samples, n_clusters)"],"delete":["114","    subclusters_ : array-like","115","        list of subclusters for a particular CFNode.","118","        prev_leaf. Useful only if is_leaf is True.","124","    init_centroids_ : ndarray, shape (branching_factor + 1, n_features)","125","        manipulate ``init_centroids_`` throughout rather than centroids_ since","128","    init_sq_norm_ : ndarray, shape (branching_factor + 1,)","131","    centroids_ : ndarray","132","        view of ``init_centroids_``.","134","    squared_norm_ : ndarray","135","        view of ``init_sq_norm_``.","251","    linear_sum : ndarray, shape (n_features,), optional","267","    centroid_ : ndarray","275","    sq_norm_ : ndarray","382","    subcluster_centers_ : ndarray,","385","    subcluster_labels_ : ndarray,","389","    labels_ : ndarray, shape (n_samples,)","450","        X : {array-like, sparse matrix}, shape (n_samples, n_features)","525","        leaves : array-like","541","        X : {array-like, sparse matrix}, shape (n_samples, n_features), None","579","        X : {array-like, sparse matrix}, shape (n_samples, n_features)","584","        labels : ndarray, shape(n_samples)","605","        X : {array-like, sparse matrix}, shape (n_samples, n_features)","610","        X_trans : {array-like, sparse matrix}, shape (n_samples, n_clusters)"]}]}},"1b1c869ef3d5443d2104a1ea444fa41b86d6e3a7":{"changes":{"sklearn\/utils\/validation.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY","sklearn\/utils\/tests\/test_validation.py":"MODIFY"},"diff":{"sklearn\/utils\/validation.py":[{"add":["456","        dtypes_orig = list(array.dtypes)","457","        # pandas boolean dtype __array__ interface coerces bools to objects","458","        for i, dtype_iter in enumerate(dtypes_orig):","459","            if dtype_iter.kind == 'b':","460","                dtypes_orig[i] = np.object","461","","463","            dtype_orig = np.result_type(*dtypes_orig)"],"delete":["456","        dtypes_orig = np.array(array.dtypes)","458","            dtype_orig = np.result_type(*array.dtypes)"]}],"doc\/whats_new\/v0.22.rst":[{"add":["4",".. _changes_0_22_1:","5","","6","Version 0.22.1","7","==============","8","","9","**In Development**","10","","11","This is a bug-fix release to primarily resolve some packaging issues in version","12","0.22.0. It also includes minor documentation improvements and some bug fixes.","13","","14","Changelog","15","---------","16","","17",":mod:`sklearn.utils`","18","....................","19","","20","- |Fix| :func:`utils.check_array` now correctly converts pandas DataFrame with","21","  boolean columns to floats. :pr:`15797` by `Thomas Fan`_.","22",""],"delete":[]}],"sklearn\/utils\/tests\/test_validation.py":[{"add":["828","def test_check_dataframe_mixed_float_dtypes():","829","    # pandas dataframe will coerce a boolean into a object, this is a mismatch","830","    # with np.result_type which will return a float","831","    # check_array needs to explicitly check for bool dtype in a dataframe for","832","    # this situation","833","    # https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/15787","834","","835","    pd = importorskip(\"pandas\")","836","    df = pd.DataFrame({","837","        'int': [1, 2, 3],","838","        'float': [0, 0.1, 2.1],","839","        'bool': [True, False, True]}, columns=['int', 'float', 'bool'])","840","","841","    array = check_array(df, dtype=(np.float64, np.float32, np.float16))","842","    expected_array = np.array(","843","        [[1.0, 0.0, 1.0],","844","         [2.0, 0.1, 0.0],","845","         [3.0, 2.1, 1.0]], dtype=np.float)","846","    assert_allclose_dense_sparse(array, expected_array)","847","","848",""],"delete":[]}]}},"a83b8e0d486acb6d0958602dc3b8b48de151e44a":{"changes":{"examples\/inspection\/plot_permutation_importance_multicollinear.py":"MODIFY"},"diff":{"examples\/inspection\/plot_permutation_importance_multicollinear.py":[{"add":["62","ax1.set_yticklabels(data.feature_names[tree_importance_sorted_idx])","66","            labels=data.feature_names[perm_sorted_idx])"],"delete":["62","ax1.set_yticklabels(data.feature_names)","66","            labels=data.feature_names)"]}]}},"4c29be44facbdaef188f84bdc8bf1190b2eebe07":{"changes":{"doc\/whats_new\/v0.23.rst":"MODIFY","sklearn\/gaussian_process\/tests\/test_gpr.py":"MODIFY","sklearn\/gaussian_process\/_gpr.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.23.rst":[{"add":["198","  :pr:`15503` by :user:`Sam Dixon` <sam-dixon>.","199","","200","- |Fix| Fixed bug in :class:`gaussian_process.GaussianProcessRegressor` that","201","  caused predicted standard deviations to only be between 0 and 1 when","202","  WhiteKernel is not used. :pr:`15782`","203","  by :user:`plgreenLIRU`.","399","- |Fix| :class:`cluster.AgglomerativeClustering` add specific error when","400","  distance matrix is not square and `affinity=precomputed`."],"delete":["198","  :pr:`15503` by :user:`Sam Dixon <sam-dixon>`.","220","","395","- |Fix| :class:`cluster.AgglomerativeClustering` add specific error when ","396","  distance matrix is not square and `affinity=precomputed`. "]}],"sklearn\/gaussian_process\/tests\/test_gpr.py":[{"add":["3","# Modified by: Pete Green <p.l.green@liverpool.ac.uk>","22","            assert_array_almost_equal, assert_array_equal,","23","            assert_allclose)","236","    \"\"\"","237","    Test normalization of the target values in GP","239","    Fitting non-normalizing GP on normalized y and fitting normalizing GP","240","    on unnormalized y should yield identical results. Note that, here,","241","    'normalized y' refers to y that has been made zero mean and unit","242","    variance.","243","","244","    \"\"\"","245","","246","    y_mean = np.mean(y)","247","    y_std = np.std(y)","248","    y_norm = (y - y_mean) \/ y_std","253","","260","    y_pred = y_pred * y_std + y_mean","261","    y_pred_std = y_pred_std * y_std","268","    y_cov = y_cov * y_std**2","270","","274","def test_large_variance_y():","275","    \"\"\"","276","    Here we test that, when noramlize_y=True, our GP can produce a","277","    sensible fit to training data whose variance is significantly","278","    larger than unity. This test was made in response to issue #15612.","279","","280","    GP predictions are verified against predictions that were made","281","    using GPy which, here, is treated as the 'gold standard'. Note that we","282","    only investigate the RBF kernel here, as that is what was used in the","283","    GPy implementation.","284","","285","    The following code can be used to recreate the GPy data:","286","","287","    --------------------------------------------------------------------------","288","    import GPy","289","","290","    kernel_gpy = GPy.kern.RBF(input_dim=1, lengthscale=1.)","291","    gpy = GPy.models.GPRegression(X, np.vstack(y_large), kernel_gpy)","292","    gpy.optimize()","293","    y_pred_gpy, y_var_gpy = gpy.predict(X2)","294","    y_pred_std_gpy = np.sqrt(y_var_gpy)","295","    --------------------------------------------------------------------------","296","    \"\"\"","297","","298","    # Here we utilise a larger variance version of the training data","299","    y_large = 10 * y","300","","301","    # Standard GP with normalize_y=True","302","    RBF_params = {'length_scale': 1.0}","303","    kernel = RBF(**RBF_params)","304","    gpr = GaussianProcessRegressor(kernel=kernel, normalize_y=True)","305","    gpr.fit(X, y_large)","306","    y_pred, y_pred_std = gpr.predict(X2, return_std=True)","307","","308","    # 'Gold standard' mean predictions from GPy","309","    y_pred_gpy = np.array([15.16918303,","310","                           -27.98707845,","311","                           -39.31636019,","312","                           14.52605515,","313","                           69.18503589])","314","","315","    # 'Gold standard' std predictions from GPy","316","    y_pred_std_gpy = np.array([7.78860962,","317","                               3.83179178,","318","                               0.63149951,","319","                               0.52745188,","320","                               0.86170042])","321","","322","    # Based on numerical experiments, it's reasonable to expect our","323","    # GP's mean predictions to get within 7% of predictions of those","324","    # made by GPy.","325","    assert_allclose(y_pred, y_pred_gpy, rtol=0.07, atol=0)","326","","327","    # Based on numerical experiments, it's reasonable to expect our","328","    # GP's std predictions to get within 15% of predictions of those","329","    # made by GPy.","330","    assert_allclose(y_pred_std, y_pred_std_gpy, rtol=0.15, atol=0)","331","","332",""],"delete":["21","            assert_array_almost_equal, assert_array_equal)","234","    # Test normalization of the target values in GP","236","    # Fitting non-normalizing GP on normalized y and fitting normalizing GP","237","    # on unnormalized y should yield identical results","238","    y_mean = y.mean(0)","239","    y_norm = y - y_mean","250","    y_pred = y_mean + y_pred"]}],"sklearn\/gaussian_process\/_gpr.py":[{"add":["3","# Modified by: Pete Green <p.l.green@liverpool.ac.uk>","94","    normalize_y : boolean, optional (default: False)","95","        Whether the target values y are normalized, the mean and variance of","96","        the target values are set equal to 0 and 1 respectively. This is","97","        recommended for cases where zero-mean, unit-variance priors are used.","98","        Note that, in this implementation, the normalisation is reversed","99","        before the GP predictions are reported.","100","","101","        .. versionchanged:: 0.23","195","            self._y_train_std = np.std(y, axis=0)","196","","197","            # Remove mean and make unit variance","198","            y = (y - self._y_train_mean) \/ self._y_train_std","199","","202","            self._y_train_std = 1","337","","338","            # undo normalisation","339","            y_mean = self._y_train_std * y_mean + self._y_train_mean","340","","344","","345","                # undo normalisation","346","                y_cov = y_cov * self._y_train_std**2","347","","370","","371","                # undo normalisation","372","                y_var = y_var * self._y_train_std**2","373",""],"delete":["3","#","94","    normalize_y : bool, default=False","95","        Whether the target values y are normalized, i.e., the mean of the","96","        observed target values become zero. This parameter should be set to","97","        True if the target values' mean is expected to differ considerable from","98","        zero. When enabled, the normalization effectively modifies the GP's","99","        prior based on the data, which contradicts the likelihood principle;","100","        normalization is thus disabled per default.","194","            # demean y","195","            y = y - self._y_train_mean","332","            y_mean = self._y_train_mean + y_mean  # undo normal."]}]}},"c96e0958da46ebef482a4084cdda3285d5f5ad23":{"changes":{"sklearn\/impute\/tests\/test_impute.py":"MODIFY","sklearn\/impute\/_iterative.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY"},"diff":{"sklearn\/impute\/tests\/test_impute.py":[{"add":["28","from sklearn.exceptions import ConvergenceWarning","866","                               imputation_order='descending',","1275","","1276","","1277","@pytest.mark.parametrize(","1278","    \"order, idx_order\",","1279","    [","1280","        (\"ascending\", [3, 4, 2, 0, 1]),","1281","        (\"descending\", [1, 0, 2, 4, 3])","1282","    ]","1283",")","1284","def test_imputation_order(order, idx_order):","1285","    # regression test for #15393","1286","    rng = np.random.RandomState(42)","1287","    X = rng.rand(100, 5)","1288","    X[:50, 1] = np.nan","1289","    X[:30, 0] = np.nan","1290","    X[:20, 2] = np.nan","1291","    X[:10, 4] = np.nan","1292","","1293","    with pytest.warns(ConvergenceWarning):","1294","        trs = IterativeImputer(max_iter=1,","1295","                               imputation_order=order,","1296","                               random_state=0).fit(X)","1297","        idx = [x.feat_idx for x in trs.imputation_sequence_]","1298","        assert idx == idx_order"],"delete":[]}],"sklearn\/impute\/_iterative.py":[{"add":["422","                                     kind='mergesort')[n:]","426","                                     kind='mergesort')[n:][::-1]"],"delete":["422","                                     kind='mergesort')[n:][::-1]","426","                                     kind='mergesort')[n:]"]}],"doc\/whats_new\/v0.22.rst":[{"add":["429","- |Fix| Fixed a bug in :class:`impute.IterativeImputer` where features where","430","  imputed in the reverse desired order with ``imputation_order`` either","431","  ``\"ascending\"`` or ``\"descending\"``. :pr:`15393` by","432","  :user:`Venkatachalam N <venkyyuvy>`.","433",""],"delete":[]}]}},"0831dfbe9595e54587f92dcafd30d030dcfcc631":{"changes":{"doc\/whats_new\/v0.22.rst":"MODIFY","sklearn\/cross_decomposition\/_pls.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.22.rst":[{"add":["103","- :class:`cross_decomposition.CCA` when using scipy >= 1.3 |Fix|","212","- |Fix| :class:`cross_decomposition.CCA` now produces the same results with ","213","  scipy 1.3 and previous scipy versions. :pr:`15661` by `Thomas Fan`_.","214",""],"delete":[]}],"sklearn\/cross_decomposition\/_pls.py":[{"add":["42","","43","    if mode == \"B\":","44","        # Uses condition from scipy<1.3 in pinv2 which was changed in","45","        # https:\/\/github.com\/scipy\/scipy\/pull\/10067. In scipy 1.3, the","46","        # condition was changed to depend on the largest singular value","47","        X_t = X.dtype.char.lower()","48","        Y_t = Y.dtype.char.lower()","49","        factor = {'f': 1E3, 'd': 1E6}","50","","51","        cond_X = factor[X_t] * eps","52","        cond_Y = factor[Y_t] * eps","53","","61","                X_pinv = pinv2(X, check_finite=False, cond=cond_X)","78","                # compute once pinv(Y)","79","                Y_pinv = pinv2(Y, check_finite=False, cond=cond_Y)"],"delete":["49","                X_pinv = pinv2(X, check_finite=False)","66","                Y_pinv = pinv2(Y, check_finite=False)  # compute once pinv(Y)"]}]}},"91261c2013795a319f3075661827f631e2f883cb":{"changes":{"sklearn\/cluster\/tests\/test_k_means.py":"MODIFY","doc\/whats_new\/v0.23.rst":"MODIFY","sklearn\/cluster\/_k_means_elkan.pyx":"MODIFY"},"diff":{"sklearn\/cluster\/tests\/test_k_means.py":[{"add":["71","@pytest.mark.parametrize('tol', [0, 1e-2, 1e-4, 1e-8])"],"delete":["71","@pytest.mark.parametrize('tol', [1e-2, 1e-4, 1e-8])"]}],"doc\/whats_new\/v0.23.rst":[{"add":["52","- |Fix| :class:`cluster.KMeans` with ``algorithm=\"elkan\"`` now converges with","53","  ``tol=0`` as with the default ``algorithm=\"full\"``. :pr:`16075` by","54","  :user:`Erich Schubert <kno10>`."],"delete":[]}],"sklearn\/cluster\/_k_means_elkan.pyx":[{"add":["247","        if center_shift_total <= tol:"],"delete":["247","        if center_shift_total < tol:"]}]}},"29932e690e00fa0c610a8fba2dfb0abc1d66cbac":{"changes":{"sklearn\/neighbors\/_base.py":"MODIFY"},"diff":{"sklearn\/neighbors\/_base.py":[{"add":["278","        neigh_dist = np.array(np.split(data, indptr[1:-1]), dtype=object)","279","    neigh_ind = np.array(np.split(indices, indptr[1:-1]), dtype=object)"],"delete":["278","        neigh_dist = np.array(np.split(data, indptr[1:-1]))","279","    neigh_ind = np.array(np.split(indices, indptr[1:-1]))"]}]}},"c4ea377198f4289af16d33f60b74c6258158bf9f":{"changes":{"sklearn\/preprocessing\/tests\/test_label.py":"MODIFY","sklearn\/utils\/tests\/test_utils.py":"MODIFY","sklearn\/neighbors\/tests\/test_neighbors.py":"MODIFY","sklearn\/neighbors\/_base.py":"MODIFY","doc\/whats_new\/v0.23.rst":"MODIFY","sklearn\/utils\/__init__.py":"MODIFY"},"diff":{"sklearn\/preprocessing\/tests\/test_label.py":[{"add":["16","from sklearn.utils import _to_object_array","436","    tuple_classes = _to_object_array([(1,), (2,), (3,)])"],"delete":["435","    tuple_classes = np.empty(3, dtype=object)","436","    tuple_classes[:] = [(1,), (2,), (3,)]"]}],"sklearn\/utils\/tests\/test_utils.py":[{"add":["30","from sklearn.utils import _to_object_array","649","","650","","651","@pytest.mark.parametrize(","652","    \"sequence\",","653","    [[np.array(1), np.array(2)], [[1, 2], [3, 4]]]","654",")","655","def test_to_object_array(sequence):","656","    out = _to_object_array(sequence)","657","    assert isinstance(out, np.ndarray)","658","    assert out.dtype.kind == 'O'","659","    assert out.ndim == 1"],"delete":[]}],"sklearn\/neighbors\/tests\/test_neighbors.py":[{"add":["651","def test_radius_neighbors_returns_array_of_objects():","652","    # check that we can pass precomputed distances to","653","    # NearestNeighbors.radius_neighbors()","654","    # non-regression test for","655","    # https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/16036","656","    X = csr_matrix(np.ones((4, 4)))","657","    X.setdiag([0, 0, 0, 0])","658","","659","    nbrs = neighbors.NearestNeighbors(radius=0.5, algorithm='auto',","660","                                      leaf_size=30,","661","                                      metric='precomputed').fit(X)","662","    neigh_dist, neigh_ind = nbrs.radius_neighbors(X, return_distance=True)","663","","664","    expected_dist = np.empty(X.shape[0], dtype=object)","665","    expected_dist[:] = [np.array([0]), np.array([0]), np.array([0]),","666","                        np.array([0])]","667","    expected_ind = np.empty(X.shape[0], dtype=object)","668","    expected_ind[:] = [np.array([0]), np.array([1]), np.array([2]),","669","                       np.array([3])]","670","","671","    assert_array_equal(neigh_dist, expected_dist)","672","    assert_array_equal(neigh_ind, expected_ind)","673","","674",""],"delete":[]}],"sklearn\/neighbors\/_base.py":[{"add":["26","from ..utils import _to_object_array","279","        neigh_dist = _to_object_array(np.split(data, indptr[1:-1]))","280","    neigh_ind = _to_object_array(np.split(indices, indptr[1:-1]))","943","                neigh_dist = _to_object_array(neigh_dist_list)","944","                neigh_ind = _to_object_array(neigh_ind_list)","948","                results = _to_object_array(neigh_ind_list)"],"delete":["278","        neigh_dist = np.array(np.split(data, indptr[1:-1]), dtype=object)","279","    neigh_ind = np.array(np.split(indices, indptr[1:-1]), dtype=object)","942","                # See https:\/\/github.com\/numpy\/numpy\/issues\/5456","943","                # to understand why this is initialized this way.","944","                neigh_dist = np.empty(len(neigh_dist_list), dtype='object')","945","                neigh_dist[:] = neigh_dist_list","946","                neigh_ind = np.empty(len(neigh_ind_list), dtype='object')","947","                neigh_ind[:] = neigh_ind_list","951","                results = np.empty(len(neigh_ind_list), dtype='object')","952","                results[:] = neigh_ind_list"]}],"doc\/whats_new\/v0.23.rst":[{"add":["138",":mod:`sklearn.neighbors`","139","..............................","140","","141","- |Fix| Fix a bug which converted a list of arrays into a 2-D object ","142","  array instead of a 1-D array containing NumPy arrays. This bug","143","  was affecting :meth:`neighbors.NearestNeighbors.radius_neighbors`.","144","  :pr:`16076` by :user:`Guillaume Lemaitre <glemaitre>` and  ","145","  :user:`Alex Shacked <alexshacked>`.","146",""],"delete":[]}],"sklearn\/utils\/__init__.py":[{"add":["821","def _to_object_array(sequence):","822","    \"\"\"Convert sequence to a 1-D NumPy array of object dtype.","823","","824","    numpy.array constructor has a similar use but it's output","825","    is ambiguous. It can be 1-D NumPy array of object dtype if","826","    the input is a ragged array, but if the input is a list of","827","    equal length arrays, then the output is a 2D numpy.array.","828","    _to_object_array solves this ambiguity by guarantying that","829","    the output is a 1-D NumPy array of objects for any input.","830","","831","    Parameters","832","    ----------","833","    sequence : array-like of shape (n_elements,)","834","        The sequence to be converted.","835","","836","    Returns","837","    -------","838","    out : ndarray of shape (n_elements,), dtype=object","839","        The converted sequence into a 1-D NumPy array of object dtype.","840","","841","    Examples","842","    --------","843","    >>> import numpy as np","844","    >>> from sklearn.utils import _to_object_array","845","    >>> _to_object_array([np.array([0]), np.array([1])])","846","    array([array([0]), array([1])], dtype=object)","847","    >>> _to_object_array([np.array([0]), np.array([1, 2])])","848","    array([array([0]), array([1, 2])], dtype=object)","849","    >>> np.array([np.array([0]), np.array([1])])","850","    array([[0],","851","       [1]])","852","    >>> np.array([np.array([0]), np.array([1, 2])])","853","    array([array([0]), array([1, 2])], dtype=object)","854","    \"\"\"","855","    out = np.empty(len(sequence), dtype=object)","856","    out[:] = sequence","857","    return out","858","","859",""],"delete":[]}]}},"5e2d74bc5f61b382758c0403c577253539e77156":{"changes":{"sklearn\/model_selection\/_validation.py":"MODIFY","sklearn\/multioutput.py":"MODIFY","sklearn\/neighbors\/_nearest_centroid.py":"MODIFY","sklearn\/decomposition\/_lda.py":"MODIFY","sklearn\/feature_extraction\/_hash.py":"MODIFY","sklearn\/multiclass.py":"MODIFY","sklearn\/model_selection\/_search.py":"MODIFY"},"diff":{"sklearn\/model_selection\/_validation.py":[{"add":["136","        .. versionadded:: 0.19","137","","138","        .. versionchanged:: 0.21","139","            Default value was changed from ``True`` to ``False``","140",""],"delete":[]}],"sklearn\/multioutput.py":[{"add":["364","","365","            .. versionchanged:: 0.19","366","                This function now returns a list of arrays where the length of","367","                the list is ``n_outputs``, and each array is (``n_samples``,","368","                ``n_classes``) for that particular output."],"delete":[]}],"sklearn\/neighbors\/_nearest_centroid.py":[{"add":["43","        .. versionchanged:: 0.19","44","            ``metric='precomputed'`` was deprecated and now raises an error","45",""],"delete":[]}],"sklearn\/decomposition\/_lda.py":[{"add":["145","        .. versionchanged:: 0.19","146","            ``n_topics `` was renamed to ``n_components``","147",""],"delete":[]}],"sklearn\/feature_extraction\/_hash.py":[{"add":["71","    .. versionchanged:: 0.19","72","        ``alternate_sign`` replaces the now deprecated ``non_negative``","73","        parameter.","74",""],"delete":[]}],"sklearn\/multiclass.py":[{"add":["392","","393","            .. versionchanged:: 0.19","394","                output shape changed to ``(n_samples,)`` to conform to","395","                scikit-learn conventions for binary classification.","649","","650","            .. versionchanged:: 0.19","651","                output shape changed to ``(n_samples,)`` to conform to","652","                scikit-learn conventions for binary classification."],"delete":[]}],"sklearn\/model_selection\/_search.py":[{"add":["1004","        .. versionadded:: 0.19","1005","","1006","        .. versionchanged:: 0.21","1007","            Default value was changed from ``True`` to ``False``","1008","","1345","        .. versionadded:: 0.19","1346","","1347","        .. versionchanged:: 0.21","1348","            Default value was changed from ``True`` to ``False``","1349",""],"delete":[]}]}},"6d2b7bc6da26249cfcc99a45ee78a76b459fc3da":{"changes":{"sklearn\/datasets\/descr\/linnerud.rst":"MODIFY"},"diff":{"sklearn\/datasets\/descr\/linnerud.rst":[{"add":["13","- *physiological* - CSV containing 20 observations on 3 physiological variables:","16","- *exercise* - CSV containing 20 observations on 3 exercise variables:"],"delete":["13","- *physiological* - CSV containing 20 observations on 3 exercise variables:","16","- *exercise* - CSV containing 20 observations on 3 physiological variables:"]}]}},"6b646da89c516a14adf36bad2994b5782df67050":{"changes":{"sklearn\/kernel_approximation.py":"MODIFY"},"diff":{"sklearn\/kernel_approximation.py":[{"add":["48","    random_offset_ : ndarray of shape (n_components,), dtype=float64","52","    random_weights_ : ndarray of shape (n_features, n_components),\\","53","        dtype=float64"],"delete":["48","    random_offset_: ndarray of shape (n_components,), dtype=float64","52","    random_weights_: ndarray of shape (n_features, n_components), dtype=float64"]}]}},"406184ef9ca0d526780b6a250eb7fa92966e52e5":{"changes":{"sklearn\/isotonic.py":"MODIFY","doc\/modules\/isotonic.rst":"MODIFY"},"diff":{"sklearn\/isotonic.py":[{"add":["79","    \"\"\"Solve the isotonic regression model.","92","    y_min : float, default=None","93","        Lower bound on the lowest predicted value (the minimum value may","94","        still be higher). If not set, defaults to -inf.","96","    y_max : float, default=None","97","        Upper bound on the highest predicted value (the maximum may still be","98","        lower). If not set, defaults to +inf.","141","        Lower bound on the lowest predicted value (the minimum value may","142","        still be higher). If not set, defaults to -inf.","145","        Upper bound on the highest predicted value (the maximum may still be","146","        lower). If not set, defaults to +inf.","148","    increasing : bool or 'auto', default=True","149","        Determines whether the predictions should be constrained to increase","150","        or decrease with `X`. 'auto' will decide based on the Spearman","151","        correlation estimate's sign.","154","        The ``out_of_bounds`` parameter handles how `X` values outside of the","155","        training domain are handled.  When set to \"nan\", predictions","156","        will be NaN.  When set to \"clip\", predictions will be","158","        When set to \"raise\" a `ValueError` is raised.","330","        y_pred : ndarray of shape (n_samples,)","370","        y_pred : ndarray of shape (n_samples,)"],"delete":["79","    \"\"\"Solve the isotonic regression model::","80","","81","        min sum w[i] (y[i] - y_[i]) ** 2","82","","83","        subject to y_min = y_[1] <= y_[2] ... <= y_[n] = y_max","84","","85","    where:","86","        - y[i] are inputs (real numbers)","87","        - y_[i] are fitted","88","        - w[i] are optional strictly positive weights (default to 1.0)","101","    y_min : optional, default: None","102","        If not None, set the lowest value of the fit to y_min.","104","    y_max : optional, default: None","105","        If not None, set the highest value of the fit to y_max.","141","    The isotonic regression optimization problem is defined by::","142","","143","        min sum w_i (y[i] - y_[i]) ** 2","144","","145","        subject to y_[i] <= y_[j] whenever X[i] <= X[j]","146","        and min(y_) = y_min, max(y_) = y_max","147","","148","    where:","149","        - ``y[i]`` are inputs (real numbers)","150","        - ``y_[i]`` are fitted","151","        - ``X`` specifies the order.","152","          If ``X`` is non-decreasing then ``y_`` is non-decreasing.","153","        - ``w[i]`` are optional strictly positive weights (default to 1.0)","154","","162","        If not None, set the lowest value of the fit to y_min.","165","        If not None, set the highest value of the fit to y_max.","167","    increasing : bool or string, default=True","168","        If boolean, whether or not to fit the isotonic regression with y","169","        increasing or decreasing.","170","","171","        The string value \"auto\" determines whether y should","172","        increase or decrease based on the Spearman correlation estimate's","173","        sign.","176","        The ``out_of_bounds`` parameter handles how x-values outside of the","177","        training domain are handled.  When set to \"nan\", predicted y-values","178","        will be NaN.  When set to \"clip\", predicted y-values will be","180","        When set to \"raise\", allow ``interp1d`` to throw ValueError.","352","        T_ : array, shape=(n_samples,)","392","        T_ : array, shape=(n_samples,)"]}],"doc\/modules\/isotonic.rst":[{"add":["8","The class :class:`IsotonicRegression` fits a non-decreasing real function to","9","1-dimensional data. It solves the following problem:","13","  subject to :math:`\\hat{y}_i \\le \\hat{y}_j` whenever :math:`X_i \\le X_j`,","15","where the weights :math:`w_i` are strictly positive, and both `X` and `y` are","16","arbitrary real quantities.","17","","18","The `increasing` parameter changes the constraint to","19",":math:`\\hat{y}_i \\ge \\hat{y}_j` whenever :math:`X_i \\le X_j`. Setting it to","20","'auto' will automatically choose the constraint based on `Spearman's rank","21","correlation coefficient","22","<https:\/\/en.wikipedia.org\/wiki\/Spearman%27s_rank_correlation_coefficient>`_.","23","","24",":class:`IsotonicRegression` produces a series of predictions","25",":math:`\\hat{y}_i` for the training data which are the closest to the targets","26",":math:`y` in terms of mean squared error. These predictions are interpolated","27","for predicting to unseen data. The predictions of :class:`IsotonicRegression`","28","thus form a function that is piecewise linear:"],"delete":["8","The class :class:`IsotonicRegression` fits a non-decreasing function to data.","9","It solves the following problem:","13","  subject to :math:`\\hat{y}_{min} = \\hat{y}_1 \\le \\hat{y}_2 ... \\le \\hat{y}_n = \\hat{y}_{max}`","15","where each :math:`w_i` is strictly positive and each :math:`y_i` is an","16","arbitrary real number. It yields the vector which is composed of non-decreasing","17","elements the closest in terms of mean squared error. In practice this list","18","of elements forms a function that is piecewise linear."]}]}},"53082e92af18d533061f62d457e781c3c3617a4d":{"changes":{"doc\/conf.py":"MODIFY"},"diff":{"doc\/conf.py":[{"add":["340","","341","    # searchindex only exist when generating html","342","    if app.builder.name != 'html':","343","        return","344",""],"delete":[]}]}},"4f97facc3a992c6e2459c3da86c9d69b0688d5ab":{"changes":{"doc\/whats_new\/_contributors.rst":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY"},"diff":{"doc\/whats_new\/_contributors.rst":[{"add":["177","","178",".. _Guillaume Lemaitre: https:\/\/github.com\/glemaitre"],"delete":[]}],"doc\/whats_new\/v0.22.rst":[{"add":["184","  :pr:`14510` by `Guillaume Lemaitre`_.","199","- |Feature| :class:`cross_decomposition.PLSCanonical` and","200","  :class:`cross_decomposition.PLSRegression` have a new function","201","  ``inverse_transform`` to transform data to the original space.","202","  :pr:`15304` by :user:`Jaime Ferrando Huertas <jiwidi>`.","203","","216","- |Fix| :class:`cross_decomposition.CCA` now produces the same results with","236","- |Enhancement| The parameter `normalize` was added to","237","   :func:`datasets.fetch_20newsgroups_vectorized`.","238","   :pr:`14740` by :user:`St¨¦phan Tulkens <stephantul>`","239","","247","- |Efficiency| :class:`decomposition.NMF(solver='mu')` fitted on sparse input","248","  matrices now uses batching to avoid briefly allocating an array with size","249","  (#non-zero elements, n_components). :pr:`15257` by `Mart Willocx <Maocx>`_.","250","","275","- |Fix| :class:`dummy.DummyClassifier` now handles checking the existence","276","  of the provided constant in multiouput cases.","277","  :pr:`14908` by :user:`Martina G. Vilas <martinagvilas>`.","278","","307","  - |Feature| :func:`inspection.partial_dependence` and","308","    :func:`inspection.plot_partial_dependence` now support the fast 'recursion'","309","    method for both estimators. :pr:`13769` by `Nicolas Hug`_.","319","    :pr:`14710` by `Guillaume Lemaitre`_.","326","- |Enhancement| Addition of ``max_samples`` argument allows limiting","327","  size of bootstrap samples to be less than size of dataset. Added to","328","  :class:`ensemble.ForestClassifier`,","329","  :class:`ensemble.ForestRegressor`,","330","  :class:`ensemble.RandomForestClassifier`,","331","  :class:`ensemble.RandomForestRegressor`,","332","  :class:`ensemble.ExtraTreesClassifier`,","333","  :class:`ensemble.ExtraTreesRegressor`,","334","  :class:`ensemble.RandomTreesEmbedding`. :pr:`14682` by","335","  :user:`Matt Hancock <notmatthancock>` and","336","  :pr:`5963` by :user:`Pablo Duboue <DrDub>`.","337","","353","  :pr:`14305` by `Guillaume Lemaitre`_.","358","  :pr:`14114` by `Guillaume Lemaitre`_.","359","","360","- |Fix| Stacking and Voting estimators now ensure that their underlying","361","  estimators are either all classifiers or all regressors.","362","  :class:`ensemble.StackingClassifier`, :class:`ensemble.StackingRegressor`,","363","  and :class:`ensemble.VotingClassifier` and :class:`VotingRegressor`","364","  now raise consistent error messages.","365","  :pr:`15084` by `Guillaume Lemaitre`_.","366","","367","- |Fix| :class:`ensemble.AdaBoostRegressor` where the loss should be normalized","368","  by the max of the samples with non-null weights only.","369","  :pr:`14294` by `Guillaume Lemaitre`_.","400","- |API| Deprecated unused `copy` param for","401","  :meth:`feature_extraction.text.TfidfVectorizer.transform` it will be","402","  removed in v0.24. :pr:`14520` by","403","  :user:`Guillem G. Subies <guillemgsubies>`.","404","","440","- |API| From version 0.24 :meth:`gaussian_process.kernels.Kernel.get_params` will raise an","441","  ``AttributeError`` rather than return ``None`` for parameters that are in the","442","  estimator's constructor but not stored as attributes on the instance.","443","  :pr:`14464` by `Joel Nothman`_.","444","","450","  `Thomas Fan`_ and :pr:`15010` by `Guillaume Lemaitre`_.","460","- |Fix| :class:`impute.IterativeImputer` now works when there is only one feature.","461","  By :user:`Sergey Feldman <sergeyf>`.","462","","489","  :pr:`14028` and :pr:`15429` by `Guillaume Lemaitre`_.","501","- |Efficiency| The 'liblinear' logistic regression solver is now faster and","502","  requires less memory.","503","  :pr:`14108`, :pr:`14170`, :pr:`14296` by :user:`Alex Henrie <alexhenrie>`.","504","","517","  :pr:`14458` by `Guillaume Lemaitre`_.","536","  :pr:`15038` by `Guillaume Lemaitre`_.","566","- |API| Deprecate ``training_data_`` unused attribute in","567","  :class:`manifold.Isomap`. :issue:`10482` by `Tom Dupre la Tour`_.","568","","572","- |MajorFeature| :func:`metrics.plot_roc_curve` has been added to plot roc","573","  curves. This function introduces the visualization API described in","574","  the :ref:`User Guide <visualizations>`. :pr:`14357` by `Thomas Fan`_.","575","","613","- |Efficiency| Improved performance of","614","  :func:`metrics.pairwise.manhattan_distances` in the case of sparse matrices.","615","  :pr:`15049` by `Paolo Toccaceli <ptocca>`.","616","","646","- |Fix| Raise a ValueError in :func:`metrics.silhouette_score` when a","647","  precomputed distance matrix contains non-zero diagonal entries.","648","  :pr:`12258` by :user:`Stephen Tierney <sjtrny>`.","649","","650","- |API| ``scoring=\"neg_brier_score\"`` should be used instead of","651","  ``scoring=\"brier_score_loss\"`` which is now deprecated.","652","  :pr:`14898` by :user:`Stefan Matcovici <stefan-matcovici>`.","653","","674","- |Fix| The `cv_results_` attribute of :class:`model_selection.GridSearchCV`","675","  and :class:`model_selection.RandomizedSearchCV` now only contains unfitted","676","  estimators. This potentially saves a lot of memory since the state of the","677","  estimators isn't stored. :pr:`#15096` by `Andreas M¨¹ller`_.","678","","750","- |Fix| The `fit` in :class:`~pipeline.FeatureUnion` now accepts `fit_params`","751","  to pass to the underlying transformers. :pr:`15119` by `Adrin Jalali`_.","752","","813","  :pr:`15038` by `Guillaume Lemaitre`_.","853","- |Feature| A new random variable, :class:`utils.fixes.loguniform` implements a","864","  by `Guillaume Lemaitre`_.","886","  - ``utils.choose_check_classifiers_labels``","887","  - ``utils.enforce_estimator_tags_y``","888","  - ``utils.optimize.newton_cg``","889","  - ``utils.random.random_choice_csc``","890","  - ``utils.safe_indexing``","891","  - ``utils.mocking``","892","  - ``utils.fast_dict``","893","  - ``utils.seq_dataset``","894","  - ``utils.weight_vector``","895","  - ``utils.fixes.parallel_helper`` (removed)","896","  - All of ``utils.testing`` except for ``all_estimators`` which is now in","897","    ``utils``.","909","- |Fix| Port `lobpcg` from SciPy which implement some bug fixes but only","910","  available in 1.3+.","911","  :pr:`13609` and :pr:`14971` by `Guillaume Lemaitre`_.","912","","959","  to be overridable only once. :pr:`14884` by `Andreas M¨¹ller`_."],"delete":["184","  :pr:`14510` by :user:`Guillaume Lemaitre <glemaitre>`.","211","- |Feature| :class:`cross_decomposition.PLSCanonical` and","212","  :class:`cross_decomposition.PLSRegression` have a new function","213","  ``inverse_transform`` to transform data to the original space`.","214","  :pr:`15304` by :user:`Jaime Ferrando Huertas <jiwidi>`.","215","","216","- |Fix| :class:`cross_decomposition.CCA` now produces the same results with ","240"," - |Enhancement| The parameter `normalize` was added to","241","   :func:`datasets.fetch_20newsgroups_vectorized`.","242","   :pr:`14740` by :user:`St¨¦phan Tulkens <stephantul>`","243","","268","- |Efficiency| :class:`decomposition.NMF(solver='mu')` fitted on sparse input","269","  matrices now uses batching to avoid briefly allocating an array with size","270","  (#non-zero elements, n_components). :pr:`15257` by `Mart Willocx <Maocx>`_.","271","","280","- |Fix| :class:`dummy.DummyClassifier` now handles checking the existence","281","  of the provided constant in multiouput cases.","282","  :pr:`14908` by :user:`Martina G. Vilas <martinagvilas>`.","283","","311","  - |Feature| :func:`inspection.partial_dependence` and","312","    :func:`inspection.plot_partial_dependence` now support the fast 'recursion'","313","    method for both estimators. :pr:`13769` by `Nicolas Hug`_.","319","    :pr:`14710` by :user:`Guillaume Lemaitre <glemaitre>`.","341","  :pr:`14305` by :user:`Guillaume Lemaitre <glemaitre>`.","346","  :pr:`14114` by :user:`Guillaume Lemaitre <glemaitre>`.","355","- |Enhancement| Addition of ``max_samples`` argument allows limiting","356","  size of bootstrap samples to be less than size of dataset. Added to","357","  :class:`ensemble.ForestClassifier`,","358","  :class:`ensemble.ForestRegressor`,","359","  :class:`ensemble.RandomForestClassifier`,","360","  :class:`ensemble.RandomForestRegressor`,","361","  :class:`ensemble.ExtraTreesClassifier`,","362","  :class:`ensemble.ExtraTreesRegressor`,","363","  :class:`ensemble.RandomTreesEmbedding`. :pr:`14682` by","364","  :user:`Matt Hancock <notmatthancock>` and","365","  :pr:`5963` by :user:`Pablo Duboue <DrDub>`.","366","","367","- |Fix| Stacking and Voting estimators now ensure that their underlying","368","  estimators are either all classifiers or all regressors.","369","  :class:`ensemble.StackingClassifier`, :class:`ensemble.StackingRegressor`,","370","  and :class:`ensemble.VotingClassifier` and :class:`VotingRegressor`","371","  now raise consistent error messages.","372","  :pr:`15084` by :user:`Guillaume Lemaitre <glemaitre>`.","373","","374","- |Fix| :class:`ensemble.AdaBoostRegressor` where the loss should be normalized","375","  by the max of the samples with non-null weights only.","376","  :pr:`14294` by :user:`Guillaume Lemaitre <glemaitre>`.","377","","392","- |API| Deprecated unused `copy` param for","393","  :meth:`feature_extraction.text.TfidfVectorizer.transform` it will be","394","  removed in v0.24. :pr:`14520` by","395","  :user:`Guillem G. Subies <guillemgsubies>`.","396","","431","- |API| From version 0.24 :meth:`gaussian_process.kernels.Kernel.get_params` will raise an","432","  ``AttributeError`` rather than return ``None`` for parameters that are in the","433","  estimator's constructor but not stored as attributes on the instance.","434","  :pr:`14464` by `Joel Nothman`_.","435","","450","  `Thomas Fan`_.","451","","452","- |Enhancement| Adds parameter `add_indicator` to :class:`impute.KNNImputer`","453","  to get indicator of missing data.","454","  :pr:`15010` by :user:`Guillaume Lemaitre <glemaitre>`.","461","- |Fix| :class:`impute.IterativeImputer` now works when there is only one feature.","462","  By :user:`Sergey Feldman <sergeyf>`.","463","","493","  :pr:`14028` and :pr:`15429` by :user:`Guillaume Lemaitre <glemaitre>`.","510","- |Efficiency| The 'liblinear' logistic regression solver is now faster and","511","  requires less memory.","512","  :pr:`14108`, :pr:`14170`, :pr:`14296` by :user:`Alex Henrie <alexhenrie>`.","513","","521","  :pr:`14458` by :user:`Guillaume Lemaitre <glemaitre>`.","540","  :pr:`15038` by :user:`Guillaume Lemaitre <glemaitre>`.","559","- |API| Deprecate ``training_data_`` unused attribute in","560","  :class:`manifold.Isomap`. :issue:`10482` by `Tom Dupre la Tour`_.","561","","583","- |MajorFeature| :func:`metrics.plot_roc_curve` has been added to plot roc","584","  curves. This function introduces the visualization API described in","585","  the :ref:`User Guide <visualizations>`. :pr:`14357` by `Thomas Fan`_.","586","","628","- |Fix| Raise a ValueError in :func:`metrics.silhouette_score` when a","629","  precomputed distance matrix contains non-zero diagonal entries.","630","  :pr:`12258` by :user:`Stephen Tierney <sjtrny>`.","631","","637","- |API| ``scoring=\"neg_brier_score\"`` should be used instead of","638","  ``scoring=\"brier_score_loss\"`` which is now deprecated.","639","  :pr:`14898` by :user:`Stefan Matcovici <stefan-matcovici>`.","640","","641","- |Efficiency| Improved performance of","642","  :func:`metrics.pairwise.manhattan_distances` in the case of sparse matrices.","643","  :pr:`15049` by `Paolo Toccaceli <ptocca>`.","644","","683","- |Fix| The `cv_results_` attribute of :class:`model_selection.GridSearchCV`","684","  and :class:`model_selection.RandomizedSearchCV` now only contains unfitted","685","  estimators. This potentially saves a lot of memory since the state of the","686","  estimators isn't stored. :pr:`#15096` by :user:`Andreas M¨¹ller <amueller>`.","687","","758","- |Fix| The `fit` in :class:`~pipeline.FeatureUnion` now accepts `fit_params`","759","  to pass to the underlying transformers. :pr:`15119` by `Adrin Jalali`_.","760","","789","","818","  :pr:`15038` by :user:`Guillaume Lemaitre <glemaitre>`.","858","- |API| The following utils have been deprecated and are now private:","859","","860","  - ``utils.choose_check_classifiers_labels``","861","  - ``utils.enforce_estimator_tags_y``","862","  - ``utils.optimize.newton_cg``","863","  - ``utils.random.random_choice_csc``","864","  - ``utils.safe_indexing``","865","  - ``utils.mocking``","866","  - ``utils.fast_dict``","867","  - ``utils.seq_dataset``","868","  - ``utils.weight_vector``","869","  - ``utils.fixes.parallel_helper`` (removed)","870","  - All of ``utils.testing`` except for ``all_estimators`` which is now in","871","    ``utils``.","872","","873","- A new random variable, :class:`utils.fixes.loguniform` implements a","884","  by :user:`Guillaume Lemaitre <glemaitre>`.","914","","928","- |Fix| Port `lobpcg` from SciPy which implement some bug fixes but only","929","  available in 1.3+.","930","  :pr:`13609` and :pr:`14971` by :user:`Guillaume Lemaitre <glemaitre>`.","931","","968","  to be overridable only once. :pr:`14884` by :user:`Andreas M¨¹ller","969","  <amueller>`."]}]}},"4ce39dbc699cd9cd33ab2b67ca029698713aec3e":{"changes":{"sklearn\/tests\/test_common.py":"MODIFY","sklearn\/utils\/__init__.py":"MODIFY","sklearn\/utils\/tests\/test_estimator_checks.py":"MODIFY"},"diff":{"sklearn\/tests\/test_common.py":[{"add":[],"delete":["80","        if name.startswith(\"_\"):","81","            continue"]}],"sklearn\/utils\/__init__.py":[{"add":["5","from importlib import import_module","15","from pathlib import Path","1162","    modules_to_ignore = {\"tests\", \"externals\", \"setup\", \"conftest\"}","1163","    root = str(Path(__file__).parent.parent)  # sklearn package","1164","    # Ignore deprecation warnings triggered at import time and from walking","1165","    # packages","1166","    with ignore_warnings(category=FutureWarning):","1167","        for importer, modname, ispkg in pkgutil.walk_packages(","1168","                path=[root], prefix='sklearn.'):","1169","            mod_parts = modname.split(\".\")","1170","            if (any(part in modules_to_ignore for part in mod_parts)","1171","                    or '._' in modname):","1172","                continue","1173","            module = import_module(modname)","1174","            classes = inspect.getmembers(module, inspect.isclass)","1175","            classes = [(name, est_cls) for name, est_cls in classes","1176","                       if not name.startswith(\"_\")]","1177","","1178","            # TODO: Remove when FeatureHasher is implemented in PYPY","1179","            # Skips FeatureHasher for PYPY","1180","            if IS_PYPY and 'feature_extraction' in modname:","1181","                classes = [(name, est_cls) for name, est_cls in classes","1182","                           if name == \"FeatureHasher\"]","1183","","1184","            all_classes.extend(classes)"],"delete":["1133","    import sklearn","1161","    # get parent folder","1162","    path = sklearn.__path__","1163","    for importer, modname, ispkg in pkgutil.walk_packages(","1164","            path=path, prefix='sklearn.', onerror=lambda x: None):","1165","        if \".tests.\" in modname or \"externals\" in modname:","1166","            continue","1167","        if IS_PYPY and ('_svmlight_format' in modname or","1168","                        'feature_extraction._hashing' in modname):","1169","            continue","1170","        # Ignore deprecation warnings triggered at import time.","1171","        with ignore_warnings(category=FutureWarning):","1172","            module = __import__(modname, fromlist=\"dummy\")","1173","        classes = inspect.getmembers(module, inspect.isclass)","1174","        all_classes.extend(classes)"]}],"sklearn\/utils\/tests\/test_estimator_checks.py":[{"add":["36","from sklearn.utils import all_estimators","575","def test_all_estimators_all_public():","576","    # all_estimator should not fail when pytest is not installed and return","577","    # only public estimators","578","    estimators = all_estimators()","579","    for est in estimators:","580","        assert not est.__class__.__name__.startswith(\"_\")","581","","582",""],"delete":[]}]}},"54a09dc864620c4e351aa0c08cb0425a4006052d":{"changes":{"sklearn\/ensemble\/_forest.py":"MODIFY"},"diff":{"sklearn\/ensemble\/_forest.py":[{"add":["955","        whole dataset is used to build each tree.","1276","        whole dataset is used to build each tree."],"delete":["955","        whole datset is used to build each tree.","1276","        whole datset is used to build each tree."]}]}},"e3e91378114742b3ea53ae8ba1f1e0fff6f31909":{"changes":{"doc\/modules\/svm.rst":"MODIFY","sklearn\/svm\/_base.py":"MODIFY","sklearn\/svm\/_classes.py":"MODIFY"},"diff":{"doc\/modules\/svm.rst":[{"add":["6",".. TODO: Describe tol parameter","7",".. TODO: Describe max_iter parameter","8","","54","capable of performing binary and multi-class classification on a dataset.","65","other hand, :class:`LinearSVC` is another (faster) implementation of Support","67",":class:`LinearSVC` does not accept parameter ``kernel``, as this is","68","assumed to be linear. It also lacks some of the attributes of","72",":class:`LinearSVC` take as input two arrays: an array `X` of shape","73","`(n_samples, n_features)` holding the training samples, and an array `y` of","74","class labels (strings or integers), of shape `(n_samples)`::","89","SVMs decision function (detailed in the :ref:`svm_mathematical_formulation`)","90","depends on some subset of the training data, called the support vectors. Some","91","properties of these support vectors can be found in attributes","92","``support_vectors_``, ``support_`` and ``n_support``::","105",".. topic:: Examples:","106","","107"," * :ref:`sphx_glr_auto_examples_svm_plot_separating_hyperplane.py`,","108"," * :ref:`sphx_glr_auto_examples_svm_plot_svm_nonlinear.py`","109"," * :ref:`sphx_glr_auto_examples_svm_plot_svm_anova.py`,","110","","116",":class:`SVC` and :class:`NuSVC` implement the \"one-versus-one\"","117","approach for multi-class classification. In total,","118","``n_classes * (n_classes - 1) \/ 2``","121","``decision_function_shape`` option allows to monotonically transform the","122","results of the \"one-versus-one\" classifiers to a \"one-vs-rest\" decision","123","function of shape ``(n_samples, n_classes)``.","139","multi-class strategy, thus training `n_classes` models.","152","strategy, the so-called multi-class SVM formulated by Crammer and Singer","153","[#8]_, by using the option ``multi_class='crammer_singer'``. In practice,","154","one-vs-rest classification is usually preferred, since the results are mostly","155","similar, but the runtime is significantly less.","158","have the shape ``(n_classes, n_features)`` and ``(n_classes,)`` respectively.","159","Each row of the coefficients corresponds to one of the ``n_classes``","163","In the case of \"one-vs-one\" :class:`SVC` and :class:`NuSVC`, the layout of","164","the attributes is a little more involved. In the case of a linear","165","kernel, the attributes ``coef_`` and ``intercept_`` have the shape","166","``(n_classes * (n_classes - 1) \/ 2, n_features)`` and ``(n_classes *","167","(n_classes - 1) \/ 2)`` respectively. This is similar to the layout for","168",":class:`LinearSVC` described above, with each row now corresponding","173","The shape of ``dual_coef_`` is ``(n_classes-1, n_SV)`` with","176","of the ``n_classes * (n_classes - 1) \/ 2`` \"one-vs-one\" classifiers.","177","Each of the support vectors is used in ``n_classes - 1`` classifiers.","178","The ``n_classes - 1`` entries in each row correspond to the dual coefficients","181","This might be clearer with an example: consider a three class problem with","182","class 0 having three support vectors","206",".. topic:: Examples:","207","","208"," * :ref:`sphx_glr_auto_examples_svm_plot_iris_svc.py`,","220","calibrated using Platt scaling [#1]_: logistic regression on the SVM's scores,","222","In the multiclass case, this is extended as per [#2]_.","224",".. note::","225","","226","  The same probability calibration procedure is available for all estimators","227","  via the :class:`~sklearn.calibration.CalibratedClassifierCV` (see","228","  :ref:`calibration`). In the case of :class:`SVC` and :class:`NuSVC`, this","229","  procedure is builtin in `libsvm`_ which is used under the hood, so it does","230","  not rely on scikit-learn's","231","  :class:`~sklearn.calibration.CalibratedClassifierCV`.","232","","233","The cross-validation involved in Platt scaling","235","In addition, the probability estimates may be inconsistent with the scores:","236","","237","- the \"argmax\" of the scores may not be the argmax of the probabilities","238","- in binary classification, a sample may be labeled by ``predict`` as","239","  belonging to the positive class even if the output of `predict_proba` is","240","  less than 0.5; and similarly, it could be labeled as negative even if the","241","  output of `predict_proba` is more than 0.5.","242","","253","that it comes with a computational cost. See","254",":ref:`sphx_glr_auto_examples_svm_plot_svm_tie_breaking.py` for an example on","255","tie breaking.","261","classes or certain individual samples, the parameters ``class_weight`` and","264",":class:`SVC` (but not :class:`NuSVC`) implements the parameter","268","The figure below illustrates the decision boundary of an unbalanced problem,","269","with and without weight correction.","279","individual samples in the `fit` method through the ``sample_weight`` parameter.","280","Similar to ``class_weight``, this sets the parameter ``C`` for the i-th","281","example to ``C * sample_weight[i]``, which will encourage the classifier to","282","get these samples right. The figure below illustrates the effect of sample","283","weighting on the decision boundary. The size of the circles is proportional","284","to the sample weights:","310","because the cost function ignores samples whose prediction is close to their","311","target.","316","the linear kernel, while :class:`NuSVR` implements a slightly different","355","solver used by the `libsvm`_-based implementation scales between","362","For the linear case, the algorithm used in","374","    contiguous and double precision, it will be copied before calling the","380","    array will be copied and converted to the `liblinear`_ internal sparse data","383","    copying a dense numpy C-contiguous double precision array as input, we","395","","397","    choice.  If you have a lot of noisy observations you should decrease it:","398","    decreasing C corresponds to more regularization.","403","    sometimes up to 10 times longer, as shown in [#3]_.","409","    applied to the test vector to obtain meaningful results. This can be done","410","    easily by using a :class:`~sklearn.pipeline.Pipeline`::","411","","412","        >>> from sklearn.pipeline import make_pipeline","413","        >>> from sklearn.preprocessing import StandardScaler","414","        >>> from sklearn.svm import SVC","415","","416","        >>> clf = make_pipeline(StandardScaler(), SVC())","417","    ","418","    See section :ref:`preprocessing` for more details on scaling and","419","    normalization.","420","  ","421","  .. _shrinking_svm:","422","","423","  * Regarding the `shrinking` parameter, quoting [#4]_: *We found that if the","424","    number of iterations is large, then shrinking can shorten the training","425","    time. However, if we loosely solve the optimization problem (e.g., by","426","    using a large stopping tolerance), the code without using shrinking may","427","    be much faster*","432","  * In :class:`SVC`, if the data is unbalanced (e.g. many","448","    descent (i.e when ``dual`` is set to ``True``). It is thus not uncommon","450","    happens, try with a smaller `tol` parameter. This randomness can also be","458","    Increasing ``C`` yields a more complex model (more features are selected).","473","    :math:`d` is specified by parameter ``degree``, :math:`r` by ``coef0``.","476","    specified by parameter ``gamma``, must be greater than 0.","481","Different kernels are specified by the `kernel` parameter::","490","Parameters of the RBF Kernel","491","----------------------------","492","","493","When training an SVM with the *Radial Basis Function* (RBF) kernel, two","494","parameters must be considered: ``C`` and ``gamma``.  The parameter ``C``,","495","common to all SVM kernels, trades off misclassification of training examples","496","against simplicity of the decision surface. A low ``C`` makes the decision","497","surface smooth, while a high ``C`` aims at classifying all training examples","498","correctly.  ``gamma`` defines how much influence a single training example has.","499","The larger ``gamma`` is, the closer other examples must be to be affected.","500","","501","Proper choice of ``C`` and ``gamma`` is critical to the SVM's performance.  One","502","is advised to use :class:`sklearn.model_selection.GridSearchCV` with ","503","``C`` and ``gamma`` spaced exponentially far apart to choose good values.","504","","505",".. topic:: Examples:","506","","507"," * :ref:`sphx_glr_auto_examples_svm_plot_rbf_parameters.py`","508"," * :ref:`sphx_glr_auto_examples_svm_plot_svm_nonlinear.py`","509","","531","You can use your own defined kernels by passing a function to the","532","``kernel`` parameter.","555","You can pass pre-computed kernels by using the ``kernel='precomputed'``","556","option. You should then pass Gram matrix instead of X to the `fit` and","557","`predict` methods. The kernel values between *all* training vectors and the","558","test vectors must be provided:","561","    >>> from sklearn.datasets import make_classification","562","    >>> from sklearn.model_selection import train_test_split ","564","    >>> X, y = make_classification(n_samples=10, random_state=0)","565","    >>> X_train , X_test , y_train, y_test = train_test_split(X, y, random_state=0)","568","    >>> gram_train = np.dot(X_train, X_train.T)","569","    >>> clf.fit(gram_train, y_train)","572","    >>> gram_test = np.dot(X_test, X_train.T)","573","    >>> clf.predict(gram_test)","574","    array([0, 1, 0])","582","A support vector machine constructs a hyper-plane or set of hyper-planes in a","583","high or infinite dimensional space, which can be used for","588","generalization error of the classifier. The figure below shows the decision","589","function for a linearly separable problem, with three samples on the","590","margin boundaries, called \"support vectors\":","596","In general, when the problem isn't linearly separable, the support vectors","597","are the samples *within* the margin boundaries.","598","","599","We recommend [#5]_ and [#6]_ as good references for the theory and","600","practicalities of SVMs.","601","","606","vector :math:`y \\in \\{1, -1\\}^n`, our goal is to find :math:`w \\in","607","\\mathbb{R}^p` and :math:`b \\in \\mathbb{R}` such that the prediction given by","608",":math:`\\text{sign} (w^T\\phi(x) + b)` is correct for most samples.","610","SVC solves the following primal problem:","619","Intuitively, we're trying to maximize the margin (by minimizing","620",":math:`||w||^2 = w^Tw`), while incurring a penalty when a sample is","621","misclassified or within the margin boundary. Ideally, the value :math:`y_i","622","(w^T \\phi (x_i) + b)` would be :math:`\\geq 1` for all samples, which","623","indicates a perfect prediction. But problems are usually not always perfectly","624","separable with a hyperplane, so we allow some samples to be at a distance :math:`\\zeta_i` from","625","their correct margin boundary. The penalty term `C` controls the strengh of","626","this penalty, and as a result, acts as an inverse regularization parameter","627","(see note below).","628","","629","The dual problem to the primal is","639","where :math:`e` is the vector of all ones,","640","and :math:`Q` is an :math:`n` by :math:`n` positive semidefinite matrix,","642","is the kernel. The terms :math:`\\alpha_i` are called the dual coefficients,","643","and they are upper-bounded by :math:`C`.","644","This dual representation highlights the fact that training vectors are","645","implicitly mapped into a higher (maybe infinite)","646","dimensional space by the function :math:`\\phi`: see `kernel trick","647","<https:\/\/en.wikipedia.org\/wiki\/Kernel_method>`_.","649","Once the optimization problem is solved, the output of","650",":term:`decision_function` for a given sample :math:`x` becomes:","652",".. math:: \\sum_{i\\in SV} y_i \\alpha_i K(x_i, x) + b,","654","and the predicted class correspond to its sign. We only need to sum over the","655","support vectors (i.e. the samples that lie within the margin) because the","656","dual coefficients :math:`\\alpha_i` are zero for the other samples.","657","","658","These parameters can be accessed through the attributes ``dual_coef_``","659","which holds the product :math:`y_i \\alpha_i`, ``support_vectors_`` which","660","holds the support vectors, and ``intercept_`` which holds the independent","661","term :math:`b`","672","LinearSVC","673","---------","675","The primal problem can be equivalently formulated as","677",".. math::","679","    \\min_ {w, b} \\frac{1}{2} w^T w + C \\sum_{i=1}\\max(0, y_i (w^T \\phi(x_i) + b)),","681","where we make use of the `hinge loss","682","<https:\/\/en.wikipedia.org\/wiki\/Hinge_loss>`_. This is the form that is","683","directly optimized by :class:`LinearSVC`, but unlike the dual form, this one","684","does not involve inner products between samples, so the famous kernel trick","685","cannot be applied. This is why only the linear kernel is supported by","686",":class:`LinearSVC` (:math:`\\phi` is the identity function).","688",".. _nu_svc:","693","The :math:`\\nu`-SVC formulation [#7]_ is a reparameterization of the","694",":math:`C`-SVC and therefore mathematically equivalent.","696","We introduce a new parameter :math:`\\nu` (instead of :math:`C`) which","697","controls the number of support vectors and *margin errors*:","698",":math:`\\nu \\in (0, 1]` is an upper bound on the fraction of margin errors and","699","a lower bound of the fraction of support vectors. A margin error corresponds","700","to a sample that lies on the wrong side of its margin boundary: it is either","701","misclassified, or it is correctly classified but does not lie beyond the","702","margin.","722","Here, we are penalizing samples whose prediction is at least :math:`\\varepsilon`","723","away from their true target. These samples penalize the objective by","724",":math:`\\zeta_i` or :math:`\\zeta_i^*`, depending on whether their predictions","725","lie above or below the :math:`\\varepsilon` tube.","726","","727","The dual problem is","737","where :math:`e` is the vector of all ones,","743","The prediction is:","745",".. math:: \\sum_{i \\in SV}(\\alpha_i - \\alpha_i^*) K(x_i, x) + b","747","These parameters can be accessed through the attributes ``dual_coef_``","750","term :math:`b`","752","LinearSVR","753","---------","755","The primal problem can be equivalently formulated as","757",".. math::","758","","759","    \\min_ {w, b} \\frac{1}{2} w^T w + C \\sum_{i=1}\\max(0, |y_i - (w^T \\phi(x_i) + b)| - \\varepsilon),","760","","761","where we make use of the epsilon-insensitive loss, i.e. errors of less than","762",":math:`\\varepsilon` are ignored. This is the form that is directly optimized","763","by :class:`LinearSVR`.","770","Internally, we use `libsvm`_ [#4]_ and `liblinear`_ [#3]_ to handle all","772","For a description of the implementation and details of the algorithms","773","used, please refer to their respective papers.","774","","781","   .. [#1] Platt `\"Probabilistic outputs for SVMs and comparisons to","782","      regularized likelihood methods\"","783","      <https:\/\/www.cs.colorado.edu\/~mozer\/Teaching\/syllabi\/6622\/papers\/Platt1999.pdf>`_.","785","   .. [#2] Wu, Lin and Weng, `\"Probability estimates for multi-class","786","      classification by pairwise coupling\"","787","      <https:\/\/www.csie.ntu.edu.tw\/~cjlin\/papers\/svmprob\/svmprob.pdf>`_, JMLR","788","      5:975-1005, 2004.","789"," ","790","   .. [#3] Fan, Rong-En, et al.,","791","      `\"LIBLINEAR: A library for large linear classification.\"","792","      <https:\/\/www.csie.ntu.edu.tw\/~cjlin\/papers\/liblinear.pdf>`_,","793","      Journal of machine learning research 9.Aug (2008): 1871-1874.","794","","795","   .. [#4] Chang and Lin, `LIBSVM: A Library for Support Vector Machines","798","   .. [#5] Bishop, `Pattern recognition and machine learning","799","      <https:\/\/www.microsoft.com\/en-us\/research\/uploads\/prod\/2006\/01\/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf>`_,","800","      chapter 7 Sparse Kernel Machines","802","   .. [#6] `\"A Tutorial on Support Vector Regression\"","803","      <http:\/\/citeseerx.ist.psu.edu\/viewdoc\/summary?doi=10.1.1.114.4288>`_,","804","      Alex J. Smola, Bernhard Sch?lkopf - Statistics and Computing archive","805","      Volume 14 Issue 3, August 2004, p. 199-222.","807","   .. [#7] Sch?lkopf et. al `New Support Vector Algorithms","808","      <https:\/\/www.stat.purdue.edu\/~yuzhu\/stat598m3\/Papers\/NewSVM.pdf>`_","809","    ","810","   .. [#8] Crammer and Singer `On the Algorithmic Implementation ofMulticlass","811","      Kernel-based Vector Machines","812","      <http:\/\/jmlr.csail.mit.edu\/papers\/volume2\/crammer01a\/crammer01a.pdf>`_,","813","      JMLR 2001."],"delete":["51","capable of performing multi-class classification on a dataset.","62","other hand, :class:`LinearSVC` is another implementation of Support","64",":class:`LinearSVC` does not accept keyword ``kernel``, as this is","65","assumed to be linear. It also lacks some of the members of","69",":class:`LinearSVC` take as input two arrays: an array X of size ``[n_samples,","70","n_features]`` holding the training samples, and an array y of class labels","71","(strings or integers), size ``[n_samples]``::","86","SVMs decision function depends on some subset of the training data,","87","called the support vectors. Some properties of these support vectors","88","can be found in members ``support_vectors_``, ``support_`` and","89","``n_support``::","107",":class:`SVC` and :class:`NuSVC` implement the \"one-against-one\"","108","approach (Knerr et al., 1990) for multi- class classification. If","109","``n_class`` is the number of classes, then ``n_class * (n_class - 1) \/ 2``","112","``decision_function_shape`` option allows to monotically transform the results of the","113","\"one-against-one\" classifiers to a decision function of shape ``(n_samples,","114","n_classes)``.","130","multi-class strategy, thus training n_class models. If there are only","131","two classes, only one model is trained::","144","strategy, the so-called multi-class SVM formulated by Crammer and Singer, by","145","using the option ``multi_class='crammer_singer'``. This method is consistent,","146","which is not true for one-vs-rest classification.","147","In practice, one-vs-rest classification is usually preferred, since the results","148","are mostly similar, but the runtime is significantly less.","151","have the shape ``[n_class, n_features]`` and ``[n_class]`` respectively.","152","Each row of the coefficients corresponds to one of the ``n_class`` many","156","In the case of \"one-vs-one\" :class:`SVC`, the layout of the attributes","157","is a little more involved. In the case of having a linear kernel, the","158","attributes ``coef_`` and ``intercept_`` have the shape","159","``[n_class * (n_class - 1) \/ 2, n_features]`` and","160","``[n_class * (n_class - 1) \/ 2]`` respectively. This is similar to the","161","layout for :class:`LinearSVC` described above, with each row now corresponding","166","The shape of ``dual_coef_`` is ``[n_class-1, n_SV]`` with","169","of the ``n_class * (n_class - 1) \/ 2`` \"one-vs-one\" classifiers.","170","Each of the support vectors is used in ``n_class - 1`` classifiers.","171","The ``n_class - 1`` entries in each row correspond to the dual coefficients","174","This might be made more clear by an example:","175","","176","Consider a three class problem with class 0 having three support vectors","211","calibrated using Platt scaling: logistic regression on the SVM's scores,","213","In the multiclass case, this is extended as per Wu et al. (2004).","215","Needless to say, the cross-validation involved in Platt scaling","217","In addition, the probability estimates may be inconsistent with the scores,","218","in the sense that the \"argmax\" of the scores","219","may not be the argmax of the probabilities.","220","(E.g., in binary classification,","221","a sample may be labeled by ``predict`` as belonging to a class","222","that has probability <? according to ``predict_proba``.)","233","that it comes with a computational cost.","234","","235",".. figure:: ..\/auto_examples\/svm\/images\/sphx_glr_plot_svm_tie_breaking_001.png","236","   :target: ..\/auto_examples\/svm\/plot_svm_tie_breaking.html","237","   :align: center","238","","239",".. topic:: References:","240","","241"," * Wu, Lin and Weng,","242","   `\"Probability estimates for multi-class classification by pairwise coupling\"","243","   <https:\/\/www.csie.ntu.edu.tw\/~cjlin\/papers\/svmprob\/svmprob.pdf>`_,","244","   JMLR 5:975-1005, 2004.","245"," ","246"," ","247"," * Platt","248","   `\"Probabilistic outputs for SVMs and comparisons to regularized likelihood methods\"","249","   <https:\/\/www.cs.colorado.edu\/~mozer\/Teaching\/syllabi\/6622\/papers\/Platt1999.pdf>`_.","255","classes or certain individual samples keywords ``class_weight`` and","258",":class:`SVC` (but not :class:`NuSVC`) implement a keyword","271","individual samples in method ``fit`` through keyword ``sample_weight``. Similar","272","to ``class_weight``, these set the parameter ``C`` for the i-th example to","273","``C * sample_weight[i]``.","274","","281","","284"," * :ref:`sphx_glr_auto_examples_svm_plot_iris_svc.py`,","285"," * :ref:`sphx_glr_auto_examples_svm_plot_separating_hyperplane.py`,","287"," * :ref:`sphx_glr_auto_examples_svm_plot_svm_anova.py`,","288"," * :ref:`sphx_glr_auto_examples_svm_plot_svm_nonlinear.py`","305","because the cost function for building the model ignores any training","306","data close to the model prediction.","311","linear kernels, while :class:`NuSVR` implements a slightly different","333","","334","","352","solver used by this `libsvm`_-based implementation scales between","359","Also note that for the linear case, the algorithm used in","371","    contiguous, and double precision, it will be copied before calling the","377","    array will be copied and converted to the liblinear internal sparse data","380","    copying a dense numpy C-contiguous double precision array as input we","393","    choice.  If you have a lot of noisy observations you should decrease it.","394","    It corresponds to regularize more the estimation.","399","    sometimes up to 10 times longer, as shown by Fan et al. (2008)","405","    applied to the test vector to obtain meaningful results. See section","406","    :ref:`preprocessing` for more details on scaling and normalization.","411","  * In :class:`SVC`, if data for classification are unbalanced (e.g. many","427","    descent (i.e when ``dual`` is set to ``True``). It is thus not uncommon,","429","    happens, try with a smaller tol parameter. This randomness can also be","437","    Increasing ``C`` yields a more complex model (more feature are selected).","442",".. topic:: References:","443","","444"," * Fan, Rong-En, et al.,","445","   `\"LIBLINEAR: A library for large linear classification.\"","446","   <https:\/\/www.csie.ntu.edu.tw\/~cjlin\/papers\/liblinear.pdf>`_,","447","   Journal of machine learning research 9.Aug (2008): 1871-1874.","448","","459","    :math:`d` is specified by keyword ``degree``, :math:`r` by ``coef0``.","462","    specified by keyword ``gamma``, must be greater than 0.","467","Different kernels are specified by keyword kernel at initialization::","497","You can also use your own defined kernels by passing a function to the","498","keyword ``kernel`` in the constructor.","521","Set ``kernel='precomputed'`` and pass the Gram matrix instead of X in the fit","522","method. At the moment, the kernel values between *all* training vectors and the","523","test vectors must be provided.","527","    >>> X = np.array([[0, 0], [1, 1]])","528","    >>> y = [0, 1]","531","    >>> gram = np.dot(X, X.T)","532","    >>> clf.fit(gram, y)","535","    >>> clf.predict(gram)","536","    array([0, 1])","538","Parameters of the RBF Kernel","539","~~~~~~~~~~~~~~~~~~~~~~~~~~~~","540","","541","When training an SVM with the *Radial Basis Function* (RBF) kernel, two","542","parameters must be considered: ``C`` and ``gamma``.  The parameter ``C``,","543","common to all SVM kernels, trades off misclassification of training examples","544","against simplicity of the decision surface. A low ``C`` makes the decision","545","surface smooth, while a high ``C`` aims at classifying all training examples","546","correctly.  ``gamma`` defines how much influence a single training example has.","547","The larger ``gamma`` is, the closer other examples must be to be affected.","548","","549","Proper choice of ``C`` and ``gamma`` is critical to the SVM's performance.  One","550","is advised to use :class:`sklearn.model_selection.GridSearchCV` with ","551","``C`` and ``gamma`` spaced exponentially far apart to choose good values.","552","","553",".. topic:: Examples:","554","","555"," * :ref:`sphx_glr_auto_examples_svm_plot_rbf_parameters.py`","562","A support vector machine constructs a hyper-plane or set of hyper-planes","563","in a high or infinite dimensional space, which can be used for","568","generalization error of the classifier.","569","","579","vector :math:`y \\in \\{1, -1\\}^n`, SVC solves the following primal problem:","586","","587","","591","Its dual is","601","where :math:`e` is the vector of all ones, :math:`C > 0` is the upper bound,","602",":math:`Q` is an :math:`n` by :math:`n` positive semidefinite matrix,","604","is the kernel. Here training vectors are implicitly mapped into a higher","605","(maybe infinite) dimensional space by the function :math:`\\phi`.","608","The decision function is:","610",".. math:: \\operatorname{sgn}(\\sum_{i=1}^n y_i \\alpha_i K(x_i, x) + \\rho)","621",".. TODO multiclass case ?\/","623","This parameters can be accessed through the members ``dual_coef_``","624","which holds the product :math:`y_i \\alpha_i`, ``support_vectors_`` which","625","holds the support vectors, and ``intercept_`` which holds the independent","626","term :math:`\\rho` :","628",".. topic:: References:","630"," * `\"Automatic Capacity Tuning of Very Large VC-dimension Classifiers\"","631","   <http:\/\/citeseerx.ist.psu.edu\/viewdoc\/summary?doi=10.1.1.17.7215>`_,","632","   I. Guyon, B. Boser, V. Vapnik - Advances in neural information","633","   processing 1993.","636"," * `\"Support-vector networks\"","637","   <https:\/\/link.springer.com\/article\/10.1007%2FBF00994018>`_,","638","   C. Cortes, V. Vapnik - Machine Learning, 20, 273-297 (1995).","639","","640","","645","We introduce a new parameter :math:`\\nu` which controls the number of","646","support vectors and training errors. The parameter :math:`\\nu \\in (0,","647","1]` is an upper bound on the fraction of training errors and a lower","648","bound of the fraction of support vectors.","650","It can be shown that the :math:`\\nu`-SVC formulation is a reparameterization","651","of the :math:`C`-SVC and therefore mathematically equivalent.","671","Its dual is","681","where :math:`e` is the vector of all ones, :math:`C > 0` is the upper bound,","687","The decision function is:","689",".. math:: \\sum_{i=1}^n (\\alpha_i - \\alpha_i^*) K(x_i, x) + \\rho","691","These parameters can be accessed through the members ``dual_coef_``","694","term :math:`\\rho`","696",".. topic:: References:","698"," * `\"A Tutorial on Support Vector Regression\"","699","   <http:\/\/citeseerx.ist.psu.edu\/viewdoc\/summary?doi=10.1.1.114.4288>`_,","700","   Alex J. Smola, Bernhard Sch?lkopf - Statistics and Computing archive","701","   Volume 14 Issue 3, August 2004, p. 199-222. ","709","Internally, we use `libsvm`_ and `liblinear`_ to handle all","717","  For a description of the implementation and details of the algorithms","718","  used, please refer to","720","    - `LIBSVM: A Library for Support Vector Machines","723","    - `LIBLINEAR -- A Library for Large Linear Classification","724","      <https:\/\/www.csie.ntu.edu.tw\/~cjlin\/liblinear\/>`_."]}],"sklearn\/svm\/_base.py":[{"add":["114","        X : {array-like, sparse matrix} of shape (n_samples, n_features) \\","115","                or (n_samples, n_samples)"],"delete":["114","        X : {array-like, sparse matrix} of shape (n_samples, n_features)"]}],"sklearn\/svm\/_classes.py":[{"add":["161","    >>> from sklearn.pipeline import make_pipeline","162","    >>> from sklearn.preprocessing import StandardScaler","165","    >>> clf = make_pipeline(StandardScaler(),","166","    ...                     LinearSVC(random_state=0, tol=1e-5))","168","    Pipeline(steps=[('standardscaler', StandardScaler()),","169","                    ('linearsvc', LinearSVC(random_state=0, tol=1e-05))])","170","","171","    >>> print(clf.named_steps['linearsvc'].coef_)","172","    [[0.141...   0.526... 0.679... 0.493...]]","173","","174","    >>> print(clf.named_steps['linearsvc'].intercept_)","175","    [0.1693...]","330","    >>> from sklearn.pipeline import make_pipeline","331","    >>> from sklearn.preprocessing import StandardScaler","334","    >>> regr = make_pipeline(StandardScaler(),","335","    ...                      LinearSVR(random_state=0, tol=1e-5))","337","    Pipeline(steps=[('standardscaler', StandardScaler()),","338","                    ('linearsvr', LinearSVR(random_state=0, tol=1e-05))])","339","","340","    >>> print(regr.named_steps['linearsvr'].coef_)","341","    [18.582... 27.023... 44.357... 64.522...]","342","    >>> print(regr.named_steps['linearsvr'].intercept_)","343","    [-4...]","345","    [-2.384...]","346","","475","        See the :ref:`User Guide <shrinking_svm>`.","510","        ('ovo') is always used as multi-class strategy. The parameter is","511","        ignored for binary classification.","533","        probability estimates. Ignored when `probability` is False.","549","        Dual coefficients of the support vector in the decision","550","        function (see :ref:`sgd_mathematical_formulation`), multiplied by","551","        their targets.","554","        non-trivial. See the :ref:`multi-class section of the User Guide","555","        <svm_multi_class>` for details.","594","    >>> from sklearn.pipeline import make_pipeline","595","    >>> from sklearn.preprocessing import StandardScaler","599","    >>> clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))","601","    Pipeline(steps=[('standardscaler', StandardScaler()),","602","                    ('svc', SVC(gamma='auto'))])","603","","659","        An upper bound on the fraction of margin errors (see :ref:`User Guide","660","        <nu_svc>`) and a lower bound of the fraction of support vectors.","661","        Should be in the interval (0, 1].","690","        See the :ref:`User Guide <shrinking_svm>`.","723","        (n_samples, n_classes * (n_classes - 1) \/ 2). However, one-vs-one","724","        ('ovo') is always used as multi-class strategy. The parameter is","725","        ignored for binary classification.","747","        probability estimates. Ignored when `probability` is False.","763","        Dual coefficients of the support vector in the decision","764","        function (see :ref:`sgd_mathematical_formulation`), multiplied by","765","        their targets.","768","        non-trivial. See the :ref:`multi-class section of the User Guide","769","        <svm_multi_class>` for details.","810","    >>> from sklearn.pipeline import make_pipeline","811","    >>> from sklearn.preprocessing import StandardScaler","813","    >>> clf = make_pipeline(StandardScaler(), NuSVC())","815","    Pipeline(steps=[('standardscaler', StandardScaler()), ('nusvc', NuSVC())])","922","        See the :ref:`User Guide <shrinking_svm>`.","962","    >>> from sklearn.pipeline import make_pipeline","963","    >>> from sklearn.preprocessing import StandardScaler","969","    >>> regr = make_pipeline(StandardScaler(), SVR(C=1.0, epsilon=0.2))","971","    Pipeline(steps=[('standardscaler', StandardScaler()),","972","                    ('svr', SVR(epsilon=0.2))])","973","","1069","        See the :ref:`User Guide <shrinking_svm>`.","1109","    >>> from sklearn.pipeline import make_pipeline","1110","    >>> from sklearn.preprocessing import StandardScaler","1116","    >>> regr = make_pipeline(StandardScaler(), NuSVR(C=1.0, nu=0.1))","1118","    Pipeline(steps=[('standardscaler', StandardScaler()),","1119","                    ('nusvr', NuSVR(nu=0.1))])","1197","        See the :ref:`User Guide <shrinking_svm>`."],"delete":["163","    >>> clf = LinearSVC(random_state=0, tol=1e-5)","165","    LinearSVC(random_state=0, tol=1e-05)","166","    >>> print(clf.coef_)","167","    [[0.085... 0.394... 0.498... 0.375...]]","168","    >>> print(clf.intercept_)","169","    [0.284...]","326","    >>> regr = LinearSVR(random_state=0, tol=1e-5)","328","    LinearSVR(random_state=0, tol=1e-05)","329","    >>> print(regr.coef_)","330","    [16.35... 26.91... 42.30... 60.47...]","331","    >>> print(regr.intercept_)","332","    [-4.29...]","334","    [-4.29...]","497","        ('ovo') is always used as multi-class strategy.","519","        probability estimates.","535","        Coefficients of the support vector in the decision function.","538","        non-trivial. See the section about multi-class classification in the","539","        SVM section of the User Guide for details.","581","    >>> clf = SVC(gamma='auto')","583","    SVC(gamma='auto')","639","        An upper bound on the fraction of training errors and a lower","640","        bound of the fraction of support vectors. Should be in the","641","        interval (0, 1].","702","        (n_samples, n_classes * (n_classes - 1) \/ 2).","724","        probability estimates.","740","        Coefficients of the support vector in the decision function.","743","        non-trivial. See the section about multi-class classification in","744","        the SVM section of the User Guide for details.","786","    >>> clf = NuSVC()","788","    NuSVC()","939","    >>> regr = SVR(C=1.0, epsilon=0.2)","941","    SVR(epsilon=0.2)","1081","    >>> regr = NuSVR(C=1.0, nu=0.1)","1083","    NuSVR(nu=0.1)"]}]}},"01c9ef3da3986f57db07c74565685f4b10eff10c":{"changes":{"sklearn\/metrics\/cluster\/_supervised.py":"MODIFY","doc\/whats_new\/v0.23.rst":"MODIFY","sklearn\/metrics\/cluster\/tests\/test_supervised.py":"MODIFY"},"diff":{"sklearn\/metrics\/cluster\/_supervised.py":[{"add":["647","    return np.clip(mi.sum(), 0.0, None)"],"delete":["647","    return mi.sum()"]}],"doc\/whats_new\/v0.23.rst":[{"add":["168","- |Fix| Fixed a bug in :func:`metrics.mutual_info_score` where negative","169","  scores could be returned. :pr:`16362` by `Thomas Fan`_.","170",""],"delete":[]}],"sklearn\/metrics\/cluster\/tests\/test_supervised.py":[{"add":["339","","340","","341","@pytest.mark.parametrize('labels_true, labels_pred', [","342","    (['a'] * 6, [1, 1, 0, 0, 1, 1]),","343","    ([1] * 6, [1, 1, 0, 0, 1, 1]),","344","    ([1, 1, 0, 0, 1, 1], ['a'] * 6),","345","    ([1, 1, 0, 0, 1, 1], [1] * 6),","346","])","347","def test_mutual_info_score_positive_constant_label(labels_true, labels_pred):","348","    # non-regression test for #16355","349","    assert mutual_info_score(labels_true, labels_pred) >= 0"],"delete":[]}]}},"b189bf60708af22dde82a00aca7b5a54290b666d":{"changes":{"sklearn\/preprocessing\/_data.py":"MODIFY","doc\/whats_new\/v0.23.rst":"MODIFY","sklearn\/preprocessing\/tests\/test_data.py":"MODIFY"},"diff":{"sklearn\/preprocessing\/_data.py":[{"add":["1720","            mins, maxes = min_max_axis(X, 1)","1721","            norms = np.maximum(abs(mins), maxes)","1731","            norms = np.max(abs(X), axis=1)","1749","    that its norm (l1, l2 or inf) equals one.","1766","        The norm to use to normalize each non zero sample. If norm='max'","1767","        is used, values will be rescaled by the maximum of the absolute","1768","        values."],"delete":["1720","            _, norms = min_max_axis(X, 1)","1730","            norms = np.max(X, axis=1)","1748","    that its norm (l1 or l2) equals one.","1765","        The norm to use to normalize each non zero sample."]}],"doc\/whats_new\/v0.23.rst":[{"add":["359","- |Fix| Fix a bug in :class:`preprocessing.Normalizer` with norm='max',","360","  which was not taking the absolute value of the maximum values before","361","  normalizing the vectors. :pr:`16632` by","362","  :user:`Maura Pintor <Maupin1991>` and :user:`Battista Biggio <bbiggio>`.","363",""],"delete":[]}],"sklearn\/preprocessing\/tests\/test_data.py":[{"add":["1949","            row_maxs = abs(X_norm).max(axis=1)","1968","def test_normalizer_max_sign():","1969","    # check that we normalize by a positive number even for negative data","1970","    rng = np.random.RandomState(0)","1971","    X_dense = rng.randn(4, 5)","1972","    # set the row number 3 to zero","1973","    X_dense[3, :] = 0.0","1974","    # check for mixed data where the value with","1975","    # largest magnitude is negative","1976","    X_dense[2, abs(X_dense[2, :]).argmax()] *= -1","1977","    X_all_neg = -np.abs(X_dense)","1978","    X_all_neg_sparse = sparse.csr_matrix(X_all_neg)","1979","","1980","    for X in (X_dense, X_all_neg, X_all_neg_sparse):","1981","        normalizer = Normalizer(norm='max')","1982","        X_norm = normalizer.transform(X)","1983","        assert X_norm is not X","1984","        X_norm = toarray(X_norm)","1985","        assert_array_equal(","1986","            np.sign(X_norm), np.sign(toarray(X)))","1987","","1988",""],"delete":["1949","            row_maxs = X_norm.max(axis=1)"]}]}},"da6065b088e03a09964a51f14ff8b069a3873cc7":{"changes":{"sklearn\/cluster\/_kmeans.py":"MODIFY"},"diff":{"sklearn\/cluster\/_kmeans.py":[{"add":["238","        Relative tolerance with regards to Frobenius norm of the difference","239","        in the cluster centers of two consecutive iterations to declare","240","        convergence.","685","        Relative tolerance with regards to Frobenius norm of the difference","686","        in the cluster centers of two consecutive iterations to declare","687","        convergence."],"delete":["238","        The relative increment in the results before declaring convergence.","683","        Relative tolerance with regards to inertia to declare convergence."]}]}},"fd12d5684ad224ad7760374b1dcca2821c644feb":{"changes":{"doc\/whats_new\/v0.23.rst":"MODIFY","sklearn\/decomposition\/_pca.py":"MODIFY","sklearn\/decomposition\/tests\/test_pca.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.23.rst":[{"add":["75",":mod:`sklearn.decomposition`","76","............................","77","","78","- |Fix| :class:`decomposition.PCA` with a float `n_components` parameter, will","79","   exclusively choose the components that explain the variance greater than","80","   `n_components`. :pr:`15669` by :user:`Krishna Chaitanya <krishnachaitanya9>`","81","","83","......................."],"delete":["76","................................."]}],"sklearn\/decomposition\/_pca.py":[{"add":["464","            # side='right' ensures that number of features selected","465","            # their variance is always greater than n_components float","466","            # passed. More discussion in issue: #15669","468","            n_components = np.searchsorted(ratio_cumsum, n_components,","469","                                           side='right') + 1"],"delete":["465","            n_components = np.searchsorted(ratio_cumsum, n_components) + 1","466",""]}],"sklearn\/decomposition\/tests\/test_pca.py":[{"add":["9","from sklearn.datasets import load_iris","558","","559","","560","def test_pca_n_components_mostly_explained_variance_ratio():","561","    # when n_components is the second highest cumulative sum of the","562","    # explained_variance_ratio_, then n_components_ should equal the","563","    # number of features in the dataset #15669","564","    X, y = load_iris(return_X_y=True)","565","    pca1 = PCA().fit(X, y)","566","","567","    n_components = pca1.explained_variance_ratio_.cumsum()[-2]","568","    pca2 = PCA(n_components=n_components).fit(X, y)","569","    assert pca2.n_components_ == X.shape[1]"],"delete":[]}]}}}