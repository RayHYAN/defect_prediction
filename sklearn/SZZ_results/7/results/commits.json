{"5d7df621a1b74b9b0c9ccd054f165a81fbb6a7ec":{"changes":{"sklearn\/metrics\/cluster\/tests\/test_supervised.py":"MODIFY","doc\/modules\/model_evaluation.rst":"MODIFY"},"diff":{"sklearn\/metrics\/cluster\/tests\/test_supervised.py":[{"add":["253","    # symmetric property","254","    score_symmetric = fowlkes_mallows_score(labels_b, labels_a)","255","    assert_almost_equal(score_symmetric, expected)","261","    # symmetric and permutation(both together)"],"delete":["253","    # symetric property","254","    score_symetric = fowlkes_mallows_score(labels_b, labels_a)","255","    assert_almost_equal(score_symetric, expected)","261","    # symetric and permutation(both together)"]}],"doc\/modules\/model_evaluation.rst":[{"add":["8","There are 3 different APIs for evaluating the quality of a model's","1123","predicted probability of the actual outcome :math:`o_t`."],"delete":["8","There are 3 different APIs for evaluating the quality of of a model's","1123","predicted probablity of the actual outcome :math:`o_t`."]}]}},"a0dfa306964f57625099b81fa0e558d3ed02ec24":{"changes":{"sklearn\/covariance\/shrunk_covariance_.py":"MODIFY","doc\/modules\/covariance.rst":"MODIFY"},"diff":{"sklearn\/covariance\/shrunk_covariance_.py":[{"add":["488","    In the original article, formula (23) states that 2\/p is multiplied by","489","    Trace(cov*cov) in both the numerator and denominator, this operation is","490","    omitted in the author's MATLAB program because for a large p, the value","491","    of 2\/p is so small that it doesn't affect the value of the estimator."],"delete":["488","    In the original article, formula (23) states that 2\/p is multiplied by ","489","    Trace(cov*cov) in both the numerator and denominator, this operation is omitted","490","    in the author's MATLAB program because for a large p, the value of 2\/p is so ","491","    small that it doesn't affect the value of the estimator. "]}],"doc\/modules\/covariance.rst":[{"add":["40","same mean vector as the training set. If not so, both should be centered by the","107",".. note:: **Case when population covariance matrix is isotropic**","108","","109","    It is important to note that when the number of samples is much larger than","110","    the number of features, one would expect that no shrinkage would be","111","    necessary. The intuition behind this is that if the population covariance","112","    is full rank, when the number of sample grows, the sample covariance will","113","    also become positive definite. As a result, no shrinkage would necessary","114","    and the method should automatically do this.","115","","116","    This, however, is not the case in the Ledoit-Wolf procedure when the","117","    population covariance happens to be a multiple of the identity matrix. In","118","    this case, the Ledoit-Wolf shrinkage estimate approaches 1 as the number of","119","    samples increases. This indicates that the optimal estimate of the","120","    covariance matrix in the Ledoit-Wolf sense is multiple of the identity.","121","    Since the population covariance is already a multiple of the identity","122","    matrix, the Ledoit-Wolf solution is indeed a reasonable estimate.","123",""],"delete":["40","same mean vector as the training set. If not so, both should be centered by the ","336",""]}]}},"add9b7f8f9774c91cacfc032f8496c239dfe9688":{"changes":{"sklearn\/model_selection\/tests\/test_validation.py":"MODIFY"},"diff":{"sklearn\/model_selection\/tests\/test_validation.py":[{"add":["318","                        \"Classification metrics can't handle a mix of \"","319","                        \"binary and continuous targets\",","323","                        \"Classification metrics can't handle a mix of \"","324","                        \"binary and continuous targets\","],"delete":["318","                        \"Can't handle mix of binary and continuous\",","322","                        \"Can't handle mix of binary and continuous\","]}]}},"c95784d14e2024ae09c425f6541bbb06554230b3":{"changes":{"sklearn\/neighbors\/lof.py":"MODIFY"},"diff":{"sklearn\/neighbors\/lof.py":[{"add":["108","        The opposite LOF of the training samples. The lower, the more abnormal."],"delete":["108","        The opposite LOF of the training samples. The lower, the more normal."]}]}},"27bbdb570bac062c71b3bb21b0876fd78adc9f7e":{"changes":{"sklearn\/externals\/joblib\/_parallel_backends.py":"MODIFY","sklearn\/utils\/validation.py":"MODIFY","sklearn\/utils\/testing.py":"MODIFY","examples\/calibration\/plot_compare_calibration.py":"MODIFY","sklearn\/externals\/joblib\/parallel.py":"MODIFY"},"diff":{"sklearn\/externals\/joblib\/_parallel_backends.py":[{"add":["90","        # Does nothing by default: to be overridden in subclasses when canceling"],"delete":["90","        # Does nothing by default: to be overriden in subclasses when canceling"]}],"sklearn\/utils\/validation.py":[{"add":["620","        Whether the parameter was found to be a named parameter of the"],"delete":["620","        Whether the parameter was found to be a a named parameter of the"]}],"sklearn\/utils\/testing.py":[{"add":["270","    category : tuple of warning class, default to Warning"],"delete":["270","    category : tuple of warning class, defaut to Warning"]}],"examples\/calibration\/plot_compare_calibration.py":[{"add":["35","  subsetting.\" As a result, the calibration curve shows a characteristic","36","  sigmoid shape, indicating that the classifier could trust its \"intuition\"","37","  more and return probabilities closer to 0 or 1 typically."],"delete":["35","  subseting.\" As a result, the calibration curve shows a characteristic sigmoid","36","  shape, indicating that the classifier could trust its \"intuition\" more and","37","  return probabilities closer to 0 or 1 typically."]}],"sklearn\/externals\/joblib\/parallel.py":[{"add":["50","# Thread local value that can be overridden by the ``parallel_backend`` context"],"delete":["50","# Thread local value that can be overriden by the ``parallel_backend`` context"]}]}},"ded2276d88c4c7b6b8954cf460c855e5ca3376fc":{"changes":{"doc\/modules\/feature_extraction.rst":"MODIFY"},"diff":{"doc\/modules\/feature_extraction.rst":[{"add":["492",":math:`\\text{tf-idf}_{\\text{term2}} = 0 \\times (log(6\/1)+1) = 0`","494",":math:`\\text{tf-idf}_{\\text{term3}} = 1 \\times (log(6\/2)+1) \\approx 2.0986`","498",":math:`\\text{tf-idf}_{\\text{raw}} = [3, 0, 2.0986].`"],"delete":["492",":math:`\\text{tf-idf}_{\\text{term2}} = 0 \\times log(6\/1)+1 = 0`","494",":math:`\\text{tf-idf}_{\\text{term3}} = 1 \\times log(6\/2)+1 \\approx 2.0986`","498",":math:`\\text{tf-idf}_raw = [3, 0, 2.0986].`"]}]}},"5cdaa90e3ec481b60a7a28edcb5c547238e4ecb2":{"changes":{"sklearn\/neighbors\/binary_tree.pxi":"MODIFY","CONTRIBUTING.md":"MODIFY","sklearn\/metrics\/tests\/test_common.py":"MODIFY","sklearn\/utils\/graph_shortest_path.pyx":"MODIFY","sklearn\/metrics\/cluster\/supervised.py":"MODIFY","doc\/developers\/contributing.rst":"MODIFY"},"diff":{"sklearn\/neighbors\/binary_tree.pxi":[{"add":["1079","        # with numbers of points between leaf_size and 2 * leaf_size"],"delete":["1079","        # with numbers of points betweeen leaf_size and 2 * leaf_size"]}],"CONTRIBUTING.md":[{"add":["182","   can be found by running the following code snippet:"],"delete":["182","   can be found by runnning the following code snippet:"]}],"sklearn\/metrics\/tests\/test_common.py":[{"add":["774","def test_normalize_option_multiclass_classification():"],"delete":["774","def test_normalize_option_multiclasss_classification():"]}],"sklearn\/utils\/graph_shortest_path.pyx":[{"add":["103","        on input, graph is the matrix of distances between connected points.","166","        dist_matrix is the matrix of distances between connected points.","173","        on input, graph is the matrix of distances between connected points."],"delete":["103","        on input, graph is the matrix of distances betweeen connected points.","166","        dist_matrix is the matrix of distances betweeen connected points.","173","        on input, graph is the matrix of distances betweeen connected points."]}],"sklearn\/metrics\/cluster\/supervised.py":[{"add":["152","    Perfectly matching labelings have a score of 1 even"],"delete":["152","    Perfectly maching labelings have a score of 1 even"]}],"doc\/developers\/contributing.rst":[{"add":["334","   can be found by running the following code snippet::"],"delete":["334","   can be found by runnning the following code snippet::"]}]}},"cceb9b22ac1c36341779aff59de8e6130671c19a":{"changes":{"examples\/applications\/plot_out_of_core_classification.py":"MODIFY"},"diff":{"examples\/applications\/plot_out_of_core_classification.py":[{"add":["43","from sklearn.externals.six.moves.urllib.request import urlretrieve","174","        urlretrieve(DOWNLOAD_URL, filename=archive_path,","175","                    reporthook=progress)"],"delete":["43","from sklearn.externals.six.moves import urllib","174","        urllib.request.urlretrieve(DOWNLOAD_URL, filename=archive_path,","175","                                   reporthook=progress)"]}]}},"b441308f36f948d2abe8bda3e666720bca120610":{"changes":{"sklearn\/model_selection\/_validation.py":"MODIFY","sklearn\/feature_extraction\/text.py":"MODIFY","sklearn\/ensemble\/gradient_boosting.py":"MODIFY","sklearn\/learning_curve.py":"MODIFY","sklearn\/utils\/__init__.py":"MODIFY"},"diff":{"sklearn\/model_selection\/_validation.py":[{"add":["1099","    if np.issubdtype(train_sizes_abs.dtype, np.floating):"],"delete":["1099","    if np.issubdtype(train_sizes_abs.dtype, np.float):"]}],"sklearn\/feature_extraction\/text.py":[{"add":["1088","        if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.floating):"],"delete":["1088","        if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):"]}],"sklearn\/ensemble\/gradient_boosting.py":[{"add":["155","        if np.issubdtype(y.dtype, np.signedinteger):"],"delete":["155","        if np.issubdtype(y.dtype, int):"]}],"sklearn\/learning_curve.py":[{"add":["208","    if np.issubdtype(train_sizes_abs.dtype, np.floating):"],"delete":["208","    if np.issubdtype(train_sizes_abs.dtype, np.float):"]}],"sklearn\/utils\/__init__.py":[{"add":["92","    if np.issubdtype(mask.dtype, np.signedinteger):"],"delete":["92","    if np.issubdtype(mask.dtype, np.int):"]}]}},"8e1efb0d9cc0e603292713b44dc78a7e8c77cd55":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/neighbors\/tests\/test_neighbors.py":"MODIFY","sklearn\/neighbors\/regression.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["129","","177","- Fixed a bug in :func:`datasets.make_circles`, where no odd number of data","178","  points could be generated. :issue:`10037` by :user:`Christian Braune","180","","192","Neighbors","193","","194","- Fixed a bug so ``predict`` in :class:`neighbors.RadiusNeighborsRegressor` can","195","  handle empty neighbor set when using non uniform weights. Also raises a new","196","  warning when no neighbors are found for samples.  :issue:`9655` by","197","  :user:`Andreas Bjerre-Nielsen <abjer>`.","198",""],"delete":["129","  ","177","- Fixed a bug in :func:`datasets.make_circles`, where no odd number of data ","178","  points could be generated. :issue:`10037` by :user:`Christian Braune ","180","  "]}],"sklearn\/neighbors\/tests\/test_neighbors.py":[{"add":["22","from sklearn.utils.testing import assert_warns_message","661","    # test that nan is returned when no nearby observations","662","    for weights in ['uniform', 'distance']:","663","        neigh = neighbors.RadiusNeighborsRegressor(radius=radius,","664","                                                   weights=weights,","665","                                                   algorithm='auto')","666","        neigh.fit(X, y)","667","        X_test_nan = np.ones((1, n_features))*-1","668","        empty_warning_msg = (\"One or more samples have no neighbors \"","669","                             \"within specified radius; predicting NaN.\")","670","        pred = assert_warns_message(UserWarning,","671","                                    empty_warning_msg,","672","                                    neigh.predict,","673","                                    X_test_nan)","674","        assert_true(np.all(np.isnan(pred)))","675",""],"delete":[]}],"sklearn\/neighbors\/regression.py":[{"add":["7","#          Empty radius support by Andreas Bjerre-Nielsen","9","# License: BSD 3 clause (C) INRIA, University of Amsterdam,","10","#                           University of Copenhagen","11","","12","import warnings","286","        y : array of float, shape = [n_samples] or [n_samples, n_outputs]","299","        empty_obs = np.full_like(_y[0], np.nan)","300","","303","                               if len(ind) else empty_obs","306","        else:","307","            y_pred = np.array([np.average(_y[ind, :], axis=0,","308","                               weights=weights[i])","309","                               if len(ind) else empty_obs","310","                               for (i, ind) in enumerate(neigh_ind)])","311","","312","        if np.max(np.isnan(y_pred)):","313","            empty_warning_msg = (\"One or more samples have no neighbors \"","314","                                 \"within specified radius; predicting NaN.\")","315","            warnings.warn(empty_warning_msg)","316","","317",""],"delete":["8","# License: BSD 3 clause (C) INRIA, University of Amsterdam","282","        y : array of int, shape = [n_samples] or [n_samples, n_outputs]","297","                               for ind in neigh_ind])","298","        else:","299","            y_pred = np.array([(np.average(_y[ind, :], axis=0,","300","                                           weights=weights[i]))"]}]}},"3f53d78ce238743eb2b76d3ea389b9b7c02f703a":{"changes":{"doc\/whats_new.rst":"MODIFY","sklearn\/decomposition\/pca.py":"MODIFY","sklearn\/decomposition\/tests\/test_pca.py":"MODIFY"},"diff":{"doc\/whats_new.rst":[{"add":["242","- Fixed the implementation of noise_variance_ in :class:`decomposition.PCA`.","243","  :issue:`9108` by `Hanmin Qin <https:\/\/github.com\/qinhanmin2014>`_.","244",""],"delete":[]}],"sklearn\/decomposition\/pca.py":[{"add":["203","        Equal to n_components largest eigenvalues","204","        of the covariance matrix of X.","205","","237","        Equal to the average of (min(n_features, n_samples) - n_components)","238","        smallest eigenvalues of the covariance matrix of X.","239","","502","        if self.n_components_ < min(n_features, n_samples):","505","            self.noise_variance_ \/= min(n_features, n_samples) - n_components"],"delete":["496","        if self.n_components_ < n_features:"]}],"sklearn\/decomposition\/tests\/test_pca.py":[{"add":["531","def test_pca_score_with_different_solvers():","532","    digits = datasets.load_digits()","533","    X_digits = digits.data","534","","535","    pca_dict = {svd_solver: PCA(n_components=30, svd_solver=svd_solver,","536","                                random_state=0)","537","                for svd_solver in solver_list}","538","","539","    for pca in pca_dict.values():","540","        pca.fit(X_digits)","541","        # Sanity check for the noise_variance_. For more details see","542","        # https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/7568","543","        # https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/8541","544","        # https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/8544","545","        assert np.all((pca.explained_variance_ - pca.noise_variance_) >= 0)","546","","547","    # Compare scores with different svd_solvers","548","    score_dict = {svd_solver: pca.score(X_digits)","549","                  for svd_solver, pca in pca_dict.items()}","550","    assert_almost_equal(score_dict['full'], score_dict['arpack'])","551","    assert_almost_equal(score_dict['full'], score_dict['randomized'],","552","                        decimal=3)","553","","554","","555","def test_pca_zero_noise_variance_edge_cases():","556","    # ensure that noise_variance_ is 0 in edge cases","557","    # when n_components == min(n_samples, n_features)","558","    n, p = 100, 3","559","","560","    rng = np.random.RandomState(0)","561","    X = rng.randn(n, p) * .1 + np.array([3, 4, 5])","562","    # arpack raises ValueError for n_components == min(n_samples,","563","    # n_features)","564","    svd_solvers = ['full', 'randomized']","565","","566","    for svd_solver in svd_solvers:","567","        pca = PCA(svd_solver=svd_solver, n_components=p)","568","        pca.fit(X)","569","        assert pca.noise_variance_ == 0","570","","571","        pca.fit(X.T)","572","        assert pca.noise_variance_ == 0","573","","574",""],"delete":[]}]}},"502261bd9deca6ba57b4a051da2b37f170f28e88":{"changes":{"build_tools\/circle\/build_doc.sh":"MODIFY","examples\/neural_networks\/plot_mlp_training_curves.py":"MODIFY"},"diff":{"build_tools\/circle\/build_doc.sh":[{"add":["109","  cython nose coverage matplotlib sphinx=1.6.2 pillow"],"delete":["109","  cython nose coverage 'matplotlib=2.0.*|>2.1.0' sphinx=1.6.2 pillow"]}],"examples\/neural_networks\/plot_mlp_training_curves.py":[{"add":["87","fig.legend(ax.get_lines(), labels, ncol=3, loc=\"upper center\")"],"delete":["87","fig.legend(ax.get_lines(), labels=labels, ncol=3, loc=\"upper center\")"]}]}},"682e85fb1afcd050d9559b4d156593b2c32280df":{"changes":{"sklearn\/utils\/tests\/test_utils.py":"MODIFY","sklearn\/utils\/__init__.py":"MODIFY"},"diff":{"sklearn\/utils\/tests\/test_utils.py":[{"add":["0","from itertools import chain, product","202","    inds_readonly = inds.copy()","203","    inds_readonly.setflags(write=False)","205","    for this_df, this_inds in product([X_df, X_df_readonly],","206","                                      [inds, inds_readonly]):","207","        with warnings.catch_warnings(record=True):","208","            X_df_indexed = safe_indexing(this_df, this_inds)","209","","210","        assert_array_equal(np.array(X_df_indexed), X_indexed)"],"delete":["0","from itertools import chain","202","    with warnings.catch_warnings(record=True):","203","        X_df_ro_indexed = safe_indexing(X_df_readonly, inds)","205","    assert_array_equal(np.array(X_df_ro_indexed), X_indexed)"]}],"sklearn\/utils\/__init__.py":[{"add":["144","        # Work-around for indexing with read-only indices in pandas","145","        indices = indices if indices.flags.writeable else indices.copy()"],"delete":[]}]}},"3593194b69c4330261925b451dd8e445c17c560a":{"changes":{"examples\/applications\/plot_stock_market.py":"MODIFY"},"diff":{"examples\/applications\/plot_stock_market.py":[{"add":["126","    min_date = min(data['date']) if len(data) else datetime.min.date()","127","    max_date = max(data['date']) if len(data) else datetime.max.date()"],"delete":["126","    min_date = min(data['date'], default=datetime.min.date())","127","    max_date = max(data['date'], default=datetime.max.date())"]}]}},"ceaa09ba291702dd304aab4538974701f86355ba":{"changes":{"sklearn\/manifold\/_barnes_hut_tsne.pyx":"MODIFY","sklearn\/manifold\/t_sne.py":"MODIFY"},"diff":{"sklearn\/manifold\/_barnes_hut_tsne.pyx":[{"add":["135","            qij = ((1.0 + dij \/ dof) ** exponent)","197","            qijZ = (1.0 + dist2s \/ dof) ** exponent  # 1\/(1+dist)"],"delete":["135","            qij = (((1.0 + dij) \/ dof) ** exponent)","197","            qijZ = ((1.0 + dist2s) \/ dof) ** exponent  # 1\/(1+dist)"]}],"sklearn\/manifold\/t_sne.py":[{"add":["161","    dist += 1."],"delete":["160","    dist += 1."]}]}},"d074e403f11a357d66a967e8469a397a44af83d3":{"changes":{"sklearn\/model_selection\/_split.py":"MODIFY","sklearn\/model_selection\/tests\/test_split.py":"MODIFY"},"diff":{"sklearn\/model_selection\/_split.py":[{"add":["1536","            # for multi-label y, map each distinct row to a string repr","1537","            # using join because str(row) uses an ellipsis if len(row) > 1000","1538","            y = np.array([' '.join(row.astype('str')) for row in y])"],"delete":["1536","            # for multi-label y, map each distinct row to its string repr:","1537","            y = np.array([str(row) for row in y])"]}],"sklearn\/model_selection\/tests\/test_split.py":[{"add":["728","def test_stratified_shuffle_split_multilabel_many_labels():","729","    # fix in PR #9922: for multilabel data with > 1000 labels, str(row)","730","    # truncates with an ellipsis for elements in positions 4 through","731","    # len(row) - 4, so labels were not being correctly split using the powerset","732","    # method for transforming a multilabel problem to a multiclass one; this","733","    # test checks that this problem is fixed.","734","    row_with_many_zeros = [1, 0, 1] + [0] * 1000 + [1, 0, 1]","735","    row_with_many_ones = [1, 0, 1] + [1] * 1000 + [1, 0, 1]","736","    y = np.array([row_with_many_zeros] * 10 + [row_with_many_ones] * 100)","737","    X = np.ones_like(y)","738","","739","    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=0)","740","    train, test = next(sss.split(X=X, y=y))","741","    y_train = y[train]","742","    y_test = y[test]","743","","744","    # correct stratification of entire rows","745","    # (by design, here y[:, 4] uniquely determines the entire row of y)","746","    expected_ratio = np.mean(y[:, 4])","747","    assert_equal(expected_ratio, np.mean(y_train[:, 4]))","748","    assert_equal(expected_ratio, np.mean(y_test[:, 4]))","749","","750",""],"delete":[]}]}},"c6fefb0caaeb433c2e11f0d8829a878c39015194":{"changes":{"sklearn\/gaussian_process\/gaussian_process.py":"MODIFY"},"diff":{"sklearn\/gaussian_process\/gaussian_process.py":[{"add":["721","                                             np.log10(theta0).ravel(),","722","                                             constraints, disp=0)"],"delete":["721","                                             np.log10(theta0).ravel(), constraints,","722","                                             iprint=0)"]}]}},"04be1a97993342dcae7ff2736f85c5ab4eeb1266":{"changes":{"sklearn\/datasets\/lfw.py":"MODIFY","doc\/modules\/clustering.rst":"MODIFY","doc\/modules\/multiclass.rst":"MODIFY","examples\/ensemble\/plot_adaboost_hastie_10_2.py":"MODIFY","examples\/ensemble\/plot_gradient_boosting_regularization.py":"MODIFY","doc\/modules\/outlier_detection.rst":"MODIFY","doc\/modules\/ensemble.rst":"MODIFY","sklearn\/covariance\/robust_covariance.py":"MODIFY","doc\/modules\/linear_model.rst":"MODIFY","sklearn\/metrics\/scorer.py":"MODIFY","sklearn\/neighbors\/approximate.py":"MODIFY","sklearn\/model_selection\/_validation.py":"MODIFY","examples\/ensemble\/plot_adaboost_regression.py":"MODIFY","doc\/tutorial\/statistical_inference\/putting_together.rst":"MODIFY","sklearn\/mixture\/dpgmm.py":"MODIFY","examples\/ensemble\/plot_ensemble_oob.py":"MODIFY","examples\/ensemble\/plot_adaboost_multiclass.py":"MODIFY","sklearn\/linear_model\/randomized_l1.py":"MODIFY","doc\/modules\/covariance.rst":"MODIFY","doc\/modules\/calibration.rst":"MODIFY","sklearn\/neighbors\/lof.py":"MODIFY","sklearn\/model_selection\/_search.py":"MODIFY"},"diff":{"sklearn\/datasets\/lfw.py":[{"add":["70",""],"delete":[]}],"doc\/modules\/clustering.rst":[{"add":["1345"," * `V-Measure: A conditional entropy-based external cluster evaluation"],"delete":["1345"," .. [RH2007] `V-Measure: A conditional entropy-based external cluster evaluation"]}],"doc\/modules\/multiclass.rst":[{"add":["253","    * \"Pattern Recognition and Machine Learning. Springer\",","254","      Christopher M. Bishop, page 183, (First Edition)","317","    * \"Solving multiclass learning problems via error-correcting output codes\",","318","      Dietterich T., Bakiri G.,","319","      Journal of Artificial Intelligence Research 2,","320","      1995.","327","    * \"The Elements of Statistical Learning\",","328","      Hastie T., Tibshirani R., Friedman J., page 606 (second-edition)","329","      2008."],"delete":["253","    .. [1] \"Pattern Recognition and Machine Learning. Springer\",","254","        Christopher M. Bishop, page 183, (First Edition)","317","    .. [2] \"Solving multiclass learning problems via error-correcting output codes\",","318","        Dietterich T., Bakiri G.,","319","        Journal of Artificial Intelligence Research 2,","320","        1995.","327","    .. [4] \"The Elements of Statistical Learning\",","328","        Hastie T., Tibshirani R., Friedman J., page 606 (second-edition)","329","        2008."]}],"examples\/ensemble\/plot_adaboost_hastie_10_2.py":[{"add":["5","This example is based on Figure 10.2 from Hastie et al 2009 [1]_ and","6","illustrates the difference in performance between the discrete SAMME [2]_","7","boosting algorithm and real SAMME.R boosting algorithm. Both algorithms are","8","evaluated on a binary classification task where the target Y is a non-linear","9","function of 10 input features."],"delete":["5","This example is based on Figure 10.2 from Hastie et al 2009 [1] and illustrates","6","the difference in performance between the discrete SAMME [2] boosting","7","algorithm and real SAMME.R boosting algorithm. Both algorithms are evaluated","8","on a binary classification task where the target Y is a non-linear function","9","of 10 input features."]}],"examples\/ensemble\/plot_gradient_boosting_regularization.py":[{"add":["6","for Gradient Boosting. The example is taken from Hastie et al 2009 [1]_."],"delete":["6","for Gradient Boosting. The example is taken from Hastie et al 2009."]}],"doc\/modules\/outlier_detection.rst":[{"add":["128","    * Rousseeuw, P.J., Van Driessen, K. \"A fast algorithm for the minimum","129","      covariance determinant estimator\" Technometrics 41(3), 212 (1999)","174","    * Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. \"Isolation forest.\"","175","      Data Mining, 2008. ICDM'08. Eighth IEEE International Conference on.","230","   *  Breunig, Kriegel, Ng, and Sander (2000)","274","        and :class:`neighbors.LocalOutlierFactor` perform as well.","281","        :class:`svm.OneClassSVM` and :class:`neighbors.LocalOutlierFactor`","282","        have difficulties to detect the two modes,","283","        and that the :class:`svm.OneClassSVM`","294","        whereas the :class:`covariance.EllipticEnvelope` completely fails."],"delete":["128","    ..  [RD1999] Rousseeuw, P.J., Van Driessen, K. \"A fast algorithm for the minimum","129","        covariance determinant estimator\" Technometrics 41(3), 212 (1999)","174","    .. [LTZ2008] Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. \"Isolation forest.\"","175","           Data Mining, 2008. ICDM'08. Eighth IEEE International Conference on.","230","   .. [BKNS2000]  Breunig, Kriegel, Ng, and Sander (2000)","274","\tand :class:`neighbors.LocalOutlierFactor` perform as well.","281","\t:class:`svm.OneClassSVM` and :class:`neighbors.LocalOutlierFactor`","282","\thave difficulties to detect the two modes,","283","\tand that the :class:`svm.OneClassSVM`","294","\twhereas the :class:`covariance.EllipticEnvelope` completely fails."]}],"doc\/modules\/ensemble.rst":[{"add":["248"," * P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized"],"delete":["248"," .. [GEW2006] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized"]}],"sklearn\/covariance\/robust_covariance.py":[{"add":["192","    [RV]_.","252","    .. [RV] A Fast Algorithm for the Minimum Covariance Determinant","341","    the correction and reweighting steps described in [RouseeuwVan]_,","347","    .. [RouseeuwVan] A Fast Algorithm for the Minimum Covariance","582","    .. [Rousseeuw] `A Fast Algorithm for the Minimum Covariance Determinant","585","    .. [ButlerDavies] `R. W. Butler, P. L. Davies and M. Jhun,","652","        by Rousseeuw and Van Driessen in [RVD]_.","661","        References","662","        ----------","663","","664","        .. [RVD] `A Fast Algorithm for the Minimum Covariance","665","            Determinant Estimator, 1999, American Statistical Association","666","            and the American Society for Quality, TECHNOMETRICS`","667","","684","        computing location and covariance estimates) described","685","        in [RVDriessen]_.","694","        References","695","        ----------","696","","697","        .. [RVDriessen] `A Fast Algorithm for the Minimum Covariance","698","            Determinant Estimator, 1999, American Statistical Association","699","            and the American Society for Quality, TECHNOMETRICS`","700",""],"delete":["192","    [Rouseeuw1999]_.","252","    .. [Rouseeuw1999] A Fast Algorithm for the Minimum Covariance Determinant","341","    the correction and reweighting steps described in [Rouseeuw1999]_,","347","    .. [Rouseeuw1999] A Fast Algorithm for the Minimum Covariance","582","    .. [Rouseeuw1999] `A Fast Algorithm for the Minimum Covariance Determinant","585","    .. [Butler1993] `R. W. Butler, P. L. Davies and M. Jhun,","652","        by Rousseeuw and Van Driessen in [Rouseeuw1984]_.","677","        computing location and covariance estimates). [Rouseeuw1984]_"]}],"doc\/modules\/linear_model.rst":[{"add":["1143","  * Peter J. Huber, Elvezio M. Ronchetti: Robust Statistics, Concomitant scale estimates, pg 172"],"delete":["1143","    .. [#f1] Peter J. Huber, Elvezio M. Ronchetti: Robust Statistics, Concomitant scale estimates, pg 172"]}],"sklearn\/metrics\/scorer.py":[{"add":["322","        See :ref:`multimetric_grid_search` for an example."],"delete":["322","        See :ref:`multivalued_scorer_wrapping` for an example."]}],"sklearn\/neighbors\/approximate.py":[{"add":[],"delete":["124","    Read more in the :ref:`User Guide <approximate_nearest_neighbors>`.","125",""]}],"sklearn\/model_selection\/_validation.py":[{"add":["71","        See :ref:`multimetric_grid_search` for an example.","805","        A single string (see :ref:`scoring_parameter`) or a callable","806","        (see :ref:`scoring`) to evaluate the predictions on the test set."],"delete":["71","        See :ref:`multivalued_scorer_wrapping` for an example.","805","        A single string (see :ref:`_scoring_parameter`) or a callable","806","        (see :ref:`_scoring`) to evaluate the predictions on the test set."]}],"examples\/ensemble\/plot_adaboost_regression.py":[{"add":["5","A decision tree is boosted using the AdaBoost.R2 [1]_ algorithm on a 1D"],"delete":["5","A decision tree is boosted using the AdaBoost.R2 [1] algorithm on a 1D"]}],"doc\/tutorial\/statistical_inference\/putting_together.rst":[{"add":["19","    :lines: 23-63"],"delete":["19","    :lines: 26-66"]}],"sklearn\/mixture\/dpgmm.py":[{"add":["674","    Read more in the :ref:`User Guide <bgmm>`."],"delete":["674","    Read more in the :ref:`User Guide <vbgmm>`."]}],"examples\/ensemble\/plot_ensemble_oob.py":[{"add":["10","``RandomForestClassifier`` to be fit and validated whilst being trained [1]_."],"delete":["10","``RandomForestClassifier`` to be fit and validated whilst being trained [1]."]}],"examples\/ensemble\/plot_adaboost_multiclass.py":[{"add":["5","This example reproduces Figure 1 of Zhu et al [1]_ and shows how boosting can","12","The performance of the SAMME and SAMME.R [1]_ algorithms are compared. SAMME.R"],"delete":["5","This example reproduces Figure 1 of Zhu et al [1] and shows how boosting can","12","The performance of the SAMME and SAMME.R [1] algorithms are compared. SAMME.R"]}],"sklearn\/linear_model\/randomized_l1.py":[{"add":["206","        features.","413","        features."],"delete":["197","    Read more in the :ref:`User Guide <randomized_l1>`.","198","","208","        features (See :ref:`User Guide <randomized_l1>` for details ).","302","    Notes","303","    -----","304","    For an example, see :ref:`examples\/linear_model\/plot_sparse_recovery.py","305","    <sphx_glr_auto_examples_linear_model_plot_sparse_recovery.py>`.","306","","409","    Read more in the :ref:`User Guide <randomized_l1>`.","410","","422","        features (See :ref:`User Guide <randomized_l1>` for details ).","503","    Notes","504","    -----","505","    For an example, see :ref:`examples\/linear_model\/plot_sparse_recovery.py","506","    <sphx_glr_auto_examples_linear_model_plot_sparse_recovery.py>`.","507","","592","    Read more in the :ref:`User Guide <randomized_l1>`.","593","","640","","641","    Notes","642","    -----","643","    For an example, see :ref:`examples\/linear_model\/plot_sparse_recovery.py","644","    <sphx_glr_auto_examples_linear_model_plot_sparse_recovery.py>`."]}],"doc\/modules\/covariance.rst":[{"add":["97","In their 2004 paper [1]_, O. Ledoit and M. Wolf propose a formula so as","114",".. topic:: References:","116","    .. [1] O. Ledoit and M. Wolf, \"A Well-Conditioned Estimator for Large-Dimensional","117","           Covariance Matrices\", Journal of Multivariate Analysis, Volume 88, Issue 2,","118","           February 2004, pages 365-411.","126","al. [2]_ derived a formula aimed at choosing a shrinkage coefficient that","144",".. topic:: References:","145","","146","    .. [2] Chen et al., \"Shrinkage Algorithms for MMSE Covariance Estimation\",","147","           IEEE Trans. on Sign. Proc., Volume 58, Issue 10, October 2010.","271","the Minimum Covariance Determinant [3]_.","278","a data set's covariance introduced by P.J. Rousseeuw in [3]_.  The idea","288","Rousseeuw and Van Driessen [4]_ developed the FastMCD algorithm in order","297",".. topic:: References:","298","","299","    .. [3] P. J. Rousseeuw. Least median of squares regression.","300","           J. Am Stat Ass, 79:871, 1984.","301","    .. [4] A Fast Algorithm for the Minimum Covariance Determinant Estimator,","302","           1999, American Statistical Association and the American Society","303","           for Quality, TECHNOMETRICS."],"delete":["97","In their 2004 paper [1], O. Ledoit and M. Wolf propose a formula so as","115","[1] O. Ledoit and M. Wolf, \"A Well-Conditioned Estimator for Large-Dimensional","116","    Covariance Matrices\", Journal of Multivariate Analysis, Volume 88, Issue 2,","117","    February 2004, pages 365-411.","125","al. [2] derived a formula aimed at choosing a shrinkage coefficient that","143","[2] Chen et al., \"Shrinkage Algorithms for MMSE Covariance Estimation\",","144","    IEEE Trans. on Sign. Proc., Volume 58, Issue 10, October 2010.","268","the Minimum Covariance Determinant [3].","275","a data set's covariance introduced by P.J. Rousseeuw in [3].  The idea","285","Rousseeuw and Van Driessen [4] developed the FastMCD algorithm in order","294","[3] P. J. Rousseeuw. Least median of squares regression.","295","    J. Am Stat Ass, 79:871, 1984.","296","[4] A Fast Algorithm for the Minimum Covariance Determinant Estimator,","297","    1999, American Statistical Association and the American Society","298","    for Quality, TECHNOMETRICS."]}],"doc\/modules\/calibration.rst":[{"add":["46","   and Caruana [4]_: \"Methods such as bagging and random forests that average","59","   calibration curve also referred to as the reliability diagram (Wilks 1995 [5]_) shows a","67","   (compare Niculescu-Mizil and Caruana [4]_), which focus on hard samples","192","    * Obtaining calibrated probability estimates from decision trees","193","      and naive Bayesian classifiers, B. Zadrozny & C. Elkan, ICML 2001","195","    * Transforming Classifier Scores into Accurate Multiclass","196","      Probability Estimates, B. Zadrozny & C. Elkan, (KDD 2002)","198","    * Probabilistic Outputs for Support Vector Machines and Comparisons to","199","      Regularized Likelihood Methods, J. Platt, (1999)","202","           A. Niculescu-Mizil & R. Caruana, ICML 2005","205","           consecutive precipitation periods. Wea. Forecasting, 5, 640\u2013650.,","206","           Wilks, D. S., 1990a"],"delete":["46","   and Caruana [4]: \"Methods such as bagging and random forests that average","59","   calibration curve also referred to as the reliability diagram (Wilks 1995[5]) shows a","67","   (compare Niculescu-Mizil and Caruana [4]), which focus on hard samples","192","    .. [1] Obtaining calibrated probability estimates from decision trees","193","          and naive Bayesian classifiers, B. Zadrozny & C. Elkan, ICML 2001","195","    .. [2] Transforming Classifier Scores into Accurate Multiclass","196","          Probability Estimates, B. Zadrozny & C. Elkan, (KDD 2002)","198","    .. [3] Probabilistic Outputs for Support Vector Machines and Comparisons to","199","          Regularized Likelihood Methods, J. Platt, (1999)","202","          A. Niculescu-Mizil & R. Caruana, ICML 2005","205","         consecutive precipitation periods. Wea. Forecasting, 5, 640\u2013","206","         650., Wilks, D. S., 1990a"]}],"sklearn\/neighbors\/lof.py":[{"add":["87","        :func:`sklearn.metrics.pairwise.pairwise_distances`. When p = 1, this","88","        is equivalent to using manhattan_distance (l1), and euclidean_distance"],"delete":["87","        :ref:`sklearn.metrics.pairwise.pairwise_distances`. When p = 1, this is","88","        equivalent to using manhattan_distance (l1), and euclidean_distance"]}],"sklearn\/model_selection\/_search.py":[{"add":["803","        See :ref:`multimetric_grid_search` for an example.","1113","        See :ref:`multimetric_grid_search` for an example."],"delete":["803","        See :ref:`multivalued_scorer_wrapping` for an example.","1113","        See :ref:`multivalued_scorer_wrapping` for an example."]}]}},"334850fa2b4c6d7881c2e4adbaeb6235c94638a3":{"changes":{"sklearn\/model_selection\/_split.py":"MODIFY","examples\/multioutput\/README.txt":"MODIFY"},"diff":{"sklearn\/model_selection\/_split.py":[{"add":["86","        Notes","87","        -----","317","        Notes","318","        -----","657","        Notes","658","        -----","744","        Notes","745","        -----","1188","        Notes","1189","        -----","1608","        Notes","1609","        -----"],"delete":["86","        Note","87","        ----","317","        Note","318","        ----","657","        Note","658","        ----","744","        Note","745","        ----","1188","        Note","1189","        ----","1608","        Note","1609","        ----"]}],"examples\/multioutput\/README.txt":[{"add":["3","-------------------","5","Examples concerning the :mod:`sklearn.multioutput` module."],"delete":["3","----------------","5","Examples concerning the :mod:`sklearn.multioutput` module."]}]}},"4d414d41e488f566c328fb97a1d475cee833b0df":{"changes":{"examples\/applications\/plot_stock_market.py":"MODIFY"},"diff":{"examples\/applications\/plot_stock_market.py":[{"add":["79","","105","    response = urlopen(url)","106","    dtype = {","107","        'names': ['date', 'open', 'high', 'low', 'close', 'volume'],","108","        'formats': ['object', 'f4', 'f4', 'f4', 'f4', 'f4']","109","    }","110","    converters = {0: lambda s: datetime.strptime(s.decode(), '%d-%b-%y')}","111","    return np.genfromtxt(response, delimiter=',', skip_header=1,","112","                         dtype=dtype, converters=converters,","113","                         missing_values='-', filling_values=-1)","185","close_prices = np.vstack([q['close'] for q in quotes])","186","open_prices = np.vstack([q['open'] for q in quotes])"],"delete":["104","    with urlopen(url) as response:","105","        dtype = {","106","            'names': ['date', 'open', 'high', 'low', 'close', 'volume'],","107","            'formats': ['object', 'f4', 'f4', 'f4', 'f4', 'f4']","108","        }","109","        converters = {0: lambda s: datetime.strptime(s.decode(), '%d-%b-%y')}","110","        return np.genfromtxt(response, delimiter=',', skip_header=1,","111","                             dtype=dtype, converters=converters,","112","                             missing_values='-', filling_values=-1)","184","close_prices = np.stack([q['close'] for q in quotes])","185","open_prices = np.stack([q['open'] for q in quotes])"]}]}},"f871e1d1e1486e8549dd8af1db0ed0ccc7737b2c":{"changes":{"sklearn\/ensemble\/gradient_boosting.py":"MODIFY"},"diff":{"sklearn\/ensemble\/gradient_boosting.py":[{"add":["1528","        y : array of shape = [n_samples]"],"delete":["1528","        y : array of shape = [\"n_samples]"]}]}},"370b642f7ae44d4157001d09d3a4cf1a2b990847":{"changes":{"sklearn\/neural_network\/multilayer_perceptron.py":"MODIFY","sklearn\/neural_network\/tests\/test_mlp.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY"},"diff":{"sklearn\/neural_network\/multilayer_perceptron.py":[{"add":["53","                 n_iter_no_change, max_fun):","77","        self.max_fun = max_fun","383","        if self.max_fun <= 0:","384","            raise ValueError(\"max_fun must be > 0, got %s.\" % self.max_fun)","463","            maxfun=self.max_fun,","464","            maxiter=self.max_iter,","468","        self.n_iter_ = d['nit']","469","        if d['warnflag'] == 1:","470","            if d['nit'] >= self.max_iter:","471","                warnings.warn(","472","                    \"LBFGS Optimizer: Maximum iterations (%d) \"","473","                    \"reached and the optimization hasn't converged yet.\"","474","                    % self.max_iter, ConvergenceWarning)","475","            if d['funcalls'] >= self.max_fun:","476","                warnings.warn(","477","                    \"LBFGS Optimizer: Maximum function evaluations (%d) \"","478","                    \"reached and the optimization hasn't converged yet.\"","479","                    % self.max_fun, ConvergenceWarning)","480","        elif d['warnflag'] == 2:","481","            warnings.warn(","482","                \"LBFGS Optimizer: Optimization hasn't converged yet, \"","483","                \"cause of LBFGS stopping: %s.\"","484","                % d['task'], ConvergenceWarning)","485","","856","    max_fun : int, optional, default 15000","857","        Only used when solver='lbfgs'. Maximum number of loss function calls.","858","        The solver iterates until convergence (determined by 'tol'), number","859","        of iterations reaches max_iter, or this number of loss function calls.","860","        Note that number of loss function calls will be greater than or equal","861","        to the number of iterations for the `MLPClassifier`.","862","","863","        .. versionadded:: 0.22","864","","930","                 epsilon=1e-8, n_iter_no_change=10, max_fun=15000):","943","            n_iter_no_change=n_iter_no_change, max_fun=max_fun)","1247","    max_fun : int, optional, default 15000","1248","        Only used when solver='lbfgs'. Maximum number of function calls.","1249","        The solver iterates until convergence (determined by 'tol'), number","1250","        of iterations reaches max_iter, or this number of function calls.","1251","        Note that number of function calls will be greater than or equal to","1252","        the number of iterations for the MLPRegressor.","1253","","1254","        .. versionadded:: 0.22","1255","","1319","                 epsilon=1e-8, n_iter_no_change=10, max_fun=15000):","1332","            n_iter_no_change=n_iter_no_change, max_fun=max_fun)"],"delete":["53","                 n_iter_no_change):","174","        self.n_iter_ += 1","461","            maxfun=self.max_iter,","900","                 epsilon=1e-8, n_iter_no_change=10):","901","","914","            n_iter_no_change=n_iter_no_change)","1281","                 epsilon=1e-8, n_iter_no_change=10):","1282","","1295","            n_iter_no_change=n_iter_no_change)"]}],"sklearn\/neural_network\/tests\/test_mlp.py":[{"add":["50","regression_datasets = [(Xboston, yboston)]","51","","232","@pytest.mark.parametrize('X,y', classification_datasets)","233","def test_lbfgs_classification(X, y):","237","    X_train = X[:150]","238","    y_train = y[:150]","239","    X_test = X[150:]","240","    expected_shape_dtype = (X_test.shape[0], y_train.dtype.kind)","242","    for activation in ACTIVATION_TYPES:","243","        mlp = MLPClassifier(solver='lbfgs', hidden_layer_sizes=50,","244","                            max_iter=150, shuffle=True, random_state=1,","245","                            activation=activation)","246","        mlp.fit(X_train, y_train)","247","        y_predict = mlp.predict(X_test)","248","        assert mlp.score(X_train, y_train) > 0.95","249","        assert ((y_predict.shape[0], y_predict.dtype.kind) ==","250","                expected_shape_dtype)","253","@pytest.mark.parametrize('X,y', regression_datasets)","254","def test_lbfgs_regression(X, y):","268","@pytest.mark.parametrize('X,y', classification_datasets)","269","def test_lbfgs_classification_maxfun(X, y):","270","    # Test lbfgs parameter max_fun.","271","    # It should independently limit the number of iterations for lbfgs.","272","    max_fun = 10","273","    # classification tests","274","    for activation in ACTIVATION_TYPES:","275","        mlp = MLPClassifier(solver='lbfgs', hidden_layer_sizes=50,","276","                            max_iter=150, max_fun=max_fun, shuffle=True,","277","                            random_state=1, activation=activation)","278","        with pytest.warns(ConvergenceWarning):","279","            mlp.fit(X, y)","280","            assert max_fun >= mlp.n_iter_","281","","282","","283","@pytest.mark.parametrize('X,y', regression_datasets)","284","def test_lbfgs_regression_maxfun(X, y):","285","    # Test lbfgs parameter max_fun.","286","    # It should independently limit the number of iterations for lbfgs.","287","    max_fun = 10","288","    # regression tests","289","    for activation in ACTIVATION_TYPES:","290","        mlp = MLPRegressor(solver='lbfgs', hidden_layer_sizes=50,","291","                           max_iter=150, max_fun=max_fun, shuffle=True,","292","                           random_state=1, activation=activation)","293","        with pytest.warns(ConvergenceWarning):","294","            mlp.fit(X, y)","295","            assert max_fun >= mlp.n_iter_","296","","297","    mlp.max_fun = -1","298","    assert_raises(ValueError, mlp.fit, X, y)","299","","300",""],"delete":["230","def test_lbfgs_classification():","234","    for X, y in classification_datasets:","235","        X_train = X[:150]","236","        y_train = y[:150]","237","        X_test = X[150:]","239","        expected_shape_dtype = (X_test.shape[0], y_train.dtype.kind)","240","","241","        for activation in ACTIVATION_TYPES:","242","            mlp = MLPClassifier(solver='lbfgs', hidden_layer_sizes=50,","243","                                max_iter=150, shuffle=True, random_state=1,","244","                                activation=activation)","245","            mlp.fit(X_train, y_train)","246","            y_predict = mlp.predict(X_test)","247","            assert mlp.score(X_train, y_train) > 0.95","248","            assert ((y_predict.shape[0], y_predict.dtype.kind) ==","249","                         expected_shape_dtype)","252","def test_lbfgs_regression():","254","    X = Xboston","255","    y = yboston"]}],"doc\/whats_new\/v0.22.rst":[{"add":["124","","125",":mod:`sklearn.neural_network`","126",".............................","127","","128","- |Feature| Add `max_fun` parameter in","129","  :class:`neural_network.BaseMultilayerPerceptron`,","130","  :class:`neural_network.MLPRegressor`, and","131","  :class:`neural_network.MLPClassifier` to give control over","132","  maximum number of function evaluation to not meet ``tol`` improvement.","133","  :issue:`9274` by :user:`Daniel Perry <daniel-perry>`.","134","","135",""],"delete":[]}]}},"8cee70d018722250929ca50ccef3e061ad8b40c5":{"changes":{"sklearn\/neighbors\/dist_metrics.pyx":"MODIFY"},"diff":{"sklearn\/neighbors\/dist_metrics.pyx":[{"add":["1095","    # in cython < 0.26, GIL was required to be acquired during definition of","1096","    # the function and inside the body of the function. This behaviour is not","1097","    # allowed in cython >= 0.26 since it is a redundant GIL acquisition. The","1098","    # only way to be back compatible is to inherit `dist` from the base class","1099","    # without GIL and called an inline `_dist` which acquire GIL.","1101","                             ITYPE_t size) nogil except -1:","1102","        return self._dist(x1, x2, size)","1103","","1104","    cdef inline DTYPE_t _dist(self, DTYPE_t* x1, DTYPE_t* x2,","1105","                              ITYPE_t size) except -1 with gil:","1108","        x1arr = _buffer_to_ndarray(x1, size)","1109","        x2arr = _buffer_to_ndarray(x2, size)","1110","        d = self.func(x1arr, x2arr, **self.kwargs)","1111","        try:","1112","            # Cython generates code here that results in a TypeError","1113","            # if d is the wrong type.","1114","            return d","1115","        except TypeError:","1116","            raise TypeError(\"Custom distance function must accept two \"","1117","                            \"vectors and return a float.\")"],"delete":["1096","                             ITYPE_t size) except -1 with gil:","1099","        with gil:","1100","            x1arr = _buffer_to_ndarray(x1, size)","1101","            x2arr = _buffer_to_ndarray(x2, size)","1102","            d = self.func(x1arr, x2arr, **self.kwargs)","1103","            try:","1104","                # Cython generates code here that results in a TypeError","1105","                # if d is the wrong type.","1106","                return d","1107","            except TypeError:","1108","                raise TypeError(\"Custom distance function must accept two \"","1109","                                \"vectors and return a float.\")","1110","            "]}]}},"896f9d97cfc9945f83ce89b4287c53a3d22a94d4":{"changes":{"sklearn\/metrics\/pairwise.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/kernel_approximation.py":"MODIFY","sklearn\/tests\/test_kernel_approximation.py":"MODIFY"},"diff":{"sklearn\/metrics\/pairwise.py":[{"add":["1300","    \"chi2\": frozenset([\"gamma\"]),"],"delete":["1300","    \"chi2\": (),","1302","    \"exp_chi2\": frozenset([\"gamma\"]),"]}],"doc\/whats_new.rst":[{"add":["433","   - Made default kernel parameters kernel-dependent in :class:`kernel_approximation.Nystroem`","434","     :issue:`5229` by :user:`mth4saurabh` and `Andreas Mller`_.","435","","436","   - Fixed passing of ``gamma`` parameter to the ``chi2`` kernel in","437","     :func:`metrics.pairwise_kernels` :issue:`5211` by :user:`nrhine1`,","438","     :user:`mth4saurabh` and `Andreas Mller`_.","439",""],"delete":[]}],"sklearn\/kernel_approximation.py":[{"add":["20","from .metrics.pairwise import pairwise_kernels, KERNEL_PARAMS","391","    degree : float, default=None","394","    coef0 : float, default=None","440","    def __init__(self, kernel=\"rbf\", gamma=None, coef0=None, degree=None,","523","            for param in (KERNEL_PARAMS[self.kernel]):","524","                if getattr(self, param) is not None:","525","                    params[param] = getattr(self, param)","526","        else:","527","            if (self.gamma is not None or","528","                    self.coef0 is not None or","529","                    self.degree is not None):","530","                warnings.warn(","531","                    \"Passing gamma, coef0 or degree to Nystroem when using a\"","532","                    \" callable kernel is deprecated in version 0.19 and will\"","533","                    \" raise an error in 0.21, as they are ignored. Use \"","534","                    \"kernel_params instead.\", DeprecationWarning)"],"delete":["20","from .metrics.pairwise import pairwise_kernels","391","    degree : float, default=3","394","    coef0 : float, default=1","440","    def __init__(self, kernel=\"rbf\", gamma=None, coef0=1, degree=3,","523","            params['gamma'] = self.gamma","524","            params['degree'] = self.degree","525","            params['coef0'] = self.coef0"]}],"sklearn\/tests\/test_kernel_approximation.py":[{"add":["7","from sklearn.utils.testing import assert_warns_message","14","from sklearn.metrics.pairwise import polynomial_kernel, rbf_kernel, chi2_kernel","168","    def linear_kernel(X, Y):","169","        return np.dot(X, Y.T)","182","def test_nystroem_default_parameters():","183","    rnd = np.random.RandomState(42)","184","    X = rnd.uniform(size=(10, 4))","185","","186","    # rbf kernel should behave as gamma=None by default","187","    # aka gamma = 1 \/ n_features","188","    nystroem = Nystroem(n_components=10)","189","    X_transformed = nystroem.fit_transform(X)","190","    K = rbf_kernel(X, gamma=None)","191","    K2 = np.dot(X_transformed, X_transformed.T)","192","    assert_array_almost_equal(K, K2)","193","","194","    # chi2 kernel should behave as gamma=1 by default","195","    nystroem = Nystroem(kernel='chi2', n_components=10)","196","    X_transformed = nystroem.fit_transform(X)","197","    K = chi2_kernel(X, gamma=1)","198","    K2 = np.dot(X_transformed, X_transformed.T)","199","    assert_array_almost_equal(K, K2)","200","","201","","247","","248","    def linear_kernel(X, Y):","249","        return np.dot(X, Y.T)","250","","251","    # if degree, gamma or coef0 is passed, we raise a warning","252","    msg = \"Passing gamma, coef0 or degree to Nystroem\"","253","    params = ({'gamma': 1}, {'coef0': 1}, {'degree': 2})","254","    for param in params:","255","        ny = Nystroem(kernel=linear_kernel, **param)","256","        assert_warns_message(DeprecationWarning, msg, ny.fit, X)"],"delete":["13","from sklearn.metrics.pairwise import polynomial_kernel, rbf_kernel","167","    linear_kernel = lambda X, Y: np.dot(X, Y.T)"]}]}},"ea41a78b9a2486bd71751cbdbd8462eed443cfdc":{"changes":{"doc\/datasets\/mldata.rst":"MODIFY","doc\/datasets\/conftest.py":"ADD","build_tools\/travis\/test_script.sh":"MODIFY","doc\/datasets\/mldata_fixture.py":"MODIFY","conftest.py":"ADD"},"diff":{"doc\/datasets\/mldata.rst":[{"add":["5","    >>> import tempfile","6","    >>> # Create a temporary folder for the data fetcher","7","    >>> custom_data_home = tempfile.mkdtemp()","8","    >>> os.makedirs(os.path.join(custom_data_home, 'mldata'))","9","","77","","78","","79","..","80","    >>> import shutil","81","    >>> shutil.rmtree(custom_data_home)"],"delete":[]}],"doc\/datasets\/conftest.py":[{"add":[],"delete":[]}],"build_tools\/travis\/test_script.sh":[{"add":["45","    # Going back to git checkout folder needed to test documentation","46","    cd $OLDPWD","47","","48","    if [[ \"$USE_PYTEST\" == \"true\" ]]; then","49","        pytest $(find doc -name '*.rst' | sort)","50","    else","51","        # Makefile is using nose"],"delete":["45","    # Test doc (only with nose until we switch completely to pytest)","46","    if [[ \"$USE_PYTEST\" != \"true\" ]]; then","47","        # Going back to git checkout folder needed for make test-doc","48","        cd $OLDPWD"]}],"doc\/datasets\/mldata_fixture.py":[{"add":[],"delete":["5","from os import makedirs","6","from os.path import join","8","import tempfile","9","import shutil","11","from sklearn import datasets","16","def globs(globs):","17","    # Create a temporary folder for the data fetcher","18","    global custom_data_home","19","    custom_data_home = tempfile.mkdtemp()","20","    makedirs(join(custom_data_home, 'mldata'))","21","    globs['custom_data_home'] = custom_data_home","22","    return globs","23","","24","","44","    shutil.rmtree(custom_data_home)"]}],"conftest.py":[{"add":[],"delete":[]}]}},"a324a61eeace54947b9253de43e6f9f039e26185":{"changes":{"sklearn\/model_selection\/_split.py":"MODIFY"},"diff":{"sklearn\/model_selection\/_split.py":[{"add":["1708","    Provides train\/test indices to split data into train\/test sets using a","1709","    predefined scheme specified by the user with the ``test_fold`` parameter.","1713","    Parameters","1714","    ----------","1715","    test_fold : array-like, shape (n_samples,)","1716","        The entry ``test_fold[i]`` represents the index of the test set that","1717","        sample ``i`` belongs to. It is possible to exclude sample ``i`` from","1718","        any test set (i.e. include sample ``i`` in every training set) by","1719","        setting ``test_fold[i]`` equal to -1.","1720",""],"delete":["1708","    Splits the data into training\/test set folds according to a predefined","1709","    scheme. Each sample can be assigned to at most one test set fold, as","1710","    specified by the user through the ``test_fold`` parameter."]}]}},"4aaf45baf1cddbd368ab39801f4de40a6caaeece":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/manifold\/tests\/test_t_sne.py":"MODIFY","sklearn\/manifold\/t_sne.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["164","- :func:`manifold.t_sne.trustworthiness` accepts metrics other than","165","  Euclidean. :issue:`9775` by :user:`William de Vazelhes <wdevazelhes>`.","166","","241","Decomposition, manifold learning and clustering","242","","243","- Deprecate ``precomputed`` parameter in function","244","  :func:`manifold.t_sne.trustworthiness`. Instead, the new parameter","245","  ``metric`` should be used with any compatible metric including","246","  'precomputed', in which case the input matrix ``X`` should be a matrix of","247","  pairwise distances or squared distances. :issue:`9775` by","248","  :user:`William de Vazelhes <wdevazelhes>`.","249","","492","Decomposition, manifold learning and clustering","493","","494","- Deprecate ``precomputed`` parameter in function","495","  :func:`manifold.t_sne.trustworthiness`. Instead, the new parameter","496","  ``metric`` should be used with any compatible metric including","497","  'precomputed', in which case the input matrix ``X`` should be a matrix of","498","  pairwise distances or squared distances. :issue:`9775` by","499","  :user:`William de Vazelhes <wdevazelhes>`.","500",""],"delete":[]}],"sklearn\/manifold\/tests\/test_t_sne.py":[{"add":["16","from sklearn.utils.testing import assert_warns","17","from sklearn.utils.testing import assert_raises","292","        t = trustworthiness(D, X_embedded, n_neighbors=1, metric=\"precomputed\")","296","def test_trustworthiness_precomputed_deprecation():","297","    # FIXME: Remove this test in v0.23","298","","299","    # Use of the flag `precomputed` in trustworthiness parameters has been","300","    # deprecated, but will still work until v0.23.","301","    random_state = check_random_state(0)","302","    X = random_state.randn(100, 2)","303","    assert_equal(assert_warns(DeprecationWarning, trustworthiness,","304","                              pairwise_distances(X), X, precomputed=True), 1.)","305","    assert_equal(assert_warns(DeprecationWarning, trustworthiness,","306","                              pairwise_distances(X), X, metric='precomputed',","307","                              precomputed=True), 1.)","308","    assert_raises(ValueError, assert_warns, DeprecationWarning,","309","                  trustworthiness, X, X, metric='euclidean', precomputed=True)","310","    assert_equal(assert_warns(DeprecationWarning, trustworthiness,","311","                              pairwise_distances(X), X, metric='euclidean',","312","                              precomputed=True), 1.)","313","","314","","315","def test_trustworthiness_not_euclidean_metric():","316","    # Test trustworthiness with a metric different from 'euclidean' and","317","    # 'precomputed'","318","    random_state = check_random_state(0)","319","    X = random_state.randn(100, 2)","320","    assert_equal(trustworthiness(X, X, metric='cosine'),","321","                 trustworthiness(pairwise_distances(X, metric='cosine'), X,","322","                                 metric='precomputed'))","323","","324",""],"delete":["290","        t = trustworthiness(D, X_embedded, n_neighbors=1,","291","                            precomputed=True)"]}],"sklearn\/manifold\/t_sne.py":[{"add":["11","import warnings","397","def trustworthiness(X, X_embedded, n_neighbors=5,","398","                    precomputed=False, metric='euclidean'):","435","        ..deprecated:: 0.20","436","            ``precomputed`` has been deprecated in version 0.20 and will be","437","            removed in version 0.22. Use ``metric`` instead.","438","","439","    metric : string, or callable, optional, default 'euclidean'","440","        Which metric to use for computing pairwise distances between samples","441","        from the original input space. If metric is 'precomputed', X must be a","442","        matrix of pairwise distances or squared distances. Otherwise, see the","443","        documentation of argument metric in sklearn.pairwise.pairwise_distances","444","        for a list of available metrics.","445","","452","        warnings.warn(\"The flag 'precomputed' has been deprecated in version \"","453","                      \"0.20 and will be removed in 0.22. See 'metric' \"","454","                      \"parameter instead.\", DeprecationWarning)","455","        metric = 'precomputed'","456","    dist_X = pairwise_distances(X, metric=metric)"],"delete":["396","def trustworthiness(X, X_embedded, n_neighbors=5, precomputed=False):","439","        dist_X = X","440","    else:","441","        dist_X = pairwise_distances(X, squared=True)"]}]}},"0dd19de33a79bb536692aecfdc7c39c18d68bb31":{"changes":{"sklearn\/model_selection\/tests\/test_validation.py":"MODIFY"},"diff":{"sklearn\/model_selection\/tests\/test_validation.py":[{"add":["454","            assert type(cv_results['fit_time']) == np.ndarray","455","            assert type(cv_results['score_time']) == np.ndarray"],"delete":["454","            assert type(cv_results['fit_time'] == np.ndarray)","455","            assert type(cv_results['score_time'] == np.ndarray)"]}]}},"fbca098c62da3de7d2a5fedc649c7a137719609f":{"changes":{"sklearn\/model_selection\/_validation.py":"MODIFY"},"diff":{"sklearn\/model_selection\/_validation.py":[{"add":["146","    >>> from sklearn.model_selection import cross_validate","155","    Single metric evaluation using ``cross_validate``","156","","163","    Multiple metric evaluation using ``cross_validate``","164","    (please refer the ``scoring`` parameter doc for more information)","165",""],"delete":["146","    >>> from sklearn.model_selection import cross_val_score","155","    # single metric evaluation using cross_validate","162","    # Multiple metric evaluation using cross_validate","163","    # (Please refer the ``scoring`` parameter doc for more information)"]}]}},"c7ca0c58eee2047ae3f127af64401d210c1218ec":{"changes":{"sklearn\/linear_model\/stochastic_gradient.py":"MODIFY","doc\/modules\/sgd.rst":"MODIFY","sklearn\/linear_model\/tests\/test_sgd.py":"MODIFY","doc\/modules\/kernel_approximation.rst":"MODIFY","sklearn\/linear_model\/passive_aggressive.py":"MODIFY"},"diff":{"sklearn\/linear_model\/stochastic_gradient.py":[{"add":["68","        self.n_iter = n_iter","69","        self.max_iter = max_iter","71","        # current tests expect init to do parameter validation","72","        # but we are not allowed to set attributes","73","        self._validate_params(set_max_iter=False)","84","    def _validate_params(self, set_max_iter=True):","88","        if self.max_iter is not None and self.max_iter <= 0:","109","        if not set_max_iter:","110","            return","111","        # n_iter deprecation, set self._max_iter, self._tol","112","        self._tol = self.tol","113","        if self.n_iter is not None:","114","            warnings.warn(\"n_iter parameter is deprecated in 0.19 and will be\"","115","                          \" removed in 0.21. Use max_iter and tol instead.\",","116","                          DeprecationWarning)","117","            # Same behavior as before 0.19","118","            max_iter = self.n_iter","119","            self._tol = None","120","","121","        elif self.tol is None and self.max_iter is None:","122","            warnings.warn(","123","                \"max_iter and tol parameters have been added in %s in 0.19. If\"","124","                \" both are left unset, they default to max_iter=5 and tol=None\"","125","                \". If tol is not None, max_iter defaults to max_iter=1000. \"","126","                \"From 0.21, default max_iter will be 1000, \"","127","                \"and default tol will be 1e-3.\" % type(self), FutureWarning)","128","            # Before 0.19, default was n_iter=5","129","            max_iter = 5","130","        else:","131","            max_iter = self.max_iter if self.max_iter is not None else 1000","132","        self._max_iter = max_iter","133","","413","        self._validate_params()","442","        self._partial_fit(X, y, alpha, C, loss, learning_rate, self._max_iter,","445","        if (self._tol is not None and self._tol > -np.inf","446","                and self.n_iter_ == self._max_iter):","539","        self._validate_params()","763","           learning_rate='optimal', loss='hinge', max_iter=None, n_iter=None,","984","        self._validate_params()","993","        self._validate_params()","1013","                          self._max_iter, sample_weight, coef_init,","1016","        if (self._tol is not None and self._tol > -np.inf","1017","                and self.n_iter_ == self._max_iter):","1106","        tol = self._tol if self._tol is not None else -np.inf","1316","           loss='squared_loss', max_iter=None, n_iter=None, penalty='l2',"],"delete":["68","","69","        if n_iter is not None:","70","            warnings.warn(\"n_iter parameter is deprecated in 0.19 and will be\"","71","                          \" removed in 0.21. Use max_iter and tol instead.\",","72","                          DeprecationWarning)","73","            # Same behavior as before 0.19","74","            self.max_iter = n_iter","75","            tol = None","76","","77","        elif tol is None and max_iter is None:","78","            warnings.warn(","79","                \"max_iter and tol parameters have been added in %s in 0.19. If\"","80","                \" both are left unset, they default to max_iter=5 and tol=None\"","81","                \". If tol is not None, max_iter defaults to max_iter=1000. \"","82","                \"From 0.21, default max_iter will be 1000, \"","83","                \"and default tol will be 1e-3.\" % type(self), FutureWarning)","84","            # Before 0.19, default was n_iter=5","85","            self.max_iter = 5","86","        else:","87","            self.max_iter = max_iter if max_iter is not None else 1000","88","","90","","91","        self._validate_params()","102","    def _validate_params(self):","106","        if self.max_iter <= 0:","367","        self._validate_params()","435","        self._partial_fit(X, y, alpha, C, loss, learning_rate, self.max_iter,","438","        if (self.tol is not None and self.tol > -np.inf","439","                and self.n_iter_ == self.max_iter):","755","           learning_rate='optimal', loss='hinge', max_iter=5, n_iter=None,","935","        self._validate_params()","936","","1005","                          self.max_iter, sample_weight, coef_init,","1008","        if (self.tol is not None and self.tol > -np.inf","1009","                and self.n_iter_ == self.max_iter):","1098","        tol = self.tol if self.tol is not None else -np.inf","1308","           loss='squared_loss', max_iter=5, n_iter=None, penalty='l2',"]}],"doc\/modules\/sgd.rst":[{"add":["65","           learning_rate='optimal', loss='hinge', max_iter=None, n_iter=None,"],"delete":["65","           learning_rate='optimal', loss='hinge', max_iter=5, n_iter=None,"]}],"sklearn\/linear_model\/tests\/test_sgd.py":[{"add":["1209","    def init(max_iter=None, tol=None, n_iter=None):","1210","        sgd = SGDClassifier(max_iter=max_iter, tol=tol, n_iter=n_iter)","1211","        sgd._validate_params()","1212","","1215","    assert_warns_message(FutureWarning, msg_future, init)","1231","    est._validate_params()","1232","    assert_equal(est._tol, None)","1233","    assert_equal(est._max_iter, 5)","1236","    est._validate_params()","1237","    assert_equal(est._tol, None)","1238","    assert_equal(est._max_iter, 42)","1241","    est._validate_params()","1242","    assert_equal(est._tol, 1e-2)","1243","    assert_equal(est._max_iter, 1000)","1246","    est._validate_params()","1247","    assert_equal(est._tol, None)","1248","    assert_equal(est._max_iter, 42)","1251","    est._validate_params()","1252","    assert_equal(est._tol, 1e-2)","1253","    assert_equal(est._max_iter, 42)"],"delete":["1211","    assert_warns_message(FutureWarning, msg_future, SGDClassifier)","1212","","1213","    def init(max_iter=None, tol=None, n_iter=None):","1214","        SGDClassifier(max_iter=max_iter, tol=tol, n_iter=n_iter)","1230","    assert_equal(est.tol, None)","1231","    assert_equal(est.max_iter, 5)","1234","    assert_equal(est.tol, None)","1235","    assert_equal(est.max_iter, 42)","1238","    assert_equal(est.tol, 1e-2)","1239","    assert_equal(est.max_iter, 1000)","1242","    assert_equal(est.tol, None)","1243","    assert_equal(est.max_iter, 42)","1246","    assert_equal(est.tol, 1e-2)","1247","    assert_equal(est.max_iter, 42)"]}],"doc\/modules\/kernel_approximation.rst":[{"add":["65","           learning_rate='optimal', loss='hinge', max_iter=None, n_iter=None,"],"delete":["65","           learning_rate='optimal', loss='hinge', max_iter=5, n_iter=None,"]}],"sklearn\/linear_model\/passive_aggressive.py":[{"add":["116","                  fit_intercept=True, loss='hinge', max_iter=None, n_iter=None,","321","                  fit_intercept=True, loss='epsilon_insensitive',","322","                  max_iter=None, n_iter=None, random_state=0, shuffle=True,","323","                  tol=None, verbose=0, warm_start=False)","379","        self._validate_params()"],"delete":["116","                  fit_intercept=True, loss='hinge', max_iter=5, n_iter=None,","321","                  fit_intercept=True, loss='epsilon_insensitive', max_iter=5,","322","                  n_iter=None, random_state=0, shuffle=True, tol=None,","323","                  verbose=0, warm_start=False)"]}]}},"7e3ad6d3fcfd5cdd55cb27775a1abb08b7e97890":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/datasets\/kddcup99.py":"MODIFY","sklearn\/datasets\/tests\/test_kddcup99.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["102","- Fixed a bug in :func:`datasets.fetch_kddcup99`, where data were not properly","103","  shuffled. :issue:`9731` by `Nicolas Goix`_.","104",""],"delete":[]}],"sklearn\/datasets\/kddcup99.py":[{"add":["179","    kddcup99 = _fetch_brute_kddcup99(data_home=data_home,","229","    if shuffle:","230","        data, target = shuffle_method(data, target, random_state=random_state)","231","","237","                          percent10=True):"],"delete":["179","    kddcup99 = _fetch_brute_kddcup99(data_home=data_home, shuffle=shuffle,","234","                          shuffle=False, percent10=True):","255","    shuffle : bool, default=False","256","        Whether to shuffle dataset.","257","","376","    if shuffle:","377","        X, y = shuffle_method(X, y, random_state=random_state)","378",""]}],"sklearn\/datasets\/tests\/test_kddcup99.py":[{"add":["39","","40","","41","def test_shuffle():","42","    try:","43","        dataset = fetch_kddcup99(random_state=0, subset='SA', shuffle=True,","44","                                 percent10=True, download_if_missing=False)","45","    except IOError:","46","        raise SkipTest(\"kddcup99 dataset can not be loaded.\")","47","","48","    assert(any(dataset.target[-100:] == b'normal.'))"],"delete":[]}]}},"fbcbc1b76c467a7e17dae2fd2032d5685ee9884c":{"changes":{"sklearn\/mixture\/base.py":"MODIFY"},"diff":{"sklearn\/mixture\/base.py":[{"add":["419","        weighted_log_prob : array, shape (n_samples, n_component)"],"delete":["419","        weighted_log_prob : array, shape (n_features, n_component)"]}]}},"692cd8b7bcc2f83a5cf11e4b98ae9090ee0b65a6":{"changes":{"sklearn\/covariance\/graph_lasso_.py":"MODIFY"},"diff":{"sklearn\/covariance\/graph_lasso_.py":[{"add":["205","        # set a sub_covariance buffer","206","        sub_covariance = np.ascontiguousarray(covariance_[1:, 1:])","209","                # To keep the contiguous matrix `sub_covariance` equal to","210","                # covariance_[indices != idx].T[indices != idx]","211","                # we only need to update 1 column and 1 line when idx changes","212","                if idx > 0:","213","                    di = idx - 1","214","                    sub_covariance[di] = covariance_[di][indices != idx]","215","                    sub_covariance[:, di] = covariance_[:, di][indices != idx]","216","                else:","217","                    sub_covariance[:] = covariance_[1:, 1:]"],"delete":["207","                sub_covariance = np.ascontiguousarray(","208","                    covariance_[indices != idx].T[indices != idx])"]}]}},"b4b5de8cf9748a07d8f3a2d1fc89ccaacdf6576f":{"changes":{"sklearn\/naive_bayes.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/tests\/test_naive_bayes.py":"MODIFY"},"diff":{"sklearn\/naive_bayes.py":[{"add":["17","import warnings","439","_ALPHA_MIN = 1e-10","440","","465","    def _check_alpha(self):","466","        if self.alpha < 0:","467","            raise ValueError('Smoothing parameter alpha = %.1e. '","468","                             'alpha should be > 0.' % self.alpha)","469","        if self.alpha < _ALPHA_MIN:","470","            warnings.warn('alpha too small will result in numeric errors, '","471","                          'setting alpha = %.1e' % _ALPHA_MIN)","472","            return _ALPHA_MIN","473","        return self.alpha","474","","553","        alpha = self._check_alpha()","554","        self._update_feature_log_prob(alpha)","604","        alpha = self._check_alpha()","605","        self._update_feature_log_prob(alpha)","711","    def _update_feature_log_prob(self, alpha):","713","        smoothed_fc = self.feature_count_ + alpha","813","    def _update_feature_log_prob(self, alpha):","815","        smoothed_fc = self.feature_count_ + alpha","816","        smoothed_cc = self.class_count_ + alpha * 2"],"delete":["540","        self._update_feature_log_prob()","590","        self._update_feature_log_prob()","696","    def _update_feature_log_prob(self):","698","        smoothed_fc = self.feature_count_ + self.alpha","798","    def _update_feature_log_prob(self):","800","        smoothed_fc = self.feature_count_ + self.alpha","801","        smoothed_cc = self.class_count_ + self.alpha * 2"]}],"doc\/whats_new.rst":[{"add":["261","   - Fixed a bug where :class:`sklearn.naive_bayes.MultinomialNB` and :class:`sklearn.naive_bayes.BernoulliNB`","262","     failed when `alpha=0`. :issue:`5814` by :user:`Yichuan Liu <yl565>` and ","263","     :user:`Herilalaina Rakotoarison <herilalaina>`.","264",""],"delete":[]}],"sklearn\/tests\/test_naive_bayes.py":[{"add":["16","from sklearn.utils.testing import assert_raise_message","18","from sklearn.utils.testing import assert_warns","484","    assert_array_almost_equal(clf.feature_log_prob_, (num - denom))","540","","541","","542","def test_alpha():","543","    # Setting alpha=0 should not output nan results when p(x_i|y_j)=0 is a case","544","    X = np.array([[1, 0], [1, 1]])","545","    y = np.array([0, 1])","546","    nb = BernoulliNB(alpha=0.)","547","    assert_warns(UserWarning, nb.partial_fit, X, y, classes=[0, 1])","548","    assert_warns(UserWarning, nb.fit, X, y)","549","    prob = np.array([[1, 0], [0, 1]])","550","    assert_array_almost_equal(nb.predict_proba(X), prob)","551","","552","    nb = MultinomialNB(alpha=0.)","553","    assert_warns(UserWarning, nb.partial_fit, X, y, classes=[0, 1])","554","    assert_warns(UserWarning, nb.fit, X, y)","555","    prob = np.array([[2.\/3, 1.\/3], [0, 1]])","556","    assert_array_almost_equal(nb.predict_proba(X), prob)","557","","558","    # Test sparse X","559","    X = scipy.sparse.csr_matrix(X)","560","    nb = BernoulliNB(alpha=0.)","561","    assert_warns(UserWarning, nb.fit, X, y)","562","    prob = np.array([[1, 0], [0, 1]])","563","    assert_array_almost_equal(nb.predict_proba(X), prob)","564","","565","    nb = MultinomialNB(alpha=0.)","566","    assert_warns(UserWarning, nb.fit, X, y)","567","    prob = np.array([[2.\/3, 1.\/3], [0, 1]])","568","    assert_array_almost_equal(nb.predict_proba(X), prob)","569","","570","    # Test for alpha < 0","571","    X = np.array([[1, 0], [1, 1]])","572","    y = np.array([0, 1])","573","    expected_msg = ('Smoothing parameter alpha = -1.0e-01. '","574","                    'alpha should be > 0.')","575","    b_nb = BernoulliNB(alpha=-0.1)","576","    m_nb = MultinomialNB(alpha=-0.1)","577","    assert_raise_message(ValueError, expected_msg, b_nb.fit, X, y)","578","    assert_raise_message(ValueError, expected_msg, m_nb.fit, X, y)","579","","580","    b_nb = BernoulliNB(alpha=-0.1)","581","    m_nb = MultinomialNB(alpha=-0.1)","582","    assert_raise_message(ValueError, expected_msg, b_nb.partial_fit,","583","                         X, y, classes=[0, 1])","584","    assert_raise_message(ValueError, expected_msg, m_nb.partial_fit,","585","                         X, y, classes=[0, 1])"],"delete":["482","    assert_array_equal(clf.feature_log_prob_, (num - denom))"]}]}},"7dcc41325c115c86e417537b7dc7123e8457d74a":{"changes":{"sklearn\/neighbors\/approximate.py":"MODIFY"},"diff":{"sklearn\/neighbors\/approximate.py":[{"add":["134","        space to use by default for the :meth:`radius_neighbors` queries.","136","    n_candidates : int (default = 50)"],"delete":["134","        space to use by default for the :meth`radius_neighbors` queries.","136","    n_candidates : int (default = 10)"]}]}},"88f529efa688246712c15a1404ae4ff0b2d87fd9":{"changes":{"doc\/modules\/ensemble.rst":"MODIFY"},"diff":{"doc\/modules\/ensemble.rst":[{"add":["204","in combination with ``min_samples_split=2`` (i.e., when fully developing the"],"delete":["204","in combination with ``min_samples_split=1`` (i.e., when fully developing the"]}]}},"ba7224869f3abeb904f59542551e965fff2e642b":{"changes":{"doc\/modules\/classes.rst":"MODIFY","doc\/whats_new.rst":"MODIFY","doc\/modules\/pipeline.rst":"MODIFY"},"diff":{"doc\/modules\/classes.rst":[{"add":["1392","To be removed in 0.21","1393","---------------------","1394","","1395",".. autosummary::","1396","   :toctree: generated\/","1397","   :template: deprecated_class.rst","1398","","1399","   linear_model.RandomizedLasso","1400","   linear_model.RandomizedLogisticRegression","1401","   neighbors.LSHForest","1402","","1403",""],"delete":["726","   linear_model.RandomizedLasso","727","   linear_model.RandomizedLogisticRegression"]}],"doc\/whats_new.rst":[{"add":["12","Highlights","13","----------","14","","15","We are excited to release a number of great new features including","16",":class:`neighbors.LocalOutlierFactor` for anomaly detection,","17",":class:`preprocessing.QuantileTransformer` for robust feature transformation,","18","and the :class:`multioutput.ClassifierChain` meta-estimator to simply account","19","for dependencies between classes in multilabel problems. We have some new","20","algorithms in existing estimators, such as multiplicative update in","21",":class:`decomposition.NMF` and multinomial","22",":class:`linear_model.LogisticRegression` with L1 loss (use ``solver='saga'``).","23","","24","You can also learn faster.  For instance, the :ref:`new option to cache","25","transformations <pipeline_cache>` in :class:`pipeline.Pipeline` makes grid","26","search over pipelines including slow transformations much more efficient.  And","27","you can predict faster: if you're sure you know what you're doing, you can turn","28","off validating that the input is finite using :func:`config_context`.","29","","30","Cross validation is now able to return the results from multiple metric","31","evaluations. The new :func:`model_selection.cross_validate` can return many","32","scores on the test data as well as training set performance and timings, and we","33","have extended the ``scoring`` and ``refit`` parameters for grid\/randomized","34","search :ref:`to handle multiple metrics <multimetric_grid_search>`.","35","","36","We've made some important fixes too.  We've fixed a longstanding implementation","37","erorr in :func:`metrics.average_precision_score`, so please be cautious with","38","prior results reported from that function.  A number of errors in the","39",":class:`manifold.TSNE` implementation have been fixed, particularly in the","40","default Barnes-Hut approximation.  :class:`semi_supervised.LabelSpreading` and","41",":class:`semi_supervised.LabelPropagation` have had substantial fixes.","42","LabelPropagation was previously broken. LabelSpreading should now correctly","43","respect its alpha parameter.","44","","53","   * :class:`cluster.KMeans` with sparse X and initial centroids given (bug fix)","54","   * :class:`cross_decomposition.PLSRegression`","55","     with ``scale=True`` (bug fix)","56","   * :class:`ensemble.GradientBoostingClassifier` and","57","     :class:`ensemble.GradientBoostingRegressor` where ``min_impurity_split`` is used (bug fix)","58","   * gradient boosting ``loss='quantile'`` (bug fix)","59","   * :class:`ensemble.IsolationForest` (bug fix)","60","   * :class:`feature_selection.SelectFdr` (bug fix)","61","   * :class:`linear_model.RANSACRegressor` (bug fix)","62","   * :class:`linear_model.LassoLars` (bug fix)","63","   * :class:`linear_model.LassoLarsIC` (bug fix)","64","   * :class:`manifold.TSNE` (bug fix)","65","   * :class:`semi_supervised.LabelSpreading` (bug fix)","66","   * :class:`semi_supervised.LabelPropagation` (bug fix)","67","   * tree based models where ``min_weight_fraction_leaf`` is used (enhancement)","80","Classifiers and regressors","81","","82","   - Added :class:`multioutput.ClassifierChain` for multi-label","83","     classification. By `Adam Kleczewski <adamklec>`_.","84","","85","   - Added solver ``'saga'`` that implements the improved version of Stochastic","86","     Average Gradient, in :class:`linear_model.LogisticRegression` and","87","     :class:`linear_model.Ridge`. It allows the use of L1 penalty with","88","     multinomial logistic loss, and behaves marginally better than 'sag'","89","     during the first epochs of ridge and logistic regression.","90","     :issue:`8446` by `Arthur Mensch`_.","91","","92","Other estimators","93","","94","   - Added the :class:`neighbors.LocalOutlierFactor` class for anomaly","95","     detection based on nearest neighbors.","96","     :issue:`5279` by `Nicolas Goix`_ and `Alexandre Gramfort`_.","97","","98","   - Added :class:`preprocessing.QuantileTransformer` class and","99","     :func:`preprocessing.quantile_transform` function for features","100","     normalization based on quantiles.","101","     :issue:`8363` by :user:`Denis Engemann <dengemann>`,","102","     :user:`Guillaume Lemaitre <glemaitre>`, `Olivier Grisel`_, `Raghav RV`_,","103","     :user:`Thierry Guillemot <tguillemot>`, and `Gael Varoquaux`_.","104","","105","   - The new solver ``'mu'`` implements a Multiplicate Update in","106","     :class:`decomposition.NMF`, allowing the optimization of all","107","     beta-divergences, including the Frobenius norm, the generalized","108","     Kullback-Leibler divergence and the Itakura-Saito divergence.","109","     :issue:`5295` by `Tom Dupre la Tour`_.","110","","111","Model selection and evaluation","112","","136","   - Added the :class:`model_selection.RepeatedKFold` and","137","     :class:`model_selection.RepeatedStratifiedKFold`.","138","     :issue:`8120` by `Neeraj Gangwar`_.","139","","140","Miscellaneous","141","","142","   - Validation that input data contains no NaN or inf can now be suppressed","143","     using :func:`config_context`, at your own risk. This will save on runtime,","144","     and may be particularly useful for prediction time. :issue:`7548` by","145","     `Joel Nothman`_.","146","","147","   - Added a test to ensure parameter listing in docstrings match the","148","     function\/class signature. :issue:`9206` by `Alexandre Gramfort`_ and","149","     `Raghav RV`_.","150","","154","Trees and ensembles","166","   - :class:`ensemble.VotingClassifier` now allows changing estimators by using","167","     :meth:`ensemble.VotingClassifier.set_params`. An estimator can also be","168","     removed by setting it to ``None``.","169","     :issue:`7674` by :user:`Yichuan Liu <yl565>`.","171","   - :func:`tree.export_graphviz` now shows configurable number of decimal","172","     places. :issue:`8698` by :user:`Guillaume Lemaitre <glemaitre>`.","174","Linear, kernelized and related models","179","     :class:`linear_model.Perceptron` now expose ``max_iter`` and","183","     convergence. :issue:`5036` by `Tom Dupre la Tour`_.","185","   - Added ``average`` parameter to perform weight averaging in","186","     :class:`linear_model.PassiveAggressiveClassifier`. :issue:`4939`","187","     by :user:`Andrea Esuli <aesuli>`.","195","   - In :class:`gaussian_process.GaussianProcessRegressor`, method ``predict``","196","     is a lot faster with ``return_std=True``. :issue:`8591` by","197","     :user:`Hadrien Bertrand <hbertrand>`.","199","   - Added ``return_std`` to ``predict`` method of","200","     :class:`linear_model.ARDRegression` and","201","     :class:`linear_model.BayesianRidge`.","202","     :issue:`7838` by :user:`Sergey Feldman <sergeyf>`.","203","","204","   - Memory usage enhancements: Prevent cast from float32 to float64 in:","205","     :class:`linear_model.MultiTaskElasticNet`;","206","     :class:`linear_model.LogisticRegression` when using newton-cg solver; and","207","     :class:`linear_model.Ridge` when using svd, sparse_cg, cholesky or lsqr","208","     solvers. :issue:`8835`, :issue:`8061` by :user:`Joan Massich <massich>` and :user:`Nicolas","209","     Cordier <ncordier>` and :user:`Thierry Guillemot`.","210","","211","Other predictors","212","","213","   - Custom metrics for the :mod:`neighbors` binary trees now have","214","     fewer constraints: they must take two 1d-arrays and return a float.","215","     :issue:`6288` by `Jake Vanderplas`_.","216","","217","   - ``algorithm='auto`` in :mod:`neighbors` estimators now chooses the most","218","     appropriate algorithm for all input types and metrics. :issue:`9145` by","219","     :user:`Herilalaina Rakotoarison <herilalaina>` and :user:`Reddy Chinthala","220","     <preddy5Pradyumna>`.","221","","222","Decomposition, manifold learning and clustering","223","","224","   - :class:`cluster.MiniBatchKMeans` and :class:`cluster.KMeans`","225","     now use significantly less memory when assigning data points to their","226","     nearest cluster center. :issue:`7721` by :user:`Jon Crall <Erotemic>`.","227","","228","   - :class:`decomposition.PCA`, :class:`decomposition.IncrementalPCA` and","229","     :class:`decomposition.TruncatedSVD` now expose the singular values","230","     from the underlying SVD. They are stored in the attribute","231","     ``singular_values_``, like in :class:`decomposition.IncrementalPCA`.","232","     :issue:`7685` by :user:`Tommy L?fstedt <tomlof>`","233","","234","   - :class:`decomposition.NMF` now faster when ``beta_loss=0``.","235","     :issue:`9277` by :user:`hongkahjun`.","236","","237","   - Memory improvements for method ``barnes_hut`` in :class:`manifold.TSNE`","238","     :issue:`7089` by :user:`Thomas Moreau <tomMoral>` and `Olivier Grisel`_.","239","","240","   - Optimization schedule improvements for Barnes-Hut :class:`manifold.TSNE`","241","     so the results are closer to the one from the reference implementation","242","     `lvdmaaten\/bhtsne <https:\/\/github.com\/lvdmaaten\/bhtsne>`_ by :user:`Thomas","243","     Moreau <tomMoral>` and `Olivier Grisel`_.","244","","245","   - Memory usage enhancements: Prevent cast from float32 to float64 in","246","     :class:`decomposition.PCA` and","247","     :func:`decomposition.randomized_svd_low_rank`.","248","     :issue:`9067` by `Raghav RV`_.","249","","250","Preprocessing and feature selection","251","","252","   - Added ``norm_order`` parameter to :class:`feature_selection.SelectFromModel`","253","     to enable selection of the norm order when ``coef_`` is more than 1D.","254","     :issue:`6181` by :user:`Antoine Wendlinger <antoinewdg>`.","255","","256","   - Added ability to use sparse matrices in :func:`feature_selection.f_regression`","257","     with ``center=True``. :issue:`8065` by :user:`Daniel LeJeune <acadiansith>`.","258","","259","   - Small performance improvement to n-gram creation in","260","     :mod:`feature_extraction.text` by binding methods for loops and","261","     special-casing unigrams. :issue:`7567` by :user:`Jaye Doepke <jtdoepke>`","262","","263","   - Relax assumption on the data for the","264","     :class:`kernel_approximation.SkewedChi2Sampler`. Since the Skewed-Chi2","265","     kernel is defined on the open interval :math:`(-skewedness; +\\infty)^d`,","266","     the transform function should not check whether ``X < 0`` but whether ``X <","267","     -self.skewedness``. :issue:`7573` by :user:`Romain Brault <RomainBrault>`.","268","","269","   - Made default kernel parameters kernel-dependent in","270","     :class:`kernel_approximation.Nystroem`.","271","     :issue:`5229` by :user:`Saurabh Bansod <mth4saurabh>` and `Andreas Mller`_.","272","","273","Model evaluation and meta-estimators","274","","275","   - :class:`pipeline.Pipeline` is now able to cache transformers","276","     within a pipeline by using the ``memory`` constructor parameter.","277","     :issue:`7990` by :user:`Guillaume Lemaitre <glemaitre>`.","278","","279","   - :class:`pipeline.Pipeline` steps can now be accessed as attributes of its","280","     ``named_steps`` attribute. :issue:`8586` by :user:`Herilalaina","281","     Rakotoarison <herilalaina>`.","282","","283","   - Added ``sample_weight`` parameter to :meth:`pipeline.Pipeline.score`.","284","     :issue:`7723` by :user:`Mikhail Korobov <kmike>`.","290","   - :class:`model_selection.GridSearchCV`,","291","     :class:`model_selection.RandomizedSearchCV` and","292","     :func:`model_selection.cross_val_score` now allow estimators with callable","293","     kernels which were previously prohibited.","294","     :issue:`8005` by `Andreas Mller`_ .","296","   - :func:`model_selection.cross_val_predict` now returns output of the","297","     correct shape for all values of the argument ``method``.","298","     :issue:`7863` by :user:`Aman Dalmia <dalmia>`.","300","   - Added ``shuffle`` and ``random_state`` parameters to shuffle training","301","     data before taking prefixes of it based on training sizes in","302","     :func:`model_selection.learning_curve`.","303","     :issue:`7506` by :user:`Narine Kokhlikyan <NarineK>`.","304","","305","   - :class:`model_selection.StratifiedShuffleSplit` now works with multioutput","306","     multiclass (or multilabel) data.  :issue:`9044` by `Vlad Niculae`_.","307","","308","   - Speed improvements to :class:`model_selection.StratifiedShuffleSplit`.","309","     :issue:`5991` by :user:`Arthur Mensch <arthurmensch>` and `Joel Nothman`_.","310","","311","   - Add ``shuffle`` parameter to :func:`model_selection.train_test_split`.","312","     :issue:`8845` by  :user:`themrmax <themrmax>`","313","","314","   - :class:`multioutput.MultiOutputRegressor` and :class:`multioutput.MultiOutputClassifier`","315","     now support online learning using ``partial_fit``.","316","     :issue: `8053` by :user:`Peng Yu <yupbank>`.","317","","318","   - Add ``max_train_size`` parameter to :class:`model_selection.TimeSeriesSplit`","319","     :issue:`8282` by :user:`Aman Dalmia <dalmia>`.","320","","321","   - More clustering metrics are now available through :func:`metrics.get_scorer`","322","     and ``scoring`` parameters. :issue:`8117` by `Raghav RV`_.","323","","324","Metrics","325","","326","   - :func:`metrics.matthews_corrcoef` now support multiclass classification.","327","     :issue:`8094` by :user:`Jon Crall <Erotemic>`.","332","Miscellaneous","334","   - :func:`utils.check_estimator` now attempts to ensure that methods","335","     transform, predict, etc.  do not set attributes on the estimator.","336","     :issue:`7533` by :user:`Ekaterina Krivich <kiote>`.","338","   - Added type checking to the ``accept_sparse`` parameter in","339","     :mod:`utils.validation` methods. This parameter now accepts only boolean,","340","     string, or list\/tuple of strings. ``accept_sparse=None`` is deprecated and","341","     should be replaced by ``accept_sparse=False``.","342","     :issue:`7880` by :user:`Josh Karnofsky <jkarno>`.","348","   - :class:`dummy.DummyClassifier` and :class:`dummy.DummyRegressor`","349","     now accept non-finite features. :issue:`8931` by :user:`Attractadore`.","354","Trees and ensembles","356","   - Fixed a memory leak in trees when using trees with ``criterion='mae'``.","357","     :issue:`8002` by `Raghav RV`_.","367","   - Fixed a bug in :class:`ensemble.GradientBoostingClassifier` and","368","     :class:`ensemble.GradientBoostingRegressor` where a float being compared","369","     to ``0.0`` using ``==`` caused a divide by zero error. :issue:`7970` by","370","     :user:`He Chen <chenhe95>`.","377","   - Fixed ``oob_score`` in :class:`ensemble.BaggingClassifier`.","378","     :issue:`8936` by :user:`Michael Lewis <mlewis1729>`","379","","380","   - Fixed excessive memory usage in prediction for random forests estimators.","381","     :issue:`8672` by :user:`Mike Benfield <mikebenfield>`.","382","","383","   - Fixed a bug where ``sample_weight`` as a list broke random forests in Python 2","384","     :issue:`8068` by :user:`xor`.","385","","386","   - Fixed a bug where :class:`ensemble.IsolationForest` fails when","387","     ``max_features`` is less than 1.","388","     :issue:`5732` by :user:`Ishank Gulati <IshankGulati>`.","389","","390","   - Fix a bug where gradient boosting with ``loss='quantile'`` computed","391","     negative errors for negative values of ``ytrue - ypred`` leading to wrong","392","     values when calling ``__call__``.","393","     :issue:`8087` by :user:`Alexis Mignon <AlexisMignon>`","394","","395","   - Fix a bug where :class:`ensemble.VotingClassifier` raises an error","396","     when a numpy array is passed in for weights. :issue:`7983` by","397","     :user:`Vincent Pham <vincentpham1991>`.","398","","399","   - Fixed a bug where :func:`tree.export_graphviz` raised an error","400","     when the length of features_names does not match n_features in the decision","401","     tree. :issue:`8512` by :user:`Li Li <aikinogard>`.","402","","403","Linear, kernelized and related models","404","","405","   - Fixed a bug where :func:`linear_model.RANSACRegressor.fit` may run until","406","     ``max_iter`` if it finds a large inlier group early. :issue:`8251` by","407","     :user:`aivision2020`.","408","","409","   - Fixed a bug where :class:`naive_bayes.MultinomialNB` and","410","     :class:`naive_bayes.BernoulliNB` failed when ``alpha=0``. :issue:`5814` by","411","     :user:`Yichuan Liu <yl565>` and :user:`Herilalaina Rakotoarison","412","     <herilalaina>`.","413","","414","   - Fixed a bug where :class:`linear_model.LassoLars` does not give","415","     the same result as the LassoLars implementation available","416","     in R (lars library). :issue:`7849` by :user:`Jair Montoya Martinez <jmontoyam>`.","417","","418","   - Fixed a bug in :class:`linear_model.RandomizedLasso`,","419","     :class:`linear_model.Lars`, :class:`linear_model.LassoLars`,","420","     :class:`linear_model.LarsCV` and :class:`linear_model.LassoLarsCV`,","421","     where the parameter ``precompute`` was not used consistently across","422","     classes, and some values proposed in the docstring could raise errors.","423","     :issue:`5359` by `Tom Dupre la Tour`_.","424","","425","   - Fix inconsistent results between :class:`linear_model.RidgeCV` and","426","     :class:`linear_model.Ridge` when using ``normalize=True``. :issue:`9302`","427","     by `Alexandre Gramfort`_.","428","","429","   - Fix a bug where :func:`linear_model.LassoLars.fit` sometimes","430","     left ``coef_`` as a list, rather than an ndarray.","431","     :issue:`8160` by :user:`CJ Carey <perimosocordiae>`.","432","","433","   - Fix :func:`linear_model.BayesianRidge.fit` to return","434","     ridge parameter ``alpha_`` and ``lambda_`` consistent with calculated","435","     coefficients ``coef_`` and ``intercept_``.","436","     :issue:`8224` by :user:`Peter Gedeck <gedeck>`.","437","","438","   - Fixed a bug in :class:`svm.OneClassSVM` where it returned floats instead of","439","     integer classes. :issue:`8676` by :user:`Vathsala Achar <VathsalaAchar>`.","440","","441","   - Fix AIC\/BIC criterion computation in :class:`linear_model.LassoLarsIC`.","442","     :issue:`9022` by `Alexandre Gramfort`_ and :user:`Mehmet Basbug <mehmetbasbug>`.","443","","444","   - Fixed a memory leak in our LibLinear implementation. :issue:`9024` by","445","     :user:`Sergei Lebedev <superbobry>`","446","","447","   - Fix bug where stratified CV splitters did not work with","448","     :class:`linear_model.LassoCV`. :issue:`8973` by","449","     :user:`Paulo Haddad <paulochf>`.","450","","451","   - Fixed a bug in :class:`gaussian_process.GaussianProcessRegressor`","452","     when the standard deviation and covariance predicted without fit","453","     would fail with a unmeaningful error by default.","454","     :issue:`6573` by :user:`Quazi Marufur Rahman <qmaruf>` and","455","     `Manoj Kumar`_.","456","","457","Other predictors","458","","459","   - Fix :class:`semi_supervised.BaseLabelPropagation` to correctly implement","460","     ``LabelPropagation`` and ``LabelSpreading`` as done in the referenced","461","     papers. :issue:`9239`","462","     by :user:`Andre Ambrosio Boechat <boechat107>`, :user:`Utkarsh Upadhyay","463","     <musically-ut>`, and `Joel Nothman`_.","464","","465","Decomposition, manifold learning and clustering","466","","467","   - Fixed the implementation of :class:`manifold.TSNE`:","468","      - ``early_exageration`` parameter had no effect and is now used for the","469","        first 250 optimization iterations.","470","      - Fixed the ``InsersionError`` reported in :issue:`8992`.","471","      - Improve the learning schedule to match the one from the reference","472","        implementation `lvdmaaten\/bhtsne <https:\/\/github.com\/lvdmaaten\/bhtsne>`_.","473","     by :user:`Thomas Moreau <tomMoral>` and `Olivier Grisel`_.","474","","475","   - Fix a bug in :class:`decomposition.LatentDirichletAllocation`","476","     where the ``perplexity`` method was returning incorrect results because","477","     the ``transform`` method returns normalized document topic distributions","478","     as of version 0.18. :issue:`7954` by :user:`Gary Foreman <garyForeman>`.","487","   - Fixed the implementation of ``explained_variance_``","488","     in :class:`decomposition.PCA`,","489","     :class:`decomposition.RandomizedPCA` and","490","     :class:`decomposition.IncrementalPCA`.","491","     :issue:`9105` by `Hanmin Qin <https:\/\/github.com\/qinhanmin2014>`_. ","492","","493","   - Fixed a bug where :class:`cluster.DBSCAN` gives incorrect","494","     result when input is a precomputed sparse matrix with initial","495","     rows all zero. :issue:`8306` by :user:`Akshay Gupta <Akshay0724>`","496","","497","   - Fix a bug regarding fitting :class:`cluster.KMeans` with a sparse","498","     array X and initial centroids, where X's means were unnecessarily being","499","     subtracted from the centroids. :issue:`7872` by :user:`Josh Karnofsky <jkarno>`.","500","","501","   - Fixes to the input validation in :class:`covariance.EllipticEnvelope`.","504","   - Fixed a bug in :class:`covariance.MinCovDet` where inputting data","505","     that produced a singular covariance matrix would cause the helper method","506","     ``_c_step`` to throw an exception.","507","     :issue:`3367` by :user:`Jeremy Steward <ThatGeoGuy>`","509","   - Fixed a bug in :class:`manifold.TSNE` affecting convergence of the","510","     gradient descent. :issue:`8768` by :user:`David DeTomaso <deto>`.","512","   - Fixed a bug in :class:`manifold.TSNE` where it stored the incorrect","513","     ``kl_divergence_``. :issue:`6507` by :user:`Sebastian Saeger <ssaeger>`.","514","","515","   - Fixed improper scaling in :class:`cross_decomposition.PLSRegression`","516","     with ``scale=True``. :issue:`7819` by :user:`jayzed82 <jayzed82>`.","517","","518","   - :class:`cluster.bicluster.SpectralCoclustering` and","519","     :class:`cluster.bicluster.SpectralBiclustering` ``fit`` method conforms","520","     with API by accepting ``y`` and returning the object.  :issue:`6126`,","521","     :issue:`7814` by :user:`Laurent Direr <ldirer>` and :user:`Maniteja","522","     Nandana <maniteja123>`.","523","","524","   - Fix bug where :mod:`mixture` ``sample`` methods did not return as many","525","     samples as requested. :issue:`7702` by :user:`Levi John Wolf <ljwolf>`.","526","","527","Preprocessing and feature selection","528","","529","   - For sparse matrices, :func:`preprocessing.normalize` with ``return_norm=True``","530","     will now raise a ``NotImplementedError`` with 'l1' or 'l2' norm and with","531","     norm 'max' the norms returned will be the same as for dense matrices.","532","     :issue:`7771` by `Ang Lu <https:\/\/github.com\/luang008>`_.","533","","534","   - Fix a bug where :class:`feature_selection.SelectFdr` did not","535","     exactly implement Benjamini-Hochberg procedure. It formerly may have","536","     selected fewer features than it should.","537","     :issue:`7490` by :user:`Peng Meng <mpjlu>`.","538","","539","   - Fixed a bug where :class:`linear_model.RandomizedLasso` and","540","     :class:`linear_model.RandomizedLogisticRegression` breaks for","541","     sparse input. :issue:`8259` by :user:`Aman Dalmia <dalmia>`.","548","     :issue:`7565` by :user:`Roman Yurchak <rth>`.","549","","550","   - Fix a bug where :class:`feature_selection.mutual_info_regression` did not","551","     correctly use ``n_neighbors``. :issue:`8181` by :user:`Guillaume Lemaitre","552","     <glemaitre>`.","553","","554","Model evaluation and meta-estimators","555","","556","   - Fixed a bug where :func:`model_selection.BaseSearchCV.inverse_transform`","557","     returns ``self.best_estimator_.transform()`` instead of","558","     ``self.best_estimator_.inverse_transform()``.","559","     :issue:`8344` by :user:`Akshay Gupta <Akshay0724>` and :user:`Rasmus Eriksson <MrMjauh>`.","560","","561","   - Added ``classes_`` attribute to :class:`model_selection.GridSearchCV`,","562","     :class:`model_selection.RandomizedSearchCV`,  :class:`grid_search.GridSearchCV`,","563","     and  :class:`grid_search.RandomizedSearchCV` that matches the ``classes_``","564","     attribute of ``best_estimator_``. :issue:`7661` and :issue:`8295`","565","     by :user:`Alyssa Batula <abatula>`, :user:`Dylan Werner-Meier <unautre>`,","566","     and :user:`Stephen Hoover <stephen-hoover>`.","567","","568","   - Fixed a bug where :func:`model_selection.validation_curve`","569","     reused the same estimator for each parameter value.","570","     :issue:`7365` by :user:`Aleksandr Sandrovskii <Sundrique>`.","571","","572","   - :func:`model_selection.permutation_test_score` now works with Pandas","573","     types. :issue:`5697` by :user:`Stijn Tonk <equialgo>`.","574","","575","   - Several fixes to input validation in","576","     :class:`multiclass.OutputCodeClassifier`","577","     :issue:`8086` by `Andreas Mller`_.","578","","579","   - :class:`multiclass.OneVsOneClassifier`'s ``partial_fit`` now ensures all","580","     classes are provided up-front. :issue:`6250` by","581","     :user:`Asish Panda <kaichogami>`.","582","","583","   - Fix :func:`multioutput.MultiOutputClassifier.predict_proba` to return a","584","     list of 2d arrays, rather than a 3d array. In the case where different","585","     target columns had different numbers of classes, a ``ValueError`` would be","586","     raised on trying to stack matrices with different dimensions.","587","     :issue:`8093` by :user:`Peter Bull <pjbull>`.","588","","589","Metrics","590","","591","   - :func:`metrics.average_precision_score` no longer linearly","592","     interpolates between operating points, and instead weighs precisions","593","     by the change in recall since the last operating point, as per the","594","     `Wikipedia entry <http:\/\/en.wikipedia.org\/wiki\/Average_precision>`_.","595","     (`#7356 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/7356>`_). By","596","     :user:`Nick Dingwall <ndingwall>` and `Gael Varoquaux`_.","597","","598","   - Fix a bug in :func:`metrics.classification._check_targets`","599","     which would return ``'binary'`` if ``y_true`` and ``y_pred`` were","600","     both ``'binary'`` but the union of ``y_true`` and ``y_pred`` was","601","     ``'multiclass'``. :issue:`8377` by `Loic Esteve`_.","602","","603","   - Fixed an integer overflow bug in :func:`metrics.confusion_matrix` and","604","     hence :func:`metrics.cohen_kappa_score`. :issue:`8354`, :issue:`7929`","605","     by `Joel Nothman`_ and :user:`Jon Crall <Erotemic>`.","606","","607","   - Fixed passing of ``gamma`` parameter to the ``chi2`` kernel in","608","     :func:`metrics.pairwise.pairwise_kernels` :issue:`5211` by","609","     :user:`Nick Rhinehart <nrhine1>`,","610","     :user:`Saurabh Bansod <mth4saurabh>` and `Andreas Mller`_.","611","","612","Miscellaneous","613","","614","   - Fixed a bug when :func:`datasets.make_classification` fails","615","     when generating more than 30 features. :issue:`8159` by","616","     :user:`Herilalaina Rakotoarison <herilalaina>`.","617","","618","   - Fixed a bug where :func:`datasets.make_moons` gives an","619","     incorrect result when ``n_samples`` is odd.","620","     :issue:`8198` by :user:`Josh Levy <levy5674>`.","621","","622","   - Some ``fetch_`` functions in :mod:`datasets` were ignoring the","623","     ``download_if_missing`` keyword. :issue:`7944` by :user:`Ralf Gommers <rgommers>`.","624","","625","   - Fix estimators to accept a ``sample_weight`` parameter of type","626","     ``pandas.Series`` in their ``fit`` function. :issue:`7825` by","627","     `Kathleen Chen`_.","638","   - Update Sphinx-Gallery from 0.1.4 to 0.1.7 for resolving links in","639","     documentation build with Sphinx>1.5 :issue:`8010`, :issue:`7986` by","640","     :user:`Oscar Najera <Titan-C>`","642","   - Add ``data_home`` parameter to :func:`sklearn.datasets.fetch_kddcup99`.","643","     :issue:`9289` by `Loic Esteve`_.","645","   - Fix dataset loaders using Python 3 version of makedirs to also work in","646","     Python 2. :issue:`9284` by :user:`Sebastin Santy <SebastinSanty>`.","648","   - Several minor issues were fixed with thanks to the alerts of","649","     [lgtm.com](http:\/\/lgtm.com). :issue:`9278` by :user:`Jean Helie <jhelie>`,","650","     among others.","655","Trees and ensembles","657","   - Gradient boosting base models are no longer estimators. By `Andreas Mller`_.","658","","659","   - All tree based estimators now accept a ``min_impurity_decrease``","660","     parameter in lieu of the ``min_impurity_split``, which is now deprecated.","661","     The ``min_impurity_decrease`` helps stop splitting the nodes in which","662","     the weighted impurity decrease from splitting is no longer alteast","663","     ``min_impurity_decrease``.  :issue:`8449` by `Raghav RV`_.","664","","665","Linear, kernelized and related models","666","","667","   - ``n_iter`` parameter is deprecated in :class:`linear_model.SGDClassifier`,","668","     :class:`linear_model.SGDRegressor`,","669","     :class:`linear_model.PassiveAggressiveClassifier`,","670","     :class:`linear_model.PassiveAggressiveRegressor` and","671","     :class:`linear_model.Perceptron`. By `Tom Dupre la Tour`_.","672","","673","Other predictors","674","","675","   - :class:`neighbors.LSHForest` has been deprecated and will be","676","     removed in 0.21 due to poor performance.","677","     :issue:`9078` by :user:`Laurent Direr <ldirer>`.","678","","679","   - :class:`neighbors.NearestCentroid` no longer purports to support","680","     ``metric='precomputed'`` which now raises an error. :issue:`8515` by","681","     :user:`Sergul Aydore <sergulaydore>`.","682","","683","   - The ``alpha`` parameter of :class:`semi_supervised.LabelPropagation` now","684","     has no effect and is deprecated to be removed in 0.21. :issue:`9239`","685","     by :user:`Andre Ambrosio Boechat <boechat107>`, :user:`Utkarsh Upadhyay","686","     <musically-ut>`, and `Joel Nothman`_.","687","","688","Decomposition, manifold learning and clustering","696","   - The ``n_topics`` parameter of :class:`decomposition.LatentDirichletAllocation`","697","     has been renamed to ``n_components`` and will be removed in version 0.21.","698","     :issue:`8922` by :user:`Attractadore`.","700","   - :meth:`decomposition.SparsePCA.transform`'s ``ridge_alpha`` parameter is","701","     deprecated in preference for class parameter.","702","     :issue:`8137` by :user:`Naoya Kanai <naoyak>`.","703","","704","   - :class:`cluster.DBSCAN` now has a ``metric_params`` parameter.","705","     :issue:`8139` by :user:`Naoya Kanai <naoyak>`.","706","","707","Preprocessing and feature selection","708","","709","   - :class:`feature_selection.SelectFromModel` now has a ``partial_fit``","710","     method only if the underlying estimator does. By `Andreas Mller`_.","711","","712","   - :class:`feature_selection.SelectFromModel` now validates the ``threshold``","713","     parameter and sets the ``threshold_`` attribute during the call to","714","     ``fit``, and no longer during the call to ``transform```. By `Andreas","715","     Mller`_.","716","","717","   - The ``non_negative`` parameter in :class:`feature_extraction.FeatureHasher`","718","     has been deprecated, and replaced with a more principled alternative,","719","     ``alternate_sign``.","720","     :issue:`7565` by :user:`Roman Yurchak <rth>`.","721","","722","   - :class:`linear_model.RandomizedLogisticRegression`,","723","     and :class:`linear_model.RandomizedLasso` have been deprecated and will","724","     be removed in version 0.21.","725","     :issue:`8995` by :user:`Ramana.S <sentient07>`.","726","","727","Model evaluation and meta-estimators","745","   - :class:`multiclass.OneVsRestClassifier` now has ``partial_fit``,","746","     ``decision_function`` and ``predict_proba`` methods only when the","747","     underlying estimator does.  :issue:`7812` by `Andreas Mller`_ and","748","     :user:`Mikhail Korobov <kmike>`.","750","   - :class:`multiclass.OneVsRestClassifier` now has a ``partial_fit`` method","751","     only if the underlying estimator does.  By `Andreas Mller`_.","753","   - The ``decision_function`` output shape for binary classification in","754","     :class:`multiclass.OneVsRestClassifier` and","755","     :class:`multiclass.OneVsOneClassifier` is now ``(n_samples,)`` to conform","756","     to scikit-learn conventions. :issue:`9100` by `Andreas Mller`_.","758","   - The :func:`multioutput.MultiOutputClassifier.predict_proba`","759","     function used to return a 3d array (``n_samples``, ``n_classes``,","760","     ``n_outputs``). In the case where different target columns had different","761","     numbers of classes, a ``ValueError`` would be raised on trying to stack","762","     matrices with different dimensions. This function now returns a list of","763","     arrays where the length of the list is ``n_outputs``, and each array is","764","     (``n_samples``, ``n_classes``) for that particular output.","765","     :issue:`8093` by :user:`Peter Bull <pjbull>`.","766","","767","   - Replace attribute ``named_steps`` ``dict`` to :class:`utils.Bunch`","768","     in :class:`pipeline.Pipeline` to enable tab completion in interactive","769","     environment. In the case conflict value on ``named_steps`` and ``dict``","770","     attribute, ``dict`` behavior will be prioritized.","771","     :issue:`8481` by :user:`Herilalaina Rakotoarison <herilalaina>`.","772","","773","Miscellaneous","774","","775","   - Deprecate the ``y`` parameter in ``transform`` and ``inverse_transform``.","776","     The method  should not accept ``y`` parameter, as it's used at the prediction time.","777","     :issue:`8174` by :user:`Tahar Zanouda <tzano>`, `Alexandre Gramfort`_","778","     and `Raghav RV`_.","782","     :mod:`utils` have been removed or deprecated accordingly.","812","   - Estimators with both methods ``decision_function`` and ``predict_proba``","813","     are now required to have a monotonic relation between them. The","814","     method ``check_decision_proba_consistency`` has been added in","815","     **utils.estimator_checks** to check their consistency.","816","     :issue:`7578` by :user:`Shubham Bhardwaj <shubham0704>`","817","","818","   - All checks in ``utils.estimator_checks``, in particular","819","     :func:`utils.estimator_checks.check_estimator` now accept estimator","820","     instances. Most other checks do not accept","821","     estimator classes any more. :issue:`9019` by `Andreas Mller`_.","822","","823","   - Ensure that estimators' attributes ending with ``_`` are not set","824","     in the constructor but only in the ``fit`` method. Most notably,","825","     ensemble estimators (deriving from :class:`ensemble.BaseEnsemble`)","826","     now only have ``self.estimators_`` available after ``fit``.","827","     :issue:`7464` by `Lars Buitinck`_ and `Loic Esteve`_.","828","","829","","830",".. _changes_0_18_2:","831","","832","Version 0.18.2","833","==============","834","","835","**June 20, 2017**","836","","837",".. topic:: Last release with Python 2.6 support","838","","839","    Scikit-learn 0.18 is the last major release of scikit-learn to support Python 2.6.","840","    Later versions of scikit-learn will require Python 2.7 or above.","841","","842","","843","Changelog","844","---------","845","","846","    - Fixes for compatibility with NumPy 1.13.0: :issue:`7946` :issue:`8355` by","847","      `Loic Esteve`_.","848","","849","    - Minor compatibility changes in the examples :issue:`9010` :issue:`8040`","850","      :issue:`9149`.","851","","852","Code Contributors","853","-----------------","854","Aman Dalmia, Loic Esteve, Nate Guerin, Sergei Lebedev","964","   - Fix bug where :meth:`preprocessing.MultiLabelBinarizer.fit_transform`","965","     returned an invalid CSR matrix.","966","     :issue:`7750` by :user:`CJ Carey <perimosocordiae>`.","967","","968","   - Fixed a bug where :func:`metrics.pairwise.cosine_distances` could return a","969","     small negative distance. :issue:`7732` by :user:`Artsion <asanakoy>`.","970","","1582","   - Error and loss names for ``scoring`` parameters are now prefixed by","1583","     ``'neg_'``, such as ``neg_mean_squared_error``. The unprefixed versions","1584","     are deprecated and will be removed in version 0.20.","1585","     :issue:`7261` by :user:`Tim Head <betatim>`."],"delete":["20","   * :class:`sklearn.ensemble.IsolationForest` (bug fix)","21","   * :class:`sklearn.manifold.TSNE` (bug fix)","46","     ","47","   - Added :class:`multioutput.ClassifierChain` for multi-label","48","     classification. By `Adam Kleczewski <adamklec>`_.","49","","50","   - Validation that input data contains no NaN or inf can now be suppressed","51","     using :func:`config_context`, at your own risk. This will save on runtime,","52","     and may be particularly useful for prediction time. :issue:`7548` by","53","     `Joel Nothman`_.","54","","55","   - Added the :class:`neighbors.LocalOutlierFactor` class for anomaly","56","     detection based on nearest neighbors.","57","     :issue:`5279` by `Nicolas Goix`_ and `Alexandre Gramfort`_.","58","","59","   - The new solver ``'mu'`` implements a Multiplicate Update in","60","     :class:`decomposition.NMF`, allowing the optimization of all","61","     beta-divergences, including the Frobenius norm, the generalized","62","     Kullback-Leibler divergence and the Itakura-Saito divergence.","63","     :issue:`5295` by `Tom Dupre la Tour`_.","64","","65","   - Added the :class:`model_selection.RepeatedKFold` and","66","     :class:`model_selection.RepeatedStratifiedKFold`.","67","     :issue:`8120` by `Neeraj Gangwar`_.","74","   - Added solver ``'saga'`` that implements the improved version of Stochastic","75","     Average Gradient, in :class:`linear_model.LogisticRegression` and","76","     :class:`linear_model.Ridge`. It allows the use of L1 penalty with","77","     multinomial logistic loss, and behaves marginally better than 'sag'","78","     during the first epochs of ridge and logistic regression.","79","     :issue:`8446` by `Arthur Mensch`_.","80","","81","   - Added :class:`preprocessing.QuantileTransformer` class and","82","     :func:`preprocessing.quantile_transform` function for features","83","     normalization based on quantiles.","84","     :issue:`8363` by :user:`Denis Engemann <dengemann>`,","85","     :user:`Guillaume Lemaitre <glemaitre>`, `Olivier Grisel`_, `Raghav RV`_,","86","     :user:`Thierry Guillemot <tguillemot>`, and `Gael Varoquaux`_.","87","","96","   - :func:`metrics.matthews_corrcoef` now support multiclass classification.","97","     :issue:`8094` by :user:`Jon Crall <Erotemic>`.","98","   - Update Sphinx-Gallery from 0.1.4 to 0.1.7 for resolving links in","99","     documentation build with Sphinx>1.5 :issue:`8010`, :issue:`7986` by","100","     :user:`Oscar Najera <Titan-C>`","101","   - :class:`multioutput.MultiOutputRegressor` and :class:`multioutput.MultiOutputClassifier`","102","     now support online learning using `partial_fit`.","103","     issue: `8053` by :user:`Peng Yu <yupbank>`.","104","   - :class:`pipeline.Pipeline` allows to cache transformers","105","     within a pipeline by using the ``memory`` constructor parameter.","106","     :issue:`7990` by :user:`Guillaume Lemaitre <glemaitre>`.","107","","108","   - :class:`decomposition.PCA`, :class:`decomposition.IncrementalPCA` and","109","     :class:`decomposition.TruncatedSVD` now expose the singular values","110","     from the underlying SVD. They are stored in the attribute","111","     ``singular_values_``, like in :class:`decomposition.IncrementalPCA`.","112","","113","   - :class:`cluster.MiniBatchKMeans` and :class:`cluster.KMeans`","114","     now uses significantly less memory when assigning data points to their","115","     nearest cluster center. :issue:`7721` by :user:`Jon Crall <Erotemic>`.","116","","117","   - Added ``classes_`` attribute to :class:`model_selection.GridSearchCV`,","118","     :class:`model_selection.RandomizedSearchCV`,  :class:`grid_search.GridSearchCV`,","119","     and  :class:`grid_search.RandomizedSearchCV` that matches the ``classes_``","120","     attribute of ``best_estimator_``. :issue:`7661` and :issue:`8295`","121","     by :user:`Alyssa Batula <abatula>`, :user:`Dylan Werner-Meier <unautre>`,","122","     and :user:`Stephen Hoover <stephen-hoover>`.","123","","124","   - Relax assumption on the data for the","125","     :class:`kernel_approximation.SkewedChi2Sampler`. Since the Skewed-Chi2","126","     kernel is defined on the open interval :math:`(-skewedness; +\\infty)^d`,","127","     the transform function should not check whether ``X < 0`` but whether ``X <","128","     -self.skewedness``. :issue:`7573` by :user:`Romain Brault <RomainBrault>`.","136","   - Added ``average`` parameter to perform weights averaging in","137","     :class:`linear_model.PassiveAggressiveClassifier`. :issue:`4939`","138","     by :user:`Andrea Esuli <aesuli>`.","139","","140","   - Custom metrics for the :mod:`sklearn.neighbors` binary trees now have","141","     fewer constraints: they must take two 1d-arrays and return a float.","142","     :issue:`6288` by `Jake Vanderplas`_.","143","","148","   - Added ``shuffle`` and ``random_state`` parameters to shuffle training","149","     data before taking prefixes of it based on training sizes in","150","     :func:`model_selection.learning_curve`.","151","     :issue:`7506` by :user:`Narine Kokhlikyan <NarineK>`.","153","   - Added ``norm_order`` parameter to :class:`feature_selection.SelectFromModel`","154","     to enable selection of the norm order when ``coef_`` is more than 1D.","155","     :issue:`6181` by :user:`Antoine Wendlinger <antoinewdg>`.","157","   - Added ``sample_weight`` parameter to :meth:`pipeline.Pipeline.score`.","158","     :issue:`7723` by :user:`Mikhail Korobov <kmike>`.","159","","160","   - ``check_estimator`` now attempts to ensure that methods transform, predict, etc.","161","     do not set attributes on the estimator.","162","     :issue:`7533` by :user:`Ekaterina Krivich <kiote>`.","167","     :class:`linear_model.Perceptron` now expose a ``max_iter`` and","171","     convergence. By `Tom Dupre la Tour`_.","173","   - For sparse matrices, :func:`preprocessing.normalize` with ``return_norm=True``","174","     will now raise a ``NotImplementedError`` with 'l1' or 'l2' norm and with","175","     norm 'max' the norms returned will be the same as for dense matrices.","176","     :issue:`7771` by `Ang Lu <https:\/\/github.com\/luang008>`_.","184","   - :func:`model_selection.cross_val_predict` now returns output of the","185","     correct shape for all values of the argument ``method``.","186","     :issue:`7863` by :user:`Aman Dalmia <dalmia>`.","188","   - Fix a bug where :class:`feature_selection.SelectFdr` did not","189","     exactly implement Benjamini-Hochberg procedure. It formerly may have","190","     selected fewer features than it should.","191","     :issue:`7490` by :user:`Peng Meng <mpjlu>`.","197","   - Added type checking to the ``accept_sparse`` parameter in","198","     :mod:`sklearn.utils.validation` methods. This parameter now accepts only","199","     boolean, string, or list\/tuple of strings. ``accept_sparse=None`` is deprecated","200","     and should be replaced by ``accept_sparse=False``.","201","     :issue:`7880` by :user:`Josh Karnofsky <jkarno>`.","203","   - :class:`model_selection.GridSearchCV`, :class:`model_selection.RandomizedSearchCV`","204","     and :func:`model_selection.cross_val_score` now allow estimators with callable","205","     kernels which were previously prohibited. :issue:`8005` by `Andreas Mller`_ .","207","   - Added ability to use sparse matrices in :func:`feature_selection.f_regression`","208","     with ``center=True``. :issue:`8065` by :user:`Daniel LeJeune <acadiansith>`.","213","   - In :class:`gaussian_process.GaussianProcessRegressor`, method ``predict``","214","     is a lot faster with ``return_std=True``. :issue:`8591` by","215","     :user:`Hadrien Bertrand <hbertrand>`.","217","   - Added ability to use sparse matrices in :func:`feature_selection.f_regression`","218","     with ``center=True``. :issue:`8065` by :user:`Daniel LeJeune <acadiansith>`.","220","   - :class:`ensemble.VotingClassifier` now allow changing estimators by using","221","     :meth:`ensemble.VotingClassifier.set_params`. Estimators can also be","222","     removed by setting it to `None`.","223","     :issue:`7674` by :user:`Yichuan Liu <yl565>`.","224","","225","   - Prevent cast from float32 to float64 in","226","     :class:`linear_model.LogisticRegression` when using newton-cg","227","     solver. :issue:`8835` by :user:`Joan Massich <massich>`.","228","","229","   - Prevent cast from float32 to float64 in","230","     :class:`linear_model.Ridge` when using svd, sparse_cg, cholesky or lsqr solvers","231","     :class:`sklearn.linear_model.Ridge` when using svd, sparse_cg, cholesky or lsqr solvers","232","     by :user:`Joan Massich <massich>`, :user:`Nicolas Cordier <ncordier>`","233","","234","   - Add ``max_train_size`` parameter to :class:`model_selection.TimeSeriesSplit`","235","     :issue:`8282` by :user:`Aman Dalmia <dalmia>`.","241","   - Small performance improvement to n-gram creation in","242","     :mod:`feature_extraction.text` by binding methods for loops and","243","     special-casing unigrams. :issue:`7567` by `Jaye Doepke <jtdoepke>`","244","","245","   - Speed improvements to :class:`model_selection.StratifiedShuffleSplit`.","246","     :issue:`5991` by :user:`Arthur Mensch <arthurmensch>` and `Joel Nothman`_.","247","","248","   - Memory improvements for method barnes_hut in :class:`manifold.TSNE`","249","     :issue:`7089` by :user:`Thomas Moreau <tomMoral>` and `Olivier Grisel`_.","250","","251","   - Optimization schedule improvements for so the results are closer to the","252","     one from the reference implementation","253","     `lvdmaaten\/bhtsne <https:\/\/github.com\/lvdmaaten\/bhtsne>`_ by","254","     :user:`Thomas Moreau <tomMoral>` and `Olivier Grisel`_.","259","   - :func:`metrics.average_precision_score` no longer linearly","260","     interpolates between operating points, and instead weighs precisions","261","     by the change in recall since the last operating point, as per the","262","     `Wikipedia entry <http:\/\/en.wikipedia.org\/wiki\/Average_precision>`_.","263","     (`#7356 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/7356>`_). By","264","     :user:`Nick Dingwall <ndingwall>` and `Gael Varoquaux`_.","266","   - Fixed a bug in :class:`covariance.MinCovDet` where inputting data","267","     that produced a singular covariance matrix would cause the helper method","268","     ``_c_step`` to throw an exception.","269","     :issue:`3367` by :user:`Jeremy Steward <ThatGeoGuy>`","275","   - Fixed a bug where :class:`cluster.DBSCAN` gives incorrect","276","     result when input is a precomputed sparse matrix with initial","277","     rows all zero. :issue:`8306` by :user:`Akshay Gupta <Akshay0724>`","278","","283","   - Fixed a bug when :func:`datasets.make_classification` fails","284","     when generating more than 30 features. :issue:`8159` by","285","     :user:`Herilalaina Rakotoarison <herilalaina>`.","286","","287","   - Fixed a bug where :func:`model_selection.BaseSearchCV.inverse_transform`","288","     returns ``self.best_estimator_.transform()`` instead of","289","     ``self.best_estimator_.inverse_transform()``.","290","     :issue:`8344` by :user:`Akshay Gupta <Akshay0724>`.","291","","292","   - Fixed same issue in :func:`grid_search.BaseSearchCV.inverse_transform`","293","     :issue:`8846` by :user:`Rasmus Eriksson <MrMjauh>`","294","","295","   - Fixed a bug where :class:`linear_model.RandomizedLasso` and","296","     :class:`linear_model.RandomizedLogisticRegression` breaks for","297","     sparse input. :issue:`8259` by :user:`Aman Dalmia <dalmia>`.","298","","299","   - Fixed a bug where :func:`linear_model.RANSACRegressor.fit` may run until","300","     ``max_iter`` if finds a large inlier group early. :issue:`8251` by :user:`aivision2020`.","301","","302","   - Fixed a bug where :class:`sklearn.naive_bayes.MultinomialNB` and :class:`sklearn.naive_bayes.BernoulliNB`","303","     failed when `alpha=0`. :issue:`5814` by :user:`Yichuan Liu <yl565>` and","304","     :user:`Herilalaina Rakotoarison <herilalaina>`.","305","","306","   - Fixed a bug where :func:`datasets.make_moons` gives an","307","     incorrect result when ``n_samples`` is odd.","308","     :issue:`8198` by :user:`Josh Levy <levy5674>`.","309","","310","   - Fixed a bug where :class:`linear_model.LassoLars` does not give","311","     the same result as the LassoLars implementation available","312","     in R (lars library). :issue:`7849` by :user:`Jair Montoya Martinez <jmontoyam>`.","313","","314","   - Some ``fetch_`` functions in :mod:`sklearn.datasets` were ignoring the","315","     ``download_if_missing`` keyword. :issue:`7944` by :user:`Ralf Gommers <rgommers>`.","316","","317","   - Fixed a bug in :class:`ensemble.GradientBoostingClassifier`","318","     and :class:`ensemble.GradientBoostingRegressor`","319","     where a float being compared to ``0.0`` using ``==`` caused a divide by zero","320","     error. issue:`7970` by :user:`He Chen <chenhe95>`.","321","","322","   - Fix a bug regarding fitting :class:`cluster.KMeans` with a sparse","323","     array X and initial centroids, where X's means were unnecessarily being","324","     subtracted from the centroids. :issue:`7872` by :user:`Josh Karnofsky <jkarno>`.","325","","326","   - Fix estimators to accept a ``sample_weight`` parameter of type","327","     ``pandas.Series`` in their ``fit`` function. :issue:`7825` by","328","     `Kathleen Chen`_.","329","","330","   - Fixed a bug where :class:`ensemble.IsolationForest` fails when","331","     ``max_features`` is less than 1.","332","     :issue:`5732` by :user:`Ishank Gulati <IshankGulati>`.","333","","334","   - Fix a bug where :class:`ensemble.VotingClassifier` raises an error","335","     when a numpy array is passed in for weights. :issue:`7983` by","336","     :user:`Vincent Pham <vincentpham1991>`.","337","","338","   - Fix a bug in :class:`decomposition.LatentDirichletAllocation`","339","     where the ``perplexity`` method was returning incorrect results because","340","     the ``transform`` method returns normalized document topic distributions","341","     as of version 0.18. :issue:`7954` by :user:`Gary Foreman <garyForeman>`.","348","   - Fixes to the input validation in :class:`covariance.EllipticEnvelope`.","349","     :issue:`8086` by `Andreas Mller`_.","358","   - Several fixes to input validation in","359","     :class:`multiclass.OutputCodeClassifier`","362","   - Fix a bug where","363","     :class:`ensemble.gradient_boosting.QuantileLossFunction` computed","364","     negative errors for negative values of ``ytrue - ypred`` leading to","365","     wrong values when calling ``__call__``.","366","     :issue:`8087` by :user:`Alexis Mignon <AlexisMignon>`","368","   - Fix :func:`multioutput.MultiOutputClassifier.predict_proba` to","369","     return a list of 2d arrays, rather than a 3d array. In the case where","370","     different target columns had different numbers of classes, a `ValueError`","371","     would be raised on trying to stack matrices with different dimensions.","372","     :issue:`8093` by :user:`Peter Bull <pjbull>`.","374","   - Fix a bug where :func:`linear_model.LassoLars.fit` sometimes","375","     left `coef_` as a list, rather than an ndarray.","376","     :issue:`8160` by :user:`CJ Carey <perimosocordiae>`.","383","     :issue:`7513` by :user:`Roman Yurchak <rth>`.","394","   - Fix a bug in :func:`metrics.classification._check_targets`","395","     which would return ``'binary'`` if ``y_true`` and ``y_pred`` were","396","     both ``'binary'`` but the union of ``y_true`` and ``y_pred`` was","397","     ``'multiclass'``. :issue:`8377` by `Loic Esteve`_.","400","   - Fix :func:`linear_model.BayesianRidge.fit` to return","401","     ridge parameter `alpha_` and `lambda_` consistent with calculated","402","     coefficients `coef_` and `intercept_`.","403","     :issue:`8224` by :user:`Peter Gedeck <gedeck>`.","405","   - Fixed a bug in :class:`manifold.TSNE` where it stored the incorrect","406","     ``kl_divergence_``. :issue:`6507` by :user:`Sebastian Saeger <ssaeger>`.","407","","408","   - Fixed a bug in :class:`svm.OneClassSVM` where it returned floats instead of","409","     integer classes. :issue:`8676` by :user:`Vathsala Achar <VathsalaAchar>`.","410","","411","   - Fixed a bug where :func:`tree.export_graphviz` raised an error","412","     when the length of features_names does not match n_features in the decision","413","     tree. :issue:`8512` by :user:`Li Li <aikinogard>`.","414","","415","   - Fixed a bug in :class:`manifold.TSNE` affecting convergence of the","416","     gradient descent. :issue:`8768` by :user:`David DeTomaso <deto>`.","417","","418","   - Fixed a memory leak in our LibLinear implementation. :issue:`9024` by","419","     :user:`Sergei Lebedev <superbobry>`","420","   - Fixed improper scaling in :class:`cross_decomposition.PLSRegression`","421","     with ``scale=True``. :issue:`7819` by :user:`jayzed82 <jayzed82>`.","422","","423","   - Fixed oob_score in :class:`ensemble.BaggingClassifier`.","424","     :issue:`8936` by :user:`mlewis1729 <mlewis1729>`","425","","426","   - Add ``shuffle`` parameter to :func:`model_selection.train_test_split`.","427","     :issue:`8845` by  :user:`themrmax <themrmax>`","428","","429","   - Fix AIC\/BIC criterion computation in :class:`linear_model.LassoLarsIC`.","430","     :issue:`9022` by `Alexandre Gramfort`_ and :user:`Mehmet Basbug <mehmetbasbug>`.","431","","432","   - Fix bug where stratified CV splitters did not work with","433","     :class:`linear_model.LassoCV`. :issue:`8973` by","434","     :user:`Paulo Haddad <paulochf>`.","435","","436","   - Fixed a bug in :class:`linear_model.RandomizedLasso`,","437","     :class:`linear_model.Lars`, :class:`linear_model.LassoLars`,","438","     :class:`linear_model.LarsCV` and :class:`linear_model.LassoLarsCV`,","439","     where the parameter ``precompute`` were not used consistently across","440","     classes, and some values proposed in the docstring could raise errors.","441","     :issue:`5359` by `Tom Dupre la Tour`_.","442","","443","   - Fixed a bug where :func:`model_selection.validation_curve`","444","     reused the same estimator for each parameter value.","445","     :issue:`7365` by :user:`Aleksandr Sandrovskii <Sundrique>`.","446","","447","   - :class:`multiclass.OneVsOneClassifier`'s ``partial_fit`` now ensures all","448","     classes are provided up-front. :issue:`6250` by","449","     :user:`Asish Panda <kaichogami>`.","450","","451","   - Fixed an integer overflow bug in :func:`metrics.confusion_matrix` and","452","     hence :func:`metrics.cohen_kappa_score`. :issue:`8354`, :issue:`7929`","453","     by `Joel Nothman`_ and :user:`Jon Crall <Erotemic>`.","454","","455","   - Made default kernel parameters kernel-dependent in :class:`kernel_approximation.Nystroem`","456","     :issue:`5229` by :user:`mth4saurabh` and `Andreas Mller`_.","457","","458","   - Fixed passing of ``gamma`` parameter to the ``chi2`` kernel in","459","     :func:`metrics.pairwise_kernels` :issue:`5211` by :user:`nrhine1`,","460","     :user:`mth4saurabh` and `Andreas Mller`_.","461","","462","  -  Fixed a bug in :class:`gaussian_process.GaussianProcessRegressor`","463","     when the standard deviation and covariance predicted without fit","464","     would fail with a unmeaningful error by default.","465","     :issue:`6573` by :user:`Quazi Marufur Rahman <qmaruf>` and","466","     `Manoj Kumar`_.","467","","468","   - Fixed the implementation of `explained_variance_`","469","     in :class:`decomposition.PCA`,","470","     :class:`decomposition.RandomizedPCA` and","471","     :class:`decomposition.IncrementalPCA`.","472","     :issue:`9105` by `Hanmin Qin <https:\/\/github.com\/qinhanmin2014>`_.","473","","474","   - Fix :class:`semi_supervised.BaseLabelPropagation` to correctly implement","475","     ``LabelPropagation`` and ``LabelSpreading`` as done in the referenced","476","     papers. :class:`semi_supervised.LabelPropagation` now always does hard","477","     clamping. Its ``alpha`` parameter has no effect and is","478","     deprecated to be removed in 0.21. :issue:`6727` :issue:`3550` issue:`5770`","479","     by :user:`Andre Ambrosio Boechat <boechat107>`, :user:`Utkarsh Upadhyay","480","     <musically-ut>`, and `Joel Nothman`_.","481","","482","   - Add ``data_home`` parameter to","483","     :func:`sklearn.datasets.fetch_kddcup99` by `Loic Esteve`_.","484","","485","   - Fix inconsistent results between :class:`linear_model.RidgeCV`","486","     and :class:`linear_model.Ridge` when using ``normalize=True``","487","     by `Alexandre Gramfort`_.","488","","489","   - Fixed the implementation of :class:`manifold.TSNE`:","490","      - ``early_exageration`` parameter had no effect and is now used for the","491","        first 250 optimization iterations.","492","      - Fixed the ``InsersionError`` reported in :issue:`8992`.","493","      - Improve the learning schedule to match the one from the reference","494","        implementation `lvdmaaten\/bhtsne <https:\/\/github.com\/lvdmaaten\/bhtsne>`_.","495","     by :user:`Thomas Moreau <tomMoral>` and `Olivier Grisel`_.","500","   - Ensure that estimators' attributes ending with ``_`` are not set","501","     in the constructor but only in the ``fit`` method. Most notably,","502","     ensemble estimators (deriving from :class:`ensemble.BaseEnsemble`)","503","     now only have ``self.estimators_`` available after ``fit``.","504","     :issue:`7464` by `Lars Buitinck`_ and `Loic Esteve`_.","506","   - All checks in ``utils.estimator_checks``, in particular","507","     :func:`utils.estimator_checks.check_estimator` now accept estimator","508","     instances. Most other checks do not accept","509","     estimator classes any more. :issue:`9019` by `Andreas Mller`_.","517","   - Replace attribute ``named_steps`` ``dict`` to :class:`utils.Bunch`","518","     in :class:`pipeline.Pipeline` to enable tab completion in interactive","519","     environment. In the case conflict value on ``named_steps`` and ``dict``","520","     attribute, ``dict`` behavior will be prioritized.","521","     :issue:`8481` by :user:`Herilalaina Rakotoarison <herilalaina>`.","523","   - The :func:`multioutput.MultiOutputClassifier.predict_proba`","524","     function used to return a 3d array (``n_samples``, ``n_classes``,","525","     ``n_outputs``). In the case where different target columns had different","526","     numbers of classes, a `ValueError` would be raised on trying to stack","527","     matrices with different dimensions. This function now returns a list of","528","     arrays where the length of the list is ``n_outputs``, and each array is","529","     (``n_samples``, ``n_classes``) for that particular output.","530","     :issue:`8093` by :user:`Peter Bull <pjbull>`.","543","   - The ``decision_function`` output shape for binary classification in","544","     :class:`multiclass.OneVsRestClassifier` and","545","     :class:`multiclass.OneVsOneClassifier` is now ``(n_samples,)`` to conform","546","     to scikit-learn conventions. :issue:`9100` by `Andreas Mller`_.","547","","548","   - Gradient boosting base models are no longer estimators. By `Andreas Mller`_.","549","","550","   - :class:`feature_selection.SelectFromModel` now validates the ``threshold``","551","     parameter and sets the ``threshold_`` attribute during the call to","552","     ``fit``, and no longer during the call to ``transform```, by `Andreas","553","     Mller`_.","554","","555","   - :class:`feature_selection.SelectFromModel` now has a ``partial_fit``","556","     method only if the underlying estimator does. By `Andreas Mller`_.","557","","558","   - :class:`multiclass.OneVsRestClassifier` now has a ``partial_fit`` method","559","     only if the underlying estimator does.  By `Andreas Mller`_.","560","","561","   - Estimators with both methods ``decision_function`` and ``predict_proba``","562","     are now required to have a monotonic relation between them. The","563","     method ``check_decision_proba_consistency`` has been added in","564","     **sklearn.utils.estimator_checks** to check their consistency.","565","     :issue:`7578` by :user:`Shubham Bhardwaj <shubham0704>`","566","","572","   - All tree based estimators now accept a ``min_impurity_decrease``","573","     parameter in lieu of the ``min_impurity_split``, which is now deprecated.","574","     The ``min_impurity_decrease`` helps stop splitting the nodes in which","575","     the weighted impurity decrease from splitting is no longer alteast","576","     ``min_impurity_decrease``.  :issue:`8449` by `Raghav RV`_.","578","   - The ``n_topics`` parameter of :class:`decomposition.LatentDirichletAllocation`","579","     has been renamed to ``n_components`` and will be removed in version 0.21.","580","     :issue:`8922` by :user:`Attractadore`","582","   - :class:`cluster.bicluster.SpectralCoclustering` and","583","     :class:`cluster.bicluster.SpectralBiclustering` now accept ``y`` in fit.","584","     :issue:`6126` by :user:ldirer","586","   - :class:`neighbors.LSHForest` has been deprecated and will be","587","     removed in 0.21 due to poor performance.","588","     :issue:`8996` by `Andreas Mller`_.","592","     :mod:`sklearn.utils` have been removed or deprecated accordingly.","621","     - ``neighbors.approximate.LSHForest``","622","     - ``linear_model.randomized_l1``","624","    - Deprecate the ``y`` parameter in `transform` and `inverse_transform`.","625","      The method  should not accept ``y`` parameter, as it's used at the prediction time.","626","      :issue:`8174` by :user:`Tahar Zanouda <tzano>`, `Alexandre Gramfort`_","627","      and `Raghav RV`_.","637",".. topic:: Last release with Python 2.6 support","638","","639","    Scikit-learn 0.18 is the last major release of scikit-learn to support Python 2.6.","640","    Later versions of scikit-learn will require Python 2.7 or above.","641","","642","","1354","   - The :mod:`sklearn.linear_model.randomized_l1` is deprecated.","1355","     :issue: `8995` by :user:`Ramana.S <sentient07>`."]}],"doc\/modules\/pipeline.rst":[{"add":["126",".. _pipeline_cache:","127",""],"delete":[]}]}},"ebf2bf81075ae1f4eb47ea0f54981c512bda5ceb":{"changes":{"sklearn\/preprocessing\/tests\/test_label.py":"MODIFY","sklearn\/tests\/test_pipeline.py":"MODIFY","sklearn\/metrics\/cluster\/unsupervised.py":"MODIFY","sklearn\/tree\/_tree.pyx":"MODIFY","sklearn\/neighbors\/dist_metrics.pyx":"MODIFY","sklearn\/datasets\/tests\/test_svmlight_format.py":"MODIFY","sklearn\/feature_selection\/base.py":"MODIFY","sklearn\/neighbors\/lof.py":"MODIFY"},"diff":{"sklearn\/preprocessing\/tests\/test_label.py":[{"add":["223","            # With fit_transform","265","        # With fit_transform"],"delete":["223","            # With fit_tranform","265","        # With fit_tranform"]}],"sklearn\/tests\/test_pipeline.py":[{"add":["876","        # Get the time stamp of the transformer in the cached pipeline"],"delete":["876","        # Get the time stamp of the tranformer in the cached pipeline"]}],"sklearn\/metrics\/cluster\/unsupervised.py":[{"add":["30","    Note that Silhouette Coefficient is only defined if number of labels","116","    Note that Silhouette Coefficient is only defined if number of labels"],"delete":["30","    Note that Silhouette Coefficent is only defined if number of labels","116","    Note that Silhouette Coefficent is only defined if number of labels"]}],"sklearn\/tree\/_tree.pyx":[{"add":["639","        # capacity is inferred during the __setstate__ using nodes"],"delete":["639","        # capacity is infered during the __setstate__ using nodes"]}],"sklearn\/neighbors\/dist_metrics.pyx":[{"add":["345","        more efficient measure which preserves the rank of the true distance.","355","        more efficient measure which preserves the rank of the true distance."],"delete":["345","        more efficent measure which preserves the rank of the true distance.","355","        more efficent measure which preserves the rank of the true distance."]}],"sklearn\/datasets\/tests\/test_svmlight_format.py":[{"add":["444","        # load the original sparse matrix into 3 independent CSR matrices"],"delete":["444","        # load the original sparse matrix into 3 independant CSR matrices"]}],"sklearn\/feature_selection\/base.py":[{"add":["19","    Transformer mixin that performs feature selection given a support mask"],"delete":["19","    Tranformer mixin that performs feature selection given a support mask"]}],"sklearn\/neighbors\/lof.py":[{"add":["296","        #  1e-10 to avoid `nan' when nb of duplicates > n_neighbors_:"],"delete":["296","        #  1e-10 to avoid `nan' when when nb of duplicates > n_neighbors_:"]}]}},"055d17b30953daa7e2ed3819786828ce9da36bfd":{"changes":{"sklearn\/linear_model\/sag_fast.pyx":"MODIFY"},"diff":{"sklearn\/linear_model\/sag_fast.pyx":[{"add":["616","                        if lagged_ind > 0:","617","                            grad_step = (cumulative_sums[lagged_ind]","618","                               - cumulative_sums[lagged_ind - 1])","619","                            prox_step = (cumulative_sums_prox[lagged_ind]","620","                               - cumulative_sums_prox[lagged_ind - 1])","621","                        else:","622","                            grad_step = cumulative_sums[lagged_ind]","623","                            prox_step = cumulative_sums_prox[lagged_ind]"],"delete":["616","                        grad_step = (cumulative_sums[lagged_ind]","617","                           - cumulative_sums[lagged_ind - 1])","618","                        prox_step = (cumulative_sums_prox[lagged_ind]","619","                           - cumulative_sums_prox[lagged_ind - 1])"]}]}},"09b4cfdfb28e95bd589a0dabfb95816c36286e7f":{"changes":{"doc\/faq.rst":"MODIFY"},"diff":{"doc\/faq.rst":[{"add":["26","usually a major and lengthy undertaking, it is recommended to start with","27",":ref:`known issues <new_contributors>`. Please do not contact the contributors","28","of scikit-learn directly regarding contributing to scikit-learn."],"delete":["26","usually a major and lengthy undertaking, it is recommended to start with :ref:`known","27","issues <easy_issues>`_. Please do not contact the contributors of scikit-learn directly","28","regarding contributing to scikit-learn."]}]}},"7dffa20363d095320ede354edf66f8b6123ba121":{"changes":{"doc\/themes\/scikit-learn\/layout.html":"MODIFY","CONTRIBUTING.md":"MODIFY","sklearn\/linear_model\/tests\/test_bayes.py":"MODIFY","doc\/faq.rst":"MODIFY","doc\/developers\/contributing.rst":"MODIFY","doc\/sphinxext\/sphinx_issues.py":"MODIFY"},"diff":{"doc\/themes\/scikit-learn\/layout.html":[{"add":["205","<!-- GitHub \"fork me\" ribbon -->"],"delete":["205","<!-- Github \"fork me\" ribbon -->"]}],"CONTRIBUTING.md":[{"add":["162","We use GitHub issues to track all bugs and feature requests; feel free to"],"delete":["162","We use Github issues to track all bugs and feature requests; feel free to"]}],"sklearn\/linear_model\/tests\/test_bayes.py":[{"add":["38","    # Test correctness of lambda_ and alpha_ parameters (GitHub issue #8224)"],"delete":["38","    # Test correctness of lambda_ and alpha_ parameters (Github issue #8224)"]}],"doc\/faq.rst":[{"add":["57","`issue tracker on GitHub <https:\/\/github.com\/scikit-learn\/scikit-learn\/issues>`_."],"delete":["57","`issue tracker on Github <https:\/\/github.com\/scikit-learn\/scikit-learn\/issues>`_."]}],"doc\/developers\/contributing.rst":[{"add":["312","We use GitHub issues to track all bugs and feature requests; feel free to","463","   versions of Sphinx as possible, the different versions tend to","513","`GitHub issue tracker <https:\/\/github.com\/scikit-learn\/scikit-learn\/issues>`_"],"delete":["312","We use Github issues to track all bugs and feature requests; feel free to","463","   version of Sphinx as possible, the different versions tend to","513","`Github issue tracker <https:\/\/github.com\/scikit-learn\/scikit-learn\/issues>`_"]}],"doc\/sphinxext\/sphinx_issues.py":[{"add":["35","    GitHub profiles, but the profile URIS can be configured via the","106","    # Shortcut for GitHub, e.g. 'sloria\/marshmallow'"],"delete":["35","    Github profiles, but the profile URIS can be configured via the","106","    # Shortcut for Github, e.g. 'sloria\/marshmallow'"]}]}},"026e10a24ffb9788ad914e642efe3b1b9559378e":{"changes":{"doc\/modules\/naive_bayes.rst":"MODIFY"},"diff":{"doc\/modules\/naive_bayes.rst":[{"add":["138","----------------------","151","    \\hat{\\theta}_{ci} = \\frac{\\alpha_i + \\sum_{j:y_j \\neq c} d_{ij}}","152","                             {\\alpha + \\sum_{j:y_j \\neq c} \\sum_{k} d_{kj}}","154","    w_{ci} = \\log \\hat{\\theta}_{ci}","155","","156","    w_{ci} = \\frac{w_{ci}}{\\sum_{j} w_{cj}}","157","","158","where the summations are over all documents :math:`j` not in class :math:`c`,","160",":math:`j`, :math:`\\alpha_i` is a smoothing hyperparameter like that found in","161","MNB, and :math:`\\alpha = \\sum_{i} \\alpha_i`. The second normalization addresses","162","the tendency for longer documents to dominate parameter estimates in MNB. The","163","classification rule is:","167","    \\hat{c} = \\arg\\min_c \\sum_{i} t_i w_{ci}"],"delete":["138","-----------------------","151","    \\hat{\\theta}_{ci} = \\frac{\\sum{j:y_j \\neq c} d_{ij} + \\alpha_i}","152","                             {\\sum{j:y_j \\neq c} \\sum{k} d_{kj} + \\alpha}","153","    w_{ci} = \\log \\hat{\\theta}_{ci}","154","    w_{ci} = \\frac{w_{ci}{\\sum{j} w_{cj}}","156","where the summation is over all documents :math:`j` not in class :math:`c`,","158",":math:`j`, and :math:`\\alpha` is a smoothing hyperparameter like that found in","159","MNB. The second normalization addresses the tendency for longer documents to","160","dominate parameter estimates in MNB. The classification rule is:","164","    \\hat{c} = \\arg\\min_c \\sum{i} t_i w_{ci}"]}]}},"8fb648af0e673a372e52b9e7ec0e4d48b47cc83f":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/metrics\/tests\/test_common.py":"MODIFY","sklearn\/metrics\/ranking.py":"MODIFY","sklearn\/metrics\/tests\/test_ranking.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["19","- :class:`metrics.roc_auc_score` (bug fix)","109","Metrics","110","","111","- Fixed a bug due to floating point error in :func:`metrics.roc_auc_score` with","112","  non-integer sample weights. :issue:`9786` by :user:`Hanmin Qin <qinhanmin2014>`.","113",""],"delete":["60","","61",""]}],"sklearn\/metrics\/tests\/test_common.py":[{"add":["214","    \"roc_auc_score\",","215","    \"micro_roc_auc\",","216","    \"weighted_roc_auc\",","217","    \"macro_roc_auc\",","218","    \"samples_roc_auc\",","219",""],"delete":["200","    \"roc_auc_score\",","201","    \"micro_roc_auc\",","202","    \"weighted_roc_auc\",","203","    \"macro_roc_auc\",","204","    \"samples_roc_auc\",","205",""]}],"sklearn\/metrics\/ranking.py":[{"add":["272","        return auc(fpr, tpr)","313","    check_consistent_length(y_true, y_score, sample_weight)","355","        # express fps as a cumsum to ensure fps is increasing even in","356","        # the presense of floating point errors","357","        fps = stable_cumsum((1 - y_true) * weight)[threshold_idxs]"],"delete":["272","        return auc(fpr, tpr, reorder=True)","313","    check_consistent_length(y_true, y_score)","355","        fps = stable_cumsum(weight)[threshold_idxs] - tps"]}],"sklearn\/metrics\/tests\/test_ranking.py":[{"add":["373","def test_roc_curve_fpr_tpr_increasing():","374","    # Ensure that fpr and tpr returned by roc_curve are increasing.","375","    # Construct an edge case with float y_score and sample_weight","376","    # when some adjacent values of fpr and tpr are actually the same.","377","    y_true = [0, 0, 1, 1, 1]","378","    y_score = [0.1, 0.7, 0.3, 0.4, 0.5]","379","    sample_weight = np.repeat(0.2, 5)","380","    fpr, tpr, _ = roc_curve(y_true, y_score, sample_weight=sample_weight)","381","    assert_equal((np.diff(fpr) < 0).sum(), 0)","382","    assert_equal((np.diff(tpr) < 0).sum(), 0)","383","","384",""],"delete":[]}]}},"ac41281451618abee3bf3004d0b72f253840626c":{"changes":{"sklearn\/covariance\/graph_lasso_.py":"MODIFY","sklearn\/tree\/export.py":"MODIFY","sklearn\/utils\/sparsefuncs.py":"MODIFY","sklearn\/random_projection.py":"MODIFY","benchmarks\/bench_plot_randomized_svd.py":"MODIFY","benchmarks\/bench_plot_nmf.py":"MODIFY","sklearn\/utils\/mocking.py":"MODIFY","sklearn\/decomposition\/sparse_pca.py":"MODIFY","sklearn\/mixture\/gaussian_mixture.py":"MODIFY"},"diff":{"sklearn\/covariance\/graph_lasso_.py":[{"add":["326","        super(GraphLasso, self).__init__(assume_centered=assume_centered)","551","        super(GraphLassoCV, self).__init__(","552","            mode=mode, tol=tol, verbose=verbose, enet_tol=enet_tol,","553","            max_iter=max_iter, assume_centered=assume_centered)"],"delete":["332","        self.assume_centered = assume_centered","333","        # The base class needs this for the score method","334","        self.store_precision = True","555","        self.mode = mode","556","        self.tol = tol","557","        self.enet_tol = enet_tol","558","        self.max_iter = max_iter","559","        self.verbose = verbose","562","        self.assume_centered = assume_centered","563","        # The base class needs this for the score method","564","        self.store_precision = True"]}],"sklearn\/tree\/export.py":[{"add":["68","    def __repr__(self):"],"delete":["68","    def __repr__():"]}],"sklearn\/utils\/sparsefuncs.py":[{"add":["304","        inplace_swap_row_csc(X, m, n)","306","        inplace_swap_row_csr(X, m, n)","331","        inplace_swap_row_csr(X, m, n)","333","        inplace_swap_row_csc(X, m, n)"],"delete":["304","        return inplace_swap_row_csc(X, m, n)","306","        return inplace_swap_row_csr(X, m, n)","331","        return inplace_swap_row_csr(X, m, n)","333","        return inplace_swap_row_csc(X, m, n)"]}],"sklearn\/random_projection.py":[{"add":["311","    def _make_random_matrix(self, n_components, n_features):"],"delete":["311","    def _make_random_matrix(n_components, n_features):"]}],"benchmarks\/bench_plot_randomized_svd.py":[{"add":["184","        if l != \"fbpca\":","202","        if l != \"fbpca\":"],"delete":["184","        if l is not \"fbpca\":","202","        if l is not \"fbpca\":"]}],"benchmarks\/bench_plot_nmf.py":[{"add":["205","        super(_PGNMF, self).__init__(","206","            n_components=n_components, init=init, solver=solver, tol=tol,","207","            max_iter=max_iter, random_state=random_state, alpha=alpha,","208","            l1_ratio=l1_ratio)"],"delete":["206","        self.n_components = n_components","207","        self.init = init","208","        self.solver = solver","209","        self.tol = tol","210","        self.max_iter = max_iter","211","        self.random_state = random_state","212","        self.alpha = alpha","213","        self.l1_ratio = l1_ratio"]}],"sklearn\/utils\/mocking.py":[{"add":["38","    def __ne__(self, other):","39","        return not self == other","40",""],"delete":[]}],"sklearn\/decomposition\/sparse_pca.py":[{"add":["259","        super(MiniBatchSparsePCA, self).__init__(","260","            n_components=n_components, alpha=alpha, verbose=verbose,","261","            ridge_alpha=ridge_alpha, n_jobs=n_jobs, method=method,","262","            random_state=random_state)"],"delete":["259","","260","        self.n_components = n_components","261","        self.alpha = alpha","262","        self.ridge_alpha = ridge_alpha","266","        self.verbose = verbose","268","        self.n_jobs = n_jobs","269","        self.method = method","270","        self.random_state = random_state"]}],"sklearn\/mixture\/gaussian_mixture.py":[{"add":["93","    for prec in precisions:"],"delete":["93","    for k, prec in enumerate(precisions):"]}]}},"6f70202ef9beefd3db9bb028755a0c38b4c5c8e7":{"changes":{"sklearn\/ensemble\/voting_classifier.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/ensemble\/tests\/test_voting_classifier.py":"MODIFY"},"diff":{"sklearn\/ensemble\/voting_classifier.py":[{"add":["14","import warnings","64","    flatten_transform : bool, optional (default=None)","65","        Affects shape of transform output only when voting='soft'","66","        If voting='soft' and flatten_transform=True, transform method returns","67","        matrix with shape (n_samples, n_classifiers * n_classes). If","68","        flatten_transform=False, it returns","69","        (n_classifiers, n_samples, n_classes).","70","","104","    ...        voting='soft', weights=[2,1,1],","105","    ...        flatten_transform=True)","109","    >>> print(eclf3.transform(X).shape)","110","    (6, 6)","114","    def __init__(self, estimators, voting='hard', weights=None, n_jobs=1,","115","                 flatten_transform=None):","120","        self.flatten_transform = flatten_transform","178","","272","        If `voting='soft'` and `flatten_transform=True`:","273","          array-like = (n_classifiers, n_samples * n_classes)","274","          otherwise array-like = (n_classifiers, n_samples, n_classes)","281","","283","            probas = self._collect_probas(X)","284","            if self.flatten_transform is None:","285","                warnings.warn(\"'flatten_transform' default value will be \"","286","                              \"changed to True in 0.21.\"","287","                              \"To silence this warning you may\"","288","                              \" explicitly set flatten_transform=False.\",","289","                              DeprecationWarning)","290","                return probas","291","            elif not self.flatten_transform:","292","                return probas","293","            else:","294","                return np.hstack(probas)","295",""],"delete":["96","    ...        voting='soft', weights=[2,1,1])","103","    def __init__(self, estimators, voting='hard', weights=None, n_jobs=1):","258","        If `voting='soft'`:","259","          array-like = [n_classifiers, n_samples, n_classes]","267","            return self._collect_probas(X)"]}],"doc\/whats_new.rst":[{"add":["173","     ","174","   - Added ``flatten_transform`` parameter to :class:`ensemble.VotingClassifier`","175","     to change output shape of `transform` method to 2 dimensional.","176","     :issue:`7794` by :user:`Ibraim Ganiev <olologin>` and","177","     :user:`Herilalaina Rakotoarison <herilalaina>`."],"delete":[]}],"sklearn\/ensemble\/tests\/test_voting_classifier.py":[{"add":["6","from sklearn.utils.testing import assert_warns_message","226","def test_parallel_fit():","367","","368","","369","def test_transform():","370","    \"\"\"Check transform method of VotingClassifier on toy dataset.\"\"\"","371","    clf1 = LogisticRegression(random_state=123)","372","    clf2 = RandomForestClassifier(random_state=123)","373","    clf3 = GaussianNB()","374","    X = np.array([[-1.1, -1.5], [-1.2, -1.4], [-3.4, -2.2], [1.1, 1.2]])","375","    y = np.array([1, 1, 2, 2])","376","","377","    eclf1 = VotingClassifier(estimators=[","378","        ('lr', clf1), ('rf', clf2), ('gnb', clf3)],","379","        voting='soft').fit(X, y)","380","    eclf2 = VotingClassifier(estimators=[","381","        ('lr', clf1), ('rf', clf2), ('gnb', clf3)],","382","        voting='soft',","383","        flatten_transform=True).fit(X, y)","384","    eclf3 = VotingClassifier(estimators=[","385","        ('lr', clf1), ('rf', clf2), ('gnb', clf3)],","386","        voting='soft',","387","        flatten_transform=False).fit(X, y)","388","","389","    warn_msg = (\"'flatten_transform' default value will be \"","390","                \"changed to True in 0.21.\"","391","                \"To silence this warning you may\"","392","                \" explicitly set flatten_transform=False.\")","393","    res = assert_warns_message(DeprecationWarning, warn_msg,","394","                               eclf1.transform, X)","395","    assert_array_equal(res.shape, (3, 4, 2))","396","    assert_array_equal(eclf2.transform(X).shape, (4, 6))","397","    assert_array_equal(eclf3.transform(X).shape, (3, 4, 2))","398","    assert_array_equal(res.swapaxes(0, 1).reshape((4, 6)),","399","                       eclf2.transform(X))","400","    assert_array_equal(eclf3.transform(X).swapaxes(0, 1).reshape((4, 6)),","401","                       eclf2.transform(X))"],"delete":["225","def test_parallel_predict():"]}]}},"cf67fa43d324003b6ff8ab6c40b89aabe6650c2c":{"changes":{"sklearn\/datasets\/covtype.py":"MODIFY","sklearn\/datasets\/rcv1.py":"MODIFY"},"diff":{"sklearn\/datasets\/covtype.py":[{"add":["91","        if not exists(covtype_dir):","92","            makedirs(covtype_dir)"],"delete":["91","        makedirs(covtype_dir, exist_ok=True)"]}],"sklearn\/datasets\/rcv1.py":[{"add":["116","        if not exists(rcv1_dir):","117","            makedirs(rcv1_dir)"],"delete":["116","        makedirs(rcv1_dir, exist_ok=True)"]}]}},"211ded8fbcdde1b82cc3ad8ae5c3babcd0e8643e":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/feature_selection\/from_model.py":"MODIFY","sklearn\/feature_selection\/tests\/test_from_model.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["391","Preprocessing and feature selection","392","","393","- Added select K best features functionality to","394","  :class:`feature_selection.SelectFromModel`.","395","  :issue:`6689` by :user:`Nihar Sheth <nsheth12>` and","396","  :user:`Quazi Rahman <qmaruf>`.","397",""],"delete":[]}],"sklearn\/feature_selection\/from_model.py":[{"add":["4","import numbers","116","    max_features : int or None, optional","117","        The maximum number of features selected scoring above ``threshold``.","118","        To disable ``threshold`` and only select based on ``max_features``,","119","        set ``threshold=-np.inf``.","120","","121","        .. versionadded:: 0.20","122","","133","    def __init__(self, estimator, threshold=None, prefit=False,","134","                 norm_order=1, max_features=None):","139","        self.max_features = max_features","148","            raise ValueError('Either fit the model before transform or set'","149","                             ' \"prefit=True\" while passing the fitted'","150","                             ' estimator to the constructor.')","153","        if self.max_features is not None:","154","            mask = np.zeros_like(scores, dtype=bool)","155","            candidate_indices = \\","156","                np.argsort(-scores, kind='mergesort')[:self.max_features]","157","            mask[candidate_indices] = True","158","        else:","159","            mask = np.ones_like(scores, dtype=bool)","160","        mask[scores < threshold] = False","161","        return mask","181","        if self.max_features is not None:","182","            if not isinstance(self.max_features, numbers.Integral):","183","                raise TypeError(\"'max_features' should be an integer between\"","184","                                \" 0 and {} features. Got {!r} instead.\"","185","                                .format(X.shape[1], self.max_features))","186","            elif self.max_features < 0 or self.max_features > X.shape[1]:","187","                raise ValueError(\"'max_features' should be 0 and {} features.\"","188","                                 \"Got {} instead.\"","189","                                 .format(X.shape[1], self.max_features))","190",""],"delete":["125","    def __init__(self, estimator, threshold=None, prefit=False, norm_order=1):","138","            raise ValueError(","139","                'Either fit SelectFromModel before transform or set \"prefit='","140","                'True\" and pass a fitted estimator to the constructor.')","143","        return scores >= threshold"]}],"sklearn\/feature_selection\/tests\/test_from_model.py":[{"add":["0","import pytest","11","from sklearn.utils.testing import assert_allclose","21","from sklearn.base import BaseEstimator","45","@pytest.mark.parametrize(","46","    \"max_features, err_type, err_msg\",","47","    [(-1, ValueError, \"'max_features' should be 0 and\"),","48","     (data.shape[1] + 1, ValueError, \"'max_features' should be 0 and\"),","49","     ('gobbledigook', TypeError, \"should be an integer\"),","50","     ('all', TypeError, \"should be an integer\")]","51",")","52","def test_max_features_error(max_features, err_type, err_msg):","53","    clf = RandomForestClassifier(n_estimators=50, random_state=0)","54","","55","    transformer = SelectFromModel(estimator=clf,","56","                                  max_features=max_features,","57","                                  threshold=-np.inf)","58","    with pytest.raises(err_type, match=err_msg):","59","        transformer.fit(data, y)","60","","61","","62","@pytest.mark.parametrize(\"max_features\", [0, 2, data.shape[1]])","63","def test_max_features_dim(max_features):","64","    clf = RandomForestClassifier(n_estimators=50, random_state=0)","65","    transformer = SelectFromModel(estimator=clf,","66","                                  max_features=max_features,","67","                                  threshold=-np.inf)","68","    X_trans = transformer.fit_transform(data, y)","69","    assert X_trans.shape[1] == max_features","70","","71","","72","class FixedImportanceEstimator(BaseEstimator):","73","    def __init__(self, importances):","74","        self.importances = importances","75","","76","    def fit(self, X, y=None):","77","        self.feature_importances_ = np.array(self.importances)","78","","79","","80","def test_max_features():","81","    # Test max_features parameter using various values","82","    X, y = datasets.make_classification(","83","        n_samples=1000, n_features=10, n_informative=3, n_redundant=0,","84","        n_repeated=0, shuffle=False, random_state=0)","85","    max_features = X.shape[1]","86","    est = RandomForestClassifier(n_estimators=50, random_state=0)","87","","88","    transformer1 = SelectFromModel(estimator=est,","89","                                   threshold=-np.inf)","90","    transformer2 = SelectFromModel(estimator=est,","91","                                   max_features=max_features,","92","                                   threshold=-np.inf)","93","    X_new1 = transformer1.fit_transform(X, y)","94","    X_new2 = transformer2.fit_transform(X, y)","95","    assert_allclose(X_new1, X_new2)","96","","97","    # Test max_features against actual model.","98","    transformer1 = SelectFromModel(estimator=Lasso(alpha=0.025,","99","                                                   random_state=42))","100","    X_new1 = transformer1.fit_transform(X, y)","101","    scores1 = np.abs(transformer1.estimator_.coef_)","102","    candidate_indices1 = np.argsort(-scores1, kind='mergesort')","103","","104","    for n_features in range(1, X_new1.shape[1] + 1):","105","        transformer2 = SelectFromModel(estimator=Lasso(alpha=0.025,","106","                                       random_state=42),","107","                                       max_features=n_features,","108","                                       threshold=-np.inf)","109","        X_new2 = transformer2.fit_transform(X, y)","110","        scores2 = np.abs(transformer2.estimator_.coef_)","111","        candidate_indices2 = np.argsort(-scores2, kind='mergesort')","112","        assert_allclose(X[:, candidate_indices1[:n_features]],","113","                        X[:, candidate_indices2[:n_features]])","114","    assert_allclose(transformer1.estimator_.coef_,","115","                    transformer2.estimator_.coef_)","116","","117","","118","def test_max_features_tiebreak():","119","    # Test if max_features can break tie among feature importance","120","    X, y = datasets.make_classification(","121","        n_samples=1000, n_features=10, n_informative=3, n_redundant=0,","122","        n_repeated=0, shuffle=False, random_state=0)","123","    max_features = X.shape[1]","124","","125","    feature_importances = np.array([4, 4, 4, 4, 3, 3, 3, 2, 2, 1])","126","    for n_features in range(1, max_features + 1):","127","        transformer = SelectFromModel(","128","            FixedImportanceEstimator(feature_importances),","129","            max_features=n_features,","130","            threshold=-np.inf)","131","        X_new = transformer.fit_transform(X, y)","132","        selected_feature_indices = np.where(transformer._get_support_mask())[0]","133","        assert_array_equal(selected_feature_indices, np.arange(n_features))","134","        assert X_new.shape[1] == n_features","135","","136","","137","def test_threshold_and_max_features():","138","    X, y = datasets.make_classification(","139","        n_samples=1000, n_features=10, n_informative=3, n_redundant=0,","140","        n_repeated=0, shuffle=False, random_state=0)","141","    est = RandomForestClassifier(n_estimators=50, random_state=0)","142","","143","    transformer1 = SelectFromModel(estimator=est, max_features=3,","144","                                   threshold=-np.inf)","145","    X_new1 = transformer1.fit_transform(X, y)","146","","147","    transformer2 = SelectFromModel(estimator=est, threshold=0.04)","148","    X_new2 = transformer2.fit_transform(X, y)","149","","150","    transformer3 = SelectFromModel(estimator=est, max_features=3,","151","                                   threshold=0.04)","152","    X_new3 = transformer3.fit_transform(X, y)","153","    assert X_new3.shape[1] == min(X_new1.shape[1], X_new2.shape[1])","154","    selected_indices = transformer3.transform(","155","        np.arange(X.shape[1])[np.newaxis, :])","156","    assert_allclose(X_new3, X[:, selected_indices[0]])","157","","158","","159","@skip_if_32bit","207","    transformer = SelectFromModel(estimator=Lasso(alpha=0.1,","208","                                  random_state=42))"],"delete":["89","    transformer = SelectFromModel(estimator=Lasso(alpha=0.1))"]}]}},"04d74bb8e3b0aa51eca3f68bb71af7c413b487c6":{"changes":{"sklearn\/ensemble\/partial_dependence.py":"MODIFY"},"diff":{"sklearn\/ensemble\/partial_dependence.py":[{"add":["55","    emp_percentiles = mquantiles(X, prob=percentiles, axis=0)"],"delete":["61","            emp_percentiles = mquantiles(X, prob=percentiles, axis=0)"]}]}},"22f0cf2de9f02f0df37896705cc87c8fb8b58f5e":{"changes":{"sklearn\/feature_extraction\/text.py":"MODIFY"},"diff":{"sklearn\/feature_extraction\/text.py":[{"add":["308","class HashingVectorizer(BaseEstimator, VectorizerMixin, TransformerMixin):"],"delete":["308","class HashingVectorizer(BaseEstimator, VectorizerMixin):","525","    # Alias transform to fit_transform for convenience","526","    fit_transform = transform","527",""]}]}},"511bbc7237012513c19ee9a8c7420eba0f59338c":{"changes":{"examples\/multioutput\/plot_classifier_chain_yeast.py":"MODIFY"},"diff":{"examples\/multioutput\/plot_classifier_chain_yeast.py":[{"add":["7","<http:\/\/mldata.org\/repository\/data\/viewslug\/yeast>`_ dataset which contains","8","2417 datapoints each with 103 features and 14 possible labels. Each","9","data point has at least one label. As a baseline we first train a logistic","10","regression classifier for each of the 14 labels. To evaluate the performance of","11","these classifiers we predict on a held-out test set and calculate the","12",":ref:`jaccard similarity score <jaccard_similarity_score>`.","81","model_names = ('Independent',","92","               'Ensemble')","94","x_pos = np.arange(len(model_names))","100","fig, ax = plt.subplots(figsize=(7, 4))","101","ax.grid(True)","102","ax.set_title('Classifier Chain Ensemble Performance Comparison')","103","ax.set_xticks(x_pos)","104","ax.set_xticklabels(model_names, rotation='vertical')","105","ax.set_ylabel('Jaccard Similarity Score')","106","ax.set_ylim([min(model_scores) * .9, max(model_scores) * 1.1])","108","ax.bar(x_pos, model_scores, alpha=0.5, color=colors)","109","plt.tight_layout()"],"delete":["7","<http:\/\/mldata.org\/repository\/data\/viewslug\/yeast>`_ dataset which","8","contains 2417 datapoints each with 103 features and 14 possible labels. Each","9","datapoint has at least one label. As a baseline we first train a logistic","10","regression classifier for each of the 14 labels. To evaluate the performance","11","of these classifiers we predict on a held-out test set and calculate the","12",":ref:`User Guide <jaccard_similarity_score>`.","81","model_names = ('Independent Models',","92","               'Ensemble Average')","94","y_pos = np.arange(len(model_names))","95","y_pos[1:] += 1","96","y_pos[-1] += 1","102","fig = plt.figure(figsize=(7, 4))","103","plt.title('Classifier Chain Ensemble')","104","plt.xticks(y_pos, model_names, rotation='vertical')","105","plt.ylabel('Jaccard Similarity Score')","106","plt.ylim([min(model_scores) * .9, max(model_scores) * 1.1])","108","plt.bar(y_pos, model_scores, align='center', alpha=0.5, color=colors)"]}]}},"a6753f3ed38d25cec0af8ed95697f48eaacaed24":{"changes":{"doc\/whats_new.rst":"MODIFY","sklearn\/linear_model\/tests\/test_ridge.py":"MODIFY","sklearn\/linear_model\/ridge.py":"MODIFY"},"diff":{"doc\/whats_new.rst":[{"add":["476","   - Fix inconsistent results between :class:`linear_model.RidgeCV`","477","     and :class:`linear_model.Ridge` when using ``normalize=True``","478","     by `Alexandre Gramfort`_.","479",""],"delete":["473",""]}],"sklearn\/linear_model\/tests\/test_ridge.py":[{"add":["385","def _test_ridge_cv_normalize(filter_):","386","    ridge_cv = RidgeCV(normalize=True, cv=3)","387","    ridge_cv.fit(filter_(10. * X_diabetes), y_diabetes)","388","","389","    gs = GridSearchCV(Ridge(normalize=True), cv=3,","390","                      param_grid={'alpha': ridge_cv.alphas})","391","    gs.fit(filter_(10. * X_diabetes), y_diabetes)","392","    assert_equal(gs.best_estimator_.alpha, ridge_cv.alpha_)","393","","394","","474","                      _test_ridge_cv_normalize,"],"delete":[]}],"sklearn\/linear_model\/ridge.py":[{"add":["1121","            gs = GridSearchCV(Ridge(fit_intercept=self.fit_intercept,","1122","                                    normalize=self.normalize),"],"delete":["1121","            gs = GridSearchCV(Ridge(fit_intercept=self.fit_intercept),"]}]}},"aae87002b96622424a16dcad2eaef3a75cc0feda":{"changes":{"sklearn\/utils\/metaestimators.py":"MODIFY","sklearn\/tests\/test_pipeline.py":"MODIFY","sklearn\/pipeline.py":"MODIFY"},"diff":{"sklearn\/utils\/metaestimators.py":[{"add":["53","        new_estimators = list(getattr(self, attr))"],"delete":["53","        new_estimators = getattr(self, attr)[:]"]}],"sklearn\/tests\/test_pipeline.py":[{"add":["210","def test_pipeline_init_tuple():","211","    # Pipeline accepts steps as tuple","212","    X = np.array([[1, 2]])","213","    pipe = Pipeline((('transf', Transf()), ('clf', FitParamT())))","214","    pipe.fit(X, y=None)","215","    pipe.score(X)","216","","217","    pipe.set_params(transf=None)","218","    pipe.fit(X, y=None)","219","    pipe.score(X)","220","","221","","439","    # test that init accepts tuples","440","    fs = FeatureUnion(((\"svd\", svd), (\"select\", select)))","441","    fs.fit(X, y)","442",""],"delete":[]}],"sklearn\/pipeline.py":[{"add":["113","        self.steps = list(steps)","625","        self.transformer_list = list(transformer_list)"],"delete":["19","from .utils import tosequence","114","        self.steps = tosequence(steps)","626","        self.transformer_list = tosequence(transformer_list)"]}]}},"4c61e8b2237a5877b47b22cbb5037430c5635e74":{"changes":{"sklearn\/metrics\/classification.py":"MODIFY","sklearn\/metrics\/tests\/test_classification.py":"MODIFY"},"diff":{"sklearn\/metrics\/classification.py":[{"add":["169","","530","    t_sum = C.sum(axis=1, dtype=np.float64)","531","    p_sum = C.sum(axis=0, dtype=np.float64)","532","    n_correct = np.trace(C, dtype=np.float64)"],"delete":["169","    ","530","    t_sum = C.sum(axis=1)","531","    p_sum = C.sum(axis=0)","532","    n_correct = np.trace(C)"]}],"sklearn\/metrics\/tests\/test_classification.py":[{"add":["485","def test_matthews_corrcoef_overflow():","486","    # https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/9622","487","    rng = np.random.RandomState(20170906)","488","","489","    def mcc_safe(y_true, y_pred):","490","        conf_matrix = confusion_matrix(y_true, y_pred)","491","        true_pos = conf_matrix[1, 1]","492","        false_pos = conf_matrix[1, 0]","493","        false_neg = conf_matrix[0, 1]","494","        n_points = len(y_true)","495","        pos_rate = (true_pos + false_neg) \/ n_points","496","        activity = (true_pos + false_pos) \/ n_points","497","        mcc_numerator = true_pos \/ n_points - pos_rate * activity","498","        mcc_denominator = activity * pos_rate * (1 - activity) * (1 - pos_rate)","499","        return mcc_numerator \/ np.sqrt(mcc_denominator)","500","","501","    def random_ys(n_points):    # binary","502","        x_true = rng.random_sample(n_points)","503","        x_pred = x_true + 0.2 * (rng.random_sample(n_points) - 0.5)","504","        y_true = (x_true > 0.5)","505","        y_pred = (x_pred > 0.5)","506","        return y_true, y_pred","507","","508","    for n_points in [100, 10000, 1000000]:","509","        arr = np.repeat([0., 1.], n_points)  # binary","510","        assert_almost_equal(matthews_corrcoef(arr, arr), 1.0)","511","        arr = np.repeat([0., 1., 2.], n_points)  # multiclass","512","        assert_almost_equal(matthews_corrcoef(arr, arr), 1.0)","513","","514","        y_true, y_pred = random_ys(n_points)","515","        assert_almost_equal(matthews_corrcoef(y_true, y_true), 1.0)","516","        assert_almost_equal(matthews_corrcoef(y_true, y_pred),","517","                            mcc_safe(y_true, y_pred))","518","","519",""],"delete":[]}]}},"7f19dbeb7566944aca64f45d18bc967e50e8a4e5":{"changes":{"benchmarks\/bench_lof.py":"MODIFY","benchmarks\/bench_isolation_forest.py":"MODIFY"},"diff":{"benchmarks\/bench_lof.py":[{"add":["7","Note that LocalOutlierFactor is not meant to predict on a test set and its","8","performance is assessed in an outlier detection context:","9","1. The model is trained on the whole dataset which is assumed to contain","10","outliers.","11","2. The ROC curve is computed on the same dataset using the knowledge of the","12","labels.","13","In this context there is no need to shuffle the dataset because the model","14","is trained and tested on the whole dataset. The randomness of this benchmark","15","is only caused by the random selection of anomalies in the SA dataset.","16","","29","random_state = 2  # to control the random selection of anomalies in SA","32","datasets = ['http', 'smtp', 'SA', 'SF', 'shuttle', 'forestcover']","34","plt.figure()","39","        dataset = fetch_kddcup99(subset=dataset_name, percent10=True,","40","                                 random_state=random_state)","56","        dataset = fetch_covtype()","70","        x1 = lb.fit_transform(X[:, 1].astype(str))","72","        y = (y != b'normal.').astype(int)","76","        x1 = lb.fit_transform(X[:, 1].astype(str))","77","        x2 = lb.fit_transform(X[:, 2].astype(str))","78","        x3 = lb.fit_transform(X[:, 3].astype(str))","80","        y = (y != b'normal.').astype(int)","83","        y = (y != b'normal.').astype(int)","90","    model.fit(X)","92","    scoring = -model.negative_outlier_factor_  # the lower, the more normal","93","    fpr, tpr, thresholds = roc_curve(y, scoring)","96","             label=('ROC for %s (area = %0.3f, train-time: %0.2fs)'","97","                    % (dataset_name, AUC, fit_time)))"],"delete":["16","from sklearn.utils import shuffle as sh","20","np.random.seed(2)","23","datasets = ['shuttle']","25","novelty_detection = True  # if False, training set polluted by outliers","26","","31","        dataset = fetch_kddcup99(subset=dataset_name, shuffle=True,","32","                                 percent10=False)","40","        X, y = sh(X, y)","49","        dataset = fetch_covtype(shuffle=True)","63","        lb.fit(X[:, 1])","64","        x1 = lb.transform(X[:, 1])","66","        y = (y != 'normal.').astype(int)","70","        lb.fit(X[:, 1])","71","        x1 = lb.transform(X[:, 1])","72","        lb.fit(X[:, 2])","73","        x2 = lb.transform(X[:, 2])","74","        lb.fit(X[:, 3])","75","        x3 = lb.transform(X[:, 3])","77","        y = (y != 'normal.').astype(int)","80","        y = (y != 'normal.').astype(int)","81","","82","    n_samples, n_features = np.shape(X)","83","    n_samples_train = n_samples \/\/ 2","84","    n_samples_test = n_samples - n_samples_train","87","    X_train = X[:n_samples_train, :]","88","    X_test = X[n_samples_train:, :]","89","    y_train = y[:n_samples_train]","90","    y_test = y[n_samples_train:]","91","","92","    if novelty_detection:","93","        X_train = X_train[y_train == 0]","94","        y_train = y_train[y_train == 0]","99","    model.fit(X_train)","101","    tstart = time()","102","","103","    scoring = -model.decision_function(X_test)  # the lower, the more normal","104","    predict_time = time() - tstart","105","    fpr, tpr, thresholds = roc_curve(y_test, scoring)","108","             label=('ROC for %s (area = %0.3f, train-time: %0.2fs,'","109","                    'test-time: %0.2fs)' % (dataset_name, AUC, fit_time,","110","                                            predict_time)))"]}],"benchmarks\/bench_isolation_forest.py":[{"add":["5","","6","The benchmark is run as follows:","7","1. The dataset is randomly split into a training set and a test set, both","8","assumed to contain outliers.","9","2. Isolation Forest is trained on the training set.","10","3. The ROC curve is computed on the test set using the knowledge of the labels.","11","","12","Note that the smtp dataset contains a very small proportion of outliers.","13","Therefore, depending on the seed of the random number generator, randomly","14","splitting the data set might lead to a test set containing no outliers. In this","15","case a warning is raised when computing the ROC curve.","25","from sklearn.preprocessing import LabelBinarizer","43","random_state = 1","49","# datasets available = ['http', 'smtp', 'SA', 'SF', 'shuttle', 'forestcover']","50","datasets = ['http', 'smtp', 'SA', 'SF', 'shuttle', 'forestcover']","59","        dataset = fetch_kddcup99(subset=dat, shuffle=True,","60","                                 percent10=True, random_state=random_state)","68","        X, y = sh(X, y, random_state=random_state)","78","        dataset = fetch_covtype(shuffle=True, random_state=random_state)","92","        lb = LabelBinarizer()","93","        x1 = lb.fit_transform(X[:, 1].astype(str))","99","        lb = LabelBinarizer()","100","        x1 = lb.fit_transform(X[:, 1].astype(str))","101","        x2 = lb.fit_transform(X[:, 2].astype(str))","102","        x3 = lb.fit_transform(X[:, 3].astype(str))","121","    model = IsolationForest(n_jobs=-1, random_state=random_state)"],"delete":["14","from sklearn.preprocessing import MultiLabelBinarizer","32","np.random.seed(1)","38","# Removed the shuttle dataset because as of 2017-03-23 mldata.org is down:","39","# datasets = ['http', 'smtp', 'SA', 'SF', 'shuttle', 'forestcover']","40","datasets = ['http', 'smtp', 'SA', 'SF', 'forestcover']","49","        dataset = fetch_kddcup99(subset=dat, shuffle=True, percent10=True)","57","        X, y = sh(X, y)","67","        dataset = fetch_covtype(shuffle=True)","81","        lb = MultiLabelBinarizer()","82","        x1 = lb.fit_transform(X[:, 1])","88","        lb = MultiLabelBinarizer()","89","        x1 = lb.fit_transform(X[:, 1])","90","        x2 = lb.fit_transform(X[:, 2])","91","        x3 = lb.fit_transform(X[:, 3])","110","    model = IsolationForest(n_jobs=-1)"]}]}},"5271a193c3fe682b359477698ae7f87741740463":{"changes":{"sklearn\/linear_model\/ransac.py":"MODIFY","sklearn\/feature_extraction\/hashing.py":"MODIFY","examples\/applications\/plot_topics_extraction_with_nmf_lda.py":"MODIFY","sklearn\/covariance\/graph_lasso_.py":"MODIFY","sklearn\/mixture\/bayesian_mixture.py":"MODIFY","doc\/faq.rst":"MODIFY","sklearn\/covariance\/tests\/test_graph_lasso.py":"MODIFY","sklearn\/model_selection\/_split.py":"MODIFY","sklearn\/linear_model\/stochastic_gradient.py":"MODIFY","sklearn\/model_selection\/_validation.py":"MODIFY","sklearn\/ensemble\/iforest.py":"MODIFY","doc\/tutorial\/statistical_inference\/putting_together.rst":"MODIFY","doc\/tutorial\/statistical_inference\/supervised_learning.rst":"MODIFY","sklearn\/utils\/validation.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/utils\/extmath.py":"MODIFY","sklearn\/manifold\/t_sne.py":"MODIFY","sklearn\/linear_model\/bayes.py":"MODIFY","sklearn\/mixture\/gaussian_mixture.py":"MODIFY","sklearn\/linear_model\/least_angle.py":"MODIFY"},"diff":{"sklearn\/linear_model\/ransac.py":[{"add":["155","        array with the i-th value of the array corresponding to the loss","156","        on ``X[i]``.","158","        If the loss on a sample is greater than the ``residual_threshold``,","159","        then this sample is classified as an outlier."],"delete":["155","        array with the ``i``th value of the array corresponding to the loss","156","        on `X[i]`.","158","        If the loss on a sample is greater than the ``residual_threshold``, then","159","        this sample is classified as an outlier."]}],"sklearn\/feature_extraction\/hashing.py":[{"add":["43","","48","","58","","63","","68","","87","        encoded as columns of integers."],"delete":["82","      encoded as columns of integers."]}],"examples\/applications\/plot_topics_extraction_with_nmf_lda.py":[{"add":["1","=======================================================================================","2","Topic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation","3","======================================================================================="],"delete":["1","========================================================","2","Topic extraction with Non-negative Matrix Factorization\\","3","and Latent Dirichlet Allocation","4","========================================================"]}],"sklearn\/covariance\/graph_lasso_.py":[{"add":["568","                \"will be removed in 0.21. Use ``grid_scores_`` instead\")"],"delete":["568","                \"will be removed in 0.21. Use 'grid_scores_' instead\")"]}],"sklearn\/mixture\/bayesian_mixture.py":[{"add":[],"delete":["79","    *BayesianGaussianMixture*."]}],"doc\/faq.rst":[{"add":["27","issues <easy_issues>`_. Please do not contact the contributors of scikit-learn directly"],"delete":["27","issues <easy_issues>`. Please do not contact the contributors of scikit-learn directly"]}],"sklearn\/covariance\/tests\/test_graph_lasso.py":[{"add":["151","                    \"``grid_scores_`` instead\")"],"delete":["151","                    \"'grid_scores_' instead\")"]}],"sklearn\/model_selection\/_split.py":[{"add":["1824","","1825","        - None, to use the default 3-fold cross-validation,","1826","        - integer, to specify the number of folds.","1827","        - An object to be used as a cross-validation generator.","1828","        - An iterable yielding train\/test splits."],"delete":["1824","          - None, to use the default 3-fold cross-validation,","1825","          - integer, to specify the number of folds.","1826","          - An object to be used as a cross-validation generator.","1827","          - An iterable yielding train\/test splits."]}],"sklearn\/linear_model\/stochastic_gradient.py":[{"add":["355","                \"will be removed in 0.21. Use ``loss_function_`` instead\")"],"delete":["355","                \"will be removed in 0.21. Use 'loss_function_' instead\")"]}],"sklearn\/model_selection\/_validation.py":[{"add":["66","","67","        - None, to use the default 3-fold cross validation,","68","        - integer, to specify the number of folds in a `(Stratified)KFold`,","69","        - An object to be used as a cross-validation generator.","70","        - An iterable yielding train, test splits.","327","","328","        - None, to use the default 3-fold cross validation,","329","        - integer, to specify the number of folds in a `(Stratified)KFold`,","330","        - An object to be used as a cross-validation generator.","331","        - An iterable yielding train, test splits.","564","","565","        - None, to use the default 3-fold cross validation,","566","        - integer, to specify the number of folds in a `(Stratified)KFold`,","567","        - An object to be used as a cross-validation generator.","568","        - An iterable yielding train, test splits.","713","","714","        - None, to use the default 3-fold cross validation,","715","        - integer, to specify the number of folds in a `(Stratified)KFold`,","716","        - An object to be used as a cross-validation generator.","717","        - An iterable yielding train, test splits.","945","","946","        - None, to use the default 3-fold cross validation,","947","        - integer, to specify the number of folds in a `(Stratified)KFold`,","948","        - An object to be used as a cross-validation generator.","949","        - An iterable yielding train, test splits."],"delete":["66","          - None, to use the default 3-fold cross validation,","67","          - integer, to specify the number of folds in a `(Stratified)KFold`,","68","          - An object to be used as a cross-validation generator.","69","          - An iterable yielding train, test splits.","326","          - None, to use the default 3-fold cross validation,","327","          - integer, to specify the number of folds in a `(Stratified)KFold`,","328","          - An object to be used as a cross-validation generator.","329","          - An iterable yielding train, test splits.","562","          - None, to use the default 3-fold cross validation,","563","          - integer, to specify the number of folds in a `(Stratified)KFold`,","564","          - An object to be used as a cross-validation generator.","565","          - An iterable yielding train, test splits.","710","          - None, to use the default 3-fold cross validation,","711","          - integer, to specify the number of folds in a `(Stratified)KFold`,","712","          - An object to be used as a cross-validation generator.","713","          - An iterable yielding train, test splits.","941","          - None, to use the default 3-fold cross validation,","942","          - integer, to specify the number of folds in a `(Stratified)KFold`,","943","          - An object to be used as a cross-validation generator.","944","          - An iterable yielding train, test splits."]}],"sklearn\/ensemble\/iforest.py":[{"add":["59",""],"delete":[]}],"doc\/tutorial\/statistical_inference\/putting_together.rst":[{"add":["34",".. literalinclude:: ..\/..\/auto_examples\/applications\/plot_face_recognition.py"],"delete":["34",".. literalinclude:: ..\/..\/auto_examples\/applications\/face_recognition.py"]}],"doc\/tutorial\/statistical_inference\/supervised_learning.rst":[{"add":["404","   .. literalinclude:: ..\/..\/auto_examples\/exercises\/plot_digits_classification_exercise.py","407","   Solution: :download:`..\/..\/auto_examples\/exercises\/plot_digits_classification_exercise.py`"],"delete":["404","   .. literalinclude:: ..\/..\/auto_examples\/exercises\/digits_classification_exercise.py","407","   Solution: :download:`..\/..\/auto_examples\/exercises\/digits_classification_exercise.py`"]}],"sklearn\/utils\/validation.py":[{"add":["698","        Eg.:","699","            ``[\"coef_\", \"estimator_\", ...], \"coef_\"``"],"delete":["698","        Eg. : [\"coef_\", \"estimator_\", ...], \"coef_\""]}],"doc\/whats_new.rst":[{"add":["69","     :user:`Thierry Guillemot <tguillemot>`, and `Gael Varoquaux`_.","239","     :user:`Nick Dingwall <ndingwall>` and `Gael Varoquaux`_."],"delete":["69","     :user:`Thierry Guillemot <tguillemot>`_, and `Gael Varoquaux`_.","239","     `Nick Dingwall`_ and `Gael Varoquaux`_."]}],"sklearn\/utils\/extmath.py":[{"add":["131","        sparse if ``a`` or ``b`` is sparse and ``dense_output=False``."],"delete":["131","        sparse if ``a`` or ``b`` is sparse and ``dense_output``=False."]}],"sklearn\/manifold\/t_sne.py":[{"add":["796","                \"will be removed in 0.21. Use ``n_iter_`` instead\")"],"delete":["796","                \"will be removed in 0.21. Use 'n_iter_' instead\")"]}],"sklearn\/linear_model\/bayes.py":[{"add":["122","    Their beta is our ``self.alpha_``","123","    Their alpha is our ``self.lambda_``","386","    Their beta is our ``self.alpha_``","387","    Their alpha is our ``self.lambda_``","389","    which ``self.lambda_ < self.threshold_lambda`` are kept and the rest are"],"delete":["122","    Their beta is our self.alpha_","123","    Their alpha is our self.lambda_","386","    Their beta is our self.alpha_","387","    Their alpha is our self.lambda_","389","    which self.lambda_ < self.threshold_lambda are kept and the rest are"]}],"sklearn\/mixture\/gaussian_mixture.py":[{"add":["443","    .. versionadded:: 0.18","444",""],"delete":["441","    .. versionadded:: 0.18","442","    *GaussianMixture*.","443",""]}],"sklearn\/linear_model\/least_angle.py":[{"add":["1163","                \"will be removed in 0.21. See ``alpha_`` instead\")","1169","    @deprecated(\"Attribute ``cv_mse_path_`` is deprecated in 0.18 and \"","1170","                \"will be removed in 0.20. Use ``mse_path_`` instead\")"],"delete":["1163","                \"will be removed in 0.21. See 'alpha_' instead\")","1169","    @deprecated(\"Attribute cv_mse_path_ is deprecated in 0.18 and \"","1170","                \"will be removed in 0.20. Use 'mse_path_' instead\")"]}]}},"f11e4d1c939e549951e6ed34c3b900ebd0d13f64":{"changes":{"sklearn\/neighbors\/tests\/test_approximate.py":"MODIFY"},"diff":{"sklearn\/neighbors\/tests\/test_approximate.py":[{"add":["48","            n_candidates=n_candidates, random_state=0)"],"delete":["48","            n_candidates=n_candidates)"]}]}},"ea9896b8f4b177b508b1136c6fdd710886787f8d":{"changes":{"sklearn\/cluster\/hierarchical.py":"MODIFY","sklearn\/cluster\/tests\/test_hierarchical.py":"MODIFY"},"diff":{"sklearn\/cluster\/hierarchical.py":[{"add":["32","def _fix_connectivity(X, connectivity, affinity):","191","    connectivity, n_components = _fix_connectivity(X, connectivity,","192","                                                   affinity='euclidean')","424","    connectivity, n_components = _fix_connectivity(X, connectivity,","425","                                                   affinity=affinity)"],"delete":["32","def _fix_connectivity(X, connectivity, n_components=None,","33","                      affinity=\"euclidean\"):","192","    connectivity, n_components = _fix_connectivity(X, connectivity)","424","    connectivity, n_components = _fix_connectivity(X, connectivity)"]}],"sklearn\/cluster\/tests\/test_hierarchical.py":[{"add":["534","","535","","536","def test_affinity_passed_to_fix_connectivity():","537","    # Test that the affinity parameter is actually passed to the pairwise","538","    # function","539","","540","    size = 2","541","    rng = np.random.RandomState(0)","542","    X = rng.randn(size, size)","543","    mask = np.array([True, False, False, True])","544","","545","    connectivity = grid_to_graph(n_x=size, n_y=size,","546","                                 mask=mask, return_as=np.ndarray)","547","","548","    class FakeAffinity:","549","        def __init__(self):","550","            self.counter = 0","551","","552","        def increment(self, *args, **kwargs):","553","            self.counter += 1","554","            return self.counter","555","","556","    fa = FakeAffinity()","557","","558","    linkage_tree(X, connectivity=connectivity, affinity=fa.increment)","559","","560","    assert_equal(fa.counter, 3)"],"delete":[]}]}},"ebc873034486eec763fc3a4435fc0066dff52155":{"changes":{"sklearn\/model_selection\/_validation.py":"MODIFY","sklearn\/model_selection\/tests\/test_validation.py":"MODIFY"},"diff":{"sklearn\/model_selection\/_validation.py":[{"add":["646","    Notes","647","    -----","648","    In the case that one or more classes are absent in a training portion, a","649","    default score needs to be assigned to all instances for that class if","650","    ``method`` produces columns per class, as in {'decision_function',","651","    'predict_proba', 'predict_log_proba'}.  For ``predict_proba`` this value is","652","    0.  In order to ensure finite output, we approximate negative infinity by","653","    the minimum finite float value for the dtype in other cases.","654","","757","        if n_classes != len(estimator.classes_):","758","            recommendation = (","759","                'To fix this, use a cross-validation '","760","                'technique resulting in properly '","761","                'stratified folds')","762","            warnings.warn('Number of classes in training fold ({}) does '","763","                          'not match total number of classes ({}). '","764","                          'Results may not be appropriate for your use case. '","765","                          '{}'.format(len(estimator.classes_),","766","                                      n_classes, recommendation),","767","                          RuntimeWarning)","768","            if method == 'decision_function':","769","                if (predictions.ndim == 2 and","770","                        predictions.shape[1] != len(estimator.classes_)):","771","                    # This handles the case when the shape of predictions","772","                    # does not match the number of classes used to train","773","                    # it with. This case is found when sklearn.svm.SVC is","774","                    # set to `decision_function_shape='ovo'`.","775","                    raise ValueError('Output shape {} of {} does not match '","776","                                     'number of classes ({}) in fold. '","777","                                     'Irregular decision_function outputs '","778","                                     'are not currently supported by '","779","                                     'cross_val_predict'.format(","780","                                        predictions.shape, method,","781","                                        len(estimator.classes_),","782","                                        recommendation))","783","                if len(estimator.classes_) <= 2:","784","                    # In this special case, `predictions` contains a 1D array.","785","                    raise ValueError('Only {} class\/es in training fold, this '","786","                                     'is not supported for decision_function '","787","                                     'with imbalanced folds. {}'.format(","788","                                        len(estimator.classes_),","789","                                        recommendation))","790","","791","            float_min = np.finfo(predictions.dtype).min","792","            default_values = {'decision_function': float_min,","793","                              'predict_log_proba': float_min,","794","                              'predict_proba': 0}","795","            predictions_for_all_classes = np.full((_num_samples(predictions),","796","                                                   n_classes),","797","                                                  default_values[method])","798","            predictions_for_all_classes[:, estimator.classes_] = predictions","799","            predictions = predictions_for_all_classes"],"delete":["748","        predictions_ = np.zeros((_num_samples(X_test), n_classes))","749","        if method == 'decision_function' and len(estimator.classes_) == 2:","750","            predictions_[:, estimator.classes_[-1]] = predictions","751","        else:","752","            predictions_[:, estimator.classes_] = predictions","753","        predictions = predictions_"]}],"sklearn\/model_selection\/tests\/test_validation.py":[{"add":["26","from sklearn.utils.testing import assert_warns_message","47","from sklearn.datasets import load_digits","58","from sklearn.linear_model import PassiveAggressiveClassifier, RidgeClassifier","804","    X, y = load_iris(return_X_y=True)","805","","806","    warning_message = ('Number of classes in training fold (2) does '","807","                       'not match total number of classes (3). '","808","                       'Results may not be appropriate for your use case.')","809","    assert_warns_message(RuntimeWarning, warning_message,","810","                         cross_val_predict, LogisticRegression(),","811","                         X, y, method='predict_proba', cv=KFold(2))","812","","813","","814","def test_cross_val_predict_decision_function_shape():","815","    X, y = make_classification(n_classes=2, n_samples=50, random_state=0)","816","","817","    preds = cross_val_predict(LogisticRegression(), X, y,","818","                              method='decision_function')","819","    assert_equal(preds.shape, (50,))","820","","821","    X, y = load_iris(return_X_y=True)","822","","823","    preds = cross_val_predict(LogisticRegression(), X, y,","824","                              method='decision_function')","825","    assert_equal(preds.shape, (150, 3))","826","","827","    # This specifically tests imbalanced splits for binary","828","    # classification with decision_function. This is only","829","    # applicable to classifiers that can be fit on a single","830","    # class.","831","    X = X[:100]","832","    y = y[:100]","833","    assert_raise_message(ValueError,","834","                         'Only 1 class\/es in training fold, this'","835","                         ' is not supported for decision_function'","836","                         ' with imbalanced folds. To fix '","837","                         'this, use a cross-validation technique '","838","                         'resulting in properly stratified folds',","839","                         cross_val_predict, RidgeClassifier(), X, y,","840","                         method='decision_function', cv=KFold(2))","841","","842","    X, y = load_digits(return_X_y=True)","843","    est = SVC(kernel='linear', decision_function_shape='ovo')","844","","845","    preds = cross_val_predict(est,","846","                              X, y,","847","                              method='decision_function')","848","    assert_equal(preds.shape, (1797, 45))","849","","850","    ind = np.argsort(y)","851","    X, y = X[ind], y[ind]","852","    assert_raises_regex(ValueError,","853","                        'Output shape \\(599L?, 21L?\\) of decision_function '","854","                        'does not match number of classes \\(7\\) in fold. '","855","                        'Irregular decision_function .*',","856","                        cross_val_predict, est, X, y,","857","                        cv=KFold(n_splits=3), method='decision_function')","858","","859","","860","def test_cross_val_predict_predict_proba_shape():","861","    X, y = make_classification(n_classes=2, n_samples=50, random_state=0)","862","","863","    preds = cross_val_predict(LogisticRegression(), X, y,","864","                              method='predict_proba')","865","    assert_equal(preds.shape, (50, 2))","866","","867","    X, y = load_iris(return_X_y=True)","868","","869","    preds = cross_val_predict(LogisticRegression(), X, y,","870","                              method='predict_proba')","871","    assert_equal(preds.shape, (150, 3))","872","","873","","874","def test_cross_val_predict_predict_log_proba_shape():","875","    X, y = make_classification(n_classes=2, n_samples=50, random_state=0)","876","","877","    preds = cross_val_predict(LogisticRegression(), X, y,","878","                              method='predict_log_proba')","879","    assert_equal(preds.shape, (50, 2))","880","","881","    X, y = load_iris(return_X_y=True)","882","","883","    preds = cross_val_predict(LogisticRegression(), X, y,","884","                              method='predict_log_proba')","885","    assert_equal(preds.shape, (150, 3))","886","","1328","        if method is 'predict_proba':","1329","            exp_pred_test = np.zeros((len(test), classes))","1331","            exp_pred_test = np.full((len(test), classes),","1332","                                    np.finfo(expected_predictions.dtype).min)","1333","        exp_pred_test[:, est.classes_] = expected_predictions_","1341","    X = np.arange(200).reshape(100, 2)","1342","    y = np.array([x\/\/10 for x in range(100)])","1343","    classes = 10","1371","        y = shuffle(np.repeat(range(10), 10), random_state=0)"],"delete":["56","from sklearn.linear_model import PassiveAggressiveClassifier","1243","        exp_pred_test = np.zeros((len(test), classes))","1244","        if method is 'decision_function' and len(est.classes_) == 2:","1245","            exp_pred_test[:, est.classes_[-1]] = expected_predictions_","1247","            exp_pred_test[:, est.classes_] = expected_predictions_","1255","    X = np.arange(8).reshape(4, 2)","1256","    y = np.array([0, 0, 1, 2])","1257","    classes = 3","1285","        y = [1, 1, -4, 6]"]}]}},"75763cfe7842af6a579bb2f57c5b030d5c529115":{"changes":{"sklearn\/tests\/test_pipeline.py":"MODIFY","sklearn\/base.py":"MODIFY","sklearn\/tests\/test_base.py":"MODIFY"},"diff":{"sklearn\/tests\/test_pipeline.py":[{"add":["26","from sklearn.linear_model import LogisticRegression, Lasso","30","from sklearn.dummy import DummyRegressor","292","                         error_msg % ('fake', pipe),","866","def test_set_params_nested_pipeline():","867","    estimator = Pipeline([","868","        ('a', Pipeline([","869","            ('b', DummyRegressor())","870","        ]))","871","    ])","872","    estimator.set_params(a__b__alpha=0.001, a__b=Lasso())","873","    estimator.set_params(a__steps=[('b', LogisticRegression())], a__b__C=5)","874","","875",""],"delete":["26","from sklearn.linear_model import LogisticRegression","291","                         error_msg % ('fake', 'Pipeline'),"]}],"sklearn\/base.py":[{"add":["7","from collections import defaultdict","251","","252","        nested_params = defaultdict(dict)  # grouped by prefix","253","        for key, value in params.items():","254","            key, delim, sub_key = key.partition('__')","255","            if key not in valid_params:","256","                raise ValueError('Invalid parameter %s for estimator %s. '","257","                                 'Check the list of available parameters '","258","                                 'with `estimator.get_params().keys()`.' %","259","                                 (key, self))","260","","261","            if delim:","262","                nested_params[key][sub_key] = value","265","","266","        for key, sub_params in nested_params.items():","267","            valid_params[key].set_params(**sub_params)","268",""],"delete":["250","        for key, value in six.iteritems(params):","251","            split = key.split('__', 1)","252","            if len(split) > 1:","253","                # nested objects case","254","                name, sub_name = split","255","                if name not in valid_params:","256","                    raise ValueError('Invalid parameter %s for estimator %s. '","257","                                     'Check the list of available parameters '","258","                                     'with `estimator.get_params().keys()`.' %","259","                                     (name, self))","260","                sub_object = valid_params[name]","261","                sub_object.set_params(**{sub_name: value})","263","                # simple objects case","264","                if key not in valid_params:","265","                    raise ValueError('Invalid parameter %s for estimator %s. '","266","                                     'Check the list of available parameters '","267","                                     'with `estimator.get_params().keys()`.' %","268","                                     (key, self.__class__.__name__))"]}],"sklearn\/tests\/test_base.py":[{"add":["230","def test_set_params_passes_all_parameters():","231","    # Make sure all parameters are passed together to set_params","232","    # of nested estimator. Regression test for #9944","233","","234","    class TestDecisionTree(DecisionTreeClassifier):","235","        def set_params(self, **kwargs):","236","            super(TestDecisionTree, self).set_params(**kwargs)","237","            # expected_kwargs is in test scope","238","            assert kwargs == expected_kwargs","239","            return self","240","","241","    expected_kwargs = {'max_depth': 5, 'min_samples_leaf': 2}","242","    for est in [Pipeline([('estimator', TestDecisionTree())]),","243","                GridSearchCV(TestDecisionTree(), {})]:","244","        est.set_params(estimator__max_depth=5,","245","                       estimator__min_samples_leaf=2)","246","","247",""],"delete":[]}]}},"66196a6b8c56dbf6b3fb654be1c1bc2c9790937b":{"changes":{"doc\/whats_new.rst":"MODIFY"},"diff":{"doc\/whats_new.rst":[{"add":["197","","198","- Added ``flatten_transform`` parameter to :class:`ensemble.VotingClassifier`","199","  to change output shape of `transform` method to 2 dimensional.","200","  :issue:`7794` by :user:`Ibraim Ganiev <olologin>` and","201","  :user:`Herilalaina Rakotoarison <herilalaina>`.","263","- Fixed the implementation of noise_variance_ in :class:`decomposition.PCA`.","264","  :issue:`9108` by `Hanmin Qin <https:\/\/github.com\/qinhanmin2014>`_.","265","","524","  :issue:`9105` by `Hanmin Qin <https:\/\/github.com\/qinhanmin2014>`_.","628","- Cross validation now works with Pandas datatypes that that have a","629","  read-only index. :issue:`9507` by `Loic Esteve`_.","630","","879","Code and Documentation Contributors","880","-----------------------------------","881","","882","Thanks to everyone who has contributed to the maintenance and improvement of the","883","project since version 0.18, including:","884","","885","Joel Nothman, Loic Esteve, Andreas Mueller, Guillaume Lemaitre, Olivier Grisel,","886","Hanmin Qin, Raghav RV, Alexandre Gramfort, themrmax, Aman Dalmia, Gael","887","Varoquaux, Naoya Kanai, Tom Dupr la Tour, Rishikesh, Nelson Liu, Taehoon Lee,","888","Nelle Varoquaux, Aashil, Mikhail Korobov, Sebastin Santy, Joan Massich, Roman","889","Yurchak, RAKOTOARISON Herilalaina, Thierry Guillemot, Alexandre Abadie, Carol","890","Willing, Balakumaran Manoharan, Josh Karnofsky, Vlad Niculae, Utkarsh Upadhyay,","891","Dmitry Petrov, Minghui Liu, Srivatsan, Vincent Pham, Albert Thomas, Jake","892","VanderPlas, Attractadore, JC Liu, alexandercbooth, chkoar, ?scar Njera,","893","Aarshay Jain, Kyle Gilliam, Ramana Subramanyam, CJ Carey, Clement Joudet, David","894","Robles, He Chen, Joris Van den Bossche, Karan Desai, Katie Luangkote, Leland","895","McInnes, Maniteja Nandana, Michele Lacchia, Sergei Lebedev, Shubham Bhardwaj,","896","akshay0724, omtcyfz, rickiepark, waterponey, Vathsala Achar, jbDelafosse, Ralf","897","Gommers, Ekaterina Krivich, Vivek Kumar, Ishank Gulati, Dave Elliott, ldirer,","898","Reiichiro Nakano, Levi John Wolf, Mathieu Blondel, Sid Kapur, Dougal J.","899","Sutherland, midinas, mikebenfield, Sourav Singh, Aseem Bansal, Ibraim Ganiev,","900","Stephen Hoover, AishwaryaRK, Steven C. Howell, Gary Foreman, Neeraj Gangwar,","901","Tahar, Jon Crall, dokato, Kathy Chen, ferria, Thomas Moreau, Charlie Brummitt,","902","Nicolas Goix, Adam Kleczewski, Sam Shleifer, Nikita Singh, Basil Beirouti,","903","Giorgio Patrini, Manoj Kumar, Rafael Possas, James Bourbeau, James A. Bednar,","904","Janine Harper, Jaye, Jean Helie, Jeremy Steward, Artsiom, John Wei, Jonathan","905","LIgo, Jonathan Rahn, seanpwilliams, Arthur Mensch, Josh Levy, Julian Kuhlmann,","906","Julien Aubert, J?rn Hees, Kai, shivamgargsya, Kat Hempstalk, Kaushik","907","Lakshmikanth, Kennedy, Kenneth Lyons, Kenneth Myers, Kevin Yap, Kirill Bobyrev,","908","Konstantin Podshumok, Arthur Imbert, Lee Murray, toastedcornflakes, Lera, Li","909","Li, Arthur Douillard, Mainak Jas, tobycheese, Manraj Singh, Manvendra Singh,","910","Marc Meketon, MarcoFalke, Matthew Brett, Matthias Gilch, Mehul Ahuja, Melanie","911","Goetz, Meng, Peng, Michael Dezube, Michal Baumgartner, vibrantabhi19, Artem","912","Golubin, Milen Paskov, Antonin Carette, Morikko, MrMjauh, NALEPA Emmanuel,","913","Namiya, Antoine Wendlinger, Narine Kokhlikyan, NarineK, Nate Guerin, Angus","914","Williams, Ang Lu, Nicole Vavrova, Nitish Pandey, Okhlopkov Daniil Olegovich,","915","Andy Craze, Om Prakash, Parminder Singh, Patrick Carlson, Patrick Pei, Paul","916","Ganssle, Paulo Haddad, Pawe? Lorek, Peng Yu, Pete Bachant, Peter Bull, Peter","917","Csizsek, Peter Wang, Pieter Arthur de Jong, Ping-Yao, Chang, Preston Parry,","918","Puneet Mathur, Quentin Hibon, Andrew Smith, Andrew Jackson, 1kastner, Rameshwar","919","Bhaskaran, Rebecca Bilbro, Remi Rampin, Andrea Esuli, Rob Hall, Robert","920","Bradshaw, Romain Brault, Aman Pratik, Ruifeng Zheng, Russell Smith, Sachin","921","Agarwal, Sailesh Choyal, Samson Tan, Samu?l Weber, Sarah Brown, Sebastian","922","P?lsterl, Sebastian Raschka, Sebastian Saeger, Alyssa Batula, Abhyuday Pratap","923","Singh, Sergey Feldman, Sergul Aydore, Sharan Yalburgi, willduan, Siddharth","924","Gupta, Sri Krishna, Almer, Stijn Tonk, Allen Riddell, Theofilos Papapanagiotou,","925","Alison, Alexis Mignon, Tommy Boucher, Tommy L?fstedt, Toshihiro Kamishima,","926","Tyler Folkman, Tyler Lanigan, Alexander Junge, Varun Shenoy, Victor Poughon,","927","Vilhelm von Ehrenheim, Aleksandr Sandrovskii, Alan Yee, Vlasios Vasileiou,","928","Warut Vijitbenjaronk, Yang Zhang, Yaroslav Halchenko, Yichuan Liu, Yuichi","929","Fujikawa, affanv14, aivision2020, xor, andreh7, brady salz, campustrampus,","930","Agamemnon Krasoulis, ditenberg, elena-sharova, filipj8, fukatani, gedeck,","931","guiniol, guoci, hakaa1, hongkahjun, i-am-xhy, jakirkham, jaroslaw-weber,","932","jayzed82, jeroko, jmontoyam, jonathan.striebel, josephsalmon, jschendel,","933","leereeves, martin-hahn, mathurinm, mehak-sachdeva, mlewis1729, mlliou112,","934","mthorrell, ndingwall, nuffe, yangarbiter, plagree, pldtc325, Breno Freitas,","935","Brett Olsen, Brian A. Alfano, Brian Burns, polmauri, Brandon Carter, Charlton","936","Austin, Chayant T15h, Chinmaya Pancholi, Christian Danielsen, Chung Yen,","937","Chyi-Kwei Yau, pravarmahajan, DOHMATOB Elvis, Daniel LeJeune, Daniel Hnyk,","938","Darius Morawiec, David DeTomaso, David Gasquez, David Haberthr, David","939","Heryanto, David Kirkby, David Nicholson, rashchedrin, Deborah Gertrude Digges,","940","Denis Engemann, Devansh D, Dickson, Bob Baxley, Don86, E. Lynch-Klarup, Ed","941","Rogers, Elizabeth Ferriss, Ellen-Co2, Fabian Egli, Fang-Chieh Chou, Bing Tian","942","Dai, Greg Stupp, Grzegorz Szpak, Bertrand Thirion, Hadrien Bertrand, Harizo","943","Rajaona, zxcvbnius, Henry Lin, Holger Peters, Icyblade Dai, Igor","944","Andriushchenko, Ilya, Isaac Laughlin, Ivn Valls, Aurlien Bellet, JPFrancoia,","945","Jacob Schreiber, Asish Mahapatra","946",""],"delete":["197","   - :func:`tree.export_graphviz` now shows configurable number of decimal","198","     places. :issue:`8698` by :user:`Guillaume Lemaitre <glemaitre>`.","199","     ","200","   - Added ``flatten_transform`` parameter to :class:`ensemble.VotingClassifier`","201","     to change output shape of `transform` method to 2 dimensional.","202","     :issue:`7794` by :user:`Ibraim Ganiev <olologin>` and","203","     :user:`Herilalaina Rakotoarison <herilalaina>`.","348","   - :class:`multioutput.MultiOutputRegressor` and :class:`multioutput.MultiOutputClassifier`","349","     now support online learning using ``partial_fit``.","350","     :issue:`8053` by :user:`Peng Yu <yupbank>`.","526","  :issue:`9105` by `Hanmin Qin <https:\/\/github.com\/qinhanmin2014>`_. "]}]}},"e674f64458242bdfc28a01a95ffa69ed96056027":{"changes":{"sklearn\/multioutput.py":"MODIFY","sklearn\/tests\/test_multioutput.py":"MODIFY"},"diff":{"sklearn\/multioutput.py":[{"add":["370","class ClassifierChain(BaseEstimator, ClassifierMixin, MetaEstimatorMixin):"],"delete":["370","class ClassifierChain(BaseEstimator):"]}],"sklearn\/tests\/test_multioutput.py":[{"add":["31","from sklearn.base import ClassifierMixin","383","    assert isinstance(classifier_chain, ClassifierMixin)","384",""],"delete":[]}]}},"8472350e5c2fc0a13f7900214adcfc1a69b58735":{"changes":{"sklearn\/preprocessing\/tests\/test_function_transformer.py":"MODIFY","doc\/whats_new\/v0.20.rst":"MODIFY","doc\/modules\/preprocessing.rst":"MODIFY","sklearn\/preprocessing\/_function_transformer.py":"MODIFY"},"diff":{"sklearn\/preprocessing\/tests\/test_function_transformer.py":[{"add":["1","from scipy import sparse","4","from sklearn.utils.testing import (assert_equal, assert_array_equal,","5","                                   assert_allclose_dense_sparse)","6","from sklearn.utils.testing import assert_warns_message, assert_no_warnings","130","","131","","132","def test_check_inverse():","133","    X_dense = np.array([1, 4, 9, 16], dtype=np.float64).reshape((2, 2))","134","","135","    X_list = [X_dense,","136","              sparse.csr_matrix(X_dense),","137","              sparse.csc_matrix(X_dense)]","138","","139","    for X in X_list:","140","        if sparse.issparse(X):","141","            accept_sparse = True","142","        else:","143","            accept_sparse = False","144","        trans = FunctionTransformer(func=np.sqrt,","145","                                    inverse_func=np.around,","146","                                    accept_sparse=accept_sparse,","147","                                    check_inverse=True)","148","        assert_warns_message(UserWarning,","149","                             \"The provided functions are not strictly\"","150","                             \" inverse of each other. If you are sure you\"","151","                             \" want to proceed regardless, set\"","152","                             \" 'check_inverse=False'.\",","153","                             trans.fit, X)","154","","155","        trans = FunctionTransformer(func=np.expm1,","156","                                    inverse_func=np.log1p,","157","                                    accept_sparse=accept_sparse,","158","                                    check_inverse=True)","159","        Xt = assert_no_warnings(trans.fit_transform, X)","160","        assert_allclose_dense_sparse(X, trans.inverse_transform(Xt))","161","","162","    # check that we don't check inverse when one of the func or inverse is not","163","    # provided.","164","    trans = FunctionTransformer(func=np.expm1, inverse_func=None,","165","                                check_inverse=True)","166","    assert_no_warnings(trans.fit, X_dense)","167","    trans = FunctionTransformer(func=None, inverse_func=np.expm1,","168","                                check_inverse=True)","169","    assert_no_warnings(trans.fit, X_dense)"],"delete":["3","from sklearn.utils.testing import assert_equal, assert_array_equal","4","from sklearn.utils.testing import assert_warns_message"]}],"doc\/whats_new\/v0.20.rst":[{"add":["42","","67","- A parameter ``check_inverse`` was added to :class:`FunctionTransformer`","68","  to ensure that ``func`` and ``inverse_func`` are the inverse of each","69","  other.","70","  :issue:`9399` by :user:`Guillaume Lemaitre <glemaitre>`.","71",""],"delete":["42","  "]}],"doc\/modules\/preprocessing.rst":[{"add":["612","You can ensure that ``func`` and ``inverse_func`` are the inverse of each other","613","by setting ``check_inverse=True`` and calling ``fit`` before","614","``transform``. Please note that a warning is raised and can be turned into an","615","error with a ``filterwarnings``::","616","","617","  >>> import warnings","618","  >>> warnings.filterwarnings(\"error\", message=\".*check_inverse*.\",","619","  ...                         category=UserWarning, append=False)","620",""],"delete":[]}],"sklearn\/preprocessing\/_function_transformer.py":[{"add":["4","from ..utils.testing import assert_allclose_dense_sparse","60","    check_inverse : bool, default=True","61","       Whether to check that or ``func`` followed by ``inverse_func`` leads to","62","       the original inputs. It can be used for a sanity check, raising a","63","       warning when the condition is not fulfilled.","64","","65","       .. versionadded:: 0.20","66","","75","                 accept_sparse=False, pass_y='deprecated', check_inverse=True,","82","        self.check_inverse = check_inverse","86","    def _check_inverse_transform(self, X):","87","        \"\"\"Check that func and inverse_func are the inverse.\"\"\"","88","        idx_selected = slice(None, None, max(1, X.shape[0] \/\/ 100))","89","        try:","90","            assert_allclose_dense_sparse(","91","                X[idx_selected],","92","                self.inverse_transform(self.transform(X[idx_selected])))","93","        except AssertionError:","94","            warnings.warn(\"The provided functions are not strictly\"","95","                          \" inverse of each other. If you are sure you\"","96","                          \" want to proceed regardless, set\"","97","                          \" 'check_inverse=False'.\", UserWarning)","98","","114","            X = check_array(X, self.accept_sparse)","115","        if (self.check_inverse and not (self.func is None or","116","                                        self.inverse_func is None)):","117","            self._check_inverse_transform(X)"],"delete":["21","    A FunctionTransformer will not do any checks on its function's output.","22","","69","                 accept_sparse=False, pass_y='deprecated',","94","            check_array(X, self.accept_sparse)"]}]}},"0dc22798e72cca233e62c0fa37408fed5d4d2394":{"changes":{"doc\/modules\/label_propagation.rst":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/semi_supervised\/tests\/test_label_propagation.py":"MODIFY","sklearn\/semi_supervised\/label_propagation.py":"MODIFY","examples\/semi_supervised\/plot_label_propagation_structure.py":"MODIFY"},"diff":{"doc\/modules\/label_propagation.rst":[{"add":["54","clamping of input labels, which means :math:`\\alpha=0`. This clamping factor","55","can be relaxed, to say :math:`\\alpha=0.2`, which means that we will always"],"delete":["54","clamping of input labels, which means :math:`\\alpha=1`. This clamping factor","55","can be relaxed, to say :math:`\\alpha=0.8`, which means that we will always"]}],"doc\/whats_new.rst":[{"add":["450","     :issue:`9105` by `Hanmin Qin <https:\/\/github.com\/qinhanmin2014>`_.","451","","452","   - Fix :class:`semi_supervised.BaseLabelPropagation` to correctly implement","453","     ``LabelPropagation`` and ``LabelSpreading`` as done in the referenced","454","     papers. :class:`semi_supervised.LabelPropagation` now always does hard","455","     clamping. Its ``alpha`` parameter has no effect and is","456","     deprecated to be removed in 0.21. :issue:`6727` :issue:`3550` issue:`5770`","457","     by :user:`Andre Ambrosio Boechat <boechat107>`, :user:`Utkarsh Upadhyay","458","     <musically-ut>`, and `Joel Nothman`_.","459",""],"delete":["450","     :issue:`9105` by `Hanmin Qin <https:\/\/github.com\/qinhanmin2014>`_. "]}],"sklearn\/semi_supervised\/tests\/test_label_propagation.py":[{"add":["5","from sklearn.utils.testing import assert_warns","6","from sklearn.utils.testing import assert_raises","7","from sklearn.utils.testing import assert_no_warnings","10","from sklearn.datasets import make_classification","65","","66","","67","def test_alpha_deprecation():","68","    X, y = make_classification(n_samples=100)","69","    y[::3] = -1","70","","71","    lp_default = label_propagation.LabelPropagation(kernel='rbf', gamma=0.1)","72","    lp_default_y = assert_no_warnings(lp_default.fit, X, y).transduction_","73","","74","    lp_0 = label_propagation.LabelPropagation(alpha=0, kernel='rbf', gamma=0.1)","75","    lp_0_y = assert_warns(DeprecationWarning, lp_0.fit, X, y).transduction_","76","","77","    assert_array_equal(lp_default_y, lp_0_y)","78","","79","","80","def test_label_spreading_closed_form():","81","    n_classes = 2","82","    X, y = make_classification(n_classes=n_classes, n_samples=200,","83","                               random_state=0)","84","    y[::3] = -1","85","    clf = label_propagation.LabelSpreading().fit(X, y)","86","    # adopting notation from Zhou et al (2004):","87","    S = clf._build_graph()","88","    Y = np.zeros((len(y), n_classes + 1))","89","    Y[np.arange(len(y)), y] = 1","90","    Y = Y[:, :-1]","91","    for alpha in [0.1, 0.3, 0.5, 0.7, 0.9]:","92","        expected = np.dot(np.linalg.inv(np.eye(len(S)) - alpha * S), Y)","93","        expected \/= expected.sum(axis=1)[:, np.newaxis]","94","        clf = label_propagation.LabelSpreading(max_iter=10000, alpha=alpha)","95","        clf.fit(X, y)","96","        assert_array_almost_equal(expected, clf.label_distributions_, 4)","97","","98","","99","def test_label_propagation_closed_form():","100","    n_classes = 2","101","    X, y = make_classification(n_classes=n_classes, n_samples=200,","102","                               random_state=0)","103","    y[::3] = -1","104","    Y = np.zeros((len(y), n_classes + 1))","105","    Y[np.arange(len(y)), y] = 1","106","    unlabelled_idx = Y[:, (-1,)].nonzero()[0]","107","    labelled_idx = (Y[:, (-1,)] == 0).nonzero()[0]","108","","109","    clf = label_propagation.LabelPropagation(max_iter=10000,","110","                                             gamma=0.1).fit(X, y)","111","    # adopting notation from Zhu et al 2002","112","    T_bar = clf._build_graph()","113","    Tuu = T_bar[np.meshgrid(unlabelled_idx, unlabelled_idx, indexing='ij')]","114","    Tul = T_bar[np.meshgrid(unlabelled_idx, labelled_idx, indexing='ij')]","115","    Y = Y[:, :-1]","116","    Y_l = Y[labelled_idx, :]","117","    Y_u = np.dot(np.dot(np.linalg.inv(np.eye(Tuu.shape[0]) - Tuu), Tul), Y_l)","118","","119","    expected = Y.copy()","120","    expected[unlabelled_idx, :] = Y_u","121","    expected \/= expected.sum(axis=1)[:, np.newaxis]","122","","123","    assert_array_almost_equal(expected, clf.label_distributions_, 4)","124","","125","","126","def test_valid_alpha():","127","    n_classes = 2","128","    X, y = make_classification(n_classes=n_classes, n_samples=200,","129","                               random_state=0)","130","    for alpha in [-0.1, 0, 1, 1.1, None]:","131","        assert_raises(ValueError,","132","                      lambda **kwargs:","133","                      label_propagation.LabelSpreading(**kwargs).fit(X, y),","134","                      alpha=alpha)","135","","136","","137","def test_convergence_speed():","138","    # This is a non-regression test for #5774","139","    X = np.array([[1., 0.], [0., 1.], [1., 2.5]])","140","    y = np.array([0, 1, -1])","141","    mdl = label_propagation.LabelSpreading(kernel='rbf', max_iter=5000)","142","    mdl.fit(X, y)","143","","144","    # this should converge quickly:","145","    assert mdl.n_iter_ < 10","146","    assert_array_equal(mdl.predict(X), [0, 1, 1])"],"delete":[]}],"sklearn\/semi_supervised\/label_propagation.py":[{"add":["16","  The algorithm tries to learn distributions of labels over the dataset given","17","  label assignments over an initial subset. In one variant, the algorithm does","18","  not allow for any errors in the initial assignment (hard-clamping) while","19","  in another variant, the algorithm allows for some wiggle room for the initial","20","  assignments, allowing them to change by a fraction alpha in each iteration","21","  (soft-clamping).","58","import warnings","243","        alpha = self.alpha","244","        if self._variant == 'spreading' and \\","245","                (alpha is None or alpha <= 0.0 or alpha >= 1.0):","246","            raise ValueError('alpha=%s is invalid: it must be inside '","247","                             'the open interval (0, 1)' % alpha)","257","        if self._variant == 'propagation':","258","            # LabelPropagation","259","            y_static[unlabeled] = 0","260","        else:","261","            # LabelSpreading","262","            y_static *= 1 - alpha","267","        unlabeled = unlabeled[:, np.newaxis]","275","","276","            if self._variant == 'propagation':","277","                normalizer = np.sum(","278","                    self.label_distributions_, axis=1)[:, np.newaxis]","279","                self.label_distributions_ \/= normalizer","280","                self.label_distributions_ = np.where(unlabeled,","281","                                                     self.label_distributions_,","282","                                                     y_static)","283","            else:","284","                # clamp","285","                self.label_distributions_ = np.multiply(","286","                    alpha, self.label_distributions_) + y_static","291","","320","        Clamping factor.","321","","322","        .. deprecated:: 0.19","323","            This parameter will be removed in 0.21.","324","            'alpha' is fixed to zero in 'LabelPropagation'.","375","    _variant = 'propagation'","376","","377","    def __init__(self, kernel='rbf', gamma=20, n_neighbors=7,","378","                 alpha=None, max_iter=30, tol=1e-3, n_jobs=1):","379","        super(LabelPropagation, self).__init__(","380","            kernel=kernel, gamma=gamma, n_neighbors=n_neighbors, alpha=alpha,","381","            max_iter=max_iter, tol=tol, n_jobs=n_jobs)","382","","399","    def fit(self, X, y):","400","        if self.alpha is not None:","401","            warnings.warn(","402","                \"alpha is deprecated since 0.19 and will be removed in 0.21.\",","403","                DeprecationWarning","404","            )","405","            self.alpha = None","406","        return super(LabelPropagation, self).fit(X, y)","407","","433","      Clamping factor. A value in [0, 1] that specifies the relative amount","434","      that an instance should adopt the information from its neighbors as","435","      opposed to its initial label.","436","      alpha=0 means keeping the initial label information; alpha=1 means","437","      replacing all initial information.","492","    _variant = 'spreading'","493",""],"delete":["16","  The algorithm tries to learn distributions of labels over the dataset. In the","17","  \"Hard Clamp\" mode, the true ground labels are never allowed to change. They","18","  are clamped into position. In the \"Soft Clamp\" mode, they are allowed some","19","  wiggle room, but some alpha of their original value will always be retained.","20","  Hard clamp is the same as soft clamping with alpha set to 1.","243","        clamp_weights = np.ones((n_samples, 1))","244","        clamp_weights[unlabeled, 0] = self.alpha","252","        if self.alpha > 0.:","253","            y_static *= 1 - self.alpha","254","        y_static[unlabeled] = 0","266","            # clamp","267","            self.label_distributions_ = np.multiply(","268","                clamp_weights, self.label_distributions_) + y_static","301","        Clamping factor","393","      clamping factor"]}],"examples\/semi_supervised\/plot_label_propagation_structure.py":[{"add":["32","label_spread = label_propagation.LabelSpreading(kernel='knn', alpha=0.2)"],"delete":["32","label_spread = label_propagation.LabelSpreading(kernel='knn', alpha=1.0)"]}]}},"4dafa52eb06a40391bbc78b79887b47b19bcd100":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/linear_model\/tests\/test_logistic.py":"MODIFY","sklearn\/linear_model\/logistic.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["182","- Fixed a bug in :class:`linear_model.LogisticRegression` where when using the","183","  parameter ``multi_class='multinomial'``, the ``predict_proba`` method was","184","  returning incorrect probabilities in the case of binary outcomes.","185","  :issue:`9939` by :user:`Roger Westover <rwolst>`.","186",""],"delete":[]}],"sklearn\/linear_model\/tests\/test_logistic.py":[{"add":["200","def test_multinomial_binary_probabilities():","201","    # Test multinomial LR gives expected probabilities based on the","202","    # decision function, for a binary problem.","203","    X, y = make_classification()","204","    clf = LogisticRegression(multi_class='multinomial', solver='saga')","205","    clf.fit(X, y)","206","","207","    decision = clf.decision_function(X)","208","    proba = clf.predict_proba(X)","209","","210","    expected_proba_class_1 = (np.exp(decision) \/","211","                              (np.exp(decision) + np.exp(-decision)))","212","    expected_proba = np.c_[1-expected_proba_class_1, expected_proba_class_1]","213","","214","    assert_almost_equal(proba, expected_proba)","215","","216",""],"delete":[]}],"sklearn\/linear_model\/logistic.py":[{"add":["1103","        `coef_` is of shape (1, n_features) when the given problem is binary.","1104","        In particular, when `multi_class='multinomial'`, `coef_` corresponds","1105","        to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False).","1111","        `intercept_` is of shape (1,) when the given problem is binary.","1112","        In particular, when `multi_class='multinomial'`, `intercept_`","1113","        corresponds to outcome 1 (True) and `-intercept_` corresponds to","1114","        outcome 0 (False).","1338","        if self.multi_class == \"ovr\":","1341","            decision = self.decision_function(X)","1342","            if decision.ndim == 1:","1343","                # Workaround for multi_class=\"multinomial\" and binary outcomes","1344","                # which requires softmax prediction with only a 1D decision.","1345","                decision_2d = np.c_[-decision, decision]","1346","            else:","1347","                decision_2d = decision","1348","            return softmax(decision_2d, copy=False)"],"delete":["1103","        `coef_` is of shape (1, n_features) when the given problem","1104","        is binary.","1110","        `intercept_` is of shape(1,) when the problem is binary.","1334","        calculate_ovr = self.coef_.shape[0] == 1 or self.multi_class == \"ovr\"","1335","        if calculate_ovr:","1338","            return softmax(self.decision_function(X), copy=False)"]}]}},"e1fb03c86d2a2c47ef008ead958e1bc10fb06e77":{"changes":{"sklearn\/linear_model\/tests\/test_logistic.py":"MODIFY","sklearn\/linear_model\/logistic.py":"MODIFY"},"diff":{"sklearn\/linear_model\/tests\/test_logistic.py":[{"add":["77","    msg = \"is not a valid scoring value\"","78","    assert_raise_message(ValueError, msg,","79","                         LogisticRegressionCV(scoring='bad-scorer', cv=2).fit,","80","                         X, Y1)","81",""],"delete":[]}],"sklearn\/linear_model\/logistic.py":[{"add":["36","from ..metrics import get_scorer","943","        scoring = get_scorer(scoring)"],"delete":["36","from ..metrics import SCORERS","943","        scoring = SCORERS[scoring]"]}]}},"c71e1275ea78a113d26843beeb2cfb21749fe06f":{"changes":{"doc\/modules\/multiclass.rst":"MODIFY","sklearn\/multioutput.py":"MODIFY","doc\/whats_new.rst":"MODIFY","doc\/modules\/model_evaluation.rst":"MODIFY"},"diff":{"doc\/modules\/multiclass.rst":[{"add":["355","of exploiting correlations among targets.","375","","377","        \"Classifier Chains for Multi-label Classification\", 2009."],"delete":["355"," of exploiting correlations among targets.","376","        \"Classifier Chains for Multi-label Classification\", 2009."]}],"sklearn\/multioutput.py":[{"add":["309","    estimators_ : list of ``n_output`` estimators","420","        A list of arrays of length ``len(estimators_)`` containing the","456","        X, Y = check_X_y(X, Y, multi_output=True, accept_sparse=True)"],"delete":["16","from abc import ABCMeta","17","","311","    estimators_ : list of `n_output` estimators","422","        A list of arrays of length len(estimators_) containing the","458","        X, Y = check_X_y(X, Y,  multi_output=True, accept_sparse=True)"]}],"doc\/whats_new.rst":[{"add":["4352","   - Randomized sparse linear models for feature","4813","     example gallery by `Fabian Pedregosa`_."],"delete":["4352","   - :ref:`randomized_l1`: Randomized sparse linear models for feature","4813","     :ref:`example gallery <examples-index>` by `Fabian Pedregosa`_."]}],"doc\/modules\/model_evaluation.rst":[{"add":[],"delete":["672","  * See :ref:`sphx_glr_auto_examples_linear_model_plot_sparse_recovery.py`","673","    for an example of :func:`precision_recall_curve` usage to select","674","    features for sparse linear models.","675",""]}]}},"7b7960ef870ddb3633fcdf2a531c458f081e1e00":{"changes":{"sklearn\/model_selection\/_split.py":"MODIFY"},"diff":{"sklearn\/model_selection\/_split.py":[{"add":["588","            raise ValueError(\"n_splits=%d cannot be greater than the\"","589","                             \" number of members in each class.\"","594","                           \" number of members in any class cannot\""],"delete":["588","            raise ValueError(\"All the n_groups for individual classes\"","589","                             \" are less than n_splits=%d.\"","594","                           \" number of groups for any class cannot\""]}]}},"dcaa26797e311d6f02e0927832de533ac0c2134f":{"changes":{"sklearn\/neighbors\/tests\/test_dist_metrics.py":"MODIFY"},"diff":{"sklearn\/neighbors\/tests\/test_dist_metrics.py":[{"add":["17","class TestMetrics(object):","18","    n1 = 20","19","    n2 = 25","20","    d = 4","21","    zero_frac = 0.5","22","    rseed = 0","23","    dtype = np.float64","24","    rng = check_random_state(rseed)","25","    X1 = rng.random_sample((n1, d)).astype(dtype)","26","    X2 = rng.random_sample((n2, d)).astype(dtype)","28","    # make boolean arrays: ones and zeros","29","    X1_bool = X1.round(0)","30","    X2_bool = X2.round(0)","32","    V = rng.random_sample((d, d))","33","    VI = np.dot(V, V.T)","35","    metrics = {'euclidean': {},","36","               'cityblock': {},","37","               'minkowski': dict(p=(1, 1.5, 2, 3)),","38","               'chebyshev': {},","39","               'seuclidean': dict(V=(rng.random_sample(d),)),","40","               'wminkowski': dict(p=(1, 1.5, 3),","41","                                  w=(rng.random_sample(d),)),","42","               'mahalanobis': dict(VI=(VI,)),","43","               'hamming': {},","44","               'canberra': {},","45","               'braycurtis': {}}","47","    bool_metrics = ['matching', 'jaccard', 'dice',","48","                    'kulsinski', 'rogerstanimoto', 'russellrao',","49","                    'sokalmichener', 'sokalsneath']"],"delete":["17","class TestMetrics:","18","    def __init__(self, n1=20, n2=25, d=4, zero_frac=0.5,","19","                 rseed=0, dtype=np.float64):","20","        rng = check_random_state(rseed)","21","        self.X1 = rng.random_sample((n1, d)).astype(dtype)","22","        self.X2 = rng.random_sample((n2, d)).astype(dtype)","24","        # make boolean arrays: ones and zeros","25","        self.X1_bool = self.X1.round(0)","26","        self.X2_bool = self.X2.round(0)","28","        V = rng.random_sample((d, d))","29","        VI = np.dot(V, V.T)","31","        self.metrics = {'euclidean': {},","32","                        'cityblock': {},","33","                        'minkowski': dict(p=(1, 1.5, 2, 3)),","34","                        'chebyshev': {},","35","                        'seuclidean': dict(V=(rng.random_sample(d),)),","36","                        'wminkowski': dict(p=(1, 1.5, 3),","37","                                           w=(rng.random_sample(d),)),","38","                        'mahalanobis': dict(VI=(VI,)),","39","                        'hamming': {},","40","                        'canberra': {},","41","                        'braycurtis': {}}","43","        self.bool_metrics = ['matching', 'jaccard', 'dice',","44","                             'kulsinski', 'rogerstanimoto', 'russellrao',","45","                             'sokalmichener', 'sokalsneath']"]}]}},"ece341cf5362558b944685d70a12c28efd055991":{"changes":{"sklearn\/neighbors\/tests\/test_neighbors.py":"MODIFY","sklearn\/neighbors\/base.py":"MODIFY"},"diff":{"sklearn\/neighbors\/tests\/test_neighbors.py":[{"add":["1285","def test_sparse_metric_callable():","1286","    def sparse_metric(x, y):  # Metric accepting sparse matrix input (only)","1287","        assert_true(issparse(x) and issparse(y))","1288","        return x.dot(y.T).A.item()","1289","","1290","    X = csr_matrix([  # Population matrix","1291","        [1, 1, 1, 1, 1],","1292","        [1, 0, 1, 0, 1],","1293","        [0, 0, 1, 0, 0]","1294","    ])","1295","","1296","    Y = csr_matrix([  # Query matrix","1297","        [1, 1, 0, 1, 1],","1298","        [1, 0, 0, 0, 1]","1299","    ])","1300","","1301","    nn = neighbors.NearestNeighbors(algorithm='brute', n_neighbors=2,","1302","                                    metric=sparse_metric).fit(X)","1303","    N = nn.kneighbors(Y, return_distance=False)","1304","","1305","    # GS indices of nearest neighbours in `X` for `sparse_metric`","1306","    gold_standard_nn = np.array([","1307","        [2, 1],","1308","        [2, 1]","1309","    ])","1310","","1311","    assert_array_equal(N, gold_standard_nn)","1312","","1313",""],"delete":[]}],"sklearn\/neighbors\/base.py":[{"add":["213","            if self.effective_metric_ not in VALID_METRICS_SPARSE['brute'] \\","214","                    and not callable(self.effective_metric_):","215",""],"delete":["213","            if self.effective_metric_ not in VALID_METRICS_SPARSE['brute']:"]}]}},"894fd72980f62c6f8eda9b5e1043e0cd3a1776e5":{"changes":{"sklearn\/neighbors\/dist_metrics.pyx":"MODIFY"},"diff":{"sklearn\/neighbors\/dist_metrics.pyx":[{"add":["116","             [3, 4, 5]]"],"delete":["116","             [3, 4, 5]])"]}]}},"9d515e960afee497183e85d5763be782c9276625":{"changes":{"sklearn\/datasets\/samples_generator.py":"MODIFY","sklearn\/datasets\/tests\/test_samples_generator.py":"MODIFY"},"diff":{"sklearn\/datasets\/samples_generator.py":[{"add":["164","        weights = weights + [1.0 - sum(weights)]"],"delete":["164","        weights.append(1.0 - sum(weights))"]}],"sklearn\/datasets\/tests\/test_samples_generator.py":[{"add":["39","    weights = [0.1, 0.25]","43","                               shift=None, scale=None, weights=weights,","46","    assert_equal(weights, [0.1, 0.25])","182","","193",""],"delete":["42","                               shift=None, scale=None, weights=[0.1, 0.25],"]}]}},"ecc96be8c5e831fd0a12f3274ed4a31dabcbffe6":{"changes":{"sklearn\/utils\/validation.py":"MODIFY","sklearn\/utils\/estimator_checks.py":"MODIFY","sklearn\/utils\/tests\/test_validation.py":"MODIFY"},"diff":{"sklearn\/utils\/validation.py":[{"add":["15","from numpy.core.numeric import ComplexWarning","310","def _ensure_no_complex_data(array):","311","    if hasattr(array, 'dtype') and array.dtype is not None \\","312","            and hasattr(array.dtype, 'kind') and array.dtype.kind == \"c\":","313","        raise ValueError(\"Complex data not supported\\n\"","314","                         \"{}\\n\".format(array))","315","","316","","437","        _ensure_no_complex_data(array)","441","        # If np.array(..) gives ComplexWarning, then we convert the warning","442","        # to an error. This is needed because specifying a non complex","443","        # dtype to the function converts complex to real dtype,","444","        # thereby passing the test made in the lines following the scope","445","        # of warnings context manager.","446","        with warnings.catch_warnings():","447","            try:","448","                warnings.simplefilter('error', ComplexWarning)","449","                array = np.array(array, dtype=dtype, order=order, copy=copy)","450","            except ComplexWarning:","451","                raise ValueError(\"Complex data not supported\\n\"","452","                                 \"{}\\n\".format(array))","453","","454","        # It is possible that the np.array(..) gave no warning. This happens","455","        # when no dtype conversion happend, for example dtype = None. The","456","        # result is that np.array(..) produces an array of complex dtype","457","        # and we need to catch and raise exception for such cases.","458","        _ensure_no_complex_data(array)"],"delete":["432","        array = np.array(array, dtype=dtype, order=order, copy=copy)"]}],"sklearn\/utils\/estimator_checks.py":[{"add":["78","    yield check_complex_data","461","def check_complex_data(name, estimator_orig):","462","    # check that estimators raise an exception on providing complex data","463","    X = np.random.sample(10) + 1j * np.random.sample(10)","464","    X = X.reshape(-1, 1)","465","    y = np.random.sample(10) + 1j * np.random.sample(10)","466","    estimator = clone(estimator_orig)","467","    assert_raises_regex(ValueError, \"Complex data not supported\",","468","                        estimator.fit, X, y)","469","","470",""],"delete":[]}],"sklearn\/utils\/tests\/test_validation.py":[{"add":["439","def test_check_array_complex_data_error():","440","    # np array","441","    X = np.array([[1 + 2j, 3 + 4j, 5 + 7j], [2 + 3j, 4 + 5j, 6 + 7j]])","442","    assert_raises_regexp(","443","        ValueError, \"Complex data not supported\", check_array, X)","444","","445","    # list of lists","446","    X = [[1 + 2j, 3 + 4j, 5 + 7j], [2 + 3j, 4 + 5j, 6 + 7j]]","447","    assert_raises_regexp(","448","        ValueError, \"Complex data not supported\", check_array, X)","449","","450","    # tuple of tuples","451","    X = ((1 + 2j, 3 + 4j, 5 + 7j), (2 + 3j, 4 + 5j, 6 + 7j))","452","    assert_raises_regexp(","453","        ValueError, \"Complex data not supported\", check_array, X)","454","","455","    # list of np arrays","456","    X = [np.array([1 + 2j, 3 + 4j, 5 + 7j]),","457","         np.array([2 + 3j, 4 + 5j, 6 + 7j])]","458","    assert_raises_regexp(","459","        ValueError, \"Complex data not supported\", check_array, X)","460","","461","    # tuple of np arrays","462","    X = (np.array([1 + 2j, 3 + 4j, 5 + 7j]),","463","         np.array([2 + 3j, 4 + 5j, 6 + 7j]))","464","    assert_raises_regexp(","465","        ValueError, \"Complex data not supported\", check_array, X)","466","","467","    # dataframe","468","    X = MockDataFrame(","469","        np.array([[1 + 2j, 3 + 4j, 5 + 7j], [2 + 3j, 4 + 5j, 6 + 7j]]))","470","    assert_raises_regexp(","471","        ValueError, \"Complex data not supported\", check_array, X)","472","","473","    # sparse matrix","474","    X = sp.coo_matrix([[0, 1 + 2j], [0, 0]])","475","    assert_raises_regexp(","476","        ValueError, \"Complex data not supported\", check_array, X)","477","","478",""],"delete":[]}]}},"6ef8595d6c2fec62395850db6b9655b729ca7ebc":{"changes":{"doc\/modules\/clustering.rst":"MODIFY"},"diff":{"doc\/modules\/clustering.rst":[{"add":["193","(use the ``init='k-means++'`` parameter). This initializes the centroids to be"],"delete":["193","(use the ``init='kmeans++'`` parameter). This initializes the centroids to be"]}]}},"030f8b21f18b6078d2a12845f7fd0a93e1b9d4de":{"changes":{"sklearn\/tests\/test_docstring_parameters.py":"MODIFY","sklearn\/linear_model\/tests\/test_bayes.py":"MODIFY","sklearn\/utils\/testing.py":"MODIFY",".travis.yml":"MODIFY","build_tools\/travis\/test_script.sh":"MODIFY","sklearn\/utils\/estimator_checks.py":"MODIFY"},"diff":{"sklearn\/tests\/test_docstring_parameters.py":[{"add":["10","from inspect import getsource, isabstract","20","PUBLIC_MODULES = set([pckg[1] for pckg in walk_packages(prefix='sklearn.',","21","                                                        path=sklearn.__path__)","22","                      if not (\"._\" in pckg[1] or \".tests.\" in pckg[1])])","26","IGNORED_MODULES = (","27","    'cross_decomposition',","28","    'covariance',","29","    'cluster',","30","    'datasets',","31","    'decomposition',","32","    'feature_extraction',","33","    'gaussian_process',","34","    'linear_model',","35","    'manifold',","36","    'metrics',","37","    'discriminant_analysis',","38","    'ensemble',","39","    'feature_selection',","40","    'kernel_approximation',","41","    'model_selection',","42","    'multioutput',","43","    'random_projection',","44","    'setup',","45","    'svm',","46","    'utils',","47","    'neighbors'","49","    'cross_validation',","50","    'grid_search',","51","    'learning_curve',","52",")","53","","89","        if name.startswith('_') or name.split(\".\")[1] in IGNORED_MODULES:","90","            continue","94","        # Exclude imported classes","95","        classes = [cls for cls in classes if cls[1].__module__ == name]","98","            if cname in _DOCSTRING_IGNORES or cname.startswith('_'):","100","            if isabstract(cls):","135","        # Exclude imported functions","136","        functions = [fn for fn in functions if fn[1].__module__ == name]","141","            if fname == \"configuration\" and name.endswith(\"setup\"):","142","                continue"],"delete":["4","from __future__ import print_function","5","","12","from inspect import getsource","22","PUBLIC_MODULES = set(['sklearn.' + modname","23","                      for _, modname, _ in walk_packages(sklearn.__path__)","24","                      if not modname.startswith('_') and","25","                      '.tests.' not in modname])","29","PUBLIC_MODULES -= set([","30","    'sklearn.ensemble',","31","    'sklearn.feature_selection',","32","    'sklearn.kernel_approximation',","33","    'sklearn.model_selection',","34","    'sklearn.multioutput',","35","    'sklearn.random_projection',","36","    'sklearn.setup',","37","    'sklearn.svm',","38","    'sklearn.utils',","40","    'sklearn.cross_validation',","41","    'sklearn.grid_search',","42","    'sklearn.learning_curve',","43","])","84","            if cname in _DOCSTRING_IGNORES:","86","            if cname.startswith('_'):"]}],"sklearn\/linear_model\/tests\/test_bayes.py":[{"add":["18","    raise SkipTest(\"test_bayesian_on_diabetes is broken\")"],"delete":["18","    raise SkipTest(\"XFailed Test\")"]}],"sklearn\/utils\/testing.py":[{"add":["883","            if (':' not in param_name or","884","                    param_name[len(param_name.split(':')[0].strip())] == ':'):"],"delete":["883","            if param_name[len(param_name.split(':')[0].strip())] == ':':"]}],".travis.yml":[{"add":["47","           TEST_DOCSTRINGS=\"true\""],"delete":["51","           TEST_DOCSTRINGS=\"true\""]}],"build_tools\/travis\/test_script.sh":[{"add":["24","=\"pytest --showlocals --durations=1 --pyargs -rs\""],"delete":[]}],"sklearn\/utils\/estimator_checks.py":[{"add":["1353","        raise SkipTest(\"Not testing NuSVC class weight as it is ignored.\")","1536","        raise SkipTest(\"Skipping check_estimators_data_not_an_array \"","1537","                       \"for cross decomposition module as estimators \"","1538","                       \"are not deterministic.\")"],"delete":["1353","        raise SkipTest","1536","        raise SkipTest"]}]}},"5a74e2f1c8470c527018dba78f86557f40eaeb47":{"changes":{"doc\/images\/sloan_banner.png":"ADD","doc\/themes\/scikit-learn\/static\/img\/sloan_logo.jpg":"ADD","doc\/about.rst":"MODIFY","doc\/index.rst":"MODIFY"},"diff":{"doc\/images\/sloan_banner.png":[{"add":[],"delete":[]}],"doc\/themes\/scikit-learn\/static\/img\/sloan_logo.jpg":[{"add":[],"delete":[]}],"doc\/about.rst":[{"add":["106","`Columbia University <http:\/\/columbia.edu>`_ funds Andreas Mller since 2016.","113","Andreas Mller also received a grant to improve scikit-learn from the `Alfred P. Sloan Foundation <https:\/\/sloan.org>`_ in 2017.","114","","115",".. image:: images\/sloan_banner.png","116","   :width: 200pt","117","   :align: center","118","   :target: https:\/\/sloan.org\/","119",""],"delete":["106","`Columbia University <http:\/\/columbia.edu>`_ funds Andreas Mueller since 2016."]}],"doc\/index.rst":[{"add":["331","                   <img id=\"index-funding-logo-small\" src=\"_static\/img\/sloan_logo.jpg\" title=\"Alfred P. Sloan Foundation\" style=\"max-height: 36px\">"],"delete":["331","                   <img id=\"index-funding-logo-small\" src=\"_static\/img\/nyu_short_color.png\" title=\"NYU CDS\">"]}]}},"c554aad456b6302a8dd8838769769eeecc1cf734":{"changes":{"sklearn\/preprocessing\/tests\/test_label.py":"MODIFY","sklearn\/preprocessing\/label.py":"MODIFY"},"diff":{"sklearn\/preprocessing\/tests\/test_label.py":[{"add":["205","    le.fit([1, 2, 3, -1, 1])","206","    msg = \"contains previously unseen labels\"","207","    assert_raise_message(ValueError, msg, le.inverse_transform, [-2])","208","    assert_raise_message(ValueError, msg, le.inverse_transform, [-2, -3, -4])"],"delete":["205","    le.fit([1, 2, 3, 1, -1])","206","    assert_raises(ValueError, le.inverse_transform, [-1])"]}],"sklearn\/preprocessing\/label.py":[{"add":["132","            raise ValueError(","133","                    \"y contains previously unseen labels: %s\" % str(diff))","151","        if len(diff):","152","            raise ValueError(","153","                    \"y contains previously unseen labels: %s\" % str(diff))"],"delete":["132","            raise ValueError(\"y contains new labels: %s\" % str(diff))","150","        if diff:","151","            raise ValueError(\"y contains new labels: %s\" % str(diff))"]}]}},"90607f16675cd8706c8de5fb3e2d33f440f2b59c":{"changes":{"sklearn\/neighbors\/tests\/test_nearest_centroid.py":"MODIFY","sklearn\/neighbors\/nearest_centroid.py":"MODIFY","doc\/whats_new.rst":"MODIFY"},"diff":{"sklearn\/neighbors\/tests\/test_nearest_centroid.py":[{"add":["99","def test_shrinkage_correct():","100","    # Ensure that the shrinking is correct.","101","    # The expected result is calculated by R (pamr),","102","    # which is implemented by the author of the original paper.","103","    # (One need to modify the code to output the new centroid in pamr.predict)","104","","105","    X = np.array([[0, 1], [1, 0], [1, 1], [2, 0], [6, 8]])","106","    y = np.array([1, 1, 2, 2, 2])","107","    clf = NearestCentroid(shrink_threshold=0.1)","108","    clf.fit(X, y)","109","    expected_result = np.array([[0.7787310, 0.8545292], [2.814179, 2.763647]])","110","    np.testing.assert_array_almost_equal(clf.centroids_, expected_result)","111","","112",""],"delete":[]}],"sklearn\/neighbors\/nearest_centroid.py":[{"add":["149","            m = np.sqrt((1. \/ nk) - (1. \/ n_samples))"],"delete":["149","            m = np.sqrt((1. \/ nk) + (1. \/ n_samples))"]}],"doc\/whats_new.rst":[{"add":["65","- :class:`neighbors.NearestCentroid` (bug fix)","539","- Fixed the shrinkage implementation in :class:`neighbors.NearestCentroid`.","540","  :issue:`9219` by `Hanmin Qin <https:\/\/github.com\/qinhanmin2014>`_.","541",""],"delete":[]}]}},"9f9ad183f0871a1ba7ed24f7cea21388e68b090a":{"changes":{"sklearn\/metrics\/pairwise.py":"MODIFY","sklearn\/metrics\/tests\/test_pairwise.py":"MODIFY"},"diff":{"sklearn\/metrics\/pairwise.py":[{"add":["13","import warnings","470","                        size_threshold=None):","523","    if size_threshold is not None:","524","        warnings.warn('Use of the \"size_threshold\" is deprecated '","525","                      'in 0.19 and it will be removed version '","526","                      '0.21 of scikit-learn', DeprecationWarning)"],"delete":["469","                        size_threshold=5e8):"]}],"sklearn\/metrics\/tests\/test_pairwise.py":[{"add":["14","from sklearn.utils.testing import assert_warns","77","    # Using size_threshold argument should raise","78","    # a deprecation warning","79","    assert_warns(DeprecationWarning,","80","                 manhattan_distances, X, Y, size_threshold=10)"],"delete":["76","    # Low-level function for manhattan can divide in blocks to avoid","77","    # using too much memory during the broadcasting","78","    S3 = manhattan_distances(X, Y, size_threshold=10)","79","    assert_array_almost_equal(S, S3)"]}]}},"1755b893df191ad593ce90848eec999f8ca6d411":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/feature_selection\/rfe.py":"MODIFY","sklearn\/feature_selection\/tests\/test_rfe.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["473","Feature selection","474","","475","- Fixed computation of ``n_features_to_compute`` for edge case with tied","476","  CV scores in :class:`RFECV`. :issue:`9222` by `Nick Hoh <nickypie>`.","477",""],"delete":[]}],"sklearn\/feature_selection\/rfe.py":[{"add":["451","        scores_rev = scores[::-1]","452","        argmax_idx = len(scores) - np.argmax(scores_rev) - 1","454","            n_features - (argmax_idx * step),"],"delete":["452","            n_features - (np.argmax(scores) * step),"]}],"sklearn\/feature_selection\/tests\/test_rfe.py":[{"add":["169","    # In the event of cross validation score ties, the expected behavior of","170","    # RFECV is to return the FEWEST features that maximize the CV score.","171","    # Because test_scorer always returns 1.0 in this example, RFECV should","172","    # reduce the dimensionality to a single feature (i.e. n_features_ = 1)","173","    assert_equal(rfecv.n_features_, 1)"],"delete":[]}]}},"45dc891c96eebdb3b81bf14c2737d8f6540fabfe":{"changes":{"sklearn\/gaussian_process\/gpc.py":"MODIFY","sklearn\/linear_model\/ransac.py":"MODIFY","sklearn\/discriminant_analysis.py":"MODIFY","sklearn\/manifold\/tests\/test_t_sne.py":"MODIFY","sklearn\/mixture\/base.py":"MODIFY","sklearn\/neighbors\/nearest_centroid.py":"MODIFY","sklearn\/utils\/estimator_checks.py":"MODIFY","sklearn\/cluster\/spectral.py":"MODIFY","sklearn\/feature_selection\/univariate_selection.py":"MODIFY","sklearn\/cluster\/mean_shift_.py":"MODIFY","sklearn\/model_selection\/_split.py":"MODIFY","sklearn\/linear_model\/stochastic_gradient.py":"MODIFY","sklearn\/ensemble\/gradient_boosting.py":"MODIFY","sklearn\/decomposition\/fastica_.py":"MODIFY","sklearn\/svm\/base.py":"MODIFY","sklearn\/manifold\/t_sne.py":"MODIFY","sklearn\/linear_model\/bayes.py":"MODIFY","sklearn\/manifold\/locally_linear.py":"MODIFY","sklearn\/cross_decomposition\/pls_.py":"MODIFY","sklearn\/cluster\/tests\/test_mean_shift.py":"MODIFY","sklearn\/linear_model\/ridge.py":"MODIFY"},"diff":{"sklearn\/gaussian_process\/gpc.py":[{"add":["191","            raise ValueError(\"{0:s} requires 2 classes; got {1:d} class\"","192","                             .format(self.__class__.__name__,","193","                                     self.classes_.size))","598","                             \"distinct classes; got %d class (only class %s \"","599","                             \"is present)\"","600","                             % (self.n_classes_, self.classes_[0]))"],"delete":["191","            raise ValueError(\"{0:s} requires 2 classes.\".format(","192","                self.__class__.__name__))","597","                             \"distinct classes. Only class %s present.\"","598","                             % self.classes_[0])"]}],"sklearn\/linear_model\/ransac.py":[{"add":["272","                             \"of samples: n_samples = %d.\" % (X.shape[0]))"],"delete":["272","                             \"of samples ``X.shape[0]``.\")"]}],"sklearn\/discriminant_analysis.py":[{"add":["652","            raise ValueError('The number of classes has to be greater than'","653","                             ' one; got %d class' % (n_classes))"],"delete":["652","            raise ValueError('y has less than 2 classes')"]}],"sklearn\/manifold\/tests\/test_t_sne.py":[{"add":["297","                         tsne.fit_transform, np.array([[0.0], [0.0]]))","304","                         np.array([[0.0], [0.0]]))"],"delete":["297","                         tsne.fit_transform, np.array([[0.0]]))","304","                         np.array([[0.0]]))"]}],"sklearn\/mixture\/base.py":[{"add":["40","def _check_X(X, n_components=None, n_features=None, ensure_min_samples=1):","53","    X = check_array(X, dtype=[np.float64, np.float32],","54","                    ensure_min_samples=ensure_min_samples)","190","        X = _check_X(X, self.n_components, ensure_min_samples=2)"],"delete":["40","def _check_X(X, n_components=None, n_features=None):","53","    X = check_array(X, dtype=[np.float64, np.float32])","189","        X = _check_X(X, self.n_components)"]}],"sklearn\/neighbors\/nearest_centroid.py":[{"add":["117","            raise ValueError('The number of classes has to be greater than'","118","                             ' one; got %d class' % (n_classes))"],"delete":["117","            raise ValueError('y has less than 2 classes')"]}],"sklearn\/utils\/estimator_checks.py":[{"add":["126","    yield check_supervised_y_no_nan","225","    if name != 'GaussianProcess':  # FIXME","226","        # XXX GaussianProcess deprecated in 0.20","227","        yield check_fit2d_1sample","229","    yield check_fit1d","591","    # Check that fitting a 2d array with only one sample either works or","592","    # returns an informative message. The error message should either mention","593","    # the number of samples or the number of classes.","606","","607","    msgs = [\"1 sample\", \"n_samples = 1\", \"n_samples=1\", \"one sample\",","608","            \"1 class\", \"one class\"]","609","","612","    except ValueError as e:","613","        if all(msg not in repr(e) for msg in msgs):","614","            raise e","619","    # check fitting a 2d array with only 1 feature either works or returns","620","    # informative message","631","    # ensure two labels in subsample for RandomizedLogisticRegression","632","    if name == 'RandomizedLogisticRegression':","633","        estimator.sample_fraction = 1","634","    # ensure non skipped trials for RANSACRegressor","635","    if name == 'RANSACRegressor':","636","        estimator.residual_threshold = 0.5","638","    y = multioutput_estimator_convert_y_2d(estimator, y)","640","","641","    msgs = [\"1 feature(s)\", \"n_features = 1\", \"n_features=1\"]","642","","645","    except ValueError as e:","646","        if all(msg not in repr(e) for msg in msgs):","647","            raise e","651","def check_fit1d(name, estimator_orig):","652","    # check fitting 1d X array raises a ValueError","665","    assert_raises(ValueError, estimator.fit, X, y)"],"delete":["224","    yield check_fit2d_1sample","226","    yield check_fit1d_1feature","227","    yield check_fit1d_1sample","589","    # check by fitting a 2d array and prediting with a 1d array","604","    except ValueError:","605","        pass","610","    # check by fitting a 2d array and prediting with a 1d array","625","    except ValueError:","626","        pass","630","def check_fit1d_1feature(name, estimator_orig):","631","    # check fitting 1d array with 1 feature","644","","645","    try:","646","        estimator.fit(X, y)","647","    except ValueError:","648","        pass","649","","650","","651","@ignore_warnings","652","def check_fit1d_1sample(name, estimator_orig):","653","    # check fitting 1d array with 1 feature","654","    rnd = np.random.RandomState(0)","655","    X = 3 * rnd.uniform(size=(20))","656","    y = np.array([1])","657","    estimator = clone(estimator_orig)","658","    y = multioutput_estimator_convert_y_2d(estimator, y)","659","","660","    if hasattr(estimator, \"n_components\"):","661","        estimator.n_components = 1","662","    if hasattr(estimator, \"n_clusters\"):","663","        estimator.n_clusters = 1","664","","665","    set_random_state(estimator, 1)","666","","667","    try:","668","        estimator.fit(X, y)","669","    except ValueError:","670","        pass"]}],"sklearn\/cluster\/spectral.py":[{"add":["439","                        dtype=np.float64, ensure_min_samples=2)","447","            connectivity = kneighbors_graph(X, n_neighbors=self.n_neighbors,","448","                                            include_self=True,"],"delete":["439","                        dtype=np.float64)","447","            connectivity = kneighbors_graph(X, n_neighbors=self.n_neighbors, include_self=True,"]}],"sklearn\/feature_selection\/univariate_selection.py":[{"add":["490","            raise ValueError(\"k should be >=0, <= n_features = %d; got %r. \"","492","                             % (X.shape[1], self.k))"],"delete":["490","            raise ValueError(\"k should be >=0, <= n_features; got %r.\"","492","                             % self.k)"]}],"sklearn\/cluster\/mean_shift_.py":[{"add":["70","    n_neighbors = int(X.shape[0] * quantile)","71","    if n_neighbors < 1:  # cannot fit NearestNeighbors with n_neighbors = 0","72","        n_neighbors = 1","73","    nbrs = NearestNeighbors(n_neighbors=n_neighbors,"],"delete":["70","    nbrs = NearestNeighbors(n_neighbors=int(X.shape[0] * quantile),"]}],"sklearn\/model_selection\/_split.py":[{"add":["328","                 \" than the number of samples: n_samples={1}.\")","329","                .format(self.n_splits, n_samples))"],"delete":["328","                 \" than the number of samples: {1}.\").format(self.n_splits,","329","                                                             n_samples))"]}],"sklearn\/linear_model\/stochastic_gradient.py":[{"add":["406","            raise ValueError(","407","                \"The number of classes has to be greater than one;\"","408","                \" got %d class\" % n_classes)"],"delete":["406","            raise ValueError(\"The number of class labels must be \"","407","                             \"greater than one.\")"]}],"sklearn\/ensemble\/gradient_boosting.py":[{"add":["471","            raise ValueError(\"{0:s} requires 2 classes; got {1:d} class(es)\"","472","                             .format(self.__class__.__name__, n_classes))","604","            raise ValueError(\"{0:s} requires 2 classes; got {1:d} class(es)\"","605","                             .format(self.__class__.__name__, n_classes))"],"delete":["471","            raise ValueError(\"{0:s} requires 2 classes.\".format(","472","                self.__class__.__name__))","604","            raise ValueError(\"{0:s} requires 2 classes.\".format(","605","                self.__class__.__name__))"]}],"sklearn\/decomposition\/fastica_.py":[{"add":["269","    X = check_array(X, copy=whiten, dtype=FLOAT_DTYPES,","270","                    ensure_min_samples=2).T"],"delete":["269","    X = check_array(X, copy=whiten, dtype=FLOAT_DTYPES).T"]}],"sklearn\/svm\/base.py":[{"add":["505","                \" class\" % len(cls))"],"delete":["505","                % len(cls))"]}],"sklearn\/manifold\/t_sne.py":[{"add":["658","        if self.method == 'barnes_hut':","659","            X = check_array(X, ensure_min_samples=2,","660","                            dtype=[np.float32, np.float64])"],"delete":[]}],"sklearn\/linear_model\/bayes.py":[{"add":["428","        X, y = check_X_y(X, y, dtype=np.float64, y_numeric=True,","429","                         ensure_min_samples=2)"],"delete":["428","        X, y = check_X_y(X, y, dtype=np.float64, y_numeric=True)"]}],"sklearn\/manifold\/locally_linear.py":[{"add":["300","        raise ValueError(","301","            \"Expected n_neighbors <= n_samples, \"","302","            \" but n_samples = %d, n_neighbors = %d\" %","303","            (N, n_neighbors)","304","        )"],"delete":["300","        raise ValueError(\"n_neighbors must be less than number of points\")"]}],"sklearn\/cross_decomposition\/pls_.py":[{"add":["247","        X = check_array(X, dtype=np.float64, copy=self.copy,","248","                        ensure_min_samples=2)","800","        X = check_array(X, dtype=np.float64, copy=self.copy,","801","                        ensure_min_samples=2)"],"delete":["247","        X = check_array(X, dtype=np.float64, copy=self.copy)","799","        X = check_array(X, dtype=np.float64, copy=self.copy)"]}],"sklearn\/cluster\/tests\/test_mean_shift.py":[{"add":["36","def test_estimate_bandwidth_1sample():","37","    # Test estimate_bandwidth when n_samples=1 and quantile<1, so that","38","    # n_neighbors is set to 1.","39","    bandwidth = estimate_bandwidth(X, n_samples=1, quantile=0.3)","40","    assert_equal(bandwidth, 0.)","41","","42",""],"delete":[]}],"sklearn\/linear_model\/ridge.py":[{"add":["806","        check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'],","807","                  multi_output=True)","808","","1353","        check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'],","1354","                  multi_output=True)","1355",""],"delete":[]}]}},"a365714481a092ec05b0194d6f73d46b6eee06f5":{"changes":{"sklearn\/ensemble\/bagging.py":"MODIFY","sklearn\/ensemble\/tests\/test_bagging.py":"MODIFY","doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/utils\/__init__.py":"MODIFY"},"diff":{"sklearn\/ensemble\/bagging.py":[{"add":["413","        Returns a dynamically generated list of indices identifying","421","        return [sample_indices","422","                for _, sample_indices in self._get_estimators_indices()]","509","        estimator. Each subset is defined by an array of the indices selected.","587","            mask = ~indices_to_mask(samples, n_samples)","882","        estimator. Each subset is defined by an array of the indices selected.","993","            mask = ~indices_to_mask(samples, n_samples)"],"delete":["112","        # Draw samples, using a mask, and then fit","414","        Returns a dynamically generated list of boolean masks identifying","422","        sample_masks = []","423","        for _, sample_indices in self._get_estimators_indices():","424","            mask = indices_to_mask(sample_indices, self._n_samples)","425","            sample_masks.append(mask)","426","","427","        return sample_masks","514","        estimator. Each subset is defined by a boolean mask.","592","            mask = ~samples","887","        estimator. Each subset is defined by a boolean mask.","998","            mask = ~samples"]}],"sklearn\/ensemble\/tests\/test_bagging.py":[{"add":["31","from sklearn.random_projection import SparseRandomProjection","36","from sklearn.utils import check_random_state, hash","225","class DummySizeEstimator(BaseEstimator):","226","","227","    def fit(self, X, y):","228","        self.training_size_ = X.shape[0]","229","        self.training_hash_ = hash(X)","230","","231","","259","    # check that each sampling correspond to a complete bootstrap resample.","260","    # the size of each bootstrap should be the same as the input data but","261","    # the data should be different (checked using the hash of the data).","262","    ensemble = BaggingRegressor(base_estimator=DummySizeEstimator(),","263","                                bootstrap=True).fit(X_train, y_train)","264","    training_hash = []","265","    for estimator in ensemble.estimators_:","266","        assert estimator.training_size_ == X_train.shape[0]","267","        training_hash.append(estimator.training_hash_)","268","    assert len(set(training_hash)) == len(training_hash)","269","","731","    assert_equal(len(estimators_samples[0]), len(X) \/\/ 2)","732","    assert_equal(estimators_samples[0].dtype.kind, 'i')","750","def test_estimators_samples_deterministic():","751","    # This test is a regression test to check that with a random step","752","    # (e.g. SparseRandomProjection) and a given random state, the results","753","    # generated at fit time can be identically reproduced at a later time using","754","    # data saved in object attributes. Check issue #9524 for full discussion.","755","","756","    iris = load_iris()","757","    X, y = iris.data, iris.target","758","","759","    base_pipeline = make_pipeline(SparseRandomProjection(n_components=2),","760","                                  LogisticRegression())","761","    clf = BaggingClassifier(base_estimator=base_pipeline,","762","                            max_samples=0.5,","763","                            random_state=0)","764","    clf.fit(X, y)","765","    pipeline_estimator_coef = clf.estimators_[0].steps[-1][1].coef_.copy()","766","","767","    estimator = clf.estimators_[0]","768","    estimator_sample = clf.estimators_samples_[0]","769","    estimator_feature = clf.estimators_features_[0]","770","","771","    X_train = (X[estimator_sample])[:, estimator_feature]","772","    y_train = y[estimator_sample]","773","","774","    estimator.fit(X_train, y_train)","775","    assert_array_equal(estimator.steps[-1][1].coef_, pipeline_estimator_coef)","776","","777",""],"delete":["35","from sklearn.utils import check_random_state","712","    assert_equal(len(estimators_samples[0]), len(X))","713","    assert_equal(estimators_samples[0].dtype.kind, 'b')"]}],"doc\/whats_new\/v0.20.rst":[{"add":["656","Ensemble","657","","658","- Fix allowing to obtain deterministic with :class:`BaseBagging` estimator,","659","  when comparing results generated at fit time with the one using the object","660","  attributes when ``random_state`` is set. :issue:`9723` by :user:`Guillaume","661","  Lemaitre <glemaitre>`.","662","","837","Ensemble","838","","839","- Classes derived from :class:`ensemble.BaseBagging`. The attribute","840","  ``estimators_samples_`` will return a list of arrays containing the indices","841","  selected for each bootstrap instead of a list of arrays containing the mask","842","  of the samples selected for each bootstrap. Indices allows to repeat samples","843","  while mask does not allow this functionality. :issue:`9524` by","844","  :user:`Guillaume Lemaitre <glemaitre>`.","845",""],"delete":[]}],"sklearn\/utils\/__init__.py":[{"add":["17","from ._joblib import cpu_count, Parallel, Memory, delayed, hash","31","           \"cpu_count\", \"Parallel\", \"Memory\", \"delayed\", \"parallel_backend\",","32","           \"hash\"]"],"delete":["17","from ._joblib import cpu_count, Parallel, Memory, delayed","31","           \"cpu_count\", \"Parallel\", \"Memory\", \"delayed\", \"parallel_backend\"]"]}]}},"534f68b17c44ad52cd7769a51034c96dc2fbdb22":{"changes":{"sklearn\/datasets\/species_distributions.py":"MODIFY","sklearn\/datasets\/california_housing.py":"MODIFY","sklearn\/datasets\/rcv1.py":"MODIFY"},"diff":{"sklearn\/datasets\/species_distributions.py":[{"add":["242","        with np.load(samples_path) as X:  # samples.zip is a valid npz","243","            for f in X.files:","244","                fhandle = BytesIO(X[f])","245","                if 'train' in f:","246","                    train = _load_csv(fhandle)","247","                if 'test' in f:","248","                    test = _load_csv(fhandle)","254","        with np.load(coverages_path) as X:  # coverages.zip is a valid npz","255","            coverages = []","256","            for f in X.files:","257","                fhandle = BytesIO(X[f])","258","                logger.debug(' - converting {}'.format(f))","259","                coverages.append(_load_coverage(fhandle))","260","            coverages = np.asarray(coverages, dtype=dtype)"],"delete":["242","        X = np.load(samples_path)  # samples.zip is a valid npz","245","        for f in X.files:","246","            fhandle = BytesIO(X[f])","247","            if 'train' in f:","248","                train = _load_csv(fhandle)","249","            if 'test' in f:","250","                test = _load_csv(fhandle)","251","","255","        X = np.load(coverages_path)  # coverages.zip is a valid npz","258","        coverages = []","259","        for f in X.files:","260","            fhandle = BytesIO(X[f])","261","            logger.debug(' - converting {}'.format(f))","262","            coverages.append(_load_coverage(fhandle))","263","        coverages = np.asarray(coverages, dtype=dtype)","264",""]}],"sklearn\/datasets\/california_housing.py":[{"add":["51","","99","","102","        with tarfile.open(mode=\"r:gz\", name=archive_path) as f:","103","            cal_housing = np.loadtxt(","104","                f.extractfile('CaliforniaHousing\/cal_housing.data'),","105","                delimiter=',')","106","            # Columns are not in the same order compared to the previous","107","            # URL resource on lib.stat.cmu.edu","108","            columns_index = [8, 7, 2, 3, 4, 5, 6, 1, 0]","109","            cal_housing = cal_housing[:, columns_index]","110","","111","            joblib.dump(cal_housing, filepath, compress=6)"],"delete":["100","        fileobj = tarfile.open(","101","            mode=\"r:gz\",","102","            name=archive_path).extractfile(","103","                'CaliforniaHousing\/cal_housing.data')","106","        cal_housing = np.loadtxt(fileobj, delimiter=',')","107","        # Columns are not in the same order compared to the previous","108","        # URL resource on lib.stat.cmu.edu","109","        columns_index = [8, 7, 2, 3, 4, 5, 6, 1, 0]","110","        cal_housing = cal_housing[:, columns_index]","111","        joblib.dump(cal_housing, filepath, compress=6)"]}],"sklearn\/datasets\/rcv1.py":[{"add":["175","","176","        # delete archives","177","        for f in files:","178","            f.close()","179","            remove(f.name)","184","","199","        with GzipFile(filename=topics_archive_path, mode='rb') as f:","200","            for line in f:","201","                line_components = line.decode(\"ascii\").split(u\" \")","202","                if len(line_components) == 3:","203","                    cat, doc, _ = line_components","204","                    if cat not in category_names:","205","                        n_cat += 1","206","                        category_names[cat] = n_cat","208","                    doc = int(doc)","209","                    if doc != doc_previous:","210","                        doc_previous = doc","211","                        n_doc += 1","212","                        sample_id_bis[n_doc] = doc","213","                    y[n_doc, category_names[cat]] = 1"],"delete":["168","        # delete archives","169","        for f in files:","170","            remove(f.name)","171","","197","        for line in GzipFile(filename=topics_archive_path, mode='rb'):","198","            line_components = line.decode(\"ascii\").split(u\" \")","199","            if len(line_components) == 3:","200","                cat, doc, _ = line_components","201","                if cat not in category_names:","202","                    n_cat += 1","203","                    category_names[cat] = n_cat","205","                doc = int(doc)","206","                if doc != doc_previous:","207","                    doc_previous = doc","208","                    n_doc += 1","209","                    sample_id_bis[n_doc] = doc","210","                y[n_doc, category_names[cat]] = 1"]}]}},"9b5561148f56a3934da9882a52f1978d7aa5bc75":{"changes":{"examples\/semi_supervised\/plot_label_propagation_digits_active_learning.py":"MODIFY","examples\/ensemble\/plot_bias_variance.py":"MODIFY"},"diff":{"examples\/semi_supervised\/plot_label_propagation_digits_active_learning.py":[{"add":["67","          % (n_labeled_points, n_total_samples - n_labeled_points,","68","             n_total_samples))","98","            sub.imshow(image, cmap=plt.cm.gray_r, interpolation='none')","111","           \"uncertain labels to learn with the next model.\", y=1.15)","112","plt.subplots_adjust(left=0.2, bottom=0.03, right=0.9, top=0.9, wspace=0.2,","113","                    hspace=0.85)"],"delete":["67","          % (n_labeled_points, n_total_samples - n_labeled_points, n_total_samples))","97","            sub.imshow(image, cmap=plt.cm.gray_r)","110","           \"uncertain labels to learn with the next model.\")","111","plt.subplots_adjust(0.12, 0.03, 0.9, 0.8, 0.2, 0.45)"]}],"examples\/ensemble\/plot_bias_variance.py":[{"add":["90","","97","","114","","125","plt.figure(figsize=(10, 8))","126","","173","    if n == n_estimators - 1:","174","        plt.legend(loc=(1.1, .5))","185","    if n == n_estimators - 1:","187","        plt.legend(loc=(1.1, .5))","188","","189","plt.subplots_adjust(right=.75)"],"delete":["168","    if n == 0:","169","        plt.legend(loc=\"upper left\", prop={\"size\": 11})","180","    if n == 0:","181","        plt.legend(loc=\"upper left\", prop={\"size\": 11})"]}]}},"f0c5568fcd9a6c9ace37082b573fc28bfdc17cec":{"changes":{"sklearn\/utils\/testing.py":"MODIFY","sklearn\/utils\/estimator_checks.py":"MODIFY"},"diff":{"sklearn\/utils\/testing.py":[{"add":["377","def assert_allclose_dense_sparse(x, y, rtol=1e-07, atol=1e-9, err_msg=''):","391","    rtol : float, optional","392","        relative tolerance; see numpy.allclose","393","","394","    atol : float, optional","395","        absolute tolerance; see numpy.allclose. Note that the default here is","396","        more tolerant than the default for numpy.testing.assert_allclose, where","397","        atol=0.","398",""],"delete":["377","def assert_allclose_dense_sparse(x, y, rtol=1e-07, atol=0, err_msg=''):"]}],"sklearn\/utils\/estimator_checks.py":[{"add":["1144","                assert_allclose(y_log_prob, np.log(y_prob), 8, atol=1e-9)","1380","        # XXX: Generally can use 0.89 here. On Windows, LinearSVC gets","1381","        #      0.88 (Issue #9111)","1382","        assert_greater(np.mean(y_pred == 0), 0.87)"],"delete":["1144","                assert_allclose(y_log_prob, np.log(y_prob), 8)","1380","        assert_greater(np.mean(y_pred == 0), 0.89)"]}]}},"1f5d22bffc081c59bf0d8c49c1e7ccbb6c84ef88":{"changes":{"sklearn\/feature_extraction\/tests\/test_feature_hasher.py":"MODIFY","sklearn\/feature_extraction\/tests\/test_text.py":"MODIFY"},"diff":{"sklearn\/feature_extraction\/tests\/test_feature_hasher.py":[{"add":["22","@ignore_warnings(category=DeprecationWarning)"],"delete":[]}],"sklearn\/feature_extraction\/tests\/test_text.py":[{"add":["30","                                   clean_warning_registry, ignore_warnings,","31","                                   SkipTest)","483","@ignore_warnings(category=DeprecationWarning)","655","@ignore_warnings(category=DeprecationWarning)","789","@ignore_warnings(category=DeprecationWarning)"],"delete":["30","                                   clean_warning_registry, SkipTest)"]}]}},"4ff7e8e658db082fe1ff0b2ba31244431fd99ef6":{"changes":{"sklearn\/utils\/random.py":"MODIFY","sklearn\/ensemble\/tests\/test_base.py":"MODIFY","sklearn\/multioutput.py":"MODIFY","sklearn\/linear_model\/tests\/test_logistic.py":"MODIFY","sklearn\/ensemble\/gradient_boosting.py":"MODIFY","sklearn\/mixture\/dpgmm.py":"MODIFY","sklearn\/metrics\/ranking.py":"MODIFY"},"diff":{"sklearn\/utils\/random.py":[{"add":["186","            # Normalize probabilities for the nonzero elements"],"delete":["186","            # Normalize probabilites for the nonzero elements"]}],"sklearn\/ensemble\/tests\/test_base.py":[{"add":["111","    # ensure multiple random_state parameters are invariant to get_params()"],"delete":["111","    # ensure multiple random_state paramaters are invariant to get_params()"]}],"sklearn\/multioutput.py":[{"add":["318","        Returns prediction probabilities for each class of each output."],"delete":["318","        Returns prediction probabilites for each class of each output."]}],"sklearn\/linear_model\/tests\/test_logistic.py":[{"add":["988","    # Predicted probabilities using the true-entropy loss should give a","998","    # Predicted probabilities using the soft-max function should give a"],"delete":["988","    # Predicted probabilites using the true-entropy loss should give a","998","    # Predicted probabilites using the soft-max function should give a"]}],"sklearn\/ensemble\/gradient_boosting.py":[{"add":["450","         the does not support probabilities raises AttributeError."],"delete":["450","         the does not support probabilites raises AttributeError."]}],"sklearn\/mixture\/dpgmm.py":[{"add":["49","    \"\"\"Normalized probabilities from unnormalized log-probabilities\"\"\""],"delete":["49","    \"\"\"Normalized probabilities from unnormalized log-probabilites\"\"\""]}],"sklearn\/metrics\/ranking.py":[{"add":["842","    # Make sure we use all the labels (max between the length and the higher"],"delete":["842","    # Make sure we use all the labels (max between the lenght and the higher"]}]}},"bd4953d143520d44c01b7392bb927fed2cfb9f97":{"changes":{"sklearn\/cluster\/hierarchical.py":"MODIFY","sklearn\/cluster\/tests\/test_hierarchical.py":"MODIFY"},"diff":{"sklearn\/cluster\/hierarchical.py":[{"add":["291","def linkage_tree(X, connectivity=None, n_components='deprecated',","370","    if n_components != 'deprecated':","371","        warnings.warn(\"n_components was deprecated in 0.18\"","372","                      \"will be removed in 0.21\", DeprecationWarning)","373",""],"delete":["291","def linkage_tree(X, connectivity=None, n_components=None,"]}],"sklearn\/cluster\/tests\/test_hierarchical.py":[{"add":["38","def test_deprecation_of_n_components_in_linkage_tree():","39","    rng = np.random.RandomState(0)","40","    X = rng.randn(50, 100)","41","    # Test for warning of deprecation of n_components in linkage_tree","42","    children, n_nodes, n_leaves, parent = assert_warns(DeprecationWarning,","43","                                                       linkage_tree,","44","                                                       X.T,","45","                                                       n_components=10)","46","    children_t, n_nodes_t, n_leaves_t, parent_t = linkage_tree(X.T)","47","    assert_array_equal(children, children_t)","48","    assert_equal(n_nodes, n_nodes_t)","49","    assert_equal(n_leaves, n_leaves_t)","50","    assert_equal(parent, parent_t)","51",""],"delete":[]}]}},"ec0dbf0399c4187a1d17e66de17265c67588373f":{"changes":{"doc\/whats_new.rst":"MODIFY","sklearn\/gaussian_process\/gpr.py":"MODIFY","sklearn\/gaussian_process\/tests\/test_gpr.py":"MODIFY"},"diff":{"doc\/whats_new.rst":[{"add":["275","     failed when `alpha=0`. :issue:`5814` by :user:`Yichuan Liu <yl565>` and","427","  -  Fixed a bug in :class:`gaussian_process.GaussianProcessRegressor`","428","     when the standard deviation and covariance predicted without fit","429","     would fail with a unmeaningful error by default.","430","     :issue:`6573` by :user:`Quazi Marufur Rahman <qmaruf>` and","431","     `Manoj Kumar`_."],"delete":["275","     failed when `alpha=0`. :issue:`5814` by :user:`Yichuan Liu <yl565>` and "]}],"sklearn\/gaussian_process\/gpr.py":[{"add":["299","            if self.kernel is None:","300","                kernel = (C(1.0, constant_value_bounds=\"fixed\") *","301","                          RBF(1.0, length_scale_bounds=\"fixed\"))","302","            else:","303","                kernel = self.kernel","306","                y_cov = kernel(X)","309","                y_var = kernel.diag(X)"],"delete":["301","                y_cov = self.kernel(X)","304","                y_var = self.kernel.diag(X)"]}],"sklearn\/gaussian_process\/tests\/test_gpr.py":[{"add":["16","            assert_almost_equal, assert_equal, assert_raise_message,","17","            assert_array_almost_equal)","330","","331","","332","def test_no_fit_default_predict():","333","    # Test that GPR predictions without fit does not break by default.","334","    default_kernel = (C(1.0, constant_value_bounds=\"fixed\") *","335","                      RBF(1.0, length_scale_bounds=\"fixed\"))","336","    gpr1 = GaussianProcessRegressor()","337","    _, y_std1 = gpr1.predict(X, return_std=True)","338","    _, y_cov1 = gpr1.predict(X, return_cov=True)","339","","340","    gpr2 = GaussianProcessRegressor(kernel=default_kernel)","341","    _, y_std2 = gpr2.predict(X, return_std=True)","342","    _, y_cov2 = gpr2.predict(X, return_cov=True)","343","","344","    assert_array_almost_equal(y_std1, y_std2)","345","    assert_array_almost_equal(y_cov1, y_cov2)"],"delete":["16","            assert_almost_equal, assert_equal, assert_raise_message)"]}]}},"c13ba26c4780d6088c40db3cc0f8367df4e27f1f":{"changes":{"doc\/Makefile":"MODIFY","sklearn\/tree\/__init__.py":"MODIFY","doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/tree\/_reingold_tilford.py":"ADD","sklearn\/tree\/tests\/test_export.py":"MODIFY","sklearn\/tree\/export.py":"MODIFY","sklearn\/tree\/tests\/test_reingold_tilford.py":"ADD","examples\/tree\/plot_iris.py":"MODIFY","doc\/modules\/tree.rst":"MODIFY"},"diff":{"doc\/Makefile":[{"add":["15","ALLSPHINXOPTS   = -T -d $(BUILDDIR)\/doctrees $(PAPEROPT_$(PAPER)) $(SPHINXOPTS)\\"],"delete":["15","ALLSPHINXOPTS   = -d $(BUILDDIR)\/doctrees $(PAPEROPT_$(PAPER)) $(SPHINXOPTS)\\"]}],"sklearn\/tree\/__init__.py":[{"add":["9","from .export import export_graphviz, plot_tree","12","           \"ExtraTreeClassifier\", \"ExtraTreeRegressor\", \"export_graphviz\",","13","           \"plot_tree\"]"],"delete":["9","from .export import export_graphviz","12","           \"ExtraTreeClassifier\", \"ExtraTreeRegressor\", \"export_graphviz\"]"]}],"doc\/whats_new\/v0.21.rst":[{"add":["64","- Decision Trees can now be plotted with matplotlib using","65","  :func:`tree.export.plot_tree` without relying on  the ``dot`` library,","66","  removing a hard-to-install dependency.","67","  :issue:`8508` by `Andreas Mller`_.","68",""],"delete":[]}],"sklearn\/tree\/_reingold_tilford.py":[{"add":[],"delete":[]}],"sklearn\/tree\/tests\/test_export.py":[{"add":["3","import pytest","12","from sklearn.tree import export_graphviz, plot_tree","95","                'value = [0.5, 0.5]>, fillcolor=\"#ffffff\"] ;\\n' \\","97","                'fillcolor=\"#e58139\"] ;\\n' \\","101","                'fillcolor=\"#399de5\"] ;\\n' \\","129","                'samples = 6\\\\nvalue = [3, 3]\", fillcolor=\"#ffffff\"] ;\\n' \\","151","                '[3.0, 1.0, 0.5]]\", fillcolor=\"#ffffff\"] ;\\n' \\","153","                '[3, 0, 0]]\", fillcolor=\"#e58139\"] ;\\n' \\","158","                '[0.0, 1.0, 0.5]]\", fillcolor=\"#f1bd97\"] ;\\n' \\","162","                '[0, 1, 0]]\", fillcolor=\"#e58139\"] ;\\n' \\","165","                '[0.0, 0.0, 0.5]]\", fillcolor=\"#e58139\"] ;\\n' \\","187","                'value = 0.0\", fillcolor=\"#f2c09c\"] ;\\n' \\","189","                'fillcolor=\"#ffffff\"] ;\\n' \\","193","                'fillcolor=\"#e58139\"] ;\\n' \\","210","                'fillcolor=\"#ffffff\"] ;\\n' \\","311","","312","","313","def test_plot_tree():","314","    # mostly smoke tests","315","    pytest.importorskip(\"matplotlib.pyplot\")","316","    # Check correctness of export_graphviz","317","    clf = DecisionTreeClassifier(max_depth=3,","318","                                 min_samples_split=2,","319","                                 criterion=\"gini\",","320","                                 random_state=2)","321","    clf.fit(X, y)","322","","323","    # Test export code","324","    feature_names = ['first feat', 'sepal_width']","325","    nodes = plot_tree(clf, feature_names=feature_names)","326","    assert len(nodes) == 3","327","    assert nodes[0].get_text() == (\"first feat <= 0.0\\nentropy = 0.5\\n\"","328","                                   \"samples = 6\\nvalue = [3, 3]\")","329","    assert nodes[1].get_text() == \"entropy = 0.0\\nsamples = 3\\nvalue = [3, 0]\"","330","    assert nodes[2].get_text() == \"entropy = 0.0\\nsamples = 3\\nvalue = [0, 3]\""],"delete":["11","from sklearn.tree import export_graphviz","94","                'value = [0.5, 0.5]>, fillcolor=\"#e5813900\"] ;\\n' \\","96","                'fillcolor=\"#e58139ff\"] ;\\n' \\","100","                'fillcolor=\"#399de5ff\"] ;\\n' \\","128","                'samples = 6\\\\nvalue = [3, 3]\", fillcolor=\"#e5813900\"] ;\\n' \\","150","                '[3.0, 1.0, 0.5]]\", fillcolor=\"#e5813900\"] ;\\n' \\","152","                '[3, 0, 0]]\", fillcolor=\"#e58139ff\"] ;\\n' \\","157","                '[0.0, 1.0, 0.5]]\", fillcolor=\"#e5813986\"] ;\\n' \\","161","                '[0, 1, 0]]\", fillcolor=\"#e58139ff\"] ;\\n' \\","164","                '[0.0, 0.0, 0.5]]\", fillcolor=\"#e58139ff\"] ;\\n' \\","186","                'value = 0.0\", fillcolor=\"#e5813980\"] ;\\n' \\","188","                'fillcolor=\"#e5813900\"] ;\\n' \\","192","                'fillcolor=\"#e58139ff\"] ;\\n' \\","209","                'fillcolor=\"#e5813900\"] ;\\n' \\"]}],"sklearn\/tree\/export.py":[{"add":["12","import warnings","23","from ._reingold_tilford import buchheim, Tree","76","def plot_tree(decision_tree, max_depth=None, feature_names=None,","77","              class_names=None, label='all', filled=False,","78","              impurity=True, node_ids=False,","79","              proportion=False, rotate=False, rounded=False,","80","              precision=3, ax=None, fontsize=None):","81","    \"\"\"Plot a decision tree.","82","","83","    The sample counts that are shown are weighted with any sample_weights that","84","    might be present.","85","    This function requires matplotlib, and works best with matplotlib >= 1.5.","86","","87","    The visualization is fit automatically to the size of the axis.","88","    Use the ``figsize`` or ``dpi`` arguments of ``plt.figure``  to control","89","    the size of the rendering.","90","","91","    Read more in the :ref:`User Guide <tree>`.","92","","93","    .. versionadded:: 0.21","94","","95","    Parameters","96","    ----------","97","    decision_tree : decision tree regressor or classifier","98","        The decision tree to be exported to GraphViz.","99","","100","    max_depth : int, optional (default=None)","101","        The maximum depth of the representation. If None, the tree is fully","102","        generated.","103","","104","    feature_names : list of strings, optional (default=None)","105","        Names of each of the features.","106","","107","    class_names : list of strings, bool or None, optional (default=None)","108","        Names of each of the target classes in ascending numerical order.","109","        Only relevant for classification and not supported for multi-output.","110","        If ``True``, shows a symbolic representation of the class name.","111","","112","    label : {'all', 'root', 'none'}, optional (default='all')","113","        Whether to show informative labels for impurity, etc.","114","        Options include 'all' to show at every node, 'root' to show only at","115","        the top root node, or 'none' to not show at any node.","116","","117","    filled : bool, optional (default=False)","118","        When set to ``True``, paint nodes to indicate majority class for","119","        classification, extremity of values for regression, or purity of node","120","        for multi-output.","121","","122","    impurity : bool, optional (default=True)","123","        When set to ``True``, show the impurity at each node.","124","","125","    node_ids : bool, optional (default=False)","126","        When set to ``True``, show the ID number on each node.","127","","128","    proportion : bool, optional (default=False)","129","        When set to ``True``, change the display of 'values' and\/or 'samples'","130","        to be proportions and percentages respectively.","131","","132","    rotate : bool, optional (default=False)","133","        When set to ``True``, orient tree left to right rather than top-down.","134","","135","    rounded : bool, optional (default=False)","136","        When set to ``True``, draw node boxes with rounded corners and use","137","        Helvetica fonts instead of Times-Roman.","138","","139","    precision : int, optional (default=3)","140","        Number of digits of precision for floating point in the values of","141","        impurity, threshold and value attributes of each node.","142","","143","    ax : matplotlib axis, optional (default=None)","144","        Axes to plot to. If None, use current axis. Any previous content","145","        is cleared.","146","","147","    fontsize : int, optional (default=None)","148","        Size of text font. If None, determined automatically to fit figure.","149","","150","    Returns","151","    -------","152","    annotations : list of artists","153","        List containing the artists for the annotation boxes making up the","154","        tree.","155","","156","    Examples","157","    --------","158","    >>> from sklearn.datasets import load_iris","159","    >>> from sklearn import tree","160","","161","    >>> clf = tree.DecisionTreeClassifier(random_state=0)","162","    >>> iris = load_iris()","163","","164","    >>> clf = clf.fit(iris.data, iris.target)","165","    >>> tree.plot_tree(clf)  # doctest: +SKIP","166","    [Text(251.5,345.217,'X[3] <= 0.8...","167","","168","    \"\"\"","169","    exporter = _MPLTreeExporter(","170","        max_depth=max_depth, feature_names=feature_names,","171","        class_names=class_names, label=label, filled=filled,","172","        impurity=impurity, node_ids=node_ids,","173","        proportion=proportion, rotate=rotate, rounded=rounded,","174","        precision=precision, fontsize=fontsize)","175","    return exporter.export(decision_tree, ax=ax)","176","","177","","178","class _BaseTreeExporter(object):","179","    def __init__(self, max_depth=None, feature_names=None,","180","                 class_names=None, label='all', filled=False,","181","                 impurity=True, node_ids=False,","182","                 proportion=False, rotate=False, rounded=False,","183","                 precision=3, fontsize=None):","184","        self.max_depth = max_depth","185","        self.feature_names = feature_names","186","        self.class_names = class_names","187","        self.label = label","188","        self.filled = filled","189","        self.impurity = impurity","190","        self.node_ids = node_ids","191","        self.proportion = proportion","192","        self.rotate = rotate","193","        self.rounded = rounded","194","        self.precision = precision","195","        self.fontsize = fontsize","196","","197","    def get_color(self, value):","198","        # Find the appropriate color & intensity for a node","199","        if self.colors['bounds'] is None:","200","            # Classification tree","201","            color = list(self.colors['rgb'][np.argmax(value)])","202","            sorted_values = sorted(value, reverse=True)","203","            if len(sorted_values) == 1:","204","                alpha = 0","205","            else:","206","                alpha = ((sorted_values[0] - sorted_values[1])","207","                         \/ (1 - sorted_values[1]))","208","        else:","209","            # Regression tree or multi-output","210","            color = list(self.colors['rgb'][0])","211","            alpha = ((value - self.colors['bounds'][0]) \/","212","                     (self.colors['bounds'][1] - self.colors['bounds'][0]))","213","        # unpack numpy scalars","214","        alpha = float(alpha)","215","        # compute the color as alpha against white","216","        color = [int(round(alpha * c + (1 - alpha) * 255, 0)) for c in color]","217","        # Return html color code in #RRGGBB format","218","        return '#%2x%2x%2x' % tuple(color)","219","","220","    def get_fill_color(self, tree, node_id):","221","        # Fetch appropriate color for node","222","        if 'rgb' not in self.colors:","223","            # Initialize colors and bounds if required","224","            self.colors['rgb'] = _color_brew(tree.n_classes[0])","225","            if tree.n_outputs != 1:","226","                # Find max and min impurities for multi-output","227","                self.colors['bounds'] = (np.min(-tree.impurity),","228","                                         np.max(-tree.impurity))","229","            elif (tree.n_classes[0] == 1 and","230","                  len(np.unique(tree.value)) != 1):","231","                # Find max and min values in leaf nodes for regression","232","                self.colors['bounds'] = (np.min(tree.value),","233","                                         np.max(tree.value))","234","        if tree.n_outputs == 1:","235","            node_val = (tree.value[node_id][0, :] \/","236","                        tree.weighted_n_node_samples[node_id])","237","            if tree.n_classes[0] == 1:","238","                # Regression","239","                node_val = tree.value[node_id][0, :]","240","        else:","241","            # If multi-output color node by impurity","242","            node_val = -tree.impurity[node_id]","243","        return self.get_color(node_val)","244","","245","    def node_to_str(self, tree, node_id, criterion):","246","        # Generate the node content string","247","        if tree.n_outputs == 1:","248","            value = tree.value[node_id][0, :]","249","        else:","250","            value = tree.value[node_id]","251","","252","        # Should labels be shown?","253","        labels = (self.label == 'root' and node_id == 0) or self.label == 'all'","254","","255","        characters = self.characters","256","        node_string = characters[-1]","257","","258","        # Write node ID","259","        if self.node_ids:","260","            if labels:","261","                node_string += 'node '","262","            node_string += characters[0] + str(node_id) + characters[4]","263","","264","        # Write decision criteria","265","        if tree.children_left[node_id] != _tree.TREE_LEAF:","266","            # Always write node decision criteria, except for leaves","267","            if self.feature_names is not None:","268","                feature = self.feature_names[tree.feature[node_id]]","269","            else:","270","                feature = \"X%s%s%s\" % (characters[1],","271","                                       tree.feature[node_id],","272","                                       characters[2])","273","            node_string += '%s %s %s%s' % (feature,","274","                                           characters[3],","275","                                           round(tree.threshold[node_id],","276","                                                 self.precision),","277","                                           characters[4])","278","","279","        # Write impurity","280","        if self.impurity:","281","            if isinstance(criterion, _criterion.FriedmanMSE):","282","                criterion = \"friedman_mse\"","283","            elif not isinstance(criterion, six.string_types):","284","                criterion = \"impurity\"","285","            if labels:","286","                node_string += '%s = ' % criterion","287","            node_string += (str(round(tree.impurity[node_id], self.precision))","288","                            + characters[4])","289","","290","        # Write node sample count","291","        if labels:","292","            node_string += 'samples = '","293","        if self.proportion:","294","            percent = (100. * tree.n_node_samples[node_id] \/","295","                       float(tree.n_node_samples[0]))","296","            node_string += (str(round(percent, 1)) + '%' +","297","                            characters[4])","298","        else:","299","            node_string += (str(tree.n_node_samples[node_id]) +","300","                            characters[4])","301","","302","        # Write node class distribution \/ regression value","303","        if self.proportion and tree.n_classes[0] != 1:","304","            # For classification this will show the proportion of samples","305","            value = value \/ tree.weighted_n_node_samples[node_id]","306","        if labels:","307","            node_string += 'value = '","308","        if tree.n_classes[0] == 1:","309","            # Regression","310","            value_text = np.around(value, self.precision)","311","        elif self.proportion:","312","            # Classification","313","            value_text = np.around(value, self.precision)","314","        elif np.all(np.equal(np.mod(value, 1), 0)):","315","            # Classification without floating-point weights","316","            value_text = value.astype(int)","317","        else:","318","            # Classification with floating-point weights","319","            value_text = np.around(value, self.precision)","320","        # Strip whitespace","321","        value_text = str(value_text.astype('S32')).replace(\"b'\", \"'\")","322","        value_text = value_text.replace(\"' '\", \", \").replace(\"'\", \"\")","323","        if tree.n_classes[0] == 1 and tree.n_outputs == 1:","324","            value_text = value_text.replace(\"[\", \"\").replace(\"]\", \"\")","325","        value_text = value_text.replace(\"\\n \", characters[4])","326","        node_string += value_text + characters[4]","327","","328","        # Write node majority class","329","        if (self.class_names is not None and","330","                tree.n_classes[0] != 1 and","331","                tree.n_outputs == 1):","332","            # Only done for single-output classification trees","333","            if labels:","334","                node_string += 'class = '","335","            if self.class_names is not True:","336","                class_name = self.class_names[np.argmax(value)]","337","            else:","338","                class_name = \"y%s%s%s\" % (characters[1],","339","                                          np.argmax(value),","340","                                          characters[2])","341","            node_string += class_name","342","","343","        # Clean up any trailing newlines","344","        if node_string.endswith(characters[4]):","345","            node_string = node_string[:-len(characters[4])]","346","","347","        return node_string + characters[5]","348","","349","","350","class _DOTTreeExporter(_BaseTreeExporter):","351","    def __init__(self, out_file=SENTINEL, max_depth=None,","352","                 feature_names=None, class_names=None, label='all',","353","                 filled=False, leaves_parallel=False, impurity=True,","354","                 node_ids=False, proportion=False, rotate=False, rounded=False,","355","                 special_characters=False, precision=3):","356","","357","        super(_DOTTreeExporter, self).__init__(","358","            max_depth=max_depth, feature_names=feature_names,","359","            class_names=class_names, label=label, filled=filled,","360","            impurity=impurity,","361","            node_ids=node_ids, proportion=proportion, rotate=rotate,","362","            rounded=rounded,","363","            precision=precision)","364","        self.leaves_parallel = leaves_parallel","365","        self.out_file = out_file","366","        self.special_characters = special_characters","367","","368","        # PostScript compatibility for special characters","369","        if special_characters:","370","            self.characters = ['&#35;', '<SUB>', '<\/SUB>', '&le;', '<br\/>',","371","                               '>', '<']","372","        else:","373","            self.characters = ['#', '[', ']', '<=', '\\\\n', '\"', '\"']","374","","375","        # validate","376","        if isinstance(precision, Integral):","377","            if precision < 0:","378","                raise ValueError(\"'precision' should be greater or equal to 0.\"","379","                                 \" Got {} instead.\".format(precision))","380","        else:","381","            raise ValueError(\"'precision' should be an integer. Got {}\"","382","                             \" instead.\".format(type(precision)))","383","","384","        # The depth of each node for plotting with 'leaf' option","385","        self.ranks = {'leaves': []}","386","        # The colors to render each node with","387","        self.colors = {'bounds': None}","388","","389","    def export(self, decision_tree):","390","        # Check length of feature_names before getting into the tree node","391","        # Raise error if length of feature_names does not match","392","        # n_features_ in the decision_tree","393","        if self.feature_names is not None:","394","            if len(self.feature_names) != decision_tree.n_features_:","395","                raise ValueError(\"Length of feature_names, %d \"","396","                                 \"does not match number of features, %d\"","397","                                 % (len(self.feature_names),","398","                                    decision_tree.n_features_))","399","        # each part writes to out_file","400","        self.head()","401","        # Now recurse the tree and add node & edge attributes","402","        if isinstance(decision_tree, _tree.Tree):","403","            self.recurse(decision_tree, 0, criterion=\"impurity\")","404","        else:","405","            self.recurse(decision_tree.tree_, 0,","406","                         criterion=decision_tree.criterion)","407","","408","        self.tail()","409","","410","    def tail(self):","411","        # If required, draw leaf nodes at same depth as each other","412","        if self.leaves_parallel:","413","            for rank in sorted(self.ranks):","414","                self.out_file.write(","415","                    \"{rank=same ; \" +","416","                    \"; \".join(r for r in self.ranks[rank]) + \"} ;\\n\")","417","        self.out_file.write(\"}\")","418","","419","    def head(self):","420","        self.out_file.write('digraph Tree {\\n')","421","","422","        # Specify node aesthetics","423","        self.out_file.write('node [shape=box')","424","        rounded_filled = []","425","        if self.filled:","426","            rounded_filled.append('filled')","427","        if self.rounded:","428","            rounded_filled.append('rounded')","429","        if len(rounded_filled) > 0:","430","            self.out_file.write(","431","                ', style=\"%s\", color=\"black\"'","432","                % \", \".join(rounded_filled))","433","        if self.rounded:","434","            self.out_file.write(', fontname=helvetica')","435","        self.out_file.write('] ;\\n')","436","","437","        # Specify graph & edge aesthetics","438","        if self.leaves_parallel:","439","            self.out_file.write(","440","                'graph [ranksep=equally, splines=polyline] ;\\n')","441","        if self.rounded:","442","            self.out_file.write('edge [fontname=helvetica] ;\\n')","443","        if self.rotate:","444","            self.out_file.write('rankdir=LR ;\\n')","445","","446","    def recurse(self, tree, node_id, criterion, parent=None, depth=0):","447","        if node_id == _tree.TREE_LEAF:","448","            raise ValueError(\"Invalid node_id %s\" % _tree.TREE_LEAF)","449","","450","        left_child = tree.children_left[node_id]","451","        right_child = tree.children_right[node_id]","452","","453","        # Add node with description","454","        if self.max_depth is None or depth <= self.max_depth:","455","","456","            # Collect ranks for 'leaf' option in plot_options","457","            if left_child == _tree.TREE_LEAF:","458","                self.ranks['leaves'].append(str(node_id))","459","            elif str(depth) not in self.ranks:","460","                self.ranks[str(depth)] = [str(node_id)]","461","            else:","462","                self.ranks[str(depth)].append(str(node_id))","463","","464","            self.out_file.write(","465","                '%d [label=%s' % (node_id, self.node_to_str(tree, node_id,","466","                                                            criterion)))","467","","468","            if self.filled:","469","                self.out_file.write(', fillcolor=\"%s\"'","470","                                    % self.get_fill_color(tree, node_id))","471","            self.out_file.write('] ;\\n')","472","","473","            if parent is not None:","474","                # Add edge to parent","475","                self.out_file.write('%d -> %d' % (parent, node_id))","476","                if parent == 0:","477","                    # Draw True\/False labels if parent is root node","478","                    angles = np.array([45, -45]) * ((self.rotate - .5) * -2)","479","                    self.out_file.write(' [labeldistance=2.5, labelangle=')","480","                    if node_id == 1:","481","                        self.out_file.write('%d, headlabel=\"True\"]' %","482","                                            angles[0])","483","                    else:","484","                        self.out_file.write('%d, headlabel=\"False\"]' %","485","                                            angles[1])","486","                self.out_file.write(' ;\\n')","487","","488","            if left_child != _tree.TREE_LEAF:","489","                self.recurse(tree, left_child, criterion=criterion,","490","                             parent=node_id, depth=depth + 1)","491","                self.recurse(tree, right_child, criterion=criterion,","492","                             parent=node_id, depth=depth + 1)","493","","494","        else:","495","            self.ranks['leaves'].append(str(node_id))","496","","497","            self.out_file.write('%d [label=\"(...)\"' % node_id)","498","            if self.filled:","499","                # color cropped nodes grey","500","                self.out_file.write(', fillcolor=\"#C0C0C0\"')","501","            self.out_file.write('] ;\\n' % node_id)","502","","503","            if parent is not None:","504","                # Add edge to parent","505","                self.out_file.write('%d -> %d ;\\n' % (parent, node_id))","506","","507","","508","class _MPLTreeExporter(_BaseTreeExporter):","509","    def __init__(self, max_depth=None, feature_names=None,","510","                 class_names=None, label='all', filled=False,","511","                 impurity=True, node_ids=False,","512","                 proportion=False, rotate=False, rounded=False,","513","                 precision=3, fontsize=None):","514","","515","        super(_MPLTreeExporter, self).__init__(","516","            max_depth=max_depth, feature_names=feature_names,","517","            class_names=class_names, label=label, filled=filled,","518","            impurity=impurity, node_ids=node_ids, proportion=proportion,","519","            rotate=rotate, rounded=rounded, precision=precision)","520","        self.fontsize = fontsize","521","","522","        # validate","523","        if isinstance(precision, Integral):","524","            if precision < 0:","525","                raise ValueError(\"'precision' should be greater or equal to 0.\"","526","                                 \" Got {} instead.\".format(precision))","527","        else:","528","            raise ValueError(\"'precision' should be an integer. Got {}\"","529","                             \" instead.\".format(type(precision)))","530","","531","        # The depth of each node for plotting with 'leaf' option","532","        self.ranks = {'leaves': []}","533","        # The colors to render each node with","534","        self.colors = {'bounds': None}","535","","536","        self.characters = ['#', '[', ']', '<=', '\\n', '', '']","537","","538","        self.bbox_args = dict(fc='w')","539","        if self.rounded:","540","            self.bbox_args['boxstyle'] = \"round\"","541","        else:","542","            # matplotlib <1.5 requires explicit boxstyle","543","            self.bbox_args['boxstyle'] = \"square\"","544","","545","        self.arrow_args = dict(arrowstyle=\"<-\")","546","","547","    def _make_tree(self, node_id, et, depth=0):","548","        # traverses _tree.Tree recursively, builds intermediate","549","        # \"_reingold_tilford.Tree\" object","550","        name = self.node_to_str(et, node_id, criterion='entropy')","551","        if (et.children_left[node_id] != _tree.TREE_LEAF","552","                and (self.max_depth is None or depth <= self.max_depth)):","553","            children = [self._make_tree(et.children_left[node_id], et,","554","                                        depth=depth + 1),","555","                        self._make_tree(et.children_right[node_id], et,","556","                                        depth=depth + 1)]","557","        else:","558","            return Tree(name, node_id)","559","        return Tree(name, node_id, *children)","560","","561","    def export(self, decision_tree, ax=None):","562","        import matplotlib.pyplot as plt","563","        from matplotlib.text import Annotation","564","        if ax is None:","565","            ax = plt.gca()","566","        ax.clear()","567","        ax.set_axis_off()","568","        my_tree = self._make_tree(0, decision_tree.tree_)","569","        draw_tree = buchheim(my_tree)","570","","571","        # important to make sure we're still","572","        # inside the axis after drawing the box","573","        # this makes sense because the width of a box","574","        # is about the same as the distance between boxes","575","        max_x, max_y = draw_tree.max_extents() + 1","576","        ax_width = ax.get_window_extent().width","577","        ax_height = ax.get_window_extent().height","578","","579","        scale_x = ax_width \/ max_x","580","        scale_y = ax_height \/ max_y","581","","582","        self.recurse(draw_tree, decision_tree.tree_, ax,","583","                     scale_x, scale_y, ax_height)","584","","585","        anns = [ann for ann in ax.get_children()","586","                if isinstance(ann, Annotation)]","587","","588","        # update sizes of all bboxes","589","        renderer = ax.figure.canvas.get_renderer()","590","","591","        for ann in anns:","592","            ann.update_bbox_position_size(renderer)","593","","594","        if self.fontsize is None:","595","            # get figure to data transform","596","            # adjust fontsize to avoid overlap","597","            # get max box width and height","598","            try:","599","                extents = [ann.get_bbox_patch().get_window_extent()","600","                           for ann in anns]","601","                max_width = max([extent.width for extent in extents])","602","                max_height = max([extent.height for extent in extents])","603","                # width should be around scale_x in axis coordinates","604","                size = anns[0].get_fontsize() * min(scale_x \/ max_width,","605","                                                    scale_y \/ max_height)","606","                for ann in anns:","607","                    ann.set_fontsize(size)","608","            except AttributeError:","609","                # matplotlib < 1.5","610","                warnings.warn(\"Automatic scaling of tree plots requires \"","611","                              \"matplotlib 1.5 or higher. Please specify \"","612","                              \"fontsize.\")","613","","614","        return anns","615","","616","    def recurse(self, node, tree, ax, scale_x, scale_y, height, depth=0):","617","        # need to copy bbox args because matplotib <1.5 modifies them","618","        kwargs = dict(bbox=self.bbox_args.copy(), ha='center', va='center',","619","                      zorder=100 - 10 * depth, xycoords='axes pixels')","620","","621","        if self.fontsize is not None:","622","            kwargs['fontsize'] = self.fontsize","623","","624","        # offset things by .5 to center them in plot","625","        xy = ((node.x + .5) * scale_x, height - (node.y + .5) * scale_y)","626","","627","        if self.max_depth is None or depth <= self.max_depth:","628","            if self.filled:","629","                kwargs['bbox']['fc'] = self.get_fill_color(tree,","630","                                                           node.tree.node_id)","631","            if node.parent is None:","632","                # root","633","                ax.annotate(node.tree.label, xy, **kwargs)","634","            else:","635","                xy_parent = ((node.parent.x + .5) * scale_x,","636","                             height - (node.parent.y + .5) * scale_y)","637","                kwargs[\"arrowprops\"] = self.arrow_args","638","                ax.annotate(node.tree.label, xy_parent, xy, **kwargs)","639","            for child in node.children:","640","                self.recurse(child, tree, ax, scale_x, scale_y, height,","641","                             depth=depth + 1)","642","","643","        else:","644","            xy_parent = ((node.parent.x + .5) * scale_x,","645","                         height - (node.parent.y + .5) * scale_y)","646","            kwargs[\"arrowprops\"] = self.arrow_args","647","            kwargs['bbox']['fc'] = 'grey'","648","            ax.annotate(\"\\n  (...)  \\n\", xy_parent, xy, **kwargs)","649","","650","","672","    decision_tree : decision tree classifier","749","    >>> tree.export_graphviz(clf) # doctest: +ELLIPSIS","750","    'digraph Tree {...","768","        exporter = _DOTTreeExporter(","769","            out_file=out_file, max_depth=max_depth,","770","            feature_names=feature_names, class_names=class_names, label=label,","771","            filled=filled, leaves_parallel=leaves_parallel, impurity=impurity,","772","            node_ids=node_ids, proportion=proportion, rotate=rotate,","773","            rounded=rounded, special_characters=special_characters,","774","            precision=precision)","775","        exporter.export(decision_tree)","778","            return exporter.out_file.getvalue()"],"delete":["95","    decision_tree : decision tree regressor or classifier","172","    >>> tree.export_graphviz(clf,","173","    ...     out_file='tree.dot')                # doctest: +SKIP","174","","177","    def get_color(value):","178","        # Find the appropriate color & intensity for a node","179","        if colors['bounds'] is None:","180","            # Classification tree","181","            color = list(colors['rgb'][np.argmax(value)])","182","            sorted_values = sorted(value, reverse=True)","183","            if len(sorted_values) == 1:","184","                alpha = 0","185","            else:","186","                alpha = int(np.round(255 * (sorted_values[0] -","187","                                            sorted_values[1]) \/","188","                                           (1 - sorted_values[1]), 0))","189","        else:","190","            # Regression tree or multi-output","191","            color = list(colors['rgb'][0])","192","            alpha = int(np.round(255 * ((value - colors['bounds'][0]) \/","193","                                        (colors['bounds'][1] -","194","                                         colors['bounds'][0])), 0))","195","","196","        # Return html color code in #RRGGBBAA format","197","        color.append(alpha)","198","        hex_codes = [str(i) for i in range(10)]","199","        hex_codes.extend(['a', 'b', 'c', 'd', 'e', 'f'])","200","        color = [hex_codes[c \/\/ 16] + hex_codes[c % 16] for c in color]","201","","202","        return '#' + ''.join(color)","203","","204","    def node_to_str(tree, node_id, criterion):","205","        # Generate the node content string","206","        if tree.n_outputs == 1:","207","            value = tree.value[node_id][0, :]","208","        else:","209","            value = tree.value[node_id]","210","","211","        # Should labels be shown?","212","        labels = (label == 'root' and node_id == 0) or label == 'all'","213","","214","        # PostScript compatibility for special characters","215","        if special_characters:","216","            characters = ['&#35;', '<SUB>', '<\/SUB>', '&le;', '<br\/>', '>']","217","            node_string = '<'","218","        else:","219","            characters = ['#', '[', ']', '<=', '\\\\n', '\"']","220","            node_string = '\"'","221","","222","        # Write node ID","223","        if node_ids:","224","            if labels:","225","                node_string += 'node '","226","            node_string += characters[0] + str(node_id) + characters[4]","227","","228","        # Write decision criteria","229","        if tree.children_left[node_id] != _tree.TREE_LEAF:","230","            # Always write node decision criteria, except for leaves","231","            if feature_names is not None:","232","                feature = feature_names[tree.feature[node_id]]","233","            else:","234","                feature = \"X%s%s%s\" % (characters[1],","235","                                       tree.feature[node_id],","236","                                       characters[2])","237","            node_string += '%s %s %s%s' % (feature,","238","                                           characters[3],","239","                                           round(tree.threshold[node_id],","240","                                                 precision),","241","                                           characters[4])","242","","243","        # Write impurity","244","        if impurity:","245","            if isinstance(criterion, _criterion.FriedmanMSE):","246","                criterion = \"friedman_mse\"","247","            elif not isinstance(criterion, six.string_types):","248","                criterion = \"impurity\"","249","            if labels:","250","                node_string += '%s = ' % criterion","251","            node_string += (str(round(tree.impurity[node_id], precision)) +","252","                            characters[4])","253","","254","        # Write node sample count","255","        if labels:","256","            node_string += 'samples = '","257","        if proportion:","258","            percent = (100. * tree.n_node_samples[node_id] \/","259","                       float(tree.n_node_samples[0]))","260","            node_string += (str(round(percent, 1)) + '%' +","261","                            characters[4])","262","        else:","263","            node_string += (str(tree.n_node_samples[node_id]) +","264","                            characters[4])","265","","266","        # Write node class distribution \/ regression value","267","        if proportion and tree.n_classes[0] != 1:","268","            # For classification this will show the proportion of samples","269","            value = value \/ tree.weighted_n_node_samples[node_id]","270","        if labels:","271","            node_string += 'value = '","272","        if tree.n_classes[0] == 1:","273","            # Regression","274","            value_text = np.around(value, precision)","275","        elif proportion:","276","            # Classification","277","            value_text = np.around(value, precision)","278","        elif np.all(np.equal(np.mod(value, 1), 0)):","279","            # Classification without floating-point weights","280","            value_text = value.astype(int)","281","        else:","282","            # Classification with floating-point weights","283","            value_text = np.around(value, precision)","284","        # Strip whitespace","285","        value_text = str(value_text.astype('S32')).replace(\"b'\", \"'\")","286","        value_text = value_text.replace(\"' '\", \", \").replace(\"'\", \"\")","287","        if tree.n_classes[0] == 1 and tree.n_outputs == 1:","288","            value_text = value_text.replace(\"[\", \"\").replace(\"]\", \"\")","289","        value_text = value_text.replace(\"\\n \", characters[4])","290","        node_string += value_text + characters[4]","291","","292","        # Write node majority class","293","        if (class_names is not None and","294","                tree.n_classes[0] != 1 and","295","                tree.n_outputs == 1):","296","            # Only done for single-output classification trees","297","            if labels:","298","                node_string += 'class = '","299","            if class_names is not True:","300","                class_name = class_names[np.argmax(value)]","301","            else:","302","                class_name = \"y%s%s%s\" % (characters[1],","303","                                          np.argmax(value),","304","                                          characters[2])","305","            node_string += class_name","306","","307","        # Clean up any trailing newlines","308","        if node_string[-2:] == '\\\\n':","309","            node_string = node_string[:-2]","310","        if node_string[-5:] == '<br\/>':","311","            node_string = node_string[:-5]","312","","313","        return node_string + characters[5]","314","","315","    def recurse(tree, node_id, criterion, parent=None, depth=0):","316","        if node_id == _tree.TREE_LEAF:","317","            raise ValueError(\"Invalid node_id %s\" % _tree.TREE_LEAF)","318","","319","        left_child = tree.children_left[node_id]","320","        right_child = tree.children_right[node_id]","321","","322","        # Add node with description","323","        if max_depth is None or depth <= max_depth:","324","","325","            # Collect ranks for 'leaf' option in plot_options","326","            if left_child == _tree.TREE_LEAF:","327","                ranks['leaves'].append(str(node_id))","328","            elif str(depth) not in ranks:","329","                ranks[str(depth)] = [str(node_id)]","330","            else:","331","                ranks[str(depth)].append(str(node_id))","332","","333","            out_file.write('%d [label=%s'","334","                           % (node_id,","335","                              node_to_str(tree, node_id, criterion)))","336","","337","            if filled:","338","                # Fetch appropriate color for node","339","                if 'rgb' not in colors:","340","                    # Initialize colors and bounds if required","341","                    colors['rgb'] = _color_brew(tree.n_classes[0])","342","                    if tree.n_outputs != 1:","343","                        # Find max and min impurities for multi-output","344","                        colors['bounds'] = (np.min(-tree.impurity),","345","                                            np.max(-tree.impurity))","346","                    elif (tree.n_classes[0] == 1 and","347","                          len(np.unique(tree.value)) != 1):","348","                        # Find max and min values in leaf nodes for regression","349","                        colors['bounds'] = (np.min(tree.value),","350","                                            np.max(tree.value))","351","                if tree.n_outputs == 1:","352","                    node_val = (tree.value[node_id][0, :] \/","353","                                tree.weighted_n_node_samples[node_id])","354","                    if tree.n_classes[0] == 1:","355","                        # Regression","356","                        node_val = tree.value[node_id][0, :]","357","                else:","358","                    # If multi-output color node by impurity","359","                    node_val = -tree.impurity[node_id]","360","                out_file.write(', fillcolor=\"%s\"' % get_color(node_val))","361","            out_file.write('] ;\\n')","362","","363","            if parent is not None:","364","                # Add edge to parent","365","                out_file.write('%d -> %d' % (parent, node_id))","366","                if parent == 0:","367","                    # Draw True\/False labels if parent is root node","368","                    angles = np.array([45, -45]) * ((rotate - .5) * -2)","369","                    out_file.write(' [labeldistance=2.5, labelangle=')","370","                    if node_id == 1:","371","                        out_file.write('%d, headlabel=\"True\"]' % angles[0])","372","                    else:","373","                        out_file.write('%d, headlabel=\"False\"]' % angles[1])","374","                out_file.write(' ;\\n')","375","","376","            if left_child != _tree.TREE_LEAF:","377","                recurse(tree, left_child, criterion=criterion, parent=node_id,","378","                        depth=depth + 1)","379","                recurse(tree, right_child, criterion=criterion, parent=node_id,","380","                        depth=depth + 1)","381","","382","        else:","383","            ranks['leaves'].append(str(node_id))","384","","385","            out_file.write('%d [label=\"(...)\"' % node_id)","386","            if filled:","387","                # color cropped nodes grey","388","                out_file.write(', fillcolor=\"#C0C0C0\"')","389","            out_file.write('] ;\\n' % node_id)","390","","391","            if parent is not None:","392","                # Add edge to parent","393","                out_file.write('%d -> %d ;\\n' % (parent, node_id))","394","","410","        if isinstance(precision, Integral):","411","            if precision < 0:","412","                raise ValueError(\"'precision' should be greater or equal to 0.\"","413","                                 \" Got {} instead.\".format(precision))","414","        else:","415","            raise ValueError(\"'precision' should be an integer. Got {}\"","416","                             \" instead.\".format(type(precision)))","417","","418","        # Check length of feature_names before getting into the tree node","419","        # Raise error if length of feature_names does not match","420","        # n_features_ in the decision_tree","421","        if feature_names is not None:","422","            if len(feature_names) != decision_tree.n_features_:","423","                raise ValueError(\"Length of feature_names, %d \"","424","                                 \"does not match number of features, %d\"","425","                                 % (len(feature_names),","426","                                    decision_tree.n_features_))","427","","428","        # The depth of each node for plotting with 'leaf' option","429","        ranks = {'leaves': []}","430","        # The colors to render each node with","431","        colors = {'bounds': None}","432","","433","        out_file.write('digraph Tree {\\n')","434","","435","        # Specify node aesthetics","436","        out_file.write('node [shape=box')","437","        rounded_filled = []","438","        if filled:","439","            rounded_filled.append('filled')","440","        if rounded:","441","            rounded_filled.append('rounded')","442","        if len(rounded_filled) > 0:","443","            out_file.write(', style=\"%s\", color=\"black\"'","444","                           % \", \".join(rounded_filled))","445","        if rounded:","446","            out_file.write(', fontname=helvetica')","447","        out_file.write('] ;\\n')","448","","449","        # Specify graph & edge aesthetics","450","        if leaves_parallel:","451","            out_file.write('graph [ranksep=equally, splines=polyline] ;\\n')","452","        if rounded:","453","            out_file.write('edge [fontname=helvetica] ;\\n')","454","        if rotate:","455","            out_file.write('rankdir=LR ;\\n')","456","","457","        # Now recurse the tree and add node & edge attributes","458","        recurse(decision_tree.tree_, 0, criterion=decision_tree.criterion)","459","","460","        # If required, draw leaf nodes at same depth as each other","461","        if leaves_parallel:","462","            for rank in sorted(ranks):","463","                out_file.write(\"{rank=same ; \" +","464","                               \"; \".join(r for r in ranks[rank]) + \"} ;\\n\")","465","        out_file.write(\"}\")","468","            return out_file.getvalue()"]}],"sklearn\/tree\/tests\/test_reingold_tilford.py":[{"add":[],"delete":[]}],"examples\/tree\/plot_iris.py":[{"add":["13","","14","We also show the tree structure of a model built on all of the features.","22","from sklearn.tree import DecisionTreeClassifier, plot_tree","66","","67","plt.figure()","68","clf = DecisionTreeClassifier().fit(iris.data, iris.target)","69","plot_tree(clf, filled=True)"],"delete":["20","from sklearn.tree import DecisionTreeClassifier"]}],"doc\/modules\/tree.rst":[{"add":["126","Once trained, you can plot the tree with the plot_tree function::","127","","128","","129","    >>> tree.plot_tree(clf.fit(iris.data, iris.target)) # doctest: +SKIP","130","","131",".. figure:: ..\/auto_examples\/tree\/images\/sphx_glr_plot_iris_002.png","132","   :target: ..\/auto_examples\/tree\/plot_iris.html","133","   :scale: 75","134","   :align: center","135","","136","We can also export the tree in `Graphviz","138","exporter. If you use the `conda <https:\/\/conda.io>`_ package manager, the graphviz binaries  ","139",""],"delete":["126","Once trained, we can export the tree in `Graphviz","128","exporter. If you use the `conda <https:\/\/conda.io\/>`_ package manager, the graphviz binaries  "]}]}},"0bfe10d1fa0e2bc275a2539f08ad5cf57bf5e4d7":{"changes":{"sklearn\/ensemble\/voting_classifier.py":"MODIFY","sklearn\/ensemble\/tests\/test_voting_classifier.py":"MODIFY"},"diff":{"sklearn\/ensemble\/voting_classifier.py":[{"add":["25","def _parallel_fit_estimator(estimator, X, y, sample_weight=None):","28","        estimator.fit(X, y, sample_weight=sample_weight)","187","                                                 sample_weight=sample_weight)"],"delete":["25","def _parallel_fit_estimator(estimator, X, y, sample_weight):","28","        estimator.fit(X, y, sample_weight)","187","                                                 sample_weight)"]}],"sklearn\/ensemble\/tests\/test_voting_classifier.py":[{"add":["19","from sklearn.base import BaseEstimator, ClassifierMixin","277","def test_sample_weight_kwargs():","278","    \"\"\"Check that VotingClassifier passes sample_weight as kwargs\"\"\"","279","    class MockClassifier(BaseEstimator, ClassifierMixin):","280","        \"\"\"Mock Classifier to check that sample_weight is received as kwargs\"\"\"","281","        def fit(self, X, y, *args, **sample_weight):","282","            assert_true('sample_weight' in sample_weight)","283","","284","    clf = MockClassifier()","285","    eclf = VotingClassifier(estimators=[('mock', clf)], voting='soft')","286","","287","    # Should not raise an error.","288","    eclf.fit(X, y, sample_weight=np.ones((len(y),)))","289","","290",""],"delete":[]}]}},"32ac22870d004b52b06d12f18ff3ffbf20862b3b":{"changes":{"examples\/tree\/plot_tree_regression_multioutput.py":"MODIFY","sklearn\/decomposition\/factor_analysis.py":"MODIFY","examples\/cluster\/plot_dict_face_patches.py":"MODIFY","examples\/gaussian_process\/plot_gpr_noisy_targets.py":"MODIFY","sklearn\/gaussian_process\/gaussian_process.py":"MODIFY","sklearn\/decomposition\/dict_learning.py":"MODIFY","examples\/neighbors\/plot_digits_kde_sampling.py":"MODIFY","examples\/ensemble\/plot_forest_iris.py":"MODIFY","sklearn\/mixture\/dpgmm.py":"MODIFY","examples\/gaussian_process\/plot_gpc_isoprobability.py":"MODIFY","sklearn\/utils\/extmath.py":"MODIFY","examples\/cluster\/plot_kmeans_stability_low_dim_dense.py":"MODIFY","examples\/decomposition\/plot_pca_3d.py":"MODIFY","examples\/cluster\/plot_color_quantization.py":"MODIFY","sklearn\/decomposition\/pca.py":"MODIFY","examples\/linear_model\/plot_lasso_coordinate_descent_path.py":"MODIFY","sklearn\/linear_model\/least_angle.py":"MODIFY"},"diff":{"examples\/tree\/plot_tree_regression_multioutput.py":[{"add":[],"delete":["44","s = 50"]}],"sklearn\/decomposition\/factor_analysis.py":[{"add":[],"delete":["328","        log_like = np.zeros(X.shape[0])"]}],"examples\/cluster\/plot_dict_face_patches.py":[{"add":[],"delete":["43","index = 1"]}],"examples\/gaussian_process\/plot_gpr_noisy_targets.py":[{"add":["63","plt.figure()","99","plt.figure()"],"delete":["63","fig = plt.figure()","99","fig = plt.figure()"]}],"sklearn\/gaussian_process\/gaussian_process.py":[{"add":[],"delete":["446","            # Initialize output","447","            y = np.zeros(n_eval)","448","            if eval_MSE:","449","                MSE = np.zeros(n_eval)","450",""]}],"sklearn\/decomposition\/dict_learning.py":[{"add":[],"delete":["826","        n_samples, n_features = X.shape"]}],"examples\/neighbors\/plot_digits_kde_sampling.py":[{"add":[],"delete":["22","data = digits.data"]}],"examples\/ensemble\/plot_forest_iris.py":[{"add":["91","        model.fit(X, y)","93","        scores = model.score(X, y)"],"delete":["48","from sklearn import clone","92","        clf = clone(model)","93","        clf = model.fit(X, y)","95","        scores = clf.score(X, y)"]}],"sklearn\/mixture\/dpgmm.py":[{"add":[],"delete":["275","        z = np.zeros((X.shape[0], self.n_components))","846","        logprior = 0."]}],"examples\/gaussian_process\/plot_gpc_isoprobability.py":[{"add":["87","plt.contour(x1, x2, y_true, [0.], colors='k', linestyles='dashdot')"],"delete":["87","cs = plt.contour(x1, x2, y_true, [0.], colors='k', linestyles='dashdot')"]}],"sklearn\/utils\/extmath.py":[{"add":[],"delete":["423","        axis = axis"]}],"examples\/cluster\/plot_kmeans_stability_low_dim_dense.py":[{"add":["71","plt.figure()","107","plt.figure()"],"delete":["71","fig = plt.figure()","107","fig = plt.figure()"]}],"examples\/decomposition\/plot_pca_3d.py":[{"add":[],"delete":["75","    x_pca_axis, y_pca_axis, z_pca_axis = V.T * pca_score \/ pca_score.min()","76",""]}],"examples\/cluster\/plot_color_quantization.py":[{"add":[],"delete":["86","ax = plt.axes([0, 0, 1, 1])","93","ax = plt.axes([0, 0, 1, 1])","100","ax = plt.axes([0, 0, 1, 1])"]}],"sklearn\/decomposition\/pca.py":[{"add":[],"delete":["552","        log_like = np.zeros(X.shape[0])"]}],"examples\/linear_model\/plot_lasso_coordinate_descent_path.py":[{"add":[],"delete":["49","ax = plt.gca()","50","","66","ax = plt.gca()","80","ax = plt.gca()"]}],"sklearn\/linear_model\/least_angle.py":[{"add":[],"delete":["416","            alpha = alphas[n_iter, np.newaxis]","417","            prev_alpha = alphas[n_iter - 1, np.newaxis]"]}]}},"a01443b70d29301d254f05c42baa1fe04abde642":{"changes":{"sklearn\/_isotonic.pyx":"MODIFY","doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/tests\/test_isotonic.py":"MODIFY"},"diff":{"sklearn\/_isotonic.pyx":[{"add":["102","            weights_out[i] = current_weight","115","    weights_out[i] = current_weight"],"delete":["102","            weights_out[i] = current_weight \/ current_count","115","    weights_out[i] = current_weight \/ current_count"]}],"doc\/whats_new\/v0.20.rst":[{"add":["18","- :class:`isotonic.IsotonicRegression` (bug fix)","71","Classifiers and regressors","72","","73","- Fixed a bug in :class:`isotonic.IsotonicRegression` which incorrectly","74","  combined weights when fitting a model to data involving points with","75","  identical X values.","76","  :issue:`9432` by :user:`Dallas Card <dallascard>`","77",""],"delete":[]}],"sklearn\/tests\/test_isotonic.py":[{"add":["168","def test_isotonic_regression_with_ties_in_differently_sized_groups():","169","    \"\"\"","170","    Non-regression test to handle issue 9432:","171","    https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/9432","172","","173","    Compare against output in R:","174","    > library(\"isotone\")","175","    > x <- c(0, 1, 1, 2, 3, 4)","176","    > y <- c(0, 0, 1, 0, 0, 1)","177","    > res1 <- gpava(x, y, ties=\"secondary\")","178","    > res1$x","179","","180","    `isotone` version: 1.1-0, 2015-07-24","181","    R version: R version 3.3.2 (2016-10-31)","182","    \"\"\"","183","    x = np.array([0, 1, 1, 2, 3, 4])","184","    y = np.array([0, 0, 1, 0, 0, 1])","185","    y_true = np.array([0., 0.25, 0.25, 0.25, 0.25, 1.])","186","    ir = IsotonicRegression()","187","    ir.fit(x, y)","188","    assert_array_almost_equal(ir.transform(x), y_true)","189","    assert_array_almost_equal(ir.fit_transform(x, y), y_true)","190","","191",""],"delete":[]}]}},"e33e84dbf7281c4e090df176b7d82e7152a1078f":{"changes":{"examples\/applications\/plot_tomography_l1_reconstruction.py":"MODIFY","examples\/linear_model\/plot_sparse_logistic_regression_mnist.py":"MODIFY"},"diff":{"examples\/applications\/plot_tomography_l1_reconstruction.py":[{"add":["103","    mask_outer = (x - l \/ 2.) ** 2 + (y - l \/ 2.) ** 2 < (l \/ 2.) ** 2"],"delete":["103","    mask_outer = (x - l \/ 2) ** 2 + (y - l \/ 2) ** 2 < (l \/ 2) ** 2"]}],"examples\/linear_model\/plot_sparse_logistic_regression_mnist.py":[{"add":["54","clf = LogisticRegression(C=50. \/ train_samples,"],"delete":["54","clf = LogisticRegression(C=50 \/ train_samples,"]}]}},"60e7e15cdebd6b5a794d10a133fef2c937541f9d":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/decomposition\/pca.py":"MODIFY","sklearn\/decomposition\/tests\/test_pca.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["109","- Fixed a bug in :class:`decomposition.PCA` where users will get unexpected error","110","  with large datasets when ``n_components='mle'`` on Python 3 versions.","111","  :issue:`9886` by :user:`Hanmin Qin <qinhanmin2014>`.","112",""],"delete":[]}],"sklearn\/decomposition\/pca.py":[{"add":["132","        If ``n_components == 'mle'`` and ``svd_solver == 'full'``, Minka\\'s","133","        MLE is used to guess the dimension. Use of ``n_components == 'mle'``","134","        will interpret ``svd_solver == 'auto'`` as ``svd_solver == 'full'``.","135","","136","        If ``0 < n_components < 1`` and ``svd_solver == 'full'``, select the","137","        number of components such that the amount of variance that needs to be","139","","140","        If ``svd_solver == 'arpack'``, the number of components must be","141","        strictly less than the minimum of n_features and n_samples.","142","","143","        Hence, the None case results in::","392","            # Small problem or n_components == 'mle', just call full PCA","393","            if max(X.shape) <= 500 or n_components == 'mle':"],"delete":["132","        if n_components == 'mle' and svd_solver == 'full', Minka\\'s MLE is used","133","        to guess the dimension","134","        if ``0 < n_components < 1`` and svd_solver == 'full', select the number","135","        of components such that the amount of variance that needs to be","137","        If svd_solver == 'arpack', the number of components must be strictly","138","        less than the minimum of n_features and n_samples.","139","        Hence, the None case results in:","388","            # Small problem, just call full PCA","389","            if max(X.shape) <= 500:"]}],"sklearn\/decomposition\/tests\/test_pca.py":[{"add":["9","from sklearn.utils.testing import assert_raise_message","456","def test_n_components_mle():","457","    # Ensure that n_components == 'mle' doesn't raise error for auto\/full","458","    # svd_solver and raises error for arpack\/randomized svd_solver","459","    rng = np.random.RandomState(0)","460","    n_samples = 600","461","    n_features = 10","462","    X = rng.randn(n_samples, n_features)","463","    n_components_dict = {}","464","    for solver in solver_list:","465","        pca = PCA(n_components='mle', svd_solver=solver)","466","        if solver in ['auto', 'full']:","467","            pca.fit(X)","468","            n_components_dict[solver] = pca.n_components_","469","        else:  # arpack\/randomized solver","470","            error_message = (\"n_components='mle' cannot be a string with \"","471","                             \"svd_solver='{}'\".format(solver))","472","            assert_raise_message(ValueError, error_message, pca.fit, X)","473","    assert_equal(n_components_dict['auto'], n_components_dict['full'])","474","","475",""],"delete":[]}]}},"4a4b711a42492b783511cba8bf170e966531c06a":{"changes":{"sklearn\/ensemble\/forest.py":"MODIFY"},"diff":{"sklearn\/ensemble\/forest.py":[{"add":["45","import threading","381","def accumulate_prediction(predict, X, out, lock):","383","    with lock:","384","        if len(out) == 1:","385","            out[0] += prediction","386","        else:","387","            for i in range(len(out)):","388","                out[i] += prediction[i]","585","        lock = threading.Lock()","587","            delayed(accumulate_prediction)(e.predict_proba, X, all_proba, lock)","692","        lock = threading.Lock()","694","            delayed(accumulate_prediction)(e.predict, X, [y_hat], lock)"],"delete":["380","def accumulate_prediction(predict, X, out):","382","    if len(out) == 1:","383","        out[0] += prediction","384","    else:","385","        for i in range(len(out)):","386","            out[i] += prediction[i]","584","            delayed(accumulate_prediction)(e.predict_proba, X, all_proba)","690","            delayed(accumulate_prediction)(e.predict, X, [y_hat])"]}]}},"d8c363f296948a9171ac8a5d69f79dcb56589335":{"changes":{".mailmap":"MODIFY"},"diff":{".mailmap":[{"add":["28","Denis Engemann <denis-alexander.engemann@inria.fr> dengemann <denis.engemann@gmail.com>"],"delete":["28","Denis Engemann <denis-alexander.engemann@inria.fr> <dengemann <denis.engemann@gmail.com>"]}]}},"823382995c046d98a4d892fff1f564c0f04ba340":{"changes":{"examples\/cluster\/plot_agglomerative_clustering.py":"MODIFY","doc\/modules\/clustering.rst":"MODIFY","doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/cluster\/_hierarchical.pyx":"MODIFY","sklearn\/cluster\/hierarchical.py":"MODIFY","examples\/cluster\/plot_linkage_comparison.py":"ADD","examples\/cluster\/plot_digits_linkage.py":"MODIFY","sklearn\/cluster\/tests\/test_hierarchical.py":"MODIFY"},"diff":{"examples\/cluster\/plot_agglomerative_clustering.py":[{"add":["11","Second, when using a connectivity matrix, single, average and complete","12","linkage are unstable and tend to create a few clusters that grow very","13","quickly. Indeed, average and complete linkage fight this percolation behavior","14","by considering all the distances between two clusters when merging them (","15","while single linkage exaggerates the behaviour by considering only the","16","shortest distance between clusters). The connectivity graph breaks this","17","mechanism for average and complete linkage, making them resemble the more","18","brittle single linkage. This effect is more pronounced for very sparse graphs","19","(try decreasing the number of neighbors in kneighbors_graph) and with","20","complete linkage. In particular, having a very small number of neighbors in","21","the graph, imposes a geometry that is close to that of single linkage,","22","which is well known to have this percolation instability. \"\"\"","55","        for index, linkage in enumerate(('average',","56","                                         'complete',","57","                                         'ward',","58","                                         'single')):","59","            plt.subplot(1, 4, index + 1)","68","            plt.title('linkage=%s\\n(time %.2fs)' % (linkage, elapsed_time),"],"delete":["11","Second, when using a connectivity matrix, average and complete linkage are","12","unstable and tend to create a few clusters that grow very quickly. Indeed,","13","average and complete linkage fight this percolation behavior by considering all","14","the distances between two clusters when merging them. The connectivity","15","graph breaks this mechanism. This effect is more pronounced for very","16","sparse graphs (try decreasing the number of neighbors in","17","kneighbors_graph) and with complete linkage. In particular, having a very","18","small number of neighbors in the graph, imposes a geometry that is","19","close to that of single linkage, which is well known to have this","20","percolation instability.","21","\"\"\"","54","        for index, linkage in enumerate(('average', 'complete', 'ward')):","55","            plt.subplot(1, 3, index + 1)","64","            plt.title('linkage=%s (time %.2fs)' % (linkage, elapsed_time),"]}],"doc\/modules\/clustering.rst":[{"add":["569","Different linkage type: Ward, complete, average, and single linkage","570","-----------------------------------------------------------------)-","572",":class:`AgglomerativeClustering` supports Ward, single, average, and complete","575",".. image:: ..\/auto_examples\/cluster\/images\/sphx_glr_plot_linkage_comparison_001.png","576","    :target: ..\/auto_examples\/cluster\/plot_linkage_comparison.html","580","uneven cluster sizes. In this regard, single linkage is the worst","583","Euclidean metrics, average linkage is a good alternative. Single linkage,","584","while not robust to noisy data, can be computed very efficiently and can","585","therefore be useful to provide hierarchical clustering of larger datasets.","586","Single linkage can also perform well on non-globular data.","648",".. warning:: **Connectivity constraints with single, average and complete linkage**","650","    Connectivity constraints and single, complete or average linkage can enhance","657","    Single linkage is the most brittle linkage option with regard to this issue.","679","Single, average and complete linkage can be used with a variety of distances (or"],"delete":["569","Different linkage type: Ward, complete and average linkage","570","-----------------------------------------------------------","572",":class:`AgglomerativeClustering` supports Ward, average, and complete","575",".. image:: ..\/auto_examples\/cluster\/images\/sphx_glr_plot_digits_linkage_001.png","576","    :target: ..\/auto_examples\/cluster\/plot_digits_linkage.html","579",".. image:: ..\/auto_examples\/cluster\/images\/sphx_glr_plot_digits_linkage_002.png","580","    :target: ..\/auto_examples\/cluster\/plot_digits_linkage.html","581","    :scale: 43","582","","583",".. image:: ..\/auto_examples\/cluster\/images\/sphx_glr_plot_digits_linkage_003.png","584","    :target: ..\/auto_examples\/cluster\/plot_digits_linkage.html","585","    :scale: 43","586","","587","","589","uneven cluster sizes. In this regard, complete linkage is the worst","592","Euclidean metrics, average linkage is a good alternative.","654",".. warning:: **Connectivity constraints with average and complete linkage**","656","    Connectivity constraints and complete or average linkage can enhance","684","Average and complete linkage can be used with a variety of distances (or"]}],"doc\/whats_new\/v0.20.rst":[{"add":["83","Clustering","84","","85","- :class:`cluster.AgglomerativeClustering` now supports Single Linkage","86","  clustering via ``linkage='single'``. :issue:`9372` by","87","  :user:`Leland McInnes <lmcinnes>` and :user:`Steve Astels <sastels>`.","88",""],"delete":["80","","84","- Added the :class:`preprocessing.TransformedTargetRegressor` which transforms","85","  the target y before fitting a regression model. The predictions are mapped","86","  back to the original space via an inverse transform. :issue:`9041` by","87","  `Andreas Mller`_ and :user:`Guillaume Lemaitre <glemaitre>`."]}],"sklearn\/cluster\/_hierarchical.pyx":[{"add":["334","","335","################################################################################","336","# Efficient labelling\/conversion of MSTs to single linkage hierarchies","337","","338","cdef class UnionFind(object):","339","","340","    cdef ITYPE_t next_label","341","    cdef ITYPE_t[:] parent","342","    cdef ITYPE_t[:] size","343","","344","    def __init__(self, N):","345","        self.parent = -1 * np.ones(2 * N - 1, dtype=ITYPE, order='C')","346","        self.next_label = N","347","        self.size = np.hstack((np.ones(N, dtype=ITYPE),","348","                               np.zeros(N - 1, dtype=ITYPE)))","349","","350","    @cython.boundscheck(False)","351","    @cython.nonecheck(False)","352","    cdef void union(self, ITYPE_t m, ITYPE_t n):","353","        self.parent[m] = self.next_label","354","        self.parent[n] = self.next_label","355","        self.size[self.next_label] = self.size[m] + self.size[n]","356","        self.next_label += 1","357","","358","        return","359","","360","    @cython.boundscheck(False)","361","    @cython.nonecheck(False)","362","    cdef ITYPE_t fast_find(self, ITYPE_t n):","363","        cdef ITYPE_t p","364","        p = n","365","        # find the highest node in the linkage graph so far","366","        while self.parent[n] != -1:","367","            n = self.parent[n]","368","        # provide a shortcut up to the highest node","369","        while self.parent[p] != n:","370","            p, self.parent[p] = self.parent[p], n","371","        return n","372","","373","@cython.boundscheck(False)","374","@cython.nonecheck(False)","375","cpdef np.ndarray[DTYPE_t, ndim=2] _single_linkage_label(","376","    np.ndarray[DTYPE_t, ndim=2] L):","377","    \"\"\"","378","    Convert an linkage array or MST to a tree by labelling clusters at merges.","379","    This is done by using a Union find structure to keep track of merges","380","    efficiently. This is the private version of the function that assumes that","381","    ``L`` has been properly validated. See ``single_linkage_label`` for the","382","    user facing version of this function.","383","","384","    Parameters","385","    ----------","386","    L: array of shape (n_samples - 1, 3)","387","        The linkage array or MST where each row specifies two samples","388","        to be merged and a distance or weight at which the merge occurs. This","389","         array is assumed to be sorted by the distance\/weight.","390","","391","    Returns","392","    -------","393","    A tree in the format used by scipy.cluster.hierarchy.","394","    \"\"\"","395","","396","    cdef np.ndarray[DTYPE_t, ndim=2] result_arr","397","    cdef DTYPE_t[:, ::1] result","398","","399","    cdef ITYPE_t left, left_cluster, right, right_cluster, index","400","    cdef DTYPE_t delta","401","","402","    result_arr = np.zeros((L.shape[0], 4), dtype=DTYPE)","403","    result = result_arr","404","    U = UnionFind(L.shape[0] + 1)","405","","406","    for index in range(L.shape[0]):","407","","408","        left = <ITYPE_t> L[index, 0]","409","        right = <ITYPE_t> L[index, 1]","410","        delta = L[index, 2]","411","","412","        left_cluster = U.fast_find(left)","413","        right_cluster = U.fast_find(right)","414","","415","        result[index][0] = left_cluster","416","        result[index][1] = right_cluster","417","        result[index][2] = delta","418","        result[index][3] = U.size[left_cluster] + U.size[right_cluster]","419","","420","        U.union(left_cluster, right_cluster)","421","","422","    return result_arr","423","","424","","425","def single_linkage_label(L):","426","    \"\"\"","427","    Convert an linkage array or MST to a tree by labelling clusters at merges.","428","    This is done by using a Union find structure to keep track of merges","429","    efficiently.","430","","431","    Parameters","432","    ----------","433","    L: array of shape (n_samples - 1, 3)","434","        The linkage array or MST where each row specifies two samples","435","        to be merged and a distance or weight at which the merge occurs. This","436","         array is assumed to be sorted by the distance\/weight.","437","","438","    Returns","439","    -------","440","    A tree in the format used by scipy.cluster.hierarchy.","441","    \"\"\"","442","    # Validate L","443","    if L[:, :2].min() < 0 or L[:, :2].max() >= 2 * L.shape[0] + 1:","444","        raise ValueError(\"Input MST array is not a validly formatted MST array\")","445","","446","    is_sorted = lambda x: np.all(x[:-1] <= x[1:])","447","    if not is_sorted(L[:, 2]):","448","        raise ValueError(\"Input MST array must be sorted by weight\")","449","","450","    return _single_linkage_label(L)"],"delete":[]}],"sklearn\/cluster\/hierarchical.py":[{"add":["82","def _single_linkage_tree(connectivity, n_samples, n_nodes, n_clusters,","83","                         n_components, return_distance):","84","    \"\"\"","85","    Perform single linkage clustering on sparse data via the minimum","86","    spanning tree from scipy.sparse.csgraph, then using union-find to label.","87","    The parent array is then generated by walking through the tree.","88","    \"\"\"","89","    from scipy.sparse.csgraph import minimum_spanning_tree","90","","91","    # explicitly cast connectivity to ensure safety","92","    connectivity = connectivity.astype('float64')","93","","94","    # Ensure zero distances aren't ignored by setting them to \"epsilon\"","95","    epsilon_value = np.nextafter(0, 1, dtype=connectivity.data.dtype)","96","    connectivity.data[connectivity.data == 0] = epsilon_value","97","","98","    # Use scipy.sparse.csgraph to generate a minimum spanning tree","99","    mst = minimum_spanning_tree(connectivity.tocsr())","100","","101","    # Convert the graph to scipy.cluster.hierarchy array format","102","    mst = mst.tocoo()","103","","104","    # Undo the epsilon values","105","    mst.data[mst.data == epsilon_value] = 0","106","","107","    mst_array = np.vstack([mst.row, mst.col, mst.data]).T","108","","109","    # Sort edges of the min_spanning_tree by weight","110","    mst_array = mst_array[np.argsort(mst_array.T[2]), :]","111","","112","    # Convert edge list into standard hierarchical clustering format","113","    single_linkage_tree = _hierarchical._single_linkage_label(mst_array)","114","    children_ = single_linkage_tree[:, :2].astype(np.int)","115","","116","    # Compute parents","117","    parent = np.arange(n_nodes, dtype=np.intp)","118","    for i, (left, right) in enumerate(children_, n_samples):","119","        if n_clusters is not None and i >= n_nodes:","120","            break","121","        if left < n_nodes:","122","            parent[left] = i","123","        if right < n_nodes:","124","            parent[right] = i","125","","126","    if return_distance:","127","        distances = single_linkage_tree[:, 2]","128","        return children_, n_components, n_samples, parent, distances","129","    return children_, n_components, n_samples, parent","130","","131","","340","# single average and complete linkage","375","    linkage : {\"average\", \"complete\", \"single\"}, optional, default: \"complete\"","382","            - single uses the minimum of the distances between all observations","383","              of the two sets.","432","                       'average': _hierarchical.average_merge,","433","                       'single': None}  # Single linkage is handled differently","489","        distances = X[connectivity.row, connectivity.col].astype('float64')","504","    if linkage == 'single':","505","        return _single_linkage_tree(connectivity, n_samples, n_nodes,","506","                                    n_clusters, n_components, return_distance)","507","","591","def _single_linkage(*args, **kwargs):","592","    kwargs['linkage'] = 'single'","593","    return linkage_tree(*args, **kwargs)","594","","595","","599","    average=_average_linkage,","600","    single=_single_linkage)","695","    linkage : {\"ward\", \"complete\", \"average\", \"single\"}, optional \\","696","            (default=\"ward\")","706","        - single uses the minimum of the distances between all observations","707","          of the two sets.","781","            raise ValueError(\"Unknown linkage type %s. \"","867","    linkage : {\"ward\", \"complete\", \"average\", \"single\"}, optional\\","868","            (default=\"ward\")","878","        - single uses the minimum of the distances between all observations","879","          of the two sets."],"delete":["290","# average and complete linkage","325","    linkage : {\"average\", \"complete\"}, optional, default: \"complete\"","380","                       'average': _hierarchical.average_merge}","436","        distances = X[connectivity.row, connectivity.col]","537","    average=_average_linkage)","632","    linkage : {\"ward\", \"complete\", \"average\"}, optional, default: \"ward\"","715","            raise ValueError(\"Unknown linkage type %s.\"","801","    linkage : {\"ward\", \"complete\", \"average\"}, optional, default \"ward\""]}],"examples\/cluster\/plot_linkage_comparison.py":[{"add":[],"delete":[]}],"examples\/cluster\/plot_digits_linkage.py":[{"add":["14","This behavior is pronounced for the average linkage strategy,","15","that ends up with a couple of singleton clusters, while in the case","16","of single linkage we get a single central cluster with all other clusters","17","being drawn from noise points around the fringes.","73","    plt.tight_layout(rect=[0, 0.03, 1, 0.95])","83","for linkage in ('ward', 'average', 'complete', 'single'):","87","    print(\"%s :\\t%.2fs\" % (linkage, time() - t0))"],"delete":["14","This behavior is especially pronounced for the average linkage strategy,","15","that ends up with a couple of singleton clusters.","71","    plt.tight_layout()","81","for linkage in ('ward', 'average', 'complete'):","85","    print(\"%s : %.2fs\" % (linkage, time() - t0))"]}],"sklearn\/cluster\/tests\/test_hierarchical.py":[{"add":["26","                                          linkage_tree, _fix_connectivity)","36","from sklearn.datasets import make_moons, make_circles","151","    for linkage in (\"ward\", \"complete\", \"average\", \"single\"):","251","def test_single_linkage_clustering():","252","    # Check that we get the correct result in two emblematic cases","253","    moons, moon_labels = make_moons(noise=0.05, random_state=42)","254","    clustering = AgglomerativeClustering(n_clusters=2, linkage='single')","255","    clustering.fit(moons)","256","    assert_almost_equal(normalized_mutual_info_score(clustering.labels_,","257","                                                     moon_labels), 1)","258","","259","    circles, circle_labels = make_circles(factor=0.5, noise=0.025,","260","                                          random_state=42)","261","    clustering = AgglomerativeClustering(n_clusters=2, linkage='single')","262","    clustering.fit(circles)","263","    assert_almost_equal(normalized_mutual_info_score(clustering.labels_,","264","                                                     circle_labels), 1)","265","","266","","298","            # Sort the order of of child nodes per row for consistency","299","            children.sort(axis=1)","300","            assert_array_equal(children, children_, 'linkage tree differs'","301","                                                    ' from scipy impl for'","302","                                                    ' linkage: ' + linkage)","303","","312","def test_identical_points():","313","    # Ensure identical points are handled correctly when using mst with","314","    # a sparse connectivity matrix","315","    X = np.array([[0, 0, 0], [0, 0, 0],","316","                  [1, 1, 1], [1, 1, 1],","317","                  [2, 2, 2], [2, 2, 2]])","318","    true_labels = np.array([0, 0, 1, 1, 2, 2])","319","    connectivity = kneighbors_graph(X, n_neighbors=3, include_self=False)","320","    connectivity = 0.5 * (connectivity + connectivity.T)","321","    connectivity, n_components = _fix_connectivity(X,","322","                                                   connectivity,","323","                                                   'euclidean')","324","","325","    for linkage in ('single', 'average', 'average', 'ward'):","326","        clustering = AgglomerativeClustering(n_clusters=3,","327","                                             linkage=linkage,","328","                                             connectivity=connectivity)","329","        clustering.fit(X)","330","","331","        assert_almost_equal(normalized_mutual_info_score(clustering.labels_,","332","                                                         true_labels), 1)","333","","334","","402","        for linkage in ['average', 'complete', 'single']:","460","    linkage_options = ['complete', 'average', 'single']"],"delete":["26","                                          linkage_tree)","150","    for linkage in (\"ward\", \"complete\", \"average\"):","356","        for linkage in ['average', 'complete']:","414","    linkage_options = ['complete', 'average']"]}]}},"d830c90c9b860e63ca2af2099d87bb1661947283":{"changes":{"sklearn\/tests\/test_docstring_parameters.py":"MODIFY","sklearn\/discriminant_analysis.py":"MODIFY","sklearn\/cross_decomposition\/pls_.py":"MODIFY"},"diff":{"sklearn\/tests\/test_docstring_parameters.py":[{"add":["22","PUBLIC_MODULES = set(['sklearn.' + modname","23","                      for _, modname, _ in walk_packages(sklearn.__path__)","24","                      if not modname.startswith('_') and","25","                      '.tests.' not in modname])","55","    'fit',","56","    'score',","57","    'fit_predict',","58","    'fit_transform',","59","    'partial_fit',","60","    'predict'","72","        raise SkipTest(\"numpydoc is required to test the docstrings, \"","73","                       \"as well as python version >= 3.5\")"],"delete":["22","PUBLIC_MODULES = set(['sklearn.' + pckg[1]","23","                      for pckg in walk_packages('sklearn.*')","24","                      if not pckg[1].startswith('_')])","29","    'sklearn.cross_decomposition',","30","    'sklearn.discriminant_analysis',","56","        'fit',","57","        'score',","58","        'fit_predict',","59","        'fit_transform',","60","        'partial_fit',","61","        'predict'","73","        raise SkipTest(","74","            \"numpydoc is required to test the docstrings\")"]}],"sklearn\/discriminant_analysis.py":[{"add":["176","    tol : float, optional, (default 1.0e-4)","556","    store_covariances : boolean","557","        If True the covariance matrices are computed and stored in the","558","        `self.covariances_` attribute.","559","","560","        .. versionadded:: 0.17","561","","562","    tol : float, optional, default 1.0e-4","563","        Threshold used for rank estimation.","564","","565","        .. versionadded:: 0.17","566","","628","            Training vector, where n_samples is the number of samples and"],"delete":["176","    tol : float, optional","578","    store_covariances : boolean","579","        If True the covariance matrices are computed and stored in the","580","        `self.covariances_` attribute.","581","","582","        .. versionadded:: 0.17","583","","584","    tol : float, optional, default 1.0e-4","585","        Threshold used for rank estimation.","586","","587","        .. versionadded:: 0.17","588","","628","            Training vector, where n_samples in the number of samples and"]}],"sklearn\/cross_decomposition\/pls_.py":[{"add":["237","            Training vectors, where n_samples is the number of samples and","240","        Y : array-like, shape = [n_samples, n_targets]","241","            Target vectors, where n_samples is the number of samples and","376","        X : array-like, shape = [n_samples, n_features]","377","            Training vectors, where n_samples is the number of samples and","378","            n_features is the number of predictors.","380","        Y : array-like, shape = [n_samples, n_targets]","381","            Target vectors, where n_samples is the number of samples and","382","            n_targets is the number of response variables.","414","        X : array-like, shape = [n_samples, n_features]","415","            Training vectors, where n_samples is the number of samples and","416","            n_features is the number of predictors.","434","    def fit_transform(self, X, y=None):","439","        X : array-like, shape = [n_samples, n_features]","440","            Training vectors, where n_samples is the number of samples and","441","            n_features is the number of predictors.","443","        y : array-like, shape = [n_samples, n_targets]","444","            Target vectors, where n_samples is the number of samples and","445","            n_targets is the number of response variables.","451","        return self.fit(X, y).transform(X, y)","606","    n_components : int, (default 2).","607","        Number of components to keep","608","","609","    scale : boolean, (default True)","610","        Option to scale data","785","        \"\"\"Fit model to data.","786","","787","        Parameters","788","        ----------","789","        X : array-like, shape = [n_samples, n_features]","790","            Training vectors, where n_samples is the number of samples and","791","            n_features is the number of predictors.","792","","793","        Y : array-like, shape = [n_samples, n_targets]","794","            Target vectors, where n_samples is the number of samples and","795","            n_targets is the number of response variables.","796","        \"\"\"","833","        \"\"\"","834","        Apply the dimension reduction learned on the train data.","835","","836","        Parameters","837","        ----------","838","        X : array-like, shape = [n_samples, n_features]","839","            Training vectors, where n_samples is the number of samples and","840","            n_features is the number of predictors.","841","","842","        Y : array-like, shape = [n_samples, n_targets]","843","            Target vectors, where n_samples is the number of samples and","844","            n_targets is the number of response variables.","845","        \"\"\"","858","    def fit_transform(self, X, y=None):","863","        X : array-like, shape = [n_samples, n_features]","864","            Training vectors, where n_samples is the number of samples and","865","            n_features is the number of predictors.","867","        y : array-like, shape = [n_samples, n_targets]","868","            Target vectors, where n_samples is the number of samples and","869","            n_targets is the number of response variables.","875","        return self.fit(X, y).transform(X, y)"],"delete":["237","            Training vectors, where n_samples in the number of samples and","240","        Y : array-like of response, shape = [n_samples, n_targets]","241","            Target vectors, where n_samples in the number of samples and","376","        X : array-like of predictors, shape = [n_samples, p]","377","            Training vectors, where n_samples in the number of samples and","378","            p is the number of predictors.","380","        Y : array-like of response, shape = [n_samples, q], optional","381","            Training vectors, where n_samples in the number of samples and","382","            q is the number of response variables.","414","        X : array-like of predictors, shape = [n_samples, p]","415","            Training vectors, where n_samples in the number of samples and","416","            p is the number of predictors.","434","    def fit_transform(self, X, y=None, **fit_params):","439","        X : array-like of predictors, shape = [n_samples, p]","440","            Training vectors, where n_samples in the number of samples and","441","            p is the number of predictors.","443","        Y : array-like of response, shape = [n_samples, q], optional","444","            Training vectors, where n_samples in the number of samples and","445","            q is the number of response variables.","446","","447","        copy : boolean, default True","448","            Whether to copy X and Y, or perform in-place normalization.","454","        return self.fit(X, y, **fit_params).transform(X, y)","609","    scale : boolean, scale data? (default True)","626","    n_components : int, number of components to keep. (default 2).","627","","822","        \"\"\"Apply the dimension reduction learned on the train data.\"\"\"","835","    def fit_transform(self, X, y=None, **fit_params):","840","        X : array-like of predictors, shape = [n_samples, p]","841","            Training vectors, where n_samples in the number of samples and","842","            p is the number of predictors.","844","        Y : array-like of response, shape = [n_samples, q], optional","845","            Training vectors, where n_samples in the number of samples and","846","            q is the number of response variables.","852","        return self.fit(X, y, **fit_params).transform(X, y)"]}]}},"f05a95b1039b307a76f9179c135e2d282bc6c901":{"changes":{"doc\/modules\/classes.rst":"MODIFY","doc\/modules\/model_evaluation.rst":"MODIFY","sklearn\/metrics\/__init__.py":"MODIFY","sklearn\/metrics\/ranking.py":"MODIFY","sklearn\/metrics\/tests\/test_ranking.py":"MODIFY"},"diff":{"doc\/modules\/classes.rst":[{"add":[],"delete":["785","   metrics.dcg_score","793","   metrics.ndcg_score"]}],"doc\/modules\/model_evaluation.rst":[{"add":[],"delete":["311","Some are typically used for ranking:","312","","313",".. autosummary::","314","   :template: function.rst","315","","316","   dcg_score","317","   ndcg_score","318",""]}],"sklearn\/metrics\/__init__.py":[{"add":[],"delete":["14","from .ranking import dcg_score","15","from .ranking import ndcg_score","120","    'dcg_score',","121","    'ndcg_score'"]}],"sklearn\/metrics\/ranking.py":[{"add":["28","from ..utils import column_or_1d, check_array"],"delete":["28","from ..utils import column_or_1d, check_array, check_X_y","34","from ..preprocessing import LabelBinarizer","806","","807","","808","def dcg_score(y_true, y_score, k=5):","809","    \"\"\"Discounted cumulative gain (DCG) at rank K.","810","","811","    Parameters","812","    ----------","813","    y_true : array, shape = [n_samples]","814","        Ground truth (true relevance labels).","815","    y_score : array, shape = [n_samples]","816","        Predicted scores.","817","    k : int","818","        Rank.","819","","820","    Returns","821","    -------","822","    score : float","823","","824","    References","825","    ----------","826","    .. [1] `Wikipedia entry for the Discounted Cumulative Gain","827","           <https:\/\/en.wikipedia.org\/wiki\/Discounted_cumulative_gain>`_","828","    \"\"\"","829","    order = np.argsort(y_score)[::-1]","830","    y_true = np.take(y_true, order[:k])","831","","832","    gain = 2 ** y_true - 1","833","","834","    discounts = np.log2(np.arange(len(y_true)) + 2)","835","    return np.sum(gain \/ discounts)","836","","837","","838","def ndcg_score(y_true, y_score, k=5):","839","    \"\"\"Normalized discounted cumulative gain (NDCG) at rank K.","840","","841","    Normalized Discounted Cumulative Gain (NDCG) measures the performance of a","842","    recommendation system based on the graded relevance of the recommended","843","    entities. It varies from 0.0 to 1.0, with 1.0 representing the ideal","844","    ranking of the entities.","845","","846","    Parameters","847","    ----------","848","    y_true : array, shape = [n_samples]","849","        Ground truth (true labels represended as integers).","850","    y_score : array, shape = [n_samples, n_classes]","851","        Predicted probabilities.","852","    k : int","853","        Rank.","854","","855","    Returns","856","    -------","857","    score : float","858","","859","    Examples","860","    --------","861","    >>> y_true = [1, 0, 2]","862","    >>> y_score = [[0.15, 0.55, 0.2], [0.7, 0.2, 0.1], [0.06, 0.04, 0.9]]","863","    >>> ndcg_score(y_true, y_score, k=2)","864","    1.0","865","    >>> y_score = [[0.9, 0.5, 0.8], [0.7, 0.2, 0.1], [0.06, 0.04, 0.9]]","866","    >>> ndcg_score(y_true, y_score, k=2)","867","    0.66666666666666663","868","","869","    References","870","    ----------","871","    .. [1] `Kaggle entry for the Normalized Discounted Cumulative Gain","872","           <https:\/\/www.kaggle.com\/wiki\/NormalizedDiscountedCumulativeGain>`_","873","    \"\"\"","874","    y_score, y_true = check_X_y(y_score, y_true)","875","","876","    # Make sure we use all the labels (max between the length and the higher","877","    # number in the array)","878","    lb = LabelBinarizer()","879","    lb.fit(np.arange(max(np.max(y_true) + 1, len(y_true))))","880","    binarized_y_true = lb.transform(y_true)","881","","882","    if binarized_y_true.shape != y_score.shape:","883","        raise ValueError(\"y_true and y_score have different value ranges\")","884","","885","    scores = []","886","","887","    # Iterate over each y_value_true and compute the DCG score","888","    for y_value_true, y_value_score in zip(binarized_y_true, y_score):","889","        actual = dcg_score(y_value_true, y_value_score, k)","890","        best = dcg_score(y_value_true, y_value_true, k)","891","        scores.append(actual \/ best)","892","","893","    return np.mean(scores)"]}],"sklearn\/metrics\/tests\/test_ranking.py":[{"add":[],"delete":["31","from sklearn.metrics import ndcg_score","760","def test_ndcg_score():","761","    # Check perfect ranking","762","    y_true = [1, 0, 2]","763","    y_score = [","764","        [0.15, 0.55, 0.2],","765","        [0.7, 0.2, 0.1],","766","        [0.06, 0.04, 0.9]","767","    ]","768","    perfect = ndcg_score(y_true, y_score)","769","    assert_equal(perfect, 1.0)","770","","771","    # Check bad ranking with a small K","772","    y_true = [0, 2, 1]","773","    y_score = [","774","        [0.15, 0.55, 0.2],","775","        [0.7, 0.2, 0.1],","776","        [0.06, 0.04, 0.9]","777","    ]","778","    short_k = ndcg_score(y_true, y_score, k=1)","779","    assert_equal(short_k, 0.0)","780","","781","    # Check a random scoring","782","    y_true = [2, 1, 0]","783","    y_score = [","784","        [0.15, 0.55, 0.2],","785","        [0.7, 0.2, 0.1],","786","        [0.06, 0.04, 0.9]","787","    ]","788","    average_ranking = ndcg_score(y_true, y_score, k=2)","789","    assert_almost_equal(average_ranking, 0.63092975)","790","","791",""]}]}},"d9eba751bdb2492f9d77f10cb2ef28ecceed6ba5":{"changes":{"sklearn\/linear_model\/logistic.py":"MODIFY"},"diff":{"sklearn\/linear_model\/logistic.py":[{"add":["1417","        default: 'lbfgs'"],"delete":["1417","        default: 'liblinear'"]}]}}}