{"fbcbc1b76c467a7e17dae2fd2032d5685ee9884c":{"changes":{"sklearn\/mixture\/base.py":"MODIFY"},"diff":{"sklearn\/mixture\/base.py":[{"add":["419","        weighted_log_prob : array, shape (n_samples, n_component)"],"delete":["419","        weighted_log_prob : array, shape (n_features, n_component)"]}]}},"a0dfa306964f57625099b81fa0e558d3ed02ec24":{"changes":{"sklearn\/covariance\/shrunk_covariance_.py":"MODIFY","doc\/modules\/covariance.rst":"MODIFY"},"diff":{"sklearn\/covariance\/shrunk_covariance_.py":[{"add":["488","    In the original article, formula (23) states that 2\/p is multiplied by","489","    Trace(cov*cov) in both the numerator and denominator, this operation is","490","    omitted in the author's MATLAB program because for a large p, the value","491","    of 2\/p is so small that it doesn't affect the value of the estimator."],"delete":["488","    In the original article, formula (23) states that 2\/p is multiplied by ","489","    Trace(cov*cov) in both the numerator and denominator, this operation is omitted","490","    in the author's MATLAB program because for a large p, the value of 2\/p is so ","491","    small that it doesn't affect the value of the estimator. "]}],"doc\/modules\/covariance.rst":[{"add":["40","same mean vector as the training set. If not so, both should be centered by the","107",".. note:: **Case when population covariance matrix is isotropic**","108","","109","    It is important to note that when the number of samples is much larger than","110","    the number of features, one would expect that no shrinkage would be","111","    necessary. The intuition behind this is that if the population covariance","112","    is full rank, when the number of sample grows, the sample covariance will","113","    also become positive definite. As a result, no shrinkage would necessary","114","    and the method should automatically do this.","115","","116","    This, however, is not the case in the Ledoit-Wolf procedure when the","117","    population covariance happens to be a multiple of the identity matrix. In","118","    this case, the Ledoit-Wolf shrinkage estimate approaches 1 as the number of","119","    samples increases. This indicates that the optimal estimate of the","120","    covariance matrix in the Ledoit-Wolf sense is multiple of the identity.","121","    Since the population covariance is already a multiple of the identity","122","    matrix, the Ledoit-Wolf solution is indeed a reasonable estimate.","123",""],"delete":["40","same mean vector as the training set. If not so, both should be centered by the ","336",""]}]}},"692cd8b7bcc2f83a5cf11e4b98ae9090ee0b65a6":{"changes":{"sklearn\/covariance\/graph_lasso_.py":"MODIFY"},"diff":{"sklearn\/covariance\/graph_lasso_.py":[{"add":["205","        # set a sub_covariance buffer","206","        sub_covariance = np.ascontiguousarray(covariance_[1:, 1:])","209","                # To keep the contiguous matrix `sub_covariance` equal to","210","                # covariance_[indices != idx].T[indices != idx]","211","                # we only need to update 1 column and 1 line when idx changes","212","                if idx > 0:","213","                    di = idx - 1","214","                    sub_covariance[di] = covariance_[di][indices != idx]","215","                    sub_covariance[:, di] = covariance_[:, di][indices != idx]","216","                else:","217","                    sub_covariance[:] = covariance_[1:, 1:]"],"delete":["207","                sub_covariance = np.ascontiguousarray(","208","                    covariance_[indices != idx].T[indices != idx])"]}]}},"ebc873034486eec763fc3a4435fc0066dff52155":{"changes":{"sklearn\/model_selection\/_validation.py":"MODIFY","sklearn\/model_selection\/tests\/test_validation.py":"MODIFY"},"diff":{"sklearn\/model_selection\/_validation.py":[{"add":["646","    Notes","647","    -----","648","    In the case that one or more classes are absent in a training portion, a","649","    default score needs to be assigned to all instances for that class if","650","    ``method`` produces columns per class, as in {'decision_function',","651","    'predict_proba', 'predict_log_proba'}.  For ``predict_proba`` this value is","652","    0.  In order to ensure finite output, we approximate negative infinity by","653","    the minimum finite float value for the dtype in other cases.","654","","757","        if n_classes != len(estimator.classes_):","758","            recommendation = (","759","                'To fix this, use a cross-validation '","760","                'technique resulting in properly '","761","                'stratified folds')","762","            warnings.warn('Number of classes in training fold ({}) does '","763","                          'not match total number of classes ({}). '","764","                          'Results may not be appropriate for your use case. '","765","                          '{}'.format(len(estimator.classes_),","766","                                      n_classes, recommendation),","767","                          RuntimeWarning)","768","            if method == 'decision_function':","769","                if (predictions.ndim == 2 and","770","                        predictions.shape[1] != len(estimator.classes_)):","771","                    # This handles the case when the shape of predictions","772","                    # does not match the number of classes used to train","773","                    # it with. This case is found when sklearn.svm.SVC is","774","                    # set to `decision_function_shape='ovo'`.","775","                    raise ValueError('Output shape {} of {} does not match '","776","                                     'number of classes ({}) in fold. '","777","                                     'Irregular decision_function outputs '","778","                                     'are not currently supported by '","779","                                     'cross_val_predict'.format(","780","                                        predictions.shape, method,","781","                                        len(estimator.classes_),","782","                                        recommendation))","783","                if len(estimator.classes_) <= 2:","784","                    # In this special case, `predictions` contains a 1D array.","785","                    raise ValueError('Only {} class\/es in training fold, this '","786","                                     'is not supported for decision_function '","787","                                     'with imbalanced folds. {}'.format(","788","                                        len(estimator.classes_),","789","                                        recommendation))","790","","791","            float_min = np.finfo(predictions.dtype).min","792","            default_values = {'decision_function': float_min,","793","                              'predict_log_proba': float_min,","794","                              'predict_proba': 0}","795","            predictions_for_all_classes = np.full((_num_samples(predictions),","796","                                                   n_classes),","797","                                                  default_values[method])","798","            predictions_for_all_classes[:, estimator.classes_] = predictions","799","            predictions = predictions_for_all_classes"],"delete":["748","        predictions_ = np.zeros((_num_samples(X_test), n_classes))","749","        if method == 'decision_function' and len(estimator.classes_) == 2:","750","            predictions_[:, estimator.classes_[-1]] = predictions","751","        else:","752","            predictions_[:, estimator.classes_] = predictions","753","        predictions = predictions_"]}],"sklearn\/model_selection\/tests\/test_validation.py":[{"add":["26","from sklearn.utils.testing import assert_warns_message","47","from sklearn.datasets import load_digits","58","from sklearn.linear_model import PassiveAggressiveClassifier, RidgeClassifier","804","    X, y = load_iris(return_X_y=True)","805","","806","    warning_message = ('Number of classes in training fold (2) does '","807","                       'not match total number of classes (3). '","808","                       'Results may not be appropriate for your use case.')","809","    assert_warns_message(RuntimeWarning, warning_message,","810","                         cross_val_predict, LogisticRegression(),","811","                         X, y, method='predict_proba', cv=KFold(2))","812","","813","","814","def test_cross_val_predict_decision_function_shape():","815","    X, y = make_classification(n_classes=2, n_samples=50, random_state=0)","816","","817","    preds = cross_val_predict(LogisticRegression(), X, y,","818","                              method='decision_function')","819","    assert_equal(preds.shape, (50,))","820","","821","    X, y = load_iris(return_X_y=True)","822","","823","    preds = cross_val_predict(LogisticRegression(), X, y,","824","                              method='decision_function')","825","    assert_equal(preds.shape, (150, 3))","826","","827","    # This specifically tests imbalanced splits for binary","828","    # classification with decision_function. This is only","829","    # applicable to classifiers that can be fit on a single","830","    # class.","831","    X = X[:100]","832","    y = y[:100]","833","    assert_raise_message(ValueError,","834","                         'Only 1 class\/es in training fold, this'","835","                         ' is not supported for decision_function'","836","                         ' with imbalanced folds. To fix '","837","                         'this, use a cross-validation technique '","838","                         'resulting in properly stratified folds',","839","                         cross_val_predict, RidgeClassifier(), X, y,","840","                         method='decision_function', cv=KFold(2))","841","","842","    X, y = load_digits(return_X_y=True)","843","    est = SVC(kernel='linear', decision_function_shape='ovo')","844","","845","    preds = cross_val_predict(est,","846","                              X, y,","847","                              method='decision_function')","848","    assert_equal(preds.shape, (1797, 45))","849","","850","    ind = np.argsort(y)","851","    X, y = X[ind], y[ind]","852","    assert_raises_regex(ValueError,","853","                        'Output shape \\(599L?, 21L?\\) of decision_function '","854","                        'does not match number of classes \\(7\\) in fold. '","855","                        'Irregular decision_function .*',","856","                        cross_val_predict, est, X, y,","857","                        cv=KFold(n_splits=3), method='decision_function')","858","","859","","860","def test_cross_val_predict_predict_proba_shape():","861","    X, y = make_classification(n_classes=2, n_samples=50, random_state=0)","862","","863","    preds = cross_val_predict(LogisticRegression(), X, y,","864","                              method='predict_proba')","865","    assert_equal(preds.shape, (50, 2))","866","","867","    X, y = load_iris(return_X_y=True)","868","","869","    preds = cross_val_predict(LogisticRegression(), X, y,","870","                              method='predict_proba')","871","    assert_equal(preds.shape, (150, 3))","872","","873","","874","def test_cross_val_predict_predict_log_proba_shape():","875","    X, y = make_classification(n_classes=2, n_samples=50, random_state=0)","876","","877","    preds = cross_val_predict(LogisticRegression(), X, y,","878","                              method='predict_log_proba')","879","    assert_equal(preds.shape, (50, 2))","880","","881","    X, y = load_iris(return_X_y=True)","882","","883","    preds = cross_val_predict(LogisticRegression(), X, y,","884","                              method='predict_log_proba')","885","    assert_equal(preds.shape, (150, 3))","886","","1328","        if method is 'predict_proba':","1329","            exp_pred_test = np.zeros((len(test), classes))","1331","            exp_pred_test = np.full((len(test), classes),","1332","                                    np.finfo(expected_predictions.dtype).min)","1333","        exp_pred_test[:, est.classes_] = expected_predictions_","1341","    X = np.arange(200).reshape(100, 2)","1342","    y = np.array([x\/\/10 for x in range(100)])","1343","    classes = 10","1371","        y = shuffle(np.repeat(range(10), 10), random_state=0)"],"delete":["56","from sklearn.linear_model import PassiveAggressiveClassifier","1243","        exp_pred_test = np.zeros((len(test), classes))","1244","        if method is 'decision_function' and len(est.classes_) == 2:","1245","            exp_pred_test[:, est.classes_[-1]] = expected_predictions_","1247","            exp_pred_test[:, est.classes_] = expected_predictions_","1255","    X = np.arange(8).reshape(4, 2)","1256","    y = np.array([0, 0, 1, 2])","1257","    classes = 3","1285","        y = [1, 1, -4, 6]"]}]}},"e674f64458242bdfc28a01a95ffa69ed96056027":{"changes":{"sklearn\/multioutput.py":"MODIFY","sklearn\/tests\/test_multioutput.py":"MODIFY"},"diff":{"sklearn\/multioutput.py":[{"add":["370","class ClassifierChain(BaseEstimator, ClassifierMixin, MetaEstimatorMixin):"],"delete":["370","class ClassifierChain(BaseEstimator):"]}],"sklearn\/tests\/test_multioutput.py":[{"add":["31","from sklearn.base import ClassifierMixin","383","    assert isinstance(classifier_chain, ClassifierMixin)","384",""],"delete":[]}]}},"f0c5568fcd9a6c9ace37082b573fc28bfdc17cec":{"changes":{"sklearn\/utils\/testing.py":"MODIFY","sklearn\/utils\/estimator_checks.py":"MODIFY"},"diff":{"sklearn\/utils\/testing.py":[{"add":["377","def assert_allclose_dense_sparse(x, y, rtol=1e-07, atol=1e-9, err_msg=''):","391","    rtol : float, optional","392","        relative tolerance; see numpy.allclose","393","","394","    atol : float, optional","395","        absolute tolerance; see numpy.allclose. Note that the default here is","396","        more tolerant than the default for numpy.testing.assert_allclose, where","397","        atol=0.","398",""],"delete":["377","def assert_allclose_dense_sparse(x, y, rtol=1e-07, atol=0, err_msg=''):"]}],"sklearn\/utils\/estimator_checks.py":[{"add":["1144","                assert_allclose(y_log_prob, np.log(y_prob), 8, atol=1e-9)","1380","        # XXX: Generally can use 0.89 here. On Windows, LinearSVC gets","1381","        #      0.88 (Issue #9111)","1382","        assert_greater(np.mean(y_pred == 0), 0.87)"],"delete":["1144","                assert_allclose(y_log_prob, np.log(y_prob), 8)","1380","        assert_greater(np.mean(y_pred == 0), 0.89)"]}]}},"0dc22798e72cca233e62c0fa37408fed5d4d2394":{"changes":{"doc\/modules\/label_propagation.rst":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/semi_supervised\/tests\/test_label_propagation.py":"MODIFY","sklearn\/semi_supervised\/label_propagation.py":"MODIFY","examples\/semi_supervised\/plot_label_propagation_structure.py":"MODIFY"},"diff":{"doc\/modules\/label_propagation.rst":[{"add":["54","clamping of input labels, which means :math:`\\alpha=0`. This clamping factor","55","can be relaxed, to say :math:`\\alpha=0.2`, which means that we will always"],"delete":["54","clamping of input labels, which means :math:`\\alpha=1`. This clamping factor","55","can be relaxed, to say :math:`\\alpha=0.8`, which means that we will always"]}],"doc\/whats_new.rst":[{"add":["450","     :issue:`9105` by `Hanmin Qin <https:\/\/github.com\/qinhanmin2014>`_.","451","","452","   - Fix :class:`semi_supervised.BaseLabelPropagation` to correctly implement","453","     ``LabelPropagation`` and ``LabelSpreading`` as done in the referenced","454","     papers. :class:`semi_supervised.LabelPropagation` now always does hard","455","     clamping. Its ``alpha`` parameter has no effect and is","456","     deprecated to be removed in 0.21. :issue:`6727` :issue:`3550` issue:`5770`","457","     by :user:`Andre Ambrosio Boechat <boechat107>`, :user:`Utkarsh Upadhyay","458","     <musically-ut>`, and `Joel Nothman`_.","459",""],"delete":["450","     :issue:`9105` by `Hanmin Qin <https:\/\/github.com\/qinhanmin2014>`_. "]}],"sklearn\/semi_supervised\/tests\/test_label_propagation.py":[{"add":["5","from sklearn.utils.testing import assert_warns","6","from sklearn.utils.testing import assert_raises","7","from sklearn.utils.testing import assert_no_warnings","10","from sklearn.datasets import make_classification","65","","66","","67","def test_alpha_deprecation():","68","    X, y = make_classification(n_samples=100)","69","    y[::3] = -1","70","","71","    lp_default = label_propagation.LabelPropagation(kernel='rbf', gamma=0.1)","72","    lp_default_y = assert_no_warnings(lp_default.fit, X, y).transduction_","73","","74","    lp_0 = label_propagation.LabelPropagation(alpha=0, kernel='rbf', gamma=0.1)","75","    lp_0_y = assert_warns(DeprecationWarning, lp_0.fit, X, y).transduction_","76","","77","    assert_array_equal(lp_default_y, lp_0_y)","78","","79","","80","def test_label_spreading_closed_form():","81","    n_classes = 2","82","    X, y = make_classification(n_classes=n_classes, n_samples=200,","83","                               random_state=0)","84","    y[::3] = -1","85","    clf = label_propagation.LabelSpreading().fit(X, y)","86","    # adopting notation from Zhou et al (2004):","87","    S = clf._build_graph()","88","    Y = np.zeros((len(y), n_classes + 1))","89","    Y[np.arange(len(y)), y] = 1","90","    Y = Y[:, :-1]","91","    for alpha in [0.1, 0.3, 0.5, 0.7, 0.9]:","92","        expected = np.dot(np.linalg.inv(np.eye(len(S)) - alpha * S), Y)","93","        expected \/= expected.sum(axis=1)[:, np.newaxis]","94","        clf = label_propagation.LabelSpreading(max_iter=10000, alpha=alpha)","95","        clf.fit(X, y)","96","        assert_array_almost_equal(expected, clf.label_distributions_, 4)","97","","98","","99","def test_label_propagation_closed_form():","100","    n_classes = 2","101","    X, y = make_classification(n_classes=n_classes, n_samples=200,","102","                               random_state=0)","103","    y[::3] = -1","104","    Y = np.zeros((len(y), n_classes + 1))","105","    Y[np.arange(len(y)), y] = 1","106","    unlabelled_idx = Y[:, (-1,)].nonzero()[0]","107","    labelled_idx = (Y[:, (-1,)] == 0).nonzero()[0]","108","","109","    clf = label_propagation.LabelPropagation(max_iter=10000,","110","                                             gamma=0.1).fit(X, y)","111","    # adopting notation from Zhu et al 2002","112","    T_bar = clf._build_graph()","113","    Tuu = T_bar[np.meshgrid(unlabelled_idx, unlabelled_idx, indexing='ij')]","114","    Tul = T_bar[np.meshgrid(unlabelled_idx, labelled_idx, indexing='ij')]","115","    Y = Y[:, :-1]","116","    Y_l = Y[labelled_idx, :]","117","    Y_u = np.dot(np.dot(np.linalg.inv(np.eye(Tuu.shape[0]) - Tuu), Tul), Y_l)","118","","119","    expected = Y.copy()","120","    expected[unlabelled_idx, :] = Y_u","121","    expected \/= expected.sum(axis=1)[:, np.newaxis]","122","","123","    assert_array_almost_equal(expected, clf.label_distributions_, 4)","124","","125","","126","def test_valid_alpha():","127","    n_classes = 2","128","    X, y = make_classification(n_classes=n_classes, n_samples=200,","129","                               random_state=0)","130","    for alpha in [-0.1, 0, 1, 1.1, None]:","131","        assert_raises(ValueError,","132","                      lambda **kwargs:","133","                      label_propagation.LabelSpreading(**kwargs).fit(X, y),","134","                      alpha=alpha)","135","","136","","137","def test_convergence_speed():","138","    # This is a non-regression test for #5774","139","    X = np.array([[1., 0.], [0., 1.], [1., 2.5]])","140","    y = np.array([0, 1, -1])","141","    mdl = label_propagation.LabelSpreading(kernel='rbf', max_iter=5000)","142","    mdl.fit(X, y)","143","","144","    # this should converge quickly:","145","    assert mdl.n_iter_ < 10","146","    assert_array_equal(mdl.predict(X), [0, 1, 1])"],"delete":[]}],"sklearn\/semi_supervised\/label_propagation.py":[{"add":["16","  The algorithm tries to learn distributions of labels over the dataset given","17","  label assignments over an initial subset. In one variant, the algorithm does","18","  not allow for any errors in the initial assignment (hard-clamping) while","19","  in another variant, the algorithm allows for some wiggle room for the initial","20","  assignments, allowing them to change by a fraction alpha in each iteration","21","  (soft-clamping).","58","import warnings","243","        alpha = self.alpha","244","        if self._variant == 'spreading' and \\","245","                (alpha is None or alpha <= 0.0 or alpha >= 1.0):","246","            raise ValueError('alpha=%s is invalid: it must be inside '","247","                             'the open interval (0, 1)' % alpha)","257","        if self._variant == 'propagation':","258","            # LabelPropagation","259","            y_static[unlabeled] = 0","260","        else:","261","            # LabelSpreading","262","            y_static *= 1 - alpha","267","        unlabeled = unlabeled[:, np.newaxis]","275","","276","            if self._variant == 'propagation':","277","                normalizer = np.sum(","278","                    self.label_distributions_, axis=1)[:, np.newaxis]","279","                self.label_distributions_ \/= normalizer","280","                self.label_distributions_ = np.where(unlabeled,","281","                                                     self.label_distributions_,","282","                                                     y_static)","283","            else:","284","                # clamp","285","                self.label_distributions_ = np.multiply(","286","                    alpha, self.label_distributions_) + y_static","291","","320","        Clamping factor.","321","","322","        .. deprecated:: 0.19","323","            This parameter will be removed in 0.21.","324","            'alpha' is fixed to zero in 'LabelPropagation'.","375","    _variant = 'propagation'","376","","377","    def __init__(self, kernel='rbf', gamma=20, n_neighbors=7,","378","                 alpha=None, max_iter=30, tol=1e-3, n_jobs=1):","379","        super(LabelPropagation, self).__init__(","380","            kernel=kernel, gamma=gamma, n_neighbors=n_neighbors, alpha=alpha,","381","            max_iter=max_iter, tol=tol, n_jobs=n_jobs)","382","","399","    def fit(self, X, y):","400","        if self.alpha is not None:","401","            warnings.warn(","402","                \"alpha is deprecated since 0.19 and will be removed in 0.21.\",","403","                DeprecationWarning","404","            )","405","            self.alpha = None","406","        return super(LabelPropagation, self).fit(X, y)","407","","433","      Clamping factor. A value in [0, 1] that specifies the relative amount","434","      that an instance should adopt the information from its neighbors as","435","      opposed to its initial label.","436","      alpha=0 means keeping the initial label information; alpha=1 means","437","      replacing all initial information.","492","    _variant = 'spreading'","493",""],"delete":["16","  The algorithm tries to learn distributions of labels over the dataset. In the","17","  \"Hard Clamp\" mode, the true ground labels are never allowed to change. They","18","  are clamped into position. In the \"Soft Clamp\" mode, they are allowed some","19","  wiggle room, but some alpha of their original value will always be retained.","20","  Hard clamp is the same as soft clamping with alpha set to 1.","243","        clamp_weights = np.ones((n_samples, 1))","244","        clamp_weights[unlabeled, 0] = self.alpha","252","        if self.alpha > 0.:","253","            y_static *= 1 - self.alpha","254","        y_static[unlabeled] = 0","266","            # clamp","267","            self.label_distributions_ = np.multiply(","268","                clamp_weights, self.label_distributions_) + y_static","301","        Clamping factor","393","      clamping factor"]}],"examples\/semi_supervised\/plot_label_propagation_structure.py":[{"add":["32","label_spread = label_propagation.LabelSpreading(kernel='knn', alpha=0.2)"],"delete":["32","label_spread = label_propagation.LabelSpreading(kernel='knn', alpha=1.0)"]}]}},"1f5d22bffc081c59bf0d8c49c1e7ccbb6c84ef88":{"changes":{"sklearn\/feature_extraction\/tests\/test_feature_hasher.py":"MODIFY","sklearn\/feature_extraction\/tests\/test_text.py":"MODIFY"},"diff":{"sklearn\/feature_extraction\/tests\/test_feature_hasher.py":[{"add":["22","@ignore_warnings(category=DeprecationWarning)"],"delete":[]}],"sklearn\/feature_extraction\/tests\/test_text.py":[{"add":["30","                                   clean_warning_registry, ignore_warnings,","31","                                   SkipTest)","483","@ignore_warnings(category=DeprecationWarning)","655","@ignore_warnings(category=DeprecationWarning)","789","@ignore_warnings(category=DeprecationWarning)"],"delete":["30","                                   clean_warning_registry, SkipTest)"]}]}},"88f529efa688246712c15a1404ae4ff0b2d87fd9":{"changes":{"doc\/modules\/ensemble.rst":"MODIFY"},"diff":{"doc\/modules\/ensemble.rst":[{"add":["204","in combination with ``min_samples_split=2`` (i.e., when fully developing the"],"delete":["204","in combination with ``min_samples_split=1`` (i.e., when fully developing the"]}]}},"3f53d78ce238743eb2b76d3ea389b9b7c02f703a":{"changes":{"doc\/whats_new.rst":"MODIFY","sklearn\/decomposition\/pca.py":"MODIFY","sklearn\/decomposition\/tests\/test_pca.py":"MODIFY"},"diff":{"doc\/whats_new.rst":[{"add":["242","- Fixed the implementation of noise_variance_ in :class:`decomposition.PCA`.","243","  :issue:`9108` by `Hanmin Qin <https:\/\/github.com\/qinhanmin2014>`_.","244",""],"delete":[]}],"sklearn\/decomposition\/pca.py":[{"add":["203","        Equal to n_components largest eigenvalues","204","        of the covariance matrix of X.","205","","237","        Equal to the average of (min(n_features, n_samples) - n_components)","238","        smallest eigenvalues of the covariance matrix of X.","239","","502","        if self.n_components_ < min(n_features, n_samples):","505","            self.noise_variance_ \/= min(n_features, n_samples) - n_components"],"delete":["496","        if self.n_components_ < n_features:"]}],"sklearn\/decomposition\/tests\/test_pca.py":[{"add":["531","def test_pca_score_with_different_solvers():","532","    digits = datasets.load_digits()","533","    X_digits = digits.data","534","","535","    pca_dict = {svd_solver: PCA(n_components=30, svd_solver=svd_solver,","536","                                random_state=0)","537","                for svd_solver in solver_list}","538","","539","    for pca in pca_dict.values():","540","        pca.fit(X_digits)","541","        # Sanity check for the noise_variance_. For more details see","542","        # https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/7568","543","        # https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/8541","544","        # https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/8544","545","        assert np.all((pca.explained_variance_ - pca.noise_variance_) >= 0)","546","","547","    # Compare scores with different svd_solvers","548","    score_dict = {svd_solver: pca.score(X_digits)","549","                  for svd_solver, pca in pca_dict.items()}","550","    assert_almost_equal(score_dict['full'], score_dict['arpack'])","551","    assert_almost_equal(score_dict['full'], score_dict['randomized'],","552","                        decimal=3)","553","","554","","555","def test_pca_zero_noise_variance_edge_cases():","556","    # ensure that noise_variance_ is 0 in edge cases","557","    # when n_components == min(n_samples, n_features)","558","    n, p = 100, 3","559","","560","    rng = np.random.RandomState(0)","561","    X = rng.randn(n, p) * .1 + np.array([3, 4, 5])","562","    # arpack raises ValueError for n_components == min(n_samples,","563","    # n_features)","564","    svd_solvers = ['full', 'randomized']","565","","566","    for svd_solver in svd_solvers:","567","        pca = PCA(svd_solver=svd_solver, n_components=p)","568","        pca.fit(X)","569","        assert pca.noise_variance_ == 0","570","","571","        pca.fit(X.T)","572","        assert pca.noise_variance_ == 0","573","","574",""],"delete":[]}]}},"4ff7e8e658db082fe1ff0b2ba31244431fd99ef6":{"changes":{"sklearn\/utils\/random.py":"MODIFY","sklearn\/ensemble\/tests\/test_base.py":"MODIFY","sklearn\/multioutput.py":"MODIFY","sklearn\/linear_model\/tests\/test_logistic.py":"MODIFY","sklearn\/ensemble\/gradient_boosting.py":"MODIFY","sklearn\/mixture\/dpgmm.py":"MODIFY","sklearn\/metrics\/ranking.py":"MODIFY"},"diff":{"sklearn\/utils\/random.py":[{"add":["186","            # Normalize probabilities for the nonzero elements"],"delete":["186","            # Normalize probabilites for the nonzero elements"]}],"sklearn\/ensemble\/tests\/test_base.py":[{"add":["111","    # ensure multiple random_state parameters are invariant to get_params()"],"delete":["111","    # ensure multiple random_state paramaters are invariant to get_params()"]}],"sklearn\/multioutput.py":[{"add":["318","        Returns prediction probabilities for each class of each output."],"delete":["318","        Returns prediction probabilites for each class of each output."]}],"sklearn\/linear_model\/tests\/test_logistic.py":[{"add":["988","    # Predicted probabilities using the true-entropy loss should give a","998","    # Predicted probabilities using the soft-max function should give a"],"delete":["988","    # Predicted probabilites using the true-entropy loss should give a","998","    # Predicted probabilites using the soft-max function should give a"]}],"sklearn\/ensemble\/gradient_boosting.py":[{"add":["450","         the does not support probabilities raises AttributeError."],"delete":["450","         the does not support probabilites raises AttributeError."]}],"sklearn\/mixture\/dpgmm.py":[{"add":["49","    \"\"\"Normalized probabilities from unnormalized log-probabilities\"\"\""],"delete":["49","    \"\"\"Normalized probabilities from unnormalized log-probabilites\"\"\""]}],"sklearn\/metrics\/ranking.py":[{"add":["842","    # Make sure we use all the labels (max between the length and the higher"],"delete":["842","    # Make sure we use all the labels (max between the lenght and the higher"]}]}},"682e85fb1afcd050d9559b4d156593b2c32280df":{"changes":{"sklearn\/utils\/tests\/test_utils.py":"MODIFY","sklearn\/utils\/__init__.py":"MODIFY"},"diff":{"sklearn\/utils\/tests\/test_utils.py":[{"add":["0","from itertools import chain, product","202","    inds_readonly = inds.copy()","203","    inds_readonly.setflags(write=False)","205","    for this_df, this_inds in product([X_df, X_df_readonly],","206","                                      [inds, inds_readonly]):","207","        with warnings.catch_warnings(record=True):","208","            X_df_indexed = safe_indexing(this_df, this_inds)","209","","210","        assert_array_equal(np.array(X_df_indexed), X_indexed)"],"delete":["0","from itertools import chain","202","    with warnings.catch_warnings(record=True):","203","        X_df_ro_indexed = safe_indexing(X_df_readonly, inds)","205","    assert_array_equal(np.array(X_df_ro_indexed), X_indexed)"]}],"sklearn\/utils\/__init__.py":[{"add":["144","        # Work-around for indexing with read-only indices in pandas","145","        indices = indices if indices.flags.writeable else indices.copy()"],"delete":[]}]}},"026e10a24ffb9788ad914e642efe3b1b9559378e":{"changes":{"doc\/modules\/naive_bayes.rst":"MODIFY"},"diff":{"doc\/modules\/naive_bayes.rst":[{"add":["138","----------------------","151","    \\hat{\\theta}_{ci} = \\frac{\\alpha_i + \\sum_{j:y_j \\neq c} d_{ij}}","152","                             {\\alpha + \\sum_{j:y_j \\neq c} \\sum_{k} d_{kj}}","154","    w_{ci} = \\log \\hat{\\theta}_{ci}","155","","156","    w_{ci} = \\frac{w_{ci}}{\\sum_{j} w_{cj}}","157","","158","where the summations are over all documents :math:`j` not in class :math:`c`,","160",":math:`j`, :math:`\\alpha_i` is a smoothing hyperparameter like that found in","161","MNB, and :math:`\\alpha = \\sum_{i} \\alpha_i`. The second normalization addresses","162","the tendency for longer documents to dominate parameter estimates in MNB. The","163","classification rule is:","167","    \\hat{c} = \\arg\\min_c \\sum_{i} t_i w_{ci}"],"delete":["138","-----------------------","151","    \\hat{\\theta}_{ci} = \\frac{\\sum{j:y_j \\neq c} d_{ij} + \\alpha_i}","152","                             {\\sum{j:y_j \\neq c} \\sum{k} d_{kj} + \\alpha}","153","    w_{ci} = \\log \\hat{\\theta}_{ci}","154","    w_{ci} = \\frac{w_{ci}{\\sum{j} w_{cj}}","156","where the summation is over all documents :math:`j` not in class :math:`c`,","158",":math:`j`, and :math:`\\alpha` is a smoothing hyperparameter like that found in","159","MNB. The second normalization addresses the tendency for longer documents to","160","dominate parameter estimates in MNB. The classification rule is:","164","    \\hat{c} = \\arg\\min_c \\sum{i} t_i w_{ci}"]}]}},"6f70202ef9beefd3db9bb028755a0c38b4c5c8e7":{"changes":{"sklearn\/ensemble\/voting_classifier.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/ensemble\/tests\/test_voting_classifier.py":"MODIFY"},"diff":{"sklearn\/ensemble\/voting_classifier.py":[{"add":["14","import warnings","64","    flatten_transform : bool, optional (default=None)","65","        Affects shape of transform output only when voting='soft'","66","        If voting='soft' and flatten_transform=True, transform method returns","67","        matrix with shape (n_samples, n_classifiers * n_classes). If","68","        flatten_transform=False, it returns","69","        (n_classifiers, n_samples, n_classes).","70","","104","    ...        voting='soft', weights=[2,1,1],","105","    ...        flatten_transform=True)","109","    >>> print(eclf3.transform(X).shape)","110","    (6, 6)","114","    def __init__(self, estimators, voting='hard', weights=None, n_jobs=1,","115","                 flatten_transform=None):","120","        self.flatten_transform = flatten_transform","178","","272","        If `voting='soft'` and `flatten_transform=True`:","273","          array-like = (n_classifiers, n_samples * n_classes)","274","          otherwise array-like = (n_classifiers, n_samples, n_classes)","281","","283","            probas = self._collect_probas(X)","284","            if self.flatten_transform is None:","285","                warnings.warn(\"'flatten_transform' default value will be \"","286","                              \"changed to True in 0.21.\"","287","                              \"To silence this warning you may\"","288","                              \" explicitly set flatten_transform=False.\",","289","                              DeprecationWarning)","290","                return probas","291","            elif not self.flatten_transform:","292","                return probas","293","            else:","294","                return np.hstack(probas)","295",""],"delete":["96","    ...        voting='soft', weights=[2,1,1])","103","    def __init__(self, estimators, voting='hard', weights=None, n_jobs=1):","258","        If `voting='soft'`:","259","          array-like = [n_classifiers, n_samples, n_classes]","267","            return self._collect_probas(X)"]}],"doc\/whats_new.rst":[{"add":["173","     ","174","   - Added ``flatten_transform`` parameter to :class:`ensemble.VotingClassifier`","175","     to change output shape of `transform` method to 2 dimensional.","176","     :issue:`7794` by :user:`Ibraim Ganiev <olologin>` and","177","     :user:`Herilalaina Rakotoarison <herilalaina>`."],"delete":[]}],"sklearn\/ensemble\/tests\/test_voting_classifier.py":[{"add":["6","from sklearn.utils.testing import assert_warns_message","226","def test_parallel_fit():","367","","368","","369","def test_transform():","370","    \"\"\"Check transform method of VotingClassifier on toy dataset.\"\"\"","371","    clf1 = LogisticRegression(random_state=123)","372","    clf2 = RandomForestClassifier(random_state=123)","373","    clf3 = GaussianNB()","374","    X = np.array([[-1.1, -1.5], [-1.2, -1.4], [-3.4, -2.2], [1.1, 1.2]])","375","    y = np.array([1, 1, 2, 2])","376","","377","    eclf1 = VotingClassifier(estimators=[","378","        ('lr', clf1), ('rf', clf2), ('gnb', clf3)],","379","        voting='soft').fit(X, y)","380","    eclf2 = VotingClassifier(estimators=[","381","        ('lr', clf1), ('rf', clf2), ('gnb', clf3)],","382","        voting='soft',","383","        flatten_transform=True).fit(X, y)","384","    eclf3 = VotingClassifier(estimators=[","385","        ('lr', clf1), ('rf', clf2), ('gnb', clf3)],","386","        voting='soft',","387","        flatten_transform=False).fit(X, y)","388","","389","    warn_msg = (\"'flatten_transform' default value will be \"","390","                \"changed to True in 0.21.\"","391","                \"To silence this warning you may\"","392","                \" explicitly set flatten_transform=False.\")","393","    res = assert_warns_message(DeprecationWarning, warn_msg,","394","                               eclf1.transform, X)","395","    assert_array_equal(res.shape, (3, 4, 2))","396","    assert_array_equal(eclf2.transform(X).shape, (4, 6))","397","    assert_array_equal(eclf3.transform(X).shape, (3, 4, 2))","398","    assert_array_equal(res.swapaxes(0, 1).reshape((4, 6)),","399","                       eclf2.transform(X))","400","    assert_array_equal(eclf3.transform(X).swapaxes(0, 1).reshape((4, 6)),","401","                       eclf2.transform(X))"],"delete":["225","def test_parallel_predict():"]}]}},"211ded8fbcdde1b82cc3ad8ae5c3babcd0e8643e":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/feature_selection\/from_model.py":"MODIFY","sklearn\/feature_selection\/tests\/test_from_model.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["391","Preprocessing and feature selection","392","","393","- Added select K best features functionality to","394","  :class:`feature_selection.SelectFromModel`.","395","  :issue:`6689` by :user:`Nihar Sheth <nsheth12>` and","396","  :user:`Quazi Rahman <qmaruf>`.","397",""],"delete":[]}],"sklearn\/feature_selection\/from_model.py":[{"add":["4","import numbers","116","    max_features : int or None, optional","117","        The maximum number of features selected scoring above ``threshold``.","118","        To disable ``threshold`` and only select based on ``max_features``,","119","        set ``threshold=-np.inf``.","120","","121","        .. versionadded:: 0.20","122","","133","    def __init__(self, estimator, threshold=None, prefit=False,","134","                 norm_order=1, max_features=None):","139","        self.max_features = max_features","148","            raise ValueError('Either fit the model before transform or set'","149","                             ' \"prefit=True\" while passing the fitted'","150","                             ' estimator to the constructor.')","153","        if self.max_features is not None:","154","            mask = np.zeros_like(scores, dtype=bool)","155","            candidate_indices = \\","156","                np.argsort(-scores, kind='mergesort')[:self.max_features]","157","            mask[candidate_indices] = True","158","        else:","159","            mask = np.ones_like(scores, dtype=bool)","160","        mask[scores < threshold] = False","161","        return mask","181","        if self.max_features is not None:","182","            if not isinstance(self.max_features, numbers.Integral):","183","                raise TypeError(\"'max_features' should be an integer between\"","184","                                \" 0 and {} features. Got {!r} instead.\"","185","                                .format(X.shape[1], self.max_features))","186","            elif self.max_features < 0 or self.max_features > X.shape[1]:","187","                raise ValueError(\"'max_features' should be 0 and {} features.\"","188","                                 \"Got {} instead.\"","189","                                 .format(X.shape[1], self.max_features))","190",""],"delete":["125","    def __init__(self, estimator, threshold=None, prefit=False, norm_order=1):","138","            raise ValueError(","139","                'Either fit SelectFromModel before transform or set \"prefit='","140","                'True\" and pass a fitted estimator to the constructor.')","143","        return scores >= threshold"]}],"sklearn\/feature_selection\/tests\/test_from_model.py":[{"add":["0","import pytest","11","from sklearn.utils.testing import assert_allclose","21","from sklearn.base import BaseEstimator","45","@pytest.mark.parametrize(","46","    \"max_features, err_type, err_msg\",","47","    [(-1, ValueError, \"'max_features' should be 0 and\"),","48","     (data.shape[1] + 1, ValueError, \"'max_features' should be 0 and\"),","49","     ('gobbledigook', TypeError, \"should be an integer\"),","50","     ('all', TypeError, \"should be an integer\")]","51",")","52","def test_max_features_error(max_features, err_type, err_msg):","53","    clf = RandomForestClassifier(n_estimators=50, random_state=0)","54","","55","    transformer = SelectFromModel(estimator=clf,","56","                                  max_features=max_features,","57","                                  threshold=-np.inf)","58","    with pytest.raises(err_type, match=err_msg):","59","        transformer.fit(data, y)","60","","61","","62","@pytest.mark.parametrize(\"max_features\", [0, 2, data.shape[1]])","63","def test_max_features_dim(max_features):","64","    clf = RandomForestClassifier(n_estimators=50, random_state=0)","65","    transformer = SelectFromModel(estimator=clf,","66","                                  max_features=max_features,","67","                                  threshold=-np.inf)","68","    X_trans = transformer.fit_transform(data, y)","69","    assert X_trans.shape[1] == max_features","70","","71","","72","class FixedImportanceEstimator(BaseEstimator):","73","    def __init__(self, importances):","74","        self.importances = importances","75","","76","    def fit(self, X, y=None):","77","        self.feature_importances_ = np.array(self.importances)","78","","79","","80","def test_max_features():","81","    # Test max_features parameter using various values","82","    X, y = datasets.make_classification(","83","        n_samples=1000, n_features=10, n_informative=3, n_redundant=0,","84","        n_repeated=0, shuffle=False, random_state=0)","85","    max_features = X.shape[1]","86","    est = RandomForestClassifier(n_estimators=50, random_state=0)","87","","88","    transformer1 = SelectFromModel(estimator=est,","89","                                   threshold=-np.inf)","90","    transformer2 = SelectFromModel(estimator=est,","91","                                   max_features=max_features,","92","                                   threshold=-np.inf)","93","    X_new1 = transformer1.fit_transform(X, y)","94","    X_new2 = transformer2.fit_transform(X, y)","95","    assert_allclose(X_new1, X_new2)","96","","97","    # Test max_features against actual model.","98","    transformer1 = SelectFromModel(estimator=Lasso(alpha=0.025,","99","                                                   random_state=42))","100","    X_new1 = transformer1.fit_transform(X, y)","101","    scores1 = np.abs(transformer1.estimator_.coef_)","102","    candidate_indices1 = np.argsort(-scores1, kind='mergesort')","103","","104","    for n_features in range(1, X_new1.shape[1] + 1):","105","        transformer2 = SelectFromModel(estimator=Lasso(alpha=0.025,","106","                                       random_state=42),","107","                                       max_features=n_features,","108","                                       threshold=-np.inf)","109","        X_new2 = transformer2.fit_transform(X, y)","110","        scores2 = np.abs(transformer2.estimator_.coef_)","111","        candidate_indices2 = np.argsort(-scores2, kind='mergesort')","112","        assert_allclose(X[:, candidate_indices1[:n_features]],","113","                        X[:, candidate_indices2[:n_features]])","114","    assert_allclose(transformer1.estimator_.coef_,","115","                    transformer2.estimator_.coef_)","116","","117","","118","def test_max_features_tiebreak():","119","    # Test if max_features can break tie among feature importance","120","    X, y = datasets.make_classification(","121","        n_samples=1000, n_features=10, n_informative=3, n_redundant=0,","122","        n_repeated=0, shuffle=False, random_state=0)","123","    max_features = X.shape[1]","124","","125","    feature_importances = np.array([4, 4, 4, 4, 3, 3, 3, 2, 2, 1])","126","    for n_features in range(1, max_features + 1):","127","        transformer = SelectFromModel(","128","            FixedImportanceEstimator(feature_importances),","129","            max_features=n_features,","130","            threshold=-np.inf)","131","        X_new = transformer.fit_transform(X, y)","132","        selected_feature_indices = np.where(transformer._get_support_mask())[0]","133","        assert_array_equal(selected_feature_indices, np.arange(n_features))","134","        assert X_new.shape[1] == n_features","135","","136","","137","def test_threshold_and_max_features():","138","    X, y = datasets.make_classification(","139","        n_samples=1000, n_features=10, n_informative=3, n_redundant=0,","140","        n_repeated=0, shuffle=False, random_state=0)","141","    est = RandomForestClassifier(n_estimators=50, random_state=0)","142","","143","    transformer1 = SelectFromModel(estimator=est, max_features=3,","144","                                   threshold=-np.inf)","145","    X_new1 = transformer1.fit_transform(X, y)","146","","147","    transformer2 = SelectFromModel(estimator=est, threshold=0.04)","148","    X_new2 = transformer2.fit_transform(X, y)","149","","150","    transformer3 = SelectFromModel(estimator=est, max_features=3,","151","                                   threshold=0.04)","152","    X_new3 = transformer3.fit_transform(X, y)","153","    assert X_new3.shape[1] == min(X_new1.shape[1], X_new2.shape[1])","154","    selected_indices = transformer3.transform(","155","        np.arange(X.shape[1])[np.newaxis, :])","156","    assert_allclose(X_new3, X[:, selected_indices[0]])","157","","158","","159","@skip_if_32bit","207","    transformer = SelectFromModel(estimator=Lasso(alpha=0.1,","208","                                  random_state=42))"],"delete":["89","    transformer = SelectFromModel(estimator=Lasso(alpha=0.1))"]}]}},"22f0cf2de9f02f0df37896705cc87c8fb8b58f5e":{"changes":{"sklearn\/feature_extraction\/text.py":"MODIFY"},"diff":{"sklearn\/feature_extraction\/text.py":[{"add":["308","class HashingVectorizer(BaseEstimator, VectorizerMixin, TransformerMixin):"],"delete":["308","class HashingVectorizer(BaseEstimator, VectorizerMixin):","525","    # Alias transform to fit_transform for convenience","526","    fit_transform = transform","527",""]}]}},"dcaa26797e311d6f02e0927832de533ac0c2134f":{"changes":{"sklearn\/neighbors\/tests\/test_dist_metrics.py":"MODIFY"},"diff":{"sklearn\/neighbors\/tests\/test_dist_metrics.py":[{"add":["17","class TestMetrics(object):","18","    n1 = 20","19","    n2 = 25","20","    d = 4","21","    zero_frac = 0.5","22","    rseed = 0","23","    dtype = np.float64","24","    rng = check_random_state(rseed)","25","    X1 = rng.random_sample((n1, d)).astype(dtype)","26","    X2 = rng.random_sample((n2, d)).astype(dtype)","28","    # make boolean arrays: ones and zeros","29","    X1_bool = X1.round(0)","30","    X2_bool = X2.round(0)","32","    V = rng.random_sample((d, d))","33","    VI = np.dot(V, V.T)","35","    metrics = {'euclidean': {},","36","               'cityblock': {},","37","               'minkowski': dict(p=(1, 1.5, 2, 3)),","38","               'chebyshev': {},","39","               'seuclidean': dict(V=(rng.random_sample(d),)),","40","               'wminkowski': dict(p=(1, 1.5, 3),","41","                                  w=(rng.random_sample(d),)),","42","               'mahalanobis': dict(VI=(VI,)),","43","               'hamming': {},","44","               'canberra': {},","45","               'braycurtis': {}}","47","    bool_metrics = ['matching', 'jaccard', 'dice',","48","                    'kulsinski', 'rogerstanimoto', 'russellrao',","49","                    'sokalmichener', 'sokalsneath']"],"delete":["17","class TestMetrics:","18","    def __init__(self, n1=20, n2=25, d=4, zero_frac=0.5,","19","                 rseed=0, dtype=np.float64):","20","        rng = check_random_state(rseed)","21","        self.X1 = rng.random_sample((n1, d)).astype(dtype)","22","        self.X2 = rng.random_sample((n2, d)).astype(dtype)","24","        # make boolean arrays: ones and zeros","25","        self.X1_bool = self.X1.round(0)","26","        self.X2_bool = self.X2.round(0)","28","        V = rng.random_sample((d, d))","29","        VI = np.dot(V, V.T)","31","        self.metrics = {'euclidean': {},","32","                        'cityblock': {},","33","                        'minkowski': dict(p=(1, 1.5, 2, 3)),","34","                        'chebyshev': {},","35","                        'seuclidean': dict(V=(rng.random_sample(d),)),","36","                        'wminkowski': dict(p=(1, 1.5, 3),","37","                                           w=(rng.random_sample(d),)),","38","                        'mahalanobis': dict(VI=(VI,)),","39","                        'hamming': {},","40","                        'canberra': {},","41","                        'braycurtis': {}}","43","        self.bool_metrics = ['matching', 'jaccard', 'dice',","44","                             'kulsinski', 'rogerstanimoto', 'russellrao',","45","                             'sokalmichener', 'sokalsneath']"]}]}},"60e7e15cdebd6b5a794d10a133fef2c937541f9d":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/decomposition\/pca.py":"MODIFY","sklearn\/decomposition\/tests\/test_pca.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["109","- Fixed a bug in :class:`decomposition.PCA` where users will get unexpected error","110","  with large datasets when ``n_components='mle'`` on Python 3 versions.","111","  :issue:`9886` by :user:`Hanmin Qin <qinhanmin2014>`.","112",""],"delete":[]}],"sklearn\/decomposition\/pca.py":[{"add":["132","        If ``n_components == 'mle'`` and ``svd_solver == 'full'``, Minka\\'s","133","        MLE is used to guess the dimension. Use of ``n_components == 'mle'``","134","        will interpret ``svd_solver == 'auto'`` as ``svd_solver == 'full'``.","135","","136","        If ``0 < n_components < 1`` and ``svd_solver == 'full'``, select the","137","        number of components such that the amount of variance that needs to be","139","","140","        If ``svd_solver == 'arpack'``, the number of components must be","141","        strictly less than the minimum of n_features and n_samples.","142","","143","        Hence, the None case results in::","392","            # Small problem or n_components == 'mle', just call full PCA","393","            if max(X.shape) <= 500 or n_components == 'mle':"],"delete":["132","        if n_components == 'mle' and svd_solver == 'full', Minka\\'s MLE is used","133","        to guess the dimension","134","        if ``0 < n_components < 1`` and svd_solver == 'full', select the number","135","        of components such that the amount of variance that needs to be","137","        If svd_solver == 'arpack', the number of components must be strictly","138","        less than the minimum of n_features and n_samples.","139","        Hence, the None case results in:","388","            # Small problem, just call full PCA","389","            if max(X.shape) <= 500:"]}],"sklearn\/decomposition\/tests\/test_pca.py":[{"add":["9","from sklearn.utils.testing import assert_raise_message","456","def test_n_components_mle():","457","    # Ensure that n_components == 'mle' doesn't raise error for auto\/full","458","    # svd_solver and raises error for arpack\/randomized svd_solver","459","    rng = np.random.RandomState(0)","460","    n_samples = 600","461","    n_features = 10","462","    X = rng.randn(n_samples, n_features)","463","    n_components_dict = {}","464","    for solver in solver_list:","465","        pca = PCA(n_components='mle', svd_solver=solver)","466","        if solver in ['auto', 'full']:","467","            pca.fit(X)","468","            n_components_dict[solver] = pca.n_components_","469","        else:  # arpack\/randomized solver","470","            error_message = (\"n_components='mle' cannot be a string with \"","471","                             \"svd_solver='{}'\".format(solver))","472","            assert_raise_message(ValueError, error_message, pca.fit, X)","473","    assert_equal(n_components_dict['auto'], n_components_dict['full'])","474","","475",""],"delete":[]}]}},"4d414d41e488f566c328fb97a1d475cee833b0df":{"changes":{"examples\/applications\/plot_stock_market.py":"MODIFY"},"diff":{"examples\/applications\/plot_stock_market.py":[{"add":["79","","105","    response = urlopen(url)","106","    dtype = {","107","        'names': ['date', 'open', 'high', 'low', 'close', 'volume'],","108","        'formats': ['object', 'f4', 'f4', 'f4', 'f4', 'f4']","109","    }","110","    converters = {0: lambda s: datetime.strptime(s.decode(), '%d-%b-%y')}","111","    return np.genfromtxt(response, delimiter=',', skip_header=1,","112","                         dtype=dtype, converters=converters,","113","                         missing_values='-', filling_values=-1)","185","close_prices = np.vstack([q['close'] for q in quotes])","186","open_prices = np.vstack([q['open'] for q in quotes])"],"delete":["104","    with urlopen(url) as response:","105","        dtype = {","106","            'names': ['date', 'open', 'high', 'low', 'close', 'volume'],","107","            'formats': ['object', 'f4', 'f4', 'f4', 'f4', 'f4']","108","        }","109","        converters = {0: lambda s: datetime.strptime(s.decode(), '%d-%b-%y')}","110","        return np.genfromtxt(response, delimiter=',', skip_header=1,","111","                             dtype=dtype, converters=converters,","112","                             missing_values='-', filling_values=-1)","184","close_prices = np.stack([q['close'] for q in quotes])","185","open_prices = np.stack([q['open'] for q in quotes])"]}]}},"5a74e2f1c8470c527018dba78f86557f40eaeb47":{"changes":{"doc\/images\/sloan_banner.png":"ADD","doc\/themes\/scikit-learn\/static\/img\/sloan_logo.jpg":"ADD","doc\/about.rst":"MODIFY","doc\/index.rst":"MODIFY"},"diff":{"doc\/images\/sloan_banner.png":[{"add":[],"delete":[]}],"doc\/themes\/scikit-learn\/static\/img\/sloan_logo.jpg":[{"add":[],"delete":[]}],"doc\/about.rst":[{"add":["106","`Columbia University <http:\/\/columbia.edu>`_ funds Andreas Mller since 2016.","113","Andreas Mller also received a grant to improve scikit-learn from the `Alfred P. Sloan Foundation <https:\/\/sloan.org>`_ in 2017.","114","","115",".. image:: images\/sloan_banner.png","116","   :width: 200pt","117","   :align: center","118","   :target: https:\/\/sloan.org\/","119",""],"delete":["106","`Columbia University <http:\/\/columbia.edu>`_ funds Andreas Mueller since 2016."]}],"doc\/index.rst":[{"add":["331","                   <img id=\"index-funding-logo-small\" src=\"_static\/img\/sloan_logo.jpg\" title=\"Alfred P. Sloan Foundation\" style=\"max-height: 36px\">"],"delete":["331","                   <img id=\"index-funding-logo-small\" src=\"_static\/img\/nyu_short_color.png\" title=\"NYU CDS\">"]}]}},"d8c363f296948a9171ac8a5d69f79dcb56589335":{"changes":{".mailmap":"MODIFY"},"diff":{".mailmap":[{"add":["28","Denis Engemann <denis-alexander.engemann@inria.fr> dengemann <denis.engemann@gmail.com>"],"delete":["28","Denis Engemann <denis-alexander.engemann@inria.fr> <dengemann <denis.engemann@gmail.com>"]}]}},"90607f16675cd8706c8de5fb3e2d33f440f2b59c":{"changes":{"sklearn\/neighbors\/tests\/test_nearest_centroid.py":"MODIFY","sklearn\/neighbors\/nearest_centroid.py":"MODIFY","doc\/whats_new.rst":"MODIFY"},"diff":{"sklearn\/neighbors\/tests\/test_nearest_centroid.py":[{"add":["99","def test_shrinkage_correct():","100","    # Ensure that the shrinking is correct.","101","    # The expected result is calculated by R (pamr),","102","    # which is implemented by the author of the original paper.","103","    # (One need to modify the code to output the new centroid in pamr.predict)","104","","105","    X = np.array([[0, 1], [1, 0], [1, 1], [2, 0], [6, 8]])","106","    y = np.array([1, 1, 2, 2, 2])","107","    clf = NearestCentroid(shrink_threshold=0.1)","108","    clf.fit(X, y)","109","    expected_result = np.array([[0.7787310, 0.8545292], [2.814179, 2.763647]])","110","    np.testing.assert_array_almost_equal(clf.centroids_, expected_result)","111","","112",""],"delete":[]}],"sklearn\/neighbors\/nearest_centroid.py":[{"add":["149","            m = np.sqrt((1. \/ nk) - (1. \/ n_samples))"],"delete":["149","            m = np.sqrt((1. \/ nk) + (1. \/ n_samples))"]}],"doc\/whats_new.rst":[{"add":["65","- :class:`neighbors.NearestCentroid` (bug fix)","539","- Fixed the shrinkage implementation in :class:`neighbors.NearestCentroid`.","540","  :issue:`9219` by `Hanmin Qin <https:\/\/github.com\/qinhanmin2014>`_.","541",""],"delete":[]}]}},"ea9896b8f4b177b508b1136c6fdd710886787f8d":{"changes":{"sklearn\/cluster\/hierarchical.py":"MODIFY","sklearn\/cluster\/tests\/test_hierarchical.py":"MODIFY"},"diff":{"sklearn\/cluster\/hierarchical.py":[{"add":["32","def _fix_connectivity(X, connectivity, affinity):","191","    connectivity, n_components = _fix_connectivity(X, connectivity,","192","                                                   affinity='euclidean')","424","    connectivity, n_components = _fix_connectivity(X, connectivity,","425","                                                   affinity=affinity)"],"delete":["32","def _fix_connectivity(X, connectivity, n_components=None,","33","                      affinity=\"euclidean\"):","192","    connectivity, n_components = _fix_connectivity(X, connectivity)","424","    connectivity, n_components = _fix_connectivity(X, connectivity)"]}],"sklearn\/cluster\/tests\/test_hierarchical.py":[{"add":["534","","535","","536","def test_affinity_passed_to_fix_connectivity():","537","    # Test that the affinity parameter is actually passed to the pairwise","538","    # function","539","","540","    size = 2","541","    rng = np.random.RandomState(0)","542","    X = rng.randn(size, size)","543","    mask = np.array([True, False, False, True])","544","","545","    connectivity = grid_to_graph(n_x=size, n_y=size,","546","                                 mask=mask, return_as=np.ndarray)","547","","548","    class FakeAffinity:","549","        def __init__(self):","550","            self.counter = 0","551","","552","        def increment(self, *args, **kwargs):","553","            self.counter += 1","554","            return self.counter","555","","556","    fa = FakeAffinity()","557","","558","    linkage_tree(X, connectivity=connectivity, affinity=fa.increment)","559","","560","    assert_equal(fa.counter, 3)"],"delete":[]}]}},"1755b893df191ad593ce90848eec999f8ca6d411":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/feature_selection\/rfe.py":"MODIFY","sklearn\/feature_selection\/tests\/test_rfe.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["473","Feature selection","474","","475","- Fixed computation of ``n_features_to_compute`` for edge case with tied","476","  CV scores in :class:`RFECV`. :issue:`9222` by `Nick Hoh <nickypie>`.","477",""],"delete":[]}],"sklearn\/feature_selection\/rfe.py":[{"add":["451","        scores_rev = scores[::-1]","452","        argmax_idx = len(scores) - np.argmax(scores_rev) - 1","454","            n_features - (argmax_idx * step),"],"delete":["452","            n_features - (np.argmax(scores) * step),"]}],"sklearn\/feature_selection\/tests\/test_rfe.py":[{"add":["169","    # In the event of cross validation score ties, the expected behavior of","170","    # RFECV is to return the FEWEST features that maximize the CV score.","171","    # Because test_scorer always returns 1.0 in this example, RFECV should","172","    # reduce the dimensionality to a single feature (i.e. n_features_ = 1)","173","    assert_equal(rfecv.n_features_, 1)"],"delete":[]}]}},"45dc891c96eebdb3b81bf14c2737d8f6540fabfe":{"changes":{"sklearn\/gaussian_process\/gpc.py":"MODIFY","sklearn\/linear_model\/ransac.py":"MODIFY","sklearn\/discriminant_analysis.py":"MODIFY","sklearn\/manifold\/tests\/test_t_sne.py":"MODIFY","sklearn\/mixture\/base.py":"MODIFY","sklearn\/neighbors\/nearest_centroid.py":"MODIFY","sklearn\/utils\/estimator_checks.py":"MODIFY","sklearn\/cluster\/spectral.py":"MODIFY","sklearn\/feature_selection\/univariate_selection.py":"MODIFY","sklearn\/cluster\/mean_shift_.py":"MODIFY","sklearn\/model_selection\/_split.py":"MODIFY","sklearn\/linear_model\/stochastic_gradient.py":"MODIFY","sklearn\/ensemble\/gradient_boosting.py":"MODIFY","sklearn\/decomposition\/fastica_.py":"MODIFY","sklearn\/svm\/base.py":"MODIFY","sklearn\/manifold\/t_sne.py":"MODIFY","sklearn\/linear_model\/bayes.py":"MODIFY","sklearn\/manifold\/locally_linear.py":"MODIFY","sklearn\/cross_decomposition\/pls_.py":"MODIFY","sklearn\/cluster\/tests\/test_mean_shift.py":"MODIFY","sklearn\/linear_model\/ridge.py":"MODIFY"},"diff":{"sklearn\/gaussian_process\/gpc.py":[{"add":["191","            raise ValueError(\"{0:s} requires 2 classes; got {1:d} class\"","192","                             .format(self.__class__.__name__,","193","                                     self.classes_.size))","598","                             \"distinct classes; got %d class (only class %s \"","599","                             \"is present)\"","600","                             % (self.n_classes_, self.classes_[0]))"],"delete":["191","            raise ValueError(\"{0:s} requires 2 classes.\".format(","192","                self.__class__.__name__))","597","                             \"distinct classes. Only class %s present.\"","598","                             % self.classes_[0])"]}],"sklearn\/linear_model\/ransac.py":[{"add":["272","                             \"of samples: n_samples = %d.\" % (X.shape[0]))"],"delete":["272","                             \"of samples ``X.shape[0]``.\")"]}],"sklearn\/discriminant_analysis.py":[{"add":["652","            raise ValueError('The number of classes has to be greater than'","653","                             ' one; got %d class' % (n_classes))"],"delete":["652","            raise ValueError('y has less than 2 classes')"]}],"sklearn\/manifold\/tests\/test_t_sne.py":[{"add":["297","                         tsne.fit_transform, np.array([[0.0], [0.0]]))","304","                         np.array([[0.0], [0.0]]))"],"delete":["297","                         tsne.fit_transform, np.array([[0.0]]))","304","                         np.array([[0.0]]))"]}],"sklearn\/mixture\/base.py":[{"add":["40","def _check_X(X, n_components=None, n_features=None, ensure_min_samples=1):","53","    X = check_array(X, dtype=[np.float64, np.float32],","54","                    ensure_min_samples=ensure_min_samples)","190","        X = _check_X(X, self.n_components, ensure_min_samples=2)"],"delete":["40","def _check_X(X, n_components=None, n_features=None):","53","    X = check_array(X, dtype=[np.float64, np.float32])","189","        X = _check_X(X, self.n_components)"]}],"sklearn\/neighbors\/nearest_centroid.py":[{"add":["117","            raise ValueError('The number of classes has to be greater than'","118","                             ' one; got %d class' % (n_classes))"],"delete":["117","            raise ValueError('y has less than 2 classes')"]}],"sklearn\/utils\/estimator_checks.py":[{"add":["126","    yield check_supervised_y_no_nan","225","    if name != 'GaussianProcess':  # FIXME","226","        # XXX GaussianProcess deprecated in 0.20","227","        yield check_fit2d_1sample","229","    yield check_fit1d","591","    # Check that fitting a 2d array with only one sample either works or","592","    # returns an informative message. The error message should either mention","593","    # the number of samples or the number of classes.","606","","607","    msgs = [\"1 sample\", \"n_samples = 1\", \"n_samples=1\", \"one sample\",","608","            \"1 class\", \"one class\"]","609","","612","    except ValueError as e:","613","        if all(msg not in repr(e) for msg in msgs):","614","            raise e","619","    # check fitting a 2d array with only 1 feature either works or returns","620","    # informative message","631","    # ensure two labels in subsample for RandomizedLogisticRegression","632","    if name == 'RandomizedLogisticRegression':","633","        estimator.sample_fraction = 1","634","    # ensure non skipped trials for RANSACRegressor","635","    if name == 'RANSACRegressor':","636","        estimator.residual_threshold = 0.5","638","    y = multioutput_estimator_convert_y_2d(estimator, y)","640","","641","    msgs = [\"1 feature(s)\", \"n_features = 1\", \"n_features=1\"]","642","","645","    except ValueError as e:","646","        if all(msg not in repr(e) for msg in msgs):","647","            raise e","651","def check_fit1d(name, estimator_orig):","652","    # check fitting 1d X array raises a ValueError","665","    assert_raises(ValueError, estimator.fit, X, y)"],"delete":["224","    yield check_fit2d_1sample","226","    yield check_fit1d_1feature","227","    yield check_fit1d_1sample","589","    # check by fitting a 2d array and prediting with a 1d array","604","    except ValueError:","605","        pass","610","    # check by fitting a 2d array and prediting with a 1d array","625","    except ValueError:","626","        pass","630","def check_fit1d_1feature(name, estimator_orig):","631","    # check fitting 1d array with 1 feature","644","","645","    try:","646","        estimator.fit(X, y)","647","    except ValueError:","648","        pass","649","","650","","651","@ignore_warnings","652","def check_fit1d_1sample(name, estimator_orig):","653","    # check fitting 1d array with 1 feature","654","    rnd = np.random.RandomState(0)","655","    X = 3 * rnd.uniform(size=(20))","656","    y = np.array([1])","657","    estimator = clone(estimator_orig)","658","    y = multioutput_estimator_convert_y_2d(estimator, y)","659","","660","    if hasattr(estimator, \"n_components\"):","661","        estimator.n_components = 1","662","    if hasattr(estimator, \"n_clusters\"):","663","        estimator.n_clusters = 1","664","","665","    set_random_state(estimator, 1)","666","","667","    try:","668","        estimator.fit(X, y)","669","    except ValueError:","670","        pass"]}],"sklearn\/cluster\/spectral.py":[{"add":["439","                        dtype=np.float64, ensure_min_samples=2)","447","            connectivity = kneighbors_graph(X, n_neighbors=self.n_neighbors,","448","                                            include_self=True,"],"delete":["439","                        dtype=np.float64)","447","            connectivity = kneighbors_graph(X, n_neighbors=self.n_neighbors, include_self=True,"]}],"sklearn\/feature_selection\/univariate_selection.py":[{"add":["490","            raise ValueError(\"k should be >=0, <= n_features = %d; got %r. \"","492","                             % (X.shape[1], self.k))"],"delete":["490","            raise ValueError(\"k should be >=0, <= n_features; got %r.\"","492","                             % self.k)"]}],"sklearn\/cluster\/mean_shift_.py":[{"add":["70","    n_neighbors = int(X.shape[0] * quantile)","71","    if n_neighbors < 1:  # cannot fit NearestNeighbors with n_neighbors = 0","72","        n_neighbors = 1","73","    nbrs = NearestNeighbors(n_neighbors=n_neighbors,"],"delete":["70","    nbrs = NearestNeighbors(n_neighbors=int(X.shape[0] * quantile),"]}],"sklearn\/model_selection\/_split.py":[{"add":["328","                 \" than the number of samples: n_samples={1}.\")","329","                .format(self.n_splits, n_samples))"],"delete":["328","                 \" than the number of samples: {1}.\").format(self.n_splits,","329","                                                             n_samples))"]}],"sklearn\/linear_model\/stochastic_gradient.py":[{"add":["406","            raise ValueError(","407","                \"The number of classes has to be greater than one;\"","408","                \" got %d class\" % n_classes)"],"delete":["406","            raise ValueError(\"The number of class labels must be \"","407","                             \"greater than one.\")"]}],"sklearn\/ensemble\/gradient_boosting.py":[{"add":["471","            raise ValueError(\"{0:s} requires 2 classes; got {1:d} class(es)\"","472","                             .format(self.__class__.__name__, n_classes))","604","            raise ValueError(\"{0:s} requires 2 classes; got {1:d} class(es)\"","605","                             .format(self.__class__.__name__, n_classes))"],"delete":["471","            raise ValueError(\"{0:s} requires 2 classes.\".format(","472","                self.__class__.__name__))","604","            raise ValueError(\"{0:s} requires 2 classes.\".format(","605","                self.__class__.__name__))"]}],"sklearn\/decomposition\/fastica_.py":[{"add":["269","    X = check_array(X, copy=whiten, dtype=FLOAT_DTYPES,","270","                    ensure_min_samples=2).T"],"delete":["269","    X = check_array(X, copy=whiten, dtype=FLOAT_DTYPES).T"]}],"sklearn\/svm\/base.py":[{"add":["505","                \" class\" % len(cls))"],"delete":["505","                % len(cls))"]}],"sklearn\/manifold\/t_sne.py":[{"add":["658","        if self.method == 'barnes_hut':","659","            X = check_array(X, ensure_min_samples=2,","660","                            dtype=[np.float32, np.float64])"],"delete":[]}],"sklearn\/linear_model\/bayes.py":[{"add":["428","        X, y = check_X_y(X, y, dtype=np.float64, y_numeric=True,","429","                         ensure_min_samples=2)"],"delete":["428","        X, y = check_X_y(X, y, dtype=np.float64, y_numeric=True)"]}],"sklearn\/manifold\/locally_linear.py":[{"add":["300","        raise ValueError(","301","            \"Expected n_neighbors <= n_samples, \"","302","            \" but n_samples = %d, n_neighbors = %d\" %","303","            (N, n_neighbors)","304","        )"],"delete":["300","        raise ValueError(\"n_neighbors must be less than number of points\")"]}],"sklearn\/cross_decomposition\/pls_.py":[{"add":["247","        X = check_array(X, dtype=np.float64, copy=self.copy,","248","                        ensure_min_samples=2)","800","        X = check_array(X, dtype=np.float64, copy=self.copy,","801","                        ensure_min_samples=2)"],"delete":["247","        X = check_array(X, dtype=np.float64, copy=self.copy)","799","        X = check_array(X, dtype=np.float64, copy=self.copy)"]}],"sklearn\/cluster\/tests\/test_mean_shift.py":[{"add":["36","def test_estimate_bandwidth_1sample():","37","    # Test estimate_bandwidth when n_samples=1 and quantile<1, so that","38","    # n_neighbors is set to 1.","39","    bandwidth = estimate_bandwidth(X, n_samples=1, quantile=0.3)","40","    assert_equal(bandwidth, 0.)","41","","42",""],"delete":[]}],"sklearn\/linear_model\/ridge.py":[{"add":["806","        check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'],","807","                  multi_output=True)","808","","1353","        check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'],","1354","                  multi_output=True)","1355",""],"delete":[]}]}}}