{"5d7df621a1b74b9b0c9ccd054f165a81fbb6a7ec":{"changes":{"sklearn\/metrics\/cluster\/tests\/test_supervised.py":"MODIFY","doc\/modules\/model_evaluation.rst":"MODIFY"},"diff":{"sklearn\/metrics\/cluster\/tests\/test_supervised.py":[{"add":["253","    # symmetric property","254","    score_symmetric = fowlkes_mallows_score(labels_b, labels_a)","255","    assert_almost_equal(score_symmetric, expected)","261","    # symmetric and permutation(both together)"],"delete":["253","    # symetric property","254","    score_symetric = fowlkes_mallows_score(labels_b, labels_a)","255","    assert_almost_equal(score_symetric, expected)","261","    # symetric and permutation(both together)"]}],"doc\/modules\/model_evaluation.rst":[{"add":["8","There are 3 different APIs for evaluating the quality of a model's","1123","predicted probability of the actual outcome :math:`o_t`."],"delete":["8","There are 3 different APIs for evaluating the quality of of a model's","1123","predicted probablity of the actual outcome :math:`o_t`."]}]}},"9b5561148f56a3934da9882a52f1978d7aa5bc75":{"changes":{"examples\/semi_supervised\/plot_label_propagation_digits_active_learning.py":"MODIFY","examples\/ensemble\/plot_bias_variance.py":"MODIFY"},"diff":{"examples\/semi_supervised\/plot_label_propagation_digits_active_learning.py":[{"add":["67","          % (n_labeled_points, n_total_samples - n_labeled_points,","68","             n_total_samples))","98","            sub.imshow(image, cmap=plt.cm.gray_r, interpolation='none')","111","           \"uncertain labels to learn with the next model.\", y=1.15)","112","plt.subplots_adjust(left=0.2, bottom=0.03, right=0.9, top=0.9, wspace=0.2,","113","                    hspace=0.85)"],"delete":["67","          % (n_labeled_points, n_total_samples - n_labeled_points, n_total_samples))","97","            sub.imshow(image, cmap=plt.cm.gray_r)","110","           \"uncertain labels to learn with the next model.\")","111","plt.subplots_adjust(0.12, 0.03, 0.9, 0.8, 0.2, 0.45)"]}],"examples\/ensemble\/plot_bias_variance.py":[{"add":["90","","97","","114","","125","plt.figure(figsize=(10, 8))","126","","173","    if n == n_estimators - 1:","174","        plt.legend(loc=(1.1, .5))","185","    if n == n_estimators - 1:","187","        plt.legend(loc=(1.1, .5))","188","","189","plt.subplots_adjust(right=.75)"],"delete":["168","    if n == 0:","169","        plt.legend(loc=\"upper left\", prop={\"size\": 11})","180","    if n == 0:","181","        plt.legend(loc=\"upper left\", prop={\"size\": 11})"]}]}},"b4b5de8cf9748a07d8f3a2d1fc89ccaacdf6576f":{"changes":{"sklearn\/naive_bayes.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/tests\/test_naive_bayes.py":"MODIFY"},"diff":{"sklearn\/naive_bayes.py":[{"add":["17","import warnings","439","_ALPHA_MIN = 1e-10","440","","465","    def _check_alpha(self):","466","        if self.alpha < 0:","467","            raise ValueError('Smoothing parameter alpha = %.1e. '","468","                             'alpha should be > 0.' % self.alpha)","469","        if self.alpha < _ALPHA_MIN:","470","            warnings.warn('alpha too small will result in numeric errors, '","471","                          'setting alpha = %.1e' % _ALPHA_MIN)","472","            return _ALPHA_MIN","473","        return self.alpha","474","","553","        alpha = self._check_alpha()","554","        self._update_feature_log_prob(alpha)","604","        alpha = self._check_alpha()","605","        self._update_feature_log_prob(alpha)","711","    def _update_feature_log_prob(self, alpha):","713","        smoothed_fc = self.feature_count_ + alpha","813","    def _update_feature_log_prob(self, alpha):","815","        smoothed_fc = self.feature_count_ + alpha","816","        smoothed_cc = self.class_count_ + alpha * 2"],"delete":["540","        self._update_feature_log_prob()","590","        self._update_feature_log_prob()","696","    def _update_feature_log_prob(self):","698","        smoothed_fc = self.feature_count_ + self.alpha","798","    def _update_feature_log_prob(self):","800","        smoothed_fc = self.feature_count_ + self.alpha","801","        smoothed_cc = self.class_count_ + self.alpha * 2"]}],"doc\/whats_new.rst":[{"add":["261","   - Fixed a bug where :class:`sklearn.naive_bayes.MultinomialNB` and :class:`sklearn.naive_bayes.BernoulliNB`","262","     failed when `alpha=0`. :issue:`5814` by :user:`Yichuan Liu <yl565>` and ","263","     :user:`Herilalaina Rakotoarison <herilalaina>`.","264",""],"delete":[]}],"sklearn\/tests\/test_naive_bayes.py":[{"add":["16","from sklearn.utils.testing import assert_raise_message","18","from sklearn.utils.testing import assert_warns","484","    assert_array_almost_equal(clf.feature_log_prob_, (num - denom))","540","","541","","542","def test_alpha():","543","    # Setting alpha=0 should not output nan results when p(x_i|y_j)=0 is a case","544","    X = np.array([[1, 0], [1, 1]])","545","    y = np.array([0, 1])","546","    nb = BernoulliNB(alpha=0.)","547","    assert_warns(UserWarning, nb.partial_fit, X, y, classes=[0, 1])","548","    assert_warns(UserWarning, nb.fit, X, y)","549","    prob = np.array([[1, 0], [0, 1]])","550","    assert_array_almost_equal(nb.predict_proba(X), prob)","551","","552","    nb = MultinomialNB(alpha=0.)","553","    assert_warns(UserWarning, nb.partial_fit, X, y, classes=[0, 1])","554","    assert_warns(UserWarning, nb.fit, X, y)","555","    prob = np.array([[2.\/3, 1.\/3], [0, 1]])","556","    assert_array_almost_equal(nb.predict_proba(X), prob)","557","","558","    # Test sparse X","559","    X = scipy.sparse.csr_matrix(X)","560","    nb = BernoulliNB(alpha=0.)","561","    assert_warns(UserWarning, nb.fit, X, y)","562","    prob = np.array([[1, 0], [0, 1]])","563","    assert_array_almost_equal(nb.predict_proba(X), prob)","564","","565","    nb = MultinomialNB(alpha=0.)","566","    assert_warns(UserWarning, nb.fit, X, y)","567","    prob = np.array([[2.\/3, 1.\/3], [0, 1]])","568","    assert_array_almost_equal(nb.predict_proba(X), prob)","569","","570","    # Test for alpha < 0","571","    X = np.array([[1, 0], [1, 1]])","572","    y = np.array([0, 1])","573","    expected_msg = ('Smoothing parameter alpha = -1.0e-01. '","574","                    'alpha should be > 0.')","575","    b_nb = BernoulliNB(alpha=-0.1)","576","    m_nb = MultinomialNB(alpha=-0.1)","577","    assert_raise_message(ValueError, expected_msg, b_nb.fit, X, y)","578","    assert_raise_message(ValueError, expected_msg, m_nb.fit, X, y)","579","","580","    b_nb = BernoulliNB(alpha=-0.1)","581","    m_nb = MultinomialNB(alpha=-0.1)","582","    assert_raise_message(ValueError, expected_msg, b_nb.partial_fit,","583","                         X, y, classes=[0, 1])","584","    assert_raise_message(ValueError, expected_msg, m_nb.partial_fit,","585","                         X, y, classes=[0, 1])"],"delete":["482","    assert_array_equal(clf.feature_log_prob_, (num - denom))"]}]}},"66196a6b8c56dbf6b3fb654be1c1bc2c9790937b":{"changes":{"doc\/whats_new.rst":"MODIFY"},"diff":{"doc\/whats_new.rst":[{"add":["197","","198","- Added ``flatten_transform`` parameter to :class:`ensemble.VotingClassifier`","199","  to change output shape of `transform` method to 2 dimensional.","200","  :issue:`7794` by :user:`Ibraim Ganiev <olologin>` and","201","  :user:`Herilalaina Rakotoarison <herilalaina>`.","263","- Fixed the implementation of noise_variance_ in :class:`decomposition.PCA`.","264","  :issue:`9108` by `Hanmin Qin <https:\/\/github.com\/qinhanmin2014>`_.","265","","524","  :issue:`9105` by `Hanmin Qin <https:\/\/github.com\/qinhanmin2014>`_.","628","- Cross validation now works with Pandas datatypes that that have a","629","  read-only index. :issue:`9507` by `Loic Esteve`_.","630","","879","Code and Documentation Contributors","880","-----------------------------------","881","","882","Thanks to everyone who has contributed to the maintenance and improvement of the","883","project since version 0.18, including:","884","","885","Joel Nothman, Loic Esteve, Andreas Mueller, Guillaume Lemaitre, Olivier Grisel,","886","Hanmin Qin, Raghav RV, Alexandre Gramfort, themrmax, Aman Dalmia, Gael","887","Varoquaux, Naoya Kanai, Tom Dupr¨¦ la Tour, Rishikesh, Nelson Liu, Taehoon Lee,","888","Nelle Varoquaux, Aashil, Mikhail Korobov, Sebastin Santy, Joan Massich, Roman","889","Yurchak, RAKOTOARISON Herilalaina, Thierry Guillemot, Alexandre Abadie, Carol","890","Willing, Balakumaran Manoharan, Josh Karnofsky, Vlad Niculae, Utkarsh Upadhyay,","891","Dmitry Petrov, Minghui Liu, Srivatsan, Vincent Pham, Albert Thomas, Jake","892","VanderPlas, Attractadore, JC Liu, alexandercbooth, chkoar, ?scar N¨¢jera,","893","Aarshay Jain, Kyle Gilliam, Ramana Subramanyam, CJ Carey, Clement Joudet, David","894","Robles, He Chen, Joris Van den Bossche, Karan Desai, Katie Luangkote, Leland","895","McInnes, Maniteja Nandana, Michele Lacchia, Sergei Lebedev, Shubham Bhardwaj,","896","akshay0724, omtcyfz, rickiepark, waterponey, Vathsala Achar, jbDelafosse, Ralf","897","Gommers, Ekaterina Krivich, Vivek Kumar, Ishank Gulati, Dave Elliott, ldirer,","898","Reiichiro Nakano, Levi John Wolf, Mathieu Blondel, Sid Kapur, Dougal J.","899","Sutherland, midinas, mikebenfield, Sourav Singh, Aseem Bansal, Ibraim Ganiev,","900","Stephen Hoover, AishwaryaRK, Steven C. Howell, Gary Foreman, Neeraj Gangwar,","901","Tahar, Jon Crall, dokato, Kathy Chen, ferria, Thomas Moreau, Charlie Brummitt,","902","Nicolas Goix, Adam Kleczewski, Sam Shleifer, Nikita Singh, Basil Beirouti,","903","Giorgio Patrini, Manoj Kumar, Rafael Possas, James Bourbeau, James A. Bednar,","904","Janine Harper, Jaye, Jean Helie, Jeremy Steward, Artsiom, John Wei, Jonathan","905","LIgo, Jonathan Rahn, seanpwilliams, Arthur Mensch, Josh Levy, Julian Kuhlmann,","906","Julien Aubert, J?rn Hees, Kai, shivamgargsya, Kat Hempstalk, Kaushik","907","Lakshmikanth, Kennedy, Kenneth Lyons, Kenneth Myers, Kevin Yap, Kirill Bobyrev,","908","Konstantin Podshumok, Arthur Imbert, Lee Murray, toastedcornflakes, Lera, Li","909","Li, Arthur Douillard, Mainak Jas, tobycheese, Manraj Singh, Manvendra Singh,","910","Marc Meketon, MarcoFalke, Matthew Brett, Matthias Gilch, Mehul Ahuja, Melanie","911","Goetz, Meng, Peng, Michael Dezube, Michal Baumgartner, vibrantabhi19, Artem","912","Golubin, Milen Paskov, Antonin Carette, Morikko, MrMjauh, NALEPA Emmanuel,","913","Namiya, Antoine Wendlinger, Narine Kokhlikyan, NarineK, Nate Guerin, Angus","914","Williams, Ang Lu, Nicole Vavrova, Nitish Pandey, Okhlopkov Daniil Olegovich,","915","Andy Craze, Om Prakash, Parminder Singh, Patrick Carlson, Patrick Pei, Paul","916","Ganssle, Paulo Haddad, Pawe? Lorek, Peng Yu, Pete Bachant, Peter Bull, Peter","917","Csizsek, Peter Wang, Pieter Arthur de Jong, Ping-Yao, Chang, Preston Parry,","918","Puneet Mathur, Quentin Hibon, Andrew Smith, Andrew Jackson, 1kastner, Rameshwar","919","Bhaskaran, Rebecca Bilbro, Remi Rampin, Andrea Esuli, Rob Hall, Robert","920","Bradshaw, Romain Brault, Aman Pratik, Ruifeng Zheng, Russell Smith, Sachin","921","Agarwal, Sailesh Choyal, Samson Tan, Samu?l Weber, Sarah Brown, Sebastian","922","P?lsterl, Sebastian Raschka, Sebastian Saeger, Alyssa Batula, Abhyuday Pratap","923","Singh, Sergey Feldman, Sergul Aydore, Sharan Yalburgi, willduan, Siddharth","924","Gupta, Sri Krishna, Almer, Stijn Tonk, Allen Riddell, Theofilos Papapanagiotou,","925","Alison, Alexis Mignon, Tommy Boucher, Tommy L?fstedt, Toshihiro Kamishima,","926","Tyler Folkman, Tyler Lanigan, Alexander Junge, Varun Shenoy, Victor Poughon,","927","Vilhelm von Ehrenheim, Aleksandr Sandrovskii, Alan Yee, Vlasios Vasileiou,","928","Warut Vijitbenjaronk, Yang Zhang, Yaroslav Halchenko, Yichuan Liu, Yuichi","929","Fujikawa, affanv14, aivision2020, xor, andreh7, brady salz, campustrampus,","930","Agamemnon Krasoulis, ditenberg, elena-sharova, filipj8, fukatani, gedeck,","931","guiniol, guoci, hakaa1, hongkahjun, i-am-xhy, jakirkham, jaroslaw-weber,","932","jayzed82, jeroko, jmontoyam, jonathan.striebel, josephsalmon, jschendel,","933","leereeves, martin-hahn, mathurinm, mehak-sachdeva, mlewis1729, mlliou112,","934","mthorrell, ndingwall, nuffe, yangarbiter, plagree, pldtc325, Breno Freitas,","935","Brett Olsen, Brian A. Alfano, Brian Burns, polmauri, Brandon Carter, Charlton","936","Austin, Chayant T15h, Chinmaya Pancholi, Christian Danielsen, Chung Yen,","937","Chyi-Kwei Yau, pravarmahajan, DOHMATOB Elvis, Daniel LeJeune, Daniel Hnyk,","938","Darius Morawiec, David DeTomaso, David Gasquez, David Haberth¨¹r, David","939","Heryanto, David Kirkby, David Nicholson, rashchedrin, Deborah Gertrude Digges,","940","Denis Engemann, Devansh D, Dickson, Bob Baxley, Don86, E. Lynch-Klarup, Ed","941","Rogers, Elizabeth Ferriss, Ellen-Co2, Fabian Egli, Fang-Chieh Chou, Bing Tian","942","Dai, Greg Stupp, Grzegorz Szpak, Bertrand Thirion, Hadrien Bertrand, Harizo","943","Rajaona, zxcvbnius, Henry Lin, Holger Peters, Icyblade Dai, Igor","944","Andriushchenko, Ilya, Isaac Laughlin, Iv¨¢n Vall¨¦s, Aur¨¦lien Bellet, JPFrancoia,","945","Jacob Schreiber, Asish Mahapatra","946",""],"delete":["197","   - :func:`tree.export_graphviz` now shows configurable number of decimal","198","     places. :issue:`8698` by :user:`Guillaume Lemaitre <glemaitre>`.","199","     ","200","   - Added ``flatten_transform`` parameter to :class:`ensemble.VotingClassifier`","201","     to change output shape of `transform` method to 2 dimensional.","202","     :issue:`7794` by :user:`Ibraim Ganiev <olologin>` and","203","     :user:`Herilalaina Rakotoarison <herilalaina>`.","348","   - :class:`multioutput.MultiOutputRegressor` and :class:`multioutput.MultiOutputClassifier`","349","     now support online learning using ``partial_fit``.","350","     :issue:`8053` by :user:`Peng Yu <yupbank>`.","526","  :issue:`9105` by `Hanmin Qin <https:\/\/github.com\/qinhanmin2014>`_. "]}]}},"c95784d14e2024ae09c425f6541bbb06554230b3":{"changes":{"sklearn\/neighbors\/lof.py":"MODIFY"},"diff":{"sklearn\/neighbors\/lof.py":[{"add":["108","        The opposite LOF of the training samples. The lower, the more abnormal."],"delete":["108","        The opposite LOF of the training samples. The lower, the more normal."]}]}},"8e1efb0d9cc0e603292713b44dc78a7e8c77cd55":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/neighbors\/tests\/test_neighbors.py":"MODIFY","sklearn\/neighbors\/regression.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["129","","177","- Fixed a bug in :func:`datasets.make_circles`, where no odd number of data","178","  points could be generated. :issue:`10037` by :user:`Christian Braune","180","","192","Neighbors","193","","194","- Fixed a bug so ``predict`` in :class:`neighbors.RadiusNeighborsRegressor` can","195","  handle empty neighbor set when using non uniform weights. Also raises a new","196","  warning when no neighbors are found for samples.  :issue:`9655` by","197","  :user:`Andreas Bjerre-Nielsen <abjer>`.","198",""],"delete":["129","  ","177","- Fixed a bug in :func:`datasets.make_circles`, where no odd number of data ","178","  points could be generated. :issue:`10037` by :user:`Christian Braune ","180","  "]}],"sklearn\/neighbors\/tests\/test_neighbors.py":[{"add":["22","from sklearn.utils.testing import assert_warns_message","661","    # test that nan is returned when no nearby observations","662","    for weights in ['uniform', 'distance']:","663","        neigh = neighbors.RadiusNeighborsRegressor(radius=radius,","664","                                                   weights=weights,","665","                                                   algorithm='auto')","666","        neigh.fit(X, y)","667","        X_test_nan = np.ones((1, n_features))*-1","668","        empty_warning_msg = (\"One or more samples have no neighbors \"","669","                             \"within specified radius; predicting NaN.\")","670","        pred = assert_warns_message(UserWarning,","671","                                    empty_warning_msg,","672","                                    neigh.predict,","673","                                    X_test_nan)","674","        assert_true(np.all(np.isnan(pred)))","675",""],"delete":[]}],"sklearn\/neighbors\/regression.py":[{"add":["7","#          Empty radius support by Andreas Bjerre-Nielsen","9","# License: BSD 3 clause (C) INRIA, University of Amsterdam,","10","#                           University of Copenhagen","11","","12","import warnings","286","        y : array of float, shape = [n_samples] or [n_samples, n_outputs]","299","        empty_obs = np.full_like(_y[0], np.nan)","300","","303","                               if len(ind) else empty_obs","306","        else:","307","            y_pred = np.array([np.average(_y[ind, :], axis=0,","308","                               weights=weights[i])","309","                               if len(ind) else empty_obs","310","                               for (i, ind) in enumerate(neigh_ind)])","311","","312","        if np.max(np.isnan(y_pred)):","313","            empty_warning_msg = (\"One or more samples have no neighbors \"","314","                                 \"within specified radius; predicting NaN.\")","315","            warnings.warn(empty_warning_msg)","316","","317",""],"delete":["8","# License: BSD 3 clause (C) INRIA, University of Amsterdam","282","        y : array of int, shape = [n_samples] or [n_samples, n_outputs]","297","                               for ind in neigh_ind])","298","        else:","299","            y_pred = np.array([(np.average(_y[ind, :], axis=0,","300","                                           weights=weights[i]))"]}]}},"ba7224869f3abeb904f59542551e965fff2e642b":{"changes":{"doc\/modules\/classes.rst":"MODIFY","doc\/whats_new.rst":"MODIFY","doc\/modules\/pipeline.rst":"MODIFY"},"diff":{"doc\/modules\/classes.rst":[{"add":["1392","To be removed in 0.21","1393","---------------------","1394","","1395",".. autosummary::","1396","   :toctree: generated\/","1397","   :template: deprecated_class.rst","1398","","1399","   linear_model.RandomizedLasso","1400","   linear_model.RandomizedLogisticRegression","1401","   neighbors.LSHForest","1402","","1403",""],"delete":["726","   linear_model.RandomizedLasso","727","   linear_model.RandomizedLogisticRegression"]}],"doc\/whats_new.rst":[{"add":["12","Highlights","13","----------","14","","15","We are excited to release a number of great new features including","16",":class:`neighbors.LocalOutlierFactor` for anomaly detection,","17",":class:`preprocessing.QuantileTransformer` for robust feature transformation,","18","and the :class:`multioutput.ClassifierChain` meta-estimator to simply account","19","for dependencies between classes in multilabel problems. We have some new","20","algorithms in existing estimators, such as multiplicative update in","21",":class:`decomposition.NMF` and multinomial","22",":class:`linear_model.LogisticRegression` with L1 loss (use ``solver='saga'``).","23","","24","You can also learn faster.  For instance, the :ref:`new option to cache","25","transformations <pipeline_cache>` in :class:`pipeline.Pipeline` makes grid","26","search over pipelines including slow transformations much more efficient.  And","27","you can predict faster: if you're sure you know what you're doing, you can turn","28","off validating that the input is finite using :func:`config_context`.","29","","30","Cross validation is now able to return the results from multiple metric","31","evaluations. The new :func:`model_selection.cross_validate` can return many","32","scores on the test data as well as training set performance and timings, and we","33","have extended the ``scoring`` and ``refit`` parameters for grid\/randomized","34","search :ref:`to handle multiple metrics <multimetric_grid_search>`.","35","","36","We've made some important fixes too.  We've fixed a longstanding implementation","37","erorr in :func:`metrics.average_precision_score`, so please be cautious with","38","prior results reported from that function.  A number of errors in the","39",":class:`manifold.TSNE` implementation have been fixed, particularly in the","40","default Barnes-Hut approximation.  :class:`semi_supervised.LabelSpreading` and","41",":class:`semi_supervised.LabelPropagation` have had substantial fixes.","42","LabelPropagation was previously broken. LabelSpreading should now correctly","43","respect its alpha parameter.","44","","53","   * :class:`cluster.KMeans` with sparse X and initial centroids given (bug fix)","54","   * :class:`cross_decomposition.PLSRegression`","55","     with ``scale=True`` (bug fix)","56","   * :class:`ensemble.GradientBoostingClassifier` and","57","     :class:`ensemble.GradientBoostingRegressor` where ``min_impurity_split`` is used (bug fix)","58","   * gradient boosting ``loss='quantile'`` (bug fix)","59","   * :class:`ensemble.IsolationForest` (bug fix)","60","   * :class:`feature_selection.SelectFdr` (bug fix)","61","   * :class:`linear_model.RANSACRegressor` (bug fix)","62","   * :class:`linear_model.LassoLars` (bug fix)","63","   * :class:`linear_model.LassoLarsIC` (bug fix)","64","   * :class:`manifold.TSNE` (bug fix)","65","   * :class:`semi_supervised.LabelSpreading` (bug fix)","66","   * :class:`semi_supervised.LabelPropagation` (bug fix)","67","   * tree based models where ``min_weight_fraction_leaf`` is used (enhancement)","80","Classifiers and regressors","81","","82","   - Added :class:`multioutput.ClassifierChain` for multi-label","83","     classification. By `Adam Kleczewski <adamklec>`_.","84","","85","   - Added solver ``'saga'`` that implements the improved version of Stochastic","86","     Average Gradient, in :class:`linear_model.LogisticRegression` and","87","     :class:`linear_model.Ridge`. It allows the use of L1 penalty with","88","     multinomial logistic loss, and behaves marginally better than 'sag'","89","     during the first epochs of ridge and logistic regression.","90","     :issue:`8446` by `Arthur Mensch`_.","91","","92","Other estimators","93","","94","   - Added the :class:`neighbors.LocalOutlierFactor` class for anomaly","95","     detection based on nearest neighbors.","96","     :issue:`5279` by `Nicolas Goix`_ and `Alexandre Gramfort`_.","97","","98","   - Added :class:`preprocessing.QuantileTransformer` class and","99","     :func:`preprocessing.quantile_transform` function for features","100","     normalization based on quantiles.","101","     :issue:`8363` by :user:`Denis Engemann <dengemann>`,","102","     :user:`Guillaume Lemaitre <glemaitre>`, `Olivier Grisel`_, `Raghav RV`_,","103","     :user:`Thierry Guillemot <tguillemot>`, and `Gael Varoquaux`_.","104","","105","   - The new solver ``'mu'`` implements a Multiplicate Update in","106","     :class:`decomposition.NMF`, allowing the optimization of all","107","     beta-divergences, including the Frobenius norm, the generalized","108","     Kullback-Leibler divergence and the Itakura-Saito divergence.","109","     :issue:`5295` by `Tom Dupre la Tour`_.","110","","111","Model selection and evaluation","112","","136","   - Added the :class:`model_selection.RepeatedKFold` and","137","     :class:`model_selection.RepeatedStratifiedKFold`.","138","     :issue:`8120` by `Neeraj Gangwar`_.","139","","140","Miscellaneous","141","","142","   - Validation that input data contains no NaN or inf can now be suppressed","143","     using :func:`config_context`, at your own risk. This will save on runtime,","144","     and may be particularly useful for prediction time. :issue:`7548` by","145","     `Joel Nothman`_.","146","","147","   - Added a test to ensure parameter listing in docstrings match the","148","     function\/class signature. :issue:`9206` by `Alexandre Gramfort`_ and","149","     `Raghav RV`_.","150","","154","Trees and ensembles","166","   - :class:`ensemble.VotingClassifier` now allows changing estimators by using","167","     :meth:`ensemble.VotingClassifier.set_params`. An estimator can also be","168","     removed by setting it to ``None``.","169","     :issue:`7674` by :user:`Yichuan Liu <yl565>`.","171","   - :func:`tree.export_graphviz` now shows configurable number of decimal","172","     places. :issue:`8698` by :user:`Guillaume Lemaitre <glemaitre>`.","174","Linear, kernelized and related models","179","     :class:`linear_model.Perceptron` now expose ``max_iter`` and","183","     convergence. :issue:`5036` by `Tom Dupre la Tour`_.","185","   - Added ``average`` parameter to perform weight averaging in","186","     :class:`linear_model.PassiveAggressiveClassifier`. :issue:`4939`","187","     by :user:`Andrea Esuli <aesuli>`.","195","   - In :class:`gaussian_process.GaussianProcessRegressor`, method ``predict``","196","     is a lot faster with ``return_std=True``. :issue:`8591` by","197","     :user:`Hadrien Bertrand <hbertrand>`.","199","   - Added ``return_std`` to ``predict`` method of","200","     :class:`linear_model.ARDRegression` and","201","     :class:`linear_model.BayesianRidge`.","202","     :issue:`7838` by :user:`Sergey Feldman <sergeyf>`.","203","","204","   - Memory usage enhancements: Prevent cast from float32 to float64 in:","205","     :class:`linear_model.MultiTaskElasticNet`;","206","     :class:`linear_model.LogisticRegression` when using newton-cg solver; and","207","     :class:`linear_model.Ridge` when using svd, sparse_cg, cholesky or lsqr","208","     solvers. :issue:`8835`, :issue:`8061` by :user:`Joan Massich <massich>` and :user:`Nicolas","209","     Cordier <ncordier>` and :user:`Thierry Guillemot`.","210","","211","Other predictors","212","","213","   - Custom metrics for the :mod:`neighbors` binary trees now have","214","     fewer constraints: they must take two 1d-arrays and return a float.","215","     :issue:`6288` by `Jake Vanderplas`_.","216","","217","   - ``algorithm='auto`` in :mod:`neighbors` estimators now chooses the most","218","     appropriate algorithm for all input types and metrics. :issue:`9145` by","219","     :user:`Herilalaina Rakotoarison <herilalaina>` and :user:`Reddy Chinthala","220","     <preddy5Pradyumna>`.","221","","222","Decomposition, manifold learning and clustering","223","","224","   - :class:`cluster.MiniBatchKMeans` and :class:`cluster.KMeans`","225","     now use significantly less memory when assigning data points to their","226","     nearest cluster center. :issue:`7721` by :user:`Jon Crall <Erotemic>`.","227","","228","   - :class:`decomposition.PCA`, :class:`decomposition.IncrementalPCA` and","229","     :class:`decomposition.TruncatedSVD` now expose the singular values","230","     from the underlying SVD. They are stored in the attribute","231","     ``singular_values_``, like in :class:`decomposition.IncrementalPCA`.","232","     :issue:`7685` by :user:`Tommy L?fstedt <tomlof>`","233","","234","   - :class:`decomposition.NMF` now faster when ``beta_loss=0``.","235","     :issue:`9277` by :user:`hongkahjun`.","236","","237","   - Memory improvements for method ``barnes_hut`` in :class:`manifold.TSNE`","238","     :issue:`7089` by :user:`Thomas Moreau <tomMoral>` and `Olivier Grisel`_.","239","","240","   - Optimization schedule improvements for Barnes-Hut :class:`manifold.TSNE`","241","     so the results are closer to the one from the reference implementation","242","     `lvdmaaten\/bhtsne <https:\/\/github.com\/lvdmaaten\/bhtsne>`_ by :user:`Thomas","243","     Moreau <tomMoral>` and `Olivier Grisel`_.","244","","245","   - Memory usage enhancements: Prevent cast from float32 to float64 in","246","     :class:`decomposition.PCA` and","247","     :func:`decomposition.randomized_svd_low_rank`.","248","     :issue:`9067` by `Raghav RV`_.","249","","250","Preprocessing and feature selection","251","","252","   - Added ``norm_order`` parameter to :class:`feature_selection.SelectFromModel`","253","     to enable selection of the norm order when ``coef_`` is more than 1D.","254","     :issue:`6181` by :user:`Antoine Wendlinger <antoinewdg>`.","255","","256","   - Added ability to use sparse matrices in :func:`feature_selection.f_regression`","257","     with ``center=True``. :issue:`8065` by :user:`Daniel LeJeune <acadiansith>`.","258","","259","   - Small performance improvement to n-gram creation in","260","     :mod:`feature_extraction.text` by binding methods for loops and","261","     special-casing unigrams. :issue:`7567` by :user:`Jaye Doepke <jtdoepke>`","262","","263","   - Relax assumption on the data for the","264","     :class:`kernel_approximation.SkewedChi2Sampler`. Since the Skewed-Chi2","265","     kernel is defined on the open interval :math:`(-skewedness; +\\infty)^d`,","266","     the transform function should not check whether ``X < 0`` but whether ``X <","267","     -self.skewedness``. :issue:`7573` by :user:`Romain Brault <RomainBrault>`.","268","","269","   - Made default kernel parameters kernel-dependent in","270","     :class:`kernel_approximation.Nystroem`.","271","     :issue:`5229` by :user:`Saurabh Bansod <mth4saurabh>` and `Andreas M¨¹ller`_.","272","","273","Model evaluation and meta-estimators","274","","275","   - :class:`pipeline.Pipeline` is now able to cache transformers","276","     within a pipeline by using the ``memory`` constructor parameter.","277","     :issue:`7990` by :user:`Guillaume Lemaitre <glemaitre>`.","278","","279","   - :class:`pipeline.Pipeline` steps can now be accessed as attributes of its","280","     ``named_steps`` attribute. :issue:`8586` by :user:`Herilalaina","281","     Rakotoarison <herilalaina>`.","282","","283","   - Added ``sample_weight`` parameter to :meth:`pipeline.Pipeline.score`.","284","     :issue:`7723` by :user:`Mikhail Korobov <kmike>`.","290","   - :class:`model_selection.GridSearchCV`,","291","     :class:`model_selection.RandomizedSearchCV` and","292","     :func:`model_selection.cross_val_score` now allow estimators with callable","293","     kernels which were previously prohibited.","294","     :issue:`8005` by `Andreas M¨¹ller`_ .","296","   - :func:`model_selection.cross_val_predict` now returns output of the","297","     correct shape for all values of the argument ``method``.","298","     :issue:`7863` by :user:`Aman Dalmia <dalmia>`.","300","   - Added ``shuffle`` and ``random_state`` parameters to shuffle training","301","     data before taking prefixes of it based on training sizes in","302","     :func:`model_selection.learning_curve`.","303","     :issue:`7506` by :user:`Narine Kokhlikyan <NarineK>`.","304","","305","   - :class:`model_selection.StratifiedShuffleSplit` now works with multioutput","306","     multiclass (or multilabel) data.  :issue:`9044` by `Vlad Niculae`_.","307","","308","   - Speed improvements to :class:`model_selection.StratifiedShuffleSplit`.","309","     :issue:`5991` by :user:`Arthur Mensch <arthurmensch>` and `Joel Nothman`_.","310","","311","   - Add ``shuffle`` parameter to :func:`model_selection.train_test_split`.","312","     :issue:`8845` by  :user:`themrmax <themrmax>`","313","","314","   - :class:`multioutput.MultiOutputRegressor` and :class:`multioutput.MultiOutputClassifier`","315","     now support online learning using ``partial_fit``.","316","     :issue: `8053` by :user:`Peng Yu <yupbank>`.","317","","318","   - Add ``max_train_size`` parameter to :class:`model_selection.TimeSeriesSplit`","319","     :issue:`8282` by :user:`Aman Dalmia <dalmia>`.","320","","321","   - More clustering metrics are now available through :func:`metrics.get_scorer`","322","     and ``scoring`` parameters. :issue:`8117` by `Raghav RV`_.","323","","324","Metrics","325","","326","   - :func:`metrics.matthews_corrcoef` now support multiclass classification.","327","     :issue:`8094` by :user:`Jon Crall <Erotemic>`.","332","Miscellaneous","334","   - :func:`utils.check_estimator` now attempts to ensure that methods","335","     transform, predict, etc.  do not set attributes on the estimator.","336","     :issue:`7533` by :user:`Ekaterina Krivich <kiote>`.","338","   - Added type checking to the ``accept_sparse`` parameter in","339","     :mod:`utils.validation` methods. This parameter now accepts only boolean,","340","     string, or list\/tuple of strings. ``accept_sparse=None`` is deprecated and","341","     should be replaced by ``accept_sparse=False``.","342","     :issue:`7880` by :user:`Josh Karnofsky <jkarno>`.","348","   - :class:`dummy.DummyClassifier` and :class:`dummy.DummyRegressor`","349","     now accept non-finite features. :issue:`8931` by :user:`Attractadore`.","354","Trees and ensembles","356","   - Fixed a memory leak in trees when using trees with ``criterion='mae'``.","357","     :issue:`8002` by `Raghav RV`_.","367","   - Fixed a bug in :class:`ensemble.GradientBoostingClassifier` and","368","     :class:`ensemble.GradientBoostingRegressor` where a float being compared","369","     to ``0.0`` using ``==`` caused a divide by zero error. :issue:`7970` by","370","     :user:`He Chen <chenhe95>`.","377","   - Fixed ``oob_score`` in :class:`ensemble.BaggingClassifier`.","378","     :issue:`8936` by :user:`Michael Lewis <mlewis1729>`","379","","380","   - Fixed excessive memory usage in prediction for random forests estimators.","381","     :issue:`8672` by :user:`Mike Benfield <mikebenfield>`.","382","","383","   - Fixed a bug where ``sample_weight`` as a list broke random forests in Python 2","384","     :issue:`8068` by :user:`xor`.","385","","386","   - Fixed a bug where :class:`ensemble.IsolationForest` fails when","387","     ``max_features`` is less than 1.","388","     :issue:`5732` by :user:`Ishank Gulati <IshankGulati>`.","389","","390","   - Fix a bug where gradient boosting with ``loss='quantile'`` computed","391","     negative errors for negative values of ``ytrue - ypred`` leading to wrong","392","     values when calling ``__call__``.","393","     :issue:`8087` by :user:`Alexis Mignon <AlexisMignon>`","394","","395","   - Fix a bug where :class:`ensemble.VotingClassifier` raises an error","396","     when a numpy array is passed in for weights. :issue:`7983` by","397","     :user:`Vincent Pham <vincentpham1991>`.","398","","399","   - Fixed a bug where :func:`tree.export_graphviz` raised an error","400","     when the length of features_names does not match n_features in the decision","401","     tree. :issue:`8512` by :user:`Li Li <aikinogard>`.","402","","403","Linear, kernelized and related models","404","","405","   - Fixed a bug where :func:`linear_model.RANSACRegressor.fit` may run until","406","     ``max_iter`` if it finds a large inlier group early. :issue:`8251` by","407","     :user:`aivision2020`.","408","","409","   - Fixed a bug where :class:`naive_bayes.MultinomialNB` and","410","     :class:`naive_bayes.BernoulliNB` failed when ``alpha=0``. :issue:`5814` by","411","     :user:`Yichuan Liu <yl565>` and :user:`Herilalaina Rakotoarison","412","     <herilalaina>`.","413","","414","   - Fixed a bug where :class:`linear_model.LassoLars` does not give","415","     the same result as the LassoLars implementation available","416","     in R (lars library). :issue:`7849` by :user:`Jair Montoya Martinez <jmontoyam>`.","417","","418","   - Fixed a bug in :class:`linear_model.RandomizedLasso`,","419","     :class:`linear_model.Lars`, :class:`linear_model.LassoLars`,","420","     :class:`linear_model.LarsCV` and :class:`linear_model.LassoLarsCV`,","421","     where the parameter ``precompute`` was not used consistently across","422","     classes, and some values proposed in the docstring could raise errors.","423","     :issue:`5359` by `Tom Dupre la Tour`_.","424","","425","   - Fix inconsistent results between :class:`linear_model.RidgeCV` and","426","     :class:`linear_model.Ridge` when using ``normalize=True``. :issue:`9302`","427","     by `Alexandre Gramfort`_.","428","","429","   - Fix a bug where :func:`linear_model.LassoLars.fit` sometimes","430","     left ``coef_`` as a list, rather than an ndarray.","431","     :issue:`8160` by :user:`CJ Carey <perimosocordiae>`.","432","","433","   - Fix :func:`linear_model.BayesianRidge.fit` to return","434","     ridge parameter ``alpha_`` and ``lambda_`` consistent with calculated","435","     coefficients ``coef_`` and ``intercept_``.","436","     :issue:`8224` by :user:`Peter Gedeck <gedeck>`.","437","","438","   - Fixed a bug in :class:`svm.OneClassSVM` where it returned floats instead of","439","     integer classes. :issue:`8676` by :user:`Vathsala Achar <VathsalaAchar>`.","440","","441","   - Fix AIC\/BIC criterion computation in :class:`linear_model.LassoLarsIC`.","442","     :issue:`9022` by `Alexandre Gramfort`_ and :user:`Mehmet Basbug <mehmetbasbug>`.","443","","444","   - Fixed a memory leak in our LibLinear implementation. :issue:`9024` by","445","     :user:`Sergei Lebedev <superbobry>`","446","","447","   - Fix bug where stratified CV splitters did not work with","448","     :class:`linear_model.LassoCV`. :issue:`8973` by","449","     :user:`Paulo Haddad <paulochf>`.","450","","451","   - Fixed a bug in :class:`gaussian_process.GaussianProcessRegressor`","452","     when the standard deviation and covariance predicted without fit","453","     would fail with a unmeaningful error by default.","454","     :issue:`6573` by :user:`Quazi Marufur Rahman <qmaruf>` and","455","     `Manoj Kumar`_.","456","","457","Other predictors","458","","459","   - Fix :class:`semi_supervised.BaseLabelPropagation` to correctly implement","460","     ``LabelPropagation`` and ``LabelSpreading`` as done in the referenced","461","     papers. :issue:`9239`","462","     by :user:`Andre Ambrosio Boechat <boechat107>`, :user:`Utkarsh Upadhyay","463","     <musically-ut>`, and `Joel Nothman`_.","464","","465","Decomposition, manifold learning and clustering","466","","467","   - Fixed the implementation of :class:`manifold.TSNE`:","468","      - ``early_exageration`` parameter had no effect and is now used for the","469","        first 250 optimization iterations.","470","      - Fixed the ``InsersionError`` reported in :issue:`8992`.","471","      - Improve the learning schedule to match the one from the reference","472","        implementation `lvdmaaten\/bhtsne <https:\/\/github.com\/lvdmaaten\/bhtsne>`_.","473","     by :user:`Thomas Moreau <tomMoral>` and `Olivier Grisel`_.","474","","475","   - Fix a bug in :class:`decomposition.LatentDirichletAllocation`","476","     where the ``perplexity`` method was returning incorrect results because","477","     the ``transform`` method returns normalized document topic distributions","478","     as of version 0.18. :issue:`7954` by :user:`Gary Foreman <garyForeman>`.","487","   - Fixed the implementation of ``explained_variance_``","488","     in :class:`decomposition.PCA`,","489","     :class:`decomposition.RandomizedPCA` and","490","     :class:`decomposition.IncrementalPCA`.","491","     :issue:`9105` by `Hanmin Qin <https:\/\/github.com\/qinhanmin2014>`_. ","492","","493","   - Fixed a bug where :class:`cluster.DBSCAN` gives incorrect","494","     result when input is a precomputed sparse matrix with initial","495","     rows all zero. :issue:`8306` by :user:`Akshay Gupta <Akshay0724>`","496","","497","   - Fix a bug regarding fitting :class:`cluster.KMeans` with a sparse","498","     array X and initial centroids, where X's means were unnecessarily being","499","     subtracted from the centroids. :issue:`7872` by :user:`Josh Karnofsky <jkarno>`.","500","","501","   - Fixes to the input validation in :class:`covariance.EllipticEnvelope`.","504","   - Fixed a bug in :class:`covariance.MinCovDet` where inputting data","505","     that produced a singular covariance matrix would cause the helper method","506","     ``_c_step`` to throw an exception.","507","     :issue:`3367` by :user:`Jeremy Steward <ThatGeoGuy>`","509","   - Fixed a bug in :class:`manifold.TSNE` affecting convergence of the","510","     gradient descent. :issue:`8768` by :user:`David DeTomaso <deto>`.","512","   - Fixed a bug in :class:`manifold.TSNE` where it stored the incorrect","513","     ``kl_divergence_``. :issue:`6507` by :user:`Sebastian Saeger <ssaeger>`.","514","","515","   - Fixed improper scaling in :class:`cross_decomposition.PLSRegression`","516","     with ``scale=True``. :issue:`7819` by :user:`jayzed82 <jayzed82>`.","517","","518","   - :class:`cluster.bicluster.SpectralCoclustering` and","519","     :class:`cluster.bicluster.SpectralBiclustering` ``fit`` method conforms","520","     with API by accepting ``y`` and returning the object.  :issue:`6126`,","521","     :issue:`7814` by :user:`Laurent Direr <ldirer>` and :user:`Maniteja","522","     Nandana <maniteja123>`.","523","","524","   - Fix bug where :mod:`mixture` ``sample`` methods did not return as many","525","     samples as requested. :issue:`7702` by :user:`Levi John Wolf <ljwolf>`.","526","","527","Preprocessing and feature selection","528","","529","   - For sparse matrices, :func:`preprocessing.normalize` with ``return_norm=True``","530","     will now raise a ``NotImplementedError`` with 'l1' or 'l2' norm and with","531","     norm 'max' the norms returned will be the same as for dense matrices.","532","     :issue:`7771` by `Ang Lu <https:\/\/github.com\/luang008>`_.","533","","534","   - Fix a bug where :class:`feature_selection.SelectFdr` did not","535","     exactly implement Benjamini-Hochberg procedure. It formerly may have","536","     selected fewer features than it should.","537","     :issue:`7490` by :user:`Peng Meng <mpjlu>`.","538","","539","   - Fixed a bug where :class:`linear_model.RandomizedLasso` and","540","     :class:`linear_model.RandomizedLogisticRegression` breaks for","541","     sparse input. :issue:`8259` by :user:`Aman Dalmia <dalmia>`.","548","     :issue:`7565` by :user:`Roman Yurchak <rth>`.","549","","550","   - Fix a bug where :class:`feature_selection.mutual_info_regression` did not","551","     correctly use ``n_neighbors``. :issue:`8181` by :user:`Guillaume Lemaitre","552","     <glemaitre>`.","553","","554","Model evaluation and meta-estimators","555","","556","   - Fixed a bug where :func:`model_selection.BaseSearchCV.inverse_transform`","557","     returns ``self.best_estimator_.transform()`` instead of","558","     ``self.best_estimator_.inverse_transform()``.","559","     :issue:`8344` by :user:`Akshay Gupta <Akshay0724>` and :user:`Rasmus Eriksson <MrMjauh>`.","560","","561","   - Added ``classes_`` attribute to :class:`model_selection.GridSearchCV`,","562","     :class:`model_selection.RandomizedSearchCV`,  :class:`grid_search.GridSearchCV`,","563","     and  :class:`grid_search.RandomizedSearchCV` that matches the ``classes_``","564","     attribute of ``best_estimator_``. :issue:`7661` and :issue:`8295`","565","     by :user:`Alyssa Batula <abatula>`, :user:`Dylan Werner-Meier <unautre>`,","566","     and :user:`Stephen Hoover <stephen-hoover>`.","567","","568","   - Fixed a bug where :func:`model_selection.validation_curve`","569","     reused the same estimator for each parameter value.","570","     :issue:`7365` by :user:`Aleksandr Sandrovskii <Sundrique>`.","571","","572","   - :func:`model_selection.permutation_test_score` now works with Pandas","573","     types. :issue:`5697` by :user:`Stijn Tonk <equialgo>`.","574","","575","   - Several fixes to input validation in","576","     :class:`multiclass.OutputCodeClassifier`","577","     :issue:`8086` by `Andreas M¨¹ller`_.","578","","579","   - :class:`multiclass.OneVsOneClassifier`'s ``partial_fit`` now ensures all","580","     classes are provided up-front. :issue:`6250` by","581","     :user:`Asish Panda <kaichogami>`.","582","","583","   - Fix :func:`multioutput.MultiOutputClassifier.predict_proba` to return a","584","     list of 2d arrays, rather than a 3d array. In the case where different","585","     target columns had different numbers of classes, a ``ValueError`` would be","586","     raised on trying to stack matrices with different dimensions.","587","     :issue:`8093` by :user:`Peter Bull <pjbull>`.","588","","589","Metrics","590","","591","   - :func:`metrics.average_precision_score` no longer linearly","592","     interpolates between operating points, and instead weighs precisions","593","     by the change in recall since the last operating point, as per the","594","     `Wikipedia entry <http:\/\/en.wikipedia.org\/wiki\/Average_precision>`_.","595","     (`#7356 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/7356>`_). By","596","     :user:`Nick Dingwall <ndingwall>` and `Gael Varoquaux`_.","597","","598","   - Fix a bug in :func:`metrics.classification._check_targets`","599","     which would return ``'binary'`` if ``y_true`` and ``y_pred`` were","600","     both ``'binary'`` but the union of ``y_true`` and ``y_pred`` was","601","     ``'multiclass'``. :issue:`8377` by `Loic Esteve`_.","602","","603","   - Fixed an integer overflow bug in :func:`metrics.confusion_matrix` and","604","     hence :func:`metrics.cohen_kappa_score`. :issue:`8354`, :issue:`7929`","605","     by `Joel Nothman`_ and :user:`Jon Crall <Erotemic>`.","606","","607","   - Fixed passing of ``gamma`` parameter to the ``chi2`` kernel in","608","     :func:`metrics.pairwise.pairwise_kernels` :issue:`5211` by","609","     :user:`Nick Rhinehart <nrhine1>`,","610","     :user:`Saurabh Bansod <mth4saurabh>` and `Andreas M¨¹ller`_.","611","","612","Miscellaneous","613","","614","   - Fixed a bug when :func:`datasets.make_classification` fails","615","     when generating more than 30 features. :issue:`8159` by","616","     :user:`Herilalaina Rakotoarison <herilalaina>`.","617","","618","   - Fixed a bug where :func:`datasets.make_moons` gives an","619","     incorrect result when ``n_samples`` is odd.","620","     :issue:`8198` by :user:`Josh Levy <levy5674>`.","621","","622","   - Some ``fetch_`` functions in :mod:`datasets` were ignoring the","623","     ``download_if_missing`` keyword. :issue:`7944` by :user:`Ralf Gommers <rgommers>`.","624","","625","   - Fix estimators to accept a ``sample_weight`` parameter of type","626","     ``pandas.Series`` in their ``fit`` function. :issue:`7825` by","627","     `Kathleen Chen`_.","638","   - Update Sphinx-Gallery from 0.1.4 to 0.1.7 for resolving links in","639","     documentation build with Sphinx>1.5 :issue:`8010`, :issue:`7986` by","640","     :user:`Oscar Najera <Titan-C>`","642","   - Add ``data_home`` parameter to :func:`sklearn.datasets.fetch_kddcup99`.","643","     :issue:`9289` by `Loic Esteve`_.","645","   - Fix dataset loaders using Python 3 version of makedirs to also work in","646","     Python 2. :issue:`9284` by :user:`Sebastin Santy <SebastinSanty>`.","648","   - Several minor issues were fixed with thanks to the alerts of","649","     [lgtm.com](http:\/\/lgtm.com). :issue:`9278` by :user:`Jean Helie <jhelie>`,","650","     among others.","655","Trees and ensembles","657","   - Gradient boosting base models are no longer estimators. By `Andreas M¨¹ller`_.","658","","659","   - All tree based estimators now accept a ``min_impurity_decrease``","660","     parameter in lieu of the ``min_impurity_split``, which is now deprecated.","661","     The ``min_impurity_decrease`` helps stop splitting the nodes in which","662","     the weighted impurity decrease from splitting is no longer alteast","663","     ``min_impurity_decrease``.  :issue:`8449` by `Raghav RV`_.","664","","665","Linear, kernelized and related models","666","","667","   - ``n_iter`` parameter is deprecated in :class:`linear_model.SGDClassifier`,","668","     :class:`linear_model.SGDRegressor`,","669","     :class:`linear_model.PassiveAggressiveClassifier`,","670","     :class:`linear_model.PassiveAggressiveRegressor` and","671","     :class:`linear_model.Perceptron`. By `Tom Dupre la Tour`_.","672","","673","Other predictors","674","","675","   - :class:`neighbors.LSHForest` has been deprecated and will be","676","     removed in 0.21 due to poor performance.","677","     :issue:`9078` by :user:`Laurent Direr <ldirer>`.","678","","679","   - :class:`neighbors.NearestCentroid` no longer purports to support","680","     ``metric='precomputed'`` which now raises an error. :issue:`8515` by","681","     :user:`Sergul Aydore <sergulaydore>`.","682","","683","   - The ``alpha`` parameter of :class:`semi_supervised.LabelPropagation` now","684","     has no effect and is deprecated to be removed in 0.21. :issue:`9239`","685","     by :user:`Andre Ambrosio Boechat <boechat107>`, :user:`Utkarsh Upadhyay","686","     <musically-ut>`, and `Joel Nothman`_.","687","","688","Decomposition, manifold learning and clustering","696","   - The ``n_topics`` parameter of :class:`decomposition.LatentDirichletAllocation`","697","     has been renamed to ``n_components`` and will be removed in version 0.21.","698","     :issue:`8922` by :user:`Attractadore`.","700","   - :meth:`decomposition.SparsePCA.transform`'s ``ridge_alpha`` parameter is","701","     deprecated in preference for class parameter.","702","     :issue:`8137` by :user:`Naoya Kanai <naoyak>`.","703","","704","   - :class:`cluster.DBSCAN` now has a ``metric_params`` parameter.","705","     :issue:`8139` by :user:`Naoya Kanai <naoyak>`.","706","","707","Preprocessing and feature selection","708","","709","   - :class:`feature_selection.SelectFromModel` now has a ``partial_fit``","710","     method only if the underlying estimator does. By `Andreas M¨¹ller`_.","711","","712","   - :class:`feature_selection.SelectFromModel` now validates the ``threshold``","713","     parameter and sets the ``threshold_`` attribute during the call to","714","     ``fit``, and no longer during the call to ``transform```. By `Andreas","715","     M¨¹ller`_.","716","","717","   - The ``non_negative`` parameter in :class:`feature_extraction.FeatureHasher`","718","     has been deprecated, and replaced with a more principled alternative,","719","     ``alternate_sign``.","720","     :issue:`7565` by :user:`Roman Yurchak <rth>`.","721","","722","   - :class:`linear_model.RandomizedLogisticRegression`,","723","     and :class:`linear_model.RandomizedLasso` have been deprecated and will","724","     be removed in version 0.21.","725","     :issue:`8995` by :user:`Ramana.S <sentient07>`.","726","","727","Model evaluation and meta-estimators","745","   - :class:`multiclass.OneVsRestClassifier` now has ``partial_fit``,","746","     ``decision_function`` and ``predict_proba`` methods only when the","747","     underlying estimator does.  :issue:`7812` by `Andreas M¨¹ller`_ and","748","     :user:`Mikhail Korobov <kmike>`.","750","   - :class:`multiclass.OneVsRestClassifier` now has a ``partial_fit`` method","751","     only if the underlying estimator does.  By `Andreas M¨¹ller`_.","753","   - The ``decision_function`` output shape for binary classification in","754","     :class:`multiclass.OneVsRestClassifier` and","755","     :class:`multiclass.OneVsOneClassifier` is now ``(n_samples,)`` to conform","756","     to scikit-learn conventions. :issue:`9100` by `Andreas M¨¹ller`_.","758","   - The :func:`multioutput.MultiOutputClassifier.predict_proba`","759","     function used to return a 3d array (``n_samples``, ``n_classes``,","760","     ``n_outputs``). In the case where different target columns had different","761","     numbers of classes, a ``ValueError`` would be raised on trying to stack","762","     matrices with different dimensions. This function now returns a list of","763","     arrays where the length of the list is ``n_outputs``, and each array is","764","     (``n_samples``, ``n_classes``) for that particular output.","765","     :issue:`8093` by :user:`Peter Bull <pjbull>`.","766","","767","   - Replace attribute ``named_steps`` ``dict`` to :class:`utils.Bunch`","768","     in :class:`pipeline.Pipeline` to enable tab completion in interactive","769","     environment. In the case conflict value on ``named_steps`` and ``dict``","770","     attribute, ``dict`` behavior will be prioritized.","771","     :issue:`8481` by :user:`Herilalaina Rakotoarison <herilalaina>`.","772","","773","Miscellaneous","774","","775","   - Deprecate the ``y`` parameter in ``transform`` and ``inverse_transform``.","776","     The method  should not accept ``y`` parameter, as it's used at the prediction time.","777","     :issue:`8174` by :user:`Tahar Zanouda <tzano>`, `Alexandre Gramfort`_","778","     and `Raghav RV`_.","782","     :mod:`utils` have been removed or deprecated accordingly.","812","   - Estimators with both methods ``decision_function`` and ``predict_proba``","813","     are now required to have a monotonic relation between them. The","814","     method ``check_decision_proba_consistency`` has been added in","815","     **utils.estimator_checks** to check their consistency.","816","     :issue:`7578` by :user:`Shubham Bhardwaj <shubham0704>`","817","","818","   - All checks in ``utils.estimator_checks``, in particular","819","     :func:`utils.estimator_checks.check_estimator` now accept estimator","820","     instances. Most other checks do not accept","821","     estimator classes any more. :issue:`9019` by `Andreas M¨¹ller`_.","822","","823","   - Ensure that estimators' attributes ending with ``_`` are not set","824","     in the constructor but only in the ``fit`` method. Most notably,","825","     ensemble estimators (deriving from :class:`ensemble.BaseEnsemble`)","826","     now only have ``self.estimators_`` available after ``fit``.","827","     :issue:`7464` by `Lars Buitinck`_ and `Loic Esteve`_.","828","","829","","830",".. _changes_0_18_2:","831","","832","Version 0.18.2","833","==============","834","","835","**June 20, 2017**","836","","837",".. topic:: Last release with Python 2.6 support","838","","839","    Scikit-learn 0.18 is the last major release of scikit-learn to support Python 2.6.","840","    Later versions of scikit-learn will require Python 2.7 or above.","841","","842","","843","Changelog","844","---------","845","","846","    - Fixes for compatibility with NumPy 1.13.0: :issue:`7946` :issue:`8355` by","847","      `Loic Esteve`_.","848","","849","    - Minor compatibility changes in the examples :issue:`9010` :issue:`8040`","850","      :issue:`9149`.","851","","852","Code Contributors","853","-----------------","854","Aman Dalmia, Loic Esteve, Nate Guerin, Sergei Lebedev","964","   - Fix bug where :meth:`preprocessing.MultiLabelBinarizer.fit_transform`","965","     returned an invalid CSR matrix.","966","     :issue:`7750` by :user:`CJ Carey <perimosocordiae>`.","967","","968","   - Fixed a bug where :func:`metrics.pairwise.cosine_distances` could return a","969","     small negative distance. :issue:`7732` by :user:`Artsion <asanakoy>`.","970","","1582","   - Error and loss names for ``scoring`` parameters are now prefixed by","1583","     ``'neg_'``, such as ``neg_mean_squared_error``. The unprefixed versions","1584","     are deprecated and will be removed in version 0.20.","1585","     :issue:`7261` by :user:`Tim Head <betatim>`."],"delete":["20","   * :class:`sklearn.ensemble.IsolationForest` (bug fix)","21","   * :class:`sklearn.manifold.TSNE` (bug fix)","46","     ","47","   - Added :class:`multioutput.ClassifierChain` for multi-label","48","     classification. By `Adam Kleczewski <adamklec>`_.","49","","50","   - Validation that input data contains no NaN or inf can now be suppressed","51","     using :func:`config_context`, at your own risk. This will save on runtime,","52","     and may be particularly useful for prediction time. :issue:`7548` by","53","     `Joel Nothman`_.","54","","55","   - Added the :class:`neighbors.LocalOutlierFactor` class for anomaly","56","     detection based on nearest neighbors.","57","     :issue:`5279` by `Nicolas Goix`_ and `Alexandre Gramfort`_.","58","","59","   - The new solver ``'mu'`` implements a Multiplicate Update in","60","     :class:`decomposition.NMF`, allowing the optimization of all","61","     beta-divergences, including the Frobenius norm, the generalized","62","     Kullback-Leibler divergence and the Itakura-Saito divergence.","63","     :issue:`5295` by `Tom Dupre la Tour`_.","64","","65","   - Added the :class:`model_selection.RepeatedKFold` and","66","     :class:`model_selection.RepeatedStratifiedKFold`.","67","     :issue:`8120` by `Neeraj Gangwar`_.","74","   - Added solver ``'saga'`` that implements the improved version of Stochastic","75","     Average Gradient, in :class:`linear_model.LogisticRegression` and","76","     :class:`linear_model.Ridge`. It allows the use of L1 penalty with","77","     multinomial logistic loss, and behaves marginally better than 'sag'","78","     during the first epochs of ridge and logistic regression.","79","     :issue:`8446` by `Arthur Mensch`_.","80","","81","   - Added :class:`preprocessing.QuantileTransformer` class and","82","     :func:`preprocessing.quantile_transform` function for features","83","     normalization based on quantiles.","84","     :issue:`8363` by :user:`Denis Engemann <dengemann>`,","85","     :user:`Guillaume Lemaitre <glemaitre>`, `Olivier Grisel`_, `Raghav RV`_,","86","     :user:`Thierry Guillemot <tguillemot>`, and `Gael Varoquaux`_.","87","","96","   - :func:`metrics.matthews_corrcoef` now support multiclass classification.","97","     :issue:`8094` by :user:`Jon Crall <Erotemic>`.","98","   - Update Sphinx-Gallery from 0.1.4 to 0.1.7 for resolving links in","99","     documentation build with Sphinx>1.5 :issue:`8010`, :issue:`7986` by","100","     :user:`Oscar Najera <Titan-C>`","101","   - :class:`multioutput.MultiOutputRegressor` and :class:`multioutput.MultiOutputClassifier`","102","     now support online learning using `partial_fit`.","103","     issue: `8053` by :user:`Peng Yu <yupbank>`.","104","   - :class:`pipeline.Pipeline` allows to cache transformers","105","     within a pipeline by using the ``memory`` constructor parameter.","106","     :issue:`7990` by :user:`Guillaume Lemaitre <glemaitre>`.","107","","108","   - :class:`decomposition.PCA`, :class:`decomposition.IncrementalPCA` and","109","     :class:`decomposition.TruncatedSVD` now expose the singular values","110","     from the underlying SVD. They are stored in the attribute","111","     ``singular_values_``, like in :class:`decomposition.IncrementalPCA`.","112","","113","   - :class:`cluster.MiniBatchKMeans` and :class:`cluster.KMeans`","114","     now uses significantly less memory when assigning data points to their","115","     nearest cluster center. :issue:`7721` by :user:`Jon Crall <Erotemic>`.","116","","117","   - Added ``classes_`` attribute to :class:`model_selection.GridSearchCV`,","118","     :class:`model_selection.RandomizedSearchCV`,  :class:`grid_search.GridSearchCV`,","119","     and  :class:`grid_search.RandomizedSearchCV` that matches the ``classes_``","120","     attribute of ``best_estimator_``. :issue:`7661` and :issue:`8295`","121","     by :user:`Alyssa Batula <abatula>`, :user:`Dylan Werner-Meier <unautre>`,","122","     and :user:`Stephen Hoover <stephen-hoover>`.","123","","124","   - Relax assumption on the data for the","125","     :class:`kernel_approximation.SkewedChi2Sampler`. Since the Skewed-Chi2","126","     kernel is defined on the open interval :math:`(-skewedness; +\\infty)^d`,","127","     the transform function should not check whether ``X < 0`` but whether ``X <","128","     -self.skewedness``. :issue:`7573` by :user:`Romain Brault <RomainBrault>`.","136","   - Added ``average`` parameter to perform weights averaging in","137","     :class:`linear_model.PassiveAggressiveClassifier`. :issue:`4939`","138","     by :user:`Andrea Esuli <aesuli>`.","139","","140","   - Custom metrics for the :mod:`sklearn.neighbors` binary trees now have","141","     fewer constraints: they must take two 1d-arrays and return a float.","142","     :issue:`6288` by `Jake Vanderplas`_.","143","","148","   - Added ``shuffle`` and ``random_state`` parameters to shuffle training","149","     data before taking prefixes of it based on training sizes in","150","     :func:`model_selection.learning_curve`.","151","     :issue:`7506` by :user:`Narine Kokhlikyan <NarineK>`.","153","   - Added ``norm_order`` parameter to :class:`feature_selection.SelectFromModel`","154","     to enable selection of the norm order when ``coef_`` is more than 1D.","155","     :issue:`6181` by :user:`Antoine Wendlinger <antoinewdg>`.","157","   - Added ``sample_weight`` parameter to :meth:`pipeline.Pipeline.score`.","158","     :issue:`7723` by :user:`Mikhail Korobov <kmike>`.","159","","160","   - ``check_estimator`` now attempts to ensure that methods transform, predict, etc.","161","     do not set attributes on the estimator.","162","     :issue:`7533` by :user:`Ekaterina Krivich <kiote>`.","167","     :class:`linear_model.Perceptron` now expose a ``max_iter`` and","171","     convergence. By `Tom Dupre la Tour`_.","173","   - For sparse matrices, :func:`preprocessing.normalize` with ``return_norm=True``","174","     will now raise a ``NotImplementedError`` with 'l1' or 'l2' norm and with","175","     norm 'max' the norms returned will be the same as for dense matrices.","176","     :issue:`7771` by `Ang Lu <https:\/\/github.com\/luang008>`_.","184","   - :func:`model_selection.cross_val_predict` now returns output of the","185","     correct shape for all values of the argument ``method``.","186","     :issue:`7863` by :user:`Aman Dalmia <dalmia>`.","188","   - Fix a bug where :class:`feature_selection.SelectFdr` did not","189","     exactly implement Benjamini-Hochberg procedure. It formerly may have","190","     selected fewer features than it should.","191","     :issue:`7490` by :user:`Peng Meng <mpjlu>`.","197","   - Added type checking to the ``accept_sparse`` parameter in","198","     :mod:`sklearn.utils.validation` methods. This parameter now accepts only","199","     boolean, string, or list\/tuple of strings. ``accept_sparse=None`` is deprecated","200","     and should be replaced by ``accept_sparse=False``.","201","     :issue:`7880` by :user:`Josh Karnofsky <jkarno>`.","203","   - :class:`model_selection.GridSearchCV`, :class:`model_selection.RandomizedSearchCV`","204","     and :func:`model_selection.cross_val_score` now allow estimators with callable","205","     kernels which were previously prohibited. :issue:`8005` by `Andreas M¨¹ller`_ .","207","   - Added ability to use sparse matrices in :func:`feature_selection.f_regression`","208","     with ``center=True``. :issue:`8065` by :user:`Daniel LeJeune <acadiansith>`.","213","   - In :class:`gaussian_process.GaussianProcessRegressor`, method ``predict``","214","     is a lot faster with ``return_std=True``. :issue:`8591` by","215","     :user:`Hadrien Bertrand <hbertrand>`.","217","   - Added ability to use sparse matrices in :func:`feature_selection.f_regression`","218","     with ``center=True``. :issue:`8065` by :user:`Daniel LeJeune <acadiansith>`.","220","   - :class:`ensemble.VotingClassifier` now allow changing estimators by using","221","     :meth:`ensemble.VotingClassifier.set_params`. Estimators can also be","222","     removed by setting it to `None`.","223","     :issue:`7674` by :user:`Yichuan Liu <yl565>`.","224","","225","   - Prevent cast from float32 to float64 in","226","     :class:`linear_model.LogisticRegression` when using newton-cg","227","     solver. :issue:`8835` by :user:`Joan Massich <massich>`.","228","","229","   - Prevent cast from float32 to float64 in","230","     :class:`linear_model.Ridge` when using svd, sparse_cg, cholesky or lsqr solvers","231","     :class:`sklearn.linear_model.Ridge` when using svd, sparse_cg, cholesky or lsqr solvers","232","     by :user:`Joan Massich <massich>`, :user:`Nicolas Cordier <ncordier>`","233","","234","   - Add ``max_train_size`` parameter to :class:`model_selection.TimeSeriesSplit`","235","     :issue:`8282` by :user:`Aman Dalmia <dalmia>`.","241","   - Small performance improvement to n-gram creation in","242","     :mod:`feature_extraction.text` by binding methods for loops and","243","     special-casing unigrams. :issue:`7567` by `Jaye Doepke <jtdoepke>`","244","","245","   - Speed improvements to :class:`model_selection.StratifiedShuffleSplit`.","246","     :issue:`5991` by :user:`Arthur Mensch <arthurmensch>` and `Joel Nothman`_.","247","","248","   - Memory improvements for method barnes_hut in :class:`manifold.TSNE`","249","     :issue:`7089` by :user:`Thomas Moreau <tomMoral>` and `Olivier Grisel`_.","250","","251","   - Optimization schedule improvements for so the results are closer to the","252","     one from the reference implementation","253","     `lvdmaaten\/bhtsne <https:\/\/github.com\/lvdmaaten\/bhtsne>`_ by","254","     :user:`Thomas Moreau <tomMoral>` and `Olivier Grisel`_.","259","   - :func:`metrics.average_precision_score` no longer linearly","260","     interpolates between operating points, and instead weighs precisions","261","     by the change in recall since the last operating point, as per the","262","     `Wikipedia entry <http:\/\/en.wikipedia.org\/wiki\/Average_precision>`_.","263","     (`#7356 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/7356>`_). By","264","     :user:`Nick Dingwall <ndingwall>` and `Gael Varoquaux`_.","266","   - Fixed a bug in :class:`covariance.MinCovDet` where inputting data","267","     that produced a singular covariance matrix would cause the helper method","268","     ``_c_step`` to throw an exception.","269","     :issue:`3367` by :user:`Jeremy Steward <ThatGeoGuy>`","275","   - Fixed a bug where :class:`cluster.DBSCAN` gives incorrect","276","     result when input is a precomputed sparse matrix with initial","277","     rows all zero. :issue:`8306` by :user:`Akshay Gupta <Akshay0724>`","278","","283","   - Fixed a bug when :func:`datasets.make_classification` fails","284","     when generating more than 30 features. :issue:`8159` by","285","     :user:`Herilalaina Rakotoarison <herilalaina>`.","286","","287","   - Fixed a bug where :func:`model_selection.BaseSearchCV.inverse_transform`","288","     returns ``self.best_estimator_.transform()`` instead of","289","     ``self.best_estimator_.inverse_transform()``.","290","     :issue:`8344` by :user:`Akshay Gupta <Akshay0724>`.","291","","292","   - Fixed same issue in :func:`grid_search.BaseSearchCV.inverse_transform`","293","     :issue:`8846` by :user:`Rasmus Eriksson <MrMjauh>`","294","","295","   - Fixed a bug where :class:`linear_model.RandomizedLasso` and","296","     :class:`linear_model.RandomizedLogisticRegression` breaks for","297","     sparse input. :issue:`8259` by :user:`Aman Dalmia <dalmia>`.","298","","299","   - Fixed a bug where :func:`linear_model.RANSACRegressor.fit` may run until","300","     ``max_iter`` if finds a large inlier group early. :issue:`8251` by :user:`aivision2020`.","301","","302","   - Fixed a bug where :class:`sklearn.naive_bayes.MultinomialNB` and :class:`sklearn.naive_bayes.BernoulliNB`","303","     failed when `alpha=0`. :issue:`5814` by :user:`Yichuan Liu <yl565>` and","304","     :user:`Herilalaina Rakotoarison <herilalaina>`.","305","","306","   - Fixed a bug where :func:`datasets.make_moons` gives an","307","     incorrect result when ``n_samples`` is odd.","308","     :issue:`8198` by :user:`Josh Levy <levy5674>`.","309","","310","   - Fixed a bug where :class:`linear_model.LassoLars` does not give","311","     the same result as the LassoLars implementation available","312","     in R (lars library). :issue:`7849` by :user:`Jair Montoya Martinez <jmontoyam>`.","313","","314","   - Some ``fetch_`` functions in :mod:`sklearn.datasets` were ignoring the","315","     ``download_if_missing`` keyword. :issue:`7944` by :user:`Ralf Gommers <rgommers>`.","316","","317","   - Fixed a bug in :class:`ensemble.GradientBoostingClassifier`","318","     and :class:`ensemble.GradientBoostingRegressor`","319","     where a float being compared to ``0.0`` using ``==`` caused a divide by zero","320","     error. issue:`7970` by :user:`He Chen <chenhe95>`.","321","","322","   - Fix a bug regarding fitting :class:`cluster.KMeans` with a sparse","323","     array X and initial centroids, where X's means were unnecessarily being","324","     subtracted from the centroids. :issue:`7872` by :user:`Josh Karnofsky <jkarno>`.","325","","326","   - Fix estimators to accept a ``sample_weight`` parameter of type","327","     ``pandas.Series`` in their ``fit`` function. :issue:`7825` by","328","     `Kathleen Chen`_.","329","","330","   - Fixed a bug where :class:`ensemble.IsolationForest` fails when","331","     ``max_features`` is less than 1.","332","     :issue:`5732` by :user:`Ishank Gulati <IshankGulati>`.","333","","334","   - Fix a bug where :class:`ensemble.VotingClassifier` raises an error","335","     when a numpy array is passed in for weights. :issue:`7983` by","336","     :user:`Vincent Pham <vincentpham1991>`.","337","","338","   - Fix a bug in :class:`decomposition.LatentDirichletAllocation`","339","     where the ``perplexity`` method was returning incorrect results because","340","     the ``transform`` method returns normalized document topic distributions","341","     as of version 0.18. :issue:`7954` by :user:`Gary Foreman <garyForeman>`.","348","   - Fixes to the input validation in :class:`covariance.EllipticEnvelope`.","349","     :issue:`8086` by `Andreas M¨¹ller`_.","358","   - Several fixes to input validation in","359","     :class:`multiclass.OutputCodeClassifier`","362","   - Fix a bug where","363","     :class:`ensemble.gradient_boosting.QuantileLossFunction` computed","364","     negative errors for negative values of ``ytrue - ypred`` leading to","365","     wrong values when calling ``__call__``.","366","     :issue:`8087` by :user:`Alexis Mignon <AlexisMignon>`","368","   - Fix :func:`multioutput.MultiOutputClassifier.predict_proba` to","369","     return a list of 2d arrays, rather than a 3d array. In the case where","370","     different target columns had different numbers of classes, a `ValueError`","371","     would be raised on trying to stack matrices with different dimensions.","372","     :issue:`8093` by :user:`Peter Bull <pjbull>`.","374","   - Fix a bug where :func:`linear_model.LassoLars.fit` sometimes","375","     left `coef_` as a list, rather than an ndarray.","376","     :issue:`8160` by :user:`CJ Carey <perimosocordiae>`.","383","     :issue:`7513` by :user:`Roman Yurchak <rth>`.","394","   - Fix a bug in :func:`metrics.classification._check_targets`","395","     which would return ``'binary'`` if ``y_true`` and ``y_pred`` were","396","     both ``'binary'`` but the union of ``y_true`` and ``y_pred`` was","397","     ``'multiclass'``. :issue:`8377` by `Loic Esteve`_.","400","   - Fix :func:`linear_model.BayesianRidge.fit` to return","401","     ridge parameter `alpha_` and `lambda_` consistent with calculated","402","     coefficients `coef_` and `intercept_`.","403","     :issue:`8224` by :user:`Peter Gedeck <gedeck>`.","405","   - Fixed a bug in :class:`manifold.TSNE` where it stored the incorrect","406","     ``kl_divergence_``. :issue:`6507` by :user:`Sebastian Saeger <ssaeger>`.","407","","408","   - Fixed a bug in :class:`svm.OneClassSVM` where it returned floats instead of","409","     integer classes. :issue:`8676` by :user:`Vathsala Achar <VathsalaAchar>`.","410","","411","   - Fixed a bug where :func:`tree.export_graphviz` raised an error","412","     when the length of features_names does not match n_features in the decision","413","     tree. :issue:`8512` by :user:`Li Li <aikinogard>`.","414","","415","   - Fixed a bug in :class:`manifold.TSNE` affecting convergence of the","416","     gradient descent. :issue:`8768` by :user:`David DeTomaso <deto>`.","417","","418","   - Fixed a memory leak in our LibLinear implementation. :issue:`9024` by","419","     :user:`Sergei Lebedev <superbobry>`","420","   - Fixed improper scaling in :class:`cross_decomposition.PLSRegression`","421","     with ``scale=True``. :issue:`7819` by :user:`jayzed82 <jayzed82>`.","422","","423","   - Fixed oob_score in :class:`ensemble.BaggingClassifier`.","424","     :issue:`8936` by :user:`mlewis1729 <mlewis1729>`","425","","426","   - Add ``shuffle`` parameter to :func:`model_selection.train_test_split`.","427","     :issue:`8845` by  :user:`themrmax <themrmax>`","428","","429","   - Fix AIC\/BIC criterion computation in :class:`linear_model.LassoLarsIC`.","430","     :issue:`9022` by `Alexandre Gramfort`_ and :user:`Mehmet Basbug <mehmetbasbug>`.","431","","432","   - Fix bug where stratified CV splitters did not work with","433","     :class:`linear_model.LassoCV`. :issue:`8973` by","434","     :user:`Paulo Haddad <paulochf>`.","435","","436","   - Fixed a bug in :class:`linear_model.RandomizedLasso`,","437","     :class:`linear_model.Lars`, :class:`linear_model.LassoLars`,","438","     :class:`linear_model.LarsCV` and :class:`linear_model.LassoLarsCV`,","439","     where the parameter ``precompute`` were not used consistently across","440","     classes, and some values proposed in the docstring could raise errors.","441","     :issue:`5359` by `Tom Dupre la Tour`_.","442","","443","   - Fixed a bug where :func:`model_selection.validation_curve`","444","     reused the same estimator for each parameter value.","445","     :issue:`7365` by :user:`Aleksandr Sandrovskii <Sundrique>`.","446","","447","   - :class:`multiclass.OneVsOneClassifier`'s ``partial_fit`` now ensures all","448","     classes are provided up-front. :issue:`6250` by","449","     :user:`Asish Panda <kaichogami>`.","450","","451","   - Fixed an integer overflow bug in :func:`metrics.confusion_matrix` and","452","     hence :func:`metrics.cohen_kappa_score`. :issue:`8354`, :issue:`7929`","453","     by `Joel Nothman`_ and :user:`Jon Crall <Erotemic>`.","454","","455","   - Made default kernel parameters kernel-dependent in :class:`kernel_approximation.Nystroem`","456","     :issue:`5229` by :user:`mth4saurabh` and `Andreas M¨¹ller`_.","457","","458","   - Fixed passing of ``gamma`` parameter to the ``chi2`` kernel in","459","     :func:`metrics.pairwise_kernels` :issue:`5211` by :user:`nrhine1`,","460","     :user:`mth4saurabh` and `Andreas M¨¹ller`_.","461","","462","  -  Fixed a bug in :class:`gaussian_process.GaussianProcessRegressor`","463","     when the standard deviation and covariance predicted without fit","464","     would fail with a unmeaningful error by default.","465","     :issue:`6573` by :user:`Quazi Marufur Rahman <qmaruf>` and","466","     `Manoj Kumar`_.","467","","468","   - Fixed the implementation of `explained_variance_`","469","     in :class:`decomposition.PCA`,","470","     :class:`decomposition.RandomizedPCA` and","471","     :class:`decomposition.IncrementalPCA`.","472","     :issue:`9105` by `Hanmin Qin <https:\/\/github.com\/qinhanmin2014>`_.","473","","474","   - Fix :class:`semi_supervised.BaseLabelPropagation` to correctly implement","475","     ``LabelPropagation`` and ``LabelSpreading`` as done in the referenced","476","     papers. :class:`semi_supervised.LabelPropagation` now always does hard","477","     clamping. Its ``alpha`` parameter has no effect and is","478","     deprecated to be removed in 0.21. :issue:`6727` :issue:`3550` issue:`5770`","479","     by :user:`Andre Ambrosio Boechat <boechat107>`, :user:`Utkarsh Upadhyay","480","     <musically-ut>`, and `Joel Nothman`_.","481","","482","   - Add ``data_home`` parameter to","483","     :func:`sklearn.datasets.fetch_kddcup99` by `Loic Esteve`_.","484","","485","   - Fix inconsistent results between :class:`linear_model.RidgeCV`","486","     and :class:`linear_model.Ridge` when using ``normalize=True``","487","     by `Alexandre Gramfort`_.","488","","489","   - Fixed the implementation of :class:`manifold.TSNE`:","490","      - ``early_exageration`` parameter had no effect and is now used for the","491","        first 250 optimization iterations.","492","      - Fixed the ``InsersionError`` reported in :issue:`8992`.","493","      - Improve the learning schedule to match the one from the reference","494","        implementation `lvdmaaten\/bhtsne <https:\/\/github.com\/lvdmaaten\/bhtsne>`_.","495","     by :user:`Thomas Moreau <tomMoral>` and `Olivier Grisel`_.","500","   - Ensure that estimators' attributes ending with ``_`` are not set","501","     in the constructor but only in the ``fit`` method. Most notably,","502","     ensemble estimators (deriving from :class:`ensemble.BaseEnsemble`)","503","     now only have ``self.estimators_`` available after ``fit``.","504","     :issue:`7464` by `Lars Buitinck`_ and `Loic Esteve`_.","506","   - All checks in ``utils.estimator_checks``, in particular","507","     :func:`utils.estimator_checks.check_estimator` now accept estimator","508","     instances. Most other checks do not accept","509","     estimator classes any more. :issue:`9019` by `Andreas M¨¹ller`_.","517","   - Replace attribute ``named_steps`` ``dict`` to :class:`utils.Bunch`","518","     in :class:`pipeline.Pipeline` to enable tab completion in interactive","519","     environment. In the case conflict value on ``named_steps`` and ``dict``","520","     attribute, ``dict`` behavior will be prioritized.","521","     :issue:`8481` by :user:`Herilalaina Rakotoarison <herilalaina>`.","523","   - The :func:`multioutput.MultiOutputClassifier.predict_proba`","524","     function used to return a 3d array (``n_samples``, ``n_classes``,","525","     ``n_outputs``). In the case where different target columns had different","526","     numbers of classes, a `ValueError` would be raised on trying to stack","527","     matrices with different dimensions. This function now returns a list of","528","     arrays where the length of the list is ``n_outputs``, and each array is","529","     (``n_samples``, ``n_classes``) for that particular output.","530","     :issue:`8093` by :user:`Peter Bull <pjbull>`.","543","   - The ``decision_function`` output shape for binary classification in","544","     :class:`multiclass.OneVsRestClassifier` and","545","     :class:`multiclass.OneVsOneClassifier` is now ``(n_samples,)`` to conform","546","     to scikit-learn conventions. :issue:`9100` by `Andreas M¨¹ller`_.","547","","548","   - Gradient boosting base models are no longer estimators. By `Andreas M¨¹ller`_.","549","","550","   - :class:`feature_selection.SelectFromModel` now validates the ``threshold``","551","     parameter and sets the ``threshold_`` attribute during the call to","552","     ``fit``, and no longer during the call to ``transform```, by `Andreas","553","     M¨¹ller`_.","554","","555","   - :class:`feature_selection.SelectFromModel` now has a ``partial_fit``","556","     method only if the underlying estimator does. By `Andreas M¨¹ller`_.","557","","558","   - :class:`multiclass.OneVsRestClassifier` now has a ``partial_fit`` method","559","     only if the underlying estimator does.  By `Andreas M¨¹ller`_.","560","","561","   - Estimators with both methods ``decision_function`` and ``predict_proba``","562","     are now required to have a monotonic relation between them. The","563","     method ``check_decision_proba_consistency`` has been added in","564","     **sklearn.utils.estimator_checks** to check their consistency.","565","     :issue:`7578` by :user:`Shubham Bhardwaj <shubham0704>`","566","","572","   - All tree based estimators now accept a ``min_impurity_decrease``","573","     parameter in lieu of the ``min_impurity_split``, which is now deprecated.","574","     The ``min_impurity_decrease`` helps stop splitting the nodes in which","575","     the weighted impurity decrease from splitting is no longer alteast","576","     ``min_impurity_decrease``.  :issue:`8449` by `Raghav RV`_.","578","   - The ``n_topics`` parameter of :class:`decomposition.LatentDirichletAllocation`","579","     has been renamed to ``n_components`` and will be removed in version 0.21.","580","     :issue:`8922` by :user:`Attractadore`","582","   - :class:`cluster.bicluster.SpectralCoclustering` and","583","     :class:`cluster.bicluster.SpectralBiclustering` now accept ``y`` in fit.","584","     :issue:`6126` by :user:ldirer","586","   - :class:`neighbors.LSHForest` has been deprecated and will be","587","     removed in 0.21 due to poor performance.","588","     :issue:`8996` by `Andreas M¨¹ller`_.","592","     :mod:`sklearn.utils` have been removed or deprecated accordingly.","621","     - ``neighbors.approximate.LSHForest``","622","     - ``linear_model.randomized_l1``","624","    - Deprecate the ``y`` parameter in `transform` and `inverse_transform`.","625","      The method  should not accept ``y`` parameter, as it's used at the prediction time.","626","      :issue:`8174` by :user:`Tahar Zanouda <tzano>`, `Alexandre Gramfort`_","627","      and `Raghav RV`_.","637",".. topic:: Last release with Python 2.6 support","638","","639","    Scikit-learn 0.18 is the last major release of scikit-learn to support Python 2.6.","640","    Later versions of scikit-learn will require Python 2.7 or above.","641","","642","","1354","   - The :mod:`sklearn.linear_model.randomized_l1` is deprecated.","1355","     :issue: `8995` by :user:`Ramana.S <sentient07>`."]}],"doc\/modules\/pipeline.rst":[{"add":["126",".. _pipeline_cache:","127",""],"delete":[]}]}},"c13ba26c4780d6088c40db3cc0f8367df4e27f1f":{"changes":{"doc\/Makefile":"MODIFY","sklearn\/tree\/__init__.py":"MODIFY","doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/tree\/_reingold_tilford.py":"ADD","sklearn\/tree\/tests\/test_export.py":"MODIFY","sklearn\/tree\/export.py":"MODIFY","sklearn\/tree\/tests\/test_reingold_tilford.py":"ADD","examples\/tree\/plot_iris.py":"MODIFY","doc\/modules\/tree.rst":"MODIFY"},"diff":{"doc\/Makefile":[{"add":["15","ALLSPHINXOPTS   = -T -d $(BUILDDIR)\/doctrees $(PAPEROPT_$(PAPER)) $(SPHINXOPTS)\\"],"delete":["15","ALLSPHINXOPTS   = -d $(BUILDDIR)\/doctrees $(PAPEROPT_$(PAPER)) $(SPHINXOPTS)\\"]}],"sklearn\/tree\/__init__.py":[{"add":["9","from .export import export_graphviz, plot_tree","12","           \"ExtraTreeClassifier\", \"ExtraTreeRegressor\", \"export_graphviz\",","13","           \"plot_tree\"]"],"delete":["9","from .export import export_graphviz","12","           \"ExtraTreeClassifier\", \"ExtraTreeRegressor\", \"export_graphviz\"]"]}],"doc\/whats_new\/v0.21.rst":[{"add":["64","- Decision Trees can now be plotted with matplotlib using","65","  :func:`tree.export.plot_tree` without relying on  the ``dot`` library,","66","  removing a hard-to-install dependency.","67","  :issue:`8508` by `Andreas M¨¹ller`_.","68",""],"delete":[]}],"sklearn\/tree\/_reingold_tilford.py":[{"add":[],"delete":[]}],"sklearn\/tree\/tests\/test_export.py":[{"add":["3","import pytest","12","from sklearn.tree import export_graphviz, plot_tree","95","                'value = [0.5, 0.5]>, fillcolor=\"#ffffff\"] ;\\n' \\","97","                'fillcolor=\"#e58139\"] ;\\n' \\","101","                'fillcolor=\"#399de5\"] ;\\n' \\","129","                'samples = 6\\\\nvalue = [3, 3]\", fillcolor=\"#ffffff\"] ;\\n' \\","151","                '[3.0, 1.0, 0.5]]\", fillcolor=\"#ffffff\"] ;\\n' \\","153","                '[3, 0, 0]]\", fillcolor=\"#e58139\"] ;\\n' \\","158","                '[0.0, 1.0, 0.5]]\", fillcolor=\"#f1bd97\"] ;\\n' \\","162","                '[0, 1, 0]]\", fillcolor=\"#e58139\"] ;\\n' \\","165","                '[0.0, 0.0, 0.5]]\", fillcolor=\"#e58139\"] ;\\n' \\","187","                'value = 0.0\", fillcolor=\"#f2c09c\"] ;\\n' \\","189","                'fillcolor=\"#ffffff\"] ;\\n' \\","193","                'fillcolor=\"#e58139\"] ;\\n' \\","210","                'fillcolor=\"#ffffff\"] ;\\n' \\","311","","312","","313","def test_plot_tree():","314","    # mostly smoke tests","315","    pytest.importorskip(\"matplotlib.pyplot\")","316","    # Check correctness of export_graphviz","317","    clf = DecisionTreeClassifier(max_depth=3,","318","                                 min_samples_split=2,","319","                                 criterion=\"gini\",","320","                                 random_state=2)","321","    clf.fit(X, y)","322","","323","    # Test export code","324","    feature_names = ['first feat', 'sepal_width']","325","    nodes = plot_tree(clf, feature_names=feature_names)","326","    assert len(nodes) == 3","327","    assert nodes[0].get_text() == (\"first feat <= 0.0\\nentropy = 0.5\\n\"","328","                                   \"samples = 6\\nvalue = [3, 3]\")","329","    assert nodes[1].get_text() == \"entropy = 0.0\\nsamples = 3\\nvalue = [3, 0]\"","330","    assert nodes[2].get_text() == \"entropy = 0.0\\nsamples = 3\\nvalue = [0, 3]\""],"delete":["11","from sklearn.tree import export_graphviz","94","                'value = [0.5, 0.5]>, fillcolor=\"#e5813900\"] ;\\n' \\","96","                'fillcolor=\"#e58139ff\"] ;\\n' \\","100","                'fillcolor=\"#399de5ff\"] ;\\n' \\","128","                'samples = 6\\\\nvalue = [3, 3]\", fillcolor=\"#e5813900\"] ;\\n' \\","150","                '[3.0, 1.0, 0.5]]\", fillcolor=\"#e5813900\"] ;\\n' \\","152","                '[3, 0, 0]]\", fillcolor=\"#e58139ff\"] ;\\n' \\","157","                '[0.0, 1.0, 0.5]]\", fillcolor=\"#e5813986\"] ;\\n' \\","161","                '[0, 1, 0]]\", fillcolor=\"#e58139ff\"] ;\\n' \\","164","                '[0.0, 0.0, 0.5]]\", fillcolor=\"#e58139ff\"] ;\\n' \\","186","                'value = 0.0\", fillcolor=\"#e5813980\"] ;\\n' \\","188","                'fillcolor=\"#e5813900\"] ;\\n' \\","192","                'fillcolor=\"#e58139ff\"] ;\\n' \\","209","                'fillcolor=\"#e5813900\"] ;\\n' \\"]}],"sklearn\/tree\/export.py":[{"add":["12","import warnings","23","from ._reingold_tilford import buchheim, Tree","76","def plot_tree(decision_tree, max_depth=None, feature_names=None,","77","              class_names=None, label='all', filled=False,","78","              impurity=True, node_ids=False,","79","              proportion=False, rotate=False, rounded=False,","80","              precision=3, ax=None, fontsize=None):","81","    \"\"\"Plot a decision tree.","82","","83","    The sample counts that are shown are weighted with any sample_weights that","84","    might be present.","85","    This function requires matplotlib, and works best with matplotlib >= 1.5.","86","","87","    The visualization is fit automatically to the size of the axis.","88","    Use the ``figsize`` or ``dpi`` arguments of ``plt.figure``  to control","89","    the size of the rendering.","90","","91","    Read more in the :ref:`User Guide <tree>`.","92","","93","    .. versionadded:: 0.21","94","","95","    Parameters","96","    ----------","97","    decision_tree : decision tree regressor or classifier","98","        The decision tree to be exported to GraphViz.","99","","100","    max_depth : int, optional (default=None)","101","        The maximum depth of the representation. If None, the tree is fully","102","        generated.","103","","104","    feature_names : list of strings, optional (default=None)","105","        Names of each of the features.","106","","107","    class_names : list of strings, bool or None, optional (default=None)","108","        Names of each of the target classes in ascending numerical order.","109","        Only relevant for classification and not supported for multi-output.","110","        If ``True``, shows a symbolic representation of the class name.","111","","112","    label : {'all', 'root', 'none'}, optional (default='all')","113","        Whether to show informative labels for impurity, etc.","114","        Options include 'all' to show at every node, 'root' to show only at","115","        the top root node, or 'none' to not show at any node.","116","","117","    filled : bool, optional (default=False)","118","        When set to ``True``, paint nodes to indicate majority class for","119","        classification, extremity of values for regression, or purity of node","120","        for multi-output.","121","","122","    impurity : bool, optional (default=True)","123","        When set to ``True``, show the impurity at each node.","124","","125","    node_ids : bool, optional (default=False)","126","        When set to ``True``, show the ID number on each node.","127","","128","    proportion : bool, optional (default=False)","129","        When set to ``True``, change the display of 'values' and\/or 'samples'","130","        to be proportions and percentages respectively.","131","","132","    rotate : bool, optional (default=False)","133","        When set to ``True``, orient tree left to right rather than top-down.","134","","135","    rounded : bool, optional (default=False)","136","        When set to ``True``, draw node boxes with rounded corners and use","137","        Helvetica fonts instead of Times-Roman.","138","","139","    precision : int, optional (default=3)","140","        Number of digits of precision for floating point in the values of","141","        impurity, threshold and value attributes of each node.","142","","143","    ax : matplotlib axis, optional (default=None)","144","        Axes to plot to. If None, use current axis. Any previous content","145","        is cleared.","146","","147","    fontsize : int, optional (default=None)","148","        Size of text font. If None, determined automatically to fit figure.","149","","150","    Returns","151","    -------","152","    annotations : list of artists","153","        List containing the artists for the annotation boxes making up the","154","        tree.","155","","156","    Examples","157","    --------","158","    >>> from sklearn.datasets import load_iris","159","    >>> from sklearn import tree","160","","161","    >>> clf = tree.DecisionTreeClassifier(random_state=0)","162","    >>> iris = load_iris()","163","","164","    >>> clf = clf.fit(iris.data, iris.target)","165","    >>> tree.plot_tree(clf)  # doctest: +SKIP","166","    [Text(251.5,345.217,'X[3] <= 0.8...","167","","168","    \"\"\"","169","    exporter = _MPLTreeExporter(","170","        max_depth=max_depth, feature_names=feature_names,","171","        class_names=class_names, label=label, filled=filled,","172","        impurity=impurity, node_ids=node_ids,","173","        proportion=proportion, rotate=rotate, rounded=rounded,","174","        precision=precision, fontsize=fontsize)","175","    return exporter.export(decision_tree, ax=ax)","176","","177","","178","class _BaseTreeExporter(object):","179","    def __init__(self, max_depth=None, feature_names=None,","180","                 class_names=None, label='all', filled=False,","181","                 impurity=True, node_ids=False,","182","                 proportion=False, rotate=False, rounded=False,","183","                 precision=3, fontsize=None):","184","        self.max_depth = max_depth","185","        self.feature_names = feature_names","186","        self.class_names = class_names","187","        self.label = label","188","        self.filled = filled","189","        self.impurity = impurity","190","        self.node_ids = node_ids","191","        self.proportion = proportion","192","        self.rotate = rotate","193","        self.rounded = rounded","194","        self.precision = precision","195","        self.fontsize = fontsize","196","","197","    def get_color(self, value):","198","        # Find the appropriate color & intensity for a node","199","        if self.colors['bounds'] is None:","200","            # Classification tree","201","            color = list(self.colors['rgb'][np.argmax(value)])","202","            sorted_values = sorted(value, reverse=True)","203","            if len(sorted_values) == 1:","204","                alpha = 0","205","            else:","206","                alpha = ((sorted_values[0] - sorted_values[1])","207","                         \/ (1 - sorted_values[1]))","208","        else:","209","            # Regression tree or multi-output","210","            color = list(self.colors['rgb'][0])","211","            alpha = ((value - self.colors['bounds'][0]) \/","212","                     (self.colors['bounds'][1] - self.colors['bounds'][0]))","213","        # unpack numpy scalars","214","        alpha = float(alpha)","215","        # compute the color as alpha against white","216","        color = [int(round(alpha * c + (1 - alpha) * 255, 0)) for c in color]","217","        # Return html color code in #RRGGBB format","218","        return '#%2x%2x%2x' % tuple(color)","219","","220","    def get_fill_color(self, tree, node_id):","221","        # Fetch appropriate color for node","222","        if 'rgb' not in self.colors:","223","            # Initialize colors and bounds if required","224","            self.colors['rgb'] = _color_brew(tree.n_classes[0])","225","            if tree.n_outputs != 1:","226","                # Find max and min impurities for multi-output","227","                self.colors['bounds'] = (np.min(-tree.impurity),","228","                                         np.max(-tree.impurity))","229","            elif (tree.n_classes[0] == 1 and","230","                  len(np.unique(tree.value)) != 1):","231","                # Find max and min values in leaf nodes for regression","232","                self.colors['bounds'] = (np.min(tree.value),","233","                                         np.max(tree.value))","234","        if tree.n_outputs == 1:","235","            node_val = (tree.value[node_id][0, :] \/","236","                        tree.weighted_n_node_samples[node_id])","237","            if tree.n_classes[0] == 1:","238","                # Regression","239","                node_val = tree.value[node_id][0, :]","240","        else:","241","            # If multi-output color node by impurity","242","            node_val = -tree.impurity[node_id]","243","        return self.get_color(node_val)","244","","245","    def node_to_str(self, tree, node_id, criterion):","246","        # Generate the node content string","247","        if tree.n_outputs == 1:","248","            value = tree.value[node_id][0, :]","249","        else:","250","            value = tree.value[node_id]","251","","252","        # Should labels be shown?","253","        labels = (self.label == 'root' and node_id == 0) or self.label == 'all'","254","","255","        characters = self.characters","256","        node_string = characters[-1]","257","","258","        # Write node ID","259","        if self.node_ids:","260","            if labels:","261","                node_string += 'node '","262","            node_string += characters[0] + str(node_id) + characters[4]","263","","264","        # Write decision criteria","265","        if tree.children_left[node_id] != _tree.TREE_LEAF:","266","            # Always write node decision criteria, except for leaves","267","            if self.feature_names is not None:","268","                feature = self.feature_names[tree.feature[node_id]]","269","            else:","270","                feature = \"X%s%s%s\" % (characters[1],","271","                                       tree.feature[node_id],","272","                                       characters[2])","273","            node_string += '%s %s %s%s' % (feature,","274","                                           characters[3],","275","                                           round(tree.threshold[node_id],","276","                                                 self.precision),","277","                                           characters[4])","278","","279","        # Write impurity","280","        if self.impurity:","281","            if isinstance(criterion, _criterion.FriedmanMSE):","282","                criterion = \"friedman_mse\"","283","            elif not isinstance(criterion, six.string_types):","284","                criterion = \"impurity\"","285","            if labels:","286","                node_string += '%s = ' % criterion","287","            node_string += (str(round(tree.impurity[node_id], self.precision))","288","                            + characters[4])","289","","290","        # Write node sample count","291","        if labels:","292","            node_string += 'samples = '","293","        if self.proportion:","294","            percent = (100. * tree.n_node_samples[node_id] \/","295","                       float(tree.n_node_samples[0]))","296","            node_string += (str(round(percent, 1)) + '%' +","297","                            characters[4])","298","        else:","299","            node_string += (str(tree.n_node_samples[node_id]) +","300","                            characters[4])","301","","302","        # Write node class distribution \/ regression value","303","        if self.proportion and tree.n_classes[0] != 1:","304","            # For classification this will show the proportion of samples","305","            value = value \/ tree.weighted_n_node_samples[node_id]","306","        if labels:","307","            node_string += 'value = '","308","        if tree.n_classes[0] == 1:","309","            # Regression","310","            value_text = np.around(value, self.precision)","311","        elif self.proportion:","312","            # Classification","313","            value_text = np.around(value, self.precision)","314","        elif np.all(np.equal(np.mod(value, 1), 0)):","315","            # Classification without floating-point weights","316","            value_text = value.astype(int)","317","        else:","318","            # Classification with floating-point weights","319","            value_text = np.around(value, self.precision)","320","        # Strip whitespace","321","        value_text = str(value_text.astype('S32')).replace(\"b'\", \"'\")","322","        value_text = value_text.replace(\"' '\", \", \").replace(\"'\", \"\")","323","        if tree.n_classes[0] == 1 and tree.n_outputs == 1:","324","            value_text = value_text.replace(\"[\", \"\").replace(\"]\", \"\")","325","        value_text = value_text.replace(\"\\n \", characters[4])","326","        node_string += value_text + characters[4]","327","","328","        # Write node majority class","329","        if (self.class_names is not None and","330","                tree.n_classes[0] != 1 and","331","                tree.n_outputs == 1):","332","            # Only done for single-output classification trees","333","            if labels:","334","                node_string += 'class = '","335","            if self.class_names is not True:","336","                class_name = self.class_names[np.argmax(value)]","337","            else:","338","                class_name = \"y%s%s%s\" % (characters[1],","339","                                          np.argmax(value),","340","                                          characters[2])","341","            node_string += class_name","342","","343","        # Clean up any trailing newlines","344","        if node_string.endswith(characters[4]):","345","            node_string = node_string[:-len(characters[4])]","346","","347","        return node_string + characters[5]","348","","349","","350","class _DOTTreeExporter(_BaseTreeExporter):","351","    def __init__(self, out_file=SENTINEL, max_depth=None,","352","                 feature_names=None, class_names=None, label='all',","353","                 filled=False, leaves_parallel=False, impurity=True,","354","                 node_ids=False, proportion=False, rotate=False, rounded=False,","355","                 special_characters=False, precision=3):","356","","357","        super(_DOTTreeExporter, self).__init__(","358","            max_depth=max_depth, feature_names=feature_names,","359","            class_names=class_names, label=label, filled=filled,","360","            impurity=impurity,","361","            node_ids=node_ids, proportion=proportion, rotate=rotate,","362","            rounded=rounded,","363","            precision=precision)","364","        self.leaves_parallel = leaves_parallel","365","        self.out_file = out_file","366","        self.special_characters = special_characters","367","","368","        # PostScript compatibility for special characters","369","        if special_characters:","370","            self.characters = ['&#35;', '<SUB>', '<\/SUB>', '&le;', '<br\/>',","371","                               '>', '<']","372","        else:","373","            self.characters = ['#', '[', ']', '<=', '\\\\n', '\"', '\"']","374","","375","        # validate","376","        if isinstance(precision, Integral):","377","            if precision < 0:","378","                raise ValueError(\"'precision' should be greater or equal to 0.\"","379","                                 \" Got {} instead.\".format(precision))","380","        else:","381","            raise ValueError(\"'precision' should be an integer. Got {}\"","382","                             \" instead.\".format(type(precision)))","383","","384","        # The depth of each node for plotting with 'leaf' option","385","        self.ranks = {'leaves': []}","386","        # The colors to render each node with","387","        self.colors = {'bounds': None}","388","","389","    def export(self, decision_tree):","390","        # Check length of feature_names before getting into the tree node","391","        # Raise error if length of feature_names does not match","392","        # n_features_ in the decision_tree","393","        if self.feature_names is not None:","394","            if len(self.feature_names) != decision_tree.n_features_:","395","                raise ValueError(\"Length of feature_names, %d \"","396","                                 \"does not match number of features, %d\"","397","                                 % (len(self.feature_names),","398","                                    decision_tree.n_features_))","399","        # each part writes to out_file","400","        self.head()","401","        # Now recurse the tree and add node & edge attributes","402","        if isinstance(decision_tree, _tree.Tree):","403","            self.recurse(decision_tree, 0, criterion=\"impurity\")","404","        else:","405","            self.recurse(decision_tree.tree_, 0,","406","                         criterion=decision_tree.criterion)","407","","408","        self.tail()","409","","410","    def tail(self):","411","        # If required, draw leaf nodes at same depth as each other","412","        if self.leaves_parallel:","413","            for rank in sorted(self.ranks):","414","                self.out_file.write(","415","                    \"{rank=same ; \" +","416","                    \"; \".join(r for r in self.ranks[rank]) + \"} ;\\n\")","417","        self.out_file.write(\"}\")","418","","419","    def head(self):","420","        self.out_file.write('digraph Tree {\\n')","421","","422","        # Specify node aesthetics","423","        self.out_file.write('node [shape=box')","424","        rounded_filled = []","425","        if self.filled:","426","            rounded_filled.append('filled')","427","        if self.rounded:","428","            rounded_filled.append('rounded')","429","        if len(rounded_filled) > 0:","430","            self.out_file.write(","431","                ', style=\"%s\", color=\"black\"'","432","                % \", \".join(rounded_filled))","433","        if self.rounded:","434","            self.out_file.write(', fontname=helvetica')","435","        self.out_file.write('] ;\\n')","436","","437","        # Specify graph & edge aesthetics","438","        if self.leaves_parallel:","439","            self.out_file.write(","440","                'graph [ranksep=equally, splines=polyline] ;\\n')","441","        if self.rounded:","442","            self.out_file.write('edge [fontname=helvetica] ;\\n')","443","        if self.rotate:","444","            self.out_file.write('rankdir=LR ;\\n')","445","","446","    def recurse(self, tree, node_id, criterion, parent=None, depth=0):","447","        if node_id == _tree.TREE_LEAF:","448","            raise ValueError(\"Invalid node_id %s\" % _tree.TREE_LEAF)","449","","450","        left_child = tree.children_left[node_id]","451","        right_child = tree.children_right[node_id]","452","","453","        # Add node with description","454","        if self.max_depth is None or depth <= self.max_depth:","455","","456","            # Collect ranks for 'leaf' option in plot_options","457","            if left_child == _tree.TREE_LEAF:","458","                self.ranks['leaves'].append(str(node_id))","459","            elif str(depth) not in self.ranks:","460","                self.ranks[str(depth)] = [str(node_id)]","461","            else:","462","                self.ranks[str(depth)].append(str(node_id))","463","","464","            self.out_file.write(","465","                '%d [label=%s' % (node_id, self.node_to_str(tree, node_id,","466","                                                            criterion)))","467","","468","            if self.filled:","469","                self.out_file.write(', fillcolor=\"%s\"'","470","                                    % self.get_fill_color(tree, node_id))","471","            self.out_file.write('] ;\\n')","472","","473","            if parent is not None:","474","                # Add edge to parent","475","                self.out_file.write('%d -> %d' % (parent, node_id))","476","                if parent == 0:","477","                    # Draw True\/False labels if parent is root node","478","                    angles = np.array([45, -45]) * ((self.rotate - .5) * -2)","479","                    self.out_file.write(' [labeldistance=2.5, labelangle=')","480","                    if node_id == 1:","481","                        self.out_file.write('%d, headlabel=\"True\"]' %","482","                                            angles[0])","483","                    else:","484","                        self.out_file.write('%d, headlabel=\"False\"]' %","485","                                            angles[1])","486","                self.out_file.write(' ;\\n')","487","","488","            if left_child != _tree.TREE_LEAF:","489","                self.recurse(tree, left_child, criterion=criterion,","490","                             parent=node_id, depth=depth + 1)","491","                self.recurse(tree, right_child, criterion=criterion,","492","                             parent=node_id, depth=depth + 1)","493","","494","        else:","495","            self.ranks['leaves'].append(str(node_id))","496","","497","            self.out_file.write('%d [label=\"(...)\"' % node_id)","498","            if self.filled:","499","                # color cropped nodes grey","500","                self.out_file.write(', fillcolor=\"#C0C0C0\"')","501","            self.out_file.write('] ;\\n' % node_id)","502","","503","            if parent is not None:","504","                # Add edge to parent","505","                self.out_file.write('%d -> %d ;\\n' % (parent, node_id))","506","","507","","508","class _MPLTreeExporter(_BaseTreeExporter):","509","    def __init__(self, max_depth=None, feature_names=None,","510","                 class_names=None, label='all', filled=False,","511","                 impurity=True, node_ids=False,","512","                 proportion=False, rotate=False, rounded=False,","513","                 precision=3, fontsize=None):","514","","515","        super(_MPLTreeExporter, self).__init__(","516","            max_depth=max_depth, feature_names=feature_names,","517","            class_names=class_names, label=label, filled=filled,","518","            impurity=impurity, node_ids=node_ids, proportion=proportion,","519","            rotate=rotate, rounded=rounded, precision=precision)","520","        self.fontsize = fontsize","521","","522","        # validate","523","        if isinstance(precision, Integral):","524","            if precision < 0:","525","                raise ValueError(\"'precision' should be greater or equal to 0.\"","526","                                 \" Got {} instead.\".format(precision))","527","        else:","528","            raise ValueError(\"'precision' should be an integer. Got {}\"","529","                             \" instead.\".format(type(precision)))","530","","531","        # The depth of each node for plotting with 'leaf' option","532","        self.ranks = {'leaves': []}","533","        # The colors to render each node with","534","        self.colors = {'bounds': None}","535","","536","        self.characters = ['#', '[', ']', '<=', '\\n', '', '']","537","","538","        self.bbox_args = dict(fc='w')","539","        if self.rounded:","540","            self.bbox_args['boxstyle'] = \"round\"","541","        else:","542","            # matplotlib <1.5 requires explicit boxstyle","543","            self.bbox_args['boxstyle'] = \"square\"","544","","545","        self.arrow_args = dict(arrowstyle=\"<-\")","546","","547","    def _make_tree(self, node_id, et, depth=0):","548","        # traverses _tree.Tree recursively, builds intermediate","549","        # \"_reingold_tilford.Tree\" object","550","        name = self.node_to_str(et, node_id, criterion='entropy')","551","        if (et.children_left[node_id] != _tree.TREE_LEAF","552","                and (self.max_depth is None or depth <= self.max_depth)):","553","            children = [self._make_tree(et.children_left[node_id], et,","554","                                        depth=depth + 1),","555","                        self._make_tree(et.children_right[node_id], et,","556","                                        depth=depth + 1)]","557","        else:","558","            return Tree(name, node_id)","559","        return Tree(name, node_id, *children)","560","","561","    def export(self, decision_tree, ax=None):","562","        import matplotlib.pyplot as plt","563","        from matplotlib.text import Annotation","564","        if ax is None:","565","            ax = plt.gca()","566","        ax.clear()","567","        ax.set_axis_off()","568","        my_tree = self._make_tree(0, decision_tree.tree_)","569","        draw_tree = buchheim(my_tree)","570","","571","        # important to make sure we're still","572","        # inside the axis after drawing the box","573","        # this makes sense because the width of a box","574","        # is about the same as the distance between boxes","575","        max_x, max_y = draw_tree.max_extents() + 1","576","        ax_width = ax.get_window_extent().width","577","        ax_height = ax.get_window_extent().height","578","","579","        scale_x = ax_width \/ max_x","580","        scale_y = ax_height \/ max_y","581","","582","        self.recurse(draw_tree, decision_tree.tree_, ax,","583","                     scale_x, scale_y, ax_height)","584","","585","        anns = [ann for ann in ax.get_children()","586","                if isinstance(ann, Annotation)]","587","","588","        # update sizes of all bboxes","589","        renderer = ax.figure.canvas.get_renderer()","590","","591","        for ann in anns:","592","            ann.update_bbox_position_size(renderer)","593","","594","        if self.fontsize is None:","595","            # get figure to data transform","596","            # adjust fontsize to avoid overlap","597","            # get max box width and height","598","            try:","599","                extents = [ann.get_bbox_patch().get_window_extent()","600","                           for ann in anns]","601","                max_width = max([extent.width for extent in extents])","602","                max_height = max([extent.height for extent in extents])","603","                # width should be around scale_x in axis coordinates","604","                size = anns[0].get_fontsize() * min(scale_x \/ max_width,","605","                                                    scale_y \/ max_height)","606","                for ann in anns:","607","                    ann.set_fontsize(size)","608","            except AttributeError:","609","                # matplotlib < 1.5","610","                warnings.warn(\"Automatic scaling of tree plots requires \"","611","                              \"matplotlib 1.5 or higher. Please specify \"","612","                              \"fontsize.\")","613","","614","        return anns","615","","616","    def recurse(self, node, tree, ax, scale_x, scale_y, height, depth=0):","617","        # need to copy bbox args because matplotib <1.5 modifies them","618","        kwargs = dict(bbox=self.bbox_args.copy(), ha='center', va='center',","619","                      zorder=100 - 10 * depth, xycoords='axes pixels')","620","","621","        if self.fontsize is not None:","622","            kwargs['fontsize'] = self.fontsize","623","","624","        # offset things by .5 to center them in plot","625","        xy = ((node.x + .5) * scale_x, height - (node.y + .5) * scale_y)","626","","627","        if self.max_depth is None or depth <= self.max_depth:","628","            if self.filled:","629","                kwargs['bbox']['fc'] = self.get_fill_color(tree,","630","                                                           node.tree.node_id)","631","            if node.parent is None:","632","                # root","633","                ax.annotate(node.tree.label, xy, **kwargs)","634","            else:","635","                xy_parent = ((node.parent.x + .5) * scale_x,","636","                             height - (node.parent.y + .5) * scale_y)","637","                kwargs[\"arrowprops\"] = self.arrow_args","638","                ax.annotate(node.tree.label, xy_parent, xy, **kwargs)","639","            for child in node.children:","640","                self.recurse(child, tree, ax, scale_x, scale_y, height,","641","                             depth=depth + 1)","642","","643","        else:","644","            xy_parent = ((node.parent.x + .5) * scale_x,","645","                         height - (node.parent.y + .5) * scale_y)","646","            kwargs[\"arrowprops\"] = self.arrow_args","647","            kwargs['bbox']['fc'] = 'grey'","648","            ax.annotate(\"\\n  (...)  \\n\", xy_parent, xy, **kwargs)","649","","650","","672","    decision_tree : decision tree classifier","749","    >>> tree.export_graphviz(clf) # doctest: +ELLIPSIS","750","    'digraph Tree {...","768","        exporter = _DOTTreeExporter(","769","            out_file=out_file, max_depth=max_depth,","770","            feature_names=feature_names, class_names=class_names, label=label,","771","            filled=filled, leaves_parallel=leaves_parallel, impurity=impurity,","772","            node_ids=node_ids, proportion=proportion, rotate=rotate,","773","            rounded=rounded, special_characters=special_characters,","774","            precision=precision)","775","        exporter.export(decision_tree)","778","            return exporter.out_file.getvalue()"],"delete":["95","    decision_tree : decision tree regressor or classifier","172","    >>> tree.export_graphviz(clf,","173","    ...     out_file='tree.dot')                # doctest: +SKIP","174","","177","    def get_color(value):","178","        # Find the appropriate color & intensity for a node","179","        if colors['bounds'] is None:","180","            # Classification tree","181","            color = list(colors['rgb'][np.argmax(value)])","182","            sorted_values = sorted(value, reverse=True)","183","            if len(sorted_values) == 1:","184","                alpha = 0","185","            else:","186","                alpha = int(np.round(255 * (sorted_values[0] -","187","                                            sorted_values[1]) \/","188","                                           (1 - sorted_values[1]), 0))","189","        else:","190","            # Regression tree or multi-output","191","            color = list(colors['rgb'][0])","192","            alpha = int(np.round(255 * ((value - colors['bounds'][0]) \/","193","                                        (colors['bounds'][1] -","194","                                         colors['bounds'][0])), 0))","195","","196","        # Return html color code in #RRGGBBAA format","197","        color.append(alpha)","198","        hex_codes = [str(i) for i in range(10)]","199","        hex_codes.extend(['a', 'b', 'c', 'd', 'e', 'f'])","200","        color = [hex_codes[c \/\/ 16] + hex_codes[c % 16] for c in color]","201","","202","        return '#' + ''.join(color)","203","","204","    def node_to_str(tree, node_id, criterion):","205","        # Generate the node content string","206","        if tree.n_outputs == 1:","207","            value = tree.value[node_id][0, :]","208","        else:","209","            value = tree.value[node_id]","210","","211","        # Should labels be shown?","212","        labels = (label == 'root' and node_id == 0) or label == 'all'","213","","214","        # PostScript compatibility for special characters","215","        if special_characters:","216","            characters = ['&#35;', '<SUB>', '<\/SUB>', '&le;', '<br\/>', '>']","217","            node_string = '<'","218","        else:","219","            characters = ['#', '[', ']', '<=', '\\\\n', '\"']","220","            node_string = '\"'","221","","222","        # Write node ID","223","        if node_ids:","224","            if labels:","225","                node_string += 'node '","226","            node_string += characters[0] + str(node_id) + characters[4]","227","","228","        # Write decision criteria","229","        if tree.children_left[node_id] != _tree.TREE_LEAF:","230","            # Always write node decision criteria, except for leaves","231","            if feature_names is not None:","232","                feature = feature_names[tree.feature[node_id]]","233","            else:","234","                feature = \"X%s%s%s\" % (characters[1],","235","                                       tree.feature[node_id],","236","                                       characters[2])","237","            node_string += '%s %s %s%s' % (feature,","238","                                           characters[3],","239","                                           round(tree.threshold[node_id],","240","                                                 precision),","241","                                           characters[4])","242","","243","        # Write impurity","244","        if impurity:","245","            if isinstance(criterion, _criterion.FriedmanMSE):","246","                criterion = \"friedman_mse\"","247","            elif not isinstance(criterion, six.string_types):","248","                criterion = \"impurity\"","249","            if labels:","250","                node_string += '%s = ' % criterion","251","            node_string += (str(round(tree.impurity[node_id], precision)) +","252","                            characters[4])","253","","254","        # Write node sample count","255","        if labels:","256","            node_string += 'samples = '","257","        if proportion:","258","            percent = (100. * tree.n_node_samples[node_id] \/","259","                       float(tree.n_node_samples[0]))","260","            node_string += (str(round(percent, 1)) + '%' +","261","                            characters[4])","262","        else:","263","            node_string += (str(tree.n_node_samples[node_id]) +","264","                            characters[4])","265","","266","        # Write node class distribution \/ regression value","267","        if proportion and tree.n_classes[0] != 1:","268","            # For classification this will show the proportion of samples","269","            value = value \/ tree.weighted_n_node_samples[node_id]","270","        if labels:","271","            node_string += 'value = '","272","        if tree.n_classes[0] == 1:","273","            # Regression","274","            value_text = np.around(value, precision)","275","        elif proportion:","276","            # Classification","277","            value_text = np.around(value, precision)","278","        elif np.all(np.equal(np.mod(value, 1), 0)):","279","            # Classification without floating-point weights","280","            value_text = value.astype(int)","281","        else:","282","            # Classification with floating-point weights","283","            value_text = np.around(value, precision)","284","        # Strip whitespace","285","        value_text = str(value_text.astype('S32')).replace(\"b'\", \"'\")","286","        value_text = value_text.replace(\"' '\", \", \").replace(\"'\", \"\")","287","        if tree.n_classes[0] == 1 and tree.n_outputs == 1:","288","            value_text = value_text.replace(\"[\", \"\").replace(\"]\", \"\")","289","        value_text = value_text.replace(\"\\n \", characters[4])","290","        node_string += value_text + characters[4]","291","","292","        # Write node majority class","293","        if (class_names is not None and","294","                tree.n_classes[0] != 1 and","295","                tree.n_outputs == 1):","296","            # Only done for single-output classification trees","297","            if labels:","298","                node_string += 'class = '","299","            if class_names is not True:","300","                class_name = class_names[np.argmax(value)]","301","            else:","302","                class_name = \"y%s%s%s\" % (characters[1],","303","                                          np.argmax(value),","304","                                          characters[2])","305","            node_string += class_name","306","","307","        # Clean up any trailing newlines","308","        if node_string[-2:] == '\\\\n':","309","            node_string = node_string[:-2]","310","        if node_string[-5:] == '<br\/>':","311","            node_string = node_string[:-5]","312","","313","        return node_string + characters[5]","314","","315","    def recurse(tree, node_id, criterion, parent=None, depth=0):","316","        if node_id == _tree.TREE_LEAF:","317","            raise ValueError(\"Invalid node_id %s\" % _tree.TREE_LEAF)","318","","319","        left_child = tree.children_left[node_id]","320","        right_child = tree.children_right[node_id]","321","","322","        # Add node with description","323","        if max_depth is None or depth <= max_depth:","324","","325","            # Collect ranks for 'leaf' option in plot_options","326","            if left_child == _tree.TREE_LEAF:","327","                ranks['leaves'].append(str(node_id))","328","            elif str(depth) not in ranks:","329","                ranks[str(depth)] = [str(node_id)]","330","            else:","331","                ranks[str(depth)].append(str(node_id))","332","","333","            out_file.write('%d [label=%s'","334","                           % (node_id,","335","                              node_to_str(tree, node_id, criterion)))","336","","337","            if filled:","338","                # Fetch appropriate color for node","339","                if 'rgb' not in colors:","340","                    # Initialize colors and bounds if required","341","                    colors['rgb'] = _color_brew(tree.n_classes[0])","342","                    if tree.n_outputs != 1:","343","                        # Find max and min impurities for multi-output","344","                        colors['bounds'] = (np.min(-tree.impurity),","345","                                            np.max(-tree.impurity))","346","                    elif (tree.n_classes[0] == 1 and","347","                          len(np.unique(tree.value)) != 1):","348","                        # Find max and min values in leaf nodes for regression","349","                        colors['bounds'] = (np.min(tree.value),","350","                                            np.max(tree.value))","351","                if tree.n_outputs == 1:","352","                    node_val = (tree.value[node_id][0, :] \/","353","                                tree.weighted_n_node_samples[node_id])","354","                    if tree.n_classes[0] == 1:","355","                        # Regression","356","                        node_val = tree.value[node_id][0, :]","357","                else:","358","                    # If multi-output color node by impurity","359","                    node_val = -tree.impurity[node_id]","360","                out_file.write(', fillcolor=\"%s\"' % get_color(node_val))","361","            out_file.write('] ;\\n')","362","","363","            if parent is not None:","364","                # Add edge to parent","365","                out_file.write('%d -> %d' % (parent, node_id))","366","                if parent == 0:","367","                    # Draw True\/False labels if parent is root node","368","                    angles = np.array([45, -45]) * ((rotate - .5) * -2)","369","                    out_file.write(' [labeldistance=2.5, labelangle=')","370","                    if node_id == 1:","371","                        out_file.write('%d, headlabel=\"True\"]' % angles[0])","372","                    else:","373","                        out_file.write('%d, headlabel=\"False\"]' % angles[1])","374","                out_file.write(' ;\\n')","375","","376","            if left_child != _tree.TREE_LEAF:","377","                recurse(tree, left_child, criterion=criterion, parent=node_id,","378","                        depth=depth + 1)","379","                recurse(tree, right_child, criterion=criterion, parent=node_id,","380","                        depth=depth + 1)","381","","382","        else:","383","            ranks['leaves'].append(str(node_id))","384","","385","            out_file.write('%d [label=\"(...)\"' % node_id)","386","            if filled:","387","                # color cropped nodes grey","388","                out_file.write(', fillcolor=\"#C0C0C0\"')","389","            out_file.write('] ;\\n' % node_id)","390","","391","            if parent is not None:","392","                # Add edge to parent","393","                out_file.write('%d -> %d ;\\n' % (parent, node_id))","394","","410","        if isinstance(precision, Integral):","411","            if precision < 0:","412","                raise ValueError(\"'precision' should be greater or equal to 0.\"","413","                                 \" Got {} instead.\".format(precision))","414","        else:","415","            raise ValueError(\"'precision' should be an integer. Got {}\"","416","                             \" instead.\".format(type(precision)))","417","","418","        # Check length of feature_names before getting into the tree node","419","        # Raise error if length of feature_names does not match","420","        # n_features_ in the decision_tree","421","        if feature_names is not None:","422","            if len(feature_names) != decision_tree.n_features_:","423","                raise ValueError(\"Length of feature_names, %d \"","424","                                 \"does not match number of features, %d\"","425","                                 % (len(feature_names),","426","                                    decision_tree.n_features_))","427","","428","        # The depth of each node for plotting with 'leaf' option","429","        ranks = {'leaves': []}","430","        # The colors to render each node with","431","        colors = {'bounds': None}","432","","433","        out_file.write('digraph Tree {\\n')","434","","435","        # Specify node aesthetics","436","        out_file.write('node [shape=box')","437","        rounded_filled = []","438","        if filled:","439","            rounded_filled.append('filled')","440","        if rounded:","441","            rounded_filled.append('rounded')","442","        if len(rounded_filled) > 0:","443","            out_file.write(', style=\"%s\", color=\"black\"'","444","                           % \", \".join(rounded_filled))","445","        if rounded:","446","            out_file.write(', fontname=helvetica')","447","        out_file.write('] ;\\n')","448","","449","        # Specify graph & edge aesthetics","450","        if leaves_parallel:","451","            out_file.write('graph [ranksep=equally, splines=polyline] ;\\n')","452","        if rounded:","453","            out_file.write('edge [fontname=helvetica] ;\\n')","454","        if rotate:","455","            out_file.write('rankdir=LR ;\\n')","456","","457","        # Now recurse the tree and add node & edge attributes","458","        recurse(decision_tree.tree_, 0, criterion=decision_tree.criterion)","459","","460","        # If required, draw leaf nodes at same depth as each other","461","        if leaves_parallel:","462","            for rank in sorted(ranks):","463","                out_file.write(\"{rank=same ; \" +","464","                               \"; \".join(r for r in ranks[rank]) + \"} ;\\n\")","465","        out_file.write(\"}\")","468","            return out_file.getvalue()"]}],"sklearn\/tree\/tests\/test_reingold_tilford.py":[{"add":[],"delete":[]}],"examples\/tree\/plot_iris.py":[{"add":["13","","14","We also show the tree structure of a model built on all of the features.","22","from sklearn.tree import DecisionTreeClassifier, plot_tree","66","","67","plt.figure()","68","clf = DecisionTreeClassifier().fit(iris.data, iris.target)","69","plot_tree(clf, filled=True)"],"delete":["20","from sklearn.tree import DecisionTreeClassifier"]}],"doc\/modules\/tree.rst":[{"add":["126","Once trained, you can plot the tree with the plot_tree function::","127","","128","","129","    >>> tree.plot_tree(clf.fit(iris.data, iris.target)) # doctest: +SKIP","130","","131",".. figure:: ..\/auto_examples\/tree\/images\/sphx_glr_plot_iris_002.png","132","   :target: ..\/auto_examples\/tree\/plot_iris.html","133","   :scale: 75","134","   :align: center","135","","136","We can also export the tree in `Graphviz","138","exporter. If you use the `conda <https:\/\/conda.io>`_ package manager, the graphviz binaries  ","139",""],"delete":["126","Once trained, we can export the tree in `Graphviz","128","exporter. If you use the `conda <https:\/\/conda.io\/>`_ package manager, the graphviz binaries  "]}]}},"0bfe10d1fa0e2bc275a2539f08ad5cf57bf5e4d7":{"changes":{"sklearn\/ensemble\/voting_classifier.py":"MODIFY","sklearn\/ensemble\/tests\/test_voting_classifier.py":"MODIFY"},"diff":{"sklearn\/ensemble\/voting_classifier.py":[{"add":["25","def _parallel_fit_estimator(estimator, X, y, sample_weight=None):","28","        estimator.fit(X, y, sample_weight=sample_weight)","187","                                                 sample_weight=sample_weight)"],"delete":["25","def _parallel_fit_estimator(estimator, X, y, sample_weight):","28","        estimator.fit(X, y, sample_weight)","187","                                                 sample_weight)"]}],"sklearn\/ensemble\/tests\/test_voting_classifier.py":[{"add":["19","from sklearn.base import BaseEstimator, ClassifierMixin","277","def test_sample_weight_kwargs():","278","    \"\"\"Check that VotingClassifier passes sample_weight as kwargs\"\"\"","279","    class MockClassifier(BaseEstimator, ClassifierMixin):","280","        \"\"\"Mock Classifier to check that sample_weight is received as kwargs\"\"\"","281","        def fit(self, X, y, *args, **sample_weight):","282","            assert_true('sample_weight' in sample_weight)","283","","284","    clf = MockClassifier()","285","    eclf = VotingClassifier(estimators=[('mock', clf)], voting='soft')","286","","287","    # Should not raise an error.","288","    eclf.fit(X, y, sample_weight=np.ones((len(y),)))","289","","290",""],"delete":[]}]}},"502261bd9deca6ba57b4a051da2b37f170f28e88":{"changes":{"build_tools\/circle\/build_doc.sh":"MODIFY","examples\/neural_networks\/plot_mlp_training_curves.py":"MODIFY"},"diff":{"build_tools\/circle\/build_doc.sh":[{"add":["109","  cython nose coverage matplotlib sphinx=1.6.2 pillow"],"delete":["109","  cython nose coverage 'matplotlib=2.0.*|>2.1.0' sphinx=1.6.2 pillow"]}],"examples\/neural_networks\/plot_mlp_training_curves.py":[{"add":["87","fig.legend(ax.get_lines(), labels, ncol=3, loc=\"upper center\")"],"delete":["87","fig.legend(ax.get_lines(), labels=labels, ncol=3, loc=\"upper center\")"]}]}},"055d17b30953daa7e2ed3819786828ce9da36bfd":{"changes":{"sklearn\/linear_model\/sag_fast.pyx":"MODIFY"},"diff":{"sklearn\/linear_model\/sag_fast.pyx":[{"add":["616","                        if lagged_ind > 0:","617","                            grad_step = (cumulative_sums[lagged_ind]","618","                               - cumulative_sums[lagged_ind - 1])","619","                            prox_step = (cumulative_sums_prox[lagged_ind]","620","                               - cumulative_sums_prox[lagged_ind - 1])","621","                        else:","622","                            grad_step = cumulative_sums[lagged_ind]","623","                            prox_step = cumulative_sums_prox[lagged_ind]"],"delete":["616","                        grad_step = (cumulative_sums[lagged_ind]","617","                           - cumulative_sums[lagged_ind - 1])","618","                        prox_step = (cumulative_sums_prox[lagged_ind]","619","                           - cumulative_sums_prox[lagged_ind - 1])"]}]}},"c71e1275ea78a113d26843beeb2cfb21749fe06f":{"changes":{"doc\/modules\/multiclass.rst":"MODIFY","sklearn\/multioutput.py":"MODIFY","doc\/whats_new.rst":"MODIFY","doc\/modules\/model_evaluation.rst":"MODIFY"},"diff":{"doc\/modules\/multiclass.rst":[{"add":["355","of exploiting correlations among targets.","375","","377","        \"Classifier Chains for Multi-label Classification\", 2009."],"delete":["355"," of exploiting correlations among targets.","376","        \"Classifier Chains for Multi-label Classification\", 2009."]}],"sklearn\/multioutput.py":[{"add":["309","    estimators_ : list of ``n_output`` estimators","420","        A list of arrays of length ``len(estimators_)`` containing the","456","        X, Y = check_X_y(X, Y, multi_output=True, accept_sparse=True)"],"delete":["16","from abc import ABCMeta","17","","311","    estimators_ : list of `n_output` estimators","422","        A list of arrays of length len(estimators_) containing the","458","        X, Y = check_X_y(X, Y,  multi_output=True, accept_sparse=True)"]}],"doc\/whats_new.rst":[{"add":["4352","   - Randomized sparse linear models for feature","4813","     example gallery by `Fabian Pedregosa`_."],"delete":["4352","   - :ref:`randomized_l1`: Randomized sparse linear models for feature","4813","     :ref:`example gallery <examples-index>` by `Fabian Pedregosa`_."]}],"doc\/modules\/model_evaluation.rst":[{"add":[],"delete":["672","  * See :ref:`sphx_glr_auto_examples_linear_model_plot_sparse_recovery.py`","673","    for an example of :func:`precision_recall_curve` usage to select","674","    features for sparse linear models.","675",""]}]}},"d074e403f11a357d66a967e8469a397a44af83d3":{"changes":{"sklearn\/model_selection\/_split.py":"MODIFY","sklearn\/model_selection\/tests\/test_split.py":"MODIFY"},"diff":{"sklearn\/model_selection\/_split.py":[{"add":["1536","            # for multi-label y, map each distinct row to a string repr","1537","            # using join because str(row) uses an ellipsis if len(row) > 1000","1538","            y = np.array([' '.join(row.astype('str')) for row in y])"],"delete":["1536","            # for multi-label y, map each distinct row to its string repr:","1537","            y = np.array([str(row) for row in y])"]}],"sklearn\/model_selection\/tests\/test_split.py":[{"add":["728","def test_stratified_shuffle_split_multilabel_many_labels():","729","    # fix in PR #9922: for multilabel data with > 1000 labels, str(row)","730","    # truncates with an ellipsis for elements in positions 4 through","731","    # len(row) - 4, so labels were not being correctly split using the powerset","732","    # method for transforming a multilabel problem to a multiclass one; this","733","    # test checks that this problem is fixed.","734","    row_with_many_zeros = [1, 0, 1] + [0] * 1000 + [1, 0, 1]","735","    row_with_many_ones = [1, 0, 1] + [1] * 1000 + [1, 0, 1]","736","    y = np.array([row_with_many_zeros] * 10 + [row_with_many_ones] * 100)","737","    X = np.ones_like(y)","738","","739","    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=0)","740","    train, test = next(sss.split(X=X, y=y))","741","    y_train = y[train]","742","    y_test = y[test]","743","","744","    # correct stratification of entire rows","745","    # (by design, here y[:, 4] uniquely determines the entire row of y)","746","    expected_ratio = np.mean(y[:, 4])","747","    assert_equal(expected_ratio, np.mean(y_train[:, 4]))","748","    assert_equal(expected_ratio, np.mean(y_test[:, 4]))","749","","750",""],"delete":[]}]}},"7dffa20363d095320ede354edf66f8b6123ba121":{"changes":{"doc\/themes\/scikit-learn\/layout.html":"MODIFY","CONTRIBUTING.md":"MODIFY","sklearn\/linear_model\/tests\/test_bayes.py":"MODIFY","doc\/faq.rst":"MODIFY","doc\/developers\/contributing.rst":"MODIFY","doc\/sphinxext\/sphinx_issues.py":"MODIFY"},"diff":{"doc\/themes\/scikit-learn\/layout.html":[{"add":["205","<!-- GitHub \"fork me\" ribbon -->"],"delete":["205","<!-- Github \"fork me\" ribbon -->"]}],"CONTRIBUTING.md":[{"add":["162","We use GitHub issues to track all bugs and feature requests; feel free to"],"delete":["162","We use Github issues to track all bugs and feature requests; feel free to"]}],"sklearn\/linear_model\/tests\/test_bayes.py":[{"add":["38","    # Test correctness of lambda_ and alpha_ parameters (GitHub issue #8224)"],"delete":["38","    # Test correctness of lambda_ and alpha_ parameters (Github issue #8224)"]}],"doc\/faq.rst":[{"add":["57","`issue tracker on GitHub <https:\/\/github.com\/scikit-learn\/scikit-learn\/issues>`_."],"delete":["57","`issue tracker on Github <https:\/\/github.com\/scikit-learn\/scikit-learn\/issues>`_."]}],"doc\/developers\/contributing.rst":[{"add":["312","We use GitHub issues to track all bugs and feature requests; feel free to","463","   versions of Sphinx as possible, the different versions tend to","513","`GitHub issue tracker <https:\/\/github.com\/scikit-learn\/scikit-learn\/issues>`_"],"delete":["312","We use Github issues to track all bugs and feature requests; feel free to","463","   version of Sphinx as possible, the different versions tend to","513","`Github issue tracker <https:\/\/github.com\/scikit-learn\/scikit-learn\/issues>`_"]}],"doc\/sphinxext\/sphinx_issues.py":[{"add":["35","    GitHub profiles, but the profile URIS can be configured via the","106","    # Shortcut for GitHub, e.g. 'sloria\/marshmallow'"],"delete":["35","    Github profiles, but the profile URIS can be configured via the","106","    # Shortcut for Github, e.g. 'sloria\/marshmallow'"]}]}},"a01443b70d29301d254f05c42baa1fe04abde642":{"changes":{"sklearn\/_isotonic.pyx":"MODIFY","doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/tests\/test_isotonic.py":"MODIFY"},"diff":{"sklearn\/_isotonic.pyx":[{"add":["102","            weights_out[i] = current_weight","115","    weights_out[i] = current_weight"],"delete":["102","            weights_out[i] = current_weight \/ current_count","115","    weights_out[i] = current_weight \/ current_count"]}],"doc\/whats_new\/v0.20.rst":[{"add":["18","- :class:`isotonic.IsotonicRegression` (bug fix)","71","Classifiers and regressors","72","","73","- Fixed a bug in :class:`isotonic.IsotonicRegression` which incorrectly","74","  combined weights when fitting a model to data involving points with","75","  identical X values.","76","  :issue:`9432` by :user:`Dallas Card <dallascard>`","77",""],"delete":[]}],"sklearn\/tests\/test_isotonic.py":[{"add":["168","def test_isotonic_regression_with_ties_in_differently_sized_groups():","169","    \"\"\"","170","    Non-regression test to handle issue 9432:","171","    https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/9432","172","","173","    Compare against output in R:","174","    > library(\"isotone\")","175","    > x <- c(0, 1, 1, 2, 3, 4)","176","    > y <- c(0, 0, 1, 0, 0, 1)","177","    > res1 <- gpava(x, y, ties=\"secondary\")","178","    > res1$x","179","","180","    `isotone` version: 1.1-0, 2015-07-24","181","    R version: R version 3.3.2 (2016-10-31)","182","    \"\"\"","183","    x = np.array([0, 1, 1, 2, 3, 4])","184","    y = np.array([0, 0, 1, 0, 0, 1])","185","    y_true = np.array([0., 0.25, 0.25, 0.25, 0.25, 1.])","186","    ir = IsotonicRegression()","187","    ir.fit(x, y)","188","    assert_array_almost_equal(ir.transform(x), y_true)","189","    assert_array_almost_equal(ir.fit_transform(x, y), y_true)","190","","191",""],"delete":[]}]}},"511bbc7237012513c19ee9a8c7420eba0f59338c":{"changes":{"examples\/multioutput\/plot_classifier_chain_yeast.py":"MODIFY"},"diff":{"examples\/multioutput\/plot_classifier_chain_yeast.py":[{"add":["7","<http:\/\/mldata.org\/repository\/data\/viewslug\/yeast>`_ dataset which contains","8","2417 datapoints each with 103 features and 14 possible labels. Each","9","data point has at least one label. As a baseline we first train a logistic","10","regression classifier for each of the 14 labels. To evaluate the performance of","11","these classifiers we predict on a held-out test set and calculate the","12",":ref:`jaccard similarity score <jaccard_similarity_score>`.","81","model_names = ('Independent',","92","               'Ensemble')","94","x_pos = np.arange(len(model_names))","100","fig, ax = plt.subplots(figsize=(7, 4))","101","ax.grid(True)","102","ax.set_title('Classifier Chain Ensemble Performance Comparison')","103","ax.set_xticks(x_pos)","104","ax.set_xticklabels(model_names, rotation='vertical')","105","ax.set_ylabel('Jaccard Similarity Score')","106","ax.set_ylim([min(model_scores) * .9, max(model_scores) * 1.1])","108","ax.bar(x_pos, model_scores, alpha=0.5, color=colors)","109","plt.tight_layout()"],"delete":["7","<http:\/\/mldata.org\/repository\/data\/viewslug\/yeast>`_ dataset which","8","contains 2417 datapoints each with 103 features and 14 possible labels. Each","9","datapoint has at least one label. As a baseline we first train a logistic","10","regression classifier for each of the 14 labels. To evaluate the performance","11","of these classifiers we predict on a held-out test set and calculate the","12",":ref:`User Guide <jaccard_similarity_score>`.","81","model_names = ('Independent Models',","92","               'Ensemble Average')","94","y_pos = np.arange(len(model_names))","95","y_pos[1:] += 1","96","y_pos[-1] += 1","102","fig = plt.figure(figsize=(7, 4))","103","plt.title('Classifier Chain Ensemble')","104","plt.xticks(y_pos, model_names, rotation='vertical')","105","plt.ylabel('Jaccard Similarity Score')","106","plt.ylim([min(model_scores) * .9, max(model_scores) * 1.1])","108","plt.bar(y_pos, model_scores, align='center', alpha=0.5, color=colors)"]}]}},"9d515e960afee497183e85d5763be782c9276625":{"changes":{"sklearn\/datasets\/samples_generator.py":"MODIFY","sklearn\/datasets\/tests\/test_samples_generator.py":"MODIFY"},"diff":{"sklearn\/datasets\/samples_generator.py":[{"add":["164","        weights = weights + [1.0 - sum(weights)]"],"delete":["164","        weights.append(1.0 - sum(weights))"]}],"sklearn\/datasets\/tests\/test_samples_generator.py":[{"add":["39","    weights = [0.1, 0.25]","43","                               shift=None, scale=None, weights=weights,","46","    assert_equal(weights, [0.1, 0.25])","182","","193",""],"delete":["42","                               shift=None, scale=None, weights=[0.1, 0.25],"]}]}},"6ef8595d6c2fec62395850db6b9655b729ca7ebc":{"changes":{"doc\/modules\/clustering.rst":"MODIFY"},"diff":{"doc\/modules\/clustering.rst":[{"add":["193","(use the ``init='k-means++'`` parameter). This initializes the centroids to be"],"delete":["193","(use the ``init='kmeans++'`` parameter). This initializes the centroids to be"]}]}},"030f8b21f18b6078d2a12845f7fd0a93e1b9d4de":{"changes":{"sklearn\/tests\/test_docstring_parameters.py":"MODIFY","sklearn\/linear_model\/tests\/test_bayes.py":"MODIFY","sklearn\/utils\/testing.py":"MODIFY",".travis.yml":"MODIFY","build_tools\/travis\/test_script.sh":"MODIFY","sklearn\/utils\/estimator_checks.py":"MODIFY"},"diff":{"sklearn\/tests\/test_docstring_parameters.py":[{"add":["10","from inspect import getsource, isabstract","20","PUBLIC_MODULES = set([pckg[1] for pckg in walk_packages(prefix='sklearn.',","21","                                                        path=sklearn.__path__)","22","                      if not (\"._\" in pckg[1] or \".tests.\" in pckg[1])])","26","IGNORED_MODULES = (","27","    'cross_decomposition',","28","    'covariance',","29","    'cluster',","30","    'datasets',","31","    'decomposition',","32","    'feature_extraction',","33","    'gaussian_process',","34","    'linear_model',","35","    'manifold',","36","    'metrics',","37","    'discriminant_analysis',","38","    'ensemble',","39","    'feature_selection',","40","    'kernel_approximation',","41","    'model_selection',","42","    'multioutput',","43","    'random_projection',","44","    'setup',","45","    'svm',","46","    'utils',","47","    'neighbors'","49","    'cross_validation',","50","    'grid_search',","51","    'learning_curve',","52",")","53","","89","        if name.startswith('_') or name.split(\".\")[1] in IGNORED_MODULES:","90","            continue","94","        # Exclude imported classes","95","        classes = [cls for cls in classes if cls[1].__module__ == name]","98","            if cname in _DOCSTRING_IGNORES or cname.startswith('_'):","100","            if isabstract(cls):","135","        # Exclude imported functions","136","        functions = [fn for fn in functions if fn[1].__module__ == name]","141","            if fname == \"configuration\" and name.endswith(\"setup\"):","142","                continue"],"delete":["4","from __future__ import print_function","5","","12","from inspect import getsource","22","PUBLIC_MODULES = set(['sklearn.' + modname","23","                      for _, modname, _ in walk_packages(sklearn.__path__)","24","                      if not modname.startswith('_') and","25","                      '.tests.' not in modname])","29","PUBLIC_MODULES -= set([","30","    'sklearn.ensemble',","31","    'sklearn.feature_selection',","32","    'sklearn.kernel_approximation',","33","    'sklearn.model_selection',","34","    'sklearn.multioutput',","35","    'sklearn.random_projection',","36","    'sklearn.setup',","37","    'sklearn.svm',","38","    'sklearn.utils',","40","    'sklearn.cross_validation',","41","    'sklearn.grid_search',","42","    'sklearn.learning_curve',","43","])","84","            if cname in _DOCSTRING_IGNORES:","86","            if cname.startswith('_'):"]}],"sklearn\/linear_model\/tests\/test_bayes.py":[{"add":["18","    raise SkipTest(\"test_bayesian_on_diabetes is broken\")"],"delete":["18","    raise SkipTest(\"XFailed Test\")"]}],"sklearn\/utils\/testing.py":[{"add":["883","            if (':' not in param_name or","884","                    param_name[len(param_name.split(':')[0].strip())] == ':'):"],"delete":["883","            if param_name[len(param_name.split(':')[0].strip())] == ':':"]}],".travis.yml":[{"add":["47","           TEST_DOCSTRINGS=\"true\""],"delete":["51","           TEST_DOCSTRINGS=\"true\""]}],"build_tools\/travis\/test_script.sh":[{"add":["24","=\"pytest --showlocals --durations=1 --pyargs -rs\""],"delete":[]}],"sklearn\/utils\/estimator_checks.py":[{"add":["1353","        raise SkipTest(\"Not testing NuSVC class weight as it is ignored.\")","1536","        raise SkipTest(\"Skipping check_estimators_data_not_an_array \"","1537","                       \"for cross decomposition module as estimators \"","1538","                       \"are not deterministic.\")"],"delete":["1353","        raise SkipTest","1536","        raise SkipTest"]}]}},"a324a61eeace54947b9253de43e6f9f039e26185":{"changes":{"sklearn\/model_selection\/_split.py":"MODIFY"},"diff":{"sklearn\/model_selection\/_split.py":[{"add":["1708","    Provides train\/test indices to split data into train\/test sets using a","1709","    predefined scheme specified by the user with the ``test_fold`` parameter.","1713","    Parameters","1714","    ----------","1715","    test_fold : array-like, shape (n_samples,)","1716","        The entry ``test_fold[i]`` represents the index of the test set that","1717","        sample ``i`` belongs to. It is possible to exclude sample ``i`` from","1718","        any test set (i.e. include sample ``i`` in every training set) by","1719","        setting ``test_fold[i]`` equal to -1.","1720",""],"delete":["1708","    Splits the data into training\/test set folds according to a predefined","1709","    scheme. Each sample can be assigned to at most one test set fold, as","1710","    specified by the user through the ``test_fold`` parameter."]}]}},"4aaf45baf1cddbd368ab39801f4de40a6caaeece":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/manifold\/tests\/test_t_sne.py":"MODIFY","sklearn\/manifold\/t_sne.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["164","- :func:`manifold.t_sne.trustworthiness` accepts metrics other than","165","  Euclidean. :issue:`9775` by :user:`William de Vazelhes <wdevazelhes>`.","166","","241","Decomposition, manifold learning and clustering","242","","243","- Deprecate ``precomputed`` parameter in function","244","  :func:`manifold.t_sne.trustworthiness`. Instead, the new parameter","245","  ``metric`` should be used with any compatible metric including","246","  'precomputed', in which case the input matrix ``X`` should be a matrix of","247","  pairwise distances or squared distances. :issue:`9775` by","248","  :user:`William de Vazelhes <wdevazelhes>`.","249","","492","Decomposition, manifold learning and clustering","493","","494","- Deprecate ``precomputed`` parameter in function","495","  :func:`manifold.t_sne.trustworthiness`. Instead, the new parameter","496","  ``metric`` should be used with any compatible metric including","497","  'precomputed', in which case the input matrix ``X`` should be a matrix of","498","  pairwise distances or squared distances. :issue:`9775` by","499","  :user:`William de Vazelhes <wdevazelhes>`.","500",""],"delete":[]}],"sklearn\/manifold\/tests\/test_t_sne.py":[{"add":["16","from sklearn.utils.testing import assert_warns","17","from sklearn.utils.testing import assert_raises","292","        t = trustworthiness(D, X_embedded, n_neighbors=1, metric=\"precomputed\")","296","def test_trustworthiness_precomputed_deprecation():","297","    # FIXME: Remove this test in v0.23","298","","299","    # Use of the flag `precomputed` in trustworthiness parameters has been","300","    # deprecated, but will still work until v0.23.","301","    random_state = check_random_state(0)","302","    X = random_state.randn(100, 2)","303","    assert_equal(assert_warns(DeprecationWarning, trustworthiness,","304","                              pairwise_distances(X), X, precomputed=True), 1.)","305","    assert_equal(assert_warns(DeprecationWarning, trustworthiness,","306","                              pairwise_distances(X), X, metric='precomputed',","307","                              precomputed=True), 1.)","308","    assert_raises(ValueError, assert_warns, DeprecationWarning,","309","                  trustworthiness, X, X, metric='euclidean', precomputed=True)","310","    assert_equal(assert_warns(DeprecationWarning, trustworthiness,","311","                              pairwise_distances(X), X, metric='euclidean',","312","                              precomputed=True), 1.)","313","","314","","315","def test_trustworthiness_not_euclidean_metric():","316","    # Test trustworthiness with a metric different from 'euclidean' and","317","    # 'precomputed'","318","    random_state = check_random_state(0)","319","    X = random_state.randn(100, 2)","320","    assert_equal(trustworthiness(X, X, metric='cosine'),","321","                 trustworthiness(pairwise_distances(X, metric='cosine'), X,","322","                                 metric='precomputed'))","323","","324",""],"delete":["290","        t = trustworthiness(D, X_embedded, n_neighbors=1,","291","                            precomputed=True)"]}],"sklearn\/manifold\/t_sne.py":[{"add":["11","import warnings","397","def trustworthiness(X, X_embedded, n_neighbors=5,","398","                    precomputed=False, metric='euclidean'):","435","        ..deprecated:: 0.20","436","            ``precomputed`` has been deprecated in version 0.20 and will be","437","            removed in version 0.22. Use ``metric`` instead.","438","","439","    metric : string, or callable, optional, default 'euclidean'","440","        Which metric to use for computing pairwise distances between samples","441","        from the original input space. If metric is 'precomputed', X must be a","442","        matrix of pairwise distances or squared distances. Otherwise, see the","443","        documentation of argument metric in sklearn.pairwise.pairwise_distances","444","        for a list of available metrics.","445","","452","        warnings.warn(\"The flag 'precomputed' has been deprecated in version \"","453","                      \"0.20 and will be removed in 0.22. See 'metric' \"","454","                      \"parameter instead.\", DeprecationWarning)","455","        metric = 'precomputed'","456","    dist_X = pairwise_distances(X, metric=metric)"],"delete":["396","def trustworthiness(X, X_embedded, n_neighbors=5, precomputed=False):","439","        dist_X = X","440","    else:","441","        dist_X = pairwise_distances(X, squared=True)"]}]}},"823382995c046d98a4d892fff1f564c0f04ba340":{"changes":{"examples\/cluster\/plot_agglomerative_clustering.py":"MODIFY","doc\/modules\/clustering.rst":"MODIFY","doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/cluster\/_hierarchical.pyx":"MODIFY","sklearn\/cluster\/hierarchical.py":"MODIFY","examples\/cluster\/plot_linkage_comparison.py":"ADD","examples\/cluster\/plot_digits_linkage.py":"MODIFY","sklearn\/cluster\/tests\/test_hierarchical.py":"MODIFY"},"diff":{"examples\/cluster\/plot_agglomerative_clustering.py":[{"add":["11","Second, when using a connectivity matrix, single, average and complete","12","linkage are unstable and tend to create a few clusters that grow very","13","quickly. Indeed, average and complete linkage fight this percolation behavior","14","by considering all the distances between two clusters when merging them (","15","while single linkage exaggerates the behaviour by considering only the","16","shortest distance between clusters). The connectivity graph breaks this","17","mechanism for average and complete linkage, making them resemble the more","18","brittle single linkage. This effect is more pronounced for very sparse graphs","19","(try decreasing the number of neighbors in kneighbors_graph) and with","20","complete linkage. In particular, having a very small number of neighbors in","21","the graph, imposes a geometry that is close to that of single linkage,","22","which is well known to have this percolation instability. \"\"\"","55","        for index, linkage in enumerate(('average',","56","                                         'complete',","57","                                         'ward',","58","                                         'single')):","59","            plt.subplot(1, 4, index + 1)","68","            plt.title('linkage=%s\\n(time %.2fs)' % (linkage, elapsed_time),"],"delete":["11","Second, when using a connectivity matrix, average and complete linkage are","12","unstable and tend to create a few clusters that grow very quickly. Indeed,","13","average and complete linkage fight this percolation behavior by considering all","14","the distances between two clusters when merging them. The connectivity","15","graph breaks this mechanism. This effect is more pronounced for very","16","sparse graphs (try decreasing the number of neighbors in","17","kneighbors_graph) and with complete linkage. In particular, having a very","18","small number of neighbors in the graph, imposes a geometry that is","19","close to that of single linkage, which is well known to have this","20","percolation instability.","21","\"\"\"","54","        for index, linkage in enumerate(('average', 'complete', 'ward')):","55","            plt.subplot(1, 3, index + 1)","64","            plt.title('linkage=%s (time %.2fs)' % (linkage, elapsed_time),"]}],"doc\/modules\/clustering.rst":[{"add":["569","Different linkage type: Ward, complete, average, and single linkage","570","-----------------------------------------------------------------)-","572",":class:`AgglomerativeClustering` supports Ward, single, average, and complete","575",".. image:: ..\/auto_examples\/cluster\/images\/sphx_glr_plot_linkage_comparison_001.png","576","    :target: ..\/auto_examples\/cluster\/plot_linkage_comparison.html","580","uneven cluster sizes. In this regard, single linkage is the worst","583","Euclidean metrics, average linkage is a good alternative. Single linkage,","584","while not robust to noisy data, can be computed very efficiently and can","585","therefore be useful to provide hierarchical clustering of larger datasets.","586","Single linkage can also perform well on non-globular data.","648",".. warning:: **Connectivity constraints with single, average and complete linkage**","650","    Connectivity constraints and single, complete or average linkage can enhance","657","    Single linkage is the most brittle linkage option with regard to this issue.","679","Single, average and complete linkage can be used with a variety of distances (or"],"delete":["569","Different linkage type: Ward, complete and average linkage","570","-----------------------------------------------------------","572",":class:`AgglomerativeClustering` supports Ward, average, and complete","575",".. image:: ..\/auto_examples\/cluster\/images\/sphx_glr_plot_digits_linkage_001.png","576","    :target: ..\/auto_examples\/cluster\/plot_digits_linkage.html","579",".. image:: ..\/auto_examples\/cluster\/images\/sphx_glr_plot_digits_linkage_002.png","580","    :target: ..\/auto_examples\/cluster\/plot_digits_linkage.html","581","    :scale: 43","582","","583",".. image:: ..\/auto_examples\/cluster\/images\/sphx_glr_plot_digits_linkage_003.png","584","    :target: ..\/auto_examples\/cluster\/plot_digits_linkage.html","585","    :scale: 43","586","","587","","589","uneven cluster sizes. In this regard, complete linkage is the worst","592","Euclidean metrics, average linkage is a good alternative.","654",".. warning:: **Connectivity constraints with average and complete linkage**","656","    Connectivity constraints and complete or average linkage can enhance","684","Average and complete linkage can be used with a variety of distances (or"]}],"doc\/whats_new\/v0.20.rst":[{"add":["83","Clustering","84","","85","- :class:`cluster.AgglomerativeClustering` now supports Single Linkage","86","  clustering via ``linkage='single'``. :issue:`9372` by","87","  :user:`Leland McInnes <lmcinnes>` and :user:`Steve Astels <sastels>`.","88",""],"delete":["80","","84","- Added the :class:`preprocessing.TransformedTargetRegressor` which transforms","85","  the target y before fitting a regression model. The predictions are mapped","86","  back to the original space via an inverse transform. :issue:`9041` by","87","  `Andreas M¨¹ller`_ and :user:`Guillaume Lemaitre <glemaitre>`."]}],"sklearn\/cluster\/_hierarchical.pyx":[{"add":["334","","335","################################################################################","336","# Efficient labelling\/conversion of MSTs to single linkage hierarchies","337","","338","cdef class UnionFind(object):","339","","340","    cdef ITYPE_t next_label","341","    cdef ITYPE_t[:] parent","342","    cdef ITYPE_t[:] size","343","","344","    def __init__(self, N):","345","        self.parent = -1 * np.ones(2 * N - 1, dtype=ITYPE, order='C')","346","        self.next_label = N","347","        self.size = np.hstack((np.ones(N, dtype=ITYPE),","348","                               np.zeros(N - 1, dtype=ITYPE)))","349","","350","    @cython.boundscheck(False)","351","    @cython.nonecheck(False)","352","    cdef void union(self, ITYPE_t m, ITYPE_t n):","353","        self.parent[m] = self.next_label","354","        self.parent[n] = self.next_label","355","        self.size[self.next_label] = self.size[m] + self.size[n]","356","        self.next_label += 1","357","","358","        return","359","","360","    @cython.boundscheck(False)","361","    @cython.nonecheck(False)","362","    cdef ITYPE_t fast_find(self, ITYPE_t n):","363","        cdef ITYPE_t p","364","        p = n","365","        # find the highest node in the linkage graph so far","366","        while self.parent[n] != -1:","367","            n = self.parent[n]","368","        # provide a shortcut up to the highest node","369","        while self.parent[p] != n:","370","            p, self.parent[p] = self.parent[p], n","371","        return n","372","","373","@cython.boundscheck(False)","374","@cython.nonecheck(False)","375","cpdef np.ndarray[DTYPE_t, ndim=2] _single_linkage_label(","376","    np.ndarray[DTYPE_t, ndim=2] L):","377","    \"\"\"","378","    Convert an linkage array or MST to a tree by labelling clusters at merges.","379","    This is done by using a Union find structure to keep track of merges","380","    efficiently. This is the private version of the function that assumes that","381","    ``L`` has been properly validated. See ``single_linkage_label`` for the","382","    user facing version of this function.","383","","384","    Parameters","385","    ----------","386","    L: array of shape (n_samples - 1, 3)","387","        The linkage array or MST where each row specifies two samples","388","        to be merged and a distance or weight at which the merge occurs. This","389","         array is assumed to be sorted by the distance\/weight.","390","","391","    Returns","392","    -------","393","    A tree in the format used by scipy.cluster.hierarchy.","394","    \"\"\"","395","","396","    cdef np.ndarray[DTYPE_t, ndim=2] result_arr","397","    cdef DTYPE_t[:, ::1] result","398","","399","    cdef ITYPE_t left, left_cluster, right, right_cluster, index","400","    cdef DTYPE_t delta","401","","402","    result_arr = np.zeros((L.shape[0], 4), dtype=DTYPE)","403","    result = result_arr","404","    U = UnionFind(L.shape[0] + 1)","405","","406","    for index in range(L.shape[0]):","407","","408","        left = <ITYPE_t> L[index, 0]","409","        right = <ITYPE_t> L[index, 1]","410","        delta = L[index, 2]","411","","412","        left_cluster = U.fast_find(left)","413","        right_cluster = U.fast_find(right)","414","","415","        result[index][0] = left_cluster","416","        result[index][1] = right_cluster","417","        result[index][2] = delta","418","        result[index][3] = U.size[left_cluster] + U.size[right_cluster]","419","","420","        U.union(left_cluster, right_cluster)","421","","422","    return result_arr","423","","424","","425","def single_linkage_label(L):","426","    \"\"\"","427","    Convert an linkage array or MST to a tree by labelling clusters at merges.","428","    This is done by using a Union find structure to keep track of merges","429","    efficiently.","430","","431","    Parameters","432","    ----------","433","    L: array of shape (n_samples - 1, 3)","434","        The linkage array or MST where each row specifies two samples","435","        to be merged and a distance or weight at which the merge occurs. This","436","         array is assumed to be sorted by the distance\/weight.","437","","438","    Returns","439","    -------","440","    A tree in the format used by scipy.cluster.hierarchy.","441","    \"\"\"","442","    # Validate L","443","    if L[:, :2].min() < 0 or L[:, :2].max() >= 2 * L.shape[0] + 1:","444","        raise ValueError(\"Input MST array is not a validly formatted MST array\")","445","","446","    is_sorted = lambda x: np.all(x[:-1] <= x[1:])","447","    if not is_sorted(L[:, 2]):","448","        raise ValueError(\"Input MST array must be sorted by weight\")","449","","450","    return _single_linkage_label(L)"],"delete":[]}],"sklearn\/cluster\/hierarchical.py":[{"add":["82","def _single_linkage_tree(connectivity, n_samples, n_nodes, n_clusters,","83","                         n_components, return_distance):","84","    \"\"\"","85","    Perform single linkage clustering on sparse data via the minimum","86","    spanning tree from scipy.sparse.csgraph, then using union-find to label.","87","    The parent array is then generated by walking through the tree.","88","    \"\"\"","89","    from scipy.sparse.csgraph import minimum_spanning_tree","90","","91","    # explicitly cast connectivity to ensure safety","92","    connectivity = connectivity.astype('float64')","93","","94","    # Ensure zero distances aren't ignored by setting them to \"epsilon\"","95","    epsilon_value = np.nextafter(0, 1, dtype=connectivity.data.dtype)","96","    connectivity.data[connectivity.data == 0] = epsilon_value","97","","98","    # Use scipy.sparse.csgraph to generate a minimum spanning tree","99","    mst = minimum_spanning_tree(connectivity.tocsr())","100","","101","    # Convert the graph to scipy.cluster.hierarchy array format","102","    mst = mst.tocoo()","103","","104","    # Undo the epsilon values","105","    mst.data[mst.data == epsilon_value] = 0","106","","107","    mst_array = np.vstack([mst.row, mst.col, mst.data]).T","108","","109","    # Sort edges of the min_spanning_tree by weight","110","    mst_array = mst_array[np.argsort(mst_array.T[2]), :]","111","","112","    # Convert edge list into standard hierarchical clustering format","113","    single_linkage_tree = _hierarchical._single_linkage_label(mst_array)","114","    children_ = single_linkage_tree[:, :2].astype(np.int)","115","","116","    # Compute parents","117","    parent = np.arange(n_nodes, dtype=np.intp)","118","    for i, (left, right) in enumerate(children_, n_samples):","119","        if n_clusters is not None and i >= n_nodes:","120","            break","121","        if left < n_nodes:","122","            parent[left] = i","123","        if right < n_nodes:","124","            parent[right] = i","125","","126","    if return_distance:","127","        distances = single_linkage_tree[:, 2]","128","        return children_, n_components, n_samples, parent, distances","129","    return children_, n_components, n_samples, parent","130","","131","","340","# single average and complete linkage","375","    linkage : {\"average\", \"complete\", \"single\"}, optional, default: \"complete\"","382","            - single uses the minimum of the distances between all observations","383","              of the two sets.","432","                       'average': _hierarchical.average_merge,","433","                       'single': None}  # Single linkage is handled differently","489","        distances = X[connectivity.row, connectivity.col].astype('float64')","504","    if linkage == 'single':","505","        return _single_linkage_tree(connectivity, n_samples, n_nodes,","506","                                    n_clusters, n_components, return_distance)","507","","591","def _single_linkage(*args, **kwargs):","592","    kwargs['linkage'] = 'single'","593","    return linkage_tree(*args, **kwargs)","594","","595","","599","    average=_average_linkage,","600","    single=_single_linkage)","695","    linkage : {\"ward\", \"complete\", \"average\", \"single\"}, optional \\","696","            (default=\"ward\")","706","        - single uses the minimum of the distances between all observations","707","          of the two sets.","781","            raise ValueError(\"Unknown linkage type %s. \"","867","    linkage : {\"ward\", \"complete\", \"average\", \"single\"}, optional\\","868","            (default=\"ward\")","878","        - single uses the minimum of the distances between all observations","879","          of the two sets."],"delete":["290","# average and complete linkage","325","    linkage : {\"average\", \"complete\"}, optional, default: \"complete\"","380","                       'average': _hierarchical.average_merge}","436","        distances = X[connectivity.row, connectivity.col]","537","    average=_average_linkage)","632","    linkage : {\"ward\", \"complete\", \"average\"}, optional, default: \"ward\"","715","            raise ValueError(\"Unknown linkage type %s.\"","801","    linkage : {\"ward\", \"complete\", \"average\"}, optional, default \"ward\""]}],"examples\/cluster\/plot_linkage_comparison.py":[{"add":[],"delete":[]}],"examples\/cluster\/plot_digits_linkage.py":[{"add":["14","This behavior is pronounced for the average linkage strategy,","15","that ends up with a couple of singleton clusters, while in the case","16","of single linkage we get a single central cluster with all other clusters","17","being drawn from noise points around the fringes.","73","    plt.tight_layout(rect=[0, 0.03, 1, 0.95])","83","for linkage in ('ward', 'average', 'complete', 'single'):","87","    print(\"%s :\\t%.2fs\" % (linkage, time() - t0))"],"delete":["14","This behavior is especially pronounced for the average linkage strategy,","15","that ends up with a couple of singleton clusters.","71","    plt.tight_layout()","81","for linkage in ('ward', 'average', 'complete'):","85","    print(\"%s : %.2fs\" % (linkage, time() - t0))"]}],"sklearn\/cluster\/tests\/test_hierarchical.py":[{"add":["26","                                          linkage_tree, _fix_connectivity)","36","from sklearn.datasets import make_moons, make_circles","151","    for linkage in (\"ward\", \"complete\", \"average\", \"single\"):","251","def test_single_linkage_clustering():","252","    # Check that we get the correct result in two emblematic cases","253","    moons, moon_labels = make_moons(noise=0.05, random_state=42)","254","    clustering = AgglomerativeClustering(n_clusters=2, linkage='single')","255","    clustering.fit(moons)","256","    assert_almost_equal(normalized_mutual_info_score(clustering.labels_,","257","                                                     moon_labels), 1)","258","","259","    circles, circle_labels = make_circles(factor=0.5, noise=0.025,","260","                                          random_state=42)","261","    clustering = AgglomerativeClustering(n_clusters=2, linkage='single')","262","    clustering.fit(circles)","263","    assert_almost_equal(normalized_mutual_info_score(clustering.labels_,","264","                                                     circle_labels), 1)","265","","266","","298","            # Sort the order of of child nodes per row for consistency","299","            children.sort(axis=1)","300","            assert_array_equal(children, children_, 'linkage tree differs'","301","                                                    ' from scipy impl for'","302","                                                    ' linkage: ' + linkage)","303","","312","def test_identical_points():","313","    # Ensure identical points are handled correctly when using mst with","314","    # a sparse connectivity matrix","315","    X = np.array([[0, 0, 0], [0, 0, 0],","316","                  [1, 1, 1], [1, 1, 1],","317","                  [2, 2, 2], [2, 2, 2]])","318","    true_labels = np.array([0, 0, 1, 1, 2, 2])","319","    connectivity = kneighbors_graph(X, n_neighbors=3, include_self=False)","320","    connectivity = 0.5 * (connectivity + connectivity.T)","321","    connectivity, n_components = _fix_connectivity(X,","322","                                                   connectivity,","323","                                                   'euclidean')","324","","325","    for linkage in ('single', 'average', 'average', 'ward'):","326","        clustering = AgglomerativeClustering(n_clusters=3,","327","                                             linkage=linkage,","328","                                             connectivity=connectivity)","329","        clustering.fit(X)","330","","331","        assert_almost_equal(normalized_mutual_info_score(clustering.labels_,","332","                                                         true_labels), 1)","333","","334","","402","        for linkage in ['average', 'complete', 'single']:","460","    linkage_options = ['complete', 'average', 'single']"],"delete":["26","                                          linkage_tree)","150","    for linkage in (\"ward\", \"complete\", \"average\"):","356","        for linkage in ['average', 'complete']:","414","    linkage_options = ['complete', 'average']"]}]}},"0dd19de33a79bb536692aecfdc7c39c18d68bb31":{"changes":{"sklearn\/model_selection\/tests\/test_validation.py":"MODIFY"},"diff":{"sklearn\/model_selection\/tests\/test_validation.py":[{"add":["454","            assert type(cv_results['fit_time']) == np.ndarray","455","            assert type(cv_results['score_time']) == np.ndarray"],"delete":["454","            assert type(cv_results['fit_time'] == np.ndarray)","455","            assert type(cv_results['score_time'] == np.ndarray)"]}]}},"f05a95b1039b307a76f9179c135e2d282bc6c901":{"changes":{"doc\/modules\/classes.rst":"MODIFY","doc\/modules\/model_evaluation.rst":"MODIFY","sklearn\/metrics\/__init__.py":"MODIFY","sklearn\/metrics\/ranking.py":"MODIFY","sklearn\/metrics\/tests\/test_ranking.py":"MODIFY"},"diff":{"doc\/modules\/classes.rst":[{"add":[],"delete":["785","   metrics.dcg_score","793","   metrics.ndcg_score"]}],"doc\/modules\/model_evaluation.rst":[{"add":[],"delete":["311","Some are typically used for ranking:","312","","313",".. autosummary::","314","   :template: function.rst","315","","316","   dcg_score","317","   ndcg_score","318",""]}],"sklearn\/metrics\/__init__.py":[{"add":[],"delete":["14","from .ranking import dcg_score","15","from .ranking import ndcg_score","120","    'dcg_score',","121","    'ndcg_score'"]}],"sklearn\/metrics\/ranking.py":[{"add":["28","from ..utils import column_or_1d, check_array"],"delete":["28","from ..utils import column_or_1d, check_array, check_X_y","34","from ..preprocessing import LabelBinarizer","806","","807","","808","def dcg_score(y_true, y_score, k=5):","809","    \"\"\"Discounted cumulative gain (DCG) at rank K.","810","","811","    Parameters","812","    ----------","813","    y_true : array, shape = [n_samples]","814","        Ground truth (true relevance labels).","815","    y_score : array, shape = [n_samples]","816","        Predicted scores.","817","    k : int","818","        Rank.","819","","820","    Returns","821","    -------","822","    score : float","823","","824","    References","825","    ----------","826","    .. [1] `Wikipedia entry for the Discounted Cumulative Gain","827","           <https:\/\/en.wikipedia.org\/wiki\/Discounted_cumulative_gain>`_","828","    \"\"\"","829","    order = np.argsort(y_score)[::-1]","830","    y_true = np.take(y_true, order[:k])","831","","832","    gain = 2 ** y_true - 1","833","","834","    discounts = np.log2(np.arange(len(y_true)) + 2)","835","    return np.sum(gain \/ discounts)","836","","837","","838","def ndcg_score(y_true, y_score, k=5):","839","    \"\"\"Normalized discounted cumulative gain (NDCG) at rank K.","840","","841","    Normalized Discounted Cumulative Gain (NDCG) measures the performance of a","842","    recommendation system based on the graded relevance of the recommended","843","    entities. It varies from 0.0 to 1.0, with 1.0 representing the ideal","844","    ranking of the entities.","845","","846","    Parameters","847","    ----------","848","    y_true : array, shape = [n_samples]","849","        Ground truth (true labels represended as integers).","850","    y_score : array, shape = [n_samples, n_classes]","851","        Predicted probabilities.","852","    k : int","853","        Rank.","854","","855","    Returns","856","    -------","857","    score : float","858","","859","    Examples","860","    --------","861","    >>> y_true = [1, 0, 2]","862","    >>> y_score = [[0.15, 0.55, 0.2], [0.7, 0.2, 0.1], [0.06, 0.04, 0.9]]","863","    >>> ndcg_score(y_true, y_score, k=2)","864","    1.0","865","    >>> y_score = [[0.9, 0.5, 0.8], [0.7, 0.2, 0.1], [0.06, 0.04, 0.9]]","866","    >>> ndcg_score(y_true, y_score, k=2)","867","    0.66666666666666663","868","","869","    References","870","    ----------","871","    .. [1] `Kaggle entry for the Normalized Discounted Cumulative Gain","872","           <https:\/\/www.kaggle.com\/wiki\/NormalizedDiscountedCumulativeGain>`_","873","    \"\"\"","874","    y_score, y_true = check_X_y(y_score, y_true)","875","","876","    # Make sure we use all the labels (max between the length and the higher","877","    # number in the array)","878","    lb = LabelBinarizer()","879","    lb.fit(np.arange(max(np.max(y_true) + 1, len(y_true))))","880","    binarized_y_true = lb.transform(y_true)","881","","882","    if binarized_y_true.shape != y_score.shape:","883","        raise ValueError(\"y_true and y_score have different value ranges\")","884","","885","    scores = []","886","","887","    # Iterate over each y_value_true and compute the DCG score","888","    for y_value_true, y_value_score in zip(binarized_y_true, y_score):","889","        actual = dcg_score(y_value_true, y_value_score, k)","890","        best = dcg_score(y_value_true, y_value_true, k)","891","        scores.append(actual \/ best)","892","","893","    return np.mean(scores)"]}],"sklearn\/metrics\/tests\/test_ranking.py":[{"add":[],"delete":["31","from sklearn.metrics import ndcg_score","760","def test_ndcg_score():","761","    # Check perfect ranking","762","    y_true = [1, 0, 2]","763","    y_score = [","764","        [0.15, 0.55, 0.2],","765","        [0.7, 0.2, 0.1],","766","        [0.06, 0.04, 0.9]","767","    ]","768","    perfect = ndcg_score(y_true, y_score)","769","    assert_equal(perfect, 1.0)","770","","771","    # Check bad ranking with a small K","772","    y_true = [0, 2, 1]","773","    y_score = [","774","        [0.15, 0.55, 0.2],","775","        [0.7, 0.2, 0.1],","776","        [0.06, 0.04, 0.9]","777","    ]","778","    short_k = ndcg_score(y_true, y_score, k=1)","779","    assert_equal(short_k, 0.0)","780","","781","    # Check a random scoring","782","    y_true = [2, 1, 0]","783","    y_score = [","784","        [0.15, 0.55, 0.2],","785","        [0.7, 0.2, 0.1],","786","        [0.06, 0.04, 0.9]","787","    ]","788","    average_ranking = ndcg_score(y_true, y_score, k=2)","789","    assert_almost_equal(average_ranking, 0.63092975)","790","","791",""]}]}},"fbca098c62da3de7d2a5fedc649c7a137719609f":{"changes":{"sklearn\/model_selection\/_validation.py":"MODIFY"},"diff":{"sklearn\/model_selection\/_validation.py":[{"add":["146","    >>> from sklearn.model_selection import cross_validate","155","    Single metric evaluation using ``cross_validate``","156","","163","    Multiple metric evaluation using ``cross_validate``","164","    (please refer the ``scoring`` parameter doc for more information)","165",""],"delete":["146","    >>> from sklearn.model_selection import cross_val_score","155","    # single metric evaluation using cross_validate","162","    # Multiple metric evaluation using cross_validate","163","    # (Please refer the ``scoring`` parameter doc for more information)"]}]}}}