{"a365714481a092ec05b0194d6f73d46b6eee06f5":{"changes":{"sklearn\/ensemble\/bagging.py":"MODIFY","sklearn\/ensemble\/tests\/test_bagging.py":"MODIFY","doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/utils\/__init__.py":"MODIFY"},"diff":{"sklearn\/ensemble\/bagging.py":[{"add":["413","        Returns a dynamically generated list of indices identifying","421","        return [sample_indices","422","                for _, sample_indices in self._get_estimators_indices()]","509","        estimator. Each subset is defined by an array of the indices selected.","587","            mask = ~indices_to_mask(samples, n_samples)","882","        estimator. Each subset is defined by an array of the indices selected.","993","            mask = ~indices_to_mask(samples, n_samples)"],"delete":["112","        # Draw samples, using a mask, and then fit","414","        Returns a dynamically generated list of boolean masks identifying","422","        sample_masks = []","423","        for _, sample_indices in self._get_estimators_indices():","424","            mask = indices_to_mask(sample_indices, self._n_samples)","425","            sample_masks.append(mask)","426","","427","        return sample_masks","514","        estimator. Each subset is defined by a boolean mask.","592","            mask = ~samples","887","        estimator. Each subset is defined by a boolean mask.","998","            mask = ~samples"]}],"sklearn\/ensemble\/tests\/test_bagging.py":[{"add":["31","from sklearn.random_projection import SparseRandomProjection","36","from sklearn.utils import check_random_state, hash","225","class DummySizeEstimator(BaseEstimator):","226","","227","    def fit(self, X, y):","228","        self.training_size_ = X.shape[0]","229","        self.training_hash_ = hash(X)","230","","231","","259","    # check that each sampling correspond to a complete bootstrap resample.","260","    # the size of each bootstrap should be the same as the input data but","261","    # the data should be different (checked using the hash of the data).","262","    ensemble = BaggingRegressor(base_estimator=DummySizeEstimator(),","263","                                bootstrap=True).fit(X_train, y_train)","264","    training_hash = []","265","    for estimator in ensemble.estimators_:","266","        assert estimator.training_size_ == X_train.shape[0]","267","        training_hash.append(estimator.training_hash_)","268","    assert len(set(training_hash)) == len(training_hash)","269","","731","    assert_equal(len(estimators_samples[0]), len(X) \/\/ 2)","732","    assert_equal(estimators_samples[0].dtype.kind, 'i')","750","def test_estimators_samples_deterministic():","751","    # This test is a regression test to check that with a random step","752","    # (e.g. SparseRandomProjection) and a given random state, the results","753","    # generated at fit time can be identically reproduced at a later time using","754","    # data saved in object attributes. Check issue #9524 for full discussion.","755","","756","    iris = load_iris()","757","    X, y = iris.data, iris.target","758","","759","    base_pipeline = make_pipeline(SparseRandomProjection(n_components=2),","760","                                  LogisticRegression())","761","    clf = BaggingClassifier(base_estimator=base_pipeline,","762","                            max_samples=0.5,","763","                            random_state=0)","764","    clf.fit(X, y)","765","    pipeline_estimator_coef = clf.estimators_[0].steps[-1][1].coef_.copy()","766","","767","    estimator = clf.estimators_[0]","768","    estimator_sample = clf.estimators_samples_[0]","769","    estimator_feature = clf.estimators_features_[0]","770","","771","    X_train = (X[estimator_sample])[:, estimator_feature]","772","    y_train = y[estimator_sample]","773","","774","    estimator.fit(X_train, y_train)","775","    assert_array_equal(estimator.steps[-1][1].coef_, pipeline_estimator_coef)","776","","777",""],"delete":["35","from sklearn.utils import check_random_state","712","    assert_equal(len(estimators_samples[0]), len(X))","713","    assert_equal(estimators_samples[0].dtype.kind, 'b')"]}],"doc\/whats_new\/v0.20.rst":[{"add":["656","Ensemble","657","","658","- Fix allowing to obtain deterministic with :class:`BaseBagging` estimator,","659","  when comparing results generated at fit time with the one using the object","660","  attributes when ``random_state`` is set. :issue:`9723` by :user:`Guillaume","661","  Lemaitre <glemaitre>`.","662","","837","Ensemble","838","","839","- Classes derived from :class:`ensemble.BaseBagging`. The attribute","840","  ``estimators_samples_`` will return a list of arrays containing the indices","841","  selected for each bootstrap instead of a list of arrays containing the mask","842","  of the samples selected for each bootstrap. Indices allows to repeat samples","843","  while mask does not allow this functionality. :issue:`9524` by","844","  :user:`Guillaume Lemaitre <glemaitre>`.","845",""],"delete":[]}],"sklearn\/utils\/__init__.py":[{"add":["17","from ._joblib import cpu_count, Parallel, Memory, delayed, hash","31","           \"cpu_count\", \"Parallel\", \"Memory\", \"delayed\", \"parallel_backend\",","32","           \"hash\"]"],"delete":["17","from ._joblib import cpu_count, Parallel, Memory, delayed","31","           \"cpu_count\", \"Parallel\", \"Memory\", \"delayed\", \"parallel_backend\"]"]}]}},"534f68b17c44ad52cd7769a51034c96dc2fbdb22":{"changes":{"sklearn\/datasets\/species_distributions.py":"MODIFY","sklearn\/datasets\/california_housing.py":"MODIFY","sklearn\/datasets\/rcv1.py":"MODIFY"},"diff":{"sklearn\/datasets\/species_distributions.py":[{"add":["242","        with np.load(samples_path) as X:  # samples.zip is a valid npz","243","            for f in X.files:","244","                fhandle = BytesIO(X[f])","245","                if 'train' in f:","246","                    train = _load_csv(fhandle)","247","                if 'test' in f:","248","                    test = _load_csv(fhandle)","254","        with np.load(coverages_path) as X:  # coverages.zip is a valid npz","255","            coverages = []","256","            for f in X.files:","257","                fhandle = BytesIO(X[f])","258","                logger.debug(' - converting {}'.format(f))","259","                coverages.append(_load_coverage(fhandle))","260","            coverages = np.asarray(coverages, dtype=dtype)"],"delete":["242","        X = np.load(samples_path)  # samples.zip is a valid npz","245","        for f in X.files:","246","            fhandle = BytesIO(X[f])","247","            if 'train' in f:","248","                train = _load_csv(fhandle)","249","            if 'test' in f:","250","                test = _load_csv(fhandle)","251","","255","        X = np.load(coverages_path)  # coverages.zip is a valid npz","258","        coverages = []","259","        for f in X.files:","260","            fhandle = BytesIO(X[f])","261","            logger.debug(' - converting {}'.format(f))","262","            coverages.append(_load_coverage(fhandle))","263","        coverages = np.asarray(coverages, dtype=dtype)","264",""]}],"sklearn\/datasets\/california_housing.py":[{"add":["51","","99","","102","        with tarfile.open(mode=\"r:gz\", name=archive_path) as f:","103","            cal_housing = np.loadtxt(","104","                f.extractfile('CaliforniaHousing\/cal_housing.data'),","105","                delimiter=',')","106","            # Columns are not in the same order compared to the previous","107","            # URL resource on lib.stat.cmu.edu","108","            columns_index = [8, 7, 2, 3, 4, 5, 6, 1, 0]","109","            cal_housing = cal_housing[:, columns_index]","110","","111","            joblib.dump(cal_housing, filepath, compress=6)"],"delete":["100","        fileobj = tarfile.open(","101","            mode=\"r:gz\",","102","            name=archive_path).extractfile(","103","                'CaliforniaHousing\/cal_housing.data')","106","        cal_housing = np.loadtxt(fileobj, delimiter=',')","107","        # Columns are not in the same order compared to the previous","108","        # URL resource on lib.stat.cmu.edu","109","        columns_index = [8, 7, 2, 3, 4, 5, 6, 1, 0]","110","        cal_housing = cal_housing[:, columns_index]","111","        joblib.dump(cal_housing, filepath, compress=6)"]}],"sklearn\/datasets\/rcv1.py":[{"add":["175","","176","        # delete archives","177","        for f in files:","178","            f.close()","179","            remove(f.name)","184","","199","        with GzipFile(filename=topics_archive_path, mode='rb') as f:","200","            for line in f:","201","                line_components = line.decode(\"ascii\").split(u\" \")","202","                if len(line_components) == 3:","203","                    cat, doc, _ = line_components","204","                    if cat not in category_names:","205","                        n_cat += 1","206","                        category_names[cat] = n_cat","208","                    doc = int(doc)","209","                    if doc != doc_previous:","210","                        doc_previous = doc","211","                        n_doc += 1","212","                        sample_id_bis[n_doc] = doc","213","                    y[n_doc, category_names[cat]] = 1"],"delete":["168","        # delete archives","169","        for f in files:","170","            remove(f.name)","171","","197","        for line in GzipFile(filename=topics_archive_path, mode='rb'):","198","            line_components = line.decode(\"ascii\").split(u\" \")","199","            if len(line_components) == 3:","200","                cat, doc, _ = line_components","201","                if cat not in category_names:","202","                    n_cat += 1","203","                    category_names[cat] = n_cat","205","                doc = int(doc)","206","                if doc != doc_previous:","207","                    doc_previous = doc","208","                    n_doc += 1","209","                    sample_id_bis[n_doc] = doc","210","                y[n_doc, category_names[cat]] = 1"]}]}},"7dcc41325c115c86e417537b7dc7123e8457d74a":{"changes":{"sklearn\/neighbors\/approximate.py":"MODIFY"},"diff":{"sklearn\/neighbors\/approximate.py":[{"add":["134","        space to use by default for the :meth:`radius_neighbors` queries.","136","    n_candidates : int (default = 50)"],"delete":["134","        space to use by default for the :meth`radius_neighbors` queries.","136","    n_candidates : int (default = 10)"]}]}},"75763cfe7842af6a579bb2f57c5b030d5c529115":{"changes":{"sklearn\/tests\/test_pipeline.py":"MODIFY","sklearn\/base.py":"MODIFY","sklearn\/tests\/test_base.py":"MODIFY"},"diff":{"sklearn\/tests\/test_pipeline.py":[{"add":["26","from sklearn.linear_model import LogisticRegression, Lasso","30","from sklearn.dummy import DummyRegressor","292","                         error_msg % ('fake', pipe),","866","def test_set_params_nested_pipeline():","867","    estimator = Pipeline([","868","        ('a', Pipeline([","869","            ('b', DummyRegressor())","870","        ]))","871","    ])","872","    estimator.set_params(a__b__alpha=0.001, a__b=Lasso())","873","    estimator.set_params(a__steps=[('b', LogisticRegression())], a__b__C=5)","874","","875",""],"delete":["26","from sklearn.linear_model import LogisticRegression","291","                         error_msg % ('fake', 'Pipeline'),"]}],"sklearn\/base.py":[{"add":["7","from collections import defaultdict","251","","252","        nested_params = defaultdict(dict)  # grouped by prefix","253","        for key, value in params.items():","254","            key, delim, sub_key = key.partition('__')","255","            if key not in valid_params:","256","                raise ValueError('Invalid parameter %s for estimator %s. '","257","                                 'Check the list of available parameters '","258","                                 'with `estimator.get_params().keys()`.' %","259","                                 (key, self))","260","","261","            if delim:","262","                nested_params[key][sub_key] = value","265","","266","        for key, sub_params in nested_params.items():","267","            valid_params[key].set_params(**sub_params)","268",""],"delete":["250","        for key, value in six.iteritems(params):","251","            split = key.split('__', 1)","252","            if len(split) > 1:","253","                # nested objects case","254","                name, sub_name = split","255","                if name not in valid_params:","256","                    raise ValueError('Invalid parameter %s for estimator %s. '","257","                                     'Check the list of available parameters '","258","                                     'with `estimator.get_params().keys()`.' %","259","                                     (name, self))","260","                sub_object = valid_params[name]","261","                sub_object.set_params(**{sub_name: value})","263","                # simple objects case","264","                if key not in valid_params:","265","                    raise ValueError('Invalid parameter %s for estimator %s. '","266","                                     'Check the list of available parameters '","267","                                     'with `estimator.get_params().keys()`.' %","268","                                     (key, self.__class__.__name__))"]}],"sklearn\/tests\/test_base.py":[{"add":["230","def test_set_params_passes_all_parameters():","231","    # Make sure all parameters are passed together to set_params","232","    # of nested estimator. Regression test for #9944","233","","234","    class TestDecisionTree(DecisionTreeClassifier):","235","        def set_params(self, **kwargs):","236","            super(TestDecisionTree, self).set_params(**kwargs)","237","            # expected_kwargs is in test scope","238","            assert kwargs == expected_kwargs","239","            return self","240","","241","    expected_kwargs = {'max_depth': 5, 'min_samples_leaf': 2}","242","    for est in [Pipeline([('estimator', TestDecisionTree())]),","243","                GridSearchCV(TestDecisionTree(), {})]:","244","        est.set_params(estimator__max_depth=5,","245","                       estimator__min_samples_leaf=2)","246","","247",""],"delete":[]}]}},"add9b7f8f9774c91cacfc032f8496c239dfe9688":{"changes":{"sklearn\/model_selection\/tests\/test_validation.py":"MODIFY"},"diff":{"sklearn\/model_selection\/tests\/test_validation.py":[{"add":["318","                        \"Classification metrics can't handle a mix of \"","319","                        \"binary and continuous targets\",","323","                        \"Classification metrics can't handle a mix of \"","324","                        \"binary and continuous targets\","],"delete":["318","                        \"Can't handle mix of binary and continuous\",","322","                        \"Can't handle mix of binary and continuous\","]}]}},"27bbdb570bac062c71b3bb21b0876fd78adc9f7e":{"changes":{"sklearn\/externals\/joblib\/_parallel_backends.py":"MODIFY","sklearn\/utils\/validation.py":"MODIFY","sklearn\/utils\/testing.py":"MODIFY","examples\/calibration\/plot_compare_calibration.py":"MODIFY","sklearn\/externals\/joblib\/parallel.py":"MODIFY"},"diff":{"sklearn\/externals\/joblib\/_parallel_backends.py":[{"add":["90","        # Does nothing by default: to be overridden in subclasses when canceling"],"delete":["90","        # Does nothing by default: to be overriden in subclasses when canceling"]}],"sklearn\/utils\/validation.py":[{"add":["620","        Whether the parameter was found to be a named parameter of the"],"delete":["620","        Whether the parameter was found to be a a named parameter of the"]}],"sklearn\/utils\/testing.py":[{"add":["270","    category : tuple of warning class, default to Warning"],"delete":["270","    category : tuple of warning class, defaut to Warning"]}],"examples\/calibration\/plot_compare_calibration.py":[{"add":["35","  subsetting.\" As a result, the calibration curve shows a characteristic","36","  sigmoid shape, indicating that the classifier could trust its \"intuition\"","37","  more and return probabilities closer to 0 or 1 typically."],"delete":["35","  subseting.\" As a result, the calibration curve shows a characteristic sigmoid","36","  shape, indicating that the classifier could trust its \"intuition\" more and","37","  return probabilities closer to 0 or 1 typically."]}],"sklearn\/externals\/joblib\/parallel.py":[{"add":["50","# Thread local value that can be overridden by the ``parallel_backend`` context"],"delete":["50","# Thread local value that can be overriden by the ``parallel_backend`` context"]}]}},"b441308f36f948d2abe8bda3e666720bca120610":{"changes":{"sklearn\/model_selection\/_validation.py":"MODIFY","sklearn\/feature_extraction\/text.py":"MODIFY","sklearn\/ensemble\/gradient_boosting.py":"MODIFY","sklearn\/learning_curve.py":"MODIFY","sklearn\/utils\/__init__.py":"MODIFY"},"diff":{"sklearn\/model_selection\/_validation.py":[{"add":["1099","    if np.issubdtype(train_sizes_abs.dtype, np.floating):"],"delete":["1099","    if np.issubdtype(train_sizes_abs.dtype, np.float):"]}],"sklearn\/feature_extraction\/text.py":[{"add":["1088","        if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.floating):"],"delete":["1088","        if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):"]}],"sklearn\/ensemble\/gradient_boosting.py":[{"add":["155","        if np.issubdtype(y.dtype, np.signedinteger):"],"delete":["155","        if np.issubdtype(y.dtype, int):"]}],"sklearn\/learning_curve.py":[{"add":["208","    if np.issubdtype(train_sizes_abs.dtype, np.floating):"],"delete":["208","    if np.issubdtype(train_sizes_abs.dtype, np.float):"]}],"sklearn\/utils\/__init__.py":[{"add":["92","    if np.issubdtype(mask.dtype, np.signedinteger):"],"delete":["92","    if np.issubdtype(mask.dtype, np.int):"]}]}},"bd4953d143520d44c01b7392bb927fed2cfb9f97":{"changes":{"sklearn\/cluster\/hierarchical.py":"MODIFY","sklearn\/cluster\/tests\/test_hierarchical.py":"MODIFY"},"diff":{"sklearn\/cluster\/hierarchical.py":[{"add":["291","def linkage_tree(X, connectivity=None, n_components='deprecated',","370","    if n_components != 'deprecated':","371","        warnings.warn(\"n_components was deprecated in 0.18\"","372","                      \"will be removed in 0.21\", DeprecationWarning)","373",""],"delete":["291","def linkage_tree(X, connectivity=None, n_components=None,"]}],"sklearn\/cluster\/tests\/test_hierarchical.py":[{"add":["38","def test_deprecation_of_n_components_in_linkage_tree():","39","    rng = np.random.RandomState(0)","40","    X = rng.randn(50, 100)","41","    # Test for warning of deprecation of n_components in linkage_tree","42","    children, n_nodes, n_leaves, parent = assert_warns(DeprecationWarning,","43","                                                       linkage_tree,","44","                                                       X.T,","45","                                                       n_components=10)","46","    children_t, n_nodes_t, n_leaves_t, parent_t = linkage_tree(X.T)","47","    assert_array_equal(children, children_t)","48","    assert_equal(n_nodes, n_nodes_t)","49","    assert_equal(n_leaves, n_leaves_t)","50","    assert_equal(parent, parent_t)","51",""],"delete":[]}]}},"ec0dbf0399c4187a1d17e66de17265c67588373f":{"changes":{"doc\/whats_new.rst":"MODIFY","sklearn\/gaussian_process\/gpr.py":"MODIFY","sklearn\/gaussian_process\/tests\/test_gpr.py":"MODIFY"},"diff":{"doc\/whats_new.rst":[{"add":["275","     failed when `alpha=0`. :issue:`5814` by :user:`Yichuan Liu <yl565>` and","427","  -  Fixed a bug in :class:`gaussian_process.GaussianProcessRegressor`","428","     when the standard deviation and covariance predicted without fit","429","     would fail with a unmeaningful error by default.","430","     :issue:`6573` by :user:`Quazi Marufur Rahman <qmaruf>` and","431","     `Manoj Kumar`_."],"delete":["275","     failed when `alpha=0`. :issue:`5814` by :user:`Yichuan Liu <yl565>` and "]}],"sklearn\/gaussian_process\/gpr.py":[{"add":["299","            if self.kernel is None:","300","                kernel = (C(1.0, constant_value_bounds=\"fixed\") *","301","                          RBF(1.0, length_scale_bounds=\"fixed\"))","302","            else:","303","                kernel = self.kernel","306","                y_cov = kernel(X)","309","                y_var = kernel.diag(X)"],"delete":["301","                y_cov = self.kernel(X)","304","                y_var = self.kernel.diag(X)"]}],"sklearn\/gaussian_process\/tests\/test_gpr.py":[{"add":["16","            assert_almost_equal, assert_equal, assert_raise_message,","17","            assert_array_almost_equal)","330","","331","","332","def test_no_fit_default_predict():","333","    # Test that GPR predictions without fit does not break by default.","334","    default_kernel = (C(1.0, constant_value_bounds=\"fixed\") *","335","                      RBF(1.0, length_scale_bounds=\"fixed\"))","336","    gpr1 = GaussianProcessRegressor()","337","    _, y_std1 = gpr1.predict(X, return_std=True)","338","    _, y_cov1 = gpr1.predict(X, return_cov=True)","339","","340","    gpr2 = GaussianProcessRegressor(kernel=default_kernel)","341","    _, y_std2 = gpr2.predict(X, return_std=True)","342","    _, y_cov2 = gpr2.predict(X, return_cov=True)","343","","344","    assert_array_almost_equal(y_std1, y_std2)","345","    assert_array_almost_equal(y_cov1, y_cov2)"],"delete":["16","            assert_almost_equal, assert_equal, assert_raise_message)"]}]}},"e1fb03c86d2a2c47ef008ead958e1bc10fb06e77":{"changes":{"sklearn\/linear_model\/tests\/test_logistic.py":"MODIFY","sklearn\/linear_model\/logistic.py":"MODIFY"},"diff":{"sklearn\/linear_model\/tests\/test_logistic.py":[{"add":["77","    msg = \"is not a valid scoring value\"","78","    assert_raise_message(ValueError, msg,","79","                         LogisticRegressionCV(scoring='bad-scorer', cv=2).fit,","80","                         X, Y1)","81",""],"delete":[]}],"sklearn\/linear_model\/logistic.py":[{"add":["36","from ..metrics import get_scorer","943","        scoring = get_scorer(scoring)"],"delete":["36","from ..metrics import SCORERS","943","        scoring = SCORERS[scoring]"]}]}},"ebf2bf81075ae1f4eb47ea0f54981c512bda5ceb":{"changes":{"sklearn\/preprocessing\/tests\/test_label.py":"MODIFY","sklearn\/tests\/test_pipeline.py":"MODIFY","sklearn\/metrics\/cluster\/unsupervised.py":"MODIFY","sklearn\/tree\/_tree.pyx":"MODIFY","sklearn\/neighbors\/dist_metrics.pyx":"MODIFY","sklearn\/datasets\/tests\/test_svmlight_format.py":"MODIFY","sklearn\/feature_selection\/base.py":"MODIFY","sklearn\/neighbors\/lof.py":"MODIFY"},"diff":{"sklearn\/preprocessing\/tests\/test_label.py":[{"add":["223","            # With fit_transform","265","        # With fit_transform"],"delete":["223","            # With fit_tranform","265","        # With fit_tranform"]}],"sklearn\/tests\/test_pipeline.py":[{"add":["876","        # Get the time stamp of the transformer in the cached pipeline"],"delete":["876","        # Get the time stamp of the tranformer in the cached pipeline"]}],"sklearn\/metrics\/cluster\/unsupervised.py":[{"add":["30","    Note that Silhouette Coefficient is only defined if number of labels","116","    Note that Silhouette Coefficient is only defined if number of labels"],"delete":["30","    Note that Silhouette Coefficent is only defined if number of labels","116","    Note that Silhouette Coefficent is only defined if number of labels"]}],"sklearn\/tree\/_tree.pyx":[{"add":["639","        # capacity is inferred during the __setstate__ using nodes"],"delete":["639","        # capacity is infered during the __setstate__ using nodes"]}],"sklearn\/neighbors\/dist_metrics.pyx":[{"add":["345","        more efficient measure which preserves the rank of the true distance.","355","        more efficient measure which preserves the rank of the true distance."],"delete":["345","        more efficent measure which preserves the rank of the true distance.","355","        more efficent measure which preserves the rank of the true distance."]}],"sklearn\/datasets\/tests\/test_svmlight_format.py":[{"add":["444","        # load the original sparse matrix into 3 independent CSR matrices"],"delete":["444","        # load the original sparse matrix into 3 independant CSR matrices"]}],"sklearn\/feature_selection\/base.py":[{"add":["19","    Transformer mixin that performs feature selection given a support mask"],"delete":["19","    Tranformer mixin that performs feature selection given a support mask"]}],"sklearn\/neighbors\/lof.py":[{"add":["296","        #  1e-10 to avoid `nan' when nb of duplicates > n_neighbors_:"],"delete":["296","        #  1e-10 to avoid `nan' when when nb of duplicates > n_neighbors_:"]}]}},"32ac22870d004b52b06d12f18ff3ffbf20862b3b":{"changes":{"examples\/tree\/plot_tree_regression_multioutput.py":"MODIFY","sklearn\/decomposition\/factor_analysis.py":"MODIFY","examples\/cluster\/plot_dict_face_patches.py":"MODIFY","examples\/gaussian_process\/plot_gpr_noisy_targets.py":"MODIFY","sklearn\/gaussian_process\/gaussian_process.py":"MODIFY","sklearn\/decomposition\/dict_learning.py":"MODIFY","examples\/neighbors\/plot_digits_kde_sampling.py":"MODIFY","examples\/ensemble\/plot_forest_iris.py":"MODIFY","sklearn\/mixture\/dpgmm.py":"MODIFY","examples\/gaussian_process\/plot_gpc_isoprobability.py":"MODIFY","sklearn\/utils\/extmath.py":"MODIFY","examples\/cluster\/plot_kmeans_stability_low_dim_dense.py":"MODIFY","examples\/decomposition\/plot_pca_3d.py":"MODIFY","examples\/cluster\/plot_color_quantization.py":"MODIFY","sklearn\/decomposition\/pca.py":"MODIFY","examples\/linear_model\/plot_lasso_coordinate_descent_path.py":"MODIFY","sklearn\/linear_model\/least_angle.py":"MODIFY"},"diff":{"examples\/tree\/plot_tree_regression_multioutput.py":[{"add":[],"delete":["44","s = 50"]}],"sklearn\/decomposition\/factor_analysis.py":[{"add":[],"delete":["328","        log_like = np.zeros(X.shape[0])"]}],"examples\/cluster\/plot_dict_face_patches.py":[{"add":[],"delete":["43","index = 1"]}],"examples\/gaussian_process\/plot_gpr_noisy_targets.py":[{"add":["63","plt.figure()","99","plt.figure()"],"delete":["63","fig = plt.figure()","99","fig = plt.figure()"]}],"sklearn\/gaussian_process\/gaussian_process.py":[{"add":[],"delete":["446","            # Initialize output","447","            y = np.zeros(n_eval)","448","            if eval_MSE:","449","                MSE = np.zeros(n_eval)","450",""]}],"sklearn\/decomposition\/dict_learning.py":[{"add":[],"delete":["826","        n_samples, n_features = X.shape"]}],"examples\/neighbors\/plot_digits_kde_sampling.py":[{"add":[],"delete":["22","data = digits.data"]}],"examples\/ensemble\/plot_forest_iris.py":[{"add":["91","        model.fit(X, y)","93","        scores = model.score(X, y)"],"delete":["48","from sklearn import clone","92","        clf = clone(model)","93","        clf = model.fit(X, y)","95","        scores = clf.score(X, y)"]}],"sklearn\/mixture\/dpgmm.py":[{"add":[],"delete":["275","        z = np.zeros((X.shape[0], self.n_components))","846","        logprior = 0."]}],"examples\/gaussian_process\/plot_gpc_isoprobability.py":[{"add":["87","plt.contour(x1, x2, y_true, [0.], colors='k', linestyles='dashdot')"],"delete":["87","cs = plt.contour(x1, x2, y_true, [0.], colors='k', linestyles='dashdot')"]}],"sklearn\/utils\/extmath.py":[{"add":[],"delete":["423","        axis = axis"]}],"examples\/cluster\/plot_kmeans_stability_low_dim_dense.py":[{"add":["71","plt.figure()","107","plt.figure()"],"delete":["71","fig = plt.figure()","107","fig = plt.figure()"]}],"examples\/decomposition\/plot_pca_3d.py":[{"add":[],"delete":["75","    x_pca_axis, y_pca_axis, z_pca_axis = V.T * pca_score \/ pca_score.min()","76",""]}],"examples\/cluster\/plot_color_quantization.py":[{"add":[],"delete":["86","ax = plt.axes([0, 0, 1, 1])","93","ax = plt.axes([0, 0, 1, 1])","100","ax = plt.axes([0, 0, 1, 1])"]}],"sklearn\/decomposition\/pca.py":[{"add":[],"delete":["552","        log_like = np.zeros(X.shape[0])"]}],"examples\/linear_model\/plot_lasso_coordinate_descent_path.py":[{"add":[],"delete":["49","ax = plt.gca()","50","","66","ax = plt.gca()","80","ax = plt.gca()"]}],"sklearn\/linear_model\/least_angle.py":[{"add":[],"delete":["416","            alpha = alphas[n_iter, np.newaxis]","417","            prev_alpha = alphas[n_iter - 1, np.newaxis]"]}]}},"3593194b69c4330261925b451dd8e445c17c560a":{"changes":{"examples\/applications\/plot_stock_market.py":"MODIFY"},"diff":{"examples\/applications\/plot_stock_market.py":[{"add":["126","    min_date = min(data['date']) if len(data) else datetime.min.date()","127","    max_date = max(data['date']) if len(data) else datetime.max.date()"],"delete":["126","    min_date = min(data['date'], default=datetime.min.date())","127","    max_date = max(data['date'], default=datetime.max.date())"]}]}},"cf67fa43d324003b6ff8ab6c40b89aabe6650c2c":{"changes":{"sklearn\/datasets\/covtype.py":"MODIFY","sklearn\/datasets\/rcv1.py":"MODIFY"},"diff":{"sklearn\/datasets\/covtype.py":[{"add":["91","        if not exists(covtype_dir):","92","            makedirs(covtype_dir)"],"delete":["91","        makedirs(covtype_dir, exist_ok=True)"]}],"sklearn\/datasets\/rcv1.py":[{"add":["116","        if not exists(rcv1_dir):","117","            makedirs(rcv1_dir)"],"delete":["116","        makedirs(rcv1_dir, exist_ok=True)"]}]}},"ece341cf5362558b944685d70a12c28efd055991":{"changes":{"sklearn\/neighbors\/tests\/test_neighbors.py":"MODIFY","sklearn\/neighbors\/base.py":"MODIFY"},"diff":{"sklearn\/neighbors\/tests\/test_neighbors.py":[{"add":["1285","def test_sparse_metric_callable():","1286","    def sparse_metric(x, y):  # Metric accepting sparse matrix input (only)","1287","        assert_true(issparse(x) and issparse(y))","1288","        return x.dot(y.T).A.item()","1289","","1290","    X = csr_matrix([  # Population matrix","1291","        [1, 1, 1, 1, 1],","1292","        [1, 0, 1, 0, 1],","1293","        [0, 0, 1, 0, 0]","1294","    ])","1295","","1296","    Y = csr_matrix([  # Query matrix","1297","        [1, 1, 0, 1, 1],","1298","        [1, 0, 0, 0, 1]","1299","    ])","1300","","1301","    nn = neighbors.NearestNeighbors(algorithm='brute', n_neighbors=2,","1302","                                    metric=sparse_metric).fit(X)","1303","    N = nn.kneighbors(Y, return_distance=False)","1304","","1305","    # GS indices of nearest neighbours in `X` for `sparse_metric`","1306","    gold_standard_nn = np.array([","1307","        [2, 1],","1308","        [2, 1]","1309","    ])","1310","","1311","    assert_array_equal(N, gold_standard_nn)","1312","","1313",""],"delete":[]}],"sklearn\/neighbors\/base.py":[{"add":["213","            if self.effective_metric_ not in VALID_METRICS_SPARSE['brute'] \\","214","                    and not callable(self.effective_metric_):","215",""],"delete":["213","            if self.effective_metric_ not in VALID_METRICS_SPARSE['brute']:"]}]}},"894fd72980f62c6f8eda9b5e1043e0cd3a1776e5":{"changes":{"sklearn\/neighbors\/dist_metrics.pyx":"MODIFY"},"diff":{"sklearn\/neighbors\/dist_metrics.pyx":[{"add":["116","             [3, 4, 5]]"],"delete":["116","             [3, 4, 5]])"]}]}},"aae87002b96622424a16dcad2eaef3a75cc0feda":{"changes":{"sklearn\/utils\/metaestimators.py":"MODIFY","sklearn\/tests\/test_pipeline.py":"MODIFY","sklearn\/pipeline.py":"MODIFY"},"diff":{"sklearn\/utils\/metaestimators.py":[{"add":["53","        new_estimators = list(getattr(self, attr))"],"delete":["53","        new_estimators = getattr(self, attr)[:]"]}],"sklearn\/tests\/test_pipeline.py":[{"add":["210","def test_pipeline_init_tuple():","211","    # Pipeline accepts steps as tuple","212","    X = np.array([[1, 2]])","213","    pipe = Pipeline((('transf', Transf()), ('clf', FitParamT())))","214","    pipe.fit(X, y=None)","215","    pipe.score(X)","216","","217","    pipe.set_params(transf=None)","218","    pipe.fit(X, y=None)","219","    pipe.score(X)","220","","221","","439","    # test that init accepts tuples","440","    fs = FeatureUnion(((\"svd\", svd), (\"select\", select)))","441","    fs.fit(X, y)","442",""],"delete":[]}],"sklearn\/pipeline.py":[{"add":["113","        self.steps = list(steps)","625","        self.transformer_list = list(transformer_list)"],"delete":["19","from .utils import tosequence","114","        self.steps = tosequence(steps)","626","        self.transformer_list = tosequence(transformer_list)"]}]}},"4c61e8b2237a5877b47b22cbb5037430c5635e74":{"changes":{"sklearn\/metrics\/classification.py":"MODIFY","sklearn\/metrics\/tests\/test_classification.py":"MODIFY"},"diff":{"sklearn\/metrics\/classification.py":[{"add":["169","","530","    t_sum = C.sum(axis=1, dtype=np.float64)","531","    p_sum = C.sum(axis=0, dtype=np.float64)","532","    n_correct = np.trace(C, dtype=np.float64)"],"delete":["169","    ","530","    t_sum = C.sum(axis=1)","531","    p_sum = C.sum(axis=0)","532","    n_correct = np.trace(C)"]}],"sklearn\/metrics\/tests\/test_classification.py":[{"add":["485","def test_matthews_corrcoef_overflow():","486","    # https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/9622","487","    rng = np.random.RandomState(20170906)","488","","489","    def mcc_safe(y_true, y_pred):","490","        conf_matrix = confusion_matrix(y_true, y_pred)","491","        true_pos = conf_matrix[1, 1]","492","        false_pos = conf_matrix[1, 0]","493","        false_neg = conf_matrix[0, 1]","494","        n_points = len(y_true)","495","        pos_rate = (true_pos + false_neg) \/ n_points","496","        activity = (true_pos + false_pos) \/ n_points","497","        mcc_numerator = true_pos \/ n_points - pos_rate * activity","498","        mcc_denominator = activity * pos_rate * (1 - activity) * (1 - pos_rate)","499","        return mcc_numerator \/ np.sqrt(mcc_denominator)","500","","501","    def random_ys(n_points):    # binary","502","        x_true = rng.random_sample(n_points)","503","        x_pred = x_true + 0.2 * (rng.random_sample(n_points) - 0.5)","504","        y_true = (x_true > 0.5)","505","        y_pred = (x_pred > 0.5)","506","        return y_true, y_pred","507","","508","    for n_points in [100, 10000, 1000000]:","509","        arr = np.repeat([0., 1.], n_points)  # binary","510","        assert_almost_equal(matthews_corrcoef(arr, arr), 1.0)","511","        arr = np.repeat([0., 1., 2.], n_points)  # multiclass","512","        assert_almost_equal(matthews_corrcoef(arr, arr), 1.0)","513","","514","        y_true, y_pred = random_ys(n_points)","515","        assert_almost_equal(matthews_corrcoef(y_true, y_true), 1.0)","516","        assert_almost_equal(matthews_corrcoef(y_true, y_pred),","517","                            mcc_safe(y_true, y_pred))","518","","519",""],"delete":[]}]}},"f871e1d1e1486e8549dd8af1db0ed0ccc7737b2c":{"changes":{"sklearn\/ensemble\/gradient_boosting.py":"MODIFY"},"diff":{"sklearn\/ensemble\/gradient_boosting.py":[{"add":["1528","        y : array of shape = [n_samples]"],"delete":["1528","        y : array of shape = [\"n_samples]"]}]}},"896f9d97cfc9945f83ce89b4287c53a3d22a94d4":{"changes":{"sklearn\/metrics\/pairwise.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/kernel_approximation.py":"MODIFY","sklearn\/tests\/test_kernel_approximation.py":"MODIFY"},"diff":{"sklearn\/metrics\/pairwise.py":[{"add":["1300","    \"chi2\": frozenset([\"gamma\"]),"],"delete":["1300","    \"chi2\": (),","1302","    \"exp_chi2\": frozenset([\"gamma\"]),"]}],"doc\/whats_new.rst":[{"add":["433","   - Made default kernel parameters kernel-dependent in :class:`kernel_approximation.Nystroem`","434","     :issue:`5229` by :user:`mth4saurabh` and `Andreas M¨¹ller`_.","435","","436","   - Fixed passing of ``gamma`` parameter to the ``chi2`` kernel in","437","     :func:`metrics.pairwise_kernels` :issue:`5211` by :user:`nrhine1`,","438","     :user:`mth4saurabh` and `Andreas M¨¹ller`_.","439",""],"delete":[]}],"sklearn\/kernel_approximation.py":[{"add":["20","from .metrics.pairwise import pairwise_kernels, KERNEL_PARAMS","391","    degree : float, default=None","394","    coef0 : float, default=None","440","    def __init__(self, kernel=\"rbf\", gamma=None, coef0=None, degree=None,","523","            for param in (KERNEL_PARAMS[self.kernel]):","524","                if getattr(self, param) is not None:","525","                    params[param] = getattr(self, param)","526","        else:","527","            if (self.gamma is not None or","528","                    self.coef0 is not None or","529","                    self.degree is not None):","530","                warnings.warn(","531","                    \"Passing gamma, coef0 or degree to Nystroem when using a\"","532","                    \" callable kernel is deprecated in version 0.19 and will\"","533","                    \" raise an error in 0.21, as they are ignored. Use \"","534","                    \"kernel_params instead.\", DeprecationWarning)"],"delete":["20","from .metrics.pairwise import pairwise_kernels","391","    degree : float, default=3","394","    coef0 : float, default=1","440","    def __init__(self, kernel=\"rbf\", gamma=None, coef0=1, degree=3,","523","            params['gamma'] = self.gamma","524","            params['degree'] = self.degree","525","            params['coef0'] = self.coef0"]}],"sklearn\/tests\/test_kernel_approximation.py":[{"add":["7","from sklearn.utils.testing import assert_warns_message","14","from sklearn.metrics.pairwise import polynomial_kernel, rbf_kernel, chi2_kernel","168","    def linear_kernel(X, Y):","169","        return np.dot(X, Y.T)","182","def test_nystroem_default_parameters():","183","    rnd = np.random.RandomState(42)","184","    X = rnd.uniform(size=(10, 4))","185","","186","    # rbf kernel should behave as gamma=None by default","187","    # aka gamma = 1 \/ n_features","188","    nystroem = Nystroem(n_components=10)","189","    X_transformed = nystroem.fit_transform(X)","190","    K = rbf_kernel(X, gamma=None)","191","    K2 = np.dot(X_transformed, X_transformed.T)","192","    assert_array_almost_equal(K, K2)","193","","194","    # chi2 kernel should behave as gamma=1 by default","195","    nystroem = Nystroem(kernel='chi2', n_components=10)","196","    X_transformed = nystroem.fit_transform(X)","197","    K = chi2_kernel(X, gamma=1)","198","    K2 = np.dot(X_transformed, X_transformed.T)","199","    assert_array_almost_equal(K, K2)","200","","201","","247","","248","    def linear_kernel(X, Y):","249","        return np.dot(X, Y.T)","250","","251","    # if degree, gamma or coef0 is passed, we raise a warning","252","    msg = \"Passing gamma, coef0 or degree to Nystroem\"","253","    params = ({'gamma': 1}, {'coef0': 1}, {'degree': 2})","254","    for param in params:","255","        ny = Nystroem(kernel=linear_kernel, **param)","256","        assert_warns_message(DeprecationWarning, msg, ny.fit, X)"],"delete":["13","from sklearn.metrics.pairwise import polynomial_kernel, rbf_kernel","167","    linear_kernel = lambda X, Y: np.dot(X, Y.T)"]}]}},"ea41a78b9a2486bd71751cbdbd8462eed443cfdc":{"changes":{"doc\/datasets\/mldata.rst":"MODIFY","doc\/datasets\/conftest.py":"ADD","build_tools\/travis\/test_script.sh":"MODIFY","doc\/datasets\/mldata_fixture.py":"MODIFY","conftest.py":"ADD"},"diff":{"doc\/datasets\/mldata.rst":[{"add":["5","    >>> import tempfile","6","    >>> # Create a temporary folder for the data fetcher","7","    >>> custom_data_home = tempfile.mkdtemp()","8","    >>> os.makedirs(os.path.join(custom_data_home, 'mldata'))","9","","77","","78","","79","..","80","    >>> import shutil","81","    >>> shutil.rmtree(custom_data_home)"],"delete":[]}],"doc\/datasets\/conftest.py":[{"add":[],"delete":[]}],"build_tools\/travis\/test_script.sh":[{"add":["45","    # Going back to git checkout folder needed to test documentation","46","    cd $OLDPWD","47","","48","    if [[ \"$USE_PYTEST\" == \"true\" ]]; then","49","        pytest $(find doc -name '*.rst' | sort)","50","    else","51","        # Makefile is using nose"],"delete":["45","    # Test doc (only with nose until we switch completely to pytest)","46","    if [[ \"$USE_PYTEST\" != \"true\" ]]; then","47","        # Going back to git checkout folder needed for make test-doc","48","        cd $OLDPWD"]}],"doc\/datasets\/mldata_fixture.py":[{"add":[],"delete":["5","from os import makedirs","6","from os.path import join","8","import tempfile","9","import shutil","11","from sklearn import datasets","16","def globs(globs):","17","    # Create a temporary folder for the data fetcher","18","    global custom_data_home","19","    custom_data_home = tempfile.mkdtemp()","20","    makedirs(join(custom_data_home, 'mldata'))","21","    globs['custom_data_home'] = custom_data_home","22","    return globs","23","","24","","44","    shutil.rmtree(custom_data_home)"]}],"conftest.py":[{"add":[],"delete":[]}]}},"f11e4d1c939e549951e6ed34c3b900ebd0d13f64":{"changes":{"sklearn\/neighbors\/tests\/test_approximate.py":"MODIFY"},"diff":{"sklearn\/neighbors\/tests\/test_approximate.py":[{"add":["48","            n_candidates=n_candidates, random_state=0)"],"delete":["48","            n_candidates=n_candidates)"]}]}},"9f9ad183f0871a1ba7ed24f7cea21388e68b090a":{"changes":{"sklearn\/metrics\/pairwise.py":"MODIFY","sklearn\/metrics\/tests\/test_pairwise.py":"MODIFY"},"diff":{"sklearn\/metrics\/pairwise.py":[{"add":["13","import warnings","470","                        size_threshold=None):","523","    if size_threshold is not None:","524","        warnings.warn('Use of the \"size_threshold\" is deprecated '","525","                      'in 0.19 and it will be removed version '","526","                      '0.21 of scikit-learn', DeprecationWarning)"],"delete":["469","                        size_threshold=5e8):"]}],"sklearn\/metrics\/tests\/test_pairwise.py":[{"add":["14","from sklearn.utils.testing import assert_warns","77","    # Using size_threshold argument should raise","78","    # a deprecation warning","79","    assert_warns(DeprecationWarning,","80","                 manhattan_distances, X, Y, size_threshold=10)"],"delete":["76","    # Low-level function for manhattan can divide in blocks to avoid","77","    # using too much memory during the broadcasting","78","    S3 = manhattan_distances(X, Y, size_threshold=10)","79","    assert_array_almost_equal(S, S3)"]}]}},"d9eba751bdb2492f9d77f10cb2ef28ecceed6ba5":{"changes":{"sklearn\/linear_model\/logistic.py":"MODIFY"},"diff":{"sklearn\/linear_model\/logistic.py":[{"add":["1417","        default: 'lbfgs'"],"delete":["1417","        default: 'liblinear'"]}]}},"7e3ad6d3fcfd5cdd55cb27775a1abb08b7e97890":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/datasets\/kddcup99.py":"MODIFY","sklearn\/datasets\/tests\/test_kddcup99.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["102","- Fixed a bug in :func:`datasets.fetch_kddcup99`, where data were not properly","103","  shuffled. :issue:`9731` by `Nicolas Goix`_.","104",""],"delete":[]}],"sklearn\/datasets\/kddcup99.py":[{"add":["179","    kddcup99 = _fetch_brute_kddcup99(data_home=data_home,","229","    if shuffle:","230","        data, target = shuffle_method(data, target, random_state=random_state)","231","","237","                          percent10=True):"],"delete":["179","    kddcup99 = _fetch_brute_kddcup99(data_home=data_home, shuffle=shuffle,","234","                          shuffle=False, percent10=True):","255","    shuffle : bool, default=False","256","        Whether to shuffle dataset.","257","","376","    if shuffle:","377","        X, y = shuffle_method(X, y, random_state=random_state)","378",""]}],"sklearn\/datasets\/tests\/test_kddcup99.py":[{"add":["39","","40","","41","def test_shuffle():","42","    try:","43","        dataset = fetch_kddcup99(random_state=0, subset='SA', shuffle=True,","44","                                 percent10=True, download_if_missing=False)","45","    except IOError:","46","        raise SkipTest(\"kddcup99 dataset can not be loaded.\")","47","","48","    assert(any(dataset.target[-100:] == b'normal.'))"],"delete":[]}]}}}