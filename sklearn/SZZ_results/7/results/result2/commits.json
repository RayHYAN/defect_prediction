{"8472350e5c2fc0a13f7900214adcfc1a69b58735":{"changes":{"sklearn\/preprocessing\/tests\/test_function_transformer.py":"MODIFY","doc\/whats_new\/v0.20.rst":"MODIFY","doc\/modules\/preprocessing.rst":"MODIFY","sklearn\/preprocessing\/_function_transformer.py":"MODIFY"},"diff":{"sklearn\/preprocessing\/tests\/test_function_transformer.py":[{"add":["1","from scipy import sparse","4","from sklearn.utils.testing import (assert_equal, assert_array_equal,","5","                                   assert_allclose_dense_sparse)","6","from sklearn.utils.testing import assert_warns_message, assert_no_warnings","130","","131","","132","def test_check_inverse():","133","    X_dense = np.array([1, 4, 9, 16], dtype=np.float64).reshape((2, 2))","134","","135","    X_list = [X_dense,","136","              sparse.csr_matrix(X_dense),","137","              sparse.csc_matrix(X_dense)]","138","","139","    for X in X_list:","140","        if sparse.issparse(X):","141","            accept_sparse = True","142","        else:","143","            accept_sparse = False","144","        trans = FunctionTransformer(func=np.sqrt,","145","                                    inverse_func=np.around,","146","                                    accept_sparse=accept_sparse,","147","                                    check_inverse=True)","148","        assert_warns_message(UserWarning,","149","                             \"The provided functions are not strictly\"","150","                             \" inverse of each other. If you are sure you\"","151","                             \" want to proceed regardless, set\"","152","                             \" 'check_inverse=False'.\",","153","                             trans.fit, X)","154","","155","        trans = FunctionTransformer(func=np.expm1,","156","                                    inverse_func=np.log1p,","157","                                    accept_sparse=accept_sparse,","158","                                    check_inverse=True)","159","        Xt = assert_no_warnings(trans.fit_transform, X)","160","        assert_allclose_dense_sparse(X, trans.inverse_transform(Xt))","161","","162","    # check that we don't check inverse when one of the func or inverse is not","163","    # provided.","164","    trans = FunctionTransformer(func=np.expm1, inverse_func=None,","165","                                check_inverse=True)","166","    assert_no_warnings(trans.fit, X_dense)","167","    trans = FunctionTransformer(func=None, inverse_func=np.expm1,","168","                                check_inverse=True)","169","    assert_no_warnings(trans.fit, X_dense)"],"delete":["3","from sklearn.utils.testing import assert_equal, assert_array_equal","4","from sklearn.utils.testing import assert_warns_message"]}],"doc\/whats_new\/v0.20.rst":[{"add":["42","","67","- A parameter ``check_inverse`` was added to :class:`FunctionTransformer`","68","  to ensure that ``func`` and ``inverse_func`` are the inverse of each","69","  other.","70","  :issue:`9399` by :user:`Guillaume Lemaitre <glemaitre>`.","71",""],"delete":["42","  "]}],"doc\/modules\/preprocessing.rst":[{"add":["612","You can ensure that ``func`` and ``inverse_func`` are the inverse of each other","613","by setting ``check_inverse=True`` and calling ``fit`` before","614","``transform``. Please note that a warning is raised and can be turned into an","615","error with a ``filterwarnings``::","616","","617","  >>> import warnings","618","  >>> warnings.filterwarnings(\"error\", message=\".*check_inverse*.\",","619","  ...                         category=UserWarning, append=False)","620",""],"delete":[]}],"sklearn\/preprocessing\/_function_transformer.py":[{"add":["4","from ..utils.testing import assert_allclose_dense_sparse","60","    check_inverse : bool, default=True","61","       Whether to check that or ``func`` followed by ``inverse_func`` leads to","62","       the original inputs. It can be used for a sanity check, raising a","63","       warning when the condition is not fulfilled.","64","","65","       .. versionadded:: 0.20","66","","75","                 accept_sparse=False, pass_y='deprecated', check_inverse=True,","82","        self.check_inverse = check_inverse","86","    def _check_inverse_transform(self, X):","87","        \"\"\"Check that func and inverse_func are the inverse.\"\"\"","88","        idx_selected = slice(None, None, max(1, X.shape[0] \/\/ 100))","89","        try:","90","            assert_allclose_dense_sparse(","91","                X[idx_selected],","92","                self.inverse_transform(self.transform(X[idx_selected])))","93","        except AssertionError:","94","            warnings.warn(\"The provided functions are not strictly\"","95","                          \" inverse of each other. If you are sure you\"","96","                          \" want to proceed regardless, set\"","97","                          \" 'check_inverse=False'.\", UserWarning)","98","","114","            X = check_array(X, self.accept_sparse)","115","        if (self.check_inverse and not (self.func is None or","116","                                        self.inverse_func is None)):","117","            self._check_inverse_transform(X)"],"delete":["21","    A FunctionTransformer will not do any checks on its function's output.","22","","69","                 accept_sparse=False, pass_y='deprecated',","94","            check_array(X, self.accept_sparse)"]}]}},"ded2276d88c4c7b6b8954cf460c855e5ca3376fc":{"changes":{"doc\/modules\/feature_extraction.rst":"MODIFY"},"diff":{"doc\/modules\/feature_extraction.rst":[{"add":["492",":math:`\\text{tf-idf}_{\\text{term2}} = 0 \\times (log(6\/1)+1) = 0`","494",":math:`\\text{tf-idf}_{\\text{term3}} = 1 \\times (log(6\/2)+1) \\approx 2.0986`","498",":math:`\\text{tf-idf}_{\\text{raw}} = [3, 0, 2.0986].`"],"delete":["492",":math:`\\text{tf-idf}_{\\text{term2}} = 0 \\times log(6\/1)+1 = 0`","494",":math:`\\text{tf-idf}_{\\text{term3}} = 1 \\times log(6\/2)+1 \\approx 2.0986`","498",":math:`\\text{tf-idf}_raw = [3, 0, 2.0986].`"]}]}},"5cdaa90e3ec481b60a7a28edcb5c547238e4ecb2":{"changes":{"sklearn\/neighbors\/binary_tree.pxi":"MODIFY","CONTRIBUTING.md":"MODIFY","sklearn\/metrics\/tests\/test_common.py":"MODIFY","sklearn\/utils\/graph_shortest_path.pyx":"MODIFY","sklearn\/metrics\/cluster\/supervised.py":"MODIFY","doc\/developers\/contributing.rst":"MODIFY"},"diff":{"sklearn\/neighbors\/binary_tree.pxi":[{"add":["1079","        # with numbers of points between leaf_size and 2 * leaf_size"],"delete":["1079","        # with numbers of points betweeen leaf_size and 2 * leaf_size"]}],"CONTRIBUTING.md":[{"add":["182","   can be found by running the following code snippet:"],"delete":["182","   can be found by runnning the following code snippet:"]}],"sklearn\/metrics\/tests\/test_common.py":[{"add":["774","def test_normalize_option_multiclass_classification():"],"delete":["774","def test_normalize_option_multiclasss_classification():"]}],"sklearn\/utils\/graph_shortest_path.pyx":[{"add":["103","        on input, graph is the matrix of distances between connected points.","166","        dist_matrix is the matrix of distances between connected points.","173","        on input, graph is the matrix of distances between connected points."],"delete":["103","        on input, graph is the matrix of distances betweeen connected points.","166","        dist_matrix is the matrix of distances betweeen connected points.","173","        on input, graph is the matrix of distances betweeen connected points."]}],"sklearn\/metrics\/cluster\/supervised.py":[{"add":["152","    Perfectly matching labelings have a score of 1 even"],"delete":["152","    Perfectly maching labelings have a score of 1 even"]}],"doc\/developers\/contributing.rst":[{"add":["334","   can be found by running the following code snippet::"],"delete":["334","   can be found by runnning the following code snippet::"]}]}},"cceb9b22ac1c36341779aff59de8e6130671c19a":{"changes":{"examples\/applications\/plot_out_of_core_classification.py":"MODIFY"},"diff":{"examples\/applications\/plot_out_of_core_classification.py":[{"add":["43","from sklearn.externals.six.moves.urllib.request import urlretrieve","174","        urlretrieve(DOWNLOAD_URL, filename=archive_path,","175","                    reporthook=progress)"],"delete":["43","from sklearn.externals.six.moves import urllib","174","        urllib.request.urlretrieve(DOWNLOAD_URL, filename=archive_path,","175","                                   reporthook=progress)"]}]}},"4dafa52eb06a40391bbc78b79887b47b19bcd100":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/linear_model\/tests\/test_logistic.py":"MODIFY","sklearn\/linear_model\/logistic.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["182","- Fixed a bug in :class:`linear_model.LogisticRegression` where when using the","183","  parameter ``multi_class='multinomial'``, the ``predict_proba`` method was","184","  returning incorrect probabilities in the case of binary outcomes.","185","  :issue:`9939` by :user:`Roger Westover <rwolst>`.","186",""],"delete":[]}],"sklearn\/linear_model\/tests\/test_logistic.py":[{"add":["200","def test_multinomial_binary_probabilities():","201","    # Test multinomial LR gives expected probabilities based on the","202","    # decision function, for a binary problem.","203","    X, y = make_classification()","204","    clf = LogisticRegression(multi_class='multinomial', solver='saga')","205","    clf.fit(X, y)","206","","207","    decision = clf.decision_function(X)","208","    proba = clf.predict_proba(X)","209","","210","    expected_proba_class_1 = (np.exp(decision) \/","211","                              (np.exp(decision) + np.exp(-decision)))","212","    expected_proba = np.c_[1-expected_proba_class_1, expected_proba_class_1]","213","","214","    assert_almost_equal(proba, expected_proba)","215","","216",""],"delete":[]}],"sklearn\/linear_model\/logistic.py":[{"add":["1103","        `coef_` is of shape (1, n_features) when the given problem is binary.","1104","        In particular, when `multi_class='multinomial'`, `coef_` corresponds","1105","        to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False).","1111","        `intercept_` is of shape (1,) when the given problem is binary.","1112","        In particular, when `multi_class='multinomial'`, `intercept_`","1113","        corresponds to outcome 1 (True) and `-intercept_` corresponds to","1114","        outcome 0 (False).","1338","        if self.multi_class == \"ovr\":","1341","            decision = self.decision_function(X)","1342","            if decision.ndim == 1:","1343","                # Workaround for multi_class=\"multinomial\" and binary outcomes","1344","                # which requires softmax prediction with only a 1D decision.","1345","                decision_2d = np.c_[-decision, decision]","1346","            else:","1347","                decision_2d = decision","1348","            return softmax(decision_2d, copy=False)"],"delete":["1103","        `coef_` is of shape (1, n_features) when the given problem","1104","        is binary.","1110","        `intercept_` is of shape(1,) when the problem is binary.","1334","        calculate_ovr = self.coef_.shape[0] == 1 or self.multi_class == \"ovr\"","1335","        if calculate_ovr:","1338","            return softmax(self.decision_function(X), copy=False)"]}]}},"09b4cfdfb28e95bd589a0dabfb95816c36286e7f":{"changes":{"doc\/faq.rst":"MODIFY"},"diff":{"doc\/faq.rst":[{"add":["26","usually a major and lengthy undertaking, it is recommended to start with","27",":ref:`known issues <new_contributors>`. Please do not contact the contributors","28","of scikit-learn directly regarding contributing to scikit-learn."],"delete":["26","usually a major and lengthy undertaking, it is recommended to start with :ref:`known","27","issues <easy_issues>`_. Please do not contact the contributors of scikit-learn directly","28","regarding contributing to scikit-learn."]}]}},"7b7960ef870ddb3633fcdf2a531c458f081e1e00":{"changes":{"sklearn\/model_selection\/_split.py":"MODIFY"},"diff":{"sklearn\/model_selection\/_split.py":[{"add":["588","            raise ValueError(\"n_splits=%d cannot be greater than the\"","589","                             \" number of members in each class.\"","594","                           \" number of members in any class cannot\""],"delete":["588","            raise ValueError(\"All the n_groups for individual classes\"","589","                             \" are less than n_splits=%d.\"","594","                           \" number of groups for any class cannot\""]}]}},"ceaa09ba291702dd304aab4538974701f86355ba":{"changes":{"sklearn\/manifold\/_barnes_hut_tsne.pyx":"MODIFY","sklearn\/manifold\/t_sne.py":"MODIFY"},"diff":{"sklearn\/manifold\/_barnes_hut_tsne.pyx":[{"add":["135","            qij = ((1.0 + dij \/ dof) ** exponent)","197","            qijZ = (1.0 + dist2s \/ dof) ** exponent  # 1\/(1+dist)"],"delete":["135","            qij = (((1.0 + dij) \/ dof) ** exponent)","197","            qijZ = ((1.0 + dist2s) \/ dof) ** exponent  # 1\/(1+dist)"]}],"sklearn\/manifold\/t_sne.py":[{"add":["161","    dist += 1."],"delete":["160","    dist += 1."]}]}},"c6fefb0caaeb433c2e11f0d8829a878c39015194":{"changes":{"sklearn\/gaussian_process\/gaussian_process.py":"MODIFY"},"diff":{"sklearn\/gaussian_process\/gaussian_process.py":[{"add":["721","                                             np.log10(theta0).ravel(),","722","                                             constraints, disp=0)"],"delete":["721","                                             np.log10(theta0).ravel(), constraints,","722","                                             iprint=0)"]}]}},"8fb648af0e673a372e52b9e7ec0e4d48b47cc83f":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/metrics\/tests\/test_common.py":"MODIFY","sklearn\/metrics\/ranking.py":"MODIFY","sklearn\/metrics\/tests\/test_ranking.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["19","- :class:`metrics.roc_auc_score` (bug fix)","109","Metrics","110","","111","- Fixed a bug due to floating point error in :func:`metrics.roc_auc_score` with","112","  non-integer sample weights. :issue:`9786` by :user:`Hanmin Qin <qinhanmin2014>`.","113",""],"delete":["60","","61",""]}],"sklearn\/metrics\/tests\/test_common.py":[{"add":["214","    \"roc_auc_score\",","215","    \"micro_roc_auc\",","216","    \"weighted_roc_auc\",","217","    \"macro_roc_auc\",","218","    \"samples_roc_auc\",","219",""],"delete":["200","    \"roc_auc_score\",","201","    \"micro_roc_auc\",","202","    \"weighted_roc_auc\",","203","    \"macro_roc_auc\",","204","    \"samples_roc_auc\",","205",""]}],"sklearn\/metrics\/ranking.py":[{"add":["272","        return auc(fpr, tpr)","313","    check_consistent_length(y_true, y_score, sample_weight)","355","        # express fps as a cumsum to ensure fps is increasing even in","356","        # the presense of floating point errors","357","        fps = stable_cumsum((1 - y_true) * weight)[threshold_idxs]"],"delete":["272","        return auc(fpr, tpr, reorder=True)","313","    check_consistent_length(y_true, y_score)","355","        fps = stable_cumsum(weight)[threshold_idxs] - tps"]}],"sklearn\/metrics\/tests\/test_ranking.py":[{"add":["373","def test_roc_curve_fpr_tpr_increasing():","374","    # Ensure that fpr and tpr returned by roc_curve are increasing.","375","    # Construct an edge case with float y_score and sample_weight","376","    # when some adjacent values of fpr and tpr are actually the same.","377","    y_true = [0, 0, 1, 1, 1]","378","    y_score = [0.1, 0.7, 0.3, 0.4, 0.5]","379","    sample_weight = np.repeat(0.2, 5)","380","    fpr, tpr, _ = roc_curve(y_true, y_score, sample_weight=sample_weight)","381","    assert_equal((np.diff(fpr) < 0).sum(), 0)","382","    assert_equal((np.diff(tpr) < 0).sum(), 0)","383","","384",""],"delete":[]}]}},"ac41281451618abee3bf3004d0b72f253840626c":{"changes":{"sklearn\/covariance\/graph_lasso_.py":"MODIFY","sklearn\/tree\/export.py":"MODIFY","sklearn\/utils\/sparsefuncs.py":"MODIFY","sklearn\/random_projection.py":"MODIFY","benchmarks\/bench_plot_randomized_svd.py":"MODIFY","benchmarks\/bench_plot_nmf.py":"MODIFY","sklearn\/utils\/mocking.py":"MODIFY","sklearn\/decomposition\/sparse_pca.py":"MODIFY","sklearn\/mixture\/gaussian_mixture.py":"MODIFY"},"diff":{"sklearn\/covariance\/graph_lasso_.py":[{"add":["326","        super(GraphLasso, self).__init__(assume_centered=assume_centered)","551","        super(GraphLassoCV, self).__init__(","552","            mode=mode, tol=tol, verbose=verbose, enet_tol=enet_tol,","553","            max_iter=max_iter, assume_centered=assume_centered)"],"delete":["332","        self.assume_centered = assume_centered","333","        # The base class needs this for the score method","334","        self.store_precision = True","555","        self.mode = mode","556","        self.tol = tol","557","        self.enet_tol = enet_tol","558","        self.max_iter = max_iter","559","        self.verbose = verbose","562","        self.assume_centered = assume_centered","563","        # The base class needs this for the score method","564","        self.store_precision = True"]}],"sklearn\/tree\/export.py":[{"add":["68","    def __repr__(self):"],"delete":["68","    def __repr__():"]}],"sklearn\/utils\/sparsefuncs.py":[{"add":["304","        inplace_swap_row_csc(X, m, n)","306","        inplace_swap_row_csr(X, m, n)","331","        inplace_swap_row_csr(X, m, n)","333","        inplace_swap_row_csc(X, m, n)"],"delete":["304","        return inplace_swap_row_csc(X, m, n)","306","        return inplace_swap_row_csr(X, m, n)","331","        return inplace_swap_row_csr(X, m, n)","333","        return inplace_swap_row_csc(X, m, n)"]}],"sklearn\/random_projection.py":[{"add":["311","    def _make_random_matrix(self, n_components, n_features):"],"delete":["311","    def _make_random_matrix(n_components, n_features):"]}],"benchmarks\/bench_plot_randomized_svd.py":[{"add":["184","        if l != \"fbpca\":","202","        if l != \"fbpca\":"],"delete":["184","        if l is not \"fbpca\":","202","        if l is not \"fbpca\":"]}],"benchmarks\/bench_plot_nmf.py":[{"add":["205","        super(_PGNMF, self).__init__(","206","            n_components=n_components, init=init, solver=solver, tol=tol,","207","            max_iter=max_iter, random_state=random_state, alpha=alpha,","208","            l1_ratio=l1_ratio)"],"delete":["206","        self.n_components = n_components","207","        self.init = init","208","        self.solver = solver","209","        self.tol = tol","210","        self.max_iter = max_iter","211","        self.random_state = random_state","212","        self.alpha = alpha","213","        self.l1_ratio = l1_ratio"]}],"sklearn\/utils\/mocking.py":[{"add":["38","    def __ne__(self, other):","39","        return not self == other","40",""],"delete":[]}],"sklearn\/decomposition\/sparse_pca.py":[{"add":["259","        super(MiniBatchSparsePCA, self).__init__(","260","            n_components=n_components, alpha=alpha, verbose=verbose,","261","            ridge_alpha=ridge_alpha, n_jobs=n_jobs, method=method,","262","            random_state=random_state)"],"delete":["259","","260","        self.n_components = n_components","261","        self.alpha = alpha","262","        self.ridge_alpha = ridge_alpha","266","        self.verbose = verbose","268","        self.n_jobs = n_jobs","269","        self.method = method","270","        self.random_state = random_state"]}],"sklearn\/mixture\/gaussian_mixture.py":[{"add":["93","    for prec in precisions:"],"delete":["93","    for k, prec in enumerate(precisions):"]}]}},"04d74bb8e3b0aa51eca3f68bb71af7c413b487c6":{"changes":{"sklearn\/ensemble\/partial_dependence.py":"MODIFY"},"diff":{"sklearn\/ensemble\/partial_dependence.py":[{"add":["55","    emp_percentiles = mquantiles(X, prob=percentiles, axis=0)"],"delete":["61","            emp_percentiles = mquantiles(X, prob=percentiles, axis=0)"]}]}},"e33e84dbf7281c4e090df176b7d82e7152a1078f":{"changes":{"examples\/applications\/plot_tomography_l1_reconstruction.py":"MODIFY","examples\/linear_model\/plot_sparse_logistic_regression_mnist.py":"MODIFY"},"diff":{"examples\/applications\/plot_tomography_l1_reconstruction.py":[{"add":["103","    mask_outer = (x - l \/ 2.) ** 2 + (y - l \/ 2.) ** 2 < (l \/ 2.) ** 2"],"delete":["103","    mask_outer = (x - l \/ 2) ** 2 + (y - l \/ 2) ** 2 < (l \/ 2) ** 2"]}],"examples\/linear_model\/plot_sparse_logistic_regression_mnist.py":[{"add":["54","clf = LogisticRegression(C=50. \/ train_samples,"],"delete":["54","clf = LogisticRegression(C=50 \/ train_samples,"]}]}},"04be1a97993342dcae7ff2736f85c5ab4eeb1266":{"changes":{"sklearn\/datasets\/lfw.py":"MODIFY","doc\/modules\/clustering.rst":"MODIFY","doc\/modules\/multiclass.rst":"MODIFY","examples\/ensemble\/plot_adaboost_hastie_10_2.py":"MODIFY","examples\/ensemble\/plot_gradient_boosting_regularization.py":"MODIFY","doc\/modules\/outlier_detection.rst":"MODIFY","doc\/modules\/ensemble.rst":"MODIFY","sklearn\/covariance\/robust_covariance.py":"MODIFY","doc\/modules\/linear_model.rst":"MODIFY","sklearn\/metrics\/scorer.py":"MODIFY","sklearn\/neighbors\/approximate.py":"MODIFY","sklearn\/model_selection\/_validation.py":"MODIFY","examples\/ensemble\/plot_adaboost_regression.py":"MODIFY","doc\/tutorial\/statistical_inference\/putting_together.rst":"MODIFY","sklearn\/mixture\/dpgmm.py":"MODIFY","examples\/ensemble\/plot_ensemble_oob.py":"MODIFY","examples\/ensemble\/plot_adaboost_multiclass.py":"MODIFY","sklearn\/linear_model\/randomized_l1.py":"MODIFY","doc\/modules\/covariance.rst":"MODIFY","doc\/modules\/calibration.rst":"MODIFY","sklearn\/neighbors\/lof.py":"MODIFY","sklearn\/model_selection\/_search.py":"MODIFY"},"diff":{"sklearn\/datasets\/lfw.py":[{"add":["70",""],"delete":[]}],"doc\/modules\/clustering.rst":[{"add":["1345"," * `V-Measure: A conditional entropy-based external cluster evaluation"],"delete":["1345"," .. [RH2007] `V-Measure: A conditional entropy-based external cluster evaluation"]}],"doc\/modules\/multiclass.rst":[{"add":["253","    * \"Pattern Recognition and Machine Learning. Springer\",","254","      Christopher M. Bishop, page 183, (First Edition)","317","    * \"Solving multiclass learning problems via error-correcting output codes\",","318","      Dietterich T., Bakiri G.,","319","      Journal of Artificial Intelligence Research 2,","320","      1995.","327","    * \"The Elements of Statistical Learning\",","328","      Hastie T., Tibshirani R., Friedman J., page 606 (second-edition)","329","      2008."],"delete":["253","    .. [1] \"Pattern Recognition and Machine Learning. Springer\",","254","        Christopher M. Bishop, page 183, (First Edition)","317","    .. [2] \"Solving multiclass learning problems via error-correcting output codes\",","318","        Dietterich T., Bakiri G.,","319","        Journal of Artificial Intelligence Research 2,","320","        1995.","327","    .. [4] \"The Elements of Statistical Learning\",","328","        Hastie T., Tibshirani R., Friedman J., page 606 (second-edition)","329","        2008."]}],"examples\/ensemble\/plot_adaboost_hastie_10_2.py":[{"add":["5","This example is based on Figure 10.2 from Hastie et al 2009 [1]_ and","6","illustrates the difference in performance between the discrete SAMME [2]_","7","boosting algorithm and real SAMME.R boosting algorithm. Both algorithms are","8","evaluated on a binary classification task where the target Y is a non-linear","9","function of 10 input features."],"delete":["5","This example is based on Figure 10.2 from Hastie et al 2009 [1] and illustrates","6","the difference in performance between the discrete SAMME [2] boosting","7","algorithm and real SAMME.R boosting algorithm. Both algorithms are evaluated","8","on a binary classification task where the target Y is a non-linear function","9","of 10 input features."]}],"examples\/ensemble\/plot_gradient_boosting_regularization.py":[{"add":["6","for Gradient Boosting. The example is taken from Hastie et al 2009 [1]_."],"delete":["6","for Gradient Boosting. The example is taken from Hastie et al 2009."]}],"doc\/modules\/outlier_detection.rst":[{"add":["128","    * Rousseeuw, P.J., Van Driessen, K. \"A fast algorithm for the minimum","129","      covariance determinant estimator\" Technometrics 41(3), 212 (1999)","174","    * Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. \"Isolation forest.\"","175","      Data Mining, 2008. ICDM'08. Eighth IEEE International Conference on.","230","   *  Breunig, Kriegel, Ng, and Sander (2000)","274","        and :class:`neighbors.LocalOutlierFactor` perform as well.","281","        :class:`svm.OneClassSVM` and :class:`neighbors.LocalOutlierFactor`","282","        have difficulties to detect the two modes,","283","        and that the :class:`svm.OneClassSVM`","294","        whereas the :class:`covariance.EllipticEnvelope` completely fails."],"delete":["128","    ..  [RD1999] Rousseeuw, P.J., Van Driessen, K. \"A fast algorithm for the minimum","129","        covariance determinant estimator\" Technometrics 41(3), 212 (1999)","174","    .. [LTZ2008] Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. \"Isolation forest.\"","175","           Data Mining, 2008. ICDM'08. Eighth IEEE International Conference on.","230","   .. [BKNS2000]  Breunig, Kriegel, Ng, and Sander (2000)","274","\tand :class:`neighbors.LocalOutlierFactor` perform as well.","281","\t:class:`svm.OneClassSVM` and :class:`neighbors.LocalOutlierFactor`","282","\thave difficulties to detect the two modes,","283","\tand that the :class:`svm.OneClassSVM`","294","\twhereas the :class:`covariance.EllipticEnvelope` completely fails."]}],"doc\/modules\/ensemble.rst":[{"add":["248"," * P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized"],"delete":["248"," .. [GEW2006] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized"]}],"sklearn\/covariance\/robust_covariance.py":[{"add":["192","    [RV]_.","252","    .. [RV] A Fast Algorithm for the Minimum Covariance Determinant","341","    the correction and reweighting steps described in [RouseeuwVan]_,","347","    .. [RouseeuwVan] A Fast Algorithm for the Minimum Covariance","582","    .. [Rousseeuw] `A Fast Algorithm for the Minimum Covariance Determinant","585","    .. [ButlerDavies] `R. W. Butler, P. L. Davies and M. Jhun,","652","        by Rousseeuw and Van Driessen in [RVD]_.","661","        References","662","        ----------","663","","664","        .. [RVD] `A Fast Algorithm for the Minimum Covariance","665","            Determinant Estimator, 1999, American Statistical Association","666","            and the American Society for Quality, TECHNOMETRICS`","667","","684","        computing location and covariance estimates) described","685","        in [RVDriessen]_.","694","        References","695","        ----------","696","","697","        .. [RVDriessen] `A Fast Algorithm for the Minimum Covariance","698","            Determinant Estimator, 1999, American Statistical Association","699","            and the American Society for Quality, TECHNOMETRICS`","700",""],"delete":["192","    [Rouseeuw1999]_.","252","    .. [Rouseeuw1999] A Fast Algorithm for the Minimum Covariance Determinant","341","    the correction and reweighting steps described in [Rouseeuw1999]_,","347","    .. [Rouseeuw1999] A Fast Algorithm for the Minimum Covariance","582","    .. [Rouseeuw1999] `A Fast Algorithm for the Minimum Covariance Determinant","585","    .. [Butler1993] `R. W. Butler, P. L. Davies and M. Jhun,","652","        by Rousseeuw and Van Driessen in [Rouseeuw1984]_.","677","        computing location and covariance estimates). [Rouseeuw1984]_"]}],"doc\/modules\/linear_model.rst":[{"add":["1143","  * Peter J. Huber, Elvezio M. Ronchetti: Robust Statistics, Concomitant scale estimates, pg 172"],"delete":["1143","    .. [#f1] Peter J. Huber, Elvezio M. Ronchetti: Robust Statistics, Concomitant scale estimates, pg 172"]}],"sklearn\/metrics\/scorer.py":[{"add":["322","        See :ref:`multimetric_grid_search` for an example."],"delete":["322","        See :ref:`multivalued_scorer_wrapping` for an example."]}],"sklearn\/neighbors\/approximate.py":[{"add":[],"delete":["124","    Read more in the :ref:`User Guide <approximate_nearest_neighbors>`.","125",""]}],"sklearn\/model_selection\/_validation.py":[{"add":["71","        See :ref:`multimetric_grid_search` for an example.","805","        A single string (see :ref:`scoring_parameter`) or a callable","806","        (see :ref:`scoring`) to evaluate the predictions on the test set."],"delete":["71","        See :ref:`multivalued_scorer_wrapping` for an example.","805","        A single string (see :ref:`_scoring_parameter`) or a callable","806","        (see :ref:`_scoring`) to evaluate the predictions on the test set."]}],"examples\/ensemble\/plot_adaboost_regression.py":[{"add":["5","A decision tree is boosted using the AdaBoost.R2 [1]_ algorithm on a 1D"],"delete":["5","A decision tree is boosted using the AdaBoost.R2 [1] algorithm on a 1D"]}],"doc\/tutorial\/statistical_inference\/putting_together.rst":[{"add":["19","    :lines: 23-63"],"delete":["19","    :lines: 26-66"]}],"sklearn\/mixture\/dpgmm.py":[{"add":["674","    Read more in the :ref:`User Guide <bgmm>`."],"delete":["674","    Read more in the :ref:`User Guide <vbgmm>`."]}],"examples\/ensemble\/plot_ensemble_oob.py":[{"add":["10","``RandomForestClassifier`` to be fit and validated whilst being trained [1]_."],"delete":["10","``RandomForestClassifier`` to be fit and validated whilst being trained [1]."]}],"examples\/ensemble\/plot_adaboost_multiclass.py":[{"add":["5","This example reproduces Figure 1 of Zhu et al [1]_ and shows how boosting can","12","The performance of the SAMME and SAMME.R [1]_ algorithms are compared. SAMME.R"],"delete":["5","This example reproduces Figure 1 of Zhu et al [1] and shows how boosting can","12","The performance of the SAMME and SAMME.R [1] algorithms are compared. SAMME.R"]}],"sklearn\/linear_model\/randomized_l1.py":[{"add":["206","        features.","413","        features."],"delete":["197","    Read more in the :ref:`User Guide <randomized_l1>`.","198","","208","        features (See :ref:`User Guide <randomized_l1>` for details ).","302","    Notes","303","    -----","304","    For an example, see :ref:`examples\/linear_model\/plot_sparse_recovery.py","305","    <sphx_glr_auto_examples_linear_model_plot_sparse_recovery.py>`.","306","","409","    Read more in the :ref:`User Guide <randomized_l1>`.","410","","422","        features (See :ref:`User Guide <randomized_l1>` for details ).","503","    Notes","504","    -----","505","    For an example, see :ref:`examples\/linear_model\/plot_sparse_recovery.py","506","    <sphx_glr_auto_examples_linear_model_plot_sparse_recovery.py>`.","507","","592","    Read more in the :ref:`User Guide <randomized_l1>`.","593","","640","","641","    Notes","642","    -----","643","    For an example, see :ref:`examples\/linear_model\/plot_sparse_recovery.py","644","    <sphx_glr_auto_examples_linear_model_plot_sparse_recovery.py>`."]}],"doc\/modules\/covariance.rst":[{"add":["97","In their 2004 paper [1]_, O. Ledoit and M. Wolf propose a formula so as","114",".. topic:: References:","116","    .. [1] O. Ledoit and M. Wolf, \"A Well-Conditioned Estimator for Large-Dimensional","117","           Covariance Matrices\", Journal of Multivariate Analysis, Volume 88, Issue 2,","118","           February 2004, pages 365-411.","126","al. [2]_ derived a formula aimed at choosing a shrinkage coefficient that","144",".. topic:: References:","145","","146","    .. [2] Chen et al., \"Shrinkage Algorithms for MMSE Covariance Estimation\",","147","           IEEE Trans. on Sign. Proc., Volume 58, Issue 10, October 2010.","271","the Minimum Covariance Determinant [3]_.","278","a data set's covariance introduced by P.J. Rousseeuw in [3]_.  The idea","288","Rousseeuw and Van Driessen [4]_ developed the FastMCD algorithm in order","297",".. topic:: References:","298","","299","    .. [3] P. J. Rousseeuw. Least median of squares regression.","300","           J. Am Stat Ass, 79:871, 1984.","301","    .. [4] A Fast Algorithm for the Minimum Covariance Determinant Estimator,","302","           1999, American Statistical Association and the American Society","303","           for Quality, TECHNOMETRICS."],"delete":["97","In their 2004 paper [1], O. Ledoit and M. Wolf propose a formula so as","115","[1] O. Ledoit and M. Wolf, \"A Well-Conditioned Estimator for Large-Dimensional","116","    Covariance Matrices\", Journal of Multivariate Analysis, Volume 88, Issue 2,","117","    February 2004, pages 365-411.","125","al. [2] derived a formula aimed at choosing a shrinkage coefficient that","143","[2] Chen et al., \"Shrinkage Algorithms for MMSE Covariance Estimation\",","144","    IEEE Trans. on Sign. Proc., Volume 58, Issue 10, October 2010.","268","the Minimum Covariance Determinant [3].","275","a data set's covariance introduced by P.J. Rousseeuw in [3].  The idea","285","Rousseeuw and Van Driessen [4] developed the FastMCD algorithm in order","294","[3] P. J. Rousseeuw. Least median of squares regression.","295","    J. Am Stat Ass, 79:871, 1984.","296","[4] A Fast Algorithm for the Minimum Covariance Determinant Estimator,","297","    1999, American Statistical Association and the American Society","298","    for Quality, TECHNOMETRICS."]}],"doc\/modules\/calibration.rst":[{"add":["46","   and Caruana [4]_: \"Methods such as bagging and random forests that average","59","   calibration curve also referred to as the reliability diagram (Wilks 1995 [5]_) shows a","67","   (compare Niculescu-Mizil and Caruana [4]_), which focus on hard samples","192","    * Obtaining calibrated probability estimates from decision trees","193","      and naive Bayesian classifiers, B. Zadrozny & C. Elkan, ICML 2001","195","    * Transforming Classifier Scores into Accurate Multiclass","196","      Probability Estimates, B. Zadrozny & C. Elkan, (KDD 2002)","198","    * Probabilistic Outputs for Support Vector Machines and Comparisons to","199","      Regularized Likelihood Methods, J. Platt, (1999)","202","           A. Niculescu-Mizil & R. Caruana, ICML 2005","205","           consecutive precipitation periods. Wea. Forecasting, 5, 640\u2013650.,","206","           Wilks, D. S., 1990a"],"delete":["46","   and Caruana [4]: \"Methods such as bagging and random forests that average","59","   calibration curve also referred to as the reliability diagram (Wilks 1995[5]) shows a","67","   (compare Niculescu-Mizil and Caruana [4]), which focus on hard samples","192","    .. [1] Obtaining calibrated probability estimates from decision trees","193","          and naive Bayesian classifiers, B. Zadrozny & C. Elkan, ICML 2001","195","    .. [2] Transforming Classifier Scores into Accurate Multiclass","196","          Probability Estimates, B. Zadrozny & C. Elkan, (KDD 2002)","198","    .. [3] Probabilistic Outputs for Support Vector Machines and Comparisons to","199","          Regularized Likelihood Methods, J. Platt, (1999)","202","          A. Niculescu-Mizil & R. Caruana, ICML 2005","205","         consecutive precipitation periods. Wea. Forecasting, 5, 640\u2013","206","         650., Wilks, D. S., 1990a"]}],"sklearn\/neighbors\/lof.py":[{"add":["87","        :func:`sklearn.metrics.pairwise.pairwise_distances`. When p = 1, this","88","        is equivalent to using manhattan_distance (l1), and euclidean_distance"],"delete":["87","        :ref:`sklearn.metrics.pairwise.pairwise_distances`. When p = 1, this is","88","        equivalent to using manhattan_distance (l1), and euclidean_distance"]}],"sklearn\/model_selection\/_search.py":[{"add":["803","        See :ref:`multimetric_grid_search` for an example.","1113","        See :ref:`multimetric_grid_search` for an example."],"delete":["803","        See :ref:`multivalued_scorer_wrapping` for an example.","1113","        See :ref:`multivalued_scorer_wrapping` for an example."]}]}},"334850fa2b4c6d7881c2e4adbaeb6235c94638a3":{"changes":{"sklearn\/model_selection\/_split.py":"MODIFY","examples\/multioutput\/README.txt":"MODIFY"},"diff":{"sklearn\/model_selection\/_split.py":[{"add":["86","        Notes","87","        -----","317","        Notes","318","        -----","657","        Notes","658","        -----","744","        Notes","745","        -----","1188","        Notes","1189","        -----","1608","        Notes","1609","        -----"],"delete":["86","        Note","87","        ----","317","        Note","318","        ----","657","        Note","658","        ----","744","        Note","745","        ----","1188","        Note","1189","        ----","1608","        Note","1609","        ----"]}],"examples\/multioutput\/README.txt":[{"add":["3","-------------------","5","Examples concerning the :mod:`sklearn.multioutput` module."],"delete":["3","----------------","5","Examples concerning the :mod:`sklearn.multioutput` module."]}]}},"a6753f3ed38d25cec0af8ed95697f48eaacaed24":{"changes":{"doc\/whats_new.rst":"MODIFY","sklearn\/linear_model\/tests\/test_ridge.py":"MODIFY","sklearn\/linear_model\/ridge.py":"MODIFY"},"diff":{"doc\/whats_new.rst":[{"add":["476","   - Fix inconsistent results between :class:`linear_model.RidgeCV`","477","     and :class:`linear_model.Ridge` when using ``normalize=True``","478","     by `Alexandre Gramfort`_.","479",""],"delete":["473",""]}],"sklearn\/linear_model\/tests\/test_ridge.py":[{"add":["385","def _test_ridge_cv_normalize(filter_):","386","    ridge_cv = RidgeCV(normalize=True, cv=3)","387","    ridge_cv.fit(filter_(10. * X_diabetes), y_diabetes)","388","","389","    gs = GridSearchCV(Ridge(normalize=True), cv=3,","390","                      param_grid={'alpha': ridge_cv.alphas})","391","    gs.fit(filter_(10. * X_diabetes), y_diabetes)","392","    assert_equal(gs.best_estimator_.alpha, ridge_cv.alpha_)","393","","394","","474","                      _test_ridge_cv_normalize,"],"delete":[]}],"sklearn\/linear_model\/ridge.py":[{"add":["1121","            gs = GridSearchCV(Ridge(fit_intercept=self.fit_intercept,","1122","                                    normalize=self.normalize),"],"delete":["1121","            gs = GridSearchCV(Ridge(fit_intercept=self.fit_intercept),"]}]}},"ecc96be8c5e831fd0a12f3274ed4a31dabcbffe6":{"changes":{"sklearn\/utils\/validation.py":"MODIFY","sklearn\/utils\/estimator_checks.py":"MODIFY","sklearn\/utils\/tests\/test_validation.py":"MODIFY"},"diff":{"sklearn\/utils\/validation.py":[{"add":["15","from numpy.core.numeric import ComplexWarning","310","def _ensure_no_complex_data(array):","311","    if hasattr(array, 'dtype') and array.dtype is not None \\","312","            and hasattr(array.dtype, 'kind') and array.dtype.kind == \"c\":","313","        raise ValueError(\"Complex data not supported\\n\"","314","                         \"{}\\n\".format(array))","315","","316","","437","        _ensure_no_complex_data(array)","441","        # If np.array(..) gives ComplexWarning, then we convert the warning","442","        # to an error. This is needed because specifying a non complex","443","        # dtype to the function converts complex to real dtype,","444","        # thereby passing the test made in the lines following the scope","445","        # of warnings context manager.","446","        with warnings.catch_warnings():","447","            try:","448","                warnings.simplefilter('error', ComplexWarning)","449","                array = np.array(array, dtype=dtype, order=order, copy=copy)","450","            except ComplexWarning:","451","                raise ValueError(\"Complex data not supported\\n\"","452","                                 \"{}\\n\".format(array))","453","","454","        # It is possible that the np.array(..) gave no warning. This happens","455","        # when no dtype conversion happend, for example dtype = None. The","456","        # result is that np.array(..) produces an array of complex dtype","457","        # and we need to catch and raise exception for such cases.","458","        _ensure_no_complex_data(array)"],"delete":["432","        array = np.array(array, dtype=dtype, order=order, copy=copy)"]}],"sklearn\/utils\/estimator_checks.py":[{"add":["78","    yield check_complex_data","461","def check_complex_data(name, estimator_orig):","462","    # check that estimators raise an exception on providing complex data","463","    X = np.random.sample(10) + 1j * np.random.sample(10)","464","    X = X.reshape(-1, 1)","465","    y = np.random.sample(10) + 1j * np.random.sample(10)","466","    estimator = clone(estimator_orig)","467","    assert_raises_regex(ValueError, \"Complex data not supported\",","468","                        estimator.fit, X, y)","469","","470",""],"delete":[]}],"sklearn\/utils\/tests\/test_validation.py":[{"add":["439","def test_check_array_complex_data_error():","440","    # np array","441","    X = np.array([[1 + 2j, 3 + 4j, 5 + 7j], [2 + 3j, 4 + 5j, 6 + 7j]])","442","    assert_raises_regexp(","443","        ValueError, \"Complex data not supported\", check_array, X)","444","","445","    # list of lists","446","    X = [[1 + 2j, 3 + 4j, 5 + 7j], [2 + 3j, 4 + 5j, 6 + 7j]]","447","    assert_raises_regexp(","448","        ValueError, \"Complex data not supported\", check_array, X)","449","","450","    # tuple of tuples","451","    X = ((1 + 2j, 3 + 4j, 5 + 7j), (2 + 3j, 4 + 5j, 6 + 7j))","452","    assert_raises_regexp(","453","        ValueError, \"Complex data not supported\", check_array, X)","454","","455","    # list of np arrays","456","    X = [np.array([1 + 2j, 3 + 4j, 5 + 7j]),","457","         np.array([2 + 3j, 4 + 5j, 6 + 7j])]","458","    assert_raises_regexp(","459","        ValueError, \"Complex data not supported\", check_array, X)","460","","461","    # tuple of np arrays","462","    X = (np.array([1 + 2j, 3 + 4j, 5 + 7j]),","463","         np.array([2 + 3j, 4 + 5j, 6 + 7j]))","464","    assert_raises_regexp(","465","        ValueError, \"Complex data not supported\", check_array, X)","466","","467","    # dataframe","468","    X = MockDataFrame(","469","        np.array([[1 + 2j, 3 + 4j, 5 + 7j], [2 + 3j, 4 + 5j, 6 + 7j]]))","470","    assert_raises_regexp(","471","        ValueError, \"Complex data not supported\", check_array, X)","472","","473","    # sparse matrix","474","    X = sp.coo_matrix([[0, 1 + 2j], [0, 0]])","475","    assert_raises_regexp(","476","        ValueError, \"Complex data not supported\", check_array, X)","477","","478",""],"delete":[]}]}},"370b642f7ae44d4157001d09d3a4cf1a2b990847":{"changes":{"sklearn\/neural_network\/multilayer_perceptron.py":"MODIFY","sklearn\/neural_network\/tests\/test_mlp.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY"},"diff":{"sklearn\/neural_network\/multilayer_perceptron.py":[{"add":["53","                 n_iter_no_change, max_fun):","77","        self.max_fun = max_fun","383","        if self.max_fun <= 0:","384","            raise ValueError(\"max_fun must be > 0, got %s.\" % self.max_fun)","463","            maxfun=self.max_fun,","464","            maxiter=self.max_iter,","468","        self.n_iter_ = d['nit']","469","        if d['warnflag'] == 1:","470","            if d['nit'] >= self.max_iter:","471","                warnings.warn(","472","                    \"LBFGS Optimizer: Maximum iterations (%d) \"","473","                    \"reached and the optimization hasn't converged yet.\"","474","                    % self.max_iter, ConvergenceWarning)","475","            if d['funcalls'] >= self.max_fun:","476","                warnings.warn(","477","                    \"LBFGS Optimizer: Maximum function evaluations (%d) \"","478","                    \"reached and the optimization hasn't converged yet.\"","479","                    % self.max_fun, ConvergenceWarning)","480","        elif d['warnflag'] == 2:","481","            warnings.warn(","482","                \"LBFGS Optimizer: Optimization hasn't converged yet, \"","483","                \"cause of LBFGS stopping: %s.\"","484","                % d['task'], ConvergenceWarning)","485","","856","    max_fun : int, optional, default 15000","857","        Only used when solver='lbfgs'. Maximum number of loss function calls.","858","        The solver iterates until convergence (determined by 'tol'), number","859","        of iterations reaches max_iter, or this number of loss function calls.","860","        Note that number of loss function calls will be greater than or equal","861","        to the number of iterations for the `MLPClassifier`.","862","","863","        .. versionadded:: 0.22","864","","930","                 epsilon=1e-8, n_iter_no_change=10, max_fun=15000):","943","            n_iter_no_change=n_iter_no_change, max_fun=max_fun)","1247","    max_fun : int, optional, default 15000","1248","        Only used when solver='lbfgs'. Maximum number of function calls.","1249","        The solver iterates until convergence (determined by 'tol'), number","1250","        of iterations reaches max_iter, or this number of function calls.","1251","        Note that number of function calls will be greater than or equal to","1252","        the number of iterations for the MLPRegressor.","1253","","1254","        .. versionadded:: 0.22","1255","","1319","                 epsilon=1e-8, n_iter_no_change=10, max_fun=15000):","1332","            n_iter_no_change=n_iter_no_change, max_fun=max_fun)"],"delete":["53","                 n_iter_no_change):","174","        self.n_iter_ += 1","461","            maxfun=self.max_iter,","900","                 epsilon=1e-8, n_iter_no_change=10):","901","","914","            n_iter_no_change=n_iter_no_change)","1281","                 epsilon=1e-8, n_iter_no_change=10):","1282","","1295","            n_iter_no_change=n_iter_no_change)"]}],"sklearn\/neural_network\/tests\/test_mlp.py":[{"add":["50","regression_datasets = [(Xboston, yboston)]","51","","232","@pytest.mark.parametrize('X,y', classification_datasets)","233","def test_lbfgs_classification(X, y):","237","    X_train = X[:150]","238","    y_train = y[:150]","239","    X_test = X[150:]","240","    expected_shape_dtype = (X_test.shape[0], y_train.dtype.kind)","242","    for activation in ACTIVATION_TYPES:","243","        mlp = MLPClassifier(solver='lbfgs', hidden_layer_sizes=50,","244","                            max_iter=150, shuffle=True, random_state=1,","245","                            activation=activation)","246","        mlp.fit(X_train, y_train)","247","        y_predict = mlp.predict(X_test)","248","        assert mlp.score(X_train, y_train) > 0.95","249","        assert ((y_predict.shape[0], y_predict.dtype.kind) ==","250","                expected_shape_dtype)","253","@pytest.mark.parametrize('X,y', regression_datasets)","254","def test_lbfgs_regression(X, y):","268","@pytest.mark.parametrize('X,y', classification_datasets)","269","def test_lbfgs_classification_maxfun(X, y):","270","    # Test lbfgs parameter max_fun.","271","    # It should independently limit the number of iterations for lbfgs.","272","    max_fun = 10","273","    # classification tests","274","    for activation in ACTIVATION_TYPES:","275","        mlp = MLPClassifier(solver='lbfgs', hidden_layer_sizes=50,","276","                            max_iter=150, max_fun=max_fun, shuffle=True,","277","                            random_state=1, activation=activation)","278","        with pytest.warns(ConvergenceWarning):","279","            mlp.fit(X, y)","280","            assert max_fun >= mlp.n_iter_","281","","282","","283","@pytest.mark.parametrize('X,y', regression_datasets)","284","def test_lbfgs_regression_maxfun(X, y):","285","    # Test lbfgs parameter max_fun.","286","    # It should independently limit the number of iterations for lbfgs.","287","    max_fun = 10","288","    # regression tests","289","    for activation in ACTIVATION_TYPES:","290","        mlp = MLPRegressor(solver='lbfgs', hidden_layer_sizes=50,","291","                           max_iter=150, max_fun=max_fun, shuffle=True,","292","                           random_state=1, activation=activation)","293","        with pytest.warns(ConvergenceWarning):","294","            mlp.fit(X, y)","295","            assert max_fun >= mlp.n_iter_","296","","297","    mlp.max_fun = -1","298","    assert_raises(ValueError, mlp.fit, X, y)","299","","300",""],"delete":["230","def test_lbfgs_classification():","234","    for X, y in classification_datasets:","235","        X_train = X[:150]","236","        y_train = y[:150]","237","        X_test = X[150:]","239","        expected_shape_dtype = (X_test.shape[0], y_train.dtype.kind)","240","","241","        for activation in ACTIVATION_TYPES:","242","            mlp = MLPClassifier(solver='lbfgs', hidden_layer_sizes=50,","243","                                max_iter=150, shuffle=True, random_state=1,","244","                                activation=activation)","245","            mlp.fit(X_train, y_train)","246","            y_predict = mlp.predict(X_test)","247","            assert mlp.score(X_train, y_train) > 0.95","248","            assert ((y_predict.shape[0], y_predict.dtype.kind) ==","249","                         expected_shape_dtype)","252","def test_lbfgs_regression():","254","    X = Xboston","255","    y = yboston"]}],"doc\/whats_new\/v0.22.rst":[{"add":["124","","125",":mod:`sklearn.neural_network`","126",".............................","127","","128","- |Feature| Add `max_fun` parameter in","129","  :class:`neural_network.BaseMultilayerPerceptron`,","130","  :class:`neural_network.MLPRegressor`, and","131","  :class:`neural_network.MLPClassifier` to give control over","132","  maximum number of function evaluation to not meet ``tol`` improvement.","133","  :issue:`9274` by :user:`Daniel Perry <daniel-perry>`.","134","","135",""],"delete":[]}]}},"8cee70d018722250929ca50ccef3e061ad8b40c5":{"changes":{"sklearn\/neighbors\/dist_metrics.pyx":"MODIFY"},"diff":{"sklearn\/neighbors\/dist_metrics.pyx":[{"add":["1095","    # in cython < 0.26, GIL was required to be acquired during definition of","1096","    # the function and inside the body of the function. This behaviour is not","1097","    # allowed in cython >= 0.26 since it is a redundant GIL acquisition. The","1098","    # only way to be back compatible is to inherit `dist` from the base class","1099","    # without GIL and called an inline `_dist` which acquire GIL.","1101","                             ITYPE_t size) nogil except -1:","1102","        return self._dist(x1, x2, size)","1103","","1104","    cdef inline DTYPE_t _dist(self, DTYPE_t* x1, DTYPE_t* x2,","1105","                              ITYPE_t size) except -1 with gil:","1108","        x1arr = _buffer_to_ndarray(x1, size)","1109","        x2arr = _buffer_to_ndarray(x2, size)","1110","        d = self.func(x1arr, x2arr, **self.kwargs)","1111","        try:","1112","            # Cython generates code here that results in a TypeError","1113","            # if d is the wrong type.","1114","            return d","1115","        except TypeError:","1116","            raise TypeError(\"Custom distance function must accept two \"","1117","                            \"vectors and return a float.\")"],"delete":["1096","                             ITYPE_t size) except -1 with gil:","1099","        with gil:","1100","            x1arr = _buffer_to_ndarray(x1, size)","1101","            x2arr = _buffer_to_ndarray(x2, size)","1102","            d = self.func(x1arr, x2arr, **self.kwargs)","1103","            try:","1104","                # Cython generates code here that results in a TypeError","1105","                # if d is the wrong type.","1106","                return d","1107","            except TypeError:","1108","                raise TypeError(\"Custom distance function must accept two \"","1109","                                \"vectors and return a float.\")","1110","            "]}]}},"7f19dbeb7566944aca64f45d18bc967e50e8a4e5":{"changes":{"benchmarks\/bench_lof.py":"MODIFY","benchmarks\/bench_isolation_forest.py":"MODIFY"},"diff":{"benchmarks\/bench_lof.py":[{"add":["7","Note that LocalOutlierFactor is not meant to predict on a test set and its","8","performance is assessed in an outlier detection context:","9","1. The model is trained on the whole dataset which is assumed to contain","10","outliers.","11","2. The ROC curve is computed on the same dataset using the knowledge of the","12","labels.","13","In this context there is no need to shuffle the dataset because the model","14","is trained and tested on the whole dataset. The randomness of this benchmark","15","is only caused by the random selection of anomalies in the SA dataset.","16","","29","random_state = 2  # to control the random selection of anomalies in SA","32","datasets = ['http', 'smtp', 'SA', 'SF', 'shuttle', 'forestcover']","34","plt.figure()","39","        dataset = fetch_kddcup99(subset=dataset_name, percent10=True,","40","                                 random_state=random_state)","56","        dataset = fetch_covtype()","70","        x1 = lb.fit_transform(X[:, 1].astype(str))","72","        y = (y != b'normal.').astype(int)","76","        x1 = lb.fit_transform(X[:, 1].astype(str))","77","        x2 = lb.fit_transform(X[:, 2].astype(str))","78","        x3 = lb.fit_transform(X[:, 3].astype(str))","80","        y = (y != b'normal.').astype(int)","83","        y = (y != b'normal.').astype(int)","90","    model.fit(X)","92","    scoring = -model.negative_outlier_factor_  # the lower, the more normal","93","    fpr, tpr, thresholds = roc_curve(y, scoring)","96","             label=('ROC for %s (area = %0.3f, train-time: %0.2fs)'","97","                    % (dataset_name, AUC, fit_time)))"],"delete":["16","from sklearn.utils import shuffle as sh","20","np.random.seed(2)","23","datasets = ['shuttle']","25","novelty_detection = True  # if False, training set polluted by outliers","26","","31","        dataset = fetch_kddcup99(subset=dataset_name, shuffle=True,","32","                                 percent10=False)","40","        X, y = sh(X, y)","49","        dataset = fetch_covtype(shuffle=True)","63","        lb.fit(X[:, 1])","64","        x1 = lb.transform(X[:, 1])","66","        y = (y != 'normal.').astype(int)","70","        lb.fit(X[:, 1])","71","        x1 = lb.transform(X[:, 1])","72","        lb.fit(X[:, 2])","73","        x2 = lb.transform(X[:, 2])","74","        lb.fit(X[:, 3])","75","        x3 = lb.transform(X[:, 3])","77","        y = (y != 'normal.').astype(int)","80","        y = (y != 'normal.').astype(int)","81","","82","    n_samples, n_features = np.shape(X)","83","    n_samples_train = n_samples \/\/ 2","84","    n_samples_test = n_samples - n_samples_train","87","    X_train = X[:n_samples_train, :]","88","    X_test = X[n_samples_train:, :]","89","    y_train = y[:n_samples_train]","90","    y_test = y[n_samples_train:]","91","","92","    if novelty_detection:","93","        X_train = X_train[y_train == 0]","94","        y_train = y_train[y_train == 0]","99","    model.fit(X_train)","101","    tstart = time()","102","","103","    scoring = -model.decision_function(X_test)  # the lower, the more normal","104","    predict_time = time() - tstart","105","    fpr, tpr, thresholds = roc_curve(y_test, scoring)","108","             label=('ROC for %s (area = %0.3f, train-time: %0.2fs,'","109","                    'test-time: %0.2fs)' % (dataset_name, AUC, fit_time,","110","                                            predict_time)))"]}],"benchmarks\/bench_isolation_forest.py":[{"add":["5","","6","The benchmark is run as follows:","7","1. The dataset is randomly split into a training set and a test set, both","8","assumed to contain outliers.","9","2. Isolation Forest is trained on the training set.","10","3. The ROC curve is computed on the test set using the knowledge of the labels.","11","","12","Note that the smtp dataset contains a very small proportion of outliers.","13","Therefore, depending on the seed of the random number generator, randomly","14","splitting the data set might lead to a test set containing no outliers. In this","15","case a warning is raised when computing the ROC curve.","25","from sklearn.preprocessing import LabelBinarizer","43","random_state = 1","49","# datasets available = ['http', 'smtp', 'SA', 'SF', 'shuttle', 'forestcover']","50","datasets = ['http', 'smtp', 'SA', 'SF', 'shuttle', 'forestcover']","59","        dataset = fetch_kddcup99(subset=dat, shuffle=True,","60","                                 percent10=True, random_state=random_state)","68","        X, y = sh(X, y, random_state=random_state)","78","        dataset = fetch_covtype(shuffle=True, random_state=random_state)","92","        lb = LabelBinarizer()","93","        x1 = lb.fit_transform(X[:, 1].astype(str))","99","        lb = LabelBinarizer()","100","        x1 = lb.fit_transform(X[:, 1].astype(str))","101","        x2 = lb.fit_transform(X[:, 2].astype(str))","102","        x3 = lb.fit_transform(X[:, 3].astype(str))","121","    model = IsolationForest(n_jobs=-1, random_state=random_state)"],"delete":["14","from sklearn.preprocessing import MultiLabelBinarizer","32","np.random.seed(1)","38","# Removed the shuttle dataset because as of 2017-03-23 mldata.org is down:","39","# datasets = ['http', 'smtp', 'SA', 'SF', 'shuttle', 'forestcover']","40","datasets = ['http', 'smtp', 'SA', 'SF', 'forestcover']","49","        dataset = fetch_kddcup99(subset=dat, shuffle=True, percent10=True)","57","        X, y = sh(X, y)","67","        dataset = fetch_covtype(shuffle=True)","81","        lb = MultiLabelBinarizer()","82","        x1 = lb.fit_transform(X[:, 1])","88","        lb = MultiLabelBinarizer()","89","        x1 = lb.fit_transform(X[:, 1])","90","        x2 = lb.fit_transform(X[:, 2])","91","        x3 = lb.fit_transform(X[:, 3])","110","    model = IsolationForest(n_jobs=-1)"]}]}},"5271a193c3fe682b359477698ae7f87741740463":{"changes":{"sklearn\/linear_model\/ransac.py":"MODIFY","sklearn\/feature_extraction\/hashing.py":"MODIFY","examples\/applications\/plot_topics_extraction_with_nmf_lda.py":"MODIFY","sklearn\/covariance\/graph_lasso_.py":"MODIFY","sklearn\/mixture\/bayesian_mixture.py":"MODIFY","doc\/faq.rst":"MODIFY","sklearn\/covariance\/tests\/test_graph_lasso.py":"MODIFY","sklearn\/model_selection\/_split.py":"MODIFY","sklearn\/linear_model\/stochastic_gradient.py":"MODIFY","sklearn\/model_selection\/_validation.py":"MODIFY","sklearn\/ensemble\/iforest.py":"MODIFY","doc\/tutorial\/statistical_inference\/putting_together.rst":"MODIFY","doc\/tutorial\/statistical_inference\/supervised_learning.rst":"MODIFY","sklearn\/utils\/validation.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/utils\/extmath.py":"MODIFY","sklearn\/manifold\/t_sne.py":"MODIFY","sklearn\/linear_model\/bayes.py":"MODIFY","sklearn\/mixture\/gaussian_mixture.py":"MODIFY","sklearn\/linear_model\/least_angle.py":"MODIFY"},"diff":{"sklearn\/linear_model\/ransac.py":[{"add":["155","        array with the i-th value of the array corresponding to the loss","156","        on ``X[i]``.","158","        If the loss on a sample is greater than the ``residual_threshold``,","159","        then this sample is classified as an outlier."],"delete":["155","        array with the ``i``th value of the array corresponding to the loss","156","        on `X[i]`.","158","        If the loss on a sample is greater than the ``residual_threshold``, then","159","        this sample is classified as an outlier."]}],"sklearn\/feature_extraction\/hashing.py":[{"add":["43","","48","","58","","63","","68","","87","        encoded as columns of integers."],"delete":["82","      encoded as columns of integers."]}],"examples\/applications\/plot_topics_extraction_with_nmf_lda.py":[{"add":["1","=======================================================================================","2","Topic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation","3","======================================================================================="],"delete":["1","========================================================","2","Topic extraction with Non-negative Matrix Factorization\\","3","and Latent Dirichlet Allocation","4","========================================================"]}],"sklearn\/covariance\/graph_lasso_.py":[{"add":["568","                \"will be removed in 0.21. Use ``grid_scores_`` instead\")"],"delete":["568","                \"will be removed in 0.21. Use 'grid_scores_' instead\")"]}],"sklearn\/mixture\/bayesian_mixture.py":[{"add":[],"delete":["79","    *BayesianGaussianMixture*."]}],"doc\/faq.rst":[{"add":["27","issues <easy_issues>`_. Please do not contact the contributors of scikit-learn directly"],"delete":["27","issues <easy_issues>`. Please do not contact the contributors of scikit-learn directly"]}],"sklearn\/covariance\/tests\/test_graph_lasso.py":[{"add":["151","                    \"``grid_scores_`` instead\")"],"delete":["151","                    \"'grid_scores_' instead\")"]}],"sklearn\/model_selection\/_split.py":[{"add":["1824","","1825","        - None, to use the default 3-fold cross-validation,","1826","        - integer, to specify the number of folds.","1827","        - An object to be used as a cross-validation generator.","1828","        - An iterable yielding train\/test splits."],"delete":["1824","          - None, to use the default 3-fold cross-validation,","1825","          - integer, to specify the number of folds.","1826","          - An object to be used as a cross-validation generator.","1827","          - An iterable yielding train\/test splits."]}],"sklearn\/linear_model\/stochastic_gradient.py":[{"add":["355","                \"will be removed in 0.21. Use ``loss_function_`` instead\")"],"delete":["355","                \"will be removed in 0.21. Use 'loss_function_' instead\")"]}],"sklearn\/model_selection\/_validation.py":[{"add":["66","","67","        - None, to use the default 3-fold cross validation,","68","        - integer, to specify the number of folds in a `(Stratified)KFold`,","69","        - An object to be used as a cross-validation generator.","70","        - An iterable yielding train, test splits.","327","","328","        - None, to use the default 3-fold cross validation,","329","        - integer, to specify the number of folds in a `(Stratified)KFold`,","330","        - An object to be used as a cross-validation generator.","331","        - An iterable yielding train, test splits.","564","","565","        - None, to use the default 3-fold cross validation,","566","        - integer, to specify the number of folds in a `(Stratified)KFold`,","567","        - An object to be used as a cross-validation generator.","568","        - An iterable yielding train, test splits.","713","","714","        - None, to use the default 3-fold cross validation,","715","        - integer, to specify the number of folds in a `(Stratified)KFold`,","716","        - An object to be used as a cross-validation generator.","717","        - An iterable yielding train, test splits.","945","","946","        - None, to use the default 3-fold cross validation,","947","        - integer, to specify the number of folds in a `(Stratified)KFold`,","948","        - An object to be used as a cross-validation generator.","949","        - An iterable yielding train, test splits."],"delete":["66","          - None, to use the default 3-fold cross validation,","67","          - integer, to specify the number of folds in a `(Stratified)KFold`,","68","          - An object to be used as a cross-validation generator.","69","          - An iterable yielding train, test splits.","326","          - None, to use the default 3-fold cross validation,","327","          - integer, to specify the number of folds in a `(Stratified)KFold`,","328","          - An object to be used as a cross-validation generator.","329","          - An iterable yielding train, test splits.","562","          - None, to use the default 3-fold cross validation,","563","          - integer, to specify the number of folds in a `(Stratified)KFold`,","564","          - An object to be used as a cross-validation generator.","565","          - An iterable yielding train, test splits.","710","          - None, to use the default 3-fold cross validation,","711","          - integer, to specify the number of folds in a `(Stratified)KFold`,","712","          - An object to be used as a cross-validation generator.","713","          - An iterable yielding train, test splits.","941","          - None, to use the default 3-fold cross validation,","942","          - integer, to specify the number of folds in a `(Stratified)KFold`,","943","          - An object to be used as a cross-validation generator.","944","          - An iterable yielding train, test splits."]}],"sklearn\/ensemble\/iforest.py":[{"add":["59",""],"delete":[]}],"doc\/tutorial\/statistical_inference\/putting_together.rst":[{"add":["34",".. literalinclude:: ..\/..\/auto_examples\/applications\/plot_face_recognition.py"],"delete":["34",".. literalinclude:: ..\/..\/auto_examples\/applications\/face_recognition.py"]}],"doc\/tutorial\/statistical_inference\/supervised_learning.rst":[{"add":["404","   .. literalinclude:: ..\/..\/auto_examples\/exercises\/plot_digits_classification_exercise.py","407","   Solution: :download:`..\/..\/auto_examples\/exercises\/plot_digits_classification_exercise.py`"],"delete":["404","   .. literalinclude:: ..\/..\/auto_examples\/exercises\/digits_classification_exercise.py","407","   Solution: :download:`..\/..\/auto_examples\/exercises\/digits_classification_exercise.py`"]}],"sklearn\/utils\/validation.py":[{"add":["698","        Eg.:","699","            ``[\"coef_\", \"estimator_\", ...], \"coef_\"``"],"delete":["698","        Eg. : [\"coef_\", \"estimator_\", ...], \"coef_\""]}],"doc\/whats_new.rst":[{"add":["69","     :user:`Thierry Guillemot <tguillemot>`, and `Gael Varoquaux`_.","239","     :user:`Nick Dingwall <ndingwall>` and `Gael Varoquaux`_."],"delete":["69","     :user:`Thierry Guillemot <tguillemot>`_, and `Gael Varoquaux`_.","239","     `Nick Dingwall`_ and `Gael Varoquaux`_."]}],"sklearn\/utils\/extmath.py":[{"add":["131","        sparse if ``a`` or ``b`` is sparse and ``dense_output=False``."],"delete":["131","        sparse if ``a`` or ``b`` is sparse and ``dense_output``=False."]}],"sklearn\/manifold\/t_sne.py":[{"add":["796","                \"will be removed in 0.21. Use ``n_iter_`` instead\")"],"delete":["796","                \"will be removed in 0.21. Use 'n_iter_' instead\")"]}],"sklearn\/linear_model\/bayes.py":[{"add":["122","    Their beta is our ``self.alpha_``","123","    Their alpha is our ``self.lambda_``","386","    Their beta is our ``self.alpha_``","387","    Their alpha is our ``self.lambda_``","389","    which ``self.lambda_ < self.threshold_lambda`` are kept and the rest are"],"delete":["122","    Their beta is our self.alpha_","123","    Their alpha is our self.lambda_","386","    Their beta is our self.alpha_","387","    Their alpha is our self.lambda_","389","    which self.lambda_ < self.threshold_lambda are kept and the rest are"]}],"sklearn\/mixture\/gaussian_mixture.py":[{"add":["443","    .. versionadded:: 0.18","444",""],"delete":["441","    .. versionadded:: 0.18","442","    *GaussianMixture*.","443",""]}],"sklearn\/linear_model\/least_angle.py":[{"add":["1163","                \"will be removed in 0.21. See ``alpha_`` instead\")","1169","    @deprecated(\"Attribute ``cv_mse_path_`` is deprecated in 0.18 and \"","1170","                \"will be removed in 0.20. Use ``mse_path_`` instead\")"],"delete":["1163","                \"will be removed in 0.21. See 'alpha_' instead\")","1169","    @deprecated(\"Attribute cv_mse_path_ is deprecated in 0.18 and \"","1170","                \"will be removed in 0.20. Use 'mse_path_' instead\")"]}]}},"4a4b711a42492b783511cba8bf170e966531c06a":{"changes":{"sklearn\/ensemble\/forest.py":"MODIFY"},"diff":{"sklearn\/ensemble\/forest.py":[{"add":["45","import threading","381","def accumulate_prediction(predict, X, out, lock):","383","    with lock:","384","        if len(out) == 1:","385","            out[0] += prediction","386","        else:","387","            for i in range(len(out)):","388","                out[i] += prediction[i]","585","        lock = threading.Lock()","587","            delayed(accumulate_prediction)(e.predict_proba, X, all_proba, lock)","692","        lock = threading.Lock()","694","            delayed(accumulate_prediction)(e.predict, X, [y_hat], lock)"],"delete":["380","def accumulate_prediction(predict, X, out):","382","    if len(out) == 1:","383","        out[0] += prediction","384","    else:","385","        for i in range(len(out)):","386","            out[i] += prediction[i]","584","            delayed(accumulate_prediction)(e.predict_proba, X, all_proba)","690","            delayed(accumulate_prediction)(e.predict, X, [y_hat])"]}]}},"c554aad456b6302a8dd8838769769eeecc1cf734":{"changes":{"sklearn\/preprocessing\/tests\/test_label.py":"MODIFY","sklearn\/preprocessing\/label.py":"MODIFY"},"diff":{"sklearn\/preprocessing\/tests\/test_label.py":[{"add":["205","    le.fit([1, 2, 3, -1, 1])","206","    msg = \"contains previously unseen labels\"","207","    assert_raise_message(ValueError, msg, le.inverse_transform, [-2])","208","    assert_raise_message(ValueError, msg, le.inverse_transform, [-2, -3, -4])"],"delete":["205","    le.fit([1, 2, 3, 1, -1])","206","    assert_raises(ValueError, le.inverse_transform, [-1])"]}],"sklearn\/preprocessing\/label.py":[{"add":["132","            raise ValueError(","133","                    \"y contains previously unseen labels: %s\" % str(diff))","151","        if len(diff):","152","            raise ValueError(","153","                    \"y contains previously unseen labels: %s\" % str(diff))"],"delete":["132","            raise ValueError(\"y contains new labels: %s\" % str(diff))","150","        if diff:","151","            raise ValueError(\"y contains new labels: %s\" % str(diff))"]}]}},"d830c90c9b860e63ca2af2099d87bb1661947283":{"changes":{"sklearn\/tests\/test_docstring_parameters.py":"MODIFY","sklearn\/discriminant_analysis.py":"MODIFY","sklearn\/cross_decomposition\/pls_.py":"MODIFY"},"diff":{"sklearn\/tests\/test_docstring_parameters.py":[{"add":["22","PUBLIC_MODULES = set(['sklearn.' + modname","23","                      for _, modname, _ in walk_packages(sklearn.__path__)","24","                      if not modname.startswith('_') and","25","                      '.tests.' not in modname])","55","    'fit',","56","    'score',","57","    'fit_predict',","58","    'fit_transform',","59","    'partial_fit',","60","    'predict'","72","        raise SkipTest(\"numpydoc is required to test the docstrings, \"","73","                       \"as well as python version >= 3.5\")"],"delete":["22","PUBLIC_MODULES = set(['sklearn.' + pckg[1]","23","                      for pckg in walk_packages('sklearn.*')","24","                      if not pckg[1].startswith('_')])","29","    'sklearn.cross_decomposition',","30","    'sklearn.discriminant_analysis',","56","        'fit',","57","        'score',","58","        'fit_predict',","59","        'fit_transform',","60","        'partial_fit',","61","        'predict'","73","        raise SkipTest(","74","            \"numpydoc is required to test the docstrings\")"]}],"sklearn\/discriminant_analysis.py":[{"add":["176","    tol : float, optional, (default 1.0e-4)","556","    store_covariances : boolean","557","        If True the covariance matrices are computed and stored in the","558","        `self.covariances_` attribute.","559","","560","        .. versionadded:: 0.17","561","","562","    tol : float, optional, default 1.0e-4","563","        Threshold used for rank estimation.","564","","565","        .. versionadded:: 0.17","566","","628","            Training vector, where n_samples is the number of samples and"],"delete":["176","    tol : float, optional","578","    store_covariances : boolean","579","        If True the covariance matrices are computed and stored in the","580","        `self.covariances_` attribute.","581","","582","        .. versionadded:: 0.17","583","","584","    tol : float, optional, default 1.0e-4","585","        Threshold used for rank estimation.","586","","587","        .. versionadded:: 0.17","588","","628","            Training vector, where n_samples in the number of samples and"]}],"sklearn\/cross_decomposition\/pls_.py":[{"add":["237","            Training vectors, where n_samples is the number of samples and","240","        Y : array-like, shape = [n_samples, n_targets]","241","            Target vectors, where n_samples is the number of samples and","376","        X : array-like, shape = [n_samples, n_features]","377","            Training vectors, where n_samples is the number of samples and","378","            n_features is the number of predictors.","380","        Y : array-like, shape = [n_samples, n_targets]","381","            Target vectors, where n_samples is the number of samples and","382","            n_targets is the number of response variables.","414","        X : array-like, shape = [n_samples, n_features]","415","            Training vectors, where n_samples is the number of samples and","416","            n_features is the number of predictors.","434","    def fit_transform(self, X, y=None):","439","        X : array-like, shape = [n_samples, n_features]","440","            Training vectors, where n_samples is the number of samples and","441","            n_features is the number of predictors.","443","        y : array-like, shape = [n_samples, n_targets]","444","            Target vectors, where n_samples is the number of samples and","445","            n_targets is the number of response variables.","451","        return self.fit(X, y).transform(X, y)","606","    n_components : int, (default 2).","607","        Number of components to keep","608","","609","    scale : boolean, (default True)","610","        Option to scale data","785","        \"\"\"Fit model to data.","786","","787","        Parameters","788","        ----------","789","        X : array-like, shape = [n_samples, n_features]","790","            Training vectors, where n_samples is the number of samples and","791","            n_features is the number of predictors.","792","","793","        Y : array-like, shape = [n_samples, n_targets]","794","            Target vectors, where n_samples is the number of samples and","795","            n_targets is the number of response variables.","796","        \"\"\"","833","        \"\"\"","834","        Apply the dimension reduction learned on the train data.","835","","836","        Parameters","837","        ----------","838","        X : array-like, shape = [n_samples, n_features]","839","            Training vectors, where n_samples is the number of samples and","840","            n_features is the number of predictors.","841","","842","        Y : array-like, shape = [n_samples, n_targets]","843","            Target vectors, where n_samples is the number of samples and","844","            n_targets is the number of response variables.","845","        \"\"\"","858","    def fit_transform(self, X, y=None):","863","        X : array-like, shape = [n_samples, n_features]","864","            Training vectors, where n_samples is the number of samples and","865","            n_features is the number of predictors.","867","        y : array-like, shape = [n_samples, n_targets]","868","            Target vectors, where n_samples is the number of samples and","869","            n_targets is the number of response variables.","875","        return self.fit(X, y).transform(X, y)"],"delete":["237","            Training vectors, where n_samples in the number of samples and","240","        Y : array-like of response, shape = [n_samples, n_targets]","241","            Target vectors, where n_samples in the number of samples and","376","        X : array-like of predictors, shape = [n_samples, p]","377","            Training vectors, where n_samples in the number of samples and","378","            p is the number of predictors.","380","        Y : array-like of response, shape = [n_samples, q], optional","381","            Training vectors, where n_samples in the number of samples and","382","            q is the number of response variables.","414","        X : array-like of predictors, shape = [n_samples, p]","415","            Training vectors, where n_samples in the number of samples and","416","            p is the number of predictors.","434","    def fit_transform(self, X, y=None, **fit_params):","439","        X : array-like of predictors, shape = [n_samples, p]","440","            Training vectors, where n_samples in the number of samples and","441","            p is the number of predictors.","443","        Y : array-like of response, shape = [n_samples, q], optional","444","            Training vectors, where n_samples in the number of samples and","445","            q is the number of response variables.","446","","447","        copy : boolean, default True","448","            Whether to copy X and Y, or perform in-place normalization.","454","        return self.fit(X, y, **fit_params).transform(X, y)","609","    scale : boolean, scale data? (default True)","626","    n_components : int, number of components to keep. (default 2).","627","","822","        \"\"\"Apply the dimension reduction learned on the train data.\"\"\"","835","    def fit_transform(self, X, y=None, **fit_params):","840","        X : array-like of predictors, shape = [n_samples, p]","841","            Training vectors, where n_samples in the number of samples and","842","            p is the number of predictors.","844","        Y : array-like of response, shape = [n_samples, q], optional","845","            Training vectors, where n_samples in the number of samples and","846","            q is the number of response variables.","852","        return self.fit(X, y, **fit_params).transform(X, y)"]}]}},"c7ca0c58eee2047ae3f127af64401d210c1218ec":{"changes":{"sklearn\/linear_model\/stochastic_gradient.py":"MODIFY","doc\/modules\/sgd.rst":"MODIFY","sklearn\/linear_model\/tests\/test_sgd.py":"MODIFY","doc\/modules\/kernel_approximation.rst":"MODIFY","sklearn\/linear_model\/passive_aggressive.py":"MODIFY"},"diff":{"sklearn\/linear_model\/stochastic_gradient.py":[{"add":["68","        self.n_iter = n_iter","69","        self.max_iter = max_iter","71","        # current tests expect init to do parameter validation","72","        # but we are not allowed to set attributes","73","        self._validate_params(set_max_iter=False)","84","    def _validate_params(self, set_max_iter=True):","88","        if self.max_iter is not None and self.max_iter <= 0:","109","        if not set_max_iter:","110","            return","111","        # n_iter deprecation, set self._max_iter, self._tol","112","        self._tol = self.tol","113","        if self.n_iter is not None:","114","            warnings.warn(\"n_iter parameter is deprecated in 0.19 and will be\"","115","                          \" removed in 0.21. Use max_iter and tol instead.\",","116","                          DeprecationWarning)","117","            # Same behavior as before 0.19","118","            max_iter = self.n_iter","119","            self._tol = None","120","","121","        elif self.tol is None and self.max_iter is None:","122","            warnings.warn(","123","                \"max_iter and tol parameters have been added in %s in 0.19. If\"","124","                \" both are left unset, they default to max_iter=5 and tol=None\"","125","                \". If tol is not None, max_iter defaults to max_iter=1000. \"","126","                \"From 0.21, default max_iter will be 1000, \"","127","                \"and default tol will be 1e-3.\" % type(self), FutureWarning)","128","            # Before 0.19, default was n_iter=5","129","            max_iter = 5","130","        else:","131","            max_iter = self.max_iter if self.max_iter is not None else 1000","132","        self._max_iter = max_iter","133","","413","        self._validate_params()","442","        self._partial_fit(X, y, alpha, C, loss, learning_rate, self._max_iter,","445","        if (self._tol is not None and self._tol > -np.inf","446","                and self.n_iter_ == self._max_iter):","539","        self._validate_params()","763","           learning_rate='optimal', loss='hinge', max_iter=None, n_iter=None,","984","        self._validate_params()","993","        self._validate_params()","1013","                          self._max_iter, sample_weight, coef_init,","1016","        if (self._tol is not None and self._tol > -np.inf","1017","                and self.n_iter_ == self._max_iter):","1106","        tol = self._tol if self._tol is not None else -np.inf","1316","           loss='squared_loss', max_iter=None, n_iter=None, penalty='l2',"],"delete":["68","","69","        if n_iter is not None:","70","            warnings.warn(\"n_iter parameter is deprecated in 0.19 and will be\"","71","                          \" removed in 0.21. Use max_iter and tol instead.\",","72","                          DeprecationWarning)","73","            # Same behavior as before 0.19","74","            self.max_iter = n_iter","75","            tol = None","76","","77","        elif tol is None and max_iter is None:","78","            warnings.warn(","79","                \"max_iter and tol parameters have been added in %s in 0.19. If\"","80","                \" both are left unset, they default to max_iter=5 and tol=None\"","81","                \". If tol is not None, max_iter defaults to max_iter=1000. \"","82","                \"From 0.21, default max_iter will be 1000, \"","83","                \"and default tol will be 1e-3.\" % type(self), FutureWarning)","84","            # Before 0.19, default was n_iter=5","85","            self.max_iter = 5","86","        else:","87","            self.max_iter = max_iter if max_iter is not None else 1000","88","","90","","91","        self._validate_params()","102","    def _validate_params(self):","106","        if self.max_iter <= 0:","367","        self._validate_params()","435","        self._partial_fit(X, y, alpha, C, loss, learning_rate, self.max_iter,","438","        if (self.tol is not None and self.tol > -np.inf","439","                and self.n_iter_ == self.max_iter):","755","           learning_rate='optimal', loss='hinge', max_iter=5, n_iter=None,","935","        self._validate_params()","936","","1005","                          self.max_iter, sample_weight, coef_init,","1008","        if (self.tol is not None and self.tol > -np.inf","1009","                and self.n_iter_ == self.max_iter):","1098","        tol = self.tol if self.tol is not None else -np.inf","1308","           loss='squared_loss', max_iter=5, n_iter=None, penalty='l2',"]}],"doc\/modules\/sgd.rst":[{"add":["65","           learning_rate='optimal', loss='hinge', max_iter=None, n_iter=None,"],"delete":["65","           learning_rate='optimal', loss='hinge', max_iter=5, n_iter=None,"]}],"sklearn\/linear_model\/tests\/test_sgd.py":[{"add":["1209","    def init(max_iter=None, tol=None, n_iter=None):","1210","        sgd = SGDClassifier(max_iter=max_iter, tol=tol, n_iter=n_iter)","1211","        sgd._validate_params()","1212","","1215","    assert_warns_message(FutureWarning, msg_future, init)","1231","    est._validate_params()","1232","    assert_equal(est._tol, None)","1233","    assert_equal(est._max_iter, 5)","1236","    est._validate_params()","1237","    assert_equal(est._tol, None)","1238","    assert_equal(est._max_iter, 42)","1241","    est._validate_params()","1242","    assert_equal(est._tol, 1e-2)","1243","    assert_equal(est._max_iter, 1000)","1246","    est._validate_params()","1247","    assert_equal(est._tol, None)","1248","    assert_equal(est._max_iter, 42)","1251","    est._validate_params()","1252","    assert_equal(est._tol, 1e-2)","1253","    assert_equal(est._max_iter, 42)"],"delete":["1211","    assert_warns_message(FutureWarning, msg_future, SGDClassifier)","1212","","1213","    def init(max_iter=None, tol=None, n_iter=None):","1214","        SGDClassifier(max_iter=max_iter, tol=tol, n_iter=n_iter)","1230","    assert_equal(est.tol, None)","1231","    assert_equal(est.max_iter, 5)","1234","    assert_equal(est.tol, None)","1235","    assert_equal(est.max_iter, 42)","1238","    assert_equal(est.tol, 1e-2)","1239","    assert_equal(est.max_iter, 1000)","1242","    assert_equal(est.tol, None)","1243","    assert_equal(est.max_iter, 42)","1246","    assert_equal(est.tol, 1e-2)","1247","    assert_equal(est.max_iter, 42)"]}],"doc\/modules\/kernel_approximation.rst":[{"add":["65","           learning_rate='optimal', loss='hinge', max_iter=None, n_iter=None,"],"delete":["65","           learning_rate='optimal', loss='hinge', max_iter=5, n_iter=None,"]}],"sklearn\/linear_model\/passive_aggressive.py":[{"add":["116","                  fit_intercept=True, loss='hinge', max_iter=None, n_iter=None,","321","                  fit_intercept=True, loss='epsilon_insensitive',","322","                  max_iter=None, n_iter=None, random_state=0, shuffle=True,","323","                  tol=None, verbose=0, warm_start=False)","379","        self._validate_params()"],"delete":["116","                  fit_intercept=True, loss='hinge', max_iter=5, n_iter=None,","321","                  fit_intercept=True, loss='epsilon_insensitive', max_iter=5,","322","                  n_iter=None, random_state=0, shuffle=True, tol=None,","323","                  verbose=0, warm_start=False)"]}]}}}