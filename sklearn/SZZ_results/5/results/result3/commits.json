{"d990f7208c2b5488a24f73fdd2c85bb06e43cc0c":{"changes":{"sklearn\/neighbors\/binary_tree.pxi":"MODIFY","doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/neighbors\/tests\/test_kd_tree.py":"MODIFY","sklearn\/neighbors\/tests\/test_kde.py":"MODIFY","sklearn\/neighbors\/tests\/test_ball_tree.py":"MODIFY"},"diff":{"sklearn\/neighbors\/binary_tree.pxi":[{"add":["1121","        return (newObj, (type(self),), self.__getstate__())","1138","                self.dist_metric,","1139","                self.sample_weight)","1165","        self.sample_weight = state[12]"],"delete":["1121","        return (newObj, (BinaryTree,), self.__getstate__())","1138","                self.dist_metric)"]}],"doc\/whats_new\/v0.20.rst":[{"add":["763","- |Fix| Fixed a bug in `neighbors.KDTree` and `neighbors.BallTree` where","764","  pickled tree objects would change their type to the super class `BinaryTree`.","765","  :issue:`11774` by :user:`Nicolas Hug <NicolasHug>`.","766",""],"delete":[]}],"sklearn\/neighbors\/tests\/test_kd_tree.py":[{"add":["189","        assert isinstance(kdt2, KDTree)"],"delete":[]}],"sklearn\/neighbors\/tests\/test_kde.py":[{"add":["12","from sklearn.externals import joblib","205","","206","","207","def test_pickling(tmpdir):","208","    # Make sure that predictions are the same before and after pickling. Used","209","    # to be a bug because sample_weights wasn't pickled and the resulting tree","210","    # would miss some info.","211","","212","    kde = KernelDensity()","213","    data = np.reshape([1., 2., 3.], (-1, 1))","214","    kde.fit(data)","215","","216","    X = np.reshape([1.1, 2.1], (-1, 1))","217","    scores = kde.score_samples(X)","218","","219","    file_path = str(tmpdir.join('dump.pkl'))","220","    joblib.dump(kde, file_path)","221","    kde = joblib.load(file_path)","222","    scores_pickled = kde.score_samples(X)","223","","224","    assert_allclose(scores, scores_pickled)"],"delete":[]}],"sklearn\/neighbors\/tests\/test_ball_tree.py":[{"add":["230","        assert isinstance(bt2, BallTree)","231",""],"delete":[]}]}},"f6f7e3cfd365b39d4bc5a98cc8914c3a528477d6":{"changes":{"sklearn\/model_selection\/_validation.py":"MODIFY","doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/model_selection\/tests\/test_validation.py":"MODIFY"},"diff":{"sklearn\/model_selection\/_validation.py":[{"add":["557","            for scorer_name in sorted(test_scores):","558","                msg += \", %s=\" % scorer_name","559","                if return_train_score:","560","                    msg += \"(train=%.3f,\" % train_scores[scorer_name]","561","                    msg += \" test=%.3f)\" % test_scores[scorer_name]","562","                else:","563","                    msg += \"%.3f\" % test_scores[scorer_name]","565","            msg += \", score=\"","566","            msg += (\"%.3f\" % test_scores if not return_train_score else","567","                    \"(train=%.3f, test=%.3f)\" % (train_scores, test_scores))","568",""],"delete":["506","","556","","559","            for scorer_name, score in test_scores.items():","560","                msg += \", %s=%s\" % (scorer_name, score)","562","            msg += \", score=%s\" % test_scores"]}],"doc\/whats_new\/v0.21.rst":[{"add":["83",":mod:`sklearn.model_selection`","84","......................","85","","86","- |Enhancement| Method :func:`_fit_and_score` now prints train_scores when","87","  `return_train_scores` is True and `verbose` > 2.","88","  :issue:`12613` by :user:`Marc Torrellas <marctorrellas>`.","89",""],"delete":[]}],"sklearn\/model_selection\/tests\/test_validation.py":[{"add":["31","from sklearn.model_selection import cross_val_score, ShuffleSplit","46","from sklearn.model_selection._validation import _score","1480","def test_fit_and_score_failing():","1540","","1541","","1542","def test_fit_and_score_working():","1543","    X, y = make_classification(n_samples=30, random_state=0)","1544","    clf = SVC(kernel=\"linear\", random_state=0)","1545","    train, test = next(ShuffleSplit().split(X))","1546","    # Test return_parameters option","1547","    fit_and_score_args = [clf, X, y, dict(), train, test, 0]","1548","    fit_and_score_kwargs = {'parameters': {'max_iter': 100, 'tol': 0.1},","1549","                            'fit_params': None,","1550","                            'return_parameters': True}","1551","    result = _fit_and_score(*fit_and_score_args,","1552","                            **fit_and_score_kwargs)","1553","    assert result[-1] == fit_and_score_kwargs['parameters']","1554","","1555","","1556","def three_params_scorer(i, j, k):","1557","    return 3.4213","1558","","1559","","1560","@pytest.mark.parametrize(\"return_train_score, scorer, expected\", [","1561","    (False, three_params_scorer,","1562","     \"[CV] .................................... , score=3.421, total=   0.0s\"),","1563","    (True, three_params_scorer,","1564","     \"[CV] ................ , score=(train=3.421, test=3.421), total=   0.0s\"),","1565","    (True, {'sc1': three_params_scorer, 'sc2': three_params_scorer},","1566","     \"[CV]  , sc1=(train=3.421, test=3.421)\"","1567","     \", sc2=(train=3.421, test=3.421), total=   0.0s\")","1568","])","1569","def test_fit_and_score_verbosity(capsys, return_train_score, scorer, expected):","1570","    X, y = make_classification(n_samples=30, random_state=0)","1571","    clf = SVC(kernel=\"linear\", random_state=0)","1572","    train, test = next(ShuffleSplit().split(X))","1573","","1574","    # test print without train score","1575","    fit_and_score_args = [clf, X, y, scorer, train, test, 10, None, None]","1576","    fit_and_score_kwargs = {'return_train_score': return_train_score}","1577","    _fit_and_score(*fit_and_score_args, **fit_and_score_kwargs)","1578","    out, _ = capsys.readouterr()","1579","    assert out.split('\\n')[1] == expected","1580","","1581","","1582","def test_score():","1583","    error_message = \"scoring must return a number, got None\"","1584","","1585","    def two_params_scorer(estimator, X_test):","1586","        return None","1587","    fit_and_score_args = [None, None, None, two_params_scorer]","1588","    assert_raise_message(ValueError, error_message,","1589","                         _score, *fit_and_score_args)"],"delete":["31","from sklearn.model_selection import cross_val_score","1479","def test_fit_and_score():"]}]}},"104f6847919a0a95daff711dcdc5e0722e4a3fce":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/utils\/validation.py":"MODIFY","doc\/whats_new\/_contributors.rst":"MODIFY","sklearn\/utils\/tests\/test_validation.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["182","","183","- |Fix| Calling :func:`utils.check_array` on `pandas.Series`, which","184","  raised an error in 0.20.0, now returns the expected output again.","185","  :issue:`12625` by `Andreas M¨¹ller`_"],"delete":[]}],"sklearn\/utils\/validation.py":[{"add":["479","    if hasattr(array, \"dtypes\") and len(array.dtypes):"],"delete":["479","    if hasattr(array, \"dtypes\") and hasattr(array, \"__array__\"):"]}],"doc\/whats_new\/_contributors.rst":[{"add":["50",".. _Andreas M¨¹ller: https:\/\/amueller.github.io\/"],"delete":["50",".. _Andreas M¨¹ller: https:\/\/peekaboo-vision.blogspot.com\/"]}],"sklearn\/utils\/tests\/test_validation.py":[{"add":["696","def test_check_array_series():","697","    # regression test that check_array works on pandas Series","698","    pd = importorskip(\"pandas\")","699","    res = check_array(pd.Series([1, 2, 3]), ensure_2d=False,","700","                      warn_on_dtype=True)","701","    assert_array_equal(res, np.array([1, 2, 3]))","702","","703",""],"delete":[]}]}},"5533deb8e0ad8e1de79209a588800b794f2de13f":{"changes":{"sklearn\/model_selection\/_split.py":"MODIFY","sklearn\/model_selection\/tests\/test_split.py":"MODIFY"},"diff":{"sklearn\/model_selection\/_split.py":[{"add":["1941","    if cv is None or cv is 'warn':"],"delete":["1941","    if cv is 'warn':"]}],"sklearn\/model_selection\/tests\/test_split.py":[{"add":["1446","    assert_warns_message(FutureWarning, CV_WARNING, check_cv, None)"],"delete":["1446",""]}]}},"c3f973b1de156509b7c59e2d5fafdef1c28f74dd":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/datasets\/_svmlight_format.pyx":"MODIFY","sklearn\/datasets\/svmlight_format.py":"MODIFY","sklearn\/datasets\/tests\/test_svmlight_format.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["55","","56",":mod:`sklearn.datasets`","57","  ......................","58","","59","  - |Fix| Added support for 64-bit group IDs and pointers in SVMLight files","60","    :class:`datasets.svmlight_format` :issue:`10727` by","61","    :user:`Bryan K Woods <bryan-woods>`,","62","","147","  :issue:`10580` by :user:`Reshama Shaikh <reshamas>` and `Sandra"],"delete":["55","  ","140","  :issue:`10580` by :user:`Reshama Shaikh <reshamas>` and `Sandra "]}],"sklearn\/datasets\/_svmlight_format.pyx":[{"add":["44","","45","    indices = array.array(\"q\")","46","    indptr = array.array(\"q\", [0])","111","        # increment index pointer array size"],"delete":["44","    indices = array.array(\"i\")","45","    indptr = array.array(\"i\", [0])"]}],"sklearn\/datasets\/svmlight_format.py":[{"add":["189","    indices = np.frombuffer(ind, np.longlong)","190","    indptr = np.frombuffer(indptr, dtype=np.longlong)   # never empty"],"delete":["189","    indices = np.frombuffer(ind, np.intc)","190","    indptr = np.frombuffer(indptr, dtype=np.intc)   # never empty"]}],"sklearn\/datasets\/tests\/test_svmlight_format.py":[{"add":["188","@pytest.mark.skip(\"testing the overflow of 32 bit sparse indexing requires a\"","189","                  \" large amount of memory\")","190","def test_load_large_qid():","191","    \"\"\"","192","    load large libsvm \/ svmlight file with qid attribute. Tests 64-bit query ID","193","    \"\"\"","194","    data = b\"\\n\".join((\"3 qid:{0} 1:0.53 2:0.12\\n2 qid:{0} 1:0.13 2:0.1\"","195","                      .format(i).encode() for i in range(1, 40*1000*1000)))","196","    X, y, qid = load_svmlight_file(BytesIO(data), query_id=True)","197","    assert_array_equal(y[-4:], [3, 2, 3, 2])","198","    assert_array_equal(np.unique(qid), np.arange(1, 40*1000*1000))","199","","200",""],"delete":[]}]}},"4c2eb3a0d67cbdacdb9314f585b2f73590ff0ac8":{"changes":{"sklearn\/metrics\/regression.py":"MODIFY","doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/metrics\/tests\/test_regression.py":"MODIFY"},"diff":{"sklearn\/metrics\/regression.py":[{"add":["26","import warnings","28","from ..utils.validation import (check_array, check_consistent_length,","29","                                _num_samples)","31","from ..exceptions import UndefinedMetricWarning","505","    This metric is not well-defined for single samples and will return a NaN","506","    value if n_samples is less than two.","507","","542","    if _num_samples(y_pred) < 2:","543","        msg = \"R^2 score is not well-defined with less than two samples.\"","544","        warnings.warn(msg, UndefinedMetricWarning)","545","        return float('nan')","546",""],"delete":["27","from ..utils.validation import check_array, check_consistent_length"]}],"doc\/whats_new\/v0.21.rst":[{"add":["143","- |Fix| The metric :func:`metrics.r2_score` is degenerate with a single sample","144","  and now it returns NaN and raises :class:`exceptions.UndefinedMetricWarning`.","145","  :issue:`12855` by :user:`Pawel Sendyk <psendyk>.`","146",""],"delete":[]}],"sklearn\/metrics\/tests\/test_regression.py":[{"add":["4","import pytest","22","from ...exceptions import UndefinedMetricWarning","23","","194","","195","","196","@pytest.mark.parametrize('metric', [r2_score])","197","def test_regression_single_sample(metric):","198","    y_true = [0]","199","    y_pred = [1]","200","    warning_msg = 'not well-defined with less than two samples.'","201","","202","    # Trigger the warning","203","    with pytest.warns(UndefinedMetricWarning, match=warning_msg):","204","        score = metric(y_true, y_pred)","205","        assert np.isnan(score)"],"delete":["77",""]}]}},"d4802aeac55475171850d6cf121718e879ab93eb":{"changes":{"doc\/modules\/clustering.rst":"MODIFY"},"diff":{"doc\/modules\/clustering.rst":[{"add":["389","    x_i^{t+1} = m(x_i^t)"],"delete":["389","    x_i^{t+1} = x_i^t + m(x_i^t)"]}]}},"d903436afed4725b0bb366517215bff5124c10af":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/multioutput.py":"MODIFY","sklearn\/tests\/test_multioutput.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["508",":mod:`sklearn.multioutput`","509","........................","510","","511","- |Fix| Fixed a bug in :class:`multiout.MultiOutputClassifier` where the","512","  `predict_proba` method incorrectly checked for `predict_proba` attribute in","513","  the estimator object.","514","  :issue:`12222` by :user:`Rebekah Kim <rebekahkim>`","515","  "],"delete":[]}],"sklearn\/multioutput.py":[{"add":["147","                             \" a fit method\")","188","            raise ValueError(\"The base estimator should implement\"","189","                             \" a predict method\")","330","        This method will raise a ``ValueError`` if any of the","331","        estimators do not have ``predict_proba``.","332","","346","        if not all([hasattr(estimator, \"predict_proba\")","347","                    for estimator in self.estimators_]):","348","            raise ValueError(\"The base estimator should implement \"","356","        \"\"\"Returns the mean accuracy on the given test data and labels."],"delete":["147","                             \"  a fit method\")","188","            raise ValueError(\"The base estimator should implement a predict method\")","342","        if not hasattr(self.estimator, \"predict_proba\"):","343","            raise ValueError(\"The base estimator should implement\"","351","        \"\"\"\"Returns the mean accuracy on the given test data and labels."]}],"sklearn\/tests\/test_multioutput.py":[{"add":["33","from sklearn.model_selection import GridSearchCV","179","# check predict_proba passes","180","def test_multi_output_predict_proba():","181","    sgd_linear_clf = SGDClassifier(random_state=1, max_iter=5, tol=1e-3)","182","    param = {'loss': ('hinge', 'log', 'modified_huber')}","183","","184","    # inner function for custom scoring","185","    def custom_scorer(estimator, X, y):","186","        if hasattr(estimator, \"predict_proba\"):","187","            return 1.0","188","        else:","189","            return 0.0","190","    grid_clf = GridSearchCV(sgd_linear_clf, param_grid=param,","191","                            scoring=custom_scorer, cv=3, error_score=np.nan)","192","    multi_target_linear = MultiOutputClassifier(grid_clf)","193","    multi_target_linear.fit(X, y)","194","","195","    multi_target_linear.predict_proba(X)","196","","197","    # SGDClassifier defaults to loss='hinge' which is not a probabilistic","198","    # loss function; therefore it does not expose a predict_proba method","199","    sgd_linear_clf = SGDClassifier(random_state=1, max_iter=5, tol=1e-3)","200","    multi_target_linear = MultiOutputClassifier(sgd_linear_clf)","201","    multi_target_linear.fit(X, y)","202","    err_msg = \"The base estimator should implement predict_proba method\"","203","    with pytest.raises(ValueError, match=err_msg):","204","        multi_target_linear.predict_proba(X)","205","","206",""],"delete":[]}]}},"a79d44e4a444f9b639234daadf82351f5bd71689":{"changes":{"sklearn\/cluster\/_optics_inner.pyx":"MODIFY"},"diff":{"sklearn\/cluster\/_optics_inner.pyx":[{"add":["7","# as defined in PEP485 (python3.5)","8","cdef inline isclose(double a, ","9","                    double b,","10","                    double rel_tol=1e-09,","11","                    double abs_tol=0.0):","12","    return abs(a-b) <= max(rel_tol * max(abs(a), abs(b)), abs_tol)","13","","33","        elif isclose(rdists[i], rdist):"],"delete":["26","        if rdists[i] == rdist:"]}]}},"5d8dfc9677e410de2dbd1daf1963696577869d59":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/utils\/validation.py":"MODIFY","sklearn\/utils\/tests\/test_validation.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["1316","- |Fix| Fixed a bug in validation helpers where passing a Dask DataFrame results","1317","  in an error. :issue:`12462` by :user:`Zachariah Miller <zwmiller>`","1318",""],"delete":[]}],"sklearn\/utils\/validation.py":[{"add":["142","        # Check that shape is returning an integer or default to len","143","        # Dask dataframes may not return numeric shape[0] value","144","        if isinstance(x.shape[0], numbers.Integral):","145","            return x.shape[0]","146","        else:","147","            return len(x)"],"delete":["142","        return x.shape[0]"]}],"sklearn\/utils\/tests\/test_validation.py":[{"add":["43","    _num_samples","789","","790","","791","def test_retrieve_samples_from_non_standard_shape():","792","    class TestNonNumericShape:","793","        def __init__(self):","794","            self.shape = (\"not numeric\",)","795","","796","        def __len__(self):","797","            return len([1, 2, 3])","798","","799","    X = TestNonNumericShape()","800","    assert _num_samples(X) == len(X)"],"delete":[]}]}},"c1f58745be7c4923cf0a666f1c6bf052042f131e":{"changes":{"doc\/modules\/linear_model.rst":"MODIFY","doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/linear_model\/tests\/test_logistic.py":"MODIFY","doc\/tutorial\/statistical_inference\/supervised_learning.rst":"MODIFY","sklearn\/linear_model\/logistic.py":"MODIFY","sklearn\/linear_model\/sag_fast.pyx":"MODIFY","sklearn\/linear_model\/sag.py":"MODIFY","examples\/linear_model\/plot_logistic_l1_l2_sparsity.py":"MODIFY"},"diff":{"doc\/modules\/linear_model.rst":[{"add":["340","Elastic-Net","392","Multi-task Elastic-Net","732","Rest, or multinomial logistic regression with optional L2, L1 or Elastic-Net","741","optimization problem:","745","Elastic-Net regularization is a combination of L1 and L2, and minimizes the","746","following cost function:","747","","748",".. math:: \\min_{w, c} \\frac{1 - \\rho}{2}w^T w + \\rho \\|w\\|_1 + C \\sum_{i=1}^n \\log(\\exp(- y_i (X_i^T w + c)) + 1),","749","","750","where :math:`\\rho` controls the strengh of L1 regularization vs L2","751","regularization (it corresponds to the `l1_ratio` parameter).","752","","754",":math:`{-1, 1}` at trial :math:`i`. We can also see that Elastic-Net is","755","equivalent to L1 when :math:`\\rho = 1` and equivalent to L2 when","756",":math:`\\rho=0`.","784","non-smooth `penalty=\"l1\"`. This is therefore the solver of choice for sparse","785","multinomial logistic regression. It is also the only solver that supports","786","`penalty=\"elasticnet\"`.","788","In a nutshell, the following table summarizes the penalties supported by","789","each solver:","804","| Elastic-Net                  |       no        |     no      |       no        |    no     |    yes     |","805","+------------------------------+-----------------+-------------+-----------------+-----------+------------+","815","The \"saga\" solver is often the best choice but requires scaling. The","816","\"liblinear\" solver is used by default for historical reasons.","854",":class:`LogisticRegressionCV` implements Logistic Regression with built-in","855","cross-validation support, to find the optimal `C` and `l1_ratio` parameters","856","according to the ``scoring`` attribute. The \"newton-cg\", \"sag\", \"saga\" and","857","\"lbfgs\" solvers are found to be faster for high-dimensional dense data, due","858","to warm-starting (see :term:`Glossary <warm_start>`)."],"delete":["340","Elastic Net","392","Multi-task Elastic Net","732","Rest, or multinomial logistic regression with optional L2 or L1","741","optimization problem","746",":math:`{-1, 1}` at trial :math:`i`.","774","non-smooth `penalty=\"l1\"` option. This is therefore the solver of choice","775","for sparse multinomial logistic regression.","777","In a nutshell, the following table summarizes the penalties supported by each solver:","801","The \"saga\" solver is often the best choice but requires scaling. The \"liblinear\" solver is","802","used by default for historical reasons.","840",":class:`LogisticRegressionCV` implements Logistic Regression with","841","builtin cross-validation to find out the optimal C parameter.","842","\"newton-cg\", \"sag\", \"saga\" and \"lbfgs\" solvers are found to be faster","843","for high-dimensional dense data, due to warm-starting. For the","844","multiclass case, if `multi_class` option is set to \"ovr\", an optimal C","845","is obtained for each class and if the `multi_class` option is set to","846","\"multinomial\", an optimal C is obtained by minimizing the cross-entropy","847","loss."]}],"doc\/whats_new\/v0.21.rst":[{"add":["24","- :class:`linear_model.LogisticRegression` and","25","  :class:`linear_model.LogisticRegressionCV` with 'saga' solver. |Fix|","26","","151",":mod:`sklearn.linear_model`","152","...........................","153","","154","- |Feature| :class:`linear_model.LogisticRegression` and","155","  :class:`linear_model.LogisticRegressionCV` now support Elastic-Net penalty,","156","  with the 'saga' solver. :issue:`11646` by :user:`Nicolas Hug <NicolasHug>`.","157","","158","- |Fix| Fixed a bug in the 'saga' solver where the weights would not be","159","  correctly updated in some cases. :issue:`11646` by `Tom Dupre la Tour`_."],"delete":[]}],"sklearn\/linear_model\/tests\/test_logistic.py":[{"add":["13","from sklearn.model_selection import GridSearchCV","14","from sklearn.model_selection import train_test_split","29","from sklearn.linear_model import SGDClassifier","30","from sklearn.preprocessing import scale","84","@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22","213","@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22","233","    # all solvers except 'liblinear' and 'saga'","245","    # only saga supports elasticnet. We only test for liblinear because the","246","    # error is raised before for the other solvers (solver %s supports only l2","247","    # penalties)","248","    for solver in ['liblinear']:","249","        msg = (\"Only 'saga' solver supports elasticnet penalty, got \"","250","               \"solver={}.\".format(solver))","251","        lr = LR(solver=solver, penalty='elasticnet')","252","        assert_raise_message(ValueError, msg, lr.fit, X, y)","253","","1397","def test_elastic_net_coeffs():","1398","    # make sure elasticnet penalty gives different coefficients from l1 and l2","1399","    # with saga solver (l1_ratio different from 0 or 1)","1400","    X, y = make_classification(random_state=0)","1401","","1402","    C = 2.","1403","    l1_ratio = .5","1404","    coeffs = list()","1405","    for penalty in ('elasticnet', 'l1', 'l2'):","1406","        lr = LogisticRegression(penalty=penalty, C=C, solver='saga',","1407","                                random_state=0, l1_ratio=l1_ratio)","1408","        lr.fit(X, y)","1409","        coeffs.append(lr.coef_)","1410","","1411","    elastic_net_coeffs, l1_coeffs, l2_coeffs = coeffs","1412","    # make sure coeffs differ by at least .1","1413","    assert not np.allclose(elastic_net_coeffs, l1_coeffs, rtol=0, atol=.1)","1414","    assert not np.allclose(elastic_net_coeffs, l2_coeffs, rtol=0, atol=.1)","1415","    assert not np.allclose(l2_coeffs, l1_coeffs, rtol=0, atol=.1)","1416","","1417","","1418","@pytest.mark.parametrize('C', [.001, .1, 1, 10, 100, 1000, 1e6])","1419","@pytest.mark.parametrize('penalty, l1_ratio',","1420","                         [('l1', 1),","1421","                          ('l2', 0)])","1422","def test_elastic_net_l1_l2_equivalence(C, penalty, l1_ratio):","1423","    # Make sure elasticnet is equivalent to l1 when l1_ratio=1 and to l2 when","1424","    # l1_ratio=0.","1425","    X, y = make_classification(random_state=0)","1426","","1427","    lr_enet = LogisticRegression(penalty='elasticnet', C=C, l1_ratio=l1_ratio,","1428","                                 solver='saga', random_state=0)","1429","    lr_expected = LogisticRegression(penalty=penalty, C=C, solver='saga',","1430","                                     random_state=0)","1431","    lr_enet.fit(X, y)","1432","    lr_expected.fit(X, y)","1433","","1434","    assert_array_almost_equal(lr_enet.coef_, lr_expected.coef_)","1435","","1436","","1437","@pytest.mark.parametrize('C', [.001, 1, 100, 1e6])","1438","def test_elastic_net_vs_l1_l2(C):","1439","    # Make sure that elasticnet with grid search on l1_ratio gives same or","1440","    # better results than just l1 or just l2.","1441","","1442","    X, y = make_classification(500, random_state=0)","1443","    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)","1444","","1445","    param_grid = {'l1_ratio': np.linspace(0, 1, 5)}","1446","","1447","    enet_clf = LogisticRegression(penalty='elasticnet', C=C, solver='saga',","1448","                                  random_state=0)","1449","    gs = GridSearchCV(enet_clf, param_grid, cv=5, iid=False, refit=True)","1450","","1451","    l1_clf = LogisticRegression(penalty='l1', C=C, solver='saga',","1452","                                random_state=0)","1453","    l2_clf = LogisticRegression(penalty='l2', C=C, solver='saga',","1454","                                random_state=0)","1455","","1456","    for clf in (gs, l1_clf, l2_clf):","1457","        clf.fit(X_train, y_train)","1458","","1459","    assert gs.score(X_test, y_test) >= l1_clf.score(X_test, y_test)","1460","    assert gs.score(X_test, y_test) >= l2_clf.score(X_test, y_test)","1461","","1462","","1463","@pytest.mark.parametrize('C', np.logspace(-3, 2, 4))","1464","@pytest.mark.parametrize('l1_ratio', [.1, .5, .9])","1465","def test_LogisticRegression_elastic_net_objective(C, l1_ratio):","1466","    # Check that training with a penalty matching the objective leads","1467","    # to a lower objective.","1468","    # Here we train a logistic regression with l2 (a) and elasticnet (b)","1469","    # penalties, and compute the elasticnet objective. That of a should be","1470","    # greater than that of b (both objectives are convex).","1471","    X, y = make_classification(n_samples=1000, n_classes=2, n_features=20,","1472","                               n_informative=10, n_redundant=0,","1473","                               n_repeated=0, random_state=0)","1474","    X = scale(X)","1475","","1476","    lr_enet = LogisticRegression(penalty='elasticnet', solver='saga',","1477","                                 random_state=0, C=C, l1_ratio=l1_ratio,","1478","                                 fit_intercept=False)","1479","    lr_l2 = LogisticRegression(penalty='l2', solver='saga', random_state=0,","1480","                               C=C, fit_intercept=False)","1481","    lr_enet.fit(X, y)","1482","    lr_l2.fit(X, y)","1483","","1484","    def enet_objective(lr):","1485","        coef = lr.coef_.ravel()","1486","        obj = C * log_loss(y, lr.predict_proba(X))","1487","        obj += l1_ratio * np.sum(np.abs(coef))","1488","        obj += (1. - l1_ratio) * 0.5 * np.dot(coef, coef)","1489","        return obj","1490","","1491","    assert enet_objective(lr_enet) < enet_objective(lr_l2)","1492","","1493","","1494","@pytest.mark.filterwarnings('ignore: The default of the `iid`')  # 0.22","1495","@pytest.mark.parametrize('multi_class', ('ovr', 'multinomial'))","1496","def test_LogisticRegressionCV_GridSearchCV_elastic_net(multi_class):","1497","    # make sure LogisticRegressionCV gives same best params (l1 and C) as","1498","    # GridSearchCV when penalty is elasticnet","1499","","1500","    if multi_class == 'ovr':","1501","        # This is actually binary classification, ovr multiclass is treated in","1502","        # test_LogisticRegressionCV_GridSearchCV_elastic_net_ovr","1503","        X, y = make_classification(random_state=0)","1504","    else:","1505","        X, y = make_classification(n_samples=200, n_classes=3, n_informative=3,","1506","                                   random_state=0)","1507","","1508","    cv = StratifiedKFold(5, random_state=0)","1509","","1510","    l1_ratios = np.linspace(0, 1, 5)","1511","    Cs = np.logspace(-4, 4, 5)","1512","","1513","    lrcv = LogisticRegressionCV(penalty='elasticnet', Cs=Cs, solver='saga',","1514","                                cv=cv, l1_ratios=l1_ratios, random_state=0,","1515","                                multi_class=multi_class)","1516","    lrcv.fit(X, y)","1517","","1518","    param_grid = {'C': Cs, 'l1_ratio': l1_ratios}","1519","    lr = LogisticRegression(penalty='elasticnet', solver='saga',","1520","                            random_state=0, multi_class=multi_class)","1521","    gs = GridSearchCV(lr, param_grid, cv=cv)","1522","    gs.fit(X, y)","1523","","1524","    assert gs.best_params_['l1_ratio'] == lrcv.l1_ratio_[0]","1525","    assert gs.best_params_['C'] == lrcv.C_[0]","1526","","1527","","1528","def test_LogisticRegressionCV_GridSearchCV_elastic_net_ovr():","1529","    # make sure LogisticRegressionCV gives same best params (l1 and C) as","1530","    # GridSearchCV when penalty is elasticnet and multiclass is ovr. We can't","1531","    # compare best_params like in the previous test because","1532","    # LogisticRegressionCV with multi_class='ovr' will have one C and one","1533","    # l1_param for each class, while LogisticRegression will share the","1534","    # parameters over the *n_classes* classifiers.","1535","","1536","    X, y = make_classification(n_samples=200, n_classes=3, n_informative=3,","1537","                               random_state=0)","1538","    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)","1539","    cv = StratifiedKFold(5, random_state=0)","1540","","1541","    l1_ratios = np.linspace(0, 1, 5)","1542","    Cs = np.logspace(-4, 4, 5)","1543","","1544","    lrcv = LogisticRegressionCV(penalty='elasticnet', Cs=Cs, solver='saga',","1545","                                cv=cv, l1_ratios=l1_ratios, random_state=0,","1546","                                multi_class='ovr')","1547","    lrcv.fit(X_train, y_train)","1548","","1549","    param_grid = {'C': Cs, 'l1_ratio': l1_ratios}","1550","    lr = LogisticRegression(penalty='elasticnet', solver='saga',","1551","                            random_state=0, multi_class='ovr')","1552","    gs = GridSearchCV(lr, param_grid, cv=cv, iid=False)","1553","    gs.fit(X_train, y_train)","1554","","1555","    # Check that predictions are 80% the same","1556","    assert (lrcv.predict(X_train) == gs.predict(X_train)).mean() >= .8","1557","    assert (lrcv.predict(X_test) == gs.predict(X_test)).mean() >= .8","1558","","1559","","1560","@pytest.mark.parametrize('multi_class', ('ovr', 'multinomial'))","1561","def test_LogisticRegressionCV_no_refit(multi_class):","1562","    # Test LogisticRegressionCV attribute shapes when refit is False","1563","","1564","    n_classes = 3","1565","    n_features = 20","1566","    X, y = make_classification(n_samples=200, n_classes=n_classes,","1567","                               n_informative=n_classes, n_features=n_features,","1568","                               random_state=0)","1569","","1570","    Cs = np.logspace(-4, 4, 3)","1571","    l1_ratios = np.linspace(0, 1, 2)","1572","","1573","    lrcv = LogisticRegressionCV(penalty='elasticnet', Cs=Cs, solver='saga',","1574","                                cv=5, l1_ratios=l1_ratios, random_state=0,","1575","                                multi_class=multi_class, refit=False)","1576","    lrcv.fit(X, y)","1577","    assert lrcv.C_.shape == (n_classes,)","1578","    assert lrcv.l1_ratio_.shape == (n_classes,)","1579","    assert lrcv.coef_.shape == (n_classes, n_features)","1580","","1581","","1582","@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22","1583","def test_LogisticRegressionCV_elasticnet_attribute_shapes():","1584","    # Make sure the shapes of scores_ and coefs_paths_ attributes are correct","1585","    # when using elasticnet (added one dimension for l1_ratios)","1586","","1587","    n_classes = 3","1588","    n_features = 20","1589","    X, y = make_classification(n_samples=200, n_classes=n_classes,","1590","                               n_informative=n_classes, n_features=n_features,","1591","                               random_state=0)","1592","","1593","    Cs = np.logspace(-4, 4, 3)","1594","    l1_ratios = np.linspace(0, 1, 2)","1595","","1596","    n_folds = 2","1597","    lrcv = LogisticRegressionCV(penalty='elasticnet', Cs=Cs, solver='saga',","1598","                                cv=n_folds, l1_ratios=l1_ratios,","1599","                                random_state=0)","1600","    lrcv.fit(X, y)","1601","    coefs_paths = np.asarray(list(lrcv.coefs_paths_.values()))","1602","    assert coefs_paths.shape == (n_classes, n_folds, Cs.size,","1603","                                 l1_ratios.size, n_features + 1)","1604","    scores = np.asarray(list(lrcv.scores_.values()))","1605","    assert scores.shape == (n_classes, n_folds, Cs.size, l1_ratios.size)","1606","","1607","    assert lrcv.n_iter_.shape == (n_classes, n_folds, Cs.size, l1_ratios.size)","1608","","1609","","1610","@pytest.mark.parametrize('l1_ratio', (-1, 2, None, 'something_wrong'))","1611","def test_l1_ratio_param(l1_ratio):","1612","","1613","    msg = \"l1_ratio must be between 0 and 1; got (l1_ratio=%r)\" % l1_ratio","1614","    assert_raise_message(ValueError, msg,","1615","                         LogisticRegression(penalty='elasticnet',","1616","                                            solver='saga',","1617","                                            l1_ratio=l1_ratio).fit, X, Y1)","1618","    if l1_ratio is not None:","1619","        msg = (\"l1_ratio parameter is only used when penalty is 'elasticnet'.\"","1620","               \" Got (penalty=l1)\")","1621","        assert_warns_message(UserWarning, msg,","1622","                             LogisticRegression(penalty='l1', solver='saga',","1623","                                                l1_ratio=l1_ratio).fit, X, Y1)","1624","","1625","","1626","@pytest.mark.parametrize('l1_ratios', ([], [.5, 2], None, 'something_wrong'))","1627","def test_l1_ratios_param(l1_ratios):","1628","","1629","    msg = (\"l1_ratios must be a list of numbers between 0 and 1; got \"","1630","           \"(l1_ratios=%r)\" % l1_ratios)","1631","    assert_raise_message(ValueError, msg,","1632","                         LogisticRegressionCV(penalty='elasticnet',","1633","                                              solver='saga',","1634","                                              l1_ratios=l1_ratios, cv=2).fit,","1635","                         X, Y1)","1636","    if l1_ratios is not None:","1637","        msg = (\"l1_ratios parameter is only used when penalty is \"","1638","               \"'elasticnet'. Got (penalty=l1)\")","1639","        function = LogisticRegressionCV(penalty='l1', solver='saga',","1640","                                        l1_ratios=l1_ratios, cv=2).fit","1641","        assert_warns_message(UserWarning, msg, function, X, Y1)","1642","","1643","","1644","@pytest.mark.parametrize('C', np.logspace(-3, 2, 4))","1645","@pytest.mark.parametrize('l1_ratio', [.1, .5, .9])","1646","def test_elastic_net_versus_sgd(C, l1_ratio):","1647","    # Compare elasticnet penalty in LogisticRegression() and SGD(loss='log')","1648","    n_samples = 500","1649","    X, y = make_classification(n_samples=n_samples, n_classes=2, n_features=5,","1650","                               n_informative=5, n_redundant=0, n_repeated=0,","1651","                               random_state=0)","1652","    X = scale(X)","1653","","1654","    sgd = SGDClassifier(","1655","        penalty='elasticnet', random_state=0, fit_intercept=False, tol=-np.inf,","1656","        max_iter=2000, l1_ratio=l1_ratio, alpha=1. \/ C \/ n_samples, loss='log')","1657","    log = LogisticRegression(","1658","        penalty='elasticnet', random_state=0, fit_intercept=False, tol=1e-5,","1659","        max_iter=1000, l1_ratio=l1_ratio, C=C, solver='saga')","1660","","1661","    sgd.fit(X, y)","1662","    log.fit(X, y)","1663","    assert_array_almost_equal(sgd.coef_, log.coef_, decimal=1)","1664","","1665",""],"delete":["227","    # all solvers except 'liblinear'"]}],"doc\/tutorial\/statistical_inference\/supervised_learning.rst":[{"add":["185","","381","        fit_intercept=True, intercept_scaling=1, l1_ratio=None, max_iter=100,"],"delete":["380","        fit_intercept=True, intercept_scaling=1, max_iter=100,"]}],"sklearn\/linear_model\/logistic.py":[{"add":["439","    all_penalties = ['l1', 'l2', 'elasticnet']","450","","451","    if penalty == 'elasticnet' and solver != 'saga':","452","        raise ValueError(\"Only 'saga' solver supports elasticnet penalty,\"","453","                         \" got solver={}.\".format(solver))","486","                             max_squared_sum=None, sample_weight=None,","487","                             l1_ratio=None):","556","    penalty : str, 'l1', 'l2', or 'elasticnet'","558","        'sag' and 'lbfgs' solvers support only l2 penalties. 'elasticnet' is","559","        only supported by the 'saga' solver.","607","    l1_ratio : float or None, optional (default=None)","608","        The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only","609","        used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent","610","        to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent","611","        to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a","612","        combination of L1 and L2.","613","","793","            # alpha is for L2-norm, beta is for L1-norm","797","            elif penalty == 'l2':","800","            else:  # Elastic-Net penalty","801","                alpha = (1. \/ C) * (1 - l1_ratio)","802","                beta = (1. \/ C) * l1_ratio","803","","835","                          max_squared_sum=None, sample_weight=None,","836","                          l1_ratio=None):","898","    penalty : str, 'l1', 'l2', or 'elasticnet'","900","        'sag' and 'lbfgs' solvers support only l2 penalties. 'elasticnet' is","901","        only supported by the 'saga' solver.","943","    l1_ratio : float or None, optional (default=None)","944","        The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only","945","        used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent","946","        to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent","947","        to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a","948","        combination of L1 and L2.","949","","978","        X_train, y_train, Cs=Cs, l1_ratio=l1_ratio,","979","        fit_intercept=fit_intercept, solver=solver, max_iter=max_iter,","980","        class_weight=class_weight, pos_class=pos_class,","981","        multi_class=multi_class, tol=tol, verbose=verbose, dual=dual,","982","        penalty=penalty, intercept_scaling=intercept_scaling,","983","        random_state=random_state, check_input=False,","984","        max_squared_sum=max_squared_sum, sample_weight=sample_weight)","1020","","1032","    'sag', 'saga' and 'newton-cg' solvers.)","1035","    'liblinear' library, 'newton-cg', 'sag', 'saga' and 'lbfgs' solvers. It can","1036","    handle both dense and sparse input. Use C-ordered arrays or CSR matrices","1042","    regularization, with a dual formulation only for the L2 penalty. The","1043","    Elastic-Net regularization is only supported by the 'saga' solver.","1049","    penalty : str, 'l1', 'l2', or 'elasticnet', optional (default='l2')","1051","        'sag' and 'lbfgs' solvers support only l2 penalties. 'elasticnet' is","1052","        only supported by the 'saga' solver.","1057","    dual : bool, optional (default=False)","1062","    tol : float, optional (default=1e-4)","1065","    C : float, optional (default=1.0)","1070","    fit_intercept : bool, optional (default=True)","1074","    intercept_scaling : float, optional (default=1)","1087","    class_weight : dict or 'balanced', optional (default=None)","1101","    random_state : int, RandomState instance or None, optional (default=None)","1110","             optional (default='liblinear').","1133","    max_iter : int, optional (default=100)","1137","    multi_class : str, {'ovr', 'multinomial', 'auto'}, optional (default='ovr')","1150","    verbose : int, optional (default=0)","1154","    warm_start : bool, optional (default=False)","1170","    l1_ratio : float or None, optional (default=None)","1171","        The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only","1172","        used if ``penalty='elasticnet'`. Setting ``l1_ratio=0`` is equivalent","1173","        to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent","1174","        to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a","1175","        combination of L1 and L2.","1176","","1262","                 multi_class='warn', verbose=0, warm_start=False, n_jobs=None,","1263","                 l1_ratio=None):","1279","        self.l1_ratio = l1_ratio","1304","        solver = _check_solver(self.solver, self.penalty, self.dual)","1305","","1309","        if self.penalty == 'elasticnet':","1310","            if (not isinstance(self.l1_ratio, numbers.Number) or","1311","                    self.l1_ratio < 0 or self.l1_ratio > 1):","1312","                        raise ValueError(\"l1_ratio must be between 0 and 1;\"","1313","                                         \" got (l1_ratio=%r)\" % self.l1_ratio)","1314","        elif self.l1_ratio is not None:","1315","            warnings.warn(\"l1_ratio parameter is only used when penalty is \"","1316","                          \"'elasticnet'. Got \"","1317","                          \"(penalty={})\".format(self.penalty))","1398","                      l1_ratio=self.l1_ratio, fit_intercept=self.fit_intercept,","1399","                      tol=self.tol, verbose=self.verbose, solver=solver,","1403","                      penalty=self.penalty, max_squared_sum=max_squared_sum,","1493","    Elastic-Net penalty is only supported by the saga solver.","1495","    For the grid of `Cs` values and `l1_ratios` values, the best","1496","    hyperparameter is selected by the cross-validator `StratifiedKFold`, but","1497","    it can be changed using the `cv` parameter. The 'newton-cg', 'sag',","1498","    'saga' and 'lbfgs' solvers can warm-start the coefficients (see","1499","    :term:`Glossary<warm_start>`).","1505","    Cs : list of floats or int, optional (default=10)","1512","    fit_intercept : bool, optional (default=True)","1516","    cv : int or cross-validation generator, optional (default=None)","1526","    dual : bool, optional (default=False)","1531","    penalty : str, 'l1', 'l2', or 'elasticnet', optional (default='l2')","1533","        'sag' and 'lbfgs' solvers support only l2 penalties. 'elasticnet' is","1534","        only supported by the 'saga' solver.","1536","    scoring : string, callable, or None, optional (default=None)","1544","             optional (default='lbfgs')","1567","    tol : float, optional (default=1e-4)","1570","    max_iter : int, optional (default=100)","1573","    class_weight : dict or 'balanced', optional (default=None)","1593","    verbose : int, optional (default=0)","1597","    refit : bool, optional (default=True)","1604","    intercept_scaling : float, optional (default=1)","1617","    multi_class : str, {'ovr', 'multinomial', 'auto'}, optional (default='ovr')","1630","    random_state : int, RandomState instance or None, optional (default=None)","1636","    l1_ratios : list of float or None, optional (default=None)","1637","        The list of Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``.","1638","        Only used if ``penalty='elasticnet'``. A value of 0 is equivalent to","1639","        using ``penalty='l2'``, while 1 is equivalent to using","1640","        ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a combination","1641","        of L1 and L2.","1642","","1657","    Cs_ : array, shape (n_cs)","1661","    l1_ratios_ : array, shape (n_l1_ratios)","1662","        Array of l1_ratios used for cross-validation. If no l1_ratio is used","1663","        (i.e. penalty is not 'elasticnet'), this is set to ``[None]``","1664","","1665","    coefs_paths_ : array, shape (n_folds, n_cs, n_features) or \\","1666","                   (n_folds, n_cs, n_features + 1)","1672","        Each dict value has shape ``(n_folds, n_cs, n_features)`` or","1673","        ``(n_folds, n_cs, n_features + 1)`` depending on whether the","1674","        intercept is fit or not. If ``penalty='elasticnet'``, the shape is","1675","        ``(n_folds, n_cs, n_l1_ratios_, n_features)`` or","1676","        ``(n_folds, n_cs, n_l1_ratios_, n_features + 1)``.","1683","        all classes, since this is the multinomial class. Each dict value","1684","        has shape ``(n_folds, n_cs`` or ``(n_folds, n_cs, n_l1_ratios)`` if","1685","        ``penalty='elasticnet'``.","1693","    l1_ratio_ : array, shape (n_classes,) or (n_classes - 1,)","1694","        Array of l1_ratio that maps to the best scores across every class. If","1695","        refit is set to False, then for each class, the best l1_ratio is the","1696","        average of the l1_ratio's that correspond to the best scores for each","1697","        fold.  `l1_ratio_` is of shape(n_classes,) when the problem is binary.","1698","","1702","        If ``penalty='elasticnet'``, the shape is ``(n_classes, n_folds,","1703","        n_cs, n_l1_ratios)`` or ``(1, n_folds, n_cs, n_l1_ratios)``.","1704","","1729","                 random_state=None, l1_ratios=None):","1746","        self.l1_ratios = l1_ratios","1776","        if self.penalty == 'elasticnet':","1777","            if self.l1_ratios is None or len(self.l1_ratios) == 0 or any(","1778","                    (not isinstance(l1_ratio, numbers.Number) or l1_ratio < 0","1779","                     or l1_ratio > 1) for l1_ratio in self.l1_ratios):","1780","                raise ValueError(\"l1_ratios must be a list of numbers between \"","1781","                                 \"0 and 1; got (l1_ratios=%r)\" %","1782","                                 self.l1_ratios)","1783","            l1_ratios_ = self.l1_ratios","1784","        else:","1785","            if self.l1_ratios is not None:","1786","                warnings.warn(\"l1_ratios parameter is only used when penalty \"","1787","                              \"is 'elasticnet'. Got (penalty={})\".format(","1788","                                  self.penalty))","1789","","1790","            l1_ratios_ = [None]","1860","","1872","                      sample_weight=sample_weight,","1873","                      l1_ratio=l1_ratio","1876","            for train, test in folds","1877","            for l1_ratio in l1_ratios_)","1879","        # _log_reg_scoring_path will output different shapes depending on the","1880","        # multi_class param, so we need to reshape the outputs accordingly.","1881","        # Cs is of shape (n_classes . n_folds . n_l1_ratios, n_Cs) and all the","1882","        # rows are equal, so we just take the first one.","1883","        # After reshaping,","1884","        # - scores is of shape (n_classes, n_folds, n_Cs . n_l1_ratios)","1885","        # - coefs_paths is of shape","1886","        #  (n_classes, n_folds, n_Cs . n_l1_ratios, n_features)","1887","        # - n_iter is of shape","1888","        #  (n_classes, n_folds, n_Cs . n_l1_ratios) or","1889","        #  (1, n_folds, n_Cs . n_l1_ratios)","1890","        coefs_paths, Cs, scores, n_iter_ = zip(*fold_coefs_)","1891","        self.Cs_ = Cs[0]","1893","            coefs_paths = np.reshape(","1894","                coefs_paths,","1895","                (len(folds),  len(l1_ratios_) * len(self.Cs_), n_classes, -1)","1896","            )","1897","            # equiv to coefs_paths = np.moveaxis(coefs_paths, (0, 1, 2, 3),","1898","            #                                                 (1, 2, 0, 3))","1899","            coefs_paths = np.swapaxes(coefs_paths, 0, 1)","1900","            coefs_paths = np.swapaxes(coefs_paths, 0, 2)","1901","            self.n_iter_ = np.reshape(","1902","                n_iter_,","1903","                (1, len(folds), len(self.Cs_) * len(l1_ratios_))","1904","            )","1905","            # repeat same scores across all classes","1906","            scores = np.tile(scores, (n_classes, 1, 1))","1908","            coefs_paths = np.reshape(","1909","                coefs_paths,","1910","                (n_classes, len(folds), len(self.Cs_) * len(l1_ratios_),","1911","                 -1)","1912","            )","1913","            self.n_iter_ = np.reshape(","1914","                n_iter_,","1915","                (n_classes, len(folds), len(self.Cs_) * len(l1_ratios_))","1916","            )","1919","        self.coefs_paths_ = dict(zip(classes, coefs_paths))","1922","        self.l1_ratio_ = list()","1931","            else:","1932","                # For multinomial, all scores are the same across classes","1933","                scores = scores[0]","1934","                # coefs_paths will keep its original shape because","1935","                # logistic_regression_path expects it this way","1938","                # best_index is between 0 and (n_Cs . n_l1_ratios - 1)","1939","                # for example, with n_cs=2 and n_l1_ratios=3","1940","                # the layout of scores is","1941","                # [c1, c2, c1, c2, c1, c2]","1942","                #   l1_1 ,  l1_2 ,  l1_3","1945","                best_index_C = best_index % len(self.Cs_)","1946","                C_ = self.Cs_[best_index_C]","1948","","1949","                best_index_l1 = best_index \/\/ len(self.Cs_)","1950","                l1_ratio_ = l1_ratios_[best_index_l1]","1951","                self.l1_ratio_.append(l1_ratio_)","1952","","1954","                    coef_init = np.mean(coefs_paths[:, :, best_index, :],","1955","                                        axis=1)","1971","                    sample_weight=sample_weight,","1972","                    l1_ratio=l1_ratio_)","1976","                # Take the best scores across every fold and the average of","1977","                # all coefficients corresponding to the best scores.","1979","                if self.multi_class == 'ovr':","1980","                    w = np.mean([coefs_paths[i, best_indices[i], :]","1981","                                 for i in range(len(folds))], axis=0)","1982","                else:","1983","                    w = np.mean([coefs_paths[:, i, best_indices[i], :]","1984","                                 for i in range(len(folds))], axis=0)","1985","","1986","                best_indices_C = best_indices % len(self.Cs_)","1987","                self.C_.append(np.mean(self.Cs_[best_indices_C]))","1988","","1989","                best_indices_l1 = best_indices \/\/ len(self.Cs_)","1990","                self.l1_ratio_.append(np.mean(l1_ratios_[best_indices_l1]))","1994","                self.l1_ratio_ = np.tile(self.l1_ratio_, n_classes)","2004","        self.l1_ratio_ = np.asarray(self.l1_ratio_)","2005","        self.l1_ratios_ = np.asarray(l1_ratios_)","2006","        # if elasticnet was used, add the l1_ratios dimension to some","2007","        # attributes","2008","        if self.l1_ratios is not None:","2009","            for cls, coefs_path in self.coefs_paths_.items():","2010","                self.coefs_paths_[cls] = coefs_path.reshape(","2011","                    (len(folds), self.Cs_.size, self.l1_ratios_.size, -1))","2012","            for cls, score in self.scores_.items():","2013","                self.scores_[cls] = score.reshape(","2014","                    (len(folds), self.Cs_.size, self.l1_ratios_.size))","2015","            self.n_iter_ = self.n_iter_.reshape(","2016","                (-1, len(folds), self.Cs_.size, self.l1_ratios_.size))","2017",""],"delete":["439","    all_penalties = ['l1', 'l2']","476","","483","                             max_squared_sum=None, sample_weight=None):","552","    penalty : str, 'l1' or 'l2'","554","        'sag' and 'lbfgs' solvers support only l2 penalties.","784","            else:","818","                          max_squared_sum=None, sample_weight=None):","880","    penalty : str, 'l1' or 'l2'","882","        'sag' and 'lbfgs' solvers support only l2 penalties.","952","        X_train, y_train, Cs=Cs, fit_intercept=fit_intercept,","953","        solver=solver, max_iter=max_iter, class_weight=class_weight,","954","        pos_class=pos_class, multi_class=multi_class,","955","        tol=tol, verbose=verbose, dual=dual, penalty=penalty,","956","        intercept_scaling=intercept_scaling, random_state=random_state,","957","        check_input=False, max_squared_sum=max_squared_sum,","958","        sample_weight=sample_weight)","1005","    'sag' and 'newton-cg' solvers.)","1008","    'liblinear' library, 'newton-cg', 'sag' and 'lbfgs' solvers. It can handle","1009","    both dense and sparse input. Use C-ordered arrays or CSR matrices","1015","    regularization, with a dual formulation only for the L2 penalty.","1021","    penalty : str, 'l1' or 'l2', default: 'l2'","1023","        'sag' and 'lbfgs' solvers support only l2 penalties.","1028","    dual : bool, default: False","1033","    tol : float, default: 1e-4","1036","    C : float, default: 1.0","1041","    fit_intercept : bool, default: True","1045","    intercept_scaling : float, default 1.","1058","    class_weight : dict or 'balanced', default: None","1072","    random_state : int, RandomState instance or None, optional, default: None","1081","             default: 'liblinear'.","1104","    max_iter : int, default: 100","1108","    multi_class : str, {'ovr', 'multinomial', 'auto'}, default: 'ovr'","1121","    verbose : int, default: 0","1125","    warm_start : bool, default: False","1226","                 multi_class='warn', verbose=0, warm_start=False, n_jobs=None):","1276","        solver = _check_solver(self.solver, self.penalty, self.dual)","1277","","1351","                      fit_intercept=self.fit_intercept, tol=self.tol,","1352","                      verbose=self.verbose, solver=solver,","1356","                      penalty=self.penalty,","1357","                      max_squared_sum=max_squared_sum,","1448","    For the grid of Cs values (that are set by default to be ten values in","1449","    a logarithmic scale between 1e-4 and 1e4), the best hyperparameter is","1450","    selected by the cross-validator StratifiedKFold, but it can be changed","1451","    using the cv parameter. In the case of newton-cg and lbfgs solvers,","1452","    we warm start along the path i.e guess the initial coefficients of the","1453","    present fit to be the coefficients got after convergence in the previous","1454","    fit, so it is supposed to be faster for high-dimensional dense data.","1455","","1456","    For a multiclass problem, the hyperparameters for each class are computed","1457","    using the best scores got by doing a one-vs-rest in parallel across all","1458","    folds and classes. Hence this is not the true multinomial loss.","1464","    Cs : list of floats | int","1471","    fit_intercept : bool, default: True","1475","    cv : integer or cross-validation generator, default: None","1485","    dual : bool","1490","    penalty : str, 'l1' or 'l2'","1492","        'sag' and 'lbfgs' solvers support only l2 penalties.","1494","    scoring : string, callable, or None","1502","             default: 'lbfgs'.","1525","    tol : float, optional","1528","    max_iter : int, optional","1531","    class_weight : dict or 'balanced', optional","1551","    verbose : int","1555","    refit : bool","1562","    intercept_scaling : float, default 1.","1575","    multi_class : str, {'ovr', 'multinomial', 'auto'}, default: 'ovr'","1588","    random_state : int, RandomState instance or None, optional, default None","1608","    Cs_ : array","1612","    coefs_paths_ : array, shape ``(n_folds, len(Cs_), n_features)`` or \\","1613","                   ``(n_folds, len(Cs_), n_features + 1)``","1619","        Each dict value has shape ``(n_folds, len(Cs_), n_features)`` or","1620","        ``(n_folds, len(Cs_), n_features + 1)`` depending on whether the","1621","        intercept is fit or not.","1628","        all classes, since this is the multinomial class.","1629","        Each dict value has shape (n_folds, len(Cs))","1664","                 random_state=None):","1790","                      sample_weight=sample_weight","1793","            for train, test in folds)","1796","            multi_coefs_paths, Cs, multi_scores, n_iter_ = zip(*fold_coefs_)","1797","            multi_coefs_paths = np.asarray(multi_coefs_paths)","1798","            multi_scores = np.asarray(multi_scores)","1799","","1800","            # This is just to maintain API similarity between the ovr and","1801","            # multinomial option.","1802","            # Coefs_paths in now n_folds X len(Cs) X n_classes X n_features","1803","            # we need it to be n_classes X len(Cs) X n_folds X n_features","1804","            # to be similar to \"ovr\".","1805","            coefs_paths = np.rollaxis(multi_coefs_paths, 2, 0)","1806","","1807","            # Multinomial has a true score across all labels. Hence the","1808","            # shape is n_folds X len(Cs). We need to repeat this score","1809","            # across all labels for API similarity.","1810","            scores = np.tile(multi_scores, (n_classes, 1, 1))","1811","            self.Cs_ = Cs[0]","1812","            self.n_iter_ = np.reshape(n_iter_, (1, len(folds),","1813","                                                len(self.Cs_)))","1814","","1816","            coefs_paths, Cs, scores, n_iter_ = zip(*fold_coefs_)","1817","            self.Cs_ = Cs[0]","1818","            coefs_paths = np.reshape(coefs_paths, (n_classes, len(folds),","1819","                                                   len(self.Cs_), -1))","1820","            self.n_iter_ = np.reshape(n_iter_, (n_classes, len(folds),","1821","                                                len(self.Cs_)))","1822","","1823","        self.coefs_paths_ = dict(zip(classes, coefs_paths))","1830","","1831","        # hack to iterate only once for multinomial case.","1832","        if multi_class == 'multinomial':","1833","            scores = multi_scores","1834","            coefs_paths = multi_coefs_paths","1835","","1840","                # The scores_ \/ coefs_paths_ dict have unencoded class","1841","                # labels as their keys","1848","                C_ = self.Cs_[best_index]","1851","                    coef_init = np.mean(coefs_paths[:, best_index, :, :],","1852","                                        axis=0)","1868","                    sample_weight=sample_weight)","1872","                # Take the best scores across every fold and the average of all","1873","                # coefficients corresponding to the best scores.","1875","                w = np.mean([coefs_paths[i][best_indices[i]]","1876","                             for i in range(len(folds))], axis=0)","1877","                self.C_.append(np.mean(self.Cs_[best_indices]))"]}],"sklearn\/linear_model\/sag_fast.pyx":[{"add":["610","                    last_update_ind = feature_hist[feature_ind]"],"delete":["570","     ","611","                    last_update_ind = feature_hist[feature_ind] - 1"]}],"sklearn\/linear_model\/sag.py":[{"add":["219","        fit_intercept=True, intercept_scaling=1, l1_ratio=None, max_iter=100,"],"delete":["219","        fit_intercept=True, intercept_scaling=1, max_iter=100,"]}],"examples\/linear_model\/plot_logistic_l1_l2_sparsity.py":[{"add":["6","L1, L2 and Elastic-Net penalty are used for different values of C. We can see","7","that large values of C give more freedom to the model.  Conversely, smaller","8","values of C constrain the model more. In the L1 penalty case, this leads to","9","sparser solutions. As expected, the Elastic-Net penalty sparsity is between","10","that of L1 and L2.","38","l1_ratio = 0.5  # L1 weight in the Elastic-Net regularization","39","","40","fig, axes = plt.subplots(3, 3)","43","for i, (C, axes_row) in enumerate(zip((1, 0.1, 0.01), axes)):","47","    clf_en_LR = LogisticRegression(C=C, penalty='elasticnet', solver='saga',","48","                                   l1_ratio=l1_ratio, tol=0.01)","51","    clf_en_LR.fit(X, y)","55","    coef_en_LR = clf_en_LR.coef_.ravel()","62","    sparsity_en_LR = np.mean(coef_en_LR == 0) * 100","65","    print(\"{:<40} {:.2f}%\".format(\"Sparsity with L1 penalty:\", sparsity_l1_LR))","66","    print(\"{:<40} {:.2f}%\".format(\"Sparsity with Elastic-Net penalty:\",","67","                                  sparsity_en_LR))","68","    print(\"{:<40} {:.2f}%\".format(\"Sparsity with L2 penalty:\", sparsity_l2_LR))","69","    print(\"{:<40} {:.2f}\".format(\"Score with L1 penalty:\",","70","                                 clf_l1_LR.score(X, y)))","71","    print(\"{:<40} {:.2f}\".format(\"Score with Elastic-Net penalty:\",","72","                                 clf_en_LR.score(X, y)))","73","    print(\"{:<40} {:.2f}\".format(\"Score with L2 penalty:\",","74","                                 clf_l2_LR.score(X, y)))","77","        axes_row[0].set_title(\"L1 penalty\")","78","        axes_row[1].set_title(\"Elastic-Net\\nl1_ratio = %s\" % l1_ratio)","79","        axes_row[2].set_title(\"L2 penalty\")","81","    for ax, coefs in zip(axes_row, [coef_l1_LR, coef_en_LR, coef_l2_LR]):","82","        ax.imshow(np.abs(coefs.reshape(8, 8)), interpolation='nearest',","83","                  cmap='binary', vmax=1, vmin=0)","84","        ax.set_xticks(())","85","        ax.set_yticks(())","87","    axes_row[0].set_ylabel('C = %s' % C)"],"delete":["6","L1 and L2 penalty are used for different values of C. We can see that large","7","values of C give more freedom to the model.  Conversely, smaller values of C","8","constrain the model more. In the L1 penalty case, this leads to sparser","9","solutions.","39","for i, C in enumerate((1, 0.1, 0.01)):","56","    print(\"Sparsity with L1 penalty: %.2f%%\" % sparsity_l1_LR)","57","    print(\"score with L1 penalty: %.4f\" % clf_l1_LR.score(X, y))","58","    print(\"Sparsity with L2 penalty: %.2f%%\" % sparsity_l2_LR)","59","    print(\"score with L2 penalty: %.4f\" % clf_l2_LR.score(X, y))","61","    l1_plot = plt.subplot(3, 2, 2 * i + 1)","62","    l2_plot = plt.subplot(3, 2, 2 * (i + 1))","64","        l1_plot.set_title(\"L1 penalty\")","65","        l2_plot.set_title(\"L2 penalty\")","67","    l1_plot.imshow(np.abs(coef_l1_LR.reshape(8, 8)), interpolation='nearest',","68","                   cmap='binary', vmax=1, vmin=0)","69","    l2_plot.imshow(np.abs(coef_l2_LR.reshape(8, 8)), interpolation='nearest',","70","                   cmap='binary', vmax=1, vmin=0)","71","    plt.text(-8, 3, \"C = %.2f\" % C)","73","    l1_plot.set_xticks(())","74","    l1_plot.set_yticks(())","75","    l2_plot.set_xticks(())","76","    l2_plot.set_yticks(())"]}]}},"b42a5152af2cf51d9d7c30c40abc56bf0fe327f5":{"changes":{"sklearn\/linear_model\/tests\/test_logistic.py":"MODIFY","sklearn\/utils\/testing.py":"MODIFY","sklearn\/utils\/estimator_checks.py":"MODIFY","sklearn\/utils\/__init__.py":"MODIFY"},"diff":{"sklearn\/linear_model\/tests\/test_logistic.py":[{"add":["0","import os","12","from sklearn.utils import compute_class_weight, _IS_32BIT","1258","@pytest.mark.parametrize('multi_class', ['ovr', 'multinomial'])","1259","def test_dtype_match(multi_class):","1268","    solver = 'newton-cg'","1270","    # Check type consistency","1271","    lr_32 = LogisticRegression(solver=solver, multi_class=multi_class,","1272","                               random_state=42)","1273","    lr_32.fit(X_32, y_32)","1274","    assert_equal(lr_32.coef_.dtype, X_32.dtype)","1276","    # check consistency with sparsity","1277","    lr_32_sparse = LogisticRegression(solver=solver,","1278","                                      multi_class=multi_class,","1279","                                      random_state=42)","1280","    lr_32_sparse.fit(X_sparse_32, y_32)","1281","    assert_equal(lr_32_sparse.coef_.dtype, X_sparse_32.dtype)","1283","    # Check accuracy consistency","1284","    lr_64 = LogisticRegression(solver=solver, multi_class=multi_class,","1285","                               random_state=42)","1286","    lr_64.fit(X_64, y_64)","1287","    assert_equal(lr_64.coef_.dtype, X_64.dtype)","1288","","1289","    rtol = 1e-6","1290","    if os.name == 'nt' and _IS_32BIT:","1291","        # FIXME","1292","        rtol = 1e-2","1293","","1294","    assert_allclose(lr_32.coef_, lr_64.coef_.astype(np.float32), rtol=rtol)"],"delete":["11","from sklearn.utils import compute_class_weight","1257","def test_dtype_match():","1266","    for solver in ['newton-cg']:","1267","        for multi_class in ['ovr', 'multinomial']:","1269","            # Check type consistency","1270","            lr_32 = LogisticRegression(solver=solver, multi_class=multi_class,","1271","                                       random_state=42)","1272","            lr_32.fit(X_32, y_32)","1273","            assert_equal(lr_32.coef_.dtype, X_32.dtype)","1275","            # check consistency with sparsity","1276","            lr_32_sparse = LogisticRegression(solver=solver,","1277","                                              multi_class=multi_class,","1278","                                              random_state=42)","1279","            lr_32_sparse.fit(X_sparse_32, y_32)","1280","            assert_equal(lr_32_sparse.coef_.dtype, X_sparse_32.dtype)","1282","            # Check accuracy consistency","1283","            lr_64 = LogisticRegression(solver=solver, multi_class=multi_class,","1284","                                       random_state=42)","1285","            lr_64.fit(X_64, y_64)","1286","            assert_equal(lr_64.coef_.dtype, X_64.dtype)","1287","            assert_almost_equal(lr_32.coef_, lr_64.coef_.astype(np.float32))"]}],"sklearn\/utils\/testing.py":[{"add":["48","from sklearn.utils import deprecated, IS_PYPY, _IS_32BIT","759","    skip_if_32bit = pytest.mark.skipif(_IS_32BIT,"],"delete":["17","import struct","49","from sklearn.utils import deprecated, IS_PYPY","760","    skip_if_32bit = pytest.mark.skipif(8 * struct.calcsize(\"P\") == 32,"]}],"sklearn\/utils\/estimator_checks.py":[{"add":["15","from sklearn.utils import IS_PYPY, _IS_32BIT","939","    if name in ('CCA', 'LocallyLinearEmbedding', 'KernelPCA') and _IS_32BIT:","1017","    if name in ('CCA', 'LocallyLinearEmbedding', 'KernelPCA') and _IS_32BIT:"],"delete":["8","import struct","16","from sklearn.utils import IS_PYPY","406","def _is_32bit():","407","    \"\"\"Detect if process is 32bit Python.\"\"\"","408","    return struct.calcsize('P') * 8 == 32","409","","410","","945","    if name in ('CCA', 'LocallyLinearEmbedding', 'KernelPCA') and _is_32bit():","1023","    if name in ('CCA', 'LocallyLinearEmbedding', 'KernelPCA') and _is_32bit():"]}],"sklearn\/utils\/__init__.py":[{"add":["5","import struct","37","_IS_32BIT = 8 * struct.calcsize(\"P\") == 32"],"delete":[]}]}},"03c3af5bdec29903567fa3d63f2e6776a2b3041b":{"changes":{"sklearn\/datasets\/tests\/data\/openml\/1119\/api-v1-json-data-1119.json.gz":"ADD","sklearn\/datasets\/tests\/data\/openml\/1119\/api-v1-json-data-list-data_name-adult-census-limit-2-data_version-1.json.gz":"ADD","doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/datasets\/tests\/data\/openml\/1119\/api-v1-json-data-features-1119.json.gz":"ADD","sklearn\/datasets\/tests\/test_openml.py":"MODIFY","sklearn\/datasets\/tests\/data\/openml\/1119\/data-v1-download-54002.arff.gz":"ADD","sklearn\/datasets\/tests\/data\/openml\/1119\/api-v1-json-data-list-data_name-adult-census-limit-2-status-active-.json.gz":"ADD","sklearn\/datasets\/openml.py":"MODIFY"},"diff":{"sklearn\/datasets\/tests\/data\/openml\/1119\/api-v1-json-data-1119.json.gz":[{"add":[],"delete":[]}],"sklearn\/datasets\/tests\/data\/openml\/1119\/api-v1-json-data-list-data_name-adult-census-limit-2-data_version-1.json.gz":[{"add":[],"delete":[]}],"doc\/whats_new\/v0.20.rst":[{"add":["42","- |Fix| :func:`datasets.fetch_openml` to correctly handle ignore attributes and","43","  row id attributes. :issue:`12330` by :user:`Jan N. van Rijn <janvanrijn>`.","44",""],"delete":[]}],"sklearn\/datasets\/tests\/data\/openml\/1119\/api-v1-json-data-features-1119.json.gz":[{"add":[],"delete":[]}],"sklearn\/datasets\/tests\/test_openml.py":[{"add":["413","def test_fetch_openml_adultcensus(monkeypatch, gzip_response):","414","    # Check because of the numeric row attribute (issue #12329)","415","    data_id = 1119","416","    data_name = 'adult-census'","417","    data_version = 1","418","    target_column = 'class'","419","    # Not all original instances included for space reasons","420","    expected_observations = 10","421","    expected_features = 14","422","    expected_missing = 0","423","    _monkey_patch_webbased_functions(monkeypatch, data_id, gzip_response)","424","    _fetch_dataset_from_openml(data_id, data_name, data_version, target_column,","425","                               expected_observations, expected_features,","426","                               expected_missing,","427","                               np.float64, object, expect_sparse=False,","428","                               compare_default_target=True)","429","","430","","431","@pytest.mark.parametrize('gzip_response', [True, False])"],"delete":[]}],"sklearn\/datasets\/tests\/data\/openml\/1119\/data-v1-download-54002.arff.gz":[{"add":[],"delete":[]}],"sklearn\/datasets\/tests\/data\/openml\/1119\/api-v1-json-data-list-data_name-adult-census-limit-2-status-active-.json.gz":[{"add":[],"delete":[]}],"sklearn\/datasets\/openml.py":[{"add":["371","def _valid_data_column_names(features_list, target_columns):","372","    # logic for determining on which columns can be learned. Note that from the","373","    # OpenML guide follows that columns that have the `is_row_identifier` or","374","    # `is_ignore` flag, these can not be learned on. Also target columns are","375","    # excluded.","376","    valid_data_column_names = []","377","    for feature in features_list:","378","        if (feature['name'] not in target_columns","379","                and feature['is_ignore'] != 'true'","380","                and feature['is_row_identifier'] != 'true'):","381","            valid_data_column_names.append(feature['name'])","382","    return valid_data_column_names","383","","384","","538","    data_columns = _valid_data_column_names(features_list,","539","                                            target_column)","569","    # nominal attributes is a dict mapping from the attribute name to the","570","    # possible values. Includes also the target column (which will be popped","571","    # off below, before it will be packed in the Bunch object)","573","                          if isinstance(v, list) and","574","                          k in data_columns + target_column}","575",""],"delete":["524","    data_columns = [feature['name'] for feature in features_list","525","                    if (feature['name'] not in target_column and","526","                        feature['is_ignore'] != 'true' and","527","                        feature['is_row_identifier'] != 'true')]","558","                          if isinstance(v, list)}","559","    for feature in features_list:","560","        if 'true' in (feature['is_row_identifier'],","561","                      feature['is_ignore']) and (feature['name'] not in","562","                                                 target_column):","563","            del nominal_attributes[feature['name']]"]}]}},"02dc9ed680e7f53f1b0d410dcdd37341c7958eb1":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/ensemble\/tests\/test_forest.py":"MODIFY","sklearn\/tree\/_tree.pyx":"MODIFY","sklearn\/ensemble\/tests\/test_gradient_boosting.py":"MODIFY","sklearn\/tree\/tests\/test_tree.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["20","- Decision trees and derived ensembles when both `max_depth` and","21","  `max_leaf_nodes` are set. (bug fix)","129","- |Fix| Fixed an issue with :class:`tree.BaseDecisionTree`","130","  and consequently all estimators based","131","  on it, including :class:`tree.DecisionTreeClassifier`,","132","  :class:`tree.DecisionTreeRegressor`, :class:`tree.ExtraTreeClassifier`,","133","  and :class:`tree.ExtraTreeRegressor`, where they used to exceed the given","134","  ``max_depth`` by 1 while expanding the tree if ``max_leaf_nodes`` and","135","  ``max_depth`` were both specified by the user. Please note that this also","136","  affects all ensemble methods using decision trees.","137","  :pr:`12344` by :user:`Adrin Jalali <adrinjalali>`.","138","","139",""],"delete":[]}],"sklearn\/ensemble\/tests\/test_forest.py":[{"add":["713","    assert_equal(est.estimators_[0].get_depth(), 1)","717","    assert_equal(est.estimators_[0].get_depth(), 1)"],"delete":["713","    assert_greater(est.estimators_[0].tree_.max_depth, 1)","717","    assert_equal(est.estimators_[0].tree_.max_depth, 1)"]}],"sklearn\/tree\/_tree.pyx":[{"add":["452","        is_leaf = (depth >= self.max_depth or"],"delete":["452","        is_leaf = (depth > self.max_depth or"]}],"sklearn\/ensemble\/tests\/test_gradient_boosting.py":[{"add":["1105","    assert_equal(tree.max_depth, 1)"],"delete":["1105","    assert_greater(tree.max_depth, 1)"]}],"sklearn\/tree\/tests\/test_tree.py":[{"add":["1233","        assert_equal(est.get_depth(), 1)"],"delete":["1212","    from sklearn.tree._tree import TREE_LEAF","1234","        assert_greater(est.get_depth(), 1)"]}]}},"fe35e253aeb37057af9ae242700fd5c2e6f20c25":{"changes":{"sklearn\/linear_model\/tests\/test_logistic.py":"MODIFY"},"diff":{"sklearn\/linear_model\/tests\/test_logistic.py":[{"add":["1167","@pytest.mark.parametrize(\"random_seed\", [42])","1168","@pytest.mark.parametrize(\"penalty\", [\"l1\", \"l2\"])","1169","def test_logistic_regression_cv_refit(random_seed, penalty):","1170","    # Test that when refit=True, logistic regression cv with the saga solver","1171","    # converges to the same solution as logistic regression with a fixed","1172","    # regularization parameter.","1173","    # Internally the LogisticRegressionCV model uses a warm start to refit on","1174","    # the full data model with the optimal C found by CV. As the penalized","1175","    # logistic regression loss is convex, we should still recover exactly","1176","    # the same solution as long as the stopping criterion is strict enough (and","1177","    # that there are no exactly duplicated features when penalty='l1').","1178","    X, y = make_classification(n_samples=50, n_features=20,","1179","                               random_state=random_seed)","1180","    common_params = dict(","1181","        solver='saga',","1182","        penalty=penalty,","1183","        random_state=random_seed,","1184","        max_iter=10000,","1185","        tol=1e-12,","1186","    )","1187","    lr_cv = LogisticRegressionCV(Cs=[1.0], refit=True, **common_params)","1189","    lr = LogisticRegression(C=1.0, **common_params)","1191","    assert_array_almost_equal(lr_cv.coef_, lr.coef_)"],"delete":["1167","def test_logreg_cv_penalty():","1168","    # Test that the correct penalty is passed to the final fit.","1169","    X, y = make_classification(n_samples=50, n_features=20, random_state=0)","1170","    lr_cv = LogisticRegressionCV(penalty=\"l1\", Cs=[1.0], solver='saga')","1172","    lr = LogisticRegression(penalty=\"l1\", C=1.0, solver='saga')","1174","    assert_equal(np.count_nonzero(lr_cv.coef_), np.count_nonzero(lr.coef_))"]}]}},"b24ef3631ac0d43c25fe5eab7081b2c6a07d45e3":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/linear_model\/tests\/test_logistic.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["89","Known Major Bugs","90","----------------","91","","92","* :issue:`11924`: :class:`LogisticRegressionCV` with `solver='lbfgs'` and","93","  `multi_class='multinomial'` may be non-deterministic or otherwise broken on","94","  MacOS. This appears to be the case on Travis CI servers, but has not been","95","  confirmed on personal MacBooks! This issue has been present in previous","96","  releases.","97",""],"delete":[]}],"sklearn\/linear_model\/tests\/test_logistic.py":[{"add":["1","import sys","1431","        if sys.platform == 'darwin' and solver == 'lbfgs':","1432","            pytest.xfail('Issue #11924: LogisticRegressionCV(solver=\"lbfgs\", '","1433","                         'multi_class=\"multinomial\") is nondterministic on '","1434","                         'MacOS.')  # pragma: no cover"],"delete":[]}]}},"36536c6f46ac060d4b9c9e48d79d42fafa3fb344":{"changes":{"sklearn\/model_selection\/tests\/test_search.py":"MODIFY","sklearn\/datasets\/mlcomp.py":"MODIFY","sklearn\/cluster\/tests\/test_k_means.py":"MODIFY","sklearn\/externals\/_arff.py":"MODIFY"},"diff":{"sklearn\/model_selection\/tests\/test_search.py":[{"add":["135","    [(0, TypeError, r'Parameter grid is not a dict or a list \\(0\\)'),","136","     ([{'foo': [0]}, 0], TypeError, r'Parameter grid is not a dict \\(0\\)'),","138","      r\"\\(key='foo', value=0\\)\")]"],"delete":["135","    [(0, TypeError, 'Parameter grid is not a dict or a list \\(0\\)'),","136","     ([{'foo': [0]}, 0], TypeError, 'Parameter grid is not a dict \\(0\\)'),","138","      \"\\(key='foo', value=0\\)\")]"]}],"sklearn\/datasets\/mlcomp.py":[{"add":["26","    r\"\"\"Load a datasets as downloaded from http:\/\/mlcomp.org"],"delete":["26","    \"\"\"Load a datasets as downloaded from http:\/\/mlcomp.org"]}],"sklearn\/cluster\/tests\/test_k_means.py":[{"add":["887","    msg = r\"The shape of the initial centers \\(\\(4L?, 4L?\\)\\) \" \\","971","    assert_raises_regex(ValueError, r'len\\(sample_weight\\)', km.fit, X,"],"delete":["887","    msg = \"The shape of the initial centers \\(\\(4L?, 4L?\\)\\) \" \\","971","    assert_raises_regex(ValueError, 'len\\(sample_weight\\)', km.fit, X,"]}],"sklearn\/externals\/_arff.py":[{"add":["643","        res = re.sub(r'^\\%( )?', '', s)"],"delete":["643","        res = re.sub('^\\%( )?', '', s)"]}]}},"d25da1be2054170034548b63ce87bd459ed8bee0":{"changes":{"sklearn\/neighbors\/tests\/test_neighbors.py":"MODIFY","examples\/compose\/plot_compare_reduction.py":"MODIFY","sklearn\/tests\/test_site_joblib.py":"MODIFY","sklearn\/utils\/testing.py":"MODIFY","sklearn\/covariance\/graph_lasso_.py":"MODIFY","README.rst":"MODIFY","sklearn\/utils\/_joblib.py":"MODIFY","sklearn\/datasets\/rcv1.py":"MODIFY","sklearn\/multioutput.py":"MODIFY","sklearn\/utils\/tests\/test_utils.py":"MODIFY","doc\/tutorial\/basic\/tutorial.rst":"MODIFY","benchmarks\/bench_saga.py":"MODIFY","sklearn\/ensemble\/base.py":"MODIFY","sklearn\/model_selection\/_search.py":"MODIFY","sklearn\/datasets\/species_distributions.py":"MODIFY","build_tools\/circle\/build_doc.sh":"MODIFY","sklearn\/datasets\/california_housing.py":"MODIFY","sklearn\/datasets\/olivetti_faces.py":"MODIFY","sklearn\/linear_model\/logistic.py":"MODIFY","sklearn\/ensemble\/tests\/test_forest.py":"MODIFY","sklearn\/linear_model\/omp.py":"MODIFY","sklearn\/utils\/__init__.py":"MODIFY","doc\/modules\/model_persistence.rst":"MODIFY","sklearn\/cluster\/mean_shift_.py":"MODIFY","sklearn\/ensemble\/forest.py":"MODIFY","sklearn\/metrics\/tests\/test_score_objects.py":"MODIFY","sklearn\/tests\/test_pipeline.py":"MODIFY","sklearn\/datasets\/covtype.py":"MODIFY","examples\/applications\/wikipedia_principal_eigenvector.py":"MODIFY","sklearn\/metrics\/pairwise.py":"MODIFY","sklearn\/utils\/validation.py":"MODIFY","benchmarks\/bench_mnist.py":"MODIFY","sklearn\/neighbors\/base.py":"MODIFY","sklearn\/pipeline.py":"MODIFY","benchmarks\/bench_tsne_mnist.py":"MODIFY","sklearn\/datasets\/svmlight_format.py":"MODIFY","sklearn\/linear_model\/tests\/test_sgd.py":"MODIFY","sklearn\/multiclass.py":"MODIFY","sklearn\/compose\/_column_transformer.py":"MODIFY","sklearn\/utils\/tests\/test_validation.py":"MODIFY","sklearn\/datasets\/lfw.py":"MODIFY","sklearn\/tests\/test_docstring_parameters.py":"MODIFY","sklearn\/linear_model\/coordinate_descent.py":"MODIFY","sklearn\/decomposition\/online_lda.py":"MODIFY","sklearn\/manifold\/mds.py":"MODIFY","sklearn\/datasets\/kddcup99.py":"MODIFY","benchmarks\/bench_plot_nmf.py":"MODIFY","examples\/cluster\/plot_feature_agglomeration_vs_univariate_selection.py":"MODIFY","sklearn\/neighbors\/tests\/test_kde.py":"MODIFY","sklearn\/utils\/tests\/test_estimator_checks.py":"MODIFY","benchmarks\/bench_covertype.py":"MODIFY","sklearn\/ensemble\/partial_dependence.py":"MODIFY","doc\/whats_new\/v0.20.rst":"MODIFY","doc\/modules\/classes.rst":"MODIFY","sklearn\/feature_selection\/rfe.py":"MODIFY","sklearn\/linear_model\/theil_sen.py":"MODIFY","sklearn\/datasets\/twenty_newsgroups.py":"MODIFY","benchmarks\/bench_rcv1_logreg_convergence.py":"MODIFY","sklearn\/utils\/estimator_checks.py":"MODIFY","sklearn\/tests\/test_multioutput.py":"MODIFY","sklearn\/linear_model\/base.py":"MODIFY","sklearn\/ensemble\/bagging.py":"MODIFY","sklearn\/ensemble\/tests\/test_bagging.py":"MODIFY","sklearn\/linear_model\/stochastic_gradient.py":"MODIFY","sklearn\/model_selection\/_validation.py":"MODIFY","sklearn\/decomposition\/dict_learning.py":"MODIFY","sklearn\/ensemble\/voting_classifier.py":"MODIFY","sklearn\/cluster\/k_means_.py":"MODIFY","sklearn\/linear_model\/least_angle.py":"MODIFY"},"diff":{"sklearn\/neighbors\/tests\/test_neighbors.py":[{"add":["29","from sklearn.utils._joblib import joblib","30","from sklearn.utils._joblib import parallel_backend","51","JOBLIB_BACKENDS = list(joblib.parallel.BACKENDS.keys())","1325","@pytest.mark.parametrize('backend', JOBLIB_BACKENDS)"],"delete":["29","from sklearn.externals.joblib import parallel_backend","1323","@pytest.mark.parametrize('backend', ['loky', 'multiprocessing', 'threading'])"]}],"examples\/compose\/plot_compare_reduction.py":[{"add":["107","from joblib import Memory"],"delete":["107","from sklearn.utils import Memory"]}],"sklearn\/tests\/test_site_joblib.py":[{"add":["4","from sklearn.utils._joblib import Parallel, delayed, Memory, parallel_backend"],"delete":["4","from sklearn.utils import Parallel, delayed, Memory, parallel_backend"]}],"sklearn\/utils\/testing.py":[{"add":["46","from sklearn.utils._joblib import joblib"],"delete":["46","from sklearn.externals import joblib"]}],"sklearn\/covariance\/graph_lasso_.py":[{"add":["25","from ..utils._joblib import Parallel, delayed"],"delete":["25","from ..utils import Parallel, delayed"]}],"README.rst":[{"add":["60","require scikit-image >= 0.11.3, a few examples require pandas >= 0.17.1","61","and a few example require joblib >= 0.11."],"delete":["60","require scikit-image >= 0.11.3 and a few examples require pandas >= 0.17.1."]}],"sklearn\/utils\/_joblib.py":[{"add":["12","        import joblib","14","        from joblib import dump, load","15","        from joblib import __version__","16","        from joblib import effective_n_jobs","17","        from joblib import hash","18","        from joblib import cpu_count, Parallel, Memory, delayed","19","        from joblib import parallel_backend, register_parallel_backend","21","    from ..externals import joblib","22","    from ..externals.joblib import logger","23","    from ..externals.joblib import dump, load","24","    from ..externals.joblib import __version__","25","    from ..externals.joblib import effective_n_jobs","26","    from ..externals.joblib import hash","27","    from ..externals.joblib import cpu_count, Parallel, Memory, delayed","28","    from ..externals.joblib import parallel_backend, register_parallel_backend","29","","30","","31","__all__ = [\"parallel_backend\", \"register_parallel_backend\", \"cpu_count\",","32","           \"Parallel\", \"Memory\", \"delayed\", \"effective_n_jobs\", \"hash\",","33","           \"logger\", \"dump\", \"load\", \"joblib\", \"__version__\"]"],"delete":["12","        from joblib import __all__","13","        from joblib import *  # noqa","14","        from joblib import __version__","17","    from ..externals.joblib import __all__   # noqa","18","    from ..externals.joblib import *  # noqa","19","    from ..externals.joblib import __version__  # noqa","20","    from ..externals.joblib import logger  # noqa"]}],"sklearn\/datasets\/rcv1.py":[{"add":["24","from ..utils import _joblib","184","        _joblib.dump(X, samples_path, compress=9)","185","        _joblib.dump(sample_id, sample_id_path, compress=9)","192","        X = _joblib.load(samples_path)","193","        sample_id = _joblib.load(sample_id_path)","243","        _joblib.dump(y, sample_topics_path, compress=9)","244","        _joblib.dump(categories, topics_path, compress=9)","246","        y = _joblib.load(sample_topics_path)","247","        categories = _joblib.load(topics_path)"],"delete":["24","from ..externals import joblib","184","        joblib.dump(X, samples_path, compress=9)","185","        joblib.dump(sample_id, sample_id_path, compress=9)","192","        X = joblib.load(samples_path)","193","        sample_id = joblib.load(sample_id_path)","243","        joblib.dump(y, sample_topics_path, compress=9)","244","        joblib.dump(categories, topics_path, compress=9)","246","        y = joblib.load(sample_topics_path)","247","        categories = joblib.load(topics_path)"]}],"sklearn\/multioutput.py":[{"add":["27","from .utils._joblib import Parallel, delayed"],"delete":["27","from .utils import Parallel, delayed"]}],"sklearn\/utils\/tests\/test_utils.py":[{"add":["277","","278","","279","def dummy_func():","280","    pass","281","","282","","283","def test_deprecation_joblib_api(tmpdir):","284","    def check_warning(*args, **kw):","285","        return assert_warns_message(","286","            DeprecationWarning, \"deprecated in version 0.20.1\", *args, **kw)","287","","288","    # Ensure that the joblib API is deprecated in sklearn.util","289","    from sklearn.utils import Parallel, Memory, delayed","290","    from sklearn.utils import cpu_count, hash, effective_n_jobs","291","    check_warning(Memory, str(tmpdir))","292","    check_warning(hash, 1)","293","    check_warning(Parallel)","294","    check_warning(cpu_count)","295","    check_warning(effective_n_jobs, 1)","296","    check_warning(delayed, dummy_func)","297","","298","    # Only parallel_backend and register_parallel_backend are not deprecated in","299","    # sklearn.utils","300","    from sklearn.utils import parallel_backend, register_parallel_backend","301","    assert_no_warnings(parallel_backend, 'loky', None)","302","    assert_no_warnings(register_parallel_backend, 'failing', None)","303","","304","    # Ensure that the deprecation have no side effect in sklearn.utils._joblib","305","    from sklearn.utils._joblib import Parallel, Memory, delayed","306","    from sklearn.utils._joblib import cpu_count, hash, effective_n_jobs","307","    from sklearn.utils._joblib import parallel_backend","308","    from sklearn.utils._joblib import register_parallel_backend","309","    assert_no_warnings(Memory, str(tmpdir))","310","    assert_no_warnings(hash, 1)","311","    assert_no_warnings(Parallel)","312","    assert_no_warnings(cpu_count)","313","    assert_no_warnings(effective_n_jobs, 1)","314","    assert_no_warnings(delayed, dummy_func)","315","    assert_no_warnings(parallel_backend, 'loky', None)","316","    assert_no_warnings(register_parallel_backend, 'failing', None)","317","","318","    from sklearn.utils._joblib import joblib","319","    del joblib.parallel.BACKENDS['failing']"],"delete":[]}],"doc\/tutorial\/basic\/tutorial.rst":[{"add":["240","  >>> from joblib import dump, load","241","  >>> dump(clf, 'filename.joblib') # doctest: +SKIP","246","  >>> clf = load('filename.joblib') # doctest:+SKIP"],"delete":["240","  >>> from sklearn.externals import joblib","241","  >>> joblib.dump(clf, 'filename.joblib') # doctest: +SKIP","246","  >>> clf = joblib.load('filename.joblib') # doctest:+SKIP"]}],"benchmarks\/bench_saga.py":[{"add":["9","from joblib import delayed, Parallel, Memory"],"delete":["14","from sklearn.utils import delayed, Parallel, Memory"]}],"sklearn\/ensemble\/base.py":[{"add":["14","from ..utils._joblib import effective_n_jobs"],"delete":["15","from ..externals.joblib import effective_n_jobs"]}],"sklearn\/model_selection\/_search.py":[{"add":["31","from ..utils._joblib import Parallel, delayed"],"delete":["31","from ..utils import Parallel, delayed"]}],"sklearn\/datasets\/species_distributions.py":[{"add":["53","from sklearn.utils import _joblib","267","        _joblib.dump(bunch, archive_path, compress=9)","269","        bunch = _joblib.load(archive_path)"],"delete":["53","from sklearn.externals import joblib","267","        joblib.dump(bunch, archive_path, compress=9)","269","        bunch = joblib.load(archive_path)"]}],"build_tools\/circle\/build_doc.sh":[{"add":["122","  scikit-image=\"${SCIKIT_IMAGE_VERSION:-*}\" pandas=\"${PANDAS_VERSION:-*}\" \\","123","  joblib"],"delete":["122","  scikit-image=\"${SCIKIT_IMAGE_VERSION:-*}\" pandas=\"${PANDAS_VERSION:-*}\""]}],"sklearn\/datasets\/california_housing.py":[{"add":["35","from ..utils import _joblib","126","            _joblib.dump(cal_housing, filepath, compress=6)","130","        cal_housing = _joblib.load(filepath)"],"delete":["35","from ..externals import joblib","126","            joblib.dump(cal_housing, filepath, compress=6)","130","        cal_housing = joblib.load(filepath)"]}],"sklearn\/datasets\/olivetti_faces.py":[{"add":["25","from ..utils import _joblib","106","        _joblib.dump(faces, filepath, compress=6)","109","        faces = _joblib.load(filepath)"],"delete":["26","from ..externals import joblib","106","        joblib.dump(faces, filepath, compress=6)","109","        faces = joblib.load(filepath)"]}],"sklearn\/linear_model\/logistic.py":[{"add":["34","from ..utils._joblib import Parallel, delayed, effective_n_jobs"],"delete":["34","from ..utils import Parallel, delayed, effective_n_jobs"]}],"sklearn\/ensemble\/tests\/test_forest.py":[{"add":["24","from sklearn.utils._joblib import joblib","25","from sklearn.utils._joblib import parallel_backend","26","from sklearn.utils._joblib import register_parallel_backend","27","from sklearn.utils._joblib import __version__ as __joblib_version__","88","# Get the default backend in joblib to test parallelism and interaction with","89","# different backends","90","DEFAULT_JOBLIB_BACKEND = joblib.parallel.get_active_backend()[0].__class__","91","","452","","750","    est = ForestEstimator(min_samples_split=0.5, n_estimators=1,","751","                          random_state=0)","1280","class MyBackend(DEFAULT_JOBLIB_BACKEND):","1293","@pytest.mark.skipif(__joblib_version__ < LooseVersion('0.12'),"],"delete":["24","from sklearn.utils import _joblib","25","from sklearn.utils import parallel_backend","26","from sklearn.utils import register_parallel_backend","27","from sklearn.externals.joblib.parallel import LokyBackend","745","    est = ForestEstimator(min_samples_split=0.5, n_estimators=1, random_state=0)","1274","class MyBackend(LokyBackend):","1287","@pytest.mark.skipif(_joblib.__version__ < LooseVersion('0.12'),"]}],"sklearn\/linear_model\/omp.py":[{"add":["18","from ..utils._joblib import Parallel, delayed"],"delete":["18","from ..utils import Parallel, delayed"]}],"sklearn\/utils\/__init__.py":[{"add":["7","import warnings","12","from .class_weight import compute_class_weight, compute_sample_weight","13","from . import _joblib","14","from ..exceptions import DataConversionWarning","15","from .fixes import _Sequence as Sequence","16","from .deprecation import deprecated","24","","25","# Do not deprecate parallel_backend and register_parallel_backend as they are","26","# needed to tune `scikit-learn` behavior and have different effect if called","27","# from the vendored version or or the site-package version. The other are","28","# utilities that are independent of scikit-learn so they are not part of","29","# scikit-learn public API.","30","parallel_backend = _joblib.parallel_backend","31","register_parallel_backend = _joblib.register_parallel_backend","32","","33","# deprecate the joblib API in sklearn in favor of using directly joblib","34","msg = (\"deprecated in version 0.20.1 to be removed in version 0.23. \"","35","       \"Please import this functionality directly from joblib, which can \"","36","       \"be installed with: pip install joblib.\")","37","deprecate = deprecated(msg)","38","","39","delayed = deprecate(_joblib.delayed)","40","cpu_count = deprecate(_joblib.cpu_count)","41","hash = deprecate(_joblib.hash)","42","effective_n_jobs = deprecate(_joblib.effective_n_jobs)","43","","44","","45","# for classes, deprecated will change the object in _joblib module so we need","46","# to subclass them.","47","@deprecate","48","class Memory(_joblib.Memory):","49","    pass","50","","51","","52","@deprecate","53","class Parallel(_joblib.Parallel):","54","    pass","55","","56",""],"delete":["9","import warnings","17","from .class_weight import compute_class_weight, compute_sample_weight","18","from ._joblib import cpu_count, Parallel, Memory, delayed, hash","19","from ._joblib import parallel_backend, register_parallel_backend","20","from ._joblib import effective_n_jobs","21","from ..exceptions import DataConversionWarning","22","from ..utils.fixes import _Sequence as Sequence","23","from .deprecation import deprecated"]}],"doc\/modules\/model_persistence.rst":[{"add":["37","In the specific case of scikit-learn, it may be better to use joblib's","38","replacement of pickle (``dump`` & ``load``), which is more efficient on","39","objects that carry large numpy arrays internally as is often the case for","40","fitted scikit-learn estimators, but can only pickle to the disk and not to a","41","string::","43","  >>> from joblib import dump, load","44","  >>> dump(clf, 'filename.joblib') # doctest: +SKIP","49","  >>> clf = load('filename.joblib') # doctest:+SKIP","53","   ``dump`` and ``load`` functions also accept file-like object","55","   available `here <https:\/\/joblib.readthedocs.io\/en\/latest\/persistence.html>`_."],"delete":["37","In the specific case of scikit-learn, it may be better to use","38","joblib's replacement of pickle (``joblib.dump`` & ``joblib.load``),","39","which is more efficient on objects that carry large numpy arrays internally as","40","is often the case for fitted scikit-learn estimators, but can only pickle to the","41","disk and not to a string::","43","  >>> from sklearn.externals import joblib","44","  >>> joblib.dump(clf, 'filename.joblib') # doctest: +SKIP","49","  >>> clf = joblib.load('filename.joblib') # doctest:+SKIP","53","   ``joblib.dump`` and ``joblib.load`` functions also accept file-like object","55","   available `here <https:\/\/pythonhosted.org\/joblib\/persistence.html>`_."]}],"sklearn\/cluster\/mean_shift_.py":[{"add":["26","from ..utils._joblib import Parallel","27","from ..utils._joblib import delayed"],"delete":["26","from ..utils import Parallel","27","from ..utils import delayed"]}],"sklearn\/ensemble\/forest.py":[{"add":["52","from ..utils._joblib import Parallel, delayed"],"delete":["52","from ..utils import Parallel, delayed"]}],"sklearn\/metrics\/tests\/test_score_objects.py":[{"add":["41","from sklearn.utils import _joblib","100","    _joblib.dump((X, y, y_ml), filename)","101","    X_mm, y_mm, y_ml_mm = _joblib.load(filename, mmap_mode='r')"],"delete":["41","from sklearn.externals import joblib","100","    joblib.dump((X, y, y_ml), filename)","101","    X_mm, y_mm, y_ml_mm = joblib.load(filename, mmap_mode='r')"]}],"sklearn\/tests\/test_pipeline.py":[{"add":["35","from sklearn.utils._joblib import Memory","926","                        \" have the same interface as joblib.Memory.\"","948","                        \" have the same interface as joblib.Memory.\""],"delete":["35","from sklearn.utils import Memory","926","                        \" have the same interface as \"","927","                        \"sklearn.utils.Memory.\"","949","                        \" have the same interface as \"","950","                        \"sklearn.utils.Memory.\""]}],"sklearn\/datasets\/covtype.py":[{"add":["29","from ..utils import _joblib","120","        _joblib.dump(X, samples_path, compress=9)","121","        _joblib.dump(y, targets_path, compress=9)","128","        X = _joblib.load(samples_path)","129","        y = _joblib.load(targets_path)"],"delete":["29","from ..externals import joblib","120","        joblib.dump(X, samples_path, compress=9)","121","        joblib.dump(y, targets_path, compress=9)","128","        X = joblib.load(samples_path)","129","        y = joblib.load(targets_path)"]}],"examples\/applications\/wikipedia_principal_eigenvector.py":[{"add":["46","from joblib import Memory","47",""],"delete":["47","from sklearn.utils import Memory"]}],"sklearn\/metrics\/pairwise.py":[{"add":["27","from ..utils._joblib import Parallel","28","from ..utils._joblib import delayed","29","from ..utils._joblib import effective_n_jobs"],"delete":["27","from ..utils import Parallel","28","from ..utils import delayed","29","from ..utils import effective_n_jobs"]}],"sklearn\/utils\/validation.py":[{"add":["21","from .fixes import signature","26","from ._joblib import Memory","27","from ._joblib import __version__ as joblib_version","190","    joblib.Memory instance (typically a str denoting the ``location``)","191","    or has the same interface (has a ``cache`` method).","214","                         \" interface as joblib.Memory.\""],"delete":["21","from ..utils.fixes import signature","26","from ..utils._joblib import Memory","27","from ..utils._joblib import __version__ as joblib_version","190","    sklearn.utils.Memory instance (typically a str denoting the","191","    ``cachedir``) or has the same interface (has a ``cache`` method).","214","                         \" interface as sklearn.utils.Memory.\""]}],"benchmarks\/bench_mnist.py":[{"add":["37","from joblib import Memory"],"delete":["43","from sklearn.utils import Memory"]}],"sklearn\/neighbors\/base.py":[{"add":["28","from ..utils._joblib import Parallel, delayed, effective_n_jobs","29","from ..utils._joblib import __version__ as joblib_version"],"delete":["27","from ..utils import Parallel, delayed, effective_n_jobs","28","from ..utils._joblib import __version__ as joblib_version"]}],"sklearn\/pipeline.py":[{"add":["18","from .utils._joblib import Parallel, delayed"],"delete":["18","from .utils import Parallel, delayed"]}],"benchmarks\/bench_tsne_mnist.py":[{"add":["16","from joblib import Memory"],"delete":["17","from sklearn.utils import Memory"]}],"sklearn\/datasets\/svmlight_format.py":[{"add":["143","        from joblib import Memory"],"delete":["143","        from sklearn.utils import Memory"]}],"sklearn\/linear_model\/tests\/test_sgd.py":[{"add":["31","","33","from sklearn.utils._joblib import parallel_backend"],"delete":["21","from sklearn.utils import parallel_backend"]}],"sklearn\/multiclass.py":[{"add":["54","from .utils._joblib import Parallel","55","from .utils._joblib import delayed"],"delete":["54","from .utils import Parallel","55","from .utils import delayed"]}],"sklearn\/compose\/_column_transformer.py":[{"add":["16","from ..utils._joblib import Parallel, delayed"],"delete":["16","from ..utils import Parallel, delayed"]}],"sklearn\/utils\/tests\/test_validation.py":[{"add":["747","                        \" have the same interface as joblib.Memory.\"","751","                        \" have the same interface as joblib.Memory.\"","752","                        \" Got memory='{}' instead.\".format(dummy),","753","                        check_memory, dummy)"],"delete":["747","                        \" have the same interface as \"","748","                        \"sklearn.utils.Memory.\"","752","                        \" have the same interface as \"","753","                        \"sklearn.utils.Memory. Got memory='{}' \"","754","                        \"instead.\".format(dummy), check_memory, dummy)"]}],"sklearn\/datasets\/lfw.py":[{"add":["21","from ..utils._joblib import Memory","23","from ..utils import _joblib","330","    if LooseVersion(_joblib.__version__) < LooseVersion('0.12'):","501","    if LooseVersion(_joblib.__version__) < LooseVersion('0.12'):"],"delete":["21","from ..utils import Memory","22","from ..utils._joblib import __version__ as joblib_version","330","    if LooseVersion(joblib_version) < LooseVersion('0.12'):","501","    if LooseVersion(joblib_version) < LooseVersion('0.12'):"]}],"sklearn\/tests\/test_docstring_parameters.py":[{"add":["34","    'sklearn.utils._joblib'"],"delete":[]}],"sklearn\/linear_model\/coordinate_descent.py":[{"add":["20","from ..utils._joblib import Parallel, delayed, effective_n_jobs"],"delete":["20","from ..utils import Parallel, delayed, effective_n_jobs"]}],"sklearn\/decomposition\/online_lda.py":[{"add":["22","from ..utils._joblib import Parallel, delayed, effective_n_jobs"],"delete":["22","from ..utils import Parallel, delayed, effective_n_jobs"]}],"sklearn\/manifold\/mds.py":[{"add":["14","from ..utils._joblib import Parallel","15","from ..utils._joblib import delayed","16","from ..utils._joblib import effective_n_jobs"],"delete":["14","from ..utils import Parallel","15","from ..utils import delayed","16","from ..utils import effective_n_jobs"]}],"sklearn\/datasets\/kddcup99.py":[{"add":["23","from ..externals import six","25","from ..utils import _joblib","296","        _joblib.dump(X, samples_path, compress=0)","297","        _joblib.dump(y, targets_path, compress=0)","305","        X = _joblib.load(samples_path)","306","        y = _joblib.load(targets_path)"],"delete":["24","from ..externals import joblib, six","295","        joblib.dump(X, samples_path, compress=0)","296","        joblib.dump(y, targets_path, compress=0)","304","        X = joblib.load(samples_path)","305","        y = joblib.load(targets_path)"]}],"benchmarks\/bench_plot_nmf.py":[{"add":["16","from joblib import Memory"],"delete":["24","from sklearn.utils import Memory"]}],"examples\/cluster\/plot_feature_agglomeration_vs_univariate_selection.py":[{"add":["26","from joblib import Memory"],"delete":["32","from sklearn.utils import Memory"]}],"sklearn\/neighbors\/tests\/test_kde.py":[{"add":["12","from sklearn.utils import _joblib","220","    _joblib.dump(kde, file_path)","221","    kde = _joblib.load(file_path)"],"delete":["12","from sklearn.externals import joblib","220","    joblib.dump(kde, file_path)","221","    kde = joblib.load(file_path)"]}],"sklearn\/utils\/tests\/test_estimator_checks.py":[{"add":["10","from sklearn.utils import _joblib","387","            old_hash = _joblib.hash(est)","389","        assert_equal(old_hash, _joblib.hash(est))","398","            old_hash = _joblib.hash(est)","400","        assert_equal(old_hash, _joblib.hash(est))"],"delete":["7","from sklearn.externals import joblib","387","            old_hash = joblib.hash(est)","389","        assert_equal(old_hash, joblib.hash(est))","398","            old_hash = joblib.hash(est)","400","        assert_equal(old_hash, joblib.hash(est))"]}],"benchmarks\/bench_covertype.py":[{"add":["52","from joblib import Memory"],"delete":["61","from sklearn.utils import Memory"]}],"sklearn\/ensemble\/partial_dependence.py":[{"add":["12","from ..utils._joblib import Parallel, delayed"],"delete":["12","from ..utils import Parallel, delayed"]}],"doc\/whats_new\/v0.20.rst":[{"add":["182","- |API| Removed all mentions of ``sklearn.externals.joblib``, and deprecated","183","  joblib methods exposed in ``sklearn.utils``, except for","184","  :func:`utils.parallel_backend` and :func:`utils.register_parallel_backend`,","185","  which allow users to configure parallel computation in scikit-learn.","186","  Other functionalities are part of `joblib <https:\/\/joblib.readthedocs.io\/>`_.","187","  package and should be used directly, by installing it.","188","  The goal of this change is to prepare for","189","  unvendoring joblib in future version of scikit-learn.","190","  :issue:`12345` by :user:`Thomas Moreau <tomMoral>`","191",""],"delete":["186","Miscellaneous","187",".............","188",""]}],"doc\/modules\/classes.rst":[{"add":["1487","   utils.register_parallel_backend","1497","   :template: deprecated_class.rst","1498","","1499","   utils.Memory","1500","   utils.Parallel","1501","","1502",".. autosummary::","1503","   :toctree: generated\/","1506","   utils.cpu_count","1507","   utils.delayed"],"delete":["1484","   :template: class.rst","1485","","1486","   utils.Memory","1487","   utils.Parallel","1488","","1489",".. autosummary::","1490","   :toctree: generated\/","1493","   utils.cpu_count","1494","   utils.delayed"]}],"sklearn\/feature_selection\/rfe.py":[{"add":["17","from ..utils._joblib import Parallel, delayed, effective_n_jobs"],"delete":["17","from ..utils import Parallel, delayed, effective_n_jobs"]}],"sklearn\/linear_model\/theil_sen.py":[{"add":["22","from ..utils import check_X_y","23","from ..utils._joblib import Parallel, delayed, effective_n_jobs"],"delete":["22","from ..utils import check_X_y, effective_n_jobs","23","from ..utils import Parallel, delayed"]}],"sklearn\/datasets\/twenty_newsgroups.py":[{"add":["45","from ..utils import deprecated","46","from ..utils import _joblib","47","from ..utils import check_random_state, Bunch","405","        X_train, X_test = _joblib.load(target_file)","410","        _joblib.dump((X_train, X_test), target_file, compress=9)"],"delete":["43","from ..utils import check_random_state, Bunch","44","from ..utils import deprecated","47","from ..externals import joblib","405","        X_train, X_test = joblib.load(target_file)","410","        joblib.dump((X_train, X_test), target_file, compress=9)"]}],"benchmarks\/bench_rcv1_logreg_convergence.py":[{"add":["6","from joblib import Memory"],"delete":["10","from sklearn.utils import Memory"]}],"sklearn\/utils\/estimator_checks.py":[{"add":["16","from sklearn.utils import _joblib","17","from sklearn.utils._joblib import Memory","1959","        assert_equal(_joblib.hash(new_value), _joblib.hash(original_value),"],"delete":["16","from sklearn.utils._joblib import hash, Memory","1958","        assert_equal(hash(new_value), hash(original_value),"]}],"sklearn\/tests\/test_multioutput.py":[{"add":["21","from sklearn.utils._joblib import cpu_count"],"delete":["21","from sklearn.utils import cpu_count"]}],"sklearn\/linear_model\/base.py":[{"add":["26","from ..utils._joblib import Parallel, delayed"],"delete":["26","from ..utils import Parallel, delayed"]}],"sklearn\/ensemble\/bagging.py":[{"add":["15","from ..utils._joblib import Parallel, delayed"],"delete":["15","from ..utils import Parallel, delayed"]}],"sklearn\/ensemble\/tests\/test_bagging.py":[{"add":["35","from sklearn.utils import check_random_state","36","from sklearn.utils import _joblib","229","        self.training_hash_ = _joblib.hash(X)"],"delete":["35","from sklearn.utils import check_random_state, hash","228","        self.training_hash_ = hash(X)"]}],"sklearn\/linear_model\/stochastic_gradient.py":[{"add":["11","from ..utils._joblib import Parallel, delayed"],"delete":["11","from ..utils import Parallel, delayed"]}],"sklearn\/model_selection\/_validation.py":[{"add":["26","from ..utils._joblib import Parallel, delayed"],"delete":["26","from ..utils import Parallel, delayed"]}],"sklearn\/decomposition\/dict_learning.py":[{"add":["16","from ..utils._joblib import Parallel, delayed, effective_n_jobs"],"delete":["16","from ..utils import Parallel, delayed, effective_n_jobs"]}],"sklearn\/ensemble\/voting_classifier.py":[{"add":["19","from ..utils._joblib import Parallel, delayed"],"delete":["19","from ..utils import Parallel, delayed"]}],"sklearn\/cluster\/k_means_.py":[{"add":["31","from ..utils._joblib import Parallel","32","from ..utils._joblib import delayed","33","from ..utils._joblib import effective_n_jobs"],"delete":["31","from ..utils import Parallel","32","from ..utils import delayed","33","from ..utils import effective_n_jobs"]}],"sklearn\/linear_model\/least_angle.py":[{"add":["25","from ..utils._joblib import Parallel, delayed"],"delete":["25","from ..utils import Parallel, delayed"]}]}},"e67e30cec29ba9a8c505d1c722cc1461a0e31fee":{"changes":{"sklearn\/cluster\/bicluster.py":"MODIFY","sklearn\/metrics\/scorer.py":"MODIFY","sklearn\/metrics\/tests\/test_score_objects.py":"MODIFY","doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/neighbors\/base.py":"MODIFY","sklearn\/dummy.py":"MODIFY","sklearn\/datasets\/samples_generator.py":"MODIFY","sklearn\/linear_model\/base.py":"MODIFY","sklearn\/preprocessing\/data.py":"MODIFY"},"diff":{"sklearn\/cluster\/bicluster.py":[{"add":["307","        self.rows_ = np.vstack([self.row_labels_ == c","308","                                for c in range(self.n_clusters)])","309","        self.columns_ = np.vstack([self.column_labels_ == c","310","                                   for c in range(self.n_clusters)])","506","        self.rows_ = np.vstack([self.row_labels_ == label","507","                                for label in range(n_row_clusters)","508","                                for _ in range(n_col_clusters)])","509","        self.columns_ = np.vstack([self.column_labels_ == label","510","                                   for _ in range(n_row_clusters)","511","                                   for label in range(n_col_clusters)])"],"delete":["307","        self.rows_ = np.vstack(self.row_labels_ == c","308","                               for c in range(self.n_clusters))","309","        self.columns_ = np.vstack(self.column_labels_ == c","310","                                  for c in range(self.n_clusters))","506","        self.rows_ = np.vstack(self.row_labels_ == label","507","                               for label in range(n_row_clusters)","508","                               for _ in range(n_col_clusters))","509","        self.columns_ = np.vstack(self.column_labels_ == label","510","                                  for _ in range(n_row_clusters)","511","                                  for label in range(n_col_clusters))"]}],"sklearn\/metrics\/scorer.py":[{"add":["179","                    y_pred = np.vstack([p for p in y_pred]).T"],"delete":["179","                    y_pred = np.vstack(p for p in y_pred).T"]}],"sklearn\/metrics\/tests\/test_score_objects.py":[{"add":["387","    score2 = roc_auc_score(y_test, np.vstack([p[:, -1] for p in y_proba]).T)","400","    score2 = roc_auc_score(y_test, np.vstack([p for p in y_proba]).T)"],"delete":["387","    score2 = roc_auc_score(y_test, np.vstack(p[:, -1] for p in y_proba).T)","400","    score2 = roc_auc_score(y_test, np.vstack(p for p in y_proba).T)"]}],"doc\/whats_new\/v0.20.rst":[{"add":["105","Miscellaneous","106",".............","107","","108","- |Fix| Make sure to avoid raising ``FutureWarning`` when calling","109","  ``np.vstack`` with numpy 1.16 and later (use list comprehensions","110","  instead of generator expressions in many locations of the scikit-learn","111","  code base). :issue:`12467` by :user:`Olivier Grisel`.","112",""],"delete":[]}],"sklearn\/neighbors\/base.py":[{"add":["422","            result = list(pairwise_distances_chunked(","425","                **kwds))"],"delete":["422","            result = pairwise_distances_chunked(","425","                **kwds)"]}],"sklearn\/dummy.py":[{"add":["222","                y = np.vstack([classes_[k][proba[k].argmax(axis=1)] for","223","                               k in range(self.n_outputs_)]).T"],"delete":["222","                y = np.vstack(classes_[k][proba[k].argmax(axis=1)] for","223","                              k in range(self.n_outputs_)).T"]}],"sklearn\/datasets\/samples_generator.py":[{"add":["631","    X = np.vstack([np.append(outer_circ_x, inner_circ_x),","632","                   np.append(outer_circ_y, inner_circ_y)]).T","685","    X = np.vstack([np.append(outer_circ_x, inner_circ_x),","686","                   np.append(outer_circ_y, inner_circ_y)]).T","1595","    rows = np.vstack([row_labels == c for c in range(n_clusters)])","1596","    cols = np.vstack([col_labels == c for c in range(n_clusters)])","1691","    rows = np.vstack([row_labels == label","1692","                      for label in range(n_row_clusters)","1693","                      for _ in range(n_col_clusters)])","1694","    cols = np.vstack([col_labels == label","1695","                      for _ in range(n_row_clusters)","1696","                      for label in range(n_col_clusters)])"],"delete":["631","    X = np.vstack((np.append(outer_circ_x, inner_circ_x),","632","                   np.append(outer_circ_y, inner_circ_y))).T","685","    X = np.vstack((np.append(outer_circ_x, inner_circ_x),","686","                   np.append(outer_circ_y, inner_circ_y))).T","1595","    rows = np.vstack(row_labels == c for c in range(n_clusters))","1596","    cols = np.vstack(col_labels == c for c in range(n_clusters))","1691","    rows = np.vstack(row_labels == label","1692","                     for label in range(n_row_clusters)","1693","                     for _ in range(n_col_clusters))","1694","    cols = np.vstack(col_labels == label","1695","                     for _ in range(n_row_clusters)","1696","                     for label in range(n_col_clusters))"]}],"sklearn\/linear_model\/base.py":[{"add":["480","                self.coef_ = np.vstack([out[0] for out in outs])","481","                self._residues = np.vstack([out[3] for out in outs])"],"delete":["480","                self.coef_ = np.vstack(out[0] for out in outs)","481","                self._residues = np.vstack(out[3] for out in outs)"]}],"sklearn\/preprocessing\/data.py":[{"add":["1384","        return np.vstack([np.bincount(c, minlength=self.n_input_features_)","1385","                          for c in combinations])"],"delete":["1384","        return np.vstack(np.bincount(c, minlength=self.n_input_features_)","1385","                         for c in combinations)"]}]}},"c3915c099528638323f38db89bac31db4296709e":{"changes":{"sklearn\/linear_model\/logistic.py":"MODIFY"},"diff":{"sklearn\/linear_model\/logistic.py":[{"add":["34","from ..utils import Parallel, delayed, effective_n_jobs","1251","            if effective_n_jobs(self.n_jobs) != 1:","1254","                              \" = {}.\".format(effective_n_jobs(self.n_jobs)))"],"delete":["34","from ..utils import Parallel, delayed","1251","            if self.n_jobs != 1:","1254","                              \" = {}.\".format(self.n_jobs))"]}]}},"dca915638729878683b5bca57bf7a7c35acd6045":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/neural_network\/multilayer_perceptron.py":"MODIFY","sklearn\/neural_network\/tests\/test_mlp.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["194",":mod:`sklearn.neural_network`","195",".............................","196","","197","- |Fix| Fixed a bug in :class:`neural_network.MLPClassifier` and","198","  :class:`neural_network.MLPRegressor` where the option :code:`shuffle=False`","199","  was being ignored. :issue:`12582` by :user:`Sam Waterbury <samwaterbury>`.","200",""],"delete":[]}],"sklearn\/neural_network\/multilayer_perceptron.py":[{"add":["504","                if self.shuffle:","505","                    X, y = shuffle(X, y, random_state=self._random_state)"],"delete":["504","                X, y = shuffle(X, y, random_state=self._random_state)"]}],"sklearn\/neural_network\/tests\/test_mlp.py":[{"add":["497","def test_shuffle():","498","    # Test that the shuffle parameter affects the training process (it should)","499","    X, y = make_regression(n_samples=50, n_features=5, n_targets=1,","500","                           random_state=0)","501","","502","    # The coefficients will be identical if both do or do not shuffle","503","    for shuffle in [True, False]:","504","        mlp1 = MLPRegressor(hidden_layer_sizes=1, max_iter=1, batch_size=1,","505","                            random_state=0, shuffle=shuffle)","506","        mlp2 = MLPRegressor(hidden_layer_sizes=1, max_iter=1, batch_size=1,","507","                            random_state=0, shuffle=shuffle)","508","        mlp1.fit(X, y)","509","        mlp2.fit(X, y)","510","","511","        assert np.array_equal(mlp1.coefs_[0], mlp2.coefs_[0])","512","","513","    # The coefficients will be slightly different if shuffle=True","514","    mlp1 = MLPRegressor(hidden_layer_sizes=1, max_iter=1, batch_size=1,","515","                        random_state=0, shuffle=True)","516","    mlp2 = MLPRegressor(hidden_layer_sizes=1, max_iter=1, batch_size=1,","517","                        random_state=0, shuffle=False)","518","    mlp1.fit(X, y)","519","    mlp2.fit(X, y)","520","","521","    assert not np.array_equal(mlp1.coefs_[0], mlp2.coefs_[0])","522","","523",""],"delete":[]}]}},"a1d0e96791ec6b4404303c5821e305caf6a5f777":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/utils\/extmath.py":"MODIFY","sklearn\/preprocessing\/tests\/test_data.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["88",":mod:`sklearn.utils`","89","........................","90","","91","- |Fix| Use float64 for mean accumulator to avoid floating point","92","  precision issues in :class:`preprocessing.StandardScaler` and","93","  :class:`decomposition.IncrementalPCA` when using float32 datasets.","94","  :issue:`12338` by :user:`bauks <bauks>`.","95",""],"delete":[]}],"sklearn\/utils\/extmath.py":[{"add":["712","    if np.issubdtype(X.dtype, np.floating) and X.dtype.itemsize < 8:","713","        # Use at least float64 for the accumulator to avoid precision issues;","714","        # see https:\/\/github.com\/numpy\/numpy\/issues\/9393","715","        new_sum = np.nansum(X, axis=0, dtype=np.float64).astype(X.dtype)","716","    else:","717","        new_sum = np.nansum(X, axis=0)"],"delete":["712","    new_sum = np.nansum(X, axis=0)"]}],"sklearn\/preprocessing\/tests\/test_data.py":[{"add":["232","def test_standard_scaler_dtype():","233","    # Ensure scaling does not affect dtype","234","    rng = np.random.RandomState(0)","235","    n_samples = 10","236","    n_features = 3","237","    for dtype in [np.float16, np.float32, np.float64]:","238","        X = rng.randn(n_samples, n_features).astype(dtype)","239","        scaler = StandardScaler()","240","        X_scaled = scaler.fit(X).transform(X)","241","        assert X.dtype == X_scaled.dtype","242","        assert scaler.mean_.dtype == np.float64","243","        assert scaler.scale_.dtype == np.float64","244","","245",""],"delete":[]}]}},"440c08684fb7edb31c7257e86b6f6481f69ba5c9":{"changes":{"sklearn\/model_selection\/_split.py":"MODIFY","sklearn\/model_selection\/tests\/test_split.py":"MODIFY"},"diff":{"sklearn\/model_selection\/_split.py":[{"add":["1796","            if test_size >= 1. or test_size <= 0:","1798","                    'test_size=%f should be in the (0, 1) range '","1799","                    'or be an integer' % test_size)","1806","            if train_size >= 1. or train_size <= 0:","1807","                raise ValueError('train_size=%f should be in the (0, 1) range '","1808","                                 'or be an integer' % train_size)","1810","                    (","1811","                        (train_size + test_size) > 1. or","1812","                        (train_size + test_size) < 0)):","1814","                                 'should be in the (0, 1) range. Reduce '","1828","            (np.asarray(test_size).dtype.kind == 'i' and","1829","                (test_size >= n_samples or test_size <= 0)) or","1830","            (np.asarray(test_size).dtype.kind == 'f' and","1831","                (test_size <= 0 or test_size >= 1))):","1832","        raise ValueError('test_size=%d should be either positive and smaller '","1833","                         'than the number of samples %d or a float in the '","1834","                         '(0,1) range' % (test_size, n_samples))","1837","            (np.asarray(train_size).dtype.kind == 'i' and","1838","                (train_size >= n_samples or train_size <= 0)) or","1839","            (np.asarray(train_size).dtype.kind == 'f' and","1840","                (train_size <= 0 or train_size >= 1))):","1841","        raise ValueError('train_size=%d should be either positive and smaller '","1842","                         'than the number of samples %d or a float in the '","1843","                         '(0,1) range' % (train_size, n_samples))"],"delete":["1796","            if test_size >= 1.:","1798","                    'test_size=%f should be smaller '","1799","                    'than 1.0 or be an integer' % test_size)","1806","            if train_size >= 1.:","1807","                raise ValueError(\"train_size=%f should be smaller \"","1808","                                 \"than 1.0 or be an integer\" % train_size)","1810","                    (train_size + test_size) > 1.):","1812","                                 'should be smaller than 1.0. Reduce '","1826","            np.asarray(test_size).dtype.kind == 'i' and","1827","            test_size >= n_samples):","1828","        raise ValueError('test_size=%d should be smaller than the number of '","1829","                         'samples %d' % (test_size, n_samples))","1832","            np.asarray(train_size).dtype.kind == 'i' and","1833","            train_size >= n_samples):","1834","        raise ValueError(\"train_size=%d should be smaller than the number of\"","1835","                         \" samples %d\" % (train_size, n_samples))"]}],"sklearn\/model_selection\/tests\/test_split.py":[{"add":["1008","    pytest.raises(ValueError, train_test_split)","1013","        pytest.raises(ValueError, train_test_split, range(3), train_size=1.1)","1015","    pytest.raises(ValueError, train_test_split, range(3), test_size=0.6,","1017","    pytest.raises(ValueError, train_test_split, range(3),","1019","    pytest.raises(ValueError, train_test_split, range(3),","1021","    pytest.raises(ValueError, train_test_split, range(3), test_size=2,","1023","    pytest.raises(TypeError, train_test_split, range(3),","1025","    pytest.raises(ValueError, train_test_split, range(3), range(42))","1026","    pytest.raises(ValueError, train_test_split, range(10),","1029","    with pytest.raises(ValueError,","1030","                       match=r'train_size=11 should be either positive and '","1031","                             r'smaller than the number of samples 10 or a '","1032","                             r'float in the \\(0,1\\) range'):","1033","        train_test_split(range(10), train_size=11, test_size=1)","1034","","1035","","1036","@pytest.mark.parametrize(\"train_size,test_size\", [","1037","    (1.2, 0.8),","1038","    (1., 0.8),","1039","    (0.0, 0.8),","1040","    (-.2, 0.8),","1041","    (0.8, 1.2),","1042","    (0.8, 1.),","1043","    (0.8, 0.),","1044","    (0.8, -.2)])","1045","def test_train_test_split_invalid_sizes1(train_size, test_size):","1046","    with pytest.raises(ValueError, match=r'should be in the \\(0, 1\\) range'):","1047","        train_test_split(range(10), train_size=train_size, test_size=test_size)","1048","","1049","","1050","@pytest.mark.parametrize(\"train_size,test_size\", [","1051","    (-10, 0.8),","1052","    (0, 0.8),","1053","    (11, 0.8),","1054","    (0.8, -10),","1055","    (0.8, 0),","1056","    (0.8, 11)])","1057","def test_train_test_split_invalid_sizes2(train_size, test_size):","1058","    with pytest.raises(ValueError,","1059","                       match=r'should be either positive and smaller'):","1060","        train_test_split(range(10), train_size=train_size, test_size=test_size)","1061",""],"delete":["1008","    assert_raises(ValueError, train_test_split)","1013","        assert_raises(ValueError, train_test_split, range(3), train_size=1.1)","1015","    assert_raises(ValueError, train_test_split, range(3), test_size=0.6,","1017","    assert_raises(ValueError, train_test_split, range(3),","1019","    assert_raises(ValueError, train_test_split, range(3),","1021","    assert_raises(ValueError, train_test_split, range(3), test_size=2,","1023","    assert_raises(TypeError, train_test_split, range(3),","1025","    assert_raises(ValueError, train_test_split, range(3), range(42))","1026","    assert_raises(ValueError, train_test_split, range(10),"]}]}},"24e46410bd606fd32cc18f6d9cb7e9efe695597e":{"changes":{"sklearn\/base.py":"MODIFY","sklearn\/tests\/test_base.py":"MODIFY"},"diff":{"sklearn\/base.py":[{"add":["50","    elif not hasattr(estimator, 'get_params') or isinstance(estimator, type):"],"delete":["50","    elif not hasattr(estimator, 'get_params'):"]}],"sklearn\/tests\/test_base.py":[{"add":["169","def test_clone_estimator_types():","170","    # Check that clone works for parameters that are types rather than","171","    # instances","172","    clf = MyEstimator(empty=MyEstimator)","173","    clf2 = clone(clf)","174","","175","    assert clf.empty is clf2.empty","176","","177",""],"delete":[]}]}},"98357ec4d4e4626cc70e8aaa1380789894d64a93":{"changes":{"sklearn\/utils\/sparsefuncs.py":"MODIFY","sklearn\/utils\/tests\/test_sparsefuncs.py":"MODIFY"},"diff":{"sklearn\/utils\/sparsefuncs.py":[{"add":["469","            # astype here is for consistency with axis=0 dtype","470","            return out.astype('intp')"],"delete":["469","            return out"]}],"sklearn\/utils\/tests\/test_sparsefuncs.py":[{"add":["445","    assert (count_nonzero(X_csr, axis=0).dtype ==","446","            count_nonzero(X_csr, axis=1).dtype)","447","    assert (count_nonzero(X_csr, axis=0, sample_weight=sample_weight).dtype ==","448","            count_nonzero(X_csr, axis=1, sample_weight=sample_weight).dtype)","449","","450","    # Check dtypes with large sparse matrices too","451","    X_csr.indices = X_csr.indices.astype(np.int64)","452","    X_csr.indptr = X_csr.indptr.astype(np.int64)","453","    assert (count_nonzero(X_csr, axis=0).dtype ==","454","            count_nonzero(X_csr, axis=1).dtype)","455","    assert (count_nonzero(X_csr, axis=0, sample_weight=sample_weight).dtype ==","456","            count_nonzero(X_csr, axis=1, sample_weight=sample_weight).dtype)","457",""],"delete":[]}]}}}