{"8f63e9144f51693c811eb435f8b6b2b5d3b3a63c":{"changes":{"sklearn\/gaussian_process\/_gpc.py":"MODIFY","sklearn\/gaussian_process\/kernels.py":"MODIFY","sklearn\/gaussian_process\/tests\/test_gpc.py":"MODIFY","doc\/whats_new\/v0.23.rst":"MODIFY","sklearn\/gaussian_process\/tests\/test_gpr.py":"MODIFY","sklearn\/gaussian_process\/_gpr.py":"MODIFY","doc\/whats_new\/v0.24.rst":"MODIFY"},"diff":{"sklearn\/gaussian_process\/_gpc.py":[{"add":["233","            self.kernel_._check_bounds_params()","234",""],"delete":[]}],"sklearn\/gaussian_process\/kernels.py":[{"add":["34","import warnings","35","from sklearn.exceptions import ConvergenceWarning","36","","391","    def _check_bounds_params(self):","392","        \"\"\"Called after fitting to warn if bounds may have been too tight.\"\"\"","393","        list_close = np.isclose(self.bounds,","394","                                np.atleast_2d(self.theta).T)","395","        idx = 0","396","        for hyp in self.hyperparameters:","397","            for dim in range(hyp.n_elements):","398","                if list_close[idx, 0]:","399","                    warnings.warn(\"The optimal value found for \"","400","                                  \"dimension %s of parameter %s is \"","401","                                  \"close to the specified lower \"","402","                                  \"bound %s. Decreasing the bound and\"","403","                                  \" calling fit again may find a \"","404","                                  \"better value.\" %","405","                                  (dim, hyp.name, hyp.bounds[dim][0]),","406","                                  ConvergenceWarning)","407","                elif list_close[idx, 1]:","408","                    warnings.warn(\"The optimal value found for \"","409","                                  \"dimension %s of parameter %s is \"","410","                                  \"close to the specified upper \"","411","                                  \"bound %s. Increasing the bound and\"","412","                                  \" calling fit again may find a \"","413","                                  \"better value.\" %","414","                                  (dim, hyp.name, hyp.bounds[dim][1]),","415","                                  ConvergenceWarning)","416","                idx += 1","417",""],"delete":[]}],"sklearn\/gaussian_process\/tests\/test_gpc.py":[{"add":["12","from sklearn.gaussian_process.kernels \\","13","    import RBF, ConstantKernel as C, WhiteKernel","15","from sklearn.exceptions import ConvergenceWarning","17","from sklearn.utils._testing \\","18","    import assert_almost_equal, assert_array_equal, assert_warns_message","185","","186","","187","def test_warning_bounds():","188","    kernel = RBF(length_scale_bounds=[1e-5, 1e-3])","189","    gpc = GaussianProcessClassifier(kernel=kernel)","190","    assert_warns_message(ConvergenceWarning, \"The optimal value found for \"","191","                                             \"dimension 0 of parameter \"","192","                                             \"length_scale is close to \"","193","                                             \"the specified upper bound \"","194","                                             \"0.001. Increasing the bound \"","195","                                             \"and calling fit again may \"","196","                                             \"find a better value.\",","197","                         gpc.fit, X, y)","198","","199","    kernel_sum = (WhiteKernel(noise_level_bounds=[1e-5, 1e-3]) +","200","                  RBF(length_scale_bounds=[1e3, 1e5]))","201","    gpc_sum = GaussianProcessClassifier(kernel=kernel_sum)","202","    with pytest.warns(None) as record:","203","        gpc_sum.fit(X, y)","204","","205","    assert len(record) == 2","206","    assert record[0].message.args[0] == (\"The optimal value found for \"","207","                                         \"dimension 0 of parameter \"","208","                                         \"k1__noise_level is close to the \"","209","                                         \"specified upper bound 0.001. \"","210","                                         \"Increasing the bound and calling \"","211","                                         \"fit again may find a better value.\")","212","","213","    assert record[1].message.args[0] == (\"The optimal value found for \"","214","                                         \"dimension 0 of parameter \"","215","                                         \"k2__length_scale is close to the \"","216","                                         \"specified lower bound 1000.0. \"","217","                                         \"Decreasing the bound and calling \"","218","                                         \"fit again may find a better value.\")","219","","220","    X_tile = np.tile(X, 2)","221","    kernel_dims = RBF(length_scale=[1., 2.],","222","                      length_scale_bounds=[1e1, 1e2])","223","    gpc_dims = GaussianProcessClassifier(kernel=kernel_dims)","224","","225","    with pytest.warns(None) as record:","226","        gpc_dims.fit(X_tile, y)","227","","228","    assert len(record) == 2","229","    assert record[0].message.args[0] == (\"The optimal value found for \"","230","                                         \"dimension 0 of parameter \"","231","                                         \"length_scale is close to the \"","232","                                         \"specified upper bound 100.0. \"","233","                                         \"Increasing the bound and calling \"","234","                                         \"fit again may find a better value.\")","235","","236","    assert record[1].message.args[0] == (\"The optimal value found for \"","237","                                         \"dimension 1 of parameter \"","238","                                         \"length_scale is close to the \"","239","                                         \"specified upper bound 100.0. \"","240","                                         \"Increasing the bound and calling \"","241","                                         \"fit again may find a better value.\")"],"delete":["12","from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C","15","from sklearn.utils._testing import assert_almost_equal, assert_array_equal"]}],"doc\/whats_new\/v0.23.rst":[{"add":["46",""],"delete":[]}],"sklearn\/gaussian_process\/tests\/test_gpr.py":[{"add":["18","from sklearn.exceptions import ConvergenceWarning","24","            assert_allclose, assert_warns_message)","470","","471","","472","def test_warning_bounds():","473","    kernel = RBF(length_scale_bounds=[1e-5, 1e-3])","474","    gpr = GaussianProcessRegressor(kernel=kernel)","475","    assert_warns_message(ConvergenceWarning, \"The optimal value found for \"","476","                                             \"dimension 0 of parameter \"","477","                                             \"length_scale is close to \"","478","                                             \"the specified upper bound \"","479","                                             \"0.001. Increasing the bound \"","480","                                             \"and calling fit again may \"","481","                                             \"find a better value.\",","482","                         gpr.fit, X, y)","483","","484","    kernel_sum = (WhiteKernel(noise_level_bounds=[1e-5, 1e-3]) +","485","                  RBF(length_scale_bounds=[1e3, 1e5]))","486","    gpr_sum = GaussianProcessRegressor(kernel=kernel_sum)","487","    with pytest.warns(None) as record:","488","        gpr_sum.fit(X, y)","489","","490","    assert len(record) == 2","491","    assert record[0].message.args[0] == (\"The optimal value found for \"","492","                                         \"dimension 0 of parameter \"","493","                                         \"k1__noise_level is close to the \"","494","                                         \"specified upper bound 0.001. \"","495","                                         \"Increasing the bound and calling \"","496","                                         \"fit again may find a better value.\")","497","","498","    assert record[1].message.args[0] == (\"The optimal value found for \"","499","                                         \"dimension 0 of parameter \"","500","                                         \"k2__length_scale is close to the \"","501","                                         \"specified lower bound 1000.0. \"","502","                                         \"Decreasing the bound and calling \"","503","                                         \"fit again may find a better value.\")","504","","505","    X_tile = np.tile(X, 2)","506","    kernel_dims = RBF(length_scale=[1., 2.],","507","                      length_scale_bounds=[1e1, 1e2])","508","    gpr_dims = GaussianProcessRegressor(kernel=kernel_dims)","509","","510","    with pytest.warns(None) as record:","511","        gpr_dims.fit(X_tile, y)","512","","513","    assert len(record) == 2","514","    assert record[0].message.args[0] == (\"The optimal value found for \"","515","                                         \"dimension 0 of parameter \"","516","                                         \"length_scale is close to the \"","517","                                         \"specified lower bound 10.0. \"","518","                                         \"Decreasing the bound and calling \"","519","                                         \"fit again may find a better value.\")","520","","521","    assert record[1].message.args[0] == (\"The optimal value found for \"","522","                                         \"dimension 1 of parameter \"","523","                                         \"length_scale is close to the \"","524","                                         \"specified lower bound 10.0. \"","525","                                         \"Decreasing the bound and calling \"","526","                                         \"fit again may find a better value.\")"],"delete":["23","            assert_allclose)"]}],"sklearn\/gaussian_process\/_gpr.py":[{"add":["253","            self.kernel_._check_bounds_params()","254",""],"delete":[]}],"doc\/whats_new\/v0.24.rst":[{"add":["149",":mod:`sklearn.gaussian_process`","150","...............................","151","","152","- |Enhancement| A new method","153","  :class:`gaussian_process.Kernel._check_bounds_params` is called after","154","  fitting a Gaussian Process and raises a ``ConvergenceWarning`` if the bounds","155","  of the hyperparameters are too tight.","156","  :issue:`12638` by :user:`Sylvain Lannuzel <SylvainLan>`","157",""],"delete":[]}]}},"cd6e4d52d9bcd8d2152b97c9cf8902eeec4642ca":{"changes":{"sklearn\/decomposition\/_dict_learning.py":"MODIFY","sklearn\/decomposition\/_factor_analysis.py":"MODIFY","sklearn\/decomposition\/_lda.py":"MODIFY","sklearn\/decomposition\/_kernel_pca.py":"MODIFY","sklearn\/decomposition\/_pca.py":"MODIFY","sklearn\/decomposition\/_sparse_pca.py":"MODIFY","sklearn\/decomposition\/_fastica.py":"MODIFY","sklearn\/decomposition\/_truncated_svd.py":"MODIFY","sklearn\/decomposition\/_nmf.py":"MODIFY"},"diff":{"sklearn\/decomposition\/_dict_learning.py":[{"add":["363","    random_state : int, RandomState instance, default=None","364","        Used for randomly initializing the dictionary. Pass an int for","365","        reproducible results across multiple function calls.","366","        See :term:`Glossary <random_state>`.","484","        Used for randomly initializing the dictionary. Pass an int for","485","        reproducible results across multiple function calls.","486","        See :term:`Glossary <random_state>`.","690","        Used for initializing the dictionary when ``dict_init`` is not","691","        specified, randomly shuffling the data when ``shuffle`` is set to","692","        ``True``, and updating the dictionary. Pass an int for reproducible","693","        results across multiple function calls.","694","        See :term:`Glossary <random_state>`.","1133","    random_state : int, RandomState instance or None, optional (default=None)","1134","        Used for initializing the dictionary when ``dict_init`` is not","1135","        specified, randomly shuffling the data when ``shuffle`` is set to","1136","        ``True``, and updating the dictionary. Pass an int for reproducible","1137","        results across multiple function calls.","1138","        See :term:`Glossary <random_state>`.","1325","        Used for initializing the dictionary when ``dict_init`` is not","1326","        specified, randomly shuffling the data when ``shuffle`` is set to","1327","        ``True``, and updating the dictionary. Pass an int for reproducible","1328","        results across multiple function calls.","1329","        See :term:`Glossary <random_state>`."],"delete":["363","    random_state : int, RandomState instance or None, optional (default=None)","364","        If int, random_state is the seed used by the random number generator;","365","        If RandomState instance, random_state is the random number generator;","366","        If None, the random number generator is the RandomState instance used","367","        by `np.random`.","485","        If int, random_state is the seed used by the random number generator;","486","        If RandomState instance, random_state is the random number generator;","487","        If None, the random number generator is the RandomState instance used","488","        by `np.random`.","692","        If int, random_state is the seed used by the random number generator;","693","        If RandomState instance, random_state is the random number generator;","694","        If None, the random number generator is the RandomState instance used","695","        by `np.random`.","1134","    random_state : int, RandomState instance or None, default=None","1135","        If int, random_state is the seed used by the random number generator;","1136","        If RandomState instance, random_state is the random number generator;","1137","        If None, the random number generator is the RandomState instance used","1138","        by `np.random`.","1325","        If int, random_state is the seed used by the random number generator;","1326","        If RandomState instance, random_state is the random number generator;","1327","        If None, the random number generator is the RandomState instance used","1328","        by `np.random`."]}],"sklearn\/decomposition\/_factor_analysis.py":[{"add":["91","    random_state : int, RandomState instance, default=None","92","        Only used when ``svd_method`` equals 'randomized'. Pass an int for","93","        reproducible results across multiple function calls.","94","        See :term:`Glossary <random_state>`."],"delete":["91","    random_state : int, RandomState instance or None, optional (default=0)","92","        If int, random_state is the seed used by the random number generator;","93","        If RandomState instance, random_state is the random number generator;","94","        If None, the random number generator is the RandomState instance used","95","        by `np.random`. Only used when ``svd_method`` equals 'randomized'."]}],"sklearn\/decomposition\/_lda.py":[{"add":["224","    random_state : int, RandomState instance, default=None","225","        Pass an int for reproducible results across multiple function calls.","226","        See :term:`Glossary <random_state>`."],"delete":["224","    random_state : int, RandomState instance or None, optional (default=None)","225","        If int, random_state is the seed used by the random number generator;","226","        If RandomState instance, random_state is the random number generator;","227","        If None, the random number generator is the RandomState instance used","228","        by `np.random`."]}],"sklearn\/decomposition\/_kernel_pca.py":[{"add":["78","    random_state : int, RandomState instance, default=None","79","        Used when ``eigen_solver`` == 'arpack'. Pass an int for reproducible","80","        results across multiple function calls.","81","        See :term:`Glossary <random_state>`."],"delete":["78","    random_state : int, RandomState instance or None, optional (default=None)","79","        If int, random_state is the seed used by the random number generator;","80","        If RandomState instance, random_state is the random number generator;","81","        If None, the random number generator is the RandomState instance used","82","        by `np.random`. Used when ``eigen_solver`` == 'arpack'."]}],"sklearn\/decomposition\/_pca.py":[{"add":["191","    random_state : int, RandomState instance, default=None","192","        Used when ``svd_solver`` == 'arpack' or 'randomized'. Pass an int","193","        for reproducible results across multiple function calls.","194","        See :term:`Glossary <random_state>`."],"delete":["191","    random_state : int, RandomState instance or None, optional (default None)","192","        If int, random_state is the seed used by the random number generator;","193","        If RandomState instance, random_state is the random number generator;","194","        If None, the random number generator is the RandomState instance used","195","        by `np.random`. Used when ``svd_solver`` == 'arpack' or 'randomized'."]}],"sklearn\/decomposition\/_sparse_pca.py":[{"add":["81","    random_state : int, RandomState instance, default=None","82","        Used during dictionary learning. Pass an int for reproducible results","83","        across multiple function calls.","84","        See :term:`Glossary <random_state>`.","283","    random_state : int, RandomState instance, default=None","284","        Used for random shuffling when ``shuffle`` is set to ``True``,","285","        during online dictionary learning. Pass an int for reproducible results","286","        across multiple function calls.","287","        See :term:`Glossary <random_state>`."],"delete":["81","    random_state : int, RandomState instance or None, optional (default=None)","82","        If int, random_state is the seed used by the random number generator;","83","        If RandomState instance, random_state is the random number generator;","84","        If None, the random number generator is the RandomState instance used","85","        by `np.random`.","284","    random_state : int, RandomState instance or None, optional (default=None)","285","        If int, random_state is the seed used by the random number generator;","286","        If RandomState instance, random_state is the random number generator;","287","        If None, the random number generator is the RandomState instance used","288","        by `np.random`."]}],"sklearn\/decomposition\/_fastica.py":[{"add":["204","    random_state : int, RandomState instance, default=None","205","        Used to initialize ``w_init`` when not specified, with a","206","        normal distribution. Pass an int, for reproducible results","207","        across multiple function calls.","208","        See :term:`Glossary <random_state>`.","343","    random_state : int, RandomState instance, default=None","344","        Used to initialize ``w_init`` when not specified, with a","345","        normal distribution. Pass an int, for reproducible results","346","        across multiple function calls.","347","        See :term:`Glossary <random_state>`."],"delete":["204","    random_state : int, RandomState instance or None, optional (default=None)","205","        If int, random_state is the seed used by the random number generator;","206","        If RandomState instance, random_state is the random number generator;","207","        If None, the random number generator is the RandomState instance used","208","        by `np.random`.","343","    random_state : int, RandomState instance or None, optional (default=None)","344","        If int, random_state is the seed used by the random number generator;","345","        If RandomState instance, random_state is the random number generator;","346","        If None, the random number generator is the RandomState instance used","347","        by `np.random`."]}],"sklearn\/decomposition\/_truncated_svd.py":[{"add":["58","    random_state : int, RandomState instance, default=None","59","        Used during randomized svd. Pass an int for reproducible results across","60","        multiple function calls.","61","        See :term:`Glossary <random_state>`."],"delete":["58","    random_state : int, RandomState instance or None, optional, default = None","59","        If int, random_state is the seed used by the random number generator;","60","        If RandomState instance, random_state is the random number generator;","61","        If None, the random number generator is the RandomState instance used","62","        by `np.random`."]}],"sklearn\/decomposition\/_nmf.py":[{"add":["289","    random_state : int, RandomState instance, default=None","290","        Used when ``init`` == 'nndsvdar' or 'random'. Pass an int for","291","        reproducible results across multiple function calls.","292","        See :term:`Glossary <random_state>`.","473","    random_state : int, RandomState instance, default=None","474","        Used to randomize the coordinates in the CD solver, when","475","        ``shuffle`` is set to ``True``. Pass an int for reproducible","476","        results across multiple function calls.","477","        See :term:`Glossary <random_state>`.","964","    random_state : int, RandomState instance, default=None","965","        Used for NMF initialisation (when ``init`` == 'nndsvdar' or","966","        'random'), and in Coordinate Descent. Pass an int for reproducible","967","        results across multiple function calls.","968","        See :term:`Glossary <random_state>`.","1157","    random_state : int, RandomState instance, default=None","1158","        Used for initialisation (when ``init`` == 'nndsvdar' or","1159","        'random'), and in Coordinate Descent. Pass an int for reproducible","1160","        results across multiple function calls.","1161","        See :term:`Glossary <random_state>`."],"delete":["289","    random_state : int, RandomState instance or None, optional, default: None","290","        If int, random_state is the seed used by the random number generator;","291","        If RandomState instance, random_state is the random number generator;","292","        If None, the random number generator is the RandomState instance used","293","        by `np.random`. Used when ``random`` == 'nndsvdar' or 'random'.","474","    random_state : int, RandomState instance or None, optional, default: None","475","        If int, random_state is the seed used by the random number generator;","476","        If RandomState instance, random_state is the random number generator;","477","        If None, the random number generator is the RandomState instance used","478","        by `np.random`.","965","    random_state : int, RandomState instance or None, optional, default: None","966","        If int, random_state is the seed used by the random number generator;","967","        If RandomState instance, random_state is the random number generator;","968","        If None, the random number generator is the RandomState instance used","969","        by `np.random`.","1158","    random_state : int, RandomState instance or None, optional, default: None","1159","        If int, random_state is the seed used by the random number generator;","1160","        If RandomState instance, random_state is the random number generator;","1161","        If None, the random number generator is the RandomState instance used","1162","        by `np.random`."]}]}},"bbb0d935f529b57885177fccf79f3b30f7ab568b":{"changes":{"sklearn\/preprocessing\/tests\/test_data.py":"MODIFY","sklearn\/preprocessing\/data.py":"MODIFY"},"diff":{"sklearn\/preprocessing\/tests\/test_data.py":[{"add":["2044","        X_trans_func = power_transform(","2045","            X, method='box-cox',","2046","            standardize=standardize","2047","        )","2071","        X_trans_func = power_transform(","2072","            X, method='box-cox',","2073","            standardize=standardize","2074","        )","2297","","2298","","2299","def test_power_transform_default_method():","2300","    X = np.abs(X_2d)","2301","","2302","    future_warning_message = (","2303","        \"The default value of 'method' \"","2304","        \"will change from 'box-cox'\"","2305","    )","2306","    assert_warns_message(FutureWarning, future_warning_message,","2307","                         power_transform, X)","2308","","2309","    with warnings.catch_warnings():","2310","        warnings.simplefilter('ignore')","2311","        X_trans_default = power_transform(X)","2312","","2313","    X_trans_boxcox = power_transform(X, method='box-cox')","2314","    assert_array_equal(X_trans_boxcox, X_trans_default)"],"delete":["2044","        X_trans_func = power_transform(X, standardize=standardize)","2068","        X_trans_func = power_transform(X, standardize=standardize)"]}],"sklearn\/preprocessing\/data.py":[{"add":["2491","    Yeo-Johnson transform. The optimal parameter for stabilizing variance and","2546","    NaNs are treated as missing values: disregarded in ``fit``, and maintained","2547","    in ``transform``.","2846","def power_transform(X, method='warn', standardize=True, copy=True):","2847","    \"\"\"","2853","    Currently, power_transform supports the Box-Cox transform and the","2854","    Yeo-Johnson transform. The optimal parameter for stabilizing variance and","2855","    minimizing skewness is estimated through maximum likelihood.","2856","","2857","    Box-Cox requires input data to be strictly positive, while Yeo-Johnson","2858","    supports both positive or negative data.","2870","    method : str","2871","        The power transform method. Available methods are:","2872","","2873","        - 'yeo-johnson' [1]_, works with positive and negative values","2874","        - 'box-cox' [2]_, only works with strictly positive values","2875","","2876","        The default method will be changed from 'box-cox' to 'yeo-johnson'","2877","        in version 0.23. To suppress the FutureWarning, explicitly set the","2878","        parameter.","2885","        Set to False to perform inplace computation during transformation.","2886","","2887","    Returns","2888","    -------","2889","    X_trans : array-like, shape (n_samples, n_features)","2890","        The transformed data.","2897","    >>> print(power_transform(data, method='box-cox'))  # doctest: +ELLIPSIS","2904","    PowerTransformer : Equivalent transformation with the","2905","        ``Transformer`` API (e.g. as part of a preprocessing","2906","        :class:`sklearn.pipeline.Pipeline`).","2913","    NaNs are treated as missing values: disregarded in ``fit``, and maintained","2914","    in ``transform``.","2922","","2923","    .. [1] I.K. Yeo and R.A. Johnson, \"A new family of power transformations to","2924","           improve normality or symmetry.\" Biometrika, 87(4), pp.954-959,","2925","           (2000).","2926","","2927","    .. [2] G.E.P. Box and D.R. Cox, \"An Analysis of Transformations\", Journal","2928","           of the Royal Statistical Society B, 26, 211-252 (1964).","2930","    if method == 'warn':","2931","        warnings.warn(\"The default value of 'method' will change from \"","2932","                      \"'box-cox' to 'yeo-johnson' in version 0.23. Set \"","2933","                      \"the 'method' argument explicitly to silence this \"","2934","                      \"warning in the meantime.\",","2935","                      FutureWarning)","2936","        method = 'box-cox'"],"delete":["2491","    Yeo-Johson transform. The optimal parameter for stabilizing variance and","2546","    NaNs are treated as missing values: disregarded in fit, and maintained in","2547","    transform.","2846","def power_transform(X, method='box-cox', standardize=True, copy=True):","2847","    \"\"\"Apply a power transform featurewise to make data more Gaussian-like.","2848","","2854","    Currently, power_transform() supports the Box-Cox transform. Box-Cox","2855","    requires input data to be strictly positive. The optimal parameter","2856","    for stabilizing variance and minimizing skewness is estimated","2857","    through maximum likelihood.","2869","    method : str, (default='box-cox')","2870","        The power transform method. Currently, 'box-cox' (Box-Cox transform)","2871","        is the only option available.","2878","        Set to False to perform inplace computation.","2885","    >>> print(power_transform(data))  # doctest: +ELLIPSIS","2892","    PowerTransformer: Performs power transformation using the ``Transformer``","2893","        API (as part of a preprocessing :class:`sklearn.pipeline.Pipeline`).","2900","    NaNs are treated as missing values: disregarded to compute the statistics,","2901","    and maintained during the data transformation.","2909","    G.E.P. Box and D.R. Cox, \"An Analysis of Transformations\", Journal of the","2910","    Royal Statistical Society B, 26, 211-252 (1964)."]}]}},"e73acef80de4159722b11e3cd6c20920382b9728":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/ensemble\/gradient_boosting.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["26","- :class:`ensemble.GradientBoostingClassifier` for multiclass","27","  classification. |Fix|","75","- |Fix| Fixed a bug in :class:`ensemble.GradientBoostingClassifier` where","76","  the gradients would be incorrectly computed in multiclass classification","77","  problems. :issue:`12715` by :user:`Nicolas Hug<NicolasHug>`.","78",""],"delete":[]}],"sklearn\/ensemble\/gradient_boosting.py":[{"add":["1165","        # Need to pass a copy of y_pred to negative_gradient() because y_pred","1166","        # is partially updated at the end of the loop in","1167","        # update_terminal_regions(), and gradients need to be evaluated at","1168","        # iteration i - 1.","1169","        y_pred_copy = y_pred.copy()","1170","","1175","            residual = loss.negative_gradient(y, y_pred_copy, k=k,"],"delete":["1169","            residual = loss.negative_gradient(y, y_pred, k=k,"]}]}},"e1dd0d85c4a19795523668403bb066c6d0b9592b":{"changes":{"sklearn\/utils\/tests\/test_testing.py":"MODIFY","sklearn\/utils\/testing.py":"MODIFY","sklearn\/linear_model\/tests\/test_sgd.py":"MODIFY"},"diff":{"sklearn\/utils\/tests\/test_testing.py":[{"add":["10","import pytest","11","","214","    # Check that passing warning class as first positional argument","215","    warning_class = UserWarning","216","    match = \"'obj' should be a callable.+you should use 'category=UserWarning'\"","217","","218","    with pytest.raises(ValueError, match=match):","219","        silence_warnings_func = ignore_warnings(warning_class)(","220","            _warning_function)","221","        silence_warnings_func()","222","","223","    with pytest.raises(ValueError, match=match):","224","        @ignore_warnings(warning_class)","225","        def test():","226","            pass","227",""],"delete":[]}],"sklearn\/utils\/testing.py":[{"add":["277","    obj : callable or None","278","        callable where you want to ignore the warnings.","294","    if isinstance(obj, type) and issubclass(obj, Warning):","295","        # Avoid common pitfall of passing category as the first positional","296","        # argument which result in the test not being run","297","        warning_name = obj.__name__","298","        raise ValueError(","299","            \"'obj' should be a callable where you want to ignore warnings. \"","300","            \"You passed a warning class instead: 'obj={warning_name}'. \"","301","            \"If you want to pass a warning class to ignore_warnings, \"","302","            \"you should use 'category={warning_name}'\".format(","303","                warning_name=warning_name))","304","    elif callable(obj):"],"delete":["292","    if callable(obj):"]}],"sklearn\/linear_model\/tests\/test_sgd.py":[{"add":["321","    @ignore_warnings(category=ConvergenceWarning)"],"delete":["321","    @ignore_warnings(ConvergenceWarning)"]}]}},"32e5fd4fe8ff9f66886d8ac69c6514a9295d358a":{"changes":{"sklearn\/neighbors\/base.py":"MODIFY"},"diff":{"sklearn\/neighbors\/base.py":[{"add":["285","def _tree_query_parallel_helper(tree, data, n_neighbors, return_distance):","286","    \"\"\"Helper for the Parallel calls in KNeighborsMixin.kneighbors","287","","288","    The Cython method tree.query is not directly picklable by cloudpickle","289","    under PyPy.","290","    \"\"\"","291","    return tree.query(data, n_neighbors, return_distance)","292","","293","","444","                delayed_query = delayed(_tree_query_parallel_helper,","448","                delayed_query = delayed(_tree_query_parallel_helper)","452","                    self._tree, X[s], n_neighbors, return_distance)","573","def _tree_query_radius_parallel_helper(tree, data, radius, return_distance):","574","    \"\"\"Helper for the Parallel calls in RadiusNeighborsMixin.radius_neighbors","575","","576","    The Cython method tree.query_radius is not directly picklable by","577","    cloudpickle under PyPy.","578","    \"\"\"","579","    return tree.query_radius(data, radius, return_distance)","580","","581","","738","                delayed_query = delayed(_tree_query_radius_parallel_helper,","742","                delayed_query = delayed(_tree_query_radius_parallel_helper)","745","                delayed_query(self._tree, X[s], radius, return_distance)"],"delete":["435","                delayed_query = delayed(self._tree.query,","439","                delayed_query = delayed(self._tree.query)","443","                    X[s], n_neighbors, return_distance)","720","                delayed_query = delayed(self._tree.query_radius,","724","                delayed_query = delayed(self._tree.query_radius)","727","                delayed_query(X[s], radius, return_distance)"]}]}},"3e715fdafe14ea4bb7c2329202d078f8de95b192":{"changes":{"sklearn\/linear_model\/tests\/test_coordinate_descent.py":"MODIFY","sklearn\/linear_model\/coordinate_descent.py":"MODIFY","sklearn\/linear_model\/tests\/test_sparse_coordinate_descent.py":"MODIFY","sklearn\/linear_model\/cd_fast.pyx":"MODIFY"},"diff":{"sklearn\/linear_model\/tests\/test_coordinate_descent.py":[{"add":["830","","831","","832","@pytest.mark.parametrize('klass, n_classes, kwargs',","833","                         [(Lasso, 1, dict(precompute=True)),","834","                          (Lasso, 1, dict(precompute=False)),","835","                          (MultiTaskLasso, 2, dict()),","836","                          (MultiTaskLasso, 2, dict())])","837","def test_enet_coordinate_descent(klass, n_classes, kwargs):","838","    \"\"\"Test that a warning is issued if model does not converge\"\"\"","839","    clf = klass(max_iter=2, **kwargs)","840","    n_samples = 5","841","    n_features = 2","842","    X = np.ones((n_samples, n_features)) * 1e50","843","    y = np.ones((n_samples, n_classes))","844","    if klass == Lasso:","845","        y = y.ravel()","846","    assert_warns(ConvergenceWarning, clf.fit, X, y)"],"delete":[]}],"sklearn\/linear_model\/coordinate_descent.py":[{"add":[],"delete":["25","from ..exceptions import ConvergenceWarning","483","        if dual_gap_ > eps_:","484","            warnings.warn('Objective did not converge.' +","485","                          ' You might want' +","486","                          ' to increase the number of iterations.' +","487","                          ' Fitting data with very small alpha' +","488","                          ' may cause precision problems.',","489","                          ConvergenceWarning)","1814","        if self.dual_gap_ > self.eps_:","1815","            warnings.warn('Objective did not converge, you might want'","1816","                          ' to increase the number of iterations',","1817","                          ConvergenceWarning)","1818",""]}],"sklearn\/linear_model\/tests\/test_sparse_coordinate_descent.py":[{"add":["10","from sklearn.utils.testing import assert_warns","11","from sklearn.exceptions import ConvergenceWarning","294","","295","","296","def test_sparse_enet_coordinate_descent():","297","    \"\"\"Test that a warning is issued if model does not converge\"\"\"","298","    clf = Lasso(max_iter=2)","299","    n_samples = 5","300","    n_features = 2","301","    X = sp.csc_matrix((n_samples, n_features)) * 1e50","302","    y = np.ones(n_samples)","303","    assert_warns(ConvergenceWarning, clf.fit, X, y)"],"delete":[]}],"sklearn\/linear_model\/cd_fast.pyx":[{"add":["17","from ..exceptions import ConvergenceWarning","249","","250","        else:","251","            with gil:","252","                warnings.warn(\"Objective did not converge.\"","253","                \" You might want to increase the number of iterations.\"","254","                \" Duality gap: {}, tolerance: {}\".format(gap, tol),","255","                ConvergenceWarning)","256","","467","        else:","468","            with gil:","469","                warnings.warn(\"Objective did not converge.\"","470","                \" You might want to increase the number of iterations.\"","471","                \" Duality gap: {}, tolerance: {}\".format(gap, tol),","472","                ConvergenceWarning)","473","","622","","623","        with gil:","624","            warnings.warn(\"Objective did not converge.\"","625","            \" You might want to increase the number of iterations.\"","626","            \" Duality gap: {}, tolerance: {}\".format(gap, tol),","627","            ConvergenceWarning)","628","","819","                else:","820","                    with gil:","821","                        warnings.warn(\"Objective did not converge.\"","822","                        \" You might want to increase the number of iterations.\"","823","                        \" Duality gap: {}, tolerance: {}\".format(gap, tol),","824","                        ConvergenceWarning)"],"delete":[]}]}},"121dd5ab3bb03203480941ccef2df72cf9cf791d":{"changes":{"sklearn\/neural_network\/rbm.py":"MODIFY"},"diff":{"sklearn\/neural_network\/rbm.py":[{"add":["311","        if sp.issparse(v):"],"delete":["21","from ..utils import issparse","312","        if issparse(v):"]}]}},"eb36c28fd9a5735f597502bae7e1a029408c6cf1":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/feature_extraction\/text.py":"MODIFY","sklearn\/feature_extraction\/tests\/test_text.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["87",":mod:`sklearn.feature_extraction`","88","...........................","89","","90","- |Fix| Fixed a regression in v0.20.0 where","91","  :func:`feature_extraction.text.CountVectorizer` and other text vectorizers","92","  could error during stop words validation with custom preprocessors","93","  or tokenizers. :issue:`12393` by `Roman Yurchak`_.","94",""],"delete":[]}],"sklearn\/feature_extraction\/text.py":[{"add":["272","        \"\"\"Check if stop words are consistent","273","","274","        Returns","275","        -------","276","        is_consistent : True if stop words are consistent with the preprocessor","277","                        and tokenizer, False if they are not, None if the check","278","                        was previously performed, \"error\" if it could not be","279","                        performed (e.g. because of the use of a custom","280","                        preprocessor \/ tokenizer)","281","        \"\"\"","282","        if id(self.stop_words) == getattr(self, '_stop_words_id', None):","283","            # Stop words are were previously validated","284","            return None","285","","287","        try:","297","                warnings.warn('Your stop_words may be inconsistent with '","298","                              'your preprocessing. Tokenizing the stop '","299","                              'words generated tokens %r not in '","300","                              'stop_words.' % sorted(inconsistent))","301","            return not inconsistent","302","        except Exception:","303","            # Failed to check stop words consistency (e.g. because a custom","304","            # preprocessor or tokenizer was used)","305","            self._stop_words_id = id(self.stop_words)","306","            return 'error'"],"delete":["273","        if id(self.stop_words) != getattr(self, '_stop_words_id', None):","283","                warnings.warn('Your stop_words may be inconsistent with your '","284","                              'preprocessing. Tokenizing the stop words '","285","                              'generated tokens %r not in stop_words.' %","286","                              sorted(inconsistent))"]}],"sklearn\/feature_extraction\/tests\/test_text.py":[{"add":["1","import re","1124","def _check_stop_words_consistency(estimator):","1125","    stop_words = estimator.get_stop_words()","1126","    tokenize = estimator.build_tokenizer()","1127","    preprocess = estimator.build_preprocessor()","1128","    return estimator._check_stop_words_consistency(stop_words, preprocess,","1129","                                                   tokenize)","1130","","1131","","1146","        # reset stop word validation","1147","        del vec._stop_words_id","1148","        assert _check_stop_words_consistency(vec) is False","1152","    assert _check_stop_words_consistency(vec) is None","1158","","1159","","1160","@fails_if_pypy","1161","@pytest.mark.parametrize('Estimator',","1162","                         [CountVectorizer, TfidfVectorizer, HashingVectorizer])","1163","def test_stop_word_validation_custom_preprocessor(Estimator):","1164","    data = [{'text': 'some text'}]","1165","","1166","    vec = Estimator()","1167","    assert _check_stop_words_consistency(vec) is True","1168","","1169","    vec = Estimator(preprocessor=lambda x: x['text'],","1170","                    stop_words=['and'])","1171","    assert _check_stop_words_consistency(vec) == 'error'","1172","    # checks are cached","1173","    assert _check_stop_words_consistency(vec) is None","1174","    vec.fit_transform(data)","1175","","1176","    class CustomEstimator(Estimator):","1177","        def build_preprocessor(self):","1178","            return lambda x: x['text']","1179","","1180","    vec = CustomEstimator(stop_words=['and'])","1181","    assert _check_stop_words_consistency(vec) == 'error'","1182","","1183","    vec = Estimator(tokenizer=lambda doc: re.compile(r'\\w{1,}')","1184","                                            .findall(doc),","1185","                    stop_words=['and'])","1186","    assert _check_stop_words_consistency(vec) is True"],"delete":[]}]}},"e500447596ed4e50dcfb315aad98c77f6a6876d6":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/linear_model\/tests\/test_logistic.py":"MODIFY","sklearn\/linear_model\/logistic.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["589","- Fixed a bug in :func:`logistic.logistic_regression_path` to ensure that the","590","  returned coefficients are correct when ``multiclass='multinomial'``.","591","  Previously, some of the coefficients would override each other, leading to","592","  incorrect results in :class:`logistic.LogisticRegressionCV`. :issue:`11724`","593","  by :user:`Nicolas Hug <NicolasHug>`.","594",""],"delete":[]}],"sklearn\/linear_model\/tests\/test_logistic.py":[{"add":["1309","","1310","","1311","def test_logistic_regression_path_coefs_multinomial():","1312","    # Make sure that the returned coefs by logistic_regression_path when","1313","    # multi_class='multinomial' don't override each other (used to be a","1314","    # bug).","1315","    X, y = make_classification(n_samples=200, n_classes=3, n_informative=2,","1316","                               n_redundant=0, n_clusters_per_class=1,","1317","                               random_state=0, n_features=2)","1318","    Cs = [.00001, 1, 10000]","1319","    coefs, _, _ = logistic_regression_path(X, y, penalty='l1', Cs=Cs,","1320","                                           solver='saga', random_state=0,","1321","                                           multi_class='multinomial')","1322","","1323","    with pytest.raises(AssertionError):","1324","        assert_array_almost_equal(coefs[0], coefs[1], decimal=1)","1325","    with pytest.raises(AssertionError):","1326","        assert_array_almost_equal(coefs[0], coefs[2], decimal=1)","1327","    with pytest.raises(AssertionError):","1328","        assert_array_almost_equal(coefs[1], coefs[2], decimal=1)"],"delete":[]}],"sklearn\/linear_model\/logistic.py":[{"add":["574","        n_features + 1, where the last item represents the intercept. For","575","        ``multiclass='multinomial'``, the shape is (n_classes, n_cs,","576","        n_features) or (n_classes, n_cs, n_features + 1).","766","            coefs.append(multi_w0.copy())","772","    return np.array(coefs), np.array(Cs), n_iter"],"delete":["574","        n_features + 1, where the last item represents the intercept.","764","            coefs.append(multi_w0)","770","    return coefs, np.array(Cs), n_iter"]}]}},"4e8194909ce0f8879ebe8073ae5d37e3fdc8a00f":{"changes":{"sklearn\/utils\/fixes.py":"MODIFY","sklearn\/utils\/tests\/test_fixes.py":"MODIFY"},"diff":{"sklearn\/utils\/fixes.py":[{"add":["311","# numerical or object values for all numpy versions.","312","if np_version < (1, 13):","315","else:","316","    def _object_dtype_isnan(X):","317","        return X != X"],"delete":["311","# numerical or object values for all nupy versions.","312","","313","_nan_object_array = np.array([np.nan], dtype=object)","314","_nan_object_mask = _nan_object_array != _nan_object_array","315","","316","if np.array_equal(_nan_object_mask, np.array([True])):","317","    def _object_dtype_isnan(X):","318","        return X != X","319","","320","else:"]}],"sklearn\/utils\/tests\/test_fixes.py":[{"add":["19","from sklearn.utils.fixes import _object_dtype_isnan","91","","92","","93","@pytest.mark.parametrize(\"dtype, val\", ([object, 1],","94","                                        [object, \"a\"],","95","                                        [float, 1]))","96","def test_object_dtype_isnan(dtype, val):","97","    X = np.array([[val, np.nan],","98","                  [np.nan, val]], dtype=dtype)","99","","100","    expected_mask = np.array([[False, True],","101","                              [True, False]])","102","","103","    mask = _object_dtype_isnan(X)","104","","105","    assert_array_equal(mask, expected_mask)"],"delete":[]}]}},"8681ece37388c771098c3ee9236e9993db9e92d4":{"changes":{"sklearn\/tests\/test_docstring_parameters.py":"MODIFY","sklearn\/utils\/tests\/test_testing.py":"MODIFY","sklearn\/utils\/testing.py":"MODIFY"},"diff":{"sklearn\/tests\/test_docstring_parameters.py":[{"add":["74","            if inspect.isabstract(cls):","88","                    cls.__init__, cdoc)","89","","103","                    method, ignore=param_ignore)","121","","122","    msg = '\\n'.join(incorrect)","124","        raise AssertionError(\"Docstring Error:\\n\" + msg)","140","            source = inspect.getsource(mod)"],"delete":["25","","75","            if isabstract(cls):","87","","90","                    cls.__init__, cdoc, class_name=cname)","104","                    method, ignore=param_ignore, class_name=cname)","122","    msg = '\\n' + '\\n'.join(sorted(list(set(incorrect))))","124","        raise AssertionError(\"Docstring Error: \" + msg)","140","            source = getsource(mod)"]}],"sklearn\/utils\/tests\/test_testing.py":[{"add":["325","def f_too_many_param_docstring(a, b):","326","    \"\"\"Function f","327","","328","    Parameters","329","    ----------","330","    a : int","331","        Parameter a","332","    b : int","333","        Parameter b","334","    c : int","335","        Parameter c","336","","337","    Returns","338","    -------","339","    d : list","340","        Parameter c","341","    \"\"\"","342","    d = a + b","343","    return d","344","","345","","492","","495","","498","","503","    messages = [","504","            [\"In function: sklearn.utils.tests.test_testing.f_bad_order\",","505","             \"There's a parameter name mismatch in function docstring w.r.t.\"","506","             \" function signature, at index 0 diff: 'b' != 'a'\",","507","             \"Full diff:\",","508","             \"- ['b', 'a']\",","509","             \"+ ['a', 'b']\"],","510","","511","            [\"In function: \" +","512","                \"sklearn.utils.tests.test_testing.f_too_many_param_docstring\",","513","             \"Parameters in function docstring have more items w.r.t. function\"","514","             \" signature, first extra item: c\",","515","             \"Full diff:\",","516","             \"- ['a', 'b']\",","517","             \"+ ['a', 'b', 'c']\",","518","             \"?          +++++\"],","519","","520","            [\"In function: sklearn.utils.tests.test_testing.f_missing\",","521","             \"Parameters in function docstring have less items w.r.t. function\"","522","             \" signature, first missing item: b\",","523","             \"Full diff:\",","524","             \"- ['a', 'b']\",","525","             \"+ ['a']\"],","526","","527","            [\"In function: sklearn.utils.tests.test_testing.Klass.f_missing\",","528","             \"Parameters in function docstring have less items w.r.t. function\"","529","             \" signature, first missing item: X\",","530","             \"Full diff:\",","531","             \"- ['X', 'y']\",","532","             \"+ []\"],","533","","534","            [\"In function: \" +","535","             \"sklearn.utils.tests.test_testing.MockMetaEstimator.predict\",","536","             \"There's a parameter name mismatch in function docstring w.r.t.\"","537","             \" function signature, at index 0 diff: 'X' != 'y'\",","538","             \"Full diff:\",","539","             \"- ['X']\",","540","             \"?   ^\",","541","             \"+ ['y']\",","542","             \"?   ^\"],","543","","544","            [\"In function: \" +","545","             \"sklearn.utils.tests.test_testing.MockMetaEstimator.\"","546","             + \"predict_proba\",","547","             \"Parameters in function docstring have less items w.r.t. function\"","548","             \" signature, first missing item: X\",","549","             \"Full diff:\",","550","             \"- ['X']\",","551","             \"+ []\"],","552","","553","            [\"In function: \" +","554","                \"sklearn.utils.tests.test_testing.MockMetaEstimator.score\",","555","             \"Parameters in function docstring have less items w.r.t. function\"","556","             \" signature, first missing item: X\",","557","             \"Full diff:\",","558","             \"- ['X']\",","559","             \"+ []\"],","560","","561","            [\"In function: \" +","562","                \"sklearn.utils.tests.test_testing.MockMetaEstimator.fit\",","563","             \"Parameters in function docstring have less items w.r.t. function\"","564","             \" signature, first missing item: X\",","565","             \"Full diff:\",","566","             \"- ['X', 'y']\",","567","             \"+ []\"],","568","","569","            ]","573","    for msg, f in zip(messages,","574","                      [f_bad_order,","575","                       f_too_many_param_docstring,","576","                       f_missing,","577","                       Klass.f_missing,","578","                       mock_meta.predict,","579","                       mock_meta.predict_proba,","580","                       mock_meta.score,","581","                       mock_meta.fit]):","583","        assert msg == incorrect, ('\\n\"%s\"\\n not in \\n\"%s\"' % (msg, incorrect))"],"delete":["479","    messages = [\"a != b\", \"arg mismatch: ['b']\", \"arg mismatch: ['X', 'y']\",","480","                \"predict y != X\",","481","                \"predict_proba arg mismatch: ['X']\",","482","                \"score arg mismatch: ['X']\",","483","                \".fit arg mismatch: ['X', 'y']\"]","487","    for mess, f in zip(messages,","488","                       [f_bad_order, f_missing, Klass.f_missing,","489","                        mock_meta.predict, mock_meta.predict_proba,","490","                        mock_meta.score, mock_meta.fit]):","492","        assert len(incorrect) >= 1","493","        assert mess in incorrect[0], '\"%s\" not in \"%s\"' % (mess, incorrect[0])"]}],"sklearn\/utils\/testing.py":[{"add":["699","def _get_func_name(func):","716","","717","    qualname = func.__qualname__","718","    if qualname != func.__name__:","719","        parts.append(qualname[:qualname.find('.')])","725","def check_docstring_parameters(func, doc=None, ignore=None):","746","    func_name = _get_func_name(func)","759","    # Get the arguments from the function signature","760","    param_signature = list(filter(lambda x: x not in ignore, _get_args(func)))","762","    if len(param_signature) > 0 and param_signature[0] == 'self':","763","        param_signature.remove('self')","765","    # Analyze function's docstring","776","    param_docs = []","778","        # Type hints are empty only if parameter name ended with :","789","        # Create a list of parameters to compare with the parameters gotten","790","        # from the func signature","792","            param_docs.append(name.split(':')[0].strip('` '))","794","    # If one of the docstring's parameters had an error then return that","795","    # incorrect message","796","    if len(incorrect) > 0:","797","        return incorrect","799","    # Remove the parameters that should be ignored from list","800","    param_docs = list(filter(lambda x: x not in ignore, param_docs))","801","","802","    # The following is derived from pytest, Copyright (c) 2004-2017 Holger","803","    # Krekel and others, Licensed under MIT License. See","804","    # https:\/\/github.com\/pytest-dev\/pytest","805","","806","    message = []","807","    for i in range(min(len(param_docs), len(param_signature))):","808","        if param_signature[i] != param_docs[i]:","809","            message += [\"There's a parameter name mismatch in function\"","810","                        \" docstring w.r.t. function signature, at index %s\"","811","                        \" diff: %r != %r\" %","812","                        (i, param_signature[i], param_docs[i])]","813","            break","814","    if len(param_signature) > len(param_docs):","815","        message += [\"Parameters in function docstring have less items w.r.t.\"","816","                    \" function signature, first missing item: %s\" %","817","                    param_signature[len(param_docs)]]","818","","819","    elif len(param_signature) < len(param_docs):","820","        message += [\"Parameters in function docstring have more items w.r.t.\"","821","                    \" function signature, first extra item: %s\" %","822","                    param_docs[len(param_signature)]]","823","","824","    # If there wasn't any difference in the parameters themselves between","825","    # docstring and signature including having the same length then return","826","    # empty list","827","    if len(message) == 0:","828","        return []","829","","830","    import difflib","831","    import pprint","832","","833","    param_docs_formatted = pprint.pformat(param_docs).splitlines()","834","    param_signature_formatted = pprint.pformat(param_signature).splitlines()","835","","836","    message += [\"Full diff:\"]","837","","838","    message.extend(","839","        line.strip() for line in difflib.ndiff(param_signature_formatted,","840","                                               param_docs_formatted)","841","    )","842","","843","    incorrect.extend(message)","844","","845","    # Prepend function name","846","    incorrect = ['In function: ' + func_name] + incorrect","847",""],"delete":["699","def _get_func_name(func, class_name=None):","706","    class_name : string, optional (default: None)","707","       If ``func`` is a class method and the class name is known specify","708","       class_name for the error message.","719","    if class_name is not None:","720","        parts.append(class_name)","721","    elif hasattr(func, 'im_class'):","722","        parts.append(func.im_class.__name__)","728","def check_docstring_parameters(func, doc=None, ignore=None, class_name=None):","739","    class_name : string, optional (default: None)","740","       If ``func`` is a class method and the class name is known specify","741","       class_name for the error message.","752","    func_name = _get_func_name(func, class_name=class_name)","765","    args = list(filter(lambda x: x not in ignore, _get_args(func)))","767","    if len(args) > 0 and args[0] == 'self':","768","        args.remove('self')","780","    param_names = []","793","            param_names.append(name.split(':')[0].strip('` '))","795","    param_names = list(filter(lambda x: x not in ignore, param_names))","797","    if len(param_names) != len(args):","798","        bad = str(sorted(list(set(param_names) ^ set(args))))","799","        incorrect += [func_name + ' arg mismatch: ' + bad]","800","    else:","801","        for n1, n2 in zip(param_names, args):","802","            if n1 != n2:","803","                incorrect += [func_name + ' ' + n1 + ' != ' + n2]"]}]}},"6581b0d14238e6cd7588eb6ba3511cceb0343bc0":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/ensemble\/tests\/test_forest.py":"MODIFY","sklearn\/ensemble\/forest.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["79","- |Fix| Fixed a bug in :mod:`ensemble` where the ``predict`` method would","80","   error for multiclass multioutput forests models if any targets were strings.","81","   :issue:`12834` by :user:`Elizabeth Sander <elsander>`.","82",""],"delete":[]}],"sklearn\/ensemble\/tests\/test_forest.py":[{"add":["534","            assert len(proba) == 2","535","            assert proba[0].shape == (4, 2)","536","            assert proba[1].shape == (4, 4)","539","            assert len(log_proba) == 2","540","            assert log_proba[0].shape == (4, 2)","541","            assert log_proba[1].shape == (4, 4)","550","@pytest.mark.filterwarnings('ignore:The default value of n_estimators')","551","@pytest.mark.parametrize('name', FOREST_CLASSIFIERS)","552","def test_multioutput_string(name):","553","    # Check estimators on multi-output problems with string outputs.","554","","555","    X_train = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1], [-2, 1],","556","               [-1, 1], [-1, 2], [2, -1], [1, -1], [1, -2]]","557","    y_train = [[\"red\", \"blue\"], [\"red\", \"blue\"], [\"red\", \"blue\"],","558","               [\"green\", \"green\"], [\"green\", \"green\"], [\"green\", \"green\"],","559","               [\"red\", \"purple\"], [\"red\", \"purple\"], [\"red\", \"purple\"],","560","               [\"green\", \"yellow\"], [\"green\", \"yellow\"], [\"green\", \"yellow\"]]","561","    X_test = [[-1, -1], [1, 1], [-1, 1], [1, -1]]","562","    y_test = [[\"red\", \"blue\"], [\"green\", \"green\"],","563","              [\"red\", \"purple\"], [\"green\", \"yellow\"]]","564","","565","    est = FOREST_ESTIMATORS[name](random_state=0, bootstrap=False)","566","    y_pred = est.fit(X_train, y_train).predict(X_test)","567","    assert_array_equal(y_pred, y_test)","568","","569","    with np.errstate(divide=\"ignore\"):","570","        proba = est.predict_proba(X_test)","571","        assert len(proba) == 2","572","        assert proba[0].shape == (4, 2)","573","        assert proba[1].shape == (4, 4)","574","","575","        log_proba = est.predict_log_proba(X_test)","576","        assert len(log_proba) == 2","577","        assert log_proba[0].shape == (4, 2)","578","        assert log_proba[1].shape == (4, 4)","579","","580",""],"delete":["534","            assert_equal(len(proba), 2)","535","            assert_equal(proba[0].shape, (4, 2))","536","            assert_equal(proba[1].shape, (4, 4))","539","            assert_equal(len(log_proba), 2)","540","            assert_equal(log_proba[0].shape, (4, 2))","541","            assert_equal(log_proba[1].shape, (4, 4))"]}],"sklearn\/ensemble\/forest.py":[{"add":["549","            # all dtypes should be the same, so just take the first","550","            class_type = self.classes_[0].dtype","551","            predictions = np.empty((n_samples, self.n_outputs_),","552","                                   dtype=class_type)"],"delete":["549","            predictions = np.zeros((n_samples, self.n_outputs_))"]}]}},"dfdf605f67605cc2638d49b51c0dcb177813b3b5":{"changes":{"sklearn\/preprocessing\/_encoders.py":"MODIFY","sklearn\/preprocessing\/tests\/test_encoders.py":"MODIFY"},"diff":{"sklearn\/preprocessing\/_encoders.py":[{"add":["12","from .. import get_config as _get_config","17","from ..utils.fixes import _argmax, _object_dtype_isnan","40","    def _check_X(self, X):","41","        \"\"\"","42","        Perform custom check_array:","43","        - convert list of strings to object dtype","44","        - check for missing values for object dtype data (check_array does","45","          not do that)","47","        \"\"\"","54","        if X.dtype == np.dtype('object'):","55","            if not _get_config()['assume_finite']:","56","                if _object_dtype_isnan(X).any():","57","                    raise ValueError(\"Input contains NaN\")","58","","59","        return X","60","","61","    def _fit(self, X, handle_unknown='error'):","62","        X = self._check_X(X)","63","","93","        X = self._check_X(X)"],"delete":["16","from ..utils.fixes import _argmax","39","    def _fit(self, X, handle_unknown='error'):","76","","77","        X_temp = check_array(X, dtype=None)","78","        if not hasattr(X, 'dtype') and np.issubdtype(X_temp.dtype, np.str_):","79","            X = check_array(X, dtype=np.object)","80","        else:","81","            X = X_temp"]}],"sklearn\/preprocessing\/tests\/test_encoders.py":[{"add":["499","@pytest.mark.parametrize(\"X\", [np.array([[1, np.nan]]).T,","500","                               np.array([['a', np.nan]], dtype=object).T],","501","                         ids=['numeric', 'object'])","502","@pytest.mark.parametrize(\"handle_unknown\", ['error', 'ignore'])","503","def test_one_hot_encoder_raise_missing(X, handle_unknown):","504","    ohe = OneHotEncoder(categories='auto', handle_unknown=handle_unknown)","505","","506","    with pytest.raises(ValueError, match=\"Input contains NaN\"):","507","        ohe.fit(X)","508","","509","    with pytest.raises(ValueError, match=\"Input contains NaN\"):","510","        ohe.fit_transform(X)","511","","512","    ohe.fit(X[:1, :])","513","","514","    with pytest.raises(ValueError, match=\"Input contains NaN\"):","515","        ohe.transform(X)","516","","517","","545","@pytest.mark.parametrize(\"X\", [np.array([[1, np.nan]]).T,","546","                               np.array([['a', np.nan]], dtype=object).T],","547","                         ids=['numeric', 'object'])","548","def test_ordinal_encoder_raise_missing(X):","549","    ohe = OrdinalEncoder()","550","","551","    with pytest.raises(ValueError, match=\"Input contains NaN\"):","552","        ohe.fit(X)","553","","554","    with pytest.raises(ValueError, match=\"Input contains NaN\"):","555","        ohe.fit_transform(X)","556","","557","    ohe.fit(X[:1, :])","558","","559","    with pytest.raises(ValueError, match=\"Input contains NaN\"):","560","        ohe.transform(X)","561","","562",""],"delete":[]}]}},"71525c1482f0aacecd14ba9f19668d426557dc74":{"changes":{"\/dev\/null":"DELETE","sklearn\/utils\/tests\/test_show_versions.py":"ADD"},"diff":{"\/dev\/null":[{"add":[],"delete":[]}],"sklearn\/utils\/tests\/test_show_versions.py":[{"add":[],"delete":[]}]}},"2afee939df90e92e7dcde7d738c5be5f5611dae0":{"changes":{"sklearn\/preprocessing\/_encoders.py":"MODIFY","sklearn\/preprocessing\/tests\/test_encoders.py":"MODIFY"},"diff":{"sklearn\/preprocessing\/_encoders.py":[{"add":["383","                # Set categories_ to empty list if no categorical columns exist","384","                n_features = X.shape[1]","385","                sel = np.zeros(n_features, dtype=bool)","386","                sel[np.asarray(self.categorical_features)] = True","387","                if sum(sel) == 0:","388","                    self.categories_ = []","597","        check_is_fitted(self, 'categories_')","690","        elif len(input_features) != len(self.categories_):"],"delete":["23","","26","","685","        elif(len(input_features) != len(self.categories_)):"]}],"sklearn\/preprocessing\/tests\/test_encoders.py":[{"add":["9","from sklearn.exceptions import NotFittedError","253","def test_one_hot_encoder_not_fitted():","254","    X = np.array([['a'], ['b']])","255","    enc = OneHotEncoder(categories=['a', 'b'])","256","    msg = (\"This OneHotEncoder instance is not fitted yet. \"","257","           \"Call 'fit' with appropriate arguments before using this method.\")","258","    with pytest.raises(NotFittedError, match=msg):","259","        enc.transform(X)","260","","261","","262","def test_one_hot_encoder_no_categorical_features():","263","    X = np.array([[3, 2, 1], [0, 1, 1]], dtype='float64')","264","","265","    cat = [False, False, False]","266","    enc = OneHotEncoder(categorical_features=cat)","267","    with ignore_warnings(category=(DeprecationWarning, FutureWarning)):","268","        X_tr = enc.fit_transform(X)","269","    expected_features = np.array(list(), dtype='object')","270","    assert_array_equal(X, X_tr)","271","    assert_array_equal(enc.get_feature_names(), expected_features)","272","    assert enc.categories_ == []","273","","274",""],"delete":[]}]}},"1b90237b45dfa1652831cfe72bd4b29a31b070ec":{"changes":{"sklearn\/tests\/test_pipeline.py":"MODIFY","doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/pipeline.py":"MODIFY"},"diff":{"sklearn\/tests\/test_pipeline.py":[{"add":["577","def test_pipeline_correctly_adjusts_steps(passthrough):","578","    X = np.array([[1]])","579","    y = np.array([1])","580","    mult2 = Mult(mult=2)","581","    mult3 = Mult(mult=3)","582","    mult5 = Mult(mult=5)","583","","584","    pipeline = Pipeline([","585","        ('m2', mult2),","586","        ('bad', passthrough),","587","        ('m3', mult3),","588","        ('m5', mult5)","589","    ])","590","","591","    pipeline.fit(X, y)","592","    expected_names = ['m2', 'bad', 'm3', 'm5']","593","    actual_names = [name for name, _ in pipeline.steps]","594","    assert expected_names == actual_names","595","","596","","597","@pytest.mark.parametrize('passthrough', [None, 'passthrough'])"],"delete":[]}],"doc\/whats_new\/v0.20.rst":[{"add":["17",":mod:`sklearn.pipeline`","18",".......................","20","- |Fix| Fixed a regression in :class:`pipeline.Pipeline` where the ``steps``","21","  parameter may not have been updated correctly when a step is set to ``None``","22","  or ``'passthrough'``. :user:`Thomas Fan <thomasjpfan>`."],"delete":[]}],"sklearn\/pipeline.py":[{"add":["186","        for idx, (name, trans) in enumerate(islice(self.steps, 0, stop)):","188","                yield idx, name, trans","221","        for step_idx, name, transformer in self._iter(with_final=False):","342","        for _, name, transform in self._iter(with_final=False):","391","        for _, name, transform in self._iter(with_final=False):","410","        for _, name, transform in self._iter(with_final=False):","429","        for _, name, transform in self._iter(with_final=False):","458","        for _, _, transform in self._iter():","482","        for _, _, transform in self._iter():","489","        for _, _, transform in reverse_iter:","516","        for _, name, transform in self._iter(with_final=False):"],"delete":["186","        for name, trans in islice(self.steps, 0, stop):","188","                yield name, trans","221","        for step_idx, (name, transformer) in enumerate(","222","                self._iter(with_final=False)):","343","        for name, transform in self._iter(with_final=False):","392","        for name, transform in self._iter(with_final=False):","411","        for name, transform in self._iter(with_final=False):","430","        for name, transform in self._iter(with_final=False):","459","        for _, transform in self._iter():","483","        for _, transform in self._iter():","490","        for _, transform in reverse_iter:","517","        for name, transform in self._iter(with_final=False):"]}]}},"a85eeb28eea46164d17111aaad1eb8728835534d":{"changes":{"sklearn\/svm\/tests\/test_sparse.py":"MODIFY","sklearn\/utils\/estimator_checks.py":"MODIFY"},"diff":{"sklearn\/svm\/tests\/test_sparse.py":[{"add":["12","                                   ignore_warnings, skip_if_32bit)","73","@skip_if_32bit","268","@skip_if_32bit"],"delete":["12","                                   ignore_warnings)"]}],"sklearn\/utils\/estimator_checks.py":[{"add":["1547","    assert_allclose(y_dec, decision)"],"delete":["1547","    assert_array_equal(y_dec, decision)"]}]}},"67130a583206f0ccf2f5a224129bdf90b2c5ece7":{"changes":{"sklearn\/cluster\/k_means_.py":"MODIFY"},"diff":{"sklearn\/cluster\/k_means_.py":[{"add":["109","        # XXX: numerical imprecision can result in a candidate_id out of range","110","        np.clip(candidate_ids, None, closest_dist_sq.size - 1,","111","                out=candidate_ids)"],"delete":[]}]}},"3df0c1971ad7f27e3ccd75ef63d6b5543313bb52":{"changes":{"sklearn\/feature_extraction\/image.py":"MODIFY"},"diff":{"sklearn\/feature_extraction\/image.py":[{"add":["53","    _, n_y, n_z = img.shape","333","        (n_patches, patch_height, patch_width, n_channels)","334","        The collection of patches extracted from the image, where `n_patches`","335","        is either `max_patches` or the total number of patches that can be","336","        extracted.","340","    >>> from sklearn.datasets import load_sample_images","342","    >>> # Use the array data from the first image in this dataset:","343","    >>> one_image = load_sample_images().images[0]","344","    >>> print('Image shape: {}'.format(one_image.shape))","345","    Image shape: (427, 640, 3)","347","    >>> print('Patches shape: {}'.format(patches.shape))","348","    Patches shape: (272214, 2, 2, 3)","349","    >>> # Here are just two of these patches:","350","    >>> print(patches[1]) # doctest: +NORMALIZE_WHITESPACE","351","    [[[174 201 231]","352","      [174 201 231]]","353","     [[173 200 230]","354","      [173 200 230]]]","355","    >>> print(patches[800])# doctest: +NORMALIZE_WHITESPACE","356","    [[[187 214 243]","357","      [188 215 244]]","358","     [[187 214 243]","359","      [188 215 244]]]","462","    Examples","463","    --------","464","    >>> from sklearn.datasets import load_sample_images","465","    >>> from sklearn.feature_extraction import image","466","    >>> # Use the array data from the second image in this dataset:","467","    >>> X = load_sample_images().images[1]","468","    >>> print('Image shape: {}'.format(X.shape))","469","    Image shape: (427, 640, 3)","470","    >>> pe = image.PatchExtractor(patch_size=(2, 2))","471","    >>> pe_fit = pe.fit(X)","472","    >>> pe_trans = pe.transform(X)","473","    >>> print('Patches shape: {}'.format(pe_trans.shape))","474","    Patches shape: (545706, 2, 2)","476",""],"delete":["53","    n_x, n_y, n_z = img.shape","333","         (n_patches, patch_height, patch_width, n_channels)","334","         The collection of patches extracted from the image, where `n_patches`","335","         is either `max_patches` or the total number of patches that can be","336","         extracted.","340","    >>> import numpy as np","342","    >>> one_image = np.arange(16).reshape((4, 4))","343","    >>> one_image","344","    array([[ 0,  1,  2,  3],","345","           [ 4,  5,  6,  7],","346","           [ 8,  9, 10, 11],","347","           [12, 13, 14, 15]])","349","    >>> patches.shape","350","    (9, 2, 2)","351","    >>> patches[0]","352","    array([[0, 1],","353","           [4, 5]])","354","    >>> patches[1]","355","    array([[1, 2],","356","           [5, 6]])","357","    >>> patches[8]","358","    array([[10, 11],","359","           [14, 15]])","422","","500",""]}]}},"fb0e0fcf5f94b4db515749023da4db0ff06a4fd6":{"changes":{"examples\/preprocessing\/plot_discretization_classification.py":"MODIFY","examples\/svm\/plot_svm_scale_c.py":"MODIFY"},"diff":{"examples\/preprocessing\/plot_discretization_classification.py":[{"add":["101","fig, axes = plt.subplots(nrows=len(datasets), ncols=len(classifiers) + 1,","102","                         figsize=(21, 9))","103","","106","","123","    ax = axes[ds_cnt, 0]","138","    for est_idx, (name, (estimator, param_grid)) in \\","139","            enumerate(zip(names, classifiers)):","140","        ax = axes[ds_cnt, est_idx + 1]","187","for i, suptitle in zip([1, 3, 5], suptitles):","188","    ax = axes[0, i]"],"delete":["101","figure = plt.figure(figsize=(21, 9))","104","i = 1","121","    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)","134","    i += 1","137","    for name, (estimator, param_grid) in zip(names, classifiers):","138","        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)","175","        i += 1","186","for i, suptitle in zip([2, 4, 6], suptitles):","187","    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)"]}],"examples\/svm\/plot_svm_scale_c.py":[{"add":["121","for clf, cs, X, y in clf_sets:","123","    fig, axes = plt.subplots(nrows=2, sharey=True, figsize=(9, 10))","140","        for ax, (scaler, name) in zip(axes, scales):","141","            ax.set_xlabel('C')","142","            ax.set_ylabel('CV Score')","144","            ax.semilogx(grid_cs, scores, label=\"fraction %.2f\" %","145","                        train_size, color=colors[k], lw=lw)","146","            ax.set_title('scaling=%s, penalty=%s, loss=%s' %","147","                         (name, clf.penalty, clf.loss))"],"delete":["121","for fignum, (clf, cs, X, y) in enumerate(clf_sets):","123","    plt.figure(fignum, figsize=(9, 10))","140","        for subplotnum, (scaler, name) in enumerate(scales):","141","            plt.subplot(2, 1, subplotnum + 1)","142","            plt.xlabel('C')","143","            plt.ylabel('CV Score')","145","            plt.semilogx(grid_cs, scores, label=\"fraction %.2f\" %","146","                         train_size, color=colors[k], lw=lw)","147","            plt.title('scaling=%s, penalty=%s, loss=%s' %","148","                      (name, clf.penalty, clf.loss))"]}]}},"bf11d44558d42736575e9ff2d0516a32db625e30":{"changes":{"sklearn\/utils\/random.py":"MODIFY","sklearn\/metrics\/base.py":"MODIFY","examples\/applications\/wikipedia_principal_eigenvector.py":"MODIFY","sklearn\/discriminant_analysis.py":"MODIFY"},"diff":{"sklearn\/utils\/random.py":[{"add":["160","        if not np.isclose(np.sum(class_prob_j), 1.0):"],"delete":["160","        if np.sum(class_prob_j) != 1.0:"]}],"sklearn\/metrics\/base.py":[{"add":["96","        if np.isclose(average_weight.sum(), 0.0):"],"delete":["96","        if average_weight.sum() == 0:"]}],"examples\/applications\/wikipedia_principal_eigenvector.py":[{"add":["206","    dangle = np.asarray(np.where(np.isclose(X.sum(axis=1), 0),","207","                                 1.0 \/ n, 0)).ravel()"],"delete":["206","    dangle = np.asarray(np.where(X.sum(axis=1) == 0, 1.0 \/ n, 0)).ravel()"]}],"sklearn\/discriminant_analysis.py":[{"add":["439","        if not np.isclose(self.priors_.sum(), 1.0):"],"delete":["439","        if self.priors_.sum() != 1:"]}]}},"251e58b9e2c098aa805b58dd128864ec66ec782e":{"changes":{"sklearn\/cluster\/tests\/test_optics.py":"MODIFY","sklearn\/cluster\/optics_.py":"MODIFY"},"diff":{"sklearn\/cluster\/tests\/test_optics.py":[{"add":["35","    # check attribute types and sizes","36","    assert clust.core_sample_indices_.ndim == 1","37","    assert clust.core_sample_indices_.size > 0","38","    assert clust.core_sample_indices_.dtype.kind == 'i'","39","","40","    assert clust.labels_.shape == (len(X),)","41","    assert clust.labels_.dtype.kind == 'i'","42","","43","    assert clust.reachability_.shape == (len(X),)","44","    assert clust.reachability_.dtype.kind == 'f'","45","","46","    assert clust.core_distances_.shape == (len(X),)","47","    assert clust.core_distances_.dtype.kind == 'f'","48","","49","    assert clust.ordering_.shape == (len(X),)","50","    assert clust.ordering_.dtype.kind == 'i'","51","    assert set(clust.ordering_) == set(range(len(X)))","52",""],"delete":[]}],"sklearn\/cluster\/optics_.py":[{"add":["357","        self.ordering_ = self._calculate_optics_order(X, nbrs)","370","    # OPTICS helper functions","372","    def _calculate_optics_order(self, X, nbrs):","373","        # Main OPTICS loop. Not parallelizable. The order that entries are","374","        # written to the 'ordering_' list is important!","375","        processed = np.zeros(X.shape[0], dtype=bool)","376","        ordering = np.zeros(X.shape[0], dtype=int)","377","        ordering_idx = 0","378","        for point in range(X.shape[0]):","379","            if processed[point]:","380","                continue","381","            if self.core_distances_[point] <= self.max_eps:","382","                while not processed[point]:","383","                    processed[point] = True","384","                    ordering[ordering_idx] = point","385","                    ordering_idx += 1","386","                    point = self._set_reach_dist(point, processed, X, nbrs)","387","            else:  # For very noisy points","388","                ordering[ordering_idx] = point","389","                ordering_idx += 1","390","                processed[point] = True","391","        return ordering","393","    def _set_reach_dist(self, point_index, processed, X, nbrs):","394","        P = X[point_index:point_index + 1]","399","        unproc = np.compress((~np.take(processed, indices)).ravel(),","402","        if not unproc.size:","403","            # Everything is already processed. Return to main loop","406","        dists = pairwise_distances(P, np.take(X, unproc, axis=0),","407","                                   self.metric, n_jobs=1).ravel()","408","","409","        rdists = np.maximum(dists, self.core_distances_[point_index])","410","        new_reach = np.minimum(np.take(self.reachability_, unproc), rdists)","411","        self.reachability_[unproc] = new_reach","412","","413","        # Define return order based on reachability distance","414","        return (unproc[quick_scan(np.take(self.reachability_, unproc),","415","                                  dists)])","416",""],"delete":["333","        self._processed = np.zeros((n_samples, 1), dtype=bool)","340","        self.ordering_ = []","359","        # Main OPTICS loop. Not parallelizable. The order that entries are","360","        # written to the 'ordering_' list is important!","361","        for point in range(n_samples):","362","            if not self._processed[point]:","363","                self._expand_cluster_order(point, X, nbrs)","376","    # OPTICS helper functions; these should not be public #","378","    def _expand_cluster_order(self, point, X, nbrs):","379","        # As above, not parallelizable. Parallelizing would allow items in","380","        # the 'unprocessed' list to switch to 'processed'","381","        if self.core_distances_[point] <= self.max_eps:","382","            while not self._processed[point]:","383","                self._processed[point] = True","384","                self.ordering_.append(point)","385","                point = self._set_reach_dist(point, X, nbrs)","386","        else:  # For very noisy points","387","            self.ordering_.append(point)","388","            self._processed[point] = True","390","    def _set_reach_dist(self, point_index, X, nbrs):","391","        P = np.array(X[point_index]).reshape(1, -1)","396","        unproc = np.compress((~np.take(self._processed, indices)).ravel(),","399","        if len(unproc) > 0:","400","            dists = pairwise_distances(P, np.take(X, unproc, axis=0),","401","                                       self.metric, n_jobs=None).ravel()","402","","403","            rdists = np.maximum(dists, self.core_distances_[point_index])","404","            new_reach = np.minimum(np.take(self.reachability_, unproc), rdists)","405","            self.reachability_[unproc] = new_reach","406","","407","        # Checks to see if everything is already processed;","408","        # if so, return control to main loop","409","        if unproc.size > 0:","410","            # Define return order based on reachability distance","411","            return(unproc[quick_scan(np.take(self.reachability_, unproc),","412","                                     dists)])","413","        else:"]}]}},"013d295a13721ffade7ac321437c6d4458a64c7d":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/datasets\/olivetti_faces.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["53","- |Fix| Fixed olivetti faces dataset ``DESCR`` attribute to point to the right","54","  location in :func:`datasets.fetch_olivetti_faces`. :issue:`12441` by","55","  :user:`Jrmie du Boisberranger <jeremiedbb>`","56",""],"delete":[]}],"sklearn\/datasets\/olivetti_faces.py":[{"add":["126","    with open(join(module_path, 'descr', 'olivetti_faces.rst')) as rst_file:"],"delete":["126","    with open(join(module_path, 'descr', 'covtype.rst')) as rst_file:"]}]}},"2d50b5e60093f4893d83418709ace19b91d3695b":{"changes":{"doc\/whats_new\/v0.19.rst":"MODIFY"},"diff":{"doc\/whats_new\/v0.19.rst":[{"add":["9","**July, 2018**"],"delete":["9","**October, 2018**"]}]}}}