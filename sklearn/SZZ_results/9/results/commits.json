{"d7c956afc8c67642bbd42fd9bacb007b5fbc443a":{"changes":{"sklearn\/model_selection\/_split.py":"MODIFY","sklearn\/model_selection\/tests\/test_split.py":"MODIFY"},"diff":{"sklearn\/model_selection\/_split.py":[{"add":["27","from ..utils.validation import check_array","475","        groups = check_array(groups, ensure_2d=False, dtype=None)","622","            Note that providing ``y`` is sufficient to generate the splits and","623","            hence ``np.zeros(n_samples)`` may be used as a placeholder for","624","            ``X`` instead of actual training data.","625","","628","            Stratification is done based on the y labels.","630","        groups : object","631","            Always ignored, exists for compatibility.","641","        y = check_array(y, ensure_2d=False, dtype=None)","705","            Always ignored, exists for compatibility.","708","            Always ignored, exists for compatibility.","754","    >>> logo = LeaveOneGroupOut()","755","    >>> logo.get_n_splits(X, y, groups)","757","    >>> print(logo)","759","    >>> for train_index, test_index in logo.split(X, y, groups):","779","        groups = check_array(groups, copy=True, ensure_2d=False, dtype=None)","841","    >>> lpgo = LeavePGroupsOut(n_groups=2)","842","    >>> lpgo.get_n_splits(X, y, groups)","844","    >>> print(lpgo)","846","    >>> for train_index, test_index in lpgo.split(X, y, groups):","872","        groups = check_array(groups, copy=True, ensure_2d=False, dtype=None)","894","            ``np.zeros(n_samples)`` may be used as a placeholder.","898","            ``np.zeros(n_samples)`` may be used as a placeholder.","911","        groups = check_array(groups, ensure_2d=False, dtype=None)","912","        X, y, groups = indexable(X, y, groups)","1109","        groups = check_array(groups, ensure_2d=False, dtype=None)","1250","        y = check_array(y, ensure_2d=False, dtype=None)","1304","            Note that providing ``y`` is sufficient to generate the splits and","1305","            hence ``np.zeros(n_samples)`` may be used as a placeholder for","1306","            ``X`` instead of actual training data.","1307","","1310","            Stratification is done based on the y labels.","1312","        groups : object","1313","            Always ignored, exists for compatibility.","1323","        y = check_array(y, ensure_2d=False, dtype=None)","1632","        the class labels."],"delete":["623","        groups : array-like, with shape (n_samples,), optional","624","            Group labels for the samples used while splitting the dataset into","625","            train\/test set.","698","            The target variable for supervised learning problems.","701","            Group labels for the samples used while splitting the dataset into","702","            train\/test set.","748","    >>> lol = LeaveOneGroupOut()","749","    >>> lol.get_n_splits(X, y, groups)","751","    >>> print(lol)","753","    >>> for train_index, test_index in lol.split(X, y, groups):","773","        groups = np.array(groups, copy=True)","835","    >>> lpl = LeavePGroupsOut(n_groups=2)","836","    >>> lpl.get_n_splits(X, y, groups)","838","    >>> print(lpl)","840","    >>> for train_index, test_index in lpl.split(X, y, groups):","866","        groups = np.array(groups, copy=True)","1295","        groups : array-like, with shape (n_samples,), optional","1296","            Group labels for the samples used while splitting the dataset into","1297","            train\/test set.","1615","        the groups array."]}],"sklearn\/model_selection\/tests\/test_split.py":[{"add":["61","P_sparse = coo_matrix(np.eye(5))","62","test_groups = (","63","    np.array([1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 3]),","64","    np.array([0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3]),","65","    np.array([0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2]),","66","    np.array([1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4]),","67","    [1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 3],","68","    ['1', '1', '1', '1', '2', '2', '2', '3', '3', '3', '3', '3'])","72","class MockClassifier(object):","73","    \"\"\"Dummy classifier to test the cross-validation\"\"\"","74","","75","    def __init__(self, a=0, allow_nd=False):","76","        self.a = a","77","        self.allow_nd = allow_nd","78","","79","    def fit(self, X, Y=None, sample_weight=None, class_prior=None,","80","            sparse_sample_weight=None, sparse_param=None, dummy_int=None,","81","            dummy_str=None, dummy_obj=None, callback=None):","82","        \"\"\"The dummy arguments are to test that this fit function can","83","        accept non-array arguments through cross-validation, such as:","84","            - int","85","            - str (this is actually array-like)","86","            - object","87","            - function","88","        \"\"\"","89","        self.dummy_int = dummy_int","90","        self.dummy_str = dummy_str","91","        self.dummy_obj = dummy_obj","92","        if callback is not None:","93","            callback(self)","94","","95","        if self.allow_nd:","96","            X = X.reshape(len(X), -1)","97","        if X.ndim >= 3 and not self.allow_nd:","98","            raise ValueError('X cannot be d')","99","        if sample_weight is not None:","100","            assert_true(sample_weight.shape[0] == X.shape[0],","101","                        'MockClassifier extra fit_param sample_weight.shape[0]'","102","                        ' is {0}, should be {1}'.format(sample_weight.shape[0],","103","                                                        X.shape[0]))","104","        if class_prior is not None:","105","            assert_true(class_prior.shape[0] == len(np.unique(y)),","106","                        'MockClassifier extra fit_param class_prior.shape[0]'","107","                        ' is {0}, should be {1}'.format(class_prior.shape[0],","108","                                                        len(np.unique(y))))","109","        if sparse_sample_weight is not None:","110","            fmt = ('MockClassifier extra fit_param sparse_sample_weight'","111","                   '.shape[0] is {0}, should be {1}')","112","            assert_true(sparse_sample_weight.shape[0] == X.shape[0],","113","                        fmt.format(sparse_sample_weight.shape[0], X.shape[0]))","114","        if sparse_param is not None:","115","            fmt = ('MockClassifier extra fit_param sparse_param.shape '","116","                   'is ({0}, {1}), should be ({2}, {3})')","117","            assert_true(sparse_param.shape == P_sparse.shape,","118","                        fmt.format(sparse_param.shape[0],","119","                                   sparse_param.shape[1],","120","                                   P_sparse.shape[0], P_sparse.shape[1]))","121","        return self","122","","123","    def predict(self, T):","124","        if self.allow_nd:","125","            T = T.reshape(len(T), -1)","126","        return T[:, 0]","127","","128","    def score(self, X=None, Y=None):","129","        return 1. \/ (1 + np.abs(self.a))","130","","131","    def get_params(self, deep=False):","132","        return {'a': self.a, 'allow_nd': self.allow_nd}","133","","134","","336","    # Make sure string labels are also supported","337","    X = np.ones(7)","338","    y1 = ['1', '1', '1', '0', '0', '0', '0']","339","    y2 = [1, 1, 1, 0, 0, 0, 0]","340","    np.testing.assert_equal(","341","        list(StratifiedKFold(2).split(X, y1)),","342","        list(StratifiedKFold(2).split(X, y2)))","343","","566","          np.concatenate([[i] * (100 + i) for i in range(11)]),","567","          [1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 3],","568","          ['1', '1', '1', '1', '2', '2', '2', '3', '3', '3', '3', '3'],","574","        y = np.asanyarray(y)  # To make it indexable for y[train]","682","    for groups_i in test_groups:","683","        X = y = np.ones(len(groups_i))","692","        assert_equal(slo.get_n_splits(X, y, groups=groups_i), n_splits)","694","        l_unique = np.unique(groups_i)","695","        l = np.asarray(groups_i)","697","        for train, test in slo.split(X, y, groups=groups_i):","718","def test_leave_one_p_group_out():","719","    logo = LeaveOneGroupOut()","720","    lpgo_1 = LeavePGroupsOut(n_groups=1)","721","    lpgo_2 = LeavePGroupsOut(n_groups=2)","722","","723","    # Make sure the repr works","724","    assert_equal(repr(logo), 'LeaveOneGroupOut()')","725","    assert_equal(repr(lpgo_1), 'LeavePGroupsOut(n_groups=1)')","726","    assert_equal(repr(lpgo_2), 'LeavePGroupsOut(n_groups=2)')","727","    assert_equal(repr(LeavePGroupsOut(n_groups=3)),","728","                 'LeavePGroupsOut(n_groups=3)')","729","","730","    for j, (cv, p_groups_out) in enumerate(((logo, 1), (lpgo_1, 1),","731","                                            (lpgo_2, 2))):","732","        for i, groups_i in enumerate(test_groups):","733","            n_groups = len(np.unique(groups_i))","734","            n_splits = (n_groups if p_groups_out == 1","735","                        else n_groups * (n_groups - 1) \/ 2)","736","            X = y = np.ones(len(groups_i))","737","","738","            # Test that the length is correct","739","            assert_equal(cv.get_n_splits(X, y, groups=groups_i), n_splits)","740","","741","            groups_arr = np.asarray(groups_i)","742","","743","            # Split using the original list \/ array \/ list of string groups_i","744","            for train, test in cv.split(X, y, groups=groups_i):","745","                # First test: no train group is in the test set and vice versa","746","                assert_array_equal(np.intersect1d(groups_arr[train],","747","                                                  groups_arr[test]).tolist(),","748","                                   [])","749","","750","                # Second test: train and test add up to all the data","751","                assert_equal(len(train) + len(test), len(groups_i))","752","","753","                # Third test:","754","                # The number of groups in test must be equal to p_groups_out","755","                assert_true(np.unique(groups_arr[test]).shape[0], p_groups_out)","756","","757","","775","    assert_equal(","776","        3, LeavePGroupsOut(n_groups=2).get_n_splits(X, y=X,","777","                                                    groups=groups))","779","    assert_equal(3, LeaveOneGroupOut().get_n_splits(X, y=X,","780","                                                    groups=groups))","785","    assert_raise_message(ValueError, \"Found array with 0 sample(s)\", next,","901","def train_test_split_list_input():","902","    # Check that when y is a list \/ list of string labels, it works.","903","    X = np.ones(7)","904","    y1 = ['1'] * 4 + ['0'] * 3","905","    y2 = np.hstack((np.ones(4), np.zeros(3)))","906","    y3 = y2.tolist()","907","","908","    for stratify in (True, False):","909","        X_train1, X_test1, y_train1, y_test1 = train_test_split(","910","            X, y1, stratify=y1 if stratify else None, random_state=0)","911","        X_train2, X_test2, y_train2, y_test2 = train_test_split(","912","            X, y2, stratify=y2 if stratify else None, random_state=0)","913","        X_train3, X_test3, y_train3, y_test3 = train_test_split(","914","            X, y3, stratify=y3 if stratify else None, random_state=0)","915","","916","        np.testing.assert_equal(X_train1, X_train2)","917","        np.testing.assert_equal(y_train2, y_train3)","918","        np.testing.assert_equal(X_test1, X_test3)","919","        np.testing.assert_equal(y_test3, y_test2)","920","","921","","946","def test_stratifiedshufflesplit_list_input():","947","    # Check that when y is a list \/ list of string labels, it works.","948","    sss = StratifiedShuffleSplit(test_size=2, random_state=42)","949","    X = np.ones(7)","950","    y1 = ['1'] * 4 + ['0'] * 3","951","    y2 = np.hstack((np.ones(4), np.zeros(3)))","952","    y3 = y2.tolist()","953","","954","    np.testing.assert_equal(list(sss.split(X, y1)),","955","                            list(sss.split(X, y2)))","956","    np.testing.assert_equal(list(sss.split(X, y3)),","957","                            list(sss.split(X, y2)))","958","","959","","987","                             [1, 1, 0, 1], [0, 0, 1, 0]])","1119","    # groups can also be a list","1120","    cv_iter = list(lkf.split(X, y, groups.tolist()))","1121","    for (train1, test1), (train2, test2) in zip(lkf.split(X, y, groups),","1122","                                                cv_iter):","1123","        assert_array_equal(train1, train2)","1124","        assert_array_equal(test1, test2)","1125",""],"delete":["487","          np.concatenate([[i] * (100 + i) for i in range(11)])","600","    groups = [np.array([1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 3]),","601","              np.array([0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3]),","602","              np.array([0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2]),","603","              np.array([1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4])]","604","","605","    for l in groups:","606","        X = y = np.ones(len(l))","615","        assert_equal(slo.get_n_splits(X, y, groups=l), n_splits)","617","        l_unique = np.unique(l)","619","        for train, test in slo.split(X, y, groups=l):","657","    assert_equal(3, LeavePGroupsOut(n_groups=2).get_n_splits(X, y, groups))","659","    assert_equal(3, LeaveOneGroupOut().get_n_splits(X, y, groups))","664","    msg = (\"The groups parameter contains fewer than 2 unique groups ([]). \"","665","           \"LeaveOneGroupOut expects at least 2.\")","666","    assert_raise_message(ValueError, msg, next,","833","                            [1, 1, 0, 1], [0, 0, 1, 0]])"]}]}},"66de4faf2b0edf8953cf56c90509eabc7a5845f6":{"changes":{"doc\/whats_new.rst":"MODIFY"},"diff":{"doc\/whats_new.rst":[{"add":["50","     that matches the ``classes_`` attribute of ``best_estimator_``. :issue:`7661`","51","     by :user:`Alyssa Batula <abatula>`_ and :user:`Dylan Werner-Meier <unautre>`.","98","","157","   - Fix a bug where :class:`sklearn.feature_selection.SelectFdr` did not","158","     exactly implement Benjamini-Hochberg procedure. It formerly may have","159","     selected fewer features than it should.","160","     :issue:`7490` by :user:`Peng Meng <mpjlu>`.","161","","162","   - :class:`sklearn.manifold.LocallyLinearEmbedding` now correctly handles","163","     integer inputs. :issue:`6282` by `Jake Vanderplas`_.","164","","165","   - The ``min_weight_fraction_leaf`` parameter of tree-based classifiers and","166","     regressors now assumes uniform sample weights by default if the","167","     ``sample_weight`` argument is not passed to the ``fit`` function.","168","     Previously, the parameter was silently ignored. :issue:`7301`","169","     by :user:`Nelson Liu <nelson-liu>`.","170","","171","   - Numerical issue with :class:`linear_model.RidgeCV` on centered data when","172","     `n_features > n_samples`. :issue:`6178` by `Bertrand Thirion`_","173","","174","   - Tree splitting criterion classes' cloning\/pickling is now memory safe","175","     :issue:`7680` by :user:`Ibraim Ganiev <olologin>`.","176","","177","   - Fixed a bug where :class:`decomposition.NMF` sets its ``n_iters_``","178","     attribute in `transform()`. :issue:`7553` by :user:`Ekaterina","179","     Krivich <kiote>`.","180","","181","   - :class:`sklearn.linear_model.LogisticRegressionCV` now correctly handles","182","     string labels. :issue:`5874` by `Raghav RV`_.","183","","204","Trees and forests","205","","206","   - The ``min_weight_fraction_leaf`` parameter of tree-based classifiers and","207","     regressors now assumes uniform sample weights by default if the","208","     ``sample_weight`` argument is not passed to the ``fit`` function.","209","     Previously, the parameter was silently ignored. :issue:`7301` by `Nelson","210","     Liu`_.","211","","212","   - Tree splitting criterion classes' cloning\/pickling is now memory safe.","213","     :issue:`7680` by `Ibraim Ganiev`_.","214","","215","","218","   - Length of ``explained_variance_ratio`` of","224","   - Numerical issue with :class:`linear_model.RidgeCV` on centered data when","225","     ``n_features > n_samples``. :issue:`6178` by `Bertrand Thirion`_","226",""],"delete":["50","     that matches the ``classes_`` attribute of ``best_estimator_``. (`#7661","51","     <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/7661>`_) by `Alyssa","52","     Batula`_ and :user:`Dylan Werner-Meier <unautre>`.","95","   - Fix a bug where :class:`sklearn.feature_selection.SelectFdr` did not","96","     exactly implement Benjamini-Hochberg procedure. It formerly may have","97","     selected fewer features than it should.","98","     :issue:`7490` by :user:`Peng Meng <mpjlu>`.","99","","100","   - :class:`sklearn.manifold.LocallyLinearEmbedding` now correctly handles","101","     integer inputs. :issue:`6282` by `Jake Vanderplas`_.","102","","103","   - The ``min_weight_fraction_leaf`` parameter of tree-based classifiers and","104","     regressors now assumes uniform sample weights by default if the","105","     ``sample_weight`` argument is not passed to the ``fit`` function.","106","     Previously, the parameter was silently ignored. :issue:`7301`","107","     by :user:`Nelson Liu <nelson-liu>`.","108","","109","   - Numerical issue with :class:`linear_model.RidgeCV` on centered data when","110","     `n_features > n_samples`. :issue:`6178` by `Bertrand Thirion`_","111","","112","   - Tree splitting criterion classes' cloning\/pickling is now memory safe","113","     :issue:`7680` by :user:`Ibraim Ganiev <olologin>`.","114","","115","   - Fixed a bug where :class:`decomposition.NMF` sets its ``n_iters_``","116","     attribute in `transform()`. :issue:`7553` by :user:`Ekaterina","117","     Krivich <kiote>`.","118","","119","   - :class:`sklearn.linear_model.LogisticRegressionCV` now correctly handles","120","     string labels. :issue:`5874` by `Raghav RV`_.","121","","206","   - Length of `explained_variance_ratio` of"]}]}},"e5ceda88f2a24b3dd4f9a94404828f982cdf52ad":{"changes":{"sklearn\/utils\/validation.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/utils\/tests\/test_validation.py":"MODIFY"},"diff":{"sklearn\/utils\/validation.py":[{"add":["202","    accept_sparse : string, boolean or list\/tuple of strings","204","        'csr', 'coo', 'dok', 'bsr', 'lil', 'dia'). If the input is sparse but","205","        not in the allowed format, it will be converted to the first listed","206","        format. True allows the input to be any format. False means","207","        that a sparse matrix input will raise an error.","209","    dtype : string, type or None","212","    copy : boolean","216","    force_all_finite : boolean","228","","229","    if isinstance(accept_sparse, six.string_types):","230","        accept_sparse = [accept_sparse]","231","","232","    if accept_sparse is False:","233","        raise TypeError('A sparse matrix was passed, but dense '","234","                        'data is required. Use X.toarray() to '","235","                        'convert to a dense numpy array.')","236","    elif isinstance(accept_sparse, (list, tuple)):","237","        if len(accept_sparse) == 0:","238","            raise ValueError(\"When providing 'accept_sparse' \"","239","                             \"as a tuple or list, it must contain at \"","240","                             \"least one string value.\")","241","        # ensure correct sparse format","242","        if spmatrix.format not in accept_sparse:","243","            # create new with correct sparse","244","            spmatrix = spmatrix.asformat(accept_sparse[0])","245","            changed_format = True","246","    elif accept_sparse is not True:","247","        # any other type","248","        raise ValueError(\"Parameter 'accept_sparse' should be a string, \"","249","                         \"boolean or list of strings. You provided \"","250","                         \"'accept_sparse={}'.\".format(accept_sparse))","268","def check_array(array, accept_sparse=False, dtype=\"numeric\", order=None,","283","    accept_sparse : string, boolean or list\/tuple of strings (default=False)","285","        'csr', etc. If the input is sparse but not in the allowed format,","286","        it will be converted to the first listed format. True allows the input","287","        to be any format. False means that a sparse matrix input will","288","        raise an error.","339","    # accept_sparse 'None' deprecation check","340","    if accept_sparse is None:","341","        warnings.warn(","342","            \"Passing 'None' to parameter 'accept_sparse' in methods \"","343","            \"check_array and check_X_y is deprecated in version 0.19 \"","344","            \"and will be removed in 0.21. Use 'accept_sparse=False' \"","345","            \" instead.\", DeprecationWarning)","346","        accept_sparse = False","430","def check_X_y(X, y, accept_sparse=False, dtype=\"numeric\", order=None,","451","    accept_sparse : string, boolean or list of string (default=False)","453","        'csr', etc. If the input is sparse but not in the allowed format,","454","        it will be converted to the first listed format. True allows the input","455","        to be any format. False means that a sparse matrix input will","456","        raise an error."],"delete":["202","    accept_sparse : string, list of string or None (default=None)","204","        'csr', 'coo', 'dok', 'bsr', 'lil', 'dia'). None means that sparse","205","        matrix input will raise an error.  If the input is sparse but not in","206","        the allowed format, it will be converted to the first listed format.","208","    dtype : string, type or None (default=none)","211","    copy : boolean (default=False)","215","    force_all_finite : boolean (default=True)","223","    if accept_sparse in [None, False]:","224","        raise TypeError('A sparse matrix was passed, but dense '","225","                        'data is required. Use X.toarray() to '","226","                        'convert to a dense numpy array.')","231","    if (isinstance(accept_sparse, (list, tuple))","232","            and spmatrix.format not in accept_sparse):","233","        # create new with correct sparse","234","        spmatrix = spmatrix.asformat(accept_sparse[0])","235","        changed_format = True","253","def check_array(array, accept_sparse=None, dtype=\"numeric\", order=None,","268","    accept_sparse : string, list of string or None (default=None)","270","        'csr', etc.  None means that sparse matrix input will raise an error.","271","        If the input is sparse but not in the allowed format, it will be","272","        converted to the first listed format.","323","    if isinstance(accept_sparse, str):","324","        accept_sparse = [accept_sparse]","408","def check_X_y(X, y, accept_sparse=None, dtype=\"numeric\", order=None,","429","    accept_sparse : string, list of string or None (default=None)","431","        'csr', etc.  None means that sparse matrix input will raise an error.","432","        If the input is sparse but not in the allowed format, it will be","433","        converted to the first listed format."]}],"doc\/whats_new.rst":[{"add":["100","     ","101","   - Added type checking to the ``accept_sparse`` parameter in","102","     :mod:`sklearn.utils.validation` methods. This parameter now accepts only","103","     boolean, string, or list\/tuple of strings. ``accept_sparse=None`` is deprecated","104","     and should be replaced by ``accept_sparse=False``.","105","     :issue:`7880` by :user:`Josh Karnofsky <jkarno>`."],"delete":[]}],"sklearn\/utils\/tests\/test_validation.py":[{"add":["323","def test_check_array_accept_sparse_type_exception():","324","    X = [[1, 2], [3, 4]]","325","    X_csr = sp.csr_matrix(X)","326","    invalid_type = SVR()","327","","328","    msg = (\"A sparse matrix was passed, but dense data is required. \"","329","           \"Use X.toarray() to convert to a dense numpy array.\")","330","    assert_raise_message(TypeError, msg,","331","                         check_array, X_csr, accept_sparse=False)","332","    assert_raise_message(TypeError, msg,","333","                         check_array, X_csr, accept_sparse=None)","334","","335","    msg = (\"Parameter 'accept_sparse' should be a string, \"","336","           \"boolean or list of strings. You provided 'accept_sparse={}'.\")","337","    assert_raise_message(ValueError, msg.format(invalid_type),","338","                         check_array, X_csr, accept_sparse=invalid_type)","339","","340","    msg = (\"When providing 'accept_sparse' as a tuple or list, \"","341","           \"it must contain at least one string value.\")","342","    assert_raise_message(ValueError, msg.format([]),","343","                         check_array, X_csr, accept_sparse=[])","344","    assert_raise_message(ValueError, msg.format(()),","345","                         check_array, X_csr, accept_sparse=())","346","","347","    msg = \"'SVR' object\"","348","    assert_raise_message(TypeError, msg,","349","                         check_array, X_csr, accept_sparse=[invalid_type])","350","","351","    # Test deprecation of 'None'","352","    assert_warns(DeprecationWarning, check_array, X, accept_sparse=None)","353","","354","","355","def test_check_array_accept_sparse_no_exception():","356","    X = [[1, 2], [3, 4]]","357","    X_csr = sp.csr_matrix(X)","358","","359","    check_array(X_csr, accept_sparse=True)","360","    check_array(X_csr, accept_sparse='csr')","361","    check_array(X_csr, accept_sparse=['csr'])","362","    check_array(X_csr, accept_sparse=('csr',))","363","","364",""],"delete":[]}]}},"9811b3bd062c9d07b9aff2d621e9c35ee821711d":{"changes":{"sklearn\/svm\/base.py":"MODIFY","sklearn\/svm\/tests\/test_svm.py":"MODIFY"},"diff":{"sklearn\/svm\/base.py":[{"add":["484","            raise AttributeError('coef_ is only available when using a '","485","                                 'linear kernel')"],"delete":["484","            raise ValueError('coef_ is only available when using a '","485","                             'linear kernel')"]}],"sklearn\/svm\/tests\/test_svm.py":[{"add":["59","        assert_true(hasattr(clf, \"coef_\") == (k == 'linear'))","260","    assert_false(hasattr(clf, \"coef_\"))","644","    clf = svm.LinearSVC(penalty='l1', loss='squared_hinge', dual=False,","645","                        random_state=0).fit(X, Y)"],"delete":["259","    assert_raises(ValueError, lambda: clf.coef_)","643","    clf = svm.LinearSVC(penalty='l1', loss='squared_hinge', dual=False, random_state=0).fit(X, Y)"]}]}},"53933866c00f86f74e9ccd533113bbd38062ca9b":{"changes":{"sklearn\/tests\/test_pipeline.py":"MODIFY","sklearn\/pipeline.py":"MODIFY"},"diff":{"sklearn\/tests\/test_pipeline.py":[{"add":["76","class TransfFitParams(Transf):","77","","78","    def fit(self, X, y, **fit_params):","79","        self.fit_params = fit_params","80","        return self","81","","82","","118","    def fit_predict(self, X, y, should_succeed=False):","119","        self.fit(X, y, should_succeed=should_succeed)","120","        return self.predict(X)","121","","321","def test_fit_predict_with_intermediate_fit_params():","322","    # tests that Pipeline passes fit_params to intermediate steps","323","    # when fit_predict is invoked","324","    pipe = Pipeline([('transf', TransfFitParams()), ('clf', FitParamT())])","325","    pipe.fit_predict(X=None,","326","                     y=None,","327","                     transf__should_get_this=True,","328","                     clf__should_succeed=True)","329","    assert_true(pipe.named_steps['transf'].fit_params['should_get_this'])","330","    assert_true(pipe.named_steps['clf'].successful)","331","    assert_false('should_succeed' in pipe.named_steps['transf'].fit_params)","332","","333",""],"delete":[]}],"sklearn\/pipeline.py":[{"add":["355","        Xt, fit_params = self._fit(X, y, **fit_params)"],"delete":["355","        Xt = X","356","        for name, transform in self.steps[:-1]:","357","            if transform is not None:","358","                Xt = transform.fit_transform(Xt)"]}]}},"6a2d8d5bbc261309d59ac30bf650e982f1477dd7":{"changes":{"sklearn\/linear_model\/setup.py":"MODIFY","sklearn\/setup.py":"MODIFY","sklearn\/tree\/setup.py":"MODIFY","sklearn\/feature_extraction\/setup.py":"MODIFY","build_tools\/travis\/install.sh":"MODIFY","build_tools\/travis\/test_script.sh":"MODIFY","setup.py":"MODIFY","sklearn\/ensemble\/setup.py":"MODIFY","sklearn\/metrics\/cluster\/setup.py":"MODIFY","sklearn\/utils\/setup.py":"MODIFY","sklearn\/svm\/setup.py":"MODIFY","\/dev\/null":"DELETE","sklearn\/cluster\/setup.py":"MODIFY",".travis.yml":"MODIFY","sklearn\/utils\/sparsetools\/setup.py":"MODIFY","sklearn\/decomposition\/setup.py":"MODIFY","sklearn\/manifold\/setup.py":"MODIFY","sklearn\/datasets\/setup.py":"MODIFY","sklearn\/neighbors\/setup.py":"MODIFY","sklearn\/__check_build\/setup.py":"MODIFY","sklearn\/_build_utils\/__init__.py":"MODIFY","sklearn\/metrics\/setup.py":"MODIFY"},"diff":{"sklearn\/linear_model\/setup.py":[{"add":["18","    config.add_extension('cd_fast', sources=['cd_fast.pyx'],","27","                         sources=['sgd_fast.pyx'],","37","                         sources=['sag_fast.pyx'],"],"delete":["18","    config.add_extension('cd_fast', sources=['cd_fast.c'],","27","                         sources=['sgd_fast.c'],","37","                         sources=['sag_fast.c'],"]}],"sklearn\/setup.py":[{"add":["4","from sklearn._build_utils import maybe_cythonize_extensions","5","","59","    config.add_extension('_isotonic',","60","                         sources=['_isotonic.pyx'],","61","                         include_dirs=[numpy.get_include()],","62","                         libraries=libraries,","63","                         )","81","    maybe_cythonize_extensions(top_path, config)","82",""],"delete":["57","    config.add_extension(","58","        '_isotonic',","59","        sources=['_isotonic.c'],","60","        include_dirs=[numpy.get_include()],","61","        libraries=libraries,","62","    )"]}],"sklearn\/tree\/setup.py":[{"add":["12","                         sources=[\"_tree.pyx\"],","17","                         sources=[\"_splitter.pyx\"],","22","                         sources=[\"_criterion.pyx\"],","27","                         sources=[\"_utils.pyx\"],"],"delete":["12","                         sources=[\"_tree.c\"],","17","                         sources=[\"_splitter.c\"],","22","                         sources=[\"_criterion.c\"],","27","                         sources=[\"_utils.c\"],"]}],"sklearn\/feature_extraction\/setup.py":[{"add":["13","                         sources=['_hashing.pyx'],"],"delete":["13","                         sources=['_hashing.c'],"]}],"build_tools\/travis\/install.sh":[{"add":["55","            libgfortran mkl flake8 \\","60","            numpy=$NUMPY_VERSION scipy=$SCIPY_VERSION \\","66","    # Temporary work around for Python 2.6 because cython >= 0.23 is","67","    # required for building scikit-learn but python 2.6 and cython","68","    # 0.23 are not compatible in conda. Remove the next line and","69","    # install cython via conda when Python 2.6 support is removed.","70","    pip install cython==$CYTHON_VERSION","71",""],"delete":["55","            cython=$CYTHON_VERSION libgfortran mkl flake8 \\","60","            numpy=$NUMPY_VERSION scipy=$SCIPY_VERSION cython=$CYTHON_VERSION \\","103","    if [ ! -d \"$CACHED_BUILD_DIR\" ]; then","104","        mkdir -p $CACHED_BUILD_DIR","105","    fi","106","","107","    rsync -av --exclude '.git\/' --exclude='testvenv\/' \\","108","          $TRAVIS_BUILD_DIR $CACHED_BUILD_DIR","109","","110","    cd $CACHED_BUILD_DIR\/scikit-learn","111",""]}],"build_tools\/travis\/test_script.sh":[{"add":["42","    cd $OLDPWD"],"delete":["41","    # Is directory still empty ?","42","    ls -ltra","43","","45","    cd $CACHED_BUILD_DIR\/scikit-learn"]}],"setup.py":[{"add":["217","        # For these actions, NumPy is not required"],"delete":["86","            cython_hash_file = os.path.join(cwd, 'cythonize.dat')","87","            if os.path.exists(cython_hash_file):","88","                os.unlink(cython_hash_file)","183","def generate_cython():","184","    cwd = os.path.abspath(os.path.dirname(__file__))","185","    print(\"Cythonizing sources\")","186","    p = subprocess.call([sys.executable, os.path.join(cwd,","187","                                                      'build_tools',","188","                                                      'cythonize.py'),","189","                         'sklearn'],","190","                        cwd=cwd)","191","    if p != 0:","192","        raise RuntimeError(\"Running cythonize failed!\")","193","","194","","232","        # For these actions, NumPy is not required, nor Cythonization","280","        if len(sys.argv) >= 2 and sys.argv[1] not in 'config':","281","            # Cythonize if needed","282","","283","            print('Generating cython files')","284","            cwd = os.path.abspath(os.path.dirname(__file__))","285","            if not os.path.exists(os.path.join(cwd, 'PKG-INFO')):","286","                # Generate Cython sources, unless building from source release","287","                generate_cython()","288","","289","            # Clean left-over .so file","290","            for dirpath, dirnames, filenames in os.walk(","291","                    os.path.join(cwd, 'sklearn')):","292","                for filename in filenames:","293","                    extension = os.path.splitext(filename)[1]","294","                    if extension in (\".so\", \".pyd\", \".dll\"):","295","                        pyx_file = str.replace(filename, extension, '.pyx')","296","                        print(pyx_file)","297","                        if not os.path.exists(os.path.join(dirpath, pyx_file)):","298","                            os.unlink(os.path.join(dirpath, filename))","299",""]}],"sklearn\/ensemble\/setup.py":[{"add":["7","                         sources=[\"_gradient_boosting.pyx\"],"],"delete":["7","                         sources=[\"_gradient_boosting.c\"],"]}],"sklearn\/metrics\/cluster\/setup.py":[{"add":["12","                         sources=[\"expected_mutual_info_fast.pyx\"],"],"delete":["12","                         sources=[\"expected_mutual_info_fast.c\"],"]}],"sklearn\/utils\/setup.py":[{"add":["24","    config.add_extension('sparsefuncs_fast', sources=['sparsefuncs_fast.pyx'],","28","                         sources=['arrayfuncs.pyx'],","36","    config.add_extension('murmurhash',","37","                         sources=['murmurhash.pyx', join(","38","                             'src', 'MurmurHash3.cpp')],","39","                         include_dirs=['src'])","42","                         sources=['lgamma.pyx', join('src', 'gamma.c')],","47","                         sources=['graph_shortest_path.pyx'],","51","                         sources=['fast_dict.pyx'],","57","                         sources=['seq_dataset.pyx'],","61","                         sources=['weight_vector.pyx'],","67","                         sources=[\"_random.pyx\"],","72","                         sources=[\"_logistic_sigmoid.pyx\"],"],"delete":["24","    config.add_extension('sparsefuncs_fast', sources=['sparsefuncs_fast.c'],","28","                         sources=['arrayfuncs.c'],","36","    config.add_extension(","37","        'murmurhash',","38","        sources=['murmurhash.c', join('src', 'MurmurHash3.cpp')],","39","        include_dirs=['src'])","42","                         sources=['lgamma.c', join('src', 'gamma.c')],","47","                         sources=['graph_shortest_path.c'],","51","                         sources=['fast_dict.cpp'],","57","                         sources=['seq_dataset.c'],","61","                         sources=['weight_vector.c'],","67","                         sources=[\"_random.c\"],","72","                         sources=[\"_logistic_sigmoid.c\"],"]}],"sklearn\/svm\/setup.py":[{"add":["26","    libsvm_sources = ['libsvm.pyx']","40","    # liblinear module","45","    liblinear_sources = ['liblinear.pyx',","63","    # end liblinear module","66","    libsvm_sparse_sources = ['libsvm_sparse.pyx']"],"delete":["26","    libsvm_sources = ['libsvm.c']","40","    ### liblinear module","45","    liblinear_sources = ['liblinear.c',","63","    ## end liblinear module","66","    libsvm_sparse_sources = ['libsvm_sparse.c']"]}],"\/dev\/null":[{"add":[],"delete":[]}],"sklearn\/cluster\/setup.py":[{"add":["22","                         sources=['_dbscan_inner.pyx'],","27","                         sources=['_hierarchical.pyx'],","32","                         sources=['_k_means_elkan.pyx'],","36","    config.add_extension('_k_means',","37","                         libraries=cblas_libs,","38","                         sources=['_k_means.pyx'],","39","                         include_dirs=[join('..', 'src', 'cblas'),","40","                                       numpy.get_include(),","41","                                       blas_info.pop('include_dirs', [])],","42","                         extra_compile_args=blas_info.pop(","43","                             'extra_compile_args', []),","44","                         **blas_info","45","                         )"],"delete":["22","                         sources=['_dbscan_inner.cpp'],","27","                         sources=['_hierarchical.cpp'],","32","                         sources=['_k_means_elkan.c'],","36","    config.add_extension(","37","        '_k_means',","38","        libraries=cblas_libs,","39","        sources=['_k_means.c'],","40","        include_dirs=[join('..', 'src', 'cblas'),","41","                      numpy.get_include(),","42","                      blas_info.pop('include_dirs', [])],","43","        extra_compile_args=blas_info.pop('extra_compile_args', []),","44","        **blas_info","45","    )"]}],".travis.yml":[{"add":["29","      COVERAGE=true","32","      NUMPY_VERSION=\"1.6.2\" SCIPY_VERSION=\"0.11.0\" CYTHON_VERSION=\"0.23\"","37","      CYTHON_VERSION=\"0.23.4\""],"delete":["8","  # We use three different cache directory","9","  # to work around a Travis bug with multi-platform cache","11","  - $HOME\/sklearn_build_ubuntu","12","  - $HOME\/sklearn_build_oldest","13","  - $HOME\/sklearn_build_latest","14","  - $HOME\/sklearn_build_numpy_dev","35","      CACHED_BUILD_DIR=\"$HOME\/sklearn_build_ubuntu\" COVERAGE=true","38","      NUMPY_VERSION=\"1.6.2\" SCIPY_VERSION=\"0.11.0\" CYTHON_VERSION=\"0.21\"","39","      CACHED_BUILD_DIR=\"$HOME\/sklearn_build_oldest\"","44","      CYTHON_VERSION=\"0.23.4\" CACHED_BUILD_DIR=\"$HOME\/sklearn_build_latest\"","49","      CACHED_BUILD_DIR=\"$HOME\/dummy\"","68","            CACHED_BUILD_DIR=\"$HOME\/sklearn_build_numpy_dev\""]}],"sklearn\/utils\/sparsetools\/setup.py":[{"add":["9","                         sources=['_traversal.pyx'],","10","                         include_dirs=[numpy.get_include()])","12","                         sources=['_graph_tools.pyx'],","13","                         include_dirs=[numpy.get_include()])"],"delete":["9","                         sources=['_traversal.c'],","10","                         include_dirs=[numpy.get_include()],","11","                         #libraries=libraries","12","                         )","14","                         sources=['_graph_tools.c'],","15","                         include_dirs=[numpy.get_include()],","16","                         #libraries=libraries","17","                         )"]}],"sklearn\/decomposition\/setup.py":[{"add":["13","                         sources=[\"_online_lda.pyx\"],","18","                         sources=['cdnmf_fast.pyx'],"],"delete":["13","                         sources=[\"_online_lda.c\"],","18","                         sources=['cdnmf_fast.c'],"]}],"sklearn\/manifold\/setup.py":[{"add":["14","                         sources=[\"_utils.pyx\"],","23","                         sources=[\"_barnes_hut_tsne.pyx\"],"],"delete":["14","                         sources=[\"_utils.c\"],","23","                         sources=[\"_barnes_hut_tsne.c\"],"]}],"sklearn\/datasets\/setup.py":[{"add":["13","                         sources=['_svmlight_format.pyx'],"],"delete":["13","                         sources=['_svmlight_format.c'],"]}],"sklearn\/neighbors\/setup.py":[{"add":["13","                         sources=['ball_tree.pyx'],","18","                         sources=['kd_tree.pyx'],","23","                         sources=['dist_metrics.pyx'],","30","                         sources=['typedefs.pyx'],"],"delete":["13","                         sources=['ball_tree.c'],","18","                         sources=['kd_tree.c'],","23","                         sources=['dist_metrics.c'],","30","                         sources=['typedefs.c'],"]}],"sklearn\/__check_build\/setup.py":[{"add":["10","                         sources=['_check_build.pyx'],"],"delete":["10","                         sources=['_check_build.c'],"]}],"sklearn\/_build_utils\/__init__.py":[{"add":["8","import os","9","","10","from distutils.version import LooseVersion","14","DEFAULT_ROOT = 'sklearn'","15","CYTHON_MIN_VERSION = '0.23'","16","","40","","41","","42","def build_from_c_and_cpp_files(extensions):","43","    \"\"\"Modify the extensions to build from the .c and .cpp files.","44","","45","    This is useful for releases, this way cython is not required to","46","    run python setup.py install.","47","    \"\"\"","48","    for extension in extensions:","49","        sources = []","50","        for sfile in extension.sources:","51","            path, ext = os.path.splitext(sfile)","52","            if ext in ('.pyx', '.py'):","53","                if extension.language == 'c++':","54","                    ext = '.cpp'","55","                else:","56","                    ext = '.c'","57","                sfile = path + ext","58","            sources.append(sfile)","59","        extension.sources = sources","60","","61","","62","def maybe_cythonize_extensions(top_path, config):","63","    \"\"\"Tweaks for building extensions between release and development mode.\"\"\"","64","    is_release = os.path.exists(os.path.join(top_path, 'PKG-INFO'))","65","","66","    if is_release:","67","        build_from_c_and_cpp_files(config.ext_modules)","68","    else:","69","        message = ('Please install cython with a version >= {0} in order '","70","                   'to build a scikit-learn development version.').format(","71","                       CYTHON_MIN_VERSION)","72","        try:","73","            import Cython","74","            if LooseVersion(Cython.__version__) < CYTHON_MIN_VERSION:","75","                message += ' Your version of Cython was {0}.'.format(","76","                    Cython.__version__)","77","                raise ValueError(message)","78","            from Cython.Build import cythonize","79","        except ImportError as exc:","80","            exc.args += (message,)","81","            raise","82","","83","        config.ext_modules = cythonize(config.ext_modules)"],"delete":["8","DEFAULT_ROOT = 'sklearn'"]}],"sklearn\/metrics\/setup.py":[{"add":["17","                         sources=[\"pairwise_fast.pyx\"],"],"delete":["17","                         sources=[\"pairwise_fast.c\"],"]}]}},"1c41368bac124ab336fb8c7fd4bc846d266228e5":{"changes":{"sklearn\/decomposition\/tests\/test_dict_learning.py":"MODIFY","sklearn\/feature_extraction\/text.py":"MODIFY","sklearn\/naive_bayes.py":"MODIFY","sklearn\/feature_selection\/rfe.py":"MODIFY","sklearn\/tests\/test_multiclass.py":"MODIFY","sklearn\/cluster\/tests\/test_k_means.py":"MODIFY","sklearn\/utils\/multiclass.py":"MODIFY","sklearn\/covariance\/outlier_detection.py":"MODIFY","sklearn\/decomposition\/truncated_svd.py":"MODIFY","sklearn\/tests\/test_multioutput.py":"MODIFY","sklearn\/feature_extraction\/tests\/test_text.py":"MODIFY","sklearn\/feature_selection\/tests\/test_from_model.py":"MODIFY","sklearn\/neighbors\/approximate.py":"MODIFY","sklearn\/decomposition\/dict_learning.py":"MODIFY","sklearn\/multioutput.py":"MODIFY","sklearn\/ensemble\/gradient_boosting.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/dummy.py":"MODIFY","sklearn\/feature_selection\/from_model.py":"MODIFY","sklearn\/multiclass.py":"MODIFY","doc\/modules\/model_evaluation.rst":"MODIFY","sklearn\/ensemble\/base.py":"MODIFY"},"diff":{"sklearn\/decomposition\/tests\/test_dict_learning.py":[{"add":["1","import itertools","28","def test_sparse_encode_shapes_omp():","29","    rng = np.random.RandomState(0)","30","    algorithms = ['omp', 'lasso_lars', 'lasso_cd', 'lars', 'threshold']","31","    for n_components, n_samples in itertools.product([1, 5], [1, 9]):","32","        X_ = rng.randn(n_samples, n_features)","33","        dictionary = rng.randn(n_components, n_features)","34","        for algorithm, n_jobs in itertools.product(algorithms, [1, 3]):","35","            code = sparse_encode(X_, dictionary, algorithm=algorithm,","36","                                 n_jobs=n_jobs)","37","            assert_equal(code.shape, (n_samples, n_components))","38","","39","","43","    assert_equal(dico.components_.shape, (n_components, n_features))","44","","45","    n_components = 1","46","    dico = DictionaryLearning(n_components, random_state=0).fit(X)","47","    assert_equal(dico.components_.shape, (n_components, n_features))","48","    assert_equal(dico.transform(X).shape, (X.shape[0], n_components))"],"delete":["30","    assert_true(dico.components_.shape == (n_components, n_features))"]}],"sklearn\/feature_extraction\/text.py":[{"add":["1037","            self._idf_diag = sp.spdiags(idf, diags=0, m=n_features,","1089","        # if _idf_diag is not set, this will raise an attribute error,","1090","        # which means hasattr(self, \"idf_\") is False","1091","        return np.ravel(self._idf_diag.sum(axis=0))"],"delete":["31","from ..utils import deprecated","1038","            self._idf_diag = sp.spdiags(idf, diags=0, m=n_features, ","1090","        if hasattr(self, \"_idf_diag\"):","1091","            return np.ravel(self._idf_diag.sum(axis=0))","1092","        else:","1093","            return None"]}],"sklearn\/naive_bayes.py":[{"add":["486","        classes : array-like, shape = [n_classes] (default=None)","492","        sample_weight : array-like, shape = [n_samples] (default=None)","557","        sample_weight : array-like, shape = [n_samples], (default=None)"],"delete":["486","        classes : array-like, shape = [n_classes], optional (default=None)","492","        sample_weight : array-like, shape = [n_samples], optional (default=None)","557","        sample_weight : array-like, shape = [n_samples], optional (default=None)"]}],"sklearn\/feature_selection\/rfe.py":[{"add":["32","","296","        :class:`sklearn.model_selection.StratifiedKFold` is used. If the","297","        estimator is a classifier or if ``y`` is neither binary nor multiclass,"],"delete":["295","        :class:`sklearn.model_selection.StratifiedKFold` is used. If the ","296","        estimator is a classifier or if ``y`` is neither binary nor multiclass, "]}],"sklearn\/tests\/test_multiclass.py":[{"add":["15","from sklearn.utils.multiclass import (check_classification_targets,","16","                                      type_of_target)","107","    # test partial_fit only exists if estimator has it:","108","    ovr = OneVsRestClassifier(SVC())","109","    assert_false(hasattr(ovr, \"partial_fit\"))","110","","435","    for base_classifier in [SVC(kernel='linear', random_state=0),","436","                            LinearSVC(random_state=0)]:","447","            assert_equal(sp.issparse(ovr.estimators_[0].coef_),","448","                         sp.issparse(ovr.coef_))","517","    # test partial_fit only exists if estimator has it:","518","    ovr = OneVsOneClassifier(SVC())","519","    assert_false(hasattr(ovr, \"partial_fit\"))","520","","619","def test_ovo_one_class():","620","    # Test error for OvO with one class","621","    X = np.eye(4)","622","    y = np.array(['a'] * 4)","623","","624","    ovo = OneVsOneClassifier(LinearSVC())","625","    assert_raise_message(ValueError, \"when only one class\", ovo.fit, X, y)","626","","627","","628","def test_ovo_float_y():","629","    # Test that the OvO errors on float targets","630","    X = iris.data","631","    y = iris.data[:, 0]","632","","633","    ovo = OneVsOneClassifier(LinearSVC())","634","    assert_raise_message(ValueError, \"Unknown label type\", ovo.fit, X, y)","635","","636","","665","def test_ecoc_float_y():","666","    # Test that the OCC errors on float targets","667","    X = iris.data","668","    y = iris.data[:, 0]","669","","670","    ovo = OutputCodeClassifier(LinearSVC())","671","    assert_raise_message(ValueError, \"Unknown label type\", ovo.fit, X, y)","672","","673",""],"delete":["15","from sklearn.utils.multiclass import check_classification_targets, type_of_target","430","    for base_classifier in [SVC(kernel='linear', random_state=0), LinearSVC(random_state=0)]:","441","            assert_equal(sp.issparse(ovr.estimators_[0].coef_), sp.issparse(ovr.coef_))"]}],"sklearn\/cluster\/tests\/test_k_means.py":[{"add":["406","    # should no longer be good"],"delete":["406","    # should not longer be good"]}],"sklearn\/utils\/multiclass.py":[{"add":["25","","158","","172","                      'multilabel-indicator', 'multilabel-sequences']:"],"delete":["170","            'multilabel-indicator', 'multilabel-sequences']:","174",""]}],"sklearn\/covariance\/outlier_detection.py":[{"add":["17","from ..utils.validation import check_is_fitted, check_array","18","from ..metrics import accuracy_score","65","        X = check_array(X)","93","        X = check_array(X)","103","    def score(self, X, y, sample_weight=None):","104","        \"\"\"Returns the mean accuracy on the given test data and labels.","106","        In multi-label classification, this is the subset accuracy","107","        which is a harsh metric since you require for each sample that","108","        each label set be correctly predicted.","109","","110","        Parameters","111","        ----------","112","        X : array-like, shape = (n_samples, n_features)","113","            Test samples.","114","","115","        y : array-like, shape = (n_samples,) or (n_samples, n_outputs)","116","            True labels for X.","117","","118","        sample_weight : array-like, shape = (n_samples,), optional","119","            Sample weights.","120","","121","        Returns","122","        -------","123","        score : float","124","            Mean accuracy of self.predict(X) wrt. y.","125","","126","        \"\"\"","127","        return accuracy_score(y, self.predict(X), sample_weight=sample_weight)","128","","129","","130","class EllipticEnvelope(OutlierDetectionMixin, MinCovDet):"],"delete":["17","from ..base import ClassifierMixin","18","from ..utils.validation import check_is_fitted","69","            check_is_fitted(self, 'threshold_')","103","class EllipticEnvelope(ClassifierMixin, OutlierDetectionMixin, MinCovDet):"]}],"sklearn\/decomposition\/truncated_svd.py":[{"add":["13","from ..utils import check_array, check_random_state","155","        X = check_array(X, accept_sparse=['csr', 'csc'])"],"delete":["13","from ..utils import check_array, as_float_array, check_random_state","155","        X = as_float_array(X, copy=False)","158","        # If sparse and not csr or csc, convert to csr","159","        if sp.issparse(X) and X.getformat() not in [\"csr\", \"csc\"]:","160","            X = X.tocsr()","161",""]}],"sklearn\/tests\/test_multioutput.py":[{"add":["8","from sklearn.utils.testing import assert_raise_message","339","    # ValueError when y is continuous","340","    assert_raise_message(ValueError, \"Unknown label type\", moc.fit, X, X[:, 1])"],"delete":[]}],"sklearn\/feature_extraction\/tests\/test_text.py":[{"add":["422","    assert_false(hasattr(t2, \"idf_\"))"],"delete":["422","    assert_equal(t2.idf_, None)"]}],"sklearn\/feature_selection\/tests\/test_from_model.py":[{"add":["3","from sklearn.utils.testing import assert_false","123","    # check that if est doesn't have partial_fit, neither does SelectFromModel","124","    transformer = SelectFromModel(estimator=RandomForestClassifier())","125","    assert_false(hasattr(transformer, \"partial_fit\"))","126","","178","    model = SelectFromModel(clf, threshold=\"0.1 * mean\")","183","    model.threshold = \"1.0 * mean\""],"delete":["173","    model = SelectFromModel(clf, threshold=0.1)","178","    model.threshold = 1.0"]}],"sklearn\/neighbors\/approximate.py":[{"add":["95","                 n_components=32,"],"delete":["95","                 n_components=8,"]}],"sklearn\/decomposition\/dict_learning.py":[{"add":["96","    n_components = dictionary.shape[0]","97","    if dictionary.shape[1] != X.shape[1]:","98","        raise ValueError(\"Dictionary and X have different numbers of features:\"","99","                         \"dictionary.shape: {} X.shape{}\".format(","100","                             dictionary.shape, X.shape))","164","    if new_code.ndim != 2:","165","        return new_code.reshape(n_samples, n_components)","736","                print(\"Iteration % 3i (elapsed time: % 3is, % 4.1fmn)\"","737","                      % (ii, dt, dt \/ 60))","910","    _required_parameters = [\"dictionary\"]"],"delete":["283","        # This ensure that dimensionality of code is always 2,","284","        # consistant with the case n_jobs > 1","285","        if code.ndim == 1:","286","            code = code[np.newaxis, :]","733","                print (\"Iteration % 3i (elapsed time: % 3is, % 4.1fmn)\"","734","                       % (ii, dt, dt \/ 60))","822","        # XXX : kwargs is not documented"]}],"sklearn\/multioutput.py":[{"add":["18","from abc import ABCMeta, abstractmethod","19","from .base import BaseEstimator, clone, MetaEstimatorMixin","25","from .utils.multiclass import check_classification_targets","60","class MultiOutputEstimator(six.with_metaclass(ABCMeta, BaseEstimator,","61","                                              MetaEstimatorMixin)):","62","    @abstractmethod","153","        if isinstance(self, ClassifierMixin):","154","            check_classification_targets(y)","155",""],"delete":["18","from abc import ABCMeta","19","from .base import BaseEstimator, clone","59","class MultiOutputEstimator(six.with_metaclass(ABCMeta, BaseEstimator)):","60",""]}],"sklearn\/ensemble\/gradient_boosting.py":[{"add":["66","class QuantileEstimator(object):","88","class MeanEstimator(object):","104","class LogOddsEstimator(object):","134","class PriorProbabilityEstimator(object):","152","class ZeroEstimator(object):"],"delete":["66","class QuantileEstimator(BaseEstimator):","88","class MeanEstimator(BaseEstimator):","104","class LogOddsEstimator(BaseEstimator):","134","class PriorProbabilityEstimator(BaseEstimator):","152","class ZeroEstimator(BaseEstimator):"]}],"doc\/whats_new.rst":[{"add":["215","     in R (lars library). :issue:`7849` by :user:`Jair Montoya Martinez <jmontoyam>`.","216","","226","   - Fix a bug regarding fitting :class:`sklearn.cluster.KMeans` with a sparse","227","     array X and initial centroids, where X's means were unnecessarily being","228","     subtracted from the centroids. :issue:`7872` by :user:`Josh Karnofsky <jkarno>`.","252","   - Fixes to the input validation in","253","     :class:`sklearn.covariance.EllipticEnvelope`.","254","     :issue:`8086` by `Andreas Mller`_.","255","","256","   - Fix output shape and bugs with n_jobs > 1 in  ","257","     :class:`sklearn.decomposition.SparseCoder` transform and :func:`sklarn.decomposition.sparse_encode`","258","     for one-dimensional data and one component.","259","     This also impacts the output shape of :class:`sklearn.decomposition.DictionaryLearning`.","260","     :issue:`8086` by `Andreas Mller`_.","261","","262","   - Several fixes to input validation in","263","     :class:`multiclass.OutputCodeClassifier`","264","     :issue:`8086` by `Andreas Mller`_.","265","","353","","354","   - Gradient boosting base models are no longer estimators. By `Andreas Mller`_.","355","","356","   - :class:`feature_selection.SelectFromModel` now validates the ``threshold``","357","     parameter and sets the ``threshold_`` attribute during the call to","358","     ``fit``, and no longer during the call to ``transform```, by `Andreas","359","     Mller`_.","360","","361","   - :class:`feature_selection.SelectFromModel` now has a ``partial_fit``","362","     method only if the underlying estimator does. By `Andreas Mller`_.","363","","364","   - :class:`multiclass.OneVsRestClassifier` now has a ``partial_fit`` method","365","     only if the underlying estimator does.  By `Andreas Mller`_. ","366",""],"delete":["215","     in R (lars library). :issue:`7849` by :user:`Jair Montoya Martinez <jmontoyam>`","225","   - Fix a bug regarding fitting :class:`sklearn.cluster.KMeans` with a","226","     sparse array X and initial centroids, where X's means were unnecessarily","227","     being subtracted from the centroids. :issue:`7872` by `Josh Karnofsky <https:\/\/github.com\/jkarno>`_."]}],"sklearn\/dummy.py":[{"add":["122","        check_consistent_length(X, y)","123","","188","        if self.n_outputs_ == 1 and not self.output_2d_:","197","            if self.n_outputs_ == 1 and not self.output_2d_:","403",""],"delete":["186","        if self.n_outputs_ == 1:","195","            if self.n_outputs_ == 1:"]}],"sklearn\/feature_selection\/from_model.py":[{"add":["6","from ..base import BaseEstimator, clone, MetaEstimatorMixin","11","from ..utils.metaestimators import if_delegate_has_method","79","class SelectFromModel(BaseEstimator, SelectorMixin, MetaEstimatorMixin):","138","                'Either fit SelectFromModel before transform or set \"prefit='","139","                'True\" and pass a fitted estimator to the constructor.')","141","        threshold = _calculate_threshold(estimator, scores, self.threshold)","142","        return scores >= threshold","170","    @property","171","    def threshold_(self):","172","        scores = _get_feature_importances(self.estimator_, self.norm_order)","173","        return _calculate_threshold(self.estimator, scores, self.threshold)","174","","175","    @if_delegate_has_method('estimator')"],"delete":["6","from ..base import BaseEstimator, clone","78","class SelectFromModel(BaseEstimator, SelectorMixin):","123","","138","                'Either fit the model before transform or set \"prefit=True\"'","139","                ' while passing the fitted estimator to the constructor.')","141","        self.threshold_ = _calculate_threshold(estimator, scores,","142","                                               self.threshold)","143","        return scores >= self.threshold_"]}],"sklearn\/multiclass.py":[{"add":["48","from .utils.validation import check_X_y, check_array","218","    @if_delegate_has_method('estimator')","490","        check_classification_targets(y)","493","        if len(self.classes_) == 1:","494","            raise ValueError(\"OneVsOneClassifier can not be fit when only one\"","495","                             \" class is present.\")","504","            self.pairwise_indices_ = (","505","                estimators_indices[1] if self._pairwise else None)","511","    @if_delegate_has_method(delegate='estimator')","551","                for estimator, (i, j) in izip(self.estimators_,","552","                                              (combinations)))","710","        X, y = check_X_y(X, y)","717","        check_classification_targets(y)","758","        X = check_array(X)"],"delete":["48","from .utils.validation import check_X_y","178","","500","            self.pairwise_indices_ = estimators_indices[1] \\","501","                                     if self._pairwise else None","546","                for estimator, (i, j) in izip(","547","                        self.estimators_, (combinations)))"]}],"doc\/modules\/model_evaluation.rst":[{"add":["175","    >>> ground_truth = [[1], [1]]"],"delete":["175","    >>> ground_truth = [[1, 1]]"]}],"sklearn\/ensemble\/base.py":[{"add":["14","from ..externals import six","15","from abc import ABCMeta, abstractmethod","59","class BaseEnsemble(six.with_metaclass(ABCMeta, BaseEstimator,","60","                                      MetaEstimatorMixin)):","87","    @abstractmethod"],"delete":["57","class BaseEnsemble(BaseEstimator, MetaEstimatorMixin):"]}]}},"1fa3bf775f4ac8a5575022607e5b9b5d752a0db0":{"changes":{"doc\/themes\/scikit-learn\/static\/nature.css_t":"MODIFY"},"diff":{"doc\/themes\/scikit-learn\/static\/nature.css_t":[{"add":["42","    \/* Don't leave a fixed height. It won't allow the header to expand when search results appear. *\/","107","    div.header {","108","      height: 60px;","109","    }"],"delete":["42","    \/* for the logo to correctly expand when showing results","43","       but remain cut when not *\/","44","       height: 60px;"]}]}},"73d3f03cfc83aa35edb89173a8450c4059000fce":{"changes":{"sklearn\/model_selection\/_split.py":"MODIFY","sklearn\/model_selection\/tests\/test_split.py":"MODIFY"},"diff":{"sklearn\/model_selection\/_split.py":[{"add":["775","        if len(unique_groups) <= 1:","776","            raise ValueError(","777","                \"The groups parameter contains fewer than 2 unique groups \"","778","                \"(%s). LeaveOneGroupOut expects at least 2.\" % unique_groups)","868","        if self.n_groups >= len(unique_groups):","869","            raise ValueError(","870","                \"The groups parameter contains fewer than (or equal to) \"","871","                \"n_groups (%d) numbers of unique groups (%s). LeavePGroupsOut \"","872","                \"expects that at least n_groups + 1 (%d) unique groups be \"","873","                \"present\" % (self.n_groups, unique_groups, self.n_groups + 1))"],"delete":[]}],"sklearn\/model_selection\/tests\/test_split.py":[{"add":["726","def test_leave_one_p_group_out_error_on_fewer_number_of_groups():","727","    X = y = groups = np.ones(0)","728","    msg = (\"The groups parameter contains fewer than 2 unique groups ([]). \"","729","           \"LeaveOneGroupOut expects at least 2.\")","730","    assert_raise_message(ValueError, msg, next,","731","                         LeaveOneGroupOut().split(X, y, groups))","732","    X = y = groups = np.ones(1)","733","    msg = (\"The groups parameter contains fewer than 2 unique groups ([ 1.]). \"","734","           \"LeaveOneGroupOut expects at least 2.\")","735","    assert_raise_message(ValueError, msg, next,","736","                         LeaveOneGroupOut().split(X, y, groups))","737","    X = y = groups = np.ones(1)","738","    msg = (\"The groups parameter contains fewer than (or equal to) n_groups \"","739","           \"(3) numbers of unique groups ([ 1.]). LeavePGroupsOut expects \"","740","           \"that at least n_groups + 1 (4) unique groups be present\")","741","    assert_raise_message(ValueError, msg, next,","742","                         LeavePGroupsOut(n_groups=3).split(X, y, groups))","743","    X = y = groups = np.arange(3)","744","    msg = (\"The groups parameter contains fewer than (or equal to) n_groups \"","745","           \"(3) numbers of unique groups ([0 1 2]). LeavePGroupsOut expects \"","746","           \"that at least n_groups + 1 (4) unique groups be present\")","747","    assert_raise_message(ValueError, msg, next,","748","                         LeavePGroupsOut(n_groups=3).split(X, y, groups))","749","","750",""],"delete":[]}]}},"75d6005feacfedad33df8bed31e34f7bec51f62d":{"changes":{"sklearn\/model_selection\/_split.py":"MODIFY","sklearn\/model_selection\/tests\/test_search.py":"MODIFY","sklearn\/model_selection\/tests\/test_split.py":"MODIFY","doc\/modules\/cross_validation.rst":"MODIFY","sklearn\/model_selection\/_search.py":"MODIFY"},"diff":{"sklearn\/model_selection\/_split.py":[{"add":["85","","86","        Note","87","        ----","88","        Randomized CV splitters may return different results for each call of","89","        split. You can make the results identical by setting ``random_state``","90","        to an integer.","316","","317","        Note","318","        ----","319","        Randomized CV splitters may return different results for each call of","320","        split. You can make the results identical by setting ``random_state``","321","        to an integer.","581","        rng = self.random_state","656","","657","        Note","658","        ----","659","        Randomized CV splitters may return different results for each call of","660","        split. You can make the results identical by setting ``random_state``","661","        to an integer.","743","","744","        Note","745","        ----","746","        Randomized CV splitters may return different results for each call of","747","        split. You can make the results identical by setting ``random_state``","748","        to an integer.","1187","","1188","        Note","1189","        ----","1190","        Randomized CV splitters may return different results for each call of","1191","        split. You can make the results identical by setting ``random_state``","1192","        to an integer.","1607","","1608","        Note","1609","        ----","1610","        Randomized CV splitters may return different results for each call of","1611","        split. You can make the results identical by setting ``random_state``","1612","        to an integer."],"delete":["569","        if self.shuffle:","570","            rng = check_random_state(self.random_state)","571","        else:","572","            rng = self.random_state"]}],"sklearn\/model_selection\/tests\/test_search.py":[{"add":["9","from types import GeneratorType","1073","        assert_false(np.allclose(cv_results['mean_test_score'][1],","1074","                                 cv_results['mean_test_score'][2]))","1075","        assert_false(np.allclose(cv_results['mean_train_score'][1],","1076","                                 cv_results['mean_train_score'][2]))","1418","    # Give generator as a cv parameter","1419","    assert_true(isinstance(KFold(n_splits=n_splits,","1420","                                 shuffle=True, random_state=0).split(X, y),","1421","                           GeneratorType))","1422","    gs3 = GridSearchCV(LinearSVC(random_state=0),","1423","                       param_grid={'C': [0.1, 0.2, 0.3]},","1424","                       cv=KFold(n_splits=n_splits, shuffle=True,","1425","                                random_state=0).split(X, y))","1426","    gs3.fit(X, y)","1427","","1428","    gs4 = GridSearchCV(LinearSVC(random_state=0),","1429","                       param_grid={'C': [0.1, 0.2, 0.3]},","1430","                       cv=KFold(n_splits=n_splits, shuffle=True,","1431","                                random_state=0))","1432","    gs4.fit(X, y)","1433","","1434","    def _pop_time_keys(cv_results):","1435","        for key in ('mean_fit_time', 'std_fit_time',","1436","                    'mean_score_time', 'std_score_time'):","1437","            cv_results.pop(key)","1438","        return cv_results","1439","","1440","    # Check if generators are supported as cv and","1441","    # that the splits are consistent","1442","    np.testing.assert_equal(_pop_time_keys(gs3.cv_results_),","1443","                            _pop_time_keys(gs4.cv_results_))","1444",""],"delete":["1072","        try:","1073","            assert_almost_equal(cv_results['mean_test_score'][1],","1074","                                cv_results['mean_test_score'][2])","1075","        except AssertionError:","1076","            pass","1077","        try:","1078","            assert_almost_equal(cv_results['mean_train_score'][1],","1079","                                cv_results['mean_train_score'][2])","1080","        except AssertionError:","1081","            pass"]}],"sklearn\/model_selection\/tests\/test_split.py":[{"add":["448","            # Test if the two splits are different","449","            # numpy's assert_equal properly compares nested lists","451","                np.testing.assert_array_equal(list(cv.split(*data)),","452","                                              list(cv.split(*data)))","1192","    # numpy's assert_array_equal properly compares nested lists"],"delete":["449","                np.testing.assert_equal(list(cv.split(*data)),","450","                                        list(cv.split(*data)))"]}],"doc\/modules\/cross_validation.rst":[{"add":["727","* To get identical results for each split, set ``random_state`` to an integer."],"delete":["727","* To ensure results are repeatable (*on the same platform*), use a fixed value","728","  for ``random_state``."]}],"sklearn\/model_selection\/_search.py":[{"add":["926","        |param_kernel|param_gamma|param_degree|split0_test_score|...|rank_t...|"],"delete":["926","        |param_kernel|param_gamma|param_degree|split0_test_score|...|..rank...|"]}]}},"62c86f0388fdeccc20d535178957121611ba8213":{"changes":{"sklearn\/feature_extraction\/text.py":"MODIFY","sklearn\/feature_extraction\/tests\/test_text.py":"MODIFY"},"diff":{"sklearn\/feature_extraction\/text.py":[{"add":["454","        if isinstance(X, six.string_types):","455","            raise ValueError(","456","                \"Iterable over raw text documents expected, \"","457","                \"string object received.\")","458","","480","        if isinstance(X, six.string_types):","481","            raise ValueError(","482","                \"Iterable over raw text documents expected, \"","483","                \"string object received.\")","484","","827","        if isinstance(raw_documents, six.string_types):","828","            raise ValueError(","829","                \"Iterable over raw text documents expected, \"","830","                \"string object received.\")","831","","881","        if isinstance(raw_documents, six.string_types):","882","            raise ValueError(","883","                \"Iterable over raw text documents expected, \"","884","                \"string object received.\")","885",""],"delete":[]}],"sklearn\/feature_extraction\/tests\/test_text.py":[{"add":["870","                               random_state=rng))","969","","970","","971","def test_vectorizer_string_object_as_input():","972","    message = (\"Iterable over raw text documents expected, \"","973","               \"string object received.\")","974","    for vec in [CountVectorizer(), TfidfVectorizer(), HashingVectorizer()]:","975","        assert_raise_message(","976","            ValueError, message, vec.fit_transform, \"hello world!\")","977","        assert_raise_message(","978","            ValueError, message, vec.fit, \"hello world!\")","979","        assert_raise_message(","980","            ValueError, message, vec.transform, \"hello world!\")"],"delete":["870","                        random_state=rng))"]}]}},"2caa1445fb360ecb6c590b8b1c056bb084b4337a":{"changes":{"doc\/whats_new.rst":"MODIFY","sklearn\/feature_selection\/univariate_selection.py":"MODIFY","sklearn\/feature_selection\/tests\/test_feature_select.py":"MODIFY"},"diff":{"doc\/whats_new.rst":[{"add":["52","   - Fix a bug where :class:`sklearn.feature_selection.SelectFdr` did not ","53","     exactly implement Benjamini-Hochberg procedure. It formerly may have","54","     selected fewer features than it should.","55","     (`#7490 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/7490>`_) by","56","     `Peng Meng`_.","57","","4881","","4882",".. _Peng Meng: https:\/\/github.com\/mpjlu"],"delete":[]}],"sklearn\/feature_selection\/univariate_selection.py":[{"add":["598","        selected = sv[sv <= float(self.alpha) \/ n_features *","599","                      np.arange(1, n_features + 1)]"],"delete":["598","        selected = sv[sv <= float(self.alpha) \/ n_features","599","                      * np.arange(n_features)]"]}],"sklearn\/feature_selection\/tests\/test_feature_select.py":[{"add":["373","def test_boundary_case_ch2():","374","    # Test boundary case, and always aim to select 1 feature.","375","    X = np.array([[10, 20], [20, 20], [20, 30]])","376","    y = np.array([[1], [0], [0]])","377","    scores, pvalues = chi2(X, y)","378","    assert_array_almost_equal(scores, np.array([4., 0.71428571]))","379","    assert_array_almost_equal(pvalues, np.array([0.04550026, 0.39802472]))","380","","381","    filter_fdr = SelectFdr(chi2, alpha=0.1)","382","    filter_fdr.fit(X, y)","383","    support_fdr = filter_fdr.get_support()","384","    assert_array_equal(support_fdr, np.array([True, False]))","385","","386","    filter_kbest = SelectKBest(chi2, k=1)","387","    filter_kbest.fit(X, y)","388","    support_kbest = filter_kbest.get_support()","389","    assert_array_equal(support_kbest, np.array([True, False]))","390","","391","    filter_percentile = SelectPercentile(chi2, percentile=50)","392","    filter_percentile.fit(X, y)","393","    support_percentile = filter_percentile.get_support()","394","    assert_array_equal(support_percentile, np.array([True, False]))","395","","396","    filter_fpr = SelectFpr(chi2, alpha=0.1)","397","    filter_fpr.fit(X, y)","398","    support_fpr = filter_fpr.get_support()","399","    assert_array_equal(support_fpr, np.array([True, False]))","400","","401","    filter_fwe = SelectFwe(chi2, alpha=0.1)","402","    filter_fwe.fit(X, y)","403","    support_fwe = filter_fwe.get_support()","404","    assert_array_equal(support_fwe, np.array([True, False]))","405","","406","","440","                                            random_state in range(100)])"],"delete":["406","                                            random_state in range(30)])"]}]}},"3619bd386b66b5131c5b78e21e6775982d2b984d":{"changes":{"sklearn\/model_selection\/tests\/test_search.py":"MODIFY","sklearn\/model_selection\/_search.py":"MODIFY"},"diff":{"sklearn\/model_selection\/tests\/test_search.py":[{"add":["1142","","1143","","1144","def test_search_train_scores_set_to_false():","1145","    X = np.arange(6).reshape(6, -1)","1146","    y = [0, 0, 0, 1, 1, 1]","1147","    clf = LinearSVC(random_state=0)","1148","","1149","    gs = GridSearchCV(clf, param_grid={'C': [0.1, 0.2]},","1150","                      return_train_score=False)","1151","    gs.fit(X, y)"],"delete":[]}],"sklearn\/model_selection\/_search.py":[{"add":["605","        if self.return_train_score:","606","            _store('train_score', train_scores, splits=True)"],"delete":["605","        _store('train_score', train_scores, splits=True)"]}]}},"0ea8e8b585d2aa56ad3a6f6d2c75047a11ba709a":{"changes":{"sklearn\/metrics\/pairwise.py":"MODIFY","sklearn\/metrics\/tests\/test_pairwise.py":"MODIFY"},"diff":{"sklearn\/metrics\/pairwise.py":[{"add":["572","    np.clip(S, 0, 2, out=S)","573","    if X is Y or Y is None:","574","        # Ensure that distances between vectors and themselves are set to 0.0.","575","        # This may not be the case due to floating point rounding errors.","576","        S[np.diag_indices_from(S)] = 0.0"],"delete":[]}],"sklearn\/metrics\/tests\/test_pairwise.py":[{"add":["409","def test_cosine_distances():","410","    # Check the pairwise Cosine distances computation","411","    rng = np.random.RandomState(1337)","412","    x = np.abs(rng.rand(910))","413","    XA = np.vstack([x, x])","414","    D = cosine_distances(XA)","415","    assert_array_almost_equal(D, [[0., 0.], [0., 0.]])","416","    # check that all elements are in [0, 2]","417","    assert_true(np.all(D >= 0.))","418","    assert_true(np.all(D <= 2.))","419","    # check that diagonal elements are equal to 0","420","    assert_array_equal(D[np.diag_indices_from(D)], [0., 0.])","421","","422","    XB = np.vstack([x, -x])","423","    D2 = cosine_distances(XB)","424","    # check that all elements are in [0, 2]","425","    assert_true(np.all(D2 >= 0.))","426","    assert_true(np.all(D2 <= 2.))","427","    # check that diagonal elements are equal to 0 and non diagonal to 2","428","    assert_array_equal(D2, [[0., 2.], [2., 0.]])","429","","430","    # check large random matrix","431","    X = np.abs(rng.rand(1000, 5000))","432","    D = cosine_distances(X)","433","    # check that diagonal elements are equal to 0","434","    assert_array_almost_equal(D[np.diag_indices_from(D)], [0.] * D.shape[0])","435","    assert_true(np.all(D >= 0.))","436","    assert_true(np.all(D <= 2.))","437","","438",""],"delete":[]}]}},"33ed90dc0aa0549a5963000d7d070aa18ca389c4":{"changes":{"sklearn\/tree\/tree.py":"MODIFY","sklearn\/tree\/tests\/test_tree.py":"MODIFY"},"diff":{"sklearn\/tree\/tree.py":[{"add":["218","            if not 1 <= self.min_samples_leaf:","219","                raise ValueError(\"min_samples_leaf must be at least 1 \"","220","                                 \"or in (0, 0.5], got %s\"","221","                                 % self.min_samples_leaf)","224","            if not 0. < self.min_samples_leaf <= 0.5:","225","                raise ValueError(\"min_samples_leaf must be at least 1 \"","226","                                 \"or in (0, 0.5], got %s\"","227","                                 % self.min_samples_leaf)","231","            if not 2 <= self.min_samples_split:","232","                raise ValueError(\"min_samples_split must be at least 2 \"","233","                                 \"or in (0, 1], got %s\"","234","                                 % self.min_samples_split)","237","            if not 0. < self.min_samples_split <= 1.:","238","                raise ValueError(\"min_samples_split must be at least 2 \"","239","                                 \"or in (0, 1], got %s\"","240","                                 % self.min_samples_split)","318","            raise ValueError(\"min_impurity_split must be greater than \"","319","                             \"or equal to 0\")","382","                                           max_leaf_nodes,","383","                                           self.min_impurity_split)"],"delete":["260","        if not (0. < self.min_samples_split <= 1. or","261","                2 <= self.min_samples_split):","262","            raise ValueError(\"min_samples_split must be in at least 2\"","263","                             \" or in (0, 1], got %s\" % min_samples_split)","264","        if not (0. < self.min_samples_leaf <= 0.5 or","265","                1 <= self.min_samples_leaf):","266","            raise ValueError(\"min_samples_leaf must be at least than 1 \"","267","                             \"or in (0, 0.5], got %s\" % min_samples_leaf)","268","","311","            raise ValueError(\"min_impurity_split must be greater than or equal \"","312","                             \"to 0\")","375","                                           max_leaf_nodes, self.min_impurity_split)"]}],"sklearn\/tree\/tests\/test_tree.py":[{"add":["511","        assert_raises(ValueError, TreeEstimator(min_samples_leaf=3.).fit, X, y)","524","        assert_raises(ValueError, TreeEstimator(min_samples_split=2.5).fit,","525","                      X, y)"],"delete":[]}]}},"c336a4390ef93e65f4e6641b954524d9e412947f":{"changes":{"sklearn\/feature_extraction\/_hashing.pyx":"MODIFY"},"diff":{"sklearn\/feature_extraction\/_hashing.pyx":[{"add":["45","            if isinstance(v, (str, unicode)):","55","                f = (<unicode>f).encode(\"utf-8\")","61","            h = murmurhash3_bytes_s32(<bytes>f, 0)"],"delete":["10","from ..externals.six import string_types","11","","47","            if isinstance(v, string_types):","57","                f = f.encode(\"utf-8\")","63","            h = murmurhash3_bytes_s32(f, 0)"]}]}},"31d3c3e7c511145b65ce0d1ad6643646acadaa0a":{"changes":{"doc\/modules\/preprocessing.rst":"MODIFY"},"diff":{"doc\/modules\/preprocessing.rst":[{"add":["148","    X_scaled = X_std * (max - min) + min"],"delete":["148","    X_scaled = X_std \/ (max - min) + min"]}]}},"3c1873550317bbbc63e982cccbd0afaae9cc5a66":{"changes":{"examples\/linear_model\/plot_logistic.py":"MODIFY"},"diff":{"examples\/linear_model\/plot_logistic.py":[{"add":["6","Logistic function","9","Shown in the plot is how the logistic regression would, in this","11","i.e. class one or two, using the logistic curve.","50","plt.plot(X_test, loss, color='red', linewidth=3)","59","plt.xticks(range(-5, 10))","60","plt.yticks([0, 0.5, 1])","63","plt.legend(('Logistic Regression Model', 'Linear Regression Model'),","64","           loc=\"lower right\", fontsize='small')"],"delete":["6","Logit function","9","Show in the plot is how the logistic regression would, in this","11","i.e. class one or two, using the logit-curve.","50","plt.plot(X_test, loss, color='blue', linewidth=3)","59","plt.xticks(())","60","plt.yticks(())","63",""]}]}},"fd84a567b42eb46b6f8a78b572a3d48142f25e2c":{"changes":{"sklearn\/model_selection\/_validation.py":"MODIFY","sklearn\/model_selection\/tests\/test_validation.py":"MODIFY","doc\/whats_new.rst":"MODIFY"},"diff":{"sklearn\/model_selection\/_validation.py":[{"add":["30","from ..preprocessing import LabelEncoder","367","        Invokes the passed method name of the passed estimator. For","368","        method='predict_proba', the columns correspond to the classes","369","        in sorted order.","395","    if method in ['decision_function', 'predict_proba', 'predict_log_proba']:","396","        le = LabelEncoder()","397","        y = le.fit_transform(y)","398","","481","    if method in ['decision_function', 'predict_proba', 'predict_log_proba']:","482","        n_classes = len(set(y))","483","        predictions_ = np.zeros((X_test.shape[0], n_classes))","484","        if method == 'decision_function' and len(estimator.classes_) == 2:","485","            predictions_[:, estimator.classes_[-1]] = predictions","486","        else:","487","            predictions_[:, estimator.classes_] = predictions","488","        predictions = predictions_"],"delete":["366","        Invokes the passed method name of the passed estimator."]}],"sklearn\/model_selection\/tests\/test_validation.py":[{"add":["53","from sklearn.preprocessing import LabelEncoder","943","        # Test alternative representations of y","944","        predictions_y1 = cross_val_predict(est, X, y + 1, method=method,","945","                                           cv=kfold)","946","        assert_array_equal(predictions, predictions_y1)","947","","948","        predictions_y2 = cross_val_predict(est, X, y - 2, method=method,","949","                                           cv=kfold)","950","        assert_array_equal(predictions, predictions_y2)","951","","952","        predictions_ystr = cross_val_predict(est, X, y.astype('str'),","953","                                             method=method, cv=kfold)","954","        assert_array_equal(predictions, predictions_ystr)","955","","956","","957","def get_expected_predictions(X, y, cv, classes, est, method):","958","","959","    expected_predictions = np.zeros([len(y), classes])","960","    func = getattr(est, method)","961","","962","    for train, test in cv.split(X, y):","963","        est.fit(X[train], y[train])","964","        expected_predictions_ = func(X[test])","965","        # To avoid 2 dimensional indexing","966","        exp_pred_test = np.zeros((len(test), classes))","967","        if method is 'decision_function' and len(est.classes_) == 2:","968","            exp_pred_test[:, est.classes_[-1]] = expected_predictions_","969","        else:","970","            exp_pred_test[:, est.classes_] = expected_predictions_","971","        expected_predictions[test] = exp_pred_test","972","","973","    return expected_predictions","974","","975","","976","def test_cross_val_predict_class_subset():","977","","978","    X = np.arange(8).reshape(4, 2)","979","    y = np.array([0, 0, 1, 2])","980","    classes = 3","981","","982","    kfold3 = KFold(n_splits=3)","983","    kfold4 = KFold(n_splits=4)","984","","985","    le = LabelEncoder()","986","","987","    methods = ['decision_function', 'predict_proba', 'predict_log_proba']","988","    for method in methods:","989","        est = LogisticRegression()","990","","991","        # Test with n_splits=3","992","        predictions = cross_val_predict(est, X, y, method=method,","993","                                        cv=kfold3)","994","","995","        # Runs a naive loop (should be same as cross_val_predict):","996","        expected_predictions = get_expected_predictions(X, y, kfold3, classes,","997","                                                        est, method)","998","        assert_array_almost_equal(expected_predictions, predictions)","999","","1000","        # Test with n_splits=4","1001","        predictions = cross_val_predict(est, X, y, method=method,","1002","                                        cv=kfold4)","1003","        expected_predictions = get_expected_predictions(X, y, kfold4, classes,","1004","                                                        est, method)","1005","        assert_array_almost_equal(expected_predictions, predictions)","1006","","1007","        # Testing unordered labels","1008","        y = [1, 1, -4, 6]","1009","        predictions = cross_val_predict(est, X, y, method=method,","1010","                                        cv=kfold3)","1011","        y = le.fit_transform(y)","1012","        expected_predictions = get_expected_predictions(X, y, kfold3, classes,","1013","                                                        est, method)","1014","        assert_array_almost_equal(expected_predictions, predictions)","1015",""],"delete":[]}],"doc\/whats_new.rst":[{"add":["112","   - :func:`model_selection.cross_val_predict` now returns output of the","113","     correct shape for all values of the argument ``method``.","114","     :issue:`7863` by :user:`Aman Dalmia <dalmia>`.","115",""],"delete":[]}]}},"34968d4c5dee53983c66dc44bb1d7eb173aa8a0d":{"changes":{"sklearn\/model_selection\/tests\/test_split.py":"MODIFY"},"diff":{"sklearn\/model_selection\/tests\/test_split.py":[{"add":["1030","    np.testing.assert_equal(list(kf_iter_wrapped.split(X, y)),","1031","                            list(kf_iter_wrapped.split(X, y)))","1036","    np.testing.assert_equal(list(kf_randomized_iter_wrapped.split(X, y)),","1037","                            list(kf_randomized_iter_wrapped.split(X, y)))","1038","","1039","    try:","1040","        np.testing.assert_equal(list(kf_iter_wrapped.split(X, y)),","1041","                                list(kf_randomized_iter_wrapped.split(X, y)))","1042","        splits_are_equal = True","1043","    except AssertionError:","1044","        splits_are_equal = False","1045","    assert_false(splits_are_equal, \"If the splits are randomized, \"","1046","                 \"successive calls to split should yield different results\")"],"delete":["1030","    assert_array_equal(list(kf_iter_wrapped.split(X, y)),","1031","                       list(kf_iter_wrapped.split(X, y)))","1036","    assert_array_equal(list(kf_randomized_iter_wrapped.split(X, y)),","1037","                       list(kf_randomized_iter_wrapped.split(X, y)))","1038","    assert_true(np.any(np.array(list(kf_iter_wrapped.split(X, y))) !=","1039","                       np.array(list(kf_randomized_iter_wrapped.split(X, y)))))"]}]}},"3162f980a901c5057b77dc94f97bd4787b9ff1ff":{"changes":{"sklearn\/ensemble\/gradient_boosting.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/ensemble\/tests\/test_gradient_boosting.py":"MODIFY"},"diff":{"sklearn\/ensemble\/gradient_boosting.py":[{"add":["767","                min_impurity_split=self.min_impurity_split,"],"delete":[]}],"doc\/whats_new.rst":[{"add":["120","   - Fix a bug where :class:`sklearn.ensemble.GradientBoostingClassifier` and","121","     :class:`sklearn.ensemble.GradientBoostingRegressor` ignored the","122","     ``min_impurity_split`` parameter.","123","     :issue:`8006` by :user:`Sebastian P?lsterl <sebp>`.","124",""],"delete":["129",""]}],"sklearn\/ensemble\/tests\/test_gradient_boosting.py":[{"add":["963","def test_min_impurity_split():","964","    # Test if min_impurity_split of base estimators is set","965","    # Regression test for #8006","966","    X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1)","967","    all_estimators = [GradientBoostingRegressor,","968","                      GradientBoostingClassifier]","969","","970","    for GBEstimator in all_estimators:","971","        est = GBEstimator(min_impurity_split=0.1).fit(X, y)","972","        for tree in est.estimators_.flat:","973","            assert_equal(tree.min_impurity_split, 0.1)","974","","975",""],"delete":[]}]}},"b15818e7e0d44da976e93e06eb96000b12cd059f":{"changes":{"doc\/whats_new.rst":"MODIFY","sklearn\/cross_decomposition\/pls_.py":"MODIFY","sklearn\/cross_decomposition\/tests\/test_pls.py":"MODIFY"},"diff":{"doc\/whats_new.rst":[{"add":["347","   - Fixed improper scaling in :class:`sklearn.cross_decomposition.PLSRegression`","348","     with ``scale=True``. :issue:`7819` by :user:`jayzed82 <jayzed82>`."],"delete":[]}],"sklearn\/cross_decomposition\/pls_.py":[{"add":["368","            self.coef_ = self.coef_ * self.y_std_"],"delete":["368","            self.coef_ = (1. \/ self.x_std_.reshape((p, 1)) * self.coef_ *","369","                          self.y_std_)"]}],"sklearn\/cross_decomposition\/tests\/test_pls.py":[{"add":["1","from numpy.testing import assert_approx_equal","2","","8","from sklearn.preprocessing import StandardScaler","356","","364","        assert_raise_message(ValueError, \"Invalid number of components\",","365","                             clf.fit, X, Y)","366","","367","","368","def test_pls_scaling():","369","    # sanity check for scale=True","370","    n_samples = 1000","371","    n_targets = 5","372","    n_features = 10","373","","374","    rng = np.random.RandomState(0)","375","","376","    Q = rng.randn(n_targets, n_features)","377","    Y = rng.randn(n_samples, n_targets)","378","    X = np.dot(Y, Q) + 2 * rng.randn(n_samples, n_features) + 1","379","    X *= 1000","380","    X_scaled = StandardScaler().fit_transform(X)","381","","382","    pls = pls_.PLSRegression(n_components=5, scale=True)","383","","384","    pls.fit(X, Y)","385","    score = pls.score(X, Y)","386","","387","    pls.fit(X_scaled, Y)","388","    score_scaled = pls.score(X_scaled, Y)","389","","390","    assert_approx_equal(score, score_scaled)"],"delete":["360","        assert_raise_message(ValueError, \"Invalid number of components\", clf.fit, X, Y)"]}]}},"53e6381272f0a71edcefb9010fa2e8856116076a":{"changes":{"sklearn\/ensemble\/tests\/test_base.py":"MODIFY","sklearn\/ensemble\/base.py":"MODIFY"},"diff":{"sklearn\/ensemble\/tests\/test_base.py":[{"add":["48","    np_int_ensemble = BaggingClassifier(base_estimator=Perceptron(),","49","                                        n_estimators=np.int32(3))","50","    np_int_ensemble.fit(iris.data, iris.target)","51","","56","    ensemble = BaggingClassifier(base_estimator=Perceptron(),","57","                                 n_estimators=0)","64","def test_base_not_int_n_estimators():","65","    # Check that instantiating a BaseEnsemble with a string as n_estimators","66","    # raises a ValueError demanding n_estimators to be supplied as an integer.","67","    string_ensemble = BaggingClassifier(base_estimator=Perceptron(),","68","                                        n_estimators='3')","69","    iris = load_iris()","70","    assert_raise_message(ValueError,","71","                         \"n_estimators must be an integer\",","72","                         string_ensemble.fit, iris.data, iris.target)","73","    float_ensemble = BaggingClassifier(base_estimator=Perceptron(),","74","                                       n_estimators=3.0)","75","    assert_raise_message(ValueError,","76","                         \"n_estimators must be an integer\",","77","                         float_ensemble.fit, iris.data, iris.target)","78","","79",""],"delete":["52","    ensemble = BaggingClassifier(base_estimator=Perceptron(), n_estimators=0)"]}],"sklearn\/ensemble\/base.py":[{"add":["8","import numbers","96","        if not isinstance(self.n_estimators, (numbers.Integral, np.integer)):","97","            raise ValueError(\"n_estimators must be an integer, \"","98","                             \"got {0}.\".format(type(self.n_estimators)))","99",""],"delete":[]}]}},"061803c4ff46fd5a34bb5220b572b8df4dcb2638":{"changes":{"doc\/whats_new.rst":"MODIFY","sklearn\/cluster\/k_means_.py":"MODIFY"},"diff":{"doc\/whats_new.rst":[{"add":["24","   - :class:`cluster.MiniBatchKMeans` and :class:`cluster.KMeans`","25","     now uses significantly less memory when assigning data points to their","26","     nearest cluster center.","27","     (`#7721 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/7721>`_)","28","     By `Jon Crall`_.","29",""],"delete":[]}],"sklearn\/cluster\/k_means_.py":[{"add":["20","from ..metrics.pairwise import pairwise_distances_argmin_min","555","","556","    # Breakup nearest neighbor distance computation into batches to prevent","557","    # memory blowup in the case of a large number of samples and clusters.","558","    # TODO: Once PR #7383 is merged use check_inputs=False in metric_kwargs.","559","    labels, mindist = pairwise_distances_argmin_min(","560","        X=X, Y=centers, metric='euclidean', metric_kwargs={'squared': True})","561","    # cython k-means code assumes int32 inputs","562","    labels = labels.astype(np.int32)"],"delete":["554","    k = centers.shape[0]","555","    all_distances = euclidean_distances(centers, X, x_squared_norms,","556","                                        squared=True)","557","    labels = np.empty(n_samples, dtype=np.int32)","558","    labels.fill(-1)","559","    mindist = np.empty(n_samples)","560","    mindist.fill(np.infty)","561","    for center_id in range(k):","562","        dist = all_distances[center_id]","563","        labels[dist < mindist] = center_id","564","        mindist = np.minimum(dist, mindist)"]}]}},"5c60f1f4e8f6b4df42350e14d22d4de7131c1104":{"changes":{"sklearn\/covariance\/outlier_detection.py":"MODIFY"},"diff":{"sklearn\/covariance\/outlier_detection.py":[{"add":["132","    Attributes","133","    ----------","134","    location_ : array-like, shape (n_features,)","135","        Estimated robust location","136","","137","    covariance_ : array-like, shape (n_features, n_features)","138","        Estimated robust covariance matrix","139","","140","    precision_ : array-like, shape (n_features, n_features)","141","        Estimated pseudo inverse matrix.","142","        (stored only if store_precision is True)","143","","144","    support_ : array-like, shape (n_samples,)","145","        A mask of the observations that have been used to compute the","146","        robust estimates of location and shape.","147",""],"delete":["108","    Attributes","109","    ----------","110","    location_ : array-like, shape (n_features,)","111","        Estimated robust location","112","","113","    covariance_ : array-like, shape (n_features, n_features)","114","        Estimated robust covariance matrix","115","","116","    precision_ : array-like, shape (n_features, n_features)","117","        Estimated pseudo inverse matrix.","118","        (stored only if store_precision is True)","119","","120","    support_ : array-like, shape (n_samples,)","121","        A mask of the observations that have been used to compute the","122","        robust estimates of location and shape.","123",""]}]}},"32236ff9a52e83e4c6e9984bdefc76082af537d5":{"changes":{"doc\/modules\/linear_model.rst":"MODIFY","sklearn\/linear_model\/logistic.py":"MODIFY","sklearn\/linear_model\/sag.py":"MODIFY"},"diff":{"doc\/modules\/linear_model.rst":[{"add":["791","    .. [6] Mark Schmidt, Nicolas Le Roux, and Francis Bach: `Minimizing Finite Sums with the Stochastic Average Gradient. <https:\/\/hal.inria.fr\/hal-00860051\/document>`_"],"delete":["791","    .. [6] Mark Schmidt, Nicolas Le Roux, and Francis Bach: `Minimizing Finite Sums with the Stochastic Average Gradient. <http:\/\/hal.inria.fr\/hal-00860051\/PDF\/sag_journal.pdf>`_"]}],"sklearn\/linear_model\/logistic.py":[{"add":["1118","    SAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach","1119","        Minimizing Finite Sums with the Stochastic Average Gradient","1120","        https:\/\/hal.inria.fr\/hal-00860051\/document","1121","","1661","                                                len(self.Cs_)))","1667","                                                   len(self.Cs_), -1))","1669","                                                len(self.Cs_)))"],"delete":["1657","                                      len(self.Cs_)))","1663","                                     len(self.Cs_), -1))","1665","                                      len(self.Cs_)))"]}],"sklearn\/linear_model\/sag.py":[{"add":["47","    https:\/\/hal.inria.fr\/hal-00860051\/document","188","    https:\/\/hal.inria.fr\/hal-00860051\/document"],"delete":["47","    https:\/\/hal.inria.fr\/hal-00860051\/PDF\/sag_journal.pdf","188","    https:\/\/hal.inria.fr\/hal-00860051\/PDF\/sag_journal.pdf"]}]}},"3a106fc792eb8e70e1fd078e351ba42487d3214d":{"changes":{"sklearn\/feature_selection\/rfe.py":"MODIFY","sklearn\/feature_selection\/from_model.py":"MODIFY"},"diff":{"sklearn\/feature_selection\/rfe.py":[{"add":["174","                coefs = getattr(estimator, 'feature_importances_', None)","175","            if coefs is None:"],"delete":["173","            elif hasattr(estimator, 'feature_importances_'):","174","                coefs = estimator.feature_importances_"]}],"sklearn\/feature_selection\/from_model.py":[{"add":["16","    importances = getattr(estimator, \"feature_importances_\", None)","18","    if importances is None and hasattr(estimator, \"coef_\"):","25","    elif importances is None:"],"delete":["16","    if hasattr(estimator, \"feature_importances_\"):","17","        importances = estimator.feature_importances_","19","    elif hasattr(estimator, \"coef_\"):","26","    else:"]}]}},"915458bb71e7892c271235cd65e550c1983bd6b9":{"changes":{"sklearn\/linear_model\/tests\/test_coordinate_descent.py":"MODIFY","sklearn\/linear_model\/tests\/test_least_angle.py":"MODIFY","sklearn\/linear_model\/tests\/test_sgd.py":"MODIFY","sklearn\/linear_model\/tests\/test_ransac.py":"MODIFY"},"diff":{"sklearn\/linear_model\/tests\/test_coordinate_descent.py":[{"add":["439","    rng = np.random.RandomState(0)","440","    X = rng.randn(10, 2)","441","    y = rng.randn(10, 2)"],"delete":["439","    X = np.random.randn(10, 2)","440","    y = np.random.randn(10, 2)"]}],"sklearn\/linear_model\/tests\/test_least_angle.py":[{"add":["109","    rng = np.random.RandomState(0)","118","    X = rng.rand(n_samples, 5)"],"delete":["117","    X = np.random.rand(n_samples, 5)"]}],"sklearn\/linear_model\/tests\/test_sgd.py":[{"add":["613","        rng = np.random.RandomState(0)","614","        sample_weights = rng.random_sample(Y4.shape[0])","963","        rng = np.random.RandomState(0)","977","        y = 0.5 * X.ravel() + rng.randn(n_samples, 1).ravel()","1015","        X = rng.randn(n_samples, n_features)"],"delete":["613","        sample_weights = np.random.random(Y4.shape[0])","975","        y = 0.5 * X.ravel() \\","976","            + np.random.randn(n_samples, 1).ravel()","1014","        X = np.random.randn(n_samples, n_features)"]}],"sklearn\/linear_model\/tests\/test_ransac.py":[{"add":["56","    rng = np.random.RandomState(0)","57","    X = rng.rand(10, 2)","58","    y = rng.rand(10, 1)"],"delete":["56","    X = np.random.rand(10, 2)","57","    y = np.random.rand(10, 1)"]}]}},"94c2094f3ecb1d8dfbe0d8561513b799568bda9f":{"changes":{"sklearn\/preprocessing\/tests\/test_label.py":"MODIFY","sklearn\/preprocessing\/label.py":"MODIFY"},"diff":{"sklearn\/preprocessing\/tests\/test_label.py":[{"add":["228","                # verify CSR assumption that indices and indptr have same dtype","229","                assert_equal(got.indices.dtype, got.indptr.dtype)","240","                # verify CSR assumption that indices and indptr have same dtype","241","                assert_equal(got.indices.dtype, got.indptr.dtype)"],"delete":[]}],"sklearn\/preprocessing\/label.py":[{"add":["734","        # ensure yt.indices keeps its current dtype","735","        yt.indices = np.array(inverse[yt.indices], dtype=yt.indices.dtype,","736","                              copy=False)"],"delete":["734","        yt.indices = np.take(inverse, yt.indices)"]}]}},"46001a058222db0cf6fa34eec73a6f6fade96bbd":{"changes":{"doc\/whats_new.rst":"MODIFY","sklearn\/svm\/base.py":"MODIFY","sklearn\/svm\/tests\/test_svm.py":"MODIFY"},"diff":{"doc\/whats_new.rst":[{"add":["126","   - Fix bug for svm's decision values when ``decision_function_shape``","127","     is ``ovr`` in :class:`svm.SVC`.","128","     :class:`svm.SVC`'s decision_function was incorrect from versions","129","     0.17.0 through 0.18.0.","130","     :issue:`7724` by `Bing Tian Dai`_","131","","4888",".. _Bing Tian Dai: https:\/\/github.com\/btdai","4889",""],"delete":["151",""]}],"sklearn\/svm\/base.py":[{"add":["553","            return _ovr_decision_function(dec < 0, -dec, len(self.classes_))"],"delete":["553","            return _ovr_decision_function(dec < 0, dec, len(self.classes_))"]}],"sklearn\/svm\/tests\/test_svm.py":[{"add":["951","","952","","953","def test_ovr_decision_function():","954","    # One point from each quadrant represents one class","955","    X_train = np.array([[1, 1], [-1, 1], [-1, -1], [1, -1]])","956","    y_train = [0, 1, 2, 3]","957","","958","    # First point is closer to the decision boundaries than the second point","959","    base_points = np.array([[5, 5], [10, 10]])","960","","961","    # For all the quadrants (classes)","962","    X_test = np.vstack((","963","        base_points * [1, 1],    # Q1","964","        base_points * [-1, 1],   # Q2","965","        base_points * [-1, -1],  # Q3","966","        base_points * [1, -1]    # Q4","967","        ))","968","","969","    y_test = [0] * 2 + [1] * 2 + [2] * 2 + [3] * 2","970","","971","    clf = svm.SVC(kernel='linear', decision_function_shape='ovr')","972","    clf.fit(X_train, y_train)","973","","974","    y_pred = clf.predict(X_test)","975","","976","    # Test if the prediction is the same as y","977","    assert_array_equal(y_pred, y_test)","978","","979","    deci_val = clf.decision_function(X_test)","980","","981","    # Assert that the predicted class has the maximum value","982","    assert_array_equal(np.argmax(deci_val, axis=1), y_pred)","983","","984","    # Get decision value at test points for the predicted class","985","    pred_class_deci_val = deci_val[range(8), y_pred].reshape((4, 2))","986","","987","    # Assert pred_class_deci_val > 0 here","988","    assert_greater(np.min(pred_class_deci_val), 0.0)","989","","990","    # Test if the first point has lower decision value on every quadrant","991","    # compared to the second point","992","    assert_true(np.all(pred_class_deci_val[:, 0] < pred_class_deci_val[:, 1]))"],"delete":[]}]}},"4907029b1ddff16b111c501ad010d5207e0bd177":{"changes":{"sklearn\/tree\/_splitter.pxd":"MODIFY","sklearn\/tree\/_utils.pyx":"MODIFY","sklearn\/tree\/_tree.pxd":"MODIFY","sklearn\/tree\/_criterion.pyx":"MODIFY","sklearn\/tree\/_criterion.pxd":"MODIFY","sklearn\/tree\/_tree.pyx":"MODIFY","sklearn\/tree\/_utils.pxd":"MODIFY","sklearn\/tree\/_splitter.pyx":"MODIFY"},"diff":{"sklearn\/tree\/_splitter.pxd":[{"add":["83","    cdef int init(self, object X, np.ndarray y,","84","                  DOUBLE_t* sample_weight,","85","                  np.ndarray X_idx_sorted=*) except -1","87","    cdef int node_reset(self, SIZE_t start, SIZE_t end,","88","                        double* weighted_n_node_samples) nogil except -1","90","    cdef int node_split(self,","91","                        double impurity,   # Impurity of the node","92","                        SplitRecord* split,","93","                        SIZE_t* n_constant_features) nogil except -1","97","    cdef double node_impurity(self) nogil"],"delete":["83","    cdef void init(self, object X, np.ndarray y,","84","                   DOUBLE_t* sample_weight,","85","                   np.ndarray X_idx_sorted=*) except *","87","    cdef void node_reset(self, SIZE_t start, SIZE_t end,","88","                         double* weighted_n_node_samples) nogil","90","    cdef void node_split(self,","91","                         double impurity,   # Impurity of the node","92","                         SplitRecord* split,","93","                         SIZE_t* n_constant_features) nogil","97","    cdef double node_impurity(self) nogil"]}],"sklearn\/tree\/_utils.pyx":[{"add":["26","cdef realloc_ptr safe_realloc(realloc_ptr* p, size_t nelems) nogil except *:","32","        with gil:","33","            raise MemoryError(\"could not allocate (%d * %d) bytes\"","34","                              % (nelems, sizeof(p[0][0])))","37","        with gil:","38","            raise MemoryError(\"could not allocate %d bytes\" % nbytes)","121","                  SIZE_t n_constant_features) nogil except -1:","124","        Return -1 in case of failure to allocate memory (and raise MemoryError)","125","        or 0 otherwise.","133","            # Since safe_realloc can raise MemoryError, use `except -1`","134","            safe_realloc(&self.stack_, self.capacity)","194","        safe_realloc(&self.heap_, capacity)","237","                  double impurity_right) nogil except -1:","240","        Return -1 in case of failure to allocate memory (and raise MemoryError)","241","        or 0 otherwise.","249","            # Since safe_realloc can raise MemoryError, use `except -1`","250","            safe_realloc(&self.heap_, self.capacity)","325","    cdef int reset(self) nogil except -1:","326","        \"\"\"Reset the WeightedPQueue to its state at construction","327","","328","        Return -1 in case of failure to allocate memory (and raise MemoryError)","329","        or 0 otherwise.","330","        \"\"\"","332","        # Since safe_realloc can raise MemoryError, use `except *`","333","        safe_realloc(&self.array_, self.capacity)","334","        return 0","342","    cdef int push(self, DOUBLE_t data, DOUBLE_t weight) nogil except -1:","344","","345","        Return -1 in case of failure to allocate memory (and raise MemoryError)","346","        or 0 otherwise.","355","            # Since safe_realloc can raise MemoryError, use `except -1`","356","            safe_realloc(&self.array_, self.capacity)","500","    cdef int reset(self) nogil except -1:","501","        \"\"\"Reset the WeightedMedianCalculator to its state at construction","502","","503","        Return -1 in case of failure to allocate memory (and raise MemoryError)","504","        or 0 otherwise.","505","        \"\"\"","506","        # samples.reset (WeightedPQueue.reset) uses safe_realloc, hence","507","        # except -1","512","        return 0","514","    cdef int push(self, DOUBLE_t data, DOUBLE_t weight) nogil except -1:","515","        \"\"\"Push a value and its associated weight to the WeightedMedianCalculator","516","","517","        Return -1 in case of failure to allocate memory (and raise MemoryError)","518","        or 0 otherwise.","525","        # samples.push (WeightedPQueue.push) uses safe_realloc, hence except -1","531","    cdef int update_median_parameters_post_push(","532","            self, DOUBLE_t data, DOUBLE_t weight,","533","            DOUBLE_t original_median) nogil:","608","    cdef int update_median_parameters_post_remove(","609","            self, DOUBLE_t data, DOUBLE_t weight,","610","            double original_median) nogil:"],"delete":["15","from libc.stdlib cimport calloc","27","cdef realloc_ptr safe_realloc(realloc_ptr* p, size_t nelems) except *:","33","        raise MemoryError(\"could not allocate (%d * %d) bytes\"","34","                          % (nelems, sizeof(p[0][0])))","37","        raise MemoryError(\"could not allocate %d bytes\" % nbytes)","111","        if self.stack_ == NULL:","112","            raise MemoryError()","122","                  SIZE_t n_constant_features) nogil:","125","        Returns 0 if successful; -1 on out of memory error.","133","            stack = <StackRecord*> realloc(self.stack_,","134","                                           self.capacity * sizeof(StackRecord))","135","            if stack == NULL:","136","                # no free; __dealloc__ handles that","137","                return -1","138","            self.stack_ = stack","198","        self.heap_ = <PriorityHeapRecord*> malloc(capacity * sizeof(PriorityHeapRecord))","199","        if self.heap_ == NULL:","200","            raise MemoryError()","243","                  double impurity_right) nogil:","246","        Returns 0 if successful; -1 on out of memory error.","254","            heap = <PriorityHeapRecord*> realloc(self.heap_,","255","                                                 self.capacity *","256","                                                 sizeof(PriorityHeapRecord))","257","            if heap == NULL:","258","                # no free; __dealloc__ handles that","259","                return -1","260","            self.heap_ = heap","332","        if self.array_ == NULL:","333","            raise MemoryError()","334","","338","    cdef void reset(self) nogil:","339","        \"\"\"Reset the WeightedPQueue to its state at construction\"\"\"","341","        self.array_ = <WeightedPQueueRecord*> calloc(self.capacity,","342","                                                     sizeof(WeightedPQueueRecord))","350","    cdef int push(self, DOUBLE_t data, DOUBLE_t weight) nogil:","352","        Returns 0 if successful; -1 on out of memory error.","361","            array = <WeightedPQueueRecord*> realloc(self.array_,","362","                                                    self.capacity *","363","                                                    sizeof(WeightedPQueueRecord))","364","","365","            if array == NULL:","366","                # no free; __dealloc__ handles that","367","                return -1","368","            self.array_ = array","512","    cdef void reset(self) nogil:","513","        \"\"\"Reset the WeightedMedianCalculator to its state at construction\"\"\"","519","    cdef int push(self, DOUBLE_t data, DOUBLE_t weight) nogil:","520","        \"\"\"Push a value and its associated weight","521","        to the WeightedMedianCalculator to be considered","522","        in the median calculation.","534","    cdef int update_median_parameters_post_push(self, DOUBLE_t data,","535","                                                DOUBLE_t weight,","536","                                                DOUBLE_t original_median) nogil:","611","    cdef int update_median_parameters_post_remove(self, DOUBLE_t data,","612","                                                  DOUBLE_t weight,","613","                                                  double original_median) nogil:"]}],"sklearn\/tree\/_tree.pxd":[{"add":["60","                          double weighted_n_samples) nogil except -1","61","    cdef int _resize(self, SIZE_t capacity) nogil except -1","62","    cdef int _resize_c(self, SIZE_t capacity=*) nogil except -1"],"delete":["60","                          double weighted_n_samples) nogil","61","    cdef void _resize(self, SIZE_t capacity) except *","62","    cdef int _resize_c(self, SIZE_t capacity=*) nogil"]}],"sklearn\/tree\/_criterion.pyx":[{"add":["53","    cdef int init(self, DOUBLE_t* y, SIZE_t y_stride, DOUBLE_t* sample_weight,","54","                  double weighted_n_samples, SIZE_t* samples, SIZE_t start,","55","                  SIZE_t end) nogil except -1:","58","        Returns -1 in case of failure to allocate memory (and raise MemoryError)","59","        or 0 otherwise.","60","","84","    cdef int reset(self) nogil except -1:","92","    cdef int reverse_reset(self) nogil except -1:","99","    cdef int update(self, SIZE_t new_pos) nogil except -1:","286","    cdef int init(self, DOUBLE_t* y, SIZE_t y_stride,","287","                  DOUBLE_t* sample_weight, double weighted_n_samples,","288","                  SIZE_t* samples, SIZE_t start, SIZE_t end) nogil except -1:","292","        Returns -1 in case of failure to allocate memory (and raise MemoryError)","293","        or 0 otherwise.","294","","355","        return 0","357","    cdef int reset(self) nogil except -1:","358","        \"\"\"Reset the criterion at pos=start","360","        Returns -1 in case of failure to allocate memory (and raise MemoryError)","361","        or 0 otherwise.","362","        \"\"\"","382","        return 0","384","    cdef int reverse_reset(self) nogil except -1:","385","        \"\"\"Reset the criterion at pos=end","386","","387","        Returns -1 in case of failure to allocate memory (and raise MemoryError)","388","        or 0 otherwise.","389","        \"\"\"","409","        return 0","411","    cdef int update(self, SIZE_t new_pos) nogil except -1:","414","        Returns -1 in case of failure to allocate memory (and raise MemoryError)","415","        or 0 otherwise.","416","","491","        return 0","758","    cdef int init(self, DOUBLE_t* y, SIZE_t y_stride, DOUBLE_t* sample_weight,","759","                  double weighted_n_samples, SIZE_t* samples, SIZE_t start,","760","                  SIZE_t end) nogil except -1:","800","        return 0","802","    cdef int reset(self) nogil except -1:","811","        return 0","813","    cdef int reverse_reset(self) nogil except -1:","822","        return 0","824","    cdef int update(self, SIZE_t new_pos) nogil except -1:","884","        return 0","1051","    cdef int init(self, DOUBLE_t* y, SIZE_t y_stride, DOUBLE_t* sample_weight,","1052","                  double weighted_n_samples, SIZE_t* samples, SIZE_t start,","1053","                  SIZE_t end) nogil except -1:","1091","                # push method ends up calling safe_realloc, hence `except -1`","1103","        return 0","1105","    cdef int reset(self) nogil except -1:","1106","        \"\"\"Reset the criterion at pos=start","1107","","1108","        Returns -1 in case of failure to allocate memory (and raise MemoryError)","1109","        or 0 otherwise.","1110","        \"\"\"","1132","                # push method ends up calling safe_realloc, hence `except -1`","1135","        return 0","1137","    cdef int reverse_reset(self) nogil except -1:","1138","        \"\"\"Reset the criterion at pos=end","1139","","1140","        Returns -1 in case of failure to allocate memory (and raise MemoryError)","1141","        or 0 otherwise.","1142","        \"\"\"","1161","                # push method ends up calling safe_realloc, hence `except -1`","1164","        return 0","1166","    cdef int update(self, SIZE_t new_pos) nogil except -1:","1167","        \"\"\"Updated statistics by moving samples[pos:new_pos] to the left","1168","","1169","        Returns -1 in case of failure to allocate memory (and raise MemoryError)","1170","        or 0 otherwise.","1171","        \"\"\"","1203","                    # push method ends up calling safe_realloc, hence except -1","1227","        return 0"],"delete":["53","    cdef void init(self, DOUBLE_t* y, SIZE_t y_stride, DOUBLE_t* sample_weight,","54","                   double weighted_n_samples, SIZE_t* samples, SIZE_t start,","55","                   SIZE_t end) nogil:","81","    cdef void reset(self) nogil:","89","    cdef void reverse_reset(self) nogil:","96","    cdef void update(self, SIZE_t new_pos) nogil:","283","    cdef void init(self, DOUBLE_t* y, SIZE_t y_stride,","284","                   DOUBLE_t* sample_weight, double weighted_n_samples,","285","                   SIZE_t* samples, SIZE_t start, SIZE_t end) nogil:","350","    cdef void reset(self) nogil:","351","        \"\"\"Reset the criterion at pos=start.\"\"\"","373","    cdef void reverse_reset(self) nogil:","374","        \"\"\"Reset the criterion at pos=end.\"\"\"","395","    cdef void update(self, SIZE_t new_pos) nogil:","738","    cdef void init(self, DOUBLE_t* y, SIZE_t y_stride, DOUBLE_t* sample_weight,","739","                   double weighted_n_samples, SIZE_t* samples, SIZE_t start,","740","                   SIZE_t end) nogil:","781","    cdef void reset(self) nogil:","791","    cdef void reverse_reset(self) nogil:","801","    cdef void update(self, SIZE_t new_pos) nogil:","1020","        if (self.node_medians == NULL):","1021","            raise MemoryError()","1022","","1030","    cdef void init(self, DOUBLE_t* y, SIZE_t y_stride, DOUBLE_t* sample_weight,","1031","                   double weighted_n_samples, SIZE_t* samples, SIZE_t start,","1032","                   SIZE_t end) nogil:","1082","    cdef void reset(self) nogil:","1083","        \"\"\"Reset the criterion at pos=start.\"\"\"","1108","    cdef void reverse_reset(self) nogil:","1109","        \"\"\"Reset the criterion at pos=end.\"\"\"","1131","    cdef void update(self, SIZE_t new_pos) nogil:","1132","        \"\"\"Updated statistics by moving samples[pos:new_pos] to the left.\"\"\""]}],"sklearn\/tree\/_criterion.pxd":[{"add":["55","    cdef int init(self, DOUBLE_t* y, SIZE_t y_stride, DOUBLE_t* sample_weight,","56","                  double weighted_n_samples, SIZE_t* samples, SIZE_t start,","57","                  SIZE_t end) nogil except -1","58","    cdef int reset(self) nogil except -1","59","    cdef int reverse_reset(self) nogil except -1","60","    cdef int update(self, SIZE_t new_pos) nogil except -1"],"delete":["55","    cdef void init(self, DOUBLE_t* y, SIZE_t y_stride, DOUBLE_t* sample_weight,","56","                   double weighted_n_samples, SIZE_t* samples, SIZE_t start,","57","                   SIZE_t end) nogil","58","    cdef void reset(self) nogil","59","    cdef void reverse_reset(self) nogil","60","    cdef void update(self, SIZE_t new_pos) nogil"]}],"sklearn\/tree\/_tree.pyx":[{"add":["273","                                 PriorityHeap frontier) nogil except -1:","274","    \"\"\"Adds record ``rec`` to the priority queue ``frontier``","275","","276","    Returns -1 in case of failure to allocate memory (and raise MemoryError)","277","    or 0 otherwise.","278","    \"\"\"","421","                                    PriorityHeapRecord* res) nogil except -1:","661","    cdef int _resize(self, SIZE_t capacity) nogil except -1:","663","           double the size of the inner arrays.","664","","665","        Returns -1 in case of failure to allocate memory (and raise MemoryError)","666","        or 0 otherwise.","667","        \"\"\"","669","            # Acquire gil only if we need to raise","670","            with gil:","671","                raise MemoryError()","675","    cdef int _resize_c(self, SIZE_t capacity=<SIZE_t>(-1)) nogil except -1:","676","        \"\"\"Guts of _resize","677","","678","        Returns -1 in case of failure to allocate memory (and raise MemoryError)","679","        or 0 otherwise.","680","        \"\"\"","690","        safe_realloc(&self.nodes, capacity)","691","        safe_realloc(&self.value, capacity * self.value_stride)","708","                          SIZE_t n_node_samples,","709","                          double weighted_n_node_samples) nogil except -1:"],"delete":["21","from libc.stdlib cimport realloc","274","                                 PriorityHeap frontier) nogil:","275","    \"\"\"Adds record ``rec`` to the priority queue ``frontier``; returns -1","276","    on memory-error. \"\"\"","419","                                    PriorityHeapRecord* res) nogil:","659","    cdef void _resize(self, SIZE_t capacity) except *:","661","           double the size of the inner arrays.\"\"\"","663","            raise MemoryError()","667","    cdef int _resize_c(self, SIZE_t capacity=<SIZE_t>(-1)) nogil:","668","        \"\"\"Guts of _resize. Returns 0 for success, -1 for error.\"\"\"","678","        # XXX no safe_realloc here because we need to grab the GIL","679","        cdef void* ptr = realloc(self.nodes, capacity * sizeof(Node))","680","        if ptr == NULL:","681","            return -1","682","        self.nodes = <Node*> ptr","683","        ptr = realloc(self.value,","684","                      capacity * self.value_stride * sizeof(double))","685","        if ptr == NULL:","686","            return -1","687","        self.value = <double*> ptr","704","                          SIZE_t n_node_samples, double weighted_n_node_samples) nogil:"]}],"sklearn\/tree\/_utils.pxd":[{"add":["42","    (StackRecord*)","43","    (PriorityHeapRecord*)","45","cdef realloc_ptr safe_realloc(realloc_ptr* p, size_t nelems) nogil except *","52","                     UINT32_t* random_state) nogil","56","                         UINT32_t* random_state) nogil","83","                  SIZE_t n_constant_features) nogil except -1","115","                  double impurity_right) nogil except -1","133","    cdef int reset(self) nogil except -1","135","    cdef int push(self, DOUBLE_t data, DOUBLE_t weight) nogil except -1","156","    cdef int push(self, DOUBLE_t data, DOUBLE_t weight) nogil except -1","157","    cdef int reset(self) nogil except -1","158","    cdef int update_median_parameters_post_push(","159","        self, DOUBLE_t data, DOUBLE_t weight,","160","        DOUBLE_t original_median) nogil","163","    cdef int update_median_parameters_post_remove(","164","        self, DOUBLE_t data, DOUBLE_t weight,","165","        DOUBLE_t original_median) nogil"],"delete":["43","cdef realloc_ptr safe_realloc(realloc_ptr* p, size_t nelems) except *","50","                            UINT32_t* random_state) nogil","54","                                UINT32_t* random_state) nogil","81","                  SIZE_t n_constant_features) nogil","113","                  double impurity_right) nogil","131","    cdef void reset(self) nogil","133","    cdef int push(self, DOUBLE_t data, DOUBLE_t weight) nogil","154","    cdef int push(self, DOUBLE_t data, DOUBLE_t weight) nogil","155","    cdef void reset(self) nogil","156","    cdef int update_median_parameters_post_push(self, DOUBLE_t data,","157","                                                DOUBLE_t weight,","158","                                                DOUBLE_t original_median) nogil","161","    cdef int update_median_parameters_post_remove(self, DOUBLE_t data,","162","                                                  DOUBLE_t weight,","163","                                                  DOUBLE_t original_median) nogil"]}],"sklearn\/tree\/_splitter.pyx":[{"add":["118","    cdef int init(self,","122","                   np.ndarray X_idx_sorted=None) except -1:","127","        Returns -1 in case of failure to allocate memory (and raise MemoryError)","128","        or 0 otherwise.","129","","185","        return 0","187","    cdef int node_reset(self, SIZE_t start, SIZE_t end,","188","                        double* weighted_n_node_samples) nogil except -1:","191","        Returns -1 in case of failure to allocate memory (and raise MemoryError)","192","        or 0 otherwise.","193","","216","        return 0","218","    cdef int node_split(self, double impurity, SplitRecord* split,","219","                        SIZE_t* n_constant_features) nogil except -1:","224","","225","        It should return -1 upon errors.","269","    cdef int init(self,","270","                  object X,","271","                  np.ndarray[DOUBLE_t, ndim=2, mode=\"c\"] y,","272","                  DOUBLE_t* sample_weight,","273","                  np.ndarray X_idx_sorted=None) except -1:","274","        \"\"\"Initialize the splitter","275","","276","        Returns -1 in case of failure to allocate memory (and raise MemoryError)","277","        or 0 otherwise.","278","        \"\"\"","300","        return 0","301","","313","    cdef int node_split(self, double impurity, SplitRecord* split,","314","                        SIZE_t* n_constant_features) nogil except -1:","315","        \"\"\"Find the best split on node samples[start:end]","316","","317","        Returns -1 in case of failure to allocate memory (and raise MemoryError)","318","        or 0 otherwise.","319","        \"\"\"","531","        return 0","541","cdef inline void swap(DTYPE_t* Xf, SIZE_t* samples,","542","        SIZE_t i, SIZE_t j) nogil:","570","cdef void introsort(DTYPE_t* Xf, SIZE_t *samples,","571","                    SIZE_t n, int maxd) nogil:","656","    cdef int node_split(self, double impurity, SplitRecord* split,","657","                        SIZE_t* n_constant_features) nogil except -1:","658","        \"\"\"Find the best random split on node samples[start:end]","659","","660","        Returns -1 in case of failure to allocate memory (and raise MemoryError)","661","        or 0 otherwise.","662","        \"\"\"","862","        return 0","895","    cdef int init(self,","896","                  object X,","897","                  np.ndarray[DOUBLE_t, ndim=2, mode=\"c\"] y,","898","                  DOUBLE_t* sample_weight,","899","                  np.ndarray X_idx_sorted=None) except -1:","900","        \"\"\"Initialize the splitter","902","        Returns -1 in case of failure to allocate memory (and raise MemoryError)","903","        or 0 otherwise.","904","        \"\"\"","936","        return 0","1182","                             SIZE_t pos_1, SIZE_t pos_2) nogil:","1200","    cdef int node_split(self, double impurity, SplitRecord* split,","1201","                        SIZE_t* n_constant_features) nogil except -1:","1202","        \"\"\"Find the best split on node samples[start:end], using sparse features","1203","","1204","        Returns -1 in case of failure to allocate memory (and raise MemoryError)","1205","        or 0 otherwise.","1416","        return 0","1430","    cdef int node_split(self, double impurity, SplitRecord* split,","1431","                        SIZE_t* n_constant_features) nogil except -1:","1432","        \"\"\"Find a random split on node samples[start:end], using sparse features","1433","","1434","        Returns -1 in case of failure to allocate memory (and raise MemoryError)","1435","        or 0 otherwise.","1647","        return 0"],"delete":["118","    cdef void init(self,","122","                   np.ndarray X_idx_sorted=None) except *:","183","    cdef void node_reset(self, SIZE_t start, SIZE_t end,","184","                         double* weighted_n_node_samples) nogil:","210","    cdef void node_split(self, double impurity, SplitRecord* split,","211","                         SIZE_t* n_constant_features) nogil:","259","    cdef void init(self,","260","                   object X,","261","                   np.ndarray[DOUBLE_t, ndim=2, mode=\"c\"] y,","262","                   DOUBLE_t* sample_weight,","263","                   np.ndarray X_idx_sorted=None) except *:","264","        \"\"\"Initialize the splitter.\"\"\"","297","    cdef void node_split(self, double impurity, SplitRecord* split,","298","                         SIZE_t* n_constant_features) nogil:","299","        \"\"\"Find the best split on node samples[start:end].\"\"\"","520","cdef inline void swap(DTYPE_t* Xf, SIZE_t* samples, SIZE_t i, SIZE_t j) nogil:","548","cdef void introsort(DTYPE_t* Xf, SIZE_t *samples, SIZE_t n, int maxd) nogil:","633","    cdef void node_split(self, double impurity, SplitRecord* split,","634","                         SIZE_t* n_constant_features) nogil:","635","        \"\"\"Find the best random split on node samples[start:end].\"\"\"","867","    cdef void init(self,","868","                   object X,","869","                   np.ndarray[DOUBLE_t, ndim=2, mode=\"c\"] y,","870","                   DOUBLE_t* sample_weight,","871","                   np.ndarray X_idx_sorted=None) except *:","872","        \"\"\"Initialize the splitter.\"\"\"","1150","                             SIZE_t pos_1, SIZE_t pos_2) nogil  :","1168","    cdef void node_split(self, double impurity, SplitRecord* split,","1169","                         SIZE_t* n_constant_features) nogil:","1170","        \"\"\"Find the best split on node samples[start:end], using sparse","1171","           features.","1395","    cdef void node_split(self, double impurity, SplitRecord* split,","1396","                         SIZE_t* n_constant_features) nogil:","1397","        \"\"\"Find a random split on node samples[start:end], using sparse","1398","           features."]}]}},"67a85b8ed842612d59187e5fdc81a8b86eb50afd":{"changes":{"examples\/linear_model\/plot_ard.py":"MODIFY","examples\/linear_model\/plot_bayesian_ridge.py":"MODIFY","sklearn\/linear_model\/tests\/test_bayes.py":"MODIFY","sklearn\/linear_model\/bayes.py":"MODIFY"},"diff":{"examples\/linear_model\/plot_ard.py":[{"add":["17","","18","We also plot predictions and uncertainties for ARD","19","for one dimensional regression using polynomial feature expansion.","20","Note the uncertainty starts going up on the right side of the plot.","21","This is because these test samples are outside of the range of the training","22","samples.","62","# Plot the true weights, the estimated weights, the histogram of the","63","# weights, and predictions with standard deviations","89","","90","","91","# Plotting some predictions for polynomial regression","92","def f(x, noise_amount):","93","    y = np.sqrt(x) * np.sin(x)","94","    noise = np.random.normal(0, 1, len(x))","95","    return y + noise_amount * noise","96","","97","","98","degree = 10","99","X = np.linspace(0, 10, 100)","100","y = f(X, noise_amount=1)","101","clf_poly = ARDRegression(threshold_lambda=1e5)","102","clf_poly.fit(np.vander(X, degree), y)","103","","104","X_plot = np.linspace(0, 11, 25)","105","y_plot = f(X_plot, noise_amount=0)","106","y_mean, y_std = clf_poly.predict(np.vander(X_plot, degree), return_std=True)","107","plt.figure(figsize=(6, 5))","108","plt.errorbar(X_plot, y_mean, y_std, color='navy',","109","             label=\"Polynomial ARD\", linewidth=2)","110","plt.plot(X_plot, y_plot, color='gold', linewidth=2,","111","         label=\"Ground Truth\")","112","plt.ylabel(\"Output y\")","113","plt.xlabel(\"Feature X\")","114","plt.legend(loc=\"lower left\")"],"delete":["56","# Plot the true weights, the estimated weights and the histogram of the","57","# weights"]}],"examples\/linear_model\/plot_bayesian_ridge.py":[{"add":["17","","18","We also plot predictions and uncertainties for Bayesian Ridge Regression","19","for one dimensional regression using polynomial feature expansion.","20","Note the uncertainty starts going up on the right side of the plot.","21","This is because these test samples are outside of the range of the training","22","samples.","59","# Plot true weights, estimated weights, histogram of the weights, and","60","# predictions with standard deviations","86","","87","","88","# Plotting some predictions for polynomial regression","89","def f(x, noise_amount):","90","    y = np.sqrt(x) * np.sin(x)","91","    noise = np.random.normal(0, 1, len(x))","92","    return y + noise_amount * noise","93","","94","","95","degree = 10","96","X = np.linspace(0, 10, 100)","97","y = f(X, noise_amount=0.1)","98","clf_poly = BayesianRidge()","99","clf_poly.fit(np.vander(X, degree), y)","100","","101","X_plot = np.linspace(0, 11, 25)","102","y_plot = f(X_plot, noise_amount=0)","103","y_mean, y_std = clf_poly.predict(np.vander(X_plot, degree), return_std=True)","104","plt.figure(figsize=(6, 5))","105","plt.errorbar(X_plot, y_mean, y_std, color='navy',","106","             label=\"Polynomial Bayesian Ridge Regression\", linewidth=lw)","107","plt.plot(X_plot, y_plot, color='gold', linewidth=lw,","108","         label=\"Ground Truth\")","109","plt.ylabel(\"Output y\")","110","plt.xlabel(\"Feature X\")","111","plt.legend(loc=\"lower left\")"],"delete":["53","# Plot true weights, estimated weights and histogram of the weights"]}],"sklearn\/linear_model\/tests\/test_bayes.py":[{"add":["58","","59","","60","def test_return_std():","61","    # Test return_std option for both Bayesian regressors","62","    def f(X):","63","        return np.dot(X, w) + b","64","","65","    def f_noise(X, noise_mult):","66","        return f(X) + np.random.randn(X.shape[0])*noise_mult","67","","68","    d = 5","69","    n_train = 50","70","    n_test = 10","71","","72","    w = np.array([1.0, 0.0, 1.0, -1.0, 0.0])","73","    b = 1.0","74","","75","    X = np.random.random((n_train, d))","76","    X_test = np.random.random((n_test, d))","77","","78","    for decimal, noise_mult in enumerate([1, 0.1, 0.01]):","79","        y = f_noise(X, noise_mult)","80","","81","        m1 = BayesianRidge()","82","        m1.fit(X, y)","83","        y_mean1, y_std1 = m1.predict(X_test, return_std=True)","84","        assert_array_almost_equal(y_std1, noise_mult, decimal=decimal)","85","","86","        m2 = ARDRegression()","87","        m2.fit(X, y)","88","        y_mean2, y_std2 = m2.predict(X_test, return_std=True)","89","        assert_array_almost_equal(y_std2, noise_mult, decimal=decimal)"],"delete":[]}],"sklearn\/linear_model\/bayes.py":[{"add":["93","    sigma_ : array, shape = (n_features, n_features)","94","        estimated variance-covariance matrix of the weights","95","","114","","115","    References","116","    ----------","117","    D. J. C. MacKay, Bayesian Interpolation, Computation and Neural Systems,","118","    Vol. 4, No. 3, 1992.","119","","120","    R. Salakhutdinov, Lecture notes on Statistical Machine Learning,","121","    http:\/\/www.utstat.toronto.edu\/~rsalakhu\/sta4273\/notes\/Lecture2.pdf#page=15","122","    Their beta is our self.alpha_","123","    Their alpha is our self.lambda_","157","        X, y, X_offset_, y_offset_, X_scale_ = self._preprocess_data(","159","        self.X_offset_ = X_offset_","160","        self.X_scale_ = X_scale_","188","                               Vh \/ (eigen_vals_ +","189","                                     lambda_ \/ alpha_)[:, np.newaxis])","234","        sigma_ = np.dot(Vh.T,","235","                        Vh \/ (eigen_vals_ + lambda_ \/ alpha_)[:, np.newaxis])","236","        self.sigma_ = (1. \/ alpha_) * sigma_","238","        self._set_intercept(X_offset_, y_offset_, X_scale_)","241","    def predict(self, X, return_std=False):","242","        \"\"\"Predict using the linear model.","243","","244","        In addition to the mean of the predictive distribution, also its","245","        standard deviation can be returned.","246","","247","        Parameters","248","        ----------","249","        X : {array-like, sparse matrix}, shape = (n_samples, n_features)","250","            Samples.","251","","252","        return_std : boolean, optional","253","            Whether to return the standard deviation of posterior prediction.","254","","255","        Returns","256","        -------","257","        y_mean : array, shape = (n_samples,)","258","            Mean of predictive distribution of query points.","259","","260","        y_std : array, shape = (n_samples,)","261","            Standard deviation of predictive distribution of query points.","262","        \"\"\"","263","        y_mean = self._decision_function(X)","264","        if return_std is False:","265","            return y_mean","266","        else:","267","            if self.normalize:","268","                X = (X - self.X_offset_) \/ self.X_scale_","269","            sigmas_squared_data = (np.dot(X, self.sigma_) * X).sum(axis=1)","270","            y_std = np.sqrt(sigmas_squared_data + (1. \/ self.alpha_))","271","            return y_mean, y_std","272","","376","","377","    References","378","    ----------","379","    D. J. C. MacKay, Bayesian nonlinear modeling for the prediction","380","    competition, ASHRAE Transactions, 1994.","381","","382","    R. Salakhutdinov, Lecture notes on Statistical Machine Learning,","383","    http:\/\/www.utstat.toronto.edu\/~rsalakhu\/sta4273\/notes\/Lecture2.pdf#page=15","384","    Their beta is our self.alpha_","385","    Their alpha is our self.lambda_","386","    ARD is a little different than the slide: only dimensions\/features for","387","    which self.lambda_ < self.threshold_lambda are kept and the rest are","388","    discarded.","431","        X, y, X_offset_, y_offset_, X_scale_ = self._preprocess_data(","483","                            np.sum(np.log(lambda_)))","498","        self._set_intercept(X_offset_, y_offset_, X_scale_)","500","","501","    def predict(self, X, return_std=False):","502","        \"\"\"Predict using the linear model.","503","","504","        In addition to the mean of the predictive distribution, also its","505","        standard deviation can be returned.","506","","507","        Parameters","508","        ----------","509","        X : {array-like, sparse matrix}, shape = (n_samples, n_features)","510","            Samples.","511","","512","        return_std : boolean, optional","513","            Whether to return the standard deviation of posterior prediction.","514","","515","        Returns","516","        -------","517","        y_mean : array, shape = (n_samples,)","518","            Mean of predictive distribution of query points.","519","","520","        y_std : array, shape = (n_samples,)","521","            Standard deviation of predictive distribution of query points.","522","        \"\"\"","523","        y_mean = self._decision_function(X)","524","        if return_std is False:","525","            return y_mean","526","        else:","527","            if self.normalize:","528","                X = (X - self.X_offset_) \/ self.X_scale_","529","            X = X[:, self.lambda_ < self.threshold_lambda]","530","            sigmas_squared_data = (np.dot(X, self.sigma_) * X).sum(axis=1)","531","            y_std = np.sqrt(sigmas_squared_data + (1. \/ self.alpha_))","532","            return y_mean, y_std"],"delete":["144","        X, y, X_offset, y_offset, X_scale = self._preprocess_data(","173","                               Vh \/ (eigen_vals_ + lambda_ \/ alpha_)[:, None])","219","        self._set_intercept(X_offset, y_offset, X_scale)","367","        X, y, X_offset, y_offset, X_scale = self._preprocess_data(","419","                                                np.sum(np.log(lambda_)))","434","        self._set_intercept(X_offset, y_offset, X_scale)"]}]}},"1f278e1c231e6b9b3cf813377819e25e87b6c8b6":{"changes":{"sklearn\/utils\/fixes.py":"MODIFY"},"diff":{"sklearn\/utils\/fixes.py":[{"add":["362","if np_version < (1, 12):"],"delete":["362","if np_version < (1, 12, 0):"]}]}},"ab1c4d46cc118fb507140befb67b70ace018e3f0":{"changes":{"sklearn\/neural_network\/multilayer_perceptron.py":"MODIFY","sklearn\/neural_network\/tests\/test_mlp.py":"MODIFY"},"diff":{"sklearn\/neural_network\/multilayer_perceptron.py":[{"add":["910","        elif self.warm_start:","911","            classes = unique_labels(y)","912","            if set(classes) != set(self.classes_):","913","                raise ValueError(\"warm_start can only be used where `y` has \"","914","                                 \"the same classes as in the previous \"","915","                                 \"call to fit. Previously got %s, `y` has %s\" %","916","                                 (self.classes_, classes))","948","    def fit(self, X, y):","949","        \"\"\"Fit the model to data matrix X and target(s) y.","950","","951","        Parameters","952","        ----------","953","        X : array-like or sparse matrix, shape (n_samples, n_features)","954","            The input data.","955","","956","        y : array-like, shape (n_samples,) or (n_samples, n_outputs)","957","            The target values (class labels in classification, real numbers in","958","            regression).","959","","960","        Returns","961","        -------","962","        self : returns a trained MLP model.","963","        \"\"\"","964","        return self._fit(X, y, incremental=(self.warm_start and","965","                                            hasattr(self, \"classes_\")))","966",""],"delete":[]}],"sklearn\/neural_network\/tests\/test_mlp.py":[{"add":["14","from sklearn.datasets import load_digits, load_boston, load_iris","26","from sklearn.utils.testing import assert_raise_message","52","iris = load_iris()","53","","54","X_iris = iris.data","55","y_iris = iris.target","56","","564","","565","","566","@ignore_warnings(RuntimeError)","567","def test_warm_start():","568","    X = X_iris","569","    y = y_iris","570","","571","    y_2classes = np.array([0] * 75 + [1] * 75)","572","    y_3classes = np.array([0] * 40 + [1] * 40 + [2] * 70)","573","    y_3classes_alt = np.array([0] * 50 + [1] * 50 + [3] * 50)","574","    y_4classes = np.array([0] * 37 + [1] * 37 + [2] * 38 + [3] * 38)","575","    y_5classes = np.array([0] * 30 + [1] * 30 + [2] * 30 + [3] * 30 + [4] * 30)","576","","577","    # No error raised","578","    clf = MLPClassifier(hidden_layer_sizes=2, solver='lbfgs',","579","                        warm_start=True).fit(X, y)","580","    clf.fit(X, y)","581","    clf.fit(X, y_3classes)","582","","583","    for y_i in (y_2classes, y_3classes_alt, y_4classes, y_5classes):","584","        clf = MLPClassifier(hidden_layer_sizes=2, solver='lbfgs',","585","                            warm_start=True).fit(X, y)","586","        message = ('warm_start can only be used where `y` has the same '","587","                   'classes as in the previous call to fit.'","588","                   ' Previously got [0 1 2], `y` has %s' % np.unique(y_i))","589","        assert_raise_message(ValueError, message, clf.fit, X, y_i)"],"delete":["14","from sklearn.datasets import load_digits, load_boston"]}]}},"6a42ea23760226ca657b495e8e639a02a234d348":{"changes":{"sklearn\/metrics\/regression.py":"MODIFY","sklearn\/metrics\/tests\/test_regression.py":"MODIFY"},"diff":{"sklearn\/metrics\/regression.py":[{"add":["89","    allowed_multioutput_str = ('raw_values', 'uniform_average',","90","                               'variance_weighted')","91","    if isinstance(multioutput, string_types):","92","        if multioutput not in allowed_multioutput_str:","93","            raise ValueError(\"Allowed 'multioutput' string values are {}. \"","94","                             \"You provided multioutput={!r}\".format(","95","                                 allowed_multioutput_str,","96","                                 multioutput))","97","    elif multioutput is not None:","512","    >>> r2_score(y_true, y_pred, multioutput='variance_weighted')","513","    ... # doctest: +ELLIPSIS"],"delete":["89","    multioutput_options = (None, 'raw_values', 'uniform_average',","90","                           'variance_weighted')","91","    if multioutput not in multioutput_options:","506","    >>> r2_score(y_true, y_pred, multioutput='variance_weighted')  # doctest: +ELLIPSIS"]}],"sklearn\/metrics\/tests\/test_regression.py":[{"add":["95","def test__check_reg_targets_exception():","96","    invalid_multioutput = 'this_value_is_not_valid'","97","    expected_message = (\"Allowed 'multioutput' string values are.+\"","98","                        \"You provided multioutput={!r}\".format(","99","                            invalid_multioutput))","100","    assert_raises_regex(ValueError, expected_message,","101","                        _check_reg_targets,","102","                        [1, 2, 3],","103","                        [[1], [2], [3]],","104","                        invalid_multioutput)","105","","106",""],"delete":[]}]}},"41e1b8f17cc9f8176dbce1baada62772126b9466":{"changes":{"sklearn\/calibration.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/tests\/test_calibration.py":"MODIFY"},"diff":{"sklearn\/calibration.py":[{"add":["16","from sklearn.preprocessing import LabelEncoder","19","from .preprocessing import label_binarize, LabelBinarizer","53","        with too few calibration samples ``(<<1000)`` since it tends to","54","        overfit.","67","        :class:`sklearn.model_selection.StratifiedKFold` is used. If ``y`` is","68","        neither binary nor multiclass, :class:`sklearn.model_selection.KFold`","128","        le = LabelBinarizer().fit(y)","129","        self.classes_ = le.classes_","136","                np.any([np.sum(y == class_) < n_folds for class_ in","137","                        self.classes_]):","180","                    this_estimator, method=self.method,","181","                    classes=self.classes_)","259","    classes : array-like, shape (n_classes,), optional","260","            Contains unique classes used to fit the base estimator.","261","            if None, then classes is extracted from the given target values","262","            in fit().","263","","278","    def __init__(self, base_estimator, method='sigmoid', classes=None):","281","        self.classes = classes","297","        idx_pos_class = self.label_encoder_.\\","298","            transform(self.base_estimator.classes_)","321","","322","        self.label_encoder_ = LabelEncoder()","323","        if self.classes is None:","324","            self.label_encoder_.fit(y)","325","        else:","326","            self.label_encoder_.fit(self.classes)","327","","328","        self.classes_ = self.label_encoder_.classes_","329","        Y = label_binarize(y, self.classes_)"],"delete":["18","from .preprocessing import LabelBinarizer","52","        with too few calibration samples ``(<<1000)`` since it tends to overfit.","65","        :class:`sklearn.model_selection.StratifiedKFold` is used. If ``y`` ","66","        is neither binary nor multiclass, :class:`sklearn.model_selection.KFold` ","126","        lb = LabelBinarizer().fit(y)","127","        self.classes_ = lb.classes_","134","           np.any([np.sum(y == class_) < n_folds for class_ in self.classes_]):","177","                    this_estimator, method=self.method)","269","    def __init__(self, base_estimator, method='sigmoid'):","287","        idx_pos_class = np.arange(df.shape[1])","310","        lb = LabelBinarizer()","311","        Y = lb.fit_transform(y)","312","        self.classes_ = lb.classes_"]}],"doc\/whats_new.rst":[{"add":["146","   - Fixes issue in :class:`calibration.CalibratedClassifierCV` where","147","     the sum of probabilities of each class for a data was not 1, and","148","     ``CalibratedClassifierCV`` now handles the case where the training set","149","     has less number of classes than the total data. :issue:`7799` by","150","     `Srivatsan Ramesh`_","151",""],"delete":[]}],"sklearn\/tests\/test_calibration.py":[{"add":["3","from __future__ import division","6","from sklearn.model_selection import LeaveOneOut","156","","273","","274","","275","def test_calibration_prob_sum():","276","    # Test that sum of probabilities is 1. A non-regression test for","277","    # issue #7796","278","    num_classes = 2","279","    X, y = make_classification(n_samples=10, n_features=5,","280","                               n_classes=num_classes)","281","    clf = LinearSVC(C=1.0)","282","    clf_prob = CalibratedClassifierCV(clf, method=\"sigmoid\", cv=LeaveOneOut())","283","    clf_prob.fit(X, y)","284","","285","    probs = clf_prob.predict_proba(X)","286","    assert_array_almost_equal(probs.sum(axis=1), np.ones(probs.shape[0]))","287","","288","","289","def test_calibration_less_classes():","290","    # Test to check calibration works fine when train set in a test-train","291","    # split does not contain all classes","292","    # Since this test uses LOO, at each iteration train set will not contain a","293","    # class label","294","    X = np.random.randn(10, 5)","295","    y = np.arange(10)","296","    clf = LinearSVC(C=1.0)","297","    cal_clf = CalibratedClassifierCV(clf, method=\"sigmoid\", cv=LeaveOneOut())","298","    cal_clf.fit(X, y)","299","","300","    for i, calibrated_classifier in \\","301","            enumerate(cal_clf.calibrated_classifiers_):","302","        proba = calibrated_classifier.predict_proba(X)","303","        assert_array_equal(proba[:, i], np.zeros(len(y)))","304","        assert_equal(np.all(np.hstack([proba[:, :i],","305","                                       proba[:, i + 1:]])), True)"],"delete":["16","from sklearn.linear_model import Ridge","89","        # check that calibration can also deal with regressors that have","90","        # a decision_function","91","        clf_base_regressor = CalibratedClassifierCV(Ridge())","92","        clf_base_regressor.fit(X_train, y_train)","93","        clf_base_regressor.predict(X_test)","94",""]}]}},"d39c2730223678ef7bee451a617042d9bd87f35d":{"changes":{"sklearn\/preprocessing\/label.py":"MODIFY"},"diff":{"sklearn\/preprocessing\/label.py":[{"add":["287","        y : array of shape [n_samples,] or [n_samples, n_classes]","306","    def fit_transform(self, y):","307","        \"\"\"Fit label binarizer and transform multi-class labels to binary","308","        labels.","310","        The output of transform is sometimes referred to    as","311","        the 1-of-K coding scheme.","315","        y : array or sparse matrix of shape [n_samples,] or \\","316","            [n_samples, n_classes]","317","            Target values. The 2-d matrix should only contain 0 and 1,","318","            represents multilabel classification. Sparse matrix can be","319","            CSR, CSC, COO, DOK, or LIL.","320","","321","        Returns","322","        -------","323","        Y : array or CSR matrix of shape [n_samples, n_classes]","324","            Shape will be [n_samples, 1] for binary problems.","325","        \"\"\"","326","        return self.fit(y).transform(y)","327","","328","    def transform(self, y):","329","        \"\"\"Transform multi-class labels to binary labels","330","","331","        The output of transform is sometimes referred to by some authors as","332","        the 1-of-K coding scheme.","333","","334","        Parameters","335","        ----------","336","        y : array or sparse matrix of shape [n_samples,] or \\","337","            [n_samples, n_classes]","338","            Target values. The 2-d matrix should only contain 0 and 1,","339","            represents multilabel classification. Sparse matrix can be","340","            CSR, CSC, COO, DOK, or LIL."],"delete":["287","        y : numpy array of shape (n_samples,) or (n_samples, n_classes)","306","    def transform(self, y):","307","        \"\"\"Transform multi-class labels to binary labels","309","        The output of transform is sometimes referred to by some authors as the","310","        1-of-K coding scheme.","314","        y : numpy array or sparse matrix of shape (n_samples,) or","315","            (n_samples, n_classes) Target values. The 2-d matrix should only","316","            contain 0 and 1, represents multilabel classification. Sparse","317","            matrix can be CSR, CSC, COO, DOK, or LIL."]}]}},"61ec187ea81f395de56baf1ee1564799a5075708":{"changes":{"examples\/text\/document_classification_20newsgroups.py":"MODIFY","examples\/text\/document_clustering.py":"MODIFY"},"diff":{"examples\/text\/document_classification_20newsgroups.py":[{"add":["85","","86","def is_interactive():","87","    return not hasattr(sys.modules['__main__ '], '__file__')","88","","89","# work-around for Jupyter notebook and IPython console","90","argv = [] if is_interactive() else sys.argv[1:]","91","(opts, args) = op.parse_args(argv)"],"delete":["85","(opts, args) = op.parse_args()"]}],"examples\/text\/document_clustering.py":[{"add":["30","and discover latent patterns in the data.","104","","105","def is_interactive():","106","    return not hasattr(sys.modules['__main__'], '__file__')","107","","108","# work-around for Jupyter notebook and IPython console","109","argv = [] if is_interactive() else sys.argv[1:]","110","(opts, args) = op.parse_args(argv)","125","# categories = None"],"delete":["30","and discover latent patterns in the data. ","104","(opts, args) = op.parse_args()","119","#categories = None"]}]}},"5bb983013c71d8e9bcd7bed8654e60b1bcf20a47":{"changes":{"sklearn\/__init__.py":"MODIFY","sklearn\/ensemble\/iforest.py":"MODIFY","sklearn\/mixture\/bayesian_mixture.py":"MODIFY","doc\/faq.rst":"MODIFY","doc\/whats_new.rst":"MODIFY","doc\/modules\/model_evaluation.rst":"MODIFY","examples\/ensemble\/plot_random_forest_regression_multioutput.py":"MODIFY","sklearn\/mixture\/gaussian_mixture.py":"MODIFY","doc\/index.rst":"MODIFY"},"diff":{"sklearn\/__init__.py":[{"add":["72",""],"delete":[]}],"sklearn\/ensemble\/iforest.py":[{"add":["68",""],"delete":[]}],"sklearn\/mixture\/bayesian_mixture.py":[{"add":["92","    covariance_type : {'full', 'tied', 'diag', 'spherical'}, defaults to 'full'","95","","96","            'full' (each component has its own general covariance matrix),","97","            'tied' (all components share the same general covariance matrix),","98","            'diag' (each component has its own diagonal covariance matrix),","99","            'spherical' (each component has its own single variance).","121","","122","            'kmeans' : responsibilities are initialized using kmeans.","123","            'random' : responsibilities are initialized randomly.","128","","129","            'dirichlet_process' (using the Stick-breaking representation),","130","            'dirichlet_distribution' (can favor more uniform weights).","138","        than 0. If it is None, it's set to ``1. \/ n_components``.","147","    mean_prior : array-like, shape (n_features,), optional","159","","160","                (n_features, n_features) if 'full',","161","                (n_features, n_features) if 'tied',","162","                (n_features)             if 'diag',","163","                float                    if 'spherical'","184","    weights_ : array-like, shape (n_components,)","187","    means_ : array-like, shape (n_components, n_features)","193","","206","        The shape depends on ``covariance_type``::","207","","220","        time. The shape depends on ``covariance_type``::","221","","241","        ``weight_concentration_prior_type``::","242","","245","","251","    weight_concentration_ : array-like, shape (n_components,)","261","    mean_precision_ : array-like, shape (n_components,)","264","    means_prior_ : array-like, shape (n_features,)","271","    degrees_of_freedom_ : array-like, shape (n_components,)","277","","278","            (n_features, n_features) if 'full',","279","            (n_features, n_features) if 'tied',","280","            (n_features)             if 'diag',","281","            float                    if 'spherical'"],"delete":["92","    covariance_type : {'full', 'tied', 'diag', 'spherical'}, defaults to 'full'.","95","        'full' (each component has its own general covariance matrix),","96","        'tied' (all components share the same general covariance matrix),","97","        'diag' (each component has its own diagonal covariance matrix),","98","        'spherical' (each component has its own single variance).","120","        'kmeans' : responsibilities are initialized using kmeans.","121","        'random' : responsibilities are initialized randomly.","126","        'dirichlet_process' (using the Stick-breaking representation),","127","        'dirichlet_distribution' (can favor more uniform weights).","135","        than 0. If it is None, it's set to `1. \/ n_components`.","144","    mean_prior : array-like, shape (`n_features`,), optional","156","            (`n_features`, `n_features`) if 'full',","157","            (`n_features`, `n_features`) if 'tied',","158","            (`n_features`)               if 'diag',","159","            float                        if 'spherical'","180","    weights_ : array-like, shape (`n_components`,)","183","    means_ : array-like, shape (`n_components`, `n_features`)","201","        The shape depends on `covariance_type`::","214","        time. The shape depends on `covariance_type`::","234","        `weight_concentration_prior_type`::","242","    weight_concentration_ : array-like, shape (`n_components`, )","252","    mean_precision_ : array-like, shape (`n_components`, )","255","    means_prior_ : array-like, shape (`n_features`,)","262","    degrees_of_freedom_ : array-like, shape (`n_components`,)","268","            (`n_features`, `n_features`) if 'full',","269","            (`n_features`, `n_features`) if 'tied',","270","            (`n_features`)               if 'diag',","271","            float                        if 'spherical'"]}],"doc\/faq.rst":[{"add":["129",".. _selectiveness:","316","---------------------------------------------------------"],"delete":["315","----------------------------------------------------"]}],"doc\/whats_new.rst":[{"add":["43","  - **The model_selection module**","67","  - **The enhanced cv_results_ attribute**","89","  - **Parameters n_folds and n_iter renamed to n_splits**","110","    Note the change from singular to plural form in","113","  - **Fit parameter labels renamed to groups**","122","  - **Parameter n_labels renamed to n_groups**","279","     (:class:`isotonic.IsotonicRegression`) is now much faster (over 1000x in tests with synthetic","282","   - Isotonic regression (:class:`isotonic.IsotonicRegression`) now uses a better algorithm to avoid","284","     (`#6601 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/6691>`_).","356","     (`#7324 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/7325>`_)","357","     By `Eugene Chen`_ and `Raghav R V`_.","369","     (`#7419 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/7419>`_)","477","      small (``< .1 * min(X.shape)``) `n_iter` is set to 7, unless the user specifies","501","      of ``X`` to transform function when ``copy=True`` (`#7194","562","      (`#7323 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/7323>`_) by Viacheslav Kovalevskyi.","566","      If ``n_iter<2`` numerical issues are unlikely, thus no normalization is applied.","567","      Other normalization options are available: ``'none', 'LU'`` and ``'QR'``.","639","     :func:`metrics.hamming_loss`.","653","     the parameter ``n_labels`` is renamed to ``n_groups``.","655","     by `Raghav R V`_."],"delete":["43","  - **The ``model_selection`` module**","67","  - **The enhanced ``cv_results_`` attribute**","89","  - **Parameters ``n_folds`` and ``n_iter`` renamed to ``n_splits``**","110","    NOTE the change from singular to plural form in","113","  - **Fit parameter ``labels`` renamed to ``groups``**","122","  - **Parameter ``n_labels`` renamed to ``n_groups``**","279","     (:mod:`isotonic`) is now much faster (over 1000x in tests with synthetic","282","   - Isotonic regression (:mod:`isotonic`) now uses a better algorithm to avoid","284","     (`#6601 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/6691>`).","356","     (`#7324 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/7325>`)","357","     By `Eugene Chen`_ and `Raghav RV`_.","369","     (`#7419 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/7419>_`)","422","","478","      small (< .1 * min(X.shape)) `n_iter` is set to 7, unless the user specifies","502","      of `X` to transform function when `copy=True` (`#7194","563","      (`#7323 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/7323>`_) by `Viacheslav Kovalevskyi`_.","567","      If `n_iter<2` numerical issues are unlikely, thus no normalization is applied.","568","      Other normalization options are available: 'none', 'LU' and 'QR'.","640","     :func:`metrics.classification.hamming_loss`.","654","     the parameter ``n_labels``is renamed to ``n_groups``.","656","     by `Raghav RV`_."]}],"doc\/modules\/model_evaluation.rst":[{"add":["57","===========================     =========================================     ==================================","59","===========================     =========================================     ==================================","1103","  * See :ref:`sphx_glr_auto_examples_calibration_plot_calibration.py`"],"delete":["57","==========================      =========================================     ==================================","59","==========================      =========================================     ==================================","1103","  * See :ref:`sphx_glr_calibration_plot_calibration.py`"]}],"examples\/ensemble\/plot_random_forest_regression_multioutput.py":[{"add":["6","the :ref:`multioutput.MultiOutputRegressor <multiclass>` meta-estimator.","9",":ref:`multioutput.MultiOutputRegressor <multiclass>` meta-estimator"],"delete":["6","the :ref:`multioutput.MultiOutputRegressor <_multiclass>` meta-estimator.","9",":ref:`multioutput.MultiOutputRegressor <_multiclass>` meta-estimator"]}],"sklearn\/mixture\/gaussian_mixture.py":[{"add":["452","            defaults to 'full'.","455","","456","            'full' (each component has its own general covariance matrix),","457","            'tied' (all components share the same general covariance matrix),","458","            'diag' (each component has its own diagonal covariance matrix),","459","            'spherical' (each component has its own single variance).","479","","480","            'kmeans' : responsibilities are initialized using kmeans.","481","            'random' : responsibilities are initialized randomly.","496","","530","","544","","558",""],"delete":["452","        defaults to 'full'.","455","        'full' (each component has its own general covariance matrix),","456","        'tied' (all components share the same general covariance matrix),","457","        'diag' (each component has its own diagonal covariance matrix),","458","        'spherical' (each component has its own single variance).","478","        'kmeans' : responsibilities are initialized using kmeans.","479","        'random' : responsibilities are initialized randomly."]}],"doc\/index.rst":[{"add":["209","                    <li><em>September 2016.<\/em> scikit-learn 0.18.0 is available for download (<a href=\"whats_new.html#version-0-18\">Changelog<\/a>).","210","                    <\/li>"],"delete":[]}]}},"868a58b2e0ff23427fbfb625e4fc7f997c734480":{"changes":{"sklearn\/utils\/fixes.py":"MODIFY","sklearn\/model_selection\/tests\/test_search.py":"MODIFY","sklearn\/utils\/tests\/test_fixes.py":"MODIFY","sklearn\/model_selection\/_search.py":"MODIFY"},"diff":{"sklearn\/utils\/fixes.py":[{"add":["403","","404","","405","if np_version < (1, 12, 0):","406","    class MaskedArray(np.ma.MaskedArray):","407","        # Before numpy 1.12, np.ma.MaskedArray object is not picklable","408","        # This fix is needed to make our model_selection.GridSearchCV","409","        # picklable as the ``cv_results_`` param uses MaskedArray","410","        def __getstate__(self):","411","            \"\"\"Return the internal state of the masked array, for pickling","412","            purposes.","413","","414","            \"\"\"","415","            cf = 'CF'[self.flags.fnc]","416","            data_state = super(np.ma.MaskedArray, self).__reduce__()[2]","417","            return data_state + (np.ma.getmaskarray(self).tostring(cf),","418","                                 self._fill_value)","419","else:","420","    from numpy.ma import MaskedArray    # noqa"],"delete":[]}],"sklearn\/model_selection\/tests\/test_search.py":[{"add":["942","    grid_search_pickled = pickle.loads(pickle.dumps(grid_search))","943","    assert_array_almost_equal(grid_search.predict(X),","944","                              grid_search_pickled.predict(X))","949","    random_search_pickled = pickle.loads(pickle.dumps(random_search))","950","    assert_array_almost_equal(random_search.predict(X),","951","                              random_search_pickled.predict(X))"],"delete":["942","    pickle.dumps(grid_search)  # smoke test","947","    pickle.dumps(random_search)  # smoke test"]}],"sklearn\/utils\/tests\/test_fixes.py":[{"add":["5","import pickle","8","from sklearn.utils.testing import assert_equal","9","from sklearn.utils.testing import assert_false","10","from sklearn.utils.testing import assert_true","11","from sklearn.utils.testing import assert_almost_equal","12","from sklearn.utils.testing import assert_array_equal","13","from sklearn.utils.testing import assert_array_almost_equal","14","","17","from sklearn.utils.fixes import MaskedArray","58","","59","","60","def test_masked_array_obj_dtype_pickleable():","61","    marr = MaskedArray([1, None, 'a'], dtype=object)","62","","63","    for mask in (True, False, [0, 1, 0]):","64","        marr.mask = mask","65","        marr_pickled = pickle.loads(pickle.dumps(marr))","66","        assert_array_equal(marr.data, marr_pickled.data)","67","        assert_array_equal(marr.mask, marr_pickled.mask)"],"delete":["7","from numpy.testing import (assert_almost_equal,","8","                           assert_array_almost_equal)","11","from sklearn.utils.testing import assert_equal, assert_false, assert_true"]}],"sklearn\/model_selection\/_search.py":[{"add":["32","from ..utils.fixes import MaskedArray","614","        # Use one MaskedArray and mask all the places where the param is not","617","        param_results = defaultdict(partial(MaskedArray,","618","                                            np.empty(n_candidates,),","619","                                            mask=True,"],"delete":["613","        # Use one np.MaskedArray and mask all the places where the param is not","616","        param_results = defaultdict(partial(np.ma.masked_all, (n_candidates,),"]}]}},"5dd9bfe12c759dbef4e3546dfb05519e7fb60eb1":{"changes":{"sklearn\/metrics\/classification.py":"MODIFY"},"diff":{"sklearn\/metrics\/classification.py":[{"add":["1367","        The reported averages are a prevalence-weighted macro-average across","1368","        classes (equivalent to :func:`precision_recall_fscore_support` with","1369","        ``average='weighted'``).","1370","","1371","        Note that in binary classification, recall of the positive class","1372","        is also known as \"sensitivity\"; recall of the negative class is","1373","        \"specificity\".","1374","","1401","        target_names = [u'%s' % l for l in labels]","1406","    head_fmt = u'{:>{width}s} ' + u' {:>9}' * len(headers)","1407","    report = head_fmt.format(u'', *headers, width=width)","1408","    report += u'\\n\\n'","1415","    row_fmt = u'{:>{width}s} ' + u' {:>9.{digits}f}' * 3 + u' {:>9}\\n'","1416","    rows = zip(target_names, p, r, f1, s)","1417","    for row in rows:","1418","        report += row_fmt.format(*row, width=width, digits=digits)","1420","    report += u'\\n'","1423","    report += row_fmt.format(last_line_heading,","1424","                             np.average(p, weights=s),","1425","                             np.average(r, weights=s),","1426","                             np.average(f1, weights=s),","1427","                             np.sum(s),","1428","                             width=width, digits=digits)","1429",""],"delete":["1393","        target_names = ['%s' % l for l in labels]","1398","    fmt = '%% %ds' % width  # first column: class name","1399","    fmt += '  '","1400","    fmt += ' '.join(['% 9s' for _ in headers])","1401","    fmt += '\\n'","1402","","1403","    headers = [\"\"] + headers","1404","    report = fmt % tuple(headers)","1405","    report += '\\n'","1412","    for i, label in enumerate(labels):","1413","        values = [target_names[i]]","1414","        for v in (p[i], r[i], f1[i]):","1415","            values += [\"{0:0.{1}f}\".format(v, digits)]","1416","        values += [\"{0}\".format(s[i])]","1417","        report += fmt % tuple(values)","1419","    report += '\\n'","1422","    values = [last_line_heading]","1423","    for v in (np.average(p, weights=s),","1424","              np.average(r, weights=s),","1425","              np.average(f1, weights=s)):","1426","        values += [\"{0:0.{1}f}\".format(v, digits)]","1427","    values += ['{0}'.format(np.sum(s))]","1428","    report += fmt % tuple(values)"]}]}},"568c002325c69795369e7dbada31d531c445b3eb":{"changes":{"sklearn\/utils\/estimator_checks.py":"MODIFY","sklearn\/tests\/test_common.py":"MODIFY"},"diff":{"sklearn\/utils\/estimator_checks.py":[{"add":["134","    yield check_non_transformer_estimators_n_iter","135","","176","    yield check_non_transformer_estimators_n_iter","191","    # Dependent on external solvers and hence accessing the iter","192","    # param is non-trivial.","193","    external_solver = ['Isomap', 'KernelPCA', 'LocallyLinearEmbedding',","194","                       'RandomizedLasso', 'LogisticRegressionCV']","195","    if name not in external_solver:","196","        yield check_transformer_n_iter","197","","207","    yield check_non_transformer_estimators_n_iter","231","    yield check_get_params_invariance","1491","def check_non_transformer_estimators_n_iter(name, Estimator):","1492","    # Test that estimators that are not transformers with a parameter","1493","    # max_iter, return the attribute of n_iter_ at least 1.","1495","    # These models are dependent on external solvers like","1496","    # libsvm and accessing the iter parameter is non-trivial.","1497","    not_run_check_n_iter = ['Ridge', 'SVR', 'NuSVR', 'NuSVC',","1498","                            'RidgeClassifier', 'SVC', 'RandomizedLasso',","1499","                            'LogisticRegressionCV', 'LinearSVC',","1500","                            'LogisticRegression']","1502","    # Tested in test_transformer_n_iter","1503","    not_run_check_n_iter += CROSS_DECOMPOSITION","1504","    if name in not_run_check_n_iter:","1505","        return","1507","    # LassoLars stops early for the default alpha=1.0 the iris dataset.","1508","    if name == 'LassoLars':","1509","        estimator = Estimator(alpha=0.)","1511","        estimator = Estimator()","1512","    if hasattr(estimator, 'max_iter'):","1513","        iris = load_iris()","1514","        X, y_ = iris.data, iris.target","1515","        y_ = multioutput_estimator_convert_y_2d(name, y_)","1517","        set_random_state(estimator, 0)","1518","        if name == 'AffinityPropagation':","1519","            estimator.fit(X)","1520","        else:","1521","            estimator.fit(X, y_)","1522","","1523","        # HuberRegressor depends on scipy.optimize.fmin_l_bfgs_b","1524","        # which doesn't return a n_iter for old versions of SciPy.","1525","        if not (name == 'HuberRegressor' and estimator.n_iter_ is None):","1526","            assert_greater_equal(estimator.n_iter_, 1)","1530","def check_transformer_n_iter(name, Estimator):","1531","    # Test that transformers with a parameter max_iter, return the","1532","    # attribute of n_iter_ at least 1.","1533","    estimator = Estimator()","1534","    if hasattr(estimator, \"max_iter\"):","1535","        if name in CROSS_DECOMPOSITION:","1536","            # Check using default data","1537","            X = [[0., 0., 1.], [1., 0., 0.], [2., 2., 2.], [2., 5., 4.]]","1538","            y_ = [[0.1, -0.2], [0.9, 1.1], [0.1, -0.5], [0.3, -0.2]]","1540","        else:","1541","            X, y_ = make_blobs(n_samples=30, centers=[[0, 0, 0], [1, 1, 1]],","1542","                               random_state=0, n_features=2, cluster_std=0.1)","1543","            X -= X.min() - 0.1","1544","        set_random_state(estimator, 0)","1545","        estimator.fit(X, y_)","1547","        # These return a n_iter per component.","1548","        if name in CROSS_DECOMPOSITION:","1549","            for iter_ in estimator.n_iter_:","1550","                assert_greater_equal(iter_, 1)","1551","        else:","1552","            assert_greater_equal(estimator.n_iter_, 1)","1555","@ignore_warnings(category=DeprecationWarning)","1557","    # Checks if get_params(deep=False) is a subset of get_params(deep=True)"],"delete":["1479","def check_non_transformer_estimators_n_iter(name, estimator,","1480","                                            multi_output=False):","1481","    # Check if all iterative solvers, run for more than one iteration","1483","    iris = load_iris()","1484","    X, y_ = iris.data, iris.target","1486","    if multi_output:","1487","        y_ = np.reshape(y_, (-1, 1))","1489","    set_random_state(estimator, 0)","1490","    if name == 'AffinityPropagation':","1491","        estimator.fit(X)","1493","        estimator.fit(X, y_)","1495","    # HuberRegressor depends on scipy.optimize.fmin_l_bfgs_b","1496","    # which doesn't return a n_iter for old versions of SciPy.","1497","    if not (name == 'HuberRegressor' and estimator.n_iter_ is None):","1498","        assert_greater_equal(estimator.n_iter_, 1)","1502","def check_transformer_n_iter(name, estimator):","1503","    if name in CROSS_DECOMPOSITION:","1504","        # Check using default data","1505","        X = [[0., 0., 1.], [1., 0., 0.], [2., 2., 2.], [2., 5., 4.]]","1506","        y_ = [[0.1, -0.2], [0.9, 1.1], [0.1, -0.5], [0.3, -0.2]]","1508","    else:","1509","        X, y_ = make_blobs(n_samples=30, centers=[[0, 0, 0], [1, 1, 1]],","1510","                           random_state=0, n_features=2, cluster_std=0.1)","1511","        X -= X.min() - 0.1","1512","    set_random_state(estimator, 0)","1513","    estimator.fit(X, y_)","1515","    # These return a n_iter per component.","1516","    if name in CROSS_DECOMPOSITION:","1517","        for iter_ in estimator.n_iter_:","1518","            assert_greater_equal(iter_, 1)","1519","    else:","1520","        assert_greater_equal(estimator.n_iter_, 1)"]}],"sklearn\/tests\/test_common.py":[{"add":["31","    check_class_weight_balanced_linear_classifier)"],"delete":["30","    CROSS_DECOMPOSITION,","32","    check_class_weight_balanced_linear_classifier,","33","    check_transformer_n_iter,","34","    check_non_transformer_estimators_n_iter,","35","    check_get_params_invariance)","164","","165","","166","def test_non_transformer_estimators_n_iter():","167","    # Test that all estimators of type which are non-transformer","168","    # and which have an attribute of max_iter, return the attribute","169","    # of n_iter atleast 1.","170","    for est_type in ['regressor', 'classifier', 'cluster']:","171","        regressors = all_estimators(type_filter=est_type)","172","        for name, Estimator in regressors:","173","            # LassoLars stops early for the default alpha=1.0 for","174","            # the iris dataset.","175","            if name == 'LassoLars':","176","                estimator = Estimator(alpha=0.)","177","            else:","178","                with ignore_warnings(category=DeprecationWarning):","179","                    estimator = Estimator()","180","            if hasattr(estimator, \"max_iter\"):","181","                # These models are dependent on external solvers like","182","                # libsvm and accessing the iter parameter is non-trivial.","183","                if name in (['Ridge', 'SVR', 'NuSVR', 'NuSVC',","184","                             'RidgeClassifier', 'SVC', 'RandomizedLasso',","185","                             'LogisticRegressionCV']):","186","                    continue","187","","188","                # Tested in test_transformer_n_iter below","189","                elif (name in CROSS_DECOMPOSITION or","190","                      name in ['LinearSVC', 'LogisticRegression']):","191","                    continue","192","","193","                else:","194","                    # Multitask models related to ENet cannot handle","195","                    # if y is mono-output.","196","                    yield (_named_check(","197","                        check_non_transformer_estimators_n_iter, name),","198","                        name, estimator, 'Multi' in name)","199","","200","","201","def test_transformer_n_iter():","202","    transformers = all_estimators(type_filter='transformer')","203","    for name, Estimator in transformers:","204","        with ignore_warnings(category=DeprecationWarning):","205","            estimator = Estimator()","206","        # Dependent on external solvers and hence accessing the iter","207","        # param is non-trivial.","208","        external_solver = ['Isomap', 'KernelPCA', 'LocallyLinearEmbedding',","209","                           'RandomizedLasso', 'LogisticRegressionCV']","210","","211","        if hasattr(estimator, \"max_iter\") and name not in external_solver:","212","            yield _named_check(","213","                check_transformer_n_iter, name), name, estimator","214","","215","","216","def test_get_params_invariance():","217","    # Test for estimators that support get_params, that","218","    # get_params(deep=False) is a subset of get_params(deep=True)","219","    # Related to issue #4465","220","","221","    estimators = all_estimators(include_meta_estimators=False,","222","                                include_other=True)","223","    for name, Estimator in estimators:","224","        if hasattr(Estimator, 'get_params'):","225","            # If class is deprecated, ignore deprecated warnings","226","            if hasattr(Estimator.__init__, \"deprecated_original\"):","227","                with ignore_warnings():","228","                    yield _named_check(","229","                        check_get_params_invariance, name), name, Estimator","230","            else:","231","                yield _named_check(","232","                    check_get_params_invariance, name), name, Estimator"]}]}},"f61d96c3129ce462f6cf10a5de596167817a60c9":{"changes":{"sklearn\/datasets\/kddcup99.py":"MODIFY"},"diff":{"sklearn\/datasets\/kddcup99.py":[{"add":["47","    created by MIT Lincoln Lab [1]. The artificial data was generated using","136","    percent10 : bool, default=True","157","    .. [2] K. Yamanishi, J.-I. Takeuchi, G. Williams, and P. Milne. Online","158","           unsupervised outlier detection using finite mixtures with","159","           discounting learning algorithms. In Proceedings of the sixth","160","           ACM SIGKDD international conference on Knowledge discovery","161","           and data mining, pages 320-324. ACM Press, 2000.","218","                          shuffle=False, percent10=True):","246","    percent10 : bool, default=True"],"delete":["47","    created by MIT Lincoln Lab [1] . The artificial data was generated using","136","    percent10 : bool, default=False","157","    .. [2] A Geometric Framework for Unsupervised Anomaly Detection: Detecting","158","           Intrusions in Unlabeled Data (2002) by Eleazar Eskin, Andrew Arnold,","159","           Michael Prerau, Leonid Portnoy, Sal Stolfo","216","                          shuffle=False, percent10=False):","244","    percent10 : bool, default=False"]}]}},"46913adf0757d1a6cae3fff0210a973e9d995bac":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/utils\/estimator_checks.py":"MODIFY","sklearn\/utils\/_unittest_backport.py":"MODIFY","sklearn\/utils\/tests\/test_estimator_checks.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["862","- Allow :func:`utils.estimator_checks.check_estimator` to check that there is no","866","- The set of checks in :func:`utils.estimator_checks.check_estimator` now includes a","867","  ``check_set_params`` test which checks that ``set_params`` is equivalent to","868","  passing parameters in ``__init__`` and warns if it encounters parameter","869","  validation. :issue:`7738` by :user:`Alvin Chiang <absolutelyNoWarranty>`","870","  ","871","- Add invariance tests for clustering metrics. :issue:`8102` by :user:`Ankita","872","  Sinha <anki08>` and :user:`Guillaume Lemaitre <glemaitre>`.","873",""],"delete":["862","- Allow :func:`~utils.estimator_checks.check_estimator` to check that there is no"]}],"sklearn\/utils\/estimator_checks.py":[{"add":["263","    yield check_set_params","2184","def check_set_params(name, estimator_orig):","2185","    # Check that get_params() returns the same thing","2186","    # before and after set_params() with some fuzz","2187","    estimator = clone(estimator_orig)","2188","","2189","    orig_params = estimator.get_params(deep=False)","2190","    msg = (\"get_params result does not match what was passed to set_params\")","2191","","2192","    estimator.set_params(**orig_params)","2193","    curr_params = estimator.get_params(deep=False)","2194","    assert_equal(set(orig_params.keys()), set(curr_params.keys()), msg)","2195","    for k, v in curr_params.items():","2196","        assert orig_params[k] is v, msg","2197","","2198","    # some fuzz values","2199","    test_values = [-np.inf, np.inf, None]","2200","","2201","    test_params = deepcopy(orig_params)","2202","    for param_name in orig_params.keys():","2203","        default_value = orig_params[param_name]","2204","        for value in test_values:","2205","            test_params[param_name] = value","2206","            try:","2207","                estimator.set_params(**test_params)","2208","            except (TypeError, ValueError) as e:","2209","                e_type = e.__class__.__name__","2210","                # Exception occurred, possibly parameter validation","2211","                warnings.warn(\"{} occurred during set_params. \"","2212","                              \"It is recommended to delay parameter \"","2213","                              \"validation until fit.\".format(e_type))","2214","","2215","                change_warning_msg = \"Estimator's parameters changed after \" \\","2216","                                     \"set_params raised {}\".format(e_type)","2217","                params_before_exception = curr_params","2218","                curr_params = estimator.get_params(deep=False)","2219","                try:","2220","                    assert_equal(set(params_before_exception.keys()),","2221","                                 set(curr_params.keys()))","2222","                    for k, v in curr_params.items():","2223","                        assert params_before_exception[k] is v","2224","                except AssertionError:","2225","                    warnings.warn(change_warning_msg)","2226","            else:","2227","                curr_params = estimator.get_params(deep=False)","2228","                assert_equal(set(test_params.keys()),","2229","                             set(curr_params.keys()),","2230","                             msg)","2231","                for k, v in curr_params.items():","2232","                    assert test_params[k] is v, msg","2233","        test_params[param_name] = default_value","2234","","2235","","2236","@ignore_warnings(category=(DeprecationWarning, FutureWarning))"],"delete":[]}],"sklearn\/utils\/_unittest_backport.py":[{"add":["151","    longMessage = True"],"delete":["151","    longMessage = False"]}],"sklearn\/utils\/tests\/test_estimator_checks.py":[{"add":["12","                                   assert_equal, ignore_warnings,","13","                                   assert_warns)","89","class RaisesErrorInSetParams(BaseEstimator):","90","    def __init__(self, p=0):","91","        self.p = p","92","","93","    def set_params(self, **kwargs):","94","        if 'p' in kwargs:","95","            p = kwargs.pop('p')","96","            if p < 0:","97","                raise ValueError(\"p can't be less than 0\")","98","            self.p = p","99","        return super(RaisesErrorInSetParams, self).set_params(**kwargs)","100","","101","    def fit(self, X, y=None):","102","        X, y = check_X_y(X, y)","103","        return self","104","","105","","106","class ModifiesValueInsteadOfRaisingError(BaseEstimator):","107","    def __init__(self, p=0):","108","        self.p = p","109","","110","    def set_params(self, **kwargs):","111","        if 'p' in kwargs:","112","            p = kwargs.pop('p')","113","            if p < 0:","114","                p = 0","115","            self.p = p","116","        return super(ModifiesValueInsteadOfRaisingError,","117","                     self).set_params(**kwargs)","118","","119","    def fit(self, X, y=None):","120","        X, y = check_X_y(X, y)","121","        return self","122","","123","","124","class ModifiesAnotherValue(BaseEstimator):","125","    def __init__(self, a=0, b='method1'):","126","        self.a = a","127","        self.b = b","128","","129","    def set_params(self, **kwargs):","130","        if 'a' in kwargs:","131","            a = kwargs.pop('a')","132","            self.a = a","133","            if a is None:","134","                kwargs.pop('b')","135","                self.b = 'method2'","136","        return super(ModifiesAnotherValue,","137","                     self).set_params(**kwargs)","138","","139","    def fit(self, X, y=None):","140","        X, y = check_X_y(X, y)","141","        return self","142","","143","","277","    # check that values returned by get_params match set_params","278","    msg = \"get_params result does not match what was passed to set_params\"","279","    assert_raises_regex(AssertionError, msg, check_estimator,","280","                        ModifiesValueInsteadOfRaisingError())","281","    assert_warns(UserWarning, check_estimator, RaisesErrorInSetParams())","282","    assert_raises_regex(AssertionError, msg, check_estimator,","283","                        ModifiesAnotherValue())"],"delete":["12","                                   assert_equal, ignore_warnings)"]}]}},"02cc6f5d3a4363fcc197b511f0fdfe27a7497d5b":{"changes":{"doc\/whats_new.rst":"MODIFY","sklearn\/utils\/estimator_checks.py":"MODIFY","sklearn\/decomposition\/nmf.py":"MODIFY","sklearn\/utils\/tests\/test_estimator_checks.py":"MODIFY"},"diff":{"doc\/whats_new.rst":[{"add":["67","   - ``check_estimator`` now attempts to ensure that methods transform, predict, etc.","68","     do not set attributes on the estimator.","69","     :issue:`7533` by `Ekaterina Krivich`_.","70","","74","   - Fix a bug where :class:`sklearn.feature_selection.SelectFdr` did not","94","   - Fixed a bug where :class:`decomposition.NMF` sets its ``n_iters_``","95","     attribute in `transform()`. :issue:`7553` by `Ekaterina Krivich`_.","96",""],"delete":["70","   - Fix a bug where :class:`sklearn.feature_selection.SelectFdr` did not "]}],"sklearn\/utils\/estimator_checks.py":[{"add":["32","from sklearn.utils.testing import assert_dict_equal","233","    yield check_dict_unchanged","413","def check_dict_unchanged(name, Estimator):","414","    # this estimator raises","415","    # ValueError: Found array with 0 feature(s) (shape=(23, 0))","416","    # while a minimum of 1 is required.","417","    # error","418","    if name in ['SpectralCoclustering']:","419","        return","420","    rnd = np.random.RandomState(0)","421","    if name in ['RANSACRegressor']:","422","        X = 3 * rnd.uniform(size=(20, 3))","423","    else:","424","        X = 2 * rnd.uniform(size=(20, 3))","425","","426","    y = X[:, 0].astype(np.int)","427","    y = multioutput_estimator_convert_y_2d(name, y)","428","    estimator = Estimator()","429","    set_testing_parameters(estimator)","430","    if hasattr(estimator, \"n_components\"):","431","        estimator.n_components = 1","432","","433","    if hasattr(estimator, \"n_clusters\"):","434","        estimator.n_clusters = 1","435","","436","    if hasattr(estimator, \"n_best\"):","437","        estimator.n_best = 1","438","","439","    set_random_state(estimator, 1)","440","","441","    # should be just `estimator.fit(X, y)`","442","    # after merging #6141","443","    if name in ['SpectralBiclustering']:","444","        estimator.fit(X)","445","    else:","446","        estimator.fit(X, y)","447","    for method in [\"predict\", \"transform\", \"decision_function\",","448","                   \"predict_proba\"]:","449","        if hasattr(estimator, method):","450","            dict_before = estimator.__dict__.copy()","451","            getattr(estimator, method)(X)","452","            assert_dict_equal(estimator.__dict__, dict_before,","453","                              'Estimator changes __dict__ during %s' % method)","454","","455",""],"delete":[]}],"sklearn\/decomposition\/nmf.py":[{"add":[],"delete":["1018","        Attributes","1019","        ----------","1020","        components_ : array-like, shape (n_components, n_features)","1021","            Factorization matrix, sometimes called 'dictionary'.","1022","","1023","        n_iter_ : int","1024","            Actual number of iterations for the transform.","1025","","1063","        Attributes","1064","        ----------","1065","        components_ : array-like, shape (n_components, n_features)","1066","            Factorization matrix, sometimes called 'dictionary'.","1067","","1068","        n_iter_ : int","1069","            Actual number of iterations for the transform.","1070","","1086","        Attributes","1087","        ----------","1088","        n_iter_ : int","1089","            Actual number of iterations for the transform.","1090","","1108","        self.n_iter_ = n_iter_"]}],"sklearn\/utils\/tests\/test_estimator_checks.py":[{"add":["30","class ChangesDict(BaseEstimator):","31","    def __init__(self):","32","        self.key = 0","33","","34","    def fit(self, X, y=None):","35","        X, y = check_X_y(X, y)","36","        return self","37","","38","    def predict(self, X):","39","        X = check_array(X)","40","        self.key = 1000","41","        return np.ones(X.shape[0])","42","","43","","91","    # check that estimator state does not change","92","    # at transform\/predict\/predict_proba time","93","    msg = 'Estimator changes __dict__ during predict'","94","    assert_raises_regex(AssertionError, msg, check_estimator, ChangesDict)","95",""],"delete":[]}]}},"2ddf0ce6330da02554cac1ac86af0990bb721c48":{"changes":{"sklearn\/feature_selection\/rfe.py":"MODIFY","sklearn\/feature_selection\/tests\/test_rfe.py":"MODIFY"},"diff":{"sklearn\/feature_selection\/rfe.py":[{"add":["408","                  step=self.step, verbose=self.verbose)"],"delete":["408","                  step=self.step, verbose=self.verbose - 1)"]}],"sklearn\/feature_selection\/tests\/test_rfe.py":[{"add":["205","def test_rfecv_verbose_output():","206","    # Check verbose=1 is producing an output.","207","    from sklearn.externals.six.moves import cStringIO as StringIO","208","    import sys","209","    sys.stdout = StringIO()","210","","211","    generator = check_random_state(0)","212","    iris = load_iris()","213","    X = np.c_[iris.data, generator.normal(size=(len(iris.data), 6))]","214","    y = list(iris.target)","215","","216","    rfecv = RFECV(estimator=SVC(kernel=\"linear\"), step=1, cv=5, verbose=1)","217","    rfecv.fit(X, y)","218","","219","    verbose_output = sys.stdout","220","    verbose_output.seek(0)","221","    assert_greater(len(verbose_output.readline()), 0)","222","","223",""],"delete":[]}]}},"38f6a91566bc643e2a8f76beb16f3e673faab848":{"changes":{"sklearn\/model_selection\/_split.py":"MODIFY","sklearn\/model_selection\/_validation.py":"MODIFY","sklearn\/model_selection\/tests\/test_search.py":"MODIFY","sklearn\/model_selection\/tests\/test_split.py":"MODIFY","sklearn\/model_selection\/tests\/test_validation.py":"MODIFY","sklearn\/model_selection\/tests\/common.py":"ADD","sklearn\/model_selection\/_search.py":"MODIFY"},"diff":{"sklearn\/model_selection\/_split.py":[{"add":["1479","        self.cv = list(cv)"],"delete":["1479","        self.cv = cv"]}],"sklearn\/model_selection\/_validation.py":[{"add":["130","    cv_iter = list(cv.split(X, y, groups))","139","                      for train, test in cv_iter)","387","    cv_iter = list(cv.split(X, y, groups))","400","        for train, test in cv_iter)","755","    cv_iter = list(cv.split(X, y, groups))","778","            clone(estimator), X, y, classes, train, test, train_sizes_abs,","779","            scorer, verbose) for train, test in cv_iter)","963","    cv_iter = list(cv.split(X, y, groups))","972","        for train, test in cv_iter for v in param_range)"],"delete":["0","","139","                      for train, test in cv.split(X, y, groups))","399","        for train, test in cv.split(X, y, groups))","753","    cv_iter = cv.split(X, y, groups)","755","    cv_iter = list(cv_iter)","778","            clone(estimator), X, y, classes, train,","779","            test, train_sizes_abs, scorer, verbose)","780","            for train, test in cv_iter)","972","        for train, test in cv.split(X, y, groups) for v in param_range)"]}],"sklearn\/model_selection\/tests\/test_search.py":[{"add":["62","from sklearn.model_selection.tests.common import OneTimeSplitter","63","","1158","","1159","","1160","def test_grid_search_cv_splits_consistency():","1161","    # Check if a one time iterable is accepted as a cv parameter.","1162","    n_samples = 100","1163","    n_splits = 5","1164","    X, y = make_classification(n_samples=n_samples, random_state=0)","1165","","1166","    gs = GridSearchCV(LinearSVC(random_state=0),","1167","                      param_grid={'C': [0.1, 0.2, 0.3]},","1168","                      cv=OneTimeSplitter(n_splits=n_splits,","1169","                                         n_samples=n_samples))","1170","    gs.fit(X, y)","1171","","1172","    gs2 = GridSearchCV(LinearSVC(random_state=0),","1173","                       param_grid={'C': [0.1, 0.2, 0.3]},","1174","                       cv=KFold(n_splits=n_splits))","1175","    gs2.fit(X, y)","1176","","1177","    def _pop_time_keys(cv_results):","1178","        for key in ('mean_fit_time', 'std_fit_time',","1179","                    'mean_score_time', 'std_score_time'):","1180","            cv_results.pop(key)","1181","        return cv_results","1182","","1183","    # OneTimeSplitter is a non-re-entrant cv where split can be called only","1184","    # once if ``cv.split`` is called once per param setting in GridSearchCV.fit","1185","    # the 2nd and 3rd parameter will not be evaluated as no train\/test indices","1186","    # will be generated for the 2nd and subsequent cv.split calls.","1187","    # This is a check to make sure cv.split is not called once per param","1188","    # setting.","1189","    np.testing.assert_equal(_pop_time_keys(gs.cv_results_),","1190","                            _pop_time_keys(gs2.cv_results_))","1191","","1192","    # Check consistency of folds across the parameters","1193","    gs = GridSearchCV(LinearSVC(random_state=0),","1194","                      param_grid={'C': [0.1, 0.1, 0.2, 0.2]},","1195","                      cv=KFold(n_splits=n_splits, shuffle=True))","1196","    gs.fit(X, y)","1197","","1198","    # As the first two param settings (C=0.1) and the next two param","1199","    # settings (C=0.2) are same, the test and train scores must also be","1200","    # same as long as the same train\/test indices are generated for all","1201","    # the cv splits, for both param setting","1202","    for score_type in ('train', 'test'):","1203","        per_param_scores = {}","1204","        for param_i in range(4):","1205","            per_param_scores[param_i] = list(","1206","                gs.cv_results_['split%d_%s_score' % (s, score_type)][param_i]","1207","                for s in range(5))","1208","","1209","        assert_array_almost_equal(per_param_scores[0],","1210","                                  per_param_scores[1])","1211","        assert_array_almost_equal(per_param_scores[2],","1212","                                  per_param_scores[3])"],"delete":[]}],"sklearn\/model_selection\/tests\/test_split.py":[{"add":["871","    kf_iter = KFold(n_splits=5).split(X, y)","872","    kf_iter_wrapped = check_cv(kf_iter)","873","    # Since the wrapped iterable is enlisted and stored,","874","    # split can be called any number of times to produce","875","    # consistent results.","876","    assert_array_equal(list(kf_iter_wrapped.split(X, y)),","877","                       list(kf_iter_wrapped.split(X, y)))","878","    # If the splits are randomized, successive calls to split yields different","879","    # results","880","    kf_randomized_iter = KFold(n_splits=5, shuffle=True).split(X, y)","881","    kf_randomized_iter_wrapped = check_cv(kf_randomized_iter)","882","    assert_array_equal(list(kf_randomized_iter_wrapped.split(X, y)),","883","                       list(kf_randomized_iter_wrapped.split(X, y)))","884","    assert_true(np.any(np.array(list(kf_iter_wrapped.split(X, y))) !=","885","                       np.array(list(kf_randomized_iter_wrapped.split(X, y)))))","886",""],"delete":["61","P_sparse = coo_matrix(np.eye(5))","65","class MockClassifier(object):","66","    \"\"\"Dummy classifier to test the cross-validation\"\"\"","67","","68","    def __init__(self, a=0, allow_nd=False):","69","        self.a = a","70","        self.allow_nd = allow_nd","71","","72","    def fit(self, X, Y=None, sample_weight=None, class_prior=None,","73","            sparse_sample_weight=None, sparse_param=None, dummy_int=None,","74","            dummy_str=None, dummy_obj=None, callback=None):","75","        \"\"\"The dummy arguments are to test that this fit function can","76","        accept non-array arguments through cross-validation, such as:","77","            - int","78","            - str (this is actually array-like)","79","            - object","80","            - function","81","        \"\"\"","82","        self.dummy_int = dummy_int","83","        self.dummy_str = dummy_str","84","        self.dummy_obj = dummy_obj","85","        if callback is not None:","86","            callback(self)","87","","88","        if self.allow_nd:","89","            X = X.reshape(len(X), -1)","90","        if X.ndim >= 3 and not self.allow_nd:","91","            raise ValueError('X cannot be d')","92","        if sample_weight is not None:","93","            assert_true(sample_weight.shape[0] == X.shape[0],","94","                        'MockClassifier extra fit_param sample_weight.shape[0]'","95","                        ' is {0}, should be {1}'.format(sample_weight.shape[0],","96","                                                        X.shape[0]))","97","        if class_prior is not None:","98","            assert_true(class_prior.shape[0] == len(np.unique(y)),","99","                        'MockClassifier extra fit_param class_prior.shape[0]'","100","                        ' is {0}, should be {1}'.format(class_prior.shape[0],","101","                                                        len(np.unique(y))))","102","        if sparse_sample_weight is not None:","103","            fmt = ('MockClassifier extra fit_param sparse_sample_weight'","104","                   '.shape[0] is {0}, should be {1}')","105","            assert_true(sparse_sample_weight.shape[0] == X.shape[0],","106","                        fmt.format(sparse_sample_weight.shape[0], X.shape[0]))","107","        if sparse_param is not None:","108","            fmt = ('MockClassifier extra fit_param sparse_param.shape '","109","                   'is ({0}, {1}), should be ({2}, {3})')","110","            assert_true(sparse_param.shape == P_sparse.shape,","111","                        fmt.format(sparse_param.shape[0],","112","                                   sparse_param.shape[1],","113","                                   P_sparse.shape[0], P_sparse.shape[1]))","114","        return self","115","","116","    def predict(self, T):","117","        if self.allow_nd:","118","            T = T.reshape(len(T), -1)","119","        return T[:, 0]","120","","121","    def score(self, X=None, Y=None):","122","        return 1. \/ (1 + np.abs(self.a))","123","","124","    def get_params(self, deep=False):","125","        return {'a': self.a, 'allow_nd': self.allow_nd}","126","","127",""]}],"sklearn\/model_selection\/tests\/test_validation.py":[{"add":["62","from sklearn.model_selection.tests.common import OneTimeSplitter","133","class MockClassifier(object):","134","    \"\"\"Dummy classifier to test the cross-validation\"\"\"","135","","136","    def __init__(self, a=0, allow_nd=False):","137","        self.a = a","138","        self.allow_nd = allow_nd","139","","140","    def fit(self, X, Y=None, sample_weight=None, class_prior=None,","141","            sparse_sample_weight=None, sparse_param=None, dummy_int=None,","142","            dummy_str=None, dummy_obj=None, callback=None):","143","        \"\"\"The dummy arguments are to test that this fit function can","144","        accept non-array arguments through cross-validation, such as:","145","            - int","146","            - str (this is actually array-like)","147","            - object","148","            - function","149","        \"\"\"","150","        self.dummy_int = dummy_int","151","        self.dummy_str = dummy_str","152","        self.dummy_obj = dummy_obj","153","        if callback is not None:","154","            callback(self)","155","","156","        if self.allow_nd:","157","            X = X.reshape(len(X), -1)","158","        if X.ndim >= 3 and not self.allow_nd:","159","            raise ValueError('X cannot be d')","160","        if sample_weight is not None:","161","            assert_true(sample_weight.shape[0] == X.shape[0],","162","                        'MockClassifier extra fit_param sample_weight.shape[0]'","163","                        ' is {0}, should be {1}'.format(sample_weight.shape[0],","164","                                                        X.shape[0]))","165","        if class_prior is not None:","166","            assert_true(class_prior.shape[0] == len(np.unique(y)),","167","                        'MockClassifier extra fit_param class_prior.shape[0]'","168","                        ' is {0}, should be {1}'.format(class_prior.shape[0],","169","                                                        len(np.unique(y))))","170","        if sparse_sample_weight is not None:","171","            fmt = ('MockClassifier extra fit_param sparse_sample_weight'","172","                   '.shape[0] is {0}, should be {1}')","173","            assert_true(sparse_sample_weight.shape[0] == X.shape[0],","174","                        fmt.format(sparse_sample_weight.shape[0], X.shape[0]))","175","        if sparse_param is not None:","176","            fmt = ('MockClassifier extra fit_param sparse_param.shape '","177","                   'is ({0}, {1}), should be ({2}, {3})')","178","            assert_true(sparse_param.shape == P_sparse.shape,","179","                        fmt.format(sparse_param.shape[0],","180","                                   sparse_param.shape[1],","181","                                   P_sparse.shape[0], P_sparse.shape[1]))","182","        return self","183","","184","    def predict(self, T):","185","        if self.allow_nd:","186","            T = T.reshape(len(T), -1)","187","        return T[:, 0]","188","","189","    def score(self, X=None, Y=None):","190","        return 1. \/ (1 + np.abs(self.a))","191","","192","    def get_params(self, deep=False):","193","        return {'a': self.a, 'allow_nd': self.allow_nd}","194","","195","","204","P_sparse = coo_matrix(np.eye(5))","622","    n_samples = 30","623","    n_splits = 3","624","    X, y = make_classification(n_samples=n_samples, n_features=1,","625","                               n_informative=1, n_redundant=0, n_classes=2,","627","    estimator = MockImprovingEstimator(n_samples * ((n_splits - 1) \/ n_splits))","631","                estimator, X, y, cv=KFold(n_splits=n_splits),","632","                train_sizes=np.linspace(0.1, 1.0, 10),","644","        # Test a custom cv splitter that can iterate only once","645","        with warnings.catch_warnings(record=True) as w:","646","            train_sizes2, train_scores2, test_scores2 = learning_curve(","647","                estimator, X, y,","648","                cv=OneTimeSplitter(n_splits=n_splits, n_samples=n_samples),","649","                train_sizes=np.linspace(0.1, 1.0, 10),","650","                shuffle=shuffle_train)","651","        if len(w) > 0:","652","            raise RuntimeError(\"Unexpected warning: %r\" % w[0].message)","653","        assert_array_almost_equal(train_scores2, train_scores)","654","        assert_array_almost_equal(test_scores2, test_scores)","655","","847","def test_validation_curve_cv_splits_consistency():","848","    n_samples = 100","849","    n_splits = 5","850","    X, y = make_classification(n_samples=100, random_state=0)","851","","852","    scores1 = validation_curve(SVC(kernel='linear', random_state=0), X, y,","853","                               'C', [0.1, 0.1, 0.2, 0.2],","854","                               cv=OneTimeSplitter(n_splits=n_splits,","855","                                                  n_samples=n_samples))","856","    # The OneTimeSplitter is a non-re-entrant cv splitter. Unless, the","857","    # `split` is called for each parameter, the following should produce","858","    # identical results for param setting 1 and param setting 2 as both have","859","    # the same C value.","860","    assert_array_almost_equal(*np.vsplit(np.hstack(scores1)[(0, 2, 1, 3), :],","861","                                         2))","862","","863","    scores2 = validation_curve(SVC(kernel='linear', random_state=0), X, y,","864","                               'C', [0.1, 0.1, 0.2, 0.2],","865","                               cv=KFold(n_splits=n_splits, shuffle=True))","866","","867","    # For scores2, compare the 1st and 2nd parameter's scores","868","    # (Since the C value for 1st two param setting is 0.1, they must be","869","    # consistent unless the train test folds differ between the param settings)","870","    assert_array_almost_equal(*np.vsplit(np.hstack(scores2)[(0, 2, 1, 3), :],","871","                                         2))","872","","873","    scores3 = validation_curve(SVC(kernel='linear', random_state=0), X, y,","874","                               'C', [0.1, 0.1, 0.2, 0.2],","875","                               cv=KFold(n_splits=n_splits))","876","","877","    # OneTimeSplitter is basically unshuffled KFold(n_splits=5). Sanity check.","878","    assert_array_almost_equal(np.array(scores3), np.array(scores1))","879","","880",""],"delete":["62","from sklearn.model_selection.tests.test_split import MockClassifier","558","    X, y = make_classification(n_samples=30, n_features=1, n_informative=1,","559","                               n_redundant=0, n_classes=2,","561","    estimator = MockImprovingEstimator(20)","565","                estimator, X, y, cv=3, train_sizes=np.linspace(0.1, 1.0, 10),"]}],"sklearn\/model_selection\/tests\/common.py":[{"add":[],"delete":[]}],"sklearn\/model_selection\/_search.py":[{"add":["552","        cv_iter = list(cv.split(X, y, groups))","564","          for train, test in cv_iter)"],"delete":["563","          for train, test in cv.split(X, y, groups))"]}]}},"fcb706a7c0074a16bd472ca284145be2e0f7c936":{"changes":{"sklearn\/linear_model\/tests\/test_coordinate_descent.py":"MODIFY","sklearn\/src\/cblas\/cblas_sger.c":"ADD","sklearn\/src\/cblas\/ATL_srefger.c":"ADD","sklearn\/src\/cblas\/atlas_ptalias1.h":"ADD","sklearn\/src\/cblas\/cblas_sgemv.c":"ADD","sklearn\/linear_model\/coordinate_descent.py":"MODIFY","sklearn\/src\/cblas\/atlas_ptalias2.h":"ADD","sklearn\/src\/cblas\/ATL_srefgemv.c":"ADD","sklearn\/src\/cblas\/ATL_srefgemvN.c":"ADD","sklearn\/src\/cblas\/cblas_sscal.c":"ADD","sklearn\/linear_model\/cd_fast.pyx":"MODIFY","sklearn\/src\/cblas\/ATL_srefgemvT.c":"ADD"},"diff":{"sklearn\/linear_model\/tests\/test_coordinate_descent.py":[{"add":["693","                coef[('simple', dtype)] = clf.coef_","694","                intercept[('simple', dtype)] = clf.intercept_","709","                # test multi task enet","710","                multi_y = np.hstack((y[:, np.newaxis], y[:, np.newaxis]))","711","                clf_multioutput = MultiTaskElasticNet(","712","                    alpha=0.5, max_iter=100, fit_intercept=fit_intercept,","713","                    normalize=normalize)","714","                clf_multioutput.fit(X, multi_y)","715","                coef[('multi', dtype)] = clf_multioutput.coef_","716","                intercept[('multi', dtype)] = clf_multioutput.intercept_","717","                assert_equal(clf.coef_.dtype, dtype)","718","","719","            for v in ['simple', 'multi']:","720","                assert_array_almost_equal(coef[(v, np.float32)],","721","                                          coef[(v, np.float64)],","722","                                          decimal=4)","723","                assert_array_almost_equal(intercept[(v, np.float32)],","724","                                          intercept[(v, np.float64)],","725","                                          decimal=4)"],"delete":["693","                coef[dtype] = clf.coef_","694","                intercept[dtype] = clf.intercept_","709","            assert_array_almost_equal(coef[np.float32], coef[np.float64],","710","                                      decimal=4)","711","            assert_array_almost_equal(intercept[np.float32],","712","                                      intercept[np.float64],","713","                                      decimal=4)"]}],"sklearn\/src\/cblas\/cblas_sger.c":[{"add":[],"delete":[]}],"sklearn\/src\/cblas\/ATL_srefger.c":[{"add":[],"delete":[]}],"sklearn\/src\/cblas\/atlas_ptalias1.h":[{"add":[],"delete":[]}],"sklearn\/src\/cblas\/cblas_sgemv.c":[{"add":[],"delete":[]}],"sklearn\/linear_model\/coordinate_descent.py":[{"add":["461","                precompute = check_array(precompute, dtype=X.dtype.type,","735","        # workaround since _set_intercept will cast self.coef_ into X.dtype","1040","            Training data. Pass directly as Fortran-contiguous data","1047","        y = check_array(y, copy=False, dtype=[np.float64, np.float32],","1048","                        ensure_2d=False)","1090","            # Let us not impose fortran ordering so far: it is","1104","            X = check_array(X, 'csc', dtype=[np.float64, np.float32],","1105","                            order='F', copy=copy_X)","1159","                                         dtype=X.dtype.type)","1679","        X = check_array(X, dtype=[np.float64, np.float32], order='F',","1681","        y = check_array(y, dtype=X.dtype.type, ensure_2d=False)","1701","            self.coef_ = np.zeros((n_tasks, n_features), dtype=X.dtype.type,"],"delete":["461","                precompute = check_array(precompute, dtype=np.float64,","735","        # workaround since _set_intercept will cast self.coef_ into float64","1040","            Training data. Pass directly as float64, Fortran-contiguous data","1047","        y = np.asarray(y, dtype=np.float64)","1089","            # Let us not impose fortran ordering or float64 so far: it is","1103","            X = check_array(X, 'csc', dtype=np.float64, order='F', copy=copy_X)","1157","                                         dtype=np.float64)","1677","        # X and y must be of type float64","1678","        X = check_array(X, dtype=np.float64, order='F',","1680","        y = check_array(y, dtype=np.float64, ensure_2d=False)","1700","            self.coef_ = np.zeros((n_tasks, n_features), dtype=np.float64,"]}],"sklearn\/src\/cblas\/atlas_ptalias2.h":[{"add":[],"delete":[]}],"sklearn\/src\/cblas\/ATL_srefgemv.c":[{"add":[],"delete":[]}],"sklearn\/src\/cblas\/ATL_srefgemvN.c":[{"add":[],"delete":[]}],"sklearn\/src\/cblas\/cblas_sscal.c":[{"add":[],"delete":[]}],"sklearn\/linear_model\/cd_fast.pyx":[{"add":["124","    void sger \"cblas_sger\"(CBLAS_ORDER Order, int M, int N, float alpha,","125","                           float *X, int incX, float *Y, int incY,","126","                           float *A, int lda) nogil","131","    void sgemv \"cblas_sgemv\"(CBLAS_ORDER Order, CBLAS_TRANSPOSE TransA,","132","                             int M, int N, float alpha, float *A, int lda,","133","                             float *X, int incX, float beta,","134","                             float *Y, int incY) nogil","136","    float snrm2 \"cblas_snrm2\"(int N, float *X, int incX) nogil","139","    void scopy \"cblas_scopy\"(int N, float *X, int incX, float *Y,","140","                            int incY) nogil","142","    void sscal \"cblas_sscal\"(int N, float alpha, float *X, int incX) nogil","699","def enet_coordinate_descent_multi_task(floating[::1, :] W, floating l1_reg,","700","                                       floating l2_reg,","701","                                       np.ndarray[floating, ndim=2, mode='fortran'] X,","702","                                       np.ndarray[floating, ndim=2] Y,","703","                                       int max_iter, floating tol, object rng,","713","    # fused types version of BLAS functions","714","    cdef DOT dot","715","    cdef AXPY axpy","716","    cdef ASUM asum","717","","718","    if floating is float:","719","        dtype = np.float32","720","        dot = sdot","721","        nrm2 = snrm2","722","        asum = sasum","723","        copy = scopy","724","        scal = sscal","725","        ger = sger","726","        gemv = sgemv","727","    else:","728","        dtype = np.float64","729","        dot = ddot","730","        nrm2 = dnrm2","731","        asum = dasum","732","        copy = dcopy","733","        scal = dscal","734","        ger = dger","735","        gemv = dgemv","736","","743","    cdef floating[:, ::1] XtA = np.zeros((n_features, n_tasks), dtype=dtype)","744","    cdef floating XtA_axis1norm","745","    cdef floating dual_norm_XtA","748","    cdef floating[:, ::1] R = np.zeros((n_samples, n_tasks), dtype=dtype)","750","    cdef floating[:] norm_cols_X = np.zeros(n_features, dtype=dtype)","751","    cdef floating[::1] tmp = np.zeros(n_tasks, dtype=dtype)","752","    cdef floating[:] w_ii = np.zeros(n_tasks, dtype=dtype)","753","    cdef floating d_w_max","754","    cdef floating w_max","755","    cdef floating d_w_ii","756","    cdef floating nn","757","    cdef floating W_ii_abs_max","758","    cdef floating gap = tol + 1.0","759","    cdef floating d_w_tol = tol","760","    cdef floating R_norm","761","    cdef floating w_norm","762","    cdef floating ry_sum","763","    cdef floating l21_norm","771","    cdef floating* X_ptr = &X[0, 0]","772","    cdef floating* W_ptr = &W[0, 0]","773","    cdef floating* Y_ptr = &Y[0, 0]","774","    cdef floating* wii_ptr = &w_ii[0]","790","                    dot(n_features, X_ptr + ii, n_samples, W_ptr + jj, n_tasks)","794","        tol = tol * nrm2(n_samples * n_tasks, Y_ptr, 1) ** 2","809","                copy(n_tasks, W_ptr + ii * n_tasks, 1, wii_ptr, 1)","812","                if nrm2(n_tasks, wii_ptr, 1) != 0.0:","814","                    ger(CblasRowMajor, n_samples, n_tasks, 1.0,","819","                gemv(CblasRowMajor, CblasTrans,","824","                nn = nrm2(n_tasks, &tmp[0], 1)","827","                copy(n_tasks, &tmp[0], 1, W_ptr + ii * n_tasks, 1)","828","                scal(n_tasks, fmax(1. - l1_reg \/ nn, 0) \/ (norm_cols_X[ii] + l2_reg),","832","                if nrm2(n_tasks, W_ptr + ii * n_tasks, 1) != 0.0:","835","                    ger(CblasRowMajor, n_samples, n_tasks, -1.0,","857","                        XtA[ii, jj] = dot(","866","                    XtA_axis1norm = nrm2(n_tasks, &XtA[0, 0] + ii * n_tasks, 1)","873","                R_norm = nrm2(n_samples * n_tasks, &R[0, 0], 1)","874","                w_norm = nrm2(n_features * n_tasks, W_ptr, 1)","893","                    l21_norm += nrm2(n_tasks, W_ptr + n_tasks * ii, 1)"],"delete":["688","def enet_coordinate_descent_multi_task(double[::1, :] W, double l1_reg,","689","                                       double l2_reg,","690","                                       np.ndarray[double, ndim=2, mode='fortran'] X,","691","                                       np.ndarray[double, ndim=2] Y,","692","                                       int max_iter, double tol, object rng,","708","    cdef double[:, ::1] XtA = np.zeros((n_features, n_tasks))","709","    cdef double XtA_axis1norm","710","    cdef double dual_norm_XtA","713","    cdef double[:, ::1] R = np.zeros((n_samples, n_tasks))","715","    cdef double[:] norm_cols_X = np.zeros(n_features)","716","    cdef double[::1] tmp = np.zeros(n_tasks, dtype=np.float)","717","    cdef double[:] w_ii = np.zeros(n_tasks, dtype=np.float)","718","    cdef double d_w_max","719","    cdef double w_max","720","    cdef double d_w_ii","721","    cdef double nn","722","    cdef double W_ii_abs_max","723","    cdef double gap = tol + 1.0","724","    cdef double d_w_tol = tol","725","    cdef double ry_sum","726","    cdef double l21_norm","734","    cdef double* X_ptr = &X[0, 0]","735","    cdef double* W_ptr = &W[0, 0]","736","    cdef double* Y_ptr = &Y[0, 0]","737","    cdef double* wii_ptr = &w_ii[0]","753","                    ddot(n_features, X_ptr + ii, n_samples, W_ptr + jj, n_tasks)","757","        tol = tol * dnrm2(n_samples * n_tasks, Y_ptr, 1) ** 2","772","                dcopy(n_tasks, W_ptr + ii * n_tasks, 1, wii_ptr, 1)","775","                if dnrm2(n_tasks, wii_ptr, 1) != 0.0:","777","                    dger(CblasRowMajor, n_samples, n_tasks, 1.0,","782","                dgemv(CblasRowMajor, CblasTrans,","787","                nn = dnrm2(n_tasks, &tmp[0], 1)","790","                dcopy(n_tasks, &tmp[0], 1, W_ptr + ii * n_tasks, 1)","791","                dscal(n_tasks, fmax(1. - l1_reg \/ nn, 0) \/ (norm_cols_X[ii] + l2_reg),","795","                if dnrm2(n_tasks, W_ptr + ii * n_tasks, 1) != 0.0:","798","                    dger(CblasRowMajor, n_samples, n_tasks, -1.0,","820","                        XtA[ii, jj] = ddot(","829","                    XtA_axis1norm = dnrm2(n_tasks, &XtA[0, 0] + ii * n_tasks, 1)","836","                R_norm = dnrm2(n_samples * n_tasks, &R[0, 0], 1)","837","                w_norm = dnrm2(n_features * n_tasks, W_ptr, 1)","856","                    l21_norm += dnrm2(n_tasks, W_ptr + n_tasks * ii, 1)"]}],"sklearn\/src\/cblas\/ATL_srefgemvT.c":[{"add":[],"delete":[]}]}},"dd8a6faa2a30a6df76d93b12400fececfa74c038":{"changes":{"doc\/developers\/contributing.rst":"MODIFY"},"diff":{"doc\/developers\/contributing.rst":[{"add":["187","      Two core developers will review your code and change the prefix of the pull","188","      request to ``[MRG + 1]`` and ``[MRG + 2]`` on approval, making it eligible","189","      for merging. Incomplete contributions should be prefixed ``[WIP]`` to","190","      indicate a work in progress (and changed to ``[MRG]`` when it matures).","191","      WIPs may be useful to: indicate you are working on something to avoid","192","      duplicated work, request broad review of functionality or API, or seek","193","      collaborators. WIPs often benefit from the inclusion of a"],"delete":["187","      Incomplete contributions should be prefixed ``[WIP]`` to indicate a work","188","      in progress (and changed to ``[MRG]`` when it matures). WIPs may be useful","189","      to: indicate you are working on something to avoid duplicated work,","190","      request broad review of functionality or API, or seek collaborators.","191","      WIPs often benefit from the inclusion of a"]}]}},"8daad062f1da87d1cd484d2d80d781dc21cba491":{"changes":{"sklearn\/metrics\/scorer.py":"MODIFY","sklearn\/metrics\/tests\/test_score_objects.py":"MODIFY","doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/metrics\/tests\/test_common.py":"MODIFY","doc\/modules\/classes.rst":"MODIFY","sklearn\/metrics\/classification.py":"MODIFY","doc\/modules\/model_evaluation.rst":"MODIFY","sklearn\/metrics\/__init__.py":"MODIFY"},"diff":{"sklearn\/metrics\/scorer.py":[{"add":["28","               precision_score, recall_score, log_loss, balanced_accuracy_score,","502","balanced_accuracy_scorer = make_scorer(balanced_accuracy_score)","546","               balanced_accuracy=balanced_accuracy_scorer,"],"delete":["28","               precision_score, recall_score, log_loss,"]}],"sklearn\/metrics\/tests\/test_score_objects.py":[{"add":["49","CLF_SCORERS = ['accuracy', 'balanced_accuracy',","50","               'f1', 'f1_weighted', 'f1_macro', 'f1_micro',"],"delete":["49","CLF_SCORERS = ['accuracy', 'f1', 'f1_weighted', 'f1_macro', 'f1_micro',"]}],"doc\/whats_new\/v0.20.rst":[{"add":["42","  ","43","Model evaluation","44","","45","- Added the :func:`metrics.balanced_accuracy` metric and a corresponding","46","  ``'balanced_accuracy'`` scorer for binary classification.","47","  :issue:`8066` by :user:`xyguo` and :user:`Aman Dalmia <dalmia>`."],"delete":[]}],"sklearn\/metrics\/tests\/test_common.py":[{"add":["28","from sklearn.metrics import balanced_accuracy_score","104","    \"balanced_accuracy_score\": balanced_accuracy_score,","216","    \"balanced_accuracy_score\",","358","    \"balanced_accuracy_score\","],"delete":[]}],"doc\/modules\/classes.rst":[{"add":["781","   metrics.balanced_accuracy_score"],"delete":[]}],"sklearn\/metrics\/classification.py":[{"add":["1366","def balanced_accuracy_score(y_true, y_pred, sample_weight=None):","1367","    \"\"\"Compute the balanced accuracy","1368","","1369","    The balanced accuracy is used in binary classification problems to deal","1370","    with imbalanced datasets. It is defined as the arithmetic mean of","1371","    sensitivity (true positive rate) and specificity (true negative rate),","1372","    or the average recall obtained on either class. It is also equal to the","1373","    ROC AUC score given binary inputs.","1374","","1375","    The best value is 1 and the worst value is 0.","1376","","1377","    Read more in the :ref:`User Guide <balanced_accuracy_score>`.","1378","","1379","    Parameters","1380","    ----------","1381","    y_true : 1d array-like","1382","        Ground truth (correct) target values.","1383","","1384","    y_pred : 1d array-like","1385","        Estimated targets as returned by a classifier.","1386","","1387","    sample_weight : array-like of shape = [n_samples], optional","1388","        Sample weights.","1389","","1390","    Returns","1391","    -------","1392","    balanced_accuracy : float.","1393","        The average of sensitivity and specificity","1394","","1395","    See also","1396","    --------","1397","    recall_score, roc_auc_score","1398","","1399","    References","1400","    ----------","1401","    .. [1] Brodersen, K.H.; Ong, C.S.; Stephan, K.E.; Buhmann, J.M. (2010).","1402","           The balanced accuracy and its posterior distribution.","1403","           Proceedings of the 20th International Conference on Pattern","1404","           Recognition, 3121-24.","1405","","1406","    Examples","1407","    --------","1408","    >>> from sklearn.metrics import balanced_accuracy_score","1409","    >>> y_true = [0, 1, 0, 0, 1, 0]","1410","    >>> y_pred = [0, 1, 0, 0, 0, 1]","1411","    >>> balanced_accuracy_score(y_true, y_pred)","1412","    0.625","1413","","1414","    \"\"\"","1415","    y_type, y_true, y_pred = _check_targets(y_true, y_pred)","1416","","1417","    if y_type != 'binary':","1418","        raise ValueError('Balanced accuracy is only meaningful '","1419","                         'for binary classification problems.')","1420","    # simply wrap the ``recall_score`` function","1421","    return recall_score(y_true, y_pred,","1422","                        pos_label=None,","1423","                        average='macro',","1424","                        sample_weight=sample_weight)","1425","","1426",""],"delete":[]}],"doc\/modules\/model_evaluation.rst":[{"add":["61","'balanced_accuracy'               :func:`metrics.balanced_accuracy_score`           for binary targets","106","    ValueError: 'wrong_choice' is not a valid scoring value. Valid options are ['accuracy', 'adjusted_mutual_info_score', 'adjusted_rand_score', 'average_precision', 'balanced_accuracy', 'brier_score_loss', 'completeness_score', 'explained_variance', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'fowlkes_mallows_score', 'homogeneity_score', 'mutual_info_score', 'neg_log_loss', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_mean_squared_log_error', 'neg_median_absolute_error', 'normalized_mutual_info_score', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'r2', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'roc_auc', 'v_measure_score']","282","   balanced_accuracy_score","415",".. _balanced_accuracy_score:","416","","417","Balanced accuracy score","418","-----------------------","419","","420","The :func:`balanced_accuracy_score` function computes the","421","`balanced accuracy <https:\/\/en.wikipedia.org\/wiki\/Accuracy_and_precision>`_, which","422","avoids inflated performance estimates on imbalanced datasets. It is defined as the","423","arithmetic mean of `sensitivity <https:\/\/en.wikipedia.org\/wiki\/Sensitivity_and_specificity>`_","424","(true positive rate) and `specificity <https:\/\/en.wikipedia.org\/wiki\/Sensitivity_and_specificity>`_","425","(true negative rate), or the average of `recall scores <https:\/\/en.wikipedia.org\/wiki\/Precision_and_recall>`_","426","obtained on either class.","427","","428","If the classifier performs equally well on either class, this term reduces to the","429","conventional accuracy (i.e., the number of correct predictions divided by the total","430","number of predictions). In contrast, if the conventional accuracy is above chance only","431","because the classifier takes advantage of an imbalanced test set, then the balanced","432","accuracy, as appropriate, will drop to 50%.","433","","434","If :math:`\\hat{y}_i\\in\\{0,1\\}` is the predicted value of","435","the :math:`i`-th sample and :math:`y_i\\in\\{0,1\\}` is the corresponding true value,","436","then the balanced accuracy is defined as","437","","438",".. math::","439","","440","   \\texttt{balanced-accuracy}(y, \\hat{y}) = \\frac{1}{2} \\left(\\frac{\\sum_i 1(\\hat{y}_i = 1 \\land y_i = 1)}{\\sum_i 1(y_i = 1)} + \\frac{\\sum_i 1(\\hat{y}_i = 0 \\land y_i = 0)}{\\sum_i 1(y_i = 0)}\\right)","441","","442","where :math:`1(x)` is the `indicator function <https:\/\/en.wikipedia.org\/wiki\/Indicator_function>`_.","443","","444","Under this definition, the balanced accuracy coincides with :func:`roc_auc_score`","445","given binary ``y_true`` and ``y_pred``:","446","","447","  >>> import numpy as np","448","  >>> from sklearn.metrics import balanced_accuracy_score, roc_auc_score","449","  >>> y_true = [0, 1, 0, 0, 1, 0]","450","  >>> y_pred = [0, 1, 0, 0, 0, 1]","451","  >>> balanced_accuracy_score(y_true, y_pred)","452","  0.625","453","  >>> roc_auc_score(y_true, y_pred)","454","  0.625","455","","456","(but in general, :func:`roc_auc_score` takes as its second argument non-binary scores).","457","","458",".. note::","459","","460","    Currently this score function is only defined for binary classification problems, you","461","    may need to wrap it by yourself if you want to use it for multilabel problems.","462",""],"delete":["105","    ValueError: 'wrong_choice' is not a valid scoring value. Valid options are ['accuracy', 'adjusted_mutual_info_score', 'adjusted_rand_score', 'average_precision', 'brier_score_loss', 'completeness_score', 'explained_variance', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'fowlkes_mallows_score', 'homogeneity_score', 'mutual_info_score', 'neg_log_loss', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_mean_squared_log_error', 'neg_median_absolute_error', 'normalized_mutual_info_score', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'r2', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'roc_auc', 'v_measure_score']"]}],"sklearn\/metrics\/__init__.py":[{"add":["16","from .classification import balanced_accuracy_score","71","    'balanced_accuracy_score',"],"delete":[]}]}},"e5bf61eee1c62dbb5a2103e68cf73936e4ac3e58":{"changes":{"doc\/developers\/advanced_installation.rst":"MODIFY","sklearn\/linear_model\/tests\/test_coordinate_descent.py":"MODIFY","sklearn\/utils\/fixes.py":"MODIFY","sklearn\/utils\/tests\/test_testing.py":"MODIFY","sklearn\/metrics\/classification.py":"MODIFY","sklearn\/utils\/testing.py":"MODIFY","sklearn\/model_selection\/tests\/test_split.py":"MODIFY","README.rst":"MODIFY","build_tools\/travis\/install.sh":"MODIFY","setup.py":"MODIFY","sklearn\/externals\/funcsigs.py":"MODIFY","sklearn\/gaussian_process\/gaussian_process.py":"MODIFY","doc\/developers\/contributing.rst":"MODIFY","sklearn\/preprocessing\/data.py":"MODIFY","sklearn\/utils\/tests\/test_estimator_checks.py":"MODIFY","sklearn\/ensemble\/tests\/test_base.py":"MODIFY","\/dev\/null":"DELETE","doc\/install.rst":"MODIFY","sklearn\/metrics\/pairwise.py":"MODIFY","sklearn\/tests\/test_base.py":"MODIFY",".travis.yml":"MODIFY","sklearn\/neighbors\/tests\/test_approximate.py":"MODIFY"},"diff":{"doc\/developers\/advanced_installation.rst":[{"add":["37","- Python (>= 2.7 or >= 3.3),"],"delete":["37","- Python (>= 2.6 or >= 3.3),"]}],"sklearn\/linear_model\/tests\/test_coordinate_descent.py":[{"add":[],"delete":["34","def check_warnings():","35","    if version_info < (2, 6):","36","        raise SkipTest(\"Testing for warnings is not supported in versions \\","37","        older than Python 2.6\")","38","","39",""]}],"sklearn\/utils\/fixes.py":[{"add":[],"delete":["206","try:","207","    from itertools import combinations_with_replacement","208","except ImportError:","209","    # Backport of itertools.combinations_with_replacement for Python 2.6,","210","    # from Python 3.4 documentation (http:\/\/tinyurl.com\/comb-w-r), copyright","211","    # Python Software Foundation (https:\/\/docs.python.org\/3\/license.html)","212","    def combinations_with_replacement(iterable, r):","213","        # combinations_with_replacement('ABC', 2) --> AA AB AC BB BC CC","214","        pool = tuple(iterable)","215","        n = len(pool)","216","        if not n and r:","217","            return","218","        indices = [0] * r","219","        yield tuple(pool[i] for i in indices)","220","        while True:","221","            for i in reversed(range(r)):","222","                if indices[i] != n - 1:","223","                    break","224","            else:","225","                return","226","            indices[i:] = [indices[i] + 1] * (r - i)","227","            yield tuple(pool[i] for i in indices)","228","","229","","293","if sys.version_info < (2, 7, 0):","294","    # partial cannot be pickled in Python 2.6","295","    # http:\/\/bugs.python.org\/issue1398","296","    class partial(object):","297","        def __init__(self, func, *args, **keywords):","298","            functools.update_wrapper(self, func)","299","            self.func = func","300","            self.args = args","301","            self.keywords = keywords","302","","303","        def __call__(self, *args, **keywords):","304","            args = self.args + args","305","            kwargs = self.keywords.copy()","306","            kwargs.update(keywords)","307","            return self.func(*args, **kwargs)","308","else:","309","    from functools import partial","310","","311",""]}],"sklearn\/utils\/tests\/test_testing.py":[{"add":["6","    assert_less,","7","    assert_greater,","21","def test_assert_less():","22","    assert_less(0, 1)","23","    assert_raises(AssertionError, assert_less, 1, 0)","26","def test_assert_greater():","27","    assert_greater(1, 0)","28","    assert_raises(AssertionError, assert_greater, 0, 1)"],"delete":["6","    _assert_less,","7","    _assert_greater,","20","try:","21","    from nose.tools import assert_less","23","    def test_assert_less():","24","        # Check that the nose implementation of assert_less gives the","25","        # same thing as the scikit's","26","        assert_less(0, 1)","27","        _assert_less(0, 1)","28","        assert_raises(AssertionError, assert_less, 1, 0)","29","        assert_raises(AssertionError, _assert_less, 1, 0)","31","except ImportError:","32","    pass","34","try:","35","    from nose.tools import assert_greater","36","","37","    def test_assert_greater():","38","        # Check that the nose implementation of assert_less gives the","39","        # same thing as the scikit's","40","        assert_greater(1, 0)","41","        _assert_greater(1, 0)","42","        assert_raises(AssertionError, assert_greater, 0, 1)","43","        assert_raises(AssertionError, _assert_greater, 0, 1)","44","","45","except ImportError:","46","    pass"]}],"sklearn\/metrics\/classification.py":[{"add":[],"delete":["432","","433","            # If there is no label, it results in a Nan instead, we set","434","            # the jaccard to 1: lim_{x->0} x\/x = 1","435","            # Note with py2.6 and np 1.3: we can't check safely for nan."]}],"sklearn\/utils\/testing.py":[{"add":["79","SkipTest = unittest.case.SkipTest","80","assert_dict_equal = _dummy.assertDictEqual","81","assert_in = _dummy.assertIn","82","assert_not_in = _dummy.assertNotIn","83","assert_less = _dummy.assertLess","84","assert_greater = _dummy.assertGreater","85","assert_less_equal = _dummy.assertLessEqual","86","assert_greater_equal = _dummy.assertGreaterEqual","92","    # Python 2.7","93","    assert_raises_regex = _dummy.assertRaisesRegexp","325","assert_less = _dummy.assertLess","326","assert_greater = _dummy.assertGreater"],"delete":["80","try:","81","    SkipTest = unittest.case.SkipTest","82","except AttributeError:","83","    # Python <= 2.6, we stil need nose here","84","    from nose import SkipTest","85","","86","","87","try:","88","    assert_dict_equal = _dummy.assertDictEqual","89","    assert_in = _dummy.assertIn","90","    assert_not_in = _dummy.assertNotIn","91","except AttributeError:","92","    # Python <= 2.6","93","","94","    assert_dict_equal = assert_equal","95","","96","    def assert_in(x, container):","97","        assert_true(x in container, msg=\"%r in %r\" % (x, container))","98","","99","    def assert_not_in(x, container):","100","        assert_false(x in container, msg=\"%r in %r\" % (x, container))","105","    # for Python 2.6","106","    def assert_raises_regex(expected_exception, expected_regexp,","107","                            callable_obj=None, *args, **kwargs):","108","        \"\"\"Helper function to check for message patterns in exceptions.\"\"\"","109","        not_raised = False","110","        try:","111","            callable_obj(*args, **kwargs)","112","            not_raised = True","113","        except expected_exception as e:","114","            error_message = str(e)","115","            if not re.compile(expected_regexp).search(error_message):","116","                raise AssertionError(\"Error message should match pattern \"","117","                                     \"%r. %r does not.\" %","118","                                     (expected_regexp, error_message))","119","        if not_raised:","120","            raise AssertionError(\"%s not raised by %s\" %","121","                                 (expected_exception.__name__,","122","                                  callable_obj.__name__))","123","","130","def _assert_less(a, b, msg=None):","131","    message = \"%r is not lower than %r\" % (a, b)","132","    if msg is not None:","133","        message += \": \" + msg","134","    assert a < b, message","135","","136","","137","def _assert_greater(a, b, msg=None):","138","    message = \"%r is not greater than %r\" % (a, b)","139","    if msg is not None:","140","        message += \": \" + msg","141","    assert a > b, message","142","","143","","144","def assert_less_equal(a, b, msg=None):","145","    message = \"%r is not lower than or equal to %r\" % (a, b)","146","    if msg is not None:","147","        message += \": \" + msg","148","    assert a <= b, message","149","","150","","151","def assert_greater_equal(a, b, msg=None):","152","    message = \"%r is not greater than or equal to %r\" % (a, b)","153","    if msg is not None:","154","        message += \": \" + msg","155","    assert a >= b, message","156","","157","","275","    # XXX: once we may depend on python >= 2.6, this can be replaced by the","276","","277","    # warnings module context manager.","386","try:","387","    assert_less = _dummy.assertLess","388","    assert_greater = _dummy.assertGreater","389","except AttributeError:","390","    assert_less = _assert_less","391","    assert_greater = _assert_greater"]}],"sklearn\/model_selection\/tests\/test_split.py":[{"add":["9","from itertools import combinations_with_replacement"],"delete":["9","from sklearn.utils.fixes import combinations_with_replacement"]}],"README.rst":[{"add":["50","- Python (>= 2.7 or >= 3.3)"],"delete":["50","- Python (>= 2.6 or >= 3.3)"]}],"build_tools\/travis\/install.sh":[{"add":["55","            mkl flake8 cython=$CYTHON_VERSION \\","61","            nomkl cython=$CYTHON_VERSION \\"],"delete":["55","            libgfortran mkl flake8 \\","61","            libgfortran nomkl \\","66","    # Temporary work around for Python 2.6 because cython >= 0.23 is","67","    # required for building scikit-learn but python 2.6 and cython","68","    # 0.23 are not compatible in conda. Remove the next line and","69","    # install cython via conda when Python 2.6 support is removed.","70","    pip install cython==$CYTHON_VERSION","71",""]}],"setup.py":[{"add":[],"delete":["202","                                 'Programming Language :: Python :: 2.6',"]}],"sklearn\/externals\/funcsigs.py":[{"add":["4","modified to be compatible with Python 2.7 and 3.2+.","12","from collections import OrderedDict"],"delete":["4","modified to be compatible with Python 2.6, 2.7 and 3.2+.","12","try:","13","    from collections import OrderedDict","14","except ImportError:","15","    from .odict import OrderedDict"]}],"sklearn\/gaussian_process\/gaussian_process.py":[{"add":["620","        Q, G = linalg.qr(Ft, mode='economic')"],"delete":["620","        try:","621","            Q, G = linalg.qr(Ft, econ=True)","622","        except:","623","            #\/usr\/lib\/python2.6\/dist-packages\/scipy\/linalg\/decomp.py:1177:","624","            # DeprecationWarning: qr econ argument will be removed after scipy","625","            # 0.7. The economy transform will then be available through the","626","            # mode='economic' argument.","627","            Q, G = linalg.qr(Ft, mode='economic')"]}],"doc\/developers\/contributing.rst":[{"add":["704","on both 2.7 and 3.2 or newer."],"delete":["704","on both 2.6 or 2.7, and 3.2 or newer."]}],"sklearn\/preprocessing\/data.py":[{"add":["11","from itertools import combinations_with_replacement as combinations_w_r"],"delete":["21","from ..utils.fixes import combinations_with_replacement as combinations_w_r"]}],"sklearn\/utils\/tests\/test_estimator_checks.py":[{"add":["86","    msg = \"TypeError not raised\""],"delete":["86","    msg = \"TypeError not raised by fit\""]}],"sklearn\/ensemble\/tests\/test_base.py":[{"add":["18","from collections import OrderedDict"],"delete":["18","from sklearn.externals.odict import OrderedDict"]}],"\/dev\/null":[{"add":[],"delete":[]}],"doc\/install.rst":[{"add":["17","- Python (>= 2.7 or >= 3.3),"],"delete":["17","- Python (>= 2.6 or >= 3.3),"]}],"sklearn\/metrics\/pairwise.py":[{"add":["12","from functools import partial"],"delete":["21","from ..utils.fixes import partial"]}],"sklearn\/tests\/test_base.py":[{"add":[],"delete":["182","    PY26 = sys.version_info[:2] == (2, 6)","183","    if PY26:","184","        # sp.dok_matrix can not be deepcopied in Python 2.6","185","        sparse_matrix_classes.remove(sp.dok_matrix)","186",""]}],".travis.yml":[{"add":["31","    - DISTRIB=\"conda\" PYTHON_VERSION=\"2.7\" INSTALL_MKL=\"false\""],"delete":["31","    - DISTRIB=\"conda\" PYTHON_VERSION=\"2.6\" INSTALL_MKL=\"false\""]}],"sklearn\/neighbors\/tests\/test_approximate.py":[{"add":["231","    lsfh = LSHForest(min_hash_match=0, n_candidates=n_points,","232","                     random_state=42).fit(X)"],"delete":["231","    lsfh = LSHForest(min_hash_match=0, n_candidates=n_points).fit(X)"]}]}},"6a01e89672ce68708b136b66ad817d926767ea88":{"changes":{"sklearn\/decomposition\/online_lda.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/decomposition\/tests\/test_online_lda.py":"MODIFY"},"diff":{"sklearn\/decomposition\/online_lda.py":[{"add":["507","                          DeprecationWarning)","533","                    bound = self._perplexity_precomp_distr(X, doc_topics_distr,","534","                                                           sub_sampling=False)","543","","544","        # calculate final perplexity value on train set","545","        doc_topics_distr, _ = self._e_step(X, cal_sstats=False,","546","                                           random_init=False,","547","                                           parallel=parallel)","548","        self.bound_ = self._perplexity_precomp_distr(X, doc_topics_distr,","549","                                                     sub_sampling=False)","550","","553","    def _unnormalized_transform(self, X):","554","        \"\"\"Transform data X according to fitted model.","581","","582","        return doc_topic_distr","583","","584","    def transform(self, X):","585","        \"\"\"Transform data X according to the fitted model.","586","","587","           .. versionchanged:: 0.18","588","              *doc_topic_distr* is now normalized","589","","590","        Parameters","591","        ----------","592","        X : array-like or sparse matrix, shape=(n_samples, n_features)","593","            Document word matrix.","594","","595","        Returns","596","        -------","597","        doc_topic_distr : shape=(n_samples, n_topics)","598","            Document topic distribution for X.","599","        \"\"\"","600","        doc_topic_distr = self._unnormalized_transform(X)","695","        doc_topic_distr = self._unnormalized_transform(X)","699","    def _perplexity_precomp_distr(self, X, doc_topic_distr=None,","700","                                  sub_sampling=False):","701","        \"\"\"Calculate approximate perplexity for data X with ability to accept","702","        precomputed doc_topic_distr","728","            doc_topic_distr = self._unnormalized_transform(X)","748","","749","    def perplexity(self, X, doc_topic_distr='deprecated', sub_sampling=False):","750","        \"\"\"Calculate approximate perplexity for data X.","751","","752","        Perplexity is defined as exp(-1. * log-likelihood per word)","753","","754","        .. versionchanged:: 0.19","755","           *doc_topic_distr* argument has been depricated because user no","756","           longer has access to unnormalized distribution","757","","758","        Parameters","759","        ----------","760","        X : array-like or sparse matrix, [n_samples, n_features]","761","            Document word matrix.","762","","763","        doc_topic_distr : None or array, shape=(n_samples, n_topics)","764","            Document topic distribution.","765","            If it is None, it will be generated by applying transform on X.","766","","767","            .. deprecated:: 0.19","768","","769","        Returns","770","        -------","771","        score : float","772","            Perplexity score.","773","        \"\"\"","774","        if doc_topic_distr != 'deprecated':","775","            warnings.warn(\"Argument 'doc_topic_distr' is deprecated and will \"","776","                          \"be ignored as of 0.19. Support for this argument \"","777","                          \"will be removed in 0.21.\", DeprecationWarning)","778","","779","        return self._perplexity_precomp_distr(X, sub_sampling=sub_sampling)"],"delete":["507","                          DeprecationWarning)          ","533","                    bound = self.perplexity(X, doc_topics_distr,","534","                                            sub_sampling=False)","545","    def transform(self, X):","546","        \"\"\"Transform data X according to the fitted model.","558","","574","        # normalize doc_topic_distr","667","","670","        doc_topic_distr = self.transform(X)","674","    def perplexity(self, X, doc_topic_distr=None, sub_sampling=False):","675","        \"\"\"Calculate approximate perplexity for data X.","701","            doc_topic_distr = self.transform(X)"]}],"doc\/whats_new.rst":[{"add":["124","   - Fix a bug in :class:`sklearn.decomposition.LatentDirichletAllocation`","125","     where the ``perplexity`` method was returning incorrect results because","126","     the ``transform`` method returns normalized document topic distributions","127","     as of version 0.18. :issue:`7954` by :user:`Gary Foreman <garyForeman>`.","128","     ","142","     ","143","   - Deprecate the ``doc_topic_distr`` argument of the ``perplexity`` method","144","     in :class:`sklearn.decomposition.LatentDirichletAllocation` because the","145","     user no longer has access to the unnormalized document topic distribution","146","     needed for the perplexity calculation. :issue:`7954` by","147","     :user:`Gary Foreman <garyForeman>`."],"delete":[]}],"sklearn\/decomposition\/tests\/test_online_lda.py":[{"add":["16","from sklearn.utils.testing import assert_warns","241","    assert_raises_regexp(ValueError, r'Number of samples',","242","                         lda._perplexity_precomp_distr, X, invalid_n_samples)","245","    assert_raises_regexp(ValueError, r'Number of topics',","246","                         lda._perplexity_precomp_distr, X, invalid_n_topics)","260","        lda_1.fit(X)","261","        perp_1 = lda_1.perplexity(X, sub_sampling=False)","263","        lda_2.fit(X)","264","        perp_2 = lda_2.perplexity(X, sub_sampling=False)","267","        perp_1_subsampling = lda_1.perplexity(X, sub_sampling=True)","268","        perp_2_subsampling = lda_2.perplexity(X, sub_sampling=True)","298","    lda.fit(X)","300","    perp_2 = lda.perplexity(X.toarray())","309","    lda.fit(X)","310","    perplexity_1 = lda.perplexity(X, sub_sampling=False)","317","def test_lda_fit_perplexity():","318","    # Test that the perplexity computed during fit is consistent with what is","319","    # returned by the perplexity method","320","    n_topics, X = _build_sparse_mtx()","321","    lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=1,","322","                                    learning_method='batch', random_state=0,","323","                                    evaluate_every=1)","324","    lda.fit(X)","325","","326","    # Perplexity computed at end of fit method","327","    perplexity1 = lda.bound_","328","","329","    # Result of perplexity method on the train set","330","    perplexity2 = lda.perplexity(X)","331","","332","    assert_almost_equal(perplexity1, perplexity2)","333","","334","","335","def test_doc_topic_distr_deprecation():","336","    # Test that the appropriate warning message is displayed when a user","337","    # attempts to pass the doc_topic_distr argument to the perplexity method","338","    n_topics, X = _build_sparse_mtx()","339","    lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=1,","340","                                    learning_method='batch',","341","                                    total_samples=100, random_state=0)","342","    distr1 = lda.fit_transform(X)","343","    distr2 = None","344","    assert_warns(DeprecationWarning, lda.perplexity, X, distr1)","345","    assert_warns(DeprecationWarning, lda.perplexity, X, distr2)","346","","347",""],"delete":["240","    assert_raises_regexp(ValueError, r'Number of samples', lda.perplexity, X,","241","                         invalid_n_samples)","244","    assert_raises_regexp(ValueError, r'Number of topics', lda.perplexity, X,","245","                         invalid_n_topics)","259","        distr_1 = lda_1.fit_transform(X)","260","        perp_1 = lda_1.perplexity(X, distr_1, sub_sampling=False)","262","        distr_2 = lda_2.fit_transform(X)","263","        perp_2 = lda_2.perplexity(X, distr_2, sub_sampling=False)","266","        perp_1_subsampling = lda_1.perplexity(X, distr_1, sub_sampling=True)","267","        perp_2_subsampling = lda_2.perplexity(X, distr_2, sub_sampling=True)","297","    distr = lda.fit_transform(X)","299","    perp_2 = lda.perplexity(X, distr)","300","    perp_3 = lda.perplexity(X.toarray(), distr)","302","    assert_almost_equal(perp_1, perp_3)","310","    distr = lda.fit_transform(X)","311","    perplexity_1 = lda.perplexity(X, distr, sub_sampling=False)"]}]}},"9fd70a833c8e05ffc8e6819a43ced405307142a5":{"changes":{"sklearn\/tests\/test_multiclass.py":"MODIFY","sklearn\/multiclass.py":"MODIFY"},"diff":{"sklearn\/tests\/test_multiclass.py":[{"add":["3","from sklearn.utils.testing import assert_array_equal, assert_raises_regex","24","                                  Perceptron, LogisticRegression,","25","                                  SGDClassifier)","92","    # with SGDClassifier","93","    X = np.abs(np.random.randn(14, 2))","94","    y = [1, 1, 1, 1, 2, 3, 3, 0, 0, 2, 3, 1, 2, 3]","96","    ovr = OneVsRestClassifier(SGDClassifier(n_iter=1, shuffle=False,","97","                                            random_state=0))","98","    ovr.partial_fit(X[:7], y[:7], np.unique(y))","99","    ovr.partial_fit(X[7:], y[7:])","100","    pred = ovr.predict(X)","101","    ovr1 = OneVsRestClassifier(SGDClassifier(n_iter=1, shuffle=False,","102","                                             random_state=0))","103","    pred1 = ovr1.fit(X, y).predict(X)","104","    assert_equal(np.mean(pred == y), np.mean(pred1 == y))","105","","106","","107","def test_ovr_partial_fit_exceptions():","108","    ovr = OneVsRestClassifier(MultinomialNB())","109","    X = np.abs(np.random.randn(14, 2))","110","    y = [1, 1, 1, 1, 2, 3, 3, 0, 0, 2, 3, 1, 2, 3]","111","    ovr.partial_fit(X[:7], y[:7], np.unique(y))","112","    # A new class value which was not in the first call of partial_fit","113","    # It should raise ValueError","114","    y1 = [5] + y[7:-1]","115","    assert_raises_regex(ValueError, \"Mini-batch contains \\[.+\\] while classes\"","116","                                    \" must be subset of \\[.+\\]\",","117","                        ovr.partial_fit, X=X[7:], y=y1)","121","    # test that ovr and ovo work on regressors which don't have a decision_","122","    # function"],"delete":["3","from sklearn.utils.testing import assert_array_equal","24","                                  Perceptron, LogisticRegression)","91","    ovr = OneVsRestClassifier(MultinomialNB())","92","    ovr.partial_fit(iris.data[:60], iris.target[:60], np.unique(iris.target))","93","    ovr.partial_fit(iris.data[60:], iris.target[60:])","94","    pred = ovr.predict(iris.data)","95","    ovr2 = OneVsRestClassifier(MultinomialNB())","96","    pred2 = ovr2.fit(iris.data, iris.target).predict(iris.data)","98","    assert_almost_equal(pred, pred2)","99","    assert_equal(len(ovr.estimators_), len(np.unique(iris.target)))","100","    assert_greater(np.mean(iris.target == pred), 0.65)","104","    # test that ovr and ovo work on regressors which don't have a decision_function","206",""]}],"sklearn\/multiclass.py":[{"add":["246","            if not hasattr(self.estimator, \"partial_fit\"):","247","                raise ValueError((\"Base estimator {0}, doesn't have \"","248","                                 \"partial_fit method\").format(self.estimator))","252","            # A sparse LabelBinarizer, with sparse_output=True, has been","253","            # shown to outperform or match a dense label binarizer in all","254","            # cases and has also resulted in less or equal memory consumption","255","            # in the fit_ovr function overall.","256","            self.label_binarizer_ = LabelBinarizer(sparse_output=True)","257","            self.label_binarizer_.fit(self.classes_)","258","","259","        if np.setdiff1d(y, self.classes_):","260","            raise ValueError((\"Mini-batch contains {0} while classes \" +","261","                             \"must be subset of {1}\").format(np.unique(y),","262","                                                             self.classes_))","263","","264","        Y = self.label_binarizer_.transform(y)","268","        self.estimators_ = Parallel(n_jobs=self.n_jobs)(","269","            delayed(_partial_fit_binary)(self.estimators_[i], X,","270","                                         next(columns))"],"delete":["246","            if (not hasattr(self.estimator, \"partial_fit\")):","247","                raise ValueError(\"Base estimator {0}, doesn't have partial_fit\"","248","                                 \"method\".format(self.estimator))","252","        # A sparse LabelBinarizer, with sparse_output=True, has been shown to","253","        # outperform or match a dense label binarizer in all cases and has also","254","        # resulted in less or equal memory consumption in the fit_ovr function","255","        # overall.","256","        self.label_binarizer_ = LabelBinarizer(sparse_output=True)","257","        Y = self.label_binarizer_.fit_transform(y)","261","        self.estimators_ = Parallel(n_jobs=self.n_jobs)(delayed(","262","            _partial_fit_binary)(self.estimators_[i],","263","                                 X, next(columns) if self.classes_[i] in","264","                                 self.label_binarizer_.classes_ else","265","                                 np.zeros((1, len(y))))"]}]}},"dd2e48c0912e7c03a7343f81c5b4efa3dc1fd2e4":{"changes":{"sklearn\/multioutput.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/tests\/test_multioutput.py":"MODIFY"},"diff":{"sklearn\/multioutput.py":[{"add":["216","        p : array of shape = [n_samples, n_classes], or a list of n_outputs \\","217","            such arrays if n_outputs > 1.","218","            The class probabilities of the input samples. The order of the","219","            classes corresponds to that in the attribute `classes_`.","226","        results = [estimator.predict_proba(X) for estimator in","227","                   self.estimators_]"],"delete":["216","        T : (sparse) array-like, shape = (n_samples, n_classes, n_outputs)","217","            The class probabilities of the samples for each of the outputs","224","        results = np.dstack([estimator.predict_proba(X) for estimator in","225","                            self.estimators_])"]}],"doc\/whats_new.rst":[{"add":["154","   - Fix :func:`sklearn.multioutput.MultiOutputClassifier.predict_proba` to","155","     return a list of 2d arrays, rather than a 3d array. In the case where","156","     different target columns had different numbers of classes, a `ValueError`","157","     would be raised on trying to stack matrices with different dimensions.","158","     :issue:`8093` by :user:`Peter Bull <pjbull>`.","159","","175","   - The :func:`sklearn.multioutput.MultiOutputClassifier.predict_proba`","176","     function used to return a 3d array (``n_samples``, ``n_classes``,","177","     ``n_outputs``). In the case where different target columns had different","178","     numbers of classes, a `ValueError` would be raised on trying to stack","179","     matrices with different dimensions. This function now returns a list of","180","     arrays where the length of the list is ``n_outputs``, and each array is","181","     (``n_samples``, ``n_classes``) for that particular output.","182","     :issue:`8093` by :user:`Peter Bull <pjbull>`.","183",""],"delete":[]}],"sklearn\/tests\/test_multioutput.py":[{"add":["12","from sklearn.linear_model import Lasso, LogisticRegression","121","    assert len(predict_proba) == n_outputs","122","    for class_probabilities in predict_proba:","123","        assert_equal((n_samples, n_classes), class_probabilities.shape)","124","","125","    assert_array_equal(np.argmax(np.dstack(predict_proba), axis=1),","126","                       predictions)","134","                           list(predict_proba[i]))","156","def test_multiclass_multioutput_estimator_predict_proba():","157","    seed = 542","158","","159","    # make test deterministic","160","    rng = np.random.RandomState(seed)","161","","162","    # random features","163","    X = rng.normal(size=(5, 5))","164","","165","    # random labels","166","    y1 = np.array(['b', 'a', 'a', 'b', 'a']).reshape(5, 1)  # 2 classes","167","    y2 = np.array(['d', 'e', 'f', 'e', 'd']).reshape(5, 1)  # 3 classes","168","","169","    Y = np.concatenate([y1, y2], axis=1)","170","","171","    clf = MultiOutputClassifier(LogisticRegression(random_state=seed))","172","","173","    clf.fit(X, Y)","174","","175","    y_result = clf.predict_proba(X)","176","    y_actual = [np.array([[0.23481764, 0.76518236],","177","                          [0.67196072, 0.32803928],","178","                          [0.54681448, 0.45318552],","179","                          [0.34883923, 0.65116077],","180","                          [0.73687069, 0.26312931]]),","181","                np.array([[0.5171785, 0.23878628, 0.24403522],","182","                          [0.22141451, 0.64102704, 0.13755846],","183","                          [0.16751315, 0.18256843, 0.64991843],","184","                          [0.27357372, 0.55201592, 0.17441036],","185","                          [0.65745193, 0.26062899, 0.08191907]])]","186","","187","    for i in range(len(y_actual)):","188","        assert_almost_equal(y_result[i], y_actual[i])","189","","190",""],"delete":["12","from sklearn.linear_model import Lasso","120","    assert_equal((n_samples, n_classes, n_outputs), predict_proba.shape)","122","    assert_array_equal(np.argmax(predict_proba, axis=1), predictions)","130","                           list(predict_proba[:, :, i]))"]}]}},"5230382ae65ce4c5c5499b652d07077242e47c1b":{"changes":{"sklearn\/linear_model\/tests\/test_least_angle.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/linear_model\/least_angle.py":"MODIFY"},"diff":{"sklearn\/linear_model\/tests\/test_least_angle.py":[{"add":["543","","544","","545","def test_lasso_lars_vs_R_implementation():","546","    # Test that sklearn LassoLars implementation agrees with the LassoLars","547","    # implementation available in R (lars library) under the following","548","    # scenarios:","549","    # 1) fit_intercept=False and normalize=False","550","    # 2) fit_intercept=True and normalize=True","551","","552","    # Let's generate the data used in the bug report 7778","553","    y = np.array([-6.45006793, -3.51251449, -8.52445396, 6.12277822,","554","                  -19.42109366])","555","    x = np.array([[0.47299829, 0, 0, 0, 0],","556","                  [0.08239882, 0.85784863, 0, 0, 0],","557","                  [0.30114139, -0.07501577, 0.80895216, 0, 0],","558","                  [-0.01460346, -0.1015233, 0.0407278, 0.80338378, 0],","559","                  [-0.69363927, 0.06754067, 0.18064514, -0.0803561,","560","                   0.40427291]])","561","","562","    X = x.T","563","","564","    ###########################################################################","565","    # Scenario 1: Let's compare R vs sklearn when fit_intercept=False and","566","    # normalize=False","567","    ###########################################################################","568","    #","569","    # The R result was obtained using the following code:","570","    #","571","    # library(lars)","572","    # model_lasso_lars = lars(X, t(y), type=\"lasso\", intercept=FALSE,","573","    #                         trace=TRUE, normalize=FALSE)","574","    # r = t(model_lasso_lars$beta)","575","    #","576","","577","    r = np.array([[0, 0, 0, 0, 0, -79.810362809499026, -83.528788732782829,","578","                   -83.777653739190711, -83.784156932888934,","579","                   -84.033390591756657],","580","                  [0, 0, 0, 0, -0.476624256777266, 0, 0, 0, 0,","581","                   0.025219751009936],","582","                  [0, -3.577397088285891, -4.702795355871871,","583","                   -7.016748621359461, -7.614898471899412, -0.336938391359179,","584","                   0, 0, 0.001213370600853,  0.048162321585148],","585","                  [0, 0, 0, 2.231558436628169, 2.723267514525966,","586","                   2.811549786389614, 2.813766976061531, 2.817462468949557,","587","                   2.817368178703816, 2.816221090636795],","588","                  [0, 0, -1.218422599914637, -3.457726183014808,","589","                   -4.021304522060710, -45.827461592423745,","590","                   -47.776608869312305,","591","                   -47.911561610746404, -47.914845922736234,","592","                   -48.039562334265717]])","593","","594","    model_lasso_lars = linear_model.LassoLars(alpha=0, fit_intercept=False,","595","                                              normalize=False)","596","    model_lasso_lars.fit(X, y)","597","    skl_betas = model_lasso_lars.coef_path_","598","","599","    assert_array_almost_equal(r, skl_betas, decimal=12)","600","    ###########################################################################","601","","602","    ###########################################################################","603","    # Scenario 2: Let's compare R vs sklearn when fit_intercept=True and","604","    # normalize=True","605","    #","606","    # Note: When normalize is equal to True, R returns the coefficients in","607","    # their original units, that is, they are rescaled back, whereas sklearn","608","    # does not do that, therefore, we need to do this step before comparing","609","    # their results.","610","    ###########################################################################","611","    #","612","    # The R result was obtained using the following code:","613","    #","614","    # library(lars)","615","    # model_lasso_lars2 = lars(X, t(y), type=\"lasso\", intercept=TRUE,","616","    #                           trace=TRUE, normalize=TRUE)","617","    # r2 = t(model_lasso_lars2$beta)","618","","619","    r2 = np.array([[0, 0, 0, 0, 0],","620","                   [0, 0, 0, 8.371887668009453, 19.463768371044026],","621","                   [0, 0, 0, 0, 9.901611055290553],","622","                   [0, 7.495923132833733, 9.245133544334507,","623","                    17.389369207545062, 26.971656815643499],","624","                   [0, 0, -1.569380717440311, -5.924804108067312,","625","                    -7.996385265061972]])","626","","627","    model_lasso_lars2 = linear_model.LassoLars(alpha=0, fit_intercept=True,","628","                                               normalize=True)","629","    model_lasso_lars2.fit(X, y)","630","    skl_betas2 = model_lasso_lars2.coef_path_","631","","632","    # Let's rescale back the coefficients returned by sklearn before comparing","633","    # against the R result (read the note above)","634","    temp = X - np.mean(X, axis=0)","635","    normx = np.sqrt(np.sum(temp ** 2, axis=0))","636","    skl_betas2 \/= normx[:, np.newaxis]","637","","638","    assert_array_almost_equal(r2, skl_betas2, decimal=12)","639","    ###########################################################################"],"delete":[]}],"doc\/whats_new.rst":[{"add":["102","   - Fixed a bug where :class:`sklearn.linear_model.LassoLars` does not give","103","     the same result as the LassoLars implementation available","104","     in R (lars library). :issue:`7849` by `Jair Montoya Martinez`_"],"delete":[]}],"sklearn\/linear_model\/least_angle.py":[{"add":["406","                coefs[-add_features:] = 0","408","                alphas[-add_features:] = 0"],"delete":[]}]}},"4502520ed60fa98479b3ca6349d1321d505bbe4b":{"changes":{"doc\/about.rst":"MODIFY"},"diff":{"doc\/about.rst":[{"add":["77","   :target: https:\/\/www.inria.fr","86","   :target: http:\/\/www.datascience-paris-saclay.fr","95","   :target: http:\/\/cds.nyu.edu\/mooresloan\/","96","","97","`Tlcom Paristech <http:\/\/www.telecom-paristech.com>`_ funds Manoj Kumar (2014),","98","Tom Dupr la Tour (2015), Raghav R V (2015-2016) and Thierry Guillemot (2016) to","99","work on scikit-learn.","100","","101",".. image:: themes\/scikit-learn\/static\/img\/telecom.png","102","   :width: 100pt","103","   :align: center","104","   :target: http:\/\/www.telecom-paristech.fr\/","191",".. |telecom| image:: themes\/scikit-learn\/static\/img\/telecom.png"],"delete":["179",".. |telecom| image:: http:\/\/f.hypotheses.org\/wp-content\/blogs.dir\/331\/files\/2011\/03\/Logo-TPT.jpg","228",""]}]}},"ee88cf44ea533803849379f48bd44fcdefc14a93":{"changes":{"sklearn\/__init__.py":"MODIFY","doc\/modules\/computational_performance.rst":"MODIFY","sklearn\/tests\/test_config.py":"ADD","doc\/modules\/classes.rst":"MODIFY","sklearn\/utils\/validation.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/utils\/tests\/test_validation.py":"MODIFY"},"diff":{"sklearn\/__init__.py":[{"add":["17","import os","18","from contextlib import contextmanager as _contextmanager","19","","20","_ASSUME_FINITE = bool(os.environ.get('SKLEARN_ASSUME_FINITE', False))","21","","22","","23","def get_config():","24","    \"\"\"Retrieve current values for configuration set by :func:`set_config`","25","","26","    Returns","27","    -------","28","    config : dict","29","        Keys are parameter names that can be passed to :func:`set_config`.","30","    \"\"\"","31","    return {'assume_finite': _ASSUME_FINITE}","32","","33","","34","def set_config(assume_finite=None):","35","    \"\"\"Set global scikit-learn configuration","36","","37","    Parameters","38","    ----------","39","    assume_finite : bool, optional","40","        If True, validation for finiteness will be skipped,","41","        saving time, but leading to potential crashes. If","42","        False, validation for finiteness will be performed,","43","        avoiding error.","44","    \"\"\"","45","    global _ASSUME_FINITE","46","    if assume_finite is not None:","47","        _ASSUME_FINITE = assume_finite","48","","49","","50","@_contextmanager","51","def config_context(**new_config):","52","    \"\"\"Context manager for global scikit-learn configuration","53","","54","    Parameters","55","    ----------","56","    assume_finite : bool, optional","57","        If True, validation for finiteness will be skipped,","58","        saving time, but leading to potential crashes. If","59","        False, validation for finiteness will be performed,","60","        avoiding error.","61","","62","    Notes","63","    -----","64","    All settings, not just those presently modified, will be returned to","65","    their previous values when the context manager is exited. This is not","66","    thread-safe.","67","","68","    Examples","69","    --------","70","    >>> import sklearn","71","    >>> from sklearn.utils.validation import assert_all_finite","72","    >>> with sklearn.config_context(assume_finite=True):","73","    ...     assert_all_finite([float('nan')])","74","    >>> with sklearn.config_context(assume_finite=True):","75","    ...     with sklearn.config_context(assume_finite=False):","76","    ...         assert_all_finite([float('nan')])","77","    ... # doctest: +ELLIPSIS","78","    Traceback (most recent call last):","79","    ...","80","    ValueError: Input contains NaN, ...","81","    \"\"\"","82","    old_config = get_config().copy()","83","    set_config(**new_config)","84","","85","    try:","86","        yield","87","    finally:","88","        set_config(**old_config)"],"delete":[]}],"doc\/modules\/computational_performance.rst":[{"add":["70",".. topic:: Configuring Scikit-learn for reduced validation overhead","71","","72","    Scikit-learn does some validation on data that increases the overhead per","73","    call to ``predict`` and similar functions. In particular, checking that","74","    features are finite (not NaN or infinite) involves a full pass over the","75","    data. If you ensure that your data is acceptable, you may suppress","76","    checking for finiteness by setting the environment variable","77","    ``SKLEARN_ASSUME_FINITE`` to a non-empty string before importing","78","    scikit-learn, or configure it in Python with :func:`sklearn.set_config`.","79","    For more control than these global settings, a :func:`config_context`","80","    allows you to set this configuration within a specified context::","81","","82","      >>> import sklearn","83","      >>> with sklearn.config_context(assume_finite=True):","84","      ...    pass  # do learning\/prediction here with reduced validation","85","","86","    Note that this will affect all uses of","87","    :func:`sklearn.utils.assert_all_finite` within the context.","88",""],"delete":[]}],"sklearn\/tests\/test_config.py":[{"add":[],"delete":[]}],"doc\/modules\/classes.rst":[{"add":["42","   config_context","43","   set_config","44","   get_config"],"delete":[]}],"sklearn\/utils\/validation.py":[{"add":["18","from .. import get_config as _get_config","33","    if _get_config()['assume_finite']:","34","        return"],"delete":[]}],"doc\/whats_new.rst":[{"add":["33","   - Validation that input data contains no NaN or inf can now be suppressed","34","     using :func:`config_context`, at your own risk. This will save on runtime,","35","     and may be particularly useful for prediction time. :issue:`7548` by","36","     `Joel Nothman`_.","37",""],"delete":[]}],"sklearn\/utils\/tests\/test_validation.py":[{"add":["32","    assert_all_finite,","34","import sklearn","529","","530","","531","def test_suppress_validation():","532","    X = np.array([0, np.inf])","533","    assert_raises(ValueError, assert_all_finite, X)","534","    sklearn.set_config(assume_finite=True)","535","    assert_all_finite(X)","536","    sklearn.set_config(assume_finite=False)","537","    assert_raises(ValueError, assert_all_finite, X)"],"delete":["39",""]}]}},"ec32689ae9459610e5f3d7180bc87b7300f94ca5":{"changes":{"sklearn\/feature_selection\/rfe.py":"MODIFY","sklearn\/feature_selection\/tests\/test_rfe.py":"MODIFY"},"diff":{"sklearn\/feature_selection\/rfe.py":[{"add":["398","","399","        if 0.0 < self.step < 1.0:","400","            step = int(max(1, self.step * n_features))","401","        else:","402","            step = int(self.step)","403","        if step <= 0:","404","            raise ValueError(\"Step must be >0\")","405","","433","            n_features - (np.argmax(scores) * step),"],"delete":["402","","426","            n_features - (np.argmax(scores) * self.step),"]}],"sklearn\/feature_selection\/tests\/test_rfe.py":[{"add":["184","    # Verifying that steps < 1 don't blow up.","185","    rfecv_sparse = RFECV(estimator=SVC(kernel=\"linear\"), step=.2, cv=5)","186","    X_sparse = sparse.csr_matrix(X)","187","    rfecv_sparse.fit(X_sparse, y)","188","    X_r_sparse = rfecv_sparse.transform(X_sparse)","189","    assert_array_equal(X_r_sparse.toarray(), iris.data)","190",""],"delete":[]}]}},"6e50c8f35e67719e5b528b66bfe75444ec0008ed":{"changes":{"sklearn\/datasets\/lfw.py":"MODIFY","doc\/templates\/deprecated_class.rst":"ADD","sklearn\/mixture\/gmm.py":"MODIFY","doc\/modules\/classes.rst":"MODIFY","doc\/templates\/generate_deprecated.sh":"ADD","sklearn\/cross_validation.py":"MODIFY","sklearn\/tests\/test_discriminant_analysis.py":"MODIFY","sklearn\/discriminant_analysis.py":"MODIFY","doc\/templates\/deprecated_class_with_call.rst":"ADD","sklearn\/learning_curve.py":"MODIFY","sklearn\/qda.py":"MODIFY","sklearn\/gaussian_process\/gaussian_process.py":"MODIFY","doc\/templates\/deprecated_function.rst":"ADD","sklearn\/lda.py":"MODIFY","sklearn\/mixture\/dpgmm.py":"MODIFY","doc\/templates\/deprecated_class_without_init.rst":"ADD","sklearn\/decomposition\/pca.py":"MODIFY","sklearn\/grid_search.py":"MODIFY","doc\/themes\/scikit-learn\/static\/nature.css_t":"MODIFY"},"diff":{"sklearn\/datasets\/lfw.py":[{"add":["382","    \"\"\"","383","    Alias for fetch_lfw_people(download_if_missing=False)","384","","385","    .. deprecated:: 0.17","386","        This function will be removed in 0.19.","387","        Use :func:`sklearn.datasets.fetch_lfw_people` with parameter","388","        ``download_if_missing=False`` instead.","517","    \"\"\"","518","    Alias for fetch_lfw_pairs(download_if_missing=False)","519","","520","    .. deprecated:: 0.17","521","        This function will be removed in 0.19.","522","        Use :func:`sklearn.datasets.fetch_lfw_pairs` with parameter","523","        ``download_if_missing=False`` instead."],"delete":["382","    \"\"\"Alias for fetch_lfw_people(download_if_missing=False)","511","    \"\"\"Alias for fetch_lfw_pairs(download_if_missing=False)"]}],"doc\/templates\/deprecated_class.rst":[{"add":[],"delete":[]}],"sklearn\/mixture\/gmm.py":[{"add":["672","    \"\"\"","673","    Legacy Gaussian Mixture Model","674","","675","    .. deprecated:: 0.18","676","        This class will be removed in 0.20.","677","        Use :class:`sklearn.mixture.GaussianMixture` instead.","678","","679","    \"\"\"","680",""],"delete":[]}],"doc\/modules\/classes.rst":[{"add":["188","   model_selection.check_cv","204","","205",".. autosummary::","206","   :toctree: generated\/","207","   :template: function.rst","208","","209","   model_selection.fit_grid_point","210","","1357","","1358","","1359","Recently deprecated","1360","===================","1361","","1362","To be removed in 0.19","1363","---------------------","1364","","1365",".. autosummary::","1366","   :toctree: generated\/","1367","   :template: deprecated_class.rst","1368","","1369","   lda.LDA","1370","   qda.QDA","1371","","1372",".. autosummary::","1373","   :toctree: generated\/","1374","   :template: deprecated_function.rst","1375","","1376","   datasets.load_lfw_pairs","1377","   datasets.load_lfw_people","1378","","1379","","1380","To be removed in 0.20","1381","---------------------","1382","","1383",".. autosummary::","1384","   :toctree: generated\/","1385","   :template: deprecated_class.rst","1386","","1387","   grid_search.ParameterGrid","1388","   grid_search.ParameterSampler","1389","   grid_search.GridSearchCV","1390","   grid_search.RandomizedSearchCV","1391","   cross_validation.LeaveOneOut","1392","   cross_validation.LeavePOut","1393","   cross_validation.KFold","1394","   cross_validation.LabelKFold","1395","   cross_validation.LeaveOneLabelOut","1396","   cross_validation.LeavePLabelOut","1397","   cross_validation.LabelShuffleSplit","1398","   cross_validation.StratifiedKFold","1399","   cross_validation.ShuffleSplit","1400","   cross_validation.StratifiedShuffleSplit","1401","   cross_validation.PredefinedSplit","1402","   decomposition.RandomizedPCA","1403","   gaussian_process.GaussianProcess","1404","   mixture.GMM","1405","   mixture.DPGMM","1406","   mixture.VBGMM","1407","","1408","","1409",".. autosummary::","1410","   :toctree: generated\/","1411","   :template: deprecated_function.rst","1412","","1413","   grid_search.fit_grid_point","1414","   learning_curve.learning_curve","1415","   learning_curve.validation_curve","1416","   cross_validation.cross_val_predict","1417","   cross_validation.cross_val_score","1418","   cross_validation.check_cv","1419","   cross_validation.permutation_test_score","1420","   cross_validation.train_test_split"],"delete":["317","   decomposition.RandomizedPCA","562","  gaussian_process.GaussianProcess"]}],"doc\/templates\/generate_deprecated.sh":[{"add":[],"delete":[]}],"sklearn\/cross_validation.py":[{"add":["111","    .. deprecated:: 0.18","112","        This module will be removed in 0.20.","113","        Use :class:`sklearn.model_selection.LeaveOneOut` instead.","114","","177","    .. deprecated:: 0.18","178","        This module will be removed in 0.20.","179","        Use :class:`sklearn.model_selection.LeavePOut` instead.","180","","276","    .. deprecated:: 0.18","277","        This module will be removed in 0.20.","278","        Use :class:`sklearn.model_selection.KFold` instead.","279","","371","    .. deprecated:: 0.18","372","        This module will be removed in 0.20.","373","        Use :class:`sklearn.model_selection.GroupKFold` instead.","374","","477","    .. deprecated:: 0.18","478","        This module will be removed in 0.20.","479","        Use :class:`sklearn.model_selection.StratifiedKFold` instead.","480","","603","    .. deprecated:: 0.18","604","        This module will be removed in 0.20.","605","        Use :class:`sklearn.model_selection.LeaveOneGroupOut` instead.","606","","677","    .. deprecated:: 0.18","678","        This module will be removed in 0.20.","679","        Use :class:`sklearn.model_selection.LeavePGroupsOut` instead.","680","","792","    .. deprecated:: 0.18","793","        This module will be removed in 0.20.","794","        Use :class:`sklearn.model_selection.ShuffleSplit` instead.","795","","997","    .. deprecated:: 0.18","998","        This module will be removed in 0.20.","999","        Use :class:`sklearn.model_selection.StratifiedShuffleSplit` instead.","1000","","1123","    .. deprecated:: 0.18","1124","        This module will be removed in 0.20.","1125","        Use :class:`sklearn.model_selection.PredefinedSplit` instead.","1126","","1182","    .. deprecated:: 0.18","1183","        This module will be removed in 0.20.","1184","        Use :class:`sklearn.model_selection.GroupShuffleSplit` instead.","1185","","1287","    .. deprecated:: 0.18","1288","        This module will be removed in 0.20.","1289","        Use :func:`sklearn.model_selection.cross_val_predict` instead.","1290","","1471","    .. deprecated:: 0.18","1472","        This module will be removed in 0.20.","1473","        Use :func:`sklearn.model_selection.cross_val_score` instead.","1474","","1778","    .. deprecated:: 0.18","1779","        This module will be removed in 0.20.","1780","        Use :func:`sklearn.model_selection.check_cv` instead.","1781","","1839","    .. deprecated:: 0.18","1840","        This module will be removed in 0.20.","1841","        Use :func:`sklearn.model_selection.permutation_test_score` instead.","1842","","1944","    .. deprecated:: 0.18","1945","        This module will be removed in 0.20.","1946","        Use :func:`sklearn.model_selection.train_test_split` instead.","1947",""],"delete":[]}],"sklearn\/tests\/test_discriminant_analysis.py":[{"add":["331","    assert isinstance(lda.LDA(), LinearDiscriminantAnalysis)","341","    assert isinstance(qda.QDA(), QuadraticDiscriminantAnalysis)"],"delete":["331","    assert lda.LDA is LinearDiscriminantAnalysis","341","    assert qda.QDA is QuadraticDiscriminantAnalysis"]}],"sklearn\/discriminant_analysis.py":[{"add":[],"delete":["145","    .. versionchanged:: 0.17","146","       Deprecated :class:`lda.LDA` have been moved to :class:`LinearDiscriminantAnalysis`.","147","","564","    .. versionchanged:: 0.17","565","       Deprecated :class:`qda.QDA` have been moved to :class:`QuadraticDiscriminantAnalysis`.","566",""]}],"doc\/templates\/deprecated_class_with_call.rst":[{"add":[],"delete":[]}],"sklearn\/learning_curve.py":[{"add":["34","    .. deprecated:: 0.18","35","        This module will be removed in 0.20.","36","        Use :func:`sklearn.model_selection.learning_curve` instead.","37","","265","    .. deprecated:: 0.18","266","        This module will be removed in 0.20.","267","        Use :func:`sklearn.model_selection.validation_curve` instead.","268",""],"delete":[]}],"sklearn\/qda.py":[{"add":["1","from .discriminant_analysis import QuadraticDiscriminantAnalysis as _QDA","2","","7","","8","class QDA(_QDA):","9","    \"\"\"","10","    Alias for","11","    :class:`sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis`.","12","","13","    .. deprecated:: 0.17","14","        This class will be removed in 0.19.","15","        Use","16","        :class:`sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis`","17","        instead.","18","    \"\"\"","19","    pass"],"delete":["5","from .discriminant_analysis import QuadraticDiscriminantAnalysis as QDA"]}],"sklearn\/gaussian_process\/gaussian_process.py":[{"add":["66","    .. deprecated:: 0.18","67","        This class will be removed in 0.20.","68","        Use the :class:`GaussianProcessRegressor` instead."],"delete":["66","    Note that this class was deprecated in version 0.18 and will be","67","    removed in 0.20. Use the GaussianProcessRegressor instead."]}],"doc\/templates\/deprecated_function.rst":[{"add":[],"delete":[]}],"sklearn\/lda.py":[{"add":["1","from .discriminant_analysis import LinearDiscriminantAnalysis as _LDA","2","","7","","8","class LDA(_LDA):","9","    \"\"\"","10","    Alias for","11","    :class:`sklearn.discriminant_analysis.LinearDiscriminantAnalysis`.","12","","13","    .. deprecated:: 0.17","14","        This class will be removed in 0.19.","15","        Use","16","        :class:`sklearn.discriminant_analysis.LinearDiscriminantAnalysis`","17","        instead.","18","    \"\"\"","19","    pass"],"delete":["5","from .discriminant_analysis import LinearDiscriminantAnalysis as LDA"]}],"sklearn\/mixture\/dpgmm.py":[{"add":["631","    \"\"\"Dirichlet Process Gaussian Mixture Models","632","","633","    .. deprecated:: 0.18","634","        This class will be removed in 0.20.","635","        Use :class:`sklearn.mixture.BayesianGaussianMixture` with","636","        parameter ``weight_concentration_prior_type='dirichlet_process'``","637","        instead.","638","","639","    \"\"\"","640","","659","    .. deprecated:: 0.18","660","        This class will be removed in 0.20.","661","        Use :class:`sklearn.mixture.BayesianGaussianMixture` with parameter","662","        ``weight_concentration_prior_type='dirichlet_distribution'`` instead.","663",""],"delete":[]}],"doc\/templates\/deprecated_class_without_init.rst":[{"add":[],"delete":[]}],"sklearn\/decomposition\/pca.py":[{"add":["556","    .. deprecated:: 0.18","557","        This class will be removed in 0.20.","558","        Use :class:`PCA` with parameter svd_solver 'randomized' instead.","559","        The new implementation DOES NOT store whiten ``components_``.","560","        Apply transform to get them.","561",""],"delete":[]}],"sklearn\/grid_search.py":[{"add":["48","    .. deprecated:: 0.18","49","        This module will be removed in 0.20.","50","        Use :class:`sklearn.model_selection.ParameterGrid` instead.","51","","171","    .. deprecated:: 0.18","172","        This module will be removed in 0.20.","173","        Use :class:`sklearn.model_selection.ParameterSampler` instead.","174","","275","    .. deprecated:: 0.18","276","        This module will be removed in 0.20.","277","        Use :func:`sklearn.model_selection.fit_grid_point` instead.","278","","632","    .. deprecated:: 0.18","633","        This module will be removed in 0.20.","634","        Use :class:`sklearn.model_selection.GridSearchCV` instead.","635","","838","    .. deprecated:: 0.18","839","        This module will be removed in 0.20.","840","        Use :class:`sklearn.model_selection.RandomizedSearchCV` instead."],"delete":[]}],"doc\/themes\/scikit-learn\/static\/nature.css_t":[{"add":["605","div.warning, div.deprecated {"],"delete":["605","div.warning {"]}]}},"c17156106c191c1ca6ea40122539580b1c38e13b":{"changes":{"doc\/whats_new.rst":"MODIFY","sklearn\/preprocessing\/tests\/test_data.py":"MODIFY","sklearn\/preprocessing\/data.py":"MODIFY"},"diff":{"doc\/whats_new.rst":[{"add":["87","   - For sparse matrices, :func:`preprocessing.normalize` with ``return_norm=True``","88","     will now raise a ``NotImplementedError`` with 'l1' or 'l2' norm and with norm 'max'","89","     the norms returned will be the same as for dense matrices (:issue:`7771`).","90","     By `Ang Lu <https:\/\/github.com\/luang008>`_.","91",""],"delete":[]}],"sklearn\/preprocessing\/tests\/test_data.py":[{"add":["1317","    # Test return_norm","1318","    X_dense = np.array([[3.0, 0, 4.0], [1.0, 0.0, 0.0], [2.0, 3.0, 0.0]])","1319","    for norm in ('l1', 'l2', 'max'):","1320","        _, norms = normalize(X_dense, norm=norm, return_norm=True)","1321","        if norm == 'l1':","1322","            assert_array_almost_equal(norms, np.array([7.0, 1.0, 5.0]))","1323","        elif norm == 'l2':","1324","            assert_array_almost_equal(norms, np.array([5.0, 1.0, 3.60555127]))","1325","        else:","1326","            assert_array_almost_equal(norms, np.array([4.0, 1.0, 3.0]))","1327","","1328","    X_sparse = sparse.csr_matrix(X_dense)","1329","    for norm in ('l1', 'l2'):","1330","        assert_raises(NotImplementedError, normalize, X_sparse,","1331","                      norm=norm, return_norm=True)","1332","    _, norms = normalize(X_sparse, norm='max', return_norm=True)","1333","    assert_array_almost_equal(norms, np.array([4.0, 1.0, 3.0]))","1334",""],"delete":[]}],"sklearn\/preprocessing\/data.py":[{"add":["1327","    Returns","1328","    -------","1329","    X : {array-like, sparse matrix}, shape [n_samples, n_features]","1330","        Normalized input X.","1331","","1332","    norms : array, shape [n_samples] if axis=1 else [n_features]","1333","        An array of norms along given axis for X.","1334","        When X is sparse, a NotImplementedError will be raised","1335","        for norm 'l1' or 'l2'.","1336","","1358","        if return_norm and norm in ('l1', 'l2'):","1359","            raise NotImplementedError(\"return_norm=True is not implemented \"","1360","                                      \"for sparse matrices with norm 'l1' \"","1361","                                      \"or norm 'l2'\")","1368","            norms_elementwise = norms.repeat(np.diff(X.indptr))","1369","            mask = norms_elementwise != 0","1370","            X.data[mask] \/= norms_elementwise[mask]"],"delete":["1354","            norms = norms.repeat(np.diff(X.indptr))","1355","            mask = norms != 0","1356","            X.data[mask] \/= norms[mask]"]}]}},"69ca580cd811d55cc996c9b1788bc6bae0ed8ad4":{"changes":{"circle.yml":"MODIFY","build_tools\/circle\/push_doc.sh":"MODIFY"},"diff":{"circle.yml":[{"add":["15","   branch: \/^master$|^[0-9]+\\.[0-9]+\\.X$\/"],"delete":["15","   branch: master"]}],"build_tools\/circle\/push_doc.sh":[{"add":["14","if [ \"$CIRCLE_BRANCH\" = \"master\" ]","15","then","16","\tdir=dev","17","else","18","\t# Strip off .X","19","\tdir=\"${CIRCLE_BRANCH::-2}\"","20","fi","21","","22","MSG=\"Pushing the docs to $dir\/ for branch: $CIRCLE_BRANCH, commit $CIRCLE_SHA1\"","29","git checkout $CIRCLE_BRANCH","30","git reset --hard origin\/$CIRCLE_BRANCH","31","git rm -rf $dir\/ && rm -rf $dir\/","32","cp -R $HOME\/scikit-learn\/doc\/_build\/html\/stable $dir","36","git add -f $dir\/","37","git commit -m \"$MSG\" $dir"],"delete":["14","MSG=\"Pushing the docs for revision for branch: $CIRCLE_BRANCH, commit $CIRCLE_SHA1\"","21","git checkout master","22","git reset --hard origin\/master","23","git rm -rf dev\/ && rm -rf dev\/","24","cp -R $HOME\/scikit-learn\/doc\/_build\/html\/stable dev","28","git add -f dev\/","29","git commit -m \"$MSG\" dev"]}]}},"072eefecf2d59a447317dc43a9daae36040580c8":{"changes":{"doc\/themes\/scikit-learn\/layout.html":"MODIFY"},"diff":{"doc\/themes\/scikit-learn\/layout.html":[{"add":["274","      <p class=\"doc-version\">This documentation is for {{project}} <b style=\"font-size: 110%;\">version {{ release|e }}<\/b> &mdash; <a href=\"http:\/\/scikit-learn.org\/stable\/support.html#documentation-resources\">Other versions<\/a><\/p>","276","    <p class=\"citing\">Please <b><a href=\"about.html#citing-scikit-learn\" style=\"font-size: 110%;\">cite us <\/a><\/b>if you use the software.<\/p>"],"delete":["274","      <p class=\"doc-version\">This documentation is for {{project}} <strong>version {{ release|e }}<\/strong> &mdash; <a href=\"http:\/\/scikit-learn.org\/stable\/support.html#documentation-resources\">Other versions<\/a><\/p>","276","    <p class=\"citing\">If you use the software, please consider <a href=\"{{pathto('about')}}#citing-scikit-learn\">citing scikit-learn<\/a>.<\/p>"]}]}},"f1ec4300c32387c75c222fafc1507e920196b619":{"changes":{"sklearn\/mixture\/gaussian_mixture.py":"MODIFY"},"diff":{"sklearn\/mixture\/gaussian_mixture.py":[{"add":["257","    resp : array-like, shape (n_samples, n_components)"],"delete":["257","    resp : array-like, shape (n_samples, n_features)"]}]}},"4da44c85414acdcc923f8b188849dfa0578736c3":{"changes":{"sklearn\/preprocessing\/tests\/test_imputation.py":"MODIFY","sklearn\/decomposition\/tests\/test_nmf.py":"MODIFY","sklearn\/gaussian_process\/tests\/test_gpr.py":"MODIFY","sklearn\/utils\/tests\/test_extmath.py":"MODIFY"},"diff":{"sklearn\/preprocessing\/tests\/test_imputation.py":[{"add":["14","","93","        [np.nan, 0, 0, 0, 5],","94","        [np.nan, 1, 0, np.nan, 3],","95","        [np.nan, 2, 0, 0, 0],","96","        [np.nan, 6, 0, 5, 13],","100","        [3, 5],","101","        [1, 3],","102","        [2, 7],","145","    values = np.arange(1, shape[0] + 1)","237","        [-1, -1, 0, 5],","238","        [-1, 2, -1, 3],","239","        [-1, 1, 3, -1],","240","        [-1, 2, 3, 7],","244","        [2, 0, 5],","245","        [2, 3, 3],","246","        [1, 3, 3],","247","        [2, 3, 7],","316","    assert_array_equal(X, Xt)","324","    assert_array_equal(X.data, Xt.data)","332","    assert_array_equal(X.data, Xt.data)"],"delete":["8","from sklearn.utils.testing import assert_true","15"," ","94","        [np.nan, 0, 0,  0,  5],","95","        [np.nan, 1, 0,  np.nan,  3],","96","        [np.nan, 2, 0,  0, 0],","97","        [np.nan, 6, 0,  5,  13],","101","        [3,  5],","102","        [1,  3],","103","        [2,  7],","146","    values = np.arange(1, shape[0]+1)","238","        [-1, -1,  0,  5],","239","        [-1,  2, -1,  3],","240","        [-1,  1,  3, -1],","241","        [-1,  2,  3,  7],","245","        [2,  0,  5],","246","        [2,  3,  3],","247","        [1,  3,  3],","248","        [2,  3,  7],","317","    assert_true(np.all(X == Xt))","325","    assert_true(np.all(X.data == Xt.data))","333","    assert_true(np.all(X.data == Xt.data))"]}],"sklearn\/decomposition\/tests\/test_nmf.py":[{"add":["70","        assert_almost_equal(evl[ref != 0], ref[ref != 0])","130","    m = NMF(solver='cd', n_components=n_components, init='custom',","131","            random_state=0)","132","    m.fit_transform(A, W=W_init, H=H_init)","133","    m.transform(A)","143","        m.fit_transform(A)","238","    msg = (\"Number of components must be a positive integer; \"","239","           \"got (n_components=1.5)\")","241","    msg = (\"Number of components must be a positive integer; \"","242","           \"got (n_components='2')\")"],"delete":["70","        assert_true(np.allclose(evl[ref != 0], ref[ref != 0]))","130","    m = NMF(solver='cd', n_components=n_components, init='custom', random_state=0)","131","    ft = m.fit_transform(A, W=W_init, H=H_init)","132","    t = m.transform(A)","142","        ft = m.fit_transform(A)","237","    msg = \"Number of components must be a positive integer; got (n_components=1.5)\"","239","    msg = \"Number of components must be a positive integer; got (n_components='2')\""]}],"sklearn\/gaussian_process\/tests\/test_gpr.py":[{"add":["43","        assert_almost_equal(y_pred, y)","44","        assert_almost_equal(np.diag(y_cov), 0.)"],"delete":["43","        assert_true(np.allclose(y_pred, y))","44","        assert_true(np.allclose(np.diag(y_cov), 0.))"]}],"sklearn\/utils\/tests\/test_extmath.py":[{"add":["66","        assert_array_equal(mode, mode2)","67","        assert_array_equal(score, score2)"],"delete":["19","from sklearn.utils.testing import assert_raise_message","67","        assert_true(np.all(mode == mode2))","68","        assert_true(np.all(score == score2))"]}]}},"707b6f9fb72adbba0d643e0a8b057e7833b150a5":{"changes":{"sklearn\/tests\/test_grid_search.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/grid_search.py":"MODIFY"},"diff":{"sklearn\/tests\/test_grid_search.py":[{"add":["44","from sklearn.linear_model import Ridge","788","","789","","790","def test_classes__property():","791","    # Test that classes_ property matches best_esimator_.classes_","792","    X = np.arange(100).reshape(10, 10)","793","    y = np.array([0] * 5 + [1] * 5)","794","    Cs = [.1, 1, 10]","795","","796","    grid_search = GridSearchCV(LinearSVC(random_state=0), {'C': Cs})","797","    grid_search.fit(X, y)","798","    assert_array_equal(grid_search.best_estimator_.classes_,","799","                       grid_search.classes_)","800","","801","    # Test that regressors do not have a classes_ attribute","802","    grid_search = GridSearchCV(Ridge(), {'alpha': [1.0, 2.0]})","803","    grid_search.fit(X, y)","804","    assert_false(hasattr(grid_search, 'classes_'))"],"delete":[]}],"doc\/whats_new.rst":[{"add":["21","   - Added ``classes_`` attribute to :class:`model_selection.GridSearchCV`","22","     that matches the ``classes_`` attribute of ``best_estimator_``. (`#7661","23","     <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/7661>`_) by `Alyssa","24","     Batula`_ and `Dylan Werner-Meier`_.","25",""],"delete":[]}],"sklearn\/grid_search.py":[{"add":["389","    @property","390","    def classes_(self):","391","        return self.best_estimator_.classes_","392","","694","        either binary or multiclass,","906","        either binary or multiclass,"],"delete":["690","        either binary or multiclass, ","902","        either binary or multiclass, "]}]}},"8695ff5969d84429e4a84ee8fea835e52e6d230d":{"changes":{"sklearn\/multioutput.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/tests\/test_multioutput.py":"MODIFY"},"diff":{"sklearn\/multioutput.py":[{"add":["24","from .utils.metaestimators import if_delegate_has_method","40","def _partial_fit_estimator(estimator, X, y, classes=None, sample_weight=None,","41","                           first_time=True):","42","    if first_time:","43","        estimator = clone(estimator)","44","","45","    if sample_weight is not None:","46","        if classes is not None:","47","            estimator.partial_fit(X, y, classes=classes,","48","                                  sample_weight=sample_weight)","49","        else:","50","            estimator.partial_fit(X, y, sample_weight=sample_weight)","51","    else:","52","        if classes is not None:","53","            estimator.partial_fit(X, y, classes=classes)","54","        else:","55","            estimator.partial_fit(X, y)","56","    return estimator","57","","58","","65","    @if_delegate_has_method('estimator')","66","    def partial_fit(self, X, y, classes=None, sample_weight=None):","67","        \"\"\"Incrementally fit the model to data.","68","        Fit a separate model for each output variable.","69","","70","        Parameters","71","        ----------","72","        X : (sparse) array-like, shape (n_samples, n_features)","73","            Data.","74","","75","        y : (sparse) array-like, shape (n_samples, n_outputs)","76","            Multi-output targets.","77","","78","        classes : list of numpy arrays, shape (n_outputs)","79","            Each array is unique classes for one output in str\/int","80","            Can be obtained by via","81","            ``[np.unique(y[:, i]) for i in range(y.shape[1])]``, where y is the","82","            target matrix of the entire dataset.","83","            This argument is required for the first call to partial_fit","84","            and can be omitted in the subsequent calls.","85","            Note that y doesn't need to contain all labels in `classes`.","86","","87","        sample_weight : array-like, shape = (n_samples) or None","88","            Sample weights. If None, then samples are equally weighted.","89","            Only supported if the underlying regressor supports sample","90","            weights.","91","","92","        Returns","93","        -------","94","        self : object","95","            Returns self.","96","        \"\"\"","97","        X, y = check_X_y(X, y,","98","                         multi_output=True,","99","                         accept_sparse=True)","100","","101","        if y.ndim == 1:","102","            raise ValueError(\"y must have at least two dimensions for \"","103","                             \"multi-output regression but has only one.\")","104","","105","        if (sample_weight is not None and","106","                not has_fit_parameter(self.estimator, 'sample_weight')):","107","            raise ValueError(\"Underlying estimator does not support\"","108","                             \" sample weights.\")","109","","110","        first_time = not hasattr(self, 'estimators_')","111","","112","        self.estimators_ = Parallel(n_jobs=self.n_jobs)(","113","            delayed(_partial_fit_estimator)(","114","                self.estimators_[i] if not first_time else self.estimator,","115","                X, y[:, i],","116","                classes[i] if classes is not None else None,","117","                sample_weight, first_time) for i in range(y.shape[1]))","118","        return self","119","","153","                             \"multi-output regression but has only one.\")","157","            raise ValueError(\"Underlying estimator does not support\"","160","        self.estimators_ = Parallel(n_jobs=self.n_jobs)(","161","            delayed(_fit_estimator)(","162","                self.estimator, X, y[:, i], sample_weight)","163","            for i in range(y.shape[1]))","187","        y = Parallel(n_jobs=self.n_jobs)(","188","            delayed(parallel_helper)(e, 'predict', X)","189","            for e in self.estimators_)","213","","217","    def partial_fit(self, X, y, sample_weight=None):","218","        \"\"\"Incrementally fit the model to data.","219","        Fit a separate model for each output variable.","220","","221","        Parameters","222","        ----------","223","        X : (sparse) array-like, shape (n_samples, n_features)","224","            Data.","225","","226","        y : (sparse) array-like, shape (n_samples, n_outputs)","227","            Multi-output targets.","228","","229","        sample_weight : array-like, shape = (n_samples) or None","230","            Sample weights. If None, then samples are equally weighted.","231","            Only supported if the underlying regressor supports sample","232","            weights.","233","","234","        Returns","235","        -------","236","        self : object","237","            Returns self.","238","        \"\"\"","239","        super(MultiOutputRegressor, self).partial_fit(","240","            X, y, sample_weight=sample_weight)","241",""],"delete":["78","                             \"multi target regression but has only one.\")","82","            raise ValueError(\"Underlying regressor does not support\"","85","        self.estimators_ = Parallel(n_jobs=self.n_jobs)(delayed(_fit_estimator)(","86","            self.estimator, X, y[:, i], sample_weight) for i in range(y.shape[1]))","110","        y = Parallel(n_jobs=self.n_jobs)(delayed(parallel_helper)(e, 'predict', X)","111","                                         for e in self.estimators_)"]}],"doc\/whats_new.rst":[{"add":["51","   - :class:`multioutput.MultiOutputRegressor` and :class:`multioutput.MultiOutputClassifier`","52","     now support online learning using `partial_fit`.","53","     issue: `8053` by :user:`Peng Yu <yupbank>`.","54",""],"delete":[]}],"sklearn\/tests\/test_multioutput.py":[{"add":["0","from __future__ import division","6","from sklearn.utils.testing import assert_false","10","from sklearn.utils.testing import assert_not_equal","11","from sklearn.utils.testing import assert_array_almost_equal","16","from sklearn.linear_model import Lasso","17","from sklearn.linear_model import SGDClassifier","18","from sklearn.linear_model import SGDRegressor","19","from sklearn.linear_model import LogisticRegression","34","        references[:, n] = rgr.predict(X_test)","43","def test_multi_target_regression_partial_fit():","44","    X, y = datasets.make_regression(n_targets=3)","48","    references = np.zeros_like(y_test)","49","    half_index = 25","50","    for n in range(3):","51","        sgr = SGDRegressor(random_state=0)","52","        sgr.partial_fit(X_train[:half_index], y_train[:half_index, n])","53","        sgr.partial_fit(X_train[half_index:], y_train[half_index:, n])","54","        references[:, n] = sgr.predict(X_test)","55","","56","    sgr = MultiOutputRegressor(SGDRegressor(random_state=0))","57","","58","    sgr.partial_fit(X_train[:half_index], y_train[:half_index])","59","    sgr.partial_fit(X_train[half_index:], y_train[half_index:])","60","","61","    y_pred = sgr.predict(X_test)","62","    assert_almost_equal(references, y_pred)","63","","64","","65","def test_multi_target_regression_one_target():","66","    # Test multi target regression raises","67","    X, y = datasets.make_regression(n_targets=1)","68","","70","    assert_raises(ValueError, rgr.fit, X, y)","76","    X_test = X[50:]","86","        assert_almost_equal(rgr.predict(X_test),","87","                            rgr_sparse.predict(sparse(X_test)))","91","    X = [[1, 2, 3], [4, 5, 6]]","104","def test_multi_target_sample_weight_partial_fit():","105","    # weighted regressor","106","    X = [[1, 2, 3], [4, 5, 6]]","107","    y = [[3.141, 2.718], [2.718, 3.141]]","108","    w = [2., 1.]","109","    rgr_w = MultiOutputRegressor(SGDRegressor(random_state=0))","110","    rgr_w.partial_fit(X, y, w)","111","","112","    # weighted with different weights","113","    w = [2., 2.]","114","    rgr = MultiOutputRegressor(SGDRegressor(random_state=0))","115","    rgr.partial_fit(X, y, w)","116","","117","    assert_not_equal(rgr.predict(X)[0][0], rgr_w.predict(X)[0][0])","118","","119","","122","    Xw = [[1, 2, 3], [4, 5, 6]]","129","    X = [[1, 2, 3], [1, 2, 3], [4, 5, 6]]","134","    X_test = [[1.5, 2.5, 3.5], [3.5, 4.5, 5.5]]","137","","149","classes = list(map(np.unique, (y1, y2, y3)))","150","","151","","152","def test_multi_output_classification_partial_fit_parallelism():","153","    sgd_linear_clf = SGDClassifier(loss='log', random_state=1)","154","    mor = MultiOutputClassifier(sgd_linear_clf, n_jobs=-1)","155","    mor.partial_fit(X, y, classes)","156","    est1 = mor.estimators_[0]","157","    mor.partial_fit(X, y)","158","    est2 = mor.estimators_[0]","159","    # parallelism requires this to be the case for a sane implementation","160","    assert_false(est1 is est2)","161","","162","","163","def test_multi_output_classification_partial_fit():","164","    # test if multi_target initializes correctly with base estimator and fit","165","    # assert predictions work as expected for predict","166","","167","    sgd_linear_clf = SGDClassifier(loss='log', random_state=1)","168","    multi_target_linear = MultiOutputClassifier(sgd_linear_clf)","169","","170","    # train the multi_target_linear and also get the predictions.","171","    half_index = X.shape[0] \/\/ 2","172","    multi_target_linear.partial_fit(","173","        X[:half_index], y[:half_index], classes=classes)","174","","175","    first_predictions = multi_target_linear.predict(X)","176","    assert_equal((n_samples, n_outputs), first_predictions.shape)","177","","178","    multi_target_linear.partial_fit(X[half_index:], y[half_index:])","179","    second_predictions = multi_target_linear.predict(X)","180","    assert_equal((n_samples, n_outputs), second_predictions.shape)","181","","182","    # train the linear classification with each column and assert that","183","    # predictions are equal after first partial_fit and second partial_fit","184","    for i in range(3):","185","        # create a clone with the same state","186","        sgd_linear_clf = clone(sgd_linear_clf)","187","        sgd_linear_clf.partial_fit(","188","            X[:half_index], y[:half_index, i], classes=classes[i])","189","        assert_array_equal(sgd_linear_clf.predict(X), first_predictions[:, i])","190","        sgd_linear_clf.partial_fit(X[half_index:], y[half_index:, i])","191","        assert_array_equal(sgd_linear_clf.predict(X), second_predictions[:, i])","192","","193","","194","def test_mutli_output_classifiation_partial_fit_no_first_classes_exception():","195","    sgd_linear_clf = SGDClassifier(loss='log', random_state=1)","196","    multi_target_linear = MultiOutputClassifier(sgd_linear_clf)","197","    assert_raises_regex(ValueError, \"classes must be passed on the first call \"","198","                                    \"to partial_fit.\",","199","                        multi_target_linear.partial_fit, X, y)","307","def test_multi_output_classification_partial_fit_sample_weights():","308","    # weighted classifier","309","    Xw = [[1, 2, 3], [4, 5, 6], [1.5, 2.5, 3.5]]","310","    yw = [[3, 2], [2, 3], [3, 2]]","311","    w = np.asarray([2., 1., 1.])","312","    sgd_linear_clf = SGDClassifier(random_state=1)","313","    clf_w = MultiOutputClassifier(sgd_linear_clf)","314","    clf_w.fit(Xw, yw, w)","315","","316","    # unweighted, but with repeated samples","317","    X = [[1, 2, 3], [1, 2, 3], [4, 5, 6], [1.5, 2.5, 3.5]]","318","    y = [[3, 2], [3, 2], [2, 3], [3, 2]]","319","    sgd_linear_clf = SGDClassifier(random_state=1)","320","    clf = MultiOutputClassifier(sgd_linear_clf)","321","    clf.fit(X, y)","322","    X_test = [[1.5, 2.5, 3.5]]","323","    assert_array_almost_equal(clf.predict(X_test), clf_w.predict(X_test))","324","","325",""],"delete":["12","from sklearn.linear_model import Lasso, LogisticRegression","27","        references[:,n] = rgr.predict(X_test)","36","def test_multi_target_regression_one_target():","37","    # Test multi target regression raises","38","    X, y = datasets.make_regression(n_targets=1)","43","    assert_raises(ValueError, rgr.fit, X_train, y_train)","49","    X_test, y_test = X[50:], y[50:]","59","        assert_almost_equal(rgr.predict(X_test), rgr_sparse.predict(sparse(X_test)))","63","    X = [[1,2,3], [4,5,6]]","78","    Xw = [[1,2,3], [4,5,6]]","85","    X = [[1,2,3], [1,2,3], [4,5,6]]","90","    X_test = [[1.5,2.5,3.5], [3.5,4.5,5.5]]"]}]}},"456fb5697fd31c60eac6596ba9a787319dcb8035":{"changes":{"doc\/whats_new.rst":"MODIFY","sklearn\/feature_selection\/univariate_selection.py":"MODIFY","sklearn\/feature_selection\/tests\/test_feature_select.py":"MODIFY"},"diff":{"doc\/whats_new.rst":[{"add":["112","   - Added ability to use sparse matrices in :func:`feature_selection.f_regression`","113","     with ``center=True``. :issue:`8065` by :user:`Daniel LeJeune <acadiansith>`.","114",""],"delete":[]}],"sklearn\/feature_selection\/univariate_selection.py":[{"add":["269","    n_samples = X.shape[0]","270","","271","    # compute centered values","272","    # note that E[(x - mean(x))*(y - mean(y))] = E[x*(y - mean(y))], so we","273","    # need not center X","276","        if issparse(X):","277","            X_means = X.mean(axis=0).getA1()","278","        else:","279","            X_means = X.mean(axis=0)","280","        # compute the scaled standard deviations via moments","281","        X_norms = np.sqrt(row_norms(X.T, squared=True) -","282","                          n_samples * X_means ** 2)","283","    else:","284","        X_norms = row_norms(X.T)","288","    corr \/= X_norms"],"delete":["268","    if issparse(X) and center:","269","        raise ValueError(\"center=True only allowed for dense data\")","273","        X = X.copy('F')  # faster in fortran","274","        X -= X.mean(axis=0)","278","    corr \/= row_norms(X.T)"]}],"sklearn\/feature_selection\/tests\/test_feature_select.py":[{"add":["94","    # with centering, compare with sparse","95","    F, pv = f_regression(X, y, center=True)","96","    F_sparse, pv_sparse = f_regression(sparse.csr_matrix(X), y, center=True)","97","    assert_array_almost_equal(F_sparse, F)","98","    assert_array_almost_equal(pv_sparse, pv)","99",""],"delete":[]}]}},"919b4a8fbdf0e313fb702bd083abb31d67f7d8f9":{"changes":{"sklearn\/ensemble\/gradient_boosting.py":"MODIFY","doc\/whats_new.rst":"MODIFY"},"diff":{"sklearn\/ensemble\/gradient_boosting.py":[{"add":["512","        # prevents overflow and division by zero","513","        if abs(denominator) < 1e-150:","579","        # prevents overflow and division by zero","580","        if abs(denominator) < 1e-150:","637","        # prevents overflow and division by zero","638","        if abs(denominator) < 1e-150:"],"delete":["512","        if denominator == 0.0:","578","        if denominator == 0.0:","635","        if denominator == 0.0:"]}],"doc\/whats_new.rst":[{"add":["187","   - Fixed a bug in :class:`sklearn.ensemble.GradientBoostingClassifier`","188","     and :class:`sklearn.ensemble.GradientBoostingRegressor`","189","     where a float being compared to ``0.0`` using ``==`` caused a divide by zero","190","     error. This was fixed in :issue:`7970` by :user:`He Chen <chenhe95>`."],"delete":[]}]}},"5adc83227b0d5c82c85f2f16c2c02efaaacf25c0":{"changes":{"sklearn\/metrics\/pairwise.py":"MODIFY"},"diff":{"sklearn\/metrics\/pairwise.py":[{"add":["26","from ..externals.joblib import cpu_count"],"delete":["26","from ..externals.joblib.parallel import cpu_count"]}]}},"04b67e2bcd26bfc8ce50b6c4b61bd6333809ed62":{"changes":{"sklearn\/kernel_ridge.py":"MODIFY","sklearn\/exceptions.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/utils\/estimator_checks.py":"MODIFY","sklearn\/ensemble\/weight_boosting.py":"MODIFY","sklearn\/linear_model\/ridge.py":"MODIFY","sklearn\/utils\/tests\/test_estimator_checks.py":"MODIFY"},"diff":{"sklearn\/kernel_ridge.py":[{"add":["11","from .utils import check_array, check_X_y","137","        sample_weight : float or array-like of shape [n_samples]","147","        if sample_weight is not None and not isinstance(sample_weight, float):","148","            sample_weight = check_array(sample_weight, ensure_2d=False)"],"delete":["11","from .utils import check_X_y","137","        sample_weight : float or numpy array of shape [n_samples]"]}],"sklearn\/exceptions.py":[{"add":["13","           'SkipTestWarning',","141","class SkipTestWarning(UserWarning):","142","    \"\"\"Warning class used to notify the user of a test that was skipped.","143","","144","    For example, one of the estimator checks requires a pandas import.","145","    If the pandas package cannot be imported, the test will be skipped rather","146","    than register as a failure.","147","    \"\"\"","148","","149",""],"delete":[]}],"doc\/whats_new.rst":[{"add":["106","   - Fix estimators to accept a ``sample_weight`` parameter of type","107","     ``pandas.Series`` in their ``fit`` function. :issue:`7825` by","108","     `Kathleen Chen`_.","109","","4830","","4831",".. _Kathleen Chen: https:\/\/github.com\/kchen17"],"delete":[]}],"sklearn\/utils\/estimator_checks.py":[{"add":["47","from sklearn.exceptions import SkipTestWarning","52","from sklearn.utils.validation import has_fit_parameter","84","    yield check_sample_weights_pandas_series","256","        try:","257","            check(name, Estimator)","258","        except SkipTest as message:","259","            # the only SkipTest thrown currently results from not","260","            # being able to import pandas.","261","            warnings.warn(message, SkipTestWarning)","390","@ignore_warnings(category=DeprecationWarning)","391","def check_sample_weights_pandas_series(name, Estimator):","392","    # check that estimators will accept a 'sample_weight' parameter of","393","    # type pandas.Series in the 'fit' function.","394","    estimator = Estimator()","395","    if has_fit_parameter(estimator, \"sample_weight\"):","396","        try:","397","            import pandas as pd","398","            X = pd.DataFrame([[1, 1], [1, 2], [1, 3], [2, 1], [2, 2], [2, 3]])","399","            y = pd.Series([1, 1, 1, 2, 2, 2])","400","            weights = pd.Series([1] * 6)","401","            try:","402","                estimator.fit(X, y, sample_weight=weights)","403","            except ValueError:","404","                raise ValueError(\"Estimator {0} raises error if \"","405","                                 \"'sample_weight' parameter is of \"","406","                                 \"type pandas.Series\".format(name))","407","        except ImportError:","408","            raise SkipTest(\"pandas is not installed: not testing for \"","409","                           \"input of type pandas.Series to class weight.\")","410","","411",""],"delete":["200","","254","        check(name, Estimator)"]}],"sklearn\/ensemble\/weight_boosting.py":[{"add":["118","            sample_weight = check_array(sample_weight, ensure_2d=False)"],"delete":[]}],"sklearn\/linear_model\/ridge.py":[{"add":["959","        if sample_weight is not None and not isinstance(sample_weight, float):","960","            sample_weight = check_array(sample_weight, ensure_2d=False)"],"delete":[]}],"sklearn\/utils\/tests\/test_estimator_checks.py":[{"add":["75","class NoSampleWeightPandasSeriesType(BaseEstimator):","76","    def fit(self, X, y, sample_weight=None):","77","        # Convert data","78","        X, y = check_X_y(X, y,","79","                         accept_sparse=(\"csr\", \"csc\"),","80","                         multi_output=True,","81","                         y_numeric=True)","82","        # Function is only called after we verify that pandas is installed","83","        from pandas import Series","84","        if isinstance(sample_weight, Series):","85","            raise ValueError(\"Estimator does not accept 'sample_weight'\"","86","                             \"of type pandas.Series\")","87","        return self","88","","89","    def predict(self, X):","90","        X = check_array(X)","91","        return np.ones(X.shape[0])","92","","93","","107","    # check that sample_weights in fit accepts pandas.Series type","108","    try:","109","        from pandas import Series  # noqa","110","        msg = (\"Estimator NoSampleWeightPandasSeriesType raises error if \"","111","               \"'sample_weight' parameter is of type pandas.Series\")","112","        assert_raises_regex(","113","            ValueError, msg, check_estimator, NoSampleWeightPandasSeriesType)","114","    except ImportError:","115","        pass"],"delete":[]}]}},"829efa5929c129a4112e539699f9034152332ee4":{"changes":{"sklearn\/model_selection\/_validation.py":"MODIFY","sklearn\/model_selection\/tests\/test_validation.py":"MODIFY","doc\/whats_new.rst":"MODIFY"},"diff":{"sklearn\/model_selection\/_validation.py":[{"add":["644","                   pre_dispatch=\"all\", verbose=0, shuffle=False,","645","                   random_state=None):","721","    shuffle : boolean, optional","722","        Whether to shuffle training data before taking prefixes of it","723","        based on``train_sizes``.","724","","725","    random_state : None, int or RandomState","726","        When shuffle=True, pseudo-random number generator state used for","727","        shuffling. If None, use default numpy RNG for shuffling.","728","","769","","770","    if shuffle:","771","        rng = check_random_state(random_state)","772","        cv_iter = ((rng.permutation(train), test) for train, test in cv_iter)","773","","777","            clone(estimator), X, y, classes, train,","778","            test, train_sizes_abs, scorer, verbose)","779","            for train, test in cv_iter)","781","        train_test_proportions = []","782","        for train, test in cv_iter:","783","            for n_train_samples in train_sizes_abs:","784","                train_test_proportions.append((train[:n_train_samples], test))","785","","787","            clone(estimator), X, y, scorer, train, test,","789","            for train, test in train_test_proportions)"],"delete":["644","                   pre_dispatch=\"all\", verbose=0):","720","    Returns","764","            clone(estimator), X, y, classes, train, test, train_sizes_abs,","765","            scorer, verbose) for train, test in cv.split(X, y, groups))","768","            clone(estimator), X, y, scorer, train[:n_train_samples], test,","770","            for train, test in cv_iter","771","            for n_train_samples in train_sizes_abs)"]}],"sklearn\/model_selection\/tests\/test_validation.py":[{"add":["562","    for shuffle_train in [False, True]:","563","        with warnings.catch_warnings(record=True) as w:","564","            train_sizes, train_scores, test_scores = learning_curve(","565","                estimator, X, y, cv=3, train_sizes=np.linspace(0.1, 1.0, 10),","566","                shuffle=shuffle_train)","567","        if len(w) > 0:","568","            raise RuntimeError(\"Unexpected warning: %r\" % w[0].message)","569","        assert_equal(train_scores.shape, (10, 3))","570","        assert_equal(test_scores.shape, (10, 3))","571","        assert_array_equal(train_sizes, np.linspace(2, 20, 10))","572","        assert_array_almost_equal(train_scores.mean(axis=1),","573","                                  np.linspace(1.9, 1.0, 10))","574","        assert_array_almost_equal(test_scores.mean(axis=1),","575","                                  np.linspace(0.1, 1.0, 10))","626","    for shuffle_train in [False, True]:","627","        train_sizes, train_scores, test_scores = learning_curve(","628","            estimator, X, y, cv=3, exploit_incremental_learning=True,","629","            train_sizes=np.linspace(0.1, 1.0, 10), shuffle=shuffle_train)","630","        assert_array_equal(train_sizes, np.linspace(2, 20, 10))","631","        assert_array_almost_equal(train_scores.mean(axis=1),","632","                                  np.linspace(1.9, 1.0, 10))","633","        assert_array_almost_equal(test_scores.mean(axis=1),","634","                                  np.linspace(0.1, 1.0, 10))","718","def test_learning_curve_with_shuffle():","719","    \"\"\"Following test case was designed this way to verify the code","720","    changes made in pull request: #7506.\"\"\"","721","    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [11, 12], [13, 14], [15, 16],","722","                 [17, 18], [19, 20], [7, 8], [9, 10], [11, 12], [13, 14],","723","                 [15, 16], [17, 18]])","724","    y = np.array([1, 1, 1, 2, 3, 4, 1, 1, 2, 3, 4, 1, 2, 3, 4])","725","    groups = np.array([1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 4, 4, 4, 4])","726","    estimator = PassiveAggressiveClassifier(shuffle=False)","727","","728","    cv = GroupKFold(n_splits=2)","729","    train_sizes_batch, train_scores_batch, test_scores_batch = learning_curve(","730","        estimator, X, y, cv=cv, n_jobs=1, train_sizes=np.linspace(0.3, 1.0, 3),","731","        groups=groups, shuffle=True, random_state=2)","732","    assert_array_almost_equal(train_scores_batch.mean(axis=1),","733","                              np.array([0.75, 0.3, 0.36111111]))","734","    assert_array_almost_equal(test_scores_batch.mean(axis=1),","735","                              np.array([0.36111111, 0.25, 0.25]))","736","    assert_raises(ValueError, learning_curve, estimator, X, y, cv=cv, n_jobs=1,","737","                  train_sizes=np.linspace(0.3, 1.0, 3), groups=groups)","738","","739","    train_sizes_inc, train_scores_inc, test_scores_inc = learning_curve(","740","        estimator, X, y, cv=cv, n_jobs=1, train_sizes=np.linspace(0.3, 1.0, 3),","741","        groups=groups, shuffle=True, random_state=2,","742","        exploit_incremental_learning=True)","743","    assert_array_almost_equal(train_scores_inc.mean(axis=1),","744","                              train_scores_batch.mean(axis=1))","745","    assert_array_almost_equal(test_scores_inc.mean(axis=1),","746","                              test_scores_batch.mean(axis=1))","747","","748",""],"delete":["562","    with warnings.catch_warnings(record=True) as w:","563","        train_sizes, train_scores, test_scores = learning_curve(","564","            estimator, X, y, cv=3, train_sizes=np.linspace(0.1, 1.0, 10))","565","    if len(w) > 0:","566","        raise RuntimeError(\"Unexpected warning: %r\" % w[0].message)","567","    assert_equal(train_scores.shape, (10, 3))","568","    assert_equal(test_scores.shape, (10, 3))","569","    assert_array_equal(train_sizes, np.linspace(2, 20, 10))","570","    assert_array_almost_equal(train_scores.mean(axis=1),","571","                              np.linspace(1.9, 1.0, 10))","572","    assert_array_almost_equal(test_scores.mean(axis=1),","573","                              np.linspace(0.1, 1.0, 10))","624","    train_sizes, train_scores, test_scores = learning_curve(","625","        estimator, X, y, cv=3, exploit_incremental_learning=True,","626","        train_sizes=np.linspace(0.1, 1.0, 10))","627","    assert_array_equal(train_sizes, np.linspace(2, 20, 10))","628","    assert_array_almost_equal(train_scores.mean(axis=1),","629","                              np.linspace(1.9, 1.0, 10))","630","    assert_array_almost_equal(test_scores.mean(axis=1),","631","                              np.linspace(0.1, 1.0, 10))"]}],"doc\/whats_new.rst":[{"add":["43","   - Added ``shuffle`` and ``random_state`` parameters to shuffle training","44","     data before taking prefixes of it based on training sizes in","45","     :func:`model_selection.learning_curve`.","46","     (`#7506` <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/7506>_) by","47","     `Narine Kokhlikyan`_.","48","","4869","","4870",".. _Narine Kokhlikyan: https:\/\/github.com\/NarineK"],"delete":[]}]}},"99342b6da44f7a48e51280c0d913e908c36215da":{"changes":{"README.rst":"MODIFY"},"diff":{"README.rst":[{"add":["1","","164","~~~~~~~~","165",""],"delete":["163","~~~~~~~~~~~~~"]}]}},"3d997697fdd166eff428ea9fd35734b6a8ba113e":{"changes":{"sklearn\/cluster\/hierarchical.py":"MODIFY","sklearn\/cluster\/tests\/test_hierarchical.py":"MODIFY"},"diff":{"sklearn\/cluster\/hierarchical.py":[{"add":["434","    if affinity == 'cosine' and np.any(~np.any(X, axis=1)):","435","        raise ValueError(","436","            'Cosine affinity cannot be used when X contains zero vectors')","437",""],"delete":[]}],"sklearn\/cluster\/tests\/test_hierarchical.py":[{"add":["130","def test_zero_cosine_linkage_tree():","131","    # Check that zero vectors in X produce an error when","132","    # 'cosine' affinity is used","133","    X = np.array([[0, 1],","134","                  [0, 0]])","135","    msg = 'Cosine affinity cannot be used when X contains zero vectors'","136","    assert_raise_message(ValueError, msg, linkage_tree, X, affinity='cosine')","137","","138",""],"delete":[]}]}},"3fbfb1bf5e24c826209fa5a5c080140a7abdac63":{"changes":{"sklearn\/model_selection\/_search.py":"MODIFY"},"diff":{"sklearn\/model_selection\/_search.py":[{"add":["1077","            'param_kernel' : masked_array(data = ['rbf', 'rbf', 'rbf'],"],"delete":["1077","            'param_kernel' : masked_array(data = ['rbf', rbf', 'rbf'],"]}]}},"74e4c422bd979fd68eef3ac1d3f5eedf6f9262f2":{"changes":{"sklearn\/tree\/_utils.pyx":"MODIFY","sklearn\/tree\/_criterion.pyx":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/tree\/_criterion.pxd":"MODIFY","sklearn\/tree\/_tree.pyx":"MODIFY","sklearn\/tree\/tests\/test_tree.py":"MODIFY"},"diff":{"sklearn\/tree\/_utils.pyx":[{"add":["64","    \"\"\"Return copied data as 1D numpy array of intp's.\"\"\"","67","    return np.PyArray_SimpleNewFromData(1, shape, np.NPY_INTP, data).copy()"],"delete":["64","    \"\"\"Encapsulate data into a 1D numpy array of intp's.\"\"\"","67","    return np.PyArray_SimpleNewFromData(1, shape, np.NPY_INTP, data)"]}],"sklearn\/tree\/_criterion.pyx":[{"add":["237","        self.n_samples = 0","279","        return (type(self),","712","        self.n_samples = n_samples","737","        return (type(self), (self.n_outputs, self.n_samples), self.__getstate__())","884","","1008","        self.n_samples = n_samples"],"delete":["275","","279","        return (ClassificationCriterion,","736","        return (RegressionCriterion, (self.n_outputs,), self.__getstate__())"]}],"doc\/whats_new.rst":[{"add":["69","   - Tree splitting criterion classes' cloning\/pickling is now memory safe","70","     (`#7680 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/7680>`_).","71","     By `Ibraim Ganiev`_.","72",""],"delete":[]}],"sklearn\/tree\/_criterion.pxd":[{"add":["36","    cdef SIZE_t n_samples                # Number of samples"],"delete":[]}],"sklearn\/tree\/_tree.pyx":[{"add":["549","            return sizet_ptr_to_ndarray(self.n_classes, self.n_outputs)"],"delete":["549","            # it's small; copy for memory safety","550","            return sizet_ptr_to_ndarray(self.n_classes, self.n_outputs).copy()"]}],"sklearn\/tree\/tests\/test_tree.py":[{"add":["3","import copy","45","from sklearn.tree.tree import CRITERIA_CLF","46","from sklearn.tree.tree import CRITERIA_REG","52","REG_CRITERIONS = (\"mse\", \"mae\", \"friedman_mse\")","1602","","1615","","1616","","1617","def test_criterion_copy():","1618","    # Let's check whether copy of our criterion has the same type","1619","    # and properties as original","1620","    n_outputs = 3","1621","    n_classes = np.arange(3, dtype=np.intp)","1622","    n_samples = 100","1623","","1624","    def _pickle_copy(obj):","1625","        return pickle.loads(pickle.dumps(obj))","1626","    for copy_func in [copy.copy, copy.deepcopy, _pickle_copy]:","1627","        for _, typename in CRITERIA_CLF.items():","1628","            criteria = typename(n_outputs, n_classes)","1629","            result = copy_func(criteria).__reduce__()","1630","            typename_, (n_outputs_, n_classes_), _ = result","1631","            assert_equal(typename, typename_)","1632","            assert_equal(n_outputs, n_outputs_)","1633","            assert_array_equal(n_classes, n_classes_)","1634","","1635","        for _, typename in CRITERIA_REG.items():","1636","            criteria = typename(n_outputs, n_samples)","1637","            result = copy_func(criteria).__reduce__()","1638","            typename_, (n_outputs_, n_samples_), _ = result","1639","            assert_equal(typename, typename_)","1640","            assert_equal(n_outputs, n_outputs_)","1641","            assert_equal(n_samples, n_samples_)"],"delete":["49","REG_CRITERIONS = (\"mse\", \"mae\")"]}]}},"9667ff292f1ace46bc2b1672836a3a04bd9fb5c0":{"changes":{"doc\/whats_new.rst":"MODIFY","sklearn\/feature_selection\/from_model.py":"MODIFY","sklearn\/feature_selection\/tests\/test_from_model.py":"MODIFY"},"diff":{"doc\/whats_new.rst":[{"add":["143","   - Fixed setting parameters when calling ``fit`` multiple times on","144","     :class:`feature_selection.SelectFromModel`. :issue:`7756` by `Andreas Mller`_","145",""],"delete":[]}],"sklearn\/feature_selection\/from_model.py":[{"add":["234","        self.estimator_ = clone(self.estimator)"],"delete":["234","        if not hasattr(self, \"estimator_\"):","235","            self.estimator_ = clone(self.estimator)"]}],"sklearn\/feature_selection\/tests\/test_from_model.py":[{"add":["4","from sklearn.utils.testing import assert_equal","147","def test_calling_fit_reinitializes():","148","    est = LinearSVC(random_state=0)","151","    transformer.set_params(estimator__C=100)","153","    assert_equal(transformer.estimator_.C, 100)"],"delete":["146","def test_warm_start():","147","    est = PassiveAggressiveClassifier(warm_start=True, random_state=0)","150","    old_model = transformer.estimator_","152","    new_model = transformer.estimator_","153","    assert_true(old_model is new_model)"]}]}},"89b2e45c918bc806c9f72ae184934c3b06c988c6":{"changes":{"sklearn\/cluster\/tests\/test_k_means.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/cluster\/k_means_.py":"MODIFY"},"diff":{"sklearn\/cluster\/tests\/test_k_means.py":[{"add":["814","def test_k_means_init_centers():","826","","827","","828","def test_sparse_k_means_init_centers():","829","    from sklearn.datasets import load_iris","830","","831","    iris = load_iris()","832","    X = iris.data","833","","834","    # Get a local optimum","835","    centers = KMeans(n_clusters=3).fit(X).cluster_centers_","836","","837","    # Fit starting from a local optimum shouldn't change the solution","838","    np.testing.assert_allclose(","839","        centers,","840","        KMeans(n_clusters=3,","841","               init=centers,","842","               n_init=1).fit(X).cluster_centers_","843","    )","844","","845","    # The same should be true when X is sparse","846","    X_sparse = sp.csr_matrix(X)","847","    np.testing.assert_allclose(","848","        centers,","849","        KMeans(n_clusters=3,","850","               init=centers,","851","               n_init=1).fit(X_sparse).cluster_centers_","852","    )","853","","854","","855","def test_sparse_validate_centers():","856","    from sklearn.datasets import load_iris","857","","858","    iris = load_iris()","859","    X = iris.data","860","","861","    # Get a local optimum","862","    centers = KMeans(n_clusters=4).fit(X).cluster_centers_","863","","864","    # Test that a ValueError is raised for validate_center_shape","865","    classifier = KMeans(n_clusters=3, init=centers, n_init=1)","866","","867","    msg = \"The shape of the initial centers \\(\\(4L?, 4L?\\)\\) \" \\","868","          \"does not match the number of clusters 3\"","869","    assert_raises_regex(ValueError, msg, classifier.fit, X)"],"delete":["814","def test_KMeans_init_centers():"]}],"doc\/whats_new.rst":[{"add":["99","   - Fix a bug regarding fitting :class:`sklearn.cluster.KMeans` with a","100","     sparse array X and initial centroids, where X's means were unnecessarily","101","     being subtracted from the centroids. :issue:`7872` by `Josh Karnofsky <https:\/\/github.com\/jkarno>`_.","102",""],"delete":[]}],"sklearn\/cluster\/k_means_.py":[{"add":["300","    # Validate init array","312","    # subtract of mean of x for more accurate distance computations","313","    if not sp.issparse(X):","314","        X_mean = X.mean(axis=0)","315","        # The copy was already done above","316","        X -= X_mean","317","","318","        if hasattr(init, '__array__'):","319","            init -= X_mean","320",""],"delete":["300","    # subtract of mean of x for more accurate distance computations","301","    if not sp.issparse(X) or hasattr(init, '__array__'):","302","        X_mean = X.mean(axis=0)","303","    if not sp.issparse(X):","304","        # The copy was already done above","305","        X -= X_mean","306","","311","        init -= X_mean"]}]}},"d2f8292e11af5e1e019a982fa871b7b254150f95":{"changes":{"sklearn\/tree\/_criterion.pyx":"MODIFY"},"diff":{"sklearn\/tree\/_criterion.pyx":[{"add":["174","        \"\"\"Compute the improvement in impurity","176","        This method computes the improvement in impurity when a split occurs.","177","        The weighted impurity improvement equation is the following:"],"delete":["174","        \"\"\"Placeholder for improvement in impurity after a split.","176","        Placeholder for a method which computes the improvement","177","        in impurity when a split occurs. The weighted impurity improvement","178","        equation is the following:"]}]}},"73bb593142aa344910caac17586748e2e6f4deaa":{"changes":{"doc\/whats_new.rst":"MODIFY"},"diff":{"doc\/whats_new.rst":[{"add":["43","   - Numerical issue with :class:`linear_model.RidgeCV` on centered data when","44","     `n_features > n_samples`. (`#6178","45","     <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/6178>`_) by `Bertrand","46","     Thirion`_"],"delete":["43","  - Numerical issue with :class:`linear_model.RidgeCV` on centered data when","44","    `n_features > n_samples`. (`#6178","45","    <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/6178>`_) by `Bertrand","46","    Thirion`_"]}]}},"b3a639ffc2d518b8862c61e4170403a400368571":{"changes":{"sklearn\/tests\/test_pipeline.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/pipeline.py":"MODIFY","examples\/plot_compare_reduction.py":"MODIFY","doc\/modules\/pipeline.rst":"MODIFY"},"diff":{"sklearn\/tests\/test_pipeline.py":[{"add":["3","","4","from tempfile import mkdtemp","5","import shutil","6","import time","7","","33","from sklearn.externals.joblib import Memory","133","class DummyTransf(Transf):","134","    \"\"\"Transformer which store the column means\"\"\"","135","","136","    def fit(self, X, y):","137","        self.means_ = np.mean(X, axis=0)","138","        # store timestamp to figure out whether the result of 'fit' has been","139","        # cached or not","140","        self.timestamp_ = time.time()","141","        return self","142","","143","","539","                       'memory': None,","819","","820","","821","def test_pipeline_wrong_memory():","822","    # Test that an error is raised when memory is not a string or a Memory","823","    # instance","824","    iris = load_iris()","825","    X = iris.data","826","    y = iris.target","827","    # Define memory as an integer","828","    memory = 1","829","    cached_pipe = Pipeline([('transf', DummyTransf()), ('svc', SVC())],","830","                           memory=memory)","831","    assert_raises_regex(ValueError, \"'memory' should either be a string or a\"","832","                        \" joblib.Memory instance, got 'memory=1' instead.\",","833","                        cached_pipe.fit, X, y)","834","","835","","836","def test_pipeline_memory():","837","    iris = load_iris()","838","    X = iris.data","839","    y = iris.target","840","    cachedir = mkdtemp()","841","    try:","842","        memory = Memory(cachedir=cachedir, verbose=10)","843","        # Test with Transformer + SVC","844","        clf = SVC(probability=True, random_state=0)","845","        transf = DummyTransf()","846","        pipe = Pipeline([('transf', clone(transf)), ('svc', clf)])","847","        cached_pipe = Pipeline([('transf', transf), ('svc', clf)],","848","                               memory=memory)","849","","850","        # Memoize the transformer at the first fit","851","        cached_pipe.fit(X, y)","852","        pipe.fit(X, y)","853","        # Get the time stamp of the tranformer in the cached pipeline","854","        ts = cached_pipe.named_steps['transf'].timestamp_","855","        # Check that cached_pipe and pipe yield identical results","856","        assert_array_equal(pipe.predict(X), cached_pipe.predict(X))","857","        assert_array_equal(pipe.predict_proba(X), cached_pipe.predict_proba(X))","858","        assert_array_equal(pipe.predict_log_proba(X),","859","                           cached_pipe.predict_log_proba(X))","860","        assert_array_equal(pipe.score(X, y), cached_pipe.score(X, y))","861","        assert_array_equal(pipe.named_steps['transf'].means_,","862","                           cached_pipe.named_steps['transf'].means_)","863","        assert_false(hasattr(transf, 'means_'))","864","        # Check that we are reading the cache while fitting","865","        # a second time","866","        cached_pipe.fit(X, y)","867","        # Check that cached_pipe and pipe yield identical results","868","        assert_array_equal(pipe.predict(X), cached_pipe.predict(X))","869","        assert_array_equal(pipe.predict_proba(X), cached_pipe.predict_proba(X))","870","        assert_array_equal(pipe.predict_log_proba(X),","871","                           cached_pipe.predict_log_proba(X))","872","        assert_array_equal(pipe.score(X, y), cached_pipe.score(X, y))","873","        assert_array_equal(pipe.named_steps['transf'].means_,","874","                           cached_pipe.named_steps['transf'].means_)","875","        assert_equal(ts, cached_pipe.named_steps['transf'].timestamp_)","876","        # Create a new pipeline with cloned estimators","877","        # Check that even changing the name step does not affect the cache hit","878","        clf_2 = SVC(probability=True, random_state=0)","879","        transf_2 = DummyTransf()","880","        cached_pipe_2 = Pipeline([('transf_2', transf_2), ('svc', clf_2)],","881","                                 memory=memory)","882","        cached_pipe_2.fit(X, y)","883","","884","        # Check that cached_pipe and pipe yield identical results","885","        assert_array_equal(pipe.predict(X), cached_pipe_2.predict(X))","886","        assert_array_equal(pipe.predict_proba(X),","887","                           cached_pipe_2.predict_proba(X))","888","        assert_array_equal(pipe.predict_log_proba(X),","889","                           cached_pipe_2.predict_log_proba(X))","890","        assert_array_equal(pipe.score(X, y), cached_pipe_2.score(X, y))","891","        assert_array_equal(pipe.named_steps['transf'].means_,","892","                           cached_pipe_2.named_steps['transf_2'].means_)","893","        assert_equal(ts, cached_pipe_2.named_steps['transf_2'].timestamp_)","894","    finally:","895","        shutil.rmtree(cachedir)"],"delete":[]}],"doc\/whats_new.rst":[{"add":["58","   - :class:`pipeline.Pipeline` allows to cache transformers","59","     within a pipeline by using the ``memory`` constructor parameter.","60","     By :issue:`7990` by :user:`Guillaume Lemaitre <glemaitre>`."],"delete":[]}],"sklearn\/pipeline.py":[{"add":["17","from .base import clone, BaseEstimator, TransformerMixin","18","from .externals.joblib import Parallel, delayed, Memory","91","    The transformers in the pipeline can be cached using ```memory`` argument.","110","    memory : Instance of joblib.Memory or string, optional (default=None)","111","        Used to caching the fitted transformers of the transformer of the","112","        pipeline. By default, no cache is performed.","113","        If a string is given, it is the path to the caching directory.","114","        Enabling caching triggers a clone of the transformers before fitting.","115","        Therefore, the transformer instance given to the pipeline cannot be","116","        inspected directly. Use the attribute ``named_steps`` or ``steps``","117","        to inspect estimators within the pipeline.","118","        Caching the transformers is advantageous when fitting is time","119","        consuming.","120","","121","","146","    ...                      # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE","147","    Pipeline(memory=None,","148","             steps=[('anova', SelectKBest(...)),","149","                    ('svc', SVC(...))])","159","","164","    def __init__(self, steps, memory=None):","168","        self.memory = memory","239","        # Setup the memory","240","        memory = self.memory","241","        if memory is None:","242","            memory = Memory(cachedir=None, verbose=0)","243","        elif isinstance(memory, six.string_types):","244","            memory = Memory(cachedir=memory, verbose=0)","245","        elif not isinstance(memory, Memory):","246","            raise ValueError(\"'memory' should either be a string or\"","247","                             \" a joblib.Memory instance, got\"","248","                             \" 'memory={!r}' instead.\".format(memory))","249","","250","        fit_transform_one_cached = memory.cache(_fit_transform_one)","251","","258","        for step_idx, (name, transformer) in enumerate(self.steps[:-1]):","259","            if transformer is None:","262","                if memory.cachedir is None:","263","                    # we do not clone when caching is disabled to preserve","264","                    # backward compatibility","265","                    cloned_transformer = transformer","266","                else:","267","                    cloned_transformer = clone(transformer)","268","                # Fit or load from cache the current transfomer","269","                Xt, fitted_transformer = fit_transform_one_cached(","270","                    cloned_transformer, None, Xt, y,","271","                    **fit_params_steps[name])","272","                # Replace the transformer of the step with the fitted","273","                # transformer. This is necessary when loading the transformer","274","                # from the cache.","275","                self.steps[step_idx] = (name, fitted_transformer)","592","    Pipeline(memory=None,","593","             steps=[('standardscaler',","608","def _transform_one(transformer, weight, X):","616","def _fit_transform_one(transformer, weight, X, y,","774","            delayed(_fit_transform_one)(trans, weight, X, y,","804","            delayed(_transform_one)(trans, weight, X)"],"delete":["17","from .base import BaseEstimator, TransformerMixin","18","from .externals.joblib import Parallel, delayed","133","    ...                                              # doctest: +ELLIPSIS","134","    Pipeline(steps=[...])","148","    def __init__(self, steps):","228","        for name, transform in self.steps[:-1]:","229","            if transform is None:","231","            elif hasattr(transform, \"fit_transform\"):","232","                Xt = transform.fit_transform(Xt, y, **fit_params_steps[name])","234","                Xt = transform.fit(Xt, y, **fit_params_steps[name]) \\","235","                              .transform(Xt)","552","    Pipeline(steps=[('standardscaler',","567","def _transform_one(transformer, name, weight, X):","575","def _fit_transform_one(transformer, name, weight, X, y,","733","            delayed(_fit_transform_one)(trans, name, weight, X, y,","763","            delayed(_transform_one)(trans, name, weight, X)"]}],"examples\/plot_compare_reduction.py":[{"add":["0","#!\/usr\/bin\/env python","9","classifier. It demonstrates the use of ``GridSearchCV`` and","10","``Pipeline`` to optimize over different classes of estimators in a","11","single CV run -- unsupervised ``PCA`` and ``NMF`` dimensionality","14","","15","Additionally, ``Pipeline`` can be instantiated with the ``memory``","16","argument to memoize the transformers within the pipeline, avoiding to fit","17","again the same transformers over and over.","18","","19","Note that the use of ``memory`` to enable caching becomes interesting when the","20","fitting of a transformer is costly.","22","","23","###############################################################################","24","# Illustration of ``Pipeline`` and ``GridSearchCV``","25","###############################################################################","26","# This section illustrates the use of a ``Pipeline`` with","27","# ``GridSearchCV``","28","","29","# Authors: Robert McGibbon, Joel Nothman, Guillaume Lemaitre","65","grid = GridSearchCV(pipe, cv=3, n_jobs=1, param_grid=param_grid)","88","","89","###############################################################################","90","# Caching transformers within a ``Pipeline``","91","###############################################################################","92","# It is sometimes worthwhile storing the state of a specific transformer","93","# since it could be used again. Using a pipeline in ``GridSearchCV`` triggers","94","# such situations. Therefore, we use the argument ``memory`` to enable caching.","95","#","96","# .. warning::","97","#     Note that this example is, however, only an illustration since for this","98","#     specific case fitting PCA is not necessarily slower than loading the","99","#     cache. Hence, use the ``memory`` constructor parameter when the fitting","100","#     of a transformer is costly.","101","","102","from tempfile import mkdtemp","103","from shutil import rmtree","104","from sklearn.externals.joblib import Memory","105","","106","# Create a temporary folder to store the transformers of the pipeline","107","cachedir = mkdtemp()","108","memory = Memory(cachedir=cachedir, verbose=10)","109","cached_pipe = Pipeline([('reduce_dim', PCA()),","110","                        ('classify', LinearSVC())],","111","                       memory=memory)","112","","113","# This time, a cached pipeline will be used within the grid search","114","grid = GridSearchCV(cached_pipe, cv=3, n_jobs=1, param_grid=param_grid)","115","digits = load_digits()","116","grid.fit(digits.data, digits.target)","117","","118","# Delete the temporary cache before exiting","119","rmtree(cachedir)","120","","121","###############################################################################","122","# The ``PCA`` fitting is only computed at the evaluation of the first","123","# configuration of the ``C`` parameter of the ``LinearSVC`` classifier. The","124","# other configurations of ``C`` will trigger the loading of the cached ``PCA``","125","# estimator data, leading to save processing time. Therefore, the use of","126","# caching the pipeline using ``memory`` is highly beneficial when fitting","127","# a transformer is costly.","128",""],"delete":["0","#!\/usr\/bin\/python","9","classifier. It demonstrates the use of GridSearchCV and","10","Pipeline to optimize over different classes of estimators in a","11","single CV run -- unsupervised PCA and NMF dimensionality","15","# Authors: Robert McGibbon, Joel Nothman","51","grid = GridSearchCV(pipe, cv=3, n_jobs=2, param_grid=param_grid)"]}],"doc\/modules\/pipeline.rst":[{"add":["41","    >>> pipe # doctest: +NORMALIZE_WHITESPACE, +ELLIPSIS","42","    Pipeline(memory=None,","43","             steps=[('reduce_dim', PCA(copy=True,...)),","44","                    ('clf', SVC(C=1.0,...))])","55","    Pipeline(memory=None,","56","             steps=[('binarizer', Binarizer(copy=True, threshold=0.0)),","76","    >>> pipe.set_params(clf__C=10) # doctest: +NORMALIZE_WHITESPACE, +ELLIPSIS","77","    Pipeline(memory=None,","78","             steps=[('reduce_dim', PCA(copy=True, iterated_power='auto',...)),","79","                    ('clf', SVC(C=10, cache_size=200, class_weight=None,...))])","84","    >>> param_grid = dict(reduce_dim__n_components=[2, 5, 10],","85","    ...                   clf__C=[0.1, 10, 100])","86","    >>> grid_search = GridSearchCV(pipe, param_grid=param_grid)","92","    >>> param_grid = dict(reduce_dim=[None, PCA(5), PCA(10)],","93","    ...                   clf=[SVC(), LogisticRegression()],","94","    ...                   clf__C=[0.1, 10, 100])","95","    >>> grid_search = GridSearchCV(pipe, param_grid=param_grid)","104"," * :ref:`sphx_glr_auto_examples_plot_compare_reduction.py`","121","Caching transformers: avoid repeated computation","122","-------------------------------------------------","123","","124",".. currentmodule:: sklearn.pipeline","125","","126","Fitting transformers may be computationally expensive. With its","127","``memory`` parameter set, :class:`Pipeline` will cache each transformer","128","after calling ``fit``.","129","This feature is used to avoid computing the fit transformers within a pipeline","130","if the parameters and input data are identical. A typical example is the case of","131","a grid search in which the transformers can be fitted only once and reused for","132","each configuration.","133","","134","The parameter ``memory`` is needed in order to cache the transformers.","135","``memory`` can be either a string containing the directory where to cache the","136","transformers or a `joblib.Memory <https:\/\/pythonhosted.org\/joblib\/memory.html>`_","137","object::","138","","139","    >>> from tempfile import mkdtemp","140","    >>> from shutil import rmtree","141","    >>> from sklearn.decomposition import PCA","142","    >>> from sklearn.svm import SVC","143","    >>> from sklearn.pipeline import Pipeline","144","    >>> estimators = [('reduce_dim', PCA()), ('clf', SVC())]","145","    >>> cachedir = mkdtemp()","146","    >>> pipe = Pipeline(estimators, memory=cachedir)","147","    >>> pipe # doctest: +NORMALIZE_WHITESPACE, +ELLIPSIS","148","    Pipeline(...,","149","             steps=[('reduce_dim', PCA(copy=True,...)),","150","                    ('clf', SVC(C=1.0,...))])","151","    >>> # Clear the cache directory when you don't need it anymore","152","    >>> rmtree(cachedir)","153","","154",".. warning:: **Side effect of caching transfomers**","155","","156","   Using a :class:`Pipeline` without cache enabled, it is possible to","157","   inspect the original instance such as::","158","","159","     >>> from sklearn.datasets import load_digits","160","     >>> digits = load_digits()","161","     >>> pca1 = PCA()","162","     >>> svm1 = SVC()","163","     >>> pipe = Pipeline([('reduce_dim', pca1), ('clf', svm1)])","164","     >>> pipe.fit(digits.data, digits.target)","165","     ... # doctest: +NORMALIZE_WHITESPACE, +ELLIPSIS","166","     Pipeline(memory=None,","167","              steps=[('reduce_dim', PCA(...)), ('clf', SVC(...))])","168","     >>> # The pca instance can be inspected directly","169","     >>> print(pca1.components_) # doctest: +NORMALIZE_WHITESPACE, +ELLIPSIS","170","         [[ -1.77484909e-19  ... 4.07058917e-18]]","171","","172","   Enabling caching triggers a clone of the transformers before fitting.","173","   Therefore, the transformer instance given to the pipeline cannot be","174","   inspected directly.","175","   In following example, accessing the :class:`PCA` instance ``pca2``","176","   will raise an ``AttributeError`` since ``pca2`` will be an unfitted","177","   transformer.","178","   Instead, use the attribute ``named_steps`` to inspect estimators within","179","   the pipeline::","180","","181","     >>> cachedir = mkdtemp()","182","     >>> pca2 = PCA()","183","     >>> svm2 = SVC()","184","     >>> cached_pipe = Pipeline([('reduce_dim', pca2), ('clf', svm2)],","185","     ...                        memory=cachedir)","186","     >>> cached_pipe.fit(digits.data, digits.target)","187","     ... # doctest: +NORMALIZE_WHITESPACE, +ELLIPSIS","188","      Pipeline(memory=...,","189","               steps=[('reduce_dim', PCA(...)), ('clf', SVC(...))])","190","     >>> print(cached_pipe.named_steps['reduce_dim'].components_)","191","     ... # doctest: +NORMALIZE_WHITESPACE, +ELLIPSIS","192","         [[ -1.77484909e-19  ... 4.07058917e-18]]","193","     >>> # Remove the cache directory","194","     >>> rmtree(cachedir)","195","","196",".. topic:: Examples:","197","","198"," * :ref:`sphx_glr_auto_examples_plot_compare_reduction.py`","239","    >>> combined # doctest: +NORMALIZE_WHITESPACE, +ELLIPSIS","240","    FeatureUnion(n_jobs=1,","241","                 transformer_list=[('linear_pca', PCA(copy=True,...)),","242","                                   ('kernel_pca', KernelPCA(alpha=1.0,...))],","243","                 transformer_weights=None)","253","    >>> combined.set_params(kernel_pca=None)","254","    ... # doctest: +NORMALIZE_WHITESPACE, +ELLIPSIS","255","    FeatureUnion(n_jobs=1,","256","                 transformer_list=[('linear_pca', PCA(copy=True,...)),","257","                                   ('kernel_pca', None)],","258","                 transformer_weights=None)"],"delete":["41","    >>> pipe # doctest: +NORMALIZE_WHITESPACE","42","    Pipeline(steps=[('reduce_dim', PCA(copy=True, iterated_power='auto',","43","    n_components=None, random_state=None, svd_solver='auto', tol=0.0,","44","    whiten=False)), ('clf', SVC(C=1.0, cache_size=200, class_weight=None,","45","    coef0=0.0, decision_function_shape='ovr', degree=3, gamma='auto',","46","    kernel='rbf', max_iter=-1, probability=False, random_state=None,","47","    shrinking=True, tol=0.001, verbose=False))])","58","    Pipeline(steps=[('binarizer', Binarizer(copy=True, threshold=0.0)),","78","    >>> pipe.set_params(clf__C=10) # doctest: +NORMALIZE_WHITESPACE","79","    Pipeline(steps=[('reduce_dim', PCA(copy=True, iterated_power='auto',","80","        n_components=None, random_state=None, svd_solver='auto', tol=0.0,","81","        whiten=False)), ('clf', SVC(C=10, cache_size=200, class_weight=None,","82","        coef0=0.0, decision_function_shape='ovr', degree=3, gamma='auto',","83","        kernel='rbf', max_iter=-1, probability=False, random_state=None,","84","        shrinking=True, tol=0.001, verbose=False))])","85","","90","    >>> params = dict(reduce_dim__n_components=[2, 5, 10],","91","    ...               clf__C=[0.1, 10, 100])","92","    >>> grid_search = GridSearchCV(pipe, param_grid=params)","98","    >>> params = dict(reduce_dim=[None, PCA(5), PCA(10)],","99","    ...               clf=[SVC(), LogisticRegression()],","100","    ...               clf__C=[0.1, 10, 100])","101","    >>> grid_search = GridSearchCV(pipe, param_grid=params)","166","    >>> combined # doctest: +NORMALIZE_WHITESPACE","167","    FeatureUnion(n_jobs=1, transformer_list=[('linear_pca', PCA(copy=True,","168","        iterated_power='auto', n_components=None, random_state=None,","169","        svd_solver='auto', tol=0.0, whiten=False)), ('kernel_pca',","170","        KernelPCA(alpha=1.0, coef0=1, copy_X=True, degree=3,","171","        eigen_solver='auto', fit_inverse_transform=False, gamma=None,","172","        kernel='linear', kernel_params=None, max_iter=None, n_components=None,","173","        n_jobs=1, random_state=None, remove_zero_eig=False, tol=0))],","174","        transformer_weights=None)","184","    >>> combined.set_params(kernel_pca=None) # doctest: +NORMALIZE_WHITESPACE","185","    FeatureUnion(n_jobs=1, transformer_list=[('linear_pca', PCA(copy=True,","186","          iterated_power='auto', n_components=None, random_state=None,","187","          svd_solver='auto', tol=0.0, whiten=False)), ('kernel_pca', None)],","188","        transformer_weights=None)"]}]}},"7b0b6d73441c2a29e455992c3f1e306a4feab07e":{"changes":{"examples\/model_selection\/plot_confusion_matrix.py":"MODIFY"},"diff":{"examples\/model_selection\/plot_confusion_matrix.py":[{"add":["67","    plt.imshow(cm, interpolation='nearest', cmap=cmap)","68","    plt.title(title)","69","    plt.colorbar()","70","    tick_marks = np.arange(len(classes))","71","    plt.xticks(tick_marks, classes, rotation=45)","72","    plt.yticks(tick_marks, classes)","73","","74","    fmt = '.2f' if normalize else 'd'","77","        plt.text(j, i, format(cm[i, j], fmt),"],"delete":["59","    plt.imshow(cm, interpolation='nearest', cmap=cmap)","60","    plt.title(title)","61","    plt.colorbar()","62","    tick_marks = np.arange(len(classes))","63","    plt.xticks(tick_marks, classes, rotation=45)","64","    plt.yticks(tick_marks, classes)","65","","76","        plt.text(j, i, cm[i, j],"]}]}},"3e7a7ca7fa6620afc68f2454c373c069be57dbf3":{"changes":{"sklearn\/metrics\/cluster\/tests\/test_unsupervised.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/metrics\/cluster\/unsupervised.py":"MODIFY"},"diff":{"sklearn\/metrics\/cluster\/tests\/test_unsupervised.py":[{"add":["7","from sklearn.utils.testing import assert_array_equal","13","from sklearn.metrics.cluster import silhouette_samples","15","from sklearn.metrics.cluster import calinski_harabaz_score","60","def test_cluster_size_1():","61","    # Assert Silhouette Coefficient == 0 when there is 1 sample in a cluster","62","    # (cluster 0). We also test the case where there are identical samples","63","    # as the only members of a cluster (cluster 2). To our knowledge, this case","64","    # is not discussed in reference material, and we choose for it a sample","65","    # score of 1.","66","    X = [[0.], [1.], [1.], [2.], [3.], [3.]]","67","    labels = np.array([0, 1, 1, 1, 2, 2])","68","","69","    # Cluster 0: 1 sample -> score of 0 by Rousseeuw's convention","70","    # Cluster 1: intra-cluster = [.5, .5, 1]","71","    #            inter-cluster = [1, 1, 1]","72","    #            silhouette    = [.5, .5, 0]","73","    # Cluster 2: intra-cluster = [0, 0]","74","    #            inter-cluster = [arbitrary, arbitrary]","75","    #            silhouette    = [1., 1.]","76","","77","    silhouette = silhouette_score(X, labels)","79","    ss = silhouette_samples(X, labels)","80","    assert_array_equal(ss, [0, .5, .5, 0, 1, 1])","108","        silhouette_score(X, labels * 2 + 10), silhouette_score(X, labels))","109","    assert_array_equal(","110","        silhouette_samples(X, labels * 2 + 10), silhouette_samples(X, labels))"],"delete":["12","from sklearn.metrics.cluster import calinski_harabaz_score","58","def test_no_nan():","59","    # Assert Silhouette Coefficient != nan when there is 1 sample in a class.","60","    # This tests for the condition that caused issue 960.","61","    # Note that there is only one sample in cluster 0. This used to cause the","62","    # silhouette_score to return nan (see bug #960).","63","    labels = np.array([1, 0, 1, 1, 1])","64","    # The distance matrix doesn't actually matter.","65","    D = np.random.RandomState(0).rand(len(labels), len(labels))","66","    silhouette = silhouette_score(D, labels, metric='precomputed')","95","        silhouette_score(X, labels + 10), silhouette_score(X, labels))"]}],"doc\/whats_new.rst":[{"add":["473","    - Fix bug in :func:`metrics.silhouette_score` in which clusters of","474","      size 1 were incorrectly scored. They should get a score of 0.","475","      By `Joel Nothman`_.","476","","477","    - Fix bug in :func:`metrics.silhouette_samples` so that it now works with","478","      arbitrary labels, not just those ranging from 0 to n_clusters - 1.","479","      By `Joel Nothman`_.","480",""],"delete":[]}],"sklearn\/metrics\/cluster\/unsupervised.py":[{"add":["11","from ...utils.fixes import bincount","92","        X, labels = check_X_y(X, labels, accept_sparse=['csc', 'csr'])","162","    X, labels = check_X_y(X, labels, accept_sparse=['csc', 'csr'])","165","    check_number_of_labels(len(le.classes_), X.shape[0])","169","    n_samples_per_label = bincount(labels, minlength=len(unique_labels))","173","    intra_clust_dists = np.zeros(distances.shape[0], dtype=distances.dtype)","177","    inter_clust_dists = np.inf + intra_clust_dists","179","    for curr_label in range(len(unique_labels)):","187","        n_samples_curr_lab = n_samples_per_label[curr_label] - 1","194","        for other_label in range(len(unique_labels)):","204","    # score 0 for clusters of size 1, according to the paper","205","    sil_samples[n_samples_per_label.take(labels) == 1] = 0"],"delete":["90","    X, labels = check_X_y(X, labels, accept_sparse=['csc', 'csr'])","91","    le = LabelEncoder()","92","    labels = le.fit_transform(labels)","93","    n_labels = len(le.classes_)","94","    n_samples = X.shape[0]","95","","96","    check_number_of_labels(n_labels, n_samples)","97","","176","    intra_clust_dists = np.ones(distances.shape[0], dtype=distances.dtype)","180","    inter_clust_dists = np.inf * intra_clust_dists","182","    for curr_label in unique_labels:","190","        n_samples_curr_lab = np.sum(mask) - 1","197","        for other_label in unique_labels:"]}]}},"726c8d999a0485a3319cb32aecd0cac25dd3cc53":{"changes":{"examples\/svm\/plot_svm_margin.py":"MODIFY"},"diff":{"examples\/svm\/plot_svm_margin.py":[{"add":["49","    # support vectors (margin away from hyperplane in direction","50","    # perpendicular to hyperplane). This is sqrt(1+a^2) away vertically in","51","    # 2-d.","53","    yy_down = yy - np.sqrt(1 + a ** 2) * margin","54","    yy_up = yy + np.sqrt(1 + a ** 2) * margin"],"delete":["49","    # support vectors","51","    yy_down = yy + a * margin","52","    yy_up = yy - a * margin"]}]}},"194c231d49286c72715d073d24d0aec6db4f551a":{"changes":{"sklearn\/utils\/metaestimators.py":"MODIFY","sklearn\/tests\/test_pipeline.py":"MODIFY","sklearn\/ensemble\/voting_classifier.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/pipeline.py":"MODIFY","sklearn\/ensemble\/tests\/test_voting_classifier.py":"MODIFY"},"diff":{"sklearn\/utils\/metaestimators.py":[{"add":["5","from abc import ABCMeta, abstractmethod","9","","11","from ..externals import six","12","from ..base import BaseEstimator","17","class _BaseComposition(six.with_metaclass(ABCMeta, BaseEstimator)):","18","    \"\"\"Handles parameter management for classifiers composed of named estimators.","19","    \"\"\"","20","    @abstractmethod","21","    def __init__(self):","22","        pass","23","","24","    def _get_params(self, attr, deep=True):","25","        out = super(_BaseComposition, self).get_params(deep=False)","26","        if not deep:","27","            return out","28","        estimators = getattr(self, attr)","29","        out.update(estimators)","30","        for name, estimator in estimators:","31","            if estimator is None:","32","                continue","33","            for key, value in six.iteritems(estimator.get_params(deep=True)):","34","                out['%s__%s' % (name, key)] = value","35","        return out","36","","37","    def _set_params(self, attr, **params):","38","        # Ensure strict ordering of parameter setting:","39","        # 1. All steps","40","        if attr in params:","41","            setattr(self, attr, params.pop(attr))","42","        # 2. Step replacement","43","        names, _ = zip(*getattr(self, attr))","44","        for name in list(six.iterkeys(params)):","45","            if '__' not in name and name in names:","46","                self._replace_estimator(attr, name, params.pop(name))","47","        # 3. Step parameters and other initilisation arguments","48","        super(_BaseComposition, self).set_params(**params)","49","        return self","50","","51","    def _replace_estimator(self, attr, name, new_val):","52","        # assumes `name` is a valid estimator name","53","        new_estimators = getattr(self, attr)[:]","54","        for i, (estimator_name, _) in enumerate(new_estimators):","55","            if estimator_name == name:","56","                new_estimators[i] = (name, new_val)","57","                break","58","        setattr(self, attr, new_estimators)","59","","60","    def _validate_names(self, names):","61","        if len(set(names)) != len(names):","62","            raise ValueError('Names provided are not unique: '","63","                             '{0!r}'.format(list(names)))","64","        invalid_names = set(names).intersection(self.get_params(deep=False))","65","        if invalid_names:","66","            raise ValueError('Estimator names conflict with constructor '","67","                             'arguments: {0!r}'.format(sorted(invalid_names)))","68","        invalid_names = [name for name in names if '__' in name]","69","        if invalid_names:","70","            raise ValueError('Estimator names must not contain __: got '","71","                             '{0!r}'.format(invalid_names))","72","","73",""],"delete":[]}],"sklearn\/tests\/test_pipeline.py":[{"add":["813","            (bad_steps1, \"Estimator names must not contain __: got ['a__q']\"),","815","            (bad_steps3, \"Estimator names conflict with constructor \""],"delete":["813","            (bad_steps1, \"Step names must not contain __: got ['a__q']\"),","815","            (bad_steps3, \"Step names conflict with constructor \""]}],"sklearn\/ensemble\/voting_classifier.py":[{"add":["21","from ..utils.metaestimators import _BaseComposition","33","class VotingClassifier(_BaseComposition, ClassifierMixin, TransformerMixin):","45","        ``self.estimators_``. An estimator can be set to `None` using","46","        ``set_params``.","66","        The collection of fitted sub-estimators as defined in ``estimators``","67","        that are not `None`.","109","    @property","110","    def named_estimators(self):","111","        return dict(self.estimators)","112","","156","                    raise ValueError('Underlying estimator \\'%s\\' does not'","157","                                     ' support sample weights.' % name)","158","        names, clfs = zip(*self.estimators)","159","        self._validate_names(names)","161","        n_isnone = np.sum([clf is None for _, clf in self.estimators])","162","        if n_isnone == len(self.estimators):","163","            raise ValueError('All estimators are None. At least one is '","164","                             'required to be a classifier!')","165","        self.le_ = LabelEncoder().fit(y)","173","                                                 sample_weight)","174","                for clf in clfs if clf is not None)","178","    @property","179","    def _weights_not_none(self):","180","        \"\"\"Get the weights of not `None` estimators\"\"\"","181","        if self.weights is None:","182","            return None","183","        return [w for est, w in zip(self.estimators,","184","                                    self.weights) if est[1] is not None]","185","","207","            maj = np.apply_along_axis(","208","                lambda x: np.argmax(","209","                    np.bincount(x, weights=self._weights_not_none)),","210","                axis=1, arr=predictions.astype('int'))","226","        avg = np.average(self._collect_probas(X), axis=0,","227","                         weights=self._weights_not_none)","271","    def set_params(self, **params):","272","        \"\"\" Setting the parameters for the voting classifier","273","","274","        Valid parameter keys can be listed with get_params().","275","","276","        Parameters","277","        ----------","278","        params: keyword arguments","279","            Specific parameters using e.g. set_params(parameter_name=new_value)","280","            In addition, to setting the parameters of the ``VotingClassifier``,","281","            the individual classifiers of the ``VotingClassifier`` can also be","282","            set or replaced by setting them to None.","283","","284","        Examples","285","        --------","286","        # In this example, the RandomForestClassifier is removed","287","        clf1 = LogisticRegression()","288","        clf2 = RandomForestClassifier()","289","        eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2)]","290","        eclf.set_params(rf=None)","291","","292","        \"\"\"","293","        super(VotingClassifier, self)._set_params('estimators', **params)","294","        return self","295","","297","        \"\"\" Get the parameters of the VotingClassifier","298","","299","        Parameters","300","        ----------","301","        deep: bool","302","            Setting it to True gets the various classifiers and the parameters","303","            of the classifiers as well","304","        \"\"\"","305","        return super(VotingClassifier,","306","                     self)._get_params('estimators', deep=deep)"],"delete":["15","from ..base import BaseEstimator","20","from ..externals import six","34","class VotingClassifier(BaseEstimator, ClassifierMixin, TransformerMixin):","46","        `self.estimators_`.","66","        The collection of fitted sub-estimators.","104","        self.named_estimators = dict(estimators)","152","                    raise ValueError('Underlying estimator \\'%s\\' does not support'","153","                                     ' sample weights.' % name)","155","        self.le_ = LabelEncoder()","156","        self.le_.fit(y)","164","                    sample_weight)","165","                    for _, clf in self.estimators)","190","            maj = np.apply_along_axis(lambda x:","191","                                      np.argmax(np.bincount(x,","192","                                                weights=self.weights)),","193","                                      axis=1,","194","                                      arr=predictions.astype('int'))","210","        avg = np.average(self._collect_probas(X), axis=0, weights=self.weights)","255","        \"\"\"Return estimator parameter names for GridSearch support\"\"\"","256","        if not deep:","257","            return super(VotingClassifier, self).get_params(deep=False)","258","        else:","259","            out = super(VotingClassifier, self).get_params(deep=False)","260","            out.update(self.named_estimators.copy())","261","            for name, step in six.iteritems(self.named_estimators):","262","                for key, value in six.iteritems(step.get_params(deep=True)):","263","                    out['%s__%s' % (name, key)] = value","264","            return out"]}],"doc\/whats_new.rst":[{"add":["165","   - Added ability to use sparse matrices in :func:`feature_selection.f_regression`","166","     with ``center=True``. :issue:`8065` by :user:`Daniel LeJeune <acadiansith>`.","167","","168","   - :class:`ensemble.VotingClassifier` now allow changing estimators by using","169","     :meth:`ensemble.VotingClassifier.set_params`. Estimators can also be","170","     removed by setting it to `None`.","171","     :issue:`7674` by:user:`Yichuan Liu <yl565>`."],"delete":[]}],"sklearn\/pipeline.py":[{"add":["12","","25","from .utils.metaestimators import _BaseComposition","26","","30","class Pipeline(_BaseComposition):","578","class FeatureUnion(_BaseComposition, TransformerMixin):"],"delete":["27","class _BasePipeline(six.with_metaclass(ABCMeta, BaseEstimator)):","28","    \"\"\"Handles parameter management for classifiers composed of named steps.","29","    \"\"\"","30","","31","    @abstractmethod","32","    def __init__(self):","33","        pass","34","","35","    def _replace_step(self, steps_attr, name, new_val):","36","        # assumes `name` is a valid step name","37","        new_steps = getattr(self, steps_attr)[:]","38","        for i, (step_name, _) in enumerate(new_steps):","39","            if step_name == name:","40","                new_steps[i] = (name, new_val)","41","                break","42","        setattr(self, steps_attr, new_steps)","43","","44","    def _get_params(self, steps_attr, deep=True):","45","        out = super(_BasePipeline, self).get_params(deep=False)","46","        if not deep:","47","            return out","48","        steps = getattr(self, steps_attr)","49","        out.update(steps)","50","        for name, estimator in steps:","51","            if estimator is None:","52","                continue","53","            for key, value in six.iteritems(estimator.get_params(deep=True)):","54","                out['%s__%s' % (name, key)] = value","55","        return out","56","","57","    def _set_params(self, steps_attr, **params):","58","        # Ensure strict ordering of parameter setting:","59","        # 1. All steps","60","        if steps_attr in params:","61","            setattr(self, steps_attr, params.pop(steps_attr))","62","        # 2. Step replacement","63","        step_names, _ = zip(*getattr(self, steps_attr))","64","        for name in list(six.iterkeys(params)):","65","            if '__' not in name and name in step_names:","66","                self._replace_step(steps_attr, name, params.pop(name))","67","        # 3. Step parameters and other initilisation arguments","68","        super(_BasePipeline, self).set_params(**params)","69","        return self","70","","71","    def _validate_names(self, names):","72","        if len(set(names)) != len(names):","73","            raise ValueError('Names provided are not unique: '","74","                             '{0!r}'.format(list(names)))","75","        invalid_names = set(names).intersection(self.get_params(deep=False))","76","        if invalid_names:","77","            raise ValueError('Step names conflict with constructor arguments: '","78","                             '{0!r}'.format(sorted(invalid_names)))","79","        invalid_names = [name for name in names if '__' in name]","80","        if invalid_names:","81","            raise ValueError('Step names must not contain __: got '","82","                             '{0!r}'.format(invalid_names))","83","","84","","85","class Pipeline(_BasePipeline):","633","class FeatureUnion(_BasePipeline, TransformerMixin):"]}],"sklearn\/ensemble\/tests\/test_voting_classifier.py":[{"add":["4","from sklearn.utils.testing import assert_equal, assert_true, assert_false","42","    eclf = VotingClassifier(estimators=[('lr', clf), ('lr', clf)],","43","                            weights=[1, 2])","44","    msg = \"Names provided are not unique: ['lr', 'lr']\"","45","    assert_raise_message(ValueError, msg, eclf.fit, X, y)","46","","47","    eclf = VotingClassifier(estimators=[('lr__', clf)])","48","    msg = \"Estimator names must not contain __: got ['lr__']\"","49","    assert_raise_message(ValueError, msg, eclf.fit, X, y)","50","","51","    eclf = VotingClassifier(estimators=[('estimators', clf)])","52","    msg = \"Estimator names conflict with constructor arguments: ['estimators']\"","53","    assert_raise_message(ValueError, msg, eclf.fit, X, y)","54","","275","def test_set_params():","276","    \"\"\"set_params should be able to set estimators\"\"\"","277","    clf1 = LogisticRegression(random_state=123, C=1.0)","278","    clf2 = RandomForestClassifier(random_state=123, max_depth=None)","279","    clf3 = GaussianNB()","280","    eclf1 = VotingClassifier([('lr', clf1), ('rf', clf2)], voting='soft',","281","                             weights=[1, 2])","282","    eclf1.fit(X, y)","283","    eclf2 = VotingClassifier([('lr', clf1), ('nb', clf3)], voting='soft',","284","                             weights=[1, 2])","285","    eclf2.set_params(nb=clf2).fit(X, y)","286","    assert_false(hasattr(eclf2, 'nb'))","287","","288","    assert_array_equal(eclf1.predict(X), eclf2.predict(X))","289","    assert_array_equal(eclf1.predict_proba(X), eclf2.predict_proba(X))","290","    assert_equal(eclf2.estimators[0][1].get_params(), clf1.get_params())","291","    assert_equal(eclf2.estimators[1][1].get_params(), clf2.get_params())","292","","293","    eclf1.set_params(lr__C=10.0)","294","    eclf2.set_params(nb__max_depth=5)","295","","296","    assert_true(eclf1.estimators[0][1].get_params()['C'] == 10.0)","297","    assert_true(eclf2.estimators[1][1].get_params()['max_depth'] == 5)","298","    assert_equal(eclf1.get_params()[\"lr__C\"],","299","                 eclf1.get_params()[\"lr\"].get_params()['C'])","300","","301","","302","def test_set_estimator_none():","303","    \"\"\"VotingClassifier set_params should be able to set estimators as None\"\"\"","304","    # Test predict","305","    clf1 = LogisticRegression(random_state=123)","306","    clf2 = RandomForestClassifier(random_state=123)","307","    clf3 = GaussianNB()","308","    eclf1 = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2),","309","                                         ('nb', clf3)],","310","                             voting='hard', weights=[1, 0, 0.5]).fit(X, y)","311","","312","    eclf2 = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2),","313","                                         ('nb', clf3)],","314","                             voting='hard', weights=[1, 1, 0.5])","315","    eclf2.set_params(rf=None).fit(X, y)","316","    assert_array_equal(eclf1.predict(X), eclf2.predict(X))","317","","318","    assert_true(dict(eclf2.estimators)[\"rf\"] is None)","319","    assert_true(len(eclf2.estimators_) == 2)","320","    assert_true(all([not isinstance(est, RandomForestClassifier) for est in","321","                     eclf2.estimators_]))","322","    assert_true(eclf2.get_params()[\"rf\"] is None)","323","","324","    eclf1.set_params(voting='soft').fit(X, y)","325","    eclf2.set_params(voting='soft').fit(X, y)","326","    assert_array_equal(eclf1.predict(X), eclf2.predict(X))","327","    assert_array_equal(eclf1.predict_proba(X), eclf2.predict_proba(X))","328","    msg = ('All estimators are None. At least one is required'","329","           ' to be a classifier!')","330","    assert_raise_message(","331","        ValueError, msg, eclf2.set_params(lr=None, rf=None, nb=None).fit, X, y)","332","","333","    # Test soft voting transform","334","    X1 = np.array([[1], [2]])","335","    y1 = np.array([1, 2])","336","    eclf1 = VotingClassifier(estimators=[('rf', clf2), ('nb', clf3)],","337","                             voting='soft', weights=[0, 0.5]).fit(X1, y1)","338","","339","    eclf2 = VotingClassifier(estimators=[('rf', clf2), ('nb', clf3)],","340","                             voting='soft', weights=[1, 0.5])","341","    eclf2.set_params(rf=None).fit(X1, y1)","342","    assert_array_equal(eclf1.transform(X1), np.array([[[0.7, 0.3], [0.3, 0.7]],","343","                                                      [[1., 0.], [0., 1.]]]))","344","    assert_array_equal(eclf2.transform(X1), np.array([[[1., 0.], [0., 1.]]]))","345","    eclf1.set_params(voting='hard')","346","    eclf2.set_params(voting='hard')","347","    assert_array_equal(eclf1.transform(X1), np.array([[0, 0], [1, 1]]))","348","    assert_array_equal(eclf2.transform(X1), np.array([[0], [1]]))","349","","350",""],"delete":["4","from sklearn.utils.testing import assert_equal"]}]}},"0a1f6cd98eebe7c5233692e2d9680232c23bf9a8":{"changes":{"sklearn\/tests\/test_pipeline.py":"MODIFY","sklearn\/pipeline.py":"MODIFY"},"diff":{"sklearn\/tests\/test_pipeline.py":[{"add":["280","    # As pipeline doesn't clone estimators on construction,","281","    # it must have its own estimators","282","    scaler_for_pipeline = StandardScaler()","283","    km_for_pipeline = KMeans(random_state=0)","290","    pipe = Pipeline([","291","        ('scaler', scaler_for_pipeline),","292","        ('Kmeans', km_for_pipeline)","293","    ])"],"delete":["286","    pipe = Pipeline([('scaler', scaler), ('Kmeans', km)])"]}],"sklearn\/pipeline.py":[{"add":["358","                Xt = transform.fit_transform(Xt)"],"delete":["358","                Xt = transform.transform(Xt)"]}]}},"925a017c83b0eea147cc8288c30fefdb199b5730":{"changes":{"sklearn\/feature_extraction\/tests\/test_feature_hasher.py":"MODIFY","sklearn\/feature_extraction\/hashing.py":"MODIFY","sklearn\/feature_extraction\/text.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/feature_extraction\/_hashing.pyx":"MODIFY"},"diff":{"sklearn\/feature_extraction\/tests\/test_feature_hasher.py":[{"add":["6","from sklearn.utils.testing import (assert_raises, assert_true, assert_equal,","7","                                   ignore_warnings)","109","","110","","111","@ignore_warnings(category=DeprecationWarning)","112","def test_hasher_alternate_sign():","113","    # the last two tokens produce a hash collision that sums as 0","114","    X = [[\"foo\", \"bar\", \"baz\", \"investigation need\", \"records\"]]","115","","116","    Xt = FeatureHasher(alternate_sign=True, non_negative=False,","117","                       input_type='string').fit_transform(X)","118","    assert_true(Xt.data.min() < 0 and Xt.data.max() > 0)","119","    # check that we have a collision that produces a 0 count","120","    assert_true(len(Xt.data) < len(X[0]))","121","    assert_true((Xt.data == 0.).any())","122","","123","    Xt = FeatureHasher(alternate_sign=True, non_negative=True,","124","                       input_type='string').fit_transform(X)","125","    assert_true((Xt.data >= 0).all())   # all counts are positive","126","    assert_true((Xt.data == 0.).any())  # we still have a collision","127","    Xt = FeatureHasher(alternate_sign=False, non_negative=True,","128","                       input_type='string').fit_transform(X)","129","    assert_true((Xt.data > 0).all())    # strictly positive counts","130","    Xt_2 = FeatureHasher(alternate_sign=False, non_negative=False,","131","                         input_type='string').fit_transform(X)","132","    # With initially positive features, the non_negative option should","133","    # have no impact when alternate_sign=False","134","    assert_array_equal(Xt.data, Xt_2.data)","135","","136","","137","@ignore_warnings(category=DeprecationWarning)","138","def test_hasher_negative():","139","    X = [{\"foo\": 2, \"bar\": -4, \"baz\": -1}.items()]","140","    Xt = FeatureHasher(alternate_sign=False, non_negative=False,","141","                       input_type=\"pair\").fit_transform(X)","142","    assert_true(Xt.data.min() < 0 and Xt.data.max() > 0)","143","    Xt = FeatureHasher(alternate_sign=False, non_negative=True,","144","                       input_type=\"pair\").fit_transform(X)","145","    assert_true(Xt.data.min() > 0)","146","    Xt = FeatureHasher(alternate_sign=True, non_negative=False,","147","                       input_type=\"pair\").fit_transform(X)","148","    assert_true(Xt.data.min() < 0 and Xt.data.max() > 0)","149","    Xt = FeatureHasher(alternate_sign=True, non_negative=True,","150","                       input_type=\"pair\").fit_transform(X)","151","    assert_true(Xt.data.min() > 0)"],"delete":["6","from sklearn.utils.testing import assert_raises, assert_true, assert_equal"]}],"sklearn\/feature_extraction\/hashing.py":[{"add":["4","import warnings","56","    alternate_sign : boolean, optional, default True","57","        When True, an alternating sign is added to the features as to","58","        approximately conserve the inner product in the hashed space even for","59","        small n_features. This approach is similar to sparse random projection.","61","        When True, an absolute value is applied to the features matrix prior to","62","        returning it. When used in conjunction with alternate_sign=True, this","63","        significantly reduces the inner product preservation property.","64","        .. deprecated:: 0.19","65","            This option will be removed in 0.21.","66","","86","                 dtype=np.float64, alternate_sign=True, non_negative=False):","88","        if non_negative:","89","            warnings.warn(\"the option non_negative=True has been deprecated\"","90","                          \" in 0.19 and will be removed\"","91","                          \" in version 0.21.\", DeprecationWarning)","96","        self.alternate_sign = alternate_sign","153","            _hashing.transform(raw_X, self.n_features, self.dtype,","154","                               self.alternate_sign)","163",""],"delete":["56","        Whether output matrices should contain non-negative values only;","57","        effectively calls abs on the matrix prior to returning it.","58","        When True, output values can be interpreted as frequencies.","59","        When False, output values will have expected value zero.","79","                 dtype=np.float64, non_negative=False):","141","            _hashing.transform(raw_X, self.n_features, self.dtype)"]}],"sklearn\/feature_extraction\/text.py":[{"add":["406","    alternate_sign : boolean, optional, default True","407","        When True, an alternating sign is added to the features as to","408","        approximately conserve the inner product in the hashed space even for","409","        small n_features. This approach is similar to sparse random projection.","410","","411","        .. versionadded:: 0.19","412","","413","    non_negative : boolean, optional, default False","414","        When True, an absolute value is applied to the features matrix prior to","415","        returning it. When used in conjunction with alternate_sign=True, this","416","        significantly reduces the inner product preservation property.","417","","418","        .. deprecated:: 0.19","419","            This option will be removed in 0.21.","431","                 binary=False, norm='l2', alternate_sign=True,","432","                 non_negative=False, dtype=np.float64):","447","        self.alternate_sign = alternate_sign","508","                             alternate_sign=self.alternate_sign,"],"delete":["406","    non_negative : boolean, default=False","407","        Whether output matrices should contain non-negative values only;","408","        effectively calls abs on the matrix prior to returning it.","409","        When True, output values can be interpreted as frequencies.","410","        When False, output values will have expected value zero.","422","                 binary=False, norm='l2', non_negative=False,","423","                 dtype=np.float64):"]}],"doc\/whats_new.rst":[{"add":["296","","297","   - Fix a bug where :class:`sklearn.feature_extraction.FeatureHasher`","298","     mandatorily applied a sparse random projection to the hashed features,","299","     preventing the use of ","300","     :class:`sklearn.feature_extraction.text.HashingVectorizer` in a","301","     pipeline with  :class:`sklearn.feature_extraction.text.TfidfTransformer`.","302","     :issue:`7513` by :user:`Roman Yurchak <rth>`.","303","     ","307","     "],"delete":[]}],"sklearn\/feature_extraction\/_hashing.pyx":[{"add":["17","def transform(raw_X, Py_ssize_t n_features, dtype, bint alternate_sign=1):","65","            # improve inner product preservation in the hashed space","66","            if alternate_sign:","67","                value *= (h >= 0) * 2 - 1"],"delete":["17","def transform(raw_X, Py_ssize_t n_features, dtype):","65","            value *= (h >= 0) * 2 - 1"]}]}},"5c4b1bb23192a137ac22ced229c50d6b69859ac6":{"changes":{"sklearn\/__init__.py":"MODIFY","sklearn\/tests\/test_multiclass.py":"MODIFY","sklearn\/utils\/testing.py":"MODIFY","sklearn\/utils\/class_weight.py":"MODIFY","doc\/modules\/pipeline.rst":"MODIFY","doc\/tutorial\/basic\/tutorial.rst":"MODIFY","sklearn\/svm\/classes.py":"MODIFY","sklearn\/preprocessing\/tests\/test_data.py":"MODIFY","sklearn\/model_selection\/_search.py":"MODIFY","sklearn\/tree\/tree.py":"MODIFY","sklearn\/discriminant_analysis.py":"MODIFY","sklearn\/linear_model\/logistic.py":"MODIFY","sklearn\/ensemble\/tests\/test_forest.py":"MODIFY","sklearn\/utils\/__init__.py":"MODIFY","doc\/modules\/model_persistence.rst":"MODIFY","sklearn\/ensemble\/forest.py":"MODIFY","sklearn\/tests\/test_pipeline.py":"MODIFY","sklearn\/datasets\/tests\/test_lfw.py":"MODIFY","sklearn\/ensemble\/iforest.py":"MODIFY","sklearn\/linear_model\/tests\/test_least_angle.py":"MODIFY","sklearn\/utils\/validation.py":"MODIFY","sklearn\/utils\/tests\/test_class_weight.py":"MODIFY","sklearn\/linear_model\/perceptron.py":"MODIFY","sklearn\/pipeline.py":"MODIFY","sklearn\/svm\/base.py":"MODIFY","sklearn\/grid_search.py":"MODIFY","sklearn\/utils\/tests\/test_validation.py":"MODIFY","sklearn\/datasets\/lfw.py":"MODIFY","sklearn\/linear_model\/tests\/test_logistic.py":"MODIFY","sklearn\/base.py":"MODIFY","sklearn\/tests\/test_discriminant_analysis.py":"MODIFY","sklearn\/linear_model\/coordinate_descent.py":"MODIFY","sklearn\/decomposition\/nmf.py":"MODIFY","sklearn\/feature_selection\/tests\/test_from_model.py":"MODIFY","sklearn\/datasets\/__init__.py":"MODIFY","doc\/tutorial\/statistical_inference\/supervised_learning.rst":"MODIFY","doc\/modules\/svm.rst":"MODIFY","sklearn\/covariance\/tests\/test_robust_covariance.py":"MODIFY","sklearn\/feature_selection\/from_model.py":"MODIFY","sklearn\/svm\/tests\/test_svm.py":"MODIFY","sklearn\/ensemble\/tests\/test_gradient_boosting.py":"MODIFY","sklearn\/tree\/tests\/test_tree.py":"MODIFY","sklearn\/metrics\/regression.py":"MODIFY","sklearn\/metrics\/base.py":"MODIFY","doc\/modules\/classes.rst":"MODIFY","sklearn\/utils\/estimator_checks.py":"MODIFY","sklearn\/linear_model\/base.py":"MODIFY","sklearn\/preprocessing\/data.py":"MODIFY","sklearn\/decomposition\/tests\/test_nmf.py":"MODIFY","\/dev\/null":"DELETE","sklearn\/linear_model\/stochastic_gradient.py":"MODIFY","sklearn\/decomposition\/__init__.py":"MODIFY","sklearn\/ensemble\/gradient_boosting.py":"MODIFY"},"diff":{"sklearn\/__init__.py":[{"add":["64","               'learning_curve', 'linear_model', 'manifold', 'metrics',","67","               'preprocessing', 'random_projection', 'semi_supervised',"],"delete":["64","               'lda', 'learning_curve', 'linear_model', 'manifold', 'metrics',","67","               'preprocessing', 'qda', 'random_projection', 'semi_supervised',"]}],"sklearn\/tests\/test_multiclass.py":[{"add":[],"delete":["336","        assert_true(hasattr(decision_only, 'decision_function'))","372","    assert_true(hasattr(decision_only, 'decision_function'))"]}],"sklearn\/utils\/testing.py":[{"add":[],"delete":["17","import re","18","import platform","638","def if_not_mac_os(versions=('10.7', '10.8', '10.9'),","639","                  message='Multi-process bug in Mac OS X >= 10.7 '","640","                          '(see issue #636)'):","641","    \"\"\"Test decorator that skips test if OS is Mac OS X and its","642","    major version is one of ``versions``.","643","    \"\"\"","644","    warnings.warn(\"if_not_mac_os is deprecated in 0.17 and will be removed\"","645","                  \" in 0.19: use the safer and more generic\"","646","                  \" if_safe_multiprocessing_with_blas instead\",","647","                  DeprecationWarning)","648","    mac_version, _, _ = platform.mac_ver()","649","    skip = '.'.join(mac_version.split('.')[:2]) in versions","650","","651","    def decorator(func):","652","        if skip:","653","            @wraps(func)","654","            def func(*args, **kwargs):","655","                raise SkipTest(message)","656","        return func","657","    return decorator","658","","659",""]}],"sklearn\/utils\/class_weight.py":[{"add":["49","    elif class_weight == 'balanced':","56","        recip_freq = len(y) \/ (len(le.classes_) *","57","                               bincount(y_ind).astype(np.float64))","58","        weight = recip_freq[le.transform(classes)]","99","        sample weight will be calculated over the full sample. Only \"balanced\"","100","        is supported for class_weight if this is provided.","114","        if class_weight not in ['balanced']:","137","        if class_weight == 'balanced' or n_outputs == 1:"],"delete":["4","import warnings","50","    elif class_weight in ['auto', 'balanced']:","57","        # inversely proportional to the number of samples in the class","58","        if class_weight == 'auto':","59","            recip_freq = 1. \/ bincount(y_ind)","60","            weight = recip_freq[le.transform(classes)] \/ np.mean(recip_freq)","61","            warnings.warn(\"The class_weight='auto' heuristic is deprecated in\"","62","                          \" 0.17 in favor of a new heuristic \"","63","                          \"class_weight='balanced'. 'auto' will be removed in\"","64","                          \" 0.19\", DeprecationWarning)","65","        else:","66","            recip_freq = len(y) \/ (len(le.classes_) *","67","                                   bincount(y_ind).astype(np.float64))","68","            weight = recip_freq[le.transform(classes)]","109","        sample weight will be calculated over the full sample. Only \"auto\" is","110","        supported for class_weight if this is provided.","124","        if class_weight not in ['balanced', 'auto']:","147","        if class_weight in ['balanced', 'auto'] or n_outputs == 1:"]}],"doc\/modules\/pipeline.rst":[{"add":["45","    coef0=0.0, decision_function_shape='ovr', degree=3, gamma='auto',","82","        coef0=0.0, decision_function_shape='ovr', degree=3, gamma='auto',"],"delete":["45","    coef0=0.0, decision_function_shape=None, degree=3, gamma='auto',","82","        coef0=0.0, decision_function_shape=None, degree=3, gamma='auto',"]}],"doc\/tutorial\/basic\/tutorial.rst":[{"add":["182","    decision_function_shape='ovr', degree=3, gamma=0.001, kernel='rbf',","221","    decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',","295","      decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',","304","      decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',","333","    decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',","341","    decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',"],"delete":["182","    decision_function_shape=None, degree=3, gamma=0.001, kernel='rbf',","221","    decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',","295","      decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',","304","      decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',","333","    decision_function_shape=None, degree=3, gamma='auto', kernel='linear',","341","    decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',"]}],"sklearn\/svm\/classes.py":[{"add":["13","                SparseCoefMixin):","51","        ``\"ovr\"`` trains n_classes one-vs-rest classifiers, while","52","        ``\"crammer_singer\"`` optimizes a joint objective over all classes.","56","        If ``\"crammer_singer\"`` is chosen, the options loss, penalty and dual","57","        will be ignored.","455","    decision_function_shape : 'ovo', 'ovr', default='ovr'","460","","461","        .. versionchanged:: 0.19","462","            decision_function_shape is 'ovr' by default.","511","        decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',","532","                 verbose=False, max_iter=-1, decision_function_shape='ovr',","596","        weight one. The \"balanced\" mode uses the values of y to automatically","597","        adjust weights inversely proportional to class frequencies as","608","    decision_function_shape : 'ovo', 'ovr', default='ovr'","613","","614","        .. versionchanged:: 0.19","615","            decision_function_shape is 'ovr' by default.","664","          decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',","680","    def __init__(self, nu=0.5, kernel='rbf', degree=3, gamma='auto', coef0=0.0,","681","                 shrinking=True, probability=False, tol=1e-3, cache_size=200,","682","                 class_weight=None, verbose=False, max_iter=-1,","683","                 decision_function_shape='ovr', random_state=None):"],"delete":["7","from ..feature_selection.from_model import _LearntSelectorMixin","14","                _LearntSelectorMixin, SparseCoefMixin):","52","        ``\"ovr\"`` trains n_classes one-vs-rest classifiers, while ``\"crammer_singer\"``","53","        optimizes a joint objective over all classes.","57","        If ``\"crammer_singer\"`` is chosen, the options loss, penalty and dual will","58","        be ignored.","456","    decision_function_shape : 'ovo', 'ovr' or None, default=None","461","        The default of None will currently behave as 'ovo' for backward","462","        compatibility and raise a deprecation warning, but will change 'ovr'","463","        in 0.19.","512","        decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',","533","                 verbose=False, max_iter=-1, decision_function_shape=None,","597","        weight one. The \"balanced\" mode uses the values of y to automatically adjust","598","        weights inversely proportional to class frequencies as","609","    decision_function_shape : 'ovo', 'ovr' or None, default=None","614","        The default of None will currently behave as 'ovo' for backward","615","        compatibility and raise a deprecation warning, but will change 'ovr'","616","        in 0.19.","665","          decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',","681","    def __init__(self, nu=0.5, kernel='rbf', degree=3, gamma='auto',","682","                 coef0=0.0, shrinking=True, probability=False,","683","                 tol=1e-3, cache_size=200, class_weight=None, verbose=False,","684","                 max_iter=-1, decision_function_shape=None, random_state=None):"]}],"sklearn\/preprocessing\/tests\/test_data.py":[{"add":["791","    X = [[np.nan, 5, 6, 7, 8]]","796","    X = [[np.inf, 5, 6, 7, 8]]"],"delete":["29","from sklearn.utils.testing import ignore_warnings","792","    X = [np.nan, 5, 6, 7, 8]","797","    X = [np.inf, 5, 6, 7, 8]","1021","def test_deprecation_minmax_scaler():","1022","    rng = np.random.RandomState(0)","1023","    X = rng.random_sample((5, 4))","1024","    scaler = MinMaxScaler().fit(X)","1025","","1026","    depr_message = (\"Attribute data_range will be removed in \"","1027","                    \"0.19. Use ``data_range_`` instead\")","1028","    assert_warns_message(DeprecationWarning, depr_message, getattr, scaler,","1029","                         \"data_range\")","1030","","1031","    depr_message = (\"Attribute data_min will be removed in \"","1032","                    \"0.19. Use ``data_min_`` instead\")","1033","    assert_warns_message(DeprecationWarning, depr_message, getattr, scaler,","1034","                         \"data_min\")","1035","","1036","","1445","def test_deprecation_standard_scaler():","1446","    rng = np.random.RandomState(0)","1447","    X = rng.random_sample((5, 4))","1448","    scaler = StandardScaler().fit(X)","1449","    depr_message = (\"Function std_ is deprecated; Attribute ``std_`` will be \"","1450","                    \"removed in 0.19. Use ``scale_`` instead\")","1451","    std_ = assert_warns_message(DeprecationWarning, depr_message, getattr,","1452","                                scaler, \"std_\")","1453","    assert_array_equal(std_, scaler.scale_)","1454","","1455",""]}],"sklearn\/model_selection\/_search.py":[{"add":["806","                         decision_function_shape='ovr', degree=..., gamma=...,"],"delete":["806","                         decision_function_shape=None, degree=..., gamma=...,"]}],"sklearn\/tree\/tree.py":[{"add":["72","class BaseDecisionTree(six.with_metaclass(ABCMeta, BaseEstimator)):"],"delete":["31","from ..feature_selection.from_model import _LearntSelectorMixin","73","class BaseDecisionTree(six.with_metaclass(ABCMeta, BaseEstimator,","74","                                          _LearntSelectorMixin)):","741",""]}],"sklearn\/discriminant_analysis.py":[{"add":["58","            # rescale","59","            s = sc.scale_[:, np.newaxis] * s * sc.scale_[np.newaxis, :]","410","    def fit(self, X, y):","414","           .. versionchanged:: 0.19","415","              *store_covariance* has been moved to main constructor.","417","           .. versionchanged:: 0.19","418","              *tol* has been moved to main constructor.","619","    def fit(self, X, y):","622","            .. versionchanged:: 0.19","623","               *store_covariance* has been moved to main constructor.","625","            .. versionchanged:: 0.19","626","               *tol* has been moved to main constructor."],"delete":["58","            s = sc.scale_[:, np.newaxis] * s * sc.scale_[np.newaxis, :]  # rescale","409","    def fit(self, X, y, store_covariance=None, tol=None):","413","           .. versionchanged:: 0.17","414","              Deprecated *store_covariance* have been moved to main constructor.","416","           .. versionchanged:: 0.17","417","              Deprecated *tol* have been moved to main constructor.","427","        if store_covariance:","428","            warnings.warn(\"The parameter 'store_covariance' is deprecated as \"","429","                          \"of version 0.17 and will be removed in 0.19. The \"","430","                          \"parameter is no longer necessary because the value \"","431","                          \"is set via the estimator initialisation or \"","432","                          \"set_params method.\", DeprecationWarning)","433","            self.store_covariance = store_covariance","434","        if tol:","435","            warnings.warn(\"The parameter 'tol' is deprecated as of version \"","436","                          \"0.17 and will be removed in 0.19. The parameter is \"","437","                          \"no longer necessary because the value is set via \"","438","                          \"the estimator initialisation or set_params method.\",","439","                          DeprecationWarning)","440","            self.tol = tol","632","    def fit(self, X, y, store_covariances=None, tol=None):","635","            .. versionchanged:: 0.17","636","               Deprecated *store_covariance* have been moved to main constructor.","638","            .. versionchanged:: 0.17","639","               Deprecated *tol* have been moved to main constructor.","650","        if store_covariances:","651","            warnings.warn(\"The parameter 'store_covariances' is deprecated as \"","652","                          \"of version 0.17 and will be removed in 0.19. The \"","653","                          \"parameter is no longer necessary because the value \"","654","                          \"is set via the estimator initialisation or \"","655","                          \"set_params method.\", DeprecationWarning)","656","            self.store_covariances = store_covariances","657","        if tol:","658","            warnings.warn(\"The parameter 'tol' is deprecated as of version \"","659","                          \"0.17 and will be removed in 0.19. The parameter is \"","660","                          \"no longer necessary because the value is set via \"","661","                          \"the estimator initialisation or set_params method.\",","662","                          DeprecationWarning)","663","            self.tol = tol"]}],"sklearn\/linear_model\/logistic.py":[{"add":["446","                             solver='lbfgs', coef=None,","577","    .. versionchanged:: 0.19","578","        The \"copy\" parameter was removed.","579","    \"\"\"","586","    if check_input:","588","        y = check_array(y, ensure_2d=False, dtype=None)","627","        if class_weight == \"balanced\":","939","                         SparseCoefMixin):","1005","           *class_weight='balanced'*","1231","                      verbose=self.verbose, solver=self.solver,","1306","                           LinearClassifierMixin):","1597","        if class_weight == \"balanced\":","1691","                    penalty=self.penalty,"],"delete":["19","from ..feature_selection.from_model import _LearntSelectorMixin","447","                             solver='lbfgs', coef=None, copy=False,","504","    copy : bool, default False","505","        Whether or not to produce a copy of the data. A copy is not required","506","        anymore. This parameter is deprecated and will be removed in 0.19.","507","","581","    \"\"\"","582","    if copy:","583","        warnings.warn(\"A copy is not required anymore. The 'copy' parameter \"","584","                      \"is deprecated and will be removed in 0.19.\",","585","                      DeprecationWarning)","593","    if check_input or copy:","595","        y = check_array(y, ensure_2d=False, copy=copy, dtype=None)","634","        # 'auto' is deprecated and will be removed in 0.19","635","        if class_weight in (\"auto\", \"balanced\"):","947","                         _LearntSelectorMixin, SparseCoefMixin):","1013","           *class_weight='balanced'* instead of deprecated","1014","           *class_weight='auto'*.","1240","                      verbose=self.verbose, solver=self.solver, copy=False,","1315","                           LinearClassifierMixin, _LearntSelectorMixin):","1561","        if class_weight and not(isinstance(class_weight, dict) or","1562","                                class_weight in ['balanced', 'auto']):","1563","            # 'auto' is deprecated and will be removed in 0.19","1564","            raise ValueError(\"class_weight provided should be a \"","1565","                             \"dict or 'balanced'\")","1611","        if class_weight in (\"auto\", \"balanced\"):","1705","                    penalty=self.penalty, copy=False,"]}],"sklearn\/ensemble\/tests\/test_forest.py":[{"add":["964","    # smoke test for balanced subsample","985","    clf = ForestClassifier(class_weight='balanced', warm_start=True,"],"delete":["210","    # XXX: Remove this test in 0.19 after transform support to estimators","211","    # is removed.","212","    X_new = assert_warns(","213","        DeprecationWarning, est.transform, X, threshold=\"mean\")","214","    assert_less(0 < X_new.shape[1], X.shape[1])","215","","970","    # smoke test for subsample and balanced subsample","973","    clf = ForestClassifier(class_weight='subsample', random_state=0)","974","    ignore_warnings(clf.fit)(X, _y)","993","    clf = ForestClassifier(class_weight='auto', warm_start=True,"]}],"sklearn\/utils\/__init__.py":[{"add":["18","from .deprecation import deprecated","27","           \"check_symmetric\", \"indices_to_mask\", \"deprecated\"]"],"delete":["15","from .deprecation import deprecated","18","from ..exceptions import ConvergenceWarning as _ConvergenceWarning","20","","21","","22","@deprecated(\"ConvergenceWarning has been moved into the sklearn.exceptions \"","23","            \"module. It will not be available here from version 0.19\")","24","class ConvergenceWarning(_ConvergenceWarning):","25","    pass","34","           \"check_symmetric\", \"indices_to_mask\"]"]}],"doc\/modules\/model_persistence.rst":[{"add":["25","      decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',"],"delete":["25","      decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',"]}],"sklearn\/ensemble\/forest.py":[{"add":["125","class BaseForest(six.with_metaclass(ABCMeta, BaseEnsemble)):","473","            valid_presets = ('balanced', 'balanced_subsample')","490","            if (self.class_weight != 'balanced_subsample' or","492","                if self.class_weight == \"balanced_subsample\":","496","                expanded_class_weight = compute_sample_weight(class_weight,","497","                                                              y_original)","1675","        X = check_array(X, accept_sparse=['csc'])"],"delete":["55","from ..feature_selection.from_model import _LearntSelectorMixin","126","class BaseForest(six.with_metaclass(ABCMeta, BaseEnsemble,","127","                                    _LearntSelectorMixin)):","475","            valid_presets = ('auto', 'balanced', 'subsample', 'balanced_subsample')","481","                if self.class_weight == \"subsample\":","482","                    warn(\"class_weight='subsample' is deprecated in 0.17 and\"","483","                         \"will be removed in 0.19. It was replaced by \"","484","                         \"class_weight='balanced_subsample' using the balanced\"","485","                         \"strategy.\", DeprecationWarning)","497","            if (self.class_weight not in ['subsample', 'balanced_subsample'] or","499","                if self.class_weight == 'subsample':","500","                    class_weight = 'auto'","501","                elif self.class_weight == \"balanced_subsample\":","505","                with warnings.catch_warnings():","506","                    if class_weight == \"auto\":","507","                        warnings.simplefilter('ignore', DeprecationWarning)","508","                    expanded_class_weight = compute_sample_weight(class_weight,","509","                                                                  y_original)","1687","        # ensure_2d=False because there are actually unit test checking we fail","1688","        # for 1d.","1689","        X = check_array(X, accept_sparse=['csc'], ensure_2d=False)"]}],"sklearn\/tests\/test_pipeline.py":[{"add":[],"delete":["15","from sklearn.utils.testing import assert_warns_message","702","def test_X1d_inverse_transform():","703","    transformer = Transf()","704","    pipeline = make_pipeline(transformer)","705","    X = np.ones(10)","706","    msg = \"1d X will not be reshaped in pipeline.inverse_transform\"","707","    assert_warns_message(FutureWarning, msg, pipeline.inverse_transform, X)","708","","709",""]}],"sklearn\/datasets\/tests\/test_lfw.py":[{"add":["114","    fetch_lfw_people(data_home=SCIKIT_LEARN_EMPTY_DATA,","115","                     download_if_missing=False)","120","                                  min_faces_per_person=3,","121","                                  download_if_missing=False)","137","    lfw_people = fetch_lfw_people(data_home=SCIKIT_LEARN_DATA, resize=None,","138","                                  slice_=None, color=True,","139","                                  download_if_missing=False)","152","    fetch_lfw_people(data_home=SCIKIT_LEARN_DATA, min_faces_per_person=100,","153","                     download_if_missing=False)","158","    fetch_lfw_pairs(data_home=SCIKIT_LEARN_EMPTY_DATA,","159","                    download_if_missing=False)","163","    lfw_pairs_train = fetch_lfw_pairs(data_home=SCIKIT_LEARN_DATA,","164","                                      download_if_missing=False)","179","    lfw_pairs_train = fetch_lfw_pairs(data_home=SCIKIT_LEARN_DATA, resize=None,","180","                                      slice_=None, color=True,","181","                                      download_if_missing=False)"],"delete":["24","from sklearn.datasets import load_lfw_pairs","25","from sklearn.datasets import load_lfw_people","31","from sklearn.utils.testing import assert_warns_message","117","    fetch_lfw_people(data_home=SCIKIT_LEARN_EMPTY_DATA, download_if_missing=False)","118","","119","","120","def test_load_lfw_people_deprecation():","121","    msg = (\"Function 'load_lfw_people' has been deprecated in 0.17 and will be \"","122","           \"removed in 0.19.\"","123","           \"Use fetch_lfw_people(download_if_missing=False) instead.\")","124","    assert_warns_message(DeprecationWarning, msg, load_lfw_people,","125","                         data_home=SCIKIT_LEARN_DATA)","130","                                  min_faces_per_person=3, download_if_missing=False)","146","    lfw_people = fetch_lfw_people(data_home=SCIKIT_LEARN_DATA,","147","                                  resize=None, slice_=None, color=True, download_if_missing=False)","160","    fetch_lfw_people(data_home=SCIKIT_LEARN_DATA, min_faces_per_person=100, download_if_missing=False)","165","    fetch_lfw_pairs(data_home=SCIKIT_LEARN_EMPTY_DATA, download_if_missing=False)","166","","167","","168","def test_load_lfw_pairs_deprecation():","169","    msg = (\"Function 'load_lfw_pairs' has been deprecated in 0.17 and will be \"","170","           \"removed in 0.19.\"","171","           \"Use fetch_lfw_pairs(download_if_missing=False) instead.\")","172","    assert_warns_message(DeprecationWarning, msg, load_lfw_pairs,","173","                         data_home=SCIKIT_LEARN_DATA)","177","    lfw_pairs_train = fetch_lfw_pairs(data_home=SCIKIT_LEARN_DATA, download_if_missing=False)","192","    lfw_pairs_train = fetch_lfw_pairs(data_home=SCIKIT_LEARN_DATA,","193","                                      resize=None, slice_=None, color=True, download_if_missing=False)"]}],"sklearn\/ensemble\/iforest.py":[{"add":["156","        X = check_array(X, accept_sparse=['csc'])"],"delete":["156","        # ensure_2d=False because there are actually unit test checking we fail","157","        # for 1d.","158","        X = check_array(X, accept_sparse=['csc'], ensure_2d=False)"]}],"sklearn\/linear_model\/tests\/test_least_angle.py":[{"add":[],"delete":["372","        Y_dec = assert_warns(DeprecationWarning, estimator.decision_function, X)","373","        assert_array_almost_equal(Y_pred, Y_dec)"]}],"sklearn\/utils\/validation.py":[{"add":["18","from ..exceptions import NonBLASDotWarning","19","from ..exceptions import NotFittedError","20","from ..exceptions import DataConversionWarning","27","warnings.simplefilter('ignore', NonBLASDotWarning)","295","        Whether to raise a value error if X is not 2d.","367","                raise ValueError(","368","                    \"Got X with X.ndim=1. Reshape your data either using \"","370","                    \"X.reshape(1, -1) if it contains a single sample.\")","404","        warnings.warn(msg, DataConversionWarning)","534","                          DataConversionWarning, stacklevel=2)","664","        raise NotFittedError(msg % {'name': type(estimator).__name__})"],"delete":["18","from .deprecation import deprecated","19","from ..exceptions import DataConversionWarning as _DataConversionWarning","20","from ..exceptions import NonBLASDotWarning as _NonBLASDotWarning","21","from ..exceptions import NotFittedError as _NotFittedError","24","@deprecated(\"DataConversionWarning has been moved into the sklearn.exceptions\"","25","            \" module. It will not be available here from version 0.19\")","26","class DataConversionWarning(_DataConversionWarning):","27","    pass","28","","29","","30","@deprecated(\"NonBLASDotWarning has been moved into the sklearn.exceptions\"","31","            \" module. It will not be available here from version 0.19\")","32","class NonBLASDotWarning(_NonBLASDotWarning):","33","    pass","34","","35","","36","@deprecated(\"NotFittedError has been moved into the sklearn.exceptions module.\"","37","            \" It will not be available here from version 0.19\")","38","class NotFittedError(_NotFittedError):","39","    pass","40","","45","warnings.simplefilter('ignore', _NonBLASDotWarning)","313","        Whether to make X at least 2d.","385","                if ensure_min_samples >= 2:","386","                    raise ValueError(\"%s expects at least 2 samples provided \"","387","                                     \"in a 2 dimensional array-like input\"","388","                                     % estimator_name)","389","                warnings.warn(","390","                    \"Passing 1d arrays as data is deprecated in 0.17 and will \"","391","                    \"raise ValueError in 0.19. Reshape your data either using \"","393","                    \"X.reshape(1, -1) if it contains a single sample.\",","394","                    DeprecationWarning)","428","        warnings.warn(msg, _DataConversionWarning)","558","                          _DataConversionWarning, stacklevel=2)","688","        # FIXME NotFittedError_ --> NotFittedError in 0.19","689","        raise _NotFittedError(msg % {'name': type(estimator).__name__})"]}],"sklearn\/utils\/tests\/test_class_weight.py":[{"add":["89","def test_compute_class_weight_balanced_negative():","109","def test_compute_class_weight_balanced_unordered():","139","    expected_balanced = np.array([0.7777, 0.7777, 0.7777, 0.7777, 0.7777,","140","                                  0.7777, 2.3333])","213","    # Not \"balanced\" for subsample"],"delete":["14","from sklearn.utils.testing import assert_warns","21","    cw = assert_warns(DeprecationWarning,","22","                      compute_class_weight, \"auto\", classes, y)","23","    assert_almost_equal(cw.sum(), classes.shape)","24","    assert_true(cw[0] < cw[1] < cw[2])","37","    assert_raises(ValueError, compute_class_weight, \"auto\", classes, y)","41","    assert_raises(ValueError, compute_class_weight, \"auto\", classes, y)","96","def test_compute_class_weight_auto_negative():","101","    cw = assert_warns(DeprecationWarning, compute_class_weight, \"auto\",","102","                      classes, y)","103","    assert_almost_equal(cw.sum(), classes.shape)","104","    assert_equal(len(cw), len(classes))","105","    assert_array_almost_equal(cw, np.array([1., 1., 1.]))","113","    cw = assert_warns(DeprecationWarning, compute_class_weight, \"auto\",","114","                      classes, y)","115","    assert_almost_equal(cw.sum(), classes.shape)","116","    assert_equal(len(cw), len(classes))","117","    assert_array_almost_equal(cw, np.array([0.545, 1.636, 0.818]), decimal=3)","126","def test_compute_class_weight_auto_unordered():","130","    cw = assert_warns(DeprecationWarning, compute_class_weight, \"auto\",","131","                      classes, y)","132","    assert_almost_equal(cw.sum(), classes.shape)","133","    assert_equal(len(cw), len(classes))","134","    assert_array_almost_equal(cw, np.array([1.636, 0.818, 0.545]), decimal=3)","146","    sample_weight = assert_warns(DeprecationWarning,","147","                                 compute_sample_weight, \"auto\", y)","148","    assert_array_almost_equal(sample_weight, [1., 1., 1., 1., 1., 1.])","158","    sample_weight = assert_warns(DeprecationWarning,","159","                                 compute_sample_weight, \"auto\", y)","160","    assert_array_almost_equal(sample_weight, [1., 1., 1., 1., 1., 1.])","166","    sample_weight = assert_warns(DeprecationWarning,","167","                                 compute_sample_weight, \"auto\", y)","168","    expected_auto = np.asarray([.6, .6, .6, .6, .6, .6, 1.8])","169","    assert_array_almost_equal(sample_weight, expected_auto)","171","    expected_balanced = np.array([0.7777, 0.7777, 0.7777, 0.7777, 0.7777, 0.7777, 2.3333])","180","    sample_weight = assert_warns(DeprecationWarning,","181","                                 compute_sample_weight, \"auto\", y)","182","    assert_array_almost_equal(sample_weight, [1., 1., 1., 1., 1., 1.])","193","    sample_weight = assert_warns(DeprecationWarning,","194","                                 compute_sample_weight, \"auto\", y)","195","    assert_array_almost_equal(sample_weight, expected_auto ** 2)","204","    sample_weight = assert_warns(DeprecationWarning,","205","                                 compute_sample_weight, \"auto\", y)","206","    assert_array_almost_equal(sample_weight, [1., 1., 1., 1., 1., 1.])","212","    sample_weight = assert_warns(DeprecationWarning,","213","                                 compute_sample_weight, \"auto\", y)","214","    assert_array_almost_equal(sample_weight, [1., 1., 1., 1., 1., 1.])","220","    sample_weight = assert_warns(DeprecationWarning,","221","                                 compute_sample_weight, \"auto\", y, range(4))","222","    assert_array_almost_equal(sample_weight, [.5, .5, .5, 1.5, 1.5, 1.5])","229","    sample_weight = assert_warns(DeprecationWarning, compute_sample_weight,","230","                                 \"auto\", y, [0, 1, 1, 2, 2, 3])","231","    expected_auto = np.asarray([1 \/ 3., 1 \/ 3., 1 \/ 3., 5 \/ 3., 5 \/ 3., 5 \/ 3.])","232","    assert_array_almost_equal(sample_weight, expected_auto)","239","    sample_weight = assert_warns(DeprecationWarning, compute_sample_weight,","240","                                 \"auto\", y, [0, 1, 1, 2, 2, 3])","241","    assert_array_almost_equal(sample_weight, expected_auto ** 2)","247","    sample_weight = assert_warns(DeprecationWarning, compute_sample_weight,","248","                                 \"auto\", y, range(6))","249","    assert_array_almost_equal(sample_weight, [1., 1., 1., 1., 1., 1., 0.])","255","    sample_weight = assert_warns(DeprecationWarning, compute_sample_weight,","256","                                 \"auto\", y, range(6))","257","    assert_array_almost_equal(sample_weight, [1., 1., 1., 1., 1., 1., 0.])","272","    # Not \"auto\" for subsample"]}],"sklearn\/linear_model\/perceptron.py":[{"add":["6","class Perceptron(BaseSGDClassifier):"],"delete":["4","from ..feature_selection.from_model import _LearntSelectorMixin","7","class Perceptron(BaseSGDClassifier, _LearntSelectorMixin):"]}],"sklearn\/pipeline.py":[{"add":[],"delete":["12","from warnings import warn","472","        if hasattr(X, 'ndim') and X.ndim == 1:","473","            warn(\"From version 0.19, a 1d X will not be reshaped in\"","474","                 \" pipeline.inverse_transform any more.\", FutureWarning)","475","            X = X[None, :]"]}],"sklearn\/svm\/base.py":[{"add":["14","from ..utils import compute_class_weight"],"delete":["14","from ..utils import compute_class_weight, deprecated","19","from ..exceptions import ChangedBehaviorWarning","370","    @deprecated(\" and will be removed in 0.19\")","371","    def decision_function(self, X):","372","        \"\"\"Distance of the samples X to the separating hyperplane.","373","","374","        Parameters","375","        ----------","376","        X : array-like, shape (n_samples, n_features)","377","            For kernel=\"precomputed\", the expected shape of X is","378","            [n_samples_test, n_samples_train].","379","","380","        Returns","381","        -------","382","        X : array-like, shape (n_samples, n_class * (n_class-1) \/ 2)","383","            Returns the decision function of the sample for each class","384","            in the model.","385","        \"\"\"","386","        return self._decision_function(X)","387","","547","        if self.decision_function_shape is None and len(self.classes_) > 2:","548","            warnings.warn(\"The decision_function_shape default value will \"","549","                          \"change from 'ovo' to 'ovr' in 0.19. This will change \"","550","                          \"the shape of the decision function returned by \"","551","                          \"SVC.\", ChangedBehaviorWarning)"]}],"sklearn\/grid_search.py":[{"add":["743","                         decision_function_shape='ovr', degree=..., gamma=...,"],"delete":["743","                         decision_function_shape=None, degree=..., gamma=...,"]}],"sklearn\/utils\/tests\/test_validation.py":[{"add":[],"delete":["124","    assert_warns(DeprecationWarning, check_array, [0, 1, 2])","125","    X_array = check_array([0, 1, 2])","126","    assert_equal(X_array.ndim, 2)","340","    # But this works if the input data is forced to look like a 2 array with","341","    # one sample and one feature:","342","    X_checked = assert_warns(DeprecationWarning, check_array, [42],","343","                             ensure_2d=True)","344","    assert_array_equal(np.array([[42]]), X_checked)","345",""]}],"sklearn\/datasets\/lfw.py":[{"add":[],"delete":["28","from sklearn.utils import deprecated","29","","378","@deprecated(\"Function 'load_lfw_people' has been deprecated in 0.17 and will \"","379","            \"be removed in 0.19.\"","380","            \"Use fetch_lfw_people(download_if_missing=False) instead.\")","381","def load_lfw_people(download_if_missing=False, **kwargs):","382","    \"\"\"","383","    Alias for fetch_lfw_people(download_if_missing=False)","384","","385","    .. deprecated:: 0.17","386","        This function will be removed in 0.19.","387","        Use :func:`sklearn.datasets.fetch_lfw_people` with parameter","388","        ``download_if_missing=False`` instead.","389","","390","    Check fetch_lfw_people.__doc__ for the documentation and parameter list.","391","    \"\"\"","392","    return fetch_lfw_people(download_if_missing=download_if_missing, **kwargs)","393","","394","","511","","512","","513","@deprecated(\"Function 'load_lfw_pairs' has been deprecated in 0.17 and will \"","514","            \"be removed in 0.19.\"","515","            \"Use fetch_lfw_pairs(download_if_missing=False) instead.\")","516","def load_lfw_pairs(download_if_missing=False, **kwargs):","517","    \"\"\"","518","    Alias for fetch_lfw_pairs(download_if_missing=False)","519","","520","    .. deprecated:: 0.17","521","        This function will be removed in 0.19.","522","        Use :func:`sklearn.datasets.fetch_lfw_pairs` with parameter","523","        ``download_if_missing=False`` instead.","524","","525","    Check fetch_lfw_pairs.__doc__ for the documentation and parameter list.","526","    \"\"\"","527","    return fetch_lfw_pairs(download_if_missing=download_if_missing, **kwargs)"]}],"sklearn\/linear_model\/tests\/test_logistic.py":[{"add":[],"delete":["12","from sklearn.utils.testing import assert_warns_message","735","def test_multinomial_logistic_regression_with_classweight_auto():","736","    X, y = iris.data, iris.target","737","    model = LogisticRegression(multi_class='multinomial',","738","                               class_weight='auto', solver='lbfgs')","739","    # 'auto' is deprecated and will be removed in 0.19","740","    assert_warns_message(DeprecationWarning,","741","                         \"class_weight='auto' heuristic is deprecated\",","742","                         model.fit, X, y)","743","","744",""]}],"sklearn\/base.py":[{"add":[],"delete":["12","from .utils.deprecation import deprecated","13","from .exceptions import ChangedBehaviorWarning as _ChangedBehaviorWarning","17","@deprecated(\"ChangedBehaviorWarning has been moved into the sklearn.exceptions\"","18","            \" module. It will not be available here from version 0.19\")","19","class ChangedBehaviorWarning(_ChangedBehaviorWarning):","20","    pass","21","","22",""]}],"sklearn\/tests\/test_discriminant_analysis.py":[{"add":[],"delete":["0","import sys","13","from sklearn.utils.testing import SkipTest","21","# import reload","22","version = sys.version_info","23","if version[0] == 3:","24","    # Python 3+ import for reload. Builtin in Python2","25","    if version[1] == 3:","26","        reload = None","27","    else:","28","        from importlib import reload","29","","30","","319","def test_deprecated_lda_qda_deprecation():","320","    if reload is None:","321","        raise SkipTest(\"Can't reload module on Python3.3\")","322","","323","    def import_lda_module():","324","        import sklearn.lda","325","        # ensure that we trigger DeprecationWarning even if the sklearn.lda","326","        # was loaded previously by another test.","327","        reload(sklearn.lda)","328","        return sklearn.lda","329","","330","    lda = assert_warns(DeprecationWarning, import_lda_module)","331","    assert isinstance(lda.LDA(), LinearDiscriminantAnalysis)","332","","333","    def import_qda_module():","334","        import sklearn.qda","335","        # ensure that we trigger DeprecationWarning even if the sklearn.qda","336","        # was loaded previously by another test.","337","        reload(sklearn.qda)","338","        return sklearn.qda","339","","340","    qda = assert_warns(DeprecationWarning, import_qda_module)","341","    assert isinstance(qda.QDA(), QuadraticDiscriminantAnalysis)","342","","343",""]}],"sklearn\/linear_model\/coordinate_descent.py":[{"add":["17","from ..utils import check_array, check_X_y"],"delete":["17","from ..utils import check_array, check_X_y, deprecated","748","    @deprecated(\" and will be removed in 0.19\")","749","    def decision_function(self, X):","750","        \"\"\"Decision function of the linear model","751","","752","        Parameters","753","        ----------","754","        X : numpy array or scipy.sparse matrix of shape (n_samples, n_features)","755","","756","        Returns","757","        -------","758","        T : array, shape (n_samples,)","759","            The predicted decision function","760","        \"\"\"","761","        return self._decision_function(X)","762",""]}],"sklearn\/decomposition\/nmf.py":[{"add":["476","                               verbose=0, shuffle=False):","539","    solver : 'cd'","624","    if solver == 'cd':","694","    solver : 'cd'","696","        'cd' is a Coordinate Descent solver.","758","    NMF(alpha=0.0, init='random', l1_ratio=0.0, max_iter=200,","759","      n_components=2, random_state=0, shuffle=False,","760","      solver='cd', tol=0.0001, verbose=0)","780","    def __init__(self, n_components=None, init=None, solver='cd', tol=1e-4,","781","                 max_iter=200, random_state=None, alpha=0., l1_ratio=0.,","782","                 verbose=0, shuffle=False):","823","            shuffle=self.shuffle)","869","            shuffle=self.shuffle)"],"delete":["26","from ..utils import deprecated","54","def _sparseness(x):","55","    \"\"\"Hoyer's measure of sparsity for a vector\"\"\"","56","    sqrt_n = np.sqrt(len(x))","57","    return (sqrt_n - np.linalg.norm(x, 1) \/ norm(x)) \/ (sqrt_n - 1)","58","","59","","82","def _check_string_param(sparseness, solver):","83","    allowed_sparseness = (None, 'data', 'components')","84","    if sparseness not in allowed_sparseness:","85","        raise ValueError(","86","            'Invalid sparseness parameter: got %r instead of one of %r' %","87","            (sparseness, allowed_sparseness))","88","","89","    allowed_solver = ('pg', 'cd')","90","    if solver not in allowed_solver:","91","        raise ValueError(","92","            'Invalid solver parameter: got %r instead of one of %r' %","93","            (solver, allowed_solver))","94","","95","","347","def _update_projected_gradient_w(X, W, H, tolW, nls_max_iter, alpha, l1_ratio,","348","                                 sparseness, beta, eta):","349","    \"\"\"Helper function for _fit_projected_gradient\"\"\"","350","    n_samples, n_features = X.shape","351","    n_components_ = H.shape[0]","352","","353","    if sparseness is None:","354","        Wt, gradW, iterW = _nls_subproblem(X.T, H.T, W.T, tolW, nls_max_iter,","355","                                           alpha=alpha, l1_ratio=l1_ratio)","356","    elif sparseness == 'data':","357","        Wt, gradW, iterW = _nls_subproblem(","358","            safe_vstack([X.T, np.zeros((1, n_samples))]),","359","            safe_vstack([H.T, np.sqrt(beta) * np.ones((1,","360","                         n_components_))]),","361","            W.T, tolW, nls_max_iter, alpha=alpha, l1_ratio=l1_ratio)","362","    elif sparseness == 'components':","363","        Wt, gradW, iterW = _nls_subproblem(","364","            safe_vstack([X.T,","365","                         np.zeros((n_components_, n_samples))]),","366","            safe_vstack([H.T,","367","                         np.sqrt(eta) * np.eye(n_components_)]),","368","            W.T, tolW, nls_max_iter, alpha=alpha, l1_ratio=l1_ratio)","369","","370","    return Wt.T, gradW.T, iterW","371","","372","","373","def _update_projected_gradient_h(X, W, H, tolH, nls_max_iter, alpha, l1_ratio,","374","                                 sparseness, beta, eta):","375","    \"\"\"Helper function for _fit_projected_gradient\"\"\"","376","    n_samples, n_features = X.shape","377","    n_components_ = W.shape[1]","378","","379","    if sparseness is None:","380","        H, gradH, iterH = _nls_subproblem(X, W, H, tolH, nls_max_iter,","381","                                          alpha=alpha, l1_ratio=l1_ratio)","382","    elif sparseness == 'data':","383","        H, gradH, iterH = _nls_subproblem(","384","            safe_vstack([X, np.zeros((n_components_, n_features))]),","385","            safe_vstack([W,","386","                         np.sqrt(eta) * np.eye(n_components_)]),","387","            H, tolH, nls_max_iter, alpha=alpha, l1_ratio=l1_ratio)","388","    elif sparseness == 'components':","389","        H, gradH, iterH = _nls_subproblem(","390","            safe_vstack([X, np.zeros((1, n_features))]),","391","            safe_vstack([W, np.sqrt(beta) * np.ones((1, n_components_))]),","392","            H, tolH, nls_max_iter, alpha=alpha, l1_ratio=l1_ratio)","393","","394","    return H, gradH, iterH","395","","396","","397","def _fit_projected_gradient(X, W, H, tol, max_iter,","398","                            nls_max_iter, alpha, l1_ratio,","399","                            sparseness, beta, eta):","400","    \"\"\"Compute Non-negative Matrix Factorization (NMF) with Projected Gradient","401","","402","    References","403","    ----------","404","    C.-J. Lin. Projected gradient methods for non-negative matrix","405","    factorization. Neural Computation, 19(2007), 2756-2779.","406","    http:\/\/www.csie.ntu.edu.tw\/~cjlin\/nmf\/","407","","408","    P. Hoyer. Non-negative Matrix Factorization with Sparseness Constraints.","409","    Journal of Machine Learning Research 2004.","410","    \"\"\"","411","    gradW = (np.dot(W, np.dot(H, H.T)) -","412","             safe_sparse_dot(X, H.T, dense_output=True))","413","    gradH = (np.dot(np.dot(W.T, W), H) -","414","             safe_sparse_dot(W.T, X, dense_output=True))","415","","416","    init_grad = squared_norm(gradW) + squared_norm(gradH.T)","417","    # max(0.001, tol) to force alternating minimizations of W and H","418","    tolW = max(0.001, tol) * np.sqrt(init_grad)","419","    tolH = tolW","420","","421","    for n_iter in range(1, max_iter + 1):","422","        # stopping condition","423","        # as discussed in paper","424","        proj_grad_W = squared_norm(gradW * np.logical_or(gradW < 0, W > 0))","425","        proj_grad_H = squared_norm(gradH * np.logical_or(gradH < 0, H > 0))","426","","427","        if (proj_grad_W + proj_grad_H) \/ init_grad < tol ** 2:","428","            break","429","","430","        # update W","431","        W, gradW, iterW = _update_projected_gradient_w(X, W, H, tolW,","432","                                                       nls_max_iter,","433","                                                       alpha, l1_ratio,","434","                                                       sparseness, beta, eta)","435","        if iterW == 1:","436","            tolW = 0.1 * tolW","437","","438","        # update H","439","        H, gradH, iterH = _update_projected_gradient_h(X, W, H, tolH,","440","                                                       nls_max_iter,","441","                                                       alpha, l1_ratio,","442","                                                       sparseness, beta, eta)","443","        if iterH == 1:","444","            tolH = 0.1 * tolH","445","","446","    H[H == 0] = 0   # fix up negative zeros","447","","448","    if n_iter == max_iter:","449","        W, _, _ = _update_projected_gradient_w(X, W, H, tol, nls_max_iter,","450","                                               alpha, l1_ratio, sparseness,","451","                                               beta, eta)","452","","453","    return W, H, n_iter","454","","455","","606","                               verbose=0, shuffle=False, nls_max_iter=2000,","607","                               sparseness=None, beta=1, eta=0.1):","670","    solver : 'pg' | 'cd'","672","        'pg' is a (deprecated) Projected Gradient solver.","704","    nls_max_iter : integer, default: 2000","705","        Number of iterations in NLS subproblem.","706","        Used only in the deprecated 'pg' solver.","707","","708","    sparseness : 'data' | 'components' | None, default: None","709","        Where to enforce sparsity in the model.","710","        Used only in the deprecated 'pg' solver.","711","","712","    beta : double, default: 1","713","        Degree of sparseness, if sparseness is not None. Larger values mean","714","        more sparseness. Used only in the deprecated 'pg' solver.","715","","716","    eta : double, default: 0.1","717","        Degree of correctness to maintain, if sparsity is not None. Smaller","718","        values mean larger error. Used only in the deprecated 'pg' solver.","745","    _check_string_param(sparseness, solver)","772","    if solver == 'pg':","773","        warnings.warn(\"'pg' solver will be removed in release 0.19.\"","774","                      \" Use 'cd' solver instead.\", DeprecationWarning)","775","        if update_H:  # fit_transform","776","            W, H, n_iter = _fit_projected_gradient(X, W, H, tol,","777","                                                   max_iter,","778","                                                   nls_max_iter,","779","                                                   alpha, l1_ratio,","780","                                                   sparseness,","781","                                                   beta, eta)","782","        else:  # transform","783","            W, H, n_iter = _update_projected_gradient_w(X, W, H,","784","                                                        tol, nls_max_iter,","785","                                                        alpha, l1_ratio,","786","                                                        sparseness, beta,","787","                                                        eta)","788","    elif solver == 'cd':","858","    solver : 'pg' | 'cd'","860","        'pg' is a Projected Gradient solver (deprecated).","861","        'cd' is a Coordinate Descent solver (recommended).","902","    nls_max_iter : integer, default: 2000","903","        Number of iterations in NLS subproblem.","904","        Used only in the deprecated 'pg' solver.","905","","906","        .. versionchanged:: 0.17","907","           Deprecated Projected Gradient solver. Use Coordinate Descent solver","908","           instead.","909","","910","    sparseness : 'data' | 'components' | None, default: None","911","        Where to enforce sparsity in the model.","912","        Used only in the deprecated 'pg' solver.","913","","914","        .. versionchanged:: 0.17","915","           Deprecated Projected Gradient solver. Use Coordinate Descent solver","916","           instead.","917","","918","    beta : double, default: 1","919","        Degree of sparseness, if sparseness is not None. Larger values mean","920","        more sparseness. Used only in the deprecated 'pg' solver.","921","","922","        .. versionchanged:: 0.17","923","           Deprecated Projected Gradient solver. Use Coordinate Descent solver","924","           instead.","925","","926","    eta : double, default: 0.1","927","        Degree of correctness to maintain, if sparsity is not None. Smaller","928","        values mean larger error. Used only in the deprecated 'pg' solver.","929","","930","        .. versionchanged:: 0.17","931","           Deprecated Projected Gradient solver. Use Coordinate Descent solver","932","           instead.","954","    NMF(alpha=0.0, beta=1, eta=0.1, init='random', l1_ratio=0.0, max_iter=200,","955","      n_components=2, nls_max_iter=2000, random_state=0, shuffle=False,","956","      solver='cd', sparseness=None, tol=0.0001, verbose=0)","976","    def __init__(self, n_components=None, init=None, solver='cd',","977","                 tol=1e-4, max_iter=200, random_state=None,","978","                 alpha=0., l1_ratio=0., verbose=0, shuffle=False,","979","                 nls_max_iter=2000, sparseness=None, beta=1, eta=0.1):","991","        if sparseness is not None:","992","            warnings.warn(\"Controlling regularization through the sparseness,\"","993","                          \" beta and eta arguments is only available\"","994","                          \" for 'pg' solver, which will be removed\"","995","                          \" in release 0.19. Use another solver with L1 or L2\"","996","                          \" regularization instead.\", DeprecationWarning)","997","        self.nls_max_iter = nls_max_iter","998","        self.sparseness = sparseness","999","        self.beta = beta","1000","        self.eta = eta","1001","","1031","            shuffle=self.shuffle,","1032","            nls_max_iter=self.nls_max_iter, sparseness=self.sparseness,","1033","            beta=self.beta, eta=self.eta)","1034","","1035","        if self.solver == 'pg':","1036","            self.comp_sparseness_ = _sparseness(H.ravel())","1037","            self.data_sparseness_ = _sparseness(W.ravel())","1083","            shuffle=self.shuffle,","1084","            nls_max_iter=self.nls_max_iter, sparseness=self.sparseness,","1085","            beta=self.beta, eta=self.eta)","1106","","1107","","1108","@deprecated(\"It will be removed in release 0.19. Use NMF instead.\"","1109","            \"'pg' solver is still available until release 0.19.\")","1110","class ProjectedGradientNMF(NMF):","1111","    \"\"\"Non-Negative Matrix Factorization (NMF)","1112","","1113","    Find two non-negative matrices (W, H) whose product approximates the non-","1114","    negative matrix X. This factorization can be used for example for","1115","    dimensionality reduction, source separation or topic extraction.","1116","","1117","    The objective function is::","1118","","1119","        0.5 * ||X - WH||_Fro^2","1120","        + alpha * l1_ratio * ||vec(W)||_1","1121","        + alpha * l1_ratio * ||vec(H)||_1","1122","        + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2","1123","        + 0.5 * alpha * (1 - l1_ratio) * ||H||_Fro^2","1124","","1125","    Where::","1126","","1127","        ||A||_Fro^2 = \\sum_{i,j} A_{ij}^2 (Frobenius norm)","1128","        ||vec(A)||_1 = \\sum_{i,j} abs(A_{ij}) (Elementwise L1 norm)","1129","","1130","    The objective function is minimized with an alternating minimization of W","1131","    and H.","1132","","1133","    Read more in the :ref:`User Guide <NMF>`.","1134","","1135","    Parameters","1136","    ----------","1137","    n_components : int or None","1138","        Number of components, if n_components is not set all features","1139","        are kept.","1140","","1141","    init :  'random' | 'nndsvd' |  'nndsvda' | 'nndsvdar' | 'custom'","1142","        Method used to initialize the procedure.","1143","        Default: 'nndsvdar' if n_components < n_features, otherwise random.","1144","        Valid options:","1145","","1146","        - 'random': non-negative random matrices, scaled with:","1147","            sqrt(X.mean() \/ n_components)","1148","","1149","        - 'nndsvd': Nonnegative Double Singular Value Decomposition (NNDSVD)","1150","            initialization (better for sparseness)","1151","","1152","        - 'nndsvda': NNDSVD with zeros filled with the average of X","1153","            (better when sparsity is not desired)","1154","","1155","        - 'nndsvdar': NNDSVD with zeros filled with small random values","1156","            (generally faster, less accurate alternative to NNDSVDa","1157","            for when sparsity is not desired)","1158","","1159","        - 'custom': use custom matrices W and H","1160","","1161","    solver : 'pg' | 'cd'","1162","        Numerical solver to use:","1163","        'pg' is a Projected Gradient solver (deprecated).","1164","        'cd' is a Coordinate Descent solver (recommended).","1165","","1166","        .. versionadded:: 0.17","1167","           Coordinate Descent solver.","1168","","1169","        .. versionchanged:: 0.17","1170","           Deprecated Projected Gradient solver.","1171","","1172","    tol : double, default: 1e-4","1173","        Tolerance value used in stopping conditions.","1174","","1175","    max_iter : integer, default: 200","1176","        Number of iterations to compute.","1177","","1178","    random_state : integer seed, RandomState instance, or None (default)","1179","        Random number generator seed control.","1180","","1181","    alpha : double, default: 0.","1182","        Constant that multiplies the regularization terms. Set it to zero to","1183","        have no regularization.","1184","","1185","        .. versionadded:: 0.17","1186","           *alpha* used in the Coordinate Descent solver.","1187","","1188","    l1_ratio : double, default: 0.","1189","        The regularization mixing parameter, with 0 <= l1_ratio <= 1.","1190","        For l1_ratio = 0 the penalty is an elementwise L2 penalty","1191","        (aka Frobenius Norm).","1192","        For l1_ratio = 1 it is an elementwise L1 penalty.","1193","        For 0 < l1_ratio < 1, the penalty is a combination of L1 and L2.","1194","","1195","        .. versionadded:: 0.17","1196","           Regularization parameter *l1_ratio* used in the Coordinate Descent","1197","           solver.","1198","","1199","    shuffle : boolean, default: False","1200","        If true, randomize the order of coordinates in the CD solver.","1201","","1202","        .. versionadded:: 0.17","1203","           *shuffle* parameter used in the Coordinate Descent solver.","1204","","1205","    nls_max_iter : integer, default: 2000","1206","        Number of iterations in NLS subproblem.","1207","        Used only in the deprecated 'pg' solver.","1208","","1209","        .. versionchanged:: 0.17","1210","           Deprecated Projected Gradient solver. Use Coordinate Descent solver","1211","           instead.","1212","","1213","    sparseness : 'data' | 'components' | None, default: None","1214","        Where to enforce sparsity in the model.","1215","        Used only in the deprecated 'pg' solver.","1216","","1217","        .. versionchanged:: 0.17","1218","           Deprecated Projected Gradient solver. Use Coordinate Descent solver","1219","           instead.","1220","","1221","    beta : double, default: 1","1222","        Degree of sparseness, if sparseness is not None. Larger values mean","1223","        more sparseness. Used only in the deprecated 'pg' solver.","1224","","1225","        .. versionchanged:: 0.17","1226","           Deprecated Projected Gradient solver. Use Coordinate Descent solver","1227","           instead.","1228","","1229","    eta : double, default: 0.1","1230","        Degree of correctness to maintain, if sparsity is not None. Smaller","1231","        values mean larger error. Used only in the deprecated 'pg' solver.","1232","","1233","        .. versionchanged:: 0.17","1234","           Deprecated Projected Gradient solver. Use Coordinate Descent solver","1235","           instead.","1236","","1237","    Attributes","1238","    ----------","1239","    components_ : array, [n_components, n_features]","1240","        Non-negative components of the data.","1241","","1242","    reconstruction_err_ : number","1243","        Frobenius norm of the matrix difference between","1244","        the training data and the reconstructed data from","1245","        the fit produced by the model. ``|| X - WH ||_2``","1246","","1247","    n_iter_ : int","1248","        Actual number of iterations.","1249","","1250","    Examples","1251","    --------","1252","    >>> import numpy as np","1253","    >>> X = np.array([[1,1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])","1254","    >>> from sklearn.decomposition import NMF","1255","    >>> model = NMF(n_components=2, init='random', random_state=0)","1256","    >>> model.fit(X) #doctest: +ELLIPSIS +NORMALIZE_WHITESPACE","1257","    NMF(alpha=0.0, beta=1, eta=0.1, init='random', l1_ratio=0.0, max_iter=200,","1258","      n_components=2, nls_max_iter=2000, random_state=0, shuffle=False,","1259","      solver='cd', sparseness=None, tol=0.0001, verbose=0)","1260","","1261","    >>> model.components_","1262","    array([[ 2.09783018,  0.30560234],","1263","           [ 2.13443044,  2.13171694]])","1264","    >>> model.reconstruction_err_ #doctest: +ELLIPSIS","1265","    0.00115993...","1266","","1267","    References","1268","    ----------","1269","    C.-J. Lin. Projected gradient methods for non-negative matrix","1270","    factorization. Neural Computation, 19(2007), 2756-2779.","1271","    http:\/\/www.csie.ntu.edu.tw\/~cjlin\/nmf\/","1272","","1273","    Cichocki, Andrzej, and P. H. A. N. Anh-Huy. \"Fast local algorithms for","1274","    large scale nonnegative matrix and tensor factorizations.\"","1275","    IEICE transactions on fundamentals of electronics, communications and","1276","    computer sciences 92.3: 708-721, 2009.","1277","    \"\"\"","1278","","1279","    def __init__(self, n_components=None, solver='pg', init=None,","1280","                 tol=1e-4, max_iter=200, random_state=None,","1281","                 alpha=0., l1_ratio=0., verbose=0,","1282","                 nls_max_iter=2000, sparseness=None, beta=1, eta=0.1):","1283","        super(ProjectedGradientNMF, self).__init__(","1284","            n_components=n_components, init=init, solver='pg', tol=tol,","1285","            max_iter=max_iter, random_state=random_state, alpha=alpha,","1286","            l1_ratio=l1_ratio, verbose=verbose, nls_max_iter=nls_max_iter,","1287","            sparseness=sparseness, beta=beta, eta=eta)"]}],"sklearn\/feature_selection\/tests\/test_from_model.py":[{"add":[],"delete":["1","import scipy.sparse as sp","11","from sklearn.utils.testing import assert_warns","27","def test_transform_linear_model():","28","    for clf in (LogisticRegression(C=0.1),","29","                LinearSVC(C=0.01, dual=False),","30","                SGDClassifier(alpha=0.001, n_iter=50, shuffle=True,","31","                              random_state=0)):","32","        for thresh in (None, \".09*mean\", \"1e-5 * median\"):","33","            for func in (np.array, sp.csr_matrix):","34","                X = func(data)","35","                clf.set_params(penalty=\"l1\")","36","                clf.fit(X, y)","37","                X_new = assert_warns(","38","                    DeprecationWarning, clf.transform, X, thresh)","39","                if isinstance(clf, SGDClassifier):","40","                    assert_true(X_new.shape[1] <= X.shape[1])","41","                else:","42","                    assert_less(X_new.shape[1], X.shape[1])","43","                clf.set_params(penalty=\"l2\")","44","                clf.fit(X_new, y)","45","                pred = clf.predict(X_new)","46","                assert_greater(np.mean(pred == y), 0.7)","47","","48",""]}],"sklearn\/datasets\/__init__.py":[{"add":[],"delete":["20","from .lfw import load_lfw_pairs","21","from .lfw import load_lfw_people","76","           'load_lfw_pairs',","77","           'load_lfw_people',"]}],"doc\/tutorial\/statistical_inference\/supervised_learning.rst":[{"add":["457","        decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',"],"delete":["457","        decision_function_shape=None, degree=3, gamma='auto', kernel='linear',"]}],"doc\/modules\/svm.rst":[{"add":["79","        decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',","518","        decision_function_shape='ovr', degree=3, gamma='auto',"],"delete":["79","        decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',","518","        decision_function_shape=None, degree=3, gamma='auto',"]}],"sklearn\/covariance\/tests\/test_robust_covariance.py":[{"add":["46","    assert_raise_message(ValueError, 'Got X with X.ndim=1',","53","    assert_raise_message(ValueError, 'Got X with X.ndim=1',"],"delete":["46","    assert_raise_message(ValueError, 'fast_mcd expects at least 2 samples',","53","    assert_raise_message(ValueError, 'MinCovDet expects at least 2 samples',"]}],"sklearn\/feature_selection\/from_model.py":[{"add":["6","from ..base import BaseEstimator, clone"],"delete":["6","from ..base import TransformerMixin, BaseEstimator, clone","9","from ..utils import safe_mask, check_array, deprecated","10","from ..utils.validation import check_is_fitted","80","class _LearntSelectorMixin(TransformerMixin):","81","    # Note because of the extra threshold parameter in transform, this does","82","    # not naturally extend from SelectorMixin","83","    \"\"\"Transformer mixin selecting features based on importance weights.","84","","85","    This implementation can be mixin on any estimator that exposes a","86","    ``feature_importances_`` or ``coef_`` attribute to evaluate the relative","87","    importance of individual features for feature selection.","88","    \"\"\"","89","    @deprecated('Support to use estimators as feature selectors will be '","90","                'removed in version 0.19. Use SelectFromModel instead.')","91","    def transform(self, X, threshold=None):","92","        \"\"\"Reduce X to its most important features.","93","","94","        Uses ``coef_`` or ``feature_importances_`` to determine the most","95","        important features.  For models with a ``coef_`` for each class, the","96","        absolute sum over the classes is used.","97","","98","        Parameters","99","        ----------","100","        X : array or scipy sparse matrix of shape [n_samples, n_features]","101","            The input samples.","102","","103","        threshold : string, float or None, optional (default=None)","104","            The threshold value to use for feature selection. Features whose","105","            importance is greater or equal are kept while the others are","106","            discarded. If \"median\" (resp. \"mean\"), then the threshold value is","107","            the median (resp. the mean) of the feature importances. A scaling","108","            factor (e.g., \"1.25*mean\") may also be used. If None and if","109","            available, the object attribute ``threshold`` is used. Otherwise,","110","            \"mean\" is used by default.","111","","112","        Returns","113","        -------","114","        X_r : array of shape [n_samples, n_selected_features]","115","            The input samples with only the selected features.","116","        \"\"\"","117","        check_is_fitted(self, ('coef_', 'feature_importances_'),","118","                        all_or_any=any)","119","","120","        X = check_array(X, 'csc')","121","        importances = _get_feature_importances(self)","122","        if len(importances) != X.shape[1]:","123","            raise ValueError(\"X has different number of features than\"","124","                             \" during model fitting.\")","125","","126","        if threshold is None:","127","            threshold = getattr(self, 'threshold', None)","128","        threshold = _calculate_threshold(self, importances, threshold)","129","","130","        # Selection","131","        try:","132","            mask = importances >= threshold","133","        except TypeError:","134","            # Fails in Python 3.x when threshold is str;","135","            # result is array of True","136","            raise ValueError(\"Invalid threshold: all features are discarded.\")","137","","138","        if np.any(mask):","139","            mask = safe_mask(X, mask)","140","            return X[:, mask]","141","        else:","142","            raise ValueError(\"Invalid threshold: all features are discarded.\")","143","","144",""]}],"sklearn\/svm\/tests\/test_svm.py":[{"add":[],"delete":["22","from sklearn.exceptions import ChangedBehaviorWarning","89","@ignore_warnings","90","def test_single_sample_1d():","91","    # Test whether SVCs work on a single sample given as a 1-d array","92","","93","    clf = svm.SVC().fit(X, Y)","94","    clf.predict(X[0])","95","","96","    clf = svm.LinearSVC(random_state=0).fit(X, Y)","97","    clf.predict(X[0])","98","","99","","384","    # check deprecation warning","385","    clf = svm.SVC(kernel='linear', C=0.1).fit(X_train, y_train)","386","    msg = \"change the shape of the decision function\"","387","    dec = assert_warns_message(ChangedBehaviorWarning, msg,","388","                               clf.decision_function, X_train)","389","    assert_equal(dec.shape, (len(X_train), 10))","390",""]}],"sklearn\/ensemble\/tests\/test_gradient_boosting.py":[{"add":["1066",""],"delete":["301","        # XXX: Remove this test in 0.19 after transform support to estimators","302","        # is removed.","303","        X_new = assert_warns(","304","            DeprecationWarning, clf.transform, X, threshold=\"mean\")","305","        assert_less(X_new.shape[1], X.shape[1])","306","        feature_mask = (","307","            clf.feature_importances_ > clf.feature_importances_.mean())","308","        assert_array_almost_equal(X_new, X[:, feature_mask])","309",""]}],"sklearn\/tree\/tests\/test_tree.py":[{"add":["525","        assert_raises(ValueError, TreeEstimator(min_impurity_split=-1.0).fit,","526","                      X, y)","1282","    for tree_type, dataset in product(SPARSE_TREES, (\"clf_small\", \"toy\",","1283","                                                     \"digits\", \"multilabel\",","1284","                                                     \"sparse-pos\",","1285","                                                     \"sparse-neg\",","1286","                                                     \"sparse-mix\", \"zeros\")):","1288","        yield (check_sparse_input, tree_type, dataset, max_depth)","1292","    for tree_type, dataset in product(SPARSE_TREES, [\"boston\", \"reg_small\"]):","1293","        if tree_type in REG_TREES:","1294","            yield (check_sparse_input, tree_type, dataset, 2)","1342","    for tree_type, dataset in product(SPARSE_TREES, [\"sparse-pos\",","1343","                                                     \"sparse-neg\",","1344","                                                     \"sparse-mix\", \"zeros\"]):","1345","        yield (check_sparse_parameters, tree_type, dataset)","1369","    for tree_type, dataset in product(SPARSE_TREES, [\"sparse-pos\",","1370","                                                     \"sparse-neg\",","1371","                                                     \"sparse-mix\", \"zeros\"]):","1372","        yield (check_sparse_criterion, tree_type, dataset)","1445","    for tree_type in SPARSE_TREES:","1446","        yield (check_explicit_sparse_zeros, tree_type)"],"delete":["30","from sklearn.utils.testing import assert_warns","384","        X_new = assert_warns(","385","            DeprecationWarning, clf.transform, X, threshold=\"mean\")","386","        assert_less(0, X_new.shape[1], \"Failed with {0}\".format(name))","387","        assert_less(X_new.shape[1], X.shape[1], \"Failed with {0}\".format(name))","388","","531","        assert_raises(ValueError, TreeEstimator(min_impurity_split=-1.0).fit, X, y)","604","","869","","1289","    for tree, dataset in product(SPARSE_TREES,","1290","                                 (\"clf_small\", \"toy\", \"digits\", \"multilabel\",","1291","                                  \"sparse-pos\", \"sparse-neg\", \"sparse-mix\",","1292","                                  \"zeros\")):","1294","        yield (check_sparse_input, tree, dataset, max_depth)","1298","    for tree, dataset in product(REG_TREES, [\"boston\", \"reg_small\"]):","1299","        if tree in SPARSE_TREES:","1300","            yield (check_sparse_input, tree, dataset, 2)","1348","    for tree, dataset in product(SPARSE_TREES,","1349","                                 [\"sparse-pos\", \"sparse-neg\", \"sparse-mix\",","1350","                                  \"zeros\"]):","1351","        yield (check_sparse_parameters, tree, dataset)","1375","    for tree, dataset in product(SPARSE_TREES,","1376","                                 [\"sparse-pos\", \"sparse-neg\", \"sparse-mix\",","1377","                                  \"zeros\"]):","1378","        yield (check_sparse_criterion, tree, dataset)","1451","    for tree in SPARSE_TREES:","1452","        yield (check_explicit_sparse_zeros, tree)"]}],"sklearn\/metrics\/regression.py":[{"add":["437","def r2_score(y_true, y_pred, sample_weight=None,","438","             multioutput=\"uniform_average\"):","464","        Default is \"uniform_average\".","476","        .. versionchanged:: 0.19","477","            Default value of multioutput is 'uniform_average'.","478",""],"delete":["31","import warnings","438","def r2_score(y_true, y_pred,","439","             sample_weight=None,","440","             multioutput=None):","466","        Default value corresponds to 'variance_weighted', this behaviour is","467","        deprecated since version 0.17 and will be changed to 'uniform_average'","468","        starting from 0.19.","545","    if multioutput is None and y_true.shape[1] != 1:","546","        warnings.warn(\"Default 'multioutput' behavior now corresponds to \"","547","                      \"'variance_weighted' value which is deprecated since \"","548","                      \"0.17, it will be changed to 'uniform_average' \"","549","                      \"starting from 0.19.\",","550","                      DeprecationWarning)","551","        multioutput = 'variance_weighted'"]}],"sklearn\/metrics\/base.py":[{"add":[],"delete":["21","from ..exceptions import UndefinedMetricWarning as _UndefinedMetricWarning","22","from ..utils import deprecated","23","","24","","25","@deprecated(\"UndefinedMetricWarning has been moved into the sklearn.exceptions\"","26","            \" module. It will not be available here from version 0.19\")","27","class UndefinedMetricWarning(_UndefinedMetricWarning):","28","    pass","29",""]}],"doc\/modules\/classes.rst":[{"add":[],"delete":["1363","To be removed in 0.19","1364","---------------------","1365","","1366",".. autosummary::","1367","   :toctree: generated\/","1368","   :template: deprecated_class.rst","1369","","1370","   lda.LDA","1371","   qda.QDA","1372","","1373",".. autosummary::","1374","   :toctree: generated\/","1375","   :template: deprecated_function.rst","1376","","1377","   datasets.load_lfw_pairs","1378","   datasets.load_lfw_people","1379",""]}],"sklearn\/utils\/estimator_checks.py":[{"add":["212","        for check in _yield_transformer_checks(name, Estimator):","213","            yield check","410","    if hasattr(estimator, \"transform\"):","710","    funcs = [\"score\", \"fit_transform\"]","733","    funcs = [\"fit\", \"score\", \"partial_fit\", \"fit_predict\", \"fit_transform\"]","755","    methods = [\"predict\", \"transform\", \"decision_function\", \"predict_proba\"]","843","            if hasattr(estimator, \"transform\"):","861","    check_methods = [\"predict\", \"transform\", \"decision_function\",","862","                     \"predict_proba\"]"],"delete":["44","from sklearn.decomposition import NMF, ProjectedGradientNMF","69","# Estimators with deprecated transform methods. Should be removed in 0.19 when","70","# _LearntSelectorMixin is removed.","71","DEPRECATED_TRANSFORM = [","72","    \"RandomForestClassifier\", \"RandomForestRegressor\", \"ExtraTreesClassifier\",","73","    \"ExtraTreesRegressor\", \"DecisionTreeClassifier\",","74","    \"DecisionTreeRegressor\", \"ExtraTreeClassifier\", \"ExtraTreeRegressor\",","75","    \"LinearSVC\", \"SGDClassifier\", \"SGDRegressor\", \"Perceptron\",","76","    \"LogisticRegression\", \"LogisticRegressionCV\",","77","    \"GradientBoostingClassifier\", \"GradientBoostingRegressor\"]","78","","223","        if name not in DEPRECATED_TRANSFORM:","224","            for check in _yield_transformer_checks(name, Estimator):","225","                yield check","331","    if isinstance(estimator, NMF):","332","        if not isinstance(estimator, ProjectedGradientNMF):","333","            estimator.set_params(solver='cd')","334","","426","    if (hasattr(estimator, \"transform\") and","427","            name not in DEPRECATED_TRANSFORM):","727","    if name in DEPRECATED_TRANSFORM:","728","        funcs = [\"score\"]","729","    else:","730","        funcs = [\"score\", \"fit_transform\"]","753","    if name in DEPRECATED_TRANSFORM:","754","        funcs = [\"fit\", \"score\", \"partial_fit\", \"fit_predict\"]","755","    else:","756","        funcs = [","757","            \"fit\", \"score\", \"partial_fit\", \"fit_predict\", \"fit_transform\"]","779","    if name in DEPRECATED_TRANSFORM:","780","        methods = [\"predict\", \"decision_function\", \"predict_proba\"]","781","    else:","782","        methods = [","783","            \"predict\", \"transform\", \"decision_function\", \"predict_proba\"]","871","            if (hasattr(estimator, \"transform\") and","872","                    name not in DEPRECATED_TRANSFORM):","890","    if name in DEPRECATED_TRANSFORM:","891","        check_methods = [\"predict\", \"decision_function\", \"predict_proba\"]","892","    else:","893","        check_methods = [\"predict\", \"transform\", \"decision_function\",","894","                         \"predict_proba\"]"]}],"sklearn\/linear_model\/base.py":[{"add":[],"delete":["231","    @deprecated(\" and will be removed in 0.19.\")","232","    def decision_function(self, X):","233","        \"\"\"Decision function of the linear model.","234","","235","        Parameters","236","        ----------","237","        X : {array-like, sparse matrix}, shape = (n_samples, n_features)","238","            Samples.","239","","240","        Returns","241","        -------","242","        C : array, shape = (n_samples,)","243","            Returns predicted values.","244","        \"\"\"","245","        return self._decision_function(X)","246","","480","    @property","481","    @deprecated(\"``residues_`` is deprecated and will be removed in 0.19\")","482","    def residues_(self):","483","        \"\"\"Get the residues of the fitted model.\"\"\"","484","        return self._residues","485",""]}],"sklearn\/preprocessing\/data.py":[{"add":["220","           *data_min_*","226","           *data_max_*","232","           *data_range_*","296","        X = check_array(X, copy=self.copy, warn_on_dtype=True,","330","        X = check_array(X, copy=self.copy, dtype=FLOAT_DTYPES)","346","        X = check_array(X, copy=self.copy, dtype=FLOAT_DTYPES)","394","    # Unlike the scaler object, this function allows 1d input.","466","           *scale_*","542","                        warn_on_dtype=True, estimator=self, dtype=FLOAT_DTYPES)","600","        X = check_array(X, accept_sparse='csr', copy=copy, warn_on_dtype=True,","734","                        estimator=self, dtype=FLOAT_DTYPES)","764","                        estimator=self, dtype=FLOAT_DTYPES)","782","                        estimator=self, dtype=FLOAT_DTYPES)","815","    # Unlike the scaler object, this function allows 1d input.","922","                        estimator=self, dtype=FLOAT_DTYPES)"],"delete":["19","from ..utils import deprecated","53","DEPRECATION_MSG_1D = (","54","    \"Passing 1d arrays as data is deprecated in 0.17 and will \"","55","    \"raise ValueError in 0.19. Reshape your data either using \"","56","    \"X.reshape(-1, 1) if your data has a single feature or \"","57","    \"X.reshape(1, -1) if it contains a single sample.\"","58",")","59","","228","           *data_min_* instead of deprecated *data_min*.","234","           *data_max_* instead of deprecated *data_max*.","240","           *data_range_* instead of deprecated *data_range*.","251","    @property","252","    @deprecated(\"Attribute data_range will be removed in \"","253","                \"0.19. Use ``data_range_`` instead\")","254","    def data_range(self):","255","        return self.data_range_","256","","257","    @property","258","    @deprecated(\"Attribute data_min will be removed in \"","259","                \"0.19. Use ``data_min_`` instead\")","260","    def data_min(self):","261","        return self.data_min_","262","","316","        X = check_array(X, copy=self.copy, ensure_2d=False, warn_on_dtype=True,","319","        if X.ndim == 1:","320","            warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)","321","","353","        X = check_array(X, copy=self.copy, ensure_2d=False, dtype=FLOAT_DTYPES)","354","        if X.ndim == 1:","355","            warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)","371","        X = check_array(X, copy=self.copy, ensure_2d=False, dtype=FLOAT_DTYPES)","372","        if X.ndim == 1:","373","            warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)","421","    # To allow retro-compatibility, we handle here the case of 1D-input","422","    # From 0.17, 1D-input are deprecated in scaler objects","423","    # Although, we want to allow the users to keep calling this function","424","    # with 1D-input.","425","","426","    # Cast input to array, as we need to check ndim. Prior to 0.17, that was","427","    # done inside the scaler object fit_transform.","499","           *scale_* is recommended instead of deprecated *std_*.","525","    @property","526","    @deprecated(\"Attribute ``std_`` will be removed in 0.19. \"","527","                \"Use ``scale_`` instead\")","528","    def std_(self):","529","        return self.scale_","530","","581","                        ensure_2d=False, warn_on_dtype=True,","582","                        estimator=self, dtype=FLOAT_DTYPES)","583","","584","        if X.ndim == 1:","585","            warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)","643","        X = check_array(X, accept_sparse='csr', copy=copy,","644","                        ensure_2d=False, warn_on_dtype=True,","647","        if X.ndim == 1:","648","            warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)","649","","781","                        ensure_2d=False, estimator=self, dtype=FLOAT_DTYPES)","782","","783","        if X.ndim == 1:","784","            warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)","814","                        ensure_2d=False, estimator=self, dtype=FLOAT_DTYPES)","815","","816","        if X.ndim == 1:","817","            warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)","835","                        ensure_2d=False, estimator=self, dtype=FLOAT_DTYPES)","836","        if X.ndim == 1:","837","            warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)","870","    # To allow retro-compatibility, we handle here the case of 1D-input","871","    # From 0.17, 1D-input are deprecated in scaler objects","872","    # Although, we want to allow the users to keep calling this function","873","    # with 1D-input.","875","    # Cast input to array, as we need to check ndim. Prior to 0.17, that was","876","    # done inside the scaler object fit_transform.","982","                        ensure_2d=False, estimator=self, dtype=FLOAT_DTYPES)","983","","984","        if X.ndim == 1:","985","            warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)","1006","        if X.ndim == 1:","1007","            warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)","1035","        if X.ndim == 1:","1036","            warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)","1061","        if X.ndim == 1:","1062","            warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)"]}],"sklearn\/decomposition\/tests\/test_nmf.py":[{"add":["2","from sklearn.decomposition import NMF, non_negative_factorization","29","    msg = \"Invalid solver parameter 'spam'\"","71","    for init in (None, 'nndsvd', 'nndsvda', 'nndsvdar'):","72","        model = NMF(n_components=2, init=init, random_state=0)","73","        transf = model.fit_transform(A)","74","        assert_false((model.components_ < 0).any() or","75","                     (transf < 0).any())","80","    pnmf = NMF(5, init='nndsvd', random_state=0)","81","    X = np.abs(random_state.randn(6, 5))","82","    assert_less(pnmf.fit(X).reconstruction_err_, 0.05)","103","    m = NMF(n_components=4, init='nndsvd', random_state=0)","104","    ft = m.fit_transform(A)","105","    t = m.transform(A)","106","    assert_array_almost_equal(ft, t, decimal=2)","127","    m = NMF(n_components=4, init='random', random_state=0)","128","    m.fit_transform(A)","129","    t = m.transform(A)","130","    A_new = m.inverse_transform(t)","131","    assert_array_almost_equal(A, A_new, decimal=2)","148","    est1 = NMF(n_components=5, init='random', random_state=0, tol=1e-2)","149","    est2 = clone(est1)","151","    W1 = est1.fit_transform(A)","152","    W2 = est2.fit_transform(A_sparse)","153","    H1 = est1.components_","154","    H2 = est2.components_","156","    assert_array_almost_equal(W1, W2)","157","    assert_array_almost_equal(H1, H2)","167","    model = NMF(random_state=0, tol=1e-4, n_components=2)","168","    A_fit_tr = model.fit_transform(A)","169","    A_tr = model.transform(A)","170","    assert_array_almost_equal(A_fit_tr, A_tr, decimal=1)","179","    W_nmf, H, _ = non_negative_factorization(A, random_state=1, tol=1e-2)","180","    W_nmf_2, _, _ = non_negative_factorization(","181","        A, H=H, update_H=False, random_state=1, tol=1e-2)","183","    model_class = NMF(random_state=1, tol=1e-2)","184","    W_cls = model_class.fit_transform(A)","185","    W_cls_2 = model_class.transform(A)","186","    assert_array_almost_equal(W_nmf, W_cls, decimal=10)","187","    assert_array_almost_equal(W_nmf_2, W_cls_2, decimal=10)"],"delete":["2","from sklearn.decomposition import (NMF, ProjectedGradientNMF,","3","                                   non_negative_factorization)","12","from sklearn.utils.testing import assert_greater","14","from sklearn.utils.testing import ignore_warnings","29","@ignore_warnings","33","    msg = \"Invalid solver parameter: got 'spam' instead of one of\"","37","    msg = \"Invalid sparseness parameter: got 'spam' instead of one of\"","38","    assert_raise_message(ValueError, msg, NMF(sparseness=name).fit, A)","73","@ignore_warnings","78","    for solver in ('pg', 'cd'):","79","        for init in (None, 'nndsvd', 'nndsvda', 'nndsvdar'):","80","            model = NMF(n_components=2, solver=solver, init=init,","81","                        random_state=0)","82","            transf = model.fit_transform(A)","83","            assert_false((model.components_ < 0).any() or","84","                         (transf < 0).any())","87","@ignore_warnings","90","    for solver in ('pg', 'cd'):","91","        pnmf = NMF(5, solver=solver, init='nndsvd', random_state=0)","92","        X = np.abs(random_state.randn(6, 5))","93","        assert_less(pnmf.fit(X).reconstruction_err_, 0.05)","111","@ignore_warnings","115","    for solver in ('pg', 'cd'):","116","        m = NMF(solver=solver, n_components=4, init='nndsvd', random_state=0)","117","        ft = m.fit_transform(A)","118","        t = m.transform(A)","119","        assert_array_almost_equal(ft, t, decimal=2)","136","@ignore_warnings","141","    for solver in ('pg', 'cd'):","142","        m = NMF(solver=solver, n_components=4, init='random', random_state=0)","143","        m.fit_transform(A)","144","        t = m.transform(A)","145","        A_new = m.inverse_transform(t)","146","        assert_array_almost_equal(A, A_new, decimal=2)","149","@ignore_warnings","156","@ignore_warnings","157","def test_projgrad_nmf_sparseness():","158","    # Test sparseness","159","    # Test that sparsity constraints actually increase sparseness in the","160","    # part where they are applied.","161","    tol = 1e-2","162","    A = np.abs(random_state.randn(10, 10))","163","    m = ProjectedGradientNMF(n_components=5, random_state=0, tol=tol).fit(A)","164","    data_sp = ProjectedGradientNMF(n_components=5, sparseness='data',","165","                                   random_state=0,","166","                                   tol=tol).fit(A).data_sparseness_","167","    comp_sp = ProjectedGradientNMF(n_components=5, sparseness='components',","168","                                   random_state=0,","169","                                   tol=tol).fit(A).comp_sparseness_","170","    assert_greater(data_sp, m.data_sparseness_)","171","    assert_greater(comp_sp, m.comp_sparseness_)","172","","173","","174","@ignore_warnings","183","    for solver in ('pg', 'cd'):","184","        est1 = NMF(solver=solver, n_components=5, init='random',","185","                   random_state=0, tol=1e-2)","186","        est2 = clone(est1)","188","        W1 = est1.fit_transform(A)","189","        W2 = est2.fit_transform(A_sparse)","190","        H1 = est1.components_","191","        H2 = est2.components_","193","        assert_array_almost_equal(W1, W2)","194","        assert_array_almost_equal(H1, H2)","197","@ignore_warnings","205","    for solver in ('pg', 'cd'):","206","        model = NMF(solver=solver, random_state=0, tol=1e-4, n_components=2)","207","        A_fit_tr = model.fit_transform(A)","208","        A_tr = model.transform(A)","209","        assert_array_almost_equal(A_fit_tr, A_tr, decimal=1)","212","@ignore_warnings","219","    for solver in ('pg', 'cd'):","220","        W_nmf, H, _ = non_negative_factorization(","221","            A, solver=solver, random_state=1, tol=1e-2)","222","        W_nmf_2, _, _ = non_negative_factorization(","223","            A, H=H, update_H=False, solver=solver, random_state=1, tol=1e-2)","225","        model_class = NMF(solver=solver, random_state=1, tol=1e-2)","226","        W_cls = model_class.fit_transform(A)","227","        W_cls_2 = model_class.transform(A)","228","        assert_array_almost_equal(W_nmf, W_cls, decimal=10)","229","        assert_array_almost_equal(W_nmf_2, W_cls_2, decimal=10)","232","@ignore_warnings"]}],"\/dev\/null":[{"add":[],"delete":[]}],"sklearn\/linear_model\/stochastic_gradient.py":[{"add":["15","from ..utils import check_array, check_random_state, check_X_y","497","        if self.class_weight in ['balanced']:","545","class SGDClassifier(BaseSGDClassifier):","1078","class SGDRegressor(BaseSGDRegressor):"],"delete":["15","from ..feature_selection.from_model import _LearntSelectorMixin","16","from ..utils import (check_array, check_random_state, check_X_y,","17","                     deprecated)","499","        if self.class_weight in ['balanced', 'auto']:","547","class SGDClassifier(BaseSGDClassifier, _LearntSelectorMixin):","974","    @deprecated(\" and will be removed in 0.19.\")","975","    def decision_function(self, X):","976","        \"\"\"Predict using the linear model","977","","978","        Parameters","979","        ----------","980","        X : {array-like, sparse matrix}, shape (n_samples, n_features)","981","","982","        Returns","983","        -------","984","        array, shape (n_samples,)","985","           Predicted target values per element in X.","986","        \"\"\"","987","        return self._decision_function(X)","988","","1095","class SGDRegressor(BaseSGDRegressor, _LearntSelectorMixin):"]}],"sklearn\/decomposition\/__init__.py":[{"add":["6","from .nmf import NMF, non_negative_factorization"],"delete":["6","from .nmf import NMF, ProjectedGradientNMF, non_negative_factorization","28","           'ProjectedGradientNMF',"]}],"sklearn\/ensemble\/gradient_boosting.py":[{"add":["715","class BaseGradientBoosting(six.with_metaclass(ABCMeta, BaseEnsemble)):"],"delete":["33","from ..feature_selection.from_model import _LearntSelectorMixin","57","from ..utils import deprecated","717","class BaseGradientBoosting(six.with_metaclass(ABCMeta, BaseEnsemble,","718","                                              _LearntSelectorMixin)):","1127","    @deprecated(\" and will be removed in 0.19\")","1128","    def decision_function(self, X):","1129","        \"\"\"Compute the decision function of ``X``.","1130","","1131","        Parameters","1132","        ----------","1133","        X : array-like of shape = [n_samples, n_features]","1134","            The input samples.","1135","","1136","        Returns","1137","        -------","1138","        score : array, shape = [n_samples, n_classes] or [n_samples]","1139","            The decision function of the input samples. The order of the","1140","            classes corresponds to that in the attribute `classes_`.","1141","            Regression and binary classification produce an array of shape","1142","            [n_samples].","1143","        \"\"\"","1144","","1145","        self._check_initialized()","1146","        X = self.estimators_[0, 0]._validate_X_predict(X, check_input=True)","1147","        score = self._decision_function(X)","1148","        if score.shape[1] == 1:","1149","            return score.ravel()","1150","        return score","1179","    @deprecated(\" and will be removed in 0.19\")","1180","    def staged_decision_function(self, X):","1181","        \"\"\"Compute decision function of ``X`` for each iteration.","1182","","1183","        This method allows monitoring (i.e. determine error on testing set)","1184","        after each stage.","1185","","1186","        Parameters","1187","        ----------","1188","        X : array-like of shape = [n_samples, n_features]","1189","            The input samples.","1190","","1191","        Returns","1192","        -------","1193","        score : generator of array, shape = [n_samples, k]","1194","            The decision function of the input samples. The order of the","1195","            classes corresponds to that in the attribute `classes_`.","1196","            Regression and binary classification are special cases with","1197","            ``k == 1``, otherwise ``k==n_classes``.","1198","        \"\"\"","1199","        for dec in self._staged_decision_function(X):","1200","            # no yield from in Python2.X","1201","            yield dec","1202",""]}]}},"b6941847d6f224446917ba52b4467cfbf082fcd8":{"changes":{"sklearn\/ensemble\/voting_classifier.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/ensemble\/tests\/test_voting_classifier.py":"MODIFY"},"diff":{"sklearn\/ensemble\/voting_classifier.py":[{"add":["143","        if (self.weights is not None and","144","                len(self.weights) != len(self.estimators)):"],"delete":["143","        if self.weights and len(self.weights) != len(self.estimators):"]}],"doc\/whats_new.rst":[{"add":["110","   - Fix a bug where :class:`sklearn.ensemble.VotingClassifier` raises an error","111","     when a numpy array is passed in for weights. :issue:`7983` by","112","     :user:`Vincent Pham <vincentpham1991>`.","113","","4836","","4837",".. _Vincent Pham: https:\/\/github.com\/vincentpham1991"],"delete":[]}],"sklearn\/ensemble\/tests\/test_voting_classifier.py":[{"add":["260","","261","","262","def test_estimator_weights_format():","263","    # Test estimator weights inputs as list and array","264","    clf1 = LogisticRegression(random_state=123)","265","    clf2 = RandomForestClassifier(random_state=123)","266","    eclf1 = VotingClassifier(estimators=[","267","                ('lr', clf1), ('rf', clf2)],","268","                weights=[1, 2],","269","                voting='soft')","270","    eclf2 = VotingClassifier(estimators=[","271","                ('lr', clf1), ('rf', clf2)],","272","                weights=np.array((1, 2)),","273","                voting='soft')","274","    eclf1.fit(X, y)","275","    eclf2.fit(X, y)","276","    assert_array_equal(eclf1.predict_proba(X), eclf2.predict_proba(X))"],"delete":[]}]}},"63583fe658886cc5eb48a2ba9a541d5f6be7194b":{"changes":{"sklearn\/datasets\/tests\/test_covtype.py":"MODIFY","sklearn\/datasets\/species_distributions.py":"MODIFY","sklearn\/datasets\/california_housing.py":"MODIFY","sklearn\/datasets\/covtype.py":"MODIFY","sklearn\/datasets\/olivetti_faces.py":"MODIFY","sklearn\/datasets\/kddcup99.py":"MODIFY","sklearn\/datasets\/tests\/test_kddcup99.py":"MODIFY"},"diff":{"sklearn\/datasets\/tests\/test_covtype.py":[{"add":["16","    except IOError:","17","        raise SkipTest(\"Covertype dataset can not be loaded.\")"],"delete":["5","import errno","17","    except IOError as e:","18","        if e.errno == errno.ENOENT:","19","            raise SkipTest(\"Covertype dataset can not be loaded.\")"]}],"sklearn\/datasets\/species_distributions.py":[{"add":["224","        if not download_if_missing:","225","            raise IOError(\"Data not found and `download_if_missing` is False\")","226",""],"delete":[]}],"sklearn\/datasets\/california_housing.py":[{"add":["89","","92","        if not download_if_missing:","93","            raise IOError(\"Data not found and `download_if_missing` is False\")","94",""],"delete":[]}],"sklearn\/datasets\/covtype.py":[{"add":["101","    elif not available:","102","        if not download_if_missing:","103","            raise IOError(\"Data not found and `download_if_missing` is False\")"],"delete":[]}],"sklearn\/datasets\/olivetti_faces.py":[{"add":["113","        if not download_if_missing:","114","            raise IOError(\"Data not found and `download_if_missing` is False\")","115","","126",""],"delete":[]}],"sklearn\/datasets\/kddcup99.py":[{"add":["347","    elif not available:","348","        if not download_if_missing:","349","            raise IOError(\"Data not found and `download_if_missing` is False\")"],"delete":[]}],"sklearn\/datasets\/tests\/test_kddcup99.py":[{"add":["14","    except IOError:","15","        raise SkipTest(\"kddcup99 dataset can not be loaded.\")"],"delete":["7","import errno","15","    except IOError as e:","16","        if e.errno == errno.ENOENT:","17","            raise SkipTest(\"kddcup99 dataset can not be loaded.\")"]}]}},"790119043be2d2d6c4def13d3a138c1cc19c29aa":{"changes":{"sklearn\/neural_network\/multilayer_perceptron.py":"MODIFY","doc\/modules\/neural_networks_supervised.rst":"MODIFY","sklearn\/neural_network\/tests\/test_mlp.py":"MODIFY"},"diff":{"sklearn\/neural_network\/multilayer_perceptron.py":[{"add":["136","        in lbfgs","347","        # lbfgs does not support mini-batches","348","        if self.solver == 'lbfgs':","377","        elif self.solver == 'lbfgs':","424","        supported_solvers = _STOCHASTIC_SOLVERS + [\"lbfgs\"]","706","    solver : {'lbfgs', 'sgd', 'adam'}, default 'adam'","709","        - 'lbfgs' is an optimizer in the family of quasi-Newton methods.","719","        For small datasets, however, 'lbfgs' can converge faster and perform","727","        If the solver is 'lbfgs', the classifier will not use minibatch.","1048","    solver : {'lbfgs', 'sgd', 'adam'}, default 'adam'","1051","        - 'lbfgs' is an optimizer in the family of quasi-Newton methods.","1061","        For small datasets, however, 'lbfgs' can converge faster and perform","1069","        If the solver is 'lbfgs', the classifier will not use minibatch."],"delete":["136","        in lbgfs","347","        # lbgfs does not support mini-batches","348","        if self.solver == 'lbgfs':","377","        elif self.solver == 'lbgfs':","424","        supported_solvers = _STOCHASTIC_SOLVERS + [\"lbgfs\"]","706","    solver : {'lbgfs', 'sgd', 'adam'}, default 'adam'","709","        - 'lbgfs' is an optimizer in the family of quasi-Newton methods.","719","        For small datasets, however, 'lbgfs' can converge faster and perform","727","        If the solver is 'lbgfs', the classifier will not use minibatch.","1048","    solver : {'lbgfs', 'sgd', 'adam'}, default 'adam'","1051","        - 'lbgfs' is an optimizer in the family of quasi-Newton methods.","1061","        For small datasets, however, 'lbgfs' can converge faster and perform","1069","        If the solver is 'lbgfs', the classifier will not use minibatch."]}],"doc\/modules\/neural_networks_supervised.rst":[{"add":["88","    >>> clf = MLPClassifier(solver='lbfgs', alpha=1e-5,","97","           solver='lbfgs', tol=0.0001, validation_fraction=0.1, verbose=False,","136","    >>> clf = MLPClassifier(solver='lbfgs', alpha=1e-5,","145","           solver='lbfgs', tol=0.0001, validation_fraction=0.1, verbose=False,"],"delete":["88","    >>> clf = MLPClassifier(solver='lbgfs', alpha=1e-5,","97","           solver='lbgfs', tol=0.0001, validation_fraction=0.1, verbose=False,","136","    >>> clf = MLPClassifier(solver='lbgfs', alpha=1e-5,","145","           solver='lbgfs', tol=0.0001, validation_fraction=0.1, verbose=False,"]}],"sklearn\/neural_network\/tests\/test_mlp.py":[{"add":["178","                                solver='lbfgs', alpha=1e-5,","237","            mlp = MLPClassifier(solver='lbfgs', hidden_layer_sizes=50,","252","        mlp = MLPRegressor(solver='lbfgs', hidden_layer_sizes=50,","289","    mlp = MLPClassifier(solver='lbfgs', hidden_layer_sizes=50, alpha=1e-5,","307","    mlp = MLPRegressor(solver='lbfgs', hidden_layer_sizes=50, max_iter=200,","390","    # lbfgs doesn't support partial_fit","391","    assert_false(hasattr(MLPClassifier(solver='lbfgs'), 'partial_fit'))","473","    clf = MLPClassifier(solver='lbfgs', hidden_layer_sizes=30,","495","    mlp = MLPClassifier(solver='lbfgs', hidden_layer_sizes=15,"],"delete":["178","                                solver='lbgfs', alpha=1e-5,","237","            mlp = MLPClassifier(solver='lbgfs', hidden_layer_sizes=50,","252","        mlp = MLPRegressor(solver='lbgfs', hidden_layer_sizes=50,","289","    mlp = MLPClassifier(solver='lbgfs', hidden_layer_sizes=50, alpha=1e-5,","307","    mlp = MLPRegressor(solver='lbgfs', hidden_layer_sizes=50, max_iter=200,","390","    # lbgfs doesn't support partial_fit","391","    assert_false(hasattr(MLPClassifier(solver='lbgfs'), 'partial_fit'))","473","    clf = MLPClassifier(solver='lbgfs', hidden_layer_sizes=30,","495","    mlp = MLPClassifier(solver='lbgfs', hidden_layer_sizes=15,"]}]}},"20912ed389414dcd69c538a3ed6921a9af0a5b0d":{"changes":{"build_tools\/travis\/flake8_diff.sh":"MODIFY"},"diff":{"build_tools\/travis\/flake8_diff.sh":[{"add":["34","echo \"Remotes:\"","35","echo '--------------------------------------------------------------------------------'","36","git remote --verbose","37","","38","# Travis does the git clone with a limited depth (50 at the time of","39","# writing). This may not be enough to find the common ancestor with","40","# $REMOTE\/master so we unshallow the git checkout","41","if [[ -a .git\/shallow ]]; then","42","    echo -e '\\nTrying to unshallow the repo:'","43","    echo '--------------------------------------------------------------------------------'","44","    git fetch --unshallow","45","fi","46","","63","        LOCAL_BRANCH_REF=travis_pr_$TRAVIS_PULL_REQUEST","64","        # In Travis the PR target is always origin","65","        git fetch origin pull\/$TRAVIS_PULL_REQUEST\/head:refs\/$LOCAL_BRANCH_REF","70","# ancestor between $LOCAL_BRANCH_REF and $REMOTE\/master","72","    if [[ -z \"$LOCAL_BRANCH_REF\" ]]; then","73","        LOCAL_BRANCH_REF=$(git rev-parse --abbrev-ref HEAD)","74","    fi","75","    echo -e \"\\nLast 2 commits in $LOCAL_BRANCH_REF:\"","76","    echo '--------------------------------------------------------------------------------'","77","    git log -2 $LOCAL_BRANCH_REF","78","","81","    echo -e \"\\nFetching $REMOTE_MASTER_REF\"","82","    echo '--------------------------------------------------------------------------------'","84","    LOCAL_BRANCH_SHORT_HASH=$(git rev-parse --short $LOCAL_BRANCH_REF)","85","    REMOTE_MASTER_SHORT_HASH=$(git rev-parse --short $REMOTE_MASTER_REF)","87","    COMMIT=$(git merge-base $LOCAL_BRANCH_REF $REMOTE_MASTER_REF) || \\","88","        echo \"No common ancestor found for $(git show $LOCAL_BRANCH_REF -q) and $(git show $REMOTE_MASTER_REF -q)\"","94","    COMMIT_SHORT_HASH=$(git rev-parse --short $COMMIT)","96","    echo -e \"\\nCommon ancestor between $LOCAL_BRANCH_REF ($LOCAL_BRANCH_SHORT_HASH)\"\\","97","         \"and $REMOTE_MASTER_REF ($REMOTE_MASTER_SHORT_HASH) is $COMMIT_SHORT_HASH:\"","98","    echo '--------------------------------------------------------------------------------'","99","    git show --no-patch $COMMIT_SHORT_HASH","100","","101","    COMMIT_RANGE=\"$COMMIT_SHORT_HASH..$LOCAL_BRANCH_SHORT_HASH\"","102","","103","    if [[ -n \"$TMP_REMOTE\" ]]; then","104","        git remote remove $TMP_REMOTE","105","    fi"],"delete":["22","echo \"Remotes:\"","23","git remote --verbose","24","","48","        else","49","            # Travis does the git clone with a limited depth (50 at the time of","50","            # writing). This may not be enough to find the common ancestor with","51","            # $REMOTE\/master so we unshallow the git checkout","52","            git fetch --unshallow || echo \"Unshallowing the git checkout failed\"","58","        BRANCH_NAME=travis_pr_$TRAVIS_PULL_REQUEST","59","        git fetch $REMOTE pull\/$TRAVIS_PULL_REQUEST\/head:$BRANCH_NAME","60","        git checkout $BRANCH_NAME","64","","65","echo -e '\\nLast 2 commits:'","66","echo '--------------------------------------------------------------------------------'","67","git log -2 --pretty=short","68","","70","# ancestor between HEAD and $REMOTE\/master","76","    COMMIT=$(git merge-base @ $REMOTE_MASTER_REF) || \\","77","        echo \"No common ancestor found for $(git show @ -q) and $(git show $REMOTE_MASTER_REF -q)\"","78","","79","    if [[ -n \"$TMP_REMOTE\" ]]; then","80","        git remote remove $TMP_REMOTE","81","    fi","87","    echo -e \"\\nCommon ancestor between HEAD and $REMOTE_MASTER_REF is:\"","88","    echo '--------------------------------------------------------------------------------'","89","    git show --no-patch $COMMIT","91","    COMMIT_RANGE=\"$(git rev-parse --short $COMMIT)..$(git rev-parse --short @)\""]}]}},"726fa36f2556e0d604d85a1de48ba56a8b6550db":{"changes":{"sklearn\/tests\/test_impute.py":"MODIFY","doc\/whats_new\/v0.20.rst":"MODIFY","examples\/plot_missing_values.py":"MODIFY","doc\/modules\/classes.rst":"MODIFY","doc\/modules\/impute.rst":"MODIFY","sklearn\/impute.py":"MODIFY","sklearn\/utils\/estimator_checks.py":"MODIFY"},"diff":{"sklearn\/tests\/test_impute.py":[{"add":["15","from sklearn.impute import MissingIndicator","710","@pytest.mark.parametrize(","711","    \"X_fit, X_trans, params, msg_err\",","712","    [(np.array([[-1, 1], [1, 2]]), np.array([[-1, 1], [1, -1]]),","713","      {'features': 'missing-only', 'sparse': 'auto'},","714","      'have missing values in transform but have no missing values in fit'),","715","     (np.array([[-1, 1], [1, 2]]), np.array([[-1, 1], [1, 2]]),","716","      {'features': 'random', 'sparse': 'auto'},","717","      \"'features' has to be either 'missing-only' or 'all'\"),","718","     (np.array([[-1, 1], [1, 2]]), np.array([[-1, 1], [1, 2]]),","719","      {'features': 'all', 'sparse': 'random'},","720","      \"'sparse' has to be a boolean or 'auto'\")]","721",")","722","def test_missing_indicator_error(X_fit, X_trans, params, msg_err):","723","    indicator = MissingIndicator(missing_values=-1)","724","    indicator.set_params(**params)","725","    with pytest.raises(ValueError, match=msg_err):","726","        indicator.fit(X_fit).transform(X_trans)","727","","728","","729","@pytest.mark.parametrize(","730","    \"missing_values, dtype\",","731","    [(np.nan, np.float64),","732","     (0, np.int32),","733","     (-1, np.int32)])","734","@pytest.mark.parametrize(","735","    \"arr_type\",","736","    [np.array, sparse.csc_matrix, sparse.csr_matrix, sparse.coo_matrix,","737","     sparse.lil_matrix, sparse.bsr_matrix])","738","@pytest.mark.parametrize(","739","    \"param_features, n_features, features_indices\",","740","    [('missing-only', 2, np.array([0, 1])),","741","     ('all', 3, np.array([0, 1, 2]))])","742","def test_missing_indicator_new(missing_values, arr_type, dtype, param_features,","743","                               n_features, features_indices):","744","    X_fit = np.array([[missing_values, missing_values, 1],","745","                      [4, missing_values, 2]])","746","    X_trans = np.array([[missing_values, missing_values, 1],","747","                        [4, 12, 10]])","748","    X_fit_expected = np.array([[1, 1, 0], [0, 1, 0]])","749","    X_trans_expected = np.array([[1, 1, 0], [0, 0, 0]])","750","","751","    # convert the input to the right array format and right dtype","752","    X_fit = arr_type(X_fit).astype(dtype)","753","    X_trans = arr_type(X_trans).astype(dtype)","754","    X_fit_expected = X_fit_expected.astype(dtype)","755","    X_trans_expected = X_trans_expected.astype(dtype)","756","","757","    indicator = MissingIndicator(missing_values=missing_values,","758","                                 features=param_features,","759","                                 sparse=False)","760","    X_fit_mask = indicator.fit_transform(X_fit)","761","    X_trans_mask = indicator.transform(X_trans)","762","","763","    assert X_fit_mask.shape[1] == n_features","764","    assert X_trans_mask.shape[1] == n_features","765","","766","    assert_array_equal(indicator.features_, features_indices)","767","    assert_allclose(X_fit_mask, X_fit_expected[:, features_indices])","768","    assert_allclose(X_trans_mask, X_trans_expected[:, features_indices])","769","","770","    assert X_fit_mask.dtype == bool","771","    assert X_trans_mask.dtype == bool","772","    assert isinstance(X_fit_mask, np.ndarray)","773","    assert isinstance(X_trans_mask, np.ndarray)","774","","775","    indicator.set_params(sparse=True)","776","    X_fit_mask_sparse = indicator.fit_transform(X_fit)","777","    X_trans_mask_sparse = indicator.transform(X_trans)","778","","779","    assert X_fit_mask_sparse.dtype == bool","780","    assert X_trans_mask_sparse.dtype == bool","781","    assert X_fit_mask_sparse.format == 'csc'","782","    assert X_trans_mask_sparse.format == 'csc'","783","    assert_allclose(X_fit_mask_sparse.toarray(), X_fit_mask)","784","    assert_allclose(X_trans_mask_sparse.toarray(), X_trans_mask)","785","","786","","787","@pytest.mark.parametrize(\"param_sparse\", [True, False, 'auto'])","788","@pytest.mark.parametrize(\"missing_values\", [np.nan, 0])","789","@pytest.mark.parametrize(","790","    \"arr_type\",","791","    [np.array, sparse.csc_matrix, sparse.csr_matrix, sparse.coo_matrix])","792","def test_missing_indicator_sparse_param(arr_type, missing_values,","793","                                        param_sparse):","794","    # check the format of the output with different sparse parameter","795","    X_fit = np.array([[missing_values, missing_values, 1],","796","                      [4, missing_values, 2]])","797","    X_trans = np.array([[missing_values, missing_values, 1],","798","                        [4, 12, 10]])","799","    X_fit = arr_type(X_fit).astype(np.float64)","800","    X_trans = arr_type(X_trans).astype(np.float64)","801","","802","    indicator = MissingIndicator(missing_values=missing_values,","803","                                 sparse=param_sparse)","804","    X_fit_mask = indicator.fit_transform(X_fit)","805","    X_trans_mask = indicator.transform(X_trans)","806","","807","    if param_sparse is True:","808","        assert X_fit_mask.format == 'csc'","809","        assert X_trans_mask.format == 'csc'","810","    elif param_sparse == 'auto' and missing_values == 0:","811","        assert isinstance(X_fit_mask, np.ndarray)","812","        assert isinstance(X_trans_mask, np.ndarray)","813","    elif param_sparse is False:","814","        assert isinstance(X_fit_mask, np.ndarray)","815","        assert isinstance(X_trans_mask, np.ndarray)","816","    else:","817","        if sparse.issparse(X_fit):","818","            assert X_fit_mask.format == 'csc'","819","            assert X_trans_mask.format == 'csc'","820","        else:","821","            assert isinstance(X_fit_mask, np.ndarray)","822","            assert isinstance(X_trans_mask, np.ndarray)","823","","824",""],"delete":[]}],"doc\/whats_new\/v0.20.rst":[{"add":["151","- Added :class:`MissingIndicator` which generates a binary indicator for","152","  missing values. :issue:`8075` by :user:`Maniteja Nandana <maniteja123>` and","153","  :user:`Guillaume Lemaitre <glemaitre>`.","154","  "],"delete":[]}],"examples\/plot_missing_values.py":[{"add":["6","value using the basic :func:`sklearn.impute.SimpleImputer`.","10","Another option is the :func:`sklearn.impute.ChainedImputer`. This uses","11","round-robin linear regression, treating every variable as an output in","12","turn. The version implemented assumes Gaussian (output) variables. If your","13","features are obviously non-Normal, consider transforming them to look more","14","Normal so as to improve performance.","15","","16","In addition of using an imputing method, we can also keep an indication of the","17","missing information using :func:`sklearn.impute.MissingIndicator` which might","18","carry some information.","27","from sklearn.pipeline import make_pipeline, make_union","28","from sklearn.impute import SimpleImputer, ChainedImputer, MissingIndicator","66","    estimator = make_pipeline(","67","        make_union(SimpleImputer(missing_values=0, strategy=\"mean\"),","68","                   MissingIndicator(missing_values=0)),","69","        RandomForestRegressor(random_state=0, n_estimators=100))","74","    estimator = make_pipeline(","75","        make_union(ChainedImputer(missing_values=0, random_state=0),","76","                   MissingIndicator(missing_values=0)),","77","        RandomForestRegressor(random_state=0, n_estimators=100))"],"delete":["6","value using the basic ``SimpleImputer``.","10","Another option is the ``ChainedImputer``. This uses round-robin linear","11","regression, treating every variable as an output in turn. The version","12","implemented assumes Gaussian (output) variables. If your features are obviously","13","non-Normal, consider transforming them to look more Normal so as to improve","14","performance.","23","from sklearn.pipeline import Pipeline","24","from sklearn.impute import SimpleImputer, ChainedImputer","62","    estimator = Pipeline([(\"imputer\", SimpleImputer(missing_values=0,","63","                                                    strategy=\"mean\")),","64","                          (\"forest\", RandomForestRegressor(random_state=0,","65","                                                           n_estimators=100))])","70","    estimator = Pipeline([(\"imputer\", ChainedImputer(missing_values=0,","71","                                                     random_state=0)),","72","                          (\"forest\", RandomForestRegressor(random_state=0,","73","                                                           n_estimators=100))])"]}],"doc\/modules\/classes.rst":[{"add":["658","   impute.MissingIndicator","659","   "],"delete":["658",""]}],"doc\/modules\/impute.rst":[{"add":["143","","144",".. _missing_indicator:","145","","146","Marking imputed values","147","======================","148","","149","The :class:`MissingIndicator` transformer is useful to transform a dataset into","150","corresponding binary matrix indicating the presence of missing values in the","151","dataset. This transformation is useful in conjunction with imputation. When","152","using imputation, preserving the information about which values had been","153","missing can be informative.","154","","155","``NaN`` is usually used as the placeholder for missing values. However, it","156","enforces the data type to be float. The parameter ``missing_values`` allows to","157","specify other placeholder such as integer. In the following example, we will","158","use ``-1`` as missing values::","159","","160","  >>> from sklearn.impute import MissingIndicator","161","  >>> X = np.array([[-1, -1, 1, 3],","162","  ...               [4, -1, 0, -1],","163","  ...               [8, -1, 1, 0]])","164","  >>> indicator = MissingIndicator(missing_values=-1)","165","  >>> mask_missing_values_only = indicator.fit_transform(X)","166","  >>> mask_missing_values_only","167","  array([[ True,  True, False],","168","         [False,  True,  True],","169","         [False,  True, False]])","170","","171","The ``features`` parameter is used to choose the features for which the mask is","172","constructed. By default, it is ``'missing-only'`` which returns the imputer","173","mask of the features containing missing values at ``fit`` time::","174","","175","  >>> indicator.features_","176","  array([0, 1, 3])","177","","178","The ``features`` parameter can be set to ``'all'`` to returned all features","179","whether or not they contain missing values::","180","    ","181","  >>> indicator = MissingIndicator(missing_values=-1, features=\"all\")","182","  >>> mask_all = indicator.fit_transform(X)","183","  >>> mask_all","184","  array([[ True,  True, False, False],","185","         [False,  True, False,  True],","186","         [False,  True, False, False]])","187","  >>> indicator.features_","188","  array([0, 1, 2, 3])"],"delete":["123",""]}],"sklearn\/impute.py":[{"add":["37","    'MissingIndicator',","978","","979","","980","class MissingIndicator(BaseEstimator, TransformerMixin):","981","    \"\"\"Binary indicators for missing values.","982","","983","    Parameters","984","    ----------","985","    missing_values : number, string, np.nan (default) or None","986","        The placeholder for the missing values. All occurrences of","987","        `missing_values` will be imputed.","988","","989","    features : str, optional","990","        Whether the imputer mask should represent all or a subset of","991","        features.","992","","993","        - If \"missing-only\" (default), the imputer mask will only represent","994","          features containing missing values during fit time.","995","        - If \"all\", the imputer mask will represent all features.","996","","997","    sparse : boolean or \"auto\", optional","998","        Whether the imputer mask format should be sparse or dense.","999","","1000","        - If \"auto\" (default), the imputer mask will be of same type as","1001","          input.","1002","        - If True, the imputer mask will be a sparse matrix.","1003","        - If False, the imputer mask will be a numpy array.","1004","","1005","    error_on_new : boolean, optional","1006","        If True (default), transform will raise an error when there are","1007","        features with missing values in transform that have no missing values","1008","        in fit This is applicable only when ``features=\"missing-only\"``.","1009","","1010","    Attributes","1011","    ----------","1012","    features_ : ndarray, shape (n_missing_features,) or (n_features,)","1013","        The features indices which will be returned when calling ``transform``.","1014","        They are computed during ``fit``. For ``features='all'``, it is","1015","        to ``range(n_features)``.","1016","","1017","    Examples","1018","    --------","1019","    >>> import numpy as np","1020","    >>> from sklearn.impute import MissingIndicator","1021","    >>> X1 = np.array([[np.nan, 1, 3],","1022","    ...                [4, 0, np.nan],","1023","    ...                [8, 1, 0]])","1024","    >>> X2 = np.array([[5, 1, np.nan],","1025","    ...                [np.nan, 2, 3],","1026","    ...                [2, 4, 0]])","1027","    >>> indicator = MissingIndicator()","1028","    >>> indicator.fit(X1)","1029","    MissingIndicator(error_on_new=True, features='missing-only',","1030","             missing_values=nan, sparse='auto')","1031","    >>> X2_tr = indicator.transform(X2)","1032","    >>> X2_tr","1033","    array([[False,  True],","1034","           [ True, False],","1035","           [False, False]])","1036","","1037","    \"\"\"","1038","","1039","    def __init__(self, missing_values=np.nan, features=\"missing-only\",","1040","                 sparse=\"auto\", error_on_new=True):","1041","        self.missing_values = missing_values","1042","        self.features = features","1043","        self.sparse = sparse","1044","        self.error_on_new = error_on_new","1045","","1046","    def _get_missing_features_info(self, X):","1047","        \"\"\"Compute the imputer mask and the indices of the features","1048","        containing missing values.","1049","","1050","        Parameters","1051","        ----------","1052","        X : {ndarray or sparse matrix}, shape (n_samples, n_features)","1053","            The input data with missing values. Note that ``X`` has been","1054","            checked in ``fit`` and ``transform`` before to call this function.","1055","","1056","        Returns","1057","        -------","1058","        imputer_mask : {ndarray or sparse matrix}, shape \\","1059","(n_samples, n_features) or (n_samples, n_features_with_missing)","1060","            The imputer mask of the original data.","1061","","1062","        features_with_missing : ndarray, shape (n_features_with_missing)","1063","            The features containing missing values.","1064","","1065","        \"\"\"","1066","        if sparse.issparse(X) and self.missing_values != 0:","1067","            mask = _get_mask(X.data, self.missing_values)","1068","","1069","            # The imputer mask will be constructed with the same sparse format","1070","            # as X.","1071","            sparse_constructor = (sparse.csr_matrix if X.format == 'csr'","1072","                                  else sparse.csc_matrix)","1073","            imputer_mask = sparse_constructor(","1074","                (mask, X.indices.copy(), X.indptr.copy()),","1075","                shape=X.shape, dtype=bool)","1076","","1077","            missing_values_mask = imputer_mask.copy()","1078","            missing_values_mask.eliminate_zeros()","1079","            features_with_missing = (","1080","                np.flatnonzero(np.diff(missing_values_mask.indptr))","1081","                if missing_values_mask.format == 'csc'","1082","                else np.unique(missing_values_mask.indices))","1083","","1084","            if self.sparse is False:","1085","                imputer_mask = imputer_mask.toarray()","1086","            elif imputer_mask.format == 'csr':","1087","                imputer_mask = imputer_mask.tocsc()","1088","        else:","1089","            if sparse.issparse(X):","1090","                # case of sparse matrix with 0 as missing values. Implicit and","1091","                # explicit zeros are considered as missing values.","1092","                X = X.toarray()","1093","            imputer_mask = _get_mask(X, self.missing_values)","1094","            features_with_missing = np.flatnonzero(imputer_mask.sum(axis=0))","1095","","1096","            if self.sparse is True:","1097","                imputer_mask = sparse.csc_matrix(imputer_mask)","1098","","1099","        return imputer_mask, features_with_missing","1100","","1101","    def fit(self, X, y=None):","1102","        \"\"\"Fit the transformer on X.","1103","","1104","        Parameters","1105","        ----------","1106","        X : {array-like, sparse matrix}, shape (n_samples, n_features)","1107","            Input data, where ``n_samples`` is the number of samples and","1108","            ``n_features`` is the number of features.","1109","","1110","        Returns","1111","        -------","1112","        self : object","1113","            Returns self.","1114","        \"\"\"","1115","        if not is_scalar_nan(self.missing_values):","1116","            force_all_finite = True","1117","        else:","1118","            force_all_finite = \"allow-nan\"","1119","        X = check_array(X, accept_sparse=('csc', 'csr'),","1120","                        force_all_finite=force_all_finite)","1121","        _check_inputs_dtype(X, self.missing_values)","1122","","1123","        self._n_features = X.shape[1]","1124","","1125","        if self.features not in ('missing-only', 'all'):","1126","            raise ValueError(\"'features' has to be either 'missing-only' or \"","1127","                             \"'all'. Got {} instead.\".format(self.features))","1128","","1129","        if not ((isinstance(self.sparse, six.string_types) and","1130","                self.sparse == \"auto\") or isinstance(self.sparse, bool)):","1131","            raise ValueError(\"'sparse' has to be a boolean or 'auto'. \"","1132","                             \"Got {!r} instead.\".format(self.sparse))","1133","","1134","        self.features_ = (self._get_missing_features_info(X)[1]","1135","                          if self.features == 'missing-only'","1136","                          else np.arange(self._n_features))","1137","","1138","        return self","1139","","1140","    def transform(self, X):","1141","        \"\"\"Generate missing values indicator for X.","1142","","1143","        Parameters","1144","        ----------","1145","        X : {array-like, sparse matrix}, shape (n_samples, n_features)","1146","            The input data to complete.","1147","","1148","        Returns","1149","        -------","1150","        Xt : {ndarray or sparse matrix}, shape (n_samples, n_features)","1151","            The missing indicator for input data. The data type of ``Xt``","1152","            will be boolean.","1153","","1154","        \"\"\"","1155","        check_is_fitted(self, \"features_\")","1156","","1157","        if not is_scalar_nan(self.missing_values):","1158","            force_all_finite = True","1159","        else:","1160","            force_all_finite = \"allow-nan\"","1161","        X = check_array(X, accept_sparse=('csc', 'csr'),","1162","                        force_all_finite=force_all_finite)","1163","        _check_inputs_dtype(X, self.missing_values)","1164","","1165","        if X.shape[1] != self._n_features:","1166","            raise ValueError(\"X has a different number of features \"","1167","                             \"than during fitting.\")","1168","","1169","        imputer_mask, features = self._get_missing_features_info(X)","1170","","1171","        if self.features == \"missing-only\":","1172","            features_diff_fit_trans = np.setdiff1d(features, self.features_)","1173","            if (self.error_on_new and features_diff_fit_trans.size > 0):","1174","                raise ValueError(\"The features {} have missing values \"","1175","                                 \"in transform but have no missing values \"","1176","                                 \"in fit.\".format(features_diff_fit_trans))","1177","","1178","            if (self.features_.size > 0 and","1179","                    self.features_.size < self._n_features):","1180","                imputer_mask = imputer_mask[:, self.features_]","1181","","1182","        return imputer_mask","1183","","1184","    def fit_transform(self, X, y=None):","1185","        \"\"\"Generate missing values indicator for X.","1186","","1187","        Parameters","1188","        ----------","1189","        X : {array-like, sparse matrix}, shape (n_samples, n_features)","1190","            The input data to complete.","1191","","1192","        Returns","1193","        -------","1194","        Xt : {ndarray or sparse matrix}, shape (n_samples, n_features)","1195","            The missing indicator for input data. The data type of ``Xt``","1196","            will be boolean.","1197","","1198","        \"\"\"","1199","        return self.fit(X, y).transform(X)"],"delete":[]}],"sklearn\/utils\/estimator_checks.py":[{"add":["79","ALLOW_NAN = ['Imputer', 'SimpleImputer', 'ChainedImputer', 'MissingIndicator',"],"delete":["79","ALLOW_NAN = ['Imputer', 'SimpleImputer', 'ChainedImputer',"]}]}},"d0ce0d9b385cd6df52b9c64474ba3eaf1a438bba":{"changes":{"sklearn\/linear_model\/ransac.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/linear_model\/tests\/test_ransac.py":"MODIFY"},"diff":{"sklearn\/linear_model\/ransac.py":[{"add":["113","    max_skips : int, optional","114","        Maximum number of iterations that can be skipped due to finding zero","115","        inliers or invalid data defined by ``is_data_valid`` or invalid models","116","        defined by ``is_model_valid``.","117","","118","        .. versionadded:: 0.19","119","","177","    n_skips_no_inliers_ : int","178","        Number of iterations skipped due to finding zero inliers.","179","","180","        .. versionadded:: 0.19","181","","182","    n_skips_invalid_data_ : int","183","        Number of iterations skipped due to invalid data defined by","184","        ``is_data_valid``.","185","","186","        .. versionadded:: 0.19","187","","188","    n_skips_invalid_model_ : int","189","        Number of iterations skipped due to an invalid model defined by","190","        ``is_model_valid``.","191","","192","        .. versionadded:: 0.19","193","","203","                 is_model_valid=None, max_trials=100, max_skips=np.inf,","214","        self.max_skips = max_skips","328","        n_inliers_best = 1","329","        score_best = -np.inf","333","        self.n_skips_no_inliers_ = 0","334","        self.n_skips_invalid_data_ = 0","335","        self.n_skips_invalid_model_ = 0","345","            if (self.n_skips_no_inliers_ + self.n_skips_invalid_data_ +","346","                    self.n_skips_invalid_model_) > self.max_skips:","347","                break","348","","358","                self.n_skips_invalid_data_ += 1","371","                self.n_skips_invalid_model_ += 1","392","                self.n_skips_no_inliers_ += 1","428","            if ((self.n_skips_no_inliers_ + self.n_skips_invalid_data_ +","429","                    self.n_skips_invalid_model_) > self.max_skips):","430","                raise ValueError(","431","                    \"RANSAC skipped more iterations than `max_skips` without\"","432","                    \" finding a valid consensus set. Iterations were skipped\"","433","                    \" because each randomly chosen sub-sample failed the\"","434","                    \" passing criteria. See estimator attributes for\"","435","                    \" diagnostics (n_skips*).\")","436","            else:","437","                raise ValueError(","438","                    \"RANSAC could not find a valid consensus set. All\"","439","                    \" `max_trials` iterations were skipped because each\"","440","                    \" randomly chosen sub-sample failed the passing criteria.\"","441","                    \" See estimator attributes for diagnostics (n_skips*).\")","442","        else:","443","            if (self.n_skips_no_inliers_ + self.n_skips_invalid_data_ +","444","                    self.n_skips_invalid_model_) > self.max_skips:","445","                warnings.warn(\"RANSAC found a valid consensus set but exited\"","446","                              \" early due to skipping more iterations than\"","447","                              \" `max_skips`. See estimator attributes for\"","448","                              \" diagnostics (n_skips*).\",","449","                              UserWarning)"],"delete":["179","                 is_model_valid=None, max_trials=100,","303","        n_inliers_best = 0","304","        score_best = np.inf","359","            if n_inliers_subset == 0:","360","                raise ValueError(\"No inliers found, possible cause is \"","361","                    \"setting residual_threshold ({0}) too low.\".format(","362","                    self.residual_threshold))","397","            raise ValueError(","398","                \"RANSAC could not find valid consensus set, because\"","399","                \" either the `residual_threshold` rejected all the samples or\"","400","                \" `is_data_valid` and `is_model_valid` returned False for all\"","401","                \" `max_trials` randomly \"\"chosen sub-samples. Consider \"","402","                \"relaxing the \"\"constraints.\")"]}],"doc\/whats_new.rst":[{"add":["102","   - :class:`sklearn.linear_model.RANSACRegressor` no longer throws an error","103","     when calling ``fit`` if no inliers are found in its first iteration.","104","     Furthermore, causes of skipped iterations are tracked in newly added","105","     attributes, ``n_skips_*``.","106","     :issue:`7914` by :user:`Michael Horrell <mthorrell>`.","107","","108","   - Fix a bug where :class:`sklearn.feature_selection.SelectFdr` did not","109","     exactly implement Benjamini-Hochberg procedure. It formerly may have","110","     selected fewer features than it should.","111","     :issue:`7490` by :user:`Peng Meng <mpjlu>`.","112",""],"delete":[]}],"sklearn\/linear_model\/tests\/test_ransac.py":[{"add":["13","from sklearn.utils.testing import assert_raises_regexp","154","                                       residual_threshold=0.0, random_state=0,","155","                                       max_trials=5)","157","    msg = (\"RANSAC could not find a valid consensus set\")","158","    assert_raises_regexp(ValueError, msg, ransac_estimator.fit, X, y)","159","    assert_equal(ransac_estimator.n_skips_no_inliers_, 5)","160","    assert_equal(ransac_estimator.n_skips_invalid_data_, 0)","161","    assert_equal(ransac_estimator.n_skips_invalid_model_, 0)","162","","163","","164","def test_ransac_no_valid_data():","165","    def is_data_valid(X, y):","166","        return False","167","","168","    base_estimator = LinearRegression()","169","    ransac_estimator = RANSACRegressor(base_estimator,","170","                                       is_data_valid=is_data_valid,","171","                                       max_trials=5)","172","","173","    msg = (\"RANSAC could not find a valid consensus set\")","174","    assert_raises_regexp(ValueError, msg, ransac_estimator.fit, X, y)","175","    assert_equal(ransac_estimator.n_skips_no_inliers_, 0)","176","    assert_equal(ransac_estimator.n_skips_invalid_data_, 5)","177","    assert_equal(ransac_estimator.n_skips_invalid_model_, 0)","178","","179","","180","def test_ransac_no_valid_model():","181","    def is_model_valid(estimator, X, y):","182","        return False","183","","184","    base_estimator = LinearRegression()","185","    ransac_estimator = RANSACRegressor(base_estimator,","186","                                       is_model_valid=is_model_valid,","187","                                       max_trials=5)","188","","189","    msg = (\"RANSAC could not find a valid consensus set\")","190","    assert_raises_regexp(ValueError, msg, ransac_estimator.fit, X, y)","191","    assert_equal(ransac_estimator.n_skips_no_inliers_, 0)","192","    assert_equal(ransac_estimator.n_skips_invalid_data_, 0)","193","    assert_equal(ransac_estimator.n_skips_invalid_model_, 5)","194","","195","","196","def test_ransac_exceed_max_skips():","197","    def is_data_valid(X, y):","198","        return False","199","","200","    base_estimator = LinearRegression()","201","    ransac_estimator = RANSACRegressor(base_estimator,","202","                                       is_data_valid=is_data_valid,","203","                                       max_trials=5,","204","                                       max_skips=3)","205","","206","    msg = (\"RANSAC skipped more iterations than `max_skips`\")","207","    assert_raises_regexp(ValueError, msg, ransac_estimator.fit, X, y)","208","    assert_equal(ransac_estimator.n_skips_no_inliers_, 0)","209","    assert_equal(ransac_estimator.n_skips_invalid_data_, 4)","210","    assert_equal(ransac_estimator.n_skips_invalid_model_, 0)","211","","212","","213","def test_ransac_warn_exceed_max_skips():","214","    global cause_skip","215","    cause_skip = False","216","","217","    def is_data_valid(X, y):","218","        global cause_skip","219","        if not cause_skip:","220","            cause_skip = True","221","            return True","222","        else:","223","            return False","224","","225","    base_estimator = LinearRegression()","226","    ransac_estimator = RANSACRegressor(base_estimator,","227","                                       is_data_valid=is_data_valid,","228","                                       max_skips=3,","229","                                       max_trials=5)","230","","231","    assert_warns(UserWarning, ransac_estimator.fit, X, y)","232","    assert_equal(ransac_estimator.n_skips_no_inliers_, 0)","233","    assert_equal(ransac_estimator.n_skips_invalid_data_, 4)","234","    assert_equal(ransac_estimator.n_skips_invalid_model_, 0)"],"delete":["10","from sklearn.utils.testing import assert_raises_regexp","154","                                       residual_threshold=0.0, random_state=0)","156","    assert_raises_regexp(ValueError,","157","                    \"No inliers.*residual_threshold.*0\\.0\",","158","                    ransac_estimator.fit, X, y)"]}]}}}