{"74e4c422bd979fd68eef3ac1d3f5eedf6f9262f2":{"changes":{"sklearn\/tree\/_utils.pyx":"MODIFY","sklearn\/tree\/_criterion.pyx":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/tree\/_criterion.pxd":"MODIFY","sklearn\/tree\/_tree.pyx":"MODIFY","sklearn\/tree\/tests\/test_tree.py":"MODIFY"},"diff":{"sklearn\/tree\/_utils.pyx":[{"add":["64","    \"\"\"Return copied data as 1D numpy array of intp's.\"\"\"","67","    return np.PyArray_SimpleNewFromData(1, shape, np.NPY_INTP, data).copy()"],"delete":["64","    \"\"\"Encapsulate data into a 1D numpy array of intp's.\"\"\"","67","    return np.PyArray_SimpleNewFromData(1, shape, np.NPY_INTP, data)"]}],"sklearn\/tree\/_criterion.pyx":[{"add":["237","        self.n_samples = 0","279","        return (type(self),","712","        self.n_samples = n_samples","737","        return (type(self), (self.n_outputs, self.n_samples), self.__getstate__())","884","","1008","        self.n_samples = n_samples"],"delete":["275","","279","        return (ClassificationCriterion,","736","        return (RegressionCriterion, (self.n_outputs,), self.__getstate__())"]}],"doc\/whats_new.rst":[{"add":["69","   - Tree splitting criterion classes' cloning\/pickling is now memory safe","70","     (`#7680 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/7680>`_).","71","     By `Ibraim Ganiev`_.","72",""],"delete":[]}],"sklearn\/tree\/_criterion.pxd":[{"add":["36","    cdef SIZE_t n_samples                # Number of samples"],"delete":[]}],"sklearn\/tree\/_tree.pyx":[{"add":["549","            return sizet_ptr_to_ndarray(self.n_classes, self.n_outputs)"],"delete":["549","            # it's small; copy for memory safety","550","            return sizet_ptr_to_ndarray(self.n_classes, self.n_outputs).copy()"]}],"sklearn\/tree\/tests\/test_tree.py":[{"add":["3","import copy","45","from sklearn.tree.tree import CRITERIA_CLF","46","from sklearn.tree.tree import CRITERIA_REG","52","REG_CRITERIONS = (\"mse\", \"mae\", \"friedman_mse\")","1602","","1615","","1616","","1617","def test_criterion_copy():","1618","    # Let's check whether copy of our criterion has the same type","1619","    # and properties as original","1620","    n_outputs = 3","1621","    n_classes = np.arange(3, dtype=np.intp)","1622","    n_samples = 100","1623","","1624","    def _pickle_copy(obj):","1625","        return pickle.loads(pickle.dumps(obj))","1626","    for copy_func in [copy.copy, copy.deepcopy, _pickle_copy]:","1627","        for _, typename in CRITERIA_CLF.items():","1628","            criteria = typename(n_outputs, n_classes)","1629","            result = copy_func(criteria).__reduce__()","1630","            typename_, (n_outputs_, n_classes_), _ = result","1631","            assert_equal(typename, typename_)","1632","            assert_equal(n_outputs, n_outputs_)","1633","            assert_array_equal(n_classes, n_classes_)","1634","","1635","        for _, typename in CRITERIA_REG.items():","1636","            criteria = typename(n_outputs, n_samples)","1637","            result = copy_func(criteria).__reduce__()","1638","            typename_, (n_outputs_, n_samples_), _ = result","1639","            assert_equal(typename, typename_)","1640","            assert_equal(n_outputs, n_outputs_)","1641","            assert_equal(n_samples, n_samples_)"],"delete":["49","REG_CRITERIONS = (\"mse\", \"mae\")"]}]}},"66de4faf2b0edf8953cf56c90509eabc7a5845f6":{"changes":{"doc\/whats_new.rst":"MODIFY"},"diff":{"doc\/whats_new.rst":[{"add":["50","     that matches the ``classes_`` attribute of ``best_estimator_``. :issue:`7661`","51","     by :user:`Alyssa Batula <abatula>`_ and :user:`Dylan Werner-Meier <unautre>`.","98","","157","   - Fix a bug where :class:`sklearn.feature_selection.SelectFdr` did not","158","     exactly implement Benjamini-Hochberg procedure. It formerly may have","159","     selected fewer features than it should.","160","     :issue:`7490` by :user:`Peng Meng <mpjlu>`.","161","","162","   - :class:`sklearn.manifold.LocallyLinearEmbedding` now correctly handles","163","     integer inputs. :issue:`6282` by `Jake Vanderplas`_.","164","","165","   - The ``min_weight_fraction_leaf`` parameter of tree-based classifiers and","166","     regressors now assumes uniform sample weights by default if the","167","     ``sample_weight`` argument is not passed to the ``fit`` function.","168","     Previously, the parameter was silently ignored. :issue:`7301`","169","     by :user:`Nelson Liu <nelson-liu>`.","170","","171","   - Numerical issue with :class:`linear_model.RidgeCV` on centered data when","172","     `n_features > n_samples`. :issue:`6178` by `Bertrand Thirion`_","173","","174","   - Tree splitting criterion classes' cloning\/pickling is now memory safe","175","     :issue:`7680` by :user:`Ibraim Ganiev <olologin>`.","176","","177","   - Fixed a bug where :class:`decomposition.NMF` sets its ``n_iters_``","178","     attribute in `transform()`. :issue:`7553` by :user:`Ekaterina","179","     Krivich <kiote>`.","180","","181","   - :class:`sklearn.linear_model.LogisticRegressionCV` now correctly handles","182","     string labels. :issue:`5874` by `Raghav RV`_.","183","","204","Trees and forests","205","","206","   - The ``min_weight_fraction_leaf`` parameter of tree-based classifiers and","207","     regressors now assumes uniform sample weights by default if the","208","     ``sample_weight`` argument is not passed to the ``fit`` function.","209","     Previously, the parameter was silently ignored. :issue:`7301` by `Nelson","210","     Liu`_.","211","","212","   - Tree splitting criterion classes' cloning\/pickling is now memory safe.","213","     :issue:`7680` by `Ibraim Ganiev`_.","214","","215","","218","   - Length of ``explained_variance_ratio`` of","224","   - Numerical issue with :class:`linear_model.RidgeCV` on centered data when","225","     ``n_features > n_samples``. :issue:`6178` by `Bertrand Thirion`_","226",""],"delete":["50","     that matches the ``classes_`` attribute of ``best_estimator_``. (`#7661","51","     <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/7661>`_) by `Alyssa","52","     Batula`_ and :user:`Dylan Werner-Meier <unautre>`.","95","   - Fix a bug where :class:`sklearn.feature_selection.SelectFdr` did not","96","     exactly implement Benjamini-Hochberg procedure. It formerly may have","97","     selected fewer features than it should.","98","     :issue:`7490` by :user:`Peng Meng <mpjlu>`.","99","","100","   - :class:`sklearn.manifold.LocallyLinearEmbedding` now correctly handles","101","     integer inputs. :issue:`6282` by `Jake Vanderplas`_.","102","","103","   - The ``min_weight_fraction_leaf`` parameter of tree-based classifiers and","104","     regressors now assumes uniform sample weights by default if the","105","     ``sample_weight`` argument is not passed to the ``fit`` function.","106","     Previously, the parameter was silently ignored. :issue:`7301`","107","     by :user:`Nelson Liu <nelson-liu>`.","108","","109","   - Numerical issue with :class:`linear_model.RidgeCV` on centered data when","110","     `n_features > n_samples`. :issue:`6178` by `Bertrand Thirion`_","111","","112","   - Tree splitting criterion classes' cloning\/pickling is now memory safe","113","     :issue:`7680` by :user:`Ibraim Ganiev <olologin>`.","114","","115","   - Fixed a bug where :class:`decomposition.NMF` sets its ``n_iters_``","116","     attribute in `transform()`. :issue:`7553` by :user:`Ekaterina","117","     Krivich <kiote>`.","118","","119","   - :class:`sklearn.linear_model.LogisticRegressionCV` now correctly handles","120","     string labels. :issue:`5874` by `Raghav RV`_.","121","","206","   - Length of `explained_variance_ratio` of"]}]}},"9811b3bd062c9d07b9aff2d621e9c35ee821711d":{"changes":{"sklearn\/svm\/base.py":"MODIFY","sklearn\/svm\/tests\/test_svm.py":"MODIFY"},"diff":{"sklearn\/svm\/base.py":[{"add":["484","            raise AttributeError('coef_ is only available when using a '","485","                                 'linear kernel')"],"delete":["484","            raise ValueError('coef_ is only available when using a '","485","                             'linear kernel')"]}],"sklearn\/svm\/tests\/test_svm.py":[{"add":["59","        assert_true(hasattr(clf, \"coef_\") == (k == 'linear'))","260","    assert_false(hasattr(clf, \"coef_\"))","644","    clf = svm.LinearSVC(penalty='l1', loss='squared_hinge', dual=False,","645","                        random_state=0).fit(X, Y)"],"delete":["259","    assert_raises(ValueError, lambda: clf.coef_)","643","    clf = svm.LinearSVC(penalty='l1', loss='squared_hinge', dual=False, random_state=0).fit(X, Y)"]}]}},"6a2d8d5bbc261309d59ac30bf650e982f1477dd7":{"changes":{"sklearn\/linear_model\/setup.py":"MODIFY","sklearn\/setup.py":"MODIFY","sklearn\/tree\/setup.py":"MODIFY","sklearn\/feature_extraction\/setup.py":"MODIFY","build_tools\/travis\/install.sh":"MODIFY","build_tools\/travis\/test_script.sh":"MODIFY","setup.py":"MODIFY","sklearn\/ensemble\/setup.py":"MODIFY","sklearn\/metrics\/cluster\/setup.py":"MODIFY","sklearn\/utils\/setup.py":"MODIFY","sklearn\/svm\/setup.py":"MODIFY","\/dev\/null":"DELETE","sklearn\/cluster\/setup.py":"MODIFY",".travis.yml":"MODIFY","sklearn\/utils\/sparsetools\/setup.py":"MODIFY","sklearn\/decomposition\/setup.py":"MODIFY","sklearn\/manifold\/setup.py":"MODIFY","sklearn\/datasets\/setup.py":"MODIFY","sklearn\/neighbors\/setup.py":"MODIFY","sklearn\/__check_build\/setup.py":"MODIFY","sklearn\/_build_utils\/__init__.py":"MODIFY","sklearn\/metrics\/setup.py":"MODIFY"},"diff":{"sklearn\/linear_model\/setup.py":[{"add":["18","    config.add_extension('cd_fast', sources=['cd_fast.pyx'],","27","                         sources=['sgd_fast.pyx'],","37","                         sources=['sag_fast.pyx'],"],"delete":["18","    config.add_extension('cd_fast', sources=['cd_fast.c'],","27","                         sources=['sgd_fast.c'],","37","                         sources=['sag_fast.c'],"]}],"sklearn\/setup.py":[{"add":["4","from sklearn._build_utils import maybe_cythonize_extensions","5","","59","    config.add_extension('_isotonic',","60","                         sources=['_isotonic.pyx'],","61","                         include_dirs=[numpy.get_include()],","62","                         libraries=libraries,","63","                         )","81","    maybe_cythonize_extensions(top_path, config)","82",""],"delete":["57","    config.add_extension(","58","        '_isotonic',","59","        sources=['_isotonic.c'],","60","        include_dirs=[numpy.get_include()],","61","        libraries=libraries,","62","    )"]}],"sklearn\/tree\/setup.py":[{"add":["12","                         sources=[\"_tree.pyx\"],","17","                         sources=[\"_splitter.pyx\"],","22","                         sources=[\"_criterion.pyx\"],","27","                         sources=[\"_utils.pyx\"],"],"delete":["12","                         sources=[\"_tree.c\"],","17","                         sources=[\"_splitter.c\"],","22","                         sources=[\"_criterion.c\"],","27","                         sources=[\"_utils.c\"],"]}],"sklearn\/feature_extraction\/setup.py":[{"add":["13","                         sources=['_hashing.pyx'],"],"delete":["13","                         sources=['_hashing.c'],"]}],"build_tools\/travis\/install.sh":[{"add":["55","            libgfortran mkl flake8 \\","60","            numpy=$NUMPY_VERSION scipy=$SCIPY_VERSION \\","66","    # Temporary work around for Python 2.6 because cython >= 0.23 is","67","    # required for building scikit-learn but python 2.6 and cython","68","    # 0.23 are not compatible in conda. Remove the next line and","69","    # install cython via conda when Python 2.6 support is removed.","70","    pip install cython==$CYTHON_VERSION","71",""],"delete":["55","            cython=$CYTHON_VERSION libgfortran mkl flake8 \\","60","            numpy=$NUMPY_VERSION scipy=$SCIPY_VERSION cython=$CYTHON_VERSION \\","103","    if [ ! -d \"$CACHED_BUILD_DIR\" ]; then","104","        mkdir -p $CACHED_BUILD_DIR","105","    fi","106","","107","    rsync -av --exclude '.git\/' --exclude='testvenv\/' \\","108","          $TRAVIS_BUILD_DIR $CACHED_BUILD_DIR","109","","110","    cd $CACHED_BUILD_DIR\/scikit-learn","111",""]}],"build_tools\/travis\/test_script.sh":[{"add":["42","    cd $OLDPWD"],"delete":["41","    # Is directory still empty ?","42","    ls -ltra","43","","45","    cd $CACHED_BUILD_DIR\/scikit-learn"]}],"setup.py":[{"add":["217","        # For these actions, NumPy is not required"],"delete":["86","            cython_hash_file = os.path.join(cwd, 'cythonize.dat')","87","            if os.path.exists(cython_hash_file):","88","                os.unlink(cython_hash_file)","183","def generate_cython():","184","    cwd = os.path.abspath(os.path.dirname(__file__))","185","    print(\"Cythonizing sources\")","186","    p = subprocess.call([sys.executable, os.path.join(cwd,","187","                                                      'build_tools',","188","                                                      'cythonize.py'),","189","                         'sklearn'],","190","                        cwd=cwd)","191","    if p != 0:","192","        raise RuntimeError(\"Running cythonize failed!\")","193","","194","","232","        # For these actions, NumPy is not required, nor Cythonization","280","        if len(sys.argv) >= 2 and sys.argv[1] not in 'config':","281","            # Cythonize if needed","282","","283","            print('Generating cython files')","284","            cwd = os.path.abspath(os.path.dirname(__file__))","285","            if not os.path.exists(os.path.join(cwd, 'PKG-INFO')):","286","                # Generate Cython sources, unless building from source release","287","                generate_cython()","288","","289","            # Clean left-over .so file","290","            for dirpath, dirnames, filenames in os.walk(","291","                    os.path.join(cwd, 'sklearn')):","292","                for filename in filenames:","293","                    extension = os.path.splitext(filename)[1]","294","                    if extension in (\".so\", \".pyd\", \".dll\"):","295","                        pyx_file = str.replace(filename, extension, '.pyx')","296","                        print(pyx_file)","297","                        if not os.path.exists(os.path.join(dirpath, pyx_file)):","298","                            os.unlink(os.path.join(dirpath, filename))","299",""]}],"sklearn\/ensemble\/setup.py":[{"add":["7","                         sources=[\"_gradient_boosting.pyx\"],"],"delete":["7","                         sources=[\"_gradient_boosting.c\"],"]}],"sklearn\/metrics\/cluster\/setup.py":[{"add":["12","                         sources=[\"expected_mutual_info_fast.pyx\"],"],"delete":["12","                         sources=[\"expected_mutual_info_fast.c\"],"]}],"sklearn\/utils\/setup.py":[{"add":["24","    config.add_extension('sparsefuncs_fast', sources=['sparsefuncs_fast.pyx'],","28","                         sources=['arrayfuncs.pyx'],","36","    config.add_extension('murmurhash',","37","                         sources=['murmurhash.pyx', join(","38","                             'src', 'MurmurHash3.cpp')],","39","                         include_dirs=['src'])","42","                         sources=['lgamma.pyx', join('src', 'gamma.c')],","47","                         sources=['graph_shortest_path.pyx'],","51","                         sources=['fast_dict.pyx'],","57","                         sources=['seq_dataset.pyx'],","61","                         sources=['weight_vector.pyx'],","67","                         sources=[\"_random.pyx\"],","72","                         sources=[\"_logistic_sigmoid.pyx\"],"],"delete":["24","    config.add_extension('sparsefuncs_fast', sources=['sparsefuncs_fast.c'],","28","                         sources=['arrayfuncs.c'],","36","    config.add_extension(","37","        'murmurhash',","38","        sources=['murmurhash.c', join('src', 'MurmurHash3.cpp')],","39","        include_dirs=['src'])","42","                         sources=['lgamma.c', join('src', 'gamma.c')],","47","                         sources=['graph_shortest_path.c'],","51","                         sources=['fast_dict.cpp'],","57","                         sources=['seq_dataset.c'],","61","                         sources=['weight_vector.c'],","67","                         sources=[\"_random.c\"],","72","                         sources=[\"_logistic_sigmoid.c\"],"]}],"sklearn\/svm\/setup.py":[{"add":["26","    libsvm_sources = ['libsvm.pyx']","40","    # liblinear module","45","    liblinear_sources = ['liblinear.pyx',","63","    # end liblinear module","66","    libsvm_sparse_sources = ['libsvm_sparse.pyx']"],"delete":["26","    libsvm_sources = ['libsvm.c']","40","    ### liblinear module","45","    liblinear_sources = ['liblinear.c',","63","    ## end liblinear module","66","    libsvm_sparse_sources = ['libsvm_sparse.c']"]}],"\/dev\/null":[{"add":[],"delete":[]}],"sklearn\/cluster\/setup.py":[{"add":["22","                         sources=['_dbscan_inner.pyx'],","27","                         sources=['_hierarchical.pyx'],","32","                         sources=['_k_means_elkan.pyx'],","36","    config.add_extension('_k_means',","37","                         libraries=cblas_libs,","38","                         sources=['_k_means.pyx'],","39","                         include_dirs=[join('..', 'src', 'cblas'),","40","                                       numpy.get_include(),","41","                                       blas_info.pop('include_dirs', [])],","42","                         extra_compile_args=blas_info.pop(","43","                             'extra_compile_args', []),","44","                         **blas_info","45","                         )"],"delete":["22","                         sources=['_dbscan_inner.cpp'],","27","                         sources=['_hierarchical.cpp'],","32","                         sources=['_k_means_elkan.c'],","36","    config.add_extension(","37","        '_k_means',","38","        libraries=cblas_libs,","39","        sources=['_k_means.c'],","40","        include_dirs=[join('..', 'src', 'cblas'),","41","                      numpy.get_include(),","42","                      blas_info.pop('include_dirs', [])],","43","        extra_compile_args=blas_info.pop('extra_compile_args', []),","44","        **blas_info","45","    )"]}],".travis.yml":[{"add":["29","      COVERAGE=true","32","      NUMPY_VERSION=\"1.6.2\" SCIPY_VERSION=\"0.11.0\" CYTHON_VERSION=\"0.23\"","37","      CYTHON_VERSION=\"0.23.4\""],"delete":["8","  # We use three different cache directory","9","  # to work around a Travis bug with multi-platform cache","11","  - $HOME\/sklearn_build_ubuntu","12","  - $HOME\/sklearn_build_oldest","13","  - $HOME\/sklearn_build_latest","14","  - $HOME\/sklearn_build_numpy_dev","35","      CACHED_BUILD_DIR=\"$HOME\/sklearn_build_ubuntu\" COVERAGE=true","38","      NUMPY_VERSION=\"1.6.2\" SCIPY_VERSION=\"0.11.0\" CYTHON_VERSION=\"0.21\"","39","      CACHED_BUILD_DIR=\"$HOME\/sklearn_build_oldest\"","44","      CYTHON_VERSION=\"0.23.4\" CACHED_BUILD_DIR=\"$HOME\/sklearn_build_latest\"","49","      CACHED_BUILD_DIR=\"$HOME\/dummy\"","68","            CACHED_BUILD_DIR=\"$HOME\/sklearn_build_numpy_dev\""]}],"sklearn\/utils\/sparsetools\/setup.py":[{"add":["9","                         sources=['_traversal.pyx'],","10","                         include_dirs=[numpy.get_include()])","12","                         sources=['_graph_tools.pyx'],","13","                         include_dirs=[numpy.get_include()])"],"delete":["9","                         sources=['_traversal.c'],","10","                         include_dirs=[numpy.get_include()],","11","                         #libraries=libraries","12","                         )","14","                         sources=['_graph_tools.c'],","15","                         include_dirs=[numpy.get_include()],","16","                         #libraries=libraries","17","                         )"]}],"sklearn\/decomposition\/setup.py":[{"add":["13","                         sources=[\"_online_lda.pyx\"],","18","                         sources=['cdnmf_fast.pyx'],"],"delete":["13","                         sources=[\"_online_lda.c\"],","18","                         sources=['cdnmf_fast.c'],"]}],"sklearn\/manifold\/setup.py":[{"add":["14","                         sources=[\"_utils.pyx\"],","23","                         sources=[\"_barnes_hut_tsne.pyx\"],"],"delete":["14","                         sources=[\"_utils.c\"],","23","                         sources=[\"_barnes_hut_tsne.c\"],"]}],"sklearn\/datasets\/setup.py":[{"add":["13","                         sources=['_svmlight_format.pyx'],"],"delete":["13","                         sources=['_svmlight_format.c'],"]}],"sklearn\/neighbors\/setup.py":[{"add":["13","                         sources=['ball_tree.pyx'],","18","                         sources=['kd_tree.pyx'],","23","                         sources=['dist_metrics.pyx'],","30","                         sources=['typedefs.pyx'],"],"delete":["13","                         sources=['ball_tree.c'],","18","                         sources=['kd_tree.c'],","23","                         sources=['dist_metrics.c'],","30","                         sources=['typedefs.c'],"]}],"sklearn\/__check_build\/setup.py":[{"add":["10","                         sources=['_check_build.pyx'],"],"delete":["10","                         sources=['_check_build.c'],"]}],"sklearn\/_build_utils\/__init__.py":[{"add":["8","import os","9","","10","from distutils.version import LooseVersion","14","DEFAULT_ROOT = 'sklearn'","15","CYTHON_MIN_VERSION = '0.23'","16","","40","","41","","42","def build_from_c_and_cpp_files(extensions):","43","    \"\"\"Modify the extensions to build from the .c and .cpp files.","44","","45","    This is useful for releases, this way cython is not required to","46","    run python setup.py install.","47","    \"\"\"","48","    for extension in extensions:","49","        sources = []","50","        for sfile in extension.sources:","51","            path, ext = os.path.splitext(sfile)","52","            if ext in ('.pyx', '.py'):","53","                if extension.language == 'c++':","54","                    ext = '.cpp'","55","                else:","56","                    ext = '.c'","57","                sfile = path + ext","58","            sources.append(sfile)","59","        extension.sources = sources","60","","61","","62","def maybe_cythonize_extensions(top_path, config):","63","    \"\"\"Tweaks for building extensions between release and development mode.\"\"\"","64","    is_release = os.path.exists(os.path.join(top_path, 'PKG-INFO'))","65","","66","    if is_release:","67","        build_from_c_and_cpp_files(config.ext_modules)","68","    else:","69","        message = ('Please install cython with a version >= {0} in order '","70","                   'to build a scikit-learn development version.').format(","71","                       CYTHON_MIN_VERSION)","72","        try:","73","            import Cython","74","            if LooseVersion(Cython.__version__) < CYTHON_MIN_VERSION:","75","                message += ' Your version of Cython was {0}.'.format(","76","                    Cython.__version__)","77","                raise ValueError(message)","78","            from Cython.Build import cythonize","79","        except ImportError as exc:","80","            exc.args += (message,)","81","            raise","82","","83","        config.ext_modules = cythonize(config.ext_modules)"],"delete":["8","DEFAULT_ROOT = 'sklearn'"]}],"sklearn\/metrics\/setup.py":[{"add":["17","                         sources=[\"pairwise_fast.pyx\"],"],"delete":["17","                         sources=[\"pairwise_fast.c\"],"]}]}},"1fa3bf775f4ac8a5575022607e5b9b5d752a0db0":{"changes":{"doc\/themes\/scikit-learn\/static\/nature.css_t":"MODIFY"},"diff":{"doc\/themes\/scikit-learn\/static\/nature.css_t":[{"add":["42","    \/* Don't leave a fixed height. It won't allow the header to expand when search results appear. *\/","107","    div.header {","108","      height: 60px;","109","    }"],"delete":["42","    \/* for the logo to correctly expand when showing results","43","       but remain cut when not *\/","44","       height: 60px;"]}]}},"73d3f03cfc83aa35edb89173a8450c4059000fce":{"changes":{"sklearn\/model_selection\/_split.py":"MODIFY","sklearn\/model_selection\/tests\/test_split.py":"MODIFY"},"diff":{"sklearn\/model_selection\/_split.py":[{"add":["775","        if len(unique_groups) <= 1:","776","            raise ValueError(","777","                \"The groups parameter contains fewer than 2 unique groups \"","778","                \"(%s). LeaveOneGroupOut expects at least 2.\" % unique_groups)","868","        if self.n_groups >= len(unique_groups):","869","            raise ValueError(","870","                \"The groups parameter contains fewer than (or equal to) \"","871","                \"n_groups (%d) numbers of unique groups (%s). LeavePGroupsOut \"","872","                \"expects that at least n_groups + 1 (%d) unique groups be \"","873","                \"present\" % (self.n_groups, unique_groups, self.n_groups + 1))"],"delete":[]}],"sklearn\/model_selection\/tests\/test_split.py":[{"add":["726","def test_leave_one_p_group_out_error_on_fewer_number_of_groups():","727","    X = y = groups = np.ones(0)","728","    msg = (\"The groups parameter contains fewer than 2 unique groups ([]). \"","729","           \"LeaveOneGroupOut expects at least 2.\")","730","    assert_raise_message(ValueError, msg, next,","731","                         LeaveOneGroupOut().split(X, y, groups))","732","    X = y = groups = np.ones(1)","733","    msg = (\"The groups parameter contains fewer than 2 unique groups ([ 1.]). \"","734","           \"LeaveOneGroupOut expects at least 2.\")","735","    assert_raise_message(ValueError, msg, next,","736","                         LeaveOneGroupOut().split(X, y, groups))","737","    X = y = groups = np.ones(1)","738","    msg = (\"The groups parameter contains fewer than (or equal to) n_groups \"","739","           \"(3) numbers of unique groups ([ 1.]). LeavePGroupsOut expects \"","740","           \"that at least n_groups + 1 (4) unique groups be present\")","741","    assert_raise_message(ValueError, msg, next,","742","                         LeavePGroupsOut(n_groups=3).split(X, y, groups))","743","    X = y = groups = np.arange(3)","744","    msg = (\"The groups parameter contains fewer than (or equal to) n_groups \"","745","           \"(3) numbers of unique groups ([0 1 2]). LeavePGroupsOut expects \"","746","           \"that at least n_groups + 1 (4) unique groups be present\")","747","    assert_raise_message(ValueError, msg, next,","748","                         LeavePGroupsOut(n_groups=3).split(X, y, groups))","749","","750",""],"delete":[]}]}},"dd2e48c0912e7c03a7343f81c5b4efa3dc1fd2e4":{"changes":{"sklearn\/multioutput.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/tests\/test_multioutput.py":"MODIFY"},"diff":{"sklearn\/multioutput.py":[{"add":["216","        p : array of shape = [n_samples, n_classes], or a list of n_outputs \\","217","            such arrays if n_outputs > 1.","218","            The class probabilities of the input samples. The order of the","219","            classes corresponds to that in the attribute `classes_`.","226","        results = [estimator.predict_proba(X) for estimator in","227","                   self.estimators_]"],"delete":["216","        T : (sparse) array-like, shape = (n_samples, n_classes, n_outputs)","217","            The class probabilities of the samples for each of the outputs","224","        results = np.dstack([estimator.predict_proba(X) for estimator in","225","                            self.estimators_])"]}],"doc\/whats_new.rst":[{"add":["154","   - Fix :func:`sklearn.multioutput.MultiOutputClassifier.predict_proba` to","155","     return a list of 2d arrays, rather than a 3d array. In the case where","156","     different target columns had different numbers of classes, a `ValueError`","157","     would be raised on trying to stack matrices with different dimensions.","158","     :issue:`8093` by :user:`Peter Bull <pjbull>`.","159","","175","   - The :func:`sklearn.multioutput.MultiOutputClassifier.predict_proba`","176","     function used to return a 3d array (``n_samples``, ``n_classes``,","177","     ``n_outputs``). In the case where different target columns had different","178","     numbers of classes, a `ValueError` would be raised on trying to stack","179","     matrices with different dimensions. This function now returns a list of","180","     arrays where the length of the list is ``n_outputs``, and each array is","181","     (``n_samples``, ``n_classes``) for that particular output.","182","     :issue:`8093` by :user:`Peter Bull <pjbull>`.","183",""],"delete":[]}],"sklearn\/tests\/test_multioutput.py":[{"add":["12","from sklearn.linear_model import Lasso, LogisticRegression","121","    assert len(predict_proba) == n_outputs","122","    for class_probabilities in predict_proba:","123","        assert_equal((n_samples, n_classes), class_probabilities.shape)","124","","125","    assert_array_equal(np.argmax(np.dstack(predict_proba), axis=1),","126","                       predictions)","134","                           list(predict_proba[i]))","156","def test_multiclass_multioutput_estimator_predict_proba():","157","    seed = 542","158","","159","    # make test deterministic","160","    rng = np.random.RandomState(seed)","161","","162","    # random features","163","    X = rng.normal(size=(5, 5))","164","","165","    # random labels","166","    y1 = np.array(['b', 'a', 'a', 'b', 'a']).reshape(5, 1)  # 2 classes","167","    y2 = np.array(['d', 'e', 'f', 'e', 'd']).reshape(5, 1)  # 3 classes","168","","169","    Y = np.concatenate([y1, y2], axis=1)","170","","171","    clf = MultiOutputClassifier(LogisticRegression(random_state=seed))","172","","173","    clf.fit(X, Y)","174","","175","    y_result = clf.predict_proba(X)","176","    y_actual = [np.array([[0.23481764, 0.76518236],","177","                          [0.67196072, 0.32803928],","178","                          [0.54681448, 0.45318552],","179","                          [0.34883923, 0.65116077],","180","                          [0.73687069, 0.26312931]]),","181","                np.array([[0.5171785, 0.23878628, 0.24403522],","182","                          [0.22141451, 0.64102704, 0.13755846],","183","                          [0.16751315, 0.18256843, 0.64991843],","184","                          [0.27357372, 0.55201592, 0.17441036],","185","                          [0.65745193, 0.26062899, 0.08191907]])]","186","","187","    for i in range(len(y_actual)):","188","        assert_almost_equal(y_result[i], y_actual[i])","189","","190",""],"delete":["12","from sklearn.linear_model import Lasso","120","    assert_equal((n_samples, n_classes, n_outputs), predict_proba.shape)","122","    assert_array_equal(np.argmax(predict_proba, axis=1), predictions)","130","                           list(predict_proba[:, :, i]))"]}]}},"67a85b8ed842612d59187e5fdc81a8b86eb50afd":{"changes":{"examples\/linear_model\/plot_ard.py":"MODIFY","examples\/linear_model\/plot_bayesian_ridge.py":"MODIFY","sklearn\/linear_model\/tests\/test_bayes.py":"MODIFY","sklearn\/linear_model\/bayes.py":"MODIFY"},"diff":{"examples\/linear_model\/plot_ard.py":[{"add":["17","","18","We also plot predictions and uncertainties for ARD","19","for one dimensional regression using polynomial feature expansion.","20","Note the uncertainty starts going up on the right side of the plot.","21","This is because these test samples are outside of the range of the training","22","samples.","62","# Plot the true weights, the estimated weights, the histogram of the","63","# weights, and predictions with standard deviations","89","","90","","91","# Plotting some predictions for polynomial regression","92","def f(x, noise_amount):","93","    y = np.sqrt(x) * np.sin(x)","94","    noise = np.random.normal(0, 1, len(x))","95","    return y + noise_amount * noise","96","","97","","98","degree = 10","99","X = np.linspace(0, 10, 100)","100","y = f(X, noise_amount=1)","101","clf_poly = ARDRegression(threshold_lambda=1e5)","102","clf_poly.fit(np.vander(X, degree), y)","103","","104","X_plot = np.linspace(0, 11, 25)","105","y_plot = f(X_plot, noise_amount=0)","106","y_mean, y_std = clf_poly.predict(np.vander(X_plot, degree), return_std=True)","107","plt.figure(figsize=(6, 5))","108","plt.errorbar(X_plot, y_mean, y_std, color='navy',","109","             label=\"Polynomial ARD\", linewidth=2)","110","plt.plot(X_plot, y_plot, color='gold', linewidth=2,","111","         label=\"Ground Truth\")","112","plt.ylabel(\"Output y\")","113","plt.xlabel(\"Feature X\")","114","plt.legend(loc=\"lower left\")"],"delete":["56","# Plot the true weights, the estimated weights and the histogram of the","57","# weights"]}],"examples\/linear_model\/plot_bayesian_ridge.py":[{"add":["17","","18","We also plot predictions and uncertainties for Bayesian Ridge Regression","19","for one dimensional regression using polynomial feature expansion.","20","Note the uncertainty starts going up on the right side of the plot.","21","This is because these test samples are outside of the range of the training","22","samples.","59","# Plot true weights, estimated weights, histogram of the weights, and","60","# predictions with standard deviations","86","","87","","88","# Plotting some predictions for polynomial regression","89","def f(x, noise_amount):","90","    y = np.sqrt(x) * np.sin(x)","91","    noise = np.random.normal(0, 1, len(x))","92","    return y + noise_amount * noise","93","","94","","95","degree = 10","96","X = np.linspace(0, 10, 100)","97","y = f(X, noise_amount=0.1)","98","clf_poly = BayesianRidge()","99","clf_poly.fit(np.vander(X, degree), y)","100","","101","X_plot = np.linspace(0, 11, 25)","102","y_plot = f(X_plot, noise_amount=0)","103","y_mean, y_std = clf_poly.predict(np.vander(X_plot, degree), return_std=True)","104","plt.figure(figsize=(6, 5))","105","plt.errorbar(X_plot, y_mean, y_std, color='navy',","106","             label=\"Polynomial Bayesian Ridge Regression\", linewidth=lw)","107","plt.plot(X_plot, y_plot, color='gold', linewidth=lw,","108","         label=\"Ground Truth\")","109","plt.ylabel(\"Output y\")","110","plt.xlabel(\"Feature X\")","111","plt.legend(loc=\"lower left\")"],"delete":["53","# Plot true weights, estimated weights and histogram of the weights"]}],"sklearn\/linear_model\/tests\/test_bayes.py":[{"add":["58","","59","","60","def test_return_std():","61","    # Test return_std option for both Bayesian regressors","62","    def f(X):","63","        return np.dot(X, w) + b","64","","65","    def f_noise(X, noise_mult):","66","        return f(X) + np.random.randn(X.shape[0])*noise_mult","67","","68","    d = 5","69","    n_train = 50","70","    n_test = 10","71","","72","    w = np.array([1.0, 0.0, 1.0, -1.0, 0.0])","73","    b = 1.0","74","","75","    X = np.random.random((n_train, d))","76","    X_test = np.random.random((n_test, d))","77","","78","    for decimal, noise_mult in enumerate([1, 0.1, 0.01]):","79","        y = f_noise(X, noise_mult)","80","","81","        m1 = BayesianRidge()","82","        m1.fit(X, y)","83","        y_mean1, y_std1 = m1.predict(X_test, return_std=True)","84","        assert_array_almost_equal(y_std1, noise_mult, decimal=decimal)","85","","86","        m2 = ARDRegression()","87","        m2.fit(X, y)","88","        y_mean2, y_std2 = m2.predict(X_test, return_std=True)","89","        assert_array_almost_equal(y_std2, noise_mult, decimal=decimal)"],"delete":[]}],"sklearn\/linear_model\/bayes.py":[{"add":["93","    sigma_ : array, shape = (n_features, n_features)","94","        estimated variance-covariance matrix of the weights","95","","114","","115","    References","116","    ----------","117","    D. J. C. MacKay, Bayesian Interpolation, Computation and Neural Systems,","118","    Vol. 4, No. 3, 1992.","119","","120","    R. Salakhutdinov, Lecture notes on Statistical Machine Learning,","121","    http:\/\/www.utstat.toronto.edu\/~rsalakhu\/sta4273\/notes\/Lecture2.pdf#page=15","122","    Their beta is our self.alpha_","123","    Their alpha is our self.lambda_","157","        X, y, X_offset_, y_offset_, X_scale_ = self._preprocess_data(","159","        self.X_offset_ = X_offset_","160","        self.X_scale_ = X_scale_","188","                               Vh \/ (eigen_vals_ +","189","                                     lambda_ \/ alpha_)[:, np.newaxis])","234","        sigma_ = np.dot(Vh.T,","235","                        Vh \/ (eigen_vals_ + lambda_ \/ alpha_)[:, np.newaxis])","236","        self.sigma_ = (1. \/ alpha_) * sigma_","238","        self._set_intercept(X_offset_, y_offset_, X_scale_)","241","    def predict(self, X, return_std=False):","242","        \"\"\"Predict using the linear model.","243","","244","        In addition to the mean of the predictive distribution, also its","245","        standard deviation can be returned.","246","","247","        Parameters","248","        ----------","249","        X : {array-like, sparse matrix}, shape = (n_samples, n_features)","250","            Samples.","251","","252","        return_std : boolean, optional","253","            Whether to return the standard deviation of posterior prediction.","254","","255","        Returns","256","        -------","257","        y_mean : array, shape = (n_samples,)","258","            Mean of predictive distribution of query points.","259","","260","        y_std : array, shape = (n_samples,)","261","            Standard deviation of predictive distribution of query points.","262","        \"\"\"","263","        y_mean = self._decision_function(X)","264","        if return_std is False:","265","            return y_mean","266","        else:","267","            if self.normalize:","268","                X = (X - self.X_offset_) \/ self.X_scale_","269","            sigmas_squared_data = (np.dot(X, self.sigma_) * X).sum(axis=1)","270","            y_std = np.sqrt(sigmas_squared_data + (1. \/ self.alpha_))","271","            return y_mean, y_std","272","","376","","377","    References","378","    ----------","379","    D. J. C. MacKay, Bayesian nonlinear modeling for the prediction","380","    competition, ASHRAE Transactions, 1994.","381","","382","    R. Salakhutdinov, Lecture notes on Statistical Machine Learning,","383","    http:\/\/www.utstat.toronto.edu\/~rsalakhu\/sta4273\/notes\/Lecture2.pdf#page=15","384","    Their beta is our self.alpha_","385","    Their alpha is our self.lambda_","386","    ARD is a little different than the slide: only dimensions\/features for","387","    which self.lambda_ < self.threshold_lambda are kept and the rest are","388","    discarded.","431","        X, y, X_offset_, y_offset_, X_scale_ = self._preprocess_data(","483","                            np.sum(np.log(lambda_)))","498","        self._set_intercept(X_offset_, y_offset_, X_scale_)","500","","501","    def predict(self, X, return_std=False):","502","        \"\"\"Predict using the linear model.","503","","504","        In addition to the mean of the predictive distribution, also its","505","        standard deviation can be returned.","506","","507","        Parameters","508","        ----------","509","        X : {array-like, sparse matrix}, shape = (n_samples, n_features)","510","            Samples.","511","","512","        return_std : boolean, optional","513","            Whether to return the standard deviation of posterior prediction.","514","","515","        Returns","516","        -------","517","        y_mean : array, shape = (n_samples,)","518","            Mean of predictive distribution of query points.","519","","520","        y_std : array, shape = (n_samples,)","521","            Standard deviation of predictive distribution of query points.","522","        \"\"\"","523","        y_mean = self._decision_function(X)","524","        if return_std is False:","525","            return y_mean","526","        else:","527","            if self.normalize:","528","                X = (X - self.X_offset_) \/ self.X_scale_","529","            X = X[:, self.lambda_ < self.threshold_lambda]","530","            sigmas_squared_data = (np.dot(X, self.sigma_) * X).sum(axis=1)","531","            y_std = np.sqrt(sigmas_squared_data + (1. \/ self.alpha_))","532","            return y_mean, y_std"],"delete":["144","        X, y, X_offset, y_offset, X_scale = self._preprocess_data(","173","                               Vh \/ (eigen_vals_ + lambda_ \/ alpha_)[:, None])","219","        self._set_intercept(X_offset, y_offset, X_scale)","367","        X, y, X_offset, y_offset, X_scale = self._preprocess_data(","419","                                                np.sum(np.log(lambda_)))","434","        self._set_intercept(X_offset, y_offset, X_scale)"]}]}},"4502520ed60fa98479b3ca6349d1321d505bbe4b":{"changes":{"doc\/about.rst":"MODIFY"},"diff":{"doc\/about.rst":[{"add":["77","   :target: https:\/\/www.inria.fr","86","   :target: http:\/\/www.datascience-paris-saclay.fr","95","   :target: http:\/\/cds.nyu.edu\/mooresloan\/","96","","97","`T¨¦l¨¦com Paristech <http:\/\/www.telecom-paristech.com>`_ funds Manoj Kumar (2014),","98","Tom Dupr¨¦ la Tour (2015), Raghav R V (2015-2016) and Thierry Guillemot (2016) to","99","work on scikit-learn.","100","","101",".. image:: themes\/scikit-learn\/static\/img\/telecom.png","102","   :width: 100pt","103","   :align: center","104","   :target: http:\/\/www.telecom-paristech.fr\/","191",".. |telecom| image:: themes\/scikit-learn\/static\/img\/telecom.png"],"delete":["179",".. |telecom| image:: http:\/\/f.hypotheses.org\/wp-content\/blogs.dir\/331\/files\/2011\/03\/Logo-TPT.jpg","228",""]}]}},"0ea8e8b585d2aa56ad3a6f6d2c75047a11ba709a":{"changes":{"sklearn\/metrics\/pairwise.py":"MODIFY","sklearn\/metrics\/tests\/test_pairwise.py":"MODIFY"},"diff":{"sklearn\/metrics\/pairwise.py":[{"add":["572","    np.clip(S, 0, 2, out=S)","573","    if X is Y or Y is None:","574","        # Ensure that distances between vectors and themselves are set to 0.0.","575","        # This may not be the case due to floating point rounding errors.","576","        S[np.diag_indices_from(S)] = 0.0"],"delete":[]}],"sklearn\/metrics\/tests\/test_pairwise.py":[{"add":["409","def test_cosine_distances():","410","    # Check the pairwise Cosine distances computation","411","    rng = np.random.RandomState(1337)","412","    x = np.abs(rng.rand(910))","413","    XA = np.vstack([x, x])","414","    D = cosine_distances(XA)","415","    assert_array_almost_equal(D, [[0., 0.], [0., 0.]])","416","    # check that all elements are in [0, 2]","417","    assert_true(np.all(D >= 0.))","418","    assert_true(np.all(D <= 2.))","419","    # check that diagonal elements are equal to 0","420","    assert_array_equal(D[np.diag_indices_from(D)], [0., 0.])","421","","422","    XB = np.vstack([x, -x])","423","    D2 = cosine_distances(XB)","424","    # check that all elements are in [0, 2]","425","    assert_true(np.all(D2 >= 0.))","426","    assert_true(np.all(D2 <= 2.))","427","    # check that diagonal elements are equal to 0 and non diagonal to 2","428","    assert_array_equal(D2, [[0., 2.], [2., 0.]])","429","","430","    # check large random matrix","431","    X = np.abs(rng.rand(1000, 5000))","432","    D = cosine_distances(X)","433","    # check that diagonal elements are equal to 0","434","    assert_array_almost_equal(D[np.diag_indices_from(D)], [0.] * D.shape[0])","435","    assert_true(np.all(D >= 0.))","436","    assert_true(np.all(D <= 2.))","437","","438",""],"delete":[]}]}},"6a42ea23760226ca657b495e8e639a02a234d348":{"changes":{"sklearn\/metrics\/regression.py":"MODIFY","sklearn\/metrics\/tests\/test_regression.py":"MODIFY"},"diff":{"sklearn\/metrics\/regression.py":[{"add":["89","    allowed_multioutput_str = ('raw_values', 'uniform_average',","90","                               'variance_weighted')","91","    if isinstance(multioutput, string_types):","92","        if multioutput not in allowed_multioutput_str:","93","            raise ValueError(\"Allowed 'multioutput' string values are {}. \"","94","                             \"You provided multioutput={!r}\".format(","95","                                 allowed_multioutput_str,","96","                                 multioutput))","97","    elif multioutput is not None:","512","    >>> r2_score(y_true, y_pred, multioutput='variance_weighted')","513","    ... # doctest: +ELLIPSIS"],"delete":["89","    multioutput_options = (None, 'raw_values', 'uniform_average',","90","                           'variance_weighted')","91","    if multioutput not in multioutput_options:","506","    >>> r2_score(y_true, y_pred, multioutput='variance_weighted')  # doctest: +ELLIPSIS"]}],"sklearn\/metrics\/tests\/test_regression.py":[{"add":["95","def test__check_reg_targets_exception():","96","    invalid_multioutput = 'this_value_is_not_valid'","97","    expected_message = (\"Allowed 'multioutput' string values are.+\"","98","                        \"You provided multioutput={!r}\".format(","99","                            invalid_multioutput))","100","    assert_raises_regex(ValueError, expected_message,","101","                        _check_reg_targets,","102","                        [1, 2, 3],","103","                        [[1], [2], [3]],","104","                        invalid_multioutput)","105","","106",""],"delete":[]}]}},"7b0b6d73441c2a29e455992c3f1e306a4feab07e":{"changes":{"examples\/model_selection\/plot_confusion_matrix.py":"MODIFY"},"diff":{"examples\/model_selection\/plot_confusion_matrix.py":[{"add":["67","    plt.imshow(cm, interpolation='nearest', cmap=cmap)","68","    plt.title(title)","69","    plt.colorbar()","70","    tick_marks = np.arange(len(classes))","71","    plt.xticks(tick_marks, classes, rotation=45)","72","    plt.yticks(tick_marks, classes)","73","","74","    fmt = '.2f' if normalize else 'd'","77","        plt.text(j, i, format(cm[i, j], fmt),"],"delete":["59","    plt.imshow(cm, interpolation='nearest', cmap=cmap)","60","    plt.title(title)","61","    plt.colorbar()","62","    tick_marks = np.arange(len(classes))","63","    plt.xticks(tick_marks, classes, rotation=45)","64","    plt.yticks(tick_marks, classes)","65","","76","        plt.text(j, i, cm[i, j],"]}]}},"3e7a7ca7fa6620afc68f2454c373c069be57dbf3":{"changes":{"sklearn\/metrics\/cluster\/tests\/test_unsupervised.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/metrics\/cluster\/unsupervised.py":"MODIFY"},"diff":{"sklearn\/metrics\/cluster\/tests\/test_unsupervised.py":[{"add":["7","from sklearn.utils.testing import assert_array_equal","13","from sklearn.metrics.cluster import silhouette_samples","15","from sklearn.metrics.cluster import calinski_harabaz_score","60","def test_cluster_size_1():","61","    # Assert Silhouette Coefficient == 0 when there is 1 sample in a cluster","62","    # (cluster 0). We also test the case where there are identical samples","63","    # as the only members of a cluster (cluster 2). To our knowledge, this case","64","    # is not discussed in reference material, and we choose for it a sample","65","    # score of 1.","66","    X = [[0.], [1.], [1.], [2.], [3.], [3.]]","67","    labels = np.array([0, 1, 1, 1, 2, 2])","68","","69","    # Cluster 0: 1 sample -> score of 0 by Rousseeuw's convention","70","    # Cluster 1: intra-cluster = [.5, .5, 1]","71","    #            inter-cluster = [1, 1, 1]","72","    #            silhouette    = [.5, .5, 0]","73","    # Cluster 2: intra-cluster = [0, 0]","74","    #            inter-cluster = [arbitrary, arbitrary]","75","    #            silhouette    = [1., 1.]","76","","77","    silhouette = silhouette_score(X, labels)","79","    ss = silhouette_samples(X, labels)","80","    assert_array_equal(ss, [0, .5, .5, 0, 1, 1])","108","        silhouette_score(X, labels * 2 + 10), silhouette_score(X, labels))","109","    assert_array_equal(","110","        silhouette_samples(X, labels * 2 + 10), silhouette_samples(X, labels))"],"delete":["12","from sklearn.metrics.cluster import calinski_harabaz_score","58","def test_no_nan():","59","    # Assert Silhouette Coefficient != nan when there is 1 sample in a class.","60","    # This tests for the condition that caused issue 960.","61","    # Note that there is only one sample in cluster 0. This used to cause the","62","    # silhouette_score to return nan (see bug #960).","63","    labels = np.array([1, 0, 1, 1, 1])","64","    # The distance matrix doesn't actually matter.","65","    D = np.random.RandomState(0).rand(len(labels), len(labels))","66","    silhouette = silhouette_score(D, labels, metric='precomputed')","95","        silhouette_score(X, labels + 10), silhouette_score(X, labels))"]}],"doc\/whats_new.rst":[{"add":["473","    - Fix bug in :func:`metrics.silhouette_score` in which clusters of","474","      size 1 were incorrectly scored. They should get a score of 0.","475","      By `Joel Nothman`_.","476","","477","    - Fix bug in :func:`metrics.silhouette_samples` so that it now works with","478","      arbitrary labels, not just those ranging from 0 to n_clusters - 1.","479","      By `Joel Nothman`_.","480",""],"delete":[]}],"sklearn\/metrics\/cluster\/unsupervised.py":[{"add":["11","from ...utils.fixes import bincount","92","        X, labels = check_X_y(X, labels, accept_sparse=['csc', 'csr'])","162","    X, labels = check_X_y(X, labels, accept_sparse=['csc', 'csr'])","165","    check_number_of_labels(len(le.classes_), X.shape[0])","169","    n_samples_per_label = bincount(labels, minlength=len(unique_labels))","173","    intra_clust_dists = np.zeros(distances.shape[0], dtype=distances.dtype)","177","    inter_clust_dists = np.inf + intra_clust_dists","179","    for curr_label in range(len(unique_labels)):","187","        n_samples_curr_lab = n_samples_per_label[curr_label] - 1","194","        for other_label in range(len(unique_labels)):","204","    # score 0 for clusters of size 1, according to the paper","205","    sil_samples[n_samples_per_label.take(labels) == 1] = 0"],"delete":["90","    X, labels = check_X_y(X, labels, accept_sparse=['csc', 'csr'])","91","    le = LabelEncoder()","92","    labels = le.fit_transform(labels)","93","    n_labels = len(le.classes_)","94","    n_samples = X.shape[0]","95","","96","    check_number_of_labels(n_labels, n_samples)","97","","176","    intra_clust_dists = np.ones(distances.shape[0], dtype=distances.dtype)","180","    inter_clust_dists = np.inf * intra_clust_dists","182","    for curr_label in unique_labels:","190","        n_samples_curr_lab = np.sum(mask) - 1","197","        for other_label in unique_labels:"]}]}},"31d3c3e7c511145b65ce0d1ad6643646acadaa0a":{"changes":{"doc\/modules\/preprocessing.rst":"MODIFY"},"diff":{"doc\/modules\/preprocessing.rst":[{"add":["148","    X_scaled = X_std * (max - min) + min"],"delete":["148","    X_scaled = X_std \/ (max - min) + min"]}]}},"41e1b8f17cc9f8176dbce1baada62772126b9466":{"changes":{"sklearn\/calibration.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/tests\/test_calibration.py":"MODIFY"},"diff":{"sklearn\/calibration.py":[{"add":["16","from sklearn.preprocessing import LabelEncoder","19","from .preprocessing import label_binarize, LabelBinarizer","53","        with too few calibration samples ``(<<1000)`` since it tends to","54","        overfit.","67","        :class:`sklearn.model_selection.StratifiedKFold` is used. If ``y`` is","68","        neither binary nor multiclass, :class:`sklearn.model_selection.KFold`","128","        le = LabelBinarizer().fit(y)","129","        self.classes_ = le.classes_","136","                np.any([np.sum(y == class_) < n_folds for class_ in","137","                        self.classes_]):","180","                    this_estimator, method=self.method,","181","                    classes=self.classes_)","259","    classes : array-like, shape (n_classes,), optional","260","            Contains unique classes used to fit the base estimator.","261","            if None, then classes is extracted from the given target values","262","            in fit().","263","","278","    def __init__(self, base_estimator, method='sigmoid', classes=None):","281","        self.classes = classes","297","        idx_pos_class = self.label_encoder_.\\","298","            transform(self.base_estimator.classes_)","321","","322","        self.label_encoder_ = LabelEncoder()","323","        if self.classes is None:","324","            self.label_encoder_.fit(y)","325","        else:","326","            self.label_encoder_.fit(self.classes)","327","","328","        self.classes_ = self.label_encoder_.classes_","329","        Y = label_binarize(y, self.classes_)"],"delete":["18","from .preprocessing import LabelBinarizer","52","        with too few calibration samples ``(<<1000)`` since it tends to overfit.","65","        :class:`sklearn.model_selection.StratifiedKFold` is used. If ``y`` ","66","        is neither binary nor multiclass, :class:`sklearn.model_selection.KFold` ","126","        lb = LabelBinarizer().fit(y)","127","        self.classes_ = lb.classes_","134","           np.any([np.sum(y == class_) < n_folds for class_ in self.classes_]):","177","                    this_estimator, method=self.method)","269","    def __init__(self, base_estimator, method='sigmoid'):","287","        idx_pos_class = np.arange(df.shape[1])","310","        lb = LabelBinarizer()","311","        Y = lb.fit_transform(y)","312","        self.classes_ = lb.classes_"]}],"doc\/whats_new.rst":[{"add":["146","   - Fixes issue in :class:`calibration.CalibratedClassifierCV` where","147","     the sum of probabilities of each class for a data was not 1, and","148","     ``CalibratedClassifierCV`` now handles the case where the training set","149","     has less number of classes than the total data. :issue:`7799` by","150","     `Srivatsan Ramesh`_","151",""],"delete":[]}],"sklearn\/tests\/test_calibration.py":[{"add":["3","from __future__ import division","6","from sklearn.model_selection import LeaveOneOut","156","","273","","274","","275","def test_calibration_prob_sum():","276","    # Test that sum of probabilities is 1. A non-regression test for","277","    # issue #7796","278","    num_classes = 2","279","    X, y = make_classification(n_samples=10, n_features=5,","280","                               n_classes=num_classes)","281","    clf = LinearSVC(C=1.0)","282","    clf_prob = CalibratedClassifierCV(clf, method=\"sigmoid\", cv=LeaveOneOut())","283","    clf_prob.fit(X, y)","284","","285","    probs = clf_prob.predict_proba(X)","286","    assert_array_almost_equal(probs.sum(axis=1), np.ones(probs.shape[0]))","287","","288","","289","def test_calibration_less_classes():","290","    # Test to check calibration works fine when train set in a test-train","291","    # split does not contain all classes","292","    # Since this test uses LOO, at each iteration train set will not contain a","293","    # class label","294","    X = np.random.randn(10, 5)","295","    y = np.arange(10)","296","    clf = LinearSVC(C=1.0)","297","    cal_clf = CalibratedClassifierCV(clf, method=\"sigmoid\", cv=LeaveOneOut())","298","    cal_clf.fit(X, y)","299","","300","    for i, calibrated_classifier in \\","301","            enumerate(cal_clf.calibrated_classifiers_):","302","        proba = calibrated_classifier.predict_proba(X)","303","        assert_array_equal(proba[:, i], np.zeros(len(y)))","304","        assert_equal(np.all(np.hstack([proba[:, :i],","305","                                       proba[:, i + 1:]])), True)"],"delete":["16","from sklearn.linear_model import Ridge","89","        # check that calibration can also deal with regressors that have","90","        # a decision_function","91","        clf_base_regressor = CalibratedClassifierCV(Ridge())","92","        clf_base_regressor.fit(X_train, y_train)","93","        clf_base_regressor.predict(X_test)","94",""]}]}},"0a1f6cd98eebe7c5233692e2d9680232c23bf9a8":{"changes":{"sklearn\/tests\/test_pipeline.py":"MODIFY","sklearn\/pipeline.py":"MODIFY"},"diff":{"sklearn\/tests\/test_pipeline.py":[{"add":["280","    # As pipeline doesn't clone estimators on construction,","281","    # it must have its own estimators","282","    scaler_for_pipeline = StandardScaler()","283","    km_for_pipeline = KMeans(random_state=0)","290","    pipe = Pipeline([","291","        ('scaler', scaler_for_pipeline),","292","        ('Kmeans', km_for_pipeline)","293","    ])"],"delete":["286","    pipe = Pipeline([('scaler', scaler), ('Kmeans', km)])"]}],"sklearn\/pipeline.py":[{"add":["358","                Xt = transform.fit_transform(Xt)"],"delete":["358","                Xt = transform.transform(Xt)"]}]}},"925a017c83b0eea147cc8288c30fefdb199b5730":{"changes":{"sklearn\/feature_extraction\/tests\/test_feature_hasher.py":"MODIFY","sklearn\/feature_extraction\/hashing.py":"MODIFY","sklearn\/feature_extraction\/text.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/feature_extraction\/_hashing.pyx":"MODIFY"},"diff":{"sklearn\/feature_extraction\/tests\/test_feature_hasher.py":[{"add":["6","from sklearn.utils.testing import (assert_raises, assert_true, assert_equal,","7","                                   ignore_warnings)","109","","110","","111","@ignore_warnings(category=DeprecationWarning)","112","def test_hasher_alternate_sign():","113","    # the last two tokens produce a hash collision that sums as 0","114","    X = [[\"foo\", \"bar\", \"baz\", \"investigation need\", \"records\"]]","115","","116","    Xt = FeatureHasher(alternate_sign=True, non_negative=False,","117","                       input_type='string').fit_transform(X)","118","    assert_true(Xt.data.min() < 0 and Xt.data.max() > 0)","119","    # check that we have a collision that produces a 0 count","120","    assert_true(len(Xt.data) < len(X[0]))","121","    assert_true((Xt.data == 0.).any())","122","","123","    Xt = FeatureHasher(alternate_sign=True, non_negative=True,","124","                       input_type='string').fit_transform(X)","125","    assert_true((Xt.data >= 0).all())   # all counts are positive","126","    assert_true((Xt.data == 0.).any())  # we still have a collision","127","    Xt = FeatureHasher(alternate_sign=False, non_negative=True,","128","                       input_type='string').fit_transform(X)","129","    assert_true((Xt.data > 0).all())    # strictly positive counts","130","    Xt_2 = FeatureHasher(alternate_sign=False, non_negative=False,","131","                         input_type='string').fit_transform(X)","132","    # With initially positive features, the non_negative option should","133","    # have no impact when alternate_sign=False","134","    assert_array_equal(Xt.data, Xt_2.data)","135","","136","","137","@ignore_warnings(category=DeprecationWarning)","138","def test_hasher_negative():","139","    X = [{\"foo\": 2, \"bar\": -4, \"baz\": -1}.items()]","140","    Xt = FeatureHasher(alternate_sign=False, non_negative=False,","141","                       input_type=\"pair\").fit_transform(X)","142","    assert_true(Xt.data.min() < 0 and Xt.data.max() > 0)","143","    Xt = FeatureHasher(alternate_sign=False, non_negative=True,","144","                       input_type=\"pair\").fit_transform(X)","145","    assert_true(Xt.data.min() > 0)","146","    Xt = FeatureHasher(alternate_sign=True, non_negative=False,","147","                       input_type=\"pair\").fit_transform(X)","148","    assert_true(Xt.data.min() < 0 and Xt.data.max() > 0)","149","    Xt = FeatureHasher(alternate_sign=True, non_negative=True,","150","                       input_type=\"pair\").fit_transform(X)","151","    assert_true(Xt.data.min() > 0)"],"delete":["6","from sklearn.utils.testing import assert_raises, assert_true, assert_equal"]}],"sklearn\/feature_extraction\/hashing.py":[{"add":["4","import warnings","56","    alternate_sign : boolean, optional, default True","57","        When True, an alternating sign is added to the features as to","58","        approximately conserve the inner product in the hashed space even for","59","        small n_features. This approach is similar to sparse random projection.","61","        When True, an absolute value is applied to the features matrix prior to","62","        returning it. When used in conjunction with alternate_sign=True, this","63","        significantly reduces the inner product preservation property.","64","        .. deprecated:: 0.19","65","            This option will be removed in 0.21.","66","","86","                 dtype=np.float64, alternate_sign=True, non_negative=False):","88","        if non_negative:","89","            warnings.warn(\"the option non_negative=True has been deprecated\"","90","                          \" in 0.19 and will be removed\"","91","                          \" in version 0.21.\", DeprecationWarning)","96","        self.alternate_sign = alternate_sign","153","            _hashing.transform(raw_X, self.n_features, self.dtype,","154","                               self.alternate_sign)","163",""],"delete":["56","        Whether output matrices should contain non-negative values only;","57","        effectively calls abs on the matrix prior to returning it.","58","        When True, output values can be interpreted as frequencies.","59","        When False, output values will have expected value zero.","79","                 dtype=np.float64, non_negative=False):","141","            _hashing.transform(raw_X, self.n_features, self.dtype)"]}],"sklearn\/feature_extraction\/text.py":[{"add":["406","    alternate_sign : boolean, optional, default True","407","        When True, an alternating sign is added to the features as to","408","        approximately conserve the inner product in the hashed space even for","409","        small n_features. This approach is similar to sparse random projection.","410","","411","        .. versionadded:: 0.19","412","","413","    non_negative : boolean, optional, default False","414","        When True, an absolute value is applied to the features matrix prior to","415","        returning it. When used in conjunction with alternate_sign=True, this","416","        significantly reduces the inner product preservation property.","417","","418","        .. deprecated:: 0.19","419","            This option will be removed in 0.21.","431","                 binary=False, norm='l2', alternate_sign=True,","432","                 non_negative=False, dtype=np.float64):","447","        self.alternate_sign = alternate_sign","508","                             alternate_sign=self.alternate_sign,"],"delete":["406","    non_negative : boolean, default=False","407","        Whether output matrices should contain non-negative values only;","408","        effectively calls abs on the matrix prior to returning it.","409","        When True, output values can be interpreted as frequencies.","410","        When False, output values will have expected value zero.","422","                 binary=False, norm='l2', non_negative=False,","423","                 dtype=np.float64):"]}],"doc\/whats_new.rst":[{"add":["296","","297","   - Fix a bug where :class:`sklearn.feature_extraction.FeatureHasher`","298","     mandatorily applied a sparse random projection to the hashed features,","299","     preventing the use of ","300","     :class:`sklearn.feature_extraction.text.HashingVectorizer` in a","301","     pipeline with  :class:`sklearn.feature_extraction.text.TfidfTransformer`.","302","     :issue:`7513` by :user:`Roman Yurchak <rth>`.","303","     ","307","     "],"delete":[]}],"sklearn\/feature_extraction\/_hashing.pyx":[{"add":["17","def transform(raw_X, Py_ssize_t n_features, dtype, bint alternate_sign=1):","65","            # improve inner product preservation in the hashed space","66","            if alternate_sign:","67","                value *= (h >= 0) * 2 - 1"],"delete":["17","def transform(raw_X, Py_ssize_t n_features, dtype):","65","            value *= (h >= 0) * 2 - 1"]}]}},"919b4a8fbdf0e313fb702bd083abb31d67f7d8f9":{"changes":{"sklearn\/ensemble\/gradient_boosting.py":"MODIFY","doc\/whats_new.rst":"MODIFY"},"diff":{"sklearn\/ensemble\/gradient_boosting.py":[{"add":["512","        # prevents overflow and division by zero","513","        if abs(denominator) < 1e-150:","579","        # prevents overflow and division by zero","580","        if abs(denominator) < 1e-150:","637","        # prevents overflow and division by zero","638","        if abs(denominator) < 1e-150:"],"delete":["512","        if denominator == 0.0:","578","        if denominator == 0.0:","635","        if denominator == 0.0:"]}],"doc\/whats_new.rst":[{"add":["187","   - Fixed a bug in :class:`sklearn.ensemble.GradientBoostingClassifier`","188","     and :class:`sklearn.ensemble.GradientBoostingRegressor`","189","     where a float being compared to ``0.0`` using ``==`` caused a divide by zero","190","     error. This was fixed in :issue:`7970` by :user:`He Chen <chenhe95>`."],"delete":[]}]}},"5adc83227b0d5c82c85f2f16c2c02efaaacf25c0":{"changes":{"sklearn\/metrics\/pairwise.py":"MODIFY"},"diff":{"sklearn\/metrics\/pairwise.py":[{"add":["26","from ..externals.joblib import cpu_count"],"delete":["26","from ..externals.joblib.parallel import cpu_count"]}]}},"3c1873550317bbbc63e982cccbd0afaae9cc5a66":{"changes":{"examples\/linear_model\/plot_logistic.py":"MODIFY"},"diff":{"examples\/linear_model\/plot_logistic.py":[{"add":["6","Logistic function","9","Shown in the plot is how the logistic regression would, in this","11","i.e. class one or two, using the logistic curve.","50","plt.plot(X_test, loss, color='red', linewidth=3)","59","plt.xticks(range(-5, 10))","60","plt.yticks([0, 0.5, 1])","63","plt.legend(('Logistic Regression Model', 'Linear Regression Model'),","64","           loc=\"lower right\", fontsize='small')"],"delete":["6","Logit function","9","Show in the plot is how the logistic regression would, in this","11","i.e. class one or two, using the logit-curve.","50","plt.plot(X_test, loss, color='blue', linewidth=3)","59","plt.xticks(())","60","plt.yticks(())","63",""]}]}},"3fbfb1bf5e24c826209fa5a5c080140a7abdac63":{"changes":{"sklearn\/model_selection\/_search.py":"MODIFY"},"diff":{"sklearn\/model_selection\/_search.py":[{"add":["1077","            'param_kernel' : masked_array(data = ['rbf', 'rbf', 'rbf'],"],"delete":["1077","            'param_kernel' : masked_array(data = ['rbf', rbf', 'rbf'],"]}]}},"20912ed389414dcd69c538a3ed6921a9af0a5b0d":{"changes":{"build_tools\/travis\/flake8_diff.sh":"MODIFY"},"diff":{"build_tools\/travis\/flake8_diff.sh":[{"add":["34","echo \"Remotes:\"","35","echo '--------------------------------------------------------------------------------'","36","git remote --verbose","37","","38","# Travis does the git clone with a limited depth (50 at the time of","39","# writing). This may not be enough to find the common ancestor with","40","# $REMOTE\/master so we unshallow the git checkout","41","if [[ -a .git\/shallow ]]; then","42","    echo -e '\\nTrying to unshallow the repo:'","43","    echo '--------------------------------------------------------------------------------'","44","    git fetch --unshallow","45","fi","46","","63","        LOCAL_BRANCH_REF=travis_pr_$TRAVIS_PULL_REQUEST","64","        # In Travis the PR target is always origin","65","        git fetch origin pull\/$TRAVIS_PULL_REQUEST\/head:refs\/$LOCAL_BRANCH_REF","70","# ancestor between $LOCAL_BRANCH_REF and $REMOTE\/master","72","    if [[ -z \"$LOCAL_BRANCH_REF\" ]]; then","73","        LOCAL_BRANCH_REF=$(git rev-parse --abbrev-ref HEAD)","74","    fi","75","    echo -e \"\\nLast 2 commits in $LOCAL_BRANCH_REF:\"","76","    echo '--------------------------------------------------------------------------------'","77","    git log -2 $LOCAL_BRANCH_REF","78","","81","    echo -e \"\\nFetching $REMOTE_MASTER_REF\"","82","    echo '--------------------------------------------------------------------------------'","84","    LOCAL_BRANCH_SHORT_HASH=$(git rev-parse --short $LOCAL_BRANCH_REF)","85","    REMOTE_MASTER_SHORT_HASH=$(git rev-parse --short $REMOTE_MASTER_REF)","87","    COMMIT=$(git merge-base $LOCAL_BRANCH_REF $REMOTE_MASTER_REF) || \\","88","        echo \"No common ancestor found for $(git show $LOCAL_BRANCH_REF -q) and $(git show $REMOTE_MASTER_REF -q)\"","94","    COMMIT_SHORT_HASH=$(git rev-parse --short $COMMIT)","96","    echo -e \"\\nCommon ancestor between $LOCAL_BRANCH_REF ($LOCAL_BRANCH_SHORT_HASH)\"\\","97","         \"and $REMOTE_MASTER_REF ($REMOTE_MASTER_SHORT_HASH) is $COMMIT_SHORT_HASH:\"","98","    echo '--------------------------------------------------------------------------------'","99","    git show --no-patch $COMMIT_SHORT_HASH","100","","101","    COMMIT_RANGE=\"$COMMIT_SHORT_HASH..$LOCAL_BRANCH_SHORT_HASH\"","102","","103","    if [[ -n \"$TMP_REMOTE\" ]]; then","104","        git remote remove $TMP_REMOTE","105","    fi"],"delete":["22","echo \"Remotes:\"","23","git remote --verbose","24","","48","        else","49","            # Travis does the git clone with a limited depth (50 at the time of","50","            # writing). This may not be enough to find the common ancestor with","51","            # $REMOTE\/master so we unshallow the git checkout","52","            git fetch --unshallow || echo \"Unshallowing the git checkout failed\"","58","        BRANCH_NAME=travis_pr_$TRAVIS_PULL_REQUEST","59","        git fetch $REMOTE pull\/$TRAVIS_PULL_REQUEST\/head:$BRANCH_NAME","60","        git checkout $BRANCH_NAME","64","","65","echo -e '\\nLast 2 commits:'","66","echo '--------------------------------------------------------------------------------'","67","git log -2 --pretty=short","68","","70","# ancestor between HEAD and $REMOTE\/master","76","    COMMIT=$(git merge-base @ $REMOTE_MASTER_REF) || \\","77","        echo \"No common ancestor found for $(git show @ -q) and $(git show $REMOTE_MASTER_REF -q)\"","78","","79","    if [[ -n \"$TMP_REMOTE\" ]]; then","80","        git remote remove $TMP_REMOTE","81","    fi","87","    echo -e \"\\nCommon ancestor between HEAD and $REMOTE_MASTER_REF is:\"","88","    echo '--------------------------------------------------------------------------------'","89","    git show --no-patch $COMMIT","91","    COMMIT_RANGE=\"$(git rev-parse --short $COMMIT)..$(git rev-parse --short @)\""]}]}},"53e6381272f0a71edcefb9010fa2e8856116076a":{"changes":{"sklearn\/ensemble\/tests\/test_base.py":"MODIFY","sklearn\/ensemble\/base.py":"MODIFY"},"diff":{"sklearn\/ensemble\/tests\/test_base.py":[{"add":["48","    np_int_ensemble = BaggingClassifier(base_estimator=Perceptron(),","49","                                        n_estimators=np.int32(3))","50","    np_int_ensemble.fit(iris.data, iris.target)","51","","56","    ensemble = BaggingClassifier(base_estimator=Perceptron(),","57","                                 n_estimators=0)","64","def test_base_not_int_n_estimators():","65","    # Check that instantiating a BaseEnsemble with a string as n_estimators","66","    # raises a ValueError demanding n_estimators to be supplied as an integer.","67","    string_ensemble = BaggingClassifier(base_estimator=Perceptron(),","68","                                        n_estimators='3')","69","    iris = load_iris()","70","    assert_raise_message(ValueError,","71","                         \"n_estimators must be an integer\",","72","                         string_ensemble.fit, iris.data, iris.target)","73","    float_ensemble = BaggingClassifier(base_estimator=Perceptron(),","74","                                       n_estimators=3.0)","75","    assert_raise_message(ValueError,","76","                         \"n_estimators must be an integer\",","77","                         float_ensemble.fit, iris.data, iris.target)","78","","79",""],"delete":["52","    ensemble = BaggingClassifier(base_estimator=Perceptron(), n_estimators=0)"]}],"sklearn\/ensemble\/base.py":[{"add":["8","import numbers","96","        if not isinstance(self.n_estimators, (numbers.Integral, np.integer)):","97","            raise ValueError(\"n_estimators must be an integer, \"","98","                             \"got {0}.\".format(type(self.n_estimators)))","99",""],"delete":[]}]}},"061803c4ff46fd5a34bb5220b572b8df4dcb2638":{"changes":{"doc\/whats_new.rst":"MODIFY","sklearn\/cluster\/k_means_.py":"MODIFY"},"diff":{"doc\/whats_new.rst":[{"add":["24","   - :class:`cluster.MiniBatchKMeans` and :class:`cluster.KMeans`","25","     now uses significantly less memory when assigning data points to their","26","     nearest cluster center.","27","     (`#7721 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/7721>`_)","28","     By `Jon Crall`_.","29",""],"delete":[]}],"sklearn\/cluster\/k_means_.py":[{"add":["20","from ..metrics.pairwise import pairwise_distances_argmin_min","555","","556","    # Breakup nearest neighbor distance computation into batches to prevent","557","    # memory blowup in the case of a large number of samples and clusters.","558","    # TODO: Once PR #7383 is merged use check_inputs=False in metric_kwargs.","559","    labels, mindist = pairwise_distances_argmin_min(","560","        X=X, Y=centers, metric='euclidean', metric_kwargs={'squared': True})","561","    # cython k-means code assumes int32 inputs","562","    labels = labels.astype(np.int32)"],"delete":["554","    k = centers.shape[0]","555","    all_distances = euclidean_distances(centers, X, x_squared_norms,","556","                                        squared=True)","557","    labels = np.empty(n_samples, dtype=np.int32)","558","    labels.fill(-1)","559","    mindist = np.empty(n_samples)","560","    mindist.fill(np.infty)","561","    for center_id in range(k):","562","        dist = all_distances[center_id]","563","        labels[dist < mindist] = center_id","564","        mindist = np.minimum(dist, mindist)"]}]}}}