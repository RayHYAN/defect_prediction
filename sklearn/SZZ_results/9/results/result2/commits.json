{"46913adf0757d1a6cae3fff0210a973e9d995bac":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/utils\/estimator_checks.py":"MODIFY","sklearn\/utils\/_unittest_backport.py":"MODIFY","sklearn\/utils\/tests\/test_estimator_checks.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["862","- Allow :func:`utils.estimator_checks.check_estimator` to check that there is no","866","- The set of checks in :func:`utils.estimator_checks.check_estimator` now includes a","867","  ``check_set_params`` test which checks that ``set_params`` is equivalent to","868","  passing parameters in ``__init__`` and warns if it encounters parameter","869","  validation. :issue:`7738` by :user:`Alvin Chiang <absolutelyNoWarranty>`","870","  ","871","- Add invariance tests for clustering metrics. :issue:`8102` by :user:`Ankita","872","  Sinha <anki08>` and :user:`Guillaume Lemaitre <glemaitre>`.","873",""],"delete":["862","- Allow :func:`~utils.estimator_checks.check_estimator` to check that there is no"]}],"sklearn\/utils\/estimator_checks.py":[{"add":["263","    yield check_set_params","2184","def check_set_params(name, estimator_orig):","2185","    # Check that get_params() returns the same thing","2186","    # before and after set_params() with some fuzz","2187","    estimator = clone(estimator_orig)","2188","","2189","    orig_params = estimator.get_params(deep=False)","2190","    msg = (\"get_params result does not match what was passed to set_params\")","2191","","2192","    estimator.set_params(**orig_params)","2193","    curr_params = estimator.get_params(deep=False)","2194","    assert_equal(set(orig_params.keys()), set(curr_params.keys()), msg)","2195","    for k, v in curr_params.items():","2196","        assert orig_params[k] is v, msg","2197","","2198","    # some fuzz values","2199","    test_values = [-np.inf, np.inf, None]","2200","","2201","    test_params = deepcopy(orig_params)","2202","    for param_name in orig_params.keys():","2203","        default_value = orig_params[param_name]","2204","        for value in test_values:","2205","            test_params[param_name] = value","2206","            try:","2207","                estimator.set_params(**test_params)","2208","            except (TypeError, ValueError) as e:","2209","                e_type = e.__class__.__name__","2210","                # Exception occurred, possibly parameter validation","2211","                warnings.warn(\"{} occurred during set_params. \"","2212","                              \"It is recommended to delay parameter \"","2213","                              \"validation until fit.\".format(e_type))","2214","","2215","                change_warning_msg = \"Estimator's parameters changed after \" \\","2216","                                     \"set_params raised {}\".format(e_type)","2217","                params_before_exception = curr_params","2218","                curr_params = estimator.get_params(deep=False)","2219","                try:","2220","                    assert_equal(set(params_before_exception.keys()),","2221","                                 set(curr_params.keys()))","2222","                    for k, v in curr_params.items():","2223","                        assert params_before_exception[k] is v","2224","                except AssertionError:","2225","                    warnings.warn(change_warning_msg)","2226","            else:","2227","                curr_params = estimator.get_params(deep=False)","2228","                assert_equal(set(test_params.keys()),","2229","                             set(curr_params.keys()),","2230","                             msg)","2231","                for k, v in curr_params.items():","2232","                    assert test_params[k] is v, msg","2233","        test_params[param_name] = default_value","2234","","2235","","2236","@ignore_warnings(category=(DeprecationWarning, FutureWarning))"],"delete":[]}],"sklearn\/utils\/_unittest_backport.py":[{"add":["151","    longMessage = True"],"delete":["151","    longMessage = False"]}],"sklearn\/utils\/tests\/test_estimator_checks.py":[{"add":["12","                                   assert_equal, ignore_warnings,","13","                                   assert_warns)","89","class RaisesErrorInSetParams(BaseEstimator):","90","    def __init__(self, p=0):","91","        self.p = p","92","","93","    def set_params(self, **kwargs):","94","        if 'p' in kwargs:","95","            p = kwargs.pop('p')","96","            if p < 0:","97","                raise ValueError(\"p can't be less than 0\")","98","            self.p = p","99","        return super(RaisesErrorInSetParams, self).set_params(**kwargs)","100","","101","    def fit(self, X, y=None):","102","        X, y = check_X_y(X, y)","103","        return self","104","","105","","106","class ModifiesValueInsteadOfRaisingError(BaseEstimator):","107","    def __init__(self, p=0):","108","        self.p = p","109","","110","    def set_params(self, **kwargs):","111","        if 'p' in kwargs:","112","            p = kwargs.pop('p')","113","            if p < 0:","114","                p = 0","115","            self.p = p","116","        return super(ModifiesValueInsteadOfRaisingError,","117","                     self).set_params(**kwargs)","118","","119","    def fit(self, X, y=None):","120","        X, y = check_X_y(X, y)","121","        return self","122","","123","","124","class ModifiesAnotherValue(BaseEstimator):","125","    def __init__(self, a=0, b='method1'):","126","        self.a = a","127","        self.b = b","128","","129","    def set_params(self, **kwargs):","130","        if 'a' in kwargs:","131","            a = kwargs.pop('a')","132","            self.a = a","133","            if a is None:","134","                kwargs.pop('b')","135","                self.b = 'method2'","136","        return super(ModifiesAnotherValue,","137","                     self).set_params(**kwargs)","138","","139","    def fit(self, X, y=None):","140","        X, y = check_X_y(X, y)","141","        return self","142","","143","","277","    # check that values returned by get_params match set_params","278","    msg = \"get_params result does not match what was passed to set_params\"","279","    assert_raises_regex(AssertionError, msg, check_estimator,","280","                        ModifiesValueInsteadOfRaisingError())","281","    assert_warns(UserWarning, check_estimator, RaisesErrorInSetParams())","282","    assert_raises_regex(AssertionError, msg, check_estimator,","283","                        ModifiesAnotherValue())"],"delete":["12","                                   assert_equal, ignore_warnings)"]}]}},"2ddf0ce6330da02554cac1ac86af0990bb721c48":{"changes":{"sklearn\/feature_selection\/rfe.py":"MODIFY","sklearn\/feature_selection\/tests\/test_rfe.py":"MODIFY"},"diff":{"sklearn\/feature_selection\/rfe.py":[{"add":["408","                  step=self.step, verbose=self.verbose)"],"delete":["408","                  step=self.step, verbose=self.verbose - 1)"]}],"sklearn\/feature_selection\/tests\/test_rfe.py":[{"add":["205","def test_rfecv_verbose_output():","206","    # Check verbose=1 is producing an output.","207","    from sklearn.externals.six.moves import cStringIO as StringIO","208","    import sys","209","    sys.stdout = StringIO()","210","","211","    generator = check_random_state(0)","212","    iris = load_iris()","213","    X = np.c_[iris.data, generator.normal(size=(len(iris.data), 6))]","214","    y = list(iris.target)","215","","216","    rfecv = RFECV(estimator=SVC(kernel=\"linear\"), step=1, cv=5, verbose=1)","217","    rfecv.fit(X, y)","218","","219","    verbose_output = sys.stdout","220","    verbose_output.seek(0)","221","    assert_greater(len(verbose_output.readline()), 0)","222","","223",""],"delete":[]}]}},"38f6a91566bc643e2a8f76beb16f3e673faab848":{"changes":{"sklearn\/model_selection\/_split.py":"MODIFY","sklearn\/model_selection\/_validation.py":"MODIFY","sklearn\/model_selection\/tests\/test_search.py":"MODIFY","sklearn\/model_selection\/tests\/test_split.py":"MODIFY","sklearn\/model_selection\/tests\/test_validation.py":"MODIFY","sklearn\/model_selection\/tests\/common.py":"ADD","sklearn\/model_selection\/_search.py":"MODIFY"},"diff":{"sklearn\/model_selection\/_split.py":[{"add":["1479","        self.cv = list(cv)"],"delete":["1479","        self.cv = cv"]}],"sklearn\/model_selection\/_validation.py":[{"add":["130","    cv_iter = list(cv.split(X, y, groups))","139","                      for train, test in cv_iter)","387","    cv_iter = list(cv.split(X, y, groups))","400","        for train, test in cv_iter)","755","    cv_iter = list(cv.split(X, y, groups))","778","            clone(estimator), X, y, classes, train, test, train_sizes_abs,","779","            scorer, verbose) for train, test in cv_iter)","963","    cv_iter = list(cv.split(X, y, groups))","972","        for train, test in cv_iter for v in param_range)"],"delete":["0","","139","                      for train, test in cv.split(X, y, groups))","399","        for train, test in cv.split(X, y, groups))","753","    cv_iter = cv.split(X, y, groups)","755","    cv_iter = list(cv_iter)","778","            clone(estimator), X, y, classes, train,","779","            test, train_sizes_abs, scorer, verbose)","780","            for train, test in cv_iter)","972","        for train, test in cv.split(X, y, groups) for v in param_range)"]}],"sklearn\/model_selection\/tests\/test_search.py":[{"add":["62","from sklearn.model_selection.tests.common import OneTimeSplitter","63","","1158","","1159","","1160","def test_grid_search_cv_splits_consistency():","1161","    # Check if a one time iterable is accepted as a cv parameter.","1162","    n_samples = 100","1163","    n_splits = 5","1164","    X, y = make_classification(n_samples=n_samples, random_state=0)","1165","","1166","    gs = GridSearchCV(LinearSVC(random_state=0),","1167","                      param_grid={'C': [0.1, 0.2, 0.3]},","1168","                      cv=OneTimeSplitter(n_splits=n_splits,","1169","                                         n_samples=n_samples))","1170","    gs.fit(X, y)","1171","","1172","    gs2 = GridSearchCV(LinearSVC(random_state=0),","1173","                       param_grid={'C': [0.1, 0.2, 0.3]},","1174","                       cv=KFold(n_splits=n_splits))","1175","    gs2.fit(X, y)","1176","","1177","    def _pop_time_keys(cv_results):","1178","        for key in ('mean_fit_time', 'std_fit_time',","1179","                    'mean_score_time', 'std_score_time'):","1180","            cv_results.pop(key)","1181","        return cv_results","1182","","1183","    # OneTimeSplitter is a non-re-entrant cv where split can be called only","1184","    # once if ``cv.split`` is called once per param setting in GridSearchCV.fit","1185","    # the 2nd and 3rd parameter will not be evaluated as no train\/test indices","1186","    # will be generated for the 2nd and subsequent cv.split calls.","1187","    # This is a check to make sure cv.split is not called once per param","1188","    # setting.","1189","    np.testing.assert_equal(_pop_time_keys(gs.cv_results_),","1190","                            _pop_time_keys(gs2.cv_results_))","1191","","1192","    # Check consistency of folds across the parameters","1193","    gs = GridSearchCV(LinearSVC(random_state=0),","1194","                      param_grid={'C': [0.1, 0.1, 0.2, 0.2]},","1195","                      cv=KFold(n_splits=n_splits, shuffle=True))","1196","    gs.fit(X, y)","1197","","1198","    # As the first two param settings (C=0.1) and the next two param","1199","    # settings (C=0.2) are same, the test and train scores must also be","1200","    # same as long as the same train\/test indices are generated for all","1201","    # the cv splits, for both param setting","1202","    for score_type in ('train', 'test'):","1203","        per_param_scores = {}","1204","        for param_i in range(4):","1205","            per_param_scores[param_i] = list(","1206","                gs.cv_results_['split%d_%s_score' % (s, score_type)][param_i]","1207","                for s in range(5))","1208","","1209","        assert_array_almost_equal(per_param_scores[0],","1210","                                  per_param_scores[1])","1211","        assert_array_almost_equal(per_param_scores[2],","1212","                                  per_param_scores[3])"],"delete":[]}],"sklearn\/model_selection\/tests\/test_split.py":[{"add":["871","    kf_iter = KFold(n_splits=5).split(X, y)","872","    kf_iter_wrapped = check_cv(kf_iter)","873","    # Since the wrapped iterable is enlisted and stored,","874","    # split can be called any number of times to produce","875","    # consistent results.","876","    assert_array_equal(list(kf_iter_wrapped.split(X, y)),","877","                       list(kf_iter_wrapped.split(X, y)))","878","    # If the splits are randomized, successive calls to split yields different","879","    # results","880","    kf_randomized_iter = KFold(n_splits=5, shuffle=True).split(X, y)","881","    kf_randomized_iter_wrapped = check_cv(kf_randomized_iter)","882","    assert_array_equal(list(kf_randomized_iter_wrapped.split(X, y)),","883","                       list(kf_randomized_iter_wrapped.split(X, y)))","884","    assert_true(np.any(np.array(list(kf_iter_wrapped.split(X, y))) !=","885","                       np.array(list(kf_randomized_iter_wrapped.split(X, y)))))","886",""],"delete":["61","P_sparse = coo_matrix(np.eye(5))","65","class MockClassifier(object):","66","    \"\"\"Dummy classifier to test the cross-validation\"\"\"","67","","68","    def __init__(self, a=0, allow_nd=False):","69","        self.a = a","70","        self.allow_nd = allow_nd","71","","72","    def fit(self, X, Y=None, sample_weight=None, class_prior=None,","73","            sparse_sample_weight=None, sparse_param=None, dummy_int=None,","74","            dummy_str=None, dummy_obj=None, callback=None):","75","        \"\"\"The dummy arguments are to test that this fit function can","76","        accept non-array arguments through cross-validation, such as:","77","            - int","78","            - str (this is actually array-like)","79","            - object","80","            - function","81","        \"\"\"","82","        self.dummy_int = dummy_int","83","        self.dummy_str = dummy_str","84","        self.dummy_obj = dummy_obj","85","        if callback is not None:","86","            callback(self)","87","","88","        if self.allow_nd:","89","            X = X.reshape(len(X), -1)","90","        if X.ndim >= 3 and not self.allow_nd:","91","            raise ValueError('X cannot be d')","92","        if sample_weight is not None:","93","            assert_true(sample_weight.shape[0] == X.shape[0],","94","                        'MockClassifier extra fit_param sample_weight.shape[0]'","95","                        ' is {0}, should be {1}'.format(sample_weight.shape[0],","96","                                                        X.shape[0]))","97","        if class_prior is not None:","98","            assert_true(class_prior.shape[0] == len(np.unique(y)),","99","                        'MockClassifier extra fit_param class_prior.shape[0]'","100","                        ' is {0}, should be {1}'.format(class_prior.shape[0],","101","                                                        len(np.unique(y))))","102","        if sparse_sample_weight is not None:","103","            fmt = ('MockClassifier extra fit_param sparse_sample_weight'","104","                   '.shape[0] is {0}, should be {1}')","105","            assert_true(sparse_sample_weight.shape[0] == X.shape[0],","106","                        fmt.format(sparse_sample_weight.shape[0], X.shape[0]))","107","        if sparse_param is not None:","108","            fmt = ('MockClassifier extra fit_param sparse_param.shape '","109","                   'is ({0}, {1}), should be ({2}, {3})')","110","            assert_true(sparse_param.shape == P_sparse.shape,","111","                        fmt.format(sparse_param.shape[0],","112","                                   sparse_param.shape[1],","113","                                   P_sparse.shape[0], P_sparse.shape[1]))","114","        return self","115","","116","    def predict(self, T):","117","        if self.allow_nd:","118","            T = T.reshape(len(T), -1)","119","        return T[:, 0]","120","","121","    def score(self, X=None, Y=None):","122","        return 1. \/ (1 + np.abs(self.a))","123","","124","    def get_params(self, deep=False):","125","        return {'a': self.a, 'allow_nd': self.allow_nd}","126","","127",""]}],"sklearn\/model_selection\/tests\/test_validation.py":[{"add":["62","from sklearn.model_selection.tests.common import OneTimeSplitter","133","class MockClassifier(object):","134","    \"\"\"Dummy classifier to test the cross-validation\"\"\"","135","","136","    def __init__(self, a=0, allow_nd=False):","137","        self.a = a","138","        self.allow_nd = allow_nd","139","","140","    def fit(self, X, Y=None, sample_weight=None, class_prior=None,","141","            sparse_sample_weight=None, sparse_param=None, dummy_int=None,","142","            dummy_str=None, dummy_obj=None, callback=None):","143","        \"\"\"The dummy arguments are to test that this fit function can","144","        accept non-array arguments through cross-validation, such as:","145","            - int","146","            - str (this is actually array-like)","147","            - object","148","            - function","149","        \"\"\"","150","        self.dummy_int = dummy_int","151","        self.dummy_str = dummy_str","152","        self.dummy_obj = dummy_obj","153","        if callback is not None:","154","            callback(self)","155","","156","        if self.allow_nd:","157","            X = X.reshape(len(X), -1)","158","        if X.ndim >= 3 and not self.allow_nd:","159","            raise ValueError('X cannot be d')","160","        if sample_weight is not None:","161","            assert_true(sample_weight.shape[0] == X.shape[0],","162","                        'MockClassifier extra fit_param sample_weight.shape[0]'","163","                        ' is {0}, should be {1}'.format(sample_weight.shape[0],","164","                                                        X.shape[0]))","165","        if class_prior is not None:","166","            assert_true(class_prior.shape[0] == len(np.unique(y)),","167","                        'MockClassifier extra fit_param class_prior.shape[0]'","168","                        ' is {0}, should be {1}'.format(class_prior.shape[0],","169","                                                        len(np.unique(y))))","170","        if sparse_sample_weight is not None:","171","            fmt = ('MockClassifier extra fit_param sparse_sample_weight'","172","                   '.shape[0] is {0}, should be {1}')","173","            assert_true(sparse_sample_weight.shape[0] == X.shape[0],","174","                        fmt.format(sparse_sample_weight.shape[0], X.shape[0]))","175","        if sparse_param is not None:","176","            fmt = ('MockClassifier extra fit_param sparse_param.shape '","177","                   'is ({0}, {1}), should be ({2}, {3})')","178","            assert_true(sparse_param.shape == P_sparse.shape,","179","                        fmt.format(sparse_param.shape[0],","180","                                   sparse_param.shape[1],","181","                                   P_sparse.shape[0], P_sparse.shape[1]))","182","        return self","183","","184","    def predict(self, T):","185","        if self.allow_nd:","186","            T = T.reshape(len(T), -1)","187","        return T[:, 0]","188","","189","    def score(self, X=None, Y=None):","190","        return 1. \/ (1 + np.abs(self.a))","191","","192","    def get_params(self, deep=False):","193","        return {'a': self.a, 'allow_nd': self.allow_nd}","194","","195","","204","P_sparse = coo_matrix(np.eye(5))","622","    n_samples = 30","623","    n_splits = 3","624","    X, y = make_classification(n_samples=n_samples, n_features=1,","625","                               n_informative=1, n_redundant=0, n_classes=2,","627","    estimator = MockImprovingEstimator(n_samples * ((n_splits - 1) \/ n_splits))","631","                estimator, X, y, cv=KFold(n_splits=n_splits),","632","                train_sizes=np.linspace(0.1, 1.0, 10),","644","        # Test a custom cv splitter that can iterate only once","645","        with warnings.catch_warnings(record=True) as w:","646","            train_sizes2, train_scores2, test_scores2 = learning_curve(","647","                estimator, X, y,","648","                cv=OneTimeSplitter(n_splits=n_splits, n_samples=n_samples),","649","                train_sizes=np.linspace(0.1, 1.0, 10),","650","                shuffle=shuffle_train)","651","        if len(w) > 0:","652","            raise RuntimeError(\"Unexpected warning: %r\" % w[0].message)","653","        assert_array_almost_equal(train_scores2, train_scores)","654","        assert_array_almost_equal(test_scores2, test_scores)","655","","847","def test_validation_curve_cv_splits_consistency():","848","    n_samples = 100","849","    n_splits = 5","850","    X, y = make_classification(n_samples=100, random_state=0)","851","","852","    scores1 = validation_curve(SVC(kernel='linear', random_state=0), X, y,","853","                               'C', [0.1, 0.1, 0.2, 0.2],","854","                               cv=OneTimeSplitter(n_splits=n_splits,","855","                                                  n_samples=n_samples))","856","    # The OneTimeSplitter is a non-re-entrant cv splitter. Unless, the","857","    # `split` is called for each parameter, the following should produce","858","    # identical results for param setting 1 and param setting 2 as both have","859","    # the same C value.","860","    assert_array_almost_equal(*np.vsplit(np.hstack(scores1)[(0, 2, 1, 3), :],","861","                                         2))","862","","863","    scores2 = validation_curve(SVC(kernel='linear', random_state=0), X, y,","864","                               'C', [0.1, 0.1, 0.2, 0.2],","865","                               cv=KFold(n_splits=n_splits, shuffle=True))","866","","867","    # For scores2, compare the 1st and 2nd parameter's scores","868","    # (Since the C value for 1st two param setting is 0.1, they must be","869","    # consistent unless the train test folds differ between the param settings)","870","    assert_array_almost_equal(*np.vsplit(np.hstack(scores2)[(0, 2, 1, 3), :],","871","                                         2))","872","","873","    scores3 = validation_curve(SVC(kernel='linear', random_state=0), X, y,","874","                               'C', [0.1, 0.1, 0.2, 0.2],","875","                               cv=KFold(n_splits=n_splits))","876","","877","    # OneTimeSplitter is basically unshuffled KFold(n_splits=5). Sanity check.","878","    assert_array_almost_equal(np.array(scores3), np.array(scores1))","879","","880",""],"delete":["62","from sklearn.model_selection.tests.test_split import MockClassifier","558","    X, y = make_classification(n_samples=30, n_features=1, n_informative=1,","559","                               n_redundant=0, n_classes=2,","561","    estimator = MockImprovingEstimator(20)","565","                estimator, X, y, cv=3, train_sizes=np.linspace(0.1, 1.0, 10),"]}],"sklearn\/model_selection\/tests\/common.py":[{"add":[],"delete":[]}],"sklearn\/model_selection\/_search.py":[{"add":["552","        cv_iter = list(cv.split(X, y, groups))","564","          for train, test in cv_iter)"],"delete":["563","          for train, test in cv.split(X, y, groups))"]}]}},"fcb706a7c0074a16bd472ca284145be2e0f7c936":{"changes":{"sklearn\/linear_model\/tests\/test_coordinate_descent.py":"MODIFY","sklearn\/src\/cblas\/cblas_sger.c":"ADD","sklearn\/src\/cblas\/ATL_srefger.c":"ADD","sklearn\/src\/cblas\/atlas_ptalias1.h":"ADD","sklearn\/src\/cblas\/cblas_sgemv.c":"ADD","sklearn\/linear_model\/coordinate_descent.py":"MODIFY","sklearn\/src\/cblas\/atlas_ptalias2.h":"ADD","sklearn\/src\/cblas\/ATL_srefgemv.c":"ADD","sklearn\/src\/cblas\/ATL_srefgemvN.c":"ADD","sklearn\/src\/cblas\/cblas_sscal.c":"ADD","sklearn\/linear_model\/cd_fast.pyx":"MODIFY","sklearn\/src\/cblas\/ATL_srefgemvT.c":"ADD"},"diff":{"sklearn\/linear_model\/tests\/test_coordinate_descent.py":[{"add":["693","                coef[('simple', dtype)] = clf.coef_","694","                intercept[('simple', dtype)] = clf.intercept_","709","                # test multi task enet","710","                multi_y = np.hstack((y[:, np.newaxis], y[:, np.newaxis]))","711","                clf_multioutput = MultiTaskElasticNet(","712","                    alpha=0.5, max_iter=100, fit_intercept=fit_intercept,","713","                    normalize=normalize)","714","                clf_multioutput.fit(X, multi_y)","715","                coef[('multi', dtype)] = clf_multioutput.coef_","716","                intercept[('multi', dtype)] = clf_multioutput.intercept_","717","                assert_equal(clf.coef_.dtype, dtype)","718","","719","            for v in ['simple', 'multi']:","720","                assert_array_almost_equal(coef[(v, np.float32)],","721","                                          coef[(v, np.float64)],","722","                                          decimal=4)","723","                assert_array_almost_equal(intercept[(v, np.float32)],","724","                                          intercept[(v, np.float64)],","725","                                          decimal=4)"],"delete":["693","                coef[dtype] = clf.coef_","694","                intercept[dtype] = clf.intercept_","709","            assert_array_almost_equal(coef[np.float32], coef[np.float64],","710","                                      decimal=4)","711","            assert_array_almost_equal(intercept[np.float32],","712","                                      intercept[np.float64],","713","                                      decimal=4)"]}],"sklearn\/src\/cblas\/cblas_sger.c":[{"add":[],"delete":[]}],"sklearn\/src\/cblas\/ATL_srefger.c":[{"add":[],"delete":[]}],"sklearn\/src\/cblas\/atlas_ptalias1.h":[{"add":[],"delete":[]}],"sklearn\/src\/cblas\/cblas_sgemv.c":[{"add":[],"delete":[]}],"sklearn\/linear_model\/coordinate_descent.py":[{"add":["461","                precompute = check_array(precompute, dtype=X.dtype.type,","735","        # workaround since _set_intercept will cast self.coef_ into X.dtype","1040","            Training data. Pass directly as Fortran-contiguous data","1047","        y = check_array(y, copy=False, dtype=[np.float64, np.float32],","1048","                        ensure_2d=False)","1090","            # Let us not impose fortran ordering so far: it is","1104","            X = check_array(X, 'csc', dtype=[np.float64, np.float32],","1105","                            order='F', copy=copy_X)","1159","                                         dtype=X.dtype.type)","1679","        X = check_array(X, dtype=[np.float64, np.float32], order='F',","1681","        y = check_array(y, dtype=X.dtype.type, ensure_2d=False)","1701","            self.coef_ = np.zeros((n_tasks, n_features), dtype=X.dtype.type,"],"delete":["461","                precompute = check_array(precompute, dtype=np.float64,","735","        # workaround since _set_intercept will cast self.coef_ into float64","1040","            Training data. Pass directly as float64, Fortran-contiguous data","1047","        y = np.asarray(y, dtype=np.float64)","1089","            # Let us not impose fortran ordering or float64 so far: it is","1103","            X = check_array(X, 'csc', dtype=np.float64, order='F', copy=copy_X)","1157","                                         dtype=np.float64)","1677","        # X and y must be of type float64","1678","        X = check_array(X, dtype=np.float64, order='F',","1680","        y = check_array(y, dtype=np.float64, ensure_2d=False)","1700","            self.coef_ = np.zeros((n_tasks, n_features), dtype=np.float64,"]}],"sklearn\/src\/cblas\/atlas_ptalias2.h":[{"add":[],"delete":[]}],"sklearn\/src\/cblas\/ATL_srefgemv.c":[{"add":[],"delete":[]}],"sklearn\/src\/cblas\/ATL_srefgemvN.c":[{"add":[],"delete":[]}],"sklearn\/src\/cblas\/cblas_sscal.c":[{"add":[],"delete":[]}],"sklearn\/linear_model\/cd_fast.pyx":[{"add":["124","    void sger \"cblas_sger\"(CBLAS_ORDER Order, int M, int N, float alpha,","125","                           float *X, int incX, float *Y, int incY,","126","                           float *A, int lda) nogil","131","    void sgemv \"cblas_sgemv\"(CBLAS_ORDER Order, CBLAS_TRANSPOSE TransA,","132","                             int M, int N, float alpha, float *A, int lda,","133","                             float *X, int incX, float beta,","134","                             float *Y, int incY) nogil","136","    float snrm2 \"cblas_snrm2\"(int N, float *X, int incX) nogil","139","    void scopy \"cblas_scopy\"(int N, float *X, int incX, float *Y,","140","                            int incY) nogil","142","    void sscal \"cblas_sscal\"(int N, float alpha, float *X, int incX) nogil","699","def enet_coordinate_descent_multi_task(floating[::1, :] W, floating l1_reg,","700","                                       floating l2_reg,","701","                                       np.ndarray[floating, ndim=2, mode='fortran'] X,","702","                                       np.ndarray[floating, ndim=2] Y,","703","                                       int max_iter, floating tol, object rng,","713","    # fused types version of BLAS functions","714","    cdef DOT dot","715","    cdef AXPY axpy","716","    cdef ASUM asum","717","","718","    if floating is float:","719","        dtype = np.float32","720","        dot = sdot","721","        nrm2 = snrm2","722","        asum = sasum","723","        copy = scopy","724","        scal = sscal","725","        ger = sger","726","        gemv = sgemv","727","    else:","728","        dtype = np.float64","729","        dot = ddot","730","        nrm2 = dnrm2","731","        asum = dasum","732","        copy = dcopy","733","        scal = dscal","734","        ger = dger","735","        gemv = dgemv","736","","743","    cdef floating[:, ::1] XtA = np.zeros((n_features, n_tasks), dtype=dtype)","744","    cdef floating XtA_axis1norm","745","    cdef floating dual_norm_XtA","748","    cdef floating[:, ::1] R = np.zeros((n_samples, n_tasks), dtype=dtype)","750","    cdef floating[:] norm_cols_X = np.zeros(n_features, dtype=dtype)","751","    cdef floating[::1] tmp = np.zeros(n_tasks, dtype=dtype)","752","    cdef floating[:] w_ii = np.zeros(n_tasks, dtype=dtype)","753","    cdef floating d_w_max","754","    cdef floating w_max","755","    cdef floating d_w_ii","756","    cdef floating nn","757","    cdef floating W_ii_abs_max","758","    cdef floating gap = tol + 1.0","759","    cdef floating d_w_tol = tol","760","    cdef floating R_norm","761","    cdef floating w_norm","762","    cdef floating ry_sum","763","    cdef floating l21_norm","771","    cdef floating* X_ptr = &X[0, 0]","772","    cdef floating* W_ptr = &W[0, 0]","773","    cdef floating* Y_ptr = &Y[0, 0]","774","    cdef floating* wii_ptr = &w_ii[0]","790","                    dot(n_features, X_ptr + ii, n_samples, W_ptr + jj, n_tasks)","794","        tol = tol * nrm2(n_samples * n_tasks, Y_ptr, 1) ** 2","809","                copy(n_tasks, W_ptr + ii * n_tasks, 1, wii_ptr, 1)","812","                if nrm2(n_tasks, wii_ptr, 1) != 0.0:","814","                    ger(CblasRowMajor, n_samples, n_tasks, 1.0,","819","                gemv(CblasRowMajor, CblasTrans,","824","                nn = nrm2(n_tasks, &tmp[0], 1)","827","                copy(n_tasks, &tmp[0], 1, W_ptr + ii * n_tasks, 1)","828","                scal(n_tasks, fmax(1. - l1_reg \/ nn, 0) \/ (norm_cols_X[ii] + l2_reg),","832","                if nrm2(n_tasks, W_ptr + ii * n_tasks, 1) != 0.0:","835","                    ger(CblasRowMajor, n_samples, n_tasks, -1.0,","857","                        XtA[ii, jj] = dot(","866","                    XtA_axis1norm = nrm2(n_tasks, &XtA[0, 0] + ii * n_tasks, 1)","873","                R_norm = nrm2(n_samples * n_tasks, &R[0, 0], 1)","874","                w_norm = nrm2(n_features * n_tasks, W_ptr, 1)","893","                    l21_norm += nrm2(n_tasks, W_ptr + n_tasks * ii, 1)"],"delete":["688","def enet_coordinate_descent_multi_task(double[::1, :] W, double l1_reg,","689","                                       double l2_reg,","690","                                       np.ndarray[double, ndim=2, mode='fortran'] X,","691","                                       np.ndarray[double, ndim=2] Y,","692","                                       int max_iter, double tol, object rng,","708","    cdef double[:, ::1] XtA = np.zeros((n_features, n_tasks))","709","    cdef double XtA_axis1norm","710","    cdef double dual_norm_XtA","713","    cdef double[:, ::1] R = np.zeros((n_samples, n_tasks))","715","    cdef double[:] norm_cols_X = np.zeros(n_features)","716","    cdef double[::1] tmp = np.zeros(n_tasks, dtype=np.float)","717","    cdef double[:] w_ii = np.zeros(n_tasks, dtype=np.float)","718","    cdef double d_w_max","719","    cdef double w_max","720","    cdef double d_w_ii","721","    cdef double nn","722","    cdef double W_ii_abs_max","723","    cdef double gap = tol + 1.0","724","    cdef double d_w_tol = tol","725","    cdef double ry_sum","726","    cdef double l21_norm","734","    cdef double* X_ptr = &X[0, 0]","735","    cdef double* W_ptr = &W[0, 0]","736","    cdef double* Y_ptr = &Y[0, 0]","737","    cdef double* wii_ptr = &w_ii[0]","753","                    ddot(n_features, X_ptr + ii, n_samples, W_ptr + jj, n_tasks)","757","        tol = tol * dnrm2(n_samples * n_tasks, Y_ptr, 1) ** 2","772","                dcopy(n_tasks, W_ptr + ii * n_tasks, 1, wii_ptr, 1)","775","                if dnrm2(n_tasks, wii_ptr, 1) != 0.0:","777","                    dger(CblasRowMajor, n_samples, n_tasks, 1.0,","782","                dgemv(CblasRowMajor, CblasTrans,","787","                nn = dnrm2(n_tasks, &tmp[0], 1)","790","                dcopy(n_tasks, &tmp[0], 1, W_ptr + ii * n_tasks, 1)","791","                dscal(n_tasks, fmax(1. - l1_reg \/ nn, 0) \/ (norm_cols_X[ii] + l2_reg),","795","                if dnrm2(n_tasks, W_ptr + ii * n_tasks, 1) != 0.0:","798","                    dger(CblasRowMajor, n_samples, n_tasks, -1.0,","820","                        XtA[ii, jj] = ddot(","829","                    XtA_axis1norm = dnrm2(n_tasks, &XtA[0, 0] + ii * n_tasks, 1)","836","                R_norm = dnrm2(n_samples * n_tasks, &R[0, 0], 1)","837","                w_norm = dnrm2(n_features * n_tasks, W_ptr, 1)","856","                    l21_norm += dnrm2(n_tasks, W_ptr + n_tasks * ii, 1)"]}],"sklearn\/src\/cblas\/ATL_srefgemvT.c":[{"add":[],"delete":[]}]}},"dd8a6faa2a30a6df76d93b12400fececfa74c038":{"changes":{"doc\/developers\/contributing.rst":"MODIFY"},"diff":{"doc\/developers\/contributing.rst":[{"add":["187","      Two core developers will review your code and change the prefix of the pull","188","      request to ``[MRG + 1]`` and ``[MRG + 2]`` on approval, making it eligible","189","      for merging. Incomplete contributions should be prefixed ``[WIP]`` to","190","      indicate a work in progress (and changed to ``[MRG]`` when it matures).","191","      WIPs may be useful to: indicate you are working on something to avoid","192","      duplicated work, request broad review of functionality or API, or seek","193","      collaborators. WIPs often benefit from the inclusion of a"],"delete":["187","      Incomplete contributions should be prefixed ``[WIP]`` to indicate a work","188","      in progress (and changed to ``[MRG]`` when it matures). WIPs may be useful","189","      to: indicate you are working on something to avoid duplicated work,","190","      request broad review of functionality or API, or seek collaborators.","191","      WIPs often benefit from the inclusion of a"]}]}},"8daad062f1da87d1cd484d2d80d781dc21cba491":{"changes":{"sklearn\/metrics\/scorer.py":"MODIFY","sklearn\/metrics\/tests\/test_score_objects.py":"MODIFY","doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/metrics\/tests\/test_common.py":"MODIFY","doc\/modules\/classes.rst":"MODIFY","sklearn\/metrics\/classification.py":"MODIFY","doc\/modules\/model_evaluation.rst":"MODIFY","sklearn\/metrics\/__init__.py":"MODIFY"},"diff":{"sklearn\/metrics\/scorer.py":[{"add":["28","               precision_score, recall_score, log_loss, balanced_accuracy_score,","502","balanced_accuracy_scorer = make_scorer(balanced_accuracy_score)","546","               balanced_accuracy=balanced_accuracy_scorer,"],"delete":["28","               precision_score, recall_score, log_loss,"]}],"sklearn\/metrics\/tests\/test_score_objects.py":[{"add":["49","CLF_SCORERS = ['accuracy', 'balanced_accuracy',","50","               'f1', 'f1_weighted', 'f1_macro', 'f1_micro',"],"delete":["49","CLF_SCORERS = ['accuracy', 'f1', 'f1_weighted', 'f1_macro', 'f1_micro',"]}],"doc\/whats_new\/v0.20.rst":[{"add":["42","  ","43","Model evaluation","44","","45","- Added the :func:`metrics.balanced_accuracy` metric and a corresponding","46","  ``'balanced_accuracy'`` scorer for binary classification.","47","  :issue:`8066` by :user:`xyguo` and :user:`Aman Dalmia <dalmia>`."],"delete":[]}],"sklearn\/metrics\/tests\/test_common.py":[{"add":["28","from sklearn.metrics import balanced_accuracy_score","104","    \"balanced_accuracy_score\": balanced_accuracy_score,","216","    \"balanced_accuracy_score\",","358","    \"balanced_accuracy_score\","],"delete":[]}],"doc\/modules\/classes.rst":[{"add":["781","   metrics.balanced_accuracy_score"],"delete":[]}],"sklearn\/metrics\/classification.py":[{"add":["1366","def balanced_accuracy_score(y_true, y_pred, sample_weight=None):","1367","    \"\"\"Compute the balanced accuracy","1368","","1369","    The balanced accuracy is used in binary classification problems to deal","1370","    with imbalanced datasets. It is defined as the arithmetic mean of","1371","    sensitivity (true positive rate) and specificity (true negative rate),","1372","    or the average recall obtained on either class. It is also equal to the","1373","    ROC AUC score given binary inputs.","1374","","1375","    The best value is 1 and the worst value is 0.","1376","","1377","    Read more in the :ref:`User Guide <balanced_accuracy_score>`.","1378","","1379","    Parameters","1380","    ----------","1381","    y_true : 1d array-like","1382","        Ground truth (correct) target values.","1383","","1384","    y_pred : 1d array-like","1385","        Estimated targets as returned by a classifier.","1386","","1387","    sample_weight : array-like of shape = [n_samples], optional","1388","        Sample weights.","1389","","1390","    Returns","1391","    -------","1392","    balanced_accuracy : float.","1393","        The average of sensitivity and specificity","1394","","1395","    See also","1396","    --------","1397","    recall_score, roc_auc_score","1398","","1399","    References","1400","    ----------","1401","    .. [1] Brodersen, K.H.; Ong, C.S.; Stephan, K.E.; Buhmann, J.M. (2010).","1402","           The balanced accuracy and its posterior distribution.","1403","           Proceedings of the 20th International Conference on Pattern","1404","           Recognition, 3121-24.","1405","","1406","    Examples","1407","    --------","1408","    >>> from sklearn.metrics import balanced_accuracy_score","1409","    >>> y_true = [0, 1, 0, 0, 1, 0]","1410","    >>> y_pred = [0, 1, 0, 0, 0, 1]","1411","    >>> balanced_accuracy_score(y_true, y_pred)","1412","    0.625","1413","","1414","    \"\"\"","1415","    y_type, y_true, y_pred = _check_targets(y_true, y_pred)","1416","","1417","    if y_type != 'binary':","1418","        raise ValueError('Balanced accuracy is only meaningful '","1419","                         'for binary classification problems.')","1420","    # simply wrap the ``recall_score`` function","1421","    return recall_score(y_true, y_pred,","1422","                        pos_label=None,","1423","                        average='macro',","1424","                        sample_weight=sample_weight)","1425","","1426",""],"delete":[]}],"doc\/modules\/model_evaluation.rst":[{"add":["61","'balanced_accuracy'               :func:`metrics.balanced_accuracy_score`           for binary targets","106","    ValueError: 'wrong_choice' is not a valid scoring value. Valid options are ['accuracy', 'adjusted_mutual_info_score', 'adjusted_rand_score', 'average_precision', 'balanced_accuracy', 'brier_score_loss', 'completeness_score', 'explained_variance', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'fowlkes_mallows_score', 'homogeneity_score', 'mutual_info_score', 'neg_log_loss', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_mean_squared_log_error', 'neg_median_absolute_error', 'normalized_mutual_info_score', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'r2', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'roc_auc', 'v_measure_score']","282","   balanced_accuracy_score","415",".. _balanced_accuracy_score:","416","","417","Balanced accuracy score","418","-----------------------","419","","420","The :func:`balanced_accuracy_score` function computes the","421","`balanced accuracy <https:\/\/en.wikipedia.org\/wiki\/Accuracy_and_precision>`_, which","422","avoids inflated performance estimates on imbalanced datasets. It is defined as the","423","arithmetic mean of `sensitivity <https:\/\/en.wikipedia.org\/wiki\/Sensitivity_and_specificity>`_","424","(true positive rate) and `specificity <https:\/\/en.wikipedia.org\/wiki\/Sensitivity_and_specificity>`_","425","(true negative rate), or the average of `recall scores <https:\/\/en.wikipedia.org\/wiki\/Precision_and_recall>`_","426","obtained on either class.","427","","428","If the classifier performs equally well on either class, this term reduces to the","429","conventional accuracy (i.e., the number of correct predictions divided by the total","430","number of predictions). In contrast, if the conventional accuracy is above chance only","431","because the classifier takes advantage of an imbalanced test set, then the balanced","432","accuracy, as appropriate, will drop to 50%.","433","","434","If :math:`\\hat{y}_i\\in\\{0,1\\}` is the predicted value of","435","the :math:`i`-th sample and :math:`y_i\\in\\{0,1\\}` is the corresponding true value,","436","then the balanced accuracy is defined as","437","","438",".. math::","439","","440","   \\texttt{balanced-accuracy}(y, \\hat{y}) = \\frac{1}{2} \\left(\\frac{\\sum_i 1(\\hat{y}_i = 1 \\land y_i = 1)}{\\sum_i 1(y_i = 1)} + \\frac{\\sum_i 1(\\hat{y}_i = 0 \\land y_i = 0)}{\\sum_i 1(y_i = 0)}\\right)","441","","442","where :math:`1(x)` is the `indicator function <https:\/\/en.wikipedia.org\/wiki\/Indicator_function>`_.","443","","444","Under this definition, the balanced accuracy coincides with :func:`roc_auc_score`","445","given binary ``y_true`` and ``y_pred``:","446","","447","  >>> import numpy as np","448","  >>> from sklearn.metrics import balanced_accuracy_score, roc_auc_score","449","  >>> y_true = [0, 1, 0, 0, 1, 0]","450","  >>> y_pred = [0, 1, 0, 0, 0, 1]","451","  >>> balanced_accuracy_score(y_true, y_pred)","452","  0.625","453","  >>> roc_auc_score(y_true, y_pred)","454","  0.625","455","","456","(but in general, :func:`roc_auc_score` takes as its second argument non-binary scores).","457","","458",".. note::","459","","460","    Currently this score function is only defined for binary classification problems, you","461","    may need to wrap it by yourself if you want to use it for multilabel problems.","462",""],"delete":["105","    ValueError: 'wrong_choice' is not a valid scoring value. Valid options are ['accuracy', 'adjusted_mutual_info_score', 'adjusted_rand_score', 'average_precision', 'brier_score_loss', 'completeness_score', 'explained_variance', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'fowlkes_mallows_score', 'homogeneity_score', 'mutual_info_score', 'neg_log_loss', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_mean_squared_log_error', 'neg_median_absolute_error', 'normalized_mutual_info_score', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'r2', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'roc_auc', 'v_measure_score']"]}],"sklearn\/metrics\/__init__.py":[{"add":["16","from .classification import balanced_accuracy_score","71","    'balanced_accuracy_score',"],"delete":[]}]}},"e5bf61eee1c62dbb5a2103e68cf73936e4ac3e58":{"changes":{"doc\/developers\/advanced_installation.rst":"MODIFY","sklearn\/linear_model\/tests\/test_coordinate_descent.py":"MODIFY","sklearn\/utils\/fixes.py":"MODIFY","sklearn\/utils\/tests\/test_testing.py":"MODIFY","sklearn\/metrics\/classification.py":"MODIFY","sklearn\/utils\/testing.py":"MODIFY","sklearn\/model_selection\/tests\/test_split.py":"MODIFY","README.rst":"MODIFY","build_tools\/travis\/install.sh":"MODIFY","setup.py":"MODIFY","sklearn\/externals\/funcsigs.py":"MODIFY","sklearn\/gaussian_process\/gaussian_process.py":"MODIFY","doc\/developers\/contributing.rst":"MODIFY","sklearn\/preprocessing\/data.py":"MODIFY","sklearn\/utils\/tests\/test_estimator_checks.py":"MODIFY","sklearn\/ensemble\/tests\/test_base.py":"MODIFY","\/dev\/null":"DELETE","doc\/install.rst":"MODIFY","sklearn\/metrics\/pairwise.py":"MODIFY","sklearn\/tests\/test_base.py":"MODIFY",".travis.yml":"MODIFY","sklearn\/neighbors\/tests\/test_approximate.py":"MODIFY"},"diff":{"doc\/developers\/advanced_installation.rst":[{"add":["37","- Python (>= 2.7 or >= 3.3),"],"delete":["37","- Python (>= 2.6 or >= 3.3),"]}],"sklearn\/linear_model\/tests\/test_coordinate_descent.py":[{"add":[],"delete":["34","def check_warnings():","35","    if version_info < (2, 6):","36","        raise SkipTest(\"Testing for warnings is not supported in versions \\","37","        older than Python 2.6\")","38","","39",""]}],"sklearn\/utils\/fixes.py":[{"add":[],"delete":["206","try:","207","    from itertools import combinations_with_replacement","208","except ImportError:","209","    # Backport of itertools.combinations_with_replacement for Python 2.6,","210","    # from Python 3.4 documentation (http:\/\/tinyurl.com\/comb-w-r), copyright","211","    # Python Software Foundation (https:\/\/docs.python.org\/3\/license.html)","212","    def combinations_with_replacement(iterable, r):","213","        # combinations_with_replacement('ABC', 2) --> AA AB AC BB BC CC","214","        pool = tuple(iterable)","215","        n = len(pool)","216","        if not n and r:","217","            return","218","        indices = [0] * r","219","        yield tuple(pool[i] for i in indices)","220","        while True:","221","            for i in reversed(range(r)):","222","                if indices[i] != n - 1:","223","                    break","224","            else:","225","                return","226","            indices[i:] = [indices[i] + 1] * (r - i)","227","            yield tuple(pool[i] for i in indices)","228","","229","","293","if sys.version_info < (2, 7, 0):","294","    # partial cannot be pickled in Python 2.6","295","    # http:\/\/bugs.python.org\/issue1398","296","    class partial(object):","297","        def __init__(self, func, *args, **keywords):","298","            functools.update_wrapper(self, func)","299","            self.func = func","300","            self.args = args","301","            self.keywords = keywords","302","","303","        def __call__(self, *args, **keywords):","304","            args = self.args + args","305","            kwargs = self.keywords.copy()","306","            kwargs.update(keywords)","307","            return self.func(*args, **kwargs)","308","else:","309","    from functools import partial","310","","311",""]}],"sklearn\/utils\/tests\/test_testing.py":[{"add":["6","    assert_less,","7","    assert_greater,","21","def test_assert_less():","22","    assert_less(0, 1)","23","    assert_raises(AssertionError, assert_less, 1, 0)","26","def test_assert_greater():","27","    assert_greater(1, 0)","28","    assert_raises(AssertionError, assert_greater, 0, 1)"],"delete":["6","    _assert_less,","7","    _assert_greater,","20","try:","21","    from nose.tools import assert_less","23","    def test_assert_less():","24","        # Check that the nose implementation of assert_less gives the","25","        # same thing as the scikit's","26","        assert_less(0, 1)","27","        _assert_less(0, 1)","28","        assert_raises(AssertionError, assert_less, 1, 0)","29","        assert_raises(AssertionError, _assert_less, 1, 0)","31","except ImportError:","32","    pass","34","try:","35","    from nose.tools import assert_greater","36","","37","    def test_assert_greater():","38","        # Check that the nose implementation of assert_less gives the","39","        # same thing as the scikit's","40","        assert_greater(1, 0)","41","        _assert_greater(1, 0)","42","        assert_raises(AssertionError, assert_greater, 0, 1)","43","        assert_raises(AssertionError, _assert_greater, 0, 1)","44","","45","except ImportError:","46","    pass"]}],"sklearn\/metrics\/classification.py":[{"add":[],"delete":["432","","433","            # If there is no label, it results in a Nan instead, we set","434","            # the jaccard to 1: lim_{x->0} x\/x = 1","435","            # Note with py2.6 and np 1.3: we can't check safely for nan."]}],"sklearn\/utils\/testing.py":[{"add":["79","SkipTest = unittest.case.SkipTest","80","assert_dict_equal = _dummy.assertDictEqual","81","assert_in = _dummy.assertIn","82","assert_not_in = _dummy.assertNotIn","83","assert_less = _dummy.assertLess","84","assert_greater = _dummy.assertGreater","85","assert_less_equal = _dummy.assertLessEqual","86","assert_greater_equal = _dummy.assertGreaterEqual","92","    # Python 2.7","93","    assert_raises_regex = _dummy.assertRaisesRegexp","325","assert_less = _dummy.assertLess","326","assert_greater = _dummy.assertGreater"],"delete":["80","try:","81","    SkipTest = unittest.case.SkipTest","82","except AttributeError:","83","    # Python <= 2.6, we stil need nose here","84","    from nose import SkipTest","85","","86","","87","try:","88","    assert_dict_equal = _dummy.assertDictEqual","89","    assert_in = _dummy.assertIn","90","    assert_not_in = _dummy.assertNotIn","91","except AttributeError:","92","    # Python <= 2.6","93","","94","    assert_dict_equal = assert_equal","95","","96","    def assert_in(x, container):","97","        assert_true(x in container, msg=\"%r in %r\" % (x, container))","98","","99","    def assert_not_in(x, container):","100","        assert_false(x in container, msg=\"%r in %r\" % (x, container))","105","    # for Python 2.6","106","    def assert_raises_regex(expected_exception, expected_regexp,","107","                            callable_obj=None, *args, **kwargs):","108","        \"\"\"Helper function to check for message patterns in exceptions.\"\"\"","109","        not_raised = False","110","        try:","111","            callable_obj(*args, **kwargs)","112","            not_raised = True","113","        except expected_exception as e:","114","            error_message = str(e)","115","            if not re.compile(expected_regexp).search(error_message):","116","                raise AssertionError(\"Error message should match pattern \"","117","                                     \"%r. %r does not.\" %","118","                                     (expected_regexp, error_message))","119","        if not_raised:","120","            raise AssertionError(\"%s not raised by %s\" %","121","                                 (expected_exception.__name__,","122","                                  callable_obj.__name__))","123","","130","def _assert_less(a, b, msg=None):","131","    message = \"%r is not lower than %r\" % (a, b)","132","    if msg is not None:","133","        message += \": \" + msg","134","    assert a < b, message","135","","136","","137","def _assert_greater(a, b, msg=None):","138","    message = \"%r is not greater than %r\" % (a, b)","139","    if msg is not None:","140","        message += \": \" + msg","141","    assert a > b, message","142","","143","","144","def assert_less_equal(a, b, msg=None):","145","    message = \"%r is not lower than or equal to %r\" % (a, b)","146","    if msg is not None:","147","        message += \": \" + msg","148","    assert a <= b, message","149","","150","","151","def assert_greater_equal(a, b, msg=None):","152","    message = \"%r is not greater than or equal to %r\" % (a, b)","153","    if msg is not None:","154","        message += \": \" + msg","155","    assert a >= b, message","156","","157","","275","    # XXX: once we may depend on python >= 2.6, this can be replaced by the","276","","277","    # warnings module context manager.","386","try:","387","    assert_less = _dummy.assertLess","388","    assert_greater = _dummy.assertGreater","389","except AttributeError:","390","    assert_less = _assert_less","391","    assert_greater = _assert_greater"]}],"sklearn\/model_selection\/tests\/test_split.py":[{"add":["9","from itertools import combinations_with_replacement"],"delete":["9","from sklearn.utils.fixes import combinations_with_replacement"]}],"README.rst":[{"add":["50","- Python (>= 2.7 or >= 3.3)"],"delete":["50","- Python (>= 2.6 or >= 3.3)"]}],"build_tools\/travis\/install.sh":[{"add":["55","            mkl flake8 cython=$CYTHON_VERSION \\","61","            nomkl cython=$CYTHON_VERSION \\"],"delete":["55","            libgfortran mkl flake8 \\","61","            libgfortran nomkl \\","66","    # Temporary work around for Python 2.6 because cython >= 0.23 is","67","    # required for building scikit-learn but python 2.6 and cython","68","    # 0.23 are not compatible in conda. Remove the next line and","69","    # install cython via conda when Python 2.6 support is removed.","70","    pip install cython==$CYTHON_VERSION","71",""]}],"setup.py":[{"add":[],"delete":["202","                                 'Programming Language :: Python :: 2.6',"]}],"sklearn\/externals\/funcsigs.py":[{"add":["4","modified to be compatible with Python 2.7 and 3.2+.","12","from collections import OrderedDict"],"delete":["4","modified to be compatible with Python 2.6, 2.7 and 3.2+.","12","try:","13","    from collections import OrderedDict","14","except ImportError:","15","    from .odict import OrderedDict"]}],"sklearn\/gaussian_process\/gaussian_process.py":[{"add":["620","        Q, G = linalg.qr(Ft, mode='economic')"],"delete":["620","        try:","621","            Q, G = linalg.qr(Ft, econ=True)","622","        except:","623","            #\/usr\/lib\/python2.6\/dist-packages\/scipy\/linalg\/decomp.py:1177:","624","            # DeprecationWarning: qr econ argument will be removed after scipy","625","            # 0.7. The economy transform will then be available through the","626","            # mode='economic' argument.","627","            Q, G = linalg.qr(Ft, mode='economic')"]}],"doc\/developers\/contributing.rst":[{"add":["704","on both 2.7 and 3.2 or newer."],"delete":["704","on both 2.6 or 2.7, and 3.2 or newer."]}],"sklearn\/preprocessing\/data.py":[{"add":["11","from itertools import combinations_with_replacement as combinations_w_r"],"delete":["21","from ..utils.fixes import combinations_with_replacement as combinations_w_r"]}],"sklearn\/utils\/tests\/test_estimator_checks.py":[{"add":["86","    msg = \"TypeError not raised\""],"delete":["86","    msg = \"TypeError not raised by fit\""]}],"sklearn\/ensemble\/tests\/test_base.py":[{"add":["18","from collections import OrderedDict"],"delete":["18","from sklearn.externals.odict import OrderedDict"]}],"\/dev\/null":[{"add":[],"delete":[]}],"doc\/install.rst":[{"add":["17","- Python (>= 2.7 or >= 3.3),"],"delete":["17","- Python (>= 2.6 or >= 3.3),"]}],"sklearn\/metrics\/pairwise.py":[{"add":["12","from functools import partial"],"delete":["21","from ..utils.fixes import partial"]}],"sklearn\/tests\/test_base.py":[{"add":[],"delete":["182","    PY26 = sys.version_info[:2] == (2, 6)","183","    if PY26:","184","        # sp.dok_matrix can not be deepcopied in Python 2.6","185","        sparse_matrix_classes.remove(sp.dok_matrix)","186",""]}],".travis.yml":[{"add":["31","    - DISTRIB=\"conda\" PYTHON_VERSION=\"2.7\" INSTALL_MKL=\"false\""],"delete":["31","    - DISTRIB=\"conda\" PYTHON_VERSION=\"2.6\" INSTALL_MKL=\"false\""]}],"sklearn\/neighbors\/tests\/test_approximate.py":[{"add":["231","    lsfh = LSHForest(min_hash_match=0, n_candidates=n_points,","232","                     random_state=42).fit(X)"],"delete":["231","    lsfh = LSHForest(min_hash_match=0, n_candidates=n_points).fit(X)"]}]}},"1fa3bf775f4ac8a5575022607e5b9b5d752a0db0":{"changes":{"doc\/themes\/scikit-learn\/static\/nature.css_t":"MODIFY"},"diff":{"doc\/themes\/scikit-learn\/static\/nature.css_t":[{"add":["42","    \/* Don't leave a fixed height. It won't allow the header to expand when search results appear. *\/","107","    div.header {","108","      height: 60px;","109","    }"],"delete":["42","    \/* for the logo to correctly expand when showing results","43","       but remain cut when not *\/","44","       height: 60px;"]}]}},"2caa1445fb360ecb6c590b8b1c056bb084b4337a":{"changes":{"doc\/whats_new.rst":"MODIFY","sklearn\/feature_selection\/univariate_selection.py":"MODIFY","sklearn\/feature_selection\/tests\/test_feature_select.py":"MODIFY"},"diff":{"doc\/whats_new.rst":[{"add":["52","   - Fix a bug where :class:`sklearn.feature_selection.SelectFdr` did not ","53","     exactly implement Benjamini-Hochberg procedure. It formerly may have","54","     selected fewer features than it should.","55","     (`#7490 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/7490>`_) by","56","     `Peng Meng`_.","57","","4881","","4882",".. _Peng Meng: https:\/\/github.com\/mpjlu"],"delete":[]}],"sklearn\/feature_selection\/univariate_selection.py":[{"add":["598","        selected = sv[sv <= float(self.alpha) \/ n_features *","599","                      np.arange(1, n_features + 1)]"],"delete":["598","        selected = sv[sv <= float(self.alpha) \/ n_features","599","                      * np.arange(n_features)]"]}],"sklearn\/feature_selection\/tests\/test_feature_select.py":[{"add":["373","def test_boundary_case_ch2():","374","    # Test boundary case, and always aim to select 1 feature.","375","    X = np.array([[10, 20], [20, 20], [20, 30]])","376","    y = np.array([[1], [0], [0]])","377","    scores, pvalues = chi2(X, y)","378","    assert_array_almost_equal(scores, np.array([4., 0.71428571]))","379","    assert_array_almost_equal(pvalues, np.array([0.04550026, 0.39802472]))","380","","381","    filter_fdr = SelectFdr(chi2, alpha=0.1)","382","    filter_fdr.fit(X, y)","383","    support_fdr = filter_fdr.get_support()","384","    assert_array_equal(support_fdr, np.array([True, False]))","385","","386","    filter_kbest = SelectKBest(chi2, k=1)","387","    filter_kbest.fit(X, y)","388","    support_kbest = filter_kbest.get_support()","389","    assert_array_equal(support_kbest, np.array([True, False]))","390","","391","    filter_percentile = SelectPercentile(chi2, percentile=50)","392","    filter_percentile.fit(X, y)","393","    support_percentile = filter_percentile.get_support()","394","    assert_array_equal(support_percentile, np.array([True, False]))","395","","396","    filter_fpr = SelectFpr(chi2, alpha=0.1)","397","    filter_fpr.fit(X, y)","398","    support_fpr = filter_fpr.get_support()","399","    assert_array_equal(support_fpr, np.array([True, False]))","400","","401","    filter_fwe = SelectFwe(chi2, alpha=0.1)","402","    filter_fwe.fit(X, y)","403","    support_fwe = filter_fwe.get_support()","404","    assert_array_equal(support_fwe, np.array([True, False]))","405","","406","","440","                                            random_state in range(100)])"],"delete":["406","                                            random_state in range(30)])"]}]}},"69ca580cd811d55cc996c9b1788bc6bae0ed8ad4":{"changes":{"circle.yml":"MODIFY","build_tools\/circle\/push_doc.sh":"MODIFY"},"diff":{"circle.yml":[{"add":["15","   branch: \/^master$|^[0-9]+\\.[0-9]+\\.X$\/"],"delete":["15","   branch: master"]}],"build_tools\/circle\/push_doc.sh":[{"add":["14","if [ \"$CIRCLE_BRANCH\" = \"master\" ]","15","then","16","\tdir=dev","17","else","18","\t# Strip off .X","19","\tdir=\"${CIRCLE_BRANCH::-2}\"","20","fi","21","","22","MSG=\"Pushing the docs to $dir\/ for branch: $CIRCLE_BRANCH, commit $CIRCLE_SHA1\"","29","git checkout $CIRCLE_BRANCH","30","git reset --hard origin\/$CIRCLE_BRANCH","31","git rm -rf $dir\/ && rm -rf $dir\/","32","cp -R $HOME\/scikit-learn\/doc\/_build\/html\/stable $dir","36","git add -f $dir\/","37","git commit -m \"$MSG\" $dir"],"delete":["14","MSG=\"Pushing the docs for revision for branch: $CIRCLE_BRANCH, commit $CIRCLE_SHA1\"","21","git checkout master","22","git reset --hard origin\/master","23","git rm -rf dev\/ && rm -rf dev\/","24","cp -R $HOME\/scikit-learn\/doc\/_build\/html\/stable dev","28","git add -f dev\/","29","git commit -m \"$MSG\" dev"]}]}},"8695ff5969d84429e4a84ee8fea835e52e6d230d":{"changes":{"sklearn\/multioutput.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/tests\/test_multioutput.py":"MODIFY"},"diff":{"sklearn\/multioutput.py":[{"add":["24","from .utils.metaestimators import if_delegate_has_method","40","def _partial_fit_estimator(estimator, X, y, classes=None, sample_weight=None,","41","                           first_time=True):","42","    if first_time:","43","        estimator = clone(estimator)","44","","45","    if sample_weight is not None:","46","        if classes is not None:","47","            estimator.partial_fit(X, y, classes=classes,","48","                                  sample_weight=sample_weight)","49","        else:","50","            estimator.partial_fit(X, y, sample_weight=sample_weight)","51","    else:","52","        if classes is not None:","53","            estimator.partial_fit(X, y, classes=classes)","54","        else:","55","            estimator.partial_fit(X, y)","56","    return estimator","57","","58","","65","    @if_delegate_has_method('estimator')","66","    def partial_fit(self, X, y, classes=None, sample_weight=None):","67","        \"\"\"Incrementally fit the model to data.","68","        Fit a separate model for each output variable.","69","","70","        Parameters","71","        ----------","72","        X : (sparse) array-like, shape (n_samples, n_features)","73","            Data.","74","","75","        y : (sparse) array-like, shape (n_samples, n_outputs)","76","            Multi-output targets.","77","","78","        classes : list of numpy arrays, shape (n_outputs)","79","            Each array is unique classes for one output in str\/int","80","            Can be obtained by via","81","            ``[np.unique(y[:, i]) for i in range(y.shape[1])]``, where y is the","82","            target matrix of the entire dataset.","83","            This argument is required for the first call to partial_fit","84","            and can be omitted in the subsequent calls.","85","            Note that y doesn't need to contain all labels in `classes`.","86","","87","        sample_weight : array-like, shape = (n_samples) or None","88","            Sample weights. If None, then samples are equally weighted.","89","            Only supported if the underlying regressor supports sample","90","            weights.","91","","92","        Returns","93","        -------","94","        self : object","95","            Returns self.","96","        \"\"\"","97","        X, y = check_X_y(X, y,","98","                         multi_output=True,","99","                         accept_sparse=True)","100","","101","        if y.ndim == 1:","102","            raise ValueError(\"y must have at least two dimensions for \"","103","                             \"multi-output regression but has only one.\")","104","","105","        if (sample_weight is not None and","106","                not has_fit_parameter(self.estimator, 'sample_weight')):","107","            raise ValueError(\"Underlying estimator does not support\"","108","                             \" sample weights.\")","109","","110","        first_time = not hasattr(self, 'estimators_')","111","","112","        self.estimators_ = Parallel(n_jobs=self.n_jobs)(","113","            delayed(_partial_fit_estimator)(","114","                self.estimators_[i] if not first_time else self.estimator,","115","                X, y[:, i],","116","                classes[i] if classes is not None else None,","117","                sample_weight, first_time) for i in range(y.shape[1]))","118","        return self","119","","153","                             \"multi-output regression but has only one.\")","157","            raise ValueError(\"Underlying estimator does not support\"","160","        self.estimators_ = Parallel(n_jobs=self.n_jobs)(","161","            delayed(_fit_estimator)(","162","                self.estimator, X, y[:, i], sample_weight)","163","            for i in range(y.shape[1]))","187","        y = Parallel(n_jobs=self.n_jobs)(","188","            delayed(parallel_helper)(e, 'predict', X)","189","            for e in self.estimators_)","213","","217","    def partial_fit(self, X, y, sample_weight=None):","218","        \"\"\"Incrementally fit the model to data.","219","        Fit a separate model for each output variable.","220","","221","        Parameters","222","        ----------","223","        X : (sparse) array-like, shape (n_samples, n_features)","224","            Data.","225","","226","        y : (sparse) array-like, shape (n_samples, n_outputs)","227","            Multi-output targets.","228","","229","        sample_weight : array-like, shape = (n_samples) or None","230","            Sample weights. If None, then samples are equally weighted.","231","            Only supported if the underlying regressor supports sample","232","            weights.","233","","234","        Returns","235","        -------","236","        self : object","237","            Returns self.","238","        \"\"\"","239","        super(MultiOutputRegressor, self).partial_fit(","240","            X, y, sample_weight=sample_weight)","241",""],"delete":["78","                             \"multi target regression but has only one.\")","82","            raise ValueError(\"Underlying regressor does not support\"","85","        self.estimators_ = Parallel(n_jobs=self.n_jobs)(delayed(_fit_estimator)(","86","            self.estimator, X, y[:, i], sample_weight) for i in range(y.shape[1]))","110","        y = Parallel(n_jobs=self.n_jobs)(delayed(parallel_helper)(e, 'predict', X)","111","                                         for e in self.estimators_)"]}],"doc\/whats_new.rst":[{"add":["51","   - :class:`multioutput.MultiOutputRegressor` and :class:`multioutput.MultiOutputClassifier`","52","     now support online learning using `partial_fit`.","53","     issue: `8053` by :user:`Peng Yu <yupbank>`.","54",""],"delete":[]}],"sklearn\/tests\/test_multioutput.py":[{"add":["0","from __future__ import division","6","from sklearn.utils.testing import assert_false","10","from sklearn.utils.testing import assert_not_equal","11","from sklearn.utils.testing import assert_array_almost_equal","16","from sklearn.linear_model import Lasso","17","from sklearn.linear_model import SGDClassifier","18","from sklearn.linear_model import SGDRegressor","19","from sklearn.linear_model import LogisticRegression","34","        references[:, n] = rgr.predict(X_test)","43","def test_multi_target_regression_partial_fit():","44","    X, y = datasets.make_regression(n_targets=3)","48","    references = np.zeros_like(y_test)","49","    half_index = 25","50","    for n in range(3):","51","        sgr = SGDRegressor(random_state=0)","52","        sgr.partial_fit(X_train[:half_index], y_train[:half_index, n])","53","        sgr.partial_fit(X_train[half_index:], y_train[half_index:, n])","54","        references[:, n] = sgr.predict(X_test)","55","","56","    sgr = MultiOutputRegressor(SGDRegressor(random_state=0))","57","","58","    sgr.partial_fit(X_train[:half_index], y_train[:half_index])","59","    sgr.partial_fit(X_train[half_index:], y_train[half_index:])","60","","61","    y_pred = sgr.predict(X_test)","62","    assert_almost_equal(references, y_pred)","63","","64","","65","def test_multi_target_regression_one_target():","66","    # Test multi target regression raises","67","    X, y = datasets.make_regression(n_targets=1)","68","","70","    assert_raises(ValueError, rgr.fit, X, y)","76","    X_test = X[50:]","86","        assert_almost_equal(rgr.predict(X_test),","87","                            rgr_sparse.predict(sparse(X_test)))","91","    X = [[1, 2, 3], [4, 5, 6]]","104","def test_multi_target_sample_weight_partial_fit():","105","    # weighted regressor","106","    X = [[1, 2, 3], [4, 5, 6]]","107","    y = [[3.141, 2.718], [2.718, 3.141]]","108","    w = [2., 1.]","109","    rgr_w = MultiOutputRegressor(SGDRegressor(random_state=0))","110","    rgr_w.partial_fit(X, y, w)","111","","112","    # weighted with different weights","113","    w = [2., 2.]","114","    rgr = MultiOutputRegressor(SGDRegressor(random_state=0))","115","    rgr.partial_fit(X, y, w)","116","","117","    assert_not_equal(rgr.predict(X)[0][0], rgr_w.predict(X)[0][0])","118","","119","","122","    Xw = [[1, 2, 3], [4, 5, 6]]","129","    X = [[1, 2, 3], [1, 2, 3], [4, 5, 6]]","134","    X_test = [[1.5, 2.5, 3.5], [3.5, 4.5, 5.5]]","137","","149","classes = list(map(np.unique, (y1, y2, y3)))","150","","151","","152","def test_multi_output_classification_partial_fit_parallelism():","153","    sgd_linear_clf = SGDClassifier(loss='log', random_state=1)","154","    mor = MultiOutputClassifier(sgd_linear_clf, n_jobs=-1)","155","    mor.partial_fit(X, y, classes)","156","    est1 = mor.estimators_[0]","157","    mor.partial_fit(X, y)","158","    est2 = mor.estimators_[0]","159","    # parallelism requires this to be the case for a sane implementation","160","    assert_false(est1 is est2)","161","","162","","163","def test_multi_output_classification_partial_fit():","164","    # test if multi_target initializes correctly with base estimator and fit","165","    # assert predictions work as expected for predict","166","","167","    sgd_linear_clf = SGDClassifier(loss='log', random_state=1)","168","    multi_target_linear = MultiOutputClassifier(sgd_linear_clf)","169","","170","    # train the multi_target_linear and also get the predictions.","171","    half_index = X.shape[0] \/\/ 2","172","    multi_target_linear.partial_fit(","173","        X[:half_index], y[:half_index], classes=classes)","174","","175","    first_predictions = multi_target_linear.predict(X)","176","    assert_equal((n_samples, n_outputs), first_predictions.shape)","177","","178","    multi_target_linear.partial_fit(X[half_index:], y[half_index:])","179","    second_predictions = multi_target_linear.predict(X)","180","    assert_equal((n_samples, n_outputs), second_predictions.shape)","181","","182","    # train the linear classification with each column and assert that","183","    # predictions are equal after first partial_fit and second partial_fit","184","    for i in range(3):","185","        # create a clone with the same state","186","        sgd_linear_clf = clone(sgd_linear_clf)","187","        sgd_linear_clf.partial_fit(","188","            X[:half_index], y[:half_index, i], classes=classes[i])","189","        assert_array_equal(sgd_linear_clf.predict(X), first_predictions[:, i])","190","        sgd_linear_clf.partial_fit(X[half_index:], y[half_index:, i])","191","        assert_array_equal(sgd_linear_clf.predict(X), second_predictions[:, i])","192","","193","","194","def test_mutli_output_classifiation_partial_fit_no_first_classes_exception():","195","    sgd_linear_clf = SGDClassifier(loss='log', random_state=1)","196","    multi_target_linear = MultiOutputClassifier(sgd_linear_clf)","197","    assert_raises_regex(ValueError, \"classes must be passed on the first call \"","198","                                    \"to partial_fit.\",","199","                        multi_target_linear.partial_fit, X, y)","307","def test_multi_output_classification_partial_fit_sample_weights():","308","    # weighted classifier","309","    Xw = [[1, 2, 3], [4, 5, 6], [1.5, 2.5, 3.5]]","310","    yw = [[3, 2], [2, 3], [3, 2]]","311","    w = np.asarray([2., 1., 1.])","312","    sgd_linear_clf = SGDClassifier(random_state=1)","313","    clf_w = MultiOutputClassifier(sgd_linear_clf)","314","    clf_w.fit(Xw, yw, w)","315","","316","    # unweighted, but with repeated samples","317","    X = [[1, 2, 3], [1, 2, 3], [4, 5, 6], [1.5, 2.5, 3.5]]","318","    y = [[3, 2], [3, 2], [2, 3], [3, 2]]","319","    sgd_linear_clf = SGDClassifier(random_state=1)","320","    clf = MultiOutputClassifier(sgd_linear_clf)","321","    clf.fit(X, y)","322","    X_test = [[1.5, 2.5, 3.5]]","323","    assert_array_almost_equal(clf.predict(X_test), clf_w.predict(X_test))","324","","325",""],"delete":["12","from sklearn.linear_model import Lasso, LogisticRegression","27","        references[:,n] = rgr.predict(X_test)","36","def test_multi_target_regression_one_target():","37","    # Test multi target regression raises","38","    X, y = datasets.make_regression(n_targets=1)","43","    assert_raises(ValueError, rgr.fit, X_train, y_train)","49","    X_test, y_test = X[50:], y[50:]","59","        assert_almost_equal(rgr.predict(X_test), rgr_sparse.predict(sparse(X_test)))","63","    X = [[1,2,3], [4,5,6]]","78","    Xw = [[1,2,3], [4,5,6]]","85","    X = [[1,2,3], [1,2,3], [4,5,6]]","90","    X_test = [[1.5,2.5,3.5], [3.5,4.5,5.5]]"]}]}},"456fb5697fd31c60eac6596ba9a787319dcb8035":{"changes":{"doc\/whats_new.rst":"MODIFY","sklearn\/feature_selection\/univariate_selection.py":"MODIFY","sklearn\/feature_selection\/tests\/test_feature_select.py":"MODIFY"},"diff":{"doc\/whats_new.rst":[{"add":["112","   - Added ability to use sparse matrices in :func:`feature_selection.f_regression`","113","     with ``center=True``. :issue:`8065` by :user:`Daniel LeJeune <acadiansith>`.","114",""],"delete":[]}],"sklearn\/feature_selection\/univariate_selection.py":[{"add":["269","    n_samples = X.shape[0]","270","","271","    # compute centered values","272","    # note that E[(x - mean(x))*(y - mean(y))] = E[x*(y - mean(y))], so we","273","    # need not center X","276","        if issparse(X):","277","            X_means = X.mean(axis=0).getA1()","278","        else:","279","            X_means = X.mean(axis=0)","280","        # compute the scaled standard deviations via moments","281","        X_norms = np.sqrt(row_norms(X.T, squared=True) -","282","                          n_samples * X_means ** 2)","283","    else:","284","        X_norms = row_norms(X.T)","288","    corr \/= X_norms"],"delete":["268","    if issparse(X) and center:","269","        raise ValueError(\"center=True only allowed for dense data\")","273","        X = X.copy('F')  # faster in fortran","274","        X -= X.mean(axis=0)","278","    corr \/= row_norms(X.T)"]}],"sklearn\/feature_selection\/tests\/test_feature_select.py":[{"add":["94","    # with centering, compare with sparse","95","    F, pv = f_regression(X, y, center=True)","96","    F_sparse, pv_sparse = f_regression(sparse.csr_matrix(X), y, center=True)","97","    assert_array_almost_equal(F_sparse, F)","98","    assert_array_almost_equal(pv_sparse, pv)","99",""],"delete":[]}]}},"829efa5929c129a4112e539699f9034152332ee4":{"changes":{"sklearn\/model_selection\/_validation.py":"MODIFY","sklearn\/model_selection\/tests\/test_validation.py":"MODIFY","doc\/whats_new.rst":"MODIFY"},"diff":{"sklearn\/model_selection\/_validation.py":[{"add":["644","                   pre_dispatch=\"all\", verbose=0, shuffle=False,","645","                   random_state=None):","721","    shuffle : boolean, optional","722","        Whether to shuffle training data before taking prefixes of it","723","        based on``train_sizes``.","724","","725","    random_state : None, int or RandomState","726","        When shuffle=True, pseudo-random number generator state used for","727","        shuffling. If None, use default numpy RNG for shuffling.","728","","769","","770","    if shuffle:","771","        rng = check_random_state(random_state)","772","        cv_iter = ((rng.permutation(train), test) for train, test in cv_iter)","773","","777","            clone(estimator), X, y, classes, train,","778","            test, train_sizes_abs, scorer, verbose)","779","            for train, test in cv_iter)","781","        train_test_proportions = []","782","        for train, test in cv_iter:","783","            for n_train_samples in train_sizes_abs:","784","                train_test_proportions.append((train[:n_train_samples], test))","785","","787","            clone(estimator), X, y, scorer, train, test,","789","            for train, test in train_test_proportions)"],"delete":["644","                   pre_dispatch=\"all\", verbose=0):","720","    Returns","764","            clone(estimator), X, y, classes, train, test, train_sizes_abs,","765","            scorer, verbose) for train, test in cv.split(X, y, groups))","768","            clone(estimator), X, y, scorer, train[:n_train_samples], test,","770","            for train, test in cv_iter","771","            for n_train_samples in train_sizes_abs)"]}],"sklearn\/model_selection\/tests\/test_validation.py":[{"add":["562","    for shuffle_train in [False, True]:","563","        with warnings.catch_warnings(record=True) as w:","564","            train_sizes, train_scores, test_scores = learning_curve(","565","                estimator, X, y, cv=3, train_sizes=np.linspace(0.1, 1.0, 10),","566","                shuffle=shuffle_train)","567","        if len(w) > 0:","568","            raise RuntimeError(\"Unexpected warning: %r\" % w[0].message)","569","        assert_equal(train_scores.shape, (10, 3))","570","        assert_equal(test_scores.shape, (10, 3))","571","        assert_array_equal(train_sizes, np.linspace(2, 20, 10))","572","        assert_array_almost_equal(train_scores.mean(axis=1),","573","                                  np.linspace(1.9, 1.0, 10))","574","        assert_array_almost_equal(test_scores.mean(axis=1),","575","                                  np.linspace(0.1, 1.0, 10))","626","    for shuffle_train in [False, True]:","627","        train_sizes, train_scores, test_scores = learning_curve(","628","            estimator, X, y, cv=3, exploit_incremental_learning=True,","629","            train_sizes=np.linspace(0.1, 1.0, 10), shuffle=shuffle_train)","630","        assert_array_equal(train_sizes, np.linspace(2, 20, 10))","631","        assert_array_almost_equal(train_scores.mean(axis=1),","632","                                  np.linspace(1.9, 1.0, 10))","633","        assert_array_almost_equal(test_scores.mean(axis=1),","634","                                  np.linspace(0.1, 1.0, 10))","718","def test_learning_curve_with_shuffle():","719","    \"\"\"Following test case was designed this way to verify the code","720","    changes made in pull request: #7506.\"\"\"","721","    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [11, 12], [13, 14], [15, 16],","722","                 [17, 18], [19, 20], [7, 8], [9, 10], [11, 12], [13, 14],","723","                 [15, 16], [17, 18]])","724","    y = np.array([1, 1, 1, 2, 3, 4, 1, 1, 2, 3, 4, 1, 2, 3, 4])","725","    groups = np.array([1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 4, 4, 4, 4])","726","    estimator = PassiveAggressiveClassifier(shuffle=False)","727","","728","    cv = GroupKFold(n_splits=2)","729","    train_sizes_batch, train_scores_batch, test_scores_batch = learning_curve(","730","        estimator, X, y, cv=cv, n_jobs=1, train_sizes=np.linspace(0.3, 1.0, 3),","731","        groups=groups, shuffle=True, random_state=2)","732","    assert_array_almost_equal(train_scores_batch.mean(axis=1),","733","                              np.array([0.75, 0.3, 0.36111111]))","734","    assert_array_almost_equal(test_scores_batch.mean(axis=1),","735","                              np.array([0.36111111, 0.25, 0.25]))","736","    assert_raises(ValueError, learning_curve, estimator, X, y, cv=cv, n_jobs=1,","737","                  train_sizes=np.linspace(0.3, 1.0, 3), groups=groups)","738","","739","    train_sizes_inc, train_scores_inc, test_scores_inc = learning_curve(","740","        estimator, X, y, cv=cv, n_jobs=1, train_sizes=np.linspace(0.3, 1.0, 3),","741","        groups=groups, shuffle=True, random_state=2,","742","        exploit_incremental_learning=True)","743","    assert_array_almost_equal(train_scores_inc.mean(axis=1),","744","                              train_scores_batch.mean(axis=1))","745","    assert_array_almost_equal(test_scores_inc.mean(axis=1),","746","                              test_scores_batch.mean(axis=1))","747","","748",""],"delete":["562","    with warnings.catch_warnings(record=True) as w:","563","        train_sizes, train_scores, test_scores = learning_curve(","564","            estimator, X, y, cv=3, train_sizes=np.linspace(0.1, 1.0, 10))","565","    if len(w) > 0:","566","        raise RuntimeError(\"Unexpected warning: %r\" % w[0].message)","567","    assert_equal(train_scores.shape, (10, 3))","568","    assert_equal(test_scores.shape, (10, 3))","569","    assert_array_equal(train_sizes, np.linspace(2, 20, 10))","570","    assert_array_almost_equal(train_scores.mean(axis=1),","571","                              np.linspace(1.9, 1.0, 10))","572","    assert_array_almost_equal(test_scores.mean(axis=1),","573","                              np.linspace(0.1, 1.0, 10))","624","    train_sizes, train_scores, test_scores = learning_curve(","625","        estimator, X, y, cv=3, exploit_incremental_learning=True,","626","        train_sizes=np.linspace(0.1, 1.0, 10))","627","    assert_array_equal(train_sizes, np.linspace(2, 20, 10))","628","    assert_array_almost_equal(train_scores.mean(axis=1),","629","                              np.linspace(1.9, 1.0, 10))","630","    assert_array_almost_equal(test_scores.mean(axis=1),","631","                              np.linspace(0.1, 1.0, 10))"]}],"doc\/whats_new.rst":[{"add":["43","   - Added ``shuffle`` and ``random_state`` parameters to shuffle training","44","     data before taking prefixes of it based on training sizes in","45","     :func:`model_selection.learning_curve`.","46","     (`#7506` <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/7506>_) by","47","     `Narine Kokhlikyan`_.","48","","4869","","4870",".. _Narine Kokhlikyan: https:\/\/github.com\/NarineK"],"delete":[]}]}},"fd84a567b42eb46b6f8a78b572a3d48142f25e2c":{"changes":{"sklearn\/model_selection\/_validation.py":"MODIFY","sklearn\/model_selection\/tests\/test_validation.py":"MODIFY","doc\/whats_new.rst":"MODIFY"},"diff":{"sklearn\/model_selection\/_validation.py":[{"add":["30","from ..preprocessing import LabelEncoder","367","        Invokes the passed method name of the passed estimator. For","368","        method='predict_proba', the columns correspond to the classes","369","        in sorted order.","395","    if method in ['decision_function', 'predict_proba', 'predict_log_proba']:","396","        le = LabelEncoder()","397","        y = le.fit_transform(y)","398","","481","    if method in ['decision_function', 'predict_proba', 'predict_log_proba']:","482","        n_classes = len(set(y))","483","        predictions_ = np.zeros((X_test.shape[0], n_classes))","484","        if method == 'decision_function' and len(estimator.classes_) == 2:","485","            predictions_[:, estimator.classes_[-1]] = predictions","486","        else:","487","            predictions_[:, estimator.classes_] = predictions","488","        predictions = predictions_"],"delete":["366","        Invokes the passed method name of the passed estimator."]}],"sklearn\/model_selection\/tests\/test_validation.py":[{"add":["53","from sklearn.preprocessing import LabelEncoder","943","        # Test alternative representations of y","944","        predictions_y1 = cross_val_predict(est, X, y + 1, method=method,","945","                                           cv=kfold)","946","        assert_array_equal(predictions, predictions_y1)","947","","948","        predictions_y2 = cross_val_predict(est, X, y - 2, method=method,","949","                                           cv=kfold)","950","        assert_array_equal(predictions, predictions_y2)","951","","952","        predictions_ystr = cross_val_predict(est, X, y.astype('str'),","953","                                             method=method, cv=kfold)","954","        assert_array_equal(predictions, predictions_ystr)","955","","956","","957","def get_expected_predictions(X, y, cv, classes, est, method):","958","","959","    expected_predictions = np.zeros([len(y), classes])","960","    func = getattr(est, method)","961","","962","    for train, test in cv.split(X, y):","963","        est.fit(X[train], y[train])","964","        expected_predictions_ = func(X[test])","965","        # To avoid 2 dimensional indexing","966","        exp_pred_test = np.zeros((len(test), classes))","967","        if method is 'decision_function' and len(est.classes_) == 2:","968","            exp_pred_test[:, est.classes_[-1]] = expected_predictions_","969","        else:","970","            exp_pred_test[:, est.classes_] = expected_predictions_","971","        expected_predictions[test] = exp_pred_test","972","","973","    return expected_predictions","974","","975","","976","def test_cross_val_predict_class_subset():","977","","978","    X = np.arange(8).reshape(4, 2)","979","    y = np.array([0, 0, 1, 2])","980","    classes = 3","981","","982","    kfold3 = KFold(n_splits=3)","983","    kfold4 = KFold(n_splits=4)","984","","985","    le = LabelEncoder()","986","","987","    methods = ['decision_function', 'predict_proba', 'predict_log_proba']","988","    for method in methods:","989","        est = LogisticRegression()","990","","991","        # Test with n_splits=3","992","        predictions = cross_val_predict(est, X, y, method=method,","993","                                        cv=kfold3)","994","","995","        # Runs a naive loop (should be same as cross_val_predict):","996","        expected_predictions = get_expected_predictions(X, y, kfold3, classes,","997","                                                        est, method)","998","        assert_array_almost_equal(expected_predictions, predictions)","999","","1000","        # Test with n_splits=4","1001","        predictions = cross_val_predict(est, X, y, method=method,","1002","                                        cv=kfold4)","1003","        expected_predictions = get_expected_predictions(X, y, kfold4, classes,","1004","                                                        est, method)","1005","        assert_array_almost_equal(expected_predictions, predictions)","1006","","1007","        # Testing unordered labels","1008","        y = [1, 1, -4, 6]","1009","        predictions = cross_val_predict(est, X, y, method=method,","1010","                                        cv=kfold3)","1011","        y = le.fit_transform(y)","1012","        expected_predictions = get_expected_predictions(X, y, kfold3, classes,","1013","                                                        est, method)","1014","        assert_array_almost_equal(expected_predictions, predictions)","1015",""],"delete":[]}],"doc\/whats_new.rst":[{"add":["112","   - :func:`model_selection.cross_val_predict` now returns output of the","113","     correct shape for all values of the argument ``method``.","114","     :issue:`7863` by :user:`Aman Dalmia <dalmia>`.","115",""],"delete":[]}]}},"99342b6da44f7a48e51280c0d913e908c36215da":{"changes":{"README.rst":"MODIFY"},"diff":{"README.rst":[{"add":["1","","164","~~~~~~~~","165",""],"delete":["163","~~~~~~~~~~~~~"]}]}},"32236ff9a52e83e4c6e9984bdefc76082af537d5":{"changes":{"doc\/modules\/linear_model.rst":"MODIFY","sklearn\/linear_model\/logistic.py":"MODIFY","sklearn\/linear_model\/sag.py":"MODIFY"},"diff":{"doc\/modules\/linear_model.rst":[{"add":["791","    .. [6] Mark Schmidt, Nicolas Le Roux, and Francis Bach: `Minimizing Finite Sums with the Stochastic Average Gradient. <https:\/\/hal.inria.fr\/hal-00860051\/document>`_"],"delete":["791","    .. [6] Mark Schmidt, Nicolas Le Roux, and Francis Bach: `Minimizing Finite Sums with the Stochastic Average Gradient. <http:\/\/hal.inria.fr\/hal-00860051\/PDF\/sag_journal.pdf>`_"]}],"sklearn\/linear_model\/logistic.py":[{"add":["1118","    SAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach","1119","        Minimizing Finite Sums with the Stochastic Average Gradient","1120","        https:\/\/hal.inria.fr\/hal-00860051\/document","1121","","1661","                                                len(self.Cs_)))","1667","                                                   len(self.Cs_), -1))","1669","                                                len(self.Cs_)))"],"delete":["1657","                                      len(self.Cs_)))","1663","                                     len(self.Cs_), -1))","1665","                                      len(self.Cs_)))"]}],"sklearn\/linear_model\/sag.py":[{"add":["47","    https:\/\/hal.inria.fr\/hal-00860051\/document","188","    https:\/\/hal.inria.fr\/hal-00860051\/document"],"delete":["47","    https:\/\/hal.inria.fr\/hal-00860051\/PDF\/sag_journal.pdf","188","    https:\/\/hal.inria.fr\/hal-00860051\/PDF\/sag_journal.pdf"]}]}},"3a106fc792eb8e70e1fd078e351ba42487d3214d":{"changes":{"sklearn\/feature_selection\/rfe.py":"MODIFY","sklearn\/feature_selection\/from_model.py":"MODIFY"},"diff":{"sklearn\/feature_selection\/rfe.py":[{"add":["174","                coefs = getattr(estimator, 'feature_importances_', None)","175","            if coefs is None:"],"delete":["173","            elif hasattr(estimator, 'feature_importances_'):","174","                coefs = estimator.feature_importances_"]}],"sklearn\/feature_selection\/from_model.py":[{"add":["16","    importances = getattr(estimator, \"feature_importances_\", None)","18","    if importances is None and hasattr(estimator, \"coef_\"):","25","    elif importances is None:"],"delete":["16","    if hasattr(estimator, \"feature_importances_\"):","17","        importances = estimator.feature_importances_","19","    elif hasattr(estimator, \"coef_\"):","26","    else:"]}]}},"915458bb71e7892c271235cd65e550c1983bd6b9":{"changes":{"sklearn\/linear_model\/tests\/test_coordinate_descent.py":"MODIFY","sklearn\/linear_model\/tests\/test_least_angle.py":"MODIFY","sklearn\/linear_model\/tests\/test_sgd.py":"MODIFY","sklearn\/linear_model\/tests\/test_ransac.py":"MODIFY"},"diff":{"sklearn\/linear_model\/tests\/test_coordinate_descent.py":[{"add":["439","    rng = np.random.RandomState(0)","440","    X = rng.randn(10, 2)","441","    y = rng.randn(10, 2)"],"delete":["439","    X = np.random.randn(10, 2)","440","    y = np.random.randn(10, 2)"]}],"sklearn\/linear_model\/tests\/test_least_angle.py":[{"add":["109","    rng = np.random.RandomState(0)","118","    X = rng.rand(n_samples, 5)"],"delete":["117","    X = np.random.rand(n_samples, 5)"]}],"sklearn\/linear_model\/tests\/test_sgd.py":[{"add":["613","        rng = np.random.RandomState(0)","614","        sample_weights = rng.random_sample(Y4.shape[0])","963","        rng = np.random.RandomState(0)","977","        y = 0.5 * X.ravel() + rng.randn(n_samples, 1).ravel()","1015","        X = rng.randn(n_samples, n_features)"],"delete":["613","        sample_weights = np.random.random(Y4.shape[0])","975","        y = 0.5 * X.ravel() \\","976","            + np.random.randn(n_samples, 1).ravel()","1014","        X = np.random.randn(n_samples, n_features)"]}],"sklearn\/linear_model\/tests\/test_ransac.py":[{"add":["56","    rng = np.random.RandomState(0)","57","    X = rng.rand(10, 2)","58","    y = rng.rand(10, 1)"],"delete":["56","    X = np.random.rand(10, 2)","57","    y = np.random.rand(10, 1)"]}]}},"9667ff292f1ace46bc2b1672836a3a04bd9fb5c0":{"changes":{"doc\/whats_new.rst":"MODIFY","sklearn\/feature_selection\/from_model.py":"MODIFY","sklearn\/feature_selection\/tests\/test_from_model.py":"MODIFY"},"diff":{"doc\/whats_new.rst":[{"add":["143","   - Fixed setting parameters when calling ``fit`` multiple times on","144","     :class:`feature_selection.SelectFromModel`. :issue:`7756` by `Andreas Mller`_","145",""],"delete":[]}],"sklearn\/feature_selection\/from_model.py":[{"add":["234","        self.estimator_ = clone(self.estimator)"],"delete":["234","        if not hasattr(self, \"estimator_\"):","235","            self.estimator_ = clone(self.estimator)"]}],"sklearn\/feature_selection\/tests\/test_from_model.py":[{"add":["4","from sklearn.utils.testing import assert_equal","147","def test_calling_fit_reinitializes():","148","    est = LinearSVC(random_state=0)","151","    transformer.set_params(estimator__C=100)","153","    assert_equal(transformer.estimator_.C, 100)"],"delete":["146","def test_warm_start():","147","    est = PassiveAggressiveClassifier(warm_start=True, random_state=0)","150","    old_model = transformer.estimator_","152","    new_model = transformer.estimator_","153","    assert_true(old_model is new_model)"]}]}},"726c8d999a0485a3319cb32aecd0cac25dd3cc53":{"changes":{"examples\/svm\/plot_svm_margin.py":"MODIFY"},"diff":{"examples\/svm\/plot_svm_margin.py":[{"add":["49","    # support vectors (margin away from hyperplane in direction","50","    # perpendicular to hyperplane). This is sqrt(1+a^2) away vertically in","51","    # 2-d.","53","    yy_down = yy - np.sqrt(1 + a ** 2) * margin","54","    yy_up = yy + np.sqrt(1 + a ** 2) * margin"],"delete":["49","    # support vectors","51","    yy_down = yy + a * margin","52","    yy_up = yy - a * margin"]}]}},"5c4b1bb23192a137ac22ced229c50d6b69859ac6":{"changes":{"sklearn\/__init__.py":"MODIFY","sklearn\/tests\/test_multiclass.py":"MODIFY","sklearn\/utils\/testing.py":"MODIFY","sklearn\/utils\/class_weight.py":"MODIFY","doc\/modules\/pipeline.rst":"MODIFY","doc\/tutorial\/basic\/tutorial.rst":"MODIFY","sklearn\/svm\/classes.py":"MODIFY","sklearn\/preprocessing\/tests\/test_data.py":"MODIFY","sklearn\/model_selection\/_search.py":"MODIFY","sklearn\/tree\/tree.py":"MODIFY","sklearn\/discriminant_analysis.py":"MODIFY","sklearn\/linear_model\/logistic.py":"MODIFY","sklearn\/ensemble\/tests\/test_forest.py":"MODIFY","sklearn\/utils\/__init__.py":"MODIFY","doc\/modules\/model_persistence.rst":"MODIFY","sklearn\/ensemble\/forest.py":"MODIFY","sklearn\/tests\/test_pipeline.py":"MODIFY","sklearn\/datasets\/tests\/test_lfw.py":"MODIFY","sklearn\/ensemble\/iforest.py":"MODIFY","sklearn\/linear_model\/tests\/test_least_angle.py":"MODIFY","sklearn\/utils\/validation.py":"MODIFY","sklearn\/utils\/tests\/test_class_weight.py":"MODIFY","sklearn\/linear_model\/perceptron.py":"MODIFY","sklearn\/pipeline.py":"MODIFY","sklearn\/svm\/base.py":"MODIFY","sklearn\/grid_search.py":"MODIFY","sklearn\/utils\/tests\/test_validation.py":"MODIFY","sklearn\/datasets\/lfw.py":"MODIFY","sklearn\/linear_model\/tests\/test_logistic.py":"MODIFY","sklearn\/base.py":"MODIFY","sklearn\/tests\/test_discriminant_analysis.py":"MODIFY","sklearn\/linear_model\/coordinate_descent.py":"MODIFY","sklearn\/decomposition\/nmf.py":"MODIFY","sklearn\/feature_selection\/tests\/test_from_model.py":"MODIFY","sklearn\/datasets\/__init__.py":"MODIFY","doc\/tutorial\/statistical_inference\/supervised_learning.rst":"MODIFY","doc\/modules\/svm.rst":"MODIFY","sklearn\/covariance\/tests\/test_robust_covariance.py":"MODIFY","sklearn\/feature_selection\/from_model.py":"MODIFY","sklearn\/svm\/tests\/test_svm.py":"MODIFY","sklearn\/ensemble\/tests\/test_gradient_boosting.py":"MODIFY","sklearn\/tree\/tests\/test_tree.py":"MODIFY","sklearn\/metrics\/regression.py":"MODIFY","sklearn\/metrics\/base.py":"MODIFY","doc\/modules\/classes.rst":"MODIFY","sklearn\/utils\/estimator_checks.py":"MODIFY","sklearn\/linear_model\/base.py":"MODIFY","sklearn\/preprocessing\/data.py":"MODIFY","sklearn\/decomposition\/tests\/test_nmf.py":"MODIFY","\/dev\/null":"DELETE","sklearn\/linear_model\/stochastic_gradient.py":"MODIFY","sklearn\/decomposition\/__init__.py":"MODIFY","sklearn\/ensemble\/gradient_boosting.py":"MODIFY"},"diff":{"sklearn\/__init__.py":[{"add":["64","               'learning_curve', 'linear_model', 'manifold', 'metrics',","67","               'preprocessing', 'random_projection', 'semi_supervised',"],"delete":["64","               'lda', 'learning_curve', 'linear_model', 'manifold', 'metrics',","67","               'preprocessing', 'qda', 'random_projection', 'semi_supervised',"]}],"sklearn\/tests\/test_multiclass.py":[{"add":[],"delete":["336","        assert_true(hasattr(decision_only, 'decision_function'))","372","    assert_true(hasattr(decision_only, 'decision_function'))"]}],"sklearn\/utils\/testing.py":[{"add":[],"delete":["17","import re","18","import platform","638","def if_not_mac_os(versions=('10.7', '10.8', '10.9'),","639","                  message='Multi-process bug in Mac OS X >= 10.7 '","640","                          '(see issue #636)'):","641","    \"\"\"Test decorator that skips test if OS is Mac OS X and its","642","    major version is one of ``versions``.","643","    \"\"\"","644","    warnings.warn(\"if_not_mac_os is deprecated in 0.17 and will be removed\"","645","                  \" in 0.19: use the safer and more generic\"","646","                  \" if_safe_multiprocessing_with_blas instead\",","647","                  DeprecationWarning)","648","    mac_version, _, _ = platform.mac_ver()","649","    skip = '.'.join(mac_version.split('.')[:2]) in versions","650","","651","    def decorator(func):","652","        if skip:","653","            @wraps(func)","654","            def func(*args, **kwargs):","655","                raise SkipTest(message)","656","        return func","657","    return decorator","658","","659",""]}],"sklearn\/utils\/class_weight.py":[{"add":["49","    elif class_weight == 'balanced':","56","        recip_freq = len(y) \/ (len(le.classes_) *","57","                               bincount(y_ind).astype(np.float64))","58","        weight = recip_freq[le.transform(classes)]","99","        sample weight will be calculated over the full sample. Only \"balanced\"","100","        is supported for class_weight if this is provided.","114","        if class_weight not in ['balanced']:","137","        if class_weight == 'balanced' or n_outputs == 1:"],"delete":["4","import warnings","50","    elif class_weight in ['auto', 'balanced']:","57","        # inversely proportional to the number of samples in the class","58","        if class_weight == 'auto':","59","            recip_freq = 1. \/ bincount(y_ind)","60","            weight = recip_freq[le.transform(classes)] \/ np.mean(recip_freq)","61","            warnings.warn(\"The class_weight='auto' heuristic is deprecated in\"","62","                          \" 0.17 in favor of a new heuristic \"","63","                          \"class_weight='balanced'. 'auto' will be removed in\"","64","                          \" 0.19\", DeprecationWarning)","65","        else:","66","            recip_freq = len(y) \/ (len(le.classes_) *","67","                                   bincount(y_ind).astype(np.float64))","68","            weight = recip_freq[le.transform(classes)]","109","        sample weight will be calculated over the full sample. Only \"auto\" is","110","        supported for class_weight if this is provided.","124","        if class_weight not in ['balanced', 'auto']:","147","        if class_weight in ['balanced', 'auto'] or n_outputs == 1:"]}],"doc\/modules\/pipeline.rst":[{"add":["45","    coef0=0.0, decision_function_shape='ovr', degree=3, gamma='auto',","82","        coef0=0.0, decision_function_shape='ovr', degree=3, gamma='auto',"],"delete":["45","    coef0=0.0, decision_function_shape=None, degree=3, gamma='auto',","82","        coef0=0.0, decision_function_shape=None, degree=3, gamma='auto',"]}],"doc\/tutorial\/basic\/tutorial.rst":[{"add":["182","    decision_function_shape='ovr', degree=3, gamma=0.001, kernel='rbf',","221","    decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',","295","      decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',","304","      decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',","333","    decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',","341","    decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',"],"delete":["182","    decision_function_shape=None, degree=3, gamma=0.001, kernel='rbf',","221","    decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',","295","      decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',","304","      decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',","333","    decision_function_shape=None, degree=3, gamma='auto', kernel='linear',","341","    decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',"]}],"sklearn\/svm\/classes.py":[{"add":["13","                SparseCoefMixin):","51","        ``\"ovr\"`` trains n_classes one-vs-rest classifiers, while","52","        ``\"crammer_singer\"`` optimizes a joint objective over all classes.","56","        If ``\"crammer_singer\"`` is chosen, the options loss, penalty and dual","57","        will be ignored.","455","    decision_function_shape : 'ovo', 'ovr', default='ovr'","460","","461","        .. versionchanged:: 0.19","462","            decision_function_shape is 'ovr' by default.","511","        decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',","532","                 verbose=False, max_iter=-1, decision_function_shape='ovr',","596","        weight one. The \"balanced\" mode uses the values of y to automatically","597","        adjust weights inversely proportional to class frequencies as","608","    decision_function_shape : 'ovo', 'ovr', default='ovr'","613","","614","        .. versionchanged:: 0.19","615","            decision_function_shape is 'ovr' by default.","664","          decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',","680","    def __init__(self, nu=0.5, kernel='rbf', degree=3, gamma='auto', coef0=0.0,","681","                 shrinking=True, probability=False, tol=1e-3, cache_size=200,","682","                 class_weight=None, verbose=False, max_iter=-1,","683","                 decision_function_shape='ovr', random_state=None):"],"delete":["7","from ..feature_selection.from_model import _LearntSelectorMixin","14","                _LearntSelectorMixin, SparseCoefMixin):","52","        ``\"ovr\"`` trains n_classes one-vs-rest classifiers, while ``\"crammer_singer\"``","53","        optimizes a joint objective over all classes.","57","        If ``\"crammer_singer\"`` is chosen, the options loss, penalty and dual will","58","        be ignored.","456","    decision_function_shape : 'ovo', 'ovr' or None, default=None","461","        The default of None will currently behave as 'ovo' for backward","462","        compatibility and raise a deprecation warning, but will change 'ovr'","463","        in 0.19.","512","        decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',","533","                 verbose=False, max_iter=-1, decision_function_shape=None,","597","        weight one. The \"balanced\" mode uses the values of y to automatically adjust","598","        weights inversely proportional to class frequencies as","609","    decision_function_shape : 'ovo', 'ovr' or None, default=None","614","        The default of None will currently behave as 'ovo' for backward","615","        compatibility and raise a deprecation warning, but will change 'ovr'","616","        in 0.19.","665","          decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',","681","    def __init__(self, nu=0.5, kernel='rbf', degree=3, gamma='auto',","682","                 coef0=0.0, shrinking=True, probability=False,","683","                 tol=1e-3, cache_size=200, class_weight=None, verbose=False,","684","                 max_iter=-1, decision_function_shape=None, random_state=None):"]}],"sklearn\/preprocessing\/tests\/test_data.py":[{"add":["791","    X = [[np.nan, 5, 6, 7, 8]]","796","    X = [[np.inf, 5, 6, 7, 8]]"],"delete":["29","from sklearn.utils.testing import ignore_warnings","792","    X = [np.nan, 5, 6, 7, 8]","797","    X = [np.inf, 5, 6, 7, 8]","1021","def test_deprecation_minmax_scaler():","1022","    rng = np.random.RandomState(0)","1023","    X = rng.random_sample((5, 4))","1024","    scaler = MinMaxScaler().fit(X)","1025","","1026","    depr_message = (\"Attribute data_range will be removed in \"","1027","                    \"0.19. Use ``data_range_`` instead\")","1028","    assert_warns_message(DeprecationWarning, depr_message, getattr, scaler,","1029","                         \"data_range\")","1030","","1031","    depr_message = (\"Attribute data_min will be removed in \"","1032","                    \"0.19. Use ``data_min_`` instead\")","1033","    assert_warns_message(DeprecationWarning, depr_message, getattr, scaler,","1034","                         \"data_min\")","1035","","1036","","1445","def test_deprecation_standard_scaler():","1446","    rng = np.random.RandomState(0)","1447","    X = rng.random_sample((5, 4))","1448","    scaler = StandardScaler().fit(X)","1449","    depr_message = (\"Function std_ is deprecated; Attribute ``std_`` will be \"","1450","                    \"removed in 0.19. Use ``scale_`` instead\")","1451","    std_ = assert_warns_message(DeprecationWarning, depr_message, getattr,","1452","                                scaler, \"std_\")","1453","    assert_array_equal(std_, scaler.scale_)","1454","","1455",""]}],"sklearn\/model_selection\/_search.py":[{"add":["806","                         decision_function_shape='ovr', degree=..., gamma=...,"],"delete":["806","                         decision_function_shape=None, degree=..., gamma=...,"]}],"sklearn\/tree\/tree.py":[{"add":["72","class BaseDecisionTree(six.with_metaclass(ABCMeta, BaseEstimator)):"],"delete":["31","from ..feature_selection.from_model import _LearntSelectorMixin","73","class BaseDecisionTree(six.with_metaclass(ABCMeta, BaseEstimator,","74","                                          _LearntSelectorMixin)):","741",""]}],"sklearn\/discriminant_analysis.py":[{"add":["58","            # rescale","59","            s = sc.scale_[:, np.newaxis] * s * sc.scale_[np.newaxis, :]","410","    def fit(self, X, y):","414","           .. versionchanged:: 0.19","415","              *store_covariance* has been moved to main constructor.","417","           .. versionchanged:: 0.19","418","              *tol* has been moved to main constructor.","619","    def fit(self, X, y):","622","            .. versionchanged:: 0.19","623","               *store_covariance* has been moved to main constructor.","625","            .. versionchanged:: 0.19","626","               *tol* has been moved to main constructor."],"delete":["58","            s = sc.scale_[:, np.newaxis] * s * sc.scale_[np.newaxis, :]  # rescale","409","    def fit(self, X, y, store_covariance=None, tol=None):","413","           .. versionchanged:: 0.17","414","              Deprecated *store_covariance* have been moved to main constructor.","416","           .. versionchanged:: 0.17","417","              Deprecated *tol* have been moved to main constructor.","427","        if store_covariance:","428","            warnings.warn(\"The parameter 'store_covariance' is deprecated as \"","429","                          \"of version 0.17 and will be removed in 0.19. The \"","430","                          \"parameter is no longer necessary because the value \"","431","                          \"is set via the estimator initialisation or \"","432","                          \"set_params method.\", DeprecationWarning)","433","            self.store_covariance = store_covariance","434","        if tol:","435","            warnings.warn(\"The parameter 'tol' is deprecated as of version \"","436","                          \"0.17 and will be removed in 0.19. The parameter is \"","437","                          \"no longer necessary because the value is set via \"","438","                          \"the estimator initialisation or set_params method.\",","439","                          DeprecationWarning)","440","            self.tol = tol","632","    def fit(self, X, y, store_covariances=None, tol=None):","635","            .. versionchanged:: 0.17","636","               Deprecated *store_covariance* have been moved to main constructor.","638","            .. versionchanged:: 0.17","639","               Deprecated *tol* have been moved to main constructor.","650","        if store_covariances:","651","            warnings.warn(\"The parameter 'store_covariances' is deprecated as \"","652","                          \"of version 0.17 and will be removed in 0.19. The \"","653","                          \"parameter is no longer necessary because the value \"","654","                          \"is set via the estimator initialisation or \"","655","                          \"set_params method.\", DeprecationWarning)","656","            self.store_covariances = store_covariances","657","        if tol:","658","            warnings.warn(\"The parameter 'tol' is deprecated as of version \"","659","                          \"0.17 and will be removed in 0.19. The parameter is \"","660","                          \"no longer necessary because the value is set via \"","661","                          \"the estimator initialisation or set_params method.\",","662","                          DeprecationWarning)","663","            self.tol = tol"]}],"sklearn\/linear_model\/logistic.py":[{"add":["446","                             solver='lbfgs', coef=None,","577","    .. versionchanged:: 0.19","578","        The \"copy\" parameter was removed.","579","    \"\"\"","586","    if check_input:","588","        y = check_array(y, ensure_2d=False, dtype=None)","627","        if class_weight == \"balanced\":","939","                         SparseCoefMixin):","1005","           *class_weight='balanced'*","1231","                      verbose=self.verbose, solver=self.solver,","1306","                           LinearClassifierMixin):","1597","        if class_weight == \"balanced\":","1691","                    penalty=self.penalty,"],"delete":["19","from ..feature_selection.from_model import _LearntSelectorMixin","447","                             solver='lbfgs', coef=None, copy=False,","504","    copy : bool, default False","505","        Whether or not to produce a copy of the data. A copy is not required","506","        anymore. This parameter is deprecated and will be removed in 0.19.","507","","581","    \"\"\"","582","    if copy:","583","        warnings.warn(\"A copy is not required anymore. The 'copy' parameter \"","584","                      \"is deprecated and will be removed in 0.19.\",","585","                      DeprecationWarning)","593","    if check_input or copy:","595","        y = check_array(y, ensure_2d=False, copy=copy, dtype=None)","634","        # 'auto' is deprecated and will be removed in 0.19","635","        if class_weight in (\"auto\", \"balanced\"):","947","                         _LearntSelectorMixin, SparseCoefMixin):","1013","           *class_weight='balanced'* instead of deprecated","1014","           *class_weight='auto'*.","1240","                      verbose=self.verbose, solver=self.solver, copy=False,","1315","                           LinearClassifierMixin, _LearntSelectorMixin):","1561","        if class_weight and not(isinstance(class_weight, dict) or","1562","                                class_weight in ['balanced', 'auto']):","1563","            # 'auto' is deprecated and will be removed in 0.19","1564","            raise ValueError(\"class_weight provided should be a \"","1565","                             \"dict or 'balanced'\")","1611","        if class_weight in (\"auto\", \"balanced\"):","1705","                    penalty=self.penalty, copy=False,"]}],"sklearn\/ensemble\/tests\/test_forest.py":[{"add":["964","    # smoke test for balanced subsample","985","    clf = ForestClassifier(class_weight='balanced', warm_start=True,"],"delete":["210","    # XXX: Remove this test in 0.19 after transform support to estimators","211","    # is removed.","212","    X_new = assert_warns(","213","        DeprecationWarning, est.transform, X, threshold=\"mean\")","214","    assert_less(0 < X_new.shape[1], X.shape[1])","215","","970","    # smoke test for subsample and balanced subsample","973","    clf = ForestClassifier(class_weight='subsample', random_state=0)","974","    ignore_warnings(clf.fit)(X, _y)","993","    clf = ForestClassifier(class_weight='auto', warm_start=True,"]}],"sklearn\/utils\/__init__.py":[{"add":["18","from .deprecation import deprecated","27","           \"check_symmetric\", \"indices_to_mask\", \"deprecated\"]"],"delete":["15","from .deprecation import deprecated","18","from ..exceptions import ConvergenceWarning as _ConvergenceWarning","20","","21","","22","@deprecated(\"ConvergenceWarning has been moved into the sklearn.exceptions \"","23","            \"module. It will not be available here from version 0.19\")","24","class ConvergenceWarning(_ConvergenceWarning):","25","    pass","34","           \"check_symmetric\", \"indices_to_mask\"]"]}],"doc\/modules\/model_persistence.rst":[{"add":["25","      decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',"],"delete":["25","      decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',"]}],"sklearn\/ensemble\/forest.py":[{"add":["125","class BaseForest(six.with_metaclass(ABCMeta, BaseEnsemble)):","473","            valid_presets = ('balanced', 'balanced_subsample')","490","            if (self.class_weight != 'balanced_subsample' or","492","                if self.class_weight == \"balanced_subsample\":","496","                expanded_class_weight = compute_sample_weight(class_weight,","497","                                                              y_original)","1675","        X = check_array(X, accept_sparse=['csc'])"],"delete":["55","from ..feature_selection.from_model import _LearntSelectorMixin","126","class BaseForest(six.with_metaclass(ABCMeta, BaseEnsemble,","127","                                    _LearntSelectorMixin)):","475","            valid_presets = ('auto', 'balanced', 'subsample', 'balanced_subsample')","481","                if self.class_weight == \"subsample\":","482","                    warn(\"class_weight='subsample' is deprecated in 0.17 and\"","483","                         \"will be removed in 0.19. It was replaced by \"","484","                         \"class_weight='balanced_subsample' using the balanced\"","485","                         \"strategy.\", DeprecationWarning)","497","            if (self.class_weight not in ['subsample', 'balanced_subsample'] or","499","                if self.class_weight == 'subsample':","500","                    class_weight = 'auto'","501","                elif self.class_weight == \"balanced_subsample\":","505","                with warnings.catch_warnings():","506","                    if class_weight == \"auto\":","507","                        warnings.simplefilter('ignore', DeprecationWarning)","508","                    expanded_class_weight = compute_sample_weight(class_weight,","509","                                                                  y_original)","1687","        # ensure_2d=False because there are actually unit test checking we fail","1688","        # for 1d.","1689","        X = check_array(X, accept_sparse=['csc'], ensure_2d=False)"]}],"sklearn\/tests\/test_pipeline.py":[{"add":[],"delete":["15","from sklearn.utils.testing import assert_warns_message","702","def test_X1d_inverse_transform():","703","    transformer = Transf()","704","    pipeline = make_pipeline(transformer)","705","    X = np.ones(10)","706","    msg = \"1d X will not be reshaped in pipeline.inverse_transform\"","707","    assert_warns_message(FutureWarning, msg, pipeline.inverse_transform, X)","708","","709",""]}],"sklearn\/datasets\/tests\/test_lfw.py":[{"add":["114","    fetch_lfw_people(data_home=SCIKIT_LEARN_EMPTY_DATA,","115","                     download_if_missing=False)","120","                                  min_faces_per_person=3,","121","                                  download_if_missing=False)","137","    lfw_people = fetch_lfw_people(data_home=SCIKIT_LEARN_DATA, resize=None,","138","                                  slice_=None, color=True,","139","                                  download_if_missing=False)","152","    fetch_lfw_people(data_home=SCIKIT_LEARN_DATA, min_faces_per_person=100,","153","                     download_if_missing=False)","158","    fetch_lfw_pairs(data_home=SCIKIT_LEARN_EMPTY_DATA,","159","                    download_if_missing=False)","163","    lfw_pairs_train = fetch_lfw_pairs(data_home=SCIKIT_LEARN_DATA,","164","                                      download_if_missing=False)","179","    lfw_pairs_train = fetch_lfw_pairs(data_home=SCIKIT_LEARN_DATA, resize=None,","180","                                      slice_=None, color=True,","181","                                      download_if_missing=False)"],"delete":["24","from sklearn.datasets import load_lfw_pairs","25","from sklearn.datasets import load_lfw_people","31","from sklearn.utils.testing import assert_warns_message","117","    fetch_lfw_people(data_home=SCIKIT_LEARN_EMPTY_DATA, download_if_missing=False)","118","","119","","120","def test_load_lfw_people_deprecation():","121","    msg = (\"Function 'load_lfw_people' has been deprecated in 0.17 and will be \"","122","           \"removed in 0.19.\"","123","           \"Use fetch_lfw_people(download_if_missing=False) instead.\")","124","    assert_warns_message(DeprecationWarning, msg, load_lfw_people,","125","                         data_home=SCIKIT_LEARN_DATA)","130","                                  min_faces_per_person=3, download_if_missing=False)","146","    lfw_people = fetch_lfw_people(data_home=SCIKIT_LEARN_DATA,","147","                                  resize=None, slice_=None, color=True, download_if_missing=False)","160","    fetch_lfw_people(data_home=SCIKIT_LEARN_DATA, min_faces_per_person=100, download_if_missing=False)","165","    fetch_lfw_pairs(data_home=SCIKIT_LEARN_EMPTY_DATA, download_if_missing=False)","166","","167","","168","def test_load_lfw_pairs_deprecation():","169","    msg = (\"Function 'load_lfw_pairs' has been deprecated in 0.17 and will be \"","170","           \"removed in 0.19.\"","171","           \"Use fetch_lfw_pairs(download_if_missing=False) instead.\")","172","    assert_warns_message(DeprecationWarning, msg, load_lfw_pairs,","173","                         data_home=SCIKIT_LEARN_DATA)","177","    lfw_pairs_train = fetch_lfw_pairs(data_home=SCIKIT_LEARN_DATA, download_if_missing=False)","192","    lfw_pairs_train = fetch_lfw_pairs(data_home=SCIKIT_LEARN_DATA,","193","                                      resize=None, slice_=None, color=True, download_if_missing=False)"]}],"sklearn\/ensemble\/iforest.py":[{"add":["156","        X = check_array(X, accept_sparse=['csc'])"],"delete":["156","        # ensure_2d=False because there are actually unit test checking we fail","157","        # for 1d.","158","        X = check_array(X, accept_sparse=['csc'], ensure_2d=False)"]}],"sklearn\/linear_model\/tests\/test_least_angle.py":[{"add":[],"delete":["372","        Y_dec = assert_warns(DeprecationWarning, estimator.decision_function, X)","373","        assert_array_almost_equal(Y_pred, Y_dec)"]}],"sklearn\/utils\/validation.py":[{"add":["18","from ..exceptions import NonBLASDotWarning","19","from ..exceptions import NotFittedError","20","from ..exceptions import DataConversionWarning","27","warnings.simplefilter('ignore', NonBLASDotWarning)","295","        Whether to raise a value error if X is not 2d.","367","                raise ValueError(","368","                    \"Got X with X.ndim=1. Reshape your data either using \"","370","                    \"X.reshape(1, -1) if it contains a single sample.\")","404","        warnings.warn(msg, DataConversionWarning)","534","                          DataConversionWarning, stacklevel=2)","664","        raise NotFittedError(msg % {'name': type(estimator).__name__})"],"delete":["18","from .deprecation import deprecated","19","from ..exceptions import DataConversionWarning as _DataConversionWarning","20","from ..exceptions import NonBLASDotWarning as _NonBLASDotWarning","21","from ..exceptions import NotFittedError as _NotFittedError","24","@deprecated(\"DataConversionWarning has been moved into the sklearn.exceptions\"","25","            \" module. It will not be available here from version 0.19\")","26","class DataConversionWarning(_DataConversionWarning):","27","    pass","28","","29","","30","@deprecated(\"NonBLASDotWarning has been moved into the sklearn.exceptions\"","31","            \" module. It will not be available here from version 0.19\")","32","class NonBLASDotWarning(_NonBLASDotWarning):","33","    pass","34","","35","","36","@deprecated(\"NotFittedError has been moved into the sklearn.exceptions module.\"","37","            \" It will not be available here from version 0.19\")","38","class NotFittedError(_NotFittedError):","39","    pass","40","","45","warnings.simplefilter('ignore', _NonBLASDotWarning)","313","        Whether to make X at least 2d.","385","                if ensure_min_samples >= 2:","386","                    raise ValueError(\"%s expects at least 2 samples provided \"","387","                                     \"in a 2 dimensional array-like input\"","388","                                     % estimator_name)","389","                warnings.warn(","390","                    \"Passing 1d arrays as data is deprecated in 0.17 and will \"","391","                    \"raise ValueError in 0.19. Reshape your data either using \"","393","                    \"X.reshape(1, -1) if it contains a single sample.\",","394","                    DeprecationWarning)","428","        warnings.warn(msg, _DataConversionWarning)","558","                          _DataConversionWarning, stacklevel=2)","688","        # FIXME NotFittedError_ --> NotFittedError in 0.19","689","        raise _NotFittedError(msg % {'name': type(estimator).__name__})"]}],"sklearn\/utils\/tests\/test_class_weight.py":[{"add":["89","def test_compute_class_weight_balanced_negative():","109","def test_compute_class_weight_balanced_unordered():","139","    expected_balanced = np.array([0.7777, 0.7777, 0.7777, 0.7777, 0.7777,","140","                                  0.7777, 2.3333])","213","    # Not \"balanced\" for subsample"],"delete":["14","from sklearn.utils.testing import assert_warns","21","    cw = assert_warns(DeprecationWarning,","22","                      compute_class_weight, \"auto\", classes, y)","23","    assert_almost_equal(cw.sum(), classes.shape)","24","    assert_true(cw[0] < cw[1] < cw[2])","37","    assert_raises(ValueError, compute_class_weight, \"auto\", classes, y)","41","    assert_raises(ValueError, compute_class_weight, \"auto\", classes, y)","96","def test_compute_class_weight_auto_negative():","101","    cw = assert_warns(DeprecationWarning, compute_class_weight, \"auto\",","102","                      classes, y)","103","    assert_almost_equal(cw.sum(), classes.shape)","104","    assert_equal(len(cw), len(classes))","105","    assert_array_almost_equal(cw, np.array([1., 1., 1.]))","113","    cw = assert_warns(DeprecationWarning, compute_class_weight, \"auto\",","114","                      classes, y)","115","    assert_almost_equal(cw.sum(), classes.shape)","116","    assert_equal(len(cw), len(classes))","117","    assert_array_almost_equal(cw, np.array([0.545, 1.636, 0.818]), decimal=3)","126","def test_compute_class_weight_auto_unordered():","130","    cw = assert_warns(DeprecationWarning, compute_class_weight, \"auto\",","131","                      classes, y)","132","    assert_almost_equal(cw.sum(), classes.shape)","133","    assert_equal(len(cw), len(classes))","134","    assert_array_almost_equal(cw, np.array([1.636, 0.818, 0.545]), decimal=3)","146","    sample_weight = assert_warns(DeprecationWarning,","147","                                 compute_sample_weight, \"auto\", y)","148","    assert_array_almost_equal(sample_weight, [1., 1., 1., 1., 1., 1.])","158","    sample_weight = assert_warns(DeprecationWarning,","159","                                 compute_sample_weight, \"auto\", y)","160","    assert_array_almost_equal(sample_weight, [1., 1., 1., 1., 1., 1.])","166","    sample_weight = assert_warns(DeprecationWarning,","167","                                 compute_sample_weight, \"auto\", y)","168","    expected_auto = np.asarray([.6, .6, .6, .6, .6, .6, 1.8])","169","    assert_array_almost_equal(sample_weight, expected_auto)","171","    expected_balanced = np.array([0.7777, 0.7777, 0.7777, 0.7777, 0.7777, 0.7777, 2.3333])","180","    sample_weight = assert_warns(DeprecationWarning,","181","                                 compute_sample_weight, \"auto\", y)","182","    assert_array_almost_equal(sample_weight, [1., 1., 1., 1., 1., 1.])","193","    sample_weight = assert_warns(DeprecationWarning,","194","                                 compute_sample_weight, \"auto\", y)","195","    assert_array_almost_equal(sample_weight, expected_auto ** 2)","204","    sample_weight = assert_warns(DeprecationWarning,","205","                                 compute_sample_weight, \"auto\", y)","206","    assert_array_almost_equal(sample_weight, [1., 1., 1., 1., 1., 1.])","212","    sample_weight = assert_warns(DeprecationWarning,","213","                                 compute_sample_weight, \"auto\", y)","214","    assert_array_almost_equal(sample_weight, [1., 1., 1., 1., 1., 1.])","220","    sample_weight = assert_warns(DeprecationWarning,","221","                                 compute_sample_weight, \"auto\", y, range(4))","222","    assert_array_almost_equal(sample_weight, [.5, .5, .5, 1.5, 1.5, 1.5])","229","    sample_weight = assert_warns(DeprecationWarning, compute_sample_weight,","230","                                 \"auto\", y, [0, 1, 1, 2, 2, 3])","231","    expected_auto = np.asarray([1 \/ 3., 1 \/ 3., 1 \/ 3., 5 \/ 3., 5 \/ 3., 5 \/ 3.])","232","    assert_array_almost_equal(sample_weight, expected_auto)","239","    sample_weight = assert_warns(DeprecationWarning, compute_sample_weight,","240","                                 \"auto\", y, [0, 1, 1, 2, 2, 3])","241","    assert_array_almost_equal(sample_weight, expected_auto ** 2)","247","    sample_weight = assert_warns(DeprecationWarning, compute_sample_weight,","248","                                 \"auto\", y, range(6))","249","    assert_array_almost_equal(sample_weight, [1., 1., 1., 1., 1., 1., 0.])","255","    sample_weight = assert_warns(DeprecationWarning, compute_sample_weight,","256","                                 \"auto\", y, range(6))","257","    assert_array_almost_equal(sample_weight, [1., 1., 1., 1., 1., 1., 0.])","272","    # Not \"auto\" for subsample"]}],"sklearn\/linear_model\/perceptron.py":[{"add":["6","class Perceptron(BaseSGDClassifier):"],"delete":["4","from ..feature_selection.from_model import _LearntSelectorMixin","7","class Perceptron(BaseSGDClassifier, _LearntSelectorMixin):"]}],"sklearn\/pipeline.py":[{"add":[],"delete":["12","from warnings import warn","472","        if hasattr(X, 'ndim') and X.ndim == 1:","473","            warn(\"From version 0.19, a 1d X will not be reshaped in\"","474","                 \" pipeline.inverse_transform any more.\", FutureWarning)","475","            X = X[None, :]"]}],"sklearn\/svm\/base.py":[{"add":["14","from ..utils import compute_class_weight"],"delete":["14","from ..utils import compute_class_weight, deprecated","19","from ..exceptions import ChangedBehaviorWarning","370","    @deprecated(\" and will be removed in 0.19\")","371","    def decision_function(self, X):","372","        \"\"\"Distance of the samples X to the separating hyperplane.","373","","374","        Parameters","375","        ----------","376","        X : array-like, shape (n_samples, n_features)","377","            For kernel=\"precomputed\", the expected shape of X is","378","            [n_samples_test, n_samples_train].","379","","380","        Returns","381","        -------","382","        X : array-like, shape (n_samples, n_class * (n_class-1) \/ 2)","383","            Returns the decision function of the sample for each class","384","            in the model.","385","        \"\"\"","386","        return self._decision_function(X)","387","","547","        if self.decision_function_shape is None and len(self.classes_) > 2:","548","            warnings.warn(\"The decision_function_shape default value will \"","549","                          \"change from 'ovo' to 'ovr' in 0.19. This will change \"","550","                          \"the shape of the decision function returned by \"","551","                          \"SVC.\", ChangedBehaviorWarning)"]}],"sklearn\/grid_search.py":[{"add":["743","                         decision_function_shape='ovr', degree=..., gamma=...,"],"delete":["743","                         decision_function_shape=None, degree=..., gamma=...,"]}],"sklearn\/utils\/tests\/test_validation.py":[{"add":[],"delete":["124","    assert_warns(DeprecationWarning, check_array, [0, 1, 2])","125","    X_array = check_array([0, 1, 2])","126","    assert_equal(X_array.ndim, 2)","340","    # But this works if the input data is forced to look like a 2 array with","341","    # one sample and one feature:","342","    X_checked = assert_warns(DeprecationWarning, check_array, [42],","343","                             ensure_2d=True)","344","    assert_array_equal(np.array([[42]]), X_checked)","345",""]}],"sklearn\/datasets\/lfw.py":[{"add":[],"delete":["28","from sklearn.utils import deprecated","29","","378","@deprecated(\"Function 'load_lfw_people' has been deprecated in 0.17 and will \"","379","            \"be removed in 0.19.\"","380","            \"Use fetch_lfw_people(download_if_missing=False) instead.\")","381","def load_lfw_people(download_if_missing=False, **kwargs):","382","    \"\"\"","383","    Alias for fetch_lfw_people(download_if_missing=False)","384","","385","    .. deprecated:: 0.17","386","        This function will be removed in 0.19.","387","        Use :func:`sklearn.datasets.fetch_lfw_people` with parameter","388","        ``download_if_missing=False`` instead.","389","","390","    Check fetch_lfw_people.__doc__ for the documentation and parameter list.","391","    \"\"\"","392","    return fetch_lfw_people(download_if_missing=download_if_missing, **kwargs)","393","","394","","511","","512","","513","@deprecated(\"Function 'load_lfw_pairs' has been deprecated in 0.17 and will \"","514","            \"be removed in 0.19.\"","515","            \"Use fetch_lfw_pairs(download_if_missing=False) instead.\")","516","def load_lfw_pairs(download_if_missing=False, **kwargs):","517","    \"\"\"","518","    Alias for fetch_lfw_pairs(download_if_missing=False)","519","","520","    .. deprecated:: 0.17","521","        This function will be removed in 0.19.","522","        Use :func:`sklearn.datasets.fetch_lfw_pairs` with parameter","523","        ``download_if_missing=False`` instead.","524","","525","    Check fetch_lfw_pairs.__doc__ for the documentation and parameter list.","526","    \"\"\"","527","    return fetch_lfw_pairs(download_if_missing=download_if_missing, **kwargs)"]}],"sklearn\/linear_model\/tests\/test_logistic.py":[{"add":[],"delete":["12","from sklearn.utils.testing import assert_warns_message","735","def test_multinomial_logistic_regression_with_classweight_auto():","736","    X, y = iris.data, iris.target","737","    model = LogisticRegression(multi_class='multinomial',","738","                               class_weight='auto', solver='lbfgs')","739","    # 'auto' is deprecated and will be removed in 0.19","740","    assert_warns_message(DeprecationWarning,","741","                         \"class_weight='auto' heuristic is deprecated\",","742","                         model.fit, X, y)","743","","744",""]}],"sklearn\/base.py":[{"add":[],"delete":["12","from .utils.deprecation import deprecated","13","from .exceptions import ChangedBehaviorWarning as _ChangedBehaviorWarning","17","@deprecated(\"ChangedBehaviorWarning has been moved into the sklearn.exceptions\"","18","            \" module. It will not be available here from version 0.19\")","19","class ChangedBehaviorWarning(_ChangedBehaviorWarning):","20","    pass","21","","22",""]}],"sklearn\/tests\/test_discriminant_analysis.py":[{"add":[],"delete":["0","import sys","13","from sklearn.utils.testing import SkipTest","21","# import reload","22","version = sys.version_info","23","if version[0] == 3:","24","    # Python 3+ import for reload. Builtin in Python2","25","    if version[1] == 3:","26","        reload = None","27","    else:","28","        from importlib import reload","29","","30","","319","def test_deprecated_lda_qda_deprecation():","320","    if reload is None:","321","        raise SkipTest(\"Can't reload module on Python3.3\")","322","","323","    def import_lda_module():","324","        import sklearn.lda","325","        # ensure that we trigger DeprecationWarning even if the sklearn.lda","326","        # was loaded previously by another test.","327","        reload(sklearn.lda)","328","        return sklearn.lda","329","","330","    lda = assert_warns(DeprecationWarning, import_lda_module)","331","    assert isinstance(lda.LDA(), LinearDiscriminantAnalysis)","332","","333","    def import_qda_module():","334","        import sklearn.qda","335","        # ensure that we trigger DeprecationWarning even if the sklearn.qda","336","        # was loaded previously by another test.","337","        reload(sklearn.qda)","338","        return sklearn.qda","339","","340","    qda = assert_warns(DeprecationWarning, import_qda_module)","341","    assert isinstance(qda.QDA(), QuadraticDiscriminantAnalysis)","342","","343",""]}],"sklearn\/linear_model\/coordinate_descent.py":[{"add":["17","from ..utils import check_array, check_X_y"],"delete":["17","from ..utils import check_array, check_X_y, deprecated","748","    @deprecated(\" and will be removed in 0.19\")","749","    def decision_function(self, X):","750","        \"\"\"Decision function of the linear model","751","","752","        Parameters","753","        ----------","754","        X : numpy array or scipy.sparse matrix of shape (n_samples, n_features)","755","","756","        Returns","757","        -------","758","        T : array, shape (n_samples,)","759","            The predicted decision function","760","        \"\"\"","761","        return self._decision_function(X)","762",""]}],"sklearn\/decomposition\/nmf.py":[{"add":["476","                               verbose=0, shuffle=False):","539","    solver : 'cd'","624","    if solver == 'cd':","694","    solver : 'cd'","696","        'cd' is a Coordinate Descent solver.","758","    NMF(alpha=0.0, init='random', l1_ratio=0.0, max_iter=200,","759","      n_components=2, random_state=0, shuffle=False,","760","      solver='cd', tol=0.0001, verbose=0)","780","    def __init__(self, n_components=None, init=None, solver='cd', tol=1e-4,","781","                 max_iter=200, random_state=None, alpha=0., l1_ratio=0.,","782","                 verbose=0, shuffle=False):","823","            shuffle=self.shuffle)","869","            shuffle=self.shuffle)"],"delete":["26","from ..utils import deprecated","54","def _sparseness(x):","55","    \"\"\"Hoyer's measure of sparsity for a vector\"\"\"","56","    sqrt_n = np.sqrt(len(x))","57","    return (sqrt_n - np.linalg.norm(x, 1) \/ norm(x)) \/ (sqrt_n - 1)","58","","59","","82","def _check_string_param(sparseness, solver):","83","    allowed_sparseness = (None, 'data', 'components')","84","    if sparseness not in allowed_sparseness:","85","        raise ValueError(","86","            'Invalid sparseness parameter: got %r instead of one of %r' %","87","            (sparseness, allowed_sparseness))","88","","89","    allowed_solver = ('pg', 'cd')","90","    if solver not in allowed_solver:","91","        raise ValueError(","92","            'Invalid solver parameter: got %r instead of one of %r' %","93","            (solver, allowed_solver))","94","","95","","347","def _update_projected_gradient_w(X, W, H, tolW, nls_max_iter, alpha, l1_ratio,","348","                                 sparseness, beta, eta):","349","    \"\"\"Helper function for _fit_projected_gradient\"\"\"","350","    n_samples, n_features = X.shape","351","    n_components_ = H.shape[0]","352","","353","    if sparseness is None:","354","        Wt, gradW, iterW = _nls_subproblem(X.T, H.T, W.T, tolW, nls_max_iter,","355","                                           alpha=alpha, l1_ratio=l1_ratio)","356","    elif sparseness == 'data':","357","        Wt, gradW, iterW = _nls_subproblem(","358","            safe_vstack([X.T, np.zeros((1, n_samples))]),","359","            safe_vstack([H.T, np.sqrt(beta) * np.ones((1,","360","                         n_components_))]),","361","            W.T, tolW, nls_max_iter, alpha=alpha, l1_ratio=l1_ratio)","362","    elif sparseness == 'components':","363","        Wt, gradW, iterW = _nls_subproblem(","364","            safe_vstack([X.T,","365","                         np.zeros((n_components_, n_samples))]),","366","            safe_vstack([H.T,","367","                         np.sqrt(eta) * np.eye(n_components_)]),","368","            W.T, tolW, nls_max_iter, alpha=alpha, l1_ratio=l1_ratio)","369","","370","    return Wt.T, gradW.T, iterW","371","","372","","373","def _update_projected_gradient_h(X, W, H, tolH, nls_max_iter, alpha, l1_ratio,","374","                                 sparseness, beta, eta):","375","    \"\"\"Helper function for _fit_projected_gradient\"\"\"","376","    n_samples, n_features = X.shape","377","    n_components_ = W.shape[1]","378","","379","    if sparseness is None:","380","        H, gradH, iterH = _nls_subproblem(X, W, H, tolH, nls_max_iter,","381","                                          alpha=alpha, l1_ratio=l1_ratio)","382","    elif sparseness == 'data':","383","        H, gradH, iterH = _nls_subproblem(","384","            safe_vstack([X, np.zeros((n_components_, n_features))]),","385","            safe_vstack([W,","386","                         np.sqrt(eta) * np.eye(n_components_)]),","387","            H, tolH, nls_max_iter, alpha=alpha, l1_ratio=l1_ratio)","388","    elif sparseness == 'components':","389","        H, gradH, iterH = _nls_subproblem(","390","            safe_vstack([X, np.zeros((1, n_features))]),","391","            safe_vstack([W, np.sqrt(beta) * np.ones((1, n_components_))]),","392","            H, tolH, nls_max_iter, alpha=alpha, l1_ratio=l1_ratio)","393","","394","    return H, gradH, iterH","395","","396","","397","def _fit_projected_gradient(X, W, H, tol, max_iter,","398","                            nls_max_iter, alpha, l1_ratio,","399","                            sparseness, beta, eta):","400","    \"\"\"Compute Non-negative Matrix Factorization (NMF) with Projected Gradient","401","","402","    References","403","    ----------","404","    C.-J. Lin. Projected gradient methods for non-negative matrix","405","    factorization. Neural Computation, 19(2007), 2756-2779.","406","    http:\/\/www.csie.ntu.edu.tw\/~cjlin\/nmf\/","407","","408","    P. Hoyer. Non-negative Matrix Factorization with Sparseness Constraints.","409","    Journal of Machine Learning Research 2004.","410","    \"\"\"","411","    gradW = (np.dot(W, np.dot(H, H.T)) -","412","             safe_sparse_dot(X, H.T, dense_output=True))","413","    gradH = (np.dot(np.dot(W.T, W), H) -","414","             safe_sparse_dot(W.T, X, dense_output=True))","415","","416","    init_grad = squared_norm(gradW) + squared_norm(gradH.T)","417","    # max(0.001, tol) to force alternating minimizations of W and H","418","    tolW = max(0.001, tol) * np.sqrt(init_grad)","419","    tolH = tolW","420","","421","    for n_iter in range(1, max_iter + 1):","422","        # stopping condition","423","        # as discussed in paper","424","        proj_grad_W = squared_norm(gradW * np.logical_or(gradW < 0, W > 0))","425","        proj_grad_H = squared_norm(gradH * np.logical_or(gradH < 0, H > 0))","426","","427","        if (proj_grad_W + proj_grad_H) \/ init_grad < tol ** 2:","428","            break","429","","430","        # update W","431","        W, gradW, iterW = _update_projected_gradient_w(X, W, H, tolW,","432","                                                       nls_max_iter,","433","                                                       alpha, l1_ratio,","434","                                                       sparseness, beta, eta)","435","        if iterW == 1:","436","            tolW = 0.1 * tolW","437","","438","        # update H","439","        H, gradH, iterH = _update_projected_gradient_h(X, W, H, tolH,","440","                                                       nls_max_iter,","441","                                                       alpha, l1_ratio,","442","                                                       sparseness, beta, eta)","443","        if iterH == 1:","444","            tolH = 0.1 * tolH","445","","446","    H[H == 0] = 0   # fix up negative zeros","447","","448","    if n_iter == max_iter:","449","        W, _, _ = _update_projected_gradient_w(X, W, H, tol, nls_max_iter,","450","                                               alpha, l1_ratio, sparseness,","451","                                               beta, eta)","452","","453","    return W, H, n_iter","454","","455","","606","                               verbose=0, shuffle=False, nls_max_iter=2000,","607","                               sparseness=None, beta=1, eta=0.1):","670","    solver : 'pg' | 'cd'","672","        'pg' is a (deprecated) Projected Gradient solver.","704","    nls_max_iter : integer, default: 2000","705","        Number of iterations in NLS subproblem.","706","        Used only in the deprecated 'pg' solver.","707","","708","    sparseness : 'data' | 'components' | None, default: None","709","        Where to enforce sparsity in the model.","710","        Used only in the deprecated 'pg' solver.","711","","712","    beta : double, default: 1","713","        Degree of sparseness, if sparseness is not None. Larger values mean","714","        more sparseness. Used only in the deprecated 'pg' solver.","715","","716","    eta : double, default: 0.1","717","        Degree of correctness to maintain, if sparsity is not None. Smaller","718","        values mean larger error. Used only in the deprecated 'pg' solver.","745","    _check_string_param(sparseness, solver)","772","    if solver == 'pg':","773","        warnings.warn(\"'pg' solver will be removed in release 0.19.\"","774","                      \" Use 'cd' solver instead.\", DeprecationWarning)","775","        if update_H:  # fit_transform","776","            W, H, n_iter = _fit_projected_gradient(X, W, H, tol,","777","                                                   max_iter,","778","                                                   nls_max_iter,","779","                                                   alpha, l1_ratio,","780","                                                   sparseness,","781","                                                   beta, eta)","782","        else:  # transform","783","            W, H, n_iter = _update_projected_gradient_w(X, W, H,","784","                                                        tol, nls_max_iter,","785","                                                        alpha, l1_ratio,","786","                                                        sparseness, beta,","787","                                                        eta)","788","    elif solver == 'cd':","858","    solver : 'pg' | 'cd'","860","        'pg' is a Projected Gradient solver (deprecated).","861","        'cd' is a Coordinate Descent solver (recommended).","902","    nls_max_iter : integer, default: 2000","903","        Number of iterations in NLS subproblem.","904","        Used only in the deprecated 'pg' solver.","905","","906","        .. versionchanged:: 0.17","907","           Deprecated Projected Gradient solver. Use Coordinate Descent solver","908","           instead.","909","","910","    sparseness : 'data' | 'components' | None, default: None","911","        Where to enforce sparsity in the model.","912","        Used only in the deprecated 'pg' solver.","913","","914","        .. versionchanged:: 0.17","915","           Deprecated Projected Gradient solver. Use Coordinate Descent solver","916","           instead.","917","","918","    beta : double, default: 1","919","        Degree of sparseness, if sparseness is not None. Larger values mean","920","        more sparseness. Used only in the deprecated 'pg' solver.","921","","922","        .. versionchanged:: 0.17","923","           Deprecated Projected Gradient solver. Use Coordinate Descent solver","924","           instead.","925","","926","    eta : double, default: 0.1","927","        Degree of correctness to maintain, if sparsity is not None. Smaller","928","        values mean larger error. Used only in the deprecated 'pg' solver.","929","","930","        .. versionchanged:: 0.17","931","           Deprecated Projected Gradient solver. Use Coordinate Descent solver","932","           instead.","954","    NMF(alpha=0.0, beta=1, eta=0.1, init='random', l1_ratio=0.0, max_iter=200,","955","      n_components=2, nls_max_iter=2000, random_state=0, shuffle=False,","956","      solver='cd', sparseness=None, tol=0.0001, verbose=0)","976","    def __init__(self, n_components=None, init=None, solver='cd',","977","                 tol=1e-4, max_iter=200, random_state=None,","978","                 alpha=0., l1_ratio=0., verbose=0, shuffle=False,","979","                 nls_max_iter=2000, sparseness=None, beta=1, eta=0.1):","991","        if sparseness is not None:","992","            warnings.warn(\"Controlling regularization through the sparseness,\"","993","                          \" beta and eta arguments is only available\"","994","                          \" for 'pg' solver, which will be removed\"","995","                          \" in release 0.19. Use another solver with L1 or L2\"","996","                          \" regularization instead.\", DeprecationWarning)","997","        self.nls_max_iter = nls_max_iter","998","        self.sparseness = sparseness","999","        self.beta = beta","1000","        self.eta = eta","1001","","1031","            shuffle=self.shuffle,","1032","            nls_max_iter=self.nls_max_iter, sparseness=self.sparseness,","1033","            beta=self.beta, eta=self.eta)","1034","","1035","        if self.solver == 'pg':","1036","            self.comp_sparseness_ = _sparseness(H.ravel())","1037","            self.data_sparseness_ = _sparseness(W.ravel())","1083","            shuffle=self.shuffle,","1084","            nls_max_iter=self.nls_max_iter, sparseness=self.sparseness,","1085","            beta=self.beta, eta=self.eta)","1106","","1107","","1108","@deprecated(\"It will be removed in release 0.19. Use NMF instead.\"","1109","            \"'pg' solver is still available until release 0.19.\")","1110","class ProjectedGradientNMF(NMF):","1111","    \"\"\"Non-Negative Matrix Factorization (NMF)","1112","","1113","    Find two non-negative matrices (W, H) whose product approximates the non-","1114","    negative matrix X. This factorization can be used for example for","1115","    dimensionality reduction, source separation or topic extraction.","1116","","1117","    The objective function is::","1118","","1119","        0.5 * ||X - WH||_Fro^2","1120","        + alpha * l1_ratio * ||vec(W)||_1","1121","        + alpha * l1_ratio * ||vec(H)||_1","1122","        + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2","1123","        + 0.5 * alpha * (1 - l1_ratio) * ||H||_Fro^2","1124","","1125","    Where::","1126","","1127","        ||A||_Fro^2 = \\sum_{i,j} A_{ij}^2 (Frobenius norm)","1128","        ||vec(A)||_1 = \\sum_{i,j} abs(A_{ij}) (Elementwise L1 norm)","1129","","1130","    The objective function is minimized with an alternating minimization of W","1131","    and H.","1132","","1133","    Read more in the :ref:`User Guide <NMF>`.","1134","","1135","    Parameters","1136","    ----------","1137","    n_components : int or None","1138","        Number of components, if n_components is not set all features","1139","        are kept.","1140","","1141","    init :  'random' | 'nndsvd' |  'nndsvda' | 'nndsvdar' | 'custom'","1142","        Method used to initialize the procedure.","1143","        Default: 'nndsvdar' if n_components < n_features, otherwise random.","1144","        Valid options:","1145","","1146","        - 'random': non-negative random matrices, scaled with:","1147","            sqrt(X.mean() \/ n_components)","1148","","1149","        - 'nndsvd': Nonnegative Double Singular Value Decomposition (NNDSVD)","1150","            initialization (better for sparseness)","1151","","1152","        - 'nndsvda': NNDSVD with zeros filled with the average of X","1153","            (better when sparsity is not desired)","1154","","1155","        - 'nndsvdar': NNDSVD with zeros filled with small random values","1156","            (generally faster, less accurate alternative to NNDSVDa","1157","            for when sparsity is not desired)","1158","","1159","        - 'custom': use custom matrices W and H","1160","","1161","    solver : 'pg' | 'cd'","1162","        Numerical solver to use:","1163","        'pg' is a Projected Gradient solver (deprecated).","1164","        'cd' is a Coordinate Descent solver (recommended).","1165","","1166","        .. versionadded:: 0.17","1167","           Coordinate Descent solver.","1168","","1169","        .. versionchanged:: 0.17","1170","           Deprecated Projected Gradient solver.","1171","","1172","    tol : double, default: 1e-4","1173","        Tolerance value used in stopping conditions.","1174","","1175","    max_iter : integer, default: 200","1176","        Number of iterations to compute.","1177","","1178","    random_state : integer seed, RandomState instance, or None (default)","1179","        Random number generator seed control.","1180","","1181","    alpha : double, default: 0.","1182","        Constant that multiplies the regularization terms. Set it to zero to","1183","        have no regularization.","1184","","1185","        .. versionadded:: 0.17","1186","           *alpha* used in the Coordinate Descent solver.","1187","","1188","    l1_ratio : double, default: 0.","1189","        The regularization mixing parameter, with 0 <= l1_ratio <= 1.","1190","        For l1_ratio = 0 the penalty is an elementwise L2 penalty","1191","        (aka Frobenius Norm).","1192","        For l1_ratio = 1 it is an elementwise L1 penalty.","1193","        For 0 < l1_ratio < 1, the penalty is a combination of L1 and L2.","1194","","1195","        .. versionadded:: 0.17","1196","           Regularization parameter *l1_ratio* used in the Coordinate Descent","1197","           solver.","1198","","1199","    shuffle : boolean, default: False","1200","        If true, randomize the order of coordinates in the CD solver.","1201","","1202","        .. versionadded:: 0.17","1203","           *shuffle* parameter used in the Coordinate Descent solver.","1204","","1205","    nls_max_iter : integer, default: 2000","1206","        Number of iterations in NLS subproblem.","1207","        Used only in the deprecated 'pg' solver.","1208","","1209","        .. versionchanged:: 0.17","1210","           Deprecated Projected Gradient solver. Use Coordinate Descent solver","1211","           instead.","1212","","1213","    sparseness : 'data' | 'components' | None, default: None","1214","        Where to enforce sparsity in the model.","1215","        Used only in the deprecated 'pg' solver.","1216","","1217","        .. versionchanged:: 0.17","1218","           Deprecated Projected Gradient solver. Use Coordinate Descent solver","1219","           instead.","1220","","1221","    beta : double, default: 1","1222","        Degree of sparseness, if sparseness is not None. Larger values mean","1223","        more sparseness. Used only in the deprecated 'pg' solver.","1224","","1225","        .. versionchanged:: 0.17","1226","           Deprecated Projected Gradient solver. Use Coordinate Descent solver","1227","           instead.","1228","","1229","    eta : double, default: 0.1","1230","        Degree of correctness to maintain, if sparsity is not None. Smaller","1231","        values mean larger error. Used only in the deprecated 'pg' solver.","1232","","1233","        .. versionchanged:: 0.17","1234","           Deprecated Projected Gradient solver. Use Coordinate Descent solver","1235","           instead.","1236","","1237","    Attributes","1238","    ----------","1239","    components_ : array, [n_components, n_features]","1240","        Non-negative components of the data.","1241","","1242","    reconstruction_err_ : number","1243","        Frobenius norm of the matrix difference between","1244","        the training data and the reconstructed data from","1245","        the fit produced by the model. ``|| X - WH ||_2``","1246","","1247","    n_iter_ : int","1248","        Actual number of iterations.","1249","","1250","    Examples","1251","    --------","1252","    >>> import numpy as np","1253","    >>> X = np.array([[1,1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])","1254","    >>> from sklearn.decomposition import NMF","1255","    >>> model = NMF(n_components=2, init='random', random_state=0)","1256","    >>> model.fit(X) #doctest: +ELLIPSIS +NORMALIZE_WHITESPACE","1257","    NMF(alpha=0.0, beta=1, eta=0.1, init='random', l1_ratio=0.0, max_iter=200,","1258","      n_components=2, nls_max_iter=2000, random_state=0, shuffle=False,","1259","      solver='cd', sparseness=None, tol=0.0001, verbose=0)","1260","","1261","    >>> model.components_","1262","    array([[ 2.09783018,  0.30560234],","1263","           [ 2.13443044,  2.13171694]])","1264","    >>> model.reconstruction_err_ #doctest: +ELLIPSIS","1265","    0.00115993...","1266","","1267","    References","1268","    ----------","1269","    C.-J. Lin. Projected gradient methods for non-negative matrix","1270","    factorization. Neural Computation, 19(2007), 2756-2779.","1271","    http:\/\/www.csie.ntu.edu.tw\/~cjlin\/nmf\/","1272","","1273","    Cichocki, Andrzej, and P. H. A. N. Anh-Huy. \"Fast local algorithms for","1274","    large scale nonnegative matrix and tensor factorizations.\"","1275","    IEICE transactions on fundamentals of electronics, communications and","1276","    computer sciences 92.3: 708-721, 2009.","1277","    \"\"\"","1278","","1279","    def __init__(self, n_components=None, solver='pg', init=None,","1280","                 tol=1e-4, max_iter=200, random_state=None,","1281","                 alpha=0., l1_ratio=0., verbose=0,","1282","                 nls_max_iter=2000, sparseness=None, beta=1, eta=0.1):","1283","        super(ProjectedGradientNMF, self).__init__(","1284","            n_components=n_components, init=init, solver='pg', tol=tol,","1285","            max_iter=max_iter, random_state=random_state, alpha=alpha,","1286","            l1_ratio=l1_ratio, verbose=verbose, nls_max_iter=nls_max_iter,","1287","            sparseness=sparseness, beta=beta, eta=eta)"]}],"sklearn\/feature_selection\/tests\/test_from_model.py":[{"add":[],"delete":["1","import scipy.sparse as sp","11","from sklearn.utils.testing import assert_warns","27","def test_transform_linear_model():","28","    for clf in (LogisticRegression(C=0.1),","29","                LinearSVC(C=0.01, dual=False),","30","                SGDClassifier(alpha=0.001, n_iter=50, shuffle=True,","31","                              random_state=0)):","32","        for thresh in (None, \".09*mean\", \"1e-5 * median\"):","33","            for func in (np.array, sp.csr_matrix):","34","                X = func(data)","35","                clf.set_params(penalty=\"l1\")","36","                clf.fit(X, y)","37","                X_new = assert_warns(","38","                    DeprecationWarning, clf.transform, X, thresh)","39","                if isinstance(clf, SGDClassifier):","40","                    assert_true(X_new.shape[1] <= X.shape[1])","41","                else:","42","                    assert_less(X_new.shape[1], X.shape[1])","43","                clf.set_params(penalty=\"l2\")","44","                clf.fit(X_new, y)","45","                pred = clf.predict(X_new)","46","                assert_greater(np.mean(pred == y), 0.7)","47","","48",""]}],"sklearn\/datasets\/__init__.py":[{"add":[],"delete":["20","from .lfw import load_lfw_pairs","21","from .lfw import load_lfw_people","76","           'load_lfw_pairs',","77","           'load_lfw_people',"]}],"doc\/tutorial\/statistical_inference\/supervised_learning.rst":[{"add":["457","        decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',"],"delete":["457","        decision_function_shape=None, degree=3, gamma='auto', kernel='linear',"]}],"doc\/modules\/svm.rst":[{"add":["79","        decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',","518","        decision_function_shape='ovr', degree=3, gamma='auto',"],"delete":["79","        decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',","518","        decision_function_shape=None, degree=3, gamma='auto',"]}],"sklearn\/covariance\/tests\/test_robust_covariance.py":[{"add":["46","    assert_raise_message(ValueError, 'Got X with X.ndim=1',","53","    assert_raise_message(ValueError, 'Got X with X.ndim=1',"],"delete":["46","    assert_raise_message(ValueError, 'fast_mcd expects at least 2 samples',","53","    assert_raise_message(ValueError, 'MinCovDet expects at least 2 samples',"]}],"sklearn\/feature_selection\/from_model.py":[{"add":["6","from ..base import BaseEstimator, clone"],"delete":["6","from ..base import TransformerMixin, BaseEstimator, clone","9","from ..utils import safe_mask, check_array, deprecated","10","from ..utils.validation import check_is_fitted","80","class _LearntSelectorMixin(TransformerMixin):","81","    # Note because of the extra threshold parameter in transform, this does","82","    # not naturally extend from SelectorMixin","83","    \"\"\"Transformer mixin selecting features based on importance weights.","84","","85","    This implementation can be mixin on any estimator that exposes a","86","    ``feature_importances_`` or ``coef_`` attribute to evaluate the relative","87","    importance of individual features for feature selection.","88","    \"\"\"","89","    @deprecated('Support to use estimators as feature selectors will be '","90","                'removed in version 0.19. Use SelectFromModel instead.')","91","    def transform(self, X, threshold=None):","92","        \"\"\"Reduce X to its most important features.","93","","94","        Uses ``coef_`` or ``feature_importances_`` to determine the most","95","        important features.  For models with a ``coef_`` for each class, the","96","        absolute sum over the classes is used.","97","","98","        Parameters","99","        ----------","100","        X : array or scipy sparse matrix of shape [n_samples, n_features]","101","            The input samples.","102","","103","        threshold : string, float or None, optional (default=None)","104","            The threshold value to use for feature selection. Features whose","105","            importance is greater or equal are kept while the others are","106","            discarded. If \"median\" (resp. \"mean\"), then the threshold value is","107","            the median (resp. the mean) of the feature importances. A scaling","108","            factor (e.g., \"1.25*mean\") may also be used. If None and if","109","            available, the object attribute ``threshold`` is used. Otherwise,","110","            \"mean\" is used by default.","111","","112","        Returns","113","        -------","114","        X_r : array of shape [n_samples, n_selected_features]","115","            The input samples with only the selected features.","116","        \"\"\"","117","        check_is_fitted(self, ('coef_', 'feature_importances_'),","118","                        all_or_any=any)","119","","120","        X = check_array(X, 'csc')","121","        importances = _get_feature_importances(self)","122","        if len(importances) != X.shape[1]:","123","            raise ValueError(\"X has different number of features than\"","124","                             \" during model fitting.\")","125","","126","        if threshold is None:","127","            threshold = getattr(self, 'threshold', None)","128","        threshold = _calculate_threshold(self, importances, threshold)","129","","130","        # Selection","131","        try:","132","            mask = importances >= threshold","133","        except TypeError:","134","            # Fails in Python 3.x when threshold is str;","135","            # result is array of True","136","            raise ValueError(\"Invalid threshold: all features are discarded.\")","137","","138","        if np.any(mask):","139","            mask = safe_mask(X, mask)","140","            return X[:, mask]","141","        else:","142","            raise ValueError(\"Invalid threshold: all features are discarded.\")","143","","144",""]}],"sklearn\/svm\/tests\/test_svm.py":[{"add":[],"delete":["22","from sklearn.exceptions import ChangedBehaviorWarning","89","@ignore_warnings","90","def test_single_sample_1d():","91","    # Test whether SVCs work on a single sample given as a 1-d array","92","","93","    clf = svm.SVC().fit(X, Y)","94","    clf.predict(X[0])","95","","96","    clf = svm.LinearSVC(random_state=0).fit(X, Y)","97","    clf.predict(X[0])","98","","99","","384","    # check deprecation warning","385","    clf = svm.SVC(kernel='linear', C=0.1).fit(X_train, y_train)","386","    msg = \"change the shape of the decision function\"","387","    dec = assert_warns_message(ChangedBehaviorWarning, msg,","388","                               clf.decision_function, X_train)","389","    assert_equal(dec.shape, (len(X_train), 10))","390",""]}],"sklearn\/ensemble\/tests\/test_gradient_boosting.py":[{"add":["1066",""],"delete":["301","        # XXX: Remove this test in 0.19 after transform support to estimators","302","        # is removed.","303","        X_new = assert_warns(","304","            DeprecationWarning, clf.transform, X, threshold=\"mean\")","305","        assert_less(X_new.shape[1], X.shape[1])","306","        feature_mask = (","307","            clf.feature_importances_ > clf.feature_importances_.mean())","308","        assert_array_almost_equal(X_new, X[:, feature_mask])","309",""]}],"sklearn\/tree\/tests\/test_tree.py":[{"add":["525","        assert_raises(ValueError, TreeEstimator(min_impurity_split=-1.0).fit,","526","                      X, y)","1282","    for tree_type, dataset in product(SPARSE_TREES, (\"clf_small\", \"toy\",","1283","                                                     \"digits\", \"multilabel\",","1284","                                                     \"sparse-pos\",","1285","                                                     \"sparse-neg\",","1286","                                                     \"sparse-mix\", \"zeros\")):","1288","        yield (check_sparse_input, tree_type, dataset, max_depth)","1292","    for tree_type, dataset in product(SPARSE_TREES, [\"boston\", \"reg_small\"]):","1293","        if tree_type in REG_TREES:","1294","            yield (check_sparse_input, tree_type, dataset, 2)","1342","    for tree_type, dataset in product(SPARSE_TREES, [\"sparse-pos\",","1343","                                                     \"sparse-neg\",","1344","                                                     \"sparse-mix\", \"zeros\"]):","1345","        yield (check_sparse_parameters, tree_type, dataset)","1369","    for tree_type, dataset in product(SPARSE_TREES, [\"sparse-pos\",","1370","                                                     \"sparse-neg\",","1371","                                                     \"sparse-mix\", \"zeros\"]):","1372","        yield (check_sparse_criterion, tree_type, dataset)","1445","    for tree_type in SPARSE_TREES:","1446","        yield (check_explicit_sparse_zeros, tree_type)"],"delete":["30","from sklearn.utils.testing import assert_warns","384","        X_new = assert_warns(","385","            DeprecationWarning, clf.transform, X, threshold=\"mean\")","386","        assert_less(0, X_new.shape[1], \"Failed with {0}\".format(name))","387","        assert_less(X_new.shape[1], X.shape[1], \"Failed with {0}\".format(name))","388","","531","        assert_raises(ValueError, TreeEstimator(min_impurity_split=-1.0).fit, X, y)","604","","869","","1289","    for tree, dataset in product(SPARSE_TREES,","1290","                                 (\"clf_small\", \"toy\", \"digits\", \"multilabel\",","1291","                                  \"sparse-pos\", \"sparse-neg\", \"sparse-mix\",","1292","                                  \"zeros\")):","1294","        yield (check_sparse_input, tree, dataset, max_depth)","1298","    for tree, dataset in product(REG_TREES, [\"boston\", \"reg_small\"]):","1299","        if tree in SPARSE_TREES:","1300","            yield (check_sparse_input, tree, dataset, 2)","1348","    for tree, dataset in product(SPARSE_TREES,","1349","                                 [\"sparse-pos\", \"sparse-neg\", \"sparse-mix\",","1350","                                  \"zeros\"]):","1351","        yield (check_sparse_parameters, tree, dataset)","1375","    for tree, dataset in product(SPARSE_TREES,","1376","                                 [\"sparse-pos\", \"sparse-neg\", \"sparse-mix\",","1377","                                  \"zeros\"]):","1378","        yield (check_sparse_criterion, tree, dataset)","1451","    for tree in SPARSE_TREES:","1452","        yield (check_explicit_sparse_zeros, tree)"]}],"sklearn\/metrics\/regression.py":[{"add":["437","def r2_score(y_true, y_pred, sample_weight=None,","438","             multioutput=\"uniform_average\"):","464","        Default is \"uniform_average\".","476","        .. versionchanged:: 0.19","477","            Default value of multioutput is 'uniform_average'.","478",""],"delete":["31","import warnings","438","def r2_score(y_true, y_pred,","439","             sample_weight=None,","440","             multioutput=None):","466","        Default value corresponds to 'variance_weighted', this behaviour is","467","        deprecated since version 0.17 and will be changed to 'uniform_average'","468","        starting from 0.19.","545","    if multioutput is None and y_true.shape[1] != 1:","546","        warnings.warn(\"Default 'multioutput' behavior now corresponds to \"","547","                      \"'variance_weighted' value which is deprecated since \"","548","                      \"0.17, it will be changed to 'uniform_average' \"","549","                      \"starting from 0.19.\",","550","                      DeprecationWarning)","551","        multioutput = 'variance_weighted'"]}],"sklearn\/metrics\/base.py":[{"add":[],"delete":["21","from ..exceptions import UndefinedMetricWarning as _UndefinedMetricWarning","22","from ..utils import deprecated","23","","24","","25","@deprecated(\"UndefinedMetricWarning has been moved into the sklearn.exceptions\"","26","            \" module. It will not be available here from version 0.19\")","27","class UndefinedMetricWarning(_UndefinedMetricWarning):","28","    pass","29",""]}],"doc\/modules\/classes.rst":[{"add":[],"delete":["1363","To be removed in 0.19","1364","---------------------","1365","","1366",".. autosummary::","1367","   :toctree: generated\/","1368","   :template: deprecated_class.rst","1369","","1370","   lda.LDA","1371","   qda.QDA","1372","","1373",".. autosummary::","1374","   :toctree: generated\/","1375","   :template: deprecated_function.rst","1376","","1377","   datasets.load_lfw_pairs","1378","   datasets.load_lfw_people","1379",""]}],"sklearn\/utils\/estimator_checks.py":[{"add":["212","        for check in _yield_transformer_checks(name, Estimator):","213","            yield check","410","    if hasattr(estimator, \"transform\"):","710","    funcs = [\"score\", \"fit_transform\"]","733","    funcs = [\"fit\", \"score\", \"partial_fit\", \"fit_predict\", \"fit_transform\"]","755","    methods = [\"predict\", \"transform\", \"decision_function\", \"predict_proba\"]","843","            if hasattr(estimator, \"transform\"):","861","    check_methods = [\"predict\", \"transform\", \"decision_function\",","862","                     \"predict_proba\"]"],"delete":["44","from sklearn.decomposition import NMF, ProjectedGradientNMF","69","# Estimators with deprecated transform methods. Should be removed in 0.19 when","70","# _LearntSelectorMixin is removed.","71","DEPRECATED_TRANSFORM = [","72","    \"RandomForestClassifier\", \"RandomForestRegressor\", \"ExtraTreesClassifier\",","73","    \"ExtraTreesRegressor\", \"DecisionTreeClassifier\",","74","    \"DecisionTreeRegressor\", \"ExtraTreeClassifier\", \"ExtraTreeRegressor\",","75","    \"LinearSVC\", \"SGDClassifier\", \"SGDRegressor\", \"Perceptron\",","76","    \"LogisticRegression\", \"LogisticRegressionCV\",","77","    \"GradientBoostingClassifier\", \"GradientBoostingRegressor\"]","78","","223","        if name not in DEPRECATED_TRANSFORM:","224","            for check in _yield_transformer_checks(name, Estimator):","225","                yield check","331","    if isinstance(estimator, NMF):","332","        if not isinstance(estimator, ProjectedGradientNMF):","333","            estimator.set_params(solver='cd')","334","","426","    if (hasattr(estimator, \"transform\") and","427","            name not in DEPRECATED_TRANSFORM):","727","    if name in DEPRECATED_TRANSFORM:","728","        funcs = [\"score\"]","729","    else:","730","        funcs = [\"score\", \"fit_transform\"]","753","    if name in DEPRECATED_TRANSFORM:","754","        funcs = [\"fit\", \"score\", \"partial_fit\", \"fit_predict\"]","755","    else:","756","        funcs = [","757","            \"fit\", \"score\", \"partial_fit\", \"fit_predict\", \"fit_transform\"]","779","    if name in DEPRECATED_TRANSFORM:","780","        methods = [\"predict\", \"decision_function\", \"predict_proba\"]","781","    else:","782","        methods = [","783","            \"predict\", \"transform\", \"decision_function\", \"predict_proba\"]","871","            if (hasattr(estimator, \"transform\") and","872","                    name not in DEPRECATED_TRANSFORM):","890","    if name in DEPRECATED_TRANSFORM:","891","        check_methods = [\"predict\", \"decision_function\", \"predict_proba\"]","892","    else:","893","        check_methods = [\"predict\", \"transform\", \"decision_function\",","894","                         \"predict_proba\"]"]}],"sklearn\/linear_model\/base.py":[{"add":[],"delete":["231","    @deprecated(\" and will be removed in 0.19.\")","232","    def decision_function(self, X):","233","        \"\"\"Decision function of the linear model.","234","","235","        Parameters","236","        ----------","237","        X : {array-like, sparse matrix}, shape = (n_samples, n_features)","238","            Samples.","239","","240","        Returns","241","        -------","242","        C : array, shape = (n_samples,)","243","            Returns predicted values.","244","        \"\"\"","245","        return self._decision_function(X)","246","","480","    @property","481","    @deprecated(\"``residues_`` is deprecated and will be removed in 0.19\")","482","    def residues_(self):","483","        \"\"\"Get the residues of the fitted model.\"\"\"","484","        return self._residues","485",""]}],"sklearn\/preprocessing\/data.py":[{"add":["220","           *data_min_*","226","           *data_max_*","232","           *data_range_*","296","        X = check_array(X, copy=self.copy, warn_on_dtype=True,","330","        X = check_array(X, copy=self.copy, dtype=FLOAT_DTYPES)","346","        X = check_array(X, copy=self.copy, dtype=FLOAT_DTYPES)","394","    # Unlike the scaler object, this function allows 1d input.","466","           *scale_*","542","                        warn_on_dtype=True, estimator=self, dtype=FLOAT_DTYPES)","600","        X = check_array(X, accept_sparse='csr', copy=copy, warn_on_dtype=True,","734","                        estimator=self, dtype=FLOAT_DTYPES)","764","                        estimator=self, dtype=FLOAT_DTYPES)","782","                        estimator=self, dtype=FLOAT_DTYPES)","815","    # Unlike the scaler object, this function allows 1d input.","922","                        estimator=self, dtype=FLOAT_DTYPES)"],"delete":["19","from ..utils import deprecated","53","DEPRECATION_MSG_1D = (","54","    \"Passing 1d arrays as data is deprecated in 0.17 and will \"","55","    \"raise ValueError in 0.19. Reshape your data either using \"","56","    \"X.reshape(-1, 1) if your data has a single feature or \"","57","    \"X.reshape(1, -1) if it contains a single sample.\"","58",")","59","","228","           *data_min_* instead of deprecated *data_min*.","234","           *data_max_* instead of deprecated *data_max*.","240","           *data_range_* instead of deprecated *data_range*.","251","    @property","252","    @deprecated(\"Attribute data_range will be removed in \"","253","                \"0.19. Use ``data_range_`` instead\")","254","    def data_range(self):","255","        return self.data_range_","256","","257","    @property","258","    @deprecated(\"Attribute data_min will be removed in \"","259","                \"0.19. Use ``data_min_`` instead\")","260","    def data_min(self):","261","        return self.data_min_","262","","316","        X = check_array(X, copy=self.copy, ensure_2d=False, warn_on_dtype=True,","319","        if X.ndim == 1:","320","            warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)","321","","353","        X = check_array(X, copy=self.copy, ensure_2d=False, dtype=FLOAT_DTYPES)","354","        if X.ndim == 1:","355","            warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)","371","        X = check_array(X, copy=self.copy, ensure_2d=False, dtype=FLOAT_DTYPES)","372","        if X.ndim == 1:","373","            warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)","421","    # To allow retro-compatibility, we handle here the case of 1D-input","422","    # From 0.17, 1D-input are deprecated in scaler objects","423","    # Although, we want to allow the users to keep calling this function","424","    # with 1D-input.","425","","426","    # Cast input to array, as we need to check ndim. Prior to 0.17, that was","427","    # done inside the scaler object fit_transform.","499","           *scale_* is recommended instead of deprecated *std_*.","525","    @property","526","    @deprecated(\"Attribute ``std_`` will be removed in 0.19. \"","527","                \"Use ``scale_`` instead\")","528","    def std_(self):","529","        return self.scale_","530","","581","                        ensure_2d=False, warn_on_dtype=True,","582","                        estimator=self, dtype=FLOAT_DTYPES)","583","","584","        if X.ndim == 1:","585","            warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)","643","        X = check_array(X, accept_sparse='csr', copy=copy,","644","                        ensure_2d=False, warn_on_dtype=True,","647","        if X.ndim == 1:","648","            warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)","649","","781","                        ensure_2d=False, estimator=self, dtype=FLOAT_DTYPES)","782","","783","        if X.ndim == 1:","784","            warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)","814","                        ensure_2d=False, estimator=self, dtype=FLOAT_DTYPES)","815","","816","        if X.ndim == 1:","817","            warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)","835","                        ensure_2d=False, estimator=self, dtype=FLOAT_DTYPES)","836","        if X.ndim == 1:","837","            warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)","870","    # To allow retro-compatibility, we handle here the case of 1D-input","871","    # From 0.17, 1D-input are deprecated in scaler objects","872","    # Although, we want to allow the users to keep calling this function","873","    # with 1D-input.","875","    # Cast input to array, as we need to check ndim. Prior to 0.17, that was","876","    # done inside the scaler object fit_transform.","982","                        ensure_2d=False, estimator=self, dtype=FLOAT_DTYPES)","983","","984","        if X.ndim == 1:","985","            warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)","1006","        if X.ndim == 1:","1007","            warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)","1035","        if X.ndim == 1:","1036","            warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)","1061","        if X.ndim == 1:","1062","            warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)"]}],"sklearn\/decomposition\/tests\/test_nmf.py":[{"add":["2","from sklearn.decomposition import NMF, non_negative_factorization","29","    msg = \"Invalid solver parameter 'spam'\"","71","    for init in (None, 'nndsvd', 'nndsvda', 'nndsvdar'):","72","        model = NMF(n_components=2, init=init, random_state=0)","73","        transf = model.fit_transform(A)","74","        assert_false((model.components_ < 0).any() or","75","                     (transf < 0).any())","80","    pnmf = NMF(5, init='nndsvd', random_state=0)","81","    X = np.abs(random_state.randn(6, 5))","82","    assert_less(pnmf.fit(X).reconstruction_err_, 0.05)","103","    m = NMF(n_components=4, init='nndsvd', random_state=0)","104","    ft = m.fit_transform(A)","105","    t = m.transform(A)","106","    assert_array_almost_equal(ft, t, decimal=2)","127","    m = NMF(n_components=4, init='random', random_state=0)","128","    m.fit_transform(A)","129","    t = m.transform(A)","130","    A_new = m.inverse_transform(t)","131","    assert_array_almost_equal(A, A_new, decimal=2)","148","    est1 = NMF(n_components=5, init='random', random_state=0, tol=1e-2)","149","    est2 = clone(est1)","151","    W1 = est1.fit_transform(A)","152","    W2 = est2.fit_transform(A_sparse)","153","    H1 = est1.components_","154","    H2 = est2.components_","156","    assert_array_almost_equal(W1, W2)","157","    assert_array_almost_equal(H1, H2)","167","    model = NMF(random_state=0, tol=1e-4, n_components=2)","168","    A_fit_tr = model.fit_transform(A)","169","    A_tr = model.transform(A)","170","    assert_array_almost_equal(A_fit_tr, A_tr, decimal=1)","179","    W_nmf, H, _ = non_negative_factorization(A, random_state=1, tol=1e-2)","180","    W_nmf_2, _, _ = non_negative_factorization(","181","        A, H=H, update_H=False, random_state=1, tol=1e-2)","183","    model_class = NMF(random_state=1, tol=1e-2)","184","    W_cls = model_class.fit_transform(A)","185","    W_cls_2 = model_class.transform(A)","186","    assert_array_almost_equal(W_nmf, W_cls, decimal=10)","187","    assert_array_almost_equal(W_nmf_2, W_cls_2, decimal=10)"],"delete":["2","from sklearn.decomposition import (NMF, ProjectedGradientNMF,","3","                                   non_negative_factorization)","12","from sklearn.utils.testing import assert_greater","14","from sklearn.utils.testing import ignore_warnings","29","@ignore_warnings","33","    msg = \"Invalid solver parameter: got 'spam' instead of one of\"","37","    msg = \"Invalid sparseness parameter: got 'spam' instead of one of\"","38","    assert_raise_message(ValueError, msg, NMF(sparseness=name).fit, A)","73","@ignore_warnings","78","    for solver in ('pg', 'cd'):","79","        for init in (None, 'nndsvd', 'nndsvda', 'nndsvdar'):","80","            model = NMF(n_components=2, solver=solver, init=init,","81","                        random_state=0)","82","            transf = model.fit_transform(A)","83","            assert_false((model.components_ < 0).any() or","84","                         (transf < 0).any())","87","@ignore_warnings","90","    for solver in ('pg', 'cd'):","91","        pnmf = NMF(5, solver=solver, init='nndsvd', random_state=0)","92","        X = np.abs(random_state.randn(6, 5))","93","        assert_less(pnmf.fit(X).reconstruction_err_, 0.05)","111","@ignore_warnings","115","    for solver in ('pg', 'cd'):","116","        m = NMF(solver=solver, n_components=4, init='nndsvd', random_state=0)","117","        ft = m.fit_transform(A)","118","        t = m.transform(A)","119","        assert_array_almost_equal(ft, t, decimal=2)","136","@ignore_warnings","141","    for solver in ('pg', 'cd'):","142","        m = NMF(solver=solver, n_components=4, init='random', random_state=0)","143","        m.fit_transform(A)","144","        t = m.transform(A)","145","        A_new = m.inverse_transform(t)","146","        assert_array_almost_equal(A, A_new, decimal=2)","149","@ignore_warnings","156","@ignore_warnings","157","def test_projgrad_nmf_sparseness():","158","    # Test sparseness","159","    # Test that sparsity constraints actually increase sparseness in the","160","    # part where they are applied.","161","    tol = 1e-2","162","    A = np.abs(random_state.randn(10, 10))","163","    m = ProjectedGradientNMF(n_components=5, random_state=0, tol=tol).fit(A)","164","    data_sp = ProjectedGradientNMF(n_components=5, sparseness='data',","165","                                   random_state=0,","166","                                   tol=tol).fit(A).data_sparseness_","167","    comp_sp = ProjectedGradientNMF(n_components=5, sparseness='components',","168","                                   random_state=0,","169","                                   tol=tol).fit(A).comp_sparseness_","170","    assert_greater(data_sp, m.data_sparseness_)","171","    assert_greater(comp_sp, m.comp_sparseness_)","172","","173","","174","@ignore_warnings","183","    for solver in ('pg', 'cd'):","184","        est1 = NMF(solver=solver, n_components=5, init='random',","185","                   random_state=0, tol=1e-2)","186","        est2 = clone(est1)","188","        W1 = est1.fit_transform(A)","189","        W2 = est2.fit_transform(A_sparse)","190","        H1 = est1.components_","191","        H2 = est2.components_","193","        assert_array_almost_equal(W1, W2)","194","        assert_array_almost_equal(H1, H2)","197","@ignore_warnings","205","    for solver in ('pg', 'cd'):","206","        model = NMF(solver=solver, random_state=0, tol=1e-4, n_components=2)","207","        A_fit_tr = model.fit_transform(A)","208","        A_tr = model.transform(A)","209","        assert_array_almost_equal(A_fit_tr, A_tr, decimal=1)","212","@ignore_warnings","219","    for solver in ('pg', 'cd'):","220","        W_nmf, H, _ = non_negative_factorization(","221","            A, solver=solver, random_state=1, tol=1e-2)","222","        W_nmf_2, _, _ = non_negative_factorization(","223","            A, H=H, update_H=False, solver=solver, random_state=1, tol=1e-2)","225","        model_class = NMF(solver=solver, random_state=1, tol=1e-2)","226","        W_cls = model_class.fit_transform(A)","227","        W_cls_2 = model_class.transform(A)","228","        assert_array_almost_equal(W_nmf, W_cls, decimal=10)","229","        assert_array_almost_equal(W_nmf_2, W_cls_2, decimal=10)","232","@ignore_warnings"]}],"\/dev\/null":[{"add":[],"delete":[]}],"sklearn\/linear_model\/stochastic_gradient.py":[{"add":["15","from ..utils import check_array, check_random_state, check_X_y","497","        if self.class_weight in ['balanced']:","545","class SGDClassifier(BaseSGDClassifier):","1078","class SGDRegressor(BaseSGDRegressor):"],"delete":["15","from ..feature_selection.from_model import _LearntSelectorMixin","16","from ..utils import (check_array, check_random_state, check_X_y,","17","                     deprecated)","499","        if self.class_weight in ['balanced', 'auto']:","547","class SGDClassifier(BaseSGDClassifier, _LearntSelectorMixin):","974","    @deprecated(\" and will be removed in 0.19.\")","975","    def decision_function(self, X):","976","        \"\"\"Predict using the linear model","977","","978","        Parameters","979","        ----------","980","        X : {array-like, sparse matrix}, shape (n_samples, n_features)","981","","982","        Returns","983","        -------","984","        array, shape (n_samples,)","985","           Predicted target values per element in X.","986","        \"\"\"","987","        return self._decision_function(X)","988","","1095","class SGDRegressor(BaseSGDRegressor, _LearntSelectorMixin):"]}],"sklearn\/decomposition\/__init__.py":[{"add":["6","from .nmf import NMF, non_negative_factorization"],"delete":["6","from .nmf import NMF, ProjectedGradientNMF, non_negative_factorization","28","           'ProjectedGradientNMF',"]}],"sklearn\/ensemble\/gradient_boosting.py":[{"add":["715","class BaseGradientBoosting(six.with_metaclass(ABCMeta, BaseEnsemble)):"],"delete":["33","from ..feature_selection.from_model import _LearntSelectorMixin","57","from ..utils import deprecated","717","class BaseGradientBoosting(six.with_metaclass(ABCMeta, BaseEnsemble,","718","                                              _LearntSelectorMixin)):","1127","    @deprecated(\" and will be removed in 0.19\")","1128","    def decision_function(self, X):","1129","        \"\"\"Compute the decision function of ``X``.","1130","","1131","        Parameters","1132","        ----------","1133","        X : array-like of shape = [n_samples, n_features]","1134","            The input samples.","1135","","1136","        Returns","1137","        -------","1138","        score : array, shape = [n_samples, n_classes] or [n_samples]","1139","            The decision function of the input samples. The order of the","1140","            classes corresponds to that in the attribute `classes_`.","1141","            Regression and binary classification produce an array of shape","1142","            [n_samples].","1143","        \"\"\"","1144","","1145","        self._check_initialized()","1146","        X = self.estimators_[0, 0]._validate_X_predict(X, check_input=True)","1147","        score = self._decision_function(X)","1148","        if score.shape[1] == 1:","1149","            return score.ravel()","1150","        return score","1179","    @deprecated(\" and will be removed in 0.19\")","1180","    def staged_decision_function(self, X):","1181","        \"\"\"Compute decision function of ``X`` for each iteration.","1182","","1183","        This method allows monitoring (i.e. determine error on testing set)","1184","        after each stage.","1185","","1186","        Parameters","1187","        ----------","1188","        X : array-like of shape = [n_samples, n_features]","1189","            The input samples.","1190","","1191","        Returns","1192","        -------","1193","        score : generator of array, shape = [n_samples, k]","1194","            The decision function of the input samples. The order of the","1195","            classes corresponds to that in the attribute `classes_`.","1196","            Regression and binary classification are special cases with","1197","            ``k == 1``, otherwise ``k==n_classes``.","1198","        \"\"\"","1199","        for dec in self._staged_decision_function(X):","1200","            # no yield from in Python2.X","1201","            yield dec","1202",""]}]}},"61ec187ea81f395de56baf1ee1564799a5075708":{"changes":{"examples\/text\/document_classification_20newsgroups.py":"MODIFY","examples\/text\/document_clustering.py":"MODIFY"},"diff":{"examples\/text\/document_classification_20newsgroups.py":[{"add":["85","","86","def is_interactive():","87","    return not hasattr(sys.modules['__main__ '], '__file__')","88","","89","# work-around for Jupyter notebook and IPython console","90","argv = [] if is_interactive() else sys.argv[1:]","91","(opts, args) = op.parse_args(argv)"],"delete":["85","(opts, args) = op.parse_args()"]}],"examples\/text\/document_clustering.py":[{"add":["30","and discover latent patterns in the data.","104","","105","def is_interactive():","106","    return not hasattr(sys.modules['__main__'], '__file__')","107","","108","# work-around for Jupyter notebook and IPython console","109","argv = [] if is_interactive() else sys.argv[1:]","110","(opts, args) = op.parse_args(argv)","125","# categories = None"],"delete":["30","and discover latent patterns in the data. ","104","(opts, args) = op.parse_args()","119","#categories = None"]}]}},"5bb983013c71d8e9bcd7bed8654e60b1bcf20a47":{"changes":{"sklearn\/__init__.py":"MODIFY","sklearn\/ensemble\/iforest.py":"MODIFY","sklearn\/mixture\/bayesian_mixture.py":"MODIFY","doc\/faq.rst":"MODIFY","doc\/whats_new.rst":"MODIFY","doc\/modules\/model_evaluation.rst":"MODIFY","examples\/ensemble\/plot_random_forest_regression_multioutput.py":"MODIFY","sklearn\/mixture\/gaussian_mixture.py":"MODIFY","doc\/index.rst":"MODIFY"},"diff":{"sklearn\/__init__.py":[{"add":["72",""],"delete":[]}],"sklearn\/ensemble\/iforest.py":[{"add":["68",""],"delete":[]}],"sklearn\/mixture\/bayesian_mixture.py":[{"add":["92","    covariance_type : {'full', 'tied', 'diag', 'spherical'}, defaults to 'full'","95","","96","            'full' (each component has its own general covariance matrix),","97","            'tied' (all components share the same general covariance matrix),","98","            'diag' (each component has its own diagonal covariance matrix),","99","            'spherical' (each component has its own single variance).","121","","122","            'kmeans' : responsibilities are initialized using kmeans.","123","            'random' : responsibilities are initialized randomly.","128","","129","            'dirichlet_process' (using the Stick-breaking representation),","130","            'dirichlet_distribution' (can favor more uniform weights).","138","        than 0. If it is None, it's set to ``1. \/ n_components``.","147","    mean_prior : array-like, shape (n_features,), optional","159","","160","                (n_features, n_features) if 'full',","161","                (n_features, n_features) if 'tied',","162","                (n_features)             if 'diag',","163","                float                    if 'spherical'","184","    weights_ : array-like, shape (n_components,)","187","    means_ : array-like, shape (n_components, n_features)","193","","206","        The shape depends on ``covariance_type``::","207","","220","        time. The shape depends on ``covariance_type``::","221","","241","        ``weight_concentration_prior_type``::","242","","245","","251","    weight_concentration_ : array-like, shape (n_components,)","261","    mean_precision_ : array-like, shape (n_components,)","264","    means_prior_ : array-like, shape (n_features,)","271","    degrees_of_freedom_ : array-like, shape (n_components,)","277","","278","            (n_features, n_features) if 'full',","279","            (n_features, n_features) if 'tied',","280","            (n_features)             if 'diag',","281","            float                    if 'spherical'"],"delete":["92","    covariance_type : {'full', 'tied', 'diag', 'spherical'}, defaults to 'full'.","95","        'full' (each component has its own general covariance matrix),","96","        'tied' (all components share the same general covariance matrix),","97","        'diag' (each component has its own diagonal covariance matrix),","98","        'spherical' (each component has its own single variance).","120","        'kmeans' : responsibilities are initialized using kmeans.","121","        'random' : responsibilities are initialized randomly.","126","        'dirichlet_process' (using the Stick-breaking representation),","127","        'dirichlet_distribution' (can favor more uniform weights).","135","        than 0. If it is None, it's set to `1. \/ n_components`.","144","    mean_prior : array-like, shape (`n_features`,), optional","156","            (`n_features`, `n_features`) if 'full',","157","            (`n_features`, `n_features`) if 'tied',","158","            (`n_features`)               if 'diag',","159","            float                        if 'spherical'","180","    weights_ : array-like, shape (`n_components`,)","183","    means_ : array-like, shape (`n_components`, `n_features`)","201","        The shape depends on `covariance_type`::","214","        time. The shape depends on `covariance_type`::","234","        `weight_concentration_prior_type`::","242","    weight_concentration_ : array-like, shape (`n_components`, )","252","    mean_precision_ : array-like, shape (`n_components`, )","255","    means_prior_ : array-like, shape (`n_features`,)","262","    degrees_of_freedom_ : array-like, shape (`n_components`,)","268","            (`n_features`, `n_features`) if 'full',","269","            (`n_features`, `n_features`) if 'tied',","270","            (`n_features`)               if 'diag',","271","            float                        if 'spherical'"]}],"doc\/faq.rst":[{"add":["129",".. _selectiveness:","316","---------------------------------------------------------"],"delete":["315","----------------------------------------------------"]}],"doc\/whats_new.rst":[{"add":["43","  - **The model_selection module**","67","  - **The enhanced cv_results_ attribute**","89","  - **Parameters n_folds and n_iter renamed to n_splits**","110","    Note the change from singular to plural form in","113","  - **Fit parameter labels renamed to groups**","122","  - **Parameter n_labels renamed to n_groups**","279","     (:class:`isotonic.IsotonicRegression`) is now much faster (over 1000x in tests with synthetic","282","   - Isotonic regression (:class:`isotonic.IsotonicRegression`) now uses a better algorithm to avoid","284","     (`#6601 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/6691>`_).","356","     (`#7324 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/7325>`_)","357","     By `Eugene Chen`_ and `Raghav R V`_.","369","     (`#7419 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/7419>`_)","477","      small (``< .1 * min(X.shape)``) `n_iter` is set to 7, unless the user specifies","501","      of ``X`` to transform function when ``copy=True`` (`#7194","562","      (`#7323 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/7323>`_) by Viacheslav Kovalevskyi.","566","      If ``n_iter<2`` numerical issues are unlikely, thus no normalization is applied.","567","      Other normalization options are available: ``'none', 'LU'`` and ``'QR'``.","639","     :func:`metrics.hamming_loss`.","653","     the parameter ``n_labels`` is renamed to ``n_groups``.","655","     by `Raghav R V`_."],"delete":["43","  - **The ``model_selection`` module**","67","  - **The enhanced ``cv_results_`` attribute**","89","  - **Parameters ``n_folds`` and ``n_iter`` renamed to ``n_splits``**","110","    NOTE the change from singular to plural form in","113","  - **Fit parameter ``labels`` renamed to ``groups``**","122","  - **Parameter ``n_labels`` renamed to ``n_groups``**","279","     (:mod:`isotonic`) is now much faster (over 1000x in tests with synthetic","282","   - Isotonic regression (:mod:`isotonic`) now uses a better algorithm to avoid","284","     (`#6601 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/6691>`).","356","     (`#7324 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/7325>`)","357","     By `Eugene Chen`_ and `Raghav RV`_.","369","     (`#7419 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/7419>_`)","422","","478","      small (< .1 * min(X.shape)) `n_iter` is set to 7, unless the user specifies","502","      of `X` to transform function when `copy=True` (`#7194","563","      (`#7323 <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/7323>`_) by `Viacheslav Kovalevskyi`_.","567","      If `n_iter<2` numerical issues are unlikely, thus no normalization is applied.","568","      Other normalization options are available: 'none', 'LU' and 'QR'.","640","     :func:`metrics.classification.hamming_loss`.","654","     the parameter ``n_labels``is renamed to ``n_groups``.","656","     by `Raghav RV`_."]}],"doc\/modules\/model_evaluation.rst":[{"add":["57","===========================     =========================================     ==================================","59","===========================     =========================================     ==================================","1103","  * See :ref:`sphx_glr_auto_examples_calibration_plot_calibration.py`"],"delete":["57","==========================      =========================================     ==================================","59","==========================      =========================================     ==================================","1103","  * See :ref:`sphx_glr_calibration_plot_calibration.py`"]}],"examples\/ensemble\/plot_random_forest_regression_multioutput.py":[{"add":["6","the :ref:`multioutput.MultiOutputRegressor <multiclass>` meta-estimator.","9",":ref:`multioutput.MultiOutputRegressor <multiclass>` meta-estimator"],"delete":["6","the :ref:`multioutput.MultiOutputRegressor <_multiclass>` meta-estimator.","9",":ref:`multioutput.MultiOutputRegressor <_multiclass>` meta-estimator"]}],"sklearn\/mixture\/gaussian_mixture.py":[{"add":["452","            defaults to 'full'.","455","","456","            'full' (each component has its own general covariance matrix),","457","            'tied' (all components share the same general covariance matrix),","458","            'diag' (each component has its own diagonal covariance matrix),","459","            'spherical' (each component has its own single variance).","479","","480","            'kmeans' : responsibilities are initialized using kmeans.","481","            'random' : responsibilities are initialized randomly.","496","","530","","544","","558",""],"delete":["452","        defaults to 'full'.","455","        'full' (each component has its own general covariance matrix),","456","        'tied' (all components share the same general covariance matrix),","457","        'diag' (each component has its own diagonal covariance matrix),","458","        'spherical' (each component has its own single variance).","478","        'kmeans' : responsibilities are initialized using kmeans.","479","        'random' : responsibilities are initialized randomly."]}],"doc\/index.rst":[{"add":["209","                    <li><em>September 2016.<\/em> scikit-learn 0.18.0 is available for download (<a href=\"whats_new.html#version-0-18\">Changelog<\/a>).","210","                    <\/li>"],"delete":[]}]}},"5dd9bfe12c759dbef4e3546dfb05519e7fb60eb1":{"changes":{"sklearn\/metrics\/classification.py":"MODIFY"},"diff":{"sklearn\/metrics\/classification.py":[{"add":["1367","        The reported averages are a prevalence-weighted macro-average across","1368","        classes (equivalent to :func:`precision_recall_fscore_support` with","1369","        ``average='weighted'``).","1370","","1371","        Note that in binary classification, recall of the positive class","1372","        is also known as \"sensitivity\"; recall of the negative class is","1373","        \"specificity\".","1374","","1401","        target_names = [u'%s' % l for l in labels]","1406","    head_fmt = u'{:>{width}s} ' + u' {:>9}' * len(headers)","1407","    report = head_fmt.format(u'', *headers, width=width)","1408","    report += u'\\n\\n'","1415","    row_fmt = u'{:>{width}s} ' + u' {:>9.{digits}f}' * 3 + u' {:>9}\\n'","1416","    rows = zip(target_names, p, r, f1, s)","1417","    for row in rows:","1418","        report += row_fmt.format(*row, width=width, digits=digits)","1420","    report += u'\\n'","1423","    report += row_fmt.format(last_line_heading,","1424","                             np.average(p, weights=s),","1425","                             np.average(r, weights=s),","1426","                             np.average(f1, weights=s),","1427","                             np.sum(s),","1428","                             width=width, digits=digits)","1429",""],"delete":["1393","        target_names = ['%s' % l for l in labels]","1398","    fmt = '%% %ds' % width  # first column: class name","1399","    fmt += '  '","1400","    fmt += ' '.join(['% 9s' for _ in headers])","1401","    fmt += '\\n'","1402","","1403","    headers = [\"\"] + headers","1404","    report = fmt % tuple(headers)","1405","    report += '\\n'","1412","    for i, label in enumerate(labels):","1413","        values = [target_names[i]]","1414","        for v in (p[i], r[i], f1[i]):","1415","            values += [\"{0:0.{1}f}\".format(v, digits)]","1416","        values += [\"{0}\".format(s[i])]","1417","        report += fmt % tuple(values)","1419","    report += '\\n'","1422","    values = [last_line_heading]","1423","    for v in (np.average(p, weights=s),","1424","              np.average(r, weights=s),","1425","              np.average(f1, weights=s)):","1426","        values += [\"{0:0.{1}f}\".format(v, digits)]","1427","    values += ['{0}'.format(np.sum(s))]","1428","    report += fmt % tuple(values)"]}]}},"d0ce0d9b385cd6df52b9c64474ba3eaf1a438bba":{"changes":{"sklearn\/linear_model\/ransac.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/linear_model\/tests\/test_ransac.py":"MODIFY"},"diff":{"sklearn\/linear_model\/ransac.py":[{"add":["113","    max_skips : int, optional","114","        Maximum number of iterations that can be skipped due to finding zero","115","        inliers or invalid data defined by ``is_data_valid`` or invalid models","116","        defined by ``is_model_valid``.","117","","118","        .. versionadded:: 0.19","119","","177","    n_skips_no_inliers_ : int","178","        Number of iterations skipped due to finding zero inliers.","179","","180","        .. versionadded:: 0.19","181","","182","    n_skips_invalid_data_ : int","183","        Number of iterations skipped due to invalid data defined by","184","        ``is_data_valid``.","185","","186","        .. versionadded:: 0.19","187","","188","    n_skips_invalid_model_ : int","189","        Number of iterations skipped due to an invalid model defined by","190","        ``is_model_valid``.","191","","192","        .. versionadded:: 0.19","193","","203","                 is_model_valid=None, max_trials=100, max_skips=np.inf,","214","        self.max_skips = max_skips","328","        n_inliers_best = 1","329","        score_best = -np.inf","333","        self.n_skips_no_inliers_ = 0","334","        self.n_skips_invalid_data_ = 0","335","        self.n_skips_invalid_model_ = 0","345","            if (self.n_skips_no_inliers_ + self.n_skips_invalid_data_ +","346","                    self.n_skips_invalid_model_) > self.max_skips:","347","                break","348","","358","                self.n_skips_invalid_data_ += 1","371","                self.n_skips_invalid_model_ += 1","392","                self.n_skips_no_inliers_ += 1","428","            if ((self.n_skips_no_inliers_ + self.n_skips_invalid_data_ +","429","                    self.n_skips_invalid_model_) > self.max_skips):","430","                raise ValueError(","431","                    \"RANSAC skipped more iterations than `max_skips` without\"","432","                    \" finding a valid consensus set. Iterations were skipped\"","433","                    \" because each randomly chosen sub-sample failed the\"","434","                    \" passing criteria. See estimator attributes for\"","435","                    \" diagnostics (n_skips*).\")","436","            else:","437","                raise ValueError(","438","                    \"RANSAC could not find a valid consensus set. All\"","439","                    \" `max_trials` iterations were skipped because each\"","440","                    \" randomly chosen sub-sample failed the passing criteria.\"","441","                    \" See estimator attributes for diagnostics (n_skips*).\")","442","        else:","443","            if (self.n_skips_no_inliers_ + self.n_skips_invalid_data_ +","444","                    self.n_skips_invalid_model_) > self.max_skips:","445","                warnings.warn(\"RANSAC found a valid consensus set but exited\"","446","                              \" early due to skipping more iterations than\"","447","                              \" `max_skips`. See estimator attributes for\"","448","                              \" diagnostics (n_skips*).\",","449","                              UserWarning)"],"delete":["179","                 is_model_valid=None, max_trials=100,","303","        n_inliers_best = 0","304","        score_best = np.inf","359","            if n_inliers_subset == 0:","360","                raise ValueError(\"No inliers found, possible cause is \"","361","                    \"setting residual_threshold ({0}) too low.\".format(","362","                    self.residual_threshold))","397","            raise ValueError(","398","                \"RANSAC could not find valid consensus set, because\"","399","                \" either the `residual_threshold` rejected all the samples or\"","400","                \" `is_data_valid` and `is_model_valid` returned False for all\"","401","                \" `max_trials` randomly \"\"chosen sub-samples. Consider \"","402","                \"relaxing the \"\"constraints.\")"]}],"doc\/whats_new.rst":[{"add":["102","   - :class:`sklearn.linear_model.RANSACRegressor` no longer throws an error","103","     when calling ``fit`` if no inliers are found in its first iteration.","104","     Furthermore, causes of skipped iterations are tracked in newly added","105","     attributes, ``n_skips_*``.","106","     :issue:`7914` by :user:`Michael Horrell <mthorrell>`.","107","","108","   - Fix a bug where :class:`sklearn.feature_selection.SelectFdr` did not","109","     exactly implement Benjamini-Hochberg procedure. It formerly may have","110","     selected fewer features than it should.","111","     :issue:`7490` by :user:`Peng Meng <mpjlu>`.","112",""],"delete":[]}],"sklearn\/linear_model\/tests\/test_ransac.py":[{"add":["13","from sklearn.utils.testing import assert_raises_regexp","154","                                       residual_threshold=0.0, random_state=0,","155","                                       max_trials=5)","157","    msg = (\"RANSAC could not find a valid consensus set\")","158","    assert_raises_regexp(ValueError, msg, ransac_estimator.fit, X, y)","159","    assert_equal(ransac_estimator.n_skips_no_inliers_, 5)","160","    assert_equal(ransac_estimator.n_skips_invalid_data_, 0)","161","    assert_equal(ransac_estimator.n_skips_invalid_model_, 0)","162","","163","","164","def test_ransac_no_valid_data():","165","    def is_data_valid(X, y):","166","        return False","167","","168","    base_estimator = LinearRegression()","169","    ransac_estimator = RANSACRegressor(base_estimator,","170","                                       is_data_valid=is_data_valid,","171","                                       max_trials=5)","172","","173","    msg = (\"RANSAC could not find a valid consensus set\")","174","    assert_raises_regexp(ValueError, msg, ransac_estimator.fit, X, y)","175","    assert_equal(ransac_estimator.n_skips_no_inliers_, 0)","176","    assert_equal(ransac_estimator.n_skips_invalid_data_, 5)","177","    assert_equal(ransac_estimator.n_skips_invalid_model_, 0)","178","","179","","180","def test_ransac_no_valid_model():","181","    def is_model_valid(estimator, X, y):","182","        return False","183","","184","    base_estimator = LinearRegression()","185","    ransac_estimator = RANSACRegressor(base_estimator,","186","                                       is_model_valid=is_model_valid,","187","                                       max_trials=5)","188","","189","    msg = (\"RANSAC could not find a valid consensus set\")","190","    assert_raises_regexp(ValueError, msg, ransac_estimator.fit, X, y)","191","    assert_equal(ransac_estimator.n_skips_no_inliers_, 0)","192","    assert_equal(ransac_estimator.n_skips_invalid_data_, 0)","193","    assert_equal(ransac_estimator.n_skips_invalid_model_, 5)","194","","195","","196","def test_ransac_exceed_max_skips():","197","    def is_data_valid(X, y):","198","        return False","199","","200","    base_estimator = LinearRegression()","201","    ransac_estimator = RANSACRegressor(base_estimator,","202","                                       is_data_valid=is_data_valid,","203","                                       max_trials=5,","204","                                       max_skips=3)","205","","206","    msg = (\"RANSAC skipped more iterations than `max_skips`\")","207","    assert_raises_regexp(ValueError, msg, ransac_estimator.fit, X, y)","208","    assert_equal(ransac_estimator.n_skips_no_inliers_, 0)","209","    assert_equal(ransac_estimator.n_skips_invalid_data_, 4)","210","    assert_equal(ransac_estimator.n_skips_invalid_model_, 0)","211","","212","","213","def test_ransac_warn_exceed_max_skips():","214","    global cause_skip","215","    cause_skip = False","216","","217","    def is_data_valid(X, y):","218","        global cause_skip","219","        if not cause_skip:","220","            cause_skip = True","221","            return True","222","        else:","223","            return False","224","","225","    base_estimator = LinearRegression()","226","    ransac_estimator = RANSACRegressor(base_estimator,","227","                                       is_data_valid=is_data_valid,","228","                                       max_skips=3,","229","                                       max_trials=5)","230","","231","    assert_warns(UserWarning, ransac_estimator.fit, X, y)","232","    assert_equal(ransac_estimator.n_skips_no_inliers_, 0)","233","    assert_equal(ransac_estimator.n_skips_invalid_data_, 4)","234","    assert_equal(ransac_estimator.n_skips_invalid_model_, 0)"],"delete":["10","from sklearn.utils.testing import assert_raises_regexp","154","                                       residual_threshold=0.0, random_state=0)","156","    assert_raises_regexp(ValueError,","157","                    \"No inliers.*residual_threshold.*0\\.0\",","158","                    ransac_estimator.fit, X, y)"]}]}}}