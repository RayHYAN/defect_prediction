{"02cc6f5d3a4363fcc197b511f0fdfe27a7497d5b":{"changes":{"doc\/whats_new.rst":"MODIFY","sklearn\/utils\/estimator_checks.py":"MODIFY","sklearn\/decomposition\/nmf.py":"MODIFY","sklearn\/utils\/tests\/test_estimator_checks.py":"MODIFY"},"diff":{"doc\/whats_new.rst":[{"add":["67","   - ``check_estimator`` now attempts to ensure that methods transform, predict, etc.","68","     do not set attributes on the estimator.","69","     :issue:`7533` by `Ekaterina Krivich`_.","70","","74","   - Fix a bug where :class:`sklearn.feature_selection.SelectFdr` did not","94","   - Fixed a bug where :class:`decomposition.NMF` sets its ``n_iters_``","95","     attribute in `transform()`. :issue:`7553` by `Ekaterina Krivich`_.","96",""],"delete":["70","   - Fix a bug where :class:`sklearn.feature_selection.SelectFdr` did not "]}],"sklearn\/utils\/estimator_checks.py":[{"add":["32","from sklearn.utils.testing import assert_dict_equal","233","    yield check_dict_unchanged","413","def check_dict_unchanged(name, Estimator):","414","    # this estimator raises","415","    # ValueError: Found array with 0 feature(s) (shape=(23, 0))","416","    # while a minimum of 1 is required.","417","    # error","418","    if name in ['SpectralCoclustering']:","419","        return","420","    rnd = np.random.RandomState(0)","421","    if name in ['RANSACRegressor']:","422","        X = 3 * rnd.uniform(size=(20, 3))","423","    else:","424","        X = 2 * rnd.uniform(size=(20, 3))","425","","426","    y = X[:, 0].astype(np.int)","427","    y = multioutput_estimator_convert_y_2d(name, y)","428","    estimator = Estimator()","429","    set_testing_parameters(estimator)","430","    if hasattr(estimator, \"n_components\"):","431","        estimator.n_components = 1","432","","433","    if hasattr(estimator, \"n_clusters\"):","434","        estimator.n_clusters = 1","435","","436","    if hasattr(estimator, \"n_best\"):","437","        estimator.n_best = 1","438","","439","    set_random_state(estimator, 1)","440","","441","    # should be just `estimator.fit(X, y)`","442","    # after merging #6141","443","    if name in ['SpectralBiclustering']:","444","        estimator.fit(X)","445","    else:","446","        estimator.fit(X, y)","447","    for method in [\"predict\", \"transform\", \"decision_function\",","448","                   \"predict_proba\"]:","449","        if hasattr(estimator, method):","450","            dict_before = estimator.__dict__.copy()","451","            getattr(estimator, method)(X)","452","            assert_dict_equal(estimator.__dict__, dict_before,","453","                              'Estimator changes __dict__ during %s' % method)","454","","455",""],"delete":[]}],"sklearn\/decomposition\/nmf.py":[{"add":[],"delete":["1018","        Attributes","1019","        ----------","1020","        components_ : array-like, shape (n_components, n_features)","1021","            Factorization matrix, sometimes called 'dictionary'.","1022","","1023","        n_iter_ : int","1024","            Actual number of iterations for the transform.","1025","","1063","        Attributes","1064","        ----------","1065","        components_ : array-like, shape (n_components, n_features)","1066","            Factorization matrix, sometimes called 'dictionary'.","1067","","1068","        n_iter_ : int","1069","            Actual number of iterations for the transform.","1070","","1086","        Attributes","1087","        ----------","1088","        n_iter_ : int","1089","            Actual number of iterations for the transform.","1090","","1108","        self.n_iter_ = n_iter_"]}],"sklearn\/utils\/tests\/test_estimator_checks.py":[{"add":["30","class ChangesDict(BaseEstimator):","31","    def __init__(self):","32","        self.key = 0","33","","34","    def fit(self, X, y=None):","35","        X, y = check_X_y(X, y)","36","        return self","37","","38","    def predict(self, X):","39","        X = check_array(X)","40","        self.key = 1000","41","        return np.ones(X.shape[0])","42","","43","","91","    # check that estimator state does not change","92","    # at transform\/predict\/predict_proba time","93","    msg = 'Estimator changes __dict__ during predict'","94","    assert_raises_regex(AssertionError, msg, check_estimator, ChangesDict)","95",""],"delete":[]}]}},"e5ceda88f2a24b3dd4f9a94404828f982cdf52ad":{"changes":{"sklearn\/utils\/validation.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/utils\/tests\/test_validation.py":"MODIFY"},"diff":{"sklearn\/utils\/validation.py":[{"add":["202","    accept_sparse : string, boolean or list\/tuple of strings","204","        'csr', 'coo', 'dok', 'bsr', 'lil', 'dia'). If the input is sparse but","205","        not in the allowed format, it will be converted to the first listed","206","        format. True allows the input to be any format. False means","207","        that a sparse matrix input will raise an error.","209","    dtype : string, type or None","212","    copy : boolean","216","    force_all_finite : boolean","228","","229","    if isinstance(accept_sparse, six.string_types):","230","        accept_sparse = [accept_sparse]","231","","232","    if accept_sparse is False:","233","        raise TypeError('A sparse matrix was passed, but dense '","234","                        'data is required. Use X.toarray() to '","235","                        'convert to a dense numpy array.')","236","    elif isinstance(accept_sparse, (list, tuple)):","237","        if len(accept_sparse) == 0:","238","            raise ValueError(\"When providing 'accept_sparse' \"","239","                             \"as a tuple or list, it must contain at \"","240","                             \"least one string value.\")","241","        # ensure correct sparse format","242","        if spmatrix.format not in accept_sparse:","243","            # create new with correct sparse","244","            spmatrix = spmatrix.asformat(accept_sparse[0])","245","            changed_format = True","246","    elif accept_sparse is not True:","247","        # any other type","248","        raise ValueError(\"Parameter 'accept_sparse' should be a string, \"","249","                         \"boolean or list of strings. You provided \"","250","                         \"'accept_sparse={}'.\".format(accept_sparse))","268","def check_array(array, accept_sparse=False, dtype=\"numeric\", order=None,","283","    accept_sparse : string, boolean or list\/tuple of strings (default=False)","285","        'csr', etc. If the input is sparse but not in the allowed format,","286","        it will be converted to the first listed format. True allows the input","287","        to be any format. False means that a sparse matrix input will","288","        raise an error.","339","    # accept_sparse 'None' deprecation check","340","    if accept_sparse is None:","341","        warnings.warn(","342","            \"Passing 'None' to parameter 'accept_sparse' in methods \"","343","            \"check_array and check_X_y is deprecated in version 0.19 \"","344","            \"and will be removed in 0.21. Use 'accept_sparse=False' \"","345","            \" instead.\", DeprecationWarning)","346","        accept_sparse = False","430","def check_X_y(X, y, accept_sparse=False, dtype=\"numeric\", order=None,","451","    accept_sparse : string, boolean or list of string (default=False)","453","        'csr', etc. If the input is sparse but not in the allowed format,","454","        it will be converted to the first listed format. True allows the input","455","        to be any format. False means that a sparse matrix input will","456","        raise an error."],"delete":["202","    accept_sparse : string, list of string or None (default=None)","204","        'csr', 'coo', 'dok', 'bsr', 'lil', 'dia'). None means that sparse","205","        matrix input will raise an error.  If the input is sparse but not in","206","        the allowed format, it will be converted to the first listed format.","208","    dtype : string, type or None (default=none)","211","    copy : boolean (default=False)","215","    force_all_finite : boolean (default=True)","223","    if accept_sparse in [None, False]:","224","        raise TypeError('A sparse matrix was passed, but dense '","225","                        'data is required. Use X.toarray() to '","226","                        'convert to a dense numpy array.')","231","    if (isinstance(accept_sparse, (list, tuple))","232","            and spmatrix.format not in accept_sparse):","233","        # create new with correct sparse","234","        spmatrix = spmatrix.asformat(accept_sparse[0])","235","        changed_format = True","253","def check_array(array, accept_sparse=None, dtype=\"numeric\", order=None,","268","    accept_sparse : string, list of string or None (default=None)","270","        'csr', etc.  None means that sparse matrix input will raise an error.","271","        If the input is sparse but not in the allowed format, it will be","272","        converted to the first listed format.","323","    if isinstance(accept_sparse, str):","324","        accept_sparse = [accept_sparse]","408","def check_X_y(X, y, accept_sparse=None, dtype=\"numeric\", order=None,","429","    accept_sparse : string, list of string or None (default=None)","431","        'csr', etc.  None means that sparse matrix input will raise an error.","432","        If the input is sparse but not in the allowed format, it will be","433","        converted to the first listed format."]}],"doc\/whats_new.rst":[{"add":["100","     ","101","   - Added type checking to the ``accept_sparse`` parameter in","102","     :mod:`sklearn.utils.validation` methods. This parameter now accepts only","103","     boolean, string, or list\/tuple of strings. ``accept_sparse=None`` is deprecated","104","     and should be replaced by ``accept_sparse=False``.","105","     :issue:`7880` by :user:`Josh Karnofsky <jkarno>`."],"delete":[]}],"sklearn\/utils\/tests\/test_validation.py":[{"add":["323","def test_check_array_accept_sparse_type_exception():","324","    X = [[1, 2], [3, 4]]","325","    X_csr = sp.csr_matrix(X)","326","    invalid_type = SVR()","327","","328","    msg = (\"A sparse matrix was passed, but dense data is required. \"","329","           \"Use X.toarray() to convert to a dense numpy array.\")","330","    assert_raise_message(TypeError, msg,","331","                         check_array, X_csr, accept_sparse=False)","332","    assert_raise_message(TypeError, msg,","333","                         check_array, X_csr, accept_sparse=None)","334","","335","    msg = (\"Parameter 'accept_sparse' should be a string, \"","336","           \"boolean or list of strings. You provided 'accept_sparse={}'.\")","337","    assert_raise_message(ValueError, msg.format(invalid_type),","338","                         check_array, X_csr, accept_sparse=invalid_type)","339","","340","    msg = (\"When providing 'accept_sparse' as a tuple or list, \"","341","           \"it must contain at least one string value.\")","342","    assert_raise_message(ValueError, msg.format([]),","343","                         check_array, X_csr, accept_sparse=[])","344","    assert_raise_message(ValueError, msg.format(()),","345","                         check_array, X_csr, accept_sparse=())","346","","347","    msg = \"'SVR' object\"","348","    assert_raise_message(TypeError, msg,","349","                         check_array, X_csr, accept_sparse=[invalid_type])","350","","351","    # Test deprecation of 'None'","352","    assert_warns(DeprecationWarning, check_array, X, accept_sparse=None)","353","","354","","355","def test_check_array_accept_sparse_no_exception():","356","    X = [[1, 2], [3, 4]]","357","    X_csr = sp.csr_matrix(X)","358","","359","    check_array(X_csr, accept_sparse=True)","360","    check_array(X_csr, accept_sparse='csr')","361","    check_array(X_csr, accept_sparse=['csr'])","362","    check_array(X_csr, accept_sparse=('csr',))","363","","364",""],"delete":[]}]}},"1c41368bac124ab336fb8c7fd4bc846d266228e5":{"changes":{"sklearn\/decomposition\/tests\/test_dict_learning.py":"MODIFY","sklearn\/feature_extraction\/text.py":"MODIFY","sklearn\/naive_bayes.py":"MODIFY","sklearn\/feature_selection\/rfe.py":"MODIFY","sklearn\/tests\/test_multiclass.py":"MODIFY","sklearn\/cluster\/tests\/test_k_means.py":"MODIFY","sklearn\/utils\/multiclass.py":"MODIFY","sklearn\/covariance\/outlier_detection.py":"MODIFY","sklearn\/decomposition\/truncated_svd.py":"MODIFY","sklearn\/tests\/test_multioutput.py":"MODIFY","sklearn\/feature_extraction\/tests\/test_text.py":"MODIFY","sklearn\/feature_selection\/tests\/test_from_model.py":"MODIFY","sklearn\/neighbors\/approximate.py":"MODIFY","sklearn\/decomposition\/dict_learning.py":"MODIFY","sklearn\/multioutput.py":"MODIFY","sklearn\/ensemble\/gradient_boosting.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/dummy.py":"MODIFY","sklearn\/feature_selection\/from_model.py":"MODIFY","sklearn\/multiclass.py":"MODIFY","doc\/modules\/model_evaluation.rst":"MODIFY","sklearn\/ensemble\/base.py":"MODIFY"},"diff":{"sklearn\/decomposition\/tests\/test_dict_learning.py":[{"add":["1","import itertools","28","def test_sparse_encode_shapes_omp():","29","    rng = np.random.RandomState(0)","30","    algorithms = ['omp', 'lasso_lars', 'lasso_cd', 'lars', 'threshold']","31","    for n_components, n_samples in itertools.product([1, 5], [1, 9]):","32","        X_ = rng.randn(n_samples, n_features)","33","        dictionary = rng.randn(n_components, n_features)","34","        for algorithm, n_jobs in itertools.product(algorithms, [1, 3]):","35","            code = sparse_encode(X_, dictionary, algorithm=algorithm,","36","                                 n_jobs=n_jobs)","37","            assert_equal(code.shape, (n_samples, n_components))","38","","39","","43","    assert_equal(dico.components_.shape, (n_components, n_features))","44","","45","    n_components = 1","46","    dico = DictionaryLearning(n_components, random_state=0).fit(X)","47","    assert_equal(dico.components_.shape, (n_components, n_features))","48","    assert_equal(dico.transform(X).shape, (X.shape[0], n_components))"],"delete":["30","    assert_true(dico.components_.shape == (n_components, n_features))"]}],"sklearn\/feature_extraction\/text.py":[{"add":["1037","            self._idf_diag = sp.spdiags(idf, diags=0, m=n_features,","1089","        # if _idf_diag is not set, this will raise an attribute error,","1090","        # which means hasattr(self, \"idf_\") is False","1091","        return np.ravel(self._idf_diag.sum(axis=0))"],"delete":["31","from ..utils import deprecated","1038","            self._idf_diag = sp.spdiags(idf, diags=0, m=n_features, ","1090","        if hasattr(self, \"_idf_diag\"):","1091","            return np.ravel(self._idf_diag.sum(axis=0))","1092","        else:","1093","            return None"]}],"sklearn\/naive_bayes.py":[{"add":["486","        classes : array-like, shape = [n_classes] (default=None)","492","        sample_weight : array-like, shape = [n_samples] (default=None)","557","        sample_weight : array-like, shape = [n_samples], (default=None)"],"delete":["486","        classes : array-like, shape = [n_classes], optional (default=None)","492","        sample_weight : array-like, shape = [n_samples], optional (default=None)","557","        sample_weight : array-like, shape = [n_samples], optional (default=None)"]}],"sklearn\/feature_selection\/rfe.py":[{"add":["32","","296","        :class:`sklearn.model_selection.StratifiedKFold` is used. If the","297","        estimator is a classifier or if ``y`` is neither binary nor multiclass,"],"delete":["295","        :class:`sklearn.model_selection.StratifiedKFold` is used. If the ","296","        estimator is a classifier or if ``y`` is neither binary nor multiclass, "]}],"sklearn\/tests\/test_multiclass.py":[{"add":["15","from sklearn.utils.multiclass import (check_classification_targets,","16","                                      type_of_target)","107","    # test partial_fit only exists if estimator has it:","108","    ovr = OneVsRestClassifier(SVC())","109","    assert_false(hasattr(ovr, \"partial_fit\"))","110","","435","    for base_classifier in [SVC(kernel='linear', random_state=0),","436","                            LinearSVC(random_state=0)]:","447","            assert_equal(sp.issparse(ovr.estimators_[0].coef_),","448","                         sp.issparse(ovr.coef_))","517","    # test partial_fit only exists if estimator has it:","518","    ovr = OneVsOneClassifier(SVC())","519","    assert_false(hasattr(ovr, \"partial_fit\"))","520","","619","def test_ovo_one_class():","620","    # Test error for OvO with one class","621","    X = np.eye(4)","622","    y = np.array(['a'] * 4)","623","","624","    ovo = OneVsOneClassifier(LinearSVC())","625","    assert_raise_message(ValueError, \"when only one class\", ovo.fit, X, y)","626","","627","","628","def test_ovo_float_y():","629","    # Test that the OvO errors on float targets","630","    X = iris.data","631","    y = iris.data[:, 0]","632","","633","    ovo = OneVsOneClassifier(LinearSVC())","634","    assert_raise_message(ValueError, \"Unknown label type\", ovo.fit, X, y)","635","","636","","665","def test_ecoc_float_y():","666","    # Test that the OCC errors on float targets","667","    X = iris.data","668","    y = iris.data[:, 0]","669","","670","    ovo = OutputCodeClassifier(LinearSVC())","671","    assert_raise_message(ValueError, \"Unknown label type\", ovo.fit, X, y)","672","","673",""],"delete":["15","from sklearn.utils.multiclass import check_classification_targets, type_of_target","430","    for base_classifier in [SVC(kernel='linear', random_state=0), LinearSVC(random_state=0)]:","441","            assert_equal(sp.issparse(ovr.estimators_[0].coef_), sp.issparse(ovr.coef_))"]}],"sklearn\/cluster\/tests\/test_k_means.py":[{"add":["406","    # should no longer be good"],"delete":["406","    # should not longer be good"]}],"sklearn\/utils\/multiclass.py":[{"add":["25","","158","","172","                      'multilabel-indicator', 'multilabel-sequences']:"],"delete":["170","            'multilabel-indicator', 'multilabel-sequences']:","174",""]}],"sklearn\/covariance\/outlier_detection.py":[{"add":["17","from ..utils.validation import check_is_fitted, check_array","18","from ..metrics import accuracy_score","65","        X = check_array(X)","93","        X = check_array(X)","103","    def score(self, X, y, sample_weight=None):","104","        \"\"\"Returns the mean accuracy on the given test data and labels.","106","        In multi-label classification, this is the subset accuracy","107","        which is a harsh metric since you require for each sample that","108","        each label set be correctly predicted.","109","","110","        Parameters","111","        ----------","112","        X : array-like, shape = (n_samples, n_features)","113","            Test samples.","114","","115","        y : array-like, shape = (n_samples,) or (n_samples, n_outputs)","116","            True labels for X.","117","","118","        sample_weight : array-like, shape = (n_samples,), optional","119","            Sample weights.","120","","121","        Returns","122","        -------","123","        score : float","124","            Mean accuracy of self.predict(X) wrt. y.","125","","126","        \"\"\"","127","        return accuracy_score(y, self.predict(X), sample_weight=sample_weight)","128","","129","","130","class EllipticEnvelope(OutlierDetectionMixin, MinCovDet):"],"delete":["17","from ..base import ClassifierMixin","18","from ..utils.validation import check_is_fitted","69","            check_is_fitted(self, 'threshold_')","103","class EllipticEnvelope(ClassifierMixin, OutlierDetectionMixin, MinCovDet):"]}],"sklearn\/decomposition\/truncated_svd.py":[{"add":["13","from ..utils import check_array, check_random_state","155","        X = check_array(X, accept_sparse=['csr', 'csc'])"],"delete":["13","from ..utils import check_array, as_float_array, check_random_state","155","        X = as_float_array(X, copy=False)","158","        # If sparse and not csr or csc, convert to csr","159","        if sp.issparse(X) and X.getformat() not in [\"csr\", \"csc\"]:","160","            X = X.tocsr()","161",""]}],"sklearn\/tests\/test_multioutput.py":[{"add":["8","from sklearn.utils.testing import assert_raise_message","339","    # ValueError when y is continuous","340","    assert_raise_message(ValueError, \"Unknown label type\", moc.fit, X, X[:, 1])"],"delete":[]}],"sklearn\/feature_extraction\/tests\/test_text.py":[{"add":["422","    assert_false(hasattr(t2, \"idf_\"))"],"delete":["422","    assert_equal(t2.idf_, None)"]}],"sklearn\/feature_selection\/tests\/test_from_model.py":[{"add":["3","from sklearn.utils.testing import assert_false","123","    # check that if est doesn't have partial_fit, neither does SelectFromModel","124","    transformer = SelectFromModel(estimator=RandomForestClassifier())","125","    assert_false(hasattr(transformer, \"partial_fit\"))","126","","178","    model = SelectFromModel(clf, threshold=\"0.1 * mean\")","183","    model.threshold = \"1.0 * mean\""],"delete":["173","    model = SelectFromModel(clf, threshold=0.1)","178","    model.threshold = 1.0"]}],"sklearn\/neighbors\/approximate.py":[{"add":["95","                 n_components=32,"],"delete":["95","                 n_components=8,"]}],"sklearn\/decomposition\/dict_learning.py":[{"add":["96","    n_components = dictionary.shape[0]","97","    if dictionary.shape[1] != X.shape[1]:","98","        raise ValueError(\"Dictionary and X have different numbers of features:\"","99","                         \"dictionary.shape: {} X.shape{}\".format(","100","                             dictionary.shape, X.shape))","164","    if new_code.ndim != 2:","165","        return new_code.reshape(n_samples, n_components)","736","                print(\"Iteration % 3i (elapsed time: % 3is, % 4.1fmn)\"","737","                      % (ii, dt, dt \/ 60))","910","    _required_parameters = [\"dictionary\"]"],"delete":["283","        # This ensure that dimensionality of code is always 2,","284","        # consistant with the case n_jobs > 1","285","        if code.ndim == 1:","286","            code = code[np.newaxis, :]","733","                print (\"Iteration % 3i (elapsed time: % 3is, % 4.1fmn)\"","734","                       % (ii, dt, dt \/ 60))","822","        # XXX : kwargs is not documented"]}],"sklearn\/multioutput.py":[{"add":["18","from abc import ABCMeta, abstractmethod","19","from .base import BaseEstimator, clone, MetaEstimatorMixin","25","from .utils.multiclass import check_classification_targets","60","class MultiOutputEstimator(six.with_metaclass(ABCMeta, BaseEstimator,","61","                                              MetaEstimatorMixin)):","62","    @abstractmethod","153","        if isinstance(self, ClassifierMixin):","154","            check_classification_targets(y)","155",""],"delete":["18","from abc import ABCMeta","19","from .base import BaseEstimator, clone","59","class MultiOutputEstimator(six.with_metaclass(ABCMeta, BaseEstimator)):","60",""]}],"sklearn\/ensemble\/gradient_boosting.py":[{"add":["66","class QuantileEstimator(object):","88","class MeanEstimator(object):","104","class LogOddsEstimator(object):","134","class PriorProbabilityEstimator(object):","152","class ZeroEstimator(object):"],"delete":["66","class QuantileEstimator(BaseEstimator):","88","class MeanEstimator(BaseEstimator):","104","class LogOddsEstimator(BaseEstimator):","134","class PriorProbabilityEstimator(BaseEstimator):","152","class ZeroEstimator(BaseEstimator):"]}],"doc\/whats_new.rst":[{"add":["215","     in R (lars library). :issue:`7849` by :user:`Jair Montoya Martinez <jmontoyam>`.","216","","226","   - Fix a bug regarding fitting :class:`sklearn.cluster.KMeans` with a sparse","227","     array X and initial centroids, where X's means were unnecessarily being","228","     subtracted from the centroids. :issue:`7872` by :user:`Josh Karnofsky <jkarno>`.","252","   - Fixes to the input validation in","253","     :class:`sklearn.covariance.EllipticEnvelope`.","254","     :issue:`8086` by `Andreas M¨¹ller`_.","255","","256","   - Fix output shape and bugs with n_jobs > 1 in  ","257","     :class:`sklearn.decomposition.SparseCoder` transform and :func:`sklarn.decomposition.sparse_encode`","258","     for one-dimensional data and one component.","259","     This also impacts the output shape of :class:`sklearn.decomposition.DictionaryLearning`.","260","     :issue:`8086` by `Andreas M¨¹ller`_.","261","","262","   - Several fixes to input validation in","263","     :class:`multiclass.OutputCodeClassifier`","264","     :issue:`8086` by `Andreas M¨¹ller`_.","265","","353","","354","   - Gradient boosting base models are no longer estimators. By `Andreas M¨¹ller`_.","355","","356","   - :class:`feature_selection.SelectFromModel` now validates the ``threshold``","357","     parameter and sets the ``threshold_`` attribute during the call to","358","     ``fit``, and no longer during the call to ``transform```, by `Andreas","359","     M¨¹ller`_.","360","","361","   - :class:`feature_selection.SelectFromModel` now has a ``partial_fit``","362","     method only if the underlying estimator does. By `Andreas M¨¹ller`_.","363","","364","   - :class:`multiclass.OneVsRestClassifier` now has a ``partial_fit`` method","365","     only if the underlying estimator does.  By `Andreas M¨¹ller`_. ","366",""],"delete":["215","     in R (lars library). :issue:`7849` by :user:`Jair Montoya Martinez <jmontoyam>`","225","   - Fix a bug regarding fitting :class:`sklearn.cluster.KMeans` with a","226","     sparse array X and initial centroids, where X's means were unnecessarily","227","     being subtracted from the centroids. :issue:`7872` by `Josh Karnofsky <https:\/\/github.com\/jkarno>`_."]}],"sklearn\/dummy.py":[{"add":["122","        check_consistent_length(X, y)","123","","188","        if self.n_outputs_ == 1 and not self.output_2d_:","197","            if self.n_outputs_ == 1 and not self.output_2d_:","403",""],"delete":["186","        if self.n_outputs_ == 1:","195","            if self.n_outputs_ == 1:"]}],"sklearn\/feature_selection\/from_model.py":[{"add":["6","from ..base import BaseEstimator, clone, MetaEstimatorMixin","11","from ..utils.metaestimators import if_delegate_has_method","79","class SelectFromModel(BaseEstimator, SelectorMixin, MetaEstimatorMixin):","138","                'Either fit SelectFromModel before transform or set \"prefit='","139","                'True\" and pass a fitted estimator to the constructor.')","141","        threshold = _calculate_threshold(estimator, scores, self.threshold)","142","        return scores >= threshold","170","    @property","171","    def threshold_(self):","172","        scores = _get_feature_importances(self.estimator_, self.norm_order)","173","        return _calculate_threshold(self.estimator, scores, self.threshold)","174","","175","    @if_delegate_has_method('estimator')"],"delete":["6","from ..base import BaseEstimator, clone","78","class SelectFromModel(BaseEstimator, SelectorMixin):","123","","138","                'Either fit the model before transform or set \"prefit=True\"'","139","                ' while passing the fitted estimator to the constructor.')","141","        self.threshold_ = _calculate_threshold(estimator, scores,","142","                                               self.threshold)","143","        return scores >= self.threshold_"]}],"sklearn\/multiclass.py":[{"add":["48","from .utils.validation import check_X_y, check_array","218","    @if_delegate_has_method('estimator')","490","        check_classification_targets(y)","493","        if len(self.classes_) == 1:","494","            raise ValueError(\"OneVsOneClassifier can not be fit when only one\"","495","                             \" class is present.\")","504","            self.pairwise_indices_ = (","505","                estimators_indices[1] if self._pairwise else None)","511","    @if_delegate_has_method(delegate='estimator')","551","                for estimator, (i, j) in izip(self.estimators_,","552","                                              (combinations)))","710","        X, y = check_X_y(X, y)","717","        check_classification_targets(y)","758","        X = check_array(X)"],"delete":["48","from .utils.validation import check_X_y","178","","500","            self.pairwise_indices_ = estimators_indices[1] \\","501","                                     if self._pairwise else None","546","                for estimator, (i, j) in izip(","547","                        self.estimators_, (combinations)))"]}],"doc\/modules\/model_evaluation.rst":[{"add":["175","    >>> ground_truth = [[1], [1]]"],"delete":["175","    >>> ground_truth = [[1, 1]]"]}],"sklearn\/ensemble\/base.py":[{"add":["14","from ..externals import six","15","from abc import ABCMeta, abstractmethod","59","class BaseEnsemble(six.with_metaclass(ABCMeta, BaseEstimator,","60","                                      MetaEstimatorMixin)):","87","    @abstractmethod"],"delete":["57","class BaseEnsemble(BaseEstimator, MetaEstimatorMixin):"]}]}},"75d6005feacfedad33df8bed31e34f7bec51f62d":{"changes":{"sklearn\/model_selection\/_split.py":"MODIFY","sklearn\/model_selection\/tests\/test_search.py":"MODIFY","sklearn\/model_selection\/tests\/test_split.py":"MODIFY","doc\/modules\/cross_validation.rst":"MODIFY","sklearn\/model_selection\/_search.py":"MODIFY"},"diff":{"sklearn\/model_selection\/_split.py":[{"add":["85","","86","        Note","87","        ----","88","        Randomized CV splitters may return different results for each call of","89","        split. You can make the results identical by setting ``random_state``","90","        to an integer.","316","","317","        Note","318","        ----","319","        Randomized CV splitters may return different results for each call of","320","        split. You can make the results identical by setting ``random_state``","321","        to an integer.","581","        rng = self.random_state","656","","657","        Note","658","        ----","659","        Randomized CV splitters may return different results for each call of","660","        split. You can make the results identical by setting ``random_state``","661","        to an integer.","743","","744","        Note","745","        ----","746","        Randomized CV splitters may return different results for each call of","747","        split. You can make the results identical by setting ``random_state``","748","        to an integer.","1187","","1188","        Note","1189","        ----","1190","        Randomized CV splitters may return different results for each call of","1191","        split. You can make the results identical by setting ``random_state``","1192","        to an integer.","1607","","1608","        Note","1609","        ----","1610","        Randomized CV splitters may return different results for each call of","1611","        split. You can make the results identical by setting ``random_state``","1612","        to an integer."],"delete":["569","        if self.shuffle:","570","            rng = check_random_state(self.random_state)","571","        else:","572","            rng = self.random_state"]}],"sklearn\/model_selection\/tests\/test_search.py":[{"add":["9","from types import GeneratorType","1073","        assert_false(np.allclose(cv_results['mean_test_score'][1],","1074","                                 cv_results['mean_test_score'][2]))","1075","        assert_false(np.allclose(cv_results['mean_train_score'][1],","1076","                                 cv_results['mean_train_score'][2]))","1418","    # Give generator as a cv parameter","1419","    assert_true(isinstance(KFold(n_splits=n_splits,","1420","                                 shuffle=True, random_state=0).split(X, y),","1421","                           GeneratorType))","1422","    gs3 = GridSearchCV(LinearSVC(random_state=0),","1423","                       param_grid={'C': [0.1, 0.2, 0.3]},","1424","                       cv=KFold(n_splits=n_splits, shuffle=True,","1425","                                random_state=0).split(X, y))","1426","    gs3.fit(X, y)","1427","","1428","    gs4 = GridSearchCV(LinearSVC(random_state=0),","1429","                       param_grid={'C': [0.1, 0.2, 0.3]},","1430","                       cv=KFold(n_splits=n_splits, shuffle=True,","1431","                                random_state=0))","1432","    gs4.fit(X, y)","1433","","1434","    def _pop_time_keys(cv_results):","1435","        for key in ('mean_fit_time', 'std_fit_time',","1436","                    'mean_score_time', 'std_score_time'):","1437","            cv_results.pop(key)","1438","        return cv_results","1439","","1440","    # Check if generators are supported as cv and","1441","    # that the splits are consistent","1442","    np.testing.assert_equal(_pop_time_keys(gs3.cv_results_),","1443","                            _pop_time_keys(gs4.cv_results_))","1444",""],"delete":["1072","        try:","1073","            assert_almost_equal(cv_results['mean_test_score'][1],","1074","                                cv_results['mean_test_score'][2])","1075","        except AssertionError:","1076","            pass","1077","        try:","1078","            assert_almost_equal(cv_results['mean_train_score'][1],","1079","                                cv_results['mean_train_score'][2])","1080","        except AssertionError:","1081","            pass"]}],"sklearn\/model_selection\/tests\/test_split.py":[{"add":["448","            # Test if the two splits are different","449","            # numpy's assert_equal properly compares nested lists","451","                np.testing.assert_array_equal(list(cv.split(*data)),","452","                                              list(cv.split(*data)))","1192","    # numpy's assert_array_equal properly compares nested lists"],"delete":["449","                np.testing.assert_equal(list(cv.split(*data)),","450","                                        list(cv.split(*data)))"]}],"doc\/modules\/cross_validation.rst":[{"add":["727","* To get identical results for each split, set ``random_state`` to an integer."],"delete":["727","* To ensure results are repeatable (*on the same platform*), use a fixed value","728","  for ``random_state``."]}],"sklearn\/model_selection\/_search.py":[{"add":["926","        |param_kernel|param_gamma|param_degree|split0_test_score|...|rank_t...|"],"delete":["926","        |param_kernel|param_gamma|param_degree|split0_test_score|...|..rank...|"]}]}},"9fd70a833c8e05ffc8e6819a43ced405307142a5":{"changes":{"sklearn\/tests\/test_multiclass.py":"MODIFY","sklearn\/multiclass.py":"MODIFY"},"diff":{"sklearn\/tests\/test_multiclass.py":[{"add":["3","from sklearn.utils.testing import assert_array_equal, assert_raises_regex","24","                                  Perceptron, LogisticRegression,","25","                                  SGDClassifier)","92","    # with SGDClassifier","93","    X = np.abs(np.random.randn(14, 2))","94","    y = [1, 1, 1, 1, 2, 3, 3, 0, 0, 2, 3, 1, 2, 3]","96","    ovr = OneVsRestClassifier(SGDClassifier(n_iter=1, shuffle=False,","97","                                            random_state=0))","98","    ovr.partial_fit(X[:7], y[:7], np.unique(y))","99","    ovr.partial_fit(X[7:], y[7:])","100","    pred = ovr.predict(X)","101","    ovr1 = OneVsRestClassifier(SGDClassifier(n_iter=1, shuffle=False,","102","                                             random_state=0))","103","    pred1 = ovr1.fit(X, y).predict(X)","104","    assert_equal(np.mean(pred == y), np.mean(pred1 == y))","105","","106","","107","def test_ovr_partial_fit_exceptions():","108","    ovr = OneVsRestClassifier(MultinomialNB())","109","    X = np.abs(np.random.randn(14, 2))","110","    y = [1, 1, 1, 1, 2, 3, 3, 0, 0, 2, 3, 1, 2, 3]","111","    ovr.partial_fit(X[:7], y[:7], np.unique(y))","112","    # A new class value which was not in the first call of partial_fit","113","    # It should raise ValueError","114","    y1 = [5] + y[7:-1]","115","    assert_raises_regex(ValueError, \"Mini-batch contains \\[.+\\] while classes\"","116","                                    \" must be subset of \\[.+\\]\",","117","                        ovr.partial_fit, X=X[7:], y=y1)","121","    # test that ovr and ovo work on regressors which don't have a decision_","122","    # function"],"delete":["3","from sklearn.utils.testing import assert_array_equal","24","                                  Perceptron, LogisticRegression)","91","    ovr = OneVsRestClassifier(MultinomialNB())","92","    ovr.partial_fit(iris.data[:60], iris.target[:60], np.unique(iris.target))","93","    ovr.partial_fit(iris.data[60:], iris.target[60:])","94","    pred = ovr.predict(iris.data)","95","    ovr2 = OneVsRestClassifier(MultinomialNB())","96","    pred2 = ovr2.fit(iris.data, iris.target).predict(iris.data)","98","    assert_almost_equal(pred, pred2)","99","    assert_equal(len(ovr.estimators_), len(np.unique(iris.target)))","100","    assert_greater(np.mean(iris.target == pred), 0.65)","104","    # test that ovr and ovo work on regressors which don't have a decision_function","206",""]}],"sklearn\/multiclass.py":[{"add":["246","            if not hasattr(self.estimator, \"partial_fit\"):","247","                raise ValueError((\"Base estimator {0}, doesn't have \"","248","                                 \"partial_fit method\").format(self.estimator))","252","            # A sparse LabelBinarizer, with sparse_output=True, has been","253","            # shown to outperform or match a dense label binarizer in all","254","            # cases and has also resulted in less or equal memory consumption","255","            # in the fit_ovr function overall.","256","            self.label_binarizer_ = LabelBinarizer(sparse_output=True)","257","            self.label_binarizer_.fit(self.classes_)","258","","259","        if np.setdiff1d(y, self.classes_):","260","            raise ValueError((\"Mini-batch contains {0} while classes \" +","261","                             \"must be subset of {1}\").format(np.unique(y),","262","                                                             self.classes_))","263","","264","        Y = self.label_binarizer_.transform(y)","268","        self.estimators_ = Parallel(n_jobs=self.n_jobs)(","269","            delayed(_partial_fit_binary)(self.estimators_[i], X,","270","                                         next(columns))"],"delete":["246","            if (not hasattr(self.estimator, \"partial_fit\")):","247","                raise ValueError(\"Base estimator {0}, doesn't have partial_fit\"","248","                                 \"method\".format(self.estimator))","252","        # A sparse LabelBinarizer, with sparse_output=True, has been shown to","253","        # outperform or match a dense label binarizer in all cases and has also","254","        # resulted in less or equal memory consumption in the fit_ovr function","255","        # overall.","256","        self.label_binarizer_ = LabelBinarizer(sparse_output=True)","257","        Y = self.label_binarizer_.fit_transform(y)","261","        self.estimators_ = Parallel(n_jobs=self.n_jobs)(delayed(","262","            _partial_fit_binary)(self.estimators_[i],","263","                                 X, next(columns) if self.classes_[i] in","264","                                 self.label_binarizer_.classes_ else","265","                                 np.zeros((1, len(y))))"]}]}},"4907029b1ddff16b111c501ad010d5207e0bd177":{"changes":{"sklearn\/tree\/_splitter.pxd":"MODIFY","sklearn\/tree\/_utils.pyx":"MODIFY","sklearn\/tree\/_tree.pxd":"MODIFY","sklearn\/tree\/_criterion.pyx":"MODIFY","sklearn\/tree\/_criterion.pxd":"MODIFY","sklearn\/tree\/_tree.pyx":"MODIFY","sklearn\/tree\/_utils.pxd":"MODIFY","sklearn\/tree\/_splitter.pyx":"MODIFY"},"diff":{"sklearn\/tree\/_splitter.pxd":[{"add":["83","    cdef int init(self, object X, np.ndarray y,","84","                  DOUBLE_t* sample_weight,","85","                  np.ndarray X_idx_sorted=*) except -1","87","    cdef int node_reset(self, SIZE_t start, SIZE_t end,","88","                        double* weighted_n_node_samples) nogil except -1","90","    cdef int node_split(self,","91","                        double impurity,   # Impurity of the node","92","                        SplitRecord* split,","93","                        SIZE_t* n_constant_features) nogil except -1","97","    cdef double node_impurity(self) nogil"],"delete":["83","    cdef void init(self, object X, np.ndarray y,","84","                   DOUBLE_t* sample_weight,","85","                   np.ndarray X_idx_sorted=*) except *","87","    cdef void node_reset(self, SIZE_t start, SIZE_t end,","88","                         double* weighted_n_node_samples) nogil","90","    cdef void node_split(self,","91","                         double impurity,   # Impurity of the node","92","                         SplitRecord* split,","93","                         SIZE_t* n_constant_features) nogil","97","    cdef double node_impurity(self) nogil"]}],"sklearn\/tree\/_utils.pyx":[{"add":["26","cdef realloc_ptr safe_realloc(realloc_ptr* p, size_t nelems) nogil except *:","32","        with gil:","33","            raise MemoryError(\"could not allocate (%d * %d) bytes\"","34","                              % (nelems, sizeof(p[0][0])))","37","        with gil:","38","            raise MemoryError(\"could not allocate %d bytes\" % nbytes)","121","                  SIZE_t n_constant_features) nogil except -1:","124","        Return -1 in case of failure to allocate memory (and raise MemoryError)","125","        or 0 otherwise.","133","            # Since safe_realloc can raise MemoryError, use `except -1`","134","            safe_realloc(&self.stack_, self.capacity)","194","        safe_realloc(&self.heap_, capacity)","237","                  double impurity_right) nogil except -1:","240","        Return -1 in case of failure to allocate memory (and raise MemoryError)","241","        or 0 otherwise.","249","            # Since safe_realloc can raise MemoryError, use `except -1`","250","            safe_realloc(&self.heap_, self.capacity)","325","    cdef int reset(self) nogil except -1:","326","        \"\"\"Reset the WeightedPQueue to its state at construction","327","","328","        Return -1 in case of failure to allocate memory (and raise MemoryError)","329","        or 0 otherwise.","330","        \"\"\"","332","        # Since safe_realloc can raise MemoryError, use `except *`","333","        safe_realloc(&self.array_, self.capacity)","334","        return 0","342","    cdef int push(self, DOUBLE_t data, DOUBLE_t weight) nogil except -1:","344","","345","        Return -1 in case of failure to allocate memory (and raise MemoryError)","346","        or 0 otherwise.","355","            # Since safe_realloc can raise MemoryError, use `except -1`","356","            safe_realloc(&self.array_, self.capacity)","500","    cdef int reset(self) nogil except -1:","501","        \"\"\"Reset the WeightedMedianCalculator to its state at construction","502","","503","        Return -1 in case of failure to allocate memory (and raise MemoryError)","504","        or 0 otherwise.","505","        \"\"\"","506","        # samples.reset (WeightedPQueue.reset) uses safe_realloc, hence","507","        # except -1","512","        return 0","514","    cdef int push(self, DOUBLE_t data, DOUBLE_t weight) nogil except -1:","515","        \"\"\"Push a value and its associated weight to the WeightedMedianCalculator","516","","517","        Return -1 in case of failure to allocate memory (and raise MemoryError)","518","        or 0 otherwise.","525","        # samples.push (WeightedPQueue.push) uses safe_realloc, hence except -1","531","    cdef int update_median_parameters_post_push(","532","            self, DOUBLE_t data, DOUBLE_t weight,","533","            DOUBLE_t original_median) nogil:","608","    cdef int update_median_parameters_post_remove(","609","            self, DOUBLE_t data, DOUBLE_t weight,","610","            double original_median) nogil:"],"delete":["15","from libc.stdlib cimport calloc","27","cdef realloc_ptr safe_realloc(realloc_ptr* p, size_t nelems) except *:","33","        raise MemoryError(\"could not allocate (%d * %d) bytes\"","34","                          % (nelems, sizeof(p[0][0])))","37","        raise MemoryError(\"could not allocate %d bytes\" % nbytes)","111","        if self.stack_ == NULL:","112","            raise MemoryError()","122","                  SIZE_t n_constant_features) nogil:","125","        Returns 0 if successful; -1 on out of memory error.","133","            stack = <StackRecord*> realloc(self.stack_,","134","                                           self.capacity * sizeof(StackRecord))","135","            if stack == NULL:","136","                # no free; __dealloc__ handles that","137","                return -1","138","            self.stack_ = stack","198","        self.heap_ = <PriorityHeapRecord*> malloc(capacity * sizeof(PriorityHeapRecord))","199","        if self.heap_ == NULL:","200","            raise MemoryError()","243","                  double impurity_right) nogil:","246","        Returns 0 if successful; -1 on out of memory error.","254","            heap = <PriorityHeapRecord*> realloc(self.heap_,","255","                                                 self.capacity *","256","                                                 sizeof(PriorityHeapRecord))","257","            if heap == NULL:","258","                # no free; __dealloc__ handles that","259","                return -1","260","            self.heap_ = heap","332","        if self.array_ == NULL:","333","            raise MemoryError()","334","","338","    cdef void reset(self) nogil:","339","        \"\"\"Reset the WeightedPQueue to its state at construction\"\"\"","341","        self.array_ = <WeightedPQueueRecord*> calloc(self.capacity,","342","                                                     sizeof(WeightedPQueueRecord))","350","    cdef int push(self, DOUBLE_t data, DOUBLE_t weight) nogil:","352","        Returns 0 if successful; -1 on out of memory error.","361","            array = <WeightedPQueueRecord*> realloc(self.array_,","362","                                                    self.capacity *","363","                                                    sizeof(WeightedPQueueRecord))","364","","365","            if array == NULL:","366","                # no free; __dealloc__ handles that","367","                return -1","368","            self.array_ = array","512","    cdef void reset(self) nogil:","513","        \"\"\"Reset the WeightedMedianCalculator to its state at construction\"\"\"","519","    cdef int push(self, DOUBLE_t data, DOUBLE_t weight) nogil:","520","        \"\"\"Push a value and its associated weight","521","        to the WeightedMedianCalculator to be considered","522","        in the median calculation.","534","    cdef int update_median_parameters_post_push(self, DOUBLE_t data,","535","                                                DOUBLE_t weight,","536","                                                DOUBLE_t original_median) nogil:","611","    cdef int update_median_parameters_post_remove(self, DOUBLE_t data,","612","                                                  DOUBLE_t weight,","613","                                                  double original_median) nogil:"]}],"sklearn\/tree\/_tree.pxd":[{"add":["60","                          double weighted_n_samples) nogil except -1","61","    cdef int _resize(self, SIZE_t capacity) nogil except -1","62","    cdef int _resize_c(self, SIZE_t capacity=*) nogil except -1"],"delete":["60","                          double weighted_n_samples) nogil","61","    cdef void _resize(self, SIZE_t capacity) except *","62","    cdef int _resize_c(self, SIZE_t capacity=*) nogil"]}],"sklearn\/tree\/_criterion.pyx":[{"add":["53","    cdef int init(self, DOUBLE_t* y, SIZE_t y_stride, DOUBLE_t* sample_weight,","54","                  double weighted_n_samples, SIZE_t* samples, SIZE_t start,","55","                  SIZE_t end) nogil except -1:","58","        Returns -1 in case of failure to allocate memory (and raise MemoryError)","59","        or 0 otherwise.","60","","84","    cdef int reset(self) nogil except -1:","92","    cdef int reverse_reset(self) nogil except -1:","99","    cdef int update(self, SIZE_t new_pos) nogil except -1:","286","    cdef int init(self, DOUBLE_t* y, SIZE_t y_stride,","287","                  DOUBLE_t* sample_weight, double weighted_n_samples,","288","                  SIZE_t* samples, SIZE_t start, SIZE_t end) nogil except -1:","292","        Returns -1 in case of failure to allocate memory (and raise MemoryError)","293","        or 0 otherwise.","294","","355","        return 0","357","    cdef int reset(self) nogil except -1:","358","        \"\"\"Reset the criterion at pos=start","360","        Returns -1 in case of failure to allocate memory (and raise MemoryError)","361","        or 0 otherwise.","362","        \"\"\"","382","        return 0","384","    cdef int reverse_reset(self) nogil except -1:","385","        \"\"\"Reset the criterion at pos=end","386","","387","        Returns -1 in case of failure to allocate memory (and raise MemoryError)","388","        or 0 otherwise.","389","        \"\"\"","409","        return 0","411","    cdef int update(self, SIZE_t new_pos) nogil except -1:","414","        Returns -1 in case of failure to allocate memory (and raise MemoryError)","415","        or 0 otherwise.","416","","491","        return 0","758","    cdef int init(self, DOUBLE_t* y, SIZE_t y_stride, DOUBLE_t* sample_weight,","759","                  double weighted_n_samples, SIZE_t* samples, SIZE_t start,","760","                  SIZE_t end) nogil except -1:","800","        return 0","802","    cdef int reset(self) nogil except -1:","811","        return 0","813","    cdef int reverse_reset(self) nogil except -1:","822","        return 0","824","    cdef int update(self, SIZE_t new_pos) nogil except -1:","884","        return 0","1051","    cdef int init(self, DOUBLE_t* y, SIZE_t y_stride, DOUBLE_t* sample_weight,","1052","                  double weighted_n_samples, SIZE_t* samples, SIZE_t start,","1053","                  SIZE_t end) nogil except -1:","1091","                # push method ends up calling safe_realloc, hence `except -1`","1103","        return 0","1105","    cdef int reset(self) nogil except -1:","1106","        \"\"\"Reset the criterion at pos=start","1107","","1108","        Returns -1 in case of failure to allocate memory (and raise MemoryError)","1109","        or 0 otherwise.","1110","        \"\"\"","1132","                # push method ends up calling safe_realloc, hence `except -1`","1135","        return 0","1137","    cdef int reverse_reset(self) nogil except -1:","1138","        \"\"\"Reset the criterion at pos=end","1139","","1140","        Returns -1 in case of failure to allocate memory (and raise MemoryError)","1141","        or 0 otherwise.","1142","        \"\"\"","1161","                # push method ends up calling safe_realloc, hence `except -1`","1164","        return 0","1166","    cdef int update(self, SIZE_t new_pos) nogil except -1:","1167","        \"\"\"Updated statistics by moving samples[pos:new_pos] to the left","1168","","1169","        Returns -1 in case of failure to allocate memory (and raise MemoryError)","1170","        or 0 otherwise.","1171","        \"\"\"","1203","                    # push method ends up calling safe_realloc, hence except -1","1227","        return 0"],"delete":["53","    cdef void init(self, DOUBLE_t* y, SIZE_t y_stride, DOUBLE_t* sample_weight,","54","                   double weighted_n_samples, SIZE_t* samples, SIZE_t start,","55","                   SIZE_t end) nogil:","81","    cdef void reset(self) nogil:","89","    cdef void reverse_reset(self) nogil:","96","    cdef void update(self, SIZE_t new_pos) nogil:","283","    cdef void init(self, DOUBLE_t* y, SIZE_t y_stride,","284","                   DOUBLE_t* sample_weight, double weighted_n_samples,","285","                   SIZE_t* samples, SIZE_t start, SIZE_t end) nogil:","350","    cdef void reset(self) nogil:","351","        \"\"\"Reset the criterion at pos=start.\"\"\"","373","    cdef void reverse_reset(self) nogil:","374","        \"\"\"Reset the criterion at pos=end.\"\"\"","395","    cdef void update(self, SIZE_t new_pos) nogil:","738","    cdef void init(self, DOUBLE_t* y, SIZE_t y_stride, DOUBLE_t* sample_weight,","739","                   double weighted_n_samples, SIZE_t* samples, SIZE_t start,","740","                   SIZE_t end) nogil:","781","    cdef void reset(self) nogil:","791","    cdef void reverse_reset(self) nogil:","801","    cdef void update(self, SIZE_t new_pos) nogil:","1020","        if (self.node_medians == NULL):","1021","            raise MemoryError()","1022","","1030","    cdef void init(self, DOUBLE_t* y, SIZE_t y_stride, DOUBLE_t* sample_weight,","1031","                   double weighted_n_samples, SIZE_t* samples, SIZE_t start,","1032","                   SIZE_t end) nogil:","1082","    cdef void reset(self) nogil:","1083","        \"\"\"Reset the criterion at pos=start.\"\"\"","1108","    cdef void reverse_reset(self) nogil:","1109","        \"\"\"Reset the criterion at pos=end.\"\"\"","1131","    cdef void update(self, SIZE_t new_pos) nogil:","1132","        \"\"\"Updated statistics by moving samples[pos:new_pos] to the left.\"\"\""]}],"sklearn\/tree\/_criterion.pxd":[{"add":["55","    cdef int init(self, DOUBLE_t* y, SIZE_t y_stride, DOUBLE_t* sample_weight,","56","                  double weighted_n_samples, SIZE_t* samples, SIZE_t start,","57","                  SIZE_t end) nogil except -1","58","    cdef int reset(self) nogil except -1","59","    cdef int reverse_reset(self) nogil except -1","60","    cdef int update(self, SIZE_t new_pos) nogil except -1"],"delete":["55","    cdef void init(self, DOUBLE_t* y, SIZE_t y_stride, DOUBLE_t* sample_weight,","56","                   double weighted_n_samples, SIZE_t* samples, SIZE_t start,","57","                   SIZE_t end) nogil","58","    cdef void reset(self) nogil","59","    cdef void reverse_reset(self) nogil","60","    cdef void update(self, SIZE_t new_pos) nogil"]}],"sklearn\/tree\/_tree.pyx":[{"add":["273","                                 PriorityHeap frontier) nogil except -1:","274","    \"\"\"Adds record ``rec`` to the priority queue ``frontier``","275","","276","    Returns -1 in case of failure to allocate memory (and raise MemoryError)","277","    or 0 otherwise.","278","    \"\"\"","421","                                    PriorityHeapRecord* res) nogil except -1:","661","    cdef int _resize(self, SIZE_t capacity) nogil except -1:","663","           double the size of the inner arrays.","664","","665","        Returns -1 in case of failure to allocate memory (and raise MemoryError)","666","        or 0 otherwise.","667","        \"\"\"","669","            # Acquire gil only if we need to raise","670","            with gil:","671","                raise MemoryError()","675","    cdef int _resize_c(self, SIZE_t capacity=<SIZE_t>(-1)) nogil except -1:","676","        \"\"\"Guts of _resize","677","","678","        Returns -1 in case of failure to allocate memory (and raise MemoryError)","679","        or 0 otherwise.","680","        \"\"\"","690","        safe_realloc(&self.nodes, capacity)","691","        safe_realloc(&self.value, capacity * self.value_stride)","708","                          SIZE_t n_node_samples,","709","                          double weighted_n_node_samples) nogil except -1:"],"delete":["21","from libc.stdlib cimport realloc","274","                                 PriorityHeap frontier) nogil:","275","    \"\"\"Adds record ``rec`` to the priority queue ``frontier``; returns -1","276","    on memory-error. \"\"\"","419","                                    PriorityHeapRecord* res) nogil:","659","    cdef void _resize(self, SIZE_t capacity) except *:","661","           double the size of the inner arrays.\"\"\"","663","            raise MemoryError()","667","    cdef int _resize_c(self, SIZE_t capacity=<SIZE_t>(-1)) nogil:","668","        \"\"\"Guts of _resize. Returns 0 for success, -1 for error.\"\"\"","678","        # XXX no safe_realloc here because we need to grab the GIL","679","        cdef void* ptr = realloc(self.nodes, capacity * sizeof(Node))","680","        if ptr == NULL:","681","            return -1","682","        self.nodes = <Node*> ptr","683","        ptr = realloc(self.value,","684","                      capacity * self.value_stride * sizeof(double))","685","        if ptr == NULL:","686","            return -1","687","        self.value = <double*> ptr","704","                          SIZE_t n_node_samples, double weighted_n_node_samples) nogil:"]}],"sklearn\/tree\/_utils.pxd":[{"add":["42","    (StackRecord*)","43","    (PriorityHeapRecord*)","45","cdef realloc_ptr safe_realloc(realloc_ptr* p, size_t nelems) nogil except *","52","                     UINT32_t* random_state) nogil","56","                         UINT32_t* random_state) nogil","83","                  SIZE_t n_constant_features) nogil except -1","115","                  double impurity_right) nogil except -1","133","    cdef int reset(self) nogil except -1","135","    cdef int push(self, DOUBLE_t data, DOUBLE_t weight) nogil except -1","156","    cdef int push(self, DOUBLE_t data, DOUBLE_t weight) nogil except -1","157","    cdef int reset(self) nogil except -1","158","    cdef int update_median_parameters_post_push(","159","        self, DOUBLE_t data, DOUBLE_t weight,","160","        DOUBLE_t original_median) nogil","163","    cdef int update_median_parameters_post_remove(","164","        self, DOUBLE_t data, DOUBLE_t weight,","165","        DOUBLE_t original_median) nogil"],"delete":["43","cdef realloc_ptr safe_realloc(realloc_ptr* p, size_t nelems) except *","50","                            UINT32_t* random_state) nogil","54","                                UINT32_t* random_state) nogil","81","                  SIZE_t n_constant_features) nogil","113","                  double impurity_right) nogil","131","    cdef void reset(self) nogil","133","    cdef int push(self, DOUBLE_t data, DOUBLE_t weight) nogil","154","    cdef int push(self, DOUBLE_t data, DOUBLE_t weight) nogil","155","    cdef void reset(self) nogil","156","    cdef int update_median_parameters_post_push(self, DOUBLE_t data,","157","                                                DOUBLE_t weight,","158","                                                DOUBLE_t original_median) nogil","161","    cdef int update_median_parameters_post_remove(self, DOUBLE_t data,","162","                                                  DOUBLE_t weight,","163","                                                  DOUBLE_t original_median) nogil"]}],"sklearn\/tree\/_splitter.pyx":[{"add":["118","    cdef int init(self,","122","                   np.ndarray X_idx_sorted=None) except -1:","127","        Returns -1 in case of failure to allocate memory (and raise MemoryError)","128","        or 0 otherwise.","129","","185","        return 0","187","    cdef int node_reset(self, SIZE_t start, SIZE_t end,","188","                        double* weighted_n_node_samples) nogil except -1:","191","        Returns -1 in case of failure to allocate memory (and raise MemoryError)","192","        or 0 otherwise.","193","","216","        return 0","218","    cdef int node_split(self, double impurity, SplitRecord* split,","219","                        SIZE_t* n_constant_features) nogil except -1:","224","","225","        It should return -1 upon errors.","269","    cdef int init(self,","270","                  object X,","271","                  np.ndarray[DOUBLE_t, ndim=2, mode=\"c\"] y,","272","                  DOUBLE_t* sample_weight,","273","                  np.ndarray X_idx_sorted=None) except -1:","274","        \"\"\"Initialize the splitter","275","","276","        Returns -1 in case of failure to allocate memory (and raise MemoryError)","277","        or 0 otherwise.","278","        \"\"\"","300","        return 0","301","","313","    cdef int node_split(self, double impurity, SplitRecord* split,","314","                        SIZE_t* n_constant_features) nogil except -1:","315","        \"\"\"Find the best split on node samples[start:end]","316","","317","        Returns -1 in case of failure to allocate memory (and raise MemoryError)","318","        or 0 otherwise.","319","        \"\"\"","531","        return 0","541","cdef inline void swap(DTYPE_t* Xf, SIZE_t* samples,","542","        SIZE_t i, SIZE_t j) nogil:","570","cdef void introsort(DTYPE_t* Xf, SIZE_t *samples,","571","                    SIZE_t n, int maxd) nogil:","656","    cdef int node_split(self, double impurity, SplitRecord* split,","657","                        SIZE_t* n_constant_features) nogil except -1:","658","        \"\"\"Find the best random split on node samples[start:end]","659","","660","        Returns -1 in case of failure to allocate memory (and raise MemoryError)","661","        or 0 otherwise.","662","        \"\"\"","862","        return 0","895","    cdef int init(self,","896","                  object X,","897","                  np.ndarray[DOUBLE_t, ndim=2, mode=\"c\"] y,","898","                  DOUBLE_t* sample_weight,","899","                  np.ndarray X_idx_sorted=None) except -1:","900","        \"\"\"Initialize the splitter","902","        Returns -1 in case of failure to allocate memory (and raise MemoryError)","903","        or 0 otherwise.","904","        \"\"\"","936","        return 0","1182","                             SIZE_t pos_1, SIZE_t pos_2) nogil:","1200","    cdef int node_split(self, double impurity, SplitRecord* split,","1201","                        SIZE_t* n_constant_features) nogil except -1:","1202","        \"\"\"Find the best split on node samples[start:end], using sparse features","1203","","1204","        Returns -1 in case of failure to allocate memory (and raise MemoryError)","1205","        or 0 otherwise.","1416","        return 0","1430","    cdef int node_split(self, double impurity, SplitRecord* split,","1431","                        SIZE_t* n_constant_features) nogil except -1:","1432","        \"\"\"Find a random split on node samples[start:end], using sparse features","1433","","1434","        Returns -1 in case of failure to allocate memory (and raise MemoryError)","1435","        or 0 otherwise.","1647","        return 0"],"delete":["118","    cdef void init(self,","122","                   np.ndarray X_idx_sorted=None) except *:","183","    cdef void node_reset(self, SIZE_t start, SIZE_t end,","184","                         double* weighted_n_node_samples) nogil:","210","    cdef void node_split(self, double impurity, SplitRecord* split,","211","                         SIZE_t* n_constant_features) nogil:","259","    cdef void init(self,","260","                   object X,","261","                   np.ndarray[DOUBLE_t, ndim=2, mode=\"c\"] y,","262","                   DOUBLE_t* sample_weight,","263","                   np.ndarray X_idx_sorted=None) except *:","264","        \"\"\"Initialize the splitter.\"\"\"","297","    cdef void node_split(self, double impurity, SplitRecord* split,","298","                         SIZE_t* n_constant_features) nogil:","299","        \"\"\"Find the best split on node samples[start:end].\"\"\"","520","cdef inline void swap(DTYPE_t* Xf, SIZE_t* samples, SIZE_t i, SIZE_t j) nogil:","548","cdef void introsort(DTYPE_t* Xf, SIZE_t *samples, SIZE_t n, int maxd) nogil:","633","    cdef void node_split(self, double impurity, SplitRecord* split,","634","                         SIZE_t* n_constant_features) nogil:","635","        \"\"\"Find the best random split on node samples[start:end].\"\"\"","867","    cdef void init(self,","868","                   object X,","869","                   np.ndarray[DOUBLE_t, ndim=2, mode=\"c\"] y,","870","                   DOUBLE_t* sample_weight,","871","                   np.ndarray X_idx_sorted=None) except *:","872","        \"\"\"Initialize the splitter.\"\"\"","1150","                             SIZE_t pos_1, SIZE_t pos_2) nogil  :","1168","    cdef void node_split(self, double impurity, SplitRecord* split,","1169","                         SIZE_t* n_constant_features) nogil:","1170","        \"\"\"Find the best split on node samples[start:end], using sparse","1171","           features.","1395","    cdef void node_split(self, double impurity, SplitRecord* split,","1396","                         SIZE_t* n_constant_features) nogil:","1397","        \"\"\"Find a random split on node samples[start:end], using sparse","1398","           features."]}]}},"3619bd386b66b5131c5b78e21e6775982d2b984d":{"changes":{"sklearn\/model_selection\/tests\/test_search.py":"MODIFY","sklearn\/model_selection\/_search.py":"MODIFY"},"diff":{"sklearn\/model_selection\/tests\/test_search.py":[{"add":["1142","","1143","","1144","def test_search_train_scores_set_to_false():","1145","    X = np.arange(6).reshape(6, -1)","1146","    y = [0, 0, 0, 1, 1, 1]","1147","    clf = LinearSVC(random_state=0)","1148","","1149","    gs = GridSearchCV(clf, param_grid={'C': [0.1, 0.2]},","1150","                      return_train_score=False)","1151","    gs.fit(X, y)"],"delete":[]}],"sklearn\/model_selection\/_search.py":[{"add":["605","        if self.return_train_score:","606","            _store('train_score', train_scores, splits=True)"],"delete":["605","        _store('train_score', train_scores, splits=True)"]}]}},"5230382ae65ce4c5c5499b652d07077242e47c1b":{"changes":{"sklearn\/linear_model\/tests\/test_least_angle.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/linear_model\/least_angle.py":"MODIFY"},"diff":{"sklearn\/linear_model\/tests\/test_least_angle.py":[{"add":["543","","544","","545","def test_lasso_lars_vs_R_implementation():","546","    # Test that sklearn LassoLars implementation agrees with the LassoLars","547","    # implementation available in R (lars library) under the following","548","    # scenarios:","549","    # 1) fit_intercept=False and normalize=False","550","    # 2) fit_intercept=True and normalize=True","551","","552","    # Let's generate the data used in the bug report 7778","553","    y = np.array([-6.45006793, -3.51251449, -8.52445396, 6.12277822,","554","                  -19.42109366])","555","    x = np.array([[0.47299829, 0, 0, 0, 0],","556","                  [0.08239882, 0.85784863, 0, 0, 0],","557","                  [0.30114139, -0.07501577, 0.80895216, 0, 0],","558","                  [-0.01460346, -0.1015233, 0.0407278, 0.80338378, 0],","559","                  [-0.69363927, 0.06754067, 0.18064514, -0.0803561,","560","                   0.40427291]])","561","","562","    X = x.T","563","","564","    ###########################################################################","565","    # Scenario 1: Let's compare R vs sklearn when fit_intercept=False and","566","    # normalize=False","567","    ###########################################################################","568","    #","569","    # The R result was obtained using the following code:","570","    #","571","    # library(lars)","572","    # model_lasso_lars = lars(X, t(y), type=\"lasso\", intercept=FALSE,","573","    #                         trace=TRUE, normalize=FALSE)","574","    # r = t(model_lasso_lars$beta)","575","    #","576","","577","    r = np.array([[0, 0, 0, 0, 0, -79.810362809499026, -83.528788732782829,","578","                   -83.777653739190711, -83.784156932888934,","579","                   -84.033390591756657],","580","                  [0, 0, 0, 0, -0.476624256777266, 0, 0, 0, 0,","581","                   0.025219751009936],","582","                  [0, -3.577397088285891, -4.702795355871871,","583","                   -7.016748621359461, -7.614898471899412, -0.336938391359179,","584","                   0, 0, 0.001213370600853,  0.048162321585148],","585","                  [0, 0, 0, 2.231558436628169, 2.723267514525966,","586","                   2.811549786389614, 2.813766976061531, 2.817462468949557,","587","                   2.817368178703816, 2.816221090636795],","588","                  [0, 0, -1.218422599914637, -3.457726183014808,","589","                   -4.021304522060710, -45.827461592423745,","590","                   -47.776608869312305,","591","                   -47.911561610746404, -47.914845922736234,","592","                   -48.039562334265717]])","593","","594","    model_lasso_lars = linear_model.LassoLars(alpha=0, fit_intercept=False,","595","                                              normalize=False)","596","    model_lasso_lars.fit(X, y)","597","    skl_betas = model_lasso_lars.coef_path_","598","","599","    assert_array_almost_equal(r, skl_betas, decimal=12)","600","    ###########################################################################","601","","602","    ###########################################################################","603","    # Scenario 2: Let's compare R vs sklearn when fit_intercept=True and","604","    # normalize=True","605","    #","606","    # Note: When normalize is equal to True, R returns the coefficients in","607","    # their original units, that is, they are rescaled back, whereas sklearn","608","    # does not do that, therefore, we need to do this step before comparing","609","    # their results.","610","    ###########################################################################","611","    #","612","    # The R result was obtained using the following code:","613","    #","614","    # library(lars)","615","    # model_lasso_lars2 = lars(X, t(y), type=\"lasso\", intercept=TRUE,","616","    #                           trace=TRUE, normalize=TRUE)","617","    # r2 = t(model_lasso_lars2$beta)","618","","619","    r2 = np.array([[0, 0, 0, 0, 0],","620","                   [0, 0, 0, 8.371887668009453, 19.463768371044026],","621","                   [0, 0, 0, 0, 9.901611055290553],","622","                   [0, 7.495923132833733, 9.245133544334507,","623","                    17.389369207545062, 26.971656815643499],","624","                   [0, 0, -1.569380717440311, -5.924804108067312,","625","                    -7.996385265061972]])","626","","627","    model_lasso_lars2 = linear_model.LassoLars(alpha=0, fit_intercept=True,","628","                                               normalize=True)","629","    model_lasso_lars2.fit(X, y)","630","    skl_betas2 = model_lasso_lars2.coef_path_","631","","632","    # Let's rescale back the coefficients returned by sklearn before comparing","633","    # against the R result (read the note above)","634","    temp = X - np.mean(X, axis=0)","635","    normx = np.sqrt(np.sum(temp ** 2, axis=0))","636","    skl_betas2 \/= normx[:, np.newaxis]","637","","638","    assert_array_almost_equal(r2, skl_betas2, decimal=12)","639","    ###########################################################################"],"delete":[]}],"doc\/whats_new.rst":[{"add":["102","   - Fixed a bug where :class:`sklearn.linear_model.LassoLars` does not give","103","     the same result as the LassoLars implementation available","104","     in R (lars library). :issue:`7849` by `Jair Montoya Martinez`_"],"delete":[]}],"sklearn\/linear_model\/least_angle.py":[{"add":["406","                coefs[-add_features:] = 0","408","                alphas[-add_features:] = 0"],"delete":[]}]}},"ee88cf44ea533803849379f48bd44fcdefc14a93":{"changes":{"sklearn\/__init__.py":"MODIFY","doc\/modules\/computational_performance.rst":"MODIFY","sklearn\/tests\/test_config.py":"ADD","doc\/modules\/classes.rst":"MODIFY","sklearn\/utils\/validation.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/utils\/tests\/test_validation.py":"MODIFY"},"diff":{"sklearn\/__init__.py":[{"add":["17","import os","18","from contextlib import contextmanager as _contextmanager","19","","20","_ASSUME_FINITE = bool(os.environ.get('SKLEARN_ASSUME_FINITE', False))","21","","22","","23","def get_config():","24","    \"\"\"Retrieve current values for configuration set by :func:`set_config`","25","","26","    Returns","27","    -------","28","    config : dict","29","        Keys are parameter names that can be passed to :func:`set_config`.","30","    \"\"\"","31","    return {'assume_finite': _ASSUME_FINITE}","32","","33","","34","def set_config(assume_finite=None):","35","    \"\"\"Set global scikit-learn configuration","36","","37","    Parameters","38","    ----------","39","    assume_finite : bool, optional","40","        If True, validation for finiteness will be skipped,","41","        saving time, but leading to potential crashes. If","42","        False, validation for finiteness will be performed,","43","        avoiding error.","44","    \"\"\"","45","    global _ASSUME_FINITE","46","    if assume_finite is not None:","47","        _ASSUME_FINITE = assume_finite","48","","49","","50","@_contextmanager","51","def config_context(**new_config):","52","    \"\"\"Context manager for global scikit-learn configuration","53","","54","    Parameters","55","    ----------","56","    assume_finite : bool, optional","57","        If True, validation for finiteness will be skipped,","58","        saving time, but leading to potential crashes. If","59","        False, validation for finiteness will be performed,","60","        avoiding error.","61","","62","    Notes","63","    -----","64","    All settings, not just those presently modified, will be returned to","65","    their previous values when the context manager is exited. This is not","66","    thread-safe.","67","","68","    Examples","69","    --------","70","    >>> import sklearn","71","    >>> from sklearn.utils.validation import assert_all_finite","72","    >>> with sklearn.config_context(assume_finite=True):","73","    ...     assert_all_finite([float('nan')])","74","    >>> with sklearn.config_context(assume_finite=True):","75","    ...     with sklearn.config_context(assume_finite=False):","76","    ...         assert_all_finite([float('nan')])","77","    ... # doctest: +ELLIPSIS","78","    Traceback (most recent call last):","79","    ...","80","    ValueError: Input contains NaN, ...","81","    \"\"\"","82","    old_config = get_config().copy()","83","    set_config(**new_config)","84","","85","    try:","86","        yield","87","    finally:","88","        set_config(**old_config)"],"delete":[]}],"doc\/modules\/computational_performance.rst":[{"add":["70",".. topic:: Configuring Scikit-learn for reduced validation overhead","71","","72","    Scikit-learn does some validation on data that increases the overhead per","73","    call to ``predict`` and similar functions. In particular, checking that","74","    features are finite (not NaN or infinite) involves a full pass over the","75","    data. If you ensure that your data is acceptable, you may suppress","76","    checking for finiteness by setting the environment variable","77","    ``SKLEARN_ASSUME_FINITE`` to a non-empty string before importing","78","    scikit-learn, or configure it in Python with :func:`sklearn.set_config`.","79","    For more control than these global settings, a :func:`config_context`","80","    allows you to set this configuration within a specified context::","81","","82","      >>> import sklearn","83","      >>> with sklearn.config_context(assume_finite=True):","84","      ...    pass  # do learning\/prediction here with reduced validation","85","","86","    Note that this will affect all uses of","87","    :func:`sklearn.utils.assert_all_finite` within the context.","88",""],"delete":[]}],"sklearn\/tests\/test_config.py":[{"add":[],"delete":[]}],"doc\/modules\/classes.rst":[{"add":["42","   config_context","43","   set_config","44","   get_config"],"delete":[]}],"sklearn\/utils\/validation.py":[{"add":["18","from .. import get_config as _get_config","33","    if _get_config()['assume_finite']:","34","        return"],"delete":[]}],"doc\/whats_new.rst":[{"add":["33","   - Validation that input data contains no NaN or inf can now be suppressed","34","     using :func:`config_context`, at your own risk. This will save on runtime,","35","     and may be particularly useful for prediction time. :issue:`7548` by","36","     `Joel Nothman`_.","37",""],"delete":[]}],"sklearn\/utils\/tests\/test_validation.py":[{"add":["32","    assert_all_finite,","34","import sklearn","529","","530","","531","def test_suppress_validation():","532","    X = np.array([0, np.inf])","533","    assert_raises(ValueError, assert_all_finite, X)","534","    sklearn.set_config(assume_finite=True)","535","    assert_all_finite(X)","536","    sklearn.set_config(assume_finite=False)","537","    assert_raises(ValueError, assert_all_finite, X)"],"delete":["39",""]}]}},"d2f8292e11af5e1e019a982fa871b7b254150f95":{"changes":{"sklearn\/tree\/_criterion.pyx":"MODIFY"},"diff":{"sklearn\/tree\/_criterion.pyx":[{"add":["174","        \"\"\"Compute the improvement in impurity","176","        This method computes the improvement in impurity when a split occurs.","177","        The weighted impurity improvement equation is the following:"],"delete":["174","        \"\"\"Placeholder for improvement in impurity after a split.","176","        Placeholder for a method which computes the improvement","177","        in impurity when a split occurs. The weighted impurity improvement","178","        equation is the following:"]}]}},"c17156106c191c1ca6ea40122539580b1c38e13b":{"changes":{"doc\/whats_new.rst":"MODIFY","sklearn\/preprocessing\/tests\/test_data.py":"MODIFY","sklearn\/preprocessing\/data.py":"MODIFY"},"diff":{"doc\/whats_new.rst":[{"add":["87","   - For sparse matrices, :func:`preprocessing.normalize` with ``return_norm=True``","88","     will now raise a ``NotImplementedError`` with 'l1' or 'l2' norm and with norm 'max'","89","     the norms returned will be the same as for dense matrices (:issue:`7771`).","90","     By `Ang Lu <https:\/\/github.com\/luang008>`_.","91",""],"delete":[]}],"sklearn\/preprocessing\/tests\/test_data.py":[{"add":["1317","    # Test return_norm","1318","    X_dense = np.array([[3.0, 0, 4.0], [1.0, 0.0, 0.0], [2.0, 3.0, 0.0]])","1319","    for norm in ('l1', 'l2', 'max'):","1320","        _, norms = normalize(X_dense, norm=norm, return_norm=True)","1321","        if norm == 'l1':","1322","            assert_array_almost_equal(norms, np.array([7.0, 1.0, 5.0]))","1323","        elif norm == 'l2':","1324","            assert_array_almost_equal(norms, np.array([5.0, 1.0, 3.60555127]))","1325","        else:","1326","            assert_array_almost_equal(norms, np.array([4.0, 1.0, 3.0]))","1327","","1328","    X_sparse = sparse.csr_matrix(X_dense)","1329","    for norm in ('l1', 'l2'):","1330","        assert_raises(NotImplementedError, normalize, X_sparse,","1331","                      norm=norm, return_norm=True)","1332","    _, norms = normalize(X_sparse, norm='max', return_norm=True)","1333","    assert_array_almost_equal(norms, np.array([4.0, 1.0, 3.0]))","1334",""],"delete":[]}],"sklearn\/preprocessing\/data.py":[{"add":["1327","    Returns","1328","    -------","1329","    X : {array-like, sparse matrix}, shape [n_samples, n_features]","1330","        Normalized input X.","1331","","1332","    norms : array, shape [n_samples] if axis=1 else [n_features]","1333","        An array of norms along given axis for X.","1334","        When X is sparse, a NotImplementedError will be raised","1335","        for norm 'l1' or 'l2'.","1336","","1358","        if return_norm and norm in ('l1', 'l2'):","1359","            raise NotImplementedError(\"return_norm=True is not implemented \"","1360","                                      \"for sparse matrices with norm 'l1' \"","1361","                                      \"or norm 'l2'\")","1368","            norms_elementwise = norms.repeat(np.diff(X.indptr))","1369","            mask = norms_elementwise != 0","1370","            X.data[mask] \/= norms_elementwise[mask]"],"delete":["1354","            norms = norms.repeat(np.diff(X.indptr))","1355","            mask = norms != 0","1356","            X.data[mask] \/= norms[mask]"]}]}},"072eefecf2d59a447317dc43a9daae36040580c8":{"changes":{"doc\/themes\/scikit-learn\/layout.html":"MODIFY"},"diff":{"doc\/themes\/scikit-learn\/layout.html":[{"add":["274","      <p class=\"doc-version\">This documentation is for {{project}} <b style=\"font-size: 110%;\">version {{ release|e }}<\/b> &mdash; <a href=\"http:\/\/scikit-learn.org\/stable\/support.html#documentation-resources\">Other versions<\/a><\/p>","276","    <p class=\"citing\">Please <b><a href=\"about.html#citing-scikit-learn\" style=\"font-size: 110%;\">cite us <\/a><\/b>if you use the software.<\/p>"],"delete":["274","      <p class=\"doc-version\">This documentation is for {{project}} <strong>version {{ release|e }}<\/strong> &mdash; <a href=\"http:\/\/scikit-learn.org\/stable\/support.html#documentation-resources\">Other versions<\/a><\/p>","276","    <p class=\"citing\">If you use the software, please consider <a href=\"{{pathto('about')}}#citing-scikit-learn\">citing scikit-learn<\/a>.<\/p>"]}]}},"707b6f9fb72adbba0d643e0a8b057e7833b150a5":{"changes":{"sklearn\/tests\/test_grid_search.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/grid_search.py":"MODIFY"},"diff":{"sklearn\/tests\/test_grid_search.py":[{"add":["44","from sklearn.linear_model import Ridge","788","","789","","790","def test_classes__property():","791","    # Test that classes_ property matches best_esimator_.classes_","792","    X = np.arange(100).reshape(10, 10)","793","    y = np.array([0] * 5 + [1] * 5)","794","    Cs = [.1, 1, 10]","795","","796","    grid_search = GridSearchCV(LinearSVC(random_state=0), {'C': Cs})","797","    grid_search.fit(X, y)","798","    assert_array_equal(grid_search.best_estimator_.classes_,","799","                       grid_search.classes_)","800","","801","    # Test that regressors do not have a classes_ attribute","802","    grid_search = GridSearchCV(Ridge(), {'alpha': [1.0, 2.0]})","803","    grid_search.fit(X, y)","804","    assert_false(hasattr(grid_search, 'classes_'))"],"delete":[]}],"doc\/whats_new.rst":[{"add":["21","   - Added ``classes_`` attribute to :class:`model_selection.GridSearchCV`","22","     that matches the ``classes_`` attribute of ``best_estimator_``. (`#7661","23","     <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/7661>`_) by `Alyssa","24","     Batula`_ and `Dylan Werner-Meier`_.","25",""],"delete":[]}],"sklearn\/grid_search.py":[{"add":["389","    @property","390","    def classes_(self):","391","        return self.best_estimator_.classes_","392","","694","        either binary or multiclass,","906","        either binary or multiclass,"],"delete":["690","        either binary or multiclass, ","902","        either binary or multiclass, "]}]}},"d39c2730223678ef7bee451a617042d9bd87f35d":{"changes":{"sklearn\/preprocessing\/label.py":"MODIFY"},"diff":{"sklearn\/preprocessing\/label.py":[{"add":["287","        y : array of shape [n_samples,] or [n_samples, n_classes]","306","    def fit_transform(self, y):","307","        \"\"\"Fit label binarizer and transform multi-class labels to binary","308","        labels.","310","        The output of transform is sometimes referred to    as","311","        the 1-of-K coding scheme.","315","        y : array or sparse matrix of shape [n_samples,] or \\","316","            [n_samples, n_classes]","317","            Target values. The 2-d matrix should only contain 0 and 1,","318","            represents multilabel classification. Sparse matrix can be","319","            CSR, CSC, COO, DOK, or LIL.","320","","321","        Returns","322","        -------","323","        Y : array or CSR matrix of shape [n_samples, n_classes]","324","            Shape will be [n_samples, 1] for binary problems.","325","        \"\"\"","326","        return self.fit(y).transform(y)","327","","328","    def transform(self, y):","329","        \"\"\"Transform multi-class labels to binary labels","330","","331","        The output of transform is sometimes referred to by some authors as","332","        the 1-of-K coding scheme.","333","","334","        Parameters","335","        ----------","336","        y : array or sparse matrix of shape [n_samples,] or \\","337","            [n_samples, n_classes]","338","            Target values. The 2-d matrix should only contain 0 and 1,","339","            represents multilabel classification. Sparse matrix can be","340","            CSR, CSC, COO, DOK, or LIL."],"delete":["287","        y : numpy array of shape (n_samples,) or (n_samples, n_classes)","306","    def transform(self, y):","307","        \"\"\"Transform multi-class labels to binary labels","309","        The output of transform is sometimes referred to by some authors as the","310","        1-of-K coding scheme.","314","        y : numpy array or sparse matrix of shape (n_samples,) or","315","            (n_samples, n_classes) Target values. The 2-d matrix should only","316","            contain 0 and 1, represents multilabel classification. Sparse","317","            matrix can be CSR, CSC, COO, DOK, or LIL."]}]}},"194c231d49286c72715d073d24d0aec6db4f551a":{"changes":{"sklearn\/utils\/metaestimators.py":"MODIFY","sklearn\/tests\/test_pipeline.py":"MODIFY","sklearn\/ensemble\/voting_classifier.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/pipeline.py":"MODIFY","sklearn\/ensemble\/tests\/test_voting_classifier.py":"MODIFY"},"diff":{"sklearn\/utils\/metaestimators.py":[{"add":["5","from abc import ABCMeta, abstractmethod","9","","11","from ..externals import six","12","from ..base import BaseEstimator","17","class _BaseComposition(six.with_metaclass(ABCMeta, BaseEstimator)):","18","    \"\"\"Handles parameter management for classifiers composed of named estimators.","19","    \"\"\"","20","    @abstractmethod","21","    def __init__(self):","22","        pass","23","","24","    def _get_params(self, attr, deep=True):","25","        out = super(_BaseComposition, self).get_params(deep=False)","26","        if not deep:","27","            return out","28","        estimators = getattr(self, attr)","29","        out.update(estimators)","30","        for name, estimator in estimators:","31","            if estimator is None:","32","                continue","33","            for key, value in six.iteritems(estimator.get_params(deep=True)):","34","                out['%s__%s' % (name, key)] = value","35","        return out","36","","37","    def _set_params(self, attr, **params):","38","        # Ensure strict ordering of parameter setting:","39","        # 1. All steps","40","        if attr in params:","41","            setattr(self, attr, params.pop(attr))","42","        # 2. Step replacement","43","        names, _ = zip(*getattr(self, attr))","44","        for name in list(six.iterkeys(params)):","45","            if '__' not in name and name in names:","46","                self._replace_estimator(attr, name, params.pop(name))","47","        # 3. Step parameters and other initilisation arguments","48","        super(_BaseComposition, self).set_params(**params)","49","        return self","50","","51","    def _replace_estimator(self, attr, name, new_val):","52","        # assumes `name` is a valid estimator name","53","        new_estimators = getattr(self, attr)[:]","54","        for i, (estimator_name, _) in enumerate(new_estimators):","55","            if estimator_name == name:","56","                new_estimators[i] = (name, new_val)","57","                break","58","        setattr(self, attr, new_estimators)","59","","60","    def _validate_names(self, names):","61","        if len(set(names)) != len(names):","62","            raise ValueError('Names provided are not unique: '","63","                             '{0!r}'.format(list(names)))","64","        invalid_names = set(names).intersection(self.get_params(deep=False))","65","        if invalid_names:","66","            raise ValueError('Estimator names conflict with constructor '","67","                             'arguments: {0!r}'.format(sorted(invalid_names)))","68","        invalid_names = [name for name in names if '__' in name]","69","        if invalid_names:","70","            raise ValueError('Estimator names must not contain __: got '","71","                             '{0!r}'.format(invalid_names))","72","","73",""],"delete":[]}],"sklearn\/tests\/test_pipeline.py":[{"add":["813","            (bad_steps1, \"Estimator names must not contain __: got ['a__q']\"),","815","            (bad_steps3, \"Estimator names conflict with constructor \""],"delete":["813","            (bad_steps1, \"Step names must not contain __: got ['a__q']\"),","815","            (bad_steps3, \"Step names conflict with constructor \""]}],"sklearn\/ensemble\/voting_classifier.py":[{"add":["21","from ..utils.metaestimators import _BaseComposition","33","class VotingClassifier(_BaseComposition, ClassifierMixin, TransformerMixin):","45","        ``self.estimators_``. An estimator can be set to `None` using","46","        ``set_params``.","66","        The collection of fitted sub-estimators as defined in ``estimators``","67","        that are not `None`.","109","    @property","110","    def named_estimators(self):","111","        return dict(self.estimators)","112","","156","                    raise ValueError('Underlying estimator \\'%s\\' does not'","157","                                     ' support sample weights.' % name)","158","        names, clfs = zip(*self.estimators)","159","        self._validate_names(names)","161","        n_isnone = np.sum([clf is None for _, clf in self.estimators])","162","        if n_isnone == len(self.estimators):","163","            raise ValueError('All estimators are None. At least one is '","164","                             'required to be a classifier!')","165","        self.le_ = LabelEncoder().fit(y)","173","                                                 sample_weight)","174","                for clf in clfs if clf is not None)","178","    @property","179","    def _weights_not_none(self):","180","        \"\"\"Get the weights of not `None` estimators\"\"\"","181","        if self.weights is None:","182","            return None","183","        return [w for est, w in zip(self.estimators,","184","                                    self.weights) if est[1] is not None]","185","","207","            maj = np.apply_along_axis(","208","                lambda x: np.argmax(","209","                    np.bincount(x, weights=self._weights_not_none)),","210","                axis=1, arr=predictions.astype('int'))","226","        avg = np.average(self._collect_probas(X), axis=0,","227","                         weights=self._weights_not_none)","271","    def set_params(self, **params):","272","        \"\"\" Setting the parameters for the voting classifier","273","","274","        Valid parameter keys can be listed with get_params().","275","","276","        Parameters","277","        ----------","278","        params: keyword arguments","279","            Specific parameters using e.g. set_params(parameter_name=new_value)","280","            In addition, to setting the parameters of the ``VotingClassifier``,","281","            the individual classifiers of the ``VotingClassifier`` can also be","282","            set or replaced by setting them to None.","283","","284","        Examples","285","        --------","286","        # In this example, the RandomForestClassifier is removed","287","        clf1 = LogisticRegression()","288","        clf2 = RandomForestClassifier()","289","        eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2)]","290","        eclf.set_params(rf=None)","291","","292","        \"\"\"","293","        super(VotingClassifier, self)._set_params('estimators', **params)","294","        return self","295","","297","        \"\"\" Get the parameters of the VotingClassifier","298","","299","        Parameters","300","        ----------","301","        deep: bool","302","            Setting it to True gets the various classifiers and the parameters","303","            of the classifiers as well","304","        \"\"\"","305","        return super(VotingClassifier,","306","                     self)._get_params('estimators', deep=deep)"],"delete":["15","from ..base import BaseEstimator","20","from ..externals import six","34","class VotingClassifier(BaseEstimator, ClassifierMixin, TransformerMixin):","46","        `self.estimators_`.","66","        The collection of fitted sub-estimators.","104","        self.named_estimators = dict(estimators)","152","                    raise ValueError('Underlying estimator \\'%s\\' does not support'","153","                                     ' sample weights.' % name)","155","        self.le_ = LabelEncoder()","156","        self.le_.fit(y)","164","                    sample_weight)","165","                    for _, clf in self.estimators)","190","            maj = np.apply_along_axis(lambda x:","191","                                      np.argmax(np.bincount(x,","192","                                                weights=self.weights)),","193","                                      axis=1,","194","                                      arr=predictions.astype('int'))","210","        avg = np.average(self._collect_probas(X), axis=0, weights=self.weights)","255","        \"\"\"Return estimator parameter names for GridSearch support\"\"\"","256","        if not deep:","257","            return super(VotingClassifier, self).get_params(deep=False)","258","        else:","259","            out = super(VotingClassifier, self).get_params(deep=False)","260","            out.update(self.named_estimators.copy())","261","            for name, step in six.iteritems(self.named_estimators):","262","                for key, value in six.iteritems(step.get_params(deep=True)):","263","                    out['%s__%s' % (name, key)] = value","264","            return out"]}],"doc\/whats_new.rst":[{"add":["165","   - Added ability to use sparse matrices in :func:`feature_selection.f_regression`","166","     with ``center=True``. :issue:`8065` by :user:`Daniel LeJeune <acadiansith>`.","167","","168","   - :class:`ensemble.VotingClassifier` now allow changing estimators by using","169","     :meth:`ensemble.VotingClassifier.set_params`. Estimators can also be","170","     removed by setting it to `None`.","171","     :issue:`7674` by:user:`Yichuan Liu <yl565>`."],"delete":[]}],"sklearn\/pipeline.py":[{"add":["12","","25","from .utils.metaestimators import _BaseComposition","26","","30","class Pipeline(_BaseComposition):","578","class FeatureUnion(_BaseComposition, TransformerMixin):"],"delete":["27","class _BasePipeline(six.with_metaclass(ABCMeta, BaseEstimator)):","28","    \"\"\"Handles parameter management for classifiers composed of named steps.","29","    \"\"\"","30","","31","    @abstractmethod","32","    def __init__(self):","33","        pass","34","","35","    def _replace_step(self, steps_attr, name, new_val):","36","        # assumes `name` is a valid step name","37","        new_steps = getattr(self, steps_attr)[:]","38","        for i, (step_name, _) in enumerate(new_steps):","39","            if step_name == name:","40","                new_steps[i] = (name, new_val)","41","                break","42","        setattr(self, steps_attr, new_steps)","43","","44","    def _get_params(self, steps_attr, deep=True):","45","        out = super(_BasePipeline, self).get_params(deep=False)","46","        if not deep:","47","            return out","48","        steps = getattr(self, steps_attr)","49","        out.update(steps)","50","        for name, estimator in steps:","51","            if estimator is None:","52","                continue","53","            for key, value in six.iteritems(estimator.get_params(deep=True)):","54","                out['%s__%s' % (name, key)] = value","55","        return out","56","","57","    def _set_params(self, steps_attr, **params):","58","        # Ensure strict ordering of parameter setting:","59","        # 1. All steps","60","        if steps_attr in params:","61","            setattr(self, steps_attr, params.pop(steps_attr))","62","        # 2. Step replacement","63","        step_names, _ = zip(*getattr(self, steps_attr))","64","        for name in list(six.iterkeys(params)):","65","            if '__' not in name and name in step_names:","66","                self._replace_step(steps_attr, name, params.pop(name))","67","        # 3. Step parameters and other initilisation arguments","68","        super(_BasePipeline, self).set_params(**params)","69","        return self","70","","71","    def _validate_names(self, names):","72","        if len(set(names)) != len(names):","73","            raise ValueError('Names provided are not unique: '","74","                             '{0!r}'.format(list(names)))","75","        invalid_names = set(names).intersection(self.get_params(deep=False))","76","        if invalid_names:","77","            raise ValueError('Step names conflict with constructor arguments: '","78","                             '{0!r}'.format(sorted(invalid_names)))","79","        invalid_names = [name for name in names if '__' in name]","80","        if invalid_names:","81","            raise ValueError('Step names must not contain __: got '","82","                             '{0!r}'.format(invalid_names))","83","","84","","85","class Pipeline(_BasePipeline):","633","class FeatureUnion(_BasePipeline, TransformerMixin):"]}],"sklearn\/ensemble\/tests\/test_voting_classifier.py":[{"add":["4","from sklearn.utils.testing import assert_equal, assert_true, assert_false","42","    eclf = VotingClassifier(estimators=[('lr', clf), ('lr', clf)],","43","                            weights=[1, 2])","44","    msg = \"Names provided are not unique: ['lr', 'lr']\"","45","    assert_raise_message(ValueError, msg, eclf.fit, X, y)","46","","47","    eclf = VotingClassifier(estimators=[('lr__', clf)])","48","    msg = \"Estimator names must not contain __: got ['lr__']\"","49","    assert_raise_message(ValueError, msg, eclf.fit, X, y)","50","","51","    eclf = VotingClassifier(estimators=[('estimators', clf)])","52","    msg = \"Estimator names conflict with constructor arguments: ['estimators']\"","53","    assert_raise_message(ValueError, msg, eclf.fit, X, y)","54","","275","def test_set_params():","276","    \"\"\"set_params should be able to set estimators\"\"\"","277","    clf1 = LogisticRegression(random_state=123, C=1.0)","278","    clf2 = RandomForestClassifier(random_state=123, max_depth=None)","279","    clf3 = GaussianNB()","280","    eclf1 = VotingClassifier([('lr', clf1), ('rf', clf2)], voting='soft',","281","                             weights=[1, 2])","282","    eclf1.fit(X, y)","283","    eclf2 = VotingClassifier([('lr', clf1), ('nb', clf3)], voting='soft',","284","                             weights=[1, 2])","285","    eclf2.set_params(nb=clf2).fit(X, y)","286","    assert_false(hasattr(eclf2, 'nb'))","287","","288","    assert_array_equal(eclf1.predict(X), eclf2.predict(X))","289","    assert_array_equal(eclf1.predict_proba(X), eclf2.predict_proba(X))","290","    assert_equal(eclf2.estimators[0][1].get_params(), clf1.get_params())","291","    assert_equal(eclf2.estimators[1][1].get_params(), clf2.get_params())","292","","293","    eclf1.set_params(lr__C=10.0)","294","    eclf2.set_params(nb__max_depth=5)","295","","296","    assert_true(eclf1.estimators[0][1].get_params()['C'] == 10.0)","297","    assert_true(eclf2.estimators[1][1].get_params()['max_depth'] == 5)","298","    assert_equal(eclf1.get_params()[\"lr__C\"],","299","                 eclf1.get_params()[\"lr\"].get_params()['C'])","300","","301","","302","def test_set_estimator_none():","303","    \"\"\"VotingClassifier set_params should be able to set estimators as None\"\"\"","304","    # Test predict","305","    clf1 = LogisticRegression(random_state=123)","306","    clf2 = RandomForestClassifier(random_state=123)","307","    clf3 = GaussianNB()","308","    eclf1 = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2),","309","                                         ('nb', clf3)],","310","                             voting='hard', weights=[1, 0, 0.5]).fit(X, y)","311","","312","    eclf2 = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2),","313","                                         ('nb', clf3)],","314","                             voting='hard', weights=[1, 1, 0.5])","315","    eclf2.set_params(rf=None).fit(X, y)","316","    assert_array_equal(eclf1.predict(X), eclf2.predict(X))","317","","318","    assert_true(dict(eclf2.estimators)[\"rf\"] is None)","319","    assert_true(len(eclf2.estimators_) == 2)","320","    assert_true(all([not isinstance(est, RandomForestClassifier) for est in","321","                     eclf2.estimators_]))","322","    assert_true(eclf2.get_params()[\"rf\"] is None)","323","","324","    eclf1.set_params(voting='soft').fit(X, y)","325","    eclf2.set_params(voting='soft').fit(X, y)","326","    assert_array_equal(eclf1.predict(X), eclf2.predict(X))","327","    assert_array_equal(eclf1.predict_proba(X), eclf2.predict_proba(X))","328","    msg = ('All estimators are None. At least one is required'","329","           ' to be a classifier!')","330","    assert_raise_message(","331","        ValueError, msg, eclf2.set_params(lr=None, rf=None, nb=None).fit, X, y)","332","","333","    # Test soft voting transform","334","    X1 = np.array([[1], [2]])","335","    y1 = np.array([1, 2])","336","    eclf1 = VotingClassifier(estimators=[('rf', clf2), ('nb', clf3)],","337","                             voting='soft', weights=[0, 0.5]).fit(X1, y1)","338","","339","    eclf2 = VotingClassifier(estimators=[('rf', clf2), ('nb', clf3)],","340","                             voting='soft', weights=[1, 0.5])","341","    eclf2.set_params(rf=None).fit(X1, y1)","342","    assert_array_equal(eclf1.transform(X1), np.array([[[0.7, 0.3], [0.3, 0.7]],","343","                                                      [[1., 0.], [0., 1.]]]))","344","    assert_array_equal(eclf2.transform(X1), np.array([[[1., 0.], [0., 1.]]]))","345","    eclf1.set_params(voting='hard')","346","    eclf2.set_params(voting='hard')","347","    assert_array_equal(eclf1.transform(X1), np.array([[0, 0], [1, 1]]))","348","    assert_array_equal(eclf2.transform(X1), np.array([[0], [1]]))","349","","350",""],"delete":["4","from sklearn.utils.testing import assert_equal"]}]}},"04b67e2bcd26bfc8ce50b6c4b61bd6333809ed62":{"changes":{"sklearn\/kernel_ridge.py":"MODIFY","sklearn\/exceptions.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/utils\/estimator_checks.py":"MODIFY","sklearn\/ensemble\/weight_boosting.py":"MODIFY","sklearn\/linear_model\/ridge.py":"MODIFY","sklearn\/utils\/tests\/test_estimator_checks.py":"MODIFY"},"diff":{"sklearn\/kernel_ridge.py":[{"add":["11","from .utils import check_array, check_X_y","137","        sample_weight : float or array-like of shape [n_samples]","147","        if sample_weight is not None and not isinstance(sample_weight, float):","148","            sample_weight = check_array(sample_weight, ensure_2d=False)"],"delete":["11","from .utils import check_X_y","137","        sample_weight : float or numpy array of shape [n_samples]"]}],"sklearn\/exceptions.py":[{"add":["13","           'SkipTestWarning',","141","class SkipTestWarning(UserWarning):","142","    \"\"\"Warning class used to notify the user of a test that was skipped.","143","","144","    For example, one of the estimator checks requires a pandas import.","145","    If the pandas package cannot be imported, the test will be skipped rather","146","    than register as a failure.","147","    \"\"\"","148","","149",""],"delete":[]}],"doc\/whats_new.rst":[{"add":["106","   - Fix estimators to accept a ``sample_weight`` parameter of type","107","     ``pandas.Series`` in their ``fit`` function. :issue:`7825` by","108","     `Kathleen Chen`_.","109","","4830","","4831",".. _Kathleen Chen: https:\/\/github.com\/kchen17"],"delete":[]}],"sklearn\/utils\/estimator_checks.py":[{"add":["47","from sklearn.exceptions import SkipTestWarning","52","from sklearn.utils.validation import has_fit_parameter","84","    yield check_sample_weights_pandas_series","256","        try:","257","            check(name, Estimator)","258","        except SkipTest as message:","259","            # the only SkipTest thrown currently results from not","260","            # being able to import pandas.","261","            warnings.warn(message, SkipTestWarning)","390","@ignore_warnings(category=DeprecationWarning)","391","def check_sample_weights_pandas_series(name, Estimator):","392","    # check that estimators will accept a 'sample_weight' parameter of","393","    # type pandas.Series in the 'fit' function.","394","    estimator = Estimator()","395","    if has_fit_parameter(estimator, \"sample_weight\"):","396","        try:","397","            import pandas as pd","398","            X = pd.DataFrame([[1, 1], [1, 2], [1, 3], [2, 1], [2, 2], [2, 3]])","399","            y = pd.Series([1, 1, 1, 2, 2, 2])","400","            weights = pd.Series([1] * 6)","401","            try:","402","                estimator.fit(X, y, sample_weight=weights)","403","            except ValueError:","404","                raise ValueError(\"Estimator {0} raises error if \"","405","                                 \"'sample_weight' parameter is of \"","406","                                 \"type pandas.Series\".format(name))","407","        except ImportError:","408","            raise SkipTest(\"pandas is not installed: not testing for \"","409","                           \"input of type pandas.Series to class weight.\")","410","","411",""],"delete":["200","","254","        check(name, Estimator)"]}],"sklearn\/ensemble\/weight_boosting.py":[{"add":["118","            sample_weight = check_array(sample_weight, ensure_2d=False)"],"delete":[]}],"sklearn\/linear_model\/ridge.py":[{"add":["959","        if sample_weight is not None and not isinstance(sample_weight, float):","960","            sample_weight = check_array(sample_weight, ensure_2d=False)"],"delete":[]}],"sklearn\/utils\/tests\/test_estimator_checks.py":[{"add":["75","class NoSampleWeightPandasSeriesType(BaseEstimator):","76","    def fit(self, X, y, sample_weight=None):","77","        # Convert data","78","        X, y = check_X_y(X, y,","79","                         accept_sparse=(\"csr\", \"csc\"),","80","                         multi_output=True,","81","                         y_numeric=True)","82","        # Function is only called after we verify that pandas is installed","83","        from pandas import Series","84","        if isinstance(sample_weight, Series):","85","            raise ValueError(\"Estimator does not accept 'sample_weight'\"","86","                             \"of type pandas.Series\")","87","        return self","88","","89","    def predict(self, X):","90","        X = check_array(X)","91","        return np.ones(X.shape[0])","92","","93","","107","    # check that sample_weights in fit accepts pandas.Series type","108","    try:","109","        from pandas import Series  # noqa","110","        msg = (\"Estimator NoSampleWeightPandasSeriesType raises error if \"","111","               \"'sample_weight' parameter is of type pandas.Series\")","112","        assert_raises_regex(","113","            ValueError, msg, check_estimator, NoSampleWeightPandasSeriesType)","114","    except ImportError:","115","        pass"],"delete":[]}]}},"63583fe658886cc5eb48a2ba9a541d5f6be7194b":{"changes":{"sklearn\/datasets\/tests\/test_covtype.py":"MODIFY","sklearn\/datasets\/species_distributions.py":"MODIFY","sklearn\/datasets\/california_housing.py":"MODIFY","sklearn\/datasets\/covtype.py":"MODIFY","sklearn\/datasets\/olivetti_faces.py":"MODIFY","sklearn\/datasets\/kddcup99.py":"MODIFY","sklearn\/datasets\/tests\/test_kddcup99.py":"MODIFY"},"diff":{"sklearn\/datasets\/tests\/test_covtype.py":[{"add":["16","    except IOError:","17","        raise SkipTest(\"Covertype dataset can not be loaded.\")"],"delete":["5","import errno","17","    except IOError as e:","18","        if e.errno == errno.ENOENT:","19","            raise SkipTest(\"Covertype dataset can not be loaded.\")"]}],"sklearn\/datasets\/species_distributions.py":[{"add":["224","        if not download_if_missing:","225","            raise IOError(\"Data not found and `download_if_missing` is False\")","226",""],"delete":[]}],"sklearn\/datasets\/california_housing.py":[{"add":["89","","92","        if not download_if_missing:","93","            raise IOError(\"Data not found and `download_if_missing` is False\")","94",""],"delete":[]}],"sklearn\/datasets\/covtype.py":[{"add":["101","    elif not available:","102","        if not download_if_missing:","103","            raise IOError(\"Data not found and `download_if_missing` is False\")"],"delete":[]}],"sklearn\/datasets\/olivetti_faces.py":[{"add":["113","        if not download_if_missing:","114","            raise IOError(\"Data not found and `download_if_missing` is False\")","115","","126",""],"delete":[]}],"sklearn\/datasets\/kddcup99.py":[{"add":["347","    elif not available:","348","        if not download_if_missing:","349","            raise IOError(\"Data not found and `download_if_missing` is False\")"],"delete":[]}],"sklearn\/datasets\/tests\/test_kddcup99.py":[{"add":["14","    except IOError:","15","        raise SkipTest(\"kddcup99 dataset can not be loaded.\")"],"delete":["7","import errno","15","    except IOError as e:","16","        if e.errno == errno.ENOENT:","17","            raise SkipTest(\"kddcup99 dataset can not be loaded.\")"]}]}},"568c002325c69795369e7dbada31d531c445b3eb":{"changes":{"sklearn\/utils\/estimator_checks.py":"MODIFY","sklearn\/tests\/test_common.py":"MODIFY"},"diff":{"sklearn\/utils\/estimator_checks.py":[{"add":["134","    yield check_non_transformer_estimators_n_iter","135","","176","    yield check_non_transformer_estimators_n_iter","191","    # Dependent on external solvers and hence accessing the iter","192","    # param is non-trivial.","193","    external_solver = ['Isomap', 'KernelPCA', 'LocallyLinearEmbedding',","194","                       'RandomizedLasso', 'LogisticRegressionCV']","195","    if name not in external_solver:","196","        yield check_transformer_n_iter","197","","207","    yield check_non_transformer_estimators_n_iter","231","    yield check_get_params_invariance","1491","def check_non_transformer_estimators_n_iter(name, Estimator):","1492","    # Test that estimators that are not transformers with a parameter","1493","    # max_iter, return the attribute of n_iter_ at least 1.","1495","    # These models are dependent on external solvers like","1496","    # libsvm and accessing the iter parameter is non-trivial.","1497","    not_run_check_n_iter = ['Ridge', 'SVR', 'NuSVR', 'NuSVC',","1498","                            'RidgeClassifier', 'SVC', 'RandomizedLasso',","1499","                            'LogisticRegressionCV', 'LinearSVC',","1500","                            'LogisticRegression']","1502","    # Tested in test_transformer_n_iter","1503","    not_run_check_n_iter += CROSS_DECOMPOSITION","1504","    if name in not_run_check_n_iter:","1505","        return","1507","    # LassoLars stops early for the default alpha=1.0 the iris dataset.","1508","    if name == 'LassoLars':","1509","        estimator = Estimator(alpha=0.)","1511","        estimator = Estimator()","1512","    if hasattr(estimator, 'max_iter'):","1513","        iris = load_iris()","1514","        X, y_ = iris.data, iris.target","1515","        y_ = multioutput_estimator_convert_y_2d(name, y_)","1517","        set_random_state(estimator, 0)","1518","        if name == 'AffinityPropagation':","1519","            estimator.fit(X)","1520","        else:","1521","            estimator.fit(X, y_)","1522","","1523","        # HuberRegressor depends on scipy.optimize.fmin_l_bfgs_b","1524","        # which doesn't return a n_iter for old versions of SciPy.","1525","        if not (name == 'HuberRegressor' and estimator.n_iter_ is None):","1526","            assert_greater_equal(estimator.n_iter_, 1)","1530","def check_transformer_n_iter(name, Estimator):","1531","    # Test that transformers with a parameter max_iter, return the","1532","    # attribute of n_iter_ at least 1.","1533","    estimator = Estimator()","1534","    if hasattr(estimator, \"max_iter\"):","1535","        if name in CROSS_DECOMPOSITION:","1536","            # Check using default data","1537","            X = [[0., 0., 1.], [1., 0., 0.], [2., 2., 2.], [2., 5., 4.]]","1538","            y_ = [[0.1, -0.2], [0.9, 1.1], [0.1, -0.5], [0.3, -0.2]]","1540","        else:","1541","            X, y_ = make_blobs(n_samples=30, centers=[[0, 0, 0], [1, 1, 1]],","1542","                               random_state=0, n_features=2, cluster_std=0.1)","1543","            X -= X.min() - 0.1","1544","        set_random_state(estimator, 0)","1545","        estimator.fit(X, y_)","1547","        # These return a n_iter per component.","1548","        if name in CROSS_DECOMPOSITION:","1549","            for iter_ in estimator.n_iter_:","1550","                assert_greater_equal(iter_, 1)","1551","        else:","1552","            assert_greater_equal(estimator.n_iter_, 1)","1555","@ignore_warnings(category=DeprecationWarning)","1557","    # Checks if get_params(deep=False) is a subset of get_params(deep=True)"],"delete":["1479","def check_non_transformer_estimators_n_iter(name, estimator,","1480","                                            multi_output=False):","1481","    # Check if all iterative solvers, run for more than one iteration","1483","    iris = load_iris()","1484","    X, y_ = iris.data, iris.target","1486","    if multi_output:","1487","        y_ = np.reshape(y_, (-1, 1))","1489","    set_random_state(estimator, 0)","1490","    if name == 'AffinityPropagation':","1491","        estimator.fit(X)","1493","        estimator.fit(X, y_)","1495","    # HuberRegressor depends on scipy.optimize.fmin_l_bfgs_b","1496","    # which doesn't return a n_iter for old versions of SciPy.","1497","    if not (name == 'HuberRegressor' and estimator.n_iter_ is None):","1498","        assert_greater_equal(estimator.n_iter_, 1)","1502","def check_transformer_n_iter(name, estimator):","1503","    if name in CROSS_DECOMPOSITION:","1504","        # Check using default data","1505","        X = [[0., 0., 1.], [1., 0., 0.], [2., 2., 2.], [2., 5., 4.]]","1506","        y_ = [[0.1, -0.2], [0.9, 1.1], [0.1, -0.5], [0.3, -0.2]]","1508","    else:","1509","        X, y_ = make_blobs(n_samples=30, centers=[[0, 0, 0], [1, 1, 1]],","1510","                           random_state=0, n_features=2, cluster_std=0.1)","1511","        X -= X.min() - 0.1","1512","    set_random_state(estimator, 0)","1513","    estimator.fit(X, y_)","1515","    # These return a n_iter per component.","1516","    if name in CROSS_DECOMPOSITION:","1517","        for iter_ in estimator.n_iter_:","1518","            assert_greater_equal(iter_, 1)","1519","    else:","1520","        assert_greater_equal(estimator.n_iter_, 1)"]}],"sklearn\/tests\/test_common.py":[{"add":["31","    check_class_weight_balanced_linear_classifier)"],"delete":["30","    CROSS_DECOMPOSITION,","32","    check_class_weight_balanced_linear_classifier,","33","    check_transformer_n_iter,","34","    check_non_transformer_estimators_n_iter,","35","    check_get_params_invariance)","164","","165","","166","def test_non_transformer_estimators_n_iter():","167","    # Test that all estimators of type which are non-transformer","168","    # and which have an attribute of max_iter, return the attribute","169","    # of n_iter atleast 1.","170","    for est_type in ['regressor', 'classifier', 'cluster']:","171","        regressors = all_estimators(type_filter=est_type)","172","        for name, Estimator in regressors:","173","            # LassoLars stops early for the default alpha=1.0 for","174","            # the iris dataset.","175","            if name == 'LassoLars':","176","                estimator = Estimator(alpha=0.)","177","            else:","178","                with ignore_warnings(category=DeprecationWarning):","179","                    estimator = Estimator()","180","            if hasattr(estimator, \"max_iter\"):","181","                # These models are dependent on external solvers like","182","                # libsvm and accessing the iter parameter is non-trivial.","183","                if name in (['Ridge', 'SVR', 'NuSVR', 'NuSVC',","184","                             'RidgeClassifier', 'SVC', 'RandomizedLasso',","185","                             'LogisticRegressionCV']):","186","                    continue","187","","188","                # Tested in test_transformer_n_iter below","189","                elif (name in CROSS_DECOMPOSITION or","190","                      name in ['LinearSVC', 'LogisticRegression']):","191","                    continue","192","","193","                else:","194","                    # Multitask models related to ENet cannot handle","195","                    # if y is mono-output.","196","                    yield (_named_check(","197","                        check_non_transformer_estimators_n_iter, name),","198","                        name, estimator, 'Multi' in name)","199","","200","","201","def test_transformer_n_iter():","202","    transformers = all_estimators(type_filter='transformer')","203","    for name, Estimator in transformers:","204","        with ignore_warnings(category=DeprecationWarning):","205","            estimator = Estimator()","206","        # Dependent on external solvers and hence accessing the iter","207","        # param is non-trivial.","208","        external_solver = ['Isomap', 'KernelPCA', 'LocallyLinearEmbedding',","209","                           'RandomizedLasso', 'LogisticRegressionCV']","210","","211","        if hasattr(estimator, \"max_iter\") and name not in external_solver:","212","            yield _named_check(","213","                check_transformer_n_iter, name), name, estimator","214","","215","","216","def test_get_params_invariance():","217","    # Test for estimators that support get_params, that","218","    # get_params(deep=False) is a subset of get_params(deep=True)","219","    # Related to issue #4465","220","","221","    estimators = all_estimators(include_meta_estimators=False,","222","                                include_other=True)","223","    for name, Estimator in estimators:","224","        if hasattr(Estimator, 'get_params'):","225","            # If class is deprecated, ignore deprecated warnings","226","            if hasattr(Estimator.__init__, \"deprecated_original\"):","227","                with ignore_warnings():","228","                    yield _named_check(","229","                        check_get_params_invariance, name), name, Estimator","230","            else:","231","                yield _named_check(","232","                    check_get_params_invariance, name), name, Estimator"]}]}},"34968d4c5dee53983c66dc44bb1d7eb173aa8a0d":{"changes":{"sklearn\/model_selection\/tests\/test_split.py":"MODIFY"},"diff":{"sklearn\/model_selection\/tests\/test_split.py":[{"add":["1030","    np.testing.assert_equal(list(kf_iter_wrapped.split(X, y)),","1031","                            list(kf_iter_wrapped.split(X, y)))","1036","    np.testing.assert_equal(list(kf_randomized_iter_wrapped.split(X, y)),","1037","                            list(kf_randomized_iter_wrapped.split(X, y)))","1038","","1039","    try:","1040","        np.testing.assert_equal(list(kf_iter_wrapped.split(X, y)),","1041","                                list(kf_randomized_iter_wrapped.split(X, y)))","1042","        splits_are_equal = True","1043","    except AssertionError:","1044","        splits_are_equal = False","1045","    assert_false(splits_are_equal, \"If the splits are randomized, \"","1046","                 \"successive calls to split should yield different results\")"],"delete":["1030","    assert_array_equal(list(kf_iter_wrapped.split(X, y)),","1031","                       list(kf_iter_wrapped.split(X, y)))","1036","    assert_array_equal(list(kf_randomized_iter_wrapped.split(X, y)),","1037","                       list(kf_randomized_iter_wrapped.split(X, y)))","1038","    assert_true(np.any(np.array(list(kf_iter_wrapped.split(X, y))) !=","1039","                       np.array(list(kf_randomized_iter_wrapped.split(X, y)))))"]}]}},"3d997697fdd166eff428ea9fd35734b6a8ba113e":{"changes":{"sklearn\/cluster\/hierarchical.py":"MODIFY","sklearn\/cluster\/tests\/test_hierarchical.py":"MODIFY"},"diff":{"sklearn\/cluster\/hierarchical.py":[{"add":["434","    if affinity == 'cosine' and np.any(~np.any(X, axis=1)):","435","        raise ValueError(","436","            'Cosine affinity cannot be used when X contains zero vectors')","437",""],"delete":[]}],"sklearn\/cluster\/tests\/test_hierarchical.py":[{"add":["130","def test_zero_cosine_linkage_tree():","131","    # Check that zero vectors in X produce an error when","132","    # 'cosine' affinity is used","133","    X = np.array([[0, 1],","134","                  [0, 0]])","135","    msg = 'Cosine affinity cannot be used when X contains zero vectors'","136","    assert_raise_message(ValueError, msg, linkage_tree, X, affinity='cosine')","137","","138",""],"delete":[]}]}},"3162f980a901c5057b77dc94f97bd4787b9ff1ff":{"changes":{"sklearn\/ensemble\/gradient_boosting.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/ensemble\/tests\/test_gradient_boosting.py":"MODIFY"},"diff":{"sklearn\/ensemble\/gradient_boosting.py":[{"add":["767","                min_impurity_split=self.min_impurity_split,"],"delete":[]}],"doc\/whats_new.rst":[{"add":["120","   - Fix a bug where :class:`sklearn.ensemble.GradientBoostingClassifier` and","121","     :class:`sklearn.ensemble.GradientBoostingRegressor` ignored the","122","     ``min_impurity_split`` parameter.","123","     :issue:`8006` by :user:`Sebastian P?lsterl <sebp>`.","124",""],"delete":["129",""]}],"sklearn\/ensemble\/tests\/test_gradient_boosting.py":[{"add":["963","def test_min_impurity_split():","964","    # Test if min_impurity_split of base estimators is set","965","    # Regression test for #8006","966","    X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1)","967","    all_estimators = [GradientBoostingRegressor,","968","                      GradientBoostingClassifier]","969","","970","    for GBEstimator in all_estimators:","971","        est = GBEstimator(min_impurity_split=0.1).fit(X, y)","972","        for tree in est.estimators_.flat:","973","            assert_equal(tree.min_impurity_split, 0.1)","974","","975",""],"delete":[]}]}},"b15818e7e0d44da976e93e06eb96000b12cd059f":{"changes":{"doc\/whats_new.rst":"MODIFY","sklearn\/cross_decomposition\/pls_.py":"MODIFY","sklearn\/cross_decomposition\/tests\/test_pls.py":"MODIFY"},"diff":{"doc\/whats_new.rst":[{"add":["347","   - Fixed improper scaling in :class:`sklearn.cross_decomposition.PLSRegression`","348","     with ``scale=True``. :issue:`7819` by :user:`jayzed82 <jayzed82>`."],"delete":[]}],"sklearn\/cross_decomposition\/pls_.py":[{"add":["368","            self.coef_ = self.coef_ * self.y_std_"],"delete":["368","            self.coef_ = (1. \/ self.x_std_.reshape((p, 1)) * self.coef_ *","369","                          self.y_std_)"]}],"sklearn\/cross_decomposition\/tests\/test_pls.py":[{"add":["1","from numpy.testing import assert_approx_equal","2","","8","from sklearn.preprocessing import StandardScaler","356","","364","        assert_raise_message(ValueError, \"Invalid number of components\",","365","                             clf.fit, X, Y)","366","","367","","368","def test_pls_scaling():","369","    # sanity check for scale=True","370","    n_samples = 1000","371","    n_targets = 5","372","    n_features = 10","373","","374","    rng = np.random.RandomState(0)","375","","376","    Q = rng.randn(n_targets, n_features)","377","    Y = rng.randn(n_samples, n_targets)","378","    X = np.dot(Y, Q) + 2 * rng.randn(n_samples, n_features) + 1","379","    X *= 1000","380","    X_scaled = StandardScaler().fit_transform(X)","381","","382","    pls = pls_.PLSRegression(n_components=5, scale=True)","383","","384","    pls.fit(X, Y)","385","    score = pls.score(X, Y)","386","","387","    pls.fit(X_scaled, Y)","388","    score_scaled = pls.score(X_scaled, Y)","389","","390","    assert_approx_equal(score, score_scaled)"],"delete":["360","        assert_raise_message(ValueError, \"Invalid number of components\", clf.fit, X, Y)"]}]}},"726fa36f2556e0d604d85a1de48ba56a8b6550db":{"changes":{"sklearn\/tests\/test_impute.py":"MODIFY","doc\/whats_new\/v0.20.rst":"MODIFY","examples\/plot_missing_values.py":"MODIFY","doc\/modules\/classes.rst":"MODIFY","doc\/modules\/impute.rst":"MODIFY","sklearn\/impute.py":"MODIFY","sklearn\/utils\/estimator_checks.py":"MODIFY"},"diff":{"sklearn\/tests\/test_impute.py":[{"add":["15","from sklearn.impute import MissingIndicator","710","@pytest.mark.parametrize(","711","    \"X_fit, X_trans, params, msg_err\",","712","    [(np.array([[-1, 1], [1, 2]]), np.array([[-1, 1], [1, -1]]),","713","      {'features': 'missing-only', 'sparse': 'auto'},","714","      'have missing values in transform but have no missing values in fit'),","715","     (np.array([[-1, 1], [1, 2]]), np.array([[-1, 1], [1, 2]]),","716","      {'features': 'random', 'sparse': 'auto'},","717","      \"'features' has to be either 'missing-only' or 'all'\"),","718","     (np.array([[-1, 1], [1, 2]]), np.array([[-1, 1], [1, 2]]),","719","      {'features': 'all', 'sparse': 'random'},","720","      \"'sparse' has to be a boolean or 'auto'\")]","721",")","722","def test_missing_indicator_error(X_fit, X_trans, params, msg_err):","723","    indicator = MissingIndicator(missing_values=-1)","724","    indicator.set_params(**params)","725","    with pytest.raises(ValueError, match=msg_err):","726","        indicator.fit(X_fit).transform(X_trans)","727","","728","","729","@pytest.mark.parametrize(","730","    \"missing_values, dtype\",","731","    [(np.nan, np.float64),","732","     (0, np.int32),","733","     (-1, np.int32)])","734","@pytest.mark.parametrize(","735","    \"arr_type\",","736","    [np.array, sparse.csc_matrix, sparse.csr_matrix, sparse.coo_matrix,","737","     sparse.lil_matrix, sparse.bsr_matrix])","738","@pytest.mark.parametrize(","739","    \"param_features, n_features, features_indices\",","740","    [('missing-only', 2, np.array([0, 1])),","741","     ('all', 3, np.array([0, 1, 2]))])","742","def test_missing_indicator_new(missing_values, arr_type, dtype, param_features,","743","                               n_features, features_indices):","744","    X_fit = np.array([[missing_values, missing_values, 1],","745","                      [4, missing_values, 2]])","746","    X_trans = np.array([[missing_values, missing_values, 1],","747","                        [4, 12, 10]])","748","    X_fit_expected = np.array([[1, 1, 0], [0, 1, 0]])","749","    X_trans_expected = np.array([[1, 1, 0], [0, 0, 0]])","750","","751","    # convert the input to the right array format and right dtype","752","    X_fit = arr_type(X_fit).astype(dtype)","753","    X_trans = arr_type(X_trans).astype(dtype)","754","    X_fit_expected = X_fit_expected.astype(dtype)","755","    X_trans_expected = X_trans_expected.astype(dtype)","756","","757","    indicator = MissingIndicator(missing_values=missing_values,","758","                                 features=param_features,","759","                                 sparse=False)","760","    X_fit_mask = indicator.fit_transform(X_fit)","761","    X_trans_mask = indicator.transform(X_trans)","762","","763","    assert X_fit_mask.shape[1] == n_features","764","    assert X_trans_mask.shape[1] == n_features","765","","766","    assert_array_equal(indicator.features_, features_indices)","767","    assert_allclose(X_fit_mask, X_fit_expected[:, features_indices])","768","    assert_allclose(X_trans_mask, X_trans_expected[:, features_indices])","769","","770","    assert X_fit_mask.dtype == bool","771","    assert X_trans_mask.dtype == bool","772","    assert isinstance(X_fit_mask, np.ndarray)","773","    assert isinstance(X_trans_mask, np.ndarray)","774","","775","    indicator.set_params(sparse=True)","776","    X_fit_mask_sparse = indicator.fit_transform(X_fit)","777","    X_trans_mask_sparse = indicator.transform(X_trans)","778","","779","    assert X_fit_mask_sparse.dtype == bool","780","    assert X_trans_mask_sparse.dtype == bool","781","    assert X_fit_mask_sparse.format == 'csc'","782","    assert X_trans_mask_sparse.format == 'csc'","783","    assert_allclose(X_fit_mask_sparse.toarray(), X_fit_mask)","784","    assert_allclose(X_trans_mask_sparse.toarray(), X_trans_mask)","785","","786","","787","@pytest.mark.parametrize(\"param_sparse\", [True, False, 'auto'])","788","@pytest.mark.parametrize(\"missing_values\", [np.nan, 0])","789","@pytest.mark.parametrize(","790","    \"arr_type\",","791","    [np.array, sparse.csc_matrix, sparse.csr_matrix, sparse.coo_matrix])","792","def test_missing_indicator_sparse_param(arr_type, missing_values,","793","                                        param_sparse):","794","    # check the format of the output with different sparse parameter","795","    X_fit = np.array([[missing_values, missing_values, 1],","796","                      [4, missing_values, 2]])","797","    X_trans = np.array([[missing_values, missing_values, 1],","798","                        [4, 12, 10]])","799","    X_fit = arr_type(X_fit).astype(np.float64)","800","    X_trans = arr_type(X_trans).astype(np.float64)","801","","802","    indicator = MissingIndicator(missing_values=missing_values,","803","                                 sparse=param_sparse)","804","    X_fit_mask = indicator.fit_transform(X_fit)","805","    X_trans_mask = indicator.transform(X_trans)","806","","807","    if param_sparse is True:","808","        assert X_fit_mask.format == 'csc'","809","        assert X_trans_mask.format == 'csc'","810","    elif param_sparse == 'auto' and missing_values == 0:","811","        assert isinstance(X_fit_mask, np.ndarray)","812","        assert isinstance(X_trans_mask, np.ndarray)","813","    elif param_sparse is False:","814","        assert isinstance(X_fit_mask, np.ndarray)","815","        assert isinstance(X_trans_mask, np.ndarray)","816","    else:","817","        if sparse.issparse(X_fit):","818","            assert X_fit_mask.format == 'csc'","819","            assert X_trans_mask.format == 'csc'","820","        else:","821","            assert isinstance(X_fit_mask, np.ndarray)","822","            assert isinstance(X_trans_mask, np.ndarray)","823","","824",""],"delete":[]}],"doc\/whats_new\/v0.20.rst":[{"add":["151","- Added :class:`MissingIndicator` which generates a binary indicator for","152","  missing values. :issue:`8075` by :user:`Maniteja Nandana <maniteja123>` and","153","  :user:`Guillaume Lemaitre <glemaitre>`.","154","  "],"delete":[]}],"examples\/plot_missing_values.py":[{"add":["6","value using the basic :func:`sklearn.impute.SimpleImputer`.","10","Another option is the :func:`sklearn.impute.ChainedImputer`. This uses","11","round-robin linear regression, treating every variable as an output in","12","turn. The version implemented assumes Gaussian (output) variables. If your","13","features are obviously non-Normal, consider transforming them to look more","14","Normal so as to improve performance.","15","","16","In addition of using an imputing method, we can also keep an indication of the","17","missing information using :func:`sklearn.impute.MissingIndicator` which might","18","carry some information.","27","from sklearn.pipeline import make_pipeline, make_union","28","from sklearn.impute import SimpleImputer, ChainedImputer, MissingIndicator","66","    estimator = make_pipeline(","67","        make_union(SimpleImputer(missing_values=0, strategy=\"mean\"),","68","                   MissingIndicator(missing_values=0)),","69","        RandomForestRegressor(random_state=0, n_estimators=100))","74","    estimator = make_pipeline(","75","        make_union(ChainedImputer(missing_values=0, random_state=0),","76","                   MissingIndicator(missing_values=0)),","77","        RandomForestRegressor(random_state=0, n_estimators=100))"],"delete":["6","value using the basic ``SimpleImputer``.","10","Another option is the ``ChainedImputer``. This uses round-robin linear","11","regression, treating every variable as an output in turn. The version","12","implemented assumes Gaussian (output) variables. If your features are obviously","13","non-Normal, consider transforming them to look more Normal so as to improve","14","performance.","23","from sklearn.pipeline import Pipeline","24","from sklearn.impute import SimpleImputer, ChainedImputer","62","    estimator = Pipeline([(\"imputer\", SimpleImputer(missing_values=0,","63","                                                    strategy=\"mean\")),","64","                          (\"forest\", RandomForestRegressor(random_state=0,","65","                                                           n_estimators=100))])","70","    estimator = Pipeline([(\"imputer\", ChainedImputer(missing_values=0,","71","                                                     random_state=0)),","72","                          (\"forest\", RandomForestRegressor(random_state=0,","73","                                                           n_estimators=100))])"]}],"doc\/modules\/classes.rst":[{"add":["658","   impute.MissingIndicator","659","   "],"delete":["658",""]}],"doc\/modules\/impute.rst":[{"add":["143","","144",".. _missing_indicator:","145","","146","Marking imputed values","147","======================","148","","149","The :class:`MissingIndicator` transformer is useful to transform a dataset into","150","corresponding binary matrix indicating the presence of missing values in the","151","dataset. This transformation is useful in conjunction with imputation. When","152","using imputation, preserving the information about which values had been","153","missing can be informative.","154","","155","``NaN`` is usually used as the placeholder for missing values. However, it","156","enforces the data type to be float. The parameter ``missing_values`` allows to","157","specify other placeholder such as integer. In the following example, we will","158","use ``-1`` as missing values::","159","","160","  >>> from sklearn.impute import MissingIndicator","161","  >>> X = np.array([[-1, -1, 1, 3],","162","  ...               [4, -1, 0, -1],","163","  ...               [8, -1, 1, 0]])","164","  >>> indicator = MissingIndicator(missing_values=-1)","165","  >>> mask_missing_values_only = indicator.fit_transform(X)","166","  >>> mask_missing_values_only","167","  array([[ True,  True, False],","168","         [False,  True,  True],","169","         [False,  True, False]])","170","","171","The ``features`` parameter is used to choose the features for which the mask is","172","constructed. By default, it is ``'missing-only'`` which returns the imputer","173","mask of the features containing missing values at ``fit`` time::","174","","175","  >>> indicator.features_","176","  array([0, 1, 3])","177","","178","The ``features`` parameter can be set to ``'all'`` to returned all features","179","whether or not they contain missing values::","180","    ","181","  >>> indicator = MissingIndicator(missing_values=-1, features=\"all\")","182","  >>> mask_all = indicator.fit_transform(X)","183","  >>> mask_all","184","  array([[ True,  True, False, False],","185","         [False,  True, False,  True],","186","         [False,  True, False, False]])","187","  >>> indicator.features_","188","  array([0, 1, 2, 3])"],"delete":["123",""]}],"sklearn\/impute.py":[{"add":["37","    'MissingIndicator',","978","","979","","980","class MissingIndicator(BaseEstimator, TransformerMixin):","981","    \"\"\"Binary indicators for missing values.","982","","983","    Parameters","984","    ----------","985","    missing_values : number, string, np.nan (default) or None","986","        The placeholder for the missing values. All occurrences of","987","        `missing_values` will be imputed.","988","","989","    features : str, optional","990","        Whether the imputer mask should represent all or a subset of","991","        features.","992","","993","        - If \"missing-only\" (default), the imputer mask will only represent","994","          features containing missing values during fit time.","995","        - If \"all\", the imputer mask will represent all features.","996","","997","    sparse : boolean or \"auto\", optional","998","        Whether the imputer mask format should be sparse or dense.","999","","1000","        - If \"auto\" (default), the imputer mask will be of same type as","1001","          input.","1002","        - If True, the imputer mask will be a sparse matrix.","1003","        - If False, the imputer mask will be a numpy array.","1004","","1005","    error_on_new : boolean, optional","1006","        If True (default), transform will raise an error when there are","1007","        features with missing values in transform that have no missing values","1008","        in fit This is applicable only when ``features=\"missing-only\"``.","1009","","1010","    Attributes","1011","    ----------","1012","    features_ : ndarray, shape (n_missing_features,) or (n_features,)","1013","        The features indices which will be returned when calling ``transform``.","1014","        They are computed during ``fit``. For ``features='all'``, it is","1015","        to ``range(n_features)``.","1016","","1017","    Examples","1018","    --------","1019","    >>> import numpy as np","1020","    >>> from sklearn.impute import MissingIndicator","1021","    >>> X1 = np.array([[np.nan, 1, 3],","1022","    ...                [4, 0, np.nan],","1023","    ...                [8, 1, 0]])","1024","    >>> X2 = np.array([[5, 1, np.nan],","1025","    ...                [np.nan, 2, 3],","1026","    ...                [2, 4, 0]])","1027","    >>> indicator = MissingIndicator()","1028","    >>> indicator.fit(X1)","1029","    MissingIndicator(error_on_new=True, features='missing-only',","1030","             missing_values=nan, sparse='auto')","1031","    >>> X2_tr = indicator.transform(X2)","1032","    >>> X2_tr","1033","    array([[False,  True],","1034","           [ True, False],","1035","           [False, False]])","1036","","1037","    \"\"\"","1038","","1039","    def __init__(self, missing_values=np.nan, features=\"missing-only\",","1040","                 sparse=\"auto\", error_on_new=True):","1041","        self.missing_values = missing_values","1042","        self.features = features","1043","        self.sparse = sparse","1044","        self.error_on_new = error_on_new","1045","","1046","    def _get_missing_features_info(self, X):","1047","        \"\"\"Compute the imputer mask and the indices of the features","1048","        containing missing values.","1049","","1050","        Parameters","1051","        ----------","1052","        X : {ndarray or sparse matrix}, shape (n_samples, n_features)","1053","            The input data with missing values. Note that ``X`` has been","1054","            checked in ``fit`` and ``transform`` before to call this function.","1055","","1056","        Returns","1057","        -------","1058","        imputer_mask : {ndarray or sparse matrix}, shape \\","1059","(n_samples, n_features) or (n_samples, n_features_with_missing)","1060","            The imputer mask of the original data.","1061","","1062","        features_with_missing : ndarray, shape (n_features_with_missing)","1063","            The features containing missing values.","1064","","1065","        \"\"\"","1066","        if sparse.issparse(X) and self.missing_values != 0:","1067","            mask = _get_mask(X.data, self.missing_values)","1068","","1069","            # The imputer mask will be constructed with the same sparse format","1070","            # as X.","1071","            sparse_constructor = (sparse.csr_matrix if X.format == 'csr'","1072","                                  else sparse.csc_matrix)","1073","            imputer_mask = sparse_constructor(","1074","                (mask, X.indices.copy(), X.indptr.copy()),","1075","                shape=X.shape, dtype=bool)","1076","","1077","            missing_values_mask = imputer_mask.copy()","1078","            missing_values_mask.eliminate_zeros()","1079","            features_with_missing = (","1080","                np.flatnonzero(np.diff(missing_values_mask.indptr))","1081","                if missing_values_mask.format == 'csc'","1082","                else np.unique(missing_values_mask.indices))","1083","","1084","            if self.sparse is False:","1085","                imputer_mask = imputer_mask.toarray()","1086","            elif imputer_mask.format == 'csr':","1087","                imputer_mask = imputer_mask.tocsc()","1088","        else:","1089","            if sparse.issparse(X):","1090","                # case of sparse matrix with 0 as missing values. Implicit and","1091","                # explicit zeros are considered as missing values.","1092","                X = X.toarray()","1093","            imputer_mask = _get_mask(X, self.missing_values)","1094","            features_with_missing = np.flatnonzero(imputer_mask.sum(axis=0))","1095","","1096","            if self.sparse is True:","1097","                imputer_mask = sparse.csc_matrix(imputer_mask)","1098","","1099","        return imputer_mask, features_with_missing","1100","","1101","    def fit(self, X, y=None):","1102","        \"\"\"Fit the transformer on X.","1103","","1104","        Parameters","1105","        ----------","1106","        X : {array-like, sparse matrix}, shape (n_samples, n_features)","1107","            Input data, where ``n_samples`` is the number of samples and","1108","            ``n_features`` is the number of features.","1109","","1110","        Returns","1111","        -------","1112","        self : object","1113","            Returns self.","1114","        \"\"\"","1115","        if not is_scalar_nan(self.missing_values):","1116","            force_all_finite = True","1117","        else:","1118","            force_all_finite = \"allow-nan\"","1119","        X = check_array(X, accept_sparse=('csc', 'csr'),","1120","                        force_all_finite=force_all_finite)","1121","        _check_inputs_dtype(X, self.missing_values)","1122","","1123","        self._n_features = X.shape[1]","1124","","1125","        if self.features not in ('missing-only', 'all'):","1126","            raise ValueError(\"'features' has to be either 'missing-only' or \"","1127","                             \"'all'. Got {} instead.\".format(self.features))","1128","","1129","        if not ((isinstance(self.sparse, six.string_types) and","1130","                self.sparse == \"auto\") or isinstance(self.sparse, bool)):","1131","            raise ValueError(\"'sparse' has to be a boolean or 'auto'. \"","1132","                             \"Got {!r} instead.\".format(self.sparse))","1133","","1134","        self.features_ = (self._get_missing_features_info(X)[1]","1135","                          if self.features == 'missing-only'","1136","                          else np.arange(self._n_features))","1137","","1138","        return self","1139","","1140","    def transform(self, X):","1141","        \"\"\"Generate missing values indicator for X.","1142","","1143","        Parameters","1144","        ----------","1145","        X : {array-like, sparse matrix}, shape (n_samples, n_features)","1146","            The input data to complete.","1147","","1148","        Returns","1149","        -------","1150","        Xt : {ndarray or sparse matrix}, shape (n_samples, n_features)","1151","            The missing indicator for input data. The data type of ``Xt``","1152","            will be boolean.","1153","","1154","        \"\"\"","1155","        check_is_fitted(self, \"features_\")","1156","","1157","        if not is_scalar_nan(self.missing_values):","1158","            force_all_finite = True","1159","        else:","1160","            force_all_finite = \"allow-nan\"","1161","        X = check_array(X, accept_sparse=('csc', 'csr'),","1162","                        force_all_finite=force_all_finite)","1163","        _check_inputs_dtype(X, self.missing_values)","1164","","1165","        if X.shape[1] != self._n_features:","1166","            raise ValueError(\"X has a different number of features \"","1167","                             \"than during fitting.\")","1168","","1169","        imputer_mask, features = self._get_missing_features_info(X)","1170","","1171","        if self.features == \"missing-only\":","1172","            features_diff_fit_trans = np.setdiff1d(features, self.features_)","1173","            if (self.error_on_new and features_diff_fit_trans.size > 0):","1174","                raise ValueError(\"The features {} have missing values \"","1175","                                 \"in transform but have no missing values \"","1176","                                 \"in fit.\".format(features_diff_fit_trans))","1177","","1178","            if (self.features_.size > 0 and","1179","                    self.features_.size < self._n_features):","1180","                imputer_mask = imputer_mask[:, self.features_]","1181","","1182","        return imputer_mask","1183","","1184","    def fit_transform(self, X, y=None):","1185","        \"\"\"Generate missing values indicator for X.","1186","","1187","        Parameters","1188","        ----------","1189","        X : {array-like, sparse matrix}, shape (n_samples, n_features)","1190","            The input data to complete.","1191","","1192","        Returns","1193","        -------","1194","        Xt : {ndarray or sparse matrix}, shape (n_samples, n_features)","1195","            The missing indicator for input data. The data type of ``Xt``","1196","            will be boolean.","1197","","1198","        \"\"\"","1199","        return self.fit(X, y).transform(X)"],"delete":[]}],"sklearn\/utils\/estimator_checks.py":[{"add":["79","ALLOW_NAN = ['Imputer', 'SimpleImputer', 'ChainedImputer', 'MissingIndicator',"],"delete":["79","ALLOW_NAN = ['Imputer', 'SimpleImputer', 'ChainedImputer',"]}]}},"f61d96c3129ce462f6cf10a5de596167817a60c9":{"changes":{"sklearn\/datasets\/kddcup99.py":"MODIFY"},"diff":{"sklearn\/datasets\/kddcup99.py":[{"add":["47","    created by MIT Lincoln Lab [1]. The artificial data was generated using","136","    percent10 : bool, default=True","157","    .. [2] K. Yamanishi, J.-I. Takeuchi, G. Williams, and P. Milne. Online","158","           unsupervised outlier detection using finite mixtures with","159","           discounting learning algorithms. In Proceedings of the sixth","160","           ACM SIGKDD international conference on Knowledge discovery","161","           and data mining, pages 320-324. ACM Press, 2000.","218","                          shuffle=False, percent10=True):","246","    percent10 : bool, default=True"],"delete":["47","    created by MIT Lincoln Lab [1] . The artificial data was generated using","136","    percent10 : bool, default=False","157","    .. [2] A Geometric Framework for Unsupervised Anomaly Detection: Detecting","158","           Intrusions in Unlabeled Data (2002) by Eleazar Eskin, Andrew Arnold,","159","           Michael Prerau, Leonid Portnoy, Sal Stolfo","216","                          shuffle=False, percent10=False):","244","    percent10 : bool, default=False"]}]}}}