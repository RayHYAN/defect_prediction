{"d7c956afc8c67642bbd42fd9bacb007b5fbc443a":{"changes":{"sklearn\/model_selection\/_split.py":"MODIFY","sklearn\/model_selection\/tests\/test_split.py":"MODIFY"},"diff":{"sklearn\/model_selection\/_split.py":[{"add":["27","from ..utils.validation import check_array","475","        groups = check_array(groups, ensure_2d=False, dtype=None)","622","            Note that providing ``y`` is sufficient to generate the splits and","623","            hence ``np.zeros(n_samples)`` may be used as a placeholder for","624","            ``X`` instead of actual training data.","625","","628","            Stratification is done based on the y labels.","630","        groups : object","631","            Always ignored, exists for compatibility.","641","        y = check_array(y, ensure_2d=False, dtype=None)","705","            Always ignored, exists for compatibility.","708","            Always ignored, exists for compatibility.","754","    >>> logo = LeaveOneGroupOut()","755","    >>> logo.get_n_splits(X, y, groups)","757","    >>> print(logo)","759","    >>> for train_index, test_index in logo.split(X, y, groups):","779","        groups = check_array(groups, copy=True, ensure_2d=False, dtype=None)","841","    >>> lpgo = LeavePGroupsOut(n_groups=2)","842","    >>> lpgo.get_n_splits(X, y, groups)","844","    >>> print(lpgo)","846","    >>> for train_index, test_index in lpgo.split(X, y, groups):","872","        groups = check_array(groups, copy=True, ensure_2d=False, dtype=None)","894","            ``np.zeros(n_samples)`` may be used as a placeholder.","898","            ``np.zeros(n_samples)`` may be used as a placeholder.","911","        groups = check_array(groups, ensure_2d=False, dtype=None)","912","        X, y, groups = indexable(X, y, groups)","1109","        groups = check_array(groups, ensure_2d=False, dtype=None)","1250","        y = check_array(y, ensure_2d=False, dtype=None)","1304","            Note that providing ``y`` is sufficient to generate the splits and","1305","            hence ``np.zeros(n_samples)`` may be used as a placeholder for","1306","            ``X`` instead of actual training data.","1307","","1310","            Stratification is done based on the y labels.","1312","        groups : object","1313","            Always ignored, exists for compatibility.","1323","        y = check_array(y, ensure_2d=False, dtype=None)","1632","        the class labels."],"delete":["623","        groups : array-like, with shape (n_samples,), optional","624","            Group labels for the samples used while splitting the dataset into","625","            train\/test set.","698","            The target variable for supervised learning problems.","701","            Group labels for the samples used while splitting the dataset into","702","            train\/test set.","748","    >>> lol = LeaveOneGroupOut()","749","    >>> lol.get_n_splits(X, y, groups)","751","    >>> print(lol)","753","    >>> for train_index, test_index in lol.split(X, y, groups):","773","        groups = np.array(groups, copy=True)","835","    >>> lpl = LeavePGroupsOut(n_groups=2)","836","    >>> lpl.get_n_splits(X, y, groups)","838","    >>> print(lpl)","840","    >>> for train_index, test_index in lpl.split(X, y, groups):","866","        groups = np.array(groups, copy=True)","1295","        groups : array-like, with shape (n_samples,), optional","1296","            Group labels for the samples used while splitting the dataset into","1297","            train\/test set.","1615","        the groups array."]}],"sklearn\/model_selection\/tests\/test_split.py":[{"add":["61","P_sparse = coo_matrix(np.eye(5))","62","test_groups = (","63","    np.array([1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 3]),","64","    np.array([0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3]),","65","    np.array([0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2]),","66","    np.array([1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4]),","67","    [1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 3],","68","    ['1', '1', '1', '1', '2', '2', '2', '3', '3', '3', '3', '3'])","72","class MockClassifier(object):","73","    \"\"\"Dummy classifier to test the cross-validation\"\"\"","74","","75","    def __init__(self, a=0, allow_nd=False):","76","        self.a = a","77","        self.allow_nd = allow_nd","78","","79","    def fit(self, X, Y=None, sample_weight=None, class_prior=None,","80","            sparse_sample_weight=None, sparse_param=None, dummy_int=None,","81","            dummy_str=None, dummy_obj=None, callback=None):","82","        \"\"\"The dummy arguments are to test that this fit function can","83","        accept non-array arguments through cross-validation, such as:","84","            - int","85","            - str (this is actually array-like)","86","            - object","87","            - function","88","        \"\"\"","89","        self.dummy_int = dummy_int","90","        self.dummy_str = dummy_str","91","        self.dummy_obj = dummy_obj","92","        if callback is not None:","93","            callback(self)","94","","95","        if self.allow_nd:","96","            X = X.reshape(len(X), -1)","97","        if X.ndim >= 3 and not self.allow_nd:","98","            raise ValueError('X cannot be d')","99","        if sample_weight is not None:","100","            assert_true(sample_weight.shape[0] == X.shape[0],","101","                        'MockClassifier extra fit_param sample_weight.shape[0]'","102","                        ' is {0}, should be {1}'.format(sample_weight.shape[0],","103","                                                        X.shape[0]))","104","        if class_prior is not None:","105","            assert_true(class_prior.shape[0] == len(np.unique(y)),","106","                        'MockClassifier extra fit_param class_prior.shape[0]'","107","                        ' is {0}, should be {1}'.format(class_prior.shape[0],","108","                                                        len(np.unique(y))))","109","        if sparse_sample_weight is not None:","110","            fmt = ('MockClassifier extra fit_param sparse_sample_weight'","111","                   '.shape[0] is {0}, should be {1}')","112","            assert_true(sparse_sample_weight.shape[0] == X.shape[0],","113","                        fmt.format(sparse_sample_weight.shape[0], X.shape[0]))","114","        if sparse_param is not None:","115","            fmt = ('MockClassifier extra fit_param sparse_param.shape '","116","                   'is ({0}, {1}), should be ({2}, {3})')","117","            assert_true(sparse_param.shape == P_sparse.shape,","118","                        fmt.format(sparse_param.shape[0],","119","                                   sparse_param.shape[1],","120","                                   P_sparse.shape[0], P_sparse.shape[1]))","121","        return self","122","","123","    def predict(self, T):","124","        if self.allow_nd:","125","            T = T.reshape(len(T), -1)","126","        return T[:, 0]","127","","128","    def score(self, X=None, Y=None):","129","        return 1. \/ (1 + np.abs(self.a))","130","","131","    def get_params(self, deep=False):","132","        return {'a': self.a, 'allow_nd': self.allow_nd}","133","","134","","336","    # Make sure string labels are also supported","337","    X = np.ones(7)","338","    y1 = ['1', '1', '1', '0', '0', '0', '0']","339","    y2 = [1, 1, 1, 0, 0, 0, 0]","340","    np.testing.assert_equal(","341","        list(StratifiedKFold(2).split(X, y1)),","342","        list(StratifiedKFold(2).split(X, y2)))","343","","566","          np.concatenate([[i] * (100 + i) for i in range(11)]),","567","          [1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 3],","568","          ['1', '1', '1', '1', '2', '2', '2', '3', '3', '3', '3', '3'],","574","        y = np.asanyarray(y)  # To make it indexable for y[train]","682","    for groups_i in test_groups:","683","        X = y = np.ones(len(groups_i))","692","        assert_equal(slo.get_n_splits(X, y, groups=groups_i), n_splits)","694","        l_unique = np.unique(groups_i)","695","        l = np.asarray(groups_i)","697","        for train, test in slo.split(X, y, groups=groups_i):","718","def test_leave_one_p_group_out():","719","    logo = LeaveOneGroupOut()","720","    lpgo_1 = LeavePGroupsOut(n_groups=1)","721","    lpgo_2 = LeavePGroupsOut(n_groups=2)","722","","723","    # Make sure the repr works","724","    assert_equal(repr(logo), 'LeaveOneGroupOut()')","725","    assert_equal(repr(lpgo_1), 'LeavePGroupsOut(n_groups=1)')","726","    assert_equal(repr(lpgo_2), 'LeavePGroupsOut(n_groups=2)')","727","    assert_equal(repr(LeavePGroupsOut(n_groups=3)),","728","                 'LeavePGroupsOut(n_groups=3)')","729","","730","    for j, (cv, p_groups_out) in enumerate(((logo, 1), (lpgo_1, 1),","731","                                            (lpgo_2, 2))):","732","        for i, groups_i in enumerate(test_groups):","733","            n_groups = len(np.unique(groups_i))","734","            n_splits = (n_groups if p_groups_out == 1","735","                        else n_groups * (n_groups - 1) \/ 2)","736","            X = y = np.ones(len(groups_i))","737","","738","            # Test that the length is correct","739","            assert_equal(cv.get_n_splits(X, y, groups=groups_i), n_splits)","740","","741","            groups_arr = np.asarray(groups_i)","742","","743","            # Split using the original list \/ array \/ list of string groups_i","744","            for train, test in cv.split(X, y, groups=groups_i):","745","                # First test: no train group is in the test set and vice versa","746","                assert_array_equal(np.intersect1d(groups_arr[train],","747","                                                  groups_arr[test]).tolist(),","748","                                   [])","749","","750","                # Second test: train and test add up to all the data","751","                assert_equal(len(train) + len(test), len(groups_i))","752","","753","                # Third test:","754","                # The number of groups in test must be equal to p_groups_out","755","                assert_true(np.unique(groups_arr[test]).shape[0], p_groups_out)","756","","757","","775","    assert_equal(","776","        3, LeavePGroupsOut(n_groups=2).get_n_splits(X, y=X,","777","                                                    groups=groups))","779","    assert_equal(3, LeaveOneGroupOut().get_n_splits(X, y=X,","780","                                                    groups=groups))","785","    assert_raise_message(ValueError, \"Found array with 0 sample(s)\", next,","901","def train_test_split_list_input():","902","    # Check that when y is a list \/ list of string labels, it works.","903","    X = np.ones(7)","904","    y1 = ['1'] * 4 + ['0'] * 3","905","    y2 = np.hstack((np.ones(4), np.zeros(3)))","906","    y3 = y2.tolist()","907","","908","    for stratify in (True, False):","909","        X_train1, X_test1, y_train1, y_test1 = train_test_split(","910","            X, y1, stratify=y1 if stratify else None, random_state=0)","911","        X_train2, X_test2, y_train2, y_test2 = train_test_split(","912","            X, y2, stratify=y2 if stratify else None, random_state=0)","913","        X_train3, X_test3, y_train3, y_test3 = train_test_split(","914","            X, y3, stratify=y3 if stratify else None, random_state=0)","915","","916","        np.testing.assert_equal(X_train1, X_train2)","917","        np.testing.assert_equal(y_train2, y_train3)","918","        np.testing.assert_equal(X_test1, X_test3)","919","        np.testing.assert_equal(y_test3, y_test2)","920","","921","","946","def test_stratifiedshufflesplit_list_input():","947","    # Check that when y is a list \/ list of string labels, it works.","948","    sss = StratifiedShuffleSplit(test_size=2, random_state=42)","949","    X = np.ones(7)","950","    y1 = ['1'] * 4 + ['0'] * 3","951","    y2 = np.hstack((np.ones(4), np.zeros(3)))","952","    y3 = y2.tolist()","953","","954","    np.testing.assert_equal(list(sss.split(X, y1)),","955","                            list(sss.split(X, y2)))","956","    np.testing.assert_equal(list(sss.split(X, y3)),","957","                            list(sss.split(X, y2)))","958","","959","","987","                             [1, 1, 0, 1], [0, 0, 1, 0]])","1119","    # groups can also be a list","1120","    cv_iter = list(lkf.split(X, y, groups.tolist()))","1121","    for (train1, test1), (train2, test2) in zip(lkf.split(X, y, groups),","1122","                                                cv_iter):","1123","        assert_array_equal(train1, train2)","1124","        assert_array_equal(test1, test2)","1125",""],"delete":["487","          np.concatenate([[i] * (100 + i) for i in range(11)])","600","    groups = [np.array([1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 3]),","601","              np.array([0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3]),","602","              np.array([0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2]),","603","              np.array([1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4])]","604","","605","    for l in groups:","606","        X = y = np.ones(len(l))","615","        assert_equal(slo.get_n_splits(X, y, groups=l), n_splits)","617","        l_unique = np.unique(l)","619","        for train, test in slo.split(X, y, groups=l):","657","    assert_equal(3, LeavePGroupsOut(n_groups=2).get_n_splits(X, y, groups))","659","    assert_equal(3, LeaveOneGroupOut().get_n_splits(X, y, groups))","664","    msg = (\"The groups parameter contains fewer than 2 unique groups ([]). \"","665","           \"LeaveOneGroupOut expects at least 2.\")","666","    assert_raise_message(ValueError, msg, next,","833","                            [1, 1, 0, 1], [0, 0, 1, 0]])"]}]}},"94c2094f3ecb1d8dfbe0d8561513b799568bda9f":{"changes":{"sklearn\/preprocessing\/tests\/test_label.py":"MODIFY","sklearn\/preprocessing\/label.py":"MODIFY"},"diff":{"sklearn\/preprocessing\/tests\/test_label.py":[{"add":["228","                # verify CSR assumption that indices and indptr have same dtype","229","                assert_equal(got.indices.dtype, got.indptr.dtype)","240","                # verify CSR assumption that indices and indptr have same dtype","241","                assert_equal(got.indices.dtype, got.indptr.dtype)"],"delete":[]}],"sklearn\/preprocessing\/label.py":[{"add":["734","        # ensure yt.indices keeps its current dtype","735","        yt.indices = np.array(inverse[yt.indices], dtype=yt.indices.dtype,","736","                              copy=False)"],"delete":["734","        yt.indices = np.take(inverse, yt.indices)"]}]}},"53933866c00f86f74e9ccd533113bbd38062ca9b":{"changes":{"sklearn\/tests\/test_pipeline.py":"MODIFY","sklearn\/pipeline.py":"MODIFY"},"diff":{"sklearn\/tests\/test_pipeline.py":[{"add":["76","class TransfFitParams(Transf):","77","","78","    def fit(self, X, y, **fit_params):","79","        self.fit_params = fit_params","80","        return self","81","","82","","118","    def fit_predict(self, X, y, should_succeed=False):","119","        self.fit(X, y, should_succeed=should_succeed)","120","        return self.predict(X)","121","","321","def test_fit_predict_with_intermediate_fit_params():","322","    # tests that Pipeline passes fit_params to intermediate steps","323","    # when fit_predict is invoked","324","    pipe = Pipeline([('transf', TransfFitParams()), ('clf', FitParamT())])","325","    pipe.fit_predict(X=None,","326","                     y=None,","327","                     transf__should_get_this=True,","328","                     clf__should_succeed=True)","329","    assert_true(pipe.named_steps['transf'].fit_params['should_get_this'])","330","    assert_true(pipe.named_steps['clf'].successful)","331","    assert_false('should_succeed' in pipe.named_steps['transf'].fit_params)","332","","333",""],"delete":[]}],"sklearn\/pipeline.py":[{"add":["355","        Xt, fit_params = self._fit(X, y, **fit_params)"],"delete":["355","        Xt = X","356","        for name, transform in self.steps[:-1]:","357","            if transform is not None:","358","                Xt = transform.fit_transform(Xt)"]}]}},"46001a058222db0cf6fa34eec73a6f6fade96bbd":{"changes":{"doc\/whats_new.rst":"MODIFY","sklearn\/svm\/base.py":"MODIFY","sklearn\/svm\/tests\/test_svm.py":"MODIFY"},"diff":{"doc\/whats_new.rst":[{"add":["126","   - Fix bug for svm's decision values when ``decision_function_shape``","127","     is ``ovr`` in :class:`svm.SVC`.","128","     :class:`svm.SVC`'s decision_function was incorrect from versions","129","     0.17.0 through 0.18.0.","130","     :issue:`7724` by `Bing Tian Dai`_","131","","4888",".. _Bing Tian Dai: https:\/\/github.com\/btdai","4889",""],"delete":["151",""]}],"sklearn\/svm\/base.py":[{"add":["553","            return _ovr_decision_function(dec < 0, -dec, len(self.classes_))"],"delete":["553","            return _ovr_decision_function(dec < 0, dec, len(self.classes_))"]}],"sklearn\/svm\/tests\/test_svm.py":[{"add":["951","","952","","953","def test_ovr_decision_function():","954","    # One point from each quadrant represents one class","955","    X_train = np.array([[1, 1], [-1, 1], [-1, -1], [1, -1]])","956","    y_train = [0, 1, 2, 3]","957","","958","    # First point is closer to the decision boundaries than the second point","959","    base_points = np.array([[5, 5], [10, 10]])","960","","961","    # For all the quadrants (classes)","962","    X_test = np.vstack((","963","        base_points * [1, 1],    # Q1","964","        base_points * [-1, 1],   # Q2","965","        base_points * [-1, -1],  # Q3","966","        base_points * [1, -1]    # Q4","967","        ))","968","","969","    y_test = [0] * 2 + [1] * 2 + [2] * 2 + [3] * 2","970","","971","    clf = svm.SVC(kernel='linear', decision_function_shape='ovr')","972","    clf.fit(X_train, y_train)","973","","974","    y_pred = clf.predict(X_test)","975","","976","    # Test if the prediction is the same as y","977","    assert_array_equal(y_pred, y_test)","978","","979","    deci_val = clf.decision_function(X_test)","980","","981","    # Assert that the predicted class has the maximum value","982","    assert_array_equal(np.argmax(deci_val, axis=1), y_pred)","983","","984","    # Get decision value at test points for the predicted class","985","    pred_class_deci_val = deci_val[range(8), y_pred].reshape((4, 2))","986","","987","    # Assert pred_class_deci_val > 0 here","988","    assert_greater(np.min(pred_class_deci_val), 0.0)","989","","990","    # Test if the first point has lower decision value on every quadrant","991","    # compared to the second point","992","    assert_true(np.all(pred_class_deci_val[:, 0] < pred_class_deci_val[:, 1]))"],"delete":[]}]}},"89b2e45c918bc806c9f72ae184934c3b06c988c6":{"changes":{"sklearn\/cluster\/tests\/test_k_means.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/cluster\/k_means_.py":"MODIFY"},"diff":{"sklearn\/cluster\/tests\/test_k_means.py":[{"add":["814","def test_k_means_init_centers():","826","","827","","828","def test_sparse_k_means_init_centers():","829","    from sklearn.datasets import load_iris","830","","831","    iris = load_iris()","832","    X = iris.data","833","","834","    # Get a local optimum","835","    centers = KMeans(n_clusters=3).fit(X).cluster_centers_","836","","837","    # Fit starting from a local optimum shouldn't change the solution","838","    np.testing.assert_allclose(","839","        centers,","840","        KMeans(n_clusters=3,","841","               init=centers,","842","               n_init=1).fit(X).cluster_centers_","843","    )","844","","845","    # The same should be true when X is sparse","846","    X_sparse = sp.csr_matrix(X)","847","    np.testing.assert_allclose(","848","        centers,","849","        KMeans(n_clusters=3,","850","               init=centers,","851","               n_init=1).fit(X_sparse).cluster_centers_","852","    )","853","","854","","855","def test_sparse_validate_centers():","856","    from sklearn.datasets import load_iris","857","","858","    iris = load_iris()","859","    X = iris.data","860","","861","    # Get a local optimum","862","    centers = KMeans(n_clusters=4).fit(X).cluster_centers_","863","","864","    # Test that a ValueError is raised for validate_center_shape","865","    classifier = KMeans(n_clusters=3, init=centers, n_init=1)","866","","867","    msg = \"The shape of the initial centers \\(\\(4L?, 4L?\\)\\) \" \\","868","          \"does not match the number of clusters 3\"","869","    assert_raises_regex(ValueError, msg, classifier.fit, X)"],"delete":["814","def test_KMeans_init_centers():"]}],"doc\/whats_new.rst":[{"add":["99","   - Fix a bug regarding fitting :class:`sklearn.cluster.KMeans` with a","100","     sparse array X and initial centroids, where X's means were unnecessarily","101","     being subtracted from the centroids. :issue:`7872` by `Josh Karnofsky <https:\/\/github.com\/jkarno>`_.","102",""],"delete":[]}],"sklearn\/cluster\/k_means_.py":[{"add":["300","    # Validate init array","312","    # subtract of mean of x for more accurate distance computations","313","    if not sp.issparse(X):","314","        X_mean = X.mean(axis=0)","315","        # The copy was already done above","316","        X -= X_mean","317","","318","        if hasattr(init, '__array__'):","319","            init -= X_mean","320",""],"delete":["300","    # subtract of mean of x for more accurate distance computations","301","    if not sp.issparse(X) or hasattr(init, '__array__'):","302","        X_mean = X.mean(axis=0)","303","    if not sp.issparse(X):","304","        # The copy was already done above","305","        X -= X_mean","306","","311","        init -= X_mean"]}]}},"6a01e89672ce68708b136b66ad817d926767ea88":{"changes":{"sklearn\/decomposition\/online_lda.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/decomposition\/tests\/test_online_lda.py":"MODIFY"},"diff":{"sklearn\/decomposition\/online_lda.py":[{"add":["507","                          DeprecationWarning)","533","                    bound = self._perplexity_precomp_distr(X, doc_topics_distr,","534","                                                           sub_sampling=False)","543","","544","        # calculate final perplexity value on train set","545","        doc_topics_distr, _ = self._e_step(X, cal_sstats=False,","546","                                           random_init=False,","547","                                           parallel=parallel)","548","        self.bound_ = self._perplexity_precomp_distr(X, doc_topics_distr,","549","                                                     sub_sampling=False)","550","","553","    def _unnormalized_transform(self, X):","554","        \"\"\"Transform data X according to fitted model.","581","","582","        return doc_topic_distr","583","","584","    def transform(self, X):","585","        \"\"\"Transform data X according to the fitted model.","586","","587","           .. versionchanged:: 0.18","588","              *doc_topic_distr* is now normalized","589","","590","        Parameters","591","        ----------","592","        X : array-like or sparse matrix, shape=(n_samples, n_features)","593","            Document word matrix.","594","","595","        Returns","596","        -------","597","        doc_topic_distr : shape=(n_samples, n_topics)","598","            Document topic distribution for X.","599","        \"\"\"","600","        doc_topic_distr = self._unnormalized_transform(X)","695","        doc_topic_distr = self._unnormalized_transform(X)","699","    def _perplexity_precomp_distr(self, X, doc_topic_distr=None,","700","                                  sub_sampling=False):","701","        \"\"\"Calculate approximate perplexity for data X with ability to accept","702","        precomputed doc_topic_distr","728","            doc_topic_distr = self._unnormalized_transform(X)","748","","749","    def perplexity(self, X, doc_topic_distr='deprecated', sub_sampling=False):","750","        \"\"\"Calculate approximate perplexity for data X.","751","","752","        Perplexity is defined as exp(-1. * log-likelihood per word)","753","","754","        .. versionchanged:: 0.19","755","           *doc_topic_distr* argument has been depricated because user no","756","           longer has access to unnormalized distribution","757","","758","        Parameters","759","        ----------","760","        X : array-like or sparse matrix, [n_samples, n_features]","761","            Document word matrix.","762","","763","        doc_topic_distr : None or array, shape=(n_samples, n_topics)","764","            Document topic distribution.","765","            If it is None, it will be generated by applying transform on X.","766","","767","            .. deprecated:: 0.19","768","","769","        Returns","770","        -------","771","        score : float","772","            Perplexity score.","773","        \"\"\"","774","        if doc_topic_distr != 'deprecated':","775","            warnings.warn(\"Argument 'doc_topic_distr' is deprecated and will \"","776","                          \"be ignored as of 0.19. Support for this argument \"","777","                          \"will be removed in 0.21.\", DeprecationWarning)","778","","779","        return self._perplexity_precomp_distr(X, sub_sampling=sub_sampling)"],"delete":["507","                          DeprecationWarning)          ","533","                    bound = self.perplexity(X, doc_topics_distr,","534","                                            sub_sampling=False)","545","    def transform(self, X):","546","        \"\"\"Transform data X according to the fitted model.","558","","574","        # normalize doc_topic_distr","667","","670","        doc_topic_distr = self.transform(X)","674","    def perplexity(self, X, doc_topic_distr=None, sub_sampling=False):","675","        \"\"\"Calculate approximate perplexity for data X.","701","            doc_topic_distr = self.transform(X)"]}],"doc\/whats_new.rst":[{"add":["124","   - Fix a bug in :class:`sklearn.decomposition.LatentDirichletAllocation`","125","     where the ``perplexity`` method was returning incorrect results because","126","     the ``transform`` method returns normalized document topic distributions","127","     as of version 0.18. :issue:`7954` by :user:`Gary Foreman <garyForeman>`.","128","     ","142","     ","143","   - Deprecate the ``doc_topic_distr`` argument of the ``perplexity`` method","144","     in :class:`sklearn.decomposition.LatentDirichletAllocation` because the","145","     user no longer has access to the unnormalized document topic distribution","146","     needed for the perplexity calculation. :issue:`7954` by","147","     :user:`Gary Foreman <garyForeman>`."],"delete":[]}],"sklearn\/decomposition\/tests\/test_online_lda.py":[{"add":["16","from sklearn.utils.testing import assert_warns","241","    assert_raises_regexp(ValueError, r'Number of samples',","242","                         lda._perplexity_precomp_distr, X, invalid_n_samples)","245","    assert_raises_regexp(ValueError, r'Number of topics',","246","                         lda._perplexity_precomp_distr, X, invalid_n_topics)","260","        lda_1.fit(X)","261","        perp_1 = lda_1.perplexity(X, sub_sampling=False)","263","        lda_2.fit(X)","264","        perp_2 = lda_2.perplexity(X, sub_sampling=False)","267","        perp_1_subsampling = lda_1.perplexity(X, sub_sampling=True)","268","        perp_2_subsampling = lda_2.perplexity(X, sub_sampling=True)","298","    lda.fit(X)","300","    perp_2 = lda.perplexity(X.toarray())","309","    lda.fit(X)","310","    perplexity_1 = lda.perplexity(X, sub_sampling=False)","317","def test_lda_fit_perplexity():","318","    # Test that the perplexity computed during fit is consistent with what is","319","    # returned by the perplexity method","320","    n_topics, X = _build_sparse_mtx()","321","    lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=1,","322","                                    learning_method='batch', random_state=0,","323","                                    evaluate_every=1)","324","    lda.fit(X)","325","","326","    # Perplexity computed at end of fit method","327","    perplexity1 = lda.bound_","328","","329","    # Result of perplexity method on the train set","330","    perplexity2 = lda.perplexity(X)","331","","332","    assert_almost_equal(perplexity1, perplexity2)","333","","334","","335","def test_doc_topic_distr_deprecation():","336","    # Test that the appropriate warning message is displayed when a user","337","    # attempts to pass the doc_topic_distr argument to the perplexity method","338","    n_topics, X = _build_sparse_mtx()","339","    lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=1,","340","                                    learning_method='batch',","341","                                    total_samples=100, random_state=0)","342","    distr1 = lda.fit_transform(X)","343","    distr2 = None","344","    assert_warns(DeprecationWarning, lda.perplexity, X, distr1)","345","    assert_warns(DeprecationWarning, lda.perplexity, X, distr2)","346","","347",""],"delete":["240","    assert_raises_regexp(ValueError, r'Number of samples', lda.perplexity, X,","241","                         invalid_n_samples)","244","    assert_raises_regexp(ValueError, r'Number of topics', lda.perplexity, X,","245","                         invalid_n_topics)","259","        distr_1 = lda_1.fit_transform(X)","260","        perp_1 = lda_1.perplexity(X, distr_1, sub_sampling=False)","262","        distr_2 = lda_2.fit_transform(X)","263","        perp_2 = lda_2.perplexity(X, distr_2, sub_sampling=False)","266","        perp_1_subsampling = lda_1.perplexity(X, distr_1, sub_sampling=True)","267","        perp_2_subsampling = lda_2.perplexity(X, distr_2, sub_sampling=True)","297","    distr = lda.fit_transform(X)","299","    perp_2 = lda.perplexity(X, distr)","300","    perp_3 = lda.perplexity(X.toarray(), distr)","302","    assert_almost_equal(perp_1, perp_3)","310","    distr = lda.fit_transform(X)","311","    perplexity_1 = lda.perplexity(X, distr, sub_sampling=False)"]}]}},"62c86f0388fdeccc20d535178957121611ba8213":{"changes":{"sklearn\/feature_extraction\/text.py":"MODIFY","sklearn\/feature_extraction\/tests\/test_text.py":"MODIFY"},"diff":{"sklearn\/feature_extraction\/text.py":[{"add":["454","        if isinstance(X, six.string_types):","455","            raise ValueError(","456","                \"Iterable over raw text documents expected, \"","457","                \"string object received.\")","458","","480","        if isinstance(X, six.string_types):","481","            raise ValueError(","482","                \"Iterable over raw text documents expected, \"","483","                \"string object received.\")","484","","827","        if isinstance(raw_documents, six.string_types):","828","            raise ValueError(","829","                \"Iterable over raw text documents expected, \"","830","                \"string object received.\")","831","","881","        if isinstance(raw_documents, six.string_types):","882","            raise ValueError(","883","                \"Iterable over raw text documents expected, \"","884","                \"string object received.\")","885",""],"delete":[]}],"sklearn\/feature_extraction\/tests\/test_text.py":[{"add":["870","                               random_state=rng))","969","","970","","971","def test_vectorizer_string_object_as_input():","972","    message = (\"Iterable over raw text documents expected, \"","973","               \"string object received.\")","974","    for vec in [CountVectorizer(), TfidfVectorizer(), HashingVectorizer()]:","975","        assert_raise_message(","976","            ValueError, message, vec.fit_transform, \"hello world!\")","977","        assert_raise_message(","978","            ValueError, message, vec.fit, \"hello world!\")","979","        assert_raise_message(","980","            ValueError, message, vec.transform, \"hello world!\")"],"delete":["870","                        random_state=rng))"]}]}},"1f278e1c231e6b9b3cf813377819e25e87b6c8b6":{"changes":{"sklearn\/utils\/fixes.py":"MODIFY"},"diff":{"sklearn\/utils\/fixes.py":[{"add":["362","if np_version < (1, 12):"],"delete":["362","if np_version < (1, 12, 0):"]}]}},"5230382ae65ce4c5c5499b652d07077242e47c1b":{"changes":{"sklearn\/linear_model\/tests\/test_least_angle.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/linear_model\/least_angle.py":"MODIFY"},"diff":{"sklearn\/linear_model\/tests\/test_least_angle.py":[{"add":["543","","544","","545","def test_lasso_lars_vs_R_implementation():","546","    # Test that sklearn LassoLars implementation agrees with the LassoLars","547","    # implementation available in R (lars library) under the following","548","    # scenarios:","549","    # 1) fit_intercept=False and normalize=False","550","    # 2) fit_intercept=True and normalize=True","551","","552","    # Let's generate the data used in the bug report 7778","553","    y = np.array([-6.45006793, -3.51251449, -8.52445396, 6.12277822,","554","                  -19.42109366])","555","    x = np.array([[0.47299829, 0, 0, 0, 0],","556","                  [0.08239882, 0.85784863, 0, 0, 0],","557","                  [0.30114139, -0.07501577, 0.80895216, 0, 0],","558","                  [-0.01460346, -0.1015233, 0.0407278, 0.80338378, 0],","559","                  [-0.69363927, 0.06754067, 0.18064514, -0.0803561,","560","                   0.40427291]])","561","","562","    X = x.T","563","","564","    ###########################################################################","565","    # Scenario 1: Let's compare R vs sklearn when fit_intercept=False and","566","    # normalize=False","567","    ###########################################################################","568","    #","569","    # The R result was obtained using the following code:","570","    #","571","    # library(lars)","572","    # model_lasso_lars = lars(X, t(y), type=\"lasso\", intercept=FALSE,","573","    #                         trace=TRUE, normalize=FALSE)","574","    # r = t(model_lasso_lars$beta)","575","    #","576","","577","    r = np.array([[0, 0, 0, 0, 0, -79.810362809499026, -83.528788732782829,","578","                   -83.777653739190711, -83.784156932888934,","579","                   -84.033390591756657],","580","                  [0, 0, 0, 0, -0.476624256777266, 0, 0, 0, 0,","581","                   0.025219751009936],","582","                  [0, -3.577397088285891, -4.702795355871871,","583","                   -7.016748621359461, -7.614898471899412, -0.336938391359179,","584","                   0, 0, 0.001213370600853,  0.048162321585148],","585","                  [0, 0, 0, 2.231558436628169, 2.723267514525966,","586","                   2.811549786389614, 2.813766976061531, 2.817462468949557,","587","                   2.817368178703816, 2.816221090636795],","588","                  [0, 0, -1.218422599914637, -3.457726183014808,","589","                   -4.021304522060710, -45.827461592423745,","590","                   -47.776608869312305,","591","                   -47.911561610746404, -47.914845922736234,","592","                   -48.039562334265717]])","593","","594","    model_lasso_lars = linear_model.LassoLars(alpha=0, fit_intercept=False,","595","                                              normalize=False)","596","    model_lasso_lars.fit(X, y)","597","    skl_betas = model_lasso_lars.coef_path_","598","","599","    assert_array_almost_equal(r, skl_betas, decimal=12)","600","    ###########################################################################","601","","602","    ###########################################################################","603","    # Scenario 2: Let's compare R vs sklearn when fit_intercept=True and","604","    # normalize=True","605","    #","606","    # Note: When normalize is equal to True, R returns the coefficients in","607","    # their original units, that is, they are rescaled back, whereas sklearn","608","    # does not do that, therefore, we need to do this step before comparing","609","    # their results.","610","    ###########################################################################","611","    #","612","    # The R result was obtained using the following code:","613","    #","614","    # library(lars)","615","    # model_lasso_lars2 = lars(X, t(y), type=\"lasso\", intercept=TRUE,","616","    #                           trace=TRUE, normalize=TRUE)","617","    # r2 = t(model_lasso_lars2$beta)","618","","619","    r2 = np.array([[0, 0, 0, 0, 0],","620","                   [0, 0, 0, 8.371887668009453, 19.463768371044026],","621","                   [0, 0, 0, 0, 9.901611055290553],","622","                   [0, 7.495923132833733, 9.245133544334507,","623","                    17.389369207545062, 26.971656815643499],","624","                   [0, 0, -1.569380717440311, -5.924804108067312,","625","                    -7.996385265061972]])","626","","627","    model_lasso_lars2 = linear_model.LassoLars(alpha=0, fit_intercept=True,","628","                                               normalize=True)","629","    model_lasso_lars2.fit(X, y)","630","    skl_betas2 = model_lasso_lars2.coef_path_","631","","632","    # Let's rescale back the coefficients returned by sklearn before comparing","633","    # against the R result (read the note above)","634","    temp = X - np.mean(X, axis=0)","635","    normx = np.sqrt(np.sum(temp ** 2, axis=0))","636","    skl_betas2 \/= normx[:, np.newaxis]","637","","638","    assert_array_almost_equal(r2, skl_betas2, decimal=12)","639","    ###########################################################################"],"delete":[]}],"doc\/whats_new.rst":[{"add":["102","   - Fixed a bug where :class:`sklearn.linear_model.LassoLars` does not give","103","     the same result as the LassoLars implementation available","104","     in R (lars library). :issue:`7849` by `Jair Montoya Martinez`_"],"delete":[]}],"sklearn\/linear_model\/least_angle.py":[{"add":["406","                coefs[-add_features:] = 0","408","                alphas[-add_features:] = 0"],"delete":[]}]}},"73bb593142aa344910caac17586748e2e6f4deaa":{"changes":{"doc\/whats_new.rst":"MODIFY"},"diff":{"doc\/whats_new.rst":[{"add":["43","   - Numerical issue with :class:`linear_model.RidgeCV` on centered data when","44","     `n_features > n_samples`. (`#6178","45","     <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/6178>`_) by `Bertrand","46","     Thirion`_"],"delete":["43","  - Numerical issue with :class:`linear_model.RidgeCV` on centered data when","44","    `n_features > n_samples`. (`#6178","45","    <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/6178>`_) by `Bertrand","46","    Thirion`_"]}]}},"ab1c4d46cc118fb507140befb67b70ace018e3f0":{"changes":{"sklearn\/neural_network\/multilayer_perceptron.py":"MODIFY","sklearn\/neural_network\/tests\/test_mlp.py":"MODIFY"},"diff":{"sklearn\/neural_network\/multilayer_perceptron.py":[{"add":["910","        elif self.warm_start:","911","            classes = unique_labels(y)","912","            if set(classes) != set(self.classes_):","913","                raise ValueError(\"warm_start can only be used where `y` has \"","914","                                 \"the same classes as in the previous \"","915","                                 \"call to fit. Previously got %s, `y` has %s\" %","916","                                 (self.classes_, classes))","948","    def fit(self, X, y):","949","        \"\"\"Fit the model to data matrix X and target(s) y.","950","","951","        Parameters","952","        ----------","953","        X : array-like or sparse matrix, shape (n_samples, n_features)","954","            The input data.","955","","956","        y : array-like, shape (n_samples,) or (n_samples, n_outputs)","957","            The target values (class labels in classification, real numbers in","958","            regression).","959","","960","        Returns","961","        -------","962","        self : returns a trained MLP model.","963","        \"\"\"","964","        return self._fit(X, y, incremental=(self.warm_start and","965","                                            hasattr(self, \"classes_\")))","966",""],"delete":[]}],"sklearn\/neural_network\/tests\/test_mlp.py":[{"add":["14","from sklearn.datasets import load_digits, load_boston, load_iris","26","from sklearn.utils.testing import assert_raise_message","52","iris = load_iris()","53","","54","X_iris = iris.data","55","y_iris = iris.target","56","","564","","565","","566","@ignore_warnings(RuntimeError)","567","def test_warm_start():","568","    X = X_iris","569","    y = y_iris","570","","571","    y_2classes = np.array([0] * 75 + [1] * 75)","572","    y_3classes = np.array([0] * 40 + [1] * 40 + [2] * 70)","573","    y_3classes_alt = np.array([0] * 50 + [1] * 50 + [3] * 50)","574","    y_4classes = np.array([0] * 37 + [1] * 37 + [2] * 38 + [3] * 38)","575","    y_5classes = np.array([0] * 30 + [1] * 30 + [2] * 30 + [3] * 30 + [4] * 30)","576","","577","    # No error raised","578","    clf = MLPClassifier(hidden_layer_sizes=2, solver='lbfgs',","579","                        warm_start=True).fit(X, y)","580","    clf.fit(X, y)","581","    clf.fit(X, y_3classes)","582","","583","    for y_i in (y_2classes, y_3classes_alt, y_4classes, y_5classes):","584","        clf = MLPClassifier(hidden_layer_sizes=2, solver='lbfgs',","585","                            warm_start=True).fit(X, y)","586","        message = ('warm_start can only be used where `y` has the same '","587","                   'classes as in the previous call to fit.'","588","                   ' Previously got [0 1 2], `y` has %s' % np.unique(y_i))","589","        assert_raise_message(ValueError, message, clf.fit, X, y_i)"],"delete":["14","from sklearn.datasets import load_digits, load_boston"]}]}},"b3a639ffc2d518b8862c61e4170403a400368571":{"changes":{"sklearn\/tests\/test_pipeline.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/pipeline.py":"MODIFY","examples\/plot_compare_reduction.py":"MODIFY","doc\/modules\/pipeline.rst":"MODIFY"},"diff":{"sklearn\/tests\/test_pipeline.py":[{"add":["3","","4","from tempfile import mkdtemp","5","import shutil","6","import time","7","","33","from sklearn.externals.joblib import Memory","133","class DummyTransf(Transf):","134","    \"\"\"Transformer which store the column means\"\"\"","135","","136","    def fit(self, X, y):","137","        self.means_ = np.mean(X, axis=0)","138","        # store timestamp to figure out whether the result of 'fit' has been","139","        # cached or not","140","        self.timestamp_ = time.time()","141","        return self","142","","143","","539","                       'memory': None,","819","","820","","821","def test_pipeline_wrong_memory():","822","    # Test that an error is raised when memory is not a string or a Memory","823","    # instance","824","    iris = load_iris()","825","    X = iris.data","826","    y = iris.target","827","    # Define memory as an integer","828","    memory = 1","829","    cached_pipe = Pipeline([('transf', DummyTransf()), ('svc', SVC())],","830","                           memory=memory)","831","    assert_raises_regex(ValueError, \"'memory' should either be a string or a\"","832","                        \" joblib.Memory instance, got 'memory=1' instead.\",","833","                        cached_pipe.fit, X, y)","834","","835","","836","def test_pipeline_memory():","837","    iris = load_iris()","838","    X = iris.data","839","    y = iris.target","840","    cachedir = mkdtemp()","841","    try:","842","        memory = Memory(cachedir=cachedir, verbose=10)","843","        # Test with Transformer + SVC","844","        clf = SVC(probability=True, random_state=0)","845","        transf = DummyTransf()","846","        pipe = Pipeline([('transf', clone(transf)), ('svc', clf)])","847","        cached_pipe = Pipeline([('transf', transf), ('svc', clf)],","848","                               memory=memory)","849","","850","        # Memoize the transformer at the first fit","851","        cached_pipe.fit(X, y)","852","        pipe.fit(X, y)","853","        # Get the time stamp of the tranformer in the cached pipeline","854","        ts = cached_pipe.named_steps['transf'].timestamp_","855","        # Check that cached_pipe and pipe yield identical results","856","        assert_array_equal(pipe.predict(X), cached_pipe.predict(X))","857","        assert_array_equal(pipe.predict_proba(X), cached_pipe.predict_proba(X))","858","        assert_array_equal(pipe.predict_log_proba(X),","859","                           cached_pipe.predict_log_proba(X))","860","        assert_array_equal(pipe.score(X, y), cached_pipe.score(X, y))","861","        assert_array_equal(pipe.named_steps['transf'].means_,","862","                           cached_pipe.named_steps['transf'].means_)","863","        assert_false(hasattr(transf, 'means_'))","864","        # Check that we are reading the cache while fitting","865","        # a second time","866","        cached_pipe.fit(X, y)","867","        # Check that cached_pipe and pipe yield identical results","868","        assert_array_equal(pipe.predict(X), cached_pipe.predict(X))","869","        assert_array_equal(pipe.predict_proba(X), cached_pipe.predict_proba(X))","870","        assert_array_equal(pipe.predict_log_proba(X),","871","                           cached_pipe.predict_log_proba(X))","872","        assert_array_equal(pipe.score(X, y), cached_pipe.score(X, y))","873","        assert_array_equal(pipe.named_steps['transf'].means_,","874","                           cached_pipe.named_steps['transf'].means_)","875","        assert_equal(ts, cached_pipe.named_steps['transf'].timestamp_)","876","        # Create a new pipeline with cloned estimators","877","        # Check that even changing the name step does not affect the cache hit","878","        clf_2 = SVC(probability=True, random_state=0)","879","        transf_2 = DummyTransf()","880","        cached_pipe_2 = Pipeline([('transf_2', transf_2), ('svc', clf_2)],","881","                                 memory=memory)","882","        cached_pipe_2.fit(X, y)","883","","884","        # Check that cached_pipe and pipe yield identical results","885","        assert_array_equal(pipe.predict(X), cached_pipe_2.predict(X))","886","        assert_array_equal(pipe.predict_proba(X),","887","                           cached_pipe_2.predict_proba(X))","888","        assert_array_equal(pipe.predict_log_proba(X),","889","                           cached_pipe_2.predict_log_proba(X))","890","        assert_array_equal(pipe.score(X, y), cached_pipe_2.score(X, y))","891","        assert_array_equal(pipe.named_steps['transf'].means_,","892","                           cached_pipe_2.named_steps['transf_2'].means_)","893","        assert_equal(ts, cached_pipe_2.named_steps['transf_2'].timestamp_)","894","    finally:","895","        shutil.rmtree(cachedir)"],"delete":[]}],"doc\/whats_new.rst":[{"add":["58","   - :class:`pipeline.Pipeline` allows to cache transformers","59","     within a pipeline by using the ``memory`` constructor parameter.","60","     By :issue:`7990` by :user:`Guillaume Lemaitre <glemaitre>`."],"delete":[]}],"sklearn\/pipeline.py":[{"add":["17","from .base import clone, BaseEstimator, TransformerMixin","18","from .externals.joblib import Parallel, delayed, Memory","91","    The transformers in the pipeline can be cached using ```memory`` argument.","110","    memory : Instance of joblib.Memory or string, optional (default=None)","111","        Used to caching the fitted transformers of the transformer of the","112","        pipeline. By default, no cache is performed.","113","        If a string is given, it is the path to the caching directory.","114","        Enabling caching triggers a clone of the transformers before fitting.","115","        Therefore, the transformer instance given to the pipeline cannot be","116","        inspected directly. Use the attribute ``named_steps`` or ``steps``","117","        to inspect estimators within the pipeline.","118","        Caching the transformers is advantageous when fitting is time","119","        consuming.","120","","121","","146","    ...                      # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE","147","    Pipeline(memory=None,","148","             steps=[('anova', SelectKBest(...)),","149","                    ('svc', SVC(...))])","159","","164","    def __init__(self, steps, memory=None):","168","        self.memory = memory","239","        # Setup the memory","240","        memory = self.memory","241","        if memory is None:","242","            memory = Memory(cachedir=None, verbose=0)","243","        elif isinstance(memory, six.string_types):","244","            memory = Memory(cachedir=memory, verbose=0)","245","        elif not isinstance(memory, Memory):","246","            raise ValueError(\"'memory' should either be a string or\"","247","                             \" a joblib.Memory instance, got\"","248","                             \" 'memory={!r}' instead.\".format(memory))","249","","250","        fit_transform_one_cached = memory.cache(_fit_transform_one)","251","","258","        for step_idx, (name, transformer) in enumerate(self.steps[:-1]):","259","            if transformer is None:","262","                if memory.cachedir is None:","263","                    # we do not clone when caching is disabled to preserve","264","                    # backward compatibility","265","                    cloned_transformer = transformer","266","                else:","267","                    cloned_transformer = clone(transformer)","268","                # Fit or load from cache the current transfomer","269","                Xt, fitted_transformer = fit_transform_one_cached(","270","                    cloned_transformer, None, Xt, y,","271","                    **fit_params_steps[name])","272","                # Replace the transformer of the step with the fitted","273","                # transformer. This is necessary when loading the transformer","274","                # from the cache.","275","                self.steps[step_idx] = (name, fitted_transformer)","592","    Pipeline(memory=None,","593","             steps=[('standardscaler',","608","def _transform_one(transformer, weight, X):","616","def _fit_transform_one(transformer, weight, X, y,","774","            delayed(_fit_transform_one)(trans, weight, X, y,","804","            delayed(_transform_one)(trans, weight, X)"],"delete":["17","from .base import BaseEstimator, TransformerMixin","18","from .externals.joblib import Parallel, delayed","133","    ...                                              # doctest: +ELLIPSIS","134","    Pipeline(steps=[...])","148","    def __init__(self, steps):","228","        for name, transform in self.steps[:-1]:","229","            if transform is None:","231","            elif hasattr(transform, \"fit_transform\"):","232","                Xt = transform.fit_transform(Xt, y, **fit_params_steps[name])","234","                Xt = transform.fit(Xt, y, **fit_params_steps[name]) \\","235","                              .transform(Xt)","552","    Pipeline(steps=[('standardscaler',","567","def _transform_one(transformer, name, weight, X):","575","def _fit_transform_one(transformer, name, weight, X, y,","733","            delayed(_fit_transform_one)(trans, name, weight, X, y,","763","            delayed(_transform_one)(trans, name, weight, X)"]}],"examples\/plot_compare_reduction.py":[{"add":["0","#!\/usr\/bin\/env python","9","classifier. It demonstrates the use of ``GridSearchCV`` and","10","``Pipeline`` to optimize over different classes of estimators in a","11","single CV run -- unsupervised ``PCA`` and ``NMF`` dimensionality","14","","15","Additionally, ``Pipeline`` can be instantiated with the ``memory``","16","argument to memoize the transformers within the pipeline, avoiding to fit","17","again the same transformers over and over.","18","","19","Note that the use of ``memory`` to enable caching becomes interesting when the","20","fitting of a transformer is costly.","22","","23","###############################################################################","24","# Illustration of ``Pipeline`` and ``GridSearchCV``","25","###############################################################################","26","# This section illustrates the use of a ``Pipeline`` with","27","# ``GridSearchCV``","28","","29","# Authors: Robert McGibbon, Joel Nothman, Guillaume Lemaitre","65","grid = GridSearchCV(pipe, cv=3, n_jobs=1, param_grid=param_grid)","88","","89","###############################################################################","90","# Caching transformers within a ``Pipeline``","91","###############################################################################","92","# It is sometimes worthwhile storing the state of a specific transformer","93","# since it could be used again. Using a pipeline in ``GridSearchCV`` triggers","94","# such situations. Therefore, we use the argument ``memory`` to enable caching.","95","#","96","# .. warning::","97","#     Note that this example is, however, only an illustration since for this","98","#     specific case fitting PCA is not necessarily slower than loading the","99","#     cache. Hence, use the ``memory`` constructor parameter when the fitting","100","#     of a transformer is costly.","101","","102","from tempfile import mkdtemp","103","from shutil import rmtree","104","from sklearn.externals.joblib import Memory","105","","106","# Create a temporary folder to store the transformers of the pipeline","107","cachedir = mkdtemp()","108","memory = Memory(cachedir=cachedir, verbose=10)","109","cached_pipe = Pipeline([('reduce_dim', PCA()),","110","                        ('classify', LinearSVC())],","111","                       memory=memory)","112","","113","# This time, a cached pipeline will be used within the grid search","114","grid = GridSearchCV(cached_pipe, cv=3, n_jobs=1, param_grid=param_grid)","115","digits = load_digits()","116","grid.fit(digits.data, digits.target)","117","","118","# Delete the temporary cache before exiting","119","rmtree(cachedir)","120","","121","###############################################################################","122","# The ``PCA`` fitting is only computed at the evaluation of the first","123","# configuration of the ``C`` parameter of the ``LinearSVC`` classifier. The","124","# other configurations of ``C`` will trigger the loading of the cached ``PCA``","125","# estimator data, leading to save processing time. Therefore, the use of","126","# caching the pipeline using ``memory`` is highly beneficial when fitting","127","# a transformer is costly.","128",""],"delete":["0","#!\/usr\/bin\/python","9","classifier. It demonstrates the use of GridSearchCV and","10","Pipeline to optimize over different classes of estimators in a","11","single CV run -- unsupervised PCA and NMF dimensionality","15","# Authors: Robert McGibbon, Joel Nothman","51","grid = GridSearchCV(pipe, cv=3, n_jobs=2, param_grid=param_grid)"]}],"doc\/modules\/pipeline.rst":[{"add":["41","    >>> pipe # doctest: +NORMALIZE_WHITESPACE, +ELLIPSIS","42","    Pipeline(memory=None,","43","             steps=[('reduce_dim', PCA(copy=True,...)),","44","                    ('clf', SVC(C=1.0,...))])","55","    Pipeline(memory=None,","56","             steps=[('binarizer', Binarizer(copy=True, threshold=0.0)),","76","    >>> pipe.set_params(clf__C=10) # doctest: +NORMALIZE_WHITESPACE, +ELLIPSIS","77","    Pipeline(memory=None,","78","             steps=[('reduce_dim', PCA(copy=True, iterated_power='auto',...)),","79","                    ('clf', SVC(C=10, cache_size=200, class_weight=None,...))])","84","    >>> param_grid = dict(reduce_dim__n_components=[2, 5, 10],","85","    ...                   clf__C=[0.1, 10, 100])","86","    >>> grid_search = GridSearchCV(pipe, param_grid=param_grid)","92","    >>> param_grid = dict(reduce_dim=[None, PCA(5), PCA(10)],","93","    ...                   clf=[SVC(), LogisticRegression()],","94","    ...                   clf__C=[0.1, 10, 100])","95","    >>> grid_search = GridSearchCV(pipe, param_grid=param_grid)","104"," * :ref:`sphx_glr_auto_examples_plot_compare_reduction.py`","121","Caching transformers: avoid repeated computation","122","-------------------------------------------------","123","","124",".. currentmodule:: sklearn.pipeline","125","","126","Fitting transformers may be computationally expensive. With its","127","``memory`` parameter set, :class:`Pipeline` will cache each transformer","128","after calling ``fit``.","129","This feature is used to avoid computing the fit transformers within a pipeline","130","if the parameters and input data are identical. A typical example is the case of","131","a grid search in which the transformers can be fitted only once and reused for","132","each configuration.","133","","134","The parameter ``memory`` is needed in order to cache the transformers.","135","``memory`` can be either a string containing the directory where to cache the","136","transformers or a `joblib.Memory <https:\/\/pythonhosted.org\/joblib\/memory.html>`_","137","object::","138","","139","    >>> from tempfile import mkdtemp","140","    >>> from shutil import rmtree","141","    >>> from sklearn.decomposition import PCA","142","    >>> from sklearn.svm import SVC","143","    >>> from sklearn.pipeline import Pipeline","144","    >>> estimators = [('reduce_dim', PCA()), ('clf', SVC())]","145","    >>> cachedir = mkdtemp()","146","    >>> pipe = Pipeline(estimators, memory=cachedir)","147","    >>> pipe # doctest: +NORMALIZE_WHITESPACE, +ELLIPSIS","148","    Pipeline(...,","149","             steps=[('reduce_dim', PCA(copy=True,...)),","150","                    ('clf', SVC(C=1.0,...))])","151","    >>> # Clear the cache directory when you don't need it anymore","152","    >>> rmtree(cachedir)","153","","154",".. warning:: **Side effect of caching transfomers**","155","","156","   Using a :class:`Pipeline` without cache enabled, it is possible to","157","   inspect the original instance such as::","158","","159","     >>> from sklearn.datasets import load_digits","160","     >>> digits = load_digits()","161","     >>> pca1 = PCA()","162","     >>> svm1 = SVC()","163","     >>> pipe = Pipeline([('reduce_dim', pca1), ('clf', svm1)])","164","     >>> pipe.fit(digits.data, digits.target)","165","     ... # doctest: +NORMALIZE_WHITESPACE, +ELLIPSIS","166","     Pipeline(memory=None,","167","              steps=[('reduce_dim', PCA(...)), ('clf', SVC(...))])","168","     >>> # The pca instance can be inspected directly","169","     >>> print(pca1.components_) # doctest: +NORMALIZE_WHITESPACE, +ELLIPSIS","170","         [[ -1.77484909e-19  ... 4.07058917e-18]]","171","","172","   Enabling caching triggers a clone of the transformers before fitting.","173","   Therefore, the transformer instance given to the pipeline cannot be","174","   inspected directly.","175","   In following example, accessing the :class:`PCA` instance ``pca2``","176","   will raise an ``AttributeError`` since ``pca2`` will be an unfitted","177","   transformer.","178","   Instead, use the attribute ``named_steps`` to inspect estimators within","179","   the pipeline::","180","","181","     >>> cachedir = mkdtemp()","182","     >>> pca2 = PCA()","183","     >>> svm2 = SVC()","184","     >>> cached_pipe = Pipeline([('reduce_dim', pca2), ('clf', svm2)],","185","     ...                        memory=cachedir)","186","     >>> cached_pipe.fit(digits.data, digits.target)","187","     ... # doctest: +NORMALIZE_WHITESPACE, +ELLIPSIS","188","      Pipeline(memory=...,","189","               steps=[('reduce_dim', PCA(...)), ('clf', SVC(...))])","190","     >>> print(cached_pipe.named_steps['reduce_dim'].components_)","191","     ... # doctest: +NORMALIZE_WHITESPACE, +ELLIPSIS","192","         [[ -1.77484909e-19  ... 4.07058917e-18]]","193","     >>> # Remove the cache directory","194","     >>> rmtree(cachedir)","195","","196",".. topic:: Examples:","197","","198"," * :ref:`sphx_glr_auto_examples_plot_compare_reduction.py`","239","    >>> combined # doctest: +NORMALIZE_WHITESPACE, +ELLIPSIS","240","    FeatureUnion(n_jobs=1,","241","                 transformer_list=[('linear_pca', PCA(copy=True,...)),","242","                                   ('kernel_pca', KernelPCA(alpha=1.0,...))],","243","                 transformer_weights=None)","253","    >>> combined.set_params(kernel_pca=None)","254","    ... # doctest: +NORMALIZE_WHITESPACE, +ELLIPSIS","255","    FeatureUnion(n_jobs=1,","256","                 transformer_list=[('linear_pca', PCA(copy=True,...)),","257","                                   ('kernel_pca', None)],","258","                 transformer_weights=None)"],"delete":["41","    >>> pipe # doctest: +NORMALIZE_WHITESPACE","42","    Pipeline(steps=[('reduce_dim', PCA(copy=True, iterated_power='auto',","43","    n_components=None, random_state=None, svd_solver='auto', tol=0.0,","44","    whiten=False)), ('clf', SVC(C=1.0, cache_size=200, class_weight=None,","45","    coef0=0.0, decision_function_shape='ovr', degree=3, gamma='auto',","46","    kernel='rbf', max_iter=-1, probability=False, random_state=None,","47","    shrinking=True, tol=0.001, verbose=False))])","58","    Pipeline(steps=[('binarizer', Binarizer(copy=True, threshold=0.0)),","78","    >>> pipe.set_params(clf__C=10) # doctest: +NORMALIZE_WHITESPACE","79","    Pipeline(steps=[('reduce_dim', PCA(copy=True, iterated_power='auto',","80","        n_components=None, random_state=None, svd_solver='auto', tol=0.0,","81","        whiten=False)), ('clf', SVC(C=10, cache_size=200, class_weight=None,","82","        coef0=0.0, decision_function_shape='ovr', degree=3, gamma='auto',","83","        kernel='rbf', max_iter=-1, probability=False, random_state=None,","84","        shrinking=True, tol=0.001, verbose=False))])","85","","90","    >>> params = dict(reduce_dim__n_components=[2, 5, 10],","91","    ...               clf__C=[0.1, 10, 100])","92","    >>> grid_search = GridSearchCV(pipe, param_grid=params)","98","    >>> params = dict(reduce_dim=[None, PCA(5), PCA(10)],","99","    ...               clf=[SVC(), LogisticRegression()],","100","    ...               clf__C=[0.1, 10, 100])","101","    >>> grid_search = GridSearchCV(pipe, param_grid=params)","166","    >>> combined # doctest: +NORMALIZE_WHITESPACE","167","    FeatureUnion(n_jobs=1, transformer_list=[('linear_pca', PCA(copy=True,","168","        iterated_power='auto', n_components=None, random_state=None,","169","        svd_solver='auto', tol=0.0, whiten=False)), ('kernel_pca',","170","        KernelPCA(alpha=1.0, coef0=1, copy_X=True, degree=3,","171","        eigen_solver='auto', fit_inverse_transform=False, gamma=None,","172","        kernel='linear', kernel_params=None, max_iter=None, n_components=None,","173","        n_jobs=1, random_state=None, remove_zero_eig=False, tol=0))],","174","        transformer_weights=None)","184","    >>> combined.set_params(kernel_pca=None) # doctest: +NORMALIZE_WHITESPACE","185","    FeatureUnion(n_jobs=1, transformer_list=[('linear_pca', PCA(copy=True,","186","          iterated_power='auto', n_components=None, random_state=None,","187","          svd_solver='auto', tol=0.0, whiten=False)), ('kernel_pca', None)],","188","        transformer_weights=None)"]}]}},"ec32689ae9459610e5f3d7180bc87b7300f94ca5":{"changes":{"sklearn\/feature_selection\/rfe.py":"MODIFY","sklearn\/feature_selection\/tests\/test_rfe.py":"MODIFY"},"diff":{"sklearn\/feature_selection\/rfe.py":[{"add":["398","","399","        if 0.0 < self.step < 1.0:","400","            step = int(max(1, self.step * n_features))","401","        else:","402","            step = int(self.step)","403","        if step <= 0:","404","            raise ValueError(\"Step must be >0\")","405","","433","            n_features - (np.argmax(scores) * step),"],"delete":["402","","426","            n_features - (np.argmax(scores) * self.step),"]}],"sklearn\/feature_selection\/tests\/test_rfe.py":[{"add":["184","    # Verifying that steps < 1 don't blow up.","185","    rfecv_sparse = RFECV(estimator=SVC(kernel=\"linear\"), step=.2, cv=5)","186","    X_sparse = sparse.csr_matrix(X)","187","    rfecv_sparse.fit(X_sparse, y)","188","    X_r_sparse = rfecv_sparse.transform(X_sparse)","189","    assert_array_equal(X_r_sparse.toarray(), iris.data)","190",""],"delete":[]}]}},"6e50c8f35e67719e5b528b66bfe75444ec0008ed":{"changes":{"sklearn\/datasets\/lfw.py":"MODIFY","doc\/templates\/deprecated_class.rst":"ADD","sklearn\/mixture\/gmm.py":"MODIFY","doc\/modules\/classes.rst":"MODIFY","doc\/templates\/generate_deprecated.sh":"ADD","sklearn\/cross_validation.py":"MODIFY","sklearn\/tests\/test_discriminant_analysis.py":"MODIFY","sklearn\/discriminant_analysis.py":"MODIFY","doc\/templates\/deprecated_class_with_call.rst":"ADD","sklearn\/learning_curve.py":"MODIFY","sklearn\/qda.py":"MODIFY","sklearn\/gaussian_process\/gaussian_process.py":"MODIFY","doc\/templates\/deprecated_function.rst":"ADD","sklearn\/lda.py":"MODIFY","sklearn\/mixture\/dpgmm.py":"MODIFY","doc\/templates\/deprecated_class_without_init.rst":"ADD","sklearn\/decomposition\/pca.py":"MODIFY","sklearn\/grid_search.py":"MODIFY","doc\/themes\/scikit-learn\/static\/nature.css_t":"MODIFY"},"diff":{"sklearn\/datasets\/lfw.py":[{"add":["382","    \"\"\"","383","    Alias for fetch_lfw_people(download_if_missing=False)","384","","385","    .. deprecated:: 0.17","386","        This function will be removed in 0.19.","387","        Use :func:`sklearn.datasets.fetch_lfw_people` with parameter","388","        ``download_if_missing=False`` instead.","517","    \"\"\"","518","    Alias for fetch_lfw_pairs(download_if_missing=False)","519","","520","    .. deprecated:: 0.17","521","        This function will be removed in 0.19.","522","        Use :func:`sklearn.datasets.fetch_lfw_pairs` with parameter","523","        ``download_if_missing=False`` instead."],"delete":["382","    \"\"\"Alias for fetch_lfw_people(download_if_missing=False)","511","    \"\"\"Alias for fetch_lfw_pairs(download_if_missing=False)"]}],"doc\/templates\/deprecated_class.rst":[{"add":[],"delete":[]}],"sklearn\/mixture\/gmm.py":[{"add":["672","    \"\"\"","673","    Legacy Gaussian Mixture Model","674","","675","    .. deprecated:: 0.18","676","        This class will be removed in 0.20.","677","        Use :class:`sklearn.mixture.GaussianMixture` instead.","678","","679","    \"\"\"","680",""],"delete":[]}],"doc\/modules\/classes.rst":[{"add":["188","   model_selection.check_cv","204","","205",".. autosummary::","206","   :toctree: generated\/","207","   :template: function.rst","208","","209","   model_selection.fit_grid_point","210","","1357","","1358","","1359","Recently deprecated","1360","===================","1361","","1362","To be removed in 0.19","1363","---------------------","1364","","1365",".. autosummary::","1366","   :toctree: generated\/","1367","   :template: deprecated_class.rst","1368","","1369","   lda.LDA","1370","   qda.QDA","1371","","1372",".. autosummary::","1373","   :toctree: generated\/","1374","   :template: deprecated_function.rst","1375","","1376","   datasets.load_lfw_pairs","1377","   datasets.load_lfw_people","1378","","1379","","1380","To be removed in 0.20","1381","---------------------","1382","","1383",".. autosummary::","1384","   :toctree: generated\/","1385","   :template: deprecated_class.rst","1386","","1387","   grid_search.ParameterGrid","1388","   grid_search.ParameterSampler","1389","   grid_search.GridSearchCV","1390","   grid_search.RandomizedSearchCV","1391","   cross_validation.LeaveOneOut","1392","   cross_validation.LeavePOut","1393","   cross_validation.KFold","1394","   cross_validation.LabelKFold","1395","   cross_validation.LeaveOneLabelOut","1396","   cross_validation.LeavePLabelOut","1397","   cross_validation.LabelShuffleSplit","1398","   cross_validation.StratifiedKFold","1399","   cross_validation.ShuffleSplit","1400","   cross_validation.StratifiedShuffleSplit","1401","   cross_validation.PredefinedSplit","1402","   decomposition.RandomizedPCA","1403","   gaussian_process.GaussianProcess","1404","   mixture.GMM","1405","   mixture.DPGMM","1406","   mixture.VBGMM","1407","","1408","","1409",".. autosummary::","1410","   :toctree: generated\/","1411","   :template: deprecated_function.rst","1412","","1413","   grid_search.fit_grid_point","1414","   learning_curve.learning_curve","1415","   learning_curve.validation_curve","1416","   cross_validation.cross_val_predict","1417","   cross_validation.cross_val_score","1418","   cross_validation.check_cv","1419","   cross_validation.permutation_test_score","1420","   cross_validation.train_test_split"],"delete":["317","   decomposition.RandomizedPCA","562","  gaussian_process.GaussianProcess"]}],"doc\/templates\/generate_deprecated.sh":[{"add":[],"delete":[]}],"sklearn\/cross_validation.py":[{"add":["111","    .. deprecated:: 0.18","112","        This module will be removed in 0.20.","113","        Use :class:`sklearn.model_selection.LeaveOneOut` instead.","114","","177","    .. deprecated:: 0.18","178","        This module will be removed in 0.20.","179","        Use :class:`sklearn.model_selection.LeavePOut` instead.","180","","276","    .. deprecated:: 0.18","277","        This module will be removed in 0.20.","278","        Use :class:`sklearn.model_selection.KFold` instead.","279","","371","    .. deprecated:: 0.18","372","        This module will be removed in 0.20.","373","        Use :class:`sklearn.model_selection.GroupKFold` instead.","374","","477","    .. deprecated:: 0.18","478","        This module will be removed in 0.20.","479","        Use :class:`sklearn.model_selection.StratifiedKFold` instead.","480","","603","    .. deprecated:: 0.18","604","        This module will be removed in 0.20.","605","        Use :class:`sklearn.model_selection.LeaveOneGroupOut` instead.","606","","677","    .. deprecated:: 0.18","678","        This module will be removed in 0.20.","679","        Use :class:`sklearn.model_selection.LeavePGroupsOut` instead.","680","","792","    .. deprecated:: 0.18","793","        This module will be removed in 0.20.","794","        Use :class:`sklearn.model_selection.ShuffleSplit` instead.","795","","997","    .. deprecated:: 0.18","998","        This module will be removed in 0.20.","999","        Use :class:`sklearn.model_selection.StratifiedShuffleSplit` instead.","1000","","1123","    .. deprecated:: 0.18","1124","        This module will be removed in 0.20.","1125","        Use :class:`sklearn.model_selection.PredefinedSplit` instead.","1126","","1182","    .. deprecated:: 0.18","1183","        This module will be removed in 0.20.","1184","        Use :class:`sklearn.model_selection.GroupShuffleSplit` instead.","1185","","1287","    .. deprecated:: 0.18","1288","        This module will be removed in 0.20.","1289","        Use :func:`sklearn.model_selection.cross_val_predict` instead.","1290","","1471","    .. deprecated:: 0.18","1472","        This module will be removed in 0.20.","1473","        Use :func:`sklearn.model_selection.cross_val_score` instead.","1474","","1778","    .. deprecated:: 0.18","1779","        This module will be removed in 0.20.","1780","        Use :func:`sklearn.model_selection.check_cv` instead.","1781","","1839","    .. deprecated:: 0.18","1840","        This module will be removed in 0.20.","1841","        Use :func:`sklearn.model_selection.permutation_test_score` instead.","1842","","1944","    .. deprecated:: 0.18","1945","        This module will be removed in 0.20.","1946","        Use :func:`sklearn.model_selection.train_test_split` instead.","1947",""],"delete":[]}],"sklearn\/tests\/test_discriminant_analysis.py":[{"add":["331","    assert isinstance(lda.LDA(), LinearDiscriminantAnalysis)","341","    assert isinstance(qda.QDA(), QuadraticDiscriminantAnalysis)"],"delete":["331","    assert lda.LDA is LinearDiscriminantAnalysis","341","    assert qda.QDA is QuadraticDiscriminantAnalysis"]}],"sklearn\/discriminant_analysis.py":[{"add":[],"delete":["145","    .. versionchanged:: 0.17","146","       Deprecated :class:`lda.LDA` have been moved to :class:`LinearDiscriminantAnalysis`.","147","","564","    .. versionchanged:: 0.17","565","       Deprecated :class:`qda.QDA` have been moved to :class:`QuadraticDiscriminantAnalysis`.","566",""]}],"doc\/templates\/deprecated_class_with_call.rst":[{"add":[],"delete":[]}],"sklearn\/learning_curve.py":[{"add":["34","    .. deprecated:: 0.18","35","        This module will be removed in 0.20.","36","        Use :func:`sklearn.model_selection.learning_curve` instead.","37","","265","    .. deprecated:: 0.18","266","        This module will be removed in 0.20.","267","        Use :func:`sklearn.model_selection.validation_curve` instead.","268",""],"delete":[]}],"sklearn\/qda.py":[{"add":["1","from .discriminant_analysis import QuadraticDiscriminantAnalysis as _QDA","2","","7","","8","class QDA(_QDA):","9","    \"\"\"","10","    Alias for","11","    :class:`sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis`.","12","","13","    .. deprecated:: 0.17","14","        This class will be removed in 0.19.","15","        Use","16","        :class:`sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis`","17","        instead.","18","    \"\"\"","19","    pass"],"delete":["5","from .discriminant_analysis import QuadraticDiscriminantAnalysis as QDA"]}],"sklearn\/gaussian_process\/gaussian_process.py":[{"add":["66","    .. deprecated:: 0.18","67","        This class will be removed in 0.20.","68","        Use the :class:`GaussianProcessRegressor` instead."],"delete":["66","    Note that this class was deprecated in version 0.18 and will be","67","    removed in 0.20. Use the GaussianProcessRegressor instead."]}],"doc\/templates\/deprecated_function.rst":[{"add":[],"delete":[]}],"sklearn\/lda.py":[{"add":["1","from .discriminant_analysis import LinearDiscriminantAnalysis as _LDA","2","","7","","8","class LDA(_LDA):","9","    \"\"\"","10","    Alias for","11","    :class:`sklearn.discriminant_analysis.LinearDiscriminantAnalysis`.","12","","13","    .. deprecated:: 0.17","14","        This class will be removed in 0.19.","15","        Use","16","        :class:`sklearn.discriminant_analysis.LinearDiscriminantAnalysis`","17","        instead.","18","    \"\"\"","19","    pass"],"delete":["5","from .discriminant_analysis import LinearDiscriminantAnalysis as LDA"]}],"sklearn\/mixture\/dpgmm.py":[{"add":["631","    \"\"\"Dirichlet Process Gaussian Mixture Models","632","","633","    .. deprecated:: 0.18","634","        This class will be removed in 0.20.","635","        Use :class:`sklearn.mixture.BayesianGaussianMixture` with","636","        parameter ``weight_concentration_prior_type='dirichlet_process'``","637","        instead.","638","","639","    \"\"\"","640","","659","    .. deprecated:: 0.18","660","        This class will be removed in 0.20.","661","        Use :class:`sklearn.mixture.BayesianGaussianMixture` with parameter","662","        ``weight_concentration_prior_type='dirichlet_distribution'`` instead.","663",""],"delete":[]}],"doc\/templates\/deprecated_class_without_init.rst":[{"add":[],"delete":[]}],"sklearn\/decomposition\/pca.py":[{"add":["556","    .. deprecated:: 0.18","557","        This class will be removed in 0.20.","558","        Use :class:`PCA` with parameter svd_solver 'randomized' instead.","559","        The new implementation DOES NOT store whiten ``components_``.","560","        Apply transform to get them.","561",""],"delete":[]}],"sklearn\/grid_search.py":[{"add":["48","    .. deprecated:: 0.18","49","        This module will be removed in 0.20.","50","        Use :class:`sklearn.model_selection.ParameterGrid` instead.","51","","171","    .. deprecated:: 0.18","172","        This module will be removed in 0.20.","173","        Use :class:`sklearn.model_selection.ParameterSampler` instead.","174","","275","    .. deprecated:: 0.18","276","        This module will be removed in 0.20.","277","        Use :func:`sklearn.model_selection.fit_grid_point` instead.","278","","632","    .. deprecated:: 0.18","633","        This module will be removed in 0.20.","634","        Use :class:`sklearn.model_selection.GridSearchCV` instead.","635","","838","    .. deprecated:: 0.18","839","        This module will be removed in 0.20.","840","        Use :class:`sklearn.model_selection.RandomizedSearchCV` instead."],"delete":[]}],"doc\/themes\/scikit-learn\/static\/nature.css_t":[{"add":["605","div.warning, div.deprecated {"],"delete":["605","div.warning {"]}]}},"33ed90dc0aa0549a5963000d7d070aa18ca389c4":{"changes":{"sklearn\/tree\/tree.py":"MODIFY","sklearn\/tree\/tests\/test_tree.py":"MODIFY"},"diff":{"sklearn\/tree\/tree.py":[{"add":["218","            if not 1 <= self.min_samples_leaf:","219","                raise ValueError(\"min_samples_leaf must be at least 1 \"","220","                                 \"or in (0, 0.5], got %s\"","221","                                 % self.min_samples_leaf)","224","            if not 0. < self.min_samples_leaf <= 0.5:","225","                raise ValueError(\"min_samples_leaf must be at least 1 \"","226","                                 \"or in (0, 0.5], got %s\"","227","                                 % self.min_samples_leaf)","231","            if not 2 <= self.min_samples_split:","232","                raise ValueError(\"min_samples_split must be at least 2 \"","233","                                 \"or in (0, 1], got %s\"","234","                                 % self.min_samples_split)","237","            if not 0. < self.min_samples_split <= 1.:","238","                raise ValueError(\"min_samples_split must be at least 2 \"","239","                                 \"or in (0, 1], got %s\"","240","                                 % self.min_samples_split)","318","            raise ValueError(\"min_impurity_split must be greater than \"","319","                             \"or equal to 0\")","382","                                           max_leaf_nodes,","383","                                           self.min_impurity_split)"],"delete":["260","        if not (0. < self.min_samples_split <= 1. or","261","                2 <= self.min_samples_split):","262","            raise ValueError(\"min_samples_split must be in at least 2\"","263","                             \" or in (0, 1], got %s\" % min_samples_split)","264","        if not (0. < self.min_samples_leaf <= 0.5 or","265","                1 <= self.min_samples_leaf):","266","            raise ValueError(\"min_samples_leaf must be at least than 1 \"","267","                             \"or in (0, 0.5], got %s\" % min_samples_leaf)","268","","311","            raise ValueError(\"min_impurity_split must be greater than or equal \"","312","                             \"to 0\")","375","                                           max_leaf_nodes, self.min_impurity_split)"]}],"sklearn\/tree\/tests\/test_tree.py":[{"add":["511","        assert_raises(ValueError, TreeEstimator(min_samples_leaf=3.).fit, X, y)","524","        assert_raises(ValueError, TreeEstimator(min_samples_split=2.5).fit,","525","                      X, y)"],"delete":[]}]}},"c336a4390ef93e65f4e6641b954524d9e412947f":{"changes":{"sklearn\/feature_extraction\/_hashing.pyx":"MODIFY"},"diff":{"sklearn\/feature_extraction\/_hashing.pyx":[{"add":["45","            if isinstance(v, (str, unicode)):","55","                f = (<unicode>f).encode(\"utf-8\")","61","            h = murmurhash3_bytes_s32(<bytes>f, 0)"],"delete":["10","from ..externals.six import string_types","11","","47","            if isinstance(v, string_types):","57","                f = f.encode(\"utf-8\")","63","            h = murmurhash3_bytes_s32(f, 0)"]}]}},"f1ec4300c32387c75c222fafc1507e920196b619":{"changes":{"sklearn\/mixture\/gaussian_mixture.py":"MODIFY"},"diff":{"sklearn\/mixture\/gaussian_mixture.py":[{"add":["257","    resp : array-like, shape (n_samples, n_components)"],"delete":["257","    resp : array-like, shape (n_samples, n_features)"]}]}},"4da44c85414acdcc923f8b188849dfa0578736c3":{"changes":{"sklearn\/preprocessing\/tests\/test_imputation.py":"MODIFY","sklearn\/decomposition\/tests\/test_nmf.py":"MODIFY","sklearn\/gaussian_process\/tests\/test_gpr.py":"MODIFY","sklearn\/utils\/tests\/test_extmath.py":"MODIFY"},"diff":{"sklearn\/preprocessing\/tests\/test_imputation.py":[{"add":["14","","93","        [np.nan, 0, 0, 0, 5],","94","        [np.nan, 1, 0, np.nan, 3],","95","        [np.nan, 2, 0, 0, 0],","96","        [np.nan, 6, 0, 5, 13],","100","        [3, 5],","101","        [1, 3],","102","        [2, 7],","145","    values = np.arange(1, shape[0] + 1)","237","        [-1, -1, 0, 5],","238","        [-1, 2, -1, 3],","239","        [-1, 1, 3, -1],","240","        [-1, 2, 3, 7],","244","        [2, 0, 5],","245","        [2, 3, 3],","246","        [1, 3, 3],","247","        [2, 3, 7],","316","    assert_array_equal(X, Xt)","324","    assert_array_equal(X.data, Xt.data)","332","    assert_array_equal(X.data, Xt.data)"],"delete":["8","from sklearn.utils.testing import assert_true","15"," ","94","        [np.nan, 0, 0,  0,  5],","95","        [np.nan, 1, 0,  np.nan,  3],","96","        [np.nan, 2, 0,  0, 0],","97","        [np.nan, 6, 0,  5,  13],","101","        [3,  5],","102","        [1,  3],","103","        [2,  7],","146","    values = np.arange(1, shape[0]+1)","238","        [-1, -1,  0,  5],","239","        [-1,  2, -1,  3],","240","        [-1,  1,  3, -1],","241","        [-1,  2,  3,  7],","245","        [2,  0,  5],","246","        [2,  3,  3],","247","        [1,  3,  3],","248","        [2,  3,  7],","317","    assert_true(np.all(X == Xt))","325","    assert_true(np.all(X.data == Xt.data))","333","    assert_true(np.all(X.data == Xt.data))"]}],"sklearn\/decomposition\/tests\/test_nmf.py":[{"add":["70","        assert_almost_equal(evl[ref != 0], ref[ref != 0])","130","    m = NMF(solver='cd', n_components=n_components, init='custom',","131","            random_state=0)","132","    m.fit_transform(A, W=W_init, H=H_init)","133","    m.transform(A)","143","        m.fit_transform(A)","238","    msg = (\"Number of components must be a positive integer; \"","239","           \"got (n_components=1.5)\")","241","    msg = (\"Number of components must be a positive integer; \"","242","           \"got (n_components='2')\")"],"delete":["70","        assert_true(np.allclose(evl[ref != 0], ref[ref != 0]))","130","    m = NMF(solver='cd', n_components=n_components, init='custom', random_state=0)","131","    ft = m.fit_transform(A, W=W_init, H=H_init)","132","    t = m.transform(A)","142","        ft = m.fit_transform(A)","237","    msg = \"Number of components must be a positive integer; got (n_components=1.5)\"","239","    msg = \"Number of components must be a positive integer; got (n_components='2')\""]}],"sklearn\/gaussian_process\/tests\/test_gpr.py":[{"add":["43","        assert_almost_equal(y_pred, y)","44","        assert_almost_equal(np.diag(y_cov), 0.)"],"delete":["43","        assert_true(np.allclose(y_pred, y))","44","        assert_true(np.allclose(np.diag(y_cov), 0.))"]}],"sklearn\/utils\/tests\/test_extmath.py":[{"add":["66","        assert_array_equal(mode, mode2)","67","        assert_array_equal(score, score2)"],"delete":["19","from sklearn.utils.testing import assert_raise_message","67","        assert_true(np.all(mode == mode2))","68","        assert_true(np.all(score == score2))"]}]}},"0a1f6cd98eebe7c5233692e2d9680232c23bf9a8":{"changes":{"sklearn\/tests\/test_pipeline.py":"MODIFY","sklearn\/pipeline.py":"MODIFY"},"diff":{"sklearn\/tests\/test_pipeline.py":[{"add":["280","    # As pipeline doesn't clone estimators on construction,","281","    # it must have its own estimators","282","    scaler_for_pipeline = StandardScaler()","283","    km_for_pipeline = KMeans(random_state=0)","290","    pipe = Pipeline([","291","        ('scaler', scaler_for_pipeline),","292","        ('Kmeans', km_for_pipeline)","293","    ])"],"delete":["286","    pipe = Pipeline([('scaler', scaler), ('Kmeans', km)])"]}],"sklearn\/pipeline.py":[{"add":["358","                Xt = transform.fit_transform(Xt)"],"delete":["358","                Xt = transform.transform(Xt)"]}]}},"b6941847d6f224446917ba52b4467cfbf082fcd8":{"changes":{"sklearn\/ensemble\/voting_classifier.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/ensemble\/tests\/test_voting_classifier.py":"MODIFY"},"diff":{"sklearn\/ensemble\/voting_classifier.py":[{"add":["143","        if (self.weights is not None and","144","                len(self.weights) != len(self.estimators)):"],"delete":["143","        if self.weights and len(self.weights) != len(self.estimators):"]}],"doc\/whats_new.rst":[{"add":["110","   - Fix a bug where :class:`sklearn.ensemble.VotingClassifier` raises an error","111","     when a numpy array is passed in for weights. :issue:`7983` by","112","     :user:`Vincent Pham <vincentpham1991>`.","113","","4836","","4837",".. _Vincent Pham: https:\/\/github.com\/vincentpham1991"],"delete":[]}],"sklearn\/ensemble\/tests\/test_voting_classifier.py":[{"add":["260","","261","","262","def test_estimator_weights_format():","263","    # Test estimator weights inputs as list and array","264","    clf1 = LogisticRegression(random_state=123)","265","    clf2 = RandomForestClassifier(random_state=123)","266","    eclf1 = VotingClassifier(estimators=[","267","                ('lr', clf1), ('rf', clf2)],","268","                weights=[1, 2],","269","                voting='soft')","270","    eclf2 = VotingClassifier(estimators=[","271","                ('lr', clf1), ('rf', clf2)],","272","                weights=np.array((1, 2)),","273","                voting='soft')","274","    eclf1.fit(X, y)","275","    eclf2.fit(X, y)","276","    assert_array_equal(eclf1.predict_proba(X), eclf2.predict_proba(X))"],"delete":[]}]}},"868a58b2e0ff23427fbfb625e4fc7f997c734480":{"changes":{"sklearn\/utils\/fixes.py":"MODIFY","sklearn\/model_selection\/tests\/test_search.py":"MODIFY","sklearn\/utils\/tests\/test_fixes.py":"MODIFY","sklearn\/model_selection\/_search.py":"MODIFY"},"diff":{"sklearn\/utils\/fixes.py":[{"add":["403","","404","","405","if np_version < (1, 12, 0):","406","    class MaskedArray(np.ma.MaskedArray):","407","        # Before numpy 1.12, np.ma.MaskedArray object is not picklable","408","        # This fix is needed to make our model_selection.GridSearchCV","409","        # picklable as the ``cv_results_`` param uses MaskedArray","410","        def __getstate__(self):","411","            \"\"\"Return the internal state of the masked array, for pickling","412","            purposes.","413","","414","            \"\"\"","415","            cf = 'CF'[self.flags.fnc]","416","            data_state = super(np.ma.MaskedArray, self).__reduce__()[2]","417","            return data_state + (np.ma.getmaskarray(self).tostring(cf),","418","                                 self._fill_value)","419","else:","420","    from numpy.ma import MaskedArray    # noqa"],"delete":[]}],"sklearn\/model_selection\/tests\/test_search.py":[{"add":["942","    grid_search_pickled = pickle.loads(pickle.dumps(grid_search))","943","    assert_array_almost_equal(grid_search.predict(X),","944","                              grid_search_pickled.predict(X))","949","    random_search_pickled = pickle.loads(pickle.dumps(random_search))","950","    assert_array_almost_equal(random_search.predict(X),","951","                              random_search_pickled.predict(X))"],"delete":["942","    pickle.dumps(grid_search)  # smoke test","947","    pickle.dumps(random_search)  # smoke test"]}],"sklearn\/utils\/tests\/test_fixes.py":[{"add":["5","import pickle","8","from sklearn.utils.testing import assert_equal","9","from sklearn.utils.testing import assert_false","10","from sklearn.utils.testing import assert_true","11","from sklearn.utils.testing import assert_almost_equal","12","from sklearn.utils.testing import assert_array_equal","13","from sklearn.utils.testing import assert_array_almost_equal","14","","17","from sklearn.utils.fixes import MaskedArray","58","","59","","60","def test_masked_array_obj_dtype_pickleable():","61","    marr = MaskedArray([1, None, 'a'], dtype=object)","62","","63","    for mask in (True, False, [0, 1, 0]):","64","        marr.mask = mask","65","        marr_pickled = pickle.loads(pickle.dumps(marr))","66","        assert_array_equal(marr.data, marr_pickled.data)","67","        assert_array_equal(marr.mask, marr_pickled.mask)"],"delete":["7","from numpy.testing import (assert_almost_equal,","8","                           assert_array_almost_equal)","11","from sklearn.utils.testing import assert_equal, assert_false, assert_true"]}],"sklearn\/model_selection\/_search.py":[{"add":["32","from ..utils.fixes import MaskedArray","614","        # Use one MaskedArray and mask all the places where the param is not","617","        param_results = defaultdict(partial(MaskedArray,","618","                                            np.empty(n_candidates,),","619","                                            mask=True,"],"delete":["613","        # Use one np.MaskedArray and mask all the places where the param is not","616","        param_results = defaultdict(partial(np.ma.masked_all, (n_candidates,),"]}]}},"790119043be2d2d6c4def13d3a138c1cc19c29aa":{"changes":{"sklearn\/neural_network\/multilayer_perceptron.py":"MODIFY","doc\/modules\/neural_networks_supervised.rst":"MODIFY","sklearn\/neural_network\/tests\/test_mlp.py":"MODIFY"},"diff":{"sklearn\/neural_network\/multilayer_perceptron.py":[{"add":["136","        in lbfgs","347","        # lbfgs does not support mini-batches","348","        if self.solver == 'lbfgs':","377","        elif self.solver == 'lbfgs':","424","        supported_solvers = _STOCHASTIC_SOLVERS + [\"lbfgs\"]","706","    solver : {'lbfgs', 'sgd', 'adam'}, default 'adam'","709","        - 'lbfgs' is an optimizer in the family of quasi-Newton methods.","719","        For small datasets, however, 'lbfgs' can converge faster and perform","727","        If the solver is 'lbfgs', the classifier will not use minibatch.","1048","    solver : {'lbfgs', 'sgd', 'adam'}, default 'adam'","1051","        - 'lbfgs' is an optimizer in the family of quasi-Newton methods.","1061","        For small datasets, however, 'lbfgs' can converge faster and perform","1069","        If the solver is 'lbfgs', the classifier will not use minibatch."],"delete":["136","        in lbgfs","347","        # lbgfs does not support mini-batches","348","        if self.solver == 'lbgfs':","377","        elif self.solver == 'lbgfs':","424","        supported_solvers = _STOCHASTIC_SOLVERS + [\"lbgfs\"]","706","    solver : {'lbgfs', 'sgd', 'adam'}, default 'adam'","709","        - 'lbgfs' is an optimizer in the family of quasi-Newton methods.","719","        For small datasets, however, 'lbgfs' can converge faster and perform","727","        If the solver is 'lbgfs', the classifier will not use minibatch.","1048","    solver : {'lbgfs', 'sgd', 'adam'}, default 'adam'","1051","        - 'lbgfs' is an optimizer in the family of quasi-Newton methods.","1061","        For small datasets, however, 'lbgfs' can converge faster and perform","1069","        If the solver is 'lbgfs', the classifier will not use minibatch."]}],"doc\/modules\/neural_networks_supervised.rst":[{"add":["88","    >>> clf = MLPClassifier(solver='lbfgs', alpha=1e-5,","97","           solver='lbfgs', tol=0.0001, validation_fraction=0.1, verbose=False,","136","    >>> clf = MLPClassifier(solver='lbfgs', alpha=1e-5,","145","           solver='lbfgs', tol=0.0001, validation_fraction=0.1, verbose=False,"],"delete":["88","    >>> clf = MLPClassifier(solver='lbgfs', alpha=1e-5,","97","           solver='lbgfs', tol=0.0001, validation_fraction=0.1, verbose=False,","136","    >>> clf = MLPClassifier(solver='lbgfs', alpha=1e-5,","145","           solver='lbgfs', tol=0.0001, validation_fraction=0.1, verbose=False,"]}],"sklearn\/neural_network\/tests\/test_mlp.py":[{"add":["178","                                solver='lbfgs', alpha=1e-5,","237","            mlp = MLPClassifier(solver='lbfgs', hidden_layer_sizes=50,","252","        mlp = MLPRegressor(solver='lbfgs', hidden_layer_sizes=50,","289","    mlp = MLPClassifier(solver='lbfgs', hidden_layer_sizes=50, alpha=1e-5,","307","    mlp = MLPRegressor(solver='lbfgs', hidden_layer_sizes=50, max_iter=200,","390","    # lbfgs doesn't support partial_fit","391","    assert_false(hasattr(MLPClassifier(solver='lbfgs'), 'partial_fit'))","473","    clf = MLPClassifier(solver='lbfgs', hidden_layer_sizes=30,","495","    mlp = MLPClassifier(solver='lbfgs', hidden_layer_sizes=15,"],"delete":["178","                                solver='lbgfs', alpha=1e-5,","237","            mlp = MLPClassifier(solver='lbgfs', hidden_layer_sizes=50,","252","        mlp = MLPRegressor(solver='lbgfs', hidden_layer_sizes=50,","289","    mlp = MLPClassifier(solver='lbgfs', hidden_layer_sizes=50, alpha=1e-5,","307","    mlp = MLPRegressor(solver='lbgfs', hidden_layer_sizes=50, max_iter=200,","390","    # lbgfs doesn't support partial_fit","391","    assert_false(hasattr(MLPClassifier(solver='lbgfs'), 'partial_fit'))","473","    clf = MLPClassifier(solver='lbgfs', hidden_layer_sizes=30,","495","    mlp = MLPClassifier(solver='lbgfs', hidden_layer_sizes=15,"]}]}},"5c60f1f4e8f6b4df42350e14d22d4de7131c1104":{"changes":{"sklearn\/covariance\/outlier_detection.py":"MODIFY"},"diff":{"sklearn\/covariance\/outlier_detection.py":[{"add":["132","    Attributes","133","    ----------","134","    location_ : array-like, shape (n_features,)","135","        Estimated robust location","136","","137","    covariance_ : array-like, shape (n_features, n_features)","138","        Estimated robust covariance matrix","139","","140","    precision_ : array-like, shape (n_features, n_features)","141","        Estimated pseudo inverse matrix.","142","        (stored only if store_precision is True)","143","","144","    support_ : array-like, shape (n_samples,)","145","        A mask of the observations that have been used to compute the","146","        robust estimates of location and shape.","147",""],"delete":["108","    Attributes","109","    ----------","110","    location_ : array-like, shape (n_features,)","111","        Estimated robust location","112","","113","    covariance_ : array-like, shape (n_features, n_features)","114","        Estimated robust covariance matrix","115","","116","    precision_ : array-like, shape (n_features, n_features)","117","        Estimated pseudo inverse matrix.","118","        (stored only if store_precision is True)","119","","120","    support_ : array-like, shape (n_samples,)","121","        A mask of the observations that have been used to compute the","122","        robust estimates of location and shape.","123",""]}]}},"32236ff9a52e83e4c6e9984bdefc76082af537d5":{"changes":{"doc\/modules\/linear_model.rst":"MODIFY","sklearn\/linear_model\/logistic.py":"MODIFY","sklearn\/linear_model\/sag.py":"MODIFY"},"diff":{"doc\/modules\/linear_model.rst":[{"add":["791","    .. [6] Mark Schmidt, Nicolas Le Roux, and Francis Bach: `Minimizing Finite Sums with the Stochastic Average Gradient. <https:\/\/hal.inria.fr\/hal-00860051\/document>`_"],"delete":["791","    .. [6] Mark Schmidt, Nicolas Le Roux, and Francis Bach: `Minimizing Finite Sums with the Stochastic Average Gradient. <http:\/\/hal.inria.fr\/hal-00860051\/PDF\/sag_journal.pdf>`_"]}],"sklearn\/linear_model\/logistic.py":[{"add":["1118","    SAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach","1119","        Minimizing Finite Sums with the Stochastic Average Gradient","1120","        https:\/\/hal.inria.fr\/hal-00860051\/document","1121","","1661","                                                len(self.Cs_)))","1667","                                                   len(self.Cs_), -1))","1669","                                                len(self.Cs_)))"],"delete":["1657","                                      len(self.Cs_)))","1663","                                     len(self.Cs_), -1))","1665","                                      len(self.Cs_)))"]}],"sklearn\/linear_model\/sag.py":[{"add":["47","    https:\/\/hal.inria.fr\/hal-00860051\/document","188","    https:\/\/hal.inria.fr\/hal-00860051\/document"],"delete":["47","    https:\/\/hal.inria.fr\/hal-00860051\/PDF\/sag_journal.pdf","188","    https:\/\/hal.inria.fr\/hal-00860051\/PDF\/sag_journal.pdf"]}]}}}