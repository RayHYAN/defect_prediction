{"096a9cbead13c5912e247ab2f3d14d27b7990c0d":{"changes":{".travis.yml":"MODIFY","build_tools\/travis\/install.sh":"MODIFY","sklearn\/utils\/graph.py":"MODIFY","sklearn\/utils\/tests\/test_validation.py":"MODIFY"},"diff":{".travis.yml":[{"add":["35","    - DISTRIB=\"conda\" PYTHON_VERSION=\"3.6\" INSTALL_MKL=\"true\"","36","      NUMPY_VERSION=\"1.11.2\" SCIPY_VERSION=\"0.18.1\" PANDAS_VERSION=\"0.19.1\"","37","      CYTHON_VERSION=\"0.25.2\""],"delete":["35","    - DISTRIB=\"conda\" PYTHON_VERSION=\"3.5\" INSTALL_MKL=\"true\"","36","      NUMPY_VERSION=\"1.10.4\" SCIPY_VERSION=\"0.17.0\" PANDAS_VERSION=\"0.18.0\"","37","      CYTHON_VERSION=\"0.23.4\""]}],"build_tools\/travis\/install.sh":[{"add":["54","            numpy=$NUMPY_VERSION scipy=$SCIPY_VERSION \\","55","            mkl cython=$CYTHON_VERSION \\","120","    conda install --yes flake8"],"delete":["54","            numpy=$NUMPY_VERSION scipy=$SCIPY_VERSION numpy scipy \\","55","            mkl flake8 cython=$CYTHON_VERSION \\","120","    conda install flake8"]}],"sklearn\/utils\/graph.py":[{"add":["46","    >>> list(sorted(single_source_shortest_path_length(graph, 0).items()))","47","    [(0, 0), (1, 1), (2, 2), (3, 3)]","48","    >>> graph = np.ones((6, 6))","49","    >>> list(sorted(single_source_shortest_path_length(graph, 2).items()))","50","    [(0, 1), (1, 1), (2, 0), (3, 1), (4, 1), (5, 1)]"],"delete":["46","    >>> single_source_shortest_path_length(graph, 0)","47","    {0: 0, 1: 1, 2: 2, 3: 3}","48","    >>> single_source_shortest_path_length(np.ones((6, 6)), 2)","49","    {0: 1, 1: 1, 2: 0, 3: 1, 4: 1, 5: 1}"]}],"sklearn\/utils\/tests\/test_validation.py":[{"add":["347","    assert_raise_message(TypeError, \"SVR\","],"delete":["347","    msg = \"'SVR' object\"","348","    assert_raise_message(TypeError, msg,"]}]}},"ec8d9f38aa1a3256cf4355ddbce8259a8bc3fb83":{"changes":{"examples\/model_selection\/plot_roc_crossval.py":"MODIFY"},"diff":{"examples\/model_selection\/plot_roc_crossval.py":[{"add":["64","tprs = []","65","aucs = []","69","for train, test in cv.split(X, y):","73","    tprs.append(interp(mean_fpr, fpr, tpr))","74","    tprs[-1][0] = 0.0","76","    aucs.append(roc_auc)","77","    plt.plot(fpr, tpr, lw=1, alpha=0.3,","78","             label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))","81","plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',","82","         label='Luck', alpha=.8)","84","mean_tpr = np.mean(tprs, axis=0)","87","std_auc = np.std(aucs)","88","plt.plot(mean_fpr, mean_tpr, color='b',","89","         label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),","90","         lw=2, alpha=.8)","91","","92","std_tpr = np.std(tprs, axis=0)","93","tprs_upper = np.minimum(mean_tpr + std_tpr, 1)","94","tprs_lower = np.maximum(mean_tpr - std_tpr, 0)","95","plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,","96","                 label=r'$\\pm$ 1 std. dev.')"],"delete":["64","mean_tpr = 0.0","67","colors = cycle(['cyan', 'indigo', 'seagreen', 'yellow', 'blue', 'darkorange'])","68","lw = 2","69","","71","for (train, test), color in zip(cv.split(X, y), colors):","75","    mean_tpr += interp(mean_fpr, fpr, tpr)","76","    mean_tpr[0] = 0.0","78","    plt.plot(fpr, tpr, lw=lw, color=color,","79","             label='ROC fold %d (area = %0.2f)' % (i, roc_auc))","82","plt.plot([0, 1], [0, 1], linestyle='--', lw=lw, color='k',","83","         label='Luck')","85","mean_tpr \/= cv.get_n_splits(X, y)","88","plt.plot(mean_fpr, mean_tpr, color='g', linestyle='--',","89","         label='Mean ROC (area = %0.2f)' % mean_auc, lw=lw)"]}]}},"aea6462b81e42ee5ef582de6da353f8869642735":{"changes":{"doc\/whats_new.rst":"MODIFY","sklearn\/datasets\/samples_generator.py":"MODIFY","sklearn\/datasets\/tests\/test_samples_generator.py":"MODIFY"},"diff":{"doc\/whats_new.rst":[{"add":["142","   - Fixed a bug where :func:`sklearn.datasets.make_moons` gives an","143","     incorrect result when ``n_samples`` is odd.","144","     :issue:`8198` by :user:`Josh Levy <levy5674>`.","145",""],"delete":[]}],"sklearn\/datasets\/samples_generator.py":[{"add":["667","    y = np.hstack([np.zeros(n_samples_out, dtype=np.intp),","668","                   np.ones(n_samples_in, dtype=np.intp)])"],"delete":["667","    y = np.hstack([np.zeros(n_samples_in, dtype=np.intp),","668","                   np.ones(n_samples_out, dtype=np.intp)])"]}],"sklearn\/datasets\/tests\/test_samples_generator.py":[{"add":["26","from sklearn.datasets import make_moons","363","","364","","365","def test_make_moons():","366","    X, y = make_moons(3, shuffle=False)","367","    for x, label in zip(X, y):","368","        center = [0.0, 0.0] if label == 0 else [1.0, 0.5]","369","        dist_sqr = ((x - center) ** 2).sum()","370","        assert_almost_equal(dist_sqr, 1.0,","371","                            err_msg=\"Point is not on expected unit circle\")"],"delete":[]}]}},"29d200249bfd8d4fc582e13ad462650d2c68f7a2":{"changes":{"doc\/modules\/decomposition.rst":"MODIFY"},"diff":{"doc\/modules\/decomposition.rst":[{"add":["145","If we note :math:`n_{\\max} = \\max(n_{\\mathrm{samples}}, n_{\\mathrm{features}})` and","146",":math:`n_{\\min} = \\min(n_{\\mathrm{samples}}, n_{\\mathrm{features}})`, the time complexity","147","of the randomized :class:`PCA` is :math:`O(n_{\\max}^2 \\cdot n_{\\mathrm{components}})`","148","instead of :math:`O(n_{\\max}^2 \\cdot n_{\\min})` for the exact method","152",":math:`2 \\cdot n_{\\max} \\cdot n_{\\mathrm{components}}` instead of :math:`n_{\\max}","153","\\cdot n_{\\min}` for the exact method.","435","                0 \\leq k < n_{\\mathrm{atoms}}","557","* :math:`\\Psi = \\mathrm{diag}(\\psi_1, \\psi_2, \\dots, \\psi_n)`: This model is called","663","    d_{\\mathrm{Fro}}(X, Y) = \\frac{1}{2} ||X - Y||_{\\mathrm{Fro}}^2 = \\frac{1}{2} \\sum_{i,j} (X_{ij} - {Y}_{ij})^2","716","    + \\frac{\\alpha(1-\\rho)}{2} ||W||_{\\mathrm{Fro}} ^ 2","717","    + \\frac{\\alpha(1-\\rho)}{2} ||H||_{\\mathrm{Fro}} ^ 2","722","    d_{\\mathrm{Fro}}(X, WH)","724","    + \\frac{\\alpha(1-\\rho)}{2} ||W||_{\\mathrm{Fro}} ^ 2","725","    + \\frac{\\alpha(1-\\rho)}{2} ||H||_{\\mathrm{Fro}} ^ 2","739","    d_{\\mathrm{Fro}}(X, Y) = \\frac{1}{2} ||X - Y||_{Fro}^2 = \\frac{1}{2} \\sum_{i,j} (X_{ij} - {Y}_{ij})^2","745","    d_{KL}(X, Y) = \\sum_{i,j} (X_{ij} \\log(\\frac{X_{ij}}{Y_{ij}}) - X_{ij} + Y_{ij})","750","    d_{IS}(X, Y) = \\sum_{i,j} (\\frac{X_{ij}}{Y_{ij}} - \\log(\\frac{X_{ij}}{Y_{ij}}) - 1)","843","  1. For each topic :math:`k`, draw :math:`\\beta_k \\sim \\mathrm{Dirichlet}(\\eta),\\: k =1...K`","845","  2. For each document :math:`d`, draw :math:`\\theta_d \\sim \\mathrm{Dirichlet}(\\alpha), \\: d=1...D`","849","    a. Draw a topic index :math:`z_{di} \\sim \\mathrm{Multinomial}(\\theta_d)`","850","    b. Draw the observed word :math:`w_{ij} \\sim \\mathrm{Multinomial}(beta_{z_{di}}.)`","864","  \\log\\: P(w | \\alpha, \\eta) \\geq L(w,\\phi,\\gamma,\\lambda) \\overset{\\triangle}{=}","865","    E_{q}[\\log\\:p(w,z,\\theta,\\beta|\\alpha,\\eta)] - E_{q}[\\log\\:q(z, \\theta, \\beta)]"],"delete":["145","If we note :math:`n_{max} = max(n_{samples}, n_{features})` and","146",":math:`n_{min} = min(n_{samples}, n_{features})`, the time complexity","147","of the randomized :class:`PCA` is :math:`O(n_{max}^2 \\cdot n_{components})`","148","instead of :math:`O(n_{max}^2 \\cdot n_{min})` for the exact method","152",":math:`2 \\cdot n_{max} \\cdot n_{components}` instead of :math:`n_{max}","153","\\cdot n_{min}` for the exact method.","435","                0 \\leq k < n_{atoms}","557","* :math:`\\Psi = diag(\\psi_1, \\psi_2, \\dots, \\psi_n)`: This model is called","663","    d_{Fro}(X, Y) = \\frac{1}{2} ||X - Y||_{Fro}^2 = \\frac{1}{2} \\sum_{i,j} (X_{ij} - {Y}_{ij})^2","716","    + \\frac{\\alpha(1-\\rho)}{2} ||W||_{Fro} ^ 2","717","    + \\frac{\\alpha(1-\\rho)}{2} ||H||_{Fro} ^ 2","722","    d_{Fro}(X, WH)","724","    + \\frac{\\alpha(1-\\rho)}{2} ||W||_{Fro} ^ 2","725","    + \\frac{\\alpha(1-\\rho)}{2} ||H||_{Fro} ^ 2","739","    d_{Fro}(X, Y) = \\frac{1}{2} ||X - Y||_{Fro}^2 = \\frac{1}{2} \\sum_{i,j} (X_{ij} - {Y}_{ij})^2","745","    d_{KL}(X, Y) = \\sum_{i,j} (X_{ij} log(\\frac{X_{ij}}{Y_{ij}}) - X_{ij} + Y_{ij})","750","    d_{IS}(X, Y) = \\sum_{i,j} (\\frac{X_{ij}}{Y_{ij}} - log(\\frac{X_{ij}}{Y_{ij}}) - 1)","843","  1. For each topic :math:`k`, draw :math:`\\beta_k \\sim Dirichlet(\\eta),\\: k =1...K`","845","  2. For each document :math:`d`, draw :math:`\\theta_d \\sim Dirichlet(\\alpha), \\: d=1...D`","849","    a. Draw a topic index :math:`z_{di} \\sim Multinomial(\\theta_d)`","850","    b. Draw the observed word :math:`w_{ij} \\sim Multinomial(beta_{z_{di}}.)`","864","  log\\: P(w | \\alpha, \\eta) \\geq L(w,\\phi,\\gamma,\\lambda) \\overset{\\triangle}{=}","865","    E_{q}[log\\:p(w,z,\\theta,\\beta|\\alpha,\\eta)] - E_{q}[log\\:q(z, \\theta, \\beta)]"]}]}},"45d9182e6dfaecfa47471f10de9ebbb0f5fda2de":{"changes":{"sklearn\/utils\/validation.py":"MODIFY","sklearn\/utils\/tests\/test_validation.py":"MODIFY"},"diff":{"sklearn\/utils\/validation.py":[{"add":["82","        if X.dtype.kind in 'uib' and X.dtype.itemsize <= 4:","83","            return_dtype = np.float32","84","        else:","85","            return_dtype = np.float64","86","        return X.astype(return_dtype)"],"delete":["82","        return X.astype(np.float32 if X.dtype == np.int32 else np.float64)"]}],"sklearn\/utils\/tests\/test_validation.py":[{"add":["45","    assert_equal(X2.dtype, np.float32)","51","    assert_equal(X2.dtype, np.float64)","52","    # Test int dtypes <= 32bit","53","    tested_dtypes = [np.bool,","54","                     np.int8, np.int16, np.int32,","55","                     np.uint8, np.uint16, np.uint32]","56","    for dtype in tested_dtypes:","57","        X = X.astype(dtype)","58","        X2 = as_float_array(X)","59","        assert_equal(X2.dtype, np.float32)","60","","61","    # Test object dtype","62","    X = X.astype(object)","63","    X2 = as_float_array(X, copy=True)","64","    assert_equal(X2.dtype, np.float64)","65",""],"delete":["44","    # Checks that the return type is ok","46","    np.testing.assert_equal(X2.dtype, np.float32)","52","    # Checking that the new type is ok","53","    np.testing.assert_equal(X2.dtype, np.float64)"]}]}},"9927982c4eb5773a33f8d0ef5ca22cfc7806e926":{"changes":{"examples\/svm\/plot_separating_hyperplane_unbalanced.py":"MODIFY","examples\/svm\/plot_separating_hyperplane.py":"MODIFY"},"diff":{"examples\/svm\/plot_separating_hyperplane_unbalanced.py":[{"add":["44","# fit the model and get the separating hyperplane using weighted classes","52","# plot the decision functions for both classifiers","53","ax = plt.gca()","54","xlim = ax.get_xlim()","55","ylim = ax.get_ylim()","56","","57","# create grid to evaluate model","58","xx = np.linspace(xlim[0], xlim[1], 30)","59","yy = np.linspace(ylim[0], ylim[1], 30)","60","YY, XX = np.meshgrid(yy, xx)","61","xy = np.vstack([XX.ravel(), YY.ravel()]).T","62","","63","# get the separating hyperplane","64","Z = clf.decision_function(xy).reshape(XX.shape)","65","","66","# plot decision boundary and margins","67","a = ax.contour(XX, YY, Z, colors='k', levels=[0], alpha=0.5, linestyles=['-'])","68","","69","# get the separating hyperplane for weighted classes","70","Z = wclf.decision_function(xy).reshape(XX.shape)","71","","72","# plot decision boundary and margins for weighted classes","73","b = ax.contour(XX, YY, Z, colors='r', levels=[0], alpha=0.5, linestyles=['-'])","74","","75","plt.legend([a.collections[0], b.collections[0]], [\"non weighted\", \"weighted\"],","76","           loc=\"upper right\")"],"delete":["31","#from sklearn.linear_model import SGDClassifier","45","w = clf.coef_[0]","46","a = -w[0] \/ w[1]","47","xx = np.linspace(-5, 5)","48","yy = a * xx - clf.intercept_[0] \/ w[1]","49","","50","","51","# get the separating hyperplane using weighted classes","55","ww = wclf.coef_[0]","56","wa = -ww[0] \/ ww[1]","57","wyy = wa * xx - wclf.intercept_[0] \/ ww[1]","58","","60","h0 = plt.plot(xx, yy, 'k-', label='no weights')","61","h1 = plt.plot(xx, wyy, 'k--', label='with weights')","65","plt.axis('tight')","66","plt.show()"]}],"examples\/svm\/plot_separating_hyperplane.py":[{"add":["14","from sklearn.datasets import make_blobs","15","","18","X, y = make_blobs(n_samples=40, centers=2, random_state=12, cluster_std=0.35)","22","clf.fit(X, y)","24","plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired)","26","# plot the decision function","27","ax = plt.gca()","28","xlim = ax.get_xlim()","29","ylim = ax.get_ylim()","31","# create grid to evaluate model","32","xx = np.linspace(xlim[0], xlim[1], 30)","33","yy = np.linspace(ylim[0], ylim[1], 30)","34","YY, XX = np.meshgrid(yy, xx)","35","xy = np.vstack([XX.ravel(), YY.ravel()]).T","36","Z = clf.decision_function(xy).reshape(XX.shape)","38","# plot decision boundary and margins","39","ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,","40","           linestyles=['--', '-', '--'])","41","# plot support vectors","42","ax.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=100,","43","           linewidth=1, facecolors='none')"],"delete":["16","np.random.seed(0)","17","X = np.r_[np.random.randn(20, 2) - [2, 2], np.random.randn(20, 2) + [2, 2]]","18","Y = [0] * 20 + [1] * 20","22","clf.fit(X, Y)","24","# get the separating hyperplane","25","w = clf.coef_[0]","26","a = -w[0] \/ w[1]","27","xx = np.linspace(-5, 5)","28","yy = a * xx - (clf.intercept_[0]) \/ w[1]","30","# plot the parallels to the separating hyperplane that pass through the","31","# support vectors","32","b = clf.support_vectors_[0]","33","yy_down = a * xx + (b[1] - a * b[0])","34","b = clf.support_vectors_[-1]","35","yy_up = a * xx + (b[1] - a * b[0])","37","# plot the line, the points, and the nearest vectors to the plane","38","plt.plot(xx, yy, 'k-')","39","plt.plot(xx, yy_down, 'k--')","40","plt.plot(xx, yy_up, 'k--')","42","plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],","43","            s=80, facecolors='none')","44","plt.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.Paired)","45","","46","plt.axis('tight')","47","plt.show()"]}]}},"f34816ab8935e3849ff8c1fe78337b5f687731f0":{"changes":{"doc\/tutorial\/statistical_inference\/supervised_learning.rst":"MODIFY"},"diff":{"doc\/tutorial\/statistical_inference\/supervised_learning.rst":[{"add":["111","In one dimension, this requires on average :math:`n \\sim 1\/d` points.","118","If the number of features is :math:`p`, you now require :math:`n \\sim 1\/d^p`","125","effective :math:`k`-NN estimator in a paltry :math:`p \\sim 20` dimensions would"],"delete":["111","In one dimension, this requires on average :math:`n ~ 1\/d` points.","118","If the number of features is :math:`p`, you now require :math:`n ~ 1\/d^p`","125","effective :math:`k`-NN estimator in a paltry :math:`p~20` dimensions would"]}]}},"f548128e835171f83a75f3944b83b21b5590503b":{"changes":{"sklearn\/mixture\/tests\/test_gaussian_mixture.py":"MODIFY","sklearn\/mixture\/base.py":"MODIFY"},"diff":{"sklearn\/mixture\/tests\/test_gaussian_mixture.py":[{"add":["651","                             'Initialization %d did not converge. '"],"delete":["651","                             'Initialization %d did not converged. '"]}],"sklearn\/mixture\/base.py":[{"add":["232","            warnings.warn('Initialization %d did not converge. '"],"delete":["232","            warnings.warn('Initialization %d did not converged. '"]}]}},"14977242bf22492d61598739e6ee54eb8de2b415":{"changes":{"doc\/developers\/contributing.rst":"MODIFY"},"diff":{"doc\/developers\/contributing.rst":[{"add":["157","If some conflicts arise between your branch and the ``master`` branch, you need","158","to merge ``master``. The command will be::","159","","160","  $ git merge master","161","","162","with ``master`` being synchronized with the ``upstream``.","163","","164","Subsequently, you need to solve the conflicts. You can refer to the `Git","165","documentation related to resolving merge conflict using the command line","166","<https:\/\/help.github.com\/articles\/resolving-a-merge-conflict-using-the-command-line\/>`_.","167","","168",".. note::","169","","170","   In the past, the policy to resolve conflicts was to rebase your branch on","171","   ``master``. GitHub interface deals with merging ``master`` better than in","172","   the past."],"delete":["157","In particular, if some conflicts arise between your branch and the master","158","branch, you will need to `rebase your branch on master","159","<http:\/\/docs.scipy.org\/doc\/numpy\/dev\/gitwash\/development_workflow.html#rebasing-on-master>`_.","160","Please avoid merging master branch into yours. If you did it anyway, you can fix","161","it following `this example","162","<https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/7111#issuecomment-249175383>`_."]}]}},"2709531e898d665ce3fccd145070034027090578":{"changes":{"sklearn\/linear_model\/coordinate_descent.py":"MODIFY"},"diff":{"sklearn\/linear_model\/coordinate_descent.py":[{"add":["1613","        passed in at fit (non multi-task usage), ``coef_`` is then a 1D array.","1614","        Note that ``coef_`` stores the transpose of ``W``, ``W.T``.","1795","        Parameter vector (W in the cost function formula).","1796","        Note that ``coef_`` stores the transpose of ``W``, ``W.T``.","1954","        Note that ``coef_`` stores the transpose of ``W``, ``W.T``.","2120","        Note that ``coef_`` stores the transpose of ``W``, ``W.T``."],"delete":["1613","        passed in at fit (non multi-task usage), ``coef_`` is then a 1D array","1794","        parameter vector (W in the cost function formula)"]}]}},"39076ff385c84934301409c5e2e907dceddb7b22":{"changes":{"sklearn\/gaussian_process\/tests\/test_kernels.py":"MODIFY"},"diff":{"sklearn\/gaussian_process\/tests\/test_kernels.py":[{"add":["285","            if isinstance(\"string\", type(hyperparameter.bounds)):","286","                if hyperparameter.bounds == \"fixed\":","287","                    continue","301","            if isinstance(\"string\", type(hyperparameter.bounds)):","302","                if hyperparameter.bounds == \"fixed\":","303","                    continue"],"delete":["285","            if hyperparameter.bounds == \"fixed\":","286","                continue","300","            if hyperparameter.bounds == \"fixed\":","301","                continue"]}]}},"195de6a154a4a940ebf66a57f0911c1afa7cf8ef":{"changes":{"benchmarks\/bench_isolation_forest.py":"MODIFY"},"diff":{"benchmarks\/bench_isolation_forest.py":[{"add":["10","","14","from sklearn.preprocessing import MultiLabelBinarizer","17","print(__doc__)","18","","19","","20","def print_outlier_ratio(y):","21","    \"\"\"","22","    Helper function to show the distinct value count of element in the target.","23","    Useful indicator for the datasets used in bench_isolation_forest.py.","24","    \"\"\"","25","    uniq, cnt = np.unique(y, return_counts=True)","26","    print(\"----- Target count values: \")","27","    for u, c in zip(uniq, cnt):","28","        print(\"------ %s -> %d occurences\" % (str(u), c))","29","    print(\"----- Outlier ratio: %.5f\" % (np.min(cnt) \/ len(y)))","30","","31","","35","# Set this to true for plotting score histograms for each dataset:","36","with_decision_function_histograms = False","38","# Removed the shuttle dataset because as of 2017-03-23 mldata.org is down:","39","# datasets = ['http', 'smtp', 'SA', 'SF', 'shuttle', 'forestcover']","40","datasets = ['http', 'smtp', 'SA', 'SF', 'forestcover']","41","","42","# Loop over all datasets for fitting and scoring the estimator:","44","","45","    # Loading and vectorizing the data:","46","    print('====== %s ======' % dat)","47","    print('--- Fetching data...')","48","    if dat in ['http', 'smtp', 'SF', 'SA']:","64","        print('----- ')","76","        print_outlier_ratio(y)","78","    print('--- Vectorizing data...')","81","        lb = MultiLabelBinarizer()","82","        x1 = lb.fit_transform(X[:, 1])","84","        y = (y != b'normal.').astype(int)","85","        print_outlier_ratio(y)","88","        lb = MultiLabelBinarizer()","89","        x1 = lb.fit_transform(X[:, 1])","90","        x2 = lb.fit_transform(X[:, 2])","91","        x3 = lb.fit_transform(X[:, 3])","93","        y = (y != b'normal.').astype(int)","94","        print_outlier_ratio(y)","96","    if dat in ('http', 'smtp'):","97","        y = (y != b'normal.').astype(int)","98","        print_outlier_ratio(y)","109","    print('--- Fitting the IsolationForest estimator...')","116","    scoring = - model.decision_function(X_test)  # the lower, the more abnormal","118","    print(\"--- Preparing the plot elements...\")","119","    if with_decision_function_histograms:","120","        fig, ax = plt.subplots(3, sharex=True, sharey=True)","121","        bins = np.linspace(-0.5, 0.5, 200)","122","        ax[0].hist(scoring, bins, color='black')","123","        ax[0].set_title('Decision function for %s dataset' % dat)","124","        ax[1].hist(scoring[y_test == 0], bins, color='b', label='normal data')","125","        ax[1].legend(loc=\"lower right\")","126","        ax[2].hist(scoring[y_test == 1], bins, color='r', label='outliers')","127","        ax[2].legend(loc=\"lower right\")","132","    auc_score = auc(fpr, tpr)","133","    label = ('%s (AUC: %0.3f, train_time= %0.2fs, '","134","             'test_time= %0.2fs)' % (dat, auc_score, fit_time, predict_time))","135","    # Print AUC score and train\/test time:","136","    print(label)"],"delete":["4","","6","","8","print(__doc__)","16","from sklearn.preprocessing import LabelBinarizer","20","","21","datasets = ['http', 'smtp', 'SA', 'SF', 'shuttle', 'forestcover']","22","","27","    # loading and vectorization","28","    print('loading data')","29","    if dat in ['http', 'smtp', 'SA', 'SF']:","57","    print('vectorizing data')","60","        lb = LabelBinarizer()","61","        lb.fit(X[:, 1])","62","        x1 = lb.transform(X[:, 1])","64","        y = (y != 'normal.').astype(int)","67","        lb = LabelBinarizer()","68","        lb.fit(X[:, 1])","69","        x1 = lb.transform(X[:, 1])","70","        lb.fit(X[:, 2])","71","        x2 = lb.transform(X[:, 2])","72","        lb.fit(X[:, 3])","73","        x3 = lb.transform(X[:, 3])","75","        y = (y != 'normal.').astype(int)","77","    if dat == 'http' or dat == 'smtp':","78","        y = (y != 'normal.').astype(int)","89","    print('IsolationForest processing...')","96","    scoring = - model.decision_function(X_test)  # the lower, the more normal","98","    # Show score histograms","99","    fig, ax = plt.subplots(3, sharex=True, sharey=True)","100","    bins = np.linspace(-0.5, 0.5, 200)","101","    ax[0].hist(scoring, bins, color='black')","102","    ax[0].set_title('decision function for %s dataset' % dat)","103","    ax[0].legend(loc=\"lower right\")","104","    ax[1].hist(scoring[y_test == 0], bins, color='b',","105","               label='normal data')","106","    ax[1].legend(loc=\"lower right\")","107","    ax[2].hist(scoring[y_test == 1], bins, color='r',","108","               label='outliers')","109","    ax[2].legend(loc=\"lower right\")","114","    AUC = auc(fpr, tpr)","115","    label = ('%s (area: %0.3f, train-time: %0.2fs, '","116","             'test-time: %0.2fs)' % (dat, AUC, fit_time, predict_time))"]}]}},"e19bb7ce5a0c8d1d2a2575d4a3181aec18494509":{"changes":{"sklearn\/model_selection\/tests\/test_validation.py":"MODIFY"},"diff":{"sklearn\/model_selection\/tests\/test_validation.py":[{"add":["923","    kfold = KFold()"],"delete":["923","    kfold = KFold(len(iris.target))"]}]}},"9e0e2d41fe6bf35bb98ede7ee037724a2d790f58":{"changes":{"doc\/modules\/feature_extraction.rst":"MODIFY","examples\/model_selection\/plot_nested_cross_validation_iris.py":"MODIFY"},"diff":{"doc\/modules\/feature_extraction.rst":[{"add":["0","?.. _feature_extraction:","44","  ...     {'city': 'San Francisco', 'temperature': 18.},","56","  ['city=Dubai', 'city=London', 'city=San Francisco', 'temperature']"],"delete":["0",".. _feature_extraction:","44","  ...     {'city': 'San Fransisco', 'temperature': 18.},","56","  ['city=Dubai', 'city=London', 'city=San Fransisco', 'temperature']"]}],"examples\/model_selection\/plot_nested_cross_validation_iris.py":[{"add":["66","svm = SVC(kernel=\"rbf\")","82","    clf = GridSearchCV(estimator=svm, param_grid=p_grid, cv=inner_cv)"],"delete":["66","svr = SVC(kernel=\"rbf\")","82","    clf = GridSearchCV(estimator=svr, param_grid=p_grid, cv=inner_cv)"]}]}},"2cd1220ebe1dab582dba86947a51364616804059":{"changes":{"doc\/whats_new.rst":"MODIFY","sklearn\/cluster\/dbscan_.py":"MODIFY","sklearn\/cluster\/tests\/test_dbscan.py":"MODIFY"},"diff":{"doc\/whats_new.rst":[{"add":["154","   - Fixed a bug where :class:`sklearn.cluster.DBSCAN` gives incorrect ","155","     result when input is a precomputed sparse matrix with initial","156","     rows all zero.","157","     :issue:`8306` by :user:`Akshay Gupta <Akshay0724>`"],"delete":[]}],"sklearn\/cluster\/dbscan_.py":[{"add":["126","        masked_indptr = np.concatenate(([0], np.cumsum(X_mask)))[X.indptr[1:]]","127",""],"delete":["126","        masked_indptr = np.cumsum(X_mask)[X.indptr[1:] - 1]"]}],"sklearn\/cluster\/tests\/test_dbscan.py":[{"add":["352","","353","","354","def test_dbscan_precomputed_metric_with_initial_rows_zero():","355","    # sample matrix with initial two row all zero","356","    ar = np.array([","357","        [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],","358","        [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],","359","        [0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0],","360","        [0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0],","361","        [0.0, 0.0, 0.1, 0.1, 0.0, 0.0, 0.3],","362","        [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1],","363","        [0.0, 0.0, 0.0, 0.0, 0.3, 0.1, 0.0]","364","    ])","365","    matrix = sparse.csr_matrix(ar)","366","    labels = DBSCAN(eps=0.2, metric='precomputed',","367","                    min_samples=2).fit(matrix).labels_","368","    assert_array_equal(labels, [-1, -1,  0,  0,  0,  1,  1])"],"delete":[]}]}},"2f7f5a1a50c2a2022d42160fce9d0596ecac2ada":{"changes":{"sklearn\/metrics\/scorer.py":"MODIFY","sklearn\/metrics\/tests\/test_score_objects.py":"MODIFY","sklearn\/model_selection\/tests\/test_search.py":"MODIFY","doc\/modules\/model_evaluation.rst":"MODIFY"},"diff":{"sklearn\/metrics\/scorer.py":[{"add":["29","","31","from .cluster import homogeneity_score","32","from .cluster import completeness_score","33","from .cluster import v_measure_score","34","from .cluster import mutual_info_score","35","from .cluster import adjusted_mutual_info_score","36","from .cluster import normalized_mutual_info_score","37","from .cluster import fowlkes_mallows_score","38","","404","homogeneity_scorer = make_scorer(homogeneity_score)","405","completeness_scorer = make_scorer(completeness_score)","406","v_measure_scorer = make_scorer(v_measure_score)","407","mutual_info_scorer = make_scorer(mutual_info_score)","408","adjusted_mutual_info_scorer = make_scorer(adjusted_mutual_info_score)","409","normalized_mutual_info_scorer = make_scorer(normalized_mutual_info_score)","410","fowlkes_mallows_scorer = make_scorer(fowlkes_mallows_score)","411","","425","               # Cluster metrics that use supervised evaluation","426","               adjusted_rand_score=adjusted_rand_scorer,","427","               homogeneity_score=homogeneity_scorer,","428","               completeness_score=completeness_scorer,","429","               v_measure_score=v_measure_scorer,","430","               mutual_info_score=mutual_info_scorer,","431","               adjusted_mutual_info_score=adjusted_mutual_info_scorer,","432","               normalized_mutual_info_score=normalized_mutual_info_scorer,","433","               fowlkes_mallows_score=fowlkes_mallows_scorer)","434",""],"delete":["408","               adjusted_rand_score=adjusted_rand_scorer)"]}],"sklearn\/metrics\/tests\/test_score_objects.py":[{"add":["20","from sklearn.metrics import cluster as cluster_module","49","               'neg_log_loss', 'log_loss']","50","","51","# All supervised cluster scorers (They behave like classification metric)","52","CLUSTER_SCORERS = [\"adjusted_rand_score\",","53","                   \"homogeneity_score\",","54","                   \"completeness_score\",","55","                   \"v_measure_score\",","56","                   \"mutual_info_score\",","57","                   \"adjusted_mutual_info_score\",","58","                   \"normalized_mutual_info_score\",","59","                   \"fowlkes_mallows_score\"]","75","        [(name, sensible_clf) for name in CLUSTER_SCORERS] +","341","def test_supervised_cluster_scorers():","347","    for name in CLUSTER_SCORERS:","348","        score1 = get_scorer(name)(km, X_test, y_test)","349","        score2 = getattr(cluster_module, name)(y_test, km.predict(X_test))","350","        assert_almost_equal(score1, score2)","456","                         KMeans(), cluster_module.adjusted_rand_score)"],"delete":["20","from sklearn.metrics.cluster import adjusted_rand_score","49","               'neg_log_loss', 'log_loss',","50","               'adjusted_rand_score'  # not really, but works","51","               ]","332","def test_unsupervised_scorers():","334","    # We don't have any real unsupervised Scorers yet.","339","    score1 = get_scorer('adjusted_rand_score')(km, X_test, y_test)","340","    score2 = adjusted_rand_score(y_test, km.predict(X_test))","341","    assert_almost_equal(score1, score2)","447","                         KMeans(), adjusted_rand_score)"]}],"sklearn\/model_selection\/tests\/test_search.py":[{"add":["544","    grid_search = GridSearchCV(km, param_grid=dict(n_clusters=[2, 3, 4]),","545","                               scoring='fowlkes_mallows_score')","546","    grid_search.fit(X, y)","547","    # So can FMS ;)","548","    assert_equal(grid_search.best_params_[\"n_clusters\"], 3)","549",""],"delete":[]}],"doc\/modules\/model_evaluation.rst":[{"add":["96","    ValueError: 'wrong_choice' is not a valid scoring value. Valid options are ['accuracy', 'adjusted_mutual_info_score', 'adjusted_rand_score', 'average_precision', 'completeness_score', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'fowlkes_mallows_score', 'homogeneity_score', 'mutual_info_score', 'neg_log_loss', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_mean_squared_log_error', 'neg_median_absolute_error', 'normalized_mutual_info_score', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'r2', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'roc_auc', 'v_measure_score']"],"delete":["96","    ValueError: 'wrong_choice' is not a valid scoring value. Valid options are ['accuracy', 'adjusted_rand_score', 'average_precision', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'neg_log_loss', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_mean_squared_log_error', 'neg_median_absolute_error', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'r2', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'roc_auc']"]}]}},"26a1027a830b7b658782229d936ffb84a340caad":{"changes":{"\/dev\/null":"DELETE","doc\/modules\/classes.rst":"MODIFY","sklearn\/preprocessing\/__init__.py":"MODIFY","doc\/modules\/preprocessing.rst":"MODIFY","doc\/whats_new.rst":"MODIFY","examples\/preprocessing\/plot_all_scaling.py":"ADD","sklearn\/preprocessing\/tests\/test_data.py":"MODIFY","sklearn\/preprocessing\/data.py":"MODIFY","build_tools\/travis\/flake8_diff.sh":"MODIFY"},"diff":{"\/dev\/null":[{"add":[],"delete":[]}],"doc\/modules\/classes.rst":[{"add":["1200","   preprocessing.QuantileTransformer","1214","   preprocessing.quantile_transform"],"delete":[]}],"sklearn\/preprocessing\/__init__.py":[{"add":["14","from .data import QuantileTransformer","22","from .data import quantile_transform","45","    'QuantileTransformer',","59","    'quantile_transform',"],"delete":[]}],"doc\/modules\/preprocessing.rst":[{"add":["12","In general, learning algorithms benefit from standardization of the data set. If","13","some outliers are present in the set, robust scalers or transformers are more","14","appropriate. The behaviors of the different scalers, transformers, and","15","normalizers on a dataset containing marginal outliers is highlighted in","16",":ref:`sphx_glr_auto_examples_preprocessing_plot_all_scaling.py`.","17","","18","","48","  >>> X_train = np.array([[ 1., -1.,  2.],","49","  ...                     [ 2.,  0.,  0.],","50","  ...                     [ 0.,  1., -1.]])","51","  >>> X_scaled = preprocessing.scale(X_train)","80","  >>> scaler = preprocessing.StandardScaler().fit(X_train)","90","  >>> scaler.transform(X_train)                           # doctest: +ELLIPSIS","99","  >>> X_test = [[-1., 1., 0.]]","100","  >>> scaler.transform(X_test)                # doctest: +ELLIPSIS","258",".. _preprocessing_transformer:","259","","260","Non-linear transformation","261","=========================","262","","263","Like scalers, :class:`QuantileTransformer` puts each feature into the same","264","range or distribution. However, by performing a rank transformation, it smooths","265","out unusual distributions and is less influenced by outliers than scaling","266","methods. It does, however, distort correlations and distances within and across","267","features.","268","","269",":class:`QuantileTransformer` and :func:`quantile_transform` provide a","270","non-parametric transformation based on the quantile function to map the data to","271","a uniform distribution with values between 0 and 1::","272","","273","  >>> from sklearn.datasets import load_iris","274","  >>> from sklearn.model_selection import train_test_split","275","  >>> iris = load_iris()","276","  >>> X, y = iris.data, iris.target","277","  >>> X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)","278","  >>> quantile_transformer = preprocessing.QuantileTransformer(random_state=0)","279","  >>> X_train_trans = quantile_transformer.fit_transform(X_train)","280","  >>> X_test_trans = quantile_transformer.transform(X_test)","281","  >>> np.percentile(X_train[:, 0], [0, 25, 50, 75, 100]) # doctest: +SKIP","282","  array([ 4.3,  5.1,  5.8,  6.5,  7.9])","283","","284","This feature corresponds to the sepal length in cm. Once the quantile","285","transformation applied, those landmarks approach closely the percentiles","286","previously defined::","287","","288","  >>> np.percentile(X_train_trans[:, 0], [0, 25, 50, 75, 100])","289","  ... # doctest: +ELLIPSIS +SKIP","290","  array([ 0.00... ,  0.24...,  0.49...,  0.73...,  0.99... ])","291","","292","This can be confirmed on a independent testing set with similar remarks::","293","","294","  >>> np.percentile(X_test[:, 0], [0, 25, 50, 75, 100])","295","  ... # doctest: +SKIP","296","  array([ 4.4  ,  5.125,  5.75 ,  6.175,  7.3  ])","297","  >>> np.percentile(X_test_trans[:, 0], [0, 25, 50, 75, 100])","298","  ... # doctest: +ELLIPSIS +SKIP","299","  array([ 0.01...,  0.25...,  0.46...,  0.60... ,  0.94...])","300","","301","It is also possible to map the transformed data to a normal distribution by","302","setting ``output_distribution='normal'``::","303","","304","  >>> quantile_transformer = preprocessing.QuantileTransformer(","305","  ...     output_distribution='normal', random_state=0)","306","  >>> X_trans = quantile_transformer.fit_transform(X)","307","  >>> quantile_transformer.quantiles_ # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE","308","  array([[ 4.3...,   2...,     1...,     0.1...],","309","         [ 4.31...,  2.02...,  1.01...,  0.1...],","310","         [ 4.32...,  2.05...,  1.02...,  0.1...],","311","         ...,","312","         [ 7.84...,  4.34...,  6.84...,  2.5...],","313","         [ 7.87...,  4.37...,  6.87...,  2.5...],","314","         [ 7.9...,   4.4...,   6.9...,   2.5...]])","315","","316","Thus the median of the input becomes the mean of the output, centered at 0. The","317","normal output is clipped so that the input's minimum and maximum ---","318","corresponding to the 1e-7 and 1 - 1e-7 quantiles respectively --- do not","319","become infinite under the transformation.","320",""],"delete":["41","  >>> X = np.array([[ 1., -1.,  2.],","42","  ...               [ 2.,  0.,  0.],","43","  ...               [ 0.,  1., -1.]])","44","  >>> X_scaled = preprocessing.scale(X)","73","  >>> scaler = preprocessing.StandardScaler().fit(X)","83","  >>> scaler.transform(X)                               # doctest: +ELLIPSIS","92","  >>> scaler.transform([[-1.,  1., 0.]])                # doctest: +ELLIPSIS"]}],"doc\/whats_new.rst":[{"add":["64","   - Added :class:`preprocessing.QuantileTransformer` class and","65","     :func:`preprocessing.quantile_transform` function for features","66","     normalization based on quantiles.","67","     :issue:`8363` by :user:`Denis Engemann <dengemann>`,","68","     :user:`Guillaume Lemaitre <glemaitre>`, `Olivier Grisel`_, `Raghav RV`_,","69","     :user:`Thierry Guillemot <tguillemot>`_, and `Gael Varoquaux`_.","70","","181","   - In :class:`gaussian_process.GaussianProcessRegressor`, method ``predict``","340","   - Fix :func:`sklearn.linear_model.BayesianRidge.fit` to return"],"delete":["174","   - In :class:`gaussian_process.GaussianProcessRegressor`, method ``predict`` ","333","   - Fix :func:`sklearn.linear_model.BayesianRidge.fit` to return "]}],"examples\/preprocessing\/plot_all_scaling.py":[{"add":[],"delete":[]}],"sklearn\/preprocessing\/tests\/test_data.py":[{"add":["5","from __future__ import division","44","from sklearn.preprocessing.data import QuantileTransformer","45","from sklearn.preprocessing.data import quantile_transform","145","    feature_names = poly.get_feature_names(","146","        [u\"\\u0001F40D\", u\"\\u262E\", u\"\\u05D0\"])","856","def test_quantile_transform_iris():","857","    X = iris.data","858","    # uniform output distribution","859","    transformer = QuantileTransformer(n_quantiles=30)","860","    X_trans = transformer.fit_transform(X)","861","    X_trans_inv = transformer.inverse_transform(X_trans)","862","    assert_array_almost_equal(X, X_trans_inv)","863","    # normal output distribution","864","    transformer = QuantileTransformer(n_quantiles=30,","865","                                      output_distribution='normal')","866","    X_trans = transformer.fit_transform(X)","867","    X_trans_inv = transformer.inverse_transform(X_trans)","868","    assert_array_almost_equal(X, X_trans_inv)","869","    # make sure it is possible to take the inverse of a sparse matrix","870","    # which contain negative value; this is the case in the iris dataset","871","    X_sparse = sparse.csc_matrix(X)","872","    X_sparse_tran = transformer.fit_transform(X_sparse)","873","    X_sparse_tran_inv = transformer.inverse_transform(X_sparse_tran)","874","    assert_array_almost_equal(X_sparse.A, X_sparse_tran_inv.A)","875","","876","","877","def test_quantile_transform_check_error():","878","    X = np.transpose([[0, 25, 50, 0, 0, 0, 75, 0, 0, 100],","879","                      [2, 4, 0, 0, 6, 8, 0, 10, 0, 0],","880","                      [0, 0, 2.6, 4.1, 0, 0, 2.3, 0, 9.5, 0.1]])","881","    X = sparse.csc_matrix(X)","882","    X_neg = np.transpose([[0, 25, 50, 0, 0, 0, 75, 0, 0, 100],","883","                          [-2, 4, 0, 0, 6, 8, 0, 10, 0, 0],","884","                          [0, 0, 2.6, 4.1, 0, 0, 2.3, 0, 9.5, 0.1]])","885","    X_neg = sparse.csc_matrix(X_neg)","886","","887","    assert_raises_regex(ValueError, \"Invalid value for 'n_quantiles': 0.\",","888","                        QuantileTransformer(n_quantiles=0).fit, X)","889","    assert_raises_regex(ValueError, \"Invalid value for 'subsample': 0.\",","890","                        QuantileTransformer(subsample=0).fit, X)","891","    assert_raises_regex(ValueError, \"The number of quantiles cannot be\"","892","                        \" greater than the number of samples used. Got\"","893","                        \" 1000 quantiles and 10 samples.\",","894","                        QuantileTransformer(subsample=10).fit, X)","895","","896","    transformer = QuantileTransformer(n_quantiles=10)","897","    assert_raises_regex(ValueError, \"QuantileTransformer only accepts \"","898","                        \"non-negative sparse matrices.\",","899","                        transformer.fit, X_neg)","900","    transformer.fit(X)","901","    assert_raises_regex(ValueError, \"QuantileTransformer only accepts \"","902","                        \"non-negative sparse matrices.\",","903","                        transformer.transform, X_neg)","904","","905","    X_bad_feat = np.transpose([[0, 25, 50, 0, 0, 0, 75, 0, 0, 100],","906","                               [0, 0, 2.6, 4.1, 0, 0, 2.3, 0, 9.5, 0.1]])","907","    assert_raises_regex(ValueError, \"X does not have the same number of \"","908","                        \"features as the previously fitted data. Got 2\"","909","                        \" instead of 3.\",","910","                        transformer.transform, X_bad_feat)","911","    assert_raises_regex(ValueError, \"X does not have the same number of \"","912","                        \"features as the previously fitted data. Got 2\"","913","                        \" instead of 3.\",","914","                        transformer.inverse_transform, X_bad_feat)","915","","916","    transformer = QuantileTransformer(n_quantiles=10,","917","                                      output_distribution='rnd')","918","    # check that an error is raised at fit time","919","    assert_raises_regex(ValueError, \"'output_distribution' has to be either\"","920","                        \" 'normal' or 'uniform'. Got 'rnd' instead.\",","921","                        transformer.fit, X)","922","    # check that an error is raised at transform time","923","    transformer.output_distribution = 'uniform'","924","    transformer.fit(X)","925","    X_tran = transformer.transform(X)","926","    transformer.output_distribution = 'rnd'","927","    assert_raises_regex(ValueError, \"'output_distribution' has to be either\"","928","                        \" 'normal' or 'uniform'. Got 'rnd' instead.\",","929","                        transformer.transform, X)","930","    # check that an error is raised at inverse_transform time","931","    assert_raises_regex(ValueError, \"'output_distribution' has to be either\"","932","                        \" 'normal' or 'uniform'. Got 'rnd' instead.\",","933","                        transformer.inverse_transform, X_tran)","934","","935","","936","def test_quantile_transform_sparse_ignore_zeros():","937","    X = np.array([[0, 1],","938","                  [0, 0],","939","                  [0, 2],","940","                  [0, 2],","941","                  [0, 1]])","942","    X_sparse = sparse.csc_matrix(X)","943","    transformer = QuantileTransformer(ignore_implicit_zeros=True,","944","                                      n_quantiles=5)","945","","946","    # dense case -> warning raise","947","    assert_warns_message(UserWarning, \"'ignore_implicit_zeros' takes effect\"","948","                         \" only with sparse matrix. This parameter has no\"","949","                         \" effect.\", transformer.fit, X)","950","","951","    X_expected = np.array([[0, 0],","952","                           [0, 0],","953","                           [0, 1],","954","                           [0, 1],","955","                           [0, 0]])","956","    X_trans = transformer.fit_transform(X_sparse)","957","    assert_almost_equal(X_expected, X_trans.A)","958","","959","    # consider the case where sparse entries are missing values and user-given","960","    # zeros are to be considered","961","    X_data = np.array([0, 0, 1, 0, 2, 2, 1, 0, 1, 2, 0])","962","    X_col = np.array([0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1])","963","    X_row = np.array([0, 4, 0, 1, 2, 3, 4, 5, 6, 7, 8])","964","    X_sparse = sparse.csc_matrix((X_data, (X_row, X_col)))","965","    X_trans = transformer.fit_transform(X_sparse)","966","    X_expected = np.array([[0., 0.5],","967","                           [0., 0.],","968","                           [0., 1.],","969","                           [0., 1.],","970","                           [0., 0.5],","971","                           [0., 0.],","972","                           [0., 0.5],","973","                           [0., 1.],","974","                           [0., 0.]])","975","    assert_almost_equal(X_expected, X_trans.A)","976","","977","    transformer = QuantileTransformer(ignore_implicit_zeros=True,","978","                                      n_quantiles=5)","979","    X_data = np.array([-1, -1, 1, 0, 0, 0, 1, -1, 1])","980","    X_col = np.array([0, 0, 1, 1, 1, 1, 1, 1, 1])","981","    X_row = np.array([0, 4, 0, 1, 2, 3, 4, 5, 6])","982","    X_sparse = sparse.csc_matrix((X_data, (X_row, X_col)))","983","    X_trans = transformer.fit_transform(X_sparse)","984","    X_expected = np.array([[0, 1],","985","                           [0, 0.375],","986","                           [0, 0.375],","987","                           [0, 0.375],","988","                           [0, 1],","989","                           [0, 0],","990","                           [0, 1]])","991","    assert_almost_equal(X_expected, X_trans.A)","992","    assert_almost_equal(X_sparse.A, transformer.inverse_transform(X_trans).A)","993","","994","    # check in conjunction with subsampling","995","    transformer = QuantileTransformer(ignore_implicit_zeros=True,","996","                                      n_quantiles=5,","997","                                      subsample=8,","998","                                      random_state=0)","999","    X_trans = transformer.fit_transform(X_sparse)","1000","    assert_almost_equal(X_expected, X_trans.A)","1001","    assert_almost_equal(X_sparse.A, transformer.inverse_transform(X_trans).A)","1002","","1003","","1004","def test_quantile_transform_dense_toy():","1005","    X = np.array([[0, 2, 2.6],","1006","                  [25, 4, 4.1],","1007","                  [50, 6, 2.3],","1008","                  [75, 8, 9.5],","1009","                  [100, 10, 0.1]])","1010","","1011","    transformer = QuantileTransformer(n_quantiles=5)","1012","    transformer.fit(X)","1013","","1014","    # using the a uniform output, each entry of X should be map between 0 and 1","1015","    # and equally spaced","1016","    X_trans = transformer.fit_transform(X)","1017","    X_expected = np.tile(np.linspace(0, 1, num=5), (3, 1)).T","1018","    assert_almost_equal(np.sort(X_trans, axis=0), X_expected)","1019","","1020","    X_test = np.array([","1021","        [-1, 1, 0],","1022","        [101, 11, 10],","1023","    ])","1024","    X_expected = np.array([","1025","        [0, 0, 0],","1026","        [1, 1, 1],","1027","    ])","1028","    assert_array_almost_equal(transformer.transform(X_test), X_expected)","1029","","1030","    X_trans_inv = transformer.inverse_transform(X_trans)","1031","    assert_array_almost_equal(X, X_trans_inv)","1032","","1033","","1034","def test_quantile_transform_subsampling():","1035","    # Test that subsampling the input yield to a consistent results We check","1036","    # that the computed quantiles are almost mapped to a [0, 1] vector where","1037","    # values are equally spaced. The infinite norm is checked to be smaller","1038","    # than a given threshold. This is repeated 5 times.","1039","","1040","    # dense support","1041","    n_samples = 1000000","1042","    n_quantiles = 1000","1043","    X = np.sort(np.random.sample((n_samples, 1)), axis=0)","1044","    ROUND = 5","1045","    inf_norm_arr = []","1046","    for random_state in range(ROUND):","1047","        transformer = QuantileTransformer(random_state=random_state,","1048","                                          n_quantiles=n_quantiles,","1049","                                          subsample=n_samples \/\/ 10)","1050","        transformer.fit(X)","1051","        diff = (np.linspace(0, 1, n_quantiles) -","1052","                np.ravel(transformer.quantiles_))","1053","        inf_norm = np.max(np.abs(diff))","1054","        assert_true(inf_norm < 1e-2)","1055","        inf_norm_arr.append(inf_norm)","1056","    # each random subsampling yield a unique approximation to the expected","1057","    # linspace CDF","1058","    assert_equal(len(np.unique(inf_norm_arr)), len(inf_norm_arr))","1059","","1060","    # sparse support","1061","","1062","    # TODO: rng should be seeded once we drop support for older versions of","1063","    # scipy (< 0.13) that don't support seeding.","1064","    X = sparse.rand(n_samples, 1, density=.99, format='csc')","1065","    inf_norm_arr = []","1066","    for random_state in range(ROUND):","1067","        transformer = QuantileTransformer(random_state=random_state,","1068","                                          n_quantiles=n_quantiles,","1069","                                          subsample=n_samples \/\/ 10)","1070","        transformer.fit(X)","1071","        diff = (np.linspace(0, 1, n_quantiles) -","1072","                np.ravel(transformer.quantiles_))","1073","        inf_norm = np.max(np.abs(diff))","1074","        assert_true(inf_norm < 1e-1)","1075","        inf_norm_arr.append(inf_norm)","1076","    # each random subsampling yield a unique approximation to the expected","1077","    # linspace CDF","1078","    assert_equal(len(np.unique(inf_norm_arr)), len(inf_norm_arr))","1079","","1080","","1081","def test_quantile_transform_sparse_toy():","1082","    X = np.array([[0., 2., 0.],","1083","                  [25., 4., 0.],","1084","                  [50., 0., 2.6],","1085","                  [0., 0., 4.1],","1086","                  [0., 6., 0.],","1087","                  [0., 8., 0.],","1088","                  [75., 0., 2.3],","1089","                  [0., 10., 0.],","1090","                  [0., 0., 9.5],","1091","                  [100., 0., 0.1]])","1092","","1093","    X = sparse.csc_matrix(X)","1094","","1095","    transformer = QuantileTransformer(n_quantiles=10)","1096","    transformer.fit(X)","1097","","1098","    X_trans = transformer.fit_transform(X)","1099","    assert_array_almost_equal(np.min(X_trans.toarray(), axis=0), 0.)","1100","    assert_array_almost_equal(np.max(X_trans.toarray(), axis=0), 1.)","1101","","1102","    X_trans_inv = transformer.inverse_transform(X_trans)","1103","    assert_array_almost_equal(X.toarray(), X_trans_inv.toarray())","1104","","1105","    transformer_dense = QuantileTransformer(n_quantiles=10).fit(","1106","        X.toarray())","1107","","1108","    X_trans = transformer_dense.transform(X)","1109","    assert_array_almost_equal(np.min(X_trans.toarray(), axis=0), 0.)","1110","    assert_array_almost_equal(np.max(X_trans.toarray(), axis=0), 1.)","1111","","1112","    X_trans_inv = transformer_dense.inverse_transform(X_trans)","1113","    assert_array_almost_equal(X.toarray(), X_trans_inv.toarray())","1114","","1115","","1116","def test_quantile_transform_axis1():","1117","    X = np.array([[0, 25, 50, 75, 100],","1118","                  [2, 4, 6, 8, 10],","1119","                  [2.6, 4.1, 2.3, 9.5, 0.1]])","1120","","1121","    X_trans_a0 = quantile_transform(X.T, axis=0, n_quantiles=5)","1122","    X_trans_a1 = quantile_transform(X, axis=1, n_quantiles=5)","1123","    assert_array_almost_equal(X_trans_a0, X_trans_a1.T)","1124","","1125","","1126","def test_quantile_transform_bounds():","1127","    # Lower and upper bounds are manually mapped. We checked that in the case","1128","    # of a constant feature and binary feature, the bounds are properly mapped.","1129","    X_dense = np.array([[0, 0],","1130","                        [0, 0],","1131","                        [1, 0]])","1132","    X_sparse = sparse.csc_matrix(X_dense)","1133","","1134","    # check sparse and dense are consistent","1135","    X_trans = QuantileTransformer(n_quantiles=3,","1136","                                  random_state=0).fit_transform(X_dense)","1137","    assert_array_almost_equal(X_trans, X_dense)","1138","    X_trans_sp = QuantileTransformer(n_quantiles=3,","1139","                                     random_state=0).fit_transform(X_sparse)","1140","    assert_array_almost_equal(X_trans_sp.A, X_dense)","1141","    assert_array_almost_equal(X_trans, X_trans_sp.A)","1142","","1143","    # check the consistency of the bounds by learning on 1 matrix","1144","    # and transforming another","1145","    X = np.array([[0, 1],","1146","                  [0, 0.5],","1147","                  [1, 0]])","1148","    X1 = np.array([[0, 0.1],","1149","                   [0, 0.5],","1150","                   [1, 0.1]])","1151","    transformer = QuantileTransformer(n_quantiles=3).fit(X)","1152","    X_trans = transformer.transform(X1)","1153","    assert_array_almost_equal(X_trans, X1)","1154","","1155","    # check that values outside of the range learned will be mapped properly.","1156","    X = np.random.random((1000, 1))","1157","    transformer = QuantileTransformer()","1158","    transformer.fit(X)","1159","    assert_equal(transformer.transform(-10), transformer.transform(np.min(X)))","1160","    assert_equal(transformer.transform(10), transformer.transform(np.max(X)))","1161","    assert_equal(transformer.inverse_transform(-10),","1162","                 transformer.inverse_transform(","1163","                     np.min(transformer.references_)))","1164","    assert_equal(transformer.inverse_transform(10),","1165","                 transformer.inverse_transform(","1166","                     np.max(transformer.references_)))","1167","","1168","","1169","def test_quantile_transform_and_inverse():","1170","    # iris dataset","1171","    X = iris.data","1172","    transformer = QuantileTransformer(n_quantiles=1000, random_state=0)","1173","    X_trans = transformer.fit_transform(X)","1174","    X_trans_inv = transformer.inverse_transform(X_trans)","1175","    assert_array_almost_equal(X, X_trans_inv)","1176","","1177","","1968","","1969","","1970","def test_quantile_transform_valid_axis():","1971","    X = np.array([[0, 25, 50, 75, 100],","1972","                  [2, 4, 6, 8, 10],","1973","                  [2.6, 4.1, 2.3, 9.5, 0.1]])","1974","","1975","    assert_raises_regex(ValueError, \"axis should be either equal to 0 or 1\"","1976","                        \". Got axis=2\", quantile_transform, X.T, axis=2)"],"delete":["0","","143","    feature_names = poly.get_feature_names([u\"\\u0001F40D\", u\"\\u262E\", u\"\\u05D0\"])"]}],"sklearn\/preprocessing\/data.py":[{"add":["8","from __future__ import division","9","","17","from scipy import stats","29","from ..utils.validation import (check_is_fitted, check_random_state,","30","                                FLOAT_DTYPES)","31","BOUNDS_THRESHOLD = 1e-7","47","    'QuantileTransformer',","55","    'quantile_transform',","119","    See examples\/preprocessing\/plot_all_scaling.py for a comparison of the","120","    different scalers, transformers, and normalizers.","121","","126","","246","    minmax_scale: Equivalent function without the estimator API.","247","","248","    Notes","249","    -----","250","    See examples\/preprocessing\/plot_all_scaling.py for a comparison of the","251","    different scalers, transformers, and normalizers.","408","","409","    Notes","410","    -----","411","    See examples\/preprocessing\/plot_all_scaling.py for a comparison of the","412","    different scalers, transformers, and normalizers.","501","    scale: Equivalent function without the estimator API.","505","","506","    Notes","507","    -----","508","    See examples\/preprocessing\/plot_all_scaling.py for a comparison of the","509","    different scalers, transformers, and normalizers.","711","    maxabs_scale: Equivalent function without the estimator API.","712","","713","    Notes","714","    -----","715","    See examples\/preprocessing\/plot_all_scaling.py for a comparison of the","716","    different scalers, transformers, and normalizers.","844","","845","    Notes","846","    -----","847","    See examples\/preprocessing\/plot_all_scaling.py for a comparison of the","848","    different scalers, transformers, and normalizers.","933","    robust_scale: Equivalent function without the estimator API.","941","    See examples\/preprocessing\/plot_all_scaling.py for an example.","1091","    See examples\/preprocessing\/plot_all_scaling.py for a comparison of the","1092","    different scalers, transformers, and normalizers.","1093","","1310","","1311","    Notes","1312","    -----","1313","    See examples\/preprocessing\/plot_all_scaling.py for a comparison of the","1314","    different scalers, transformers, and normalizers.","1398","    See examples\/preprocessing\/plot_all_scaling.py for a comparison of the","1399","    different scalers, transformers, and normalizers.","1400","","1403","    normalize: Equivalent function without the estimator API.","1514","    binarize: Equivalent function without the estimator API.","1949","","1950","","1951","class QuantileTransformer(BaseEstimator, TransformerMixin):","1952","    \"\"\"Transform features using quantiles information.","1953","","1954","    This method transforms the features to follow a uniform or a normal","1955","    distribution. Therefore, for a given feature, this transformation tends","1956","    to spread out the most frequent values. It also reduces the impact of","1957","    (marginal) outliers: this is therefore a robust preprocessing scheme.","1958","","1959","    The transformation is applied on each feature independently.","1960","    The cumulative density function of a feature is used to project the","1961","    original values. Features values of new\/unseen data that fall below","1962","    or above the fitted range will be mapped to the bounds of the output","1963","    distribution. Note that this transform is non-linear. It may distort linear","1964","    correlations between variables measured at the same scale but renders","1965","    variables measured at different scales more directly comparable.","1966","","1967","    Read more in the :ref:`User Guide <preprocessing_transformer>`.","1968","","1969","    Parameters","1970","    ----------","1971","    n_quantiles : int, optional (default=1000)","1972","        Number of quantiles to be computed. It corresponds to the number","1973","        of landmarks used to discretize the cumulative density function.","1974","","1975","    output_distribution : str, optional (default='uniform')","1976","        Marginal distribution for the transformed data. The choices are","1977","        'uniform' (default) or 'normal'.","1978","","1979","    ignore_implicit_zeros : bool, optional (default=False)","1980","        Only applies to sparse matrices. If True, the sparse entries of the","1981","        matrix are discarded to compute the quantile statistics. If False,","1982","        these entries are treated as zeros.","1983","","1984","    subsample : int, optional (default=1e5)","1985","        Maximum number of samples used to estimate the quantiles for","1986","        computational efficiency. Note that the subsampling procedure may","1987","        differ for value-identical sparse and dense matrices.","1988","","1989","    random_state : int, RandomState instance or None, optional (default=None)","1990","        If int, random_state is the seed used by the random number generator;","1991","        If RandomState instance, random_state is the random number generator;","1992","        If None, the random number generator is the RandomState instance used","1993","        by np.random. Note that this is used by subsampling and smoothing","1994","        noise.","1995","","1996","    copy : boolean, optional, (default=True)","1997","        Set to False to perform inplace transformation and avoid a copy (if the","1998","        input is already a numpy array).","1999","","2000","    Attributes","2001","    ----------","2002","    quantiles_ : ndarray, shape (n_quantiles, n_features)","2003","        The values corresponding the quantiles of reference.","2004","","2005","    references_ : ndarray, shape(n_quantiles, )","2006","        Quantiles of references.","2007","","2008","    Examples","2009","    --------","2010","    >>> import numpy as np","2011","    >>> from sklearn.preprocessing import QuantileTransformer","2012","    >>> rng = np.random.RandomState(0)","2013","    >>> X = np.sort(rng.normal(loc=0.5, scale=0.25, size=(25, 1)), axis=0)","2014","    >>> qt = QuantileTransformer(n_quantiles=10, random_state=0)","2015","    >>> qt.fit_transform(X) # doctest: +ELLIPSIS","2016","    array([...])","2017","","2018","    See also","2019","    --------","2020","    quantile_transform : Equivalent function without the estimator API.","2021","    StandardScaler : perform standardization that is faster, but less robust","2022","        to outliers.","2023","    RobustScaler : perform robust standardization that removes the influence","2024","        of outliers but does not put outliers and inliers on the same scale.","2025","","2026","    Notes","2027","    -----","2028","    See examples\/preprocessing\/plot_all_scaling.py for a comparison of the","2029","    different scalers, transformers, and normalizers.","2030","","2031","    \"\"\"","2032","","2033","    def __init__(self, n_quantiles=1000, output_distribution='uniform',","2034","                 ignore_implicit_zeros=False, subsample=int(1e5),","2035","                 random_state=None, copy=True):","2036","        self.n_quantiles = n_quantiles","2037","        self.output_distribution = output_distribution","2038","        self.ignore_implicit_zeros = ignore_implicit_zeros","2039","        self.subsample = subsample","2040","        self.random_state = random_state","2041","        self.copy = copy","2042","","2043","    def _dense_fit(self, X, random_state):","2044","        \"\"\"Compute percentiles for dense matrices.","2045","","2046","        Parameters","2047","        ----------","2048","        X : ndarray, shape (n_samples, n_features)","2049","            The data used to scale along the features axis.","2050","        \"\"\"","2051","        if self.ignore_implicit_zeros:","2052","            warnings.warn(\"'ignore_implicit_zeros' takes effect only with\"","2053","                          \" sparse matrix. This parameter has no effect.\")","2054","","2055","        n_samples, n_features = X.shape","2056","        # for compatibility issue with numpy<=1.8.X, references","2057","        # need to be a list scaled between 0 and 100","2058","        references = (self.references_ * 100).tolist()","2059","        self.quantiles_ = []","2060","        for col in X.T:","2061","            if self.subsample < n_samples:","2062","                subsample_idx = random_state.choice(n_samples,","2063","                                                    size=self.subsample,","2064","                                                    replace=False)","2065","                col = col.take(subsample_idx, mode='clip')","2066","            self.quantiles_.append(np.percentile(col, references))","2067","        self.quantiles_ = np.transpose(self.quantiles_)","2068","","2069","    def _sparse_fit(self, X, random_state):","2070","        \"\"\"Compute percentiles for sparse matrices.","2071","","2072","        Parameters","2073","        ----------","2074","        X : sparse matrix CSC, shape (n_samples, n_features)","2075","            The data used to scale along the features axis. The sparse matrix","2076","            needs to be nonnegative.","2077","        \"\"\"","2078","        n_samples, n_features = X.shape","2079","","2080","        # for compatibility issue with numpy<=1.8.X, references","2081","        # need to be a list scaled between 0 and 100","2082","        references = list(map(lambda x: x * 100, self.references_))","2083","        self.quantiles_ = []","2084","        for feature_idx in range(n_features):","2085","            column_nnz_data = X.data[X.indptr[feature_idx]:","2086","                                     X.indptr[feature_idx + 1]]","2087","            if len(column_nnz_data) > self.subsample:","2088","                column_subsample = (self.subsample * len(column_nnz_data) \/\/","2089","                                    n_samples)","2090","                if self.ignore_implicit_zeros:","2091","                    column_data = np.zeros(shape=column_subsample,","2092","                                           dtype=X.dtype)","2093","                else:","2094","                    column_data = np.zeros(shape=self.subsample, dtype=X.dtype)","2095","                column_data[:column_subsample] = random_state.choice(","2096","                    column_nnz_data, size=column_subsample, replace=False)","2097","            else:","2098","                if self.ignore_implicit_zeros:","2099","                    column_data = np.zeros(shape=len(column_nnz_data),","2100","                                           dtype=X.dtype)","2101","                else:","2102","                    column_data = np.zeros(shape=n_samples, dtype=X.dtype)","2103","                column_data[:len(column_nnz_data)] = column_nnz_data","2104","","2105","            if not column_data.size:","2106","                # if no nnz, an error will be raised for computing the","2107","                # quantiles. Force the quantiles to be zeros.","2108","                self.quantiles_.append([0] * len(references))","2109","            else:","2110","                self.quantiles_.append(","2111","                    np.percentile(column_data, references))","2112","        self.quantiles_ = np.transpose(self.quantiles_)","2113","","2114","    def fit(self, X, y=None):","2115","        \"\"\"Compute the quantiles used for transforming.","2116","","2117","        Parameters","2118","        ----------","2119","        X : ndarray or sparse matrix, shape (n_samples, n_features)","2120","            The data used to scale along the features axis. If a sparse","2121","            matrix is provided, it will be converted into a sparse","2122","            ``csc_matrix``. Additionally, the sparse matrix needs to be","2123","            nonnegative if `ignore_implicit_zeros` is False.","2124","","2125","        Returns","2126","        -------","2127","        self : object","2128","            Returns self","2129","        \"\"\"","2130","        if self.n_quantiles <= 0:","2131","            raise ValueError(\"Invalid value for 'n_quantiles': %d. \"","2132","                             \"The number of quantiles must be at least one.\"","2133","                             % self.n_quantiles)","2134","","2135","        if self.subsample <= 0:","2136","            raise ValueError(\"Invalid value for 'subsample': %d. \"","2137","                             \"The number of subsamples must be at least one.\"","2138","                             % self.subsample)","2139","","2140","        if self.n_quantiles > self.subsample:","2141","            raise ValueError(\"The number of quantiles cannot be greater than\"","2142","                             \" the number of samples used. Got {} quantiles\"","2143","                             \" and {} samples.\".format(self.n_quantiles,","2144","                                                       self.subsample))","2145","","2146","        X = self._check_inputs(X)","2147","        rng = check_random_state(self.random_state)","2148","","2149","        # Create the quantiles of reference","2150","        self.references_ = np.linspace(0, 1, self.n_quantiles,","2151","                                       endpoint=True)","2152","        if sparse.issparse(X):","2153","            self._sparse_fit(X, rng)","2154","        else:","2155","            self._dense_fit(X, rng)","2156","","2157","        return self","2158","","2159","    def _transform_col(self, X_col, quantiles, inverse):","2160","        \"\"\"Private function to transform a single feature\"\"\"","2161","","2162","        if self.output_distribution == 'normal':","2163","            output_distribution = 'norm'","2164","        else:","2165","            output_distribution = self.output_distribution","2166","        output_distribution = getattr(stats, output_distribution)","2167","","2168","        # older version of scipy do not handle tuple as fill_value","2169","        # clipping the value before transform solve the issue","2170","        if not inverse:","2171","            lower_bound_x = quantiles[0]","2172","            upper_bound_x = quantiles[-1]","2173","            lower_bound_y = 0","2174","            upper_bound_y = 1","2175","        else:","2176","            lower_bound_x = 0","2177","            upper_bound_x = 1","2178","            lower_bound_y = quantiles[0]","2179","            upper_bound_y = quantiles[-1]","2180","            #  for inverse transform, match a uniform PDF","2181","            X_col = output_distribution.cdf(X_col)","2182","        # find index for lower and higher bounds","2183","        lower_bounds_idx = (X_col - BOUNDS_THRESHOLD <","2184","                            lower_bound_x)","2185","        upper_bounds_idx = (X_col + BOUNDS_THRESHOLD >","2186","                            upper_bound_x)","2187","","2188","        if not inverse:","2189","            # Interpolate in one direction and in the other and take the","2190","            # mean. This is in case of repeated values in the features","2191","            # and hence repeated quantiles","2192","            #","2193","            # If we don't do this, only one extreme of the duplicated is","2194","            # used (the upper when we do assending, and the","2195","            # lower for descending). We take the mean of these two","2196","            X_col = .5 * (np.interp(X_col, quantiles, self.references_)","2197","                          - np.interp(-X_col, -quantiles[::-1],","2198","                                      -self.references_[::-1]))","2199","        else:","2200","            X_col = np.interp(X_col, self.references_, quantiles)","2201","","2202","        X_col[upper_bounds_idx] = upper_bound_y","2203","        X_col[lower_bounds_idx] = lower_bound_y","2204","        # for forward transform, match the output PDF","2205","        if not inverse:","2206","            X_col = output_distribution.ppf(X_col)","2207","            # find the value to clip the data to avoid mapping to","2208","            # infinity. Clip such that the inverse transform will be","2209","            # consistent","2210","            clip_min = output_distribution.ppf(BOUNDS_THRESHOLD -","2211","                                               np.spacing(1))","2212","            clip_max = output_distribution.ppf(1 - (BOUNDS_THRESHOLD -","2213","                                                    np.spacing(1)))","2214","            X_col = np.clip(X_col, clip_min, clip_max)","2215","","2216","        return X_col","2217","","2218","    def _check_inputs(self, X, accept_sparse_negative=False):","2219","        \"\"\"Check inputs before fit and transform\"\"\"","2220","        X = check_array(X, accept_sparse='csc', copy=self.copy,","2221","                        dtype=[np.float64, np.float32])","2222","        # we only accept positive sparse matrix when ignore_implicit_zeros is","2223","        # false and that we call fit or transform.","2224","        if (not accept_sparse_negative and not self.ignore_implicit_zeros and","2225","                (sparse.issparse(X) and np.any(X.data < 0))):","2226","            raise ValueError('QuantileTransformer only accepts non-negative'","2227","                             ' sparse matrices.')","2228","","2229","        # check the output PDF","2230","        if self.output_distribution not in ('normal', 'uniform'):","2231","            raise ValueError(\"'output_distribution' has to be either 'normal'\"","2232","                             \" or 'uniform'. Got '{}' instead.\".format(","2233","                                 self.output_distribution))","2234","","2235","        return X","2236","","2237","    def _check_is_fitted(self, X):","2238","        \"\"\"Check the inputs before transforming\"\"\"","2239","        check_is_fitted(self, 'quantiles_')","2240","        # check that the dimension of X are adequate with the fitted data","2241","        if X.shape[1] != self.quantiles_.shape[1]:","2242","            raise ValueError('X does not have the same number of features as'","2243","                             ' the previously fitted data. Got {} instead of'","2244","                             ' {}.'.format(X.shape[1],","2245","                                           self.quantiles_.shape[1]))","2246","","2247","    def _transform(self, X, inverse=False):","2248","        \"\"\"Forward and inverse transform.","2249","","2250","        Parameters","2251","        ----------","2252","        X : ndarray, shape (n_samples, n_features)","2253","            The data used to scale along the features axis.","2254","","2255","        inverse : bool, optional (default=False)","2256","            If False, apply forward transform. If True, apply","2257","            inverse transform.","2258","","2259","        Returns","2260","        -------","2261","        X : ndarray, shape (n_samples, n_features)","2262","            Projected data","2263","        \"\"\"","2264","","2265","        if sparse.issparse(X):","2266","            for feature_idx in range(X.shape[1]):","2267","                column_slice = slice(X.indptr[feature_idx],","2268","                                     X.indptr[feature_idx + 1])","2269","                X.data[column_slice] = self._transform_col(","2270","                    X.data[column_slice], self.quantiles_[:, feature_idx],","2271","                    inverse)","2272","        else:","2273","            for feature_idx in range(X.shape[1]):","2274","                X[:, feature_idx] = self._transform_col(","2275","                    X[:, feature_idx], self.quantiles_[:, feature_idx],","2276","                    inverse)","2277","","2278","        return X","2279","","2280","    def transform(self, X):","2281","        \"\"\"Feature-wise transformation of the data.","2282","","2283","        Parameters","2284","        ----------","2285","        X : ndarray or sparse matrix, shape (n_samples, n_features)","2286","            The data used to scale along the features axis. If a sparse","2287","            matrix is provided, it will be converted into a sparse","2288","            ``csc_matrix``. Additionally, the sparse matrix needs to be","2289","            nonnegative if `ignore_implicit_zeros` is False.","2290","","2291","        Returns","2292","        -------","2293","        Xt : ndarray or sparse matrix, shape (n_samples, n_features)","2294","            The projected data.","2295","        \"\"\"","2296","        X = self._check_inputs(X)","2297","        self._check_is_fitted(X)","2298","","2299","        return self._transform(X, inverse=False)","2300","","2301","    def inverse_transform(self, X):","2302","        \"\"\"Back-projection to the original space.","2303","","2304","        X : ndarray or sparse matrix, shape (n_samples, n_features)","2305","            The data used to scale along the features axis. If a sparse","2306","            matrix is provided, it will be converted into a sparse","2307","            ``csc_matrix``. Additionally, the sparse matrix needs to be","2308","            nonnegative if `ignore_implicit_zeros` is False.","2309","","2310","        Returns","2311","        -------","2312","        Xt : ndarray or sparse matrix, shape (n_samples, n_features)","2313","            The projected data.","2314","        \"\"\"","2315","        X = self._check_inputs(X, accept_sparse_negative=True)","2316","        self._check_is_fitted(X)","2317","","2318","        return self._transform(X, inverse=True)","2319","","2320","","2321","def quantile_transform(X, axis=0, n_quantiles=1000,","2322","                       output_distribution='uniform',","2323","                       ignore_implicit_zeros=False,","2324","                       subsample=int(1e5),","2325","                       random_state=None,","2326","                       copy=False):","2327","    \"\"\"Transform features using quantiles information.","2328","","2329","    This method transforms the features to follow a uniform or a normal","2330","    distribution. Therefore, for a given feature, this transformation tends","2331","    to spread out the most frequent values. It also reduces the impact of","2332","    (marginal) outliers: this is therefore a robust preprocessing scheme.","2333","","2334","    The transformation is applied on each feature independently.","2335","    The cumulative density function of a feature is used to project the","2336","    original values. Features values of new\/unseen data that fall below","2337","    or above the fitted range will be mapped to the bounds of the output","2338","    distribution. Note that this transform is non-linear. It may distort linear","2339","    correlations between variables measured at the same scale but renders","2340","    variables measured at different scales more directly comparable.","2341","","2342","    Read more in the :ref:`User Guide <preprocessing_transformer>`.","2343","","2344","    Parameters","2345","    ----------","2346","    X : array-like, sparse matrix","2347","        The data to transform.","2348","","2349","    axis : int, (default=0)","2350","        Axis used to compute the means and standard deviations along. If 0,","2351","        transform each feature, otherwise (if 1) transform each sample.","2352","","2353","    n_quantiles : int, optional (default=1000)","2354","        Number of quantiles to be computed. It corresponds to the number","2355","        of landmarks used to discretize the cumulative density function.","2356","","2357","    output_distribution : str, optional (default='uniform')","2358","        Marginal distribution for the transformed data. The choices are","2359","        'uniform' (default) or 'normal'.","2360","","2361","    ignore_implicit_zeros : bool, optional (default=False)","2362","        Only applies to sparse matrices. If True, the sparse entries of the","2363","        matrix are discarded to compute the quantile statistics. If False,","2364","        these entries are treated as zeros.","2365","","2366","    subsample : int, optional (default=1e5)","2367","        Maximum number of samples used to estimate the quantiles for","2368","        computational efficiency. Note that the subsampling procedure may","2369","        differ for value-identical sparse and dense matrices.","2370","","2371","    random_state : int, RandomState instance or None, optional (default=None)","2372","        If int, random_state is the seed used by the random number generator;","2373","        If RandomState instance, random_state is the random number generator;","2374","        If None, the random number generator is the RandomState instance used","2375","        by np.random. Note that this is used by subsampling and smoothing","2376","        noise.","2377","","2378","    copy : boolean, optional, (default=True)","2379","        Set to False to perform inplace transformation and avoid a copy (if the","2380","        input is already a numpy array).","2381","","2382","    Attributes","2383","    ----------","2384","    quantiles_ : ndarray, shape (n_quantiles, n_features)","2385","        The values corresponding the quantiles of reference.","2386","","2387","    references_ : ndarray, shape(n_quantiles, )","2388","        Quantiles of references.","2389","","2390","    Examples","2391","    --------","2392","    >>> import numpy as np","2393","    >>> from sklearn.preprocessing import quantile_transform","2394","    >>> rng = np.random.RandomState(0)","2395","    >>> X = np.sort(rng.normal(loc=0.5, scale=0.25, size=(25, 1)), axis=0)","2396","    >>> quantile_transform(X, n_quantiles=10, random_state=0)","2397","    ... # doctest: +ELLIPSIS","2398","    array([...])","2399","","2400","    See also","2401","    --------","2402","    QuantileTransformer : Performs quantile-based scaling using the","2403","        ``Transformer`` API (e.g. as part of a preprocessing","2404","        :class:`sklearn.pipeline.Pipeline`).","2405","    scale : perform standardization that is faster, but less robust","2406","        to outliers.","2407","    robust_scale : perform robust standardization that removes the influence","2408","        of outliers but does not put outliers and inliers on the same scale.","2409","","2410","    Notes","2411","    -----","2412","    See examples\/preprocessing\/plot_all_scaling.py for a comparison of the","2413","    different scalers, transformers, and normalizers.","2414","","2415","    \"\"\"","2416","    n = QuantileTransformer(n_quantiles=n_quantiles,","2417","                            output_distribution=output_distribution,","2418","                            subsample=subsample,","2419","                            ignore_implicit_zeros=ignore_implicit_zeros,","2420","                            random_state=random_state,","2421","                            copy=copy)","2422","    if axis == 0:","2423","        return n.fit_transform(X)","2424","    elif axis == 1:","2425","        return n.fit_transform(X.T).T","2426","    else:","2427","        raise ValueError(\"axis should be either equal to 0 or 1. Got\"","2428","                         \" axis={}\".format(axis))"],"delete":["26","from ..utils.validation import check_is_fitted, FLOAT_DTYPES","235","    minmax_scale: Equivalent function without the object oriented API.","480","    scale: Equivalent function without the object oriented API.","685","    maxabs_scale: Equivalent function without the object oriented API.","897","    robust_scale: Equivalent function without the object oriented API.","905","    See examples\/preprocessing\/plot_robust_scaling.py for an example.","1356","    normalize: Equivalent function without the object oriented API.","1467","    binarize: Equivalent function without the object oriented API."]}],"build_tools\/travis\/flake8_diff.sh":[{"add":["139","    check_files \"$(echo \"$MODIFIED_FILES\" | grep -v ^examples)\" --ignore=W503","141","    check_files \"$(echo \"$MODIFIED_FILES\" | grep ^examples)\" --ignore=E402,W503"],"delete":["139","    check_files \"$(echo \"$MODIFIED_FILES\" | grep -v ^examples)\"","141","    check_files \"$(echo \"$MODIFIED_FILES\" | grep ^examples)\" --ignore=E402"]}]}},"4d31e55b0ea86c3b00276337369cda463b5bb798":{"changes":{"sklearn\/model_selection\/_search.py":"MODIFY"},"diff":{"sklearn\/model_selection\/_search.py":[{"add":["820","    >>> svc = svm.SVC()","821","    >>> clf = GridSearchCV(svc, parameters)"],"delete":["820","    >>> svr = svm.SVC()","821","    >>> clf = GridSearchCV(svr, parameters)"]}]}},"38adb27bf1f977364cbead9393ca5887986fad93":{"changes":{"sklearn\/manifold\/tests\/test_t_sne.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/manifold\/t_sne.py":"MODIFY"},"diff":{"sklearn\/manifold\/tests\/test_t_sne.py":[{"add":["627","","628","","629","def test_accessible_kl_divergence():","630","    # Ensures that the accessible kl_divergence matches the computed value","631","    random_state = check_random_state(0)","632","    X = random_state.randn(100, 2)","633","    tsne = TSNE(n_iter_without_progress=2, verbose=2,","634","                random_state=0, method='exact')","635","","636","    old_stdout = sys.stdout","637","    sys.stdout = StringIO()","638","    try:","639","        tsne.fit_transform(X)","640","    finally:","641","        out = sys.stdout.getvalue()","642","        sys.stdout.close()","643","        sys.stdout = old_stdout","644","","645","    # The output needs to contain the accessible kl_divergence as the error at","646","    # the last iteration","647","    for line in out.split('\\n')[::-1]:","648","        if 'Iteration' in line:","649","            _, _, error = line.partition('error = ')","650","            if error:","651","                error, _, _ = error.partition(',')","652","                break","653","    assert_almost_equal(tsne.kl_divergence_, float(error), decimal=5)"],"delete":[]}],"doc\/whats_new.rst":[{"add":["254","   - Fixed a bug in :class:`manifold.TSNE` where it stored the incorrect","255","     ``kl_divergence_``. :issue:`6507` by :user:`Sebastian Saeger <ssaeger>`.","256","","5036",".. _Anish Shah: https:\/\/github.com\/AnishShah"],"delete":[]}],"sklearn\/manifold\/t_sne.py":[{"add":["866","        params, kl_divergence, it = _gradient_descent(obj_func, params,","867","                                                      **opt_args)"],"delete":["866","        params, error, it = _gradient_descent(obj_func, params, **opt_args)"]}]}},"fcdbaca5a91896d8aa6b395a0a9db852753a029e":{"changes":{"sklearn\/decomposition\/tests\/test_pca.py":"MODIFY"},"diff":{"sklearn\/decomposition\/tests\/test_pca.py":[{"add":["576","def test_pca_sparse_input():"],"delete":["576","def test_pca_spase_input():","577",""]}]}},"6252f99c16a5d5d84573391e1b488baf51554117":{"changes":{"sklearn\/utils\/extmath.py":"MODIFY","sklearn\/decomposition\/nmf.py":"MODIFY","sklearn\/utils\/tests\/test_extmath.py":"MODIFY"},"diff":{"sklearn\/utils\/extmath.py":[{"add":["55","    if np.issubdtype(x.dtype, np.integer):","56","        warnings.warn('Array type is integer, np.dot may overflow. '","57","                      'Data should be float type to avoid this issue',","58","                      UserWarning)"],"delete":[]}],"sklearn\/decomposition\/nmf.py":[{"add":["959","    X = check_array(X, accept_sparse=('csr', 'csc'), dtype=float)","1206","        X = check_array(X, accept_sparse=('csr', 'csc'), dtype=float)"],"delete":["959","    X = check_array(X, accept_sparse=('csr', 'csc'))","1206","        X = check_array(X, accept_sparse=('csr', 'csc'))"]}],"sklearn\/utils\/tests\/test_extmath.py":[{"add":["20","from sklearn.utils.testing import assert_warns_message","151","    # Check the warning with an int array and np.dot potential overflow","152","    assert_warns_message(","153","                    UserWarning, 'Array type is integer, np.dot may '","154","                    'overflow. Data should be float type to avoid this issue',","155","                    squared_norm, X.astype(int))"],"delete":[]}]}},"af1796ef68d5193f0ca8573a7c2e71c97c97e9ff":{"changes":{"sklearn\/model_selection\/__init__.py":"MODIFY","sklearn\/model_selection\/_split.py":"MODIFY","doc\/modules\/classes.rst":"MODIFY","sklearn\/model_selection\/tests\/test_split.py":"MODIFY","doc\/modules\/cross_validation.rst":"MODIFY","doc\/whats_new.rst":"MODIFY"},"diff":{"sklearn\/model_selection\/__init__.py":[{"add":["9","from ._split import RepeatedKFold","10","from ._split import RepeatedStratifiedKFold","40","           'RepeatedKFold',","41","           'RepeatedStratifiedKFold',"],"delete":[]}],"sklearn\/model_selection\/_split.py":[{"add":["43","           'RepeatedStratifiedKFold',","44","           'RepeatedKFold',","401","","402","    RepeatedKFold: Repeats K-Fold n times.","559","    See also","560","    --------","561","    RepeatedStratifiedKFold: Repeats Stratified K-Fold n times.","922","class _RepeatedSplits(with_metaclass(ABCMeta)):","923","    \"\"\"Repeated splits for an arbitrary randomized CV splitter.","924","","925","    Repeats splits for cross-validators n times with different randomization","926","    in each repetition.","927","","928","    Parameters","929","    ----------","930","    cv : callable","931","        Cross-validator class.","932","","933","    n_repeats : int, default=10","934","        Number of times cross-validator needs to be repeated.","935","","936","    random_state : None, int or RandomState, default=None","937","        Random state to be used to generate random state for each","938","        repetition.","939","","940","    **cvargs : additional params","941","        Constructor parameters for cv. Must not contain random_state","942","        and shuffle.","943","    \"\"\"","944","    def __init__(self, cv, n_repeats=10, random_state=None, **cvargs):","945","        if not isinstance(n_repeats, (np.integer, numbers.Integral)):","946","            raise ValueError(\"Number of repetitions must be of Integral type.\")","947","","948","        if n_repeats <= 1:","949","            raise ValueError(\"Number of repetitions must be greater than 1.\")","950","","951","        if any(key in cvargs for key in ('random_state', 'shuffle')):","952","            raise ValueError(","953","                \"cvargs must not contain random_state or shuffle.\")","954","","955","        self.cv = cv","956","        self.n_repeats = n_repeats","957","        self.random_state = random_state","958","        self.cvargs = cvargs","959","","960","    def split(self, X, y=None, groups=None):","961","        \"\"\"Generates indices to split data into training and test set.","962","","963","        Parameters","964","        ----------","965","        X : array-like, shape (n_samples, n_features)","966","            Training data, where n_samples is the number of samples","967","            and n_features is the number of features.","968","","969","        y : array-like, of length n_samples","970","            The target variable for supervised learning problems.","971","","972","        groups : array-like, with shape (n_samples,), optional","973","            Group labels for the samples used while splitting the dataset into","974","            train\/test set.","975","","976","        Returns","977","        -------","978","        train : ndarray","979","            The training set indices for that split.","980","","981","        test : ndarray","982","            The testing set indices for that split.","983","        \"\"\"","984","        n_repeats = self.n_repeats","985","        rng = check_random_state(self.random_state)","986","","987","        for idx in range(n_repeats):","988","            cv = self.cv(random_state=rng, shuffle=True,","989","                         **self.cvargs)","990","            for train_index, test_index in cv.split(X, y, groups):","991","                yield train_index, test_index","992","","993","","994","class RepeatedKFold(_RepeatedSplits):","995","    \"\"\"Repeated K-Fold cross validator.","996","","997","    Repeats K-Fold n times with different randomization in each repetition.","998","","999","    Read more in the :ref:`User Guide <cross_validation>`.","1000","","1001","    Parameters","1002","    ----------","1003","    n_splits : int, default=5","1004","        Number of folds. Must be at least 2.","1005","","1006","    n_repeats : int, default=10","1007","        Number of times cross-validator needs to be repeated.","1008","","1009","    random_state : None, int or RandomState, default=None","1010","        Random state to be used to generate random state for each","1011","        repetition.","1012","","1013","    Examples","1014","    --------","1015","    >>> from sklearn.model_selection import RepeatedKFold","1016","    >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])","1017","    >>> y = np.array([0, 0, 1, 1])","1018","    >>> rkf = RepeatedKFold(n_splits=2, n_repeats=2, random_state=2652124)","1019","    >>> for train_index, test_index in rkf.split(X):","1020","    ...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)","1021","    ...     X_train, X_test = X[train_index], X[test_index]","1022","    ...     y_train, y_test = y[train_index], y[test_index]","1023","    ...","1024","    TRAIN: [0 1] TEST: [2 3]","1025","    TRAIN: [2 3] TEST: [0 1]","1026","    TRAIN: [1 2] TEST: [0 3]","1027","    TRAIN: [0 3] TEST: [1 2]","1028","","1029","","1030","    See also","1031","    --------","1032","    RepeatedStratifiedKFold: Repeates Stratified K-Fold n times.","1033","    \"\"\"","1034","    def __init__(self, n_splits=5, n_repeats=10, random_state=None):","1035","        super(RepeatedKFold, self).__init__(","1036","            KFold, n_repeats, random_state, n_splits=n_splits)","1037","","1038","","1039","class RepeatedStratifiedKFold(_RepeatedSplits):","1040","    \"\"\"Repeated Stratified K-Fold cross validator.","1041","","1042","    Repeats Stratified K-Fold n times with different randomization in each","1043","    repetition.","1044","","1045","    Read more in the :ref:`User Guide <cross_validation>`.","1046","","1047","    Parameters","1048","    ----------","1049","    n_splits : int, default=5","1050","        Number of folds. Must be at least 2.","1051","","1052","    n_repeats : int, default=10","1053","        Number of times cross-validator needs to be repeated.","1054","","1055","    random_state : None, int or RandomState, default=None","1056","        Random state to be used to generate random state for each","1057","        repetition.","1058","","1059","    Examples","1060","    --------","1061","    >>> from sklearn.model_selection import RepeatedStratifiedKFold","1062","    >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])","1063","    >>> y = np.array([0, 0, 1, 1])","1064","    >>> rskf = RepeatedStratifiedKFold(n_splits=2, n_repeats=2,","1065","    ...     random_state=36851234)","1066","    >>> for train_index, test_index in rskf.split(X, y):","1067","    ...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)","1068","    ...     X_train, X_test = X[train_index], X[test_index]","1069","    ...     y_train, y_test = y[train_index], y[test_index]","1070","    ...","1071","    TRAIN: [1 2] TEST: [0 3]","1072","    TRAIN: [0 3] TEST: [1 2]","1073","    TRAIN: [1 3] TEST: [0 2]","1074","    TRAIN: [0 2] TEST: [1 3]","1075","","1076","","1077","    See also","1078","    --------","1079","    RepeatedKFold: Repeats K-Fold n times.","1080","    \"\"\"","1081","    def __init__(self, n_splits=5, n_repeats=10, random_state=None):","1082","        super(RepeatedStratifiedKFold, self).__init__(","1083","            StratifiedKFold, n_repeats, random_state, n_splits=n_splits)","1084","","1085",""],"delete":[]}],"doc\/modules\/classes.rst":[{"add":["172","   model_selection.RepeatedKFold","173","   model_selection.RepeatedStratifiedKFold"],"delete":[]}],"sklearn\/model_selection\/tests\/test_split.py":[{"add":["44","from sklearn.model_selection import RepeatedKFold","45","from sklearn.model_selection import RepeatedStratifiedKFold","808","def test_repeated_cv_value_errors():","809","    # n_repeats is not integer or <= 1","810","    for cv in (RepeatedKFold, RepeatedStratifiedKFold):","811","        assert_raises(ValueError, cv, n_repeats=1)","812","        assert_raises(ValueError, cv, n_repeats=1.5)","813","","814","","815","def test_repeated_kfold_determinstic_split():","816","    X = [[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]","817","    random_state = 258173307","818","    rkf = RepeatedKFold(","819","        n_splits=2,","820","        n_repeats=2,","821","        random_state=random_state)","822","","823","    # split should produce same and deterministic splits on","824","    # each call","825","    for _ in range(3):","826","        splits = rkf.split(X)","827","        train, test = next(splits)","828","        assert_array_equal(train, [2, 4])","829","        assert_array_equal(test, [0, 1, 3])","830","","831","        train, test = next(splits)","832","        assert_array_equal(train, [0, 1, 3])","833","        assert_array_equal(test, [2, 4])","834","","835","        train, test = next(splits)","836","        assert_array_equal(train, [0, 1])","837","        assert_array_equal(test, [2, 3, 4])","838","","839","        train, test = next(splits)","840","        assert_array_equal(train, [2, 3, 4])","841","        assert_array_equal(test, [0, 1])","842","","843","        assert_raises(StopIteration, next, splits)","844","","845","","846","def test_repeated_stratified_kfold_determinstic_split():","847","    X = [[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]","848","    y = [1, 1, 1, 0, 0]","849","    random_state = 1944695409","850","    rskf = RepeatedStratifiedKFold(","851","        n_splits=2,","852","        n_repeats=2,","853","        random_state=random_state)","854","","855","    # split should produce same and deterministic splits on","856","    # each call","857","    for _ in range(3):","858","        splits = rskf.split(X, y)","859","        train, test = next(splits)","860","        assert_array_equal(train, [1, 4])","861","        assert_array_equal(test, [0, 2, 3])","862","","863","        train, test = next(splits)","864","        assert_array_equal(train, [0, 2, 3])","865","        assert_array_equal(test, [1, 4])","866","","867","        train, test = next(splits)","868","        assert_array_equal(train, [2, 3])","869","        assert_array_equal(test, [0, 1, 4])","870","","871","        train, test = next(splits)","872","        assert_array_equal(train, [0, 1, 4])","873","        assert_array_equal(test, [2, 3])","874","","875","        assert_raises(StopIteration, next, splits)","876","","877",""],"delete":[]}],"doc\/modules\/cross_validation.rst":[{"add":["265","Repeated K-Fold","266","---------------","267","","268",":class:`RepeatedKFold` repeats K-Fold n times. It can be used when one","269","requires to run :class:`KFold` n times, producing different splits in","270","each repetition.","271","","272","Example of 2-fold K-Fold repeated 2 times::","273","","274","  >>> import numpy as np","275","  >>> from sklearn.model_selection import RepeatedKFold","276","  >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])","277","  >>> random_state = 12883823","278","  >>> rkf = RepeatedKFold(n_splits=2, n_repeats=2, random_state=random_state)","279","  >>> for train, test in rkf.split(X):","280","  ...     print(\"%s %s\" % (train, test))","281","  ...","282","  [2 3] [0 1]","283","  [0 1] [2 3]","284","  [0 2] [1 3]","285","  [1 3] [0 2]","286","","287","","288","Similarly, :class:`RepeatedStratifiedKFold` repeats Stratified K-Fold n times","289","with different randomization in each repetition.","290","","291","","438",":class:`RepeatedStratifiedKFold` can be used to repeat Stratified K-Fold n times","439","with different randomization in each repetition.","440","","441",""],"delete":[]}],"doc\/whats_new.rst":[{"add":["43","   - Added the :class:`sklearn.model_selection.RepeatedKFold` and","44","     :class:`sklearn.model_selection.RepeatedStratifiedKFold`.","45","     :issue:`8120` by `Neeraj Gangwar`_.","46","","5010","","5011",".. _Neeraj Gangwar: http:\/\/neerajgangwar.in"],"delete":[]}]}},"2f1c9786cc852e90799350d6fa4c6c335565c528":{"changes":{"sklearn\/tree\/tests\/test_export.py":"MODIFY","sklearn\/tree\/export.py":"MODIFY","doc\/whats_new.rst":"MODIFY"},"diff":{"sklearn\/tree\/tests\/test_export.py":[{"add":["11","from sklearn.utils.testing import assert_raise_message","221","    # Check if it errors when length of feature_names","222","    # mismatches with number of features","223","    message = (\"Length of feature_names, \"","224","               \"1 does not match number of features, 2\")","225","    assert_raise_message(ValueError, message, export_graphviz, clf, None,","226","                         feature_names=[\"a\"])","227","","228","    message = (\"Length of feature_names, \"","229","               \"3 does not match number of features, 2\")","230","    assert_raise_message(ValueError, message, export_graphviz, clf, None,","231","                         feature_names=[\"a\", \"b\", \"c\"])"],"delete":["220","    # Check feature_names error","221","    out = StringIO()","222","    assert_raises(IndexError, export_graphviz, clf, out, feature_names=[])"]}],"sklearn\/tree\/export.py":[{"add":["10","#          Li Li <aiki.nogard@gmail.com>","175","                alpha = int(np.round(255 * (sorted_values[0] -","176","                                            sorted_values[1]) \/","334","                    elif (tree.n_classes[0] == 1 and","335","                          len(np.unique(tree.value)) != 1):","404","        # Check length of feature_names before getting into the tree node","405","        # Raise error if length of feature_names does not match","406","        # n_features_ in the decision_tree","407","        if feature_names is not None:","408","            if len(feature_names) != decision_tree.n_features_:","409","                raise ValueError(\"Length of feature_names, %d \"","410","                                 \"does not match number of features, %d\"","411","                                 % (len(feature_names),","412","                                    decision_tree.n_features_))","413",""],"delete":["174","                alpha = int(np.round(255 * (sorted_values[0] - sorted_values[1]) \/","332","                    elif tree.n_classes[0] == 1 and len(np.unique(tree.value)) != 1:"]}],"doc\/whats_new.rst":[{"add":["278","   - Fixed a bug where :func:`sklearn.tree.export_graphviz` raised an error","279","     when the length of features_names does not match n_features in the decision","280","     tree.","281","     :issue:`8512` by :user:`Li Li <aikinogard>`.","282",""],"delete":[]}]}},"b89fcd303b40bddd44ee595ff135040be2b32544":{"changes":{"sklearn\/gaussian_process\/kernels.py":"MODIFY"},"diff":{"sklearn\/gaussian_process\/kernels.py":[{"add":["1854","        return np.apply_along_axis(self, 1, X).ravel()"],"delete":["1854","        return np.apply_along_axis(self, 1, X)[:, 0]"]}]}}}