{"d196ea3cc558723e25f089a4826924a6f4e1a62a":{"changes":{"sklearn\/linear_model\/coordinate_descent.py":"MODIFY","sklearn\/linear_model\/omp.py":"MODIFY","sklearn\/linear_model\/base.py":"MODIFY","sklearn\/linear_model\/bayes.py":"MODIFY","sklearn\/linear_model\/least_angle.py":"MODIFY","sklearn\/linear_model\/ridge.py":"MODIFY"},"diff":{"sklearn\/linear_model\/coordinate_descent.py":[{"add":["67","        This parameter is ignored when ``fit_intercept`` is set to False.","68","        If True, the regressors X will be normalized before regression by","69","        subtracting the mean and dividing by the l2-norm.","70","        If you wish to standardize, please use","71","        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``","72","        on an estimator with ``normalize=False``.","368","    See examples\/linear_model\/plot_lasso_coordinate_descent_path.py for an","369","    example.","531","        parameter.``alpha = 0`` is equivalent to an ordinary least square,","532","        solved by the :class:`LinearRegression` object. For numerical","547","        This parameter is ignored when ``fit_intercept`` is set to False.","548","        If True, the regressors X will be normalized before regression by","549","        subtracting the mean and dividing by the l2-norm.","550","        If you wish to standardize, please use","551","        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``","552","        on an estimator with ``normalize=False``.","793","        This parameter is ignored when ``fit_intercept`` is set to False.","794","        If True, the regressors X will be normalized before regression by","795","        subtracting the mean and dividing by the l2-norm.","796","        If you wish to standardize, please use","797","        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``","798","        on an estimator with ``normalize=False``.","1279","        This parameter is ignored when ``fit_intercept`` is set to False.","1280","        If True, the regressors X will be normalized before regression by","1281","        subtracting the mean and dividing by the l2-norm.","1282","        If you wish to standardize, please use","1283","        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``","1284","        on an estimator with ``normalize=False``.","1431","        This parameter is ignored when ``fit_intercept`` is set to False.","1432","        If True, the regressors X will be normalized before regression by","1433","        subtracting the mean and dividing by the l2-norm.","1434","        If you wish to standardize, please use","1435","        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``","1436","        on an estimator with ``normalize=False``.","1561","        This parameter is ignored when ``fit_intercept`` is set to False.","1562","        If True, the regressors X will be normalized before regression by","1563","        subtracting the mean and dividing by the l2-norm.","1564","        If you wish to standardize, please use","1565","        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``","1566","        on an estimator with ``normalize=False``.","1745","        This parameter is ignored when ``fit_intercept`` is set to False.","1746","        If True, the regressors X will be normalized before regression by","1747","        subtracting the mean and dividing by the l2-norm.","1748","        If you wish to standardize, please use","1749","        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``","1750","        on an estimator with ``normalize=False``.","1880","        This parameter is ignored when ``fit_intercept`` is set to False.","1881","        If True, the regressors X will be normalized before regression by","1882","        subtracting the mean and dividing by the l2-norm.","1883","        If you wish to standardize, please use","1884","        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``","1885","        on an estimator with ``normalize=False``.","2044","        This parameter is ignored when ``fit_intercept`` is set to False.","2045","        If True, the regressors X will be normalized before regression by","2046","        subtracting the mean and dividing by the l2-norm.","2047","        If you wish to standardize, please use","2048","        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``","2049","        on an estimator with ``normalize=False``."],"delete":["67","        If ``True``, the regressors X will be normalized before regression.","68","        This parameter is ignored when ``fit_intercept`` is set to ``False``.","69","        When the regressors are normalized, note that this makes the","70","        hyperparameters learnt more robust and almost independent of the number","71","        of samples. The same property is not valid for standardized data.","72","        However, if you wish to standardize, please use","73","        :class:`preprocessing.StandardScaler` before calling ``fit`` on an estimator","74","        with ``normalize=False``.","370","    See examples\/linear_model\/plot_lasso_coordinate_descent_path.py for an example.","532","        parameter.``alpha = 0`` is equivalent to an ordinary least square, solved","533","        by the :class:`LinearRegression` object. For numerical","548","        If ``True``, the regressors X will be normalized before regression.","549","        This parameter is ignored when ``fit_intercept`` is set to ``False``.","550","        When the regressors are normalized, note that this makes the","551","        hyperparameters learnt more robust and almost independent of the number","552","        of samples. The same property is not valid for standardized data.","553","        However, if you wish to standardize, please use","554","        :class:`preprocessing.StandardScaler` before calling ``fit`` on an estimator","555","        with ``normalize=False``.","796","        If ``True``, the regressors X will be normalized before regression.","797","        This parameter is ignored when ``fit_intercept`` is set to ``False``.","798","        When the regressors are normalized, note that this makes the","799","        hyperparameters learnt more robust and almost independent of the number","800","        of samples. The same property is not valid for standardized data.","801","        However, if you wish to standardize, please use","802","        :class:`preprocessing.StandardScaler` before calling ``fit`` on an estimator","803","        with ``normalize=False``.","1284","        If ``True``, the regressors X will be normalized before regression.","1285","        This parameter is ignored when ``fit_intercept`` is set to ``False``.","1286","        When the regressors are normalized, note that this makes the","1287","        hyperparameters learnt more robust and almost independent of the number","1288","        of samples. The same property is not valid for standardized data.","1289","        However, if you wish to standardize, please use","1290","        :class:`preprocessing.StandardScaler` before calling ``fit`` on an estimator","1291","        with ``normalize=False``.","1438","        If ``True``, the regressors X will be normalized before regression.","1439","        This parameter is ignored when ``fit_intercept`` is set to ``False``.","1440","        When the regressors are normalized, note that this makes the","1441","        hyperparameters learnt more robust and almost independent of the number","1442","        of samples. The same property is not valid for standardized data.","1443","        However, if you wish to standardize, please use","1444","        :class:`preprocessing.StandardScaler` before calling ``fit`` on an estimator","1445","        with ``normalize=False``.","1570","        If ``True``, the regressors X will be normalized before regression.","1571","        This parameter is ignored when ``fit_intercept`` is set to ``False``.","1572","        When the regressors are normalized, note that this makes the","1573","        hyperparameters learnt more robust and almost independent of the number","1574","        of samples. The same property is not valid for standardized data.","1575","        However, if you wish to standardize, please use","1576","        :class:`preprocessing.StandardScaler` before calling ``fit`` on an estimator","1577","        with ``normalize=False``.","1756","        If ``True``, the regressors X will be normalized before regression.","1757","        This parameter is ignored when ``fit_intercept`` is set to ``False``.","1758","        When the regressors are normalized, note that this makes the","1759","        hyperparameters learnt more robust and almost independent of the number","1760","        of samples. The same property is not valid for standardized data.","1761","        However, if you wish to standardize, please use","1762","        :class:`preprocessing.StandardScaler` before calling ``fit`` on an estimator","1763","        with ``normalize=False``.","1893","        If ``True``, the regressors X will be normalized before regression.","1894","        This parameter is ignored when ``fit_intercept`` is set to ``False``.","1895","        When the regressors are normalized, note that this makes the","1896","        hyperparameters learnt more robust and almost independent of the number","1897","        of samples. The same property is not valid for standardized data.","1898","        However, if you wish to standardize, please use","1899","        :class:`preprocessing.StandardScaler` before calling ``fit`` on an estimator","1900","        with ``normalize=False``.","2059","        If ``True``, the regressors X will be normalized before regression.","2060","        This parameter is ignored when ``fit_intercept`` is set to ``False``.","2061","        When the regressors are normalized, note that this makes the","2062","        hyperparameters learnt more robust and almost independent of the number","2063","        of samples. The same property is not valid for standardized data.","2064","        However, if you wish to standardize, please use","2065","        :class:`preprocessing.StandardScaler` before calling ``fit`` on an estimator","2066","        with ``normalize=False``."]}],"sklearn\/linear_model\/omp.py":[{"add":["560","        This parameter is ignored when ``fit_intercept`` is set to False.","561","        If True, the regressors X will be normalized before regression by","562","        subtracting the mean and dividing by the l2-norm.","563","        If you wish to standardize, please use","564","        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``","565","        on an estimator with ``normalize=False``.","696","        This parameter is ignored when ``fit_intercept`` is set to False.","697","        If True, the regressors X will be normalized before regression by","698","        subtracting the mean and dividing by the l2-norm.","699","        If you wish to standardize, please use","700","        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``","701","        on an estimator with ``normalize=False``.","761","        This parameter is ignored when ``fit_intercept`` is set to False.","762","        If True, the regressors X will be normalized before regression by","763","        subtracting the mean and dividing by the l2-norm.","764","        If you wish to standardize, please use","765","        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``","766","        on an estimator with ``normalize=False``."],"delete":["560","        If True, the regressors X will be normalized before regression.","561","        This parameter is ignored when `fit_intercept` is set to `False`.","562","        When the regressors are normalized, note that this makes the","563","        hyperparameters learnt more robust and almost independent of the number","564","        of samples. The same property is not valid for standardized data.","565","        However, if you wish to standardize, please use","566","        `preprocessing.StandardScaler` before calling `fit` on an estimator","567","        with `normalize=False`.","698","        If True, the regressors X will be normalized before regression.","699","        This parameter is ignored when `fit_intercept` is set to `False`.","700","        When the regressors are normalized, note that this makes the","701","        hyperparameters learnt more robust and almost independent of the number","702","        of samples. The same property is not valid for standardized data.","703","        However, if you wish to standardize, please use","704","        `preprocessing.StandardScaler` before calling `fit` on an estimator","705","        with `normalize=False`.","765","        If True, the regressors X will be normalized before regression.","766","        This parameter is ignored when `fit_intercept` is set to `False`.","767","        When the regressors are normalized, note that this makes the","768","        hyperparameters learnt more robust and almost independent of the number","769","        of samples. The same property is not valid for standardized data.","770","        However, if you wish to standardize, please use","771","        `preprocessing.StandardScaler` before calling `fit` on an estimator","772","        with `normalize=False`."]}],"sklearn\/linear_model\/base.py":[{"add":["412","        This parameter is ignored when ``fit_intercept`` is set to False.","413","        If True, the regressors X will be normalized before regression by","414","        subtracting the mean and dividing by the l2-norm.","415","        If you wish to standardize, please use","416","        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit`` on","417","        an estimator with ``normalize=False``."],"delete":["412","        If True, the regressors X will be normalized before regression.","413","        This parameter is ignored when `fit_intercept` is set to False.","414","        When the regressors are normalized, note that this makes the","415","        hyperparameters learnt more robust and almost independent of the number","416","        of samples. The same property is not valid for standardized data.","417","        However, if you wish to standardize, please use","418","        `preprocessing.StandardScaler` before calling `fit` on an estimator","419","        with `normalize=False`."]}],"sklearn\/linear_model\/bayes.py":[{"add":["66","        This parameter is ignored when ``fit_intercept`` is set to False.","67","        If True, the regressors X will be normalized before regression by","68","        subtracting the mean and dividing by the l2-norm.","69","        If you wish to standardize, please use","70","        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``","71","        on an estimator with ``normalize=False``.","329","        This parameter is ignored when ``fit_intercept`` is set to False.","330","        If True, the regressors X will be normalized before regression by","331","        subtracting the mean and dividing by the l2-norm.","332","        If you wish to standardize, please use","333","        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``","334","        on an estimator with ``normalize=False``."],"delete":["66","        If True, the regressors X will be normalized before regression.","67","        This parameter is ignored when `fit_intercept` is set to False.","68","        When the regressors are normalized, note that this makes the","69","        hyperparameters learnt more robust and almost independent of the number","70","        of samples. The same property is not valid for standardized data.","71","        However, if you wish to standardize, please use","72","        `preprocessing.StandardScaler` before calling `fit` on an estimator","73","        with `normalize=False`.","331","        If True, the regressors X will be normalized before regression.","332","        This parameter is ignored when `fit_intercept` is set to False.","333","        When the regressors are normalized, note that this makes the","334","        hyperparameters learnt more robust and almost independent of the number","335","        of samples. The same property is not valid for standardized data.","336","        However, if you wish to standardize, please use","337","        `preprocessing.StandardScaler` before calling `fit` on an estimator","338","        with `normalize=False`."]}],"sklearn\/linear_model\/least_angle.py":[{"add":["516","        This parameter is ignored when ``fit_intercept`` is set to False.","517","        If True, the regressors X will be normalized before regression by","518","        subtracting the mean and dividing by the l2-norm.","519","        If you wish to standardize, please use","520","        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``","521","        on an estimator with ``normalize=False``.","747","        This parameter is ignored when ``fit_intercept`` is set to False.","748","        If True, the regressors X will be normalized before regression by","749","        subtracting the mean and dividing by the l2-norm.","750","        If you wish to standardize, please use","751","        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``","752","        on an estimator with ``normalize=False``.","902","        This parameter is ignored when ``fit_intercept`` is set to False.","903","        If True, the regressors X will be normalized before regression by","904","        subtracting the mean and dividing by the l2-norm.","905","        If you wish to standardize, please use","906","        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``","907","        on an estimator with ``normalize=False``.","986","        This parameter is ignored when ``fit_intercept`` is set to False.","987","        If True, the regressors X will be normalized before regression by","988","        subtracting the mean and dividing by the l2-norm.","989","        If you wish to standardize, please use","990","        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``","991","        on an estimator with ``normalize=False``.","1198","        This parameter is ignored when ``fit_intercept`` is set to False.","1199","        If True, the regressors X will be normalized before regression by","1200","        subtracting the mean and dividing by the l2-norm.","1201","        If you wish to standardize, please use","1202","        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``","1203","        on an estimator with ``normalize=False``.","1331","        This parameter is ignored when ``fit_intercept`` is set to False.","1332","        If True, the regressors X will be normalized before regression by","1333","        subtracting the mean and dividing by the l2-norm.","1334","        If you wish to standardize, please use","1335","        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``","1336","        on an estimator with ``normalize=False``."],"delete":["516","        If True, the regressors X will be normalized before regression.","517","        This parameter is ignored when `fit_intercept` is set to False.","518","        When the regressors are normalized, note that this makes the","519","        hyperparameters learnt more robust and almost independent of the number","520","        of samples. The same property is not valid for standardized data.","521","        However, if you wish to standardize, please use","522","        `preprocessing.StandardScaler` before calling `fit` on an estimator","523","        with `normalize=False`.","749","        If True, the regressors X will be normalized before regression.","750","        This parameter is ignored when `fit_intercept` is set to False.","751","        When the regressors are normalized, note that this makes the","752","        hyperparameters learnt more robust and almost independent of the number","753","        of samples. The same property is not valid for standardized data.","754","        However, if you wish to standardize, please use","755","        `preprocessing.StandardScaler` before calling `fit` on an estimator","756","        with `normalize=False`.","906","        If True, the regressors X will be normalized before regression.","907","        This parameter is ignored when `fit_intercept` is set to False.","908","        When the regressors are normalized, note that this makes the","909","        hyperparameters learnt more robust and almost independent of the number","910","        of samples. The same property is not valid for standardized data.","911","        However, if you wish to standardize, please use","912","        `preprocessing.StandardScaler` before calling `fit` on an estimator","913","        with `normalize=False`.","992","        If True, the regressors X will be normalized before regression.","993","        This parameter is ignored when `fit_intercept` is set to False.","994","        When the regressors are normalized, note that this makes the","995","        hyperparameters learnt more robust and almost independent of the number","996","        of samples. The same property is not valid for standardized data.","997","        However, if you wish to standardize, please use","998","        `preprocessing.StandardScaler` before calling `fit` on an estimator","999","        with `normalize=False`.","1206","        If True, the regressors X will be normalized before regression.","1207","        This parameter is ignored when `fit_intercept` is set to False.","1208","        When the regressors are normalized, note that this makes the","1209","        hyperparameters learnt more robust and almost independent of the number","1210","        of samples. The same property is not valid for standardized data.","1211","        However, if you wish to standardize, please use","1212","        `preprocessing.StandardScaler` before calling `fit` on an estimator","1213","        with `normalize=False`.","1341","        If True, the regressors X will be normalized before regression.","1342","        This parameter is ignored when `fit_intercept` is set to False.","1343","        When the regressors are normalized, note that this makes the","1344","        hyperparameters learnt more robust and almost independent of the number","1345","        of samples. The same property is not valid for standardized data.","1346","        However, if you wish to standardize, please use","1347","        `preprocessing.StandardScaler` before calling `fit` on an estimator","1348","        with `normalize=False`."]}],"sklearn\/linear_model\/ridge.py":[{"add":["534","        This parameter is ignored when ``fit_intercept`` is set to False.","535","        If True, the regressors X will be normalized before regression by","536","        subtracting the mean and dividing by the l2-norm.","537","        If you wish to standardize, please use","538","        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``","539","        on an estimator with ``normalize=False``.","686","        This parameter is ignored when ``fit_intercept`` is set to False.","687","        If True, the regressors X will be normalized before regression by","688","        subtracting the mean and dividing by the l2-norm.","689","        If you wish to standardize, please use","690","        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``","691","        on an estimator with ``normalize=False``.","1139","        This parameter is ignored when ``fit_intercept`` is set to False.","1140","        If True, the regressors X will be normalized before regression by","1141","        subtracting the mean and dividing by the l2-norm.","1142","        If you wish to standardize, please use","1143","        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``","1144","        on an estimator with ``normalize=False``.","1240","        This parameter is ignored when ``fit_intercept`` is set to False.","1241","        If True, the regressors X will be normalized before regression by","1242","        subtracting the mean and dividing by the l2-norm.","1243","        If you wish to standardize, please use","1244","        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``","1245","        on an estimator with ``normalize=False``."],"delete":["534","        If True, the regressors X will be normalized before regression.","535","        This parameter is ignored when `fit_intercept` is set to False.","536","        When the regressors are normalized, note that this makes the","537","        hyperparameters learnt more robust and almost independent of the number","538","        of samples. The same property is not valid for standardized data.","539","        However, if you wish to standardize, please use","540","        `preprocessing.StandardScaler` before calling `fit` on an estimator","541","        with `normalize=False`.","688","        If True, the regressors X will be normalized before regression.","689","        This parameter is ignored when `fit_intercept` is set to False.","690","        When the regressors are normalized, note that this makes the","691","        hyperparameters learnt more robust and almost independent of the number","692","        of samples. The same property is not valid for standardized data.","693","        However, if you wish to standardize, please use","694","        `preprocessing.StandardScaler` before calling `fit` on an estimator","695","        with `normalize=False`.","1143","        If True, the regressors X will be normalized before regression.","1144","        This parameter is ignored when `fit_intercept` is set to False.","1145","        When the regressors are normalized, note that this makes the","1146","        hyperparameters learnt more robust and almost independent of the number","1147","        of samples. The same property is not valid for standardized data.","1148","        However, if you wish to standardize, please use","1149","        `preprocessing.StandardScaler` before calling `fit` on an estimator","1150","        with `normalize=False`.","1246","        If True, the regressors X will be normalized before regression.","1247","        This parameter is ignored when `fit_intercept` is set to False.","1248","        When the regressors are normalized, note that this makes the","1249","        hyperparameters learnt more robust and almost independent of the number","1250","        of samples. The same property is not valid for standardized data.","1251","        However, if you wish to standardize, please use","1252","        `preprocessing.StandardScaler` before calling `fit` on an estimator","1253","        with `normalize=False`."]}]}},"38c7e93b1edcbfb85060cf7c14cca3ab47b9267c":{"changes":{"sklearn\/decomposition\/online_lda.py":"MODIFY","sklearn\/decomposition\/tests\/test_online_lda.py":"MODIFY"},"diff":{"sklearn\/decomposition\/online_lda.py":[{"add":["561","                        print('iteration: %d of max_iter: %d, perplexity: %.4f'","562","                              % (i + 1, max_iter, bound))","567","","568","                elif self.verbose:","569","                    print('iteration: %d of max_iter: %d' % (i + 1, max_iter))"],"delete":["561","                        print('iteration: %d, perplexity: %.4f'","562","                              % (i + 1, bound))"]}],"sklearn\/decomposition\/tests\/test_online_lda.py":[{"add":["0","import sys","1","","13","from sklearn.utils.testing import assert_equal","23","from sklearn.externals.six import StringIO","383","def check_verbosity(verbose, evaluate_every, expected_lines,","384","                    expected_perplexities):","385","    n_components, X = _build_sparse_mtx()","386","    lda = LatentDirichletAllocation(n_components=n_components, max_iter=3,","387","                                    learning_method='batch',","388","                                    verbose=verbose,","389","                                    evaluate_every=evaluate_every,","390","                                    random_state=0)","391","    out = StringIO()","392","    old_out, sys.stdout = sys.stdout, out","393","    try:","394","        lda.fit(X)","395","    finally:","396","        sys.stdout = old_out","397","","398","    n_lines = out.getvalue().count('\\n')","399","    n_perplexity = out.getvalue().count('perplexity')","400","    assert_equal(expected_lines, n_lines)","401","    assert_equal(expected_perplexities, n_perplexity)","402","","403","","404","def test_verbosity():","405","    for verbose, evaluate_every, expected_lines, expected_perplexities in [","406","        (False, 1, 0, 0),","407","        (False, 0, 0, 0),","408","        (True, 0, 3, 0),","409","        (True, 1, 3, 3),","410","        (True, 2, 3, 1),","411","    ]:","412","        yield (check_verbosity, verbose, evaluate_every, expected_lines,","413","               expected_perplexities)","414","","415",""],"delete":[]}]}},"1652a709190b37423b2ccba45cc65d32e7d326b4":{"changes":{"sklearn\/base.py":"MODIFY","doc\/developers\/contributing.rst":"MODIFY"},"diff":{"sklearn\/base.py":[{"add":["360","        The coefficient R^2 is defined as (1 - u\/v), where u is the residual","361","        sum of squares ((y_true - y_pred) ** 2).sum() and v is the total","363","        The best possible score is 1.0 and it can be negative (because the"],"delete":["360","        The coefficient R^2 is defined as (1 - u\/v), where u is the regression","361","        sum of squares ((y_true - y_pred) ** 2).sum() and v is the residual","363","        Best possible score is 1.0 and it can be negative (because the"]}],"doc\/developers\/contributing.rst":[{"add":["745","Python versions supported","746","-------------------------","748","All scikit-learn code should work unchanged in both Python 2.7 and 3.4 or","749","newer. Since Python 3.x is not backwards compatible, that may require changes","750","to code and it certainly requires testing on both 2.7 and 3.4 or newer."],"delete":["745","Python 3.x support","746","------------------","748","All scikit-learn code should work unchanged in both Python 2.[67]","749","and 3.2 or newer. Since Python 3.x is not backwards compatible,","750","that may require changes to code and it certainly requires testing","751","on both 2.7 and 3.2 or newer."]}]}},"9927982c4eb5773a33f8d0ef5ca22cfc7806e926":{"changes":{"examples\/svm\/plot_separating_hyperplane_unbalanced.py":"MODIFY","examples\/svm\/plot_separating_hyperplane.py":"MODIFY"},"diff":{"examples\/svm\/plot_separating_hyperplane_unbalanced.py":[{"add":["44","# fit the model and get the separating hyperplane using weighted classes","52","# plot the decision functions for both classifiers","53","ax = plt.gca()","54","xlim = ax.get_xlim()","55","ylim = ax.get_ylim()","56","","57","# create grid to evaluate model","58","xx = np.linspace(xlim[0], xlim[1], 30)","59","yy = np.linspace(ylim[0], ylim[1], 30)","60","YY, XX = np.meshgrid(yy, xx)","61","xy = np.vstack([XX.ravel(), YY.ravel()]).T","62","","63","# get the separating hyperplane","64","Z = clf.decision_function(xy).reshape(XX.shape)","65","","66","# plot decision boundary and margins","67","a = ax.contour(XX, YY, Z, colors='k', levels=[0], alpha=0.5, linestyles=['-'])","68","","69","# get the separating hyperplane for weighted classes","70","Z = wclf.decision_function(xy).reshape(XX.shape)","71","","72","# plot decision boundary and margins for weighted classes","73","b = ax.contour(XX, YY, Z, colors='r', levels=[0], alpha=0.5, linestyles=['-'])","74","","75","plt.legend([a.collections[0], b.collections[0]], [\"non weighted\", \"weighted\"],","76","           loc=\"upper right\")"],"delete":["31","#from sklearn.linear_model import SGDClassifier","45","w = clf.coef_[0]","46","a = -w[0] \/ w[1]","47","xx = np.linspace(-5, 5)","48","yy = a * xx - clf.intercept_[0] \/ w[1]","49","","50","","51","# get the separating hyperplane using weighted classes","55","ww = wclf.coef_[0]","56","wa = -ww[0] \/ ww[1]","57","wyy = wa * xx - wclf.intercept_[0] \/ ww[1]","58","","60","h0 = plt.plot(xx, yy, 'k-', label='no weights')","61","h1 = plt.plot(xx, wyy, 'k--', label='with weights')","65","plt.axis('tight')","66","plt.show()"]}],"examples\/svm\/plot_separating_hyperplane.py":[{"add":["14","from sklearn.datasets import make_blobs","15","","18","X, y = make_blobs(n_samples=40, centers=2, random_state=12, cluster_std=0.35)","22","clf.fit(X, y)","24","plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired)","26","# plot the decision function","27","ax = plt.gca()","28","xlim = ax.get_xlim()","29","ylim = ax.get_ylim()","31","# create grid to evaluate model","32","xx = np.linspace(xlim[0], xlim[1], 30)","33","yy = np.linspace(ylim[0], ylim[1], 30)","34","YY, XX = np.meshgrid(yy, xx)","35","xy = np.vstack([XX.ravel(), YY.ravel()]).T","36","Z = clf.decision_function(xy).reshape(XX.shape)","38","# plot decision boundary and margins","39","ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,","40","           linestyles=['--', '-', '--'])","41","# plot support vectors","42","ax.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=100,","43","           linewidth=1, facecolors='none')"],"delete":["16","np.random.seed(0)","17","X = np.r_[np.random.randn(20, 2) - [2, 2], np.random.randn(20, 2) + [2, 2]]","18","Y = [0] * 20 + [1] * 20","22","clf.fit(X, Y)","24","# get the separating hyperplane","25","w = clf.coef_[0]","26","a = -w[0] \/ w[1]","27","xx = np.linspace(-5, 5)","28","yy = a * xx - (clf.intercept_[0]) \/ w[1]","30","# plot the parallels to the separating hyperplane that pass through the","31","# support vectors","32","b = clf.support_vectors_[0]","33","yy_down = a * xx + (b[1] - a * b[0])","34","b = clf.support_vectors_[-1]","35","yy_up = a * xx + (b[1] - a * b[0])","37","# plot the line, the points, and the nearest vectors to the plane","38","plt.plot(xx, yy, 'k-')","39","plt.plot(xx, yy_down, 'k--')","40","plt.plot(xx, yy_up, 'k--')","42","plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],","43","            s=80, facecolors='none')","44","plt.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.Paired)","45","","46","plt.axis('tight')","47","plt.show()"]}]}},"8981b25c1798b52618b730d95a870ec347e31c06":{"changes":{"sklearn\/svm\/classes.py":"MODIFY"},"diff":{"sklearn\/svm\/classes.py":[{"add":[],"delete":["1086",""]}]}},"89962f0c5bb4bea56aaf0e30bd8d2ea3789a4a46":{"changes":{"sklearn\/linear_model\/tests\/test_coordinate_descent.py":"MODIFY","sklearn\/linear_model\/coordinate_descent.py":"MODIFY","sklearn\/linear_model\/omp.py":"MODIFY","sklearn\/linear_model\/tests\/test_base.py":"MODIFY","sklearn\/linear_model\/randomized_l1.py":"MODIFY","sklearn\/linear_model\/base.py":"MODIFY","sklearn\/linear_model\/bayes.py":"MODIFY","sklearn\/linear_model\/least_angle.py":"MODIFY","sklearn\/linear_model\/ridge.py":"MODIFY"},"diff":{"sklearn\/linear_model\/tests\/test_coordinate_descent.py":[{"add":["663","    # With check_input=False, an exhaustive check is not made on y but its","664","    # dtype is still cast in _preprocess_data to X's dtype. So the test should","665","    # pass anyway","667","    clf.fit(X, y, check_input=False)"],"delete":["664","    clf.fit(X, y, check_input=True)","665","    # Check that an error is raised if data is provided in the wrong dtype,","666","    # because of check bypassing","667","    assert_raises(ValueError, clf.fit, X, y, check_input=False)","668",""]}],"sklearn\/linear_model\/coordinate_descent.py":[{"add":["655","            Target. Will be cast to X's dtype if necessary","1682","            Target. Will be cast to X's dtype if necessary"],"delete":["655","            Target","1682","            Target"]}],"sklearn\/linear_model\/omp.py":[{"add":["619","            Target values. Will be cast to X's dtype if necessary","837","            Target values. Will be cast to X's dtype if necessary"],"delete":["619","            Target values.","837","            Target values."]}],"sklearn\/linear_model\/tests\/test_base.py":[{"add":["326","def test_dtype_preprocess_data():","327","    n_samples = 200","328","    n_features = 2","329","    X = rng.rand(n_samples, n_features)","330","    y = rng.rand(n_samples)","331","","332","    X_32 = np.asarray(X, dtype=np.float32)","333","    y_32 = np.asarray(y, dtype=np.float32)","334","    X_64 = np.asarray(X, dtype=np.float64)","335","    y_64 = np.asarray(y, dtype=np.float64)","336","","337","    for fit_intercept in [True, False]:","338","        for normalize in [True, False]:","339","","340","            Xt_32, yt_32, X_mean_32, y_mean_32, X_norm_32 = _preprocess_data(","341","                X_32, y_32, fit_intercept=fit_intercept, normalize=normalize,","342","                return_mean=True)","343","","344","            Xt_64, yt_64, X_mean_64, y_mean_64, X_norm_64 = _preprocess_data(","345","                X_64, y_64, fit_intercept=fit_intercept, normalize=normalize,","346","                return_mean=True)","347","","348","            Xt_3264, yt_3264, X_mean_3264, y_mean_3264, X_norm_3264 = (","349","                _preprocess_data(X_32, y_64, fit_intercept=fit_intercept,","350","                                 normalize=normalize, return_mean=True))","351","","352","            Xt_6432, yt_6432, X_mean_6432, y_mean_6432, X_norm_6432 = (","353","                _preprocess_data(X_64, y_32, fit_intercept=fit_intercept,","354","                                 normalize=normalize, return_mean=True))","355","","356","            assert_equal(Xt_32.dtype, np.float32)","357","            assert_equal(yt_32.dtype, np.float32)","358","            assert_equal(X_mean_32.dtype, np.float32)","359","            assert_equal(y_mean_32.dtype, np.float32)","360","            assert_equal(X_norm_32.dtype, np.float32)","361","","362","            assert_equal(Xt_64.dtype, np.float64)","363","            assert_equal(yt_64.dtype, np.float64)","364","            assert_equal(X_mean_64.dtype, np.float64)","365","            assert_equal(y_mean_64.dtype, np.float64)","366","            assert_equal(X_norm_64.dtype, np.float64)","367","","368","            assert_equal(Xt_3264.dtype, np.float32)","369","            assert_equal(yt_3264.dtype, np.float32)","370","            assert_equal(X_mean_3264.dtype, np.float32)","371","            assert_equal(y_mean_3264.dtype, np.float32)","372","            assert_equal(X_norm_3264.dtype, np.float32)","373","","374","            assert_equal(Xt_6432.dtype, np.float64)","375","            assert_equal(yt_6432.dtype, np.float64)","376","            assert_equal(X_mean_6432.dtype, np.float64)","377","            assert_equal(y_mean_6432.dtype, np.float64)","378","            assert_equal(X_norm_6432.dtype, np.float64)","379","","380","            assert_equal(X_32.dtype, np.float32)","381","            assert_equal(y_32.dtype, np.float32)","382","            assert_equal(X_64.dtype, np.float64)","383","            assert_equal(y_64.dtype, np.float64)","384","","385","            assert_array_almost_equal(Xt_32, Xt_64)","386","            assert_array_almost_equal(yt_32, yt_64)","387","            assert_array_almost_equal(X_mean_32, X_mean_64)","388","            assert_array_almost_equal(y_mean_32, y_mean_64)","389","            assert_array_almost_equal(X_norm_32, X_norm_64)","390","","391",""],"delete":[]}],"sklearn\/linear_model\/randomized_l1.py":[{"add":["84","            Target values. Will be cast to X's dtype if necessary"],"delete":["84","            Target values."]}],"sklearn\/linear_model\/base.py":[{"add":["160","    centered. This function also systematically makes y consistent with X.dtype","168","    y = np.asarray(y, dtype=X.dtype)","174","                X_offset[:] = X.dtype.type(0)","204","        if y.ndim == 1:","205","            y_offset = X.dtype.type(0)","206","        else:","207","            y_offset = np.zeros(y.shape[1], dtype=X.dtype)","466","            Target values. Will be cast to X's dtype if necessary"],"delete":["160","    centered.","173","                X_offset[:] = 0","203","        y_offset = 0. if y.ndim == 1 else np.zeros(y.shape[1], dtype=X.dtype)","462","            Target values"]}],"sklearn\/linear_model\/bayes.py":[{"add":["150","            Target values. Will be cast to X's dtype if necessary","422","            Target values (integers). Will be cast to X's dtype if necessary"],"delete":["150","            Target values","422","            Target values (integers)"]}],"sklearn\/linear_model\/least_angle.py":[{"add":["1457","            target values. Will be cast to X's dtype if necessary"],"delete":["1457","            target values."]}],"sklearn\/linear_model\/ridge.py":[{"add":["977","            Target values. Will be cast to X's dtype if necessary","1096","            Target values. Will be cast to X's dtype if necessary","1338","            Target values. Will be cast to X's dtype if necessary"],"delete":["977","            Target values","1096","            Target values","1338","            Target values."]}]}},"2cd1220ebe1dab582dba86947a51364616804059":{"changes":{"doc\/whats_new.rst":"MODIFY","sklearn\/cluster\/dbscan_.py":"MODIFY","sklearn\/cluster\/tests\/test_dbscan.py":"MODIFY"},"diff":{"doc\/whats_new.rst":[{"add":["154","   - Fixed a bug where :class:`sklearn.cluster.DBSCAN` gives incorrect ","155","     result when input is a precomputed sparse matrix with initial","156","     rows all zero.","157","     :issue:`8306` by :user:`Akshay Gupta <Akshay0724>`"],"delete":[]}],"sklearn\/cluster\/dbscan_.py":[{"add":["126","        masked_indptr = np.concatenate(([0], np.cumsum(X_mask)))[X.indptr[1:]]","127",""],"delete":["126","        masked_indptr = np.cumsum(X_mask)[X.indptr[1:] - 1]"]}],"sklearn\/cluster\/tests\/test_dbscan.py":[{"add":["352","","353","","354","def test_dbscan_precomputed_metric_with_initial_rows_zero():","355","    # sample matrix with initial two row all zero","356","    ar = np.array([","357","        [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],","358","        [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],","359","        [0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0],","360","        [0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0],","361","        [0.0, 0.0, 0.1, 0.1, 0.0, 0.0, 0.3],","362","        [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1],","363","        [0.0, 0.0, 0.0, 0.0, 0.3, 0.1, 0.0]","364","    ])","365","    matrix = sparse.csr_matrix(ar)","366","    labels = DBSCAN(eps=0.2, metric='precomputed',","367","                    min_samples=2).fit(matrix).labels_","368","    assert_array_equal(labels, [-1, -1,  0,  0,  0,  1,  1])"],"delete":[]}]}},"aaebee1f6b8b8e821130b1057c8ec1431b0d1da2":{"changes":{"sklearn\/feature_selection\/tests\/test_mutual_info.py":"MODIFY","sklearn\/feature_selection\/mutual_info_.py":"MODIFY"},"diff":{"sklearn\/feature_selection\/tests\/test_mutual_info.py":[{"add":["7","                                   assert_false, assert_raises, assert_equal,","8","                                   assert_allclose, assert_greater)","161","    mi = mutual_info_classif(X, y, discrete_features=[2], n_neighbors=3,","162","                             random_state=0)","164","    for n_neighbors in [5, 7, 9]:","165","        mi_nn = mutual_info_classif(X, y, discrete_features=[2],","166","                                    n_neighbors=n_neighbors, random_state=0)","167","        # Check that the continuous values have an higher MI with greater","168","        # n_neighbors","169","        assert_greater(mi_nn[0], mi[0])","170","        assert_greater(mi_nn[1], mi[1])","171","        # The n_neighbors should not have any effect on the discrete value","172","        # The MI should be the same","173","        assert_equal(mi_nn[2], mi[2])"],"delete":["7","                                   assert_false, assert_raises, assert_equal)","160","    mi = mutual_info_classif(X, y, discrete_features=[2], random_state=0)"]}],"sklearn\/feature_selection\/mutual_info_.py":[{"add":["283","    mi = [_compute_mi(x, y, discrete_feature, discrete_target, n_neighbors) for"],"delete":["283","    mi = [_compute_mi(x, y, discrete_feature, discrete_target) for"]}]}},"e27242a62d18425886e540c213da044f209d43a8":{"changes":{"sklearn\/manifold\/isomap.py":"MODIFY","doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/manifold\/tests\/test_isomap.py":"MODIFY","sklearn\/manifold\/locally_linear.py":"MODIFY"},"diff":{"sklearn\/manifold\/isomap.py":[{"add":["102","        X = check_array(X, accept_sparse='csr')"],"delete":["102","        X = check_array(X)"]}],"doc\/whats_new\/v0.20.rst":[{"add":["338","- Support sparse input in :meth:`manifold.Isomap.fit`. :issue:`8554` by","339","  :user:`Leland McInnes <lmcinnes>`.","340",""],"delete":[]}],"sklearn\/manifold\/tests\/test_isomap.py":[{"add":["12","from scipy.sparse import rand as sparse_rand","13","","126","","127","","128","def test_sparse_input():","129","    X = sparse_rand(100, 3, density=0.1, format='csr')","130","","131","    # Should not error","132","    for eigen_solver in eigen_solvers:","133","        for path_method in path_methods:","134","            clf = manifold.Isomap(n_components=2,","135","                                  eigen_solver=eigen_solver,","136","                                  path_method=path_method)","137","            clf.fit(X)"],"delete":[]}],"sklearn\/manifold\/locally_linear.py":[{"add":["71","    X : {array-like, NearestNeighbors}","73","        numpy array or a NearestNeighbors object.","195","    X : {array-like, NearestNeighbors}","197","        numpy array or a NearestNeighbors object."],"delete":["71","    X : {array-like, sparse matrix, BallTree, KDTree, NearestNeighbors}","73","        numpy array, sparse array, precomputed tree, or NearestNeighbors","74","        object.","196","    X : {array-like, sparse matrix, BallTree, KDTree, NearestNeighbors}","198","        numpy array, sparse array, precomputed tree, or NearestNeighbors","199","        object."]}]}},"93871e2e614630ff5ff0c08903fa9591ba5bb2dc":{"changes":{"doc\/whats_new.rst":"MODIFY","sklearn\/manifold\/_barnes_hut_tsne.pyx":"MODIFY","sklearn\/tests\/test_multioutput.py":"MODIFY","sklearn\/cluster\/_k_means_elkan.pyx":"MODIFY","doc\/modules\/neural_networks_unsupervised.rst":"MODIFY"},"diff":{"doc\/whats_new.rst":[{"add":["755","     converts single output regressors to multi-output regressors by fitting"],"delete":["755","     converts single output regressors to multi-ouput regressors by fitting"]}],"sklearn\/manifold\/_barnes_hut_tsne.pyx":[{"add":["82","    # Number of dimensions in the output"],"delete":["82","    # Number of dimensions in the ouput"]}],"sklearn\/tests\/test_multioutput.py":[{"add":["195","def test_multi_output_classifiation_partial_fit_no_first_classes_exception():"],"delete":["195","def test_mutli_output_classifiation_partial_fit_no_first_classes_exception():"]}],"sklearn\/cluster\/_k_means_elkan.pyx":[{"add":["20","cdef floating euclidean_dist(floating* a, floating* b, int n_features) nogil:","91","        d_c = euclidean_dist(x, centers, n_features)","96","                dist = euclidean_dist(x, c, n_features)","199","                        upper_bound = euclidean_dist(x_p, centers_p + label * n_features, n_features)","208","                        distance = euclidean_dist(x_p, centers_p + center_index * n_features, n_features)"],"delete":["20","cdef floating euclidian_dist(floating* a, floating* b, int n_features) nogil:","91","        d_c = euclidian_dist(x, centers, n_features)","96","                dist = euclidian_dist(x, c, n_features)","199","                        upper_bound = euclidian_dist(x_p, centers_p + label * n_features, n_features)","208","                        distance = euclidian_dist(x_p, centers_p + center_index * n_features, n_features)"]}],"doc\/modules\/neural_networks_unsupervised.rst":[{"add":["55","visible and hidden unit, omitted from the image for simplicity."],"delete":["55","visible and hidden unit, ommited from the image for simplicity."]}]}},"f548128e835171f83a75f3944b83b21b5590503b":{"changes":{"sklearn\/mixture\/tests\/test_gaussian_mixture.py":"MODIFY","sklearn\/mixture\/base.py":"MODIFY"},"diff":{"sklearn\/mixture\/tests\/test_gaussian_mixture.py":[{"add":["651","                             'Initialization %d did not converge. '"],"delete":["651","                             'Initialization %d did not converged. '"]}],"sklearn\/mixture\/base.py":[{"add":["232","            warnings.warn('Initialization %d did not converge. '"],"delete":["232","            warnings.warn('Initialization %d did not converged. '"]}]}},"754109c78d90cc792f0944aaaa11371175b09277":{"changes":{"sklearn\/model_selection\/_split.py":"MODIFY","sklearn\/model_selection\/tests\/test_search.py":"MODIFY","sklearn\/model_selection\/tests\/test_split.py":"MODIFY","sklearn\/model_selection\/tests\/test_validation.py":"MODIFY"},"diff":{"sklearn\/model_selection\/_split.py":[{"add":["190","            raise ValueError(\"The 'X' parameter should not be None.\")","261","            raise ValueError(\"The 'X' parameter should not be None.\")","479","            raise ValueError(\"The 'groups' parameter should not be None.\")","767","    >>> logo.get_n_splits(groups=groups) # 'groups' is always required","768","    2","789","            raise ValueError(\"The 'groups' parameter should not be None.\")","800","    def get_n_splits(self, X=None, y=None, groups=None):","805","        X : object, optional","808","        y : object, optional","813","            train\/test set. This 'groups' parameter must always be specified to","814","            calculate the number of splits, though the other parameters can be","815","            omitted.","823","            raise ValueError(\"The 'groups' parameter should not be None.\")","824","        groups = check_array(groups, ensure_2d=False, dtype=None)","859","    >>> lpgo.get_n_splits(groups=groups)  # 'groups' is always required","860","    3","888","            raise ValueError(\"The 'groups' parameter should not be None.\")","904","    def get_n_splits(self, X=None, y=None, groups=None):","909","        X : object, optional","912","        y : object, optional","917","            train\/test set. This 'groups' parameter must always be specified to","918","            calculate the number of splits, though the other parameters can be","919","            omitted.","927","            raise ValueError(\"The 'groups' parameter should not be None.\")","1326","            raise ValueError(\"The 'groups' parameter should not be None.\")"],"delete":["190","            raise ValueError(\"The X parameter should not be None\")","261","            raise ValueError(\"The X parameter should not be None\")","479","            raise ValueError(\"The groups parameter should not be None\")","787","            raise ValueError(\"The groups parameter should not be None\")","798","    def get_n_splits(self, X, y, groups):","803","        X : object","806","        y : object","811","            train\/test set.","819","            raise ValueError(\"The groups parameter should not be None\")","881","            raise ValueError(\"The groups parameter should not be None\")","897","    def get_n_splits(self, X, y, groups):","902","        X : object","904","            ``np.zeros(n_samples)`` may be used as a placeholder.","906","        y : object","908","            ``np.zeros(n_samples)`` may be used as a placeholder.","912","            train\/test set.","920","            raise ValueError(\"The groups parameter should not be None\")","922","        X, y, groups = indexable(X, y, groups)","1320","            raise ValueError(\"The groups parameter should not be None\")"]}],"sklearn\/model_selection\/tests\/test_search.py":[{"add":["319","                             \"The 'groups' parameter should not be None.\","],"delete":["319","                             \"The groups parameter should not be None\","]}],"sklearn\/model_selection\/tests\/test_split.py":[{"add":["191","    # ValueError for get_n_splits methods","192","    msg = \"The 'X' parameter should not be None.\"","193","    assert_raise_message(ValueError, msg,","194","                         loo.get_n_splits, None, y, groups)","195","    assert_raise_message(ValueError, msg,","196","                         lpo.get_n_splits, None, y, groups)","197","","766","    # check get_n_splits() with dummy parameters","767","    assert_equal(logo.get_n_splits(None, None, ['a', 'b', 'c', 'b', 'c']), 3)","768","    assert_equal(logo.get_n_splits(groups=[1.0, 1.1, 1.0, 1.2]), 3)","769","    assert_equal(lpgo_2.get_n_splits(None, None, np.arange(4)), 6)","770","    assert_equal(lpgo_1.get_n_splits(groups=np.arange(4)), 4)","771","","772","    # raise ValueError if a `groups` parameter is illegal","773","    with assert_raises(ValueError):","774","        logo.get_n_splits(None, None, [0.0, np.nan, 0.0])","775","    with assert_raises(ValueError):","776","        lpgo_2.get_n_splits(None, None, [0.0, np.inf, 0.0])","777","","778","    msg = \"The 'groups' parameter should not be None.\"","779","    assert_raise_message(ValueError, msg,","780","                         logo.get_n_splits, None, None, None)","781","    assert_raise_message(ValueError, msg,","782","                         lpgo_1.get_n_splits, None, None, None)","783",""],"delete":[]}],"sklearn\/model_selection\/tests\/test_validation.py":[{"add":["261","                             \"The 'groups' parameter should not be None.\",","264","                             \"The 'groups' parameter should not be None.\","],"delete":["261","                             \"The groups parameter should not be None\",","264","                             \"The groups parameter should not be None\","]}]}},"bbf43483c178e9ae53a0d2f9fa98028a3c9127f1":{"changes":{"doc\/README.md":"MODIFY"},"diff":{"doc\/README.md":[{"add":["0","# Documentation for scikit-learn","28","# Development documentation automated build"],"delete":["0","#Documentation for scikit-learn","28","#Development documentation automated build"]}]}},"15921603ea2bdc5bdbc3aa1bee8f9fdd3ff87d46":{"changes":{"doc\/tutorial\/statistical_inference\/supervised_learning.rst":"MODIFY"},"diff":{"doc\/tutorial\/statistical_inference\/supervised_learning.rst":[{"add":["343","    provides the :class:`LassoLars` object using the *LARS* algorithm,"],"delete":["343","    provides the :class:`LassoLars` object using the *LARS* algorthm,"]}]}},"daeba62b9ccbf515cf9bc419562cf5ae83e73d85":{"changes":{"sklearn\/utils\/validation.py":"MODIFY","sklearn\/utils\/tests\/test_validation.py":"MODIFY"},"diff":{"sklearn\/utils\/validation.py":[{"add":["94","    if hasattr(x, 'fit') and callable(x.fit):"],"delete":["94","    if hasattr(x, 'fit'):"]}],"sklearn\/utils\/tests\/test_validation.py":[{"add":["17","from sklearn.utils.testing import SkipTest","504","","505","","506","def test_check_dataframe_fit_attribute():","507","    # check pandas dataframe with 'fit' column does not raise error","508","    # https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/8415","509","    try:","510","        import pandas as pd","511","        X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])","512","        X_df = pd.DataFrame(X, columns=['a', 'b', 'fit'])","513","        check_consistent_length(X_df)","514","    except ImportError:","515","        raise SkipTest(\"Pandas not found\")"],"delete":[]}]}},"1ada91c341c984ed7bbf1a84ed22884925280d1d":{"changes":{"sklearn\/neighbors\/regression.py":"MODIFY"},"diff":{"sklearn\/neighbors\/regression.py":[{"add":["65","    metric : string or callable, default 'minkowski'"],"delete":["65","    metric : string or DistanceMetric object (default='minkowski')"]}]}},"eaebfe05353afce37605162597485c494cdd68b2":{"changes":{"sklearn\/model_selection\/_split.py":"MODIFY","sklearn\/model_selection\/tests\/test_split.py":"MODIFY"},"diff":{"sklearn\/model_selection\/_split.py":[{"add":["1861","    shuffle : boolean, optional (default=True)","1862","        Whether or not to shuffle the data before splitting. If shuffle=False","1863","        then stratify must be None.","1864","","1908","    >>> train_test_split(y, shuffle=False)","1909","    [[0, 1, 2], [3, 4]]","1910","","1919","    shuffle = options.pop('shuffle', True)","1929","    if shuffle is False:","1930","        if stratify is not None:","1931","            raise NotImplementedError(","1932","                \"Stratified train\/test split is not implemented for \"","1933","                \"shuffle=False\")","1934","","1935","        n_samples = _num_samples(arrays[0])","1936","        n_train, n_test = _validate_shuffle_split(n_samples, test_size,","1937","                                                  train_size)","1938","","1939","        train = np.arange(n_train)","1940","        test = np.arange(n_train, n_train + n_test)","1941","","1943","        if stratify is not None:","1944","            CVClass = StratifiedShuffleSplit","1945","        else:","1946","            CVClass = ShuffleSplit","1948","        cv = CVClass(test_size=test_size,","1949","                     train_size=train_size,","1950","                     random_state=random_state)","1952","        train, test = next(cv.split(X=arrays[0], y=stratify))","1953","","1960",""],"delete":["1452","","1922","    if stratify is not None:","1923","        CVClass = StratifiedShuffleSplit","1925","        CVClass = ShuffleSplit","1927","    cv = CVClass(test_size=test_size,","1928","                 train_size=train_size,","1929","                 random_state=random_state)","1931","    train, test = next(cv.split(X=arrays[0], y=stratify))"]}],"sklearn\/model_selection\/tests\/test_split.py":[{"add":["933","    assert_raises(NotImplementedError, train_test_split, range(10),","934","                  shuffle=False, stratify=True)","977","    # test unshuffled split","978","    y = np.arange(10)","979","    for test_size in [2, 0.2]:","980","        train, test = train_test_split(y, shuffle=False, test_size=test_size)","981","        assert_array_equal(test, [8, 9])","982","        assert_array_equal(train, [0, 1, 2, 3, 4, 5, 6, 7])","983",""],"delete":[]}]}},"02c705e115cdcf03f76789774baf8fed84092924":{"changes":{"doc\/whats_new.rst":"MODIFY","sklearn\/utils\/estimator_checks.py":"MODIFY"},"diff":{"doc\/whats_new.rst":[{"add":["272","      ","273","   - Estimators with both methods ``decision_function`` and ``predict_proba`` ","274","     are now required to have a monotonic relation between them. The ","275","     method ``check_decision_proba_consistency`` has been added in ","276","     **sklearn.utils.estimator_checks** to check their consistency. ","277","     :issue:`7578` by :user:`Shubham Bhardwaj <shubham0704>`","278","      "],"delete":[]}],"sklearn\/utils\/estimator_checks.py":[{"add":["10","from scipy.stats import rankdata","115","    if (name not in","116","        [\"MultinomialNB\", \"LabelPropagation\", \"LabelSpreading\"] and","118","       name not in [\"DecisionTreeClassifier\", \"ExtraTreeClassifier\"]):","129","    # test if predict_proba is a monotonic transformation of decision_function","130","    yield check_decision_proba_consistency","273","    if (\"n_iter\" in params and estimator.__class__.__name__ != \"TSNE\"):","1115","                if (n_classes is 3 and not isinstance(classifier, BaseLibSVM)):","1576","                return (p.name != 'self' and","1577","                        p.kind != p.VAR_KEYWORD and","1578","                        p.kind != p.VAR_POSITIONAL)","1723","","1724","","1725","@ignore_warnings(category=DeprecationWarning)","1726","def check_decision_proba_consistency(name, Estimator):","1727","    # Check whether an estimator having both decision_function and","1728","    # predict_proba methods has outputs with perfect rank correlation.","1729","","1730","    centers = [(2, 2), (4, 4)]","1731","    X, y = make_blobs(n_samples=100, random_state=0, n_features=4,","1732","                      centers=centers, cluster_std=1.0, shuffle=True)","1733","    X_test = np.random.randn(20, 2) + 4","1734","    estimator = Estimator()","1735","","1736","    set_testing_parameters(estimator)","1737","","1738","    if (hasattr(estimator, \"decision_function\") and","1739","            hasattr(estimator, \"predict_proba\")):","1740","","1741","        estimator.fit(X, y)","1742","        a = estimator.predict_proba(X_test)[:, 1]","1743","        b = estimator.decision_function(X_test)","1744","        assert_array_equal(rankdata(a), rankdata(b))"],"delete":["8","","115","    if (name not in [\"MultinomialNB\", \"LabelPropagation\", \"LabelSpreading\"]","117","            and name not in [\"DecisionTreeClassifier\",","118","                             \"ExtraTreeClassifier\"]):","271","    if (\"n_iter\" in params","272","            and estimator.__class__.__name__ != \"TSNE\"):","1114","                if (n_classes is 3","1115","                        and not isinstance(classifier, BaseLibSVM)):","1576","                return (p.name != 'self'","1577","                        and p.kind != p.VAR_KEYWORD","1578","                        and p.kind != p.VAR_POSITIONAL)"]}]}},"7877f3cfae54b20cfc5416eb296dc5d5ae78b231":{"changes":{"examples\/tree\/plot_unveil_tree_structure.py":"MODIFY"},"diff":{"examples\/tree\/plot_unveil_tree_structure.py":[{"add":["116","    print(\"decision id node %s : (X_test[%s, %s] (= %s) %s %s)\"","120","             X_test[sample_id, feature[node_id]],"],"delete":["116","    print(\"decision id node %s : (X[%s, %s] (= %s) %s %s)\"","120","             X_test[i, feature[node_id]],"]}]}},"f38231e4c9cb43b8b49d789c21d2376def039c9c":{"changes":{"sklearn\/decomposition\/pca.py":"MODIFY","sklearn\/decomposition\/tests\/test_pca.py":"MODIFY"},"diff":{"sklearn\/decomposition\/pca.py":[{"add":["388","        else:","389","            raise ValueError(\"Unrecognized svd_solver='{0}'\"","390","                             \"\".format(svd_solver))"],"delete":[]}],"sklearn\/decomposition\/tests\/test_pca.py":[{"add":["586","","587","","588","def test_pca_bad_solver():","589","    X = np.random.RandomState(0).rand(5, 4)","590","    pca = PCA(n_components=3, svd_solver='bad_argument')","591","    assert_raises(ValueError, pca.fit, X)"],"delete":[]}]}},"9b75a81b2dd89daf93e483b3b32ea1759edaf937":{"changes":{"examples\/linear_model\/plot_lasso_and_elasticnet.py":"MODIFY"},"diff":{"examples\/linear_model\/plot_lasso_and_elasticnet.py":[{"add":["30","y += 0.01 * np.random.normal(size=n_samples)"],"delete":["30","y += 0.01 * np.random.normal((n_samples,))"]}]}},"cb5c1620c39352f58813b3988302c2a302fc526d":{"changes":{"doc\/whats_new.rst":"MODIFY"},"diff":{"doc\/whats_new.rst":[{"add":["46","     :issue:`5295` by `Tom Dupre la Tour`_.","48","   - Added the :class:`model_selection.RepeatedKFold` and","49","     :class:`model_selection.RepeatedStratifiedKFold`.","62","     :issue:`8446` by `Arthur Mensch`_.","75","     documentation build with Sphinx>1.5 :issue:`8010`, :issue:`7986` by","83","     :issue:`7990` by :user:`Guillaume Lemaitre <glemaitre>`.","101","   - Relax assumption on the data for the","102","     :class:`kernel_approximation.SkewedChi2Sampler`. Since the Skewed-Chi2","103","     kernel is defined on the open interval :math:`(-skewedness; +\\infty)^d`,","104","     the transform function should not check whether ``X < 0`` but whether ``X <","105","     -self.skewedness``. :issue:`7573` by `Romain Brault`_.","131","     to enable selection of the norm order when ``coef_`` is more than 1D.","132","     :issue:`6181` by :user:`Antoine Wendlinger <antoinewdg>`.","146","   - :class:`linear_model.RANSACRegressor` no longer throws an error","156","   - Fix a bug where :class:`feature_selection.SelectFdr` did not","178","   - Add ``sample_weight`` parameter to :func:`metrics.cohen_kappa_score`.","179","     :issue:`8335` by :user:`Victor Poughon <vpoughon>`.","182","     is a lot faster with ``return_std=True``. :issue:`8591` by","183","     :user:`Hadrien Bertrand <hbertrand>`.","194","     :class:`linear_model.LogisticRegression` when using newton-cg","195","     solver. :issue:`8835` by :user:`Joan Massich <massich>`.","212","     ``_c_step`` to throw an exception.","215","   - Fixed a bug where :class:`ensemble.IsolationForest` uses an","219","   - Fixed a bug where :class:`cluster.DBSCAN` gives incorrect","221","     rows all zero. :issue:`8306` by :user:`Akshay Gupta <Akshay0724>`","223","   - Fixed a bug where :class:`ensemble.AdaBoostClassifier` throws","227","   - Fixed a bug when :func:`datasets.make_classification` fails","229","     :user:`Herilalaina Rakotoarison <herilalaina>`.","231","   - Fixed a bug where :func:`model_selection.BaseSearchCV.inverse_transform`","232","     returns ``self.best_estimator_.transform()`` instead of","233","     ``self.best_estimator_.inverse_transform()``.","234","     :issue:`8344` by :user:`Akshay Gupta <Akshay0724>`.","236","   - Fixed same issue in :func:`grid_search.BaseSearchCV.inverse_transform`","239","   - Fixed a bug where :class:`linear_model.RandomizedLasso` and","240","     :class:`linear_model.RandomizedLogisticRegression` breaks for","241","     sparse input. :issue:`8259` by :user:`Aman Dalmia <dalmia>`.","243","   - Fixed a bug where :func:`linear_model.RANSACRegressor.fit` may run until","246","   - Fixed a bug where :func:`datasets.make_moons` gives an","250","   - Fixed a bug where :class:`linear_model.LassoLars` does not give","254","   - Some ``fetch_`` functions in :mod:`sklearn.datasets` were ignoring the","255","     ``download_if_missing`` keyword. :issue:`7944` by :user:`Ralf Gommers <rgommers>`.","257","   - Fixed a bug in :class:`ensemble.GradientBoostingClassifier`","258","     and :class:`ensemble.GradientBoostingRegressor`","260","     error. issue:`7970` by :user:`He Chen <chenhe95>`.","262","   - Fix a bug regarding fitting :class:`cluster.KMeans` with a sparse","270","   - Fixed a bug where :class:`ensemble.IsolationForest` fails when","274","   - Fix a bug where :class:`ensemble.VotingClassifier` raises an error","278","   - Fix a bug in :class:`decomposition.LatentDirichletAllocation`","283","   - Fix a bug where :class:`ensemble.GradientBoostingClassifier` and","284","     :class:`ensemble.GradientBoostingRegressor` ignored the","288","   - Fixes to the input validation in :class:`covariance.EllipticEnvelope`.","291","   - Fix output shape and bugs with n_jobs > 1 in","292","     :class:`decomposition.SparseCoder` transform and","293","     :func:`decomposition.sparse_encode`","295","     This also impacts the output shape of :class:`decomposition.DictionaryLearning`.","303","     :class:`ensemble.gradient_boosting.QuantileLossFunction` computed","308","   - Fix :func:`multioutput.MultiOutputClassifier.predict_proba` to","314","   - Fix a bug where :func:`linear_model.LassoLars.fit` sometimes","318","   - Fix a bug where :class:`feature_extraction.FeatureHasher`","320","     preventing the use of","321","     :class:`feature_extraction.text.HashingVectorizer` in a","322","     pipeline with  :class:`feature_extraction.text.TfidfTransformer`.","324","","325","   - Fix a bug in cases where ``numpy.cumsum`` may be numerically unstable,","326","     raising an exception if instability is identified. :issue:`7376` and","328","","329","   - Fix a bug where :meth:`base.BaseEstimator.__getstate__`","333","","334","   - Fix a bug in :func:`metrics.classification._check_targets`","339","","340","   - Fix :func:`linear_model.BayesianRidge.fit` to return","351","   - Fixed a bug where :func:`tree.export_graphviz` raised an error","353","     tree. :issue:`8512` by :user:`Li Li <aikinogard>`.","360","   - Fixed improper scaling in :class:`cross_decomposition.PLSRegression`","366","   - Add ``shuffle`` parameter to :func:`model_selection.train_test_split`.","384","     in :class:`decomposition.LatentDirichletAllocation` because the","389","   - Replace attribute ``named_steps`` ``dict`` to :class:`utils.Bunch`","390","     in :class:`pipeline.Pipeline` to enable tab completion in interactive","395","   - The :func:`multioutput.MultiOutputClassifier.predict_proba`","405","     :class:`model_selection.GridSearchCV` and","406","     :class:`model_selection.RandomizedSearchCV` in favor","412","     :func:`model_selection.cross_val_predict`.","426","     only if the underlying estimator does.  By `Andreas Mller`_.","438","     ``min_impurity_decrease``.  :issue:`8449` by `Raghav RV`_.","440","   - The ``n_topics`` parameter of :class:`decomposition.LatentDirichletAllocation`","442","     :issue:`8922` by :user:`Attractadore`","444","   - :class:`cluster.bicluster.SpectralCoclustering` and","449","     for scikit-learn. The following backported functions in","450","     :mod:`sklearn.utils` have been removed or deprecated accordingly."],"delete":["46","     By `Tom Dupre la Tour`_.","48","   - Added the :class:`sklearn.model_selection.RepeatedKFold` and","49","     :class:`sklearn.model_selection.RepeatedStratifiedKFold`.","62","     By `Arthur Mensch`_.","75","     documentation build with Sphinx>1.5 :issue:`8010`, :issue:`7986`","83","     By :issue:`7990` by :user:`Guillaume Lemaitre <glemaitre>`.","101","   - Relax assumption on the data for the ``SkewedChi2Sampler``. Since the","102","     Skewed-Chi2 kernel is defined on the open interval :math: `(-skewedness;","103","     +\\infty)^d`, the transform function should not check whether X < 0 but","104","     whether ``X < -self.skewedness``. (`#7573","105","     <https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/7573>`_) by `Romain","106","     Brault`_.","132","     to enable selection of the norm order when ``coef_`` is more than 1D","146","   - :class:`sklearn.linear_model.RANSACRegressor` no longer throws an error","156","   - Fix a bug where :class:`sklearn.feature_selection.SelectFdr` did not","178","   - Add ``sample_weight`` parameter to :func:`metrics.cohen_kappa_score` by","179","     Victor Poughon.","182","     is a lot faster with ``return_std=True`` by :user:`Hadrien Bertrand <hbertrand>`.","193","     :class:`sklearn.linear_model.LogisticRegression` when using newton-cg solver","194","     by :user:`Joan Massich <massich>`","211","     `_c_step` to throw an exception.","214","   - Fixed a bug where :class:`sklearn.ensemble.IsolationForest` uses an","218","   - Fixed a bug where :class:`sklearn.cluster.DBSCAN` gives incorrect","220","     rows all zero.","221","     :issue:`8306` by :user:`Akshay Gupta <Akshay0724>`","223","   - Fixed a bug where :class:`sklearn.ensemble.AdaBoostClassifier` throws","227","   - Fixed a bug when :func:`sklearn.datasets.make_classification` fails ","229","     :user:`Herilalaina Rakotoarison <herilalaina>`","231","   - Fixed a bug where :func:`sklearn.model_selection.BaseSearchCV.inverse_transform`","232","     returns self.best_estimator_.transform() instead of self.best_estimator_.inverse_transform()","233","     :issue:`8344` by :user:`Akshay Gupta <Akshay0724>`","235","   - Fixed same issue in :func:`sklearn.grid_search.BaseSearchCV.inverse_transform`","238","   - Fixed a bug where :class:`sklearn.linear_model.RandomizedLasso` and","239","     :class:`sklearn.linear_model.RandomizedLogisticRegression` breaks for","240","     sparse input.","241","     :issue:`8259` by :user:`Aman Dalmia <dalmia>`.","243","   - Fixed a bug where :func:`sklearn.linear_model.RANSACRegressor.fit` may run until","246","   - Fixed a bug where :func:`sklearn.datasets.make_moons` gives an","250","   - Fixed a bug where :class:`sklearn.linear_model.LassoLars` does not give","254","   - Some ``fetch_`` functions in `sklearn.datasets` were ignoring the","255","     ``download_if_missing`` keyword.  This was fixed in :issue:`7944` by","256","     :user:`Ralf Gommers <rgommers>`.","258","   - Fixed a bug in :class:`sklearn.ensemble.GradientBoostingClassifier`","259","     and :class:`sklearn.ensemble.GradientBoostingRegressor`","261","     error. This was fixed in :issue:`7970` by :user:`He Chen <chenhe95>`.","263","   - Fix a bug regarding fitting :class:`sklearn.cluster.KMeans` with a sparse","271","   - Fixed a bug where :class:`sklearn.ensemble.IsolationForest` fails when","275","   - Fix a bug where :class:`sklearn.ensemble.VotingClassifier` raises an error","279","   - Fix a bug in :class:`sklearn.decomposition.LatentDirichletAllocation`","284","   - Fix a bug where :class:`sklearn.ensemble.GradientBoostingClassifier` and","285","     :class:`sklearn.ensemble.GradientBoostingRegressor` ignored the","289","   - Fixes to the input validation in","290","     :class:`sklearn.covariance.EllipticEnvelope`.","293","   - Fix output shape and bugs with n_jobs > 1 in  ","294","     :class:`sklearn.decomposition.SparseCoder` transform and :func:`sklarn.decomposition.sparse_encode`","296","     This also impacts the output shape of :class:`sklearn.decomposition.DictionaryLearning`.","304","     :class:`sklearn.ensemble.gradient_boosting.QuantileLossFunction` computed","309","   - Fix :func:`sklearn.multioutput.MultiOutputClassifier.predict_proba` to","315","   - Fix a bug where :func:`sklearn.linear_model.LassoLars.fit` sometimes","319","","320","   - Fix a bug where :class:`sklearn.feature_extraction.FeatureHasher`","322","     preventing the use of ","323","     :class:`sklearn.feature_extraction.text.HashingVectorizer` in a","324","     pipeline with  :class:`sklearn.feature_extraction.text.TfidfTransformer`.","326","     ","327","   - Fix a bug in cases where `numpy.cumsum` may be numerically unstable,","328","     raising an exception if instability is identified.  :issue:`7376` and","330","     ","331","   - Fix a bug where :meth:`sklearn.base.BaseEstimator.__getstate__`","335","   - Fix a bug in :func:`sklearn.metrics.classification._check_targets`","340","   - Fix :func:`sklearn.linear_model.BayesianRidge.fit` to return","351","   - Fixed a bug where :func:`sklearn.tree.export_graphviz` raised an error","353","     tree.","354","     :issue:`8512` by :user:`Li Li <aikinogard>`.","361","   - Fixed improper scaling in :class:`sklearn.cross_decomposition.PLSRegression`","367","   - Add ``shuffle`` parameter to :func:`sklearn.model_selection.train_test_split`.","385","     in :class:`sklearn.decomposition.LatentDirichletAllocation` because the","390","   - Replace attribute ``named_steps`` ``dict`` to :class:`sklearn.utils.Bunch`","391","     in :class:`sklearn.pipeline.Pipeline` to enable tab completion in interactive","396","   - The :func:`sklearn.multioutput.MultiOutputClassifier.predict_proba`","406","     :class:`sklearn.model_selection.GridSearchCV` and","407","     :class:`sklearn.model_selection.RandomizedSearchCV` in favor","413","     :func:`sklearn.model_selection.cross_val_predict`.","416","","428","     only if the underlying estimator does.  By `Andreas Mller`_. ","440","     ``min_impurity_decrease``.  :issue:`8449` by `Raghav RV_`","442","   - The ``n_topics`` parameter of :class:`decomposition.LatentDirichletAllocation` ","444","     :issue:`8922` by :user:Attractadore","446","   - :class:`cluster.bicluster.SpectralCoClustering` and","451","     for scikit-learn. The following backported functions in ``sklearn.utils``","452","     have been removed or deprecated accordingly."]}]}},"2cb7e472a7aabd18680dc748ca5a0b6e5283ccdf":{"changes":{"sklearn\/metrics\/cluster\/tests\/test_supervised.py":"MODIFY"},"diff":{"sklearn\/metrics\/cluster\/tests\/test_supervised.py":[{"add":["241","","242","","243","def test_fowlkes_mallows_score_properties():","244","    # handcrafted example","245","    labels_a = np.array([0, 0, 0, 1, 1, 2])","246","    labels_b = np.array([1, 1, 2, 2, 0, 0])","247","    expected = 1. \/ np.sqrt((1. + 3.) * (1. + 2.))","248","    # FMI = TP \/ sqrt((TP + FP) * (TP + FN))","249","","250","    score_original = fowlkes_mallows_score(labels_a, labels_b)","251","    assert_almost_equal(score_original, expected)","252","","253","    # symetric property","254","    score_symetric = fowlkes_mallows_score(labels_b, labels_a)","255","    assert_almost_equal(score_symetric, expected)","256","","257","    # permutation property","258","    score_permuted = fowlkes_mallows_score((labels_a + 1) % 3, labels_b)","259","    assert_almost_equal(score_permuted, expected)","260","","261","    # symetric and permutation(both together)","262","    score_both = fowlkes_mallows_score(labels_b, (labels_a + 2) % 3)","263","    assert_almost_equal(score_both, expected)"],"delete":[]}]}},"676e8630243b894aa2976ef6fb6048f9880b8a23":{"changes":{"sklearn\/model_selection\/tests\/test_search.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/model_selection\/_search.py":"MODIFY"},"diff":{"sklearn\/model_selection\/tests\/test_search.py":[{"add":["81","    def transform(self, X):","82","        return X + self.foo_param","83","","84","    def inverse_transform(self, X):","85","        return X - self.foo_param","86","","1311","","1312","","1313","def test_transform_inverse_transform_round_trip():","1314","    clf = MockClassifier()","1315","    grid_search = GridSearchCV(clf, {'foo_param': [1, 2, 3]}, verbose=3)","1316","","1317","    grid_search.fit(X, y)","1318","    X_round_trip = grid_search.inverse_transform(grid_search.transform(X))","1319","    assert_array_equal(X, X_round_trip)"],"delete":["84","    transform = predict","85","    inverse_transform = predict"]}],"doc\/whats_new.rst":[{"add":["155","   - Fixed a bug where :func:`sklearn.model_selection.BaseSearchCV.inverse_transform`","156","     returns self.best_estimator_.transform() instead of self.best_estimator_.inverse_transform()","157","     :issue:`8344` by :user:`Akshay Gupta <Akshay0724>` ","158","","159",""],"delete":[]}],"sklearn\/model_selection\/_search.py":[{"add":["532","        return self.best_estimator_.inverse_transform(Xt)"],"delete":["532","        return self.best_estimator_.transform(Xt)"]}]}},"e548ea6d7396e3cd935a71e6e3379ce61fd4bf70":{"changes":{"doc\/themes\/scikit-learn\/layout.html":"MODIFY","doc\/themes\/scikit-learn\/static\/nature.css_t":"MODIFY"},"diff":{"doc\/themes\/scikit-learn\/layout.html":[{"add":["104","                <div class=\"gcse-search\" id=\"cse\" style=\"width: 100%;\"><\/div>","348","    <script>","349","      (function() {","350","        var cx = '016639176250731907682:tjtqbvtvij0';","351","        var gcse = document.createElement('script'); gcse.type = 'text\/javascript'; gcse.async = true;","352","        gcse.src = 'https:\/\/cse.google.com\/cse.js?cx=' + cx;","353","        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(gcse, s);","354","      })();"],"delete":["104","                <div id=\"cse\" style=\"width: 100%;\"><\/div>","348","","349","    <script src=\"http:\/\/www.google.com\/jsapi\" type=\"text\/javascript\"><\/script>","350","    <script type=\"text\/javascript\"> google.load('search', '1',","351","        {language : 'en'}); google.setOnLoadCallback(function() {","352","            var customSearchControl = new","353","            google.search.CustomSearchControl('016639176250731907682:tjtqbvtvij0');","354","            customSearchControl.setResultSetSize(google.search.Search.FILTERED_CSE_RESULTSET);","355","            var options = new google.search.DrawOptions();","356","            options.setAutoComplete(true);","357","            customSearchControl.draw('cse', options); }, true);"]}],"doc\/themes\/scikit-learn\/static\/nature.css_t":[{"add":["209","    margin-top: -40px;","215","    position: relative;","216","    top: -2px;","220","    display: none !important;","221","}","222","","223","form.gsc-search-box {","224","    padding: 0 !important;","225","}","226","","227","input.gsc-search-button {","228","    position: relative;","229","    top: -4px;","230","    border-radius: 5px !important;","231","    border-color: #FFFFFF !important;","232","    background-color: #ff9c34 !important;","233","}","234","","235","a.gs-title, a.gs-title > b{","236","    color: blue !important;","237","}","238","","239",".gsc-results .gsc-cursor-box .gsc-cursor-current-page {","240","    border-color: white !important;","241","    background-color: #ff9c34 !important;","242","    color: white !important;"],"delete":["209","    margin-top: -23px;","210","    \/*The min-height is added here, to prevent the element from shrinking","211","    too much, while the scripts are still loading the search-bar.","212","    Without it, layout glitches occur, as the element keeps dynamically","213","    changing its size while its loading-contents adjusts into position.*\/","222","    display: none;"]}]}},"964aedbebc40225f5845e807e1ed4068196c4a30":{"changes":{"sklearn\/neighbors\/classification.py":"MODIFY","sklearn\/neighbors\/regression.py":"MODIFY"},"diff":{"sklearn\/neighbors\/classification.py":[{"add":["63","    metric : string or callable, default 'minkowski'","270","    metric : string or callable, default 'minkowski'"],"delete":["63","    metric : string or DistanceMetric object (default = 'minkowski')","270","    metric : string or DistanceMetric object (default='minkowski')"]}],"sklearn\/neighbors\/regression.py":[{"add":["215","    metric : string or callable, default 'minkowski'"],"delete":["215","    metric : string or DistanceMetric object (default='minkowski')"]}]}},"096a9cbead13c5912e247ab2f3d14d27b7990c0d":{"changes":{".travis.yml":"MODIFY","build_tools\/travis\/install.sh":"MODIFY","sklearn\/utils\/graph.py":"MODIFY","sklearn\/utils\/tests\/test_validation.py":"MODIFY"},"diff":{".travis.yml":[{"add":["35","    - DISTRIB=\"conda\" PYTHON_VERSION=\"3.6\" INSTALL_MKL=\"true\"","36","      NUMPY_VERSION=\"1.11.2\" SCIPY_VERSION=\"0.18.1\" PANDAS_VERSION=\"0.19.1\"","37","      CYTHON_VERSION=\"0.25.2\""],"delete":["35","    - DISTRIB=\"conda\" PYTHON_VERSION=\"3.5\" INSTALL_MKL=\"true\"","36","      NUMPY_VERSION=\"1.10.4\" SCIPY_VERSION=\"0.17.0\" PANDAS_VERSION=\"0.18.0\"","37","      CYTHON_VERSION=\"0.23.4\""]}],"build_tools\/travis\/install.sh":[{"add":["54","            numpy=$NUMPY_VERSION scipy=$SCIPY_VERSION \\","55","            mkl cython=$CYTHON_VERSION \\","120","    conda install --yes flake8"],"delete":["54","            numpy=$NUMPY_VERSION scipy=$SCIPY_VERSION numpy scipy \\","55","            mkl flake8 cython=$CYTHON_VERSION \\","120","    conda install flake8"]}],"sklearn\/utils\/graph.py":[{"add":["46","    >>> list(sorted(single_source_shortest_path_length(graph, 0).items()))","47","    [(0, 0), (1, 1), (2, 2), (3, 3)]","48","    >>> graph = np.ones((6, 6))","49","    >>> list(sorted(single_source_shortest_path_length(graph, 2).items()))","50","    [(0, 1), (1, 1), (2, 0), (3, 1), (4, 1), (5, 1)]"],"delete":["46","    >>> single_source_shortest_path_length(graph, 0)","47","    {0: 0, 1: 1, 2: 2, 3: 3}","48","    >>> single_source_shortest_path_length(np.ones((6, 6)), 2)","49","    {0: 1, 1: 1, 2: 0, 3: 1, 4: 1, 5: 1}"]}],"sklearn\/utils\/tests\/test_validation.py":[{"add":["347","    assert_raise_message(TypeError, \"SVR\","],"delete":["347","    msg = \"'SVR' object\"","348","    assert_raise_message(TypeError, msg,"]}]}},"7978119e037ceaafdbcc2588a8e1f3fa0f36c9c6":{"changes":{"doc\/faq.rst":"MODIFY"},"diff":{"doc\/faq.rst":[{"add":["284","You can find more information about addition of gpu support at","285","`Will you add GPU support?`_.","286",""],"delete":[]}]}},"72d0529de742206f23be6a14670d21566e85835f":{"changes":{"sklearn\/tests\/test_discriminant_analysis.py":"MODIFY","sklearn\/discriminant_analysis.py":"MODIFY","doc\/whats_new.rst":"MODIFY"},"diff":{"sklearn\/tests\/test_discriminant_analysis.py":[{"add":["7","from sklearn.utils.testing import assert_false","11","from sklearn.utils.testing import assert_warns_message","227","def test_lda_store_covariance():","228","    # Test for slover 'lsqr' and 'eigen'","229","    # 'store_covariance' has no effect on 'lsqr' and 'eigen' solvers","230","    for solver in ('lsqr', 'eigen'):","231","        clf = LinearDiscriminantAnalysis(solver=solver).fit(X6, y6)","232","        assert_true(hasattr(clf, 'covariance_'))","233","","234","        # Test the actual attribute:","235","        clf = LinearDiscriminantAnalysis(solver=solver,","236","                                         store_covariance=True).fit(X6, y6)","237","        assert_true(hasattr(clf, 'covariance_'))","238","","239","        assert_array_almost_equal(","240","            clf.covariance_,","241","            np.array([[0.422222, 0.088889], [0.088889, 0.533333]])","242","        )","243","","244","    # Test for SVD slover, the default is to not set the covariances_ attribute","245","    clf = LinearDiscriminantAnalysis(solver='svd').fit(X6, y6)","246","    assert_false(hasattr(clf, 'covariance_'))","247","","248","    # Test the actual attribute:","249","    clf = LinearDiscriminantAnalysis(solver=solver,","250","                                     store_covariance=True).fit(X6, y6)","251","    assert_true(hasattr(clf, 'covariance_'))","252","","253","    assert_array_almost_equal(","254","        clf.covariance_,","255","        np.array([[0.422222, 0.088889], [0.088889, 0.533333]])","256","    )","257","","258","","298","def test_qda_store_covariance():","301","    assert_false(hasattr(clf, 'covariance_'))","304","    clf = QuadraticDiscriminantAnalysis(store_covariance=True).fit(X6, y6)","305","    assert_true(hasattr(clf, 'covariance_'))","308","        clf.covariance_[0],","313","        clf.covariance_[1],","318","def test_qda_deprecation():","319","    # Test the deprecation","320","    clf = QuadraticDiscriminantAnalysis(store_covariances=True)","321","    assert_warns_message(DeprecationWarning, \"'store_covariances' was renamed\"","322","                         \" to store_covariance in version 0.19 and will be \"","323","                         \"removed in 0.21.\", clf.fit, X, y)","324","","325","    # check that covariance_ (and covariances_ with warning) is stored","326","    assert_warns_message(DeprecationWarning, \"Attribute covariances_ was \"","327","                         \"deprecated in version 0.19 and will be removed \"","328","                         \"in 0.21. Use covariance_ instead\", getattr, clf,","329","                         'covariances_')","330","","331",""],"delete":["264","def test_qda_store_covariances():","267","    assert_true(not hasattr(clf, 'covariances_'))","270","    clf = QuadraticDiscriminantAnalysis(store_covariances=True).fit(X6, y6)","271","    assert_true(hasattr(clf, 'covariances_'))","274","        clf.covariances_[0],","279","        clf.covariances_[1],"]}],"sklearn\/discriminant_analysis.py":[{"add":["14","from .utils import deprecated","172","        Additionally compute class covariance matrix (default False), used","173","        only in 'svd' solver.","248","","558","    store_covariance : boolean","571","    covariance_ : list of array-like, shape = [n_features, n_features]","601","                                  store_covariance=False,","602","                                  store_covariances=None, tol=0.0001)","612","    def __init__(self, priors=None, reg_param=0., store_covariance=False,","613","                 tol=1.0e-4, store_covariances=None):","617","        self.store_covariance = store_covariance","620","    @property","621","    @deprecated(\"Attribute covariances_ was deprecated in version\"","622","                \" 0.19 and will be removed in 0.21. Use \"","623","                \"covariance_ instead\")","624","    def covariances_(self):","625","        return self.covariance_","626","","631","               ``store_covariances`` has been moved to main constructor as","632","               ``store_covariance``","635","               ``tol`` has been moved to main constructor.","659","        store_covariance = self.store_covariance or self.store_covariances","661","            warnings.warn(\"'store_covariances' was renamed to store_covariance\"","662","                          \" in version 0.19 and will be removed in 0.21.\",","663","                          DeprecationWarning)","664","        if store_covariance:","684","            if self.store_covariance or store_covariance:","689","        if self.store_covariance or store_covariance:","690","            self.covariance_ = cov"],"delete":["13","","172","        Additionally compute class covariance matrix (default False).","556","    store_covariances : boolean","569","    covariances_ : list of array-like, shape = [n_features, n_features]","599","                                  store_covariances=False, tol=0.0001)","609","    def __init__(self, priors=None, reg_param=0., store_covariances=False,","610","                 tol=1.0e-4):","620","               *store_covariance* has been moved to main constructor.","623","               *tol* has been moved to main constructor.","667","            if self.store_covariances:","672","        if self.store_covariances:","673","            self.covariances_ = cov"]}],"doc\/whats_new.rst":[{"add":["800","- The ``store_covariances`` and ``covariances_`` parameters of","801","  :class:`discriminant_analysis.QuadraticDiscriminantAnalysis`","802","  has been renamed to ``store_covariance`` and ``covariance_`` to be","803","  consistent with the corresponding parameter names of the","804","  :class:`discriminant_analysis.LinearDiscriminantAnalysis`. They will be","805","  removed in version 0.21. :issue:`7998` by :user:`Jiacheng <mrbeann>`","806",""],"delete":[]}]}},"cc3ce589532681ced6df4fe526b3c0c4cc33c36c":{"changes":{"sklearn\/tree\/tree.py":"MODIFY","sklearn\/ensemble\/gradient_boosting.py":"MODIFY","sklearn\/ensemble\/forest.py":"MODIFY"},"diff":{"sklearn\/tree\/tree.py":[{"add":["631","    Notes","632","    -----","633","    The features are always randomly permuted at each split. Therefore,","634","    the best found split may vary, even with the same training data and","635","    ``max_features=n_features``, if the improvement of the criterion is","636","    identical for several splits enumerated during the search of the best","637","    split. To obtain a deterministic behaviour during fitting,","638","    ``random_state`` has to be fixed.","639","","933","    Notes","934","    -----","935","    The features are always randomly permuted at each split. Therefore,","936","    the best found split may vary, even with the same training data and","937","    ``max_features=n_features``, if the improvement of the criterion is","938","    identical for several splits enumerated during the search of the best","939","    split. To obtain a deterministic behaviour during fitting,","940","    ``random_state`` has to be fixed.","941",""],"delete":[]}],"sklearn\/ensemble\/gradient_boosting.py":[{"add":["1386","    Notes","1387","    -----","1388","    The features are always randomly permuted at each split. Therefore,","1389","    the best found split may vary, even with the same training data and","1390","    ``max_features=n_features``, if the improvement of the criterion is","1391","    identical for several splits enumerated during the search of the best","1392","    split. To obtain a deterministic behaviour during fitting,","1393","    ``random_state`` has to be fixed.","1737","        p","1738","revious solution.","1781","    Notes","1782","    -----","1783","    The features are always randomly permuted at each split. Therefore,","1784","    the best found split may vary, even with the same training data and","1785","    ``max_features=n_features``, if the improvement of the criterion is","1786","    identical for several splits enumerated during the search of the best","1787","    split. To obtain a deterministic behaviour during fitting,","1788","    ``random_state`` has to be fixed.","1789",""],"delete":["1729","        previous solution."]}],"sklearn\/ensemble\/forest.py":[{"add":["891","    Notes","892","    -----","893","    The features are always randomly permuted at each split. Therefore,","894","    the best found split may vary, even with the same training data,","895","    ``max_features=n_features`` and ``bootstrap=False``, if the improvement","896","    of the criterion is identical for several splits enumerated during the","897","    search of the best split. To obtain a deterministic behaviour during","898","    fitting, ``random_state`` has to be fixed.","899","","1081","    Notes","1082","    -----","1083","    The features are always randomly permuted at each split. Therefore,","1084","    the best found split may vary, even with the same training data,","1085","    ``max_features=n_features`` and ``bootstrap=False``, if the improvement","1086","    of the criterion is identical for several splits enumerated during the","1087","    search of the best split. To obtain a deterministic behaviour during","1088","    fitting, ``random_state`` has to be fixed.","1089",""],"delete":[]}]}},"fb65a0a4bae01daad7c751e3a942ce0126ca05ff":{"changes":{"doc\/whats_new.rst":"MODIFY","sklearn\/ensemble\/weight_boosting.py":"MODIFY","sklearn\/ensemble\/tests\/test_weight_boosting.py":"MODIFY"},"diff":{"doc\/whats_new.rst":[{"add":["155","   - Fixed a bug where :class:`sklearn.ensemble.AdaBoostClassifier` throws","156","     ``ZeroDivisionError`` while fitting data with single class labels.","157","     :issue:`7501` by :user:`Dominik Krzeminski <dokato>`.","158",""],"delete":["159",""]}],"sklearn\/ensemble\/weight_boosting.py":[{"add":["758","        if n_classes == 1:","759","            return np.ones((X.shape[0], 1))","760",""],"delete":[]}],"sklearn\/ensemble\/tests\/test_weight_boosting.py":[{"add":["76","def test_oneclass_adaboost_proba():","77","    # Test predict_proba robustness for one class label input.","78","    # In response to issue #7501","79","    # https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/7501","80","    y_t = np.ones(len(X))","81","    clf = AdaBoostClassifier().fit(X, y_t)","82","    assert_array_equal(clf.predict_proba(X), np.ones((len(X), 1)))","83","","84",""],"delete":[]}]}},"511c9a8655c1360189233e788ac2d7c3e74f7b5c":{"changes":{"sklearn\/model_selection\/_split.py":"MODIFY","sklearn\/model_selection\/tests\/test_split.py":"MODIFY","doc\/modules\/cross_validation.rst":"MODIFY"},"diff":{"sklearn\/model_selection\/_split.py":[{"add":["675","    max_train_size : int, optional","676","        Maximum size for a single training set.","677","","685","    TimeSeriesSplit(max_train_size=None, n_splits=3)","701","    def __init__(self, n_splits=3, max_train_size=None):","705","        self.max_train_size = max_train_size","744","            if self.max_train_size and self.max_train_size < test_start:","745","                yield (indices[test_start - self.max_train_size:test_start],","746","                       indices[test_start:test_start + test_size])","747","            else:","748","                yield (indices[:test_start],","749","                       indices[test_start:test_start + test_size])"],"delete":["682","    TimeSeriesSplit(n_splits=3)","698","    def __init__(self, n_splits=3):","740","            yield (indices[:test_start],","741","                   indices[test_start:test_start + test_size])"]}],"sklearn\/model_selection\/tests\/test_split.py":[{"add":["236","    (ValueError, next, KFold(4).split(X1))","1291","def _check_time_series_max_train_size(splits, check_splits, max_train_size):","1292","    for (train, test), (check_train, check_test) in zip(splits, check_splits):","1293","        assert_array_equal(test, check_test)","1294","        assert_true(len(check_train) <= max_train_size)","1295","        suffix_start = max(len(train) - max_train_size, 0)","1296","        assert_array_equal(check_train, train[suffix_start:])","1297","","1298","","1299","def test_time_series_max_train_size():","1300","    X = np.zeros((6, 1))","1301","    splits = TimeSeriesSplit(n_splits=3).split(X)","1302","    check_splits = TimeSeriesSplit(n_splits=3, max_train_size=3).split(X)","1303","    _check_time_series_max_train_size(splits, check_splits, max_train_size=3)","1304","","1305","    # Test for the case where the size of a fold is greater than max_train_size","1306","    check_splits = TimeSeriesSplit(n_splits=3, max_train_size=2).split(X)","1307","    _check_time_series_max_train_size(splits, check_splits, max_train_size=2)","1308","","1309","    # Test for the case where the size of each fold is less than max_train_size","1310","    check_splits = TimeSeriesSplit(n_splits=3, max_train_size=5).split(X)","1311","    _check_time_series_max_train_size(splits, check_splits, max_train_size=2)","1312","","1313",""],"delete":["236","    assert_raises(ValueError, next, KFold(4).split(X1))"]}],"doc\/modules\/cross_validation.rst":[{"add":["466","","603","Time series data is characterised by the correlation between observations","604","that are near in time (*autocorrelation*). However, classical","605","cross-validation techniques such as :class:`KFold` and","606",":class:`ShuffleSplit` assume the samples are independent and","607","identically distributed, and would result in unreasonable correlation","608","between training and testing instances (yielding poor estimates of","609","generalisation error) on time series data. Therefore, it is very important","610","to evaluate our model for time series data on the \"future\" observations","611","least like those that are used to train the model. To achieve this, one","618",":class:`TimeSeriesSplit` is a variation of *k-fold* which","619","returns first :math:`k` folds as train set and the :math:`(k+1)` th","620","fold as test set. Note that unlike standard cross-validation methods,","625","This class can be used to cross-validate time series data samples","636","  TimeSeriesSplit(max_train_size=None, n_splits=3)"],"delete":["466"," ","603","Time series data is characterised by the correlation between observations ","604","that are near in time (*autocorrelation*). However, classical ","605","cross-validation techniques such as :class:`KFold` and ","606",":class:`ShuffleSplit` assume the samples are independent and ","607","identically distributed, and would result in unreasonable correlation ","608","between training and testing instances (yielding poor estimates of ","609","generalisation error) on time series data. Therefore, it is very important ","610","to evaluate our model for time series data on the \"future\" observations ","611","least like those that are used to train the model. To achieve this, one ","618",":class:`TimeSeriesSplit` is a variation of *k-fold* which ","619","returns first :math:`k` folds as train set and the :math:`(k+1)` th ","620","fold as test set. Note that unlike standard cross-validation methods, ","625","This class can be used to cross-validate time series data samples ","636","  TimeSeriesSplit(n_splits=3)"]}]}},"668a24c9e6e1a541bede64819612fdb5e169a341":{"changes":{"sklearn\/linear_model\/tests\/test_bayes.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/linear_model\/bayes.py":"MODIFY"},"diff":{"sklearn\/linear_model\/tests\/test_bayes.py":[{"add":["8","from sklearn.utils.testing import assert_array_almost_equal","9","from sklearn.utils.testing import assert_almost_equal","12","from sklearn.linear_model import Ridge","37","def test_bayesian_ridge_parameter():","38","    # Test correctness of lambda_ and alpha_ parameters (Github issue #8224)","39","    X = np.array([[1, 1], [3, 4], [5, 7], [4, 1], [2, 6], [3, 10], [3, 2]])","40","    y = np.array([1, 2, 3, 2, 0, 4, 5]).T","41","","42","    # A Ridge regression model using an alpha value equal to the ratio of","43","    # lambda_ and alpha_ from the Bayesian Ridge model must be identical","44","    br_model = BayesianRidge(compute_score=True).fit(X, y)","45","    rr_model = Ridge(alpha=br_model.lambda_ \/ br_model.alpha_).fit(X, y)","46","    assert_array_almost_equal(rr_model.coef_, br_model.coef_)","47","    assert_almost_equal(rr_model.intercept_, br_model.intercept_)","48","","49","","80","        return f(X) + np.random.randn(X.shape[0]) * noise_mult"],"delete":["12","from sklearn.utils.testing import assert_array_almost_equal","13","","66","        return f(X) + np.random.randn(X.shape[0])*noise_mult"]}],"doc\/whats_new.rst":[{"add":["246","   - Fix :func:`sklearn.linear_model.BayesianRidge.fit` to return ","247","     ridge parameter `alpha_` and `lambda_` consistent with calculated","248","     coefficients `coef_` and `intercept_`.","249","     :issue:`8224` by :user:`Peter Gedeck <gedeck>`.","250",""],"delete":[]}],"sklearn\/linear_model\/bayes.py":[{"add":["203","            # Preserve the alpha and lambda values that were used to","204","            # calculate the final coefficients","205","            self.alpha_ = alpha_","206","            self.lambda_ = lambda_","207",""],"delete":["231","        self.alpha_ = alpha_","232","        self.lambda_ = lambda_"]}]}},"9be0922ad25a7deed3798e39a9ee0a39e1e9fc53":{"changes":{"sklearn\/ensemble\/forest.py":"MODIFY"},"diff":{"sklearn\/ensemble\/forest.py":[{"add":["376","# This is a utility function for joblib's Parallel. It can't go locally in","377","# ForestClassifier or ForestRegressor, because joblib complains that it cannot","378","# pickle it when placed there.","379","","380","def accumulate_prediction(predict, X, out):","381","    prediction = predict(X, check_input=False)","382","    if len(out) == 1:","383","        out[0] += prediction","384","    else:","385","        for i in range(len(out)):","386","            out[i] += prediction[i]","387","","388","","580","        # avoid storing the output of every estimator by summing them here","581","        all_proba = [np.zeros((X.shape[0], j), dtype=np.float64)","582","                     for j in np.atleast_1d(self.n_classes_)]","583","        Parallel(n_jobs=n_jobs, verbose=self.verbose, backend=\"threading\")(","584","            delayed(accumulate_prediction)(e.predict_proba, X, all_proba)","587","        for proba in all_proba:","590","        if len(all_proba) == 1:","591","            return all_proba[0]","593","            return all_proba","682","        # avoid storing the output of every estimator by summing them here","683","        if self.n_outputs_ > 1:","684","            y_hat = np.zeros((X.shape[0], self.n_outputs_), dtype=np.float64)","685","        else:","686","            y_hat = np.zeros((X.shape[0]), dtype=np.float64)","687","","689","        Parallel(n_jobs=n_jobs, verbose=self.verbose, backend=\"threading\")(","690","            delayed(accumulate_prediction)(e.predict, X, [y_hat])","693","        y_hat \/= len(self.estimators_)"],"delete":["567","        # Parallel loop","568","        all_proba = Parallel(n_jobs=n_jobs, verbose=self.verbose,","569","                             backend=\"threading\")(","570","            delayed(parallel_helper)(e, 'predict_proba', X,","571","                                      check_input=False)","574","        # Reduce","575","        proba = all_proba[0]","576","","577","        if self.n_outputs_ == 1:","578","            for j in range(1, len(all_proba)):","579","                proba += all_proba[j]","580","","584","            for j in range(1, len(all_proba)):","585","                for k in range(self.n_outputs_):","586","                    proba[k] += all_proba[j][k]","587","","588","            for k in range(self.n_outputs_):","589","                proba[k] \/= self.n_estimators","590","","591","        return proba","681","        all_y_hat = Parallel(n_jobs=n_jobs, verbose=self.verbose,","682","                             backend=\"threading\")(","683","            delayed(parallel_helper)(e, 'predict', X, check_input=False)","686","        # Reduce","687","        y_hat = sum(all_y_hat) \/ len(self.estimators_)"]}]}},"b89fcd303b40bddd44ee595ff135040be2b32544":{"changes":{"sklearn\/gaussian_process\/kernels.py":"MODIFY"},"diff":{"sklearn\/gaussian_process\/kernels.py":[{"add":["1854","        return np.apply_along_axis(self, 1, X).ravel()"],"delete":["1854","        return np.apply_along_axis(self, 1, X)[:, 0]"]}]}},"1b2a9285affb385fc84057b3edf52fa6153cf76a":{"changes":{"sklearn\/feature_extraction\/text.py":"MODIFY"},"diff":{"sklearn\/feature_extraction\/text.py":[{"add":["161","        operating only inside word boundaries. n-grams at the edges","162","        of words are padded with space.\"\"\"","357","        word boundaries; n-grams at the edges of words are padded with space.","556","        word boundaries; n-grams at the edges of words are padded with space."],"delete":["161","        excluding any whitespace (operating only inside word boundaries)\"\"\"","356","        word boundaries.","555","        word boundaries."]}]}},"6e8ebdac4e2931c9a964343c0d650862efc067dd":{"changes":{"sklearn\/linear_model\/ransac.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/linear_model\/tests\/test_ransac.py":"MODIFY"},"diff":{"sklearn\/linear_model\/ransac.py":[{"add":["344","        self.n_trials_ = 0","345","        max_trials = self.max_trials","346","        while self.n_trials_ < max_trials:","347","            self.n_trials_ += 1","421","            max_trials = min(","422","                max_trials,","423","                _dynamic_max_trials(n_inliers_best, n_samples,","424","                                    min_samples, self.stop_probability))","425","","427","            if n_inliers_best >= self.stop_n_inliers or \\","428","                            score_best >= self.stop_score:"],"delete":["344","        for self.n_trials_ in range(1, self.max_trials + 1):","419","            if (n_inliers_best >= self.stop_n_inliers","420","                    or score_best >= self.stop_score","421","                    or self.n_trials_","422","                       >= _dynamic_max_trials(n_inliers_best, n_samples,","423","                                              min_samples,","424","                                              self.stop_probability)):"]}],"doc\/whats_new.rst":[{"add":["197","   - Fixed a bug where :func:`sklearn.linear_model.RANSACRegressor.fit` may run until","198","     ``max_iter`` if finds a large inlier group early. :issue:`8251` by :user:`aivision2020`.","199",""],"delete":[]}],"sklearn\/linear_model\/tests\/test_ransac.py":[{"add":["24","rng = np.random.RandomState(1000)","25","outliers = np.unique(rng.randint(len(X), size=200))","26","data[outliers, :] += 50 + rng.rand(len(outliers), 2) * 10","91","    # there is a 1e-9 chance it will take these many trials. No good reason","92","    # 1e-2 isn't enough, can still happen","93","    # 2 is the what ransac defines  as min_samples = X.shape[1] + 1","94","    max_trials = _dynamic_max_trials(","95","        len(X) - len(outliers), X.shape[0], 2, 1 - 1e-9)","96","    ransac_estimator = RANSACRegressor(base_estimator, min_samples=2)","97","    for i in range(50):","98","        ransac_estimator.set_params(min_samples=2, random_state=i)","99","        ransac_estimator.fit(X, y)","100","        assert_less(ransac_estimator.n_trials_, max_trials + 1)","387",""],"delete":["24","outliers = np.array((10, 30, 200))","25","data[outliers[0], :] = (1000, 1000)","26","data[outliers[1], :] = (-1000, -1000)","27","data[outliers[2], :] = (-100, -50)","92","    ransac_estimator = RANSACRegressor(base_estimator, min_samples=2,","93","                                       residual_threshold=5, max_trials=11,","94","                                       random_state=0)","95","    assert getattr(ransac_estimator, 'n_trials_', None) is None","96","    ransac_estimator.fit(X, y)","97","    assert_equal(ransac_estimator.n_trials_, 2)","98",""]}]}},"39076ff385c84934301409c5e2e907dceddb7b22":{"changes":{"sklearn\/gaussian_process\/tests\/test_kernels.py":"MODIFY"},"diff":{"sklearn\/gaussian_process\/tests\/test_kernels.py":[{"add":["285","            if isinstance(\"string\", type(hyperparameter.bounds)):","286","                if hyperparameter.bounds == \"fixed\":","287","                    continue","301","            if isinstance(\"string\", type(hyperparameter.bounds)):","302","                if hyperparameter.bounds == \"fixed\":","303","                    continue"],"delete":["285","            if hyperparameter.bounds == \"fixed\":","286","                continue","300","            if hyperparameter.bounds == \"fixed\":","301","                continue"]}]}},"05b5b371743300dc8bba36f43c46a1b8f7ee984c":{"changes":{"sklearn\/linear_model\/coordinate_descent.py":"MODIFY"},"diff":{"sklearn\/linear_model\/coordinate_descent.py":[{"add":["1560","        For l1_ratio = 1 the penalty is an L1\/L2 penalty. For l1_ratio = 0 it","1561","        is an L2 penalty.","1875","        For l1_ratio = 1 the penalty is an L1\/L2 penalty. For l1_ratio = 0 it","1876","        is an L2 penalty."],"delete":["1560","        For l1_ratio = 0 the penalty is an L1\/L2 penalty. For l1_ratio = 1 it","1561","        is an L1 penalty.","1875","        For l1_ratio = 0 the penalty is an L1\/L2 penalty. For l1_ratio = 1 it","1876","        is an L1 penalty."]}]}},"e19bb7ce5a0c8d1d2a2575d4a3181aec18494509":{"changes":{"sklearn\/model_selection\/tests\/test_validation.py":"MODIFY"},"diff":{"sklearn\/model_selection\/tests\/test_validation.py":[{"add":["923","    kfold = KFold()"],"delete":["923","    kfold = KFold(len(iris.target))"]}]}},"5210f810f78d7aa102cb56e6b586880a771980b4":{"changes":{"sklearn\/feature_selection\/univariate_selection.py":"MODIFY"},"diff":{"sklearn\/feature_selection\/univariate_selection.py":[{"add":["232","    Linear model for testing the individual effect of each of many regressors.","233","    This is a scoring function to be used in a feature seletion procedure, not","234","    a free standing feature selection procedure.","238","    1. The correlation between each regressor and the target is computed,","243","    For more on usage see the :ref:`User Guide <univariate_feature_selection>`.","264","","267","    mutual_info_regression: Mutual information for a continuous target.","270","    SelectKBest: Select features based on the k highest scores.","271","    SelectFpr: Select features based on a false positive rate test.","272","    SelectFdr: Select features based on an estimated false discovery rate.","273","    SelectFwe: Select features based on family-wise error rate.","274","    SelectPercentile: Select features based on percentile of the highest","275","        scores."],"delete":["232","    Quick linear model for testing the effect of a single regressor,","233","    sequentially for many regressors.","237","    1. The cross correlation between each regressor and the target is computed,","242","    Read more in the :ref:`User Guide <univariate_feature_selection>`."]}]}},"9e0e2d41fe6bf35bb98ede7ee037724a2d790f58":{"changes":{"doc\/modules\/feature_extraction.rst":"MODIFY","examples\/model_selection\/plot_nested_cross_validation_iris.py":"MODIFY"},"diff":{"doc\/modules\/feature_extraction.rst":[{"add":["0","?.. _feature_extraction:","44","  ...     {'city': 'San Francisco', 'temperature': 18.},","56","  ['city=Dubai', 'city=London', 'city=San Francisco', 'temperature']"],"delete":["0",".. _feature_extraction:","44","  ...     {'city': 'San Fransisco', 'temperature': 18.},","56","  ['city=Dubai', 'city=London', 'city=San Fransisco', 'temperature']"]}],"examples\/model_selection\/plot_nested_cross_validation_iris.py":[{"add":["66","svm = SVC(kernel=\"rbf\")","82","    clf = GridSearchCV(estimator=svm, param_grid=p_grid, cv=inner_cv)"],"delete":["66","svr = SVC(kernel=\"rbf\")","82","    clf = GridSearchCV(estimator=svr, param_grid=p_grid, cv=inner_cv)"]}]}},"fc2f24927fc37d7e42917369f17de045b14c59b5":{"changes":{"sklearn\/tree\/_tree.pxd":"MODIFY","sklearn\/tree\/tree.py":"MODIFY","sklearn\/ensemble\/gradient_boosting.py":"MODIFY","sklearn\/ensemble\/tests\/test_forest.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/tree\/_tree.pyx":"MODIFY","sklearn\/ensemble\/tests\/test_gradient_boosting.py":"MODIFY","sklearn\/ensemble\/forest.py":"MODIFY","sklearn\/tree\/tests\/test_tree.py":"MODIFY"},"diff":{"sklearn\/tree\/_tree.pxd":[{"add":["92","    cdef Splitter splitter              # Splitting algorithm","94","    cdef SIZE_t min_samples_split       # Minimum number of samples in an internal node","95","    cdef SIZE_t min_samples_leaf        # Minimum number of samples in a leaf","96","    cdef double min_weight_leaf         # Minimum weight in a leaf","97","    cdef SIZE_t max_depth               # Maximal tree depth","98","    cdef double min_impurity_split","99","    cdef double min_impurity_decrease   # Impurity threshold for early stopping"],"delete":["92","    cdef Splitter splitter          # Splitting algorithm","94","    cdef SIZE_t min_samples_split   # Minimum number of samples in an internal node","95","    cdef SIZE_t min_samples_leaf    # Minimum number of samples in a leaf","96","    cdef double min_weight_leaf     # Minimum weight in a leaf","97","    cdef SIZE_t max_depth           # Maximal tree depth","98","    cdef double min_impurity_split  # Impurity threshold for early stopping"]}],"sklearn\/tree\/tree.py":[{"add":["20","import warnings","92","                 min_impurity_decrease,","105","        self.min_impurity_decrease = min_impurity_decrease","277","        if self.min_impurity_split is not None:","278","            warnings.warn(\"The min_impurity_split parameter is deprecated and\"","279","                          \" will be removed in version 0.21. \"","280","                          \"Use the min_impurity_decrease parameter instead.\",","281","                          DeprecationWarning)","282","            min_impurity_split = self.min_impurity_split","283","        else:","284","            min_impurity_split = 1e-7","285","","286","        if min_impurity_split < 0.:","290","        if self.min_impurity_decrease < 0.:","291","            raise ValueError(\"min_impurity_decrease must be greater than \"","292","                             \"or equal to 0\")","293","","349","                                            max_depth,","350","                                            self.min_impurity_decrease,","351","                                            min_impurity_split)","358","                                           self.min_impurity_decrease,","359","                                           min_impurity_split)","608","    min_impurity_decrease : float, optional (default=0.)","609","        A node will be split if this split induces a decrease of the impurity","610","        greater than or equal to this value.","612","        The weighted impurity decrease equation is the following::","613","","614","            N_t \/ N * (impurity - N_t_R \/ N_t * right_impurity","615","                                - N_t_L \/ N_t * left_impurity)","616","","617","        where ``N`` is the total number of samples, ``N_t`` is the number of","618","        samples at the current node, ``N_t_L`` is the number of samples in the","619","        left child, and ``N_t_R`` is the number of samples in the right child.","620","","621","        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,","622","        if ``sample_weight`` is passed.","623","","624","        .. versionadded:: 0.19","712","                 min_impurity_decrease=0.,","713","                 min_impurity_split=None,","727","            min_impurity_decrease=min_impurity_decrease,","932","    min_impurity_decrease : float, optional (default=0.)","933","        A node will be split if this split induces a decrease of the impurity","934","        greater than or equal to this value.","936","        The weighted impurity decrease equation is the following::","937","","938","            N_t \/ N * (impurity - N_t_R \/ N_t * right_impurity","939","                                - N_t_L \/ N_t * left_impurity)","940","","941","        where ``N`` is the total number of samples, ``N_t`` is the number of","942","        samples at the current node, ``N_t_L`` is the number of samples in the","943","        left child, and ``N_t_R`` is the number of samples in the right child.","944","","945","        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,","946","        if ``sample_weight`` is passed.","947","","948","        .. versionadded:: 0.19","1028","                 min_impurity_decrease=0.,","1029","                 min_impurity_split=None,","1041","            min_impurity_decrease=min_impurity_decrease,","1123","                 min_impurity_decrease=0.,","1124","                 min_impurity_split=None,","1136","            min_impurity_decrease=min_impurity_decrease,","1174","                 min_impurity_decrease=0.,","1175","                 min_impurity_split=None,","1186","            min_impurity_decrease=min_impurity_decrease,"],"delete":["274","        if self.min_impurity_split < 0.:","333","                                            max_depth, self.min_impurity_split)","340","                                           self.min_impurity_split)","589","    min_impurity_split : float, optional (default=1e-7)","590","        Threshold for early stopping in tree growth. A node will split","591","        if its impurity is above the threshold, otherwise it is a leaf.","593","        .. versionadded:: 0.18","681","                 min_impurity_split=1e-7,","899","    min_impurity_split : float, optional (default=1e-7)","900","        Threshold for early stopping in tree growth. If the impurity","901","        of a node is below the threshold, the node is a leaf.","903","        .. versionadded:: 0.18","983","                 min_impurity_split=1e-7,","1076","                 min_impurity_split=1e-7,","1125","                 min_impurity_split=1e-7,"]}],"sklearn\/ensemble\/gradient_boosting.py":[{"add":["725","                 max_depth, min_impurity_decrease, min_impurity_split,","726","                 init, subsample, max_features,","740","        self.min_impurity_decrease = min_impurity_decrease","773","                min_impurity_decrease=self.min_impurity_decrease,","1329","    min_impurity_decrease : float, optional (default=0.)","1330","        A node will be split if this split induces a decrease of the impurity","1331","        greater than or equal to this value.","1333","        The weighted impurity decrease equation is the following::","1334","","1335","            N_t \/ N * (impurity - N_t_R \/ N_t * right_impurity","1336","                                - N_t_L \/ N_t * left_impurity)","1337","","1338","        where ``N`` is the total number of samples, ``N_t`` is the number of","1339","        samples at the current node, ``N_t_L`` is the number of samples in the","1340","        left child, and ``N_t_R`` is the number of samples in the right child.","1341","","1342","        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,","1343","        if ``sample_weight`` is passed.","1344","","1345","        .. versionadded:: 0.19","1434","                 max_depth=3, min_impurity_decrease=0.,","1435","                 min_impurity_split=1e-7, init=None,","1449","            min_impurity_decrease=min_impurity_decrease,","1734","    min_impurity_decrease : float, optional (default=0.)","1735","        A node will be split if this split induces a decrease of the impurity","1736","        greater than or equal to this value.","1738","        The weighted impurity decrease equation is the following::","1739","","1740","            N_t \/ N * (impurity - N_t_R \/ N_t * right_impurity","1741","                                - N_t_L \/ N_t * left_impurity)","1742","","1743","        where ``N`` is the total number of samples, ``N_t`` is the number of","1744","        samples at the current node, ``N_t_L`` is the number of samples in the","1745","        left child, and ``N_t_R`` is the number of samples in the right child.","1746","","1747","        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,","1748","        if ``sample_weight`` is passed.","1749","","1750","        .. versionadded:: 0.19","1842","                 max_depth=3, min_impurity_decrease=0.,","1843","                 min_impurity_split=1e-7, init=None, random_state=None,","1853","            max_features=max_features,","1854","            min_impurity_decrease=min_impurity_decrease,","1855","            min_impurity_split=min_impurity_split,"],"delete":["725","                 max_depth, min_impurity_split, init, subsample, max_features,","1326","    min_impurity_split : float, optional (default=1e-7)","1327","        Threshold for early stopping in tree growth. A node will split","1328","        if its impurity is above the threshold, otherwise it is a leaf.","1330","        .. versionadded:: 0.18","1419","                 max_depth=3, min_impurity_split=1e-7, init=None,","1717","    min_impurity_split : float, optional (default=1e-7)","1718","        Threshold for early stopping in tree growth. A node will split","1719","        if its impurity is above the threshold, otherwise it is a leaf.","1721","        .. versionadded:: 0.18","1813","                 max_depth=3, min_impurity_split=1e-7, init=None, random_state=None,","1823","            max_features=max_features, min_impurity_split=min_impurity_split,"]}],"sklearn\/ensemble\/tests\/test_forest.py":[{"add":["30","from sklearn.utils.testing import assert_warns_message","1183","","1184","","1185","def test_min_impurity_split():","1186","    # Test if min_impurity_split of base estimators is set","1187","    # Regression test for #8006","1188","    X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1)","1189","    all_estimators = [RandomForestClassifier, RandomForestRegressor,","1190","                      ExtraTreesClassifier, ExtraTreesRegressor]","1191","","1192","    for Estimator in all_estimators:","1193","        est = Estimator(min_impurity_split=0.1)","1194","        est = assert_warns_message(DeprecationWarning, \"min_impurity_decrease\",","1195","                                   est.fit, X, y)","1196","        for tree in est.estimators_:","1197","            assert_equal(tree.min_impurity_split, 0.1)","1198","","1199","","1200","def test_min_impurity_decrease():","1201","    X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1)","1202","    all_estimators = [RandomForestClassifier, RandomForestRegressor,","1203","                      ExtraTreesClassifier, ExtraTreesRegressor]","1204","","1205","    for Estimator in all_estimators:","1206","        est = Estimator(min_impurity_decrease=0.1)","1207","        est.fit(X, y)","1208","        for tree in est.estimators_:","1209","            # Simply check if the parameter is passed on correctly. Tree tests","1210","            # will suffice for the actual working of this param","1211","            assert_equal(tree.min_impurity_decrease, 0.1)"],"delete":[]}],"doc\/whats_new.rst":[{"add":["311","   - All tree based estimators now accept a ``min_impurity_decrease``","312","     parameter in lieu of the ``min_impurity_split``, which is now deprecated.","313","     The ``min_impurity_decrease`` helps stop splitting the nodes in which","314","     the weighted impurity decrease from splitting is no longer alteast","315","     ``min_impurity_decrease``.  :issue:`8449` by `Raghav RV_`","316",""],"delete":[]}],"sklearn\/tree\/_tree.pyx":[{"add":["21","from libc.math cimport fabs","54","cdef double EPSILON = np.finfo('double').eps","134","                  SIZE_t max_depth, double min_impurity_decrease,","135","                  double min_impurity_split):","141","        self.min_impurity_decrease = min_impurity_decrease","172","        cdef double min_impurity_decrease = self.min_impurity_decrease","236","                    # If EPSILON=0 in the below comparison, float precision","237","                    # issues stop splitting, producing trees that are","238","                    # dissimilar to v0.18","239","                    is_leaf = (is_leaf or split.pos >= end or","240","                               (split.improvement + EPSILON <","241","                                min_impurity_decrease))","305","                  double min_impurity_decrease, double min_impurity_split):","312","        self.min_impurity_decrease = min_impurity_decrease","439","        cdef double min_impurity_decrease = self.min_impurity_decrease","460","            # If EPSILON=0 in the below comparison, float precision issues stop","461","            # splitting early, producing trees that are dissimilar to v0.18","462","            is_leaf = (is_leaf or split.pos >= end or","463","                       split.improvement + EPSILON < min_impurity_decrease)"],"delete":["132","                  SIZE_t max_depth, double min_impurity_split):","231","                    is_leaf = is_leaf or (split.pos >= end)","295","                  double min_impurity_split):","448","            is_leaf = is_leaf or (split.pos >= end)"]}],"sklearn\/ensemble\/tests\/test_gradient_boosting.py":[{"add":["28","from sklearn.utils.testing import assert_warns_message","968","    all_estimators = [GradientBoostingRegressor, GradientBoostingClassifier]","971","        est = GBEstimator(min_impurity_split=0.1)","972","        est = assert_warns_message(DeprecationWarning, \"min_impurity_decrease\",","973","                                   est.fit, X, y)","978","def test_min_impurity_decrease():","979","    X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1)","980","    all_estimators = [GradientBoostingRegressor, GradientBoostingClassifier]","981","","982","    for GBEstimator in all_estimators:","983","        est = GBEstimator(min_impurity_decrease=0.1)","984","        est.fit(X, y)","985","        for tree in est.estimators_.flat:","986","            # Simply check if the parameter is passed on correctly. Tree tests","987","            # will suffice for the actual working of this param","988","            assert_equal(tree.min_impurity_decrease, 0.1)","989","","990",""],"delete":["967","    all_estimators = [GradientBoostingRegressor,","968","                      GradientBoostingClassifier]","971","        est = GBEstimator(min_impurity_split=0.1).fit(X, y)"]}],"sklearn\/ensemble\/forest.py":[{"add":["815","    min_impurity_decrease : float, optional (default=0.)","816","        A node will be split if this split induces a decrease of the impurity","817","        greater than or equal to this value.","819","        The weighted impurity decrease equation is the following::","820","","821","            N_t \/ N * (impurity - N_t_R \/ N_t * right_impurity","822","                                - N_t_L \/ N_t * left_impurity)","823","","824","        where ``N`` is the total number of samples, ``N_t`` is the number of","825","        samples at the current node, ``N_t_L`` is the number of samples in the","826","        left child, and ``N_t_R`` is the number of samples in the right child.","827","","828","        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,","829","        if ``sample_weight`` is passed.","830","","831","        .. versionadded:: 0.19","936","                 min_impurity_decrease=0.,","937","                 min_impurity_split=None,","950","                              \"max_features\", \"max_leaf_nodes\",","951","                              \"min_impurity_decrease\", \"min_impurity_split\",","968","        self.min_impurity_decrease = min_impurity_decrease","1051","    min_impurity_decrease : float, optional (default=0.)","1052","        A node will be split if this split induces a decrease of the impurity","1053","        greater than or equal to this value.","1055","        The weighted impurity decrease equation is the following::","1056","","1057","            N_t \/ N * (impurity - N_t_R \/ N_t * right_impurity","1058","                                - N_t_L \/ N_t * left_impurity)","1059","","1060","        where ``N`` is the total number of samples, ``N_t`` is the number of","1061","        samples at the current node, ``N_t_L`` is the number of samples in the","1062","        left child, and ``N_t_R`` is the number of samples in the right child.","1063","","1064","        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,","1065","        if ``sample_weight`` is passed.","1066","","1067","        .. versionadded:: 0.19","1141","                 min_impurity_decrease=0.,","1142","                 min_impurity_split=None,","1154","                              \"max_features\", \"max_leaf_nodes\",","1155","                              \"min_impurity_decrease\", \"min_impurity_split\",","1171","        self.min_impurity_decrease = min_impurity_decrease","1247","    min_impurity_decrease : float, optional (default=0.)","1248","        A node will be split if this split induces a decrease of the impurity","1249","        greater than or equal to this value.","1251","        The weighted impurity decrease equation is the following::","1252","","1253","            N_t \/ N * (impurity - N_t_R \/ N_t * right_impurity","1254","                                - N_t_L \/ N_t * left_impurity)","1255","","1256","        where ``N`` is the total number of samples, ``N_t`` is the number of","1257","        samples at the current node, ``N_t_L`` is the number of samples in the","1258","        left child, and ``N_t_R`` is the number of samples in the right child.","1259","","1260","        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,","1261","        if ``sample_weight`` is passed.","1262","","1263","        .. versionadded:: 0.19","1360","                 min_impurity_decrease=0.,","1361","                 min_impurity_split=None,","1374","                              \"max_features\", \"max_leaf_nodes\",","1375","                              \"min_impurity_decrease\", \"min_impurity_split\",","1392","        self.min_impurity_decrease = min_impurity_decrease","1473","    min_impurity_decrease : float, optional (default=0.)","1474","        A node will be split if this split induces a decrease of the impurity","1475","        greater than or equal to this value.","1477","        The weighted impurity decrease equation is the following::","1478","","1479","            N_t \/ N * (impurity - N_t_R \/ N_t * right_impurity","1480","                                - N_t_L \/ N_t * left_impurity)","1481","","1482","        where ``N`` is the total number of samples, ``N_t`` is the number of","1483","        samples at the current node, ``N_t_L`` is the number of samples in the","1484","        left child, and ``N_t_R`` is the number of samples in the right child.","1485","","1486","        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,","1487","        if ``sample_weight`` is passed.","1488","","1489","        .. versionadded:: 0.19","1555","                 min_impurity_decrease=0.,","1556","                 min_impurity_split=None,","1568","                              \"max_features\", \"max_leaf_nodes\",","1569","                              \"min_impurity_decrease\", \"min_impurity_split\",","1585","        self.min_impurity_decrease = min_impurity_decrease","1646","    min_impurity_decrease : float, optional (default=0.)","1647","        A node will be split if this split induces a decrease of the impurity","1648","        greater than or equal to this value.","1650","        The weighted impurity decrease equation is the following::","1651","","1652","            N_t \/ N * (impurity - N_t_R \/ N_t * right_impurity","1653","                                - N_t_L \/ N_t * left_impurity)","1654","","1655","        where ``N`` is the total number of samples, ``N_t`` is the number of","1656","        samples at the current node, ``N_t_L`` is the number of samples in the","1657","        left child, and ``N_t_R`` is the number of samples in the right child.","1658","","1659","        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,","1660","        if ``sample_weight`` is passed.","1661","","1662","        .. versionadded:: 0.19","1663","","1664","    bootstrap : boolean, optional (default=True)","1665","        Whether bootstrap samples are used when building trees.","1711","                 min_impurity_decrease=0.,","1712","                 min_impurity_split=None,","1723","                              \"max_features\", \"max_leaf_nodes\",","1724","                              \"min_impurity_decrease\", \"min_impurity_split\",","1740","        self.min_impurity_decrease = min_impurity_decrease"],"delete":["815","    min_impurity_split : float, optional (default=1e-7)","816","        Threshold for early stopping in tree growth. A node will split","817","        if its impurity is above the threshold, otherwise it is a leaf.","819","        .. versionadded:: 0.18","924","                 min_impurity_split=1e-7,","937","                              \"max_features\", \"max_leaf_nodes\", \"min_impurity_split\",","1036","    min_impurity_split : float, optional (default=1e-7)","1037","        Threshold for early stopping in tree growth. A node will split","1038","        if its impurity is above the threshold, otherwise it is a leaf.","1040","        .. versionadded:: 0.18","1114","                 min_impurity_split=1e-7,","1126","                              \"max_features\", \"max_leaf_nodes\", \"min_impurity_split\",","1217","    min_impurity_split : float, optional (default=1e-7)","1218","        Threshold for early stopping in tree growth. A node will split","1219","        if its impurity is above the threshold, otherwise it is a leaf.","1221","        .. versionadded:: 0.18","1318","                 min_impurity_split=1e-7,","1331","                              \"max_features\", \"max_leaf_nodes\", \"min_impurity_split\",","1428","    min_impurity_split : float, optional (default=1e-7)","1429","        Threshold for early stopping in tree growth. A node will split","1430","        if its impurity is above the threshold, otherwise it is a leaf.","1432","        .. versionadded:: 0.18","1498","                 min_impurity_split=1e-7,","1510","                              \"max_features\", \"max_leaf_nodes\", \"min_impurity_split\",","1586","    min_impurity_split : float, optional (default=1e-7)","1587","        Threshold for early stopping in tree growth. A node will split","1588","        if its impurity is above the threshold, otherwise it is a leaf.","1590","        .. versionadded:: 0.18","1636","                 min_impurity_split=1e-7,","1647","                              \"max_features\", \"max_leaf_nodes\", \"min_impurity_split\","]}],"sklearn\/tree\/tests\/test_tree.py":[{"add":["30","from sklearn.utils.testing import assert_warns","31","from sklearn.utils.testing import assert_warns_message","529","        assert_raises(ValueError,","530","                      TreeEstimator(min_impurity_decrease=-1.0).fit, X, y)","804","        assert_true(est.min_impurity_split is None,","805","                    \"Failed, min_impurity_split = {0} > 1e-7\".format(","806","                        est.min_impurity_split))","807","        try:","808","            assert_warns(DeprecationWarning, est.fit, X, y)","809","        except AssertionError:","810","            pass","813","                    est.tree_.children_right[node] == TREE_LEAF):","820","        # verify leaf nodes have impurity [0,min_impurity_split] when using","821","        # min_impurity_split","825","        assert_warns_message(DeprecationWarning,","826","                             \"Use the min_impurity_decrease\",","827","                             est.fit, X, y)","830","                    est.tree_.children_right[node] == TREE_LEAF):","843","def test_min_impurity_decrease():","844","    # test if min_impurity_decrease ensure that a split is made only if","845","    # if the impurity decrease is atleast that value","846","    X, y = datasets.make_classification(n_samples=10000, random_state=42)","847","","848","    # test both DepthFirstTreeBuilder and BestFirstTreeBuilder","849","    # by setting max_leaf_nodes","850","    for max_leaf_nodes, name in product((None, 1000), ALL_TREES.keys()):","851","        TreeEstimator = ALL_TREES[name]","852","","853","        # Check default value of min_impurity_decrease, 1e-7","854","        est1 = TreeEstimator(max_leaf_nodes=max_leaf_nodes, random_state=0)","855","        # Check with explicit value of 0.05","856","        est2 = TreeEstimator(max_leaf_nodes=max_leaf_nodes,","857","                             min_impurity_decrease=0.05, random_state=0)","858","        # Check with a much lower value of 0.0001","859","        est3 = TreeEstimator(max_leaf_nodes=max_leaf_nodes,","860","                             min_impurity_decrease=0.0001, random_state=0)","861","        # Check with a much lower value of 0.1","862","        est4 = TreeEstimator(max_leaf_nodes=max_leaf_nodes,","863","                             min_impurity_decrease=0.1, random_state=0)","864","","865","        for est, expected_decrease in ((est1, 1e-7), (est2, 0.05),","866","                                       (est3, 0.0001), (est4, 0.1)):","867","            assert_less_equal(est.min_impurity_decrease, expected_decrease,","868","                              \"Failed, min_impurity_decrease = {0} > {1}\"","869","                              .format(est.min_impurity_decrease,","870","                                      expected_decrease))","871","            est.fit(X, y)","872","            for node in range(est.tree_.node_count):","873","                # If current node is a not leaf node, check if the split was","874","                # justified w.r.t the min_impurity_decrease","875","                if est.tree_.children_left[node] != TREE_LEAF:","876","                    imp_parent = est.tree_.impurity[node]","877","                    wtd_n_node = est.tree_.weighted_n_node_samples[node]","878","","879","                    left = est.tree_.children_left[node]","880","                    wtd_n_left = est.tree_.weighted_n_node_samples[left]","881","                    imp_left = est.tree_.impurity[left]","882","                    wtd_imp_left = wtd_n_left * imp_left","883","","884","                    right = est.tree_.children_right[node]","885","                    wtd_n_right = est.tree_.weighted_n_node_samples[right]","886","                    imp_right = est.tree_.impurity[right]","887","                    wtd_imp_right = wtd_n_right * imp_right","888","","889","                    wtd_avg_left_right_imp = wtd_imp_right + wtd_imp_left","890","                    wtd_avg_left_right_imp \/= wtd_n_node","891","","892","                    fractional_node_weight = (","893","                        est.tree_.weighted_n_node_samples[node] \/ X.shape[0])","894","","895","                    actual_decrease = fractional_node_weight * (","896","                        imp_parent - wtd_avg_left_right_imp)","897","","898","                    assert_greater_equal(actual_decrease, expected_decrease,","899","                                         \"Failed with {0} \"","900","                                         \"expected min_impurity_decrease={1}\"","901","                                         .format(actual_decrease,","902","                                                 expected_decrease))","1684","    dt_mae.fit([[3], [5], [3], [8], [5]], [6, 7, 3, 4, 3])","1688","    dt_mae.fit([[3], [5], [3], [8], [5]], [6, 7, 3, 4, 3],","1689","               [0.6, 0.3, 0.1, 1.0, 0.3])"],"delete":["800","        assert_less_equal(est.min_impurity_split, 1e-7,","801","                     \"Failed, min_impurity_split = {0} > 1e-7\".format(","802","                         est.min_impurity_split))","803","        est.fit(X, y)","806","                est.tree_.children_right[node] == TREE_LEAF):","813","        # verify leaf nodes have impurity [0,min_impurity_split] when using min_impurity_split","817","        est.fit(X, y)","820","                est.tree_.children_right[node] == TREE_LEAF):","833","def test_pickle():","1615","    dt_mae.fit([[3],[5],[3],[8],[5]],[6,7,3,4,3])","1619","    dt_mae.fit([[3],[5],[3],[8],[5]],[6,7,3,4,3], [0.6,0.3,0.1,1.0,0.3])"]}]}},"306de84cf1df605f2d8963f242f84cfcb9eeb8d4":{"changes":{"doc\/modules\/sgd.rst":"MODIFY"},"diff":{"doc\/modules\/sgd.rst":[{"add":["281",":math:`x_i \\in \\mathbf{R}^m` and :math:`y_i \\in \\{-1,1\\}`, our goal is to"],"delete":["281",":math:`x_i \\in \\mathbf{R}^n` and :math:`y_i \\in \\{-1,1\\}`, our goal is to"]}]}},"2a1408a71d5656f5bfaef7d072b83045cba3b9e6":{"changes":{"sklearn\/linear_model\/least_angle.py":"MODIFY"},"diff":{"sklearn\/linear_model\/least_angle.py":[{"add":["1170","    @deprecated(\"Attribute cv_mse_path_ is deprecated in 0.18 and \"","1171","                \"will be removed in 0.20. Use 'mse_path_' instead\")"],"delete":["1170","    @deprecated(\"Attribute mse_path_ is deprecated in 0.18 and \"","1171","                \"will be removed in 0.20. Use 'cv_mse_path_' instead\")"]}]}},"29d200249bfd8d4fc582e13ad462650d2c68f7a2":{"changes":{"doc\/modules\/decomposition.rst":"MODIFY"},"diff":{"doc\/modules\/decomposition.rst":[{"add":["145","If we note :math:`n_{\\max} = \\max(n_{\\mathrm{samples}}, n_{\\mathrm{features}})` and","146",":math:`n_{\\min} = \\min(n_{\\mathrm{samples}}, n_{\\mathrm{features}})`, the time complexity","147","of the randomized :class:`PCA` is :math:`O(n_{\\max}^2 \\cdot n_{\\mathrm{components}})`","148","instead of :math:`O(n_{\\max}^2 \\cdot n_{\\min})` for the exact method","152",":math:`2 \\cdot n_{\\max} \\cdot n_{\\mathrm{components}}` instead of :math:`n_{\\max}","153","\\cdot n_{\\min}` for the exact method.","435","                0 \\leq k < n_{\\mathrm{atoms}}","557","* :math:`\\Psi = \\mathrm{diag}(\\psi_1, \\psi_2, \\dots, \\psi_n)`: This model is called","663","    d_{\\mathrm{Fro}}(X, Y) = \\frac{1}{2} ||X - Y||_{\\mathrm{Fro}}^2 = \\frac{1}{2} \\sum_{i,j} (X_{ij} - {Y}_{ij})^2","716","    + \\frac{\\alpha(1-\\rho)}{2} ||W||_{\\mathrm{Fro}} ^ 2","717","    + \\frac{\\alpha(1-\\rho)}{2} ||H||_{\\mathrm{Fro}} ^ 2","722","    d_{\\mathrm{Fro}}(X, WH)","724","    + \\frac{\\alpha(1-\\rho)}{2} ||W||_{\\mathrm{Fro}} ^ 2","725","    + \\frac{\\alpha(1-\\rho)}{2} ||H||_{\\mathrm{Fro}} ^ 2","739","    d_{\\mathrm{Fro}}(X, Y) = \\frac{1}{2} ||X - Y||_{Fro}^2 = \\frac{1}{2} \\sum_{i,j} (X_{ij} - {Y}_{ij})^2","745","    d_{KL}(X, Y) = \\sum_{i,j} (X_{ij} \\log(\\frac{X_{ij}}{Y_{ij}}) - X_{ij} + Y_{ij})","750","    d_{IS}(X, Y) = \\sum_{i,j} (\\frac{X_{ij}}{Y_{ij}} - \\log(\\frac{X_{ij}}{Y_{ij}}) - 1)","843","  1. For each topic :math:`k`, draw :math:`\\beta_k \\sim \\mathrm{Dirichlet}(\\eta),\\: k =1...K`","845","  2. For each document :math:`d`, draw :math:`\\theta_d \\sim \\mathrm{Dirichlet}(\\alpha), \\: d=1...D`","849","    a. Draw a topic index :math:`z_{di} \\sim \\mathrm{Multinomial}(\\theta_d)`","850","    b. Draw the observed word :math:`w_{ij} \\sim \\mathrm{Multinomial}(beta_{z_{di}}.)`","864","  \\log\\: P(w | \\alpha, \\eta) \\geq L(w,\\phi,\\gamma,\\lambda) \\overset{\\triangle}{=}","865","    E_{q}[\\log\\:p(w,z,\\theta,\\beta|\\alpha,\\eta)] - E_{q}[\\log\\:q(z, \\theta, \\beta)]"],"delete":["145","If we note :math:`n_{max} = max(n_{samples}, n_{features})` and","146",":math:`n_{min} = min(n_{samples}, n_{features})`, the time complexity","147","of the randomized :class:`PCA` is :math:`O(n_{max}^2 \\cdot n_{components})`","148","instead of :math:`O(n_{max}^2 \\cdot n_{min})` for the exact method","152",":math:`2 \\cdot n_{max} \\cdot n_{components}` instead of :math:`n_{max}","153","\\cdot n_{min}` for the exact method.","435","                0 \\leq k < n_{atoms}","557","* :math:`\\Psi = diag(\\psi_1, \\psi_2, \\dots, \\psi_n)`: This model is called","663","    d_{Fro}(X, Y) = \\frac{1}{2} ||X - Y||_{Fro}^2 = \\frac{1}{2} \\sum_{i,j} (X_{ij} - {Y}_{ij})^2","716","    + \\frac{\\alpha(1-\\rho)}{2} ||W||_{Fro} ^ 2","717","    + \\frac{\\alpha(1-\\rho)}{2} ||H||_{Fro} ^ 2","722","    d_{Fro}(X, WH)","724","    + \\frac{\\alpha(1-\\rho)}{2} ||W||_{Fro} ^ 2","725","    + \\frac{\\alpha(1-\\rho)}{2} ||H||_{Fro} ^ 2","739","    d_{Fro}(X, Y) = \\frac{1}{2} ||X - Y||_{Fro}^2 = \\frac{1}{2} \\sum_{i,j} (X_{ij} - {Y}_{ij})^2","745","    d_{KL}(X, Y) = \\sum_{i,j} (X_{ij} log(\\frac{X_{ij}}{Y_{ij}}) - X_{ij} + Y_{ij})","750","    d_{IS}(X, Y) = \\sum_{i,j} (\\frac{X_{ij}}{Y_{ij}} - log(\\frac{X_{ij}}{Y_{ij}}) - 1)","843","  1. For each topic :math:`k`, draw :math:`\\beta_k \\sim Dirichlet(\\eta),\\: k =1...K`","845","  2. For each document :math:`d`, draw :math:`\\theta_d \\sim Dirichlet(\\alpha), \\: d=1...D`","849","    a. Draw a topic index :math:`z_{di} \\sim Multinomial(\\theta_d)`","850","    b. Draw the observed word :math:`w_{ij} \\sim Multinomial(beta_{z_{di}}.)`","864","  log\\: P(w | \\alpha, \\eta) \\geq L(w,\\phi,\\gamma,\\lambda) \\overset{\\triangle}{=}","865","    E_{q}[log\\:p(w,z,\\theta,\\beta|\\alpha,\\eta)] - E_{q}[log\\:q(z, \\theta, \\beta)]"]}]}},"45d9182e6dfaecfa47471f10de9ebbb0f5fda2de":{"changes":{"sklearn\/utils\/validation.py":"MODIFY","sklearn\/utils\/tests\/test_validation.py":"MODIFY"},"diff":{"sklearn\/utils\/validation.py":[{"add":["82","        if X.dtype.kind in 'uib' and X.dtype.itemsize <= 4:","83","            return_dtype = np.float32","84","        else:","85","            return_dtype = np.float64","86","        return X.astype(return_dtype)"],"delete":["82","        return X.astype(np.float32 if X.dtype == np.int32 else np.float64)"]}],"sklearn\/utils\/tests\/test_validation.py":[{"add":["45","    assert_equal(X2.dtype, np.float32)","51","    assert_equal(X2.dtype, np.float64)","52","    # Test int dtypes <= 32bit","53","    tested_dtypes = [np.bool,","54","                     np.int8, np.int16, np.int32,","55","                     np.uint8, np.uint16, np.uint32]","56","    for dtype in tested_dtypes:","57","        X = X.astype(dtype)","58","        X2 = as_float_array(X)","59","        assert_equal(X2.dtype, np.float32)","60","","61","    # Test object dtype","62","    X = X.astype(object)","63","    X2 = as_float_array(X, copy=True)","64","    assert_equal(X2.dtype, np.float64)","65",""],"delete":["44","    # Checks that the return type is ok","46","    np.testing.assert_equal(X2.dtype, np.float32)","52","    # Checking that the new type is ok","53","    np.testing.assert_equal(X2.dtype, np.float64)"]}]}},"38d59c7b5a8d0658a0747b5e0ea37d0768524f62":{"changes":{"doc\/whats_new.rst":"MODIFY","sklearn\/linear_model\/randomized_l1.py":"MODIFY"},"diff":{"doc\/whats_new.rst":[{"add":["152","   - Fixed a bug where :class:`sklearn.linear_model.RandomizedLasso` and","153","     :class:`sklearn.linear_model.RandomizedLogisticRegression` breaks for","154","     sparse input.","155","     :issue:`8259` by :user:`Aman Dalmia <dalmia>`.","156",""],"delete":[]}],"sklearn\/linear_model\/randomized_l1.py":[{"add":["18","from ..base import BaseEstimator","21","from ..feature_selection.base import SelectorMixin","22","from ..utils import (as_float_array, check_random_state, check_X_y, safe_mask)","61","                                                   SelectorMixin)):","89","               Returns an instance of self.","123","    def _get_support_mask(self):","124","        \"\"\"Get the boolean mask indicating which features are selected.","125","","126","        Returns","127","        -------","128","        support : boolean array of shape [# input features]","129","                  An element is True iff its corresponding feature is selected","130","                  for retention.","131","        \"\"\"","133","        return self.scores_ > self.selection_threshold"],"delete":["18","from ..base import BaseEstimator, TransformerMixin","21","from ..utils import (as_float_array, check_random_state, check_X_y,","22","                     check_array, safe_mask)","61","                                                   TransformerMixin)):","89","            Returns an instance of self.","123","    def get_support(self, indices=False):","124","        \"\"\"Return a mask, or list, of the features\/indices selected.\"\"\"","126","","127","        mask = self.scores_ > self.selection_threshold","128","        return mask if not indices else np.where(mask)[0]","129","","130","    # XXX: the two function below are copy\/pasted from feature_selection,","131","    # Should we add an intermediate base class?","132","    def transform(self, X):","133","        \"\"\"Transform a new matrix using the selected features\"\"\"","134","        mask = self.get_support()","135","        X = check_array(X)","136","        if len(mask) != X.shape[1]:","137","            raise ValueError(\"X has a different shape than during fitting.\")","138","        return check_array(X)[:, safe_mask(X, mask)]","139","","140","    def inverse_transform(self, X):","141","        \"\"\"Transform a new matrix using the selected features\"\"\"","142","        support = self.get_support()","143","        if X.ndim == 1:","144","            X = X[None, :]","145","        Xt = np.zeros((X.shape[0], support.size))","146","        Xt[:, support] = X","147","        return Xt"]}]}},"195de6a154a4a940ebf66a57f0911c1afa7cf8ef":{"changes":{"benchmarks\/bench_isolation_forest.py":"MODIFY"},"diff":{"benchmarks\/bench_isolation_forest.py":[{"add":["10","","14","from sklearn.preprocessing import MultiLabelBinarizer","17","print(__doc__)","18","","19","","20","def print_outlier_ratio(y):","21","    \"\"\"","22","    Helper function to show the distinct value count of element in the target.","23","    Useful indicator for the datasets used in bench_isolation_forest.py.","24","    \"\"\"","25","    uniq, cnt = np.unique(y, return_counts=True)","26","    print(\"----- Target count values: \")","27","    for u, c in zip(uniq, cnt):","28","        print(\"------ %s -> %d occurences\" % (str(u), c))","29","    print(\"----- Outlier ratio: %.5f\" % (np.min(cnt) \/ len(y)))","30","","31","","35","# Set this to true for plotting score histograms for each dataset:","36","with_decision_function_histograms = False","38","# Removed the shuttle dataset because as of 2017-03-23 mldata.org is down:","39","# datasets = ['http', 'smtp', 'SA', 'SF', 'shuttle', 'forestcover']","40","datasets = ['http', 'smtp', 'SA', 'SF', 'forestcover']","41","","42","# Loop over all datasets for fitting and scoring the estimator:","44","","45","    # Loading and vectorizing the data:","46","    print('====== %s ======' % dat)","47","    print('--- Fetching data...')","48","    if dat in ['http', 'smtp', 'SF', 'SA']:","64","        print('----- ')","76","        print_outlier_ratio(y)","78","    print('--- Vectorizing data...')","81","        lb = MultiLabelBinarizer()","82","        x1 = lb.fit_transform(X[:, 1])","84","        y = (y != b'normal.').astype(int)","85","        print_outlier_ratio(y)","88","        lb = MultiLabelBinarizer()","89","        x1 = lb.fit_transform(X[:, 1])","90","        x2 = lb.fit_transform(X[:, 2])","91","        x3 = lb.fit_transform(X[:, 3])","93","        y = (y != b'normal.').astype(int)","94","        print_outlier_ratio(y)","96","    if dat in ('http', 'smtp'):","97","        y = (y != b'normal.').astype(int)","98","        print_outlier_ratio(y)","109","    print('--- Fitting the IsolationForest estimator...')","116","    scoring = - model.decision_function(X_test)  # the lower, the more abnormal","118","    print(\"--- Preparing the plot elements...\")","119","    if with_decision_function_histograms:","120","        fig, ax = plt.subplots(3, sharex=True, sharey=True)","121","        bins = np.linspace(-0.5, 0.5, 200)","122","        ax[0].hist(scoring, bins, color='black')","123","        ax[0].set_title('Decision function for %s dataset' % dat)","124","        ax[1].hist(scoring[y_test == 0], bins, color='b', label='normal data')","125","        ax[1].legend(loc=\"lower right\")","126","        ax[2].hist(scoring[y_test == 1], bins, color='r', label='outliers')","127","        ax[2].legend(loc=\"lower right\")","132","    auc_score = auc(fpr, tpr)","133","    label = ('%s (AUC: %0.3f, train_time= %0.2fs, '","134","             'test_time= %0.2fs)' % (dat, auc_score, fit_time, predict_time))","135","    # Print AUC score and train\/test time:","136","    print(label)"],"delete":["4","","6","","8","print(__doc__)","16","from sklearn.preprocessing import LabelBinarizer","20","","21","datasets = ['http', 'smtp', 'SA', 'SF', 'shuttle', 'forestcover']","22","","27","    # loading and vectorization","28","    print('loading data')","29","    if dat in ['http', 'smtp', 'SA', 'SF']:","57","    print('vectorizing data')","60","        lb = LabelBinarizer()","61","        lb.fit(X[:, 1])","62","        x1 = lb.transform(X[:, 1])","64","        y = (y != 'normal.').astype(int)","67","        lb = LabelBinarizer()","68","        lb.fit(X[:, 1])","69","        x1 = lb.transform(X[:, 1])","70","        lb.fit(X[:, 2])","71","        x2 = lb.transform(X[:, 2])","72","        lb.fit(X[:, 3])","73","        x3 = lb.transform(X[:, 3])","75","        y = (y != 'normal.').astype(int)","77","    if dat == 'http' or dat == 'smtp':","78","        y = (y != 'normal.').astype(int)","89","    print('IsolationForest processing...')","96","    scoring = - model.decision_function(X_test)  # the lower, the more normal","98","    # Show score histograms","99","    fig, ax = plt.subplots(3, sharex=True, sharey=True)","100","    bins = np.linspace(-0.5, 0.5, 200)","101","    ax[0].hist(scoring, bins, color='black')","102","    ax[0].set_title('decision function for %s dataset' % dat)","103","    ax[0].legend(loc=\"lower right\")","104","    ax[1].hist(scoring[y_test == 0], bins, color='b',","105","               label='normal data')","106","    ax[1].legend(loc=\"lower right\")","107","    ax[2].hist(scoring[y_test == 1], bins, color='r',","108","               label='outliers')","109","    ax[2].legend(loc=\"lower right\")","114","    AUC = auc(fpr, tpr)","115","    label = ('%s (area: %0.3f, train-time: %0.2fs, '","116","             'test-time: %0.2fs)' % (dat, AUC, fit_time, predict_time))"]}]}},"af412822a44a0bd66d24eb95f7d6b8e9fe0192f7":{"changes":{"sklearn\/ensemble\/bagging.py":"MODIFY","sklearn\/ensemble\/tests\/test_bagging.py":"MODIFY"},"diff":{"sklearn\/ensemble\/bagging.py":[{"add":["610","        oob_score = accuracy_score(y, np.argmax(predictions, axis=1))"],"delete":["610","        oob_score = accuracy_score(y, classes_.take(np.argmax(predictions,","611","                                                              axis=1)))"]}],"sklearn\/ensemble\/tests\/test_bagging.py":[{"add":["725","","726","","727","def test_set_oob_score_label_encoding():","728","    # Make sure the oob_score doesn't change when the labels change","729","    # See: https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/8933","730","    randState = 5","731","    X = [[-1], [0], [1]] * 5","732","    Y1 = ['A', 'B', 'C'] * 5","733","    Y2 = [-1, 0, 1] * 5","734","    Y3 = [0, 1, 2] * 5","735","    x1 = BaggingClassifier(oob_score=True,","736","                           random_state=randState).fit(X, Y1).oob_score_","737","    x2 = BaggingClassifier(oob_score=True,","738","                           random_state=randState).fit(X, Y2).oob_score_","739","    x3 = BaggingClassifier(oob_score=True,","740","                           random_state=randState).fit(X, Y3).oob_score_","741","    assert_equal([x1, x2], [x3, x3])"],"delete":[]}]}},"c5ccb72d7797eda276eb56aa450501d19be8ddc5":{"changes":{"sklearn\/feature_selection\/univariate_selection.py":"MODIFY"},"diff":{"sklearn\/feature_selection\/univariate_selection.py":[{"add":["476","    mutual_info_regression: Mutual information for a continuous target."],"delete":["476","    mutual_info_regression: Mutual information for a continious target."]}]}},"bd0fc236e00c57b9a5212e8802c325c99af7b1b4":{"changes":{"sklearn\/neural_network\/_base.py":"MODIFY","examples\/cluster\/plot_face_ward_segmentation.py":"MODIFY","sklearn\/naive_bayes.py":"MODIFY","sklearn\/covariance\/graph_lasso_.py":"MODIFY","sklearn\/mixture\/base.py":"MODIFY","sklearn\/covariance\/empirical_covariance_.py":"MODIFY","sklearn\/neighbors\/tests\/test_dist_metrics.py":"MODIFY","sklearn\/neural_network\/rbm.py":"MODIFY","sklearn\/utils\/tests\/test_utils.py":"MODIFY","sklearn\/metrics\/ranking.py":"MODIFY","sklearn\/linear_model\/bayes.py":"MODIFY","sklearn\/model_selection\/_search.py":"MODIFY","sklearn\/mixture\/gmm.py":"MODIFY","sklearn\/cluster\/hierarchical.py":"MODIFY","sklearn\/linear_model\/logistic.py":"MODIFY","sklearn\/linear_model\/omp.py":"MODIFY","examples\/cluster\/plot_face_segmentation.py":"MODIFY","examples\/linear_model\/plot_sparse_recovery.py":"MODIFY","sklearn\/feature_selection\/univariate_selection.py":"MODIFY","sklearn\/cluster\/mean_shift_.py":"MODIFY","sklearn\/feature_extraction\/tests\/test_image.py":"MODIFY","sklearn\/utils\/arpack.py":"MODIFY","sklearn\/utils\/sparsetools\/__init__.py":"MODIFY","sklearn\/utils\/fixes.py":"MODIFY","sklearn\/linear_model\/tests\/test_logistic.py":"MODIFY","sklearn\/utils\/tests\/test_fixes.py":"MODIFY","sklearn\/utils\/stats.py":"MODIFY","sklearn\/decomposition\/online_lda.py":"MODIFY","sklearn\/utils\/tests\/test_stats.py":"MODIFY","sklearn\/cluster\/spectral.py":"MODIFY","sklearn\/covariance\/robust_covariance.py":"MODIFY","sklearn\/utils\/graph.py":"MODIFY","sklearn\/cluster\/_k_means.pyx":"MODIFY","doc\/developers\/utilities.rst":"MODIFY","sklearn\/mixture\/dpgmm.py":"MODIFY","sklearn\/manifold\/spectral_embedding_.py":"MODIFY","sklearn\/utils\/extmath.py":"MODIFY","examples\/decomposition\/plot_image_denoising.py":"MODIFY","sklearn\/decomposition\/pca.py":"MODIFY","sklearn\/utils\/tests\/test_extmath.py":"MODIFY","examples\/cluster\/plot_face_compress.py":"MODIFY","sklearn\/decomposition\/truncated_svd.py":"MODIFY","sklearn\/cluster\/bicluster.py":"MODIFY","\/dev\/null":"DELETE","sklearn\/ensemble\/gradient_boosting.py":"MODIFY","sklearn\/utils\/sparsetools\/setup.py":"MODIFY","sklearn\/linear_model\/tests\/test_sag.py":"MODIFY","sklearn\/manifold\/locally_linear.py":"MODIFY","sklearn\/cross_decomposition\/pls_.py":"MODIFY","sklearn\/decomposition\/kernel_pca.py":"MODIFY","sklearn\/linear_model\/least_angle.py":"MODIFY"},"diff":{"sklearn\/neural_network\/_base.py":[{"add":["8","from scipy.special import expit as logistic_sigmoid"],"delete":["8","from ..utils.fixes import expit as logistic_sigmoid"]}],"examples\/cluster\/plot_face_ward_segmentation.py":[{"add":["29","try:  # SciPy >= 0.16 have face in misc","30","    from scipy.misc import face","31","    face = face(gray=True)","32","except ImportError:"],"delete":["25","from sklearn.utils.testing import SkipTest","26","from sklearn.utils.fixes import sp_version","27","","28","if sp_version < (0, 12):","29","    raise SkipTest(\"Skipping because SciPy version earlier than 0.12.0 and \"","30","                   \"thus does not include the scipy.misc.face() image.\")","35","try:","37","except AttributeError:","38","    # Newer versions of scipy have face in misc","39","    from scipy import misc","40","    face = misc.face(gray=True)"]}],"sklearn\/naive_bayes.py":[{"add":["21","from scipy.misc import logsumexp","29","from .utils.extmath import safe_sparse_dot"],"delete":["28","from .utils.extmath import safe_sparse_dot, logsumexp"]}],"sklearn\/covariance\/graph_lasso_.py":[{"add":["192","    precision_ = linalg.pinvh(covariance_)"],"delete":["19","from ..utils.extmath import pinvh","193","    precision_ = pinvh(covariance_)"]}],"sklearn\/mixture\/base.py":[{"add":["13","from scipy.misc import logsumexp"],"delete":["20","from ..utils.extmath import logsumexp"]}],"sklearn\/covariance\/empirical_covariance_.py":[{"add":["19","from ..utils.extmath import fast_logdet","135","            self.precision_ = linalg.pinvh(covariance)","151","            precision = linalg.pinvh(self.covariance_)"],"delete":["19","from ..utils.extmath import fast_logdet, pinvh","135","            self.precision_ = pinvh(covariance)","151","            precision = pinvh(self.covariance_)"]}],"sklearn\/neighbors\/tests\/test_dist_metrics.py":[{"add":["9","from sklearn.utils.testing import assert_raises_regex"],"delete":["6","import scipy","10","from sklearn.utils.testing import SkipTest, assert_raises_regex","17","def cmp_version(version1, version2):","18","    version1 = tuple(map(int, version1.split('.')[:2]))","19","    version2 = tuple(map(int, version2.split('.')[:2]))","20","","21","    if version1 < version2:","22","        return -1","23","    elif version1 > version2:","24","        return 1","25","    else:","26","        return 0","27","","28","","72","        if metric == 'canberra' and cmp_version(scipy.__version__, '0.9') <= 0:","73","            raise SkipTest(\"Canberra distance incorrect in scipy < 0.9\")","96","        if metric == 'canberra' and cmp_version(scipy.__version__, '0.9') <= 0:","97","            raise SkipTest(\"Canberra distance incorrect in scipy < 0.9\")"]}],"sklearn\/neural_network\/rbm.py":[{"add":["13","from scipy.special import expit  # logistic function"],"delete":["23","from ..utils.fixes import expit             # logistic function"]}],"sklearn\/utils\/tests\/test_utils.py":[{"add":["0","from itertools import chain","10","                                   assert_greater_equal, ignore_warnings)","105","@ignore_warnings  # Test deprecated backport to be removed in 0.21","113","@ignore_warnings  # Test deprecated backport to be removed in 0.21","125","@ignore_warnings  # Test deprecated backport to be removed in 0.21","134","@ignore_warnings  # Test deprecated backport to be removed in 0.21","148","    v0 = random_state.uniform(-1, 1, A.shape[0])","263","    joined_range = list(chain(*[some_range[slice] for slice in","264","                                gen_even_slices(10, 3)]))"],"delete":["5","from itertools import chain","10","                                   assert_greater_equal)","11","","145","    v0 = random_state.uniform(-1,1, A.shape[0])","260","    joined_range = list(chain(*[some_range[slice] for slice in gen_even_slices(10, 3)]))"]}],"sklearn\/metrics\/ranking.py":[{"add":["24","from scipy.stats import rankdata"],"delete":["32","from ..utils.stats import rankdata"]}],"sklearn\/linear_model\/bayes.py":[{"add":["11","from scipy.linalg import pinvh","15","from ..utils.extmath import fast_logdet"],"delete":["14","from ..utils.extmath import fast_logdet, pinvh"]}],"sklearn\/model_selection\/_search.py":[{"add":["21","from scipy.stats import rankdata"],"delete":["31","from ..utils.fixes import rankdata"]}],"sklearn\/mixture\/gmm.py":[{"add":["17","from time import time","21","from scipy.misc import logsumexp"],"delete":["20","from time import time","24","from ..utils.extmath import logsumexp"]}],"sklearn\/cluster\/hierarchical.py":[{"add":["14","from scipy.sparse.csgraph import connected_components"],"delete":["20","from ..utils.sparsetools import connected_components"]}],"sklearn\/linear_model\/logistic.py":[{"add":["17","from scipy.misc import logsumexp","18","from scipy.special import expit","26","from ..utils.extmath import (log_logistic, safe_sparse_dot, softmax,","27","                             squared_norm)"],"delete":["24","from ..utils.extmath import (logsumexp, log_logistic, safe_sparse_dot,","25","                             softmax, squared_norm)","30","from ..utils.fixes import expit"]}],"sklearn\/linear_model\/omp.py":[{"add":["19","solve_triangular_args = {'check_finite': False}"],"delete":["8","from distutils.version import LooseVersion","20","import scipy","21","solve_triangular_args = {}","22","if LooseVersion(scipy.__version__) >= LooseVersion('0.12'):","23","    # check_finite=False is an optimization available only in scipy >=0.12","24","    solve_triangular_args = {'check_finite': False}","25",""]}],"examples\/cluster\/plot_face_segmentation.py":[{"add":["35","try:  # SciPy >= 0.16 have face in misc","36","    from scipy.misc import face","37","    face = face(gray=True)","38","except ImportError:"],"delete":["32","from sklearn.utils.testing import SkipTest","33","from sklearn.utils.fixes import sp_version","34","","35","if sp_version < (0, 12):","36","    raise SkipTest(\"Skipping because SciPy version earlier than 0.12.0 and \"","37","                   \"thus does not include the scipy.misc.face() image.\")","41","try:","43","except AttributeError:","44","    # Newer versions of scipy have face in misc","45","    from scipy import misc","46","    face = misc.face(gray=True)"]}],"examples\/linear_model\/plot_sparse_recovery.py":[{"add":["64","                       linalg.pinvh(np.dot(X_relevant.T, X_relevant)))"],"delete":["57","from sklearn.utils.extmath import pinvh","65","                       pinvh(np.dot(X_relevant.T, X_relevant)))"]}],"sklearn\/feature_selection\/univariate_selection.py":[{"add":["17","from ..utils.extmath import safe_sparse_dot, row_norms","298","    corr \/= np.linalg.norm(y)"],"delete":["17","from ..utils.extmath import norm, safe_sparse_dot, row_norms","298","    corr \/= norm(y)"]}],"sklearn\/cluster\/mean_shift_.py":[{"add":["22","from ..utils import check_random_state, gen_batches, check_array","98","        if (np.linalg.norm(my_mean - my_old_mean) < stop_thresh or"],"delete":["22","from ..utils import extmath, check_random_state, gen_batches, check_array","98","        if (extmath.norm(my_mean - my_old_mean) < stop_thresh or"]}],"sklearn\/feature_extraction\/tests\/test_image.py":[{"add":["7","from scipy.sparse.csgraph import connected_components","14","from sklearn.utils.testing import assert_equal, assert_true"],"delete":["13","from sklearn.utils.graph import connected_components","14","from sklearn.utils.testing import SkipTest, assert_equal, assert_true","15","from sklearn.utils.fixes import sp_version","16","","17","if sp_version < (0, 12):","18","    raise SkipTest(\"Skipping because SciPy version earlier than 0.12.0 and \"","19","                   \"thus does not include the scipy.misc.face() image.\")"]}],"sklearn\/utils\/arpack.py":[{"add":["0","# Remove this module in version 0.21","2","from scipy.sparse.linalg import eigs as _eigs, eigsh as _eigsh, svds as _svds","4","from .deprecation import deprecated","7","@deprecated(\"sklearn.utils.arpack.eigs was deprecated in version 0.19 and\"","8","            \"will be removed in 0.21. Use scipy.sparse.linalg.eigs instead.\")","9","def eigs(A, *args, **kwargs):","10","    return _eigs(A, *args, **kwargs)","13","@deprecated(\"sklearn.utils.arpack.eigsh was deprecated in version 0.19 and\"","14","            \"will be removed in 0.21. Use scipy.sparse.linalg.eigsh instead.\")","15","def eigsh(A, *args, **kwargs):","16","    return _eigsh(A, *args, **kwargs)","19","@deprecated(\"sklearn.utils.arpack.svds was deprecated in version 0.19 and\"","20","            \"will be removed in 0.21. Use scipy.sparse.linalg.svds instead.\")","21","def svds(A, *args, **kwargs):","22","    return _svds(A, *args, **kwargs)"],"delete":["0","\"\"\"","1","This contains a copy of the future version of","2","scipy.sparse.linalg.eigen.arpack.eigsh","3","It's an upgraded wrapper of the ARPACK library which","4","allows the use of shift-invert mode for symmetric matrices.","7","Find a few eigenvectors and eigenvalues of a matrix.","10","Uses ARPACK: http:\/\/www.caam.rice.edu\/software\/ARPACK\/","12","\"\"\"","13","# Wrapper implementation notes","14","#","15","# ARPACK Entry Points","16","# -------------------","17","# The entry points to ARPACK are","18","# - (s,d)seupd : single and double precision symmetric matrix","19","# - (s,d,c,z)neupd: single,double,complex,double complex general matrix","20","# This wrapper puts the *neupd (general matrix) interfaces in eigs()","21","# and the *seupd (symmetric matrix) in eigsh().","22","# There is no Hermetian complex\/double complex interface.","23","# To find eigenvalues of a Hermetian matrix you","24","# must use eigs() and not eigsh()","25","# It might be desirable to handle the Hermetian case differently","26","# and, for example, return real eigenvalues.","28","# Number of eigenvalues returned and complex eigenvalues","29","# ------------------------------------------------------","30","# The ARPACK nonsymmetric real and double interface (s,d)naupd return","31","# eigenvalues and eigenvectors in real (float,double) arrays.","32","# Since the eigenvalues and eigenvectors are, in general, complex","33","# ARPACK puts the real and imaginary parts in consecutive entries","34","# in real-valued arrays.   This wrapper puts the real entries","35","# into complex data types and attempts to return the requested eigenvalues","36","# and eigenvectors.","39","# Solver modes","40","# ------------","41","# ARPACK and handle shifted and shift-inverse computations","42","# for eigenvalues by providing a shift (sigma) and a solver.","43","","44","from scipy.sparse.linalg.eigen.arpack import _arpack","45","import numpy as np","46","from scipy.sparse.linalg.interface import aslinearoperator, LinearOperator","47","from scipy.sparse import identity, isspmatrix, isspmatrix_csr","48","from scipy.linalg import lu_factor, lu_solve","49","from scipy.sparse.sputils import isdense","50","from scipy.sparse.linalg import gmres, splu","51","import scipy","52","import functools","53","import operator","54","from distutils.version import LooseVersion","55","","56","__docformat__ = \"restructuredtext en\"","57","","58","__all__ = ['eigs', 'eigsh', 'svds', 'ArpackError', 'ArpackNoConvergence']","59","","60","_type_conv = {'f': 's', 'd': 'd', 'F': 'c', 'D': 'z'}","61","_ndigits = {'f': 5, 'd': 12, 'F': 5, 'D': 12}","62","","63","DNAUPD_ERRORS = {","64","    0: \"Normal exit.\",","65","    1: \"Maximum number of iterations taken. \"","66","       \"All possible eigenvalues of OP has been found. IPARAM(5) \"","67","       \"returns the number of wanted converged Ritz values.\",","68","    2: \"No longer an informational error. Deprecated starting \"","69","       \"with release 2 of ARPACK.\",","70","    3: \"No shifts could be applied during a cycle of the \"","71","       \"Implicitly restarted Arnoldi iteration. One possibility \"","72","       \"is to increase the size of NCV relative to NEV. \",","73","    -1: \"N must be positive.\",","74","    -2: \"NEV must be positive.\",","75","    -3: \"NCV-NEV >= 2 and less than or equal to N.\",","76","    -4: \"The maximum number of Arnoldi update iterations allowed \"","77","        \"must be greater than zero.\",","78","    -5: \" WHICH must be one of 'LM', 'SM', 'LR', 'SR', 'LI', 'SI'\",","79","    -6: \"BMAT must be one of 'I' or 'G'.\",","80","    -7: \"Length of private work array WORKL is not sufficient.\",","81","    -8: \"Error return from LAPACK eigenvalue calculation;\",","82","    -9: \"Starting vector is zero.\",","83","    -10: \"IPARAM(7) must be 1,2,3,4.\",","84","    -11: \"IPARAM(7) = 1 and BMAT = 'G' are incompatible.\",","85","    -12: \"IPARAM(1) must be equal to 0 or 1.\",","86","    -13: \"NEV and WHICH = 'BE' are incompatible.\",","87","    -9999: \"Could not build an Arnoldi factorization. \"","88","           \"IPARAM(5) returns the size of the current Arnoldi \"","89","           \"factorization. The user is advised to check that \"","90","           \"enough workspace and array storage has been allocated.\"","91","}","92","","93","SNAUPD_ERRORS = DNAUPD_ERRORS","94","","95","ZNAUPD_ERRORS = DNAUPD_ERRORS.copy()","96","ZNAUPD_ERRORS[-10] = \"IPARAM(7) must be 1,2,3.\"","97","","98","CNAUPD_ERRORS = ZNAUPD_ERRORS","99","","100","DSAUPD_ERRORS = {","101","    0: \"Normal exit.\",","102","    1: \"Maximum number of iterations taken. \"","103","       \"All possible eigenvalues of OP has been found.\",","104","    2: \"No longer an informational error. Deprecated starting with \"","105","       \"release 2 of ARPACK.\",","106","    3: \"No shifts could be applied during a cycle of the Implicitly \"","107","       \"restarted Arnoldi iteration. One possibility is to increase \"","108","       \"the size of NCV relative to NEV. \",","109","    -1: \"N must be positive.\",","110","    -2: \"NEV must be positive.\",","111","    -3: \"NCV must be greater than NEV and less than or equal to N.\",","112","    -4: \"The maximum number of Arnoldi update iterations allowed \"","113","        \"must be greater than zero.\",","114","    -5: \"WHICH must be one of 'LM', 'SM', 'LA', 'SA' or 'BE'.\",","115","    -6: \"BMAT must be one of 'I' or 'G'.\",","116","    -7: \"Length of private work array WORKL is not sufficient.\",","117","    -8: \"Error return from trid. eigenvalue calculation; \"","118","        \"Informational error from LAPACK routine dsteqr .\",","119","    -9: \"Starting vector is zero.\",","120","    -10: \"IPARAM(7) must be 1,2,3,4,5.\",","121","    -11: \"IPARAM(7) = 1 and BMAT = 'G' are incompatible.\",","122","    -12: \"IPARAM(1) must be equal to 0 or 1.\",","123","    -13: \"NEV and WHICH = 'BE' are incompatible. \",","124","    -9999: \"Could not build an Arnoldi factorization. \"","125","           \"IPARAM(5) returns the size of the current Arnoldi \"","126","           \"factorization. The user is advised to check that \"","127","           \"enough workspace and array storage has been allocated.\",","128","}","129","","130","SSAUPD_ERRORS = DSAUPD_ERRORS","131","","132","DNEUPD_ERRORS = {","133","    0: \"Normal exit.\",","134","    1: \"The Schur form computed by LAPACK routine dlahqr \"","135","       \"could not be reordered by LAPACK routine dtrsen. \"","136","       \"Re-enter subroutine dneupd  with IPARAM(5)NCV and \"","137","       \"increase the size of the arrays DR and DI to have \"","138","       \"dimension at least dimension NCV and allocate at least NCV \"","139","       \"columns for Z. NOTE: Not necessary if Z and V share \"","140","       \"the same space. Please notify the authors if this error\"","141","       \"occurs.\",","142","    -1: \"N must be positive.\",","143","    -2: \"NEV must be positive.\",","144","    -3: \"NCV-NEV >= 2 and less than or equal to N.\",","145","    -5: \"WHICH must be one of 'LM', 'SM', 'LR', 'SR', 'LI', 'SI'\",","146","    -6: \"BMAT must be one of 'I' or 'G'.\",","147","    -7: \"Length of private work WORKL array is not sufficient.\",","148","    -8: \"Error return from calculation of a real Schur form. \"","149","        \"Informational error from LAPACK routine dlahqr .\",","150","    -9: \"Error return from calculation of eigenvectors. \"","151","        \"Informational error from LAPACK routine dtrevc.\",","152","    -10: \"IPARAM(7) must be 1,2,3,4.\",","153","    -11: \"IPARAM(7) = 1 and BMAT = 'G' are incompatible.\",","154","    -12: \"HOWMNY = 'S' not yet implemented\",","155","    -13: \"HOWMNY must be one of 'A' or 'P' if RVEC = .true.\",","156","    -14: \"DNAUPD  did not find any eigenvalues to sufficient \"","157","         \"accuracy.\",","158","    -15: \"DNEUPD got a different count of the number of converged \"","159","         \"Ritz values than DNAUPD got.  This indicates the user \"","160","         \"probably made an error in passing data from DNAUPD to \"","161","         \"DNEUPD or that the data was modified before entering \"","162","         \"DNEUPD\",","163","}","164","","165","SNEUPD_ERRORS = DNEUPD_ERRORS.copy()","166","SNEUPD_ERRORS[1] = (\"The Schur form computed by LAPACK routine slahqr \"","167","                    \"could not be reordered by LAPACK routine strsen . \"","168","                    \"Re-enter subroutine dneupd  with IPARAM(5)=NCV and \"","169","                    \"increase the size of the arrays DR and DI to have \"","170","                    \"dimension at least dimension NCV and allocate at least \"","171","                    \"NCV columns for Z. NOTE: Not necessary if Z and V share \"","172","                    \"the same space. Please notify the authors if this error \"","173","                    \"occurs.\")","174","SNEUPD_ERRORS[-14] = (\"SNAUPD did not find any eigenvalues to sufficient \"","175","                      \"accuracy.\")","176","SNEUPD_ERRORS[-15] = (\"SNEUPD got a different count of the number of \"","177","                      \"converged Ritz values than SNAUPD got.  This indicates \"","178","                      \"the user probably made an error in passing data from \"","179","                      \"SNAUPD to SNEUPD or that the data was modified before \"","180","                      \"entering SNEUPD\")","181","","182","ZNEUPD_ERRORS = {0: \"Normal exit.\",","183","                 1: \"The Schur form computed by LAPACK routine csheqr \"","184","                    \"could not be reordered by LAPACK routine ztrsen. \"","185","                    \"Re-enter subroutine zneupd with IPARAM(5)=NCV and \"","186","                    \"increase the size of the array D to have \"","187","                    \"dimension at least dimension NCV and allocate at least \"","188","                    \"NCV columns for Z. NOTE: Not necessary if Z and V share \"","189","                    \"the same space. Please notify the authors if this error \"","190","                    \"occurs.\",","191","                 -1: \"N must be positive.\",","192","                 -2: \"NEV must be positive.\",","193","                 -3: \"NCV-NEV >= 1 and less than or equal to N.\",","194","                 -5: \"WHICH must be one of 'LM', 'SM', 'LR', 'SR', 'LI', 'SI'\",","195","                 -6: \"BMAT must be one of 'I' or 'G'.\",","196","                 -7: \"Length of private work WORKL array is not sufficient.\",","197","                 -8: \"Error return from LAPACK eigenvalue calculation. \"","198","                     \"This should never happened.\",","199","                 -9: \"Error return from calculation of eigenvectors. \"","200","                     \"Informational error from LAPACK routine ztrevc.\",","201","                 -10: \"IPARAM(7) must be 1,2,3\",","202","                 -11: \"IPARAM(7) = 1 and BMAT = 'G' are incompatible.\",","203","                 -12: \"HOWMNY = 'S' not yet implemented\",","204","                 -13: \"HOWMNY must be one of 'A' or 'P' if RVEC = .true.\",","205","                 -14: \"ZNAUPD did not find any eigenvalues to sufficient \"","206","                      \"accuracy.\",","207","                 -15: \"ZNEUPD got a different count of the number of \"","208","                      \"converged Ritz values than ZNAUPD got.  This \"","209","                      \"indicates the user probably made an error in passing \"","210","                      \"data from ZNAUPD to ZNEUPD or that the data was \"","211","                      \"modified before entering ZNEUPD\"","212","                 }","213","","214","CNEUPD_ERRORS = ZNEUPD_ERRORS.copy()","215","CNEUPD_ERRORS[-14] = (\"CNAUPD did not find any eigenvalues to sufficient \"","216","                      \"accuracy.\")","217","CNEUPD_ERRORS[-15] = (\"CNEUPD got a different count of the number of \"","218","                      \"converged Ritz values than CNAUPD got.  This indicates \"","219","                      \"the user probably made an error in passing data from \"","220","                      \"CNAUPD to CNEUPD or that the data was modified before \"","221","                      \"entering CNEUPD\")","222","","223","DSEUPD_ERRORS = {","224","    0: \"Normal exit.\",","225","    -1: \"N must be positive.\",","226","    -2: \"NEV must be positive.\",","227","    -3: \"NCV must be greater than NEV and less than or equal to N.\",","228","    -5: \"WHICH must be one of 'LM', 'SM', 'LA', 'SA' or 'BE'.\",","229","    -6: \"BMAT must be one of 'I' or 'G'.\",","230","    -7: \"Length of private work WORKL array is not sufficient.\",","231","    -8: (\"Error return from trid. eigenvalue calculation; \"","232","         \"Information error from LAPACK routine dsteqr.\"),","233","    -9: \"Starting vector is zero.\",","234","    -10: \"IPARAM(7) must be 1,2,3,4,5.\",","235","    -11: \"IPARAM(7) = 1 and BMAT = 'G' are incompatible.\",","236","    -12: \"NEV and WHICH = 'BE' are incompatible.\",","237","    -14: \"DSAUPD  did not find any eigenvalues to sufficient accuracy.\",","238","    -15: \"HOWMNY must be one of 'A' or 'S' if RVEC = .true.\",","239","    -16: \"HOWMNY = 'S' not yet implemented\",","240","    -17: (\"DSEUPD  got a different count of the number of converged \"","241","          \"Ritz values than DSAUPD  got.  This indicates the user \"","242","          \"probably made an error in passing data from DSAUPD  to \"","243","          \"DSEUPD  or that the data was modified before entering  \"","244","          \"DSEUPD.\")","245","}","246","","247","SSEUPD_ERRORS = DSEUPD_ERRORS.copy()","248","SSEUPD_ERRORS[-14] = (\"SSAUPD  did not find any eigenvalues \"","249","                      \"to sufficient accuracy.\")","250","SSEUPD_ERRORS[-17] = (\"SSEUPD  got a different count of the number of \"","251","                      \"converged \"","252","                      \"Ritz values than SSAUPD  got.  This indicates the user \"","253","                      \"probably made an error in passing data from SSAUPD  to \"","254","                      \"SSEUPD  or that the data was modified before entering  \"","255","                      \"SSEUPD.\")","256","","257","_SAUPD_ERRORS = {'d': DSAUPD_ERRORS,","258","                 's': SSAUPD_ERRORS}","259","_NAUPD_ERRORS = {'d': DNAUPD_ERRORS,","260","                 's': SNAUPD_ERRORS,","261","                 'z': ZNAUPD_ERRORS,","262","                 'c': CNAUPD_ERRORS}","263","_SEUPD_ERRORS = {'d': DSEUPD_ERRORS,","264","                 's': SSEUPD_ERRORS}","265","_NEUPD_ERRORS = {'d': DNEUPD_ERRORS,","266","                 's': SNEUPD_ERRORS,","267","                 'z': ZNEUPD_ERRORS,","268","                 'c': CNEUPD_ERRORS}","269","","270","# accepted values of parameter WHICH in _SEUPD","271","_SEUPD_WHICH = ['LM', 'SM', 'LA', 'SA', 'BE']","272","","273","# accepted values of parameter WHICH in _NAUPD","274","_NEUPD_WHICH = ['LM', 'SM', 'LR', 'SR', 'LI', 'SI']","275","","276","","277","# CHECK IF BACKPORT IS ACTUALLY NEEDED","278","if scipy.version.version >= LooseVersion('0.12'):","279","    BACKPORT_TO = None","280","elif scipy.version.version >= LooseVersion('0.11'):","281","    BACKPORT_TO = '0.10'","282","else:","283","    BACKPORT_TO = '0.09'","284","","285","","286","# redefinition of the function from `scipy._lib._util._aligned_zeros`","287","def _aligned_zeros(shape, dtype=float, order=\"C\", align=None):","288","    \"\"\"Allocate a new ndarray with aligned memory.","289","    Primary use case for this currently is working around a f2py issue","290","    in Numpy 1.9.1, where dtype.alignment is such that np.zeros() does","291","    not necessarily create arrays aligned up to it.","292","    \"\"\"","293","    dtype = np.dtype(dtype)","294","    if align is None:","295","        align = dtype.alignment","296","    if not hasattr(shape, '__len__'):","297","        shape = (shape,)","298","    size = functools.reduce(operator.mul, shape) * dtype.itemsize","299","    buf = np.empty(size + align + 1, np.uint8)","300","    offset = buf.__array_interface__['data'][0] % align","301","    if offset != 0:","302","        offset = align - offset","303","    # Note: slices producing 0-size arrays do not necessarily change","304","    # data pointer --- so we use and allocate size+1","305","    buf = buf[offset:offset+size+1][:-1]","306","    data = np.ndarray(shape, dtype, buf, order=order)","307","    data.fill(0)","308","    return data","309","","310","","311","class ArpackError(RuntimeError):","312","    \"\"\"","313","    ARPACK error","314","    \"\"\"","315","    def __init__(self, info, infodict=_NAUPD_ERRORS):","316","        msg = infodict.get(info, \"Unknown error\")","317","        RuntimeError.__init__(self, \"ARPACK error %d: %s\" % (info, msg))","318","","319","","320","class ArpackNoConvergence(ArpackError):","321","    \"\"\"","322","    ARPACK iteration did not converge","323","","324","    Attributes","325","    ----------","326","    eigenvalues : ndarray","327","        Partial result. Converged eigenvalues.","328","    eigenvectors : ndarray","329","        Partial result. Converged eigenvectors.","330","","331","    \"\"\"","332","    def __init__(self, msg, eigenvalues, eigenvectors):","333","        ArpackError.__init__(self, -1, {-1: msg})","334","        self.eigenvalues = eigenvalues","335","        self.eigenvectors = eigenvectors","336","","337","","338","class _ArpackParams(object):","339","    def __init__(self, n, k, tp, mode=1, sigma=None,","340","                 ncv=None, v0=None, maxiter=None, which=\"LM\", tol=0):","341","        if k <= 0:","342","            raise ValueError(\"k must be positive, k=%d\" % k)","343","","344","        if maxiter is None:","345","            maxiter = n * 10","346","        if maxiter <= 0:","347","            raise ValueError(\"maxiter must be positive, maxiter=%d\" % maxiter)","348","","349","        if tp not in 'fdFD':","350","            raise ValueError(\"matrix type must be 'f', 'd', 'F', or 'D'\")","351","","352","        if v0 is not None:","353","            # ARPACK overwrites its initial resid,  make a copy","354","            self.resid = np.array(v0, copy=True)","355","            info = 1","356","        else:","357","            # ARPACK will use a random initial vector.","358","            self.resid = np.zeros(n, tp)","359","            info = 0","360","","361","        if sigma is None:","362","            # sigma not used","363","            self.sigma = 0","364","        else:","365","            self.sigma = sigma","366","","367","        if ncv is None:","368","            ncv = 2 * k + 1","369","        ncv = min(ncv, n)","370","","371","        self.v = np.zeros((n, ncv), tp)  # holds Ritz vectors","372","        self.iparam = np.zeros(11, \"int\")","373","","374","        # set solver mode and parameters","375","        ishfts = 1","376","        self.mode = mode","377","        self.iparam[0] = ishfts","378","        self.iparam[2] = maxiter","379","        self.iparam[3] = 1","380","        self.iparam[6] = mode","381","","382","        self.n = n","383","        self.tol = tol","384","        self.k = k","385","        self.maxiter = maxiter","386","        self.ncv = ncv","387","        self.which = which","388","        self.tp = tp","389","        self.info = info","390","","391","        self.converged = False","392","        self.ido = 0","393","","394","    def _raise_no_convergence(self):","395","        msg = \"No convergence (%d iterations, %d\/%d eigenvectors converged)\"","396","        k_ok = self.iparam[4]","397","        num_iter = self.iparam[2]","398","        try:","399","            ev, vec = self.extract(True)","400","        except ArpackError as err:","401","            msg = \"%s [%s]\" % (msg, err)","402","            ev = np.zeros((0,))","403","            vec = np.zeros((self.n, 0))","404","            k_ok = 0","405","        raise ArpackNoConvergence(msg % (num_iter, k_ok, self.k), ev, vec)","406","","407","","408","class _SymmetricArpackParams(_ArpackParams):","409","    def __init__(self, n, k, tp, matvec, mode=1, M_matvec=None,","410","                 Minv_matvec=None, sigma=None,","411","                 ncv=None, v0=None, maxiter=None, which=\"LM\", tol=0):","412","        # The following modes are supported:","413","        #  mode = 1:","414","        #    Solve the standard eigenvalue problem:","415","        #      A*x = lambda*x :","416","        #       A - symmetric","417","        #    Arguments should be","418","        #       matvec      = left multiplication by A","419","        #       M_matvec    = None [not used]","420","        #       Minv_matvec = None [not used]","421","        #","422","        #  mode = 2:","423","        #    Solve the general eigenvalue problem:","424","        #      A*x = lambda*M*x","425","        #       A - symmetric","426","        #       M - symmetric positive definite","427","        #    Arguments should be","428","        #       matvec      = left multiplication by A","429","        #       M_matvec    = left multiplication by M","430","        #       Minv_matvec = left multiplication by M^-1","431","        #","432","        #  mode = 3:","433","        #    Solve the general eigenvalue problem in shift-invert mode:","434","        #      A*x = lambda*M*x","435","        #       A - symmetric","436","        #       M - symmetric positive semi-definite","437","        #    Arguments should be","438","        #       matvec      = None [not used]","439","        #       M_matvec    = left multiplication by M","440","        #                     or None, if M is the identity","441","        #       Minv_matvec = left multiplication by [A-sigma*M]^-1","442","        #","443","        #  mode = 4:","444","        #    Solve the general eigenvalue problem in Buckling mode:","445","        #      A*x = lambda*AG*x","446","        #       A  - symmetric positive semi-definite","447","        #       AG - symmetric indefinite","448","        #    Arguments should be","449","        #       matvec      = left multiplication by A","450","        #       M_matvec    = None [not used]","451","        #       Minv_matvec = left multiplication by [A-sigma*AG]^-1","452","        #","453","        #  mode = 5:","454","        #    Solve the general eigenvalue problem in Cayley-transformed mode:","455","        #      A*x = lambda*M*x","456","        #       A - symmetric","457","        #       M - symmetric positive semi-definite","458","        #    Arguments should be","459","        #       matvec      = left multiplication by A","460","        #       M_matvec    = left multiplication by M","461","        #                     or None, if M is the identity","462","        #       Minv_matvec = left multiplication by [A-sigma*M]^-1","463","        if mode == 1:","464","            if matvec is None:","465","                raise ValueError(\"matvec must be specified for mode=1\")","466","            if M_matvec is not None:","467","                raise ValueError(\"M_matvec cannot be specified for mode=1\")","468","            if Minv_matvec is not None:","469","                raise ValueError(\"Minv_matvec cannot be specified for mode=1\")","470","","471","            self.OP = matvec","472","            self.B = lambda x: x","473","            self.bmat = 'I'","474","        elif mode == 2:","475","            if matvec is None:","476","                raise ValueError(\"matvec must be specified for mode=2\")","477","            if M_matvec is None:","478","                raise ValueError(\"M_matvec must be specified for mode=2\")","479","            if Minv_matvec is None:","480","                raise ValueError(\"Minv_matvec must be specified for mode=2\")","481","","482","            self.OP = lambda x: Minv_matvec(matvec(x))","483","            self.OPa = Minv_matvec","484","            self.OPb = matvec","485","            self.B = M_matvec","486","            self.bmat = 'G'","487","        elif mode == 3:","488","            if matvec is not None:","489","                raise ValueError(\"matvec must not be specified for mode=3\")","490","            if Minv_matvec is None:","491","                raise ValueError(\"Minv_matvec must be specified for mode=3\")","492","","493","            if M_matvec is None:","494","                self.OP = Minv_matvec","495","                self.OPa = Minv_matvec","496","                self.B = lambda x: x","497","                self.bmat = 'I'","498","            else:","499","                self.OP = lambda x: Minv_matvec(M_matvec(x))","500","                self.OPa = Minv_matvec","501","                self.B = M_matvec","502","                self.bmat = 'G'","503","        elif mode == 4:","504","            if matvec is None:","505","                raise ValueError(\"matvec must be specified for mode=4\")","506","            if M_matvec is not None:","507","                raise ValueError(\"M_matvec must not be specified for mode=4\")","508","            if Minv_matvec is None:","509","                raise ValueError(\"Minv_matvec must be specified for mode=4\")","510","            self.OPa = Minv_matvec","511","            self.OP = lambda x: self.OPa(matvec(x))","512","            self.B = matvec","513","            self.bmat = 'G'","514","        elif mode == 5:","515","            if matvec is None:","516","                raise ValueError(\"matvec must be specified for mode=5\")","517","            if Minv_matvec is None:","518","                raise ValueError(\"Minv_matvec must be specified for mode=5\")","519","","520","            self.OPa = Minv_matvec","521","            self.A_matvec = matvec","522","","523","            if M_matvec is None:","524","                self.OP = lambda x: Minv_matvec(matvec(x) + sigma * x)","525","                self.B = lambda x: x","526","                self.bmat = 'I'","527","            else:","528","                self.OP = lambda x: Minv_matvec(matvec(x) +","529","                                                sigma * M_matvec(x))","530","                self.B = M_matvec","531","                self.bmat = 'G'","532","        else:","533","            raise ValueError(\"mode=%i not implemented\" % mode)","534","","535","        if which not in _SEUPD_WHICH:","536","            raise ValueError(\"which must be one of %s\"","537","                             % ' '.join(_SEUPD_WHICH))","538","        if k >= n:","539","            raise ValueError(\"k must be less than ndim(A), k=%d\" % k)","540","","541","        _ArpackParams.__init__(self, n, k, tp, mode, sigma,","542","                               ncv, v0, maxiter, which, tol)","543","","544","        if self.ncv > n or self.ncv <= k:","545","            raise ValueError(\"ncv must be k<ncv<=n, ncv=%s\" % self.ncv)","546","","547","        # Use _aligned_zeros to work around a f2py bug in Numpy 1.9.1","548","        self.workd = _aligned_zeros(3 * n, self.tp)","549","        self.workl = _aligned_zeros(self.ncv * (self.ncv + 8), self.tp)","550","","551","        ltr = _type_conv[self.tp]","552","        if ltr not in [\"s\", \"d\"]:","553","            raise ValueError(\"Input matrix is not real-valued.\")","554","","555","        self._arpack_solver = _arpack.__dict__[ltr + 'saupd']","556","        self._arpack_extract = _arpack.__dict__[ltr + 'seupd']","557","","558","        self.iterate_infodict = _SAUPD_ERRORS[ltr]","559","        self.extract_infodict = _SEUPD_ERRORS[ltr]","560","","561","        self.ipntr = np.zeros(11, \"int\")","562","","563","    def iterate(self):","564","        if BACKPORT_TO is None:","565","            return None","566","        if BACKPORT_TO == '0.10':","567","            self.ido, self.tol, self.resid, self.v, self.iparam, self.ipntr, self.info = \\","568","                self._arpack_solver(self.ido, self.bmat, self.which, self.k,","569","                                    self.tol, self.resid, self.v, self.iparam,","570","                                    self.ipntr, self.workd, self.workl, self.info)","571","        elif BACKPORT_TO == '0.09':","572","            self.ido, self.resid, self.v, self.iparam, self.ipntr, self.info = \\","573","                self._arpack_solver(self.ido, self.bmat, self.which, self.k,","574","                                    self.tol, self.resid, self.v, self.iparam,","575","                                    self.ipntr, self.workd, self.workl, self.info)","576","","577","        xslice = slice(self.ipntr[0] - 1, self.ipntr[0] - 1 + self.n)","578","        yslice = slice(self.ipntr[1] - 1, self.ipntr[1] - 1 + self.n)","579","        if self.ido == -1:","580","            # initialization","581","            self.workd[yslice] = self.OP(self.workd[xslice])","582","        elif self.ido == 1:","583","            # compute y = Op*x","584","            if self.mode == 1:","585","                self.workd[yslice] = self.OP(self.workd[xslice])","586","            elif self.mode == 2:","587","                self.workd[xslice] = self.OPb(self.workd[xslice])","588","                self.workd[yslice] = self.OPa(self.workd[xslice])","589","            elif self.mode == 5:","590","                Bxslice = slice(self.ipntr[2] - 1, self.ipntr[2] - 1 + self.n)","591","                Ax = self.A_matvec(self.workd[xslice])","592","                self.workd[yslice] = self.OPa(Ax + (self.sigma *","593","                                                    self.workd[Bxslice]))","594","            else:","595","                Bxslice = slice(self.ipntr[2] - 1, self.ipntr[2] - 1 + self.n)","596","                self.workd[yslice] = self.OPa(self.workd[Bxslice])","597","        elif self.ido == 2:","598","            self.workd[yslice] = self.B(self.workd[xslice])","599","        elif self.ido == 3:","600","            raise ValueError(\"ARPACK requested user shifts.  Assure ISHIFT==0\")","601","        else:","602","            self.converged = True","603","","604","            if self.info == 0:","605","                pass","606","            elif self.info == 1:","607","                self._raise_no_convergence()","608","            else:","609","                raise ArpackError(self.info, infodict=self.iterate_infodict)","610","","611","    def extract(self, return_eigenvectors):","612","        rvec = return_eigenvectors","613","        ierr = 0","614","        howmny = 'A'  # return all eigenvectors","615","        sselect = np.zeros(self.ncv, 'int')  # unused","616","        d, z, ierr = self._arpack_extract(rvec, howmny, sselect, self.sigma,","617","                                          self.bmat, self.which, self.k,","618","                                          self.tol, self.resid, self.v,","619","                                          self.iparam[0:7], self.ipntr,","620","                                          self.workd[0:2 * self.n],","621","                                          self.workl, ierr)","622","        if ierr != 0:","623","            raise ArpackError(ierr, infodict=self.extract_infodict)","624","        k_ok = self.iparam[4]","625","        d = d[:k_ok]","626","        z = z[:, :k_ok]","627","","628","        if return_eigenvectors:","629","            return d, z","630","        else:","631","            return d","632","","633","","634","class _UnsymmetricArpackParams(_ArpackParams):","635","    def __init__(self, n, k, tp, matvec, mode=1, M_matvec=None,","636","                 Minv_matvec=None, sigma=None,","637","                 ncv=None, v0=None, maxiter=None, which=\"LM\", tol=0):","638","        # The following modes are supported:","639","        #  mode = 1:","640","        #    Solve the standard eigenvalue problem:","641","        #      A*x = lambda*x","642","        #       A - square matrix","643","        #    Arguments should be","644","        #       matvec      = left multiplication by A","645","        #       M_matvec    = None [not used]","646","        #       Minv_matvec = None [not used]","647","        #","648","        #  mode = 2:","649","        #    Solve the generalized eigenvalue problem:","650","        #      A*x = lambda*M*x","651","        #       A - square matrix","652","        #       M - symmetric, positive semi-definite","653","        #    Arguments should be","654","        #       matvec      = left multiplication by A","655","        #       M_matvec    = left multiplication by M","656","        #       Minv_matvec = left multiplication by M^-1","657","        #","658","        #  mode = 3,4:","659","        #    Solve the general eigenvalue problem in shift-invert mode:","660","        #      A*x = lambda*M*x","661","        #       A - square matrix","662","        #       M - symmetric, positive semi-definite","663","        #    Arguments should be","664","        #       matvec      = None [not used]","665","        #       M_matvec    = left multiplication by M","666","        #                     or None, if M is the identity","667","        #       Minv_matvec = left multiplication by [A-sigma*M]^-1","668","        #    if A is real and mode==3, use the real part of Minv_matvec","669","        #    if A is real and mode==4, use the imag part of Minv_matvec","670","        #    if A is complex and mode==3,","671","        #       use real and imag parts of Minv_matvec","672","        if mode == 1:","673","            if matvec is None:","674","                raise ValueError(\"matvec must be specified for mode=1\")","675","            if M_matvec is not None:","676","                raise ValueError(\"M_matvec cannot be specified for mode=1\")","677","            if Minv_matvec is not None:","678","                raise ValueError(\"Minv_matvec cannot be specified for mode=1\")","679","","680","            self.OP = matvec","681","            self.B = lambda x: x","682","            self.bmat = 'I'","683","        elif mode == 2:","684","            if matvec is None:","685","                raise ValueError(\"matvec must be specified for mode=2\")","686","            if M_matvec is None:","687","                raise ValueError(\"M_matvec must be specified for mode=2\")","688","            if Minv_matvec is None:","689","                raise ValueError(\"Minv_matvec must be specified for mode=2\")","690","","691","            self.OP = lambda x: Minv_matvec(matvec(x))","692","            self.OPa = Minv_matvec","693","            self.OPb = matvec","694","            self.B = M_matvec","695","            self.bmat = 'G'","696","        elif mode in (3, 4):","697","            if matvec is None:","698","                raise ValueError(\"matvec must be specified \"","699","                                 \"for mode in (3,4)\")","700","            if Minv_matvec is None:","701","                raise ValueError(\"Minv_matvec must be specified \"","702","                                 \"for mode in (3,4)\")","703","","704","            self.matvec = matvec","705","            if tp in 'DF':  # complex type","706","                if mode == 3:","707","                    self.OPa = Minv_matvec","708","                else:","709","                    raise ValueError(\"mode=4 invalid for complex A\")","710","            else:  # real type","711","                if mode == 3:","712","                    self.OPa = lambda x: np.real(Minv_matvec(x))","713","                else:","714","                    self.OPa = lambda x: np.imag(Minv_matvec(x))","715","            if M_matvec is None:","716","                self.B = lambda x: x","717","                self.bmat = 'I'","718","                self.OP = self.OPa","719","            else:","720","                self.B = M_matvec","721","                self.bmat = 'G'","722","                self.OP = lambda x: self.OPa(M_matvec(x))","723","        else:","724","            raise ValueError(\"mode=%i not implemented\" % mode)","725","","726","        if which not in _NEUPD_WHICH:","727","            raise ValueError(\"Parameter which must be one of %s\"","728","                             % ' '.join(_NEUPD_WHICH))","729","        if k >= n - 1:","730","            raise ValueError(\"k must be less than ndim(A)-1, k=%d\" % k)","731","","732","        _ArpackParams.__init__(self, n, k, tp, mode, sigma,","733","                               ncv, v0, maxiter, which, tol)","734","","735","        if self.ncv > n or self.ncv <= k + 1:","736","            raise ValueError(\"ncv must be k+1<ncv<=n, ncv=%s\" % self.ncv)","737","","738","        # Use _aligned_zeros to work around a f2py bug in Numpy 1.9.1","739","        self.workd = _aligned_zeros(3 * n, self.tp)","740","        self.workl = _aligned_zeros(3 * self.ncv * (self.ncv + 2), self.tp)","741","","742","        ltr = _type_conv[self.tp]","743","        self._arpack_solver = _arpack.__dict__[ltr + 'naupd']","744","        self._arpack_extract = _arpack.__dict__[ltr + 'neupd']","745","","746","        self.iterate_infodict = _NAUPD_ERRORS[ltr]","747","        self.extract_infodict = _NEUPD_ERRORS[ltr]","748","","749","        self.ipntr = np.zeros(14, \"int\")","750","","751","        if self.tp in 'FD':","752","            # Use _aligned_zeros to work around a f2py bug in Numpy 1.9.1","753","            self.rwork = _aligned_zeros(self.ncv, self.tp.lower())","754","        else:","755","            self.rwork = None","756","","757","    def iterate(self):","758","        if BACKPORT_TO is None:","759","            return None","760","        if BACKPORT_TO == '0.10':","761","            if self.tp in 'fd':","762","                self.ido, self.tol, self.resid, self.v, self.iparam, self.ipntr, self.info =\\","763","                    self._arpack_solver(self.ido, self.bmat, self.which, self.k,","764","                                        self.tol, self.resid, self.v, self.iparam,","765","                                        self.ipntr, self.workd, self.workl,","766","                                        self.info)","767","            else:","768","                self.ido, self.tol, self.resid, self.v, self.iparam, self.ipntr, self.info =\\","769","                    self._arpack_solver(self.ido, self.bmat, self.which, self.k,","770","                                        self.tol, self.resid, self.v, self.iparam,","771","                                        self.ipntr, self.workd, self.workl,","772","                                        self.rwork, self.info)","773","        elif BACKPORT_TO == '0.09':","774","            if self.tp in 'fd':","775","                self.ido, self.resid, self.v, self.iparam, self.ipntr, self.info =\\","776","                    self._arpack_solver(self.ido, self.bmat, self.which, self.k,","777","                                        self.tol, self.resid, self.v, self.iparam,","778","                                        self.ipntr, self.workd, self.workl,","779","                                        self.info)","780","            else:","781","                self.ido, self.resid, self.v, self.iparam, self.ipntr, self.info =\\","782","                    self._arpack_solver(self.ido, self.bmat, self.which, self.k,","783","                                        self.tol, self.resid, self.v, self.iparam,","784","                                        self.ipntr, self.workd, self.workl,","785","                                        self.rwork, self.info)","786","","787","        xslice = slice(self.ipntr[0] - 1, self.ipntr[0] - 1 + self.n)","788","        yslice = slice(self.ipntr[1] - 1, self.ipntr[1] - 1 + self.n)","789","        if self.ido == -1:","790","            # initialization","791","            self.workd[yslice] = self.OP(self.workd[xslice])","792","        elif self.ido == 1:","793","            # compute y = Op*x","794","            if self.mode in (1, 2):","795","                self.workd[yslice] = self.OP(self.workd[xslice])","796","            else:","797","                Bxslice = slice(self.ipntr[2] - 1, self.ipntr[2] - 1 + self.n)","798","                self.workd[yslice] = self.OPa(self.workd[Bxslice])","799","        elif self.ido == 2:","800","            self.workd[yslice] = self.B(self.workd[xslice])","801","        elif self.ido == 3:","802","            raise ValueError(\"ARPACK requested user shifts.  Assure ISHIFT==0\")","803","        else:","804","            self.converged = True","805","","806","            if self.info == 0:","807","                pass","808","            elif self.info == 1:","809","                self._raise_no_convergence()","810","            else:","811","                raise ArpackError(self.info, infodict=self.iterate_infodict)","812","","813","    def extract(self, return_eigenvectors):","814","        k, n = self.k, self.n","815","","816","        ierr = 0","817","        howmny = 'A'  # return all eigenvectors","818","        sselect = np.zeros(self.ncv, 'int')  # unused","819","        sigmar = np.real(self.sigma)","820","        sigmai = np.imag(self.sigma)","821","        workev = np.zeros(3 * self.ncv, self.tp)","822","","823","        if self.tp in 'fd':","824","            dr = np.zeros(k + 1, self.tp)","825","            di = np.zeros(k + 1, self.tp)","826","            zr = np.zeros((n, k + 1), self.tp)","827","            dr, di, zr, ierr = \\","828","                self._arpack_extract(return_eigenvectors,","829","                       howmny, sselect, sigmar, sigmai, workev,","830","                       self.bmat, self.which, k, self.tol, self.resid,","831","                       self.v, self.iparam, self.ipntr,","832","                       self.workd, self.workl, self.info)","833","            if ierr != 0:","834","                raise ArpackError(ierr, infodict=self.extract_infodict)","835","            nreturned = self.iparam[4]  # number of good eigenvalues returned","836","","837","            # Build complex eigenvalues from real and imaginary parts","838","            d = dr + 1.0j * di","839","","840","            # Arrange the eigenvectors: complex eigenvectors are stored as","841","            # real,imaginary in consecutive columns","842","            z = zr.astype(self.tp.upper())","843","","844","            # The ARPACK nonsymmetric real and double interface (s,d)naupd","845","            # return eigenvalues and eigenvectors in real (float,double)","846","            # arrays.","847","","848","            # Efficiency: this should check that return_eigenvectors == True","849","            #  before going through this construction.","850","            if sigmai == 0:","851","                i = 0","852","                while i <= k:","853","                    # check if complex","854","                    if abs(d[i].imag) != 0:","855","                        # this is a complex conjugate pair with eigenvalues","856","                        # in consecutive columns","857","                        if i < k:","858","                            z[:, i] = zr[:, i] + 1.0j * zr[:, i + 1]","859","                            z[:, i + 1] = z[:, i].conjugate()","860","                            i += 1","861","                        else:","862","                            # last eigenvalue is complex: the imaginary part of","863","                            # the eigenvector has not been returned","864","                            # this can only happen if nreturned > k, so we'll","865","                            # throw out this case.","866","                            nreturned -= 1","867","                    i += 1","868","","869","            else:","870","                # real matrix, mode 3 or 4, imag(sigma) is nonzero:","871","                # see remark 3 in <s,d>neupd.f","872","                # Build complex eigenvalues from real and imaginary parts","873","                i = 0","874","                while i <= k:","875","                    if abs(d[i].imag) == 0:","876","                        d[i] = np.dot(zr[:, i], self.matvec(zr[:, i]))","877","                    else:","878","                        if i < k:","879","                            z[:, i] = zr[:, i] + 1.0j * zr[:, i + 1]","880","                            z[:, i + 1] = z[:, i].conjugate()","881","                            d[i] = ((np.dot(zr[:, i],","882","                                            self.matvec(zr[:, i])) +","883","                                     np.dot(zr[:, i + 1],","884","                                            self.matvec(zr[:, i + 1]))) +","885","                                    1j * (np.dot(zr[:, i],","886","                                          self.matvec(zr[:, i + 1])) -","887","                                          np.dot(zr[:, i + 1],","888","                                          self.matvec(zr[:, i]))))","889","                            d[i + 1] = d[i].conj()","890","                            i += 1","891","                        else:","892","                            # last eigenvalue is complex: the imaginary part of","893","                            # the eigenvector has not been returned","894","                            # this can only happen if nreturned > k, so we'll","895","                            # throw out this case.","896","                            nreturned -= 1","897","                    i += 1","898","","899","            # Now we have k+1 possible eigenvalues and eigenvectors","900","            # Return the ones specified by the keyword \"which\"","901","","902","            if nreturned <= k:","903","                # we got less or equal as many eigenvalues we wanted","904","                d = d[:nreturned]","905","                z = z[:, :nreturned]","906","            else:","907","                # we got one extra eigenvalue (likely a cc pair, but which?)","908","                # cut at approx precision for sorting","909","                rd = np.round(d, decimals=_ndigits[self.tp])","910","                if self.which in ['LR', 'SR']:","911","                    ind = np.argsort(rd.real)","912","                elif self.which in ['LI', 'SI']:","913","                    # for LI,SI ARPACK returns largest,smallest","914","                    # abs(imaginary) why?","915","                    ind = np.argsort(abs(rd.imag))","916","                else:","917","                    ind = np.argsort(abs(rd))","918","                if self.which in ['LR', 'LM', 'LI']:","919","                    d = d[ind[-k:]]","920","                    z = z[:, ind[-k:]]","921","                if self.which in ['SR', 'SM', 'SI']:","922","                    d = d[ind[:k]]","923","                    z = z[:, ind[:k]]","924","        else:","925","            # complex is so much simpler...","926","            d, z, ierr =\\","927","                    self._arpack_extract(return_eigenvectors,","928","                           howmny, sselect, self.sigma, workev,","929","                           self.bmat, self.which, k, self.tol, self.resid,","930","                           self.v, self.iparam, self.ipntr,","931","                           self.workd, self.workl, self.rwork, ierr)","932","","933","            if ierr != 0:","934","                raise ArpackError(ierr, infodict=self.extract_infodict)","935","","936","            k_ok = self.iparam[4]","937","            d = d[:k_ok]","938","            z = z[:, :k_ok]","939","","940","        if return_eigenvectors:","941","            return d, z","942","        else:","943","            return d","944","","945","","946","def _aslinearoperator_with_dtype(m):","947","    m = aslinearoperator(m)","948","    if not hasattr(m, 'dtype'):","949","        x = np.zeros(m.shape[1])","950","        m.dtype = (m * x).dtype","951","    return m","952","","953","","954","class SpLuInv(LinearOperator):","955","    \"\"\"","956","    SpLuInv:","957","       helper class to repeatedly solve M*x=b","958","       using a sparse LU-decopposition of M","959","    \"\"\"","960","    def __init__(self, M):","961","        self.M_lu = splu(M)","962","        self.shape = M.shape","963","        self.dtype = M.dtype","964","        self.isreal = not np.issubdtype(self.dtype, np.complexfloating)","965","","966","    def _matvec(self, x):","967","        # careful here: splu.solve will throw away imaginary","968","        # part of x if M is real","969","        x = np.asarray(x)","970","        if self.isreal and np.issubdtype(x.dtype, np.complexfloating):","971","            return (self.M_lu.solve(np.real(x).astype(self.dtype)) +","972","                    1j * self.M_lu.solve(np.imag(x).astype(self.dtype)))","973","        else:","974","            return self.M_lu.solve(x.astype(self.dtype))","975","","976","","977","class LuInv(LinearOperator):","978","    \"\"\"","979","    LuInv:","980","       helper class to repeatedly solve M*x=b","981","       using an LU-decomposition of M","982","    \"\"\"","983","    def __init__(self, M):","984","        self.M_lu = lu_factor(M)","985","        self.shape = M.shape","986","        self.dtype = M.dtype","987","","988","    def _matvec(self, x):","989","        return lu_solve(self.M_lu, x)","990","","991","","992","class IterInv(LinearOperator):","993","    \"\"\"","994","    IterInv:","995","       helper class to repeatedly solve M*x=b","996","       using an iterative method.","997","    \"\"\"","998","    def __init__(self, M, ifunc=gmres, tol=0):","999","        if tol <= 0:","1000","            # when tol=0, ARPACK uses machine tolerance as calculated","1001","            # by LAPACK's _LAMCH function.  We should match this","1002","            tol = 2 * np.finfo(M.dtype).eps","1003","        self.M = M","1004","        self.ifunc = ifunc","1005","        self.tol = tol","1006","        if hasattr(M, 'dtype'):","1007","            self.dtype = M.dtype","1008","        else:","1009","            x = np.zeros(M.shape[1])","1010","            self.dtype = (M * x).dtype","1011","        self.shape = M.shape","1012","","1013","    def _matvec(self, x):","1014","        b, info = self.ifunc(self.M, x, tol=self.tol)","1015","        if info != 0:","1016","            raise ValueError(\"Error in inverting M: function \"","1017","                             \"%s did not converge (info = %i).\"","1018","                             % (self.ifunc.__name__, info))","1019","        return b","1020","","1021","","1022","class IterOpInv(LinearOperator):","1023","    \"\"\"","1024","    IterOpInv:","1025","       helper class to repeatedly solve [A-sigma*M]*x = b","1026","       using an iterative method","1027","    \"\"\"","1028","    def __init__(self, A, M, sigma, ifunc=gmres, tol=0):","1029","        if tol <= 0:","1030","            # when tol=0, ARPACK uses machine tolerance as calculated","1031","            # by LAPACK's _LAMCH function.  We should match this","1032","            tol = 2 * np.finfo(A.dtype).eps","1033","        self.A = A","1034","        self.M = M","1035","        self.sigma = sigma","1036","        self.ifunc = ifunc","1037","        self.tol = tol","1038","","1039","        def mult_func(x):","1040","            return A.matvec(x) - sigma * M.matvec(x)","1041","","1042","        def mult_func_M_None(x):","1043","            return A.matvec(x) - sigma * x","1044","","1045","        x = np.zeros(A.shape[1])","1046","        if M is None:","1047","            dtype = mult_func_M_None(x).dtype","1048","            self.OP = LinearOperator(self.A.shape,","1049","                                     mult_func_M_None,","1050","                                     dtype=dtype)","1051","        else:","1052","            dtype = mult_func(x).dtype","1053","            self.OP = LinearOperator(self.A.shape,","1054","                                     mult_func,","1055","                                     dtype=dtype)","1056","        self.shape = A.shape","1057","","1058","    def _matvec(self, x):","1059","        b, info = self.ifunc(self.OP, x, tol=self.tol)","1060","        if info != 0:","1061","            raise ValueError(\"Error in inverting [A-sigma*M]: function \"","1062","                             \"%s did not converge (info = %i).\"","1063","                             % (self.ifunc.__name__, info))","1064","        return b","1065","","1066","    @property","1067","    def dtype(self):","1068","        return self.OP.dtype","1069","","1070","","1071","def get_inv_matvec(M, symmetric=False, tol=0):","1072","    if isdense(M):","1073","        return LuInv(M).matvec","1074","    elif isspmatrix(M):","1075","        if isspmatrix_csr(M) and symmetric:","1076","            M = M.T","1077","        return SpLuInv(M).matvec","1078","    else:","1079","        return IterInv(M, tol=tol).matvec","1080","","1081","","1082","def get_OPinv_matvec(A, M, sigma, symmetric=False, tol=0):","1083","    if sigma == 0:","1084","        return get_inv_matvec(A, symmetric=symmetric, tol=tol)","1085","","1086","    if M is None:","1087","        # M is the identity matrix","1088","        if isdense(A):","1089","            if (np.issubdtype(A.dtype, np.complexfloating) or","1090","               np.imag(sigma) == 0):","1091","                A = np.copy(A)","1092","            else:","1093","                A = A + 0j","1094","            A.flat[::A.shape[1] + 1] -= sigma","1095","            return LuInv(A).matvec","1096","        elif isspmatrix(A):","1097","            A = A - sigma * identity(A.shape[0])","1098","            if symmetric and isspmatrix_csr(A):","1099","                A = A.T","1100","            return SpLuInv(A.tocsc()).matvec","1101","        else:","1102","            return IterOpInv(_aslinearoperator_with_dtype(A),","1103","                             M, sigma, tol=tol).matvec","1104","    else:","1105","        if ((not isdense(A) and not isspmatrix(A)) or","1106","                (not isdense(M) and not isspmatrix(M))):","1107","            return IterOpInv(_aslinearoperator_with_dtype(A),","1108","                             _aslinearoperator_with_dtype(M),","1109","                             sigma, tol=tol).matvec","1110","        elif isdense(A) or isdense(M):","1111","            return LuInv(A - sigma * M).matvec","1112","        else:","1113","            OP = A - sigma * M","1114","            if symmetric and isspmatrix_csr(OP):","1115","                OP = OP.T","1116","            return SpLuInv(OP.tocsc()).matvec","1117","","1118","","1119","def _eigs(A, k=6, M=None, sigma=None, which='LM', v0=None,","1120","          ncv=None, maxiter=None, tol=0, return_eigenvectors=True,","1121","          Minv=None, OPinv=None, OPpart=None):","1122","    \"\"\"","1123","    Find k eigenvalues and eigenvectors of the square matrix A.","1124","","1125","    Solves ``A * x[i] = w[i] * x[i]``, the standard eigenvalue problem","1126","    for w[i] eigenvalues with corresponding eigenvectors x[i].","1127","","1128","    If M is specified, solves ``A * x[i] = w[i] * M * x[i]``, the","1129","    generalized eigenvalue problem for w[i] eigenvalues","1130","    with corresponding eigenvectors x[i]","1131","","1132","    Parameters","1133","    ----------","1134","    A : ndarray, sparse matrix or LinearOperator","1135","        An array, sparse matrix, or LinearOperator representing","1136","        the operation ``A * x``, where A is a real or complex square matrix.","1137","    k : int, optional","1138","        The number of eigenvalues and eigenvectors desired.","1139","        `k` must be smaller than N. It is not possible to compute all","1140","        eigenvectors of a matrix.","1141","    M : ndarray, sparse matrix or LinearOperator, optional","1142","        An array, sparse matrix, or LinearOperator representing","1143","        the operation M*x for the generalized eigenvalue problem","1144","","1145","            A * x = w * M * x.","1146","","1147","        M must represent a real, symmetric matrix if A is real, and must","1148","        represent a complex, hermitian matrix if A is complex. For best","1149","        results, the data type of M should be the same as that of A.","1150","        Additionally:","1151","","1152","            If `sigma` is None, M is positive definite","1153","","1154","            If sigma is specified, M is positive semi-definite","1155","","1156","        If sigma is None, eigs requires an operator to compute the solution","1157","        of the linear equation ``M * x = b``.  This is done internally via a","1158","        (sparse) LU decomposition for an explicit matrix M, or via an","1159","        iterative solver for a general linear operator.  Alternatively,","1160","        the user can supply the matrix or operator Minv, which gives","1161","        ``x = Minv * b = M^-1 * b``.","1162","    sigma : real or complex, optional","1163","        Find eigenvalues near sigma using shift-invert mode.  This requires","1164","        an operator to compute the solution of the linear system","1165","        ``[A - sigma * M] * x = b``, where M is the identity matrix if","1166","        unspecified. This is computed internally via a (sparse) LU","1167","        decomposition for explicit matrices A & M, or via an iterative","1168","        solver if either A or M is a general linear operator.","1169","        Alternatively, the user can supply the matrix or operator OPinv,","1170","        which gives ``x = OPinv * b = [A - sigma * M]^-1 * b``.","1171","        For a real matrix A, shift-invert can either be done in imaginary","1172","        mode or real mode, specified by the parameter OPpart ('r' or 'i').","1173","        Note that when sigma is specified, the keyword 'which' (below)","1174","        refers to the shifted eigenvalues ``w'[i]`` where:","1175","","1176","            If A is real and OPpart == 'r' (default),","1177","              ``w'[i] = 1\/2 * [1\/(w[i]-sigma) + 1\/(w[i]-conj(sigma))]``.","1178","","1179","            If A is real and OPpart == 'i',","1180","              ``w'[i] = 1\/2i * [1\/(w[i]-sigma) - 1\/(w[i]-conj(sigma))]``.","1181","","1182","            If A is complex, ``w'[i] = 1\/(w[i]-sigma)``.","1183","","1184","    v0 : ndarray, optional","1185","        Starting vector for iteration.","1186","        Default: random","1187","    ncv : int, optional","1188","        The number of Lanczos vectors generated","1189","        `ncv` must be greater than `k`; it is recommended that ``ncv > 2*k``.","1190","        Default: ``min(n, 2*k + 1)``","1191","    which : str, ['LM' | 'SM' | 'LR' | 'SR' | 'LI' | 'SI'], optional","1192","        Which `k` eigenvectors and eigenvalues to find:","1193","","1194","            'LM' : largest magnitude","1195","","1196","            'SM' : smallest magnitude","1197","","1198","            'LR' : largest real part","1199","","1200","            'SR' : smallest real part","1201","","1202","            'LI' : largest imaginary part","1203","","1204","            'SI' : smallest imaginary part","1205","","1206","        When sigma != None, 'which' refers to the shifted eigenvalues w'[i]","1207","        (see discussion in 'sigma', above).  ARPACK is generally better","1208","        at finding large values than small values.  If small eigenvalues are","1209","        desired, consider using shift-invert mode for better performance.","1210","    maxiter : int, optional","1211","        Maximum number of Arnoldi update iterations allowed","1212","        Default: ``n*10``","1213","    tol : float, optional","1214","        Relative accuracy for eigenvalues (stopping criterion)","1215","        The default value of 0 implies machine precision.","1216","    return_eigenvectors : bool, optional","1217","        Return eigenvectors (True) in addition to eigenvalues","1218","    Minv : ndarray, sparse matrix or LinearOperator, optional","1219","        See notes in M, above.","1220","    OPinv : ndarray, sparse matrix or LinearOperator, optional","1221","        See notes in sigma, above.","1222","    OPpart : {'r' or 'i'}, optional","1223","        See notes in sigma, above","1224","","1225","    Returns","1226","    -------","1227","    w : ndarray","1228","        Array of k eigenvalues.","1229","    v : ndarray","1230","        An array of `k` eigenvectors.","1231","        ``v[:, i]`` is the eigenvector corresponding to the eigenvalue w[i].","1232","","1233","    Raises","1234","    ------","1235","    ArpackNoConvergence","1236","        When the requested convergence is not obtained.","1237","        The currently converged eigenvalues and eigenvectors can be found","1238","        as ``eigenvalues`` and ``eigenvectors`` attributes of the exception","1239","        object.","1240","","1241","    See Also","1242","    --------","1243","    eigsh : eigenvalues and eigenvectors for symmetric matrix A","1244","    svds : singular value decomposition for a matrix A","1245","","1246","    Notes","1247","    -----","1248","    This function is a wrapper to the ARPACK [1]_ SNEUPD, DNEUPD, CNEUPD,","1249","    ZNEUPD, functions which use the Implicitly Restarted Arnoldi Method to","1250","    find the eigenvalues and eigenvectors [2]_.","1251","","1252","    References","1253","    ----------","1254","    .. [1] ARPACK Software, http:\/\/www.caam.rice.edu\/software\/ARPACK\/","1255","    .. [2] R. B. Lehoucq, D. C. Sorensen, and C. Yang,  ARPACK USERS GUIDE:","1256","       Solution of Large Scale Eigenvalue Problems by Implicitly Restarted","1257","       Arnoldi Methods. SIAM, Philadelphia, PA, 1998.","1258","","1259","    Examples","1260","    --------","1261","    Find 6 eigenvectors of the identity matrix:","1262","","1263","    >>> import scipy.sparse as sparse","1264","    >>> id = np.eye(13)","1265","    >>> vals, vecs = sparse.linalg.eigs(id, k=6)","1266","    >>> vals","1267","    array([ 1.+0.j,  1.+0.j,  1.+0.j,  1.+0.j,  1.+0.j,  1.+0.j])","1268","    >>> vecs.shape","1269","    (13, 6)","1270","","1271","    \"\"\"","1272","    if A.shape[0] != A.shape[1]:","1273","        raise ValueError('expected square matrix (shape=%s)' % (A.shape,))","1274","    if M is not None:","1275","        if M.shape != A.shape:","1276","            raise ValueError('wrong M dimensions %s, should be %s'","1277","                             % (M.shape, A.shape))","1278","        if np.dtype(M.dtype).char.lower() != np.dtype(A.dtype).char.lower():","1279","            import warnings","1280","            warnings.warn('M does not have the same type precision as A. '","1281","                          'This may adversely affect ARPACK convergence')","1282","    n = A.shape[0]","1283","","1284","    if k <= 0 or k >= n:","1285","        raise ValueError(\"k=%d must be between 1 and ndim(A)-1=%d\"","1286","                         % (k, n - 1))","1287","","1288","    if sigma is None:","1289","        matvec = _aslinearoperator_with_dtype(A).matvec","1290","","1291","        if OPinv is not None:","1292","            raise ValueError(\"OPinv should not be specified \"","1293","                             \"with sigma = None.\")","1294","        if OPpart is not None:","1295","            raise ValueError(\"OPpart should not be specified with \"","1296","                             \"sigma = None or complex A\")","1297","","1298","        if M is None:","1299","            # standard eigenvalue problem","1300","            mode = 1","1301","            M_matvec = None","1302","            Minv_matvec = None","1303","            if Minv is not None:","1304","                raise ValueError(\"Minv should not be \"","1305","                                 \"specified with M = None.\")","1306","        else:","1307","            # general eigenvalue problem","1308","            mode = 2","1309","            if Minv is None:","1310","                Minv_matvec = get_inv_matvec(M, symmetric=True, tol=tol)","1311","            else:","1312","                Minv = _aslinearoperator_with_dtype(Minv)","1313","                Minv_matvec = Minv.matvec","1314","            M_matvec = _aslinearoperator_with_dtype(M).matvec","1315","    else:","1316","        # sigma is not None: shift-invert mode","1317","        if np.issubdtype(A.dtype, np.complexfloating):","1318","            if OPpart is not None:","1319","                raise ValueError(\"OPpart should not be specified \"","1320","                                 \"with sigma=None or complex A\")","1321","            mode = 3","1322","        elif OPpart is None or OPpart.lower() == 'r':","1323","            mode = 3","1324","        elif OPpart.lower() == 'i':","1325","            if np.imag(sigma) == 0:","1326","                raise ValueError(\"OPpart cannot be 'i' if sigma is real\")","1327","            mode = 4","1328","        else:","1329","            raise ValueError(\"OPpart must be one of ('r','i')\")","1330","","1331","        matvec = _aslinearoperator_with_dtype(A).matvec","1332","        if Minv is not None:","1333","            raise ValueError(\"Minv should not be specified when sigma is\")","1334","        if OPinv is None:","1335","            Minv_matvec = get_OPinv_matvec(A, M, sigma,","1336","                                           symmetric=False, tol=tol)","1337","        else:","1338","            OPinv = _aslinearoperator_with_dtype(OPinv)","1339","            Minv_matvec = OPinv.matvec","1340","        if M is None:","1341","            M_matvec = None","1342","        else:","1343","            M_matvec = _aslinearoperator_with_dtype(M).matvec","1344","","1345","    params = _UnsymmetricArpackParams(n, k, A.dtype.char, matvec, mode,","1346","                                      M_matvec, Minv_matvec, sigma,","1347","                                      ncv, v0, maxiter, which, tol)","1348","","1349","    while not params.converged:","1350","        params.iterate()","1351","","1352","    return params.extract(return_eigenvectors)","1353","","1354","","1355","def _eigsh(A, k=6, M=None, sigma=None, which='LM', v0=None,","1356","           ncv=None, maxiter=None, tol=0, return_eigenvectors=True,","1357","           Minv=None, OPinv=None, mode='normal'):","1358","    \"\"\"","1359","    Find k eigenvalues and eigenvectors of the real symmetric square matrix","1360","    or complex hermitian matrix A.","1361","","1362","    Solves ``A * x[i] = w[i] * x[i]``, the standard eigenvalue problem for","1363","    w[i] eigenvalues with corresponding eigenvectors x[i].","1364","","1365","    If M is specified, solves ``A * x[i] = w[i] * M * x[i]``, the","1366","    generalized eigenvalue problem for w[i] eigenvalues","1367","    with corresponding eigenvectors x[i]","1368","","1369","    Parameters","1370","    ----------","1371","    A : An N x N matrix, array, sparse matrix, or LinearOperator representing","1372","        the operation A * x, where A is a real symmetric matrix","1373","        For buckling mode (see below) A must additionally be positive-definite","1374","    k : int, optional","1375","        The number of eigenvalues and eigenvectors desired.","1376","        `k` must be smaller than N. It is not possible to compute all","1377","        eigenvectors of a matrix.","1378","","1379","    Returns","1380","    -------","1381","    w : array","1382","        Array of k eigenvalues","1383","    v : array","1384","        An array representing the `k` eigenvectors.  The column ``v[:, i]`` is","1385","        the eigenvector corresponding to the eigenvalue ``w[i]``.","1386","","1387","    Other Parameters","1388","    ----------------","1389","    M : An N x N matrix, array, sparse matrix, or linear operator representing","1390","        the operation M * x for the generalized eigenvalue problem","1391","","1392","            A * x = w * M * x.","1393","","1394","        M must represent a real, symmetric matrix if A is real, and must","1395","        represent a complex, hermitian matrix if A is complex. For best","1396","        results, the data type of M should be the same as that of A.","1397","        Additionally:","1398","","1399","            If sigma is None, M is symmetric positive definite","1400","","1401","            If sigma is specified, M is symmetric positive semi-definite","1402","","1403","            In buckling mode, M is symmetric indefinite.","1404","","1405","        If sigma is None, eigsh requires an operator to compute the solution","1406","        of the linear equation ``M * x = b``. This is done internally via a","1407","        (sparse) LU decomposition for an explicit matrix M, or via an","1408","        iterative solver for a general linear operator.  Alternatively,","1409","        the user can supply the matrix or operator Minv, which gives","1410","        ``x = Minv * b = M^-1 * b``.","1411","    sigma : real","1412","        Find eigenvalues near sigma using shift-invert mode.  This requires","1413","        an operator to compute the solution of the linear system","1414","        `[A - sigma * M] x = b`, where M is the identity matrix if","1415","        unspecified.  This is computed internally via a (sparse) LU","1416","        decomposition for explicit matrices A & M, or via an iterative","1417","        solver if either A or M is a general linear operator.","1418","        Alternatively, the user can supply the matrix or operator OPinv,","1419","        which gives ``x = OPinv * b = [A - sigma * M]^-1 * b``.","1420","        Note that when sigma is specified, the keyword 'which' refers to","1421","        the shifted eigenvalues ``w'[i]`` where:","1422","","1423","            if mode == 'normal', ``w'[i] = 1 \/ (w[i] - sigma)``.","1424","","1425","            if mode == 'cayley', ``w'[i] = (w[i] + sigma) \/ (w[i] - sigma)``.","1426","","1427","            if mode == 'buckling', ``w'[i] = w[i] \/ (w[i] - sigma)``.","1428","","1429","        (see further discussion in 'mode' below)","1430","    v0 : ndarray, optional","1431","        Starting vector for iteration.","1432","        Default: random","1433","    ncv : int, optional","1434","        The number of Lanczos vectors generated ncv must be greater than k and","1435","        smaller than n; it is recommended that ``ncv > 2*k``.","1436","        Default: ``min(n, 2*k + 1)``","1437","    which : str ['LM' | 'SM' | 'LA' | 'SA' | 'BE']","1438","        If A is a complex hermitian matrix, 'BE' is invalid.","1439","        Which `k` eigenvectors and eigenvalues to find:","1440","","1441","            'LM' : Largest (in magnitude) eigenvalues","1442","","1443","            'SM' : Smallest (in magnitude) eigenvalues","1444","","1445","            'LA' : Largest (algebraic) eigenvalues","1446","","1447","            'SA' : Smallest (algebraic) eigenvalues","1448","","1449","            'BE' : Half (k\/2) from each end of the spectrum","1450","","1451","        When k is odd, return one more (k\/2+1) from the high end.","1452","        When sigma != None, 'which' refers to the shifted eigenvalues ``w'[i]``","1453","        (see discussion in 'sigma', above).  ARPACK is generally better","1454","        at finding large values than small values.  If small eigenvalues are","1455","        desired, consider using shift-invert mode for better performance.","1456","    maxiter : int, optional","1457","        Maximum number of Arnoldi update iterations allowed","1458","        Default: ``n*10``","1459","    tol : float","1460","        Relative accuracy for eigenvalues (stopping criterion).","1461","        The default value of 0 implies machine precision.","1462","    Minv : N x N matrix, array, sparse matrix, or LinearOperator","1463","        See notes in M, above","1464","    OPinv : N x N matrix, array, sparse matrix, or LinearOperator","1465","        See notes in sigma, above.","1466","    return_eigenvectors : bool","1467","        Return eigenvectors (True) in addition to eigenvalues","1468","    mode : string ['normal' | 'buckling' | 'cayley']","1469","        Specify strategy to use for shift-invert mode.  This argument applies","1470","        only for real-valued A and sigma != None.  For shift-invert mode,","1471","        ARPACK internally solves the eigenvalue problem","1472","        ``OP * x'[i] = w'[i] * B * x'[i]``","1473","        and transforms the resulting Ritz vectors x'[i] and Ritz values w'[i]","1474","        into the desired eigenvectors and eigenvalues of the problem","1475","        ``A * x[i] = w[i] * M * x[i]``.","1476","        The modes are as follows:","1477","","1478","            'normal' :","1479","                OP = [A - sigma * M]^-1 * M,","1480","                B = M,","1481","                w'[i] = 1 \/ (w[i] - sigma)","1482","","1483","            'buckling' :","1484","                OP = [A - sigma * M]^-1 * A,","1485","                B = A,","1486","                w'[i] = w[i] \/ (w[i] - sigma)","1487","","1488","            'cayley' :","1489","                OP = [A - sigma * M]^-1 * [A + sigma * M],","1490","                B = M,","1491","                w'[i] = (w[i] + sigma) \/ (w[i] - sigma)","1492","","1493","        The choice of mode will affect which eigenvalues are selected by","1494","        the keyword 'which', and can also impact the stability of","1495","        convergence (see [2] for a discussion)","1496","","1497","    Raises","1498","    ------","1499","    ArpackNoConvergence","1500","        When the requested convergence is not obtained.","1501","","1502","        The currently converged eigenvalues and eigenvectors can be found","1503","        as ``eigenvalues`` and ``eigenvectors`` attributes of the exception","1504","        object.","1505","","1506","    See Also","1507","    --------","1508","    eigs : eigenvalues and eigenvectors for a general (nonsymmetric) matrix A","1509","    svds : singular value decomposition for a matrix A","1510","","1511","    Notes","1512","    -----","1513","    This function is a wrapper to the ARPACK [1]_ SSEUPD and DSEUPD","1514","    functions which use the Implicitly Restarted Lanczos Method to","1515","    find the eigenvalues and eigenvectors [2]_.","1516","","1517","    References","1518","    ----------","1519","    .. [1] ARPACK Software, http:\/\/www.caam.rice.edu\/software\/ARPACK\/","1520","    .. [2] R. B. Lehoucq, D. C. Sorensen, and C. Yang,  ARPACK USERS GUIDE:","1521","       Solution of Large Scale Eigenvalue Problems by Implicitly Restarted","1522","       Arnoldi Methods. SIAM, Philadelphia, PA, 1998.","1523","","1524","    Examples","1525","    --------","1526","    >>> import scipy.sparse as sparse","1527","    >>> id = np.eye(13)","1528","    >>> vals, vecs = sparse.linalg.eigsh(id, k=6)","1529","    >>> vals","1530","    array([ 1.,  1.,  1.,  1.,  1.,  1.])","1531","    >>> vecs.shape","1532","    (13, 6)","1533","","1534","    \"\"\"","1535","    # complex hermitian matrices should be solved with eigs","1536","    if np.issubdtype(A.dtype, np.complexfloating):","1537","        if mode != 'normal':","1538","            raise ValueError(\"mode=%s cannot be used with \"","1539","                             \"complex matrix A\" % mode)","1540","        if which == 'BE':","1541","            raise ValueError(\"which='BE' cannot be used with complex matrix A\")","1542","        elif which == 'LA':","1543","            which = 'LR'","1544","        elif which == 'SA':","1545","            which = 'SR'","1546","        ret = eigs(A, k, M=M, sigma=sigma, which=which, v0=v0,","1547","                   ncv=ncv, maxiter=maxiter, tol=tol,","1548","                   return_eigenvectors=return_eigenvectors, Minv=Minv,","1549","                   OPinv=OPinv)","1550","","1551","        if return_eigenvectors:","1552","            return ret[0].real, ret[1]","1553","        else:","1554","            return ret.real","1555","","1556","    if A.shape[0] != A.shape[1]:","1557","        raise ValueError('expected square matrix (shape=%s)' % (A.shape,))","1558","    if M is not None:","1559","        if M.shape != A.shape:","1560","            raise ValueError('wrong M dimensions %s, should be %s'","1561","                             % (M.shape, A.shape))","1562","        if np.dtype(M.dtype).char.lower() != np.dtype(A.dtype).char.lower():","1563","            import warnings","1564","            warnings.warn('M does not have the same type precision as A. '","1565","                          'This may adversely affect ARPACK convergence')","1566","    n = A.shape[0]","1567","","1568","    if k <= 0 or k >= n:","1569","        raise ValueError(\"k must be between 1 and the order of the \"","1570","                         \"square input matrix.\")","1571","","1572","    if sigma is None:","1573","        A = _aslinearoperator_with_dtype(A)","1574","        matvec = A.matvec","1575","","1576","        if OPinv is not None:","1577","            raise ValueError(\"OPinv should not be specified \"","1578","                             \"with sigma = None.\")","1579","        if M is None:","1580","            # standard eigenvalue problem","1581","            mode = 1","1582","            M_matvec = None","1583","            Minv_matvec = None","1584","            if Minv is not None:","1585","                raise ValueError(\"Minv should not be \"","1586","                                 \"specified with M = None.\")","1587","        else:","1588","            # general eigenvalue problem","1589","            mode = 2","1590","            if Minv is None:","1591","                Minv_matvec = get_inv_matvec(M, symmetric=True, tol=tol)","1592","            else:","1593","                Minv = _aslinearoperator_with_dtype(Minv)","1594","                Minv_matvec = Minv.matvec","1595","            M_matvec = _aslinearoperator_with_dtype(M).matvec","1596","    else:","1597","        # sigma is not None: shift-invert mode","1598","        if Minv is not None:","1599","            raise ValueError(\"Minv should not be specified when sigma is\")","1600","","1601","        # normal mode","1602","        if mode == 'normal':","1603","            mode = 3","1604","            matvec = None","1605","            if OPinv is None:","1606","                Minv_matvec = get_OPinv_matvec(A, M, sigma,","1607","                                               symmetric=True, tol=tol)","1608","            else:","1609","                OPinv = _aslinearoperator_with_dtype(OPinv)","1610","                Minv_matvec = OPinv.matvec","1611","            if M is None:","1612","                M_matvec = None","1613","            else:","1614","                M = _aslinearoperator_with_dtype(M)","1615","                M_matvec = M.matvec","1616","","1617","        # buckling mode","1618","        elif mode == 'buckling':","1619","            mode = 4","1620","            if OPinv is None:","1621","                Minv_matvec = get_OPinv_matvec(A, M, sigma,","1622","                                               symmetric=True, tol=tol)","1623","            else:","1624","                Minv_matvec = _aslinearoperator_with_dtype(OPinv).matvec","1625","            matvec = _aslinearoperator_with_dtype(A).matvec","1626","            M_matvec = None","1627","","1628","        # cayley-transform mode","1629","        elif mode == 'cayley':","1630","            mode = 5","1631","            matvec = _aslinearoperator_with_dtype(A).matvec","1632","            if OPinv is None:","1633","                Minv_matvec = get_OPinv_matvec(A, M, sigma,","1634","                                               symmetric=True, tol=tol)","1635","            else:","1636","                Minv_matvec = _aslinearoperator_with_dtype(OPinv).matvec","1637","            if M is None:","1638","                M_matvec = None","1639","            else:","1640","                M_matvec = _aslinearoperator_with_dtype(M).matvec","1641","","1642","        # unrecognized mode","1643","        else:","1644","            raise ValueError(\"unrecognized mode '%s'\" % mode)","1645","","1646","    params = _SymmetricArpackParams(n, k, A.dtype.char, matvec, mode,","1647","                                    M_matvec, Minv_matvec, sigma,","1648","                                    ncv, v0, maxiter, which, tol)","1649","","1650","    while not params.converged:","1651","        params.iterate()","1652","","1653","    return params.extract(return_eigenvectors)","1654","","1655","","1656","def _augmented_orthonormal_cols(x, k):","1657","    # extract the shape of the x array","1658","    n, m = x.shape","1659","    # create the expanded array and copy x into it","1660","    y = np.empty((n, m+k), dtype=x.dtype)","1661","    y[:, :m] = x","1662","    # do some modified gram schmidt to add k random orthonormal vectors","1663","    for i in range(k):","1664","        # sample a random initial vector","1665","        v = np.random.randn(n)","1666","        if np.iscomplexobj(x):","1667","            v = v + 1j*np.random.randn(n)","1668","        # subtract projections onto the existing unit length vectors","1669","        for j in range(m+i):","1670","            u = y[:, j]","1671","            v -= (np.dot(v, u.conj()) \/ np.dot(u, u.conj())) * u","1672","        # normalize v","1673","        v \/= np.sqrt(np.dot(v, v.conj()))","1674","        # add v into the output array","1675","        y[:, m+i] = v","1676","    # return the expanded array","1677","    return y","1678","","1679","","1680","def _augmented_orthonormal_rows(x, k):","1681","    return _augmented_orthonormal_cols(x.T, k).T","1682","","1683","","1684","def _herm(x):","1685","    return x.T.conj()","1686","","1687","","1688","def _svds(A, k=6, ncv=None, tol=0, which='LM', v0=None,","1689","          maxiter=None, return_singular_vectors=True):","1690","    \"\"\"Compute the largest k singular values\/vectors for a sparse matrix.","1691","","1692","    Parameters","1693","    ----------","1694","    A : {sparse matrix, LinearOperator}","1695","        Array to compute the SVD on, of shape (M, N)","1696","    k : int, optional","1697","        Number of singular values and vectors to compute.","1698","    ncv : int, optional","1699","        The number of Lanczos vectors generated","1700","        ncv must be greater than k+1 and smaller than n;","1701","        it is recommended that ncv > 2*k","1702","        Default: ``min(n, 2*k + 1)``","1703","    tol : float, optional","1704","        Tolerance for singular values. Zero (default) means machine precision.","1705","    which : str, ['LM' | 'SM'], optional","1706","        Which `k` singular values to find:","1707","","1708","            - 'LM' : largest singular values","1709","            - 'SM' : smallest singular values","1710","","1711","        .. versionadded:: 0.12.0","1712","    v0 : ndarray, optional","1713","        Starting vector for iteration, of length min(A.shape). Should be an","1714","        (approximate) left singular vector if N > M and a right singular","1715","        vector otherwise.","1716","        Default: random","1717","","1718","        .. versionadded:: 0.12.0","1719","    maxiter : int, optional","1720","        Maximum number of iterations.","1721","","1722","        .. versionadded:: 0.12.0","1723","    return_singular_vectors : bool or str, optional","1724","        - True: return singular vectors (True) in addition to singular values.","1725","","1726","        .. versionadded:: 0.12.0","1727","","1728","        - \"u\": only return the u matrix, without computing vh (if N > M).","1729","        - \"vh\": only return the vh matrix, without computing u (if N <= M).","1730","","1731","        .. versionadded:: 0.16.0","1732","","1733","    Returns","1734","    -------","1735","    u : ndarray, shape=(M, k)","1736","        Unitary matrix having left singular vectors as columns.","1737","        If `return_singular_vectors` is \"vh\", this variable is not computed,","1738","        and None is returned instead.","1739","    s : ndarray, shape=(k,)","1740","        The singular values.","1741","    vt : ndarray, shape=(k, N)","1742","        Unitary matrix having right singular vectors as rows.","1743","        If `return_singular_vectors` is \"u\", this variable is not computed,","1744","        and None is returned instead.","1745","","1746","","1747","    Notes","1748","    -----","1749","    This is a naive implementation using ARPACK as an eigensolver","1750","    on A.H * A or A * A.H, depending on which one is more efficient.","1751","","1752","    \"\"\"","1753","    if not (isinstance(A, LinearOperator) or isspmatrix(A)):","1754","        A = np.asarray(A)","1755","","1756","    n, m = A.shape","1757","","1758","    if isinstance(A, LinearOperator):","1759","        if n > m:","1760","            X_dot = A.matvec","1761","            X_matmat = A.matmat","1762","            XH_dot = A.rmatvec","1763","        else:","1764","            X_dot = A.rmatvec","1765","            XH_dot = A.matvec","1766","","1767","            dtype = getattr(A, 'dtype', None)","1768","            if dtype is None:","1769","                dtype = A.dot(np.zeros([m, 1])).dtype","1770","","1771","            # A^H * V; works around lack of LinearOperator.adjoint.","1772","            # XXX This can be slow!","1773","            def X_matmat(V):","1774","                out = np.empty((V.shape[1], m), dtype=dtype)","1775","                for i, col in enumerate(V.T):","1776","                    out[i, :] = A.rmatvec(col.reshape(-1, 1)).T","1777","                return out.T","1778","","1779","    else:","1780","        if n > m:","1781","            X_dot = X_matmat = A.dot","1782","            XH_dot = _herm(A).dot","1783","        else:","1784","            XH_dot = A.dot","1785","            X_dot = X_matmat = _herm(A).dot","1786","","1787","    def matvec_XH_X(x):","1788","        return XH_dot(X_dot(x))","1789","","1790","    XH_X = LinearOperator(matvec=matvec_XH_X, dtype=A.dtype,","1791","                          shape=(min(A.shape), min(A.shape)))","1792","","1793","    # Get a low rank approximation of the implicitly defined gramian matrix.","1794","    # This is not a stable way to approach the problem.","1795","    eigvals, eigvec = eigsh(XH_X, k=k, tol=tol ** 2, maxiter=maxiter,","1796","                            ncv=ncv, which=which, v0=v0)","1797","","1798","    # In 'LM' mode try to be clever about small eigenvalues.","1799","    # Otherwise in 'SM' mode do not try to be clever.","1800","    if which == 'LM':","1801","","1802","        # Gramian matrices have real non-negative eigenvalues.","1803","        eigvals = np.maximum(eigvals.real, 0)","1804","","1805","        # Use the sophisticated detection of small eigenvalues from pinvh.","1806","        t = eigvec.dtype.char.lower()","1807","        factor = {'f': 1E3, 'd': 1E6}","1808","        cond = factor[t] * np.finfo(t).eps","1809","        cutoff = cond * np.max(eigvals)","1810","","1811","        # Get a mask indicating which eigenpairs are not degenerately tiny,","1812","        # and create the re-ordered array of thresholded singular values.","1813","        above_cutoff = (eigvals > cutoff)","1814","        nlarge = above_cutoff.sum()","1815","        nsmall = k - nlarge","1816","        slarge = np.sqrt(eigvals[above_cutoff])","1817","        s = np.zeros_like(eigvals)","1818","        s[:nlarge] = slarge","1819","        if not return_singular_vectors:","1820","            return s","1821","","1822","        if n > m:","1823","            vlarge = eigvec[:, above_cutoff]","1824","            ularge = X_matmat(vlarge) \/ slarge if return_singular_vectors != 'vh' else None","1825","            vhlarge = _herm(vlarge)","1826","        else:","1827","            ularge = eigvec[:, above_cutoff]","1828","            vhlarge = _herm(X_matmat(ularge) \/ slarge) if return_singular_vectors != 'u' else None","1829","","1830","        u = _augmented_orthonormal_cols(ularge, nsmall) if ularge is not None else None","1831","        vh = _augmented_orthonormal_rows(vhlarge, nsmall) if vhlarge is not None else None","1832","","1833","    elif which == 'SM':","1834","","1835","        s = np.sqrt(eigvals)","1836","        if not return_singular_vectors:","1837","            return s","1838","","1839","        if n > m:","1840","            v = eigvec","1841","            u = X_matmat(v) \/ s if return_singular_vectors != 'vh' else None","1842","            vh = _herm(v)","1843","        else:","1844","            u = eigvec","1845","            vh = _herm(X_matmat(u) \/ s) if return_singular_vectors != 'u' else None","1846","","1847","    else:","1848","","1849","        raise ValueError(\"which must be either 'LM' or 'SM'.\")","1850","","1851","    return u, s, vh","1852","","1853","","1854","# Redefine the backported function","1855","if scipy.version.version >= LooseVersion('0.12'):","1856","    from scipy.sparse.linalg import eigs, eigsh, svds","1857","else:","1858","    eigs, eigsh, svds = _eigs, _eigsh, _svds"]}],"sklearn\/utils\/sparsetools\/__init__.py":[{"add":["0","# Remove in version 0.21","2","from scipy.sparse.csgraph import connected_components as \\","3","     scipy_connected_components","5","from sklearn.utils.deprecation import deprecated","6","","7","","8","@deprecated(\"sklearn.utils.sparsetools.connected_components was deprecated in\"","9","            \"version 0.19 and will be removed in 0.21. Use\"","10","            \"scipy.sparse.csgraph.connected_components instead.\")","11","def connected_components(*args, **kwargs):","12","    return scipy_connected_components(*args, **kwargs)"],"delete":["0","\"\"\"sparsetools - a collection of routines for sparse matrix operations\"\"\"","2","from ._traversal import connected_components","4","__all__ = [\"connected_components\"]"]}],"sklearn\/utils\/fixes.py":[{"add":[],"delete":["45","try:","46","    from scipy.special import expit     # SciPy >= 0.10","47","    with np.errstate(invalid='ignore', over='ignore'):","48","        if np.isnan(expit(1000)):       # SciPy < 0.14","49","            raise ImportError(\"no stable expit in scipy.special\")","50","except ImportError:","51","    def expit(x, out=None):","52","        \"\"\"Logistic sigmoid function, ``1 \/ (1 + exp(-x))``.","53","","54","        See sklearn.utils.extmath.log_logistic for the log of this function.","55","        \"\"\"","56","        if out is None:","57","            out = np.empty(np.atleast_1d(x).shape, dtype=np.float64)","58","        out[:] = x","59","","60","        # 1 \/ (1 + exp(-x)) = (1 + tanh(x \/ 2)) \/ 2","61","        # This way of computing the logistic is both fast and stable.","62","        out *= .5","63","        np.tanh(out, out)","64","        out += 1","65","        out *= .5","66","","67","        return out.reshape(np.shape(x))","68","","69","","327","if sp_version < (0, 13, 0):","328","    def rankdata(a, method='average'):","329","        if method not in ('average', 'min', 'max', 'dense', 'ordinal'):","330","            raise ValueError('unknown method \"{0}\"'.format(method))","331","","332","        arr = np.ravel(np.asarray(a))","333","        algo = 'mergesort' if method == 'ordinal' else 'quicksort'","334","        sorter = np.argsort(arr, kind=algo)","335","","336","        inv = np.empty(sorter.size, dtype=np.intp)","337","        inv[sorter] = np.arange(sorter.size, dtype=np.intp)","338","","339","        if method == 'ordinal':","340","            return inv + 1","341","","342","        arr = arr[sorter]","343","        obs = np.r_[True, arr[1:] != arr[:-1]]","344","        dense = obs.cumsum()[inv]","345","","346","        if method == 'dense':","347","            return dense","348","","349","        # cumulative counts of each unique value","350","        count = np.r_[np.nonzero(obs)[0], len(obs)]","351","","352","        if method == 'max':","353","            return count[dense]","354","","355","        if method == 'min':","356","            return count[dense - 1] + 1","357","","358","        # average method","359","        return .5 * (count[dense] + count[dense - 1] + 1)","360","else:","361","    from scipy.stats import rankdata","362",""]}],"sklearn\/linear_model\/tests\/test_logistic.py":[{"add":["998","    solvers = ['newton-cg', 'liblinear', 'sag', 'saga', 'lbfgs']","1065","    solvers = ['newton-cg', 'sag', 'saga', 'lbfgs']"],"delete":["8","from sklearn.utils.fixes import sp_version","999","    solvers = ['newton-cg', 'liblinear', 'sag', 'saga']","1000","    # old scipy doesn't have maxiter","1001","    if sp_version >= (0, 12):","1002","        solvers.append('lbfgs')","1069","    solvers = ['newton-cg', 'sag', 'saga']","1070","    # old scipy doesn't have maxiter","1071","    if sp_version >= (0, 12):","1072","        solvers.append('lbfgs')"]}],"sklearn\/utils\/tests\/test_fixes.py":[{"add":["14","from sklearn.utils.fixes import divide"],"delete":["12","from sklearn.utils.testing import assert_almost_equal","14","from sklearn.utils.testing import assert_array_almost_equal","16","from sklearn.utils.fixes import divide, expit","22","def test_expit():","23","    # Check numerical stability of expit (logistic function).","24","","25","    # Simulate our previous Cython implementation, based on","26","    #http:\/\/fa.bianp.net\/blog\/2013\/numerical-optimizers-for-logistic-regression","27","    assert_almost_equal(expit(1000.), 1. \/ (1. + np.exp(-1000.)), decimal=16)","28","    assert_almost_equal(expit(-1000.), np.exp(-1000.) \/ (1. + np.exp(-1000.)),","29","                        decimal=16)","30","","31","    x = np.arange(10)","32","    out = np.zeros_like(x, dtype=np.float32)","33","    assert_array_almost_equal(expit(x), expit(x, out=out))","34","","35",""]}],"sklearn\/utils\/stats.py":[{"add":["1","from scipy.stats import rankdata as scipy_rankdata","2","","3","from sklearn.utils.extmath import stable_cumsum","4","from sklearn.utils.deprecation import deprecated","7","# Remove in sklearn 0.21","8","@deprecated(\"sklearn.utils.stats.rankdata was deprecated in version 0.19 and\"","9","            \"will be removed in 0.21. Use scipy.stats.rankdata instead.\")","10","def rankdata(*args, **kwargs):","11","    return scipy_rankdata(*args, **kwargs)","15","    \"\"\"","16","    Compute the weighted ``percentile`` of ``array`` with ``sample_weight``.","17","    \"\"\""],"delete":["1","from scipy.stats import rankdata as _sp_rankdata","2","from .fixes import bincount","3","from ..utils.extmath import stable_cumsum","6","# To remove when we support scipy 0.13","7","def _rankdata(a, method=\"average\"):","8","    \"\"\"Assign ranks to data, dealing with ties appropriately.","9","","10","    Ranks begin at 1. The method argument controls how ranks are assigned","11","    to equal values.","12","","13","    Parameters","14","    ----------","15","    a : array_like","16","        The array of values to be ranked. The array is first flattened.","17","","18","    method : str, optional","19","        The method used to assign ranks to tied elements.","20","        The options are 'max'.","21","        'max': The maximum of the ranks that would have been assigned","22","              to all the tied values is assigned to each value.","23","","24","    Returns","25","    -------","26","    ranks : ndarray","27","        An array of length equal to the size of a, containing rank scores.","28","","29","    Notes","30","    -----","31","    We only backport the 'max' method","32","","33","    \"\"\"","34","    if method != \"max\":","35","        raise NotImplementedError()","36","","37","    unique_all, inverse = np.unique(a, return_inverse=True)","38","    count = bincount(inverse, minlength=unique_all.size)","39","    cum_count = count.cumsum()","40","    rank = cum_count[inverse]","41","    return rank","42","","43","try:","44","    _sp_rankdata([1.], 'max')","45","    rankdata = _sp_rankdata","46","","47","except TypeError as e:","48","    rankdata = _rankdata","52","    \"\"\"Compute the weighted ``percentile`` of ``array`` with ``sample_weight``. \"\"\""]}],"sklearn\/decomposition\/online_lda.py":[{"add":["15","from scipy.misc import logsumexp","687","            norm_phi = logsumexp(temp, axis=0)"],"delete":["22","from ..utils.extmath import logsumexp","687","            norm_phi = logsumexp(temp)"]}],"sklearn\/utils\/tests\/test_stats.py":[{"add":["0","from sklearn.utils.testing import assert_array_equal, ignore_warnings","15","@ignore_warnings  # Test deprecated backport to be removed in 0.21"],"delete":["0","from sklearn.utils.testing import assert_array_equal"]}],"sklearn\/cluster\/spectral.py":[{"add":["91","        vectors[:, i] = (vectors[:, i] \/ np.linalg.norm(vectors[:, i])) \\"],"delete":["14","from ..utils.extmath import norm","92","        vectors[:, i] = (vectors[:, i] \/ norm(vectors[:, i])) \\"]}],"sklearn\/covariance\/robust_covariance.py":[{"add":["16","from ..utils.extmath import fast_logdet","109","        precision = linalg.pinvh(covariance)","129","        precision = linalg.pinvh(covariance)","395","            precision = linalg.pinvh(covariance)","403","            precision = linalg.pinvh(covariance)","631","            precision = linalg.pinvh(raw_covariance)"],"delete":["16","from ..utils.extmath import fast_logdet, pinvh","109","        precision = pinvh(covariance)","129","        precision = pinvh(covariance)","395","            precision = pinvh(covariance)","403","            precision = pinvh(covariance)","631","            precision = pinvh(raw_covariance)"]}],"sklearn\/utils\/graph.py":[{"add":["17","from .deprecation import deprecated","73","@deprecated(\"sklearn.utils.graph.connected_components was deprecated in\"","74","            \"version 0.19 and will be removed in 0.21. Use\"","75","            \"scipy.sparse.csgraph.connected_components instead.\")","76","def connected_components(*args, **kwargs):","77","    return sparse.csgraph.connected_components(*args, **kwargs)"],"delete":["72","if hasattr(sparse, 'connected_components'):","73","    connected_components = sparse.connected_components","74","else:","75","    from .sparsetools import connected_components"]}],"sklearn\/cluster\/_k_means.pyx":[{"add":[],"delete":["17","from ..utils.extmath import norm"]}],"doc\/developers\/utilities.rst":[{"add":[],"delete":["91","- :func:`extmath.norm`: computes Euclidean (L2) vector norm","92","  by directly calling the BLAS","93","  ``nrm2`` function.  This is more stable than ``scipy.linalg.norm``.  See","94","  `Fabian's blog post","95","  <http:\/\/fa.bianp.net\/blog\/2011\/computing-the-vector-norm>`_ for a discussion.","106","- :func:`extmath.logsumexp`: compute the sum of X assuming X is in the log","107","  domain. This is equivalent to calling ``np.log(np.sum(np.exp(X)))``, but is","108","  robust to overflow\/underflow errors.  Note that there is similar","109","  functionality in ``np.logaddexp.reduce``, but because of the pairwise nature","110","  of this routine, it is slower for large arrays.","111","  Scipy has a similar routine in ``scipy.misc.logsumexp`` (In scipy versions","112","  < 0.10, this is found in ``scipy.maxentropy.logsumexp``),","113","  but the scipy version does not accept an ``axis`` keyword.","114","","179","Backports","180","=========","181","","182","- :func:`fixes.expit`: Logistic sigmoid function. Replacement for SciPy 0.10's","183","  ``scipy.special.expit``.","184","","185","- :func:`sparsetools.connected_components`","186","  (backported from ``scipy.sparse.connected_components`` in scipy 0.12).","187","  Used in ``sklearn.cluster.hierarchical``, as well as in tests for","188","  :mod:`sklearn.feature_extraction`.","189","","190","","191","ARPACK","192","------","193","","194","- :func:`arpack.eigs`","195","  (backported from ``scipy.sparse.linalg.eigs`` in scipy 0.10)","196","  Sparse non-symmetric eigenvalue decomposition using the Arnoldi","197","  method.  A limited version of ``eigs`` is available in earlier","198","  scipy versions.","199","","200","- :func:`arpack.eigsh`","201","  (backported from ``scipy.sparse.linalg.eigsh`` in scipy 0.10)","202","  Sparse non-symmetric eigenvalue decomposition using the Arnoldi","203","  method.  A limited version of ``eigsh`` is available in earlier","204","  scipy versions.","205","","206","- :func:`arpack.svds`","207","  (backported from ``scipy.sparse.linalg.svds`` in scipy 0.10)","208","  Sparse non-symmetric eigenvalue decomposition using the Arnoldi","209","  method.  A limited version of ``svds`` is available in earlier","210","  scipy versions.","211","","212",""]}],"sklearn\/mixture\/dpgmm.py":[{"add":["22","from scipy.linalg import pinvh","23","from scipy.misc import logsumexp","28","from ..utils.extmath import squared_norm, stable_cumsum"],"delete":["26","from ..utils.extmath import logsumexp, pinvh, squared_norm, stable_cumsum"]}],"sklearn\/manifold\/spectral_embedding_.py":[{"add":["7","","11","from scipy.sparse.linalg import eigsh, lobpcg","12","from scipy.sparse.csgraph import connected_components","13",""],"delete":["10","from scipy.sparse.linalg import lobpcg","16","from ..utils.sparsetools import connected_components","17","from ..utils.arpack import eigsh"]}],"sklearn\/utils\/extmath.py":[{"add":["20","from scipy.misc import logsumexp as scipy_logsumexp","22","from . import check_random_state, deprecated","31","@deprecated(\"sklearn.utils.extmath.norm was deprecated in version 0.19\"","32","            \"and will be removed in 0.21. Use scipy.linalg.norm instead.\")","39","    return linalg.norm(x)","401","@deprecated(\"sklearn.utils.extmath.logsumexp was deprecated in version 0.19\"","402","            \"and will be removed in 0.21. Use scipy.misc.logsumexp instead.\")","417","    return scipy_logsumexp(arr, axis)","494","@deprecated(\"sklearn.utils.extmath.pinvh was deprecated in version 0.19\"","495","            \"and will be removed in 0.21. Use scipy.linalg.pinvh instead.\")","497","    return linalg.pinvh(a, cond, rcond, lower)","597","    For the ordinary logistic function, use ``scipy.special.expit``."],"delete":["21","from . import check_random_state","36","    x = np.asarray(x)","37","    nrm2, = linalg.get_blas_funcs(['nrm2'], [x])","38","    return nrm2(x)","402","","405","","408","","417","    arr = np.rollaxis(arr, axis)","418","    # Use the max to normalize, as with the log this is what accumulates","419","    # the less errors","420","    vmax = arr.max(axis=0)","421","    out = np.log(np.sum(np.exp(arr - vmax), axis=0))","422","    out += vmax","423","    return out","501","    \"\"\"Compute the (Moore-Penrose) pseudo-inverse of a hermetian matrix.","502","","503","    Calculate a generalized inverse of a symmetric matrix using its","504","    eigenvalue decomposition and including all 'large' eigenvalues.","505","","506","    Parameters","507","    ----------","508","    a : array, shape (N, N)","509","        Real symmetric or complex hermetian matrix to be pseudo-inverted","510","","511","    cond : float or None, default None","512","        Cutoff for 'small' eigenvalues.","513","        Singular values smaller than rcond * largest_eigenvalue are considered","514","        zero.","515","","516","        If None or -1, suitable machine precision is used.","517","","518","    rcond : float or None, default None (deprecated)","519","        Cutoff for 'small' eigenvalues.","520","        Singular values smaller than rcond * largest_eigenvalue are considered","521","        zero.","522","","523","        If None or -1, suitable machine precision is used.","524","","525","    lower : boolean","526","        Whether the pertinent array data is taken from the lower or upper","527","        triangle of a. (Default: lower)","528","","529","    Returns","530","    -------","531","    B : array, shape (N, N)","532","","533","    Raises","534","    ------","535","    LinAlgError","536","        If eigenvalue does not converge","537","","538","    Examples","539","    --------","540","    >>> import numpy as np","541","    >>> a = np.random.randn(9, 6)","542","    >>> a = np.dot(a, a.T)","543","    >>> B = pinvh(a)","544","    >>> np.allclose(a, np.dot(a, np.dot(B, a)))","545","    True","546","    >>> np.allclose(B, np.dot(B, np.dot(a, B)))","547","    True","548","","549","    \"\"\"","550","    a = np.asarray_chkfinite(a)","551","    s, u = linalg.eigh(a, lower=lower)","552","","553","    if rcond is not None:","554","        cond = rcond","555","    if cond in [None, -1]:","556","        t = u.dtype.char.lower()","557","        factor = {'f': 1E3, 'd': 1E6}","558","        cond = factor[t] * np.finfo(t).eps","559","","560","    # unlike svd case, eigh can lead to negative eigenvalues","561","    above_cutoff = (abs(s) > cond * np.max(abs(s)))","562","    psigma_diag = np.zeros_like(s)","563","    psigma_diag[above_cutoff] = 1.0 \/ s[above_cutoff]","564","","565","    return np.dot(u * psigma_diag, np.conjugate(u).T)","665","    For the ordinary logistic function, use ``sklearn.utils.fixes.expit``."]}],"examples\/decomposition\/plot_image_denoising.py":[{"add":["47","try:  # SciPy >= 0.16 have face in misc","48","    from scipy.misc import face","49","    face = face(gray=True)","50","except ImportError:"],"delete":["44","from sklearn.utils.testing import SkipTest","45","from sklearn.utils.fixes import sp_version","47","if sp_version < (0, 12):","48","    raise SkipTest(\"Skipping because SciPy version earlier than 0.12.0 and \"","49","                   \"thus does not include the scipy.misc.face() image.\")","52","try:","53","    from scipy import misc","54","    face = misc.face(gray=True)","55","except AttributeError:","56","    # Old versions of scipy have face in the top level package"]}],"sklearn\/decomposition\/pca.py":[{"add":["18","from scipy.sparse.linalg import svds"],"delete":["29","from ..utils.arpack import svds"]}],"sklearn\/utils\/tests\/test_extmath.py":[{"add":["23","from sklearn.utils.testing import ignore_warnings","89","@ignore_warnings  # Test deprecated backport to be removed in 0.21","145","@ignore_warnings  # extmath.norm is deprecated to be removed in 0.21"],"delete":[]}],"examples\/cluster\/plot_face_compress.py":[{"add":["27","try:  # SciPy >= 0.16 have face in misc","28","    from scipy.misc import face","29","    face = face(gray=True)","30","except ImportError:","35",""],"delete":["25","from sklearn.utils.testing import SkipTest","26","from sklearn.utils.fixes import sp_version","28","if sp_version < (0, 12):","29","    raise SkipTest(\"Skipping because SciPy version earlier than 0.12.0 and \"","30","                   \"thus does not include the scipy.misc.face() image.\")","32","try:","34","except AttributeError:","35","    # Newer versions of scipy have face in misc","36","    from scipy import misc","37","    face = misc.face(gray=True)","41","    "]}],"sklearn\/decomposition\/truncated_svd.py":[{"add":["10","from scipy.sparse.linalg import svds"],"delete":["10","","11","try:","12","    from scipy.sparse.linalg import svds","13","except ImportError:","14","    from ..utils.arpack import svds"]}],"sklearn\/cluster\/bicluster.py":[{"add":["10","from scipy.linalg import norm","11","from scipy.sparse import dia_matrix, issparse","12","from scipy.sparse.linalg import eigsh, svds","19","from ..utils.extmath import (make_nonnegative, randomized_svd,","204","        :func:`scipy.sparse.linalg.svds`, which is more accurate, but","336","        `scipy.sparse.linalg.svds`, which is more accurate, but"],"delete":["10","from scipy.sparse import dia_matrix","11","from scipy.sparse import issparse","17","from ..utils.arpack import eigsh, svds","19","from ..utils.extmath import (make_nonnegative, norm, randomized_svd,","204","        :func:`sklearn.utils.arpack.svds`, which is more accurate, but","336","        `sklearn.utils.arpack.svds`, which is more accurate, but"]}],"\/dev\/null":[{"add":[],"delete":[]}],"sklearn\/ensemble\/gradient_boosting.py":[{"add":["42","from scipy.misc import logsumexp","46","from scipy.special import expit"],"delete":["56","from ..utils.extmath import logsumexp","57","from ..utils.fixes import expit"]}],"sklearn\/utils\/sparsetools\/setup.py":[{"add":["0","# Remove in version 0.21","11",""],"delete":["0","import numpy","7","","8","    config.add_extension('_traversal',","9","                         sources=['_traversal.pyx'],","10","                         include_dirs=[numpy.get_include()])","11","    config.add_extension('_graph_tools',","12","                         sources=['_graph_tools.pyx'],","13","                         include_dirs=[numpy.get_include()])","14",""]}],"sklearn\/linear_model\/tests\/test_sag.py":[{"add":["8","from scipy.misc import logsumexp"],"delete":["15","from sklearn.utils.extmath import logsumexp"]}],"sklearn\/manifold\/locally_linear.py":[{"add":["9","from scipy.sparse.linalg import eigsh","10",""],"delete":["11","from ..utils.arpack import eigsh"]}],"sklearn\/cross_decomposition\/pls_.py":[{"add":["9","","11","from scipy.linalg import pinv2, svd","12","from scipy.sparse.linalg import svds","13","","14","from ..base import BaseEstimator, RegressorMixin, TransformerMixin","15","from ..utils import check_array, check_consistent_length","16","from ..utils.extmath import svd_flip","18","from ..externals import six","44","                X_pinv = pinv2(X, check_finite=False)","61","                Y_pinv = pinv2(Y, check_finite=False)  # compute once pinv(Y)","85","    U, s, Vh = svd(C, full_matrices=False)","349","            pinv2(np.dot(self.x_loadings_.T, self.x_weights_),","350","                  check_finite=False))","354","                pinv2(np.dot(self.y_loadings_.T, self.y_weights_),","355","                      check_finite=False))","810","            U, s, V = svd(C, full_matrices=False)","812","            U, s, V = svds(C, k=self.n_components)"],"delete":["6","from distutils.version import LooseVersion","7","from sklearn.utils.extmath import svd_flip","8","","9","from ..base import BaseEstimator, RegressorMixin, TransformerMixin","10","from ..utils import check_array, check_consistent_length","11","from ..externals import six","16","from scipy import linalg","17","from ..utils import arpack","22","import scipy","23","pinv2_args = {}","24","if LooseVersion(scipy.__version__) >= LooseVersion('0.12'):","25","    # check_finite=False is an optimization available only in scipy >=0.12","26","    pinv2_args = {'check_finite': False}","27","","50","                X_pinv = linalg.pinv2(X, **pinv2_args)","67","                Y_pinv = linalg.pinv2(Y, **pinv2_args)  # compute once pinv(Y)","91","    U, s, Vh = linalg.svd(C, full_matrices=False)","355","            linalg.pinv2(np.dot(self.x_loadings_.T, self.x_weights_),","356","                         **pinv2_args))","360","                linalg.pinv2(np.dot(self.y_loadings_.T, self.y_weights_),","361","                             **pinv2_args))","816","            U, s, V = linalg.svd(C, full_matrices=False)","818","            U, s, V = arpack.svds(C, k=self.n_components)"]}],"sklearn\/decomposition\/kernel_pca.py":[{"add":["7","from scipy.sparse.linalg import eigsh"],"delete":["9","from ..utils.arpack import eigsh"]}],"sklearn\/linear_model\/least_angle.py":[{"add":["29","solve_triangular_args = {'check_finite': False}"],"delete":["15","from distutils.version import LooseVersion","30","import scipy","31","solve_triangular_args = {}","32","if LooseVersion(scipy.__version__) >= LooseVersion('0.12'):","33","    solve_triangular_args = {'check_finite': False}"]}]}},"0b02125b1393d8017649ab3920f1cbaec5dbf12f":{"changes":{"sklearn\/cluster\/hierarchical.py":"MODIFY","sklearn\/cluster\/tests\/test_hierarchical.py":"MODIFY"},"diff":{"sklearn\/cluster\/hierarchical.py":[{"add":["662","                 memory=None,","687","        if memory is None:","688","            memory = Memory(cachedir=None, verbose=0)","689","        elif isinstance(memory, six.string_types):","691","        elif not isinstance(memory, Memory):","692","            raise ValueError('`memory` has to be a `str` or a `joblib.Memory`'","693","                             ' instance')"],"delete":["662","                 memory=Memory(cachedir=None, verbose=0),","687","        if isinstance(memory, six.string_types):"]}],"sklearn\/cluster\/tests\/test_hierarchical.py":[{"add":["117","def test_agglomerative_clustering_wrong_arg_memory():","118","    # Test either if an error is raised when memory is not","119","    # either a str or a joblib.Memory instance","120","    rng = np.random.RandomState(0)","121","    n_samples = 100","122","    X = rng.randn(n_samples, 50)","123","    memory = 5","124","    clustering = AgglomerativeClustering(memory=memory)","125","    assert_raises(ValueError, clustering.fit, X)","126","","127",""],"delete":[]}]}},"2f1c9786cc852e90799350d6fa4c6c335565c528":{"changes":{"sklearn\/tree\/tests\/test_export.py":"MODIFY","sklearn\/tree\/export.py":"MODIFY","doc\/whats_new.rst":"MODIFY"},"diff":{"sklearn\/tree\/tests\/test_export.py":[{"add":["11","from sklearn.utils.testing import assert_raise_message","221","    # Check if it errors when length of feature_names","222","    # mismatches with number of features","223","    message = (\"Length of feature_names, \"","224","               \"1 does not match number of features, 2\")","225","    assert_raise_message(ValueError, message, export_graphviz, clf, None,","226","                         feature_names=[\"a\"])","227","","228","    message = (\"Length of feature_names, \"","229","               \"3 does not match number of features, 2\")","230","    assert_raise_message(ValueError, message, export_graphviz, clf, None,","231","                         feature_names=[\"a\", \"b\", \"c\"])"],"delete":["220","    # Check feature_names error","221","    out = StringIO()","222","    assert_raises(IndexError, export_graphviz, clf, out, feature_names=[])"]}],"sklearn\/tree\/export.py":[{"add":["10","#          Li Li <aiki.nogard@gmail.com>","175","                alpha = int(np.round(255 * (sorted_values[0] -","176","                                            sorted_values[1]) \/","334","                    elif (tree.n_classes[0] == 1 and","335","                          len(np.unique(tree.value)) != 1):","404","        # Check length of feature_names before getting into the tree node","405","        # Raise error if length of feature_names does not match","406","        # n_features_ in the decision_tree","407","        if feature_names is not None:","408","            if len(feature_names) != decision_tree.n_features_:","409","                raise ValueError(\"Length of feature_names, %d \"","410","                                 \"does not match number of features, %d\"","411","                                 % (len(feature_names),","412","                                    decision_tree.n_features_))","413",""],"delete":["174","                alpha = int(np.round(255 * (sorted_values[0] - sorted_values[1]) \/","332","                    elif tree.n_classes[0] == 1 and len(np.unique(tree.value)) != 1:"]}],"doc\/whats_new.rst":[{"add":["278","   - Fixed a bug where :func:`sklearn.tree.export_graphviz` raised an error","279","     when the length of features_names does not match n_features in the decision","280","     tree.","281","     :issue:`8512` by :user:`Li Li <aikinogard>`.","282",""],"delete":[]}]}},"ec8d9f38aa1a3256cf4355ddbce8259a8bc3fb83":{"changes":{"examples\/model_selection\/plot_roc_crossval.py":"MODIFY"},"diff":{"examples\/model_selection\/plot_roc_crossval.py":[{"add":["64","tprs = []","65","aucs = []","69","for train, test in cv.split(X, y):","73","    tprs.append(interp(mean_fpr, fpr, tpr))","74","    tprs[-1][0] = 0.0","76","    aucs.append(roc_auc)","77","    plt.plot(fpr, tpr, lw=1, alpha=0.3,","78","             label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))","81","plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',","82","         label='Luck', alpha=.8)","84","mean_tpr = np.mean(tprs, axis=0)","87","std_auc = np.std(aucs)","88","plt.plot(mean_fpr, mean_tpr, color='b',","89","         label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),","90","         lw=2, alpha=.8)","91","","92","std_tpr = np.std(tprs, axis=0)","93","tprs_upper = np.minimum(mean_tpr + std_tpr, 1)","94","tprs_lower = np.maximum(mean_tpr - std_tpr, 0)","95","plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,","96","                 label=r'$\\pm$ 1 std. dev.')"],"delete":["64","mean_tpr = 0.0","67","colors = cycle(['cyan', 'indigo', 'seagreen', 'yellow', 'blue', 'darkorange'])","68","lw = 2","69","","71","for (train, test), color in zip(cv.split(X, y), colors):","75","    mean_tpr += interp(mean_fpr, fpr, tpr)","76","    mean_tpr[0] = 0.0","78","    plt.plot(fpr, tpr, lw=lw, color=color,","79","             label='ROC fold %d (area = %0.2f)' % (i, roc_auc))","82","plt.plot([0, 1], [0, 1], linestyle='--', lw=lw, color='k',","83","         label='Luck')","85","mean_tpr \/= cv.get_n_splits(X, y)","88","plt.plot(mean_fpr, mean_tpr, color='g', linestyle='--',","89","         label='Mean ROC (area = %0.2f)' % mean_auc, lw=lw)"]}]}},"b57837188dd55d37e13ddcc8f9f548e9dbd3f0c1":{"changes":{"sklearn\/model_selection\/_split.py":"MODIFY","sklearn\/model_selection\/tests\/test_split.py":"MODIFY"},"diff":{"sklearn\/model_selection\/_split.py":[{"add":["999","    def get_n_splits(self, X=None, y=None, groups=None):","1000","        \"\"\"Returns the number of splitting iterations in the cross-validator","1001","","1002","        Parameters","1003","        ----------","1004","        X : object","1005","            Always ignored, exists for compatibility.","1006","            ``np.zeros(n_samples)`` may be used as a placeholder.","1007","","1008","        y : object","1009","            Always ignored, exists for compatibility.","1010","            ``np.zeros(n_samples)`` may be used as a placeholder.","1011","","1012","        groups : array-like, with shape (n_samples,), optional","1013","            Group labels for the samples used while splitting the dataset into","1014","            train\/test set.","1015","","1016","        Returns","1017","        -------","1018","        n_splits : int","1019","            Returns the number of splitting iterations in the cross-validator.","1020","        \"\"\"","1021","        rng = check_random_state(self.random_state)","1022","        cv = self.cv(random_state=rng, shuffle=True,","1023","                     **self.cvargs)","1024","        return cv.get_n_splits(X, y, groups) * self.n_repeats","1025",""],"delete":[]}],"sklearn\/model_selection\/tests\/test_split.py":[{"add":["846","def test_get_n_splits_for_repeated_kfold():","847","    n_splits = 3","848","    n_repeats = 4","849","    rkf = RepeatedKFold(n_splits, n_repeats)","850","    expected_n_splits = n_splits * n_repeats","851","    assert_equal(expected_n_splits, rkf.get_n_splits())","852","","853","","854","def test_get_n_splits_for_repeated_stratified_kfold():","855","    n_splits = 3","856","    n_repeats = 4","857","    rskf = RepeatedStratifiedKFold(n_splits, n_repeats)","858","    expected_n_splits = n_splits * n_repeats","859","    assert_equal(expected_n_splits, rskf.get_n_splits())","860","","861",""],"delete":[]}]}},"0eb33adb789052029eddc72135027e490d510375":{"changes":{"build_tools\/travis\/flake8_diff.sh":"MODIFY"},"diff":{"build_tools\/travis\/flake8_diff.sh":[{"add":["127","    shift","128","    options=\"$*\"","129","    if [ -n \"$files\" ]; then","130","        # Conservative approach: diff without context (--unified=0) so that code","131","        # that was not changed does not create failures","132","        git diff --unified=0 $COMMIT_RANGE -- $files | flake8 --diff --show-source $options","133","    fi"],"delete":["127","    options=\"$2\"","128","    # Conservative approach: diff without context (--unified=0) so that code","129","    # that was not changed does not create failures","130","    git diff --unified=0 $COMMIT_RANGE -- $files | flake8 --diff --show-source $options"]}]}},"fb5a498d0bd00fc2b42fbd19b6ef18e1dfeee47e":{"changes":{"sklearn\/datasets\/base.py":"MODIFY","sklearn\/datasets\/lfw.py":"MODIFY","sklearn\/datasets\/species_distributions.py":"MODIFY","sklearn\/datasets\/california_housing.py":"MODIFY","sklearn\/datasets\/olivetti_faces.py":"MODIFY","sklearn\/datasets\/twenty_newsgroups.py":"MODIFY","sklearn\/datasets\/kddcup99.py":"MODIFY","doc\/modules\/pipeline.rst":"MODIFY","sklearn\/utils\/__init__.py":"MODIFY","sklearn\/datasets\/rcv1.py":"MODIFY","sklearn\/tests\/test_pipeline.py":"MODIFY","sklearn\/datasets\/covtype.py":"MODIFY","sklearn\/datasets\/mldata.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/pipeline.py":"MODIFY"},"diff":{"sklearn\/datasets\/base.py":[{"add":["22","from ..utils import Bunch"],"delete":["28","class Bunch(dict):","29","    \"\"\"Container object for datasets","30","","31","    Dictionary-like object that exposes its keys as attributes.","32","","33","    >>> b = Bunch(a=1, b=2)","34","    >>> b['b']","35","    2","36","    >>> b.b","37","    2","38","    >>> b.a = 3","39","    >>> b['a']","40","    3","41","    >>> b.c = 6","42","    >>> b['c']","43","    6","44","","45","    \"\"\"","46","","47","    def __init__(self, **kwargs):","48","        super(Bunch, self).__init__(kwargs)","49","","50","    def __setattr__(self, key, value):","51","        self[key] = value","52","","53","    def __dir__(self):","54","        return self.keys()","55","","56","    def __getattr__(self, key):","57","        try:","58","            return self[key]","59","        except KeyError:","60","            raise AttributeError(key)","61","","62","    def __setstate__(self, state):","63","        # Bunch pickles generated with scikit-learn 0.16.* have an non","64","        # empty __dict__. This causes a surprising behaviour when","65","        # loading these pickles scikit-learn 0.17: reading bunch.key","66","        # uses __dict__ but assigning to bunch.key use __setattr__ and","67","        # only changes bunch['key']. More details can be found at:","68","        # https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/6196.","69","        # Overriding __setstate__ to be a noop has the effect of","70","        # ignoring the pickled __dict__","71","        pass","72","","73",""]}],"sklearn\/datasets\/lfw.py":[{"add":["36","from .base import get_data_home","37","from ..utils import Bunch"],"delete":["36","from .base import get_data_home, Bunch"]}],"sklearn\/datasets\/species_distributions.py":[{"add":["52","from sklearn.datasets.base import get_data_home","53","from ..utils import Bunch"],"delete":["52","from sklearn.datasets.base import get_data_home, Bunch"]}],"sklearn\/datasets\/california_housing.py":[{"add":["37","from .base import get_data_home","38","from ..utils import Bunch"],"delete":["37","from .base import get_data_home, Bunch"]}],"sklearn\/datasets\/olivetti_faces.py":[{"add":["39","from .base import get_data_home","41","from ..utils import check_random_state, Bunch","82","        Each row corresponds to a ravelled face image of original size","83","        64 x 64 pixels.","86","        Each row is a face image corresponding to one of the 40 subjects","87","        of the dataset."],"delete":["39","from .base import get_data_home, Bunch","41","from ..utils import check_random_state","82","        Each row corresponds to a ravelled face image of original size 64 x 64 pixels.","85","        Each row is a face image corresponding to one of the 40 subjects of the dataset."]}],"sklearn\/datasets\/twenty_newsgroups.py":[{"add":["51","from ..utils import check_random_state, Bunch"],"delete":["49","from .base import Bunch","52","from ..utils import check_random_state"]}],"sklearn\/datasets\/kddcup99.py":[{"add":["25","from ..utils import Bunch"],"delete":["25","from .base import Bunch"]}],"doc\/modules\/pipeline.rst":[{"add":["81","Attributes of named_steps map to keys, enabling tab completion in interactive environments::","82","","83","    >>> pipe.named_steps.reduce_dim is pipe.named_steps['reduce_dim']","84","    True","85",""],"delete":[]}],"sklearn\/utils\/__init__.py":[{"add":["30","class Bunch(dict):","31","    \"\"\"Container object for datasets","32","","33","    Dictionary-like object that exposes its keys as attributes.","34","","35","    >>> b = Bunch(a=1, b=2)","36","    >>> b['b']","37","    2","38","    >>> b.b","39","    2","40","    >>> b.a = 3","41","    >>> b['a']","42","    3","43","    >>> b.c = 6","44","    >>> b['c']","45","    6","46","","47","    \"\"\"","48","","49","    def __init__(self, **kwargs):","50","        super(Bunch, self).__init__(kwargs)","51","","52","    def __setattr__(self, key, value):","53","        self[key] = value","54","","55","    def __dir__(self):","56","        return self.keys()","57","","58","    def __getattr__(self, key):","59","        try:","60","            return self[key]","61","        except KeyError:","62","            raise AttributeError(key)","63","","64","    def __setstate__(self, state):","65","        # Bunch pickles generated with scikit-learn 0.16.* have an non","66","        # empty __dict__. This causes a surprising behaviour when","67","        # loading these pickles scikit-learn 0.17: reading bunch.key","68","        # uses __dict__ but assigning to bunch.key use __setattr__ and","69","        # only changes bunch['key']. More details can be found at:","70","        # https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/6196.","71","        # Overriding __setstate__ to be a noop has the effect of","72","        # ignoring the pickled __dict__","73","        pass","74","","75",""],"delete":[]}],"sklearn\/datasets\/rcv1.py":[{"add":["27","from ..utils import Bunch"],"delete":["22","from .base import Bunch"]}],"sklearn\/tests\/test_pipeline.py":[{"add":["511","def test_pipeline_named_steps():","512","    transf = Transf()","513","    mult2 = Mult(mult=2)","514","    pipeline = Pipeline([('mock', transf), (\"mult\", mult2)])","515","","516","    # Test access via named_steps bunch object","517","    assert_true('mock' in pipeline.named_steps)","518","    assert_true('mock2' not in pipeline.named_steps)","519","    assert_true(pipeline.named_steps.mock is transf)","520","    assert_true(pipeline.named_steps.mult is mult2)","521","","522","    # Test bunch with conflict attribute of dict","523","    pipeline = Pipeline([('values', transf), (\"mult\", mult2)])","524","    assert_true(pipeline.named_steps.values is not transf)","525","    assert_true(pipeline.named_steps.mult is mult2)","526","","527",""],"delete":[]}],"sklearn\/datasets\/covtype.py":[{"add":["28","from ..utils import Bunch"],"delete":["28","from .base import Bunch"]}],"sklearn\/datasets\/mldata.py":[{"add":["25","from .base import get_data_home","26","from ..utils import Bunch"],"delete":["25","from .base import get_data_home, Bunch"]}],"doc\/whats_new.rst":[{"add":["279","   - Replace attribute ``named_steps`` ``dict`` to :class:`sklearn.utils.Bunch`","280","     in :class:`sklearn.pipeline.Pipeline` to enable tab completion in interactive","281","     environment. In the case conflict value on ``named_steps`` and ``dict``","282","     attribute, ``dict`` behavior will be prioritized.","283","     :issue:`8481` by :user:`Herilalaina Rakotoarison <herilalaina>`.","284",""],"delete":[]}],"sklearn\/pipeline.py":[{"add":["22","from .utils import Bunch","125","    named_steps : bunch object, a dictionary with attribute access","160","    >>> # Another way to get selected features chosen by anova_filter","161","    >>> anova_svm.named_steps.anova.get_support()","162","    ... # doctest: +NORMALIZE_WHITESPACE","163","    array([False, False,  True,  True, False, False, True,  True, False,","164","           True,  False,  True,  True, False, True,  False, True, True,","165","           False, False], dtype=bool)","235","        # Use Bunch object to improve autocomplete","236","        return Bunch(**dict(self.steps))"],"delete":["124","    named_steps : dict","159","","229","        return dict(self.steps)"]}]}},"f34816ab8935e3849ff8c1fe78337b5f687731f0":{"changes":{"doc\/tutorial\/statistical_inference\/supervised_learning.rst":"MODIFY"},"diff":{"doc\/tutorial\/statistical_inference\/supervised_learning.rst":[{"add":["111","In one dimension, this requires on average :math:`n \\sim 1\/d` points.","118","If the number of features is :math:`p`, you now require :math:`n \\sim 1\/d^p`","125","effective :math:`k`-NN estimator in a paltry :math:`p \\sim 20` dimensions would"],"delete":["111","In one dimension, this requires on average :math:`n ~ 1\/d` points.","118","If the number of features is :math:`p`, you now require :math:`n ~ 1\/d^p`","125","effective :math:`k`-NN estimator in a paltry :math:`p~20` dimensions would"]}]}},"14977242bf22492d61598739e6ee54eb8de2b415":{"changes":{"doc\/developers\/contributing.rst":"MODIFY"},"diff":{"doc\/developers\/contributing.rst":[{"add":["157","If some conflicts arise between your branch and the ``master`` branch, you need","158","to merge ``master``. The command will be::","159","","160","  $ git merge master","161","","162","with ``master`` being synchronized with the ``upstream``.","163","","164","Subsequently, you need to solve the conflicts. You can refer to the `Git","165","documentation related to resolving merge conflict using the command line","166","<https:\/\/help.github.com\/articles\/resolving-a-merge-conflict-using-the-command-line\/>`_.","167","","168",".. note::","169","","170","   In the past, the policy to resolve conflicts was to rebase your branch on","171","   ``master``. GitHub interface deals with merging ``master`` better than in","172","   the past."],"delete":["157","In particular, if some conflicts arise between your branch and the master","158","branch, you will need to `rebase your branch on master","159","<http:\/\/docs.scipy.org\/doc\/numpy\/dev\/gitwash\/development_workflow.html#rebasing-on-master>`_.","160","Please avoid merging master branch into yours. If you did it anyway, you can fix","161","it following `this example","162","<https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/7111#issuecomment-249175383>`_."]}]}},"fa82eee82f2df40dae61df775d2f2c4652eec385":{"changes":{"sklearn\/gaussian_process\/gpr.py":"MODIFY","sklearn\/gaussian_process\/tests\/test_gpr.py":"MODIFY"},"diff":{"sklearn\/gaussian_process\/gpr.py":[{"add":["49","        Larger values correspond to increased noise level in the observations.","50","        This can also prevent a potential numerical issue during fitting, by","51","        ensuring that the calculated values form a positive definite matrix.","52","        If an array is passed, it must have the same number of entries as the","53","        data used for fitting and is used as datapoint-dependent noise level.","54","        Note that this is equivalent to adding a WhiteKernel with c=alpha.","55","        Allowing to specify the noise level directly as a parameter is mainly","56","        for convenience and for consistency with Ridge.","245","        try:","246","            self.L_ = cholesky(K, lower=True)  # Line 2","247","        except np.linalg.LinAlgError as exc:","248","            exc.args = (\"The kernel, %s, is not returning a \"","249","                        \"positive definite matrix. Try gradually \"","250","                        \"increasing the 'alpha' parameter of your \"","251","                        \"GaussianProcessRegressor estimator.\"","252","                        % self.kernel_,) + exc.args","253","            raise"],"delete":["49","        Larger values correspond to increased noise level in the observations","50","        and reduce potential numerical issue during fitting. If an array is","51","        passed, it must have the same number of entries as the data used for","52","        fitting and is used as datapoint-dependent noise level. Note that this","53","        is equivalent to adding a WhiteKernel with c=alpha. Allowing to specify","54","        the noise level directly as a parameter is mainly for convenience and","55","        for consistency with Ridge.","244","        self.L_ = cholesky(K, lower=True)  # Line 2","246",""]}],"sklearn\/gaussian_process\/tests\/test_gpr.py":[{"add":["12","from sklearn.gaussian_process.kernels import DotProduct","16","            assert_almost_equal, assert_equal, assert_raise_message)","293","def test_gpr_correct_error_message():","294","    X = np.arange(12).reshape(6, -1)","295","    y = np.ones(6)","296","    kernel = DotProduct()","297","    gpr = GaussianProcessRegressor(kernel=kernel, alpha=0.0)","298","    assert_raise_message(np.linalg.LinAlgError,","299","                         \"The kernel, %s, is not returning a \"","300","                         \"positive definite matrix. Try gradually increasing \"","301","                         \"the 'alpha' parameter of your \"","302","                         \"GaussianProcessRegressor estimator.\"","303","                         % kernel, gpr.fit, X, y)","304","","305",""],"delete":["15","            assert_almost_equal, assert_equal)"]}]}},"54e89511b82194f40df486573d4eb1d9c9031762":{"changes":{"sklearn\/tests\/test_base.py":"MODIFY"},"diff":{"sklearn\/tests\/test_base.py":[{"add":["16","from sklearn.utils.testing import ignore_warnings","443","@ignore_warnings(category=(UserWarning))"],"delete":[]}]}},"08772c42e7be84ba89c0c7a9d8280fd79ab53397":{"changes":{"sklearn\/linear_model\/tests\/test_least_angle.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/linear_model\/least_angle.py":"MODIFY"},"diff":{"sklearn\/linear_model\/tests\/test_least_angle.py":[{"add":["368","    estimators = [","369","        linear_model.LassoLars(),","370","        linear_model.Lars(),","371","        # regression test for gh-1615","372","        linear_model.LassoLars(fit_intercept=False),","373","        linear_model.Lars(fit_intercept=False),","374","    ]","376","    for estimator in estimators:"],"delete":["369","    for estimator in (linear_model.LassoLars(), linear_model.Lars()):"]}],"doc\/whats_new.rst":[{"add":["196","   - Fix a bug where :func:`sklearn.linear_model.LassoLars.fit` sometimes","197","     left `coef_` as a list, rather than an ndarray.","198","     :issue:`8160` by :user:`CJ Carey <perimosocordiae>`.","199",""],"delete":[]}],"sklearn\/linear_model\/least_angle.py":[{"add":["667","        self.coef_ = np.empty((n_targets, n_features))","684","                self.coef_[k] = coef_path[:, -1]"],"delete":["669","            self.coef_ = []","684","                self.coef_.append(coef_path[:, -1])","692","            self.coef_ = np.empty((n_targets, n_features))"]}]}},"d7e77ce0a32fc09840f74d8f3f07ff1c57ebd5f8":{"changes":{"doc\/themes\/scikit-learn\/layout.html":"MODIFY"},"diff":{"doc\/themes\/scikit-learn\/layout.html":[{"add":["277","    <p class=\"citing\">Please <b><a href=\"{{ pathto('about').replace('#', '') }}#citing-scikit-learn\" style=\"font-size: 110%;\">cite us <\/a><\/b>if you use the software.<\/p>"],"delete":["277","    <p class=\"citing\">Please <b><a href=\"about.html#citing-scikit-learn\" style=\"font-size: 110%;\">cite us <\/a><\/b>if you use the software.<\/p>"]}]}},"2d0bce7bc83b4da51ef1d309e9d587012920edcb":{"changes":{"doc\/tutorial\/statistical_inference\/model_selection.rst":"MODIFY"},"diff":{"doc\/tutorial\/statistical_inference\/model_selection.rst":[{"add":["109","    - :class:`StratifiedKFold` **(n_splits, shuffle, random_state)**","111","    - :class:`GroupKFold` **(n_splits)**","127","    - :class:`ShuffleSplit` **(n_splits, test_size, train_size, random_state)**","148","    - :class:`LeavePGroupsOut`  **(n_groups)**"],"delete":["109","    - :class:`StratifiedKFold` **(n_iter, test_size, train_size, random_state)**","111","    - :class:`GroupKFold` **(n_splits, shuffle, random_state)**","127","    - :class:`ShuffleSplit` **(n_iter, test_size, train_size, random_state)**","148","    - :class:`LeavePGroupsOut`  **(p)**"]}]}},"24653e8721794d47173554ec723a358a26f7300c":{"changes":{"sklearn\/metrics\/classification.py":"MODIFY","sklearn\/metrics\/tests\/test_classification.py":"MODIFY","doc\/whats_new.rst":"MODIFY"},"diff":{"sklearn\/metrics\/classification.py":[{"add":["93","        if y_type == \"binary\":","94","            unique_values = np.union1d(y_true, y_pred)","95","            if len(unique_values) > 2:","96","                y_type = \"multiclass\""],"delete":[]}],"sklearn\/metrics\/tests\/test_classification.py":[{"add":["368","    y_true_inv2 = label_binarize(y_true, [\"a\", \"b\"])","369","    y_true_inv2 = np.where(y_true_inv2, 'a', 'b')","382","                               matthews_corrcoef, y_true, ['a'] * len(y_true))","1269","def test__check_targets_multiclass_with_both_y_true_and_y_pred_binary():","1270","    # https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/8098","1271","    y_true = [0, 1]","1272","    y_pred = [0, -1]","1273","    assert_equal(_check_targets(y_true, y_pred)[0], 'multiclass')","1274","","1275",""],"delete":["368","    y_true_inv2 = label_binarize(y_true, [\"a\", \"b\"]) * -1","381","                               matthews_corrcoef, y_true,","382","                               rng.randint(-100, 100) * np.ones(20, dtype=int))"]}],"doc\/whats_new.rst":[{"add":["262","   - Fix a bug in :func:`sklearn.metrics.classification._check_targets`","263","     which would return ``'binary'`` if ``y_true`` and ``y_pred`` were","264","     both ``'binary'`` but the union of ``y_true`` and ``y_pred`` was","265","     ``'multiclass'``. :issue:`8377` by `Loic Esteve`_."],"delete":[]}]}},"4ab99c75b4486f5e1a67be730b7afa6d9551ed7e":{"changes":{"sklearn\/utils\/fixes.py":"MODIFY","sklearn\/ensemble\/iforest.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/ensemble\/tests\/test_iforest.py":"MODIFY"},"diff":{"sklearn\/utils\/fixes.py":[{"add":["38","euler_gamma = getattr(np, 'euler_gamma',","39","                      0.577215664901532860606512090082402431)"],"delete":[]}],"sklearn\/ensemble\/iforest.py":[{"add":["9","from sklearn.utils.fixes import euler_gamma","303","            return 2. * (np.log(n_samples_leaf - 1.) + euler_gamma) - 2. * (","317","            np.log(n_samples_leaf[not_mask] - 1.) + euler_gamma) - 2. * ("],"delete":["302","            return 2. * (np.log(n_samples_leaf) + 0.5772156649) - 2. * (","316","            np.log(n_samples_leaf[not_mask]) + 0.5772156649) - 2. * ("]}],"doc\/whats_new.rst":[{"add":["20","   * :class:`sklearn.ensemble.IsolationForest` (bug fix)","158","   - Fixed a bug where :class:`sklearn.ensemble.IsolationForest` uses an","159","     an incorrect formula for the average path length","160","     :issue:`8549` by `Peter Wang <https:\/\/github.com\/PTRWang>`_.","161","","162","   - Fixed a bug where :class:`sklearn.cluster.DBSCAN` gives incorrect","173","     :issue:`8344` by :user:`Akshay Gupta <Akshay0724>`","280","","281","   - Estimators with both methods ``decision_function`` and ``predict_proba``","282","     are now required to have a monotonic relation between them. The","283","     method ``check_decision_proba_consistency`` has been added in","284","     **sklearn.utils.estimator_checks** to check their consistency.","286",""],"delete":["20","* *to be listed*","158","   - Fixed a bug where :class:`sklearn.cluster.DBSCAN` gives incorrect ","169","     :issue:`8344` by :user:`Akshay Gupta <Akshay0724>` ","276","      ","277","   - Estimators with both methods ``decision_function`` and ``predict_proba`` ","278","     are now required to have a monotonic relation between them. The ","279","     method ``check_decision_proba_consistency`` has been added in ","280","     **sklearn.utils.estimator_checks** to check their consistency. ","282","      "]}],"sklearn\/ensemble\/tests\/test_iforest.py":[{"add":["10","from sklearn.utils.fixes import euler_gamma","11","from sklearn.utils.testing import assert_almost_equal","23","from sklearn.ensemble.iforest import _average_path_length","216","","217","","218","def test_iforest_average_path_length():","219","    # It tests non-regression for #8549 which used the wrong formula","220","    # for average path length, strictly for the integer case","221","","222","    result_one = 2. * (np.log(4.) + euler_gamma) - 2. * 4. \/ 5.","223","    result_two = 2. * (np.log(998.) + euler_gamma) - 2. * 998. \/ 999.","224","    assert_almost_equal(_average_path_length(1), 1., decimal=10)","225","    assert_almost_equal(_average_path_length(5), result_one, decimal=10)","226","    assert_almost_equal(_average_path_length(999), result_two, decimal=10)","227","    assert_array_almost_equal(_average_path_length(np.array([1, 5, 999])),","228","                              [1., result_one, result_two], decimal=10)"],"delete":[]}]}},"8694278c027d1017670e67cd3298fc5fd627d4c9":{"changes":{"sklearn\/decomposition\/factor_analysis.py":"MODIFY","doc\/whats_new.rst":"MODIFY",".mailmap":"MODIFY","sklearn\/decomposition\/pca.py":"MODIFY","sklearn\/utils\/tests\/test_extmath.py":"MODIFY","sklearn\/decomposition\/base.py":"MODIFY"},"diff":{"sklearn\/decomposition\/factor_analysis.py":[{"add":["17","#         Denis A. Engemann <denis-alexander.engemann@inria.fr>"],"delete":["17","#         Denis A. Engemann <d.engemann@fz-juelich.de>"]}],"doc\/whats_new.rst":[{"add":["2306","     library by `Denis Engemann`_, and `Alexandre Gramfort`_.","2319","     significantly speedup computation by `Denis Engemann`_, and","2842","   - Reduce memory footprint of FastICA by `Denis Engemann`_ and","4988","","4989",".. _Denis Engemann: http:\/\/denis-engemann.de"],"delete":["2306","     library by :user:`Denis Engemann <dengemann>`, and `Alexandre Gramfort`_.","2319","     significantly speedup computation by :user:`Denis Engemann <dengemann>`, and","2842","   - Reduce memory footprint of FastICA by :user:`Denis Engemann <dengemann>` and"]}],".mailmap":[{"add":["25","Denis Engemann <denis-alexander.engemann@inria.fr>","26","Denis Engemann <denis-alexander.engemann@inria.fr> <denis.engemann@gmail.com>","27","Denis Engemann <denis-alexander.engemann@inria.fr> <dengemann@Deniss-MacBook-Pro.local>","28","Denis Engemann <denis-alexander.engemann@inria.fr> <dengemann <denis.engemann@gmail.com>","63","Jan Schlter <scikit-learn@jan-schlueter.de>"],"delete":["25","Denis Engemann <d.engemann@fz-juelich.de>","26","Denis Engemann <d.engemann@fz-juelich.de> <denis.engemann@gmail.com>","27","Denis Engemann <d.engemann@fz-juelich.de> <dengemann@Deniss-MacBook-Pro.local>","28","Denis Engemann <d.engemann@fz-juelich.de> <dengemann <denis.engemann@gmail.com>","63","Jan Schlter <scikit-learn@jan-schlueter.de>"]}],"sklearn\/decomposition\/pca.py":[{"add":["6","#         Denis A. Engemann <denis-alexander.engemann@inria.fr>"],"delete":["6","#         Denis A. Engemann <d.engemann@fz-juelich.de>"]}],"sklearn\/utils\/tests\/test_extmath.py":[{"add":["2","#          Denis Engemann <denis-alexander.engemann@inria.fr>"],"delete":["2","#          Denis Engemann <d.engemann@fz-juelich.de>"]}],"sklearn\/decomposition\/base.py":[{"add":["5","#         Denis A. Engemann <denis-alexander.engemann@inria.fr>"],"delete":["5","#         Denis A. Engemann <d.engemann@fz-juelich.de>"]}]}},"6d604d1d9342397b90e5e59e1ec1932da3374e7a":{"changes":{"examples\/model_selection\/randomized_search.py":"MODIFY"},"diff":{"examples\/model_selection\/randomized_search.py":[{"add":["56","              \"min_samples_split\": sp_randint(2, 11),","75","              \"min_samples_split\": [2, 3, 10],"],"delete":["56","              \"min_samples_split\": sp_randint(1, 11),","75","              \"min_samples_split\": [1, 3, 10],"]}]}},"00da9cc5341e59e52d52c3278b7c81d4af3c05ce":{"changes":{"sklearn\/tree\/tests\/test_export.py":"MODIFY","sklearn\/tree\/export.py":"MODIFY"},"diff":{"sklearn\/tree\/tests\/test_export.py":[{"add":["4","from re import finditer, search","6","from numpy.random import RandomState","7","","8","from sklearn.base import ClassifierMixin","13","from sklearn.utils.testing import (assert_in, assert_equal, assert_raises,","14","                                   assert_less_equal, assert_raises_regex,","15","                                   assert_raise_message)","241","    # Check precision error","242","    out = StringIO()","243","    assert_raises_regex(ValueError, \"should be greater or equal\",","244","                        export_graphviz, clf, out, precision=-1)","245","    assert_raises_regex(ValueError, \"should be an integer\",","246","                        export_graphviz, clf, out, precision=\"1\")","247","","262","","263","","264","def test_precision():","265","","266","    rng_reg = RandomState(2)","267","    rng_clf = RandomState(8)","268","    for X, y, clf in zip(","269","            (rng_reg.random_sample((5, 2)),","270","             rng_clf.random_sample((1000, 4))),","271","            (rng_reg.random_sample((5, )),","272","             rng_clf.randint(2, size=(1000, ))),","273","            (DecisionTreeRegressor(criterion=\"friedman_mse\", random_state=0,","274","                                   max_depth=1),","275","             DecisionTreeClassifier(max_depth=1, random_state=0))):","276","","277","        clf.fit(X, y)","278","        for precision in (4, 3):","279","            dot_data = export_graphviz(clf, out_file=None, precision=precision,","280","                                       proportion=True)","281","","282","            # With the current random state, the impurity and the threshold","283","            # will have the number of precision set in the export_graphviz","284","            # function. We will check the number of precision with a strict","285","            # equality. The value reported will have only 2 precision and","286","            # therefore, only a less equal comparison will be done.","287","","288","            # check value","289","            for finding in finditer(\"value = \\d+\\.\\d+\", dot_data):","290","                assert_less_equal(","291","                    len(search(\"\\.\\d+\", finding.group()).group()),","292","                    precision + 1)","293","            # check impurity","294","            if isinstance(clf, ClassifierMixin):","295","                pattern = \"gini = \\d+\\.\\d+\"","296","            else:","297","                pattern = \"friedman_mse = \\d+\\.\\d+\"","298","","299","            # check impurity","300","            for finding in finditer(pattern, dot_data):","301","                assert_equal(len(search(\"\\.\\d+\", finding.group()).group()),","302","                             precision + 1)","303","            # check threshold","304","            for finding in finditer(\"<= \\d+\\.\\d+\", dot_data):","305","                assert_equal(len(search(\"\\.\\d+\", finding.group()).group()),","306","                             precision + 1)"],"delete":["4","from re import finditer","10","from sklearn.utils.testing import assert_in, assert_equal, assert_raises","11","from sklearn.utils.testing import assert_raise_message"]}],"sklearn\/tree\/export.py":[{"add":["13","from numbers import Integral","14","","77","                    rounded=False, special_characters=False, precision=3):","147","    precision : int, optional (default=3)","148","        Number of digits of precision for floating point in the values of","149","        impurity, threshold and value attributes of each node.","150","","170","","235","                                           round(tree.threshold[node_id],","236","                                                 precision),","247","            node_string += (str(round(tree.impurity[node_id], precision)) +","270","            value_text = np.around(value, precision)","273","            value_text = np.around(value, precision)","279","            value_text = np.around(value, precision)","412","        if isinstance(precision, Integral):","413","            if precision < 0:","414","                raise ValueError(\"'precision' should be greater or equal to 0.\"","415","                                 \" Got {} instead.\".format(precision))","416","        else:","417","            raise ValueError(\"'precision' should be an integer. Got {}\"","418","                             \" instead.\".format(type(precision)))","419",""],"delete":["75","                    rounded=False, special_characters=False):","228","                                           round(tree.threshold[node_id], 4),","239","            node_string += (str(round(tree.impurity[node_id], 4)) +","262","            value_text = np.around(value, 4)","265","            value_text = np.around(value, 2)","271","            value_text = np.around(value, 4)"]}]}},"6252f99c16a5d5d84573391e1b488baf51554117":{"changes":{"sklearn\/utils\/extmath.py":"MODIFY","sklearn\/decomposition\/nmf.py":"MODIFY","sklearn\/utils\/tests\/test_extmath.py":"MODIFY"},"diff":{"sklearn\/utils\/extmath.py":[{"add":["55","    if np.issubdtype(x.dtype, np.integer):","56","        warnings.warn('Array type is integer, np.dot may overflow. '","57","                      'Data should be float type to avoid this issue',","58","                      UserWarning)"],"delete":[]}],"sklearn\/decomposition\/nmf.py":[{"add":["959","    X = check_array(X, accept_sparse=('csr', 'csc'), dtype=float)","1206","        X = check_array(X, accept_sparse=('csr', 'csc'), dtype=float)"],"delete":["959","    X = check_array(X, accept_sparse=('csr', 'csc'))","1206","        X = check_array(X, accept_sparse=('csr', 'csc'))"]}],"sklearn\/utils\/tests\/test_extmath.py":[{"add":["20","from sklearn.utils.testing import assert_warns_message","151","    # Check the warning with an int array and np.dot potential overflow","152","    assert_warns_message(","153","                    UserWarning, 'Array type is integer, np.dot may '","154","                    'overflow. Data should be float type to avoid this issue',","155","                    squared_norm, X.astype(int))"],"delete":[]}]}},"af1796ef68d5193f0ca8573a7c2e71c97c97e9ff":{"changes":{"sklearn\/model_selection\/__init__.py":"MODIFY","sklearn\/model_selection\/_split.py":"MODIFY","doc\/modules\/classes.rst":"MODIFY","sklearn\/model_selection\/tests\/test_split.py":"MODIFY","doc\/modules\/cross_validation.rst":"MODIFY","doc\/whats_new.rst":"MODIFY"},"diff":{"sklearn\/model_selection\/__init__.py":[{"add":["9","from ._split import RepeatedKFold","10","from ._split import RepeatedStratifiedKFold","40","           'RepeatedKFold',","41","           'RepeatedStratifiedKFold',"],"delete":[]}],"sklearn\/model_selection\/_split.py":[{"add":["43","           'RepeatedStratifiedKFold',","44","           'RepeatedKFold',","401","","402","    RepeatedKFold: Repeats K-Fold n times.","559","    See also","560","    --------","561","    RepeatedStratifiedKFold: Repeats Stratified K-Fold n times.","922","class _RepeatedSplits(with_metaclass(ABCMeta)):","923","    \"\"\"Repeated splits for an arbitrary randomized CV splitter.","924","","925","    Repeats splits for cross-validators n times with different randomization","926","    in each repetition.","927","","928","    Parameters","929","    ----------","930","    cv : callable","931","        Cross-validator class.","932","","933","    n_repeats : int, default=10","934","        Number of times cross-validator needs to be repeated.","935","","936","    random_state : None, int or RandomState, default=None","937","        Random state to be used to generate random state for each","938","        repetition.","939","","940","    **cvargs : additional params","941","        Constructor parameters for cv. Must not contain random_state","942","        and shuffle.","943","    \"\"\"","944","    def __init__(self, cv, n_repeats=10, random_state=None, **cvargs):","945","        if not isinstance(n_repeats, (np.integer, numbers.Integral)):","946","            raise ValueError(\"Number of repetitions must be of Integral type.\")","947","","948","        if n_repeats <= 1:","949","            raise ValueError(\"Number of repetitions must be greater than 1.\")","950","","951","        if any(key in cvargs for key in ('random_state', 'shuffle')):","952","            raise ValueError(","953","                \"cvargs must not contain random_state or shuffle.\")","954","","955","        self.cv = cv","956","        self.n_repeats = n_repeats","957","        self.random_state = random_state","958","        self.cvargs = cvargs","959","","960","    def split(self, X, y=None, groups=None):","961","        \"\"\"Generates indices to split data into training and test set.","962","","963","        Parameters","964","        ----------","965","        X : array-like, shape (n_samples, n_features)","966","            Training data, where n_samples is the number of samples","967","            and n_features is the number of features.","968","","969","        y : array-like, of length n_samples","970","            The target variable for supervised learning problems.","971","","972","        groups : array-like, with shape (n_samples,), optional","973","            Group labels for the samples used while splitting the dataset into","974","            train\/test set.","975","","976","        Returns","977","        -------","978","        train : ndarray","979","            The training set indices for that split.","980","","981","        test : ndarray","982","            The testing set indices for that split.","983","        \"\"\"","984","        n_repeats = self.n_repeats","985","        rng = check_random_state(self.random_state)","986","","987","        for idx in range(n_repeats):","988","            cv = self.cv(random_state=rng, shuffle=True,","989","                         **self.cvargs)","990","            for train_index, test_index in cv.split(X, y, groups):","991","                yield train_index, test_index","992","","993","","994","class RepeatedKFold(_RepeatedSplits):","995","    \"\"\"Repeated K-Fold cross validator.","996","","997","    Repeats K-Fold n times with different randomization in each repetition.","998","","999","    Read more in the :ref:`User Guide <cross_validation>`.","1000","","1001","    Parameters","1002","    ----------","1003","    n_splits : int, default=5","1004","        Number of folds. Must be at least 2.","1005","","1006","    n_repeats : int, default=10","1007","        Number of times cross-validator needs to be repeated.","1008","","1009","    random_state : None, int or RandomState, default=None","1010","        Random state to be used to generate random state for each","1011","        repetition.","1012","","1013","    Examples","1014","    --------","1015","    >>> from sklearn.model_selection import RepeatedKFold","1016","    >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])","1017","    >>> y = np.array([0, 0, 1, 1])","1018","    >>> rkf = RepeatedKFold(n_splits=2, n_repeats=2, random_state=2652124)","1019","    >>> for train_index, test_index in rkf.split(X):","1020","    ...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)","1021","    ...     X_train, X_test = X[train_index], X[test_index]","1022","    ...     y_train, y_test = y[train_index], y[test_index]","1023","    ...","1024","    TRAIN: [0 1] TEST: [2 3]","1025","    TRAIN: [2 3] TEST: [0 1]","1026","    TRAIN: [1 2] TEST: [0 3]","1027","    TRAIN: [0 3] TEST: [1 2]","1028","","1029","","1030","    See also","1031","    --------","1032","    RepeatedStratifiedKFold: Repeates Stratified K-Fold n times.","1033","    \"\"\"","1034","    def __init__(self, n_splits=5, n_repeats=10, random_state=None):","1035","        super(RepeatedKFold, self).__init__(","1036","            KFold, n_repeats, random_state, n_splits=n_splits)","1037","","1038","","1039","class RepeatedStratifiedKFold(_RepeatedSplits):","1040","    \"\"\"Repeated Stratified K-Fold cross validator.","1041","","1042","    Repeats Stratified K-Fold n times with different randomization in each","1043","    repetition.","1044","","1045","    Read more in the :ref:`User Guide <cross_validation>`.","1046","","1047","    Parameters","1048","    ----------","1049","    n_splits : int, default=5","1050","        Number of folds. Must be at least 2.","1051","","1052","    n_repeats : int, default=10","1053","        Number of times cross-validator needs to be repeated.","1054","","1055","    random_state : None, int or RandomState, default=None","1056","        Random state to be used to generate random state for each","1057","        repetition.","1058","","1059","    Examples","1060","    --------","1061","    >>> from sklearn.model_selection import RepeatedStratifiedKFold","1062","    >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])","1063","    >>> y = np.array([0, 0, 1, 1])","1064","    >>> rskf = RepeatedStratifiedKFold(n_splits=2, n_repeats=2,","1065","    ...     random_state=36851234)","1066","    >>> for train_index, test_index in rskf.split(X, y):","1067","    ...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)","1068","    ...     X_train, X_test = X[train_index], X[test_index]","1069","    ...     y_train, y_test = y[train_index], y[test_index]","1070","    ...","1071","    TRAIN: [1 2] TEST: [0 3]","1072","    TRAIN: [0 3] TEST: [1 2]","1073","    TRAIN: [1 3] TEST: [0 2]","1074","    TRAIN: [0 2] TEST: [1 3]","1075","","1076","","1077","    See also","1078","    --------","1079","    RepeatedKFold: Repeats K-Fold n times.","1080","    \"\"\"","1081","    def __init__(self, n_splits=5, n_repeats=10, random_state=None):","1082","        super(RepeatedStratifiedKFold, self).__init__(","1083","            StratifiedKFold, n_repeats, random_state, n_splits=n_splits)","1084","","1085",""],"delete":[]}],"doc\/modules\/classes.rst":[{"add":["172","   model_selection.RepeatedKFold","173","   model_selection.RepeatedStratifiedKFold"],"delete":[]}],"sklearn\/model_selection\/tests\/test_split.py":[{"add":["44","from sklearn.model_selection import RepeatedKFold","45","from sklearn.model_selection import RepeatedStratifiedKFold","808","def test_repeated_cv_value_errors():","809","    # n_repeats is not integer or <= 1","810","    for cv in (RepeatedKFold, RepeatedStratifiedKFold):","811","        assert_raises(ValueError, cv, n_repeats=1)","812","        assert_raises(ValueError, cv, n_repeats=1.5)","813","","814","","815","def test_repeated_kfold_determinstic_split():","816","    X = [[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]","817","    random_state = 258173307","818","    rkf = RepeatedKFold(","819","        n_splits=2,","820","        n_repeats=2,","821","        random_state=random_state)","822","","823","    # split should produce same and deterministic splits on","824","    # each call","825","    for _ in range(3):","826","        splits = rkf.split(X)","827","        train, test = next(splits)","828","        assert_array_equal(train, [2, 4])","829","        assert_array_equal(test, [0, 1, 3])","830","","831","        train, test = next(splits)","832","        assert_array_equal(train, [0, 1, 3])","833","        assert_array_equal(test, [2, 4])","834","","835","        train, test = next(splits)","836","        assert_array_equal(train, [0, 1])","837","        assert_array_equal(test, [2, 3, 4])","838","","839","        train, test = next(splits)","840","        assert_array_equal(train, [2, 3, 4])","841","        assert_array_equal(test, [0, 1])","842","","843","        assert_raises(StopIteration, next, splits)","844","","845","","846","def test_repeated_stratified_kfold_determinstic_split():","847","    X = [[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]","848","    y = [1, 1, 1, 0, 0]","849","    random_state = 1944695409","850","    rskf = RepeatedStratifiedKFold(","851","        n_splits=2,","852","        n_repeats=2,","853","        random_state=random_state)","854","","855","    # split should produce same and deterministic splits on","856","    # each call","857","    for _ in range(3):","858","        splits = rskf.split(X, y)","859","        train, test = next(splits)","860","        assert_array_equal(train, [1, 4])","861","        assert_array_equal(test, [0, 2, 3])","862","","863","        train, test = next(splits)","864","        assert_array_equal(train, [0, 2, 3])","865","        assert_array_equal(test, [1, 4])","866","","867","        train, test = next(splits)","868","        assert_array_equal(train, [2, 3])","869","        assert_array_equal(test, [0, 1, 4])","870","","871","        train, test = next(splits)","872","        assert_array_equal(train, [0, 1, 4])","873","        assert_array_equal(test, [2, 3])","874","","875","        assert_raises(StopIteration, next, splits)","876","","877",""],"delete":[]}],"doc\/modules\/cross_validation.rst":[{"add":["265","Repeated K-Fold","266","---------------","267","","268",":class:`RepeatedKFold` repeats K-Fold n times. It can be used when one","269","requires to run :class:`KFold` n times, producing different splits in","270","each repetition.","271","","272","Example of 2-fold K-Fold repeated 2 times::","273","","274","  >>> import numpy as np","275","  >>> from sklearn.model_selection import RepeatedKFold","276","  >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])","277","  >>> random_state = 12883823","278","  >>> rkf = RepeatedKFold(n_splits=2, n_repeats=2, random_state=random_state)","279","  >>> for train, test in rkf.split(X):","280","  ...     print(\"%s %s\" % (train, test))","281","  ...","282","  [2 3] [0 1]","283","  [0 1] [2 3]","284","  [0 2] [1 3]","285","  [1 3] [0 2]","286","","287","","288","Similarly, :class:`RepeatedStratifiedKFold` repeats Stratified K-Fold n times","289","with different randomization in each repetition.","290","","291","","438",":class:`RepeatedStratifiedKFold` can be used to repeat Stratified K-Fold n times","439","with different randomization in each repetition.","440","","441",""],"delete":[]}],"doc\/whats_new.rst":[{"add":["43","   - Added the :class:`sklearn.model_selection.RepeatedKFold` and","44","     :class:`sklearn.model_selection.RepeatedStratifiedKFold`.","45","     :issue:`8120` by `Neeraj Gangwar`_.","46","","5010","","5011",".. _Neeraj Gangwar: http:\/\/neerajgangwar.in"],"delete":[]}]}},"2709531e898d665ce3fccd145070034027090578":{"changes":{"sklearn\/linear_model\/coordinate_descent.py":"MODIFY"},"diff":{"sklearn\/linear_model\/coordinate_descent.py":[{"add":["1613","        passed in at fit (non multi-task usage), ``coef_`` is then a 1D array.","1614","        Note that ``coef_`` stores the transpose of ``W``, ``W.T``.","1795","        Parameter vector (W in the cost function formula).","1796","        Note that ``coef_`` stores the transpose of ``W``, ``W.T``.","1954","        Note that ``coef_`` stores the transpose of ``W``, ``W.T``.","2120","        Note that ``coef_`` stores the transpose of ``W``, ``W.T``."],"delete":["1613","        passed in at fit (non multi-task usage), ``coef_`` is then a 1D array","1794","        parameter vector (W in the cost function formula)"]}]}},"ddd886baa8d9840e1897fed5a4ffb84138b18d81":{"changes":{"sklearn\/utils\/__init__.py":"MODIFY"},"diff":{"sklearn\/utils\/__init__.py":[{"add":["192","        raise ValueError(\"Cannot sample %d out of arrays with dim %d \""],"delete":["192","        raise ValueError(\"Cannot sample %d out of arrays with dim %d\""]}]}},"b43c79163d7e3f8477f973e8aa8bca7025f58481":{"changes":{"sklearn\/tests\/test_multiclass.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/multiclass.py":"MODIFY"},"diff":{"sklearn\/tests\/test_multiclass.py":[{"add":["253","        if hasattr(base_clf, 'decision_function'):","254","            dec = clf.decision_function(X)","255","            assert_equal(dec.shape, (5,))","529","    # first binary","530","    ovo_clf.fit(iris.data, iris.target == 0)","531","    decisions = ovo_clf.decision_function(iris.data)","532","    assert_equal(decisions.shape, (n_samples,))","533","","534","    # then multi-class"],"delete":[]}],"doc\/whats_new.rst":[{"add":["420","   - The ``decision_function`` output shape for binary classification in","421","     :class:`multi_class.OneVsRestClassifier` and","422","     :class:`multi_class.OneVsOneClassifier` is now ``(n_samples,)`` to conform","423","     to scikit-learn conventions. :issue:`9100` by `Andreas Mller`_.","424",""],"delete":[]}],"sklearn\/multiclass.py":[{"add":["370","        if len(self.estimators_) == 1:","371","            return self.estimators_[0].decision_function(X)","578","        if self.n_classes_ == 2:","579","            return self.classes_[(Y > 0).astype(np.int)]","612","        if self.n_classes_ == 2:","613","            return Y[:, 1]"],"delete":["608",""]}]}},"57415c461068f8a3d7cdca7e274dd8bc039d9efa":{"changes":{"sklearn\/manifold\/tests\/test_t_sne.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/manifold\/t_sne.py":"MODIFY"},"diff":{"sklearn\/manifold\/tests\/test_t_sne.py":[{"add":["14","from sklearn.utils.testing import skip_if_32bit","566","@skip_if_32bit","568","    # Use a dummy negative n_iter_without_progress and check output on stdout","571","    tsne = TSNE(n_iter_without_progress=-1, verbose=2,","572","                random_state=1, method='exact')","585","              \"last -1 episodes. Finished.\", out)"],"delete":["566","    # Make sure that the parameter n_iter_without_progress is used correctly","569","    tsne = TSNE(n_iter_without_progress=2, verbose=2,","570","                random_state=0, method='exact')","583","              \"last 2 episodes. Finished.\", out)"]}],"doc\/whats_new.rst":[{"add":["294","   - Fixed a bug in :class:`manifold.TSNE` affecting convergence of the","295","     gradient descent. :issue:`8768` by :user:`David DeTomaso <deto>`."],"delete":[]}],"sklearn\/manifold\/t_sne.py":[{"add":["390","        inc = update * grad < 0.0","392","        gains[inc] += 0.2","393","        gains[dec] *= 0.8","394","        np.clip(gains, min_gain, np.inf, out=gains)","633","    array([[ 0.00017619,  0.00004014],","634","           [ 0.00010268,  0.00020546],","635","           [ 0.00018298, -0.00008335],","636","           [ 0.00009501, -0.00001388]])"],"delete":["390","        inc = update * grad >= 0.0","392","        gains[inc] += 0.05","393","        gains[dec] *= 0.95","394","        np.clip(gains, min_gain, np.inf)","633","    array([[ 0.00017599,  0.00003993],","634","           [ 0.00009891,  0.00021913],","635","           [ 0.00018554, -0.00009357],","636","           [ 0.00009528, -0.00001407]])"]}]}},"b429868d07f874b81cafd1228a38f9990fb6ef91":{"changes":{"sklearn\/covariance\/tests\/test_robust_covariance.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/covariance\/robust_covariance.py":"MODIFY"},"diff":{"sklearn\/covariance\/tests\/test_robust_covariance.py":[{"add":["6","import itertools","7","","96","def test_mcd_issue3367():","97","    # Check that MCD completes when the covariance matrix is singular","98","    # i.e. one of the rows and columns are all zeros","99","    rand_gen = np.random.RandomState(0)","100","","101","    # Think of these as the values for X and Y -> 10 values between -5 and 5","102","    data_values = np.linspace(-5, 5, 10).tolist()","103","    # Get the cartesian product of all possible coordinate pairs from above set","104","    data = np.array(list(itertools.product(data_values, data_values)))","105","","106","    # Add a third column that's all zeros to make our data a set of point","107","    # within a plane, which means that the covariance matrix will be singular","108","    data = np.hstack((data, np.zeros((data.shape[0], 1))))","109","","110","    # The below line of code should raise an exception if the covariance matrix","111","    # is singular. As a further test, since we have points in XYZ, the","112","    # principle components (Eigenvectors) of these directly relate to the","113","    # geometry of the points. Since it's a plane, we should be able to test","114","    # that the Eigenvector that corresponds to the smallest Eigenvalue is the","115","    # plane normal, specifically [0, 0, 1], since everything is in the XY plane","116","    # (as I've set it up above). To do this one would start by:","117","    #","118","    #     evals, evecs = np.linalg.eigh(mcd_fit.covariance_)","119","    #     normal = evecs[:, np.argmin(evals)]","120","    #","121","    # After which we need to assert that our `normal` is equal to [0, 0, 1].","122","    # Do note that there is floating point error associated with this, so it's","123","    # best to subtract the two and then compare some small tolerance (e.g.","124","    # 1e-12).","125","    MinCovDet(random_state=rand_gen).fit(data)","126","","127",""],"delete":[]}],"doc\/whats_new.rst":[{"add":["189","","190","   - Fixed a bug in :class:`sklearn.covariance.MinCovDet` where inputting data","191","     that produced a singular covariance matrix would cause the helper method","192","     `_c_step` to throw an exception.","193","     :issue:`3367` by :user:`Jeremy Steward <ThatGeoGuy>`","194","","211",""],"delete":[]}],"sklearn\/covariance\/robust_covariance.py":[{"add":["98","    dist = np.inf","122","    # If the data already has singular covariance, calculate the precision,","123","    # as the loop below will not be entered.","124","    if np.isinf(det):","125","        precision = pinvh(covariance)","126","","128","    while (det < previous_det and remaining_iterations > 0","129","            and not np.isinf(det)):","151","    # Check if best fit already found (det => 0, logdet => -inf)","153","        results = location, covariance, det, support, dist"],"delete":["122","    while (det < previous_det) and (remaining_iterations > 0):","144","    # Catch computation errors","146","        raise ValueError(","147","            \"Singular covariance matrix. \"","148","            \"Please check that the covariance matrix corresponding \"","149","            \"to the dataset is full rank and that MinCovDet is used with \"","150","            \"Gaussian-distributed data (or at least data drawn from a \"","151","            \"unimodal, symmetric distribution.\")"]}]}},"c2b0de59aa778bff268d8eebfaaa9158487ef9a7":{"changes":{"sklearn\/datasets\/base.py":"MODIFY","sklearn\/datasets\/tests\/test_base.py":"MODIFY"},"diff":{"sklearn\/datasets\/base.py":[{"add":["570","","571","    module_path = dirname(__file__)","572","    base_dir = join(module_path, 'data')","576","    with open(join(module_path, 'descr', 'diabetes.rst')) as rst_file:","577","        fdescr = rst_file.read()","578","","582","    return Bunch(data=data, target=target, DESCR=fdescr,"],"delete":["570","    base_dir = join(dirname(__file__), 'data')","577","    return Bunch(data=data, target=target,"]}],"sklearn\/datasets\/tests\/test_base.py":[{"add":["175","    assert_true(res.DESCR)"],"delete":[]}]}},"2f7f5a1a50c2a2022d42160fce9d0596ecac2ada":{"changes":{"sklearn\/metrics\/scorer.py":"MODIFY","sklearn\/metrics\/tests\/test_score_objects.py":"MODIFY","sklearn\/model_selection\/tests\/test_search.py":"MODIFY","doc\/modules\/model_evaluation.rst":"MODIFY"},"diff":{"sklearn\/metrics\/scorer.py":[{"add":["29","","31","from .cluster import homogeneity_score","32","from .cluster import completeness_score","33","from .cluster import v_measure_score","34","from .cluster import mutual_info_score","35","from .cluster import adjusted_mutual_info_score","36","from .cluster import normalized_mutual_info_score","37","from .cluster import fowlkes_mallows_score","38","","404","homogeneity_scorer = make_scorer(homogeneity_score)","405","completeness_scorer = make_scorer(completeness_score)","406","v_measure_scorer = make_scorer(v_measure_score)","407","mutual_info_scorer = make_scorer(mutual_info_score)","408","adjusted_mutual_info_scorer = make_scorer(adjusted_mutual_info_score)","409","normalized_mutual_info_scorer = make_scorer(normalized_mutual_info_score)","410","fowlkes_mallows_scorer = make_scorer(fowlkes_mallows_score)","411","","425","               # Cluster metrics that use supervised evaluation","426","               adjusted_rand_score=adjusted_rand_scorer,","427","               homogeneity_score=homogeneity_scorer,","428","               completeness_score=completeness_scorer,","429","               v_measure_score=v_measure_scorer,","430","               mutual_info_score=mutual_info_scorer,","431","               adjusted_mutual_info_score=adjusted_mutual_info_scorer,","432","               normalized_mutual_info_score=normalized_mutual_info_scorer,","433","               fowlkes_mallows_score=fowlkes_mallows_scorer)","434",""],"delete":["408","               adjusted_rand_score=adjusted_rand_scorer)"]}],"sklearn\/metrics\/tests\/test_score_objects.py":[{"add":["20","from sklearn.metrics import cluster as cluster_module","49","               'neg_log_loss', 'log_loss']","50","","51","# All supervised cluster scorers (They behave like classification metric)","52","CLUSTER_SCORERS = [\"adjusted_rand_score\",","53","                   \"homogeneity_score\",","54","                   \"completeness_score\",","55","                   \"v_measure_score\",","56","                   \"mutual_info_score\",","57","                   \"adjusted_mutual_info_score\",","58","                   \"normalized_mutual_info_score\",","59","                   \"fowlkes_mallows_score\"]","75","        [(name, sensible_clf) for name in CLUSTER_SCORERS] +","341","def test_supervised_cluster_scorers():","347","    for name in CLUSTER_SCORERS:","348","        score1 = get_scorer(name)(km, X_test, y_test)","349","        score2 = getattr(cluster_module, name)(y_test, km.predict(X_test))","350","        assert_almost_equal(score1, score2)","456","                         KMeans(), cluster_module.adjusted_rand_score)"],"delete":["20","from sklearn.metrics.cluster import adjusted_rand_score","49","               'neg_log_loss', 'log_loss',","50","               'adjusted_rand_score'  # not really, but works","51","               ]","332","def test_unsupervised_scorers():","334","    # We don't have any real unsupervised Scorers yet.","339","    score1 = get_scorer('adjusted_rand_score')(km, X_test, y_test)","340","    score2 = adjusted_rand_score(y_test, km.predict(X_test))","341","    assert_almost_equal(score1, score2)","447","                         KMeans(), adjusted_rand_score)"]}],"sklearn\/model_selection\/tests\/test_search.py":[{"add":["544","    grid_search = GridSearchCV(km, param_grid=dict(n_clusters=[2, 3, 4]),","545","                               scoring='fowlkes_mallows_score')","546","    grid_search.fit(X, y)","547","    # So can FMS ;)","548","    assert_equal(grid_search.best_params_[\"n_clusters\"], 3)","549",""],"delete":[]}],"doc\/modules\/model_evaluation.rst":[{"add":["96","    ValueError: 'wrong_choice' is not a valid scoring value. Valid options are ['accuracy', 'adjusted_mutual_info_score', 'adjusted_rand_score', 'average_precision', 'completeness_score', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'fowlkes_mallows_score', 'homogeneity_score', 'mutual_info_score', 'neg_log_loss', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_mean_squared_log_error', 'neg_median_absolute_error', 'normalized_mutual_info_score', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'r2', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'roc_auc', 'v_measure_score']"],"delete":["96","    ValueError: 'wrong_choice' is not a valid scoring value. Valid options are ['accuracy', 'adjusted_rand_score', 'average_precision', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'neg_log_loss', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_mean_squared_log_error', 'neg_median_absolute_error', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'r2', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'roc_auc']"]}]}},"26a1027a830b7b658782229d936ffb84a340caad":{"changes":{"\/dev\/null":"DELETE","doc\/modules\/classes.rst":"MODIFY","sklearn\/preprocessing\/__init__.py":"MODIFY","doc\/modules\/preprocessing.rst":"MODIFY","doc\/whats_new.rst":"MODIFY","examples\/preprocessing\/plot_all_scaling.py":"ADD","sklearn\/preprocessing\/tests\/test_data.py":"MODIFY","sklearn\/preprocessing\/data.py":"MODIFY","build_tools\/travis\/flake8_diff.sh":"MODIFY"},"diff":{"\/dev\/null":[{"add":[],"delete":[]}],"doc\/modules\/classes.rst":[{"add":["1200","   preprocessing.QuantileTransformer","1214","   preprocessing.quantile_transform"],"delete":[]}],"sklearn\/preprocessing\/__init__.py":[{"add":["14","from .data import QuantileTransformer","22","from .data import quantile_transform","45","    'QuantileTransformer',","59","    'quantile_transform',"],"delete":[]}],"doc\/modules\/preprocessing.rst":[{"add":["12","In general, learning algorithms benefit from standardization of the data set. If","13","some outliers are present in the set, robust scalers or transformers are more","14","appropriate. The behaviors of the different scalers, transformers, and","15","normalizers on a dataset containing marginal outliers is highlighted in","16",":ref:`sphx_glr_auto_examples_preprocessing_plot_all_scaling.py`.","17","","18","","48","  >>> X_train = np.array([[ 1., -1.,  2.],","49","  ...                     [ 2.,  0.,  0.],","50","  ...                     [ 0.,  1., -1.]])","51","  >>> X_scaled = preprocessing.scale(X_train)","80","  >>> scaler = preprocessing.StandardScaler().fit(X_train)","90","  >>> scaler.transform(X_train)                           # doctest: +ELLIPSIS","99","  >>> X_test = [[-1., 1., 0.]]","100","  >>> scaler.transform(X_test)                # doctest: +ELLIPSIS","258",".. _preprocessing_transformer:","259","","260","Non-linear transformation","261","=========================","262","","263","Like scalers, :class:`QuantileTransformer` puts each feature into the same","264","range or distribution. However, by performing a rank transformation, it smooths","265","out unusual distributions and is less influenced by outliers than scaling","266","methods. It does, however, distort correlations and distances within and across","267","features.","268","","269",":class:`QuantileTransformer` and :func:`quantile_transform` provide a","270","non-parametric transformation based on the quantile function to map the data to","271","a uniform distribution with values between 0 and 1::","272","","273","  >>> from sklearn.datasets import load_iris","274","  >>> from sklearn.model_selection import train_test_split","275","  >>> iris = load_iris()","276","  >>> X, y = iris.data, iris.target","277","  >>> X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)","278","  >>> quantile_transformer = preprocessing.QuantileTransformer(random_state=0)","279","  >>> X_train_trans = quantile_transformer.fit_transform(X_train)","280","  >>> X_test_trans = quantile_transformer.transform(X_test)","281","  >>> np.percentile(X_train[:, 0], [0, 25, 50, 75, 100]) # doctest: +SKIP","282","  array([ 4.3,  5.1,  5.8,  6.5,  7.9])","283","","284","This feature corresponds to the sepal length in cm. Once the quantile","285","transformation applied, those landmarks approach closely the percentiles","286","previously defined::","287","","288","  >>> np.percentile(X_train_trans[:, 0], [0, 25, 50, 75, 100])","289","  ... # doctest: +ELLIPSIS +SKIP","290","  array([ 0.00... ,  0.24...,  0.49...,  0.73...,  0.99... ])","291","","292","This can be confirmed on a independent testing set with similar remarks::","293","","294","  >>> np.percentile(X_test[:, 0], [0, 25, 50, 75, 100])","295","  ... # doctest: +SKIP","296","  array([ 4.4  ,  5.125,  5.75 ,  6.175,  7.3  ])","297","  >>> np.percentile(X_test_trans[:, 0], [0, 25, 50, 75, 100])","298","  ... # doctest: +ELLIPSIS +SKIP","299","  array([ 0.01...,  0.25...,  0.46...,  0.60... ,  0.94...])","300","","301","It is also possible to map the transformed data to a normal distribution by","302","setting ``output_distribution='normal'``::","303","","304","  >>> quantile_transformer = preprocessing.QuantileTransformer(","305","  ...     output_distribution='normal', random_state=0)","306","  >>> X_trans = quantile_transformer.fit_transform(X)","307","  >>> quantile_transformer.quantiles_ # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE","308","  array([[ 4.3...,   2...,     1...,     0.1...],","309","         [ 4.31...,  2.02...,  1.01...,  0.1...],","310","         [ 4.32...,  2.05...,  1.02...,  0.1...],","311","         ...,","312","         [ 7.84...,  4.34...,  6.84...,  2.5...],","313","         [ 7.87...,  4.37...,  6.87...,  2.5...],","314","         [ 7.9...,   4.4...,   6.9...,   2.5...]])","315","","316","Thus the median of the input becomes the mean of the output, centered at 0. The","317","normal output is clipped so that the input's minimum and maximum ---","318","corresponding to the 1e-7 and 1 - 1e-7 quantiles respectively --- do not","319","become infinite under the transformation.","320",""],"delete":["41","  >>> X = np.array([[ 1., -1.,  2.],","42","  ...               [ 2.,  0.,  0.],","43","  ...               [ 0.,  1., -1.]])","44","  >>> X_scaled = preprocessing.scale(X)","73","  >>> scaler = preprocessing.StandardScaler().fit(X)","83","  >>> scaler.transform(X)                               # doctest: +ELLIPSIS","92","  >>> scaler.transform([[-1.,  1., 0.]])                # doctest: +ELLIPSIS"]}],"doc\/whats_new.rst":[{"add":["64","   - Added :class:`preprocessing.QuantileTransformer` class and","65","     :func:`preprocessing.quantile_transform` function for features","66","     normalization based on quantiles.","67","     :issue:`8363` by :user:`Denis Engemann <dengemann>`,","68","     :user:`Guillaume Lemaitre <glemaitre>`, `Olivier Grisel`_, `Raghav RV`_,","69","     :user:`Thierry Guillemot <tguillemot>`_, and `Gael Varoquaux`_.","70","","181","   - In :class:`gaussian_process.GaussianProcessRegressor`, method ``predict``","340","   - Fix :func:`sklearn.linear_model.BayesianRidge.fit` to return"],"delete":["174","   - In :class:`gaussian_process.GaussianProcessRegressor`, method ``predict`` ","333","   - Fix :func:`sklearn.linear_model.BayesianRidge.fit` to return "]}],"examples\/preprocessing\/plot_all_scaling.py":[{"add":[],"delete":[]}],"sklearn\/preprocessing\/tests\/test_data.py":[{"add":["5","from __future__ import division","44","from sklearn.preprocessing.data import QuantileTransformer","45","from sklearn.preprocessing.data import quantile_transform","145","    feature_names = poly.get_feature_names(","146","        [u\"\\u0001F40D\", u\"\\u262E\", u\"\\u05D0\"])","856","def test_quantile_transform_iris():","857","    X = iris.data","858","    # uniform output distribution","859","    transformer = QuantileTransformer(n_quantiles=30)","860","    X_trans = transformer.fit_transform(X)","861","    X_trans_inv = transformer.inverse_transform(X_trans)","862","    assert_array_almost_equal(X, X_trans_inv)","863","    # normal output distribution","864","    transformer = QuantileTransformer(n_quantiles=30,","865","                                      output_distribution='normal')","866","    X_trans = transformer.fit_transform(X)","867","    X_trans_inv = transformer.inverse_transform(X_trans)","868","    assert_array_almost_equal(X, X_trans_inv)","869","    # make sure it is possible to take the inverse of a sparse matrix","870","    # which contain negative value; this is the case in the iris dataset","871","    X_sparse = sparse.csc_matrix(X)","872","    X_sparse_tran = transformer.fit_transform(X_sparse)","873","    X_sparse_tran_inv = transformer.inverse_transform(X_sparse_tran)","874","    assert_array_almost_equal(X_sparse.A, X_sparse_tran_inv.A)","875","","876","","877","def test_quantile_transform_check_error():","878","    X = np.transpose([[0, 25, 50, 0, 0, 0, 75, 0, 0, 100],","879","                      [2, 4, 0, 0, 6, 8, 0, 10, 0, 0],","880","                      [0, 0, 2.6, 4.1, 0, 0, 2.3, 0, 9.5, 0.1]])","881","    X = sparse.csc_matrix(X)","882","    X_neg = np.transpose([[0, 25, 50, 0, 0, 0, 75, 0, 0, 100],","883","                          [-2, 4, 0, 0, 6, 8, 0, 10, 0, 0],","884","                          [0, 0, 2.6, 4.1, 0, 0, 2.3, 0, 9.5, 0.1]])","885","    X_neg = sparse.csc_matrix(X_neg)","886","","887","    assert_raises_regex(ValueError, \"Invalid value for 'n_quantiles': 0.\",","888","                        QuantileTransformer(n_quantiles=0).fit, X)","889","    assert_raises_regex(ValueError, \"Invalid value for 'subsample': 0.\",","890","                        QuantileTransformer(subsample=0).fit, X)","891","    assert_raises_regex(ValueError, \"The number of quantiles cannot be\"","892","                        \" greater than the number of samples used. Got\"","893","                        \" 1000 quantiles and 10 samples.\",","894","                        QuantileTransformer(subsample=10).fit, X)","895","","896","    transformer = QuantileTransformer(n_quantiles=10)","897","    assert_raises_regex(ValueError, \"QuantileTransformer only accepts \"","898","                        \"non-negative sparse matrices.\",","899","                        transformer.fit, X_neg)","900","    transformer.fit(X)","901","    assert_raises_regex(ValueError, \"QuantileTransformer only accepts \"","902","                        \"non-negative sparse matrices.\",","903","                        transformer.transform, X_neg)","904","","905","    X_bad_feat = np.transpose([[0, 25, 50, 0, 0, 0, 75, 0, 0, 100],","906","                               [0, 0, 2.6, 4.1, 0, 0, 2.3, 0, 9.5, 0.1]])","907","    assert_raises_regex(ValueError, \"X does not have the same number of \"","908","                        \"features as the previously fitted data. Got 2\"","909","                        \" instead of 3.\",","910","                        transformer.transform, X_bad_feat)","911","    assert_raises_regex(ValueError, \"X does not have the same number of \"","912","                        \"features as the previously fitted data. Got 2\"","913","                        \" instead of 3.\",","914","                        transformer.inverse_transform, X_bad_feat)","915","","916","    transformer = QuantileTransformer(n_quantiles=10,","917","                                      output_distribution='rnd')","918","    # check that an error is raised at fit time","919","    assert_raises_regex(ValueError, \"'output_distribution' has to be either\"","920","                        \" 'normal' or 'uniform'. Got 'rnd' instead.\",","921","                        transformer.fit, X)","922","    # check that an error is raised at transform time","923","    transformer.output_distribution = 'uniform'","924","    transformer.fit(X)","925","    X_tran = transformer.transform(X)","926","    transformer.output_distribution = 'rnd'","927","    assert_raises_regex(ValueError, \"'output_distribution' has to be either\"","928","                        \" 'normal' or 'uniform'. Got 'rnd' instead.\",","929","                        transformer.transform, X)","930","    # check that an error is raised at inverse_transform time","931","    assert_raises_regex(ValueError, \"'output_distribution' has to be either\"","932","                        \" 'normal' or 'uniform'. Got 'rnd' instead.\",","933","                        transformer.inverse_transform, X_tran)","934","","935","","936","def test_quantile_transform_sparse_ignore_zeros():","937","    X = np.array([[0, 1],","938","                  [0, 0],","939","                  [0, 2],","940","                  [0, 2],","941","                  [0, 1]])","942","    X_sparse = sparse.csc_matrix(X)","943","    transformer = QuantileTransformer(ignore_implicit_zeros=True,","944","                                      n_quantiles=5)","945","","946","    # dense case -> warning raise","947","    assert_warns_message(UserWarning, \"'ignore_implicit_zeros' takes effect\"","948","                         \" only with sparse matrix. This parameter has no\"","949","                         \" effect.\", transformer.fit, X)","950","","951","    X_expected = np.array([[0, 0],","952","                           [0, 0],","953","                           [0, 1],","954","                           [0, 1],","955","                           [0, 0]])","956","    X_trans = transformer.fit_transform(X_sparse)","957","    assert_almost_equal(X_expected, X_trans.A)","958","","959","    # consider the case where sparse entries are missing values and user-given","960","    # zeros are to be considered","961","    X_data = np.array([0, 0, 1, 0, 2, 2, 1, 0, 1, 2, 0])","962","    X_col = np.array([0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1])","963","    X_row = np.array([0, 4, 0, 1, 2, 3, 4, 5, 6, 7, 8])","964","    X_sparse = sparse.csc_matrix((X_data, (X_row, X_col)))","965","    X_trans = transformer.fit_transform(X_sparse)","966","    X_expected = np.array([[0., 0.5],","967","                           [0., 0.],","968","                           [0., 1.],","969","                           [0., 1.],","970","                           [0., 0.5],","971","                           [0., 0.],","972","                           [0., 0.5],","973","                           [0., 1.],","974","                           [0., 0.]])","975","    assert_almost_equal(X_expected, X_trans.A)","976","","977","    transformer = QuantileTransformer(ignore_implicit_zeros=True,","978","                                      n_quantiles=5)","979","    X_data = np.array([-1, -1, 1, 0, 0, 0, 1, -1, 1])","980","    X_col = np.array([0, 0, 1, 1, 1, 1, 1, 1, 1])","981","    X_row = np.array([0, 4, 0, 1, 2, 3, 4, 5, 6])","982","    X_sparse = sparse.csc_matrix((X_data, (X_row, X_col)))","983","    X_trans = transformer.fit_transform(X_sparse)","984","    X_expected = np.array([[0, 1],","985","                           [0, 0.375],","986","                           [0, 0.375],","987","                           [0, 0.375],","988","                           [0, 1],","989","                           [0, 0],","990","                           [0, 1]])","991","    assert_almost_equal(X_expected, X_trans.A)","992","    assert_almost_equal(X_sparse.A, transformer.inverse_transform(X_trans).A)","993","","994","    # check in conjunction with subsampling","995","    transformer = QuantileTransformer(ignore_implicit_zeros=True,","996","                                      n_quantiles=5,","997","                                      subsample=8,","998","                                      random_state=0)","999","    X_trans = transformer.fit_transform(X_sparse)","1000","    assert_almost_equal(X_expected, X_trans.A)","1001","    assert_almost_equal(X_sparse.A, transformer.inverse_transform(X_trans).A)","1002","","1003","","1004","def test_quantile_transform_dense_toy():","1005","    X = np.array([[0, 2, 2.6],","1006","                  [25, 4, 4.1],","1007","                  [50, 6, 2.3],","1008","                  [75, 8, 9.5],","1009","                  [100, 10, 0.1]])","1010","","1011","    transformer = QuantileTransformer(n_quantiles=5)","1012","    transformer.fit(X)","1013","","1014","    # using the a uniform output, each entry of X should be map between 0 and 1","1015","    # and equally spaced","1016","    X_trans = transformer.fit_transform(X)","1017","    X_expected = np.tile(np.linspace(0, 1, num=5), (3, 1)).T","1018","    assert_almost_equal(np.sort(X_trans, axis=0), X_expected)","1019","","1020","    X_test = np.array([","1021","        [-1, 1, 0],","1022","        [101, 11, 10],","1023","    ])","1024","    X_expected = np.array([","1025","        [0, 0, 0],","1026","        [1, 1, 1],","1027","    ])","1028","    assert_array_almost_equal(transformer.transform(X_test), X_expected)","1029","","1030","    X_trans_inv = transformer.inverse_transform(X_trans)","1031","    assert_array_almost_equal(X, X_trans_inv)","1032","","1033","","1034","def test_quantile_transform_subsampling():","1035","    # Test that subsampling the input yield to a consistent results We check","1036","    # that the computed quantiles are almost mapped to a [0, 1] vector where","1037","    # values are equally spaced. The infinite norm is checked to be smaller","1038","    # than a given threshold. This is repeated 5 times.","1039","","1040","    # dense support","1041","    n_samples = 1000000","1042","    n_quantiles = 1000","1043","    X = np.sort(np.random.sample((n_samples, 1)), axis=0)","1044","    ROUND = 5","1045","    inf_norm_arr = []","1046","    for random_state in range(ROUND):","1047","        transformer = QuantileTransformer(random_state=random_state,","1048","                                          n_quantiles=n_quantiles,","1049","                                          subsample=n_samples \/\/ 10)","1050","        transformer.fit(X)","1051","        diff = (np.linspace(0, 1, n_quantiles) -","1052","                np.ravel(transformer.quantiles_))","1053","        inf_norm = np.max(np.abs(diff))","1054","        assert_true(inf_norm < 1e-2)","1055","        inf_norm_arr.append(inf_norm)","1056","    # each random subsampling yield a unique approximation to the expected","1057","    # linspace CDF","1058","    assert_equal(len(np.unique(inf_norm_arr)), len(inf_norm_arr))","1059","","1060","    # sparse support","1061","","1062","    # TODO: rng should be seeded once we drop support for older versions of","1063","    # scipy (< 0.13) that don't support seeding.","1064","    X = sparse.rand(n_samples, 1, density=.99, format='csc')","1065","    inf_norm_arr = []","1066","    for random_state in range(ROUND):","1067","        transformer = QuantileTransformer(random_state=random_state,","1068","                                          n_quantiles=n_quantiles,","1069","                                          subsample=n_samples \/\/ 10)","1070","        transformer.fit(X)","1071","        diff = (np.linspace(0, 1, n_quantiles) -","1072","                np.ravel(transformer.quantiles_))","1073","        inf_norm = np.max(np.abs(diff))","1074","        assert_true(inf_norm < 1e-1)","1075","        inf_norm_arr.append(inf_norm)","1076","    # each random subsampling yield a unique approximation to the expected","1077","    # linspace CDF","1078","    assert_equal(len(np.unique(inf_norm_arr)), len(inf_norm_arr))","1079","","1080","","1081","def test_quantile_transform_sparse_toy():","1082","    X = np.array([[0., 2., 0.],","1083","                  [25., 4., 0.],","1084","                  [50., 0., 2.6],","1085","                  [0., 0., 4.1],","1086","                  [0., 6., 0.],","1087","                  [0., 8., 0.],","1088","                  [75., 0., 2.3],","1089","                  [0., 10., 0.],","1090","                  [0., 0., 9.5],","1091","                  [100., 0., 0.1]])","1092","","1093","    X = sparse.csc_matrix(X)","1094","","1095","    transformer = QuantileTransformer(n_quantiles=10)","1096","    transformer.fit(X)","1097","","1098","    X_trans = transformer.fit_transform(X)","1099","    assert_array_almost_equal(np.min(X_trans.toarray(), axis=0), 0.)","1100","    assert_array_almost_equal(np.max(X_trans.toarray(), axis=0), 1.)","1101","","1102","    X_trans_inv = transformer.inverse_transform(X_trans)","1103","    assert_array_almost_equal(X.toarray(), X_trans_inv.toarray())","1104","","1105","    transformer_dense = QuantileTransformer(n_quantiles=10).fit(","1106","        X.toarray())","1107","","1108","    X_trans = transformer_dense.transform(X)","1109","    assert_array_almost_equal(np.min(X_trans.toarray(), axis=0), 0.)","1110","    assert_array_almost_equal(np.max(X_trans.toarray(), axis=0), 1.)","1111","","1112","    X_trans_inv = transformer_dense.inverse_transform(X_trans)","1113","    assert_array_almost_equal(X.toarray(), X_trans_inv.toarray())","1114","","1115","","1116","def test_quantile_transform_axis1():","1117","    X = np.array([[0, 25, 50, 75, 100],","1118","                  [2, 4, 6, 8, 10],","1119","                  [2.6, 4.1, 2.3, 9.5, 0.1]])","1120","","1121","    X_trans_a0 = quantile_transform(X.T, axis=0, n_quantiles=5)","1122","    X_trans_a1 = quantile_transform(X, axis=1, n_quantiles=5)","1123","    assert_array_almost_equal(X_trans_a0, X_trans_a1.T)","1124","","1125","","1126","def test_quantile_transform_bounds():","1127","    # Lower and upper bounds are manually mapped. We checked that in the case","1128","    # of a constant feature and binary feature, the bounds are properly mapped.","1129","    X_dense = np.array([[0, 0],","1130","                        [0, 0],","1131","                        [1, 0]])","1132","    X_sparse = sparse.csc_matrix(X_dense)","1133","","1134","    # check sparse and dense are consistent","1135","    X_trans = QuantileTransformer(n_quantiles=3,","1136","                                  random_state=0).fit_transform(X_dense)","1137","    assert_array_almost_equal(X_trans, X_dense)","1138","    X_trans_sp = QuantileTransformer(n_quantiles=3,","1139","                                     random_state=0).fit_transform(X_sparse)","1140","    assert_array_almost_equal(X_trans_sp.A, X_dense)","1141","    assert_array_almost_equal(X_trans, X_trans_sp.A)","1142","","1143","    # check the consistency of the bounds by learning on 1 matrix","1144","    # and transforming another","1145","    X = np.array([[0, 1],","1146","                  [0, 0.5],","1147","                  [1, 0]])","1148","    X1 = np.array([[0, 0.1],","1149","                   [0, 0.5],","1150","                   [1, 0.1]])","1151","    transformer = QuantileTransformer(n_quantiles=3).fit(X)","1152","    X_trans = transformer.transform(X1)","1153","    assert_array_almost_equal(X_trans, X1)","1154","","1155","    # check that values outside of the range learned will be mapped properly.","1156","    X = np.random.random((1000, 1))","1157","    transformer = QuantileTransformer()","1158","    transformer.fit(X)","1159","    assert_equal(transformer.transform(-10), transformer.transform(np.min(X)))","1160","    assert_equal(transformer.transform(10), transformer.transform(np.max(X)))","1161","    assert_equal(transformer.inverse_transform(-10),","1162","                 transformer.inverse_transform(","1163","                     np.min(transformer.references_)))","1164","    assert_equal(transformer.inverse_transform(10),","1165","                 transformer.inverse_transform(","1166","                     np.max(transformer.references_)))","1167","","1168","","1169","def test_quantile_transform_and_inverse():","1170","    # iris dataset","1171","    X = iris.data","1172","    transformer = QuantileTransformer(n_quantiles=1000, random_state=0)","1173","    X_trans = transformer.fit_transform(X)","1174","    X_trans_inv = transformer.inverse_transform(X_trans)","1175","    assert_array_almost_equal(X, X_trans_inv)","1176","","1177","","1968","","1969","","1970","def test_quantile_transform_valid_axis():","1971","    X = np.array([[0, 25, 50, 75, 100],","1972","                  [2, 4, 6, 8, 10],","1973","                  [2.6, 4.1, 2.3, 9.5, 0.1]])","1974","","1975","    assert_raises_regex(ValueError, \"axis should be either equal to 0 or 1\"","1976","                        \". Got axis=2\", quantile_transform, X.T, axis=2)"],"delete":["0","","143","    feature_names = poly.get_feature_names([u\"\\u0001F40D\", u\"\\u262E\", u\"\\u05D0\"])"]}],"sklearn\/preprocessing\/data.py":[{"add":["8","from __future__ import division","9","","17","from scipy import stats","29","from ..utils.validation import (check_is_fitted, check_random_state,","30","                                FLOAT_DTYPES)","31","BOUNDS_THRESHOLD = 1e-7","47","    'QuantileTransformer',","55","    'quantile_transform',","119","    See examples\/preprocessing\/plot_all_scaling.py for a comparison of the","120","    different scalers, transformers, and normalizers.","121","","126","","246","    minmax_scale: Equivalent function without the estimator API.","247","","248","    Notes","249","    -----","250","    See examples\/preprocessing\/plot_all_scaling.py for a comparison of the","251","    different scalers, transformers, and normalizers.","408","","409","    Notes","410","    -----","411","    See examples\/preprocessing\/plot_all_scaling.py for a comparison of the","412","    different scalers, transformers, and normalizers.","501","    scale: Equivalent function without the estimator API.","505","","506","    Notes","507","    -----","508","    See examples\/preprocessing\/plot_all_scaling.py for a comparison of the","509","    different scalers, transformers, and normalizers.","711","    maxabs_scale: Equivalent function without the estimator API.","712","","713","    Notes","714","    -----","715","    See examples\/preprocessing\/plot_all_scaling.py for a comparison of the","716","    different scalers, transformers, and normalizers.","844","","845","    Notes","846","    -----","847","    See examples\/preprocessing\/plot_all_scaling.py for a comparison of the","848","    different scalers, transformers, and normalizers.","933","    robust_scale: Equivalent function without the estimator API.","941","    See examples\/preprocessing\/plot_all_scaling.py for an example.","1091","    See examples\/preprocessing\/plot_all_scaling.py for a comparison of the","1092","    different scalers, transformers, and normalizers.","1093","","1310","","1311","    Notes","1312","    -----","1313","    See examples\/preprocessing\/plot_all_scaling.py for a comparison of the","1314","    different scalers, transformers, and normalizers.","1398","    See examples\/preprocessing\/plot_all_scaling.py for a comparison of the","1399","    different scalers, transformers, and normalizers.","1400","","1403","    normalize: Equivalent function without the estimator API.","1514","    binarize: Equivalent function without the estimator API.","1949","","1950","","1951","class QuantileTransformer(BaseEstimator, TransformerMixin):","1952","    \"\"\"Transform features using quantiles information.","1953","","1954","    This method transforms the features to follow a uniform or a normal","1955","    distribution. Therefore, for a given feature, this transformation tends","1956","    to spread out the most frequent values. It also reduces the impact of","1957","    (marginal) outliers: this is therefore a robust preprocessing scheme.","1958","","1959","    The transformation is applied on each feature independently.","1960","    The cumulative density function of a feature is used to project the","1961","    original values. Features values of new\/unseen data that fall below","1962","    or above the fitted range will be mapped to the bounds of the output","1963","    distribution. Note that this transform is non-linear. It may distort linear","1964","    correlations between variables measured at the same scale but renders","1965","    variables measured at different scales more directly comparable.","1966","","1967","    Read more in the :ref:`User Guide <preprocessing_transformer>`.","1968","","1969","    Parameters","1970","    ----------","1971","    n_quantiles : int, optional (default=1000)","1972","        Number of quantiles to be computed. It corresponds to the number","1973","        of landmarks used to discretize the cumulative density function.","1974","","1975","    output_distribution : str, optional (default='uniform')","1976","        Marginal distribution for the transformed data. The choices are","1977","        'uniform' (default) or 'normal'.","1978","","1979","    ignore_implicit_zeros : bool, optional (default=False)","1980","        Only applies to sparse matrices. If True, the sparse entries of the","1981","        matrix are discarded to compute the quantile statistics. If False,","1982","        these entries are treated as zeros.","1983","","1984","    subsample : int, optional (default=1e5)","1985","        Maximum number of samples used to estimate the quantiles for","1986","        computational efficiency. Note that the subsampling procedure may","1987","        differ for value-identical sparse and dense matrices.","1988","","1989","    random_state : int, RandomState instance or None, optional (default=None)","1990","        If int, random_state is the seed used by the random number generator;","1991","        If RandomState instance, random_state is the random number generator;","1992","        If None, the random number generator is the RandomState instance used","1993","        by np.random. Note that this is used by subsampling and smoothing","1994","        noise.","1995","","1996","    copy : boolean, optional, (default=True)","1997","        Set to False to perform inplace transformation and avoid a copy (if the","1998","        input is already a numpy array).","1999","","2000","    Attributes","2001","    ----------","2002","    quantiles_ : ndarray, shape (n_quantiles, n_features)","2003","        The values corresponding the quantiles of reference.","2004","","2005","    references_ : ndarray, shape(n_quantiles, )","2006","        Quantiles of references.","2007","","2008","    Examples","2009","    --------","2010","    >>> import numpy as np","2011","    >>> from sklearn.preprocessing import QuantileTransformer","2012","    >>> rng = np.random.RandomState(0)","2013","    >>> X = np.sort(rng.normal(loc=0.5, scale=0.25, size=(25, 1)), axis=0)","2014","    >>> qt = QuantileTransformer(n_quantiles=10, random_state=0)","2015","    >>> qt.fit_transform(X) # doctest: +ELLIPSIS","2016","    array([...])","2017","","2018","    See also","2019","    --------","2020","    quantile_transform : Equivalent function without the estimator API.","2021","    StandardScaler : perform standardization that is faster, but less robust","2022","        to outliers.","2023","    RobustScaler : perform robust standardization that removes the influence","2024","        of outliers but does not put outliers and inliers on the same scale.","2025","","2026","    Notes","2027","    -----","2028","    See examples\/preprocessing\/plot_all_scaling.py for a comparison of the","2029","    different scalers, transformers, and normalizers.","2030","","2031","    \"\"\"","2032","","2033","    def __init__(self, n_quantiles=1000, output_distribution='uniform',","2034","                 ignore_implicit_zeros=False, subsample=int(1e5),","2035","                 random_state=None, copy=True):","2036","        self.n_quantiles = n_quantiles","2037","        self.output_distribution = output_distribution","2038","        self.ignore_implicit_zeros = ignore_implicit_zeros","2039","        self.subsample = subsample","2040","        self.random_state = random_state","2041","        self.copy = copy","2042","","2043","    def _dense_fit(self, X, random_state):","2044","        \"\"\"Compute percentiles for dense matrices.","2045","","2046","        Parameters","2047","        ----------","2048","        X : ndarray, shape (n_samples, n_features)","2049","            The data used to scale along the features axis.","2050","        \"\"\"","2051","        if self.ignore_implicit_zeros:","2052","            warnings.warn(\"'ignore_implicit_zeros' takes effect only with\"","2053","                          \" sparse matrix. This parameter has no effect.\")","2054","","2055","        n_samples, n_features = X.shape","2056","        # for compatibility issue with numpy<=1.8.X, references","2057","        # need to be a list scaled between 0 and 100","2058","        references = (self.references_ * 100).tolist()","2059","        self.quantiles_ = []","2060","        for col in X.T:","2061","            if self.subsample < n_samples:","2062","                subsample_idx = random_state.choice(n_samples,","2063","                                                    size=self.subsample,","2064","                                                    replace=False)","2065","                col = col.take(subsample_idx, mode='clip')","2066","            self.quantiles_.append(np.percentile(col, references))","2067","        self.quantiles_ = np.transpose(self.quantiles_)","2068","","2069","    def _sparse_fit(self, X, random_state):","2070","        \"\"\"Compute percentiles for sparse matrices.","2071","","2072","        Parameters","2073","        ----------","2074","        X : sparse matrix CSC, shape (n_samples, n_features)","2075","            The data used to scale along the features axis. The sparse matrix","2076","            needs to be nonnegative.","2077","        \"\"\"","2078","        n_samples, n_features = X.shape","2079","","2080","        # for compatibility issue with numpy<=1.8.X, references","2081","        # need to be a list scaled between 0 and 100","2082","        references = list(map(lambda x: x * 100, self.references_))","2083","        self.quantiles_ = []","2084","        for feature_idx in range(n_features):","2085","            column_nnz_data = X.data[X.indptr[feature_idx]:","2086","                                     X.indptr[feature_idx + 1]]","2087","            if len(column_nnz_data) > self.subsample:","2088","                column_subsample = (self.subsample * len(column_nnz_data) \/\/","2089","                                    n_samples)","2090","                if self.ignore_implicit_zeros:","2091","                    column_data = np.zeros(shape=column_subsample,","2092","                                           dtype=X.dtype)","2093","                else:","2094","                    column_data = np.zeros(shape=self.subsample, dtype=X.dtype)","2095","                column_data[:column_subsample] = random_state.choice(","2096","                    column_nnz_data, size=column_subsample, replace=False)","2097","            else:","2098","                if self.ignore_implicit_zeros:","2099","                    column_data = np.zeros(shape=len(column_nnz_data),","2100","                                           dtype=X.dtype)","2101","                else:","2102","                    column_data = np.zeros(shape=n_samples, dtype=X.dtype)","2103","                column_data[:len(column_nnz_data)] = column_nnz_data","2104","","2105","            if not column_data.size:","2106","                # if no nnz, an error will be raised for computing the","2107","                # quantiles. Force the quantiles to be zeros.","2108","                self.quantiles_.append([0] * len(references))","2109","            else:","2110","                self.quantiles_.append(","2111","                    np.percentile(column_data, references))","2112","        self.quantiles_ = np.transpose(self.quantiles_)","2113","","2114","    def fit(self, X, y=None):","2115","        \"\"\"Compute the quantiles used for transforming.","2116","","2117","        Parameters","2118","        ----------","2119","        X : ndarray or sparse matrix, shape (n_samples, n_features)","2120","            The data used to scale along the features axis. If a sparse","2121","            matrix is provided, it will be converted into a sparse","2122","            ``csc_matrix``. Additionally, the sparse matrix needs to be","2123","            nonnegative if `ignore_implicit_zeros` is False.","2124","","2125","        Returns","2126","        -------","2127","        self : object","2128","            Returns self","2129","        \"\"\"","2130","        if self.n_quantiles <= 0:","2131","            raise ValueError(\"Invalid value for 'n_quantiles': %d. \"","2132","                             \"The number of quantiles must be at least one.\"","2133","                             % self.n_quantiles)","2134","","2135","        if self.subsample <= 0:","2136","            raise ValueError(\"Invalid value for 'subsample': %d. \"","2137","                             \"The number of subsamples must be at least one.\"","2138","                             % self.subsample)","2139","","2140","        if self.n_quantiles > self.subsample:","2141","            raise ValueError(\"The number of quantiles cannot be greater than\"","2142","                             \" the number of samples used. Got {} quantiles\"","2143","                             \" and {} samples.\".format(self.n_quantiles,","2144","                                                       self.subsample))","2145","","2146","        X = self._check_inputs(X)","2147","        rng = check_random_state(self.random_state)","2148","","2149","        # Create the quantiles of reference","2150","        self.references_ = np.linspace(0, 1, self.n_quantiles,","2151","                                       endpoint=True)","2152","        if sparse.issparse(X):","2153","            self._sparse_fit(X, rng)","2154","        else:","2155","            self._dense_fit(X, rng)","2156","","2157","        return self","2158","","2159","    def _transform_col(self, X_col, quantiles, inverse):","2160","        \"\"\"Private function to transform a single feature\"\"\"","2161","","2162","        if self.output_distribution == 'normal':","2163","            output_distribution = 'norm'","2164","        else:","2165","            output_distribution = self.output_distribution","2166","        output_distribution = getattr(stats, output_distribution)","2167","","2168","        # older version of scipy do not handle tuple as fill_value","2169","        # clipping the value before transform solve the issue","2170","        if not inverse:","2171","            lower_bound_x = quantiles[0]","2172","            upper_bound_x = quantiles[-1]","2173","            lower_bound_y = 0","2174","            upper_bound_y = 1","2175","        else:","2176","            lower_bound_x = 0","2177","            upper_bound_x = 1","2178","            lower_bound_y = quantiles[0]","2179","            upper_bound_y = quantiles[-1]","2180","            #  for inverse transform, match a uniform PDF","2181","            X_col = output_distribution.cdf(X_col)","2182","        # find index for lower and higher bounds","2183","        lower_bounds_idx = (X_col - BOUNDS_THRESHOLD <","2184","                            lower_bound_x)","2185","        upper_bounds_idx = (X_col + BOUNDS_THRESHOLD >","2186","                            upper_bound_x)","2187","","2188","        if not inverse:","2189","            # Interpolate in one direction and in the other and take the","2190","            # mean. This is in case of repeated values in the features","2191","            # and hence repeated quantiles","2192","            #","2193","            # If we don't do this, only one extreme of the duplicated is","2194","            # used (the upper when we do assending, and the","2195","            # lower for descending). We take the mean of these two","2196","            X_col = .5 * (np.interp(X_col, quantiles, self.references_)","2197","                          - np.interp(-X_col, -quantiles[::-1],","2198","                                      -self.references_[::-1]))","2199","        else:","2200","            X_col = np.interp(X_col, self.references_, quantiles)","2201","","2202","        X_col[upper_bounds_idx] = upper_bound_y","2203","        X_col[lower_bounds_idx] = lower_bound_y","2204","        # for forward transform, match the output PDF","2205","        if not inverse:","2206","            X_col = output_distribution.ppf(X_col)","2207","            # find the value to clip the data to avoid mapping to","2208","            # infinity. Clip such that the inverse transform will be","2209","            # consistent","2210","            clip_min = output_distribution.ppf(BOUNDS_THRESHOLD -","2211","                                               np.spacing(1))","2212","            clip_max = output_distribution.ppf(1 - (BOUNDS_THRESHOLD -","2213","                                                    np.spacing(1)))","2214","            X_col = np.clip(X_col, clip_min, clip_max)","2215","","2216","        return X_col","2217","","2218","    def _check_inputs(self, X, accept_sparse_negative=False):","2219","        \"\"\"Check inputs before fit and transform\"\"\"","2220","        X = check_array(X, accept_sparse='csc', copy=self.copy,","2221","                        dtype=[np.float64, np.float32])","2222","        # we only accept positive sparse matrix when ignore_implicit_zeros is","2223","        # false and that we call fit or transform.","2224","        if (not accept_sparse_negative and not self.ignore_implicit_zeros and","2225","                (sparse.issparse(X) and np.any(X.data < 0))):","2226","            raise ValueError('QuantileTransformer only accepts non-negative'","2227","                             ' sparse matrices.')","2228","","2229","        # check the output PDF","2230","        if self.output_distribution not in ('normal', 'uniform'):","2231","            raise ValueError(\"'output_distribution' has to be either 'normal'\"","2232","                             \" or 'uniform'. Got '{}' instead.\".format(","2233","                                 self.output_distribution))","2234","","2235","        return X","2236","","2237","    def _check_is_fitted(self, X):","2238","        \"\"\"Check the inputs before transforming\"\"\"","2239","        check_is_fitted(self, 'quantiles_')","2240","        # check that the dimension of X are adequate with the fitted data","2241","        if X.shape[1] != self.quantiles_.shape[1]:","2242","            raise ValueError('X does not have the same number of features as'","2243","                             ' the previously fitted data. Got {} instead of'","2244","                             ' {}.'.format(X.shape[1],","2245","                                           self.quantiles_.shape[1]))","2246","","2247","    def _transform(self, X, inverse=False):","2248","        \"\"\"Forward and inverse transform.","2249","","2250","        Parameters","2251","        ----------","2252","        X : ndarray, shape (n_samples, n_features)","2253","            The data used to scale along the features axis.","2254","","2255","        inverse : bool, optional (default=False)","2256","            If False, apply forward transform. If True, apply","2257","            inverse transform.","2258","","2259","        Returns","2260","        -------","2261","        X : ndarray, shape (n_samples, n_features)","2262","            Projected data","2263","        \"\"\"","2264","","2265","        if sparse.issparse(X):","2266","            for feature_idx in range(X.shape[1]):","2267","                column_slice = slice(X.indptr[feature_idx],","2268","                                     X.indptr[feature_idx + 1])","2269","                X.data[column_slice] = self._transform_col(","2270","                    X.data[column_slice], self.quantiles_[:, feature_idx],","2271","                    inverse)","2272","        else:","2273","            for feature_idx in range(X.shape[1]):","2274","                X[:, feature_idx] = self._transform_col(","2275","                    X[:, feature_idx], self.quantiles_[:, feature_idx],","2276","                    inverse)","2277","","2278","        return X","2279","","2280","    def transform(self, X):","2281","        \"\"\"Feature-wise transformation of the data.","2282","","2283","        Parameters","2284","        ----------","2285","        X : ndarray or sparse matrix, shape (n_samples, n_features)","2286","            The data used to scale along the features axis. If a sparse","2287","            matrix is provided, it will be converted into a sparse","2288","            ``csc_matrix``. Additionally, the sparse matrix needs to be","2289","            nonnegative if `ignore_implicit_zeros` is False.","2290","","2291","        Returns","2292","        -------","2293","        Xt : ndarray or sparse matrix, shape (n_samples, n_features)","2294","            The projected data.","2295","        \"\"\"","2296","        X = self._check_inputs(X)","2297","        self._check_is_fitted(X)","2298","","2299","        return self._transform(X, inverse=False)","2300","","2301","    def inverse_transform(self, X):","2302","        \"\"\"Back-projection to the original space.","2303","","2304","        X : ndarray or sparse matrix, shape (n_samples, n_features)","2305","            The data used to scale along the features axis. If a sparse","2306","            matrix is provided, it will be converted into a sparse","2307","            ``csc_matrix``. Additionally, the sparse matrix needs to be","2308","            nonnegative if `ignore_implicit_zeros` is False.","2309","","2310","        Returns","2311","        -------","2312","        Xt : ndarray or sparse matrix, shape (n_samples, n_features)","2313","            The projected data.","2314","        \"\"\"","2315","        X = self._check_inputs(X, accept_sparse_negative=True)","2316","        self._check_is_fitted(X)","2317","","2318","        return self._transform(X, inverse=True)","2319","","2320","","2321","def quantile_transform(X, axis=0, n_quantiles=1000,","2322","                       output_distribution='uniform',","2323","                       ignore_implicit_zeros=False,","2324","                       subsample=int(1e5),","2325","                       random_state=None,","2326","                       copy=False):","2327","    \"\"\"Transform features using quantiles information.","2328","","2329","    This method transforms the features to follow a uniform or a normal","2330","    distribution. Therefore, for a given feature, this transformation tends","2331","    to spread out the most frequent values. It also reduces the impact of","2332","    (marginal) outliers: this is therefore a robust preprocessing scheme.","2333","","2334","    The transformation is applied on each feature independently.","2335","    The cumulative density function of a feature is used to project the","2336","    original values. Features values of new\/unseen data that fall below","2337","    or above the fitted range will be mapped to the bounds of the output","2338","    distribution. Note that this transform is non-linear. It may distort linear","2339","    correlations between variables measured at the same scale but renders","2340","    variables measured at different scales more directly comparable.","2341","","2342","    Read more in the :ref:`User Guide <preprocessing_transformer>`.","2343","","2344","    Parameters","2345","    ----------","2346","    X : array-like, sparse matrix","2347","        The data to transform.","2348","","2349","    axis : int, (default=0)","2350","        Axis used to compute the means and standard deviations along. If 0,","2351","        transform each feature, otherwise (if 1) transform each sample.","2352","","2353","    n_quantiles : int, optional (default=1000)","2354","        Number of quantiles to be computed. It corresponds to the number","2355","        of landmarks used to discretize the cumulative density function.","2356","","2357","    output_distribution : str, optional (default='uniform')","2358","        Marginal distribution for the transformed data. The choices are","2359","        'uniform' (default) or 'normal'.","2360","","2361","    ignore_implicit_zeros : bool, optional (default=False)","2362","        Only applies to sparse matrices. If True, the sparse entries of the","2363","        matrix are discarded to compute the quantile statistics. If False,","2364","        these entries are treated as zeros.","2365","","2366","    subsample : int, optional (default=1e5)","2367","        Maximum number of samples used to estimate the quantiles for","2368","        computational efficiency. Note that the subsampling procedure may","2369","        differ for value-identical sparse and dense matrices.","2370","","2371","    random_state : int, RandomState instance or None, optional (default=None)","2372","        If int, random_state is the seed used by the random number generator;","2373","        If RandomState instance, random_state is the random number generator;","2374","        If None, the random number generator is the RandomState instance used","2375","        by np.random. Note that this is used by subsampling and smoothing","2376","        noise.","2377","","2378","    copy : boolean, optional, (default=True)","2379","        Set to False to perform inplace transformation and avoid a copy (if the","2380","        input is already a numpy array).","2381","","2382","    Attributes","2383","    ----------","2384","    quantiles_ : ndarray, shape (n_quantiles, n_features)","2385","        The values corresponding the quantiles of reference.","2386","","2387","    references_ : ndarray, shape(n_quantiles, )","2388","        Quantiles of references.","2389","","2390","    Examples","2391","    --------","2392","    >>> import numpy as np","2393","    >>> from sklearn.preprocessing import quantile_transform","2394","    >>> rng = np.random.RandomState(0)","2395","    >>> X = np.sort(rng.normal(loc=0.5, scale=0.25, size=(25, 1)), axis=0)","2396","    >>> quantile_transform(X, n_quantiles=10, random_state=0)","2397","    ... # doctest: +ELLIPSIS","2398","    array([...])","2399","","2400","    See also","2401","    --------","2402","    QuantileTransformer : Performs quantile-based scaling using the","2403","        ``Transformer`` API (e.g. as part of a preprocessing","2404","        :class:`sklearn.pipeline.Pipeline`).","2405","    scale : perform standardization that is faster, but less robust","2406","        to outliers.","2407","    robust_scale : perform robust standardization that removes the influence","2408","        of outliers but does not put outliers and inliers on the same scale.","2409","","2410","    Notes","2411","    -----","2412","    See examples\/preprocessing\/plot_all_scaling.py for a comparison of the","2413","    different scalers, transformers, and normalizers.","2414","","2415","    \"\"\"","2416","    n = QuantileTransformer(n_quantiles=n_quantiles,","2417","                            output_distribution=output_distribution,","2418","                            subsample=subsample,","2419","                            ignore_implicit_zeros=ignore_implicit_zeros,","2420","                            random_state=random_state,","2421","                            copy=copy)","2422","    if axis == 0:","2423","        return n.fit_transform(X)","2424","    elif axis == 1:","2425","        return n.fit_transform(X.T).T","2426","    else:","2427","        raise ValueError(\"axis should be either equal to 0 or 1. Got\"","2428","                         \" axis={}\".format(axis))"],"delete":["26","from ..utils.validation import check_is_fitted, FLOAT_DTYPES","235","    minmax_scale: Equivalent function without the object oriented API.","480","    scale: Equivalent function without the object oriented API.","685","    maxabs_scale: Equivalent function without the object oriented API.","897","    robust_scale: Equivalent function without the object oriented API.","905","    See examples\/preprocessing\/plot_robust_scaling.py for an example.","1356","    normalize: Equivalent function without the object oriented API.","1467","    binarize: Equivalent function without the object oriented API."]}],"build_tools\/travis\/flake8_diff.sh":[{"add":["139","    check_files \"$(echo \"$MODIFIED_FILES\" | grep -v ^examples)\" --ignore=W503","141","    check_files \"$(echo \"$MODIFIED_FILES\" | grep ^examples)\" --ignore=E402,W503"],"delete":["139","    check_files \"$(echo \"$MODIFIED_FILES\" | grep -v ^examples)\"","141","    check_files \"$(echo \"$MODIFIED_FILES\" | grep ^examples)\" --ignore=E402"]}]}},"4d31e55b0ea86c3b00276337369cda463b5bb798":{"changes":{"sklearn\/model_selection\/_search.py":"MODIFY"},"diff":{"sklearn\/model_selection\/_search.py":[{"add":["820","    >>> svc = svm.SVC()","821","    >>> clf = GridSearchCV(svc, parameters)"],"delete":["820","    >>> svr = svm.SVC()","821","    >>> clf = GridSearchCV(svr, parameters)"]}]}},"38adb27bf1f977364cbead9393ca5887986fad93":{"changes":{"sklearn\/manifold\/tests\/test_t_sne.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/manifold\/t_sne.py":"MODIFY"},"diff":{"sklearn\/manifold\/tests\/test_t_sne.py":[{"add":["627","","628","","629","def test_accessible_kl_divergence():","630","    # Ensures that the accessible kl_divergence matches the computed value","631","    random_state = check_random_state(0)","632","    X = random_state.randn(100, 2)","633","    tsne = TSNE(n_iter_without_progress=2, verbose=2,","634","                random_state=0, method='exact')","635","","636","    old_stdout = sys.stdout","637","    sys.stdout = StringIO()","638","    try:","639","        tsne.fit_transform(X)","640","    finally:","641","        out = sys.stdout.getvalue()","642","        sys.stdout.close()","643","        sys.stdout = old_stdout","644","","645","    # The output needs to contain the accessible kl_divergence as the error at","646","    # the last iteration","647","    for line in out.split('\\n')[::-1]:","648","        if 'Iteration' in line:","649","            _, _, error = line.partition('error = ')","650","            if error:","651","                error, _, _ = error.partition(',')","652","                break","653","    assert_almost_equal(tsne.kl_divergence_, float(error), decimal=5)"],"delete":[]}],"doc\/whats_new.rst":[{"add":["254","   - Fixed a bug in :class:`manifold.TSNE` where it stored the incorrect","255","     ``kl_divergence_``. :issue:`6507` by :user:`Sebastian Saeger <ssaeger>`.","256","","5036",".. _Anish Shah: https:\/\/github.com\/AnishShah"],"delete":[]}],"sklearn\/manifold\/t_sne.py":[{"add":["866","        params, kl_divergence, it = _gradient_descent(obj_func, params,","867","                                                      **opt_args)"],"delete":["866","        params, error, it = _gradient_descent(obj_func, params, **opt_args)"]}]}},"bae38a2f5fb59b32a9501279f452178bc3f7f789":{"changes":{"doc\/tutorial\/text_analytics\/working_with_text_data.rst":"MODIFY"},"diff":{"doc\/tutorial\/text_analytics\/working_with_text_data.rst":[{"add":["326","  >>> text_clf.fit(twenty_train.data, twenty_train.target)  # doctest: +ELLIPSIS","327","  Pipeline(...)","356","  >>> text_clf.fit(twenty_train.data, twenty_train.target)  # doctest: +ELLIPSIS","357","  Pipeline(...)"],"delete":["326","  >>> text_clf = text_clf.fit(twenty_train.data, twenty_train.target)","355","  >>> _ = text_clf.fit(twenty_train.data, twenty_train.target)"]}]}},"6413febf8d2967a9f704f3e90cdb8160f98adce7":{"changes":{"sklearn\/tests\/test_grid_search.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/grid_search.py":"MODIFY"},"diff":{"sklearn\/tests\/test_grid_search.py":[{"add":["73","    def transform(self, X):","74","        return X - self.foo_param","75","","76","    def inverse_transform(self, X):","77","        return X + self.foo_param","78","","173","def test_transform_inverse_transform_round_trip():","174","    clf = MockClassifier()","175","    grid_search = GridSearchCV(clf, {'foo_param': [1, 2, 3]}, verbose=3)","176","    grid_search.fit(X, y)","177","    X_round_trip = grid_search.inverse_transform(grid_search.transform(X))","178","    assert_array_equal(X, X_round_trip)","179","","180",""],"delete":["75","    transform = predict"]}],"doc\/whats_new.rst":[{"add":["191","   - Fixed same issue in :func:`sklearn.grid_search.BaseSearchCV.inverse_transform`","192","     :issue:`8846` by :user:`Rasmus Eriksson <MrMjauh>`"],"delete":[]}],"sklearn\/grid_search.py":[{"add":["542","        return self.best_estimator_.inverse_transform(Xt)"],"delete":["542","        return self.best_estimator_.transform(Xt)"]}]}},"fcdbaca5a91896d8aa6b395a0a9db852753a029e":{"changes":{"sklearn\/decomposition\/tests\/test_pca.py":"MODIFY"},"diff":{"sklearn\/decomposition\/tests\/test_pca.py":[{"add":["576","def test_pca_sparse_input():"],"delete":["576","def test_pca_spase_input():","577",""]}]}},"4493d37058938c85f78b40562e2741812b983291":{"changes":{"sklearn\/base.py":"MODIFY","sklearn\/tests\/test_base.py":"MODIFY","doc\/whats_new.rst":"MODIFY"},"diff":{"sklearn\/base.py":[{"add":["292","        try:","293","            state = super(BaseEstimator, self).__getstate__()","294","        except AttributeError:","295","            state = self.__dict__.copy()","296","","298","            return dict(state.items(), _sklearn_version=__version__)","300","            return state","312","        try:","313","            super(BaseEstimator, self).__setstate__(state)","314","        except AttributeError:","315","            self.__dict__.update(state)","316",""],"delete":["293","            return dict(self.__dict__.items(), _sklearn_version=__version__)","295","            return dict(self.__dict__.items())","307","        self.__dict__.update(state)"]}],"sklearn\/tests\/test_base.py":[{"add":["15","from sklearn.utils.testing import assert_dict_equal","315","def test_pickle_version_warning_is_not_raised_with_matching_version():","316","    iris = datasets.load_iris()","317","    tree = DecisionTreeClassifier().fit(iris.data, iris.target)","318","    tree_pickle = pickle.dumps(tree)","319","    assert_true(b\"version\" in tree_pickle)","320","    tree_restored = assert_no_warnings(pickle.loads, tree_pickle)","321","","322","    # test that we can predict with the restored decision tree classifier","323","    score_of_original = tree.score(iris.data, iris.target)","324","    score_of_restored = tree_restored.score(iris.data, iris.target)","325","    assert_equal(score_of_original, score_of_restored)","333","pickle_error_message = (","334","    \"Trying to unpickle estimator {estimator} from \"","335","    \"version {old_version} when using version \"","336","    \"{current_version}. This might \"","337","    \"lead to breaking code or invalid results. \"","338","    \"Use at your own risk.\")","340","","341","def test_pickle_version_warning_is_issued_upon_different_version():","345","    message = pickle_error_message.format(estimator=\"TreeBadVersion\",","346","                                          old_version=\"something\",","347","                                          current_version=sklearn.__version__)","350","","351","class TreeNoVersion(DecisionTreeClassifier):","352","    def __getstate__(self):","353","        return self.__dict__","354","","355","","356","def test_pickle_version_warning_is_issued_when_no_version_info_in_pickle():","357","    iris = datasets.load_iris()","363","    message = pickle_error_message.format(estimator=\"TreeNoVersion\",","364","                                          old_version=\"pre-0.18\",","365","                                          current_version=sklearn.__version__)","370","","371","def test_pickle_version_no_warning_is_issued_with_non_sklearn_estimator():","372","    iris = datasets.load_iris()","373","    tree = TreeNoVersion().fit(iris.data, iris.target)","374","    tree_pickle_noversion = pickle.dumps(tree)","375","    try:","376","        module_backup = TreeNoVersion.__module__","377","        TreeNoVersion.__module__ = \"notsklearn\"","378","        assert_no_warnings(pickle.loads, tree_pickle_noversion)","379","    finally:","380","        TreeNoVersion.__module__ = module_backup","381","","382","","383","class DontPickleAttributeMixin(object):","384","    def __getstate__(self):","385","        data = self.__dict__.copy()","386","        data[\"_attribute_not_pickled\"] = None","387","        return data","388","","389","    def __setstate__(self, state):","390","        state[\"_restored\"] = True","391","        self.__dict__.update(state)","392","","393","","394","class MultiInheritanceEstimator(BaseEstimator, DontPickleAttributeMixin):","395","    def __init__(self, attribute_pickled=5):","396","        self.attribute_pickled = attribute_pickled","397","        self._attribute_not_pickled = None","398","","399","","400","def test_pickling_when_getstate_is_overwritten_by_mixin():","401","    estimator = MultiInheritanceEstimator()","402","    estimator._attribute_not_pickled = \"this attribute should not be pickled\"","403","","404","    serialized = pickle.dumps(estimator)","405","    estimator_restored = pickle.loads(serialized)","406","    assert_equal(estimator_restored.attribute_pickled, 5)","407","    assert_equal(estimator_restored._attribute_not_pickled, None)","408","    assert_true(estimator_restored._restored)","409","","410","","411","def test_pickling_when_getstate_is_overwritten_by_mixin_outside_of_sklearn():","412","    try:","413","        estimator = MultiInheritanceEstimator()","414","        text = \"this attribute should not be pickled\"","415","        estimator._attribute_not_pickled = text","416","        old_mod = type(estimator).__module__","417","        type(estimator).__module__ = \"notsklearn\"","418","","419","        serialized = estimator.__getstate__()","420","        assert_dict_equal(serialized, {'_attribute_not_pickled': None,","421","                                       'attribute_pickled': 5})","422","","423","        serialized['attribute_pickled'] = 4","424","        estimator.__setstate__(serialized)","425","        assert_equal(estimator.attribute_pickled, 4)","426","        assert_true(estimator._restored)","427","    finally:","428","        type(estimator).__module__ = old_mod","429","","430","","431","class SingleInheritanceEstimator(BaseEstimator):","432","    def __init__(self, attribute_pickled=5):","433","        self.attribute_pickled = attribute_pickled","434","        self._attribute_not_pickled = None","435","","436","    def __getstate__(self):","437","        data = self.__dict__.copy()","438","        data[\"_attribute_not_pickled\"] = None","439","        return data","440","","441","","442","def test_pickling_works_when_getstate_is_overwritten_in_the_child_class():","443","    estimator = SingleInheritanceEstimator()","444","    estimator._attribute_not_pickled = \"this attribute should not be pickled\"","445","","446","    serialized = pickle.dumps(estimator)","447","    estimator_restored = pickle.loads(serialized)","448","    assert_equal(estimator_restored.attribute_pickled, 5)","449","    assert_equal(estimator_restored._attribute_not_pickled, None)"],"delete":["3","import sys","4","","316","class TreeNoVersion(DecisionTreeClassifier):","317","    def __getstate__(self):","318","        return self.__dict__","326","def test_pickle_version_warning():","327","    # check that warnings are raised when unpickling in a different version","329","    # first, check no warning when in the same version:","331","    tree = DecisionTreeClassifier().fit(iris.data, iris.target)","332","    tree_pickle = pickle.dumps(tree)","333","    assert_true(b\"version\" in tree_pickle)","334","    assert_no_warnings(pickle.loads, tree_pickle)","335","","336","    # check that warning is raised on different version","339","    message = (\"Trying to unpickle estimator TreeBadVersion from \"","340","               \"version {0} when using version {1}. This might lead to \"","341","               \"breaking code or invalid results. \"","342","               \"Use at your own risk.\".format(\"something\",","343","                                              sklearn.__version__))","346","    # check that not including any version also works:","352","    message = message.replace(\"something\", \"pre-0.18\")","353","    message = message.replace(\"TreeBadVersion\", \"TreeNoVersion\")","358","    # check that no warning is raised for external estimators","359","    TreeNoVersion.__module__ = \"notsklearn\"","360","    assert_no_warnings(pickle.loads, tree_pickle_noversion)"]}],"doc\/whats_new.rst":[{"add":["225","   - Fix a bug where :meth:`sklearn.base.BaseEstimator.__getstate__`","226","     obstructed pickling customizations of child-classes, when used in a","227","     multiple inheritance context.","228","     :issue:`8316` by :user:`Holger Peters <HolgerPeters>`."],"delete":[]}]}},"673baeab9e65231489c73d5e0c8c3f04d44893c3":{"changes":{"sklearn\/__init__.py":"MODIFY"},"diff":{"sklearn\/__init__.py":[{"add":["93","                        module=r'^{0}\\.'.format(re.escape(__name__)))"],"delete":["93","                        module='^{0}\\.'.format(re.escape(__name__)))"]}]}},"e3c9ae204ffb152c151e9b61306ff8f16a2c1e0a":{"changes":{"sklearn\/gaussian_process\/gpc.py":"MODIFY","sklearn\/mixture\/bayesian_mixture.py":"MODIFY","sklearn\/random_projection.py":"MODIFY","sklearn\/gaussian_process\/gpr.py":"MODIFY","sklearn\/svm\/classes.py":"MODIFY","sklearn\/metrics\/cluster\/unsupervised.py":"MODIFY","sklearn\/ensemble\/base.py":"MODIFY","sklearn\/linear_model\/passive_aggressive.py":"MODIFY","sklearn\/model_selection\/_search.py":"MODIFY","sklearn\/linear_model\/ransac.py":"MODIFY","sklearn\/mixture\/gmm.py":"MODIFY","sklearn\/cross_validation.py":"MODIFY","sklearn\/datasets\/olivetti_faces.py":"MODIFY","sklearn\/linear_model\/logistic.py":"MODIFY","sklearn\/utils\/__init__.py":"MODIFY","sklearn\/cluster\/mean_shift_.py":"MODIFY","sklearn\/model_selection\/_split.py":"MODIFY","sklearn\/linear_model\/perceptron.py":"MODIFY","sklearn\/neighbors\/kde.py":"MODIFY","sklearn\/feature_selection\/mutual_info_.py":"MODIFY","sklearn\/svm\/base.py":"MODIFY","sklearn\/multiclass.py":"MODIFY","sklearn\/linear_model\/randomized_l1.py":"MODIFY","sklearn\/grid_search.py":"MODIFY","sklearn\/mixture\/gaussian_mixture.py":"MODIFY","sklearn\/decomposition\/factor_analysis.py":"MODIFY","sklearn\/linear_model\/coordinate_descent.py":"MODIFY","sklearn\/decomposition\/online_lda.py":"MODIFY","sklearn\/manifold\/mds.py":"MODIFY","sklearn\/linear_model\/sag.py":"MODIFY","sklearn\/cluster\/spectral.py":"MODIFY","sklearn\/decomposition\/nmf.py":"MODIFY","sklearn\/covariance\/robust_covariance.py":"MODIFY","sklearn\/decomposition\/fastica_.py":"MODIFY","sklearn\/kernel_approximation.py":"MODIFY","sklearn\/manifold\/spectral_embedding_.py":"MODIFY","sklearn\/utils\/extmath.py":"MODIFY","sklearn\/feature_extraction\/image.py":"MODIFY","sklearn\/neural_network\/multilayer_perceptron.py":"MODIFY","sklearn\/decomposition\/pca.py":"MODIFY","sklearn\/linear_model\/ridge.py":"MODIFY","sklearn\/linear_model\/theil_sen.py":"MODIFY","sklearn\/decomposition\/truncated_svd.py":"MODIFY","sklearn\/datasets\/samples_generator.py":"MODIFY","sklearn\/gaussian_process\/gaussian_process.py":"MODIFY","sklearn\/cluster\/bicluster.py":"MODIFY","sklearn\/linear_model\/stochastic_gradient.py":"MODIFY","sklearn\/model_selection\/_validation.py":"MODIFY","sklearn\/decomposition\/dict_learning.py":"MODIFY","sklearn\/ensemble\/gradient_boosting.py":"MODIFY","sklearn\/dummy.py":"MODIFY","sklearn\/manifold\/t_sne.py":"MODIFY","sklearn\/decomposition\/sparse_pca.py":"MODIFY","sklearn\/manifold\/locally_linear.py":"MODIFY","sklearn\/decomposition\/kernel_pca.py":"MODIFY","sklearn\/cluster\/k_means_.py":"MODIFY"},"diff":{"sklearn\/gaussian_process\/gpc.py":[{"add":["108","    random_state : int, RandomState instance or None, optional (default: None)","109","        The generator used to initialize the centers. If int, random_state is","110","        the seed used by the random number generator; If RandomState instance,","111","        random_state is the random number generator; If None, the random number","112","        generator is the RandomState instance used by `np.random`.","513","    random_state : int, RandomState instance or None, optional (default: None)","514","        The generator used to initialize the centers.","515","        If int, random_state is the seed used by the random number generator;","516","        If RandomState instance, random_state is the random number generator;","517","        If None, the random number generator is the RandomState instance used","518","        by `np.random`."],"delete":["108","    random_state : integer or numpy.RandomState, optional","109","        The generator used to initialize the centers. If an integer is","110","        given, it fixes the seed. Defaults to the global numpy random","111","        number generator.","512","    random_state : integer or numpy.RandomState, optional","513","        The generator used to initialize the centers. If an integer is","514","        given, it fixes the seed. Defaults to the global numpy random","515","        number generator."]}],"sklearn\/mixture\/bayesian_mixture.py":[{"add":["165","    random_state : int, RandomState instance or None, optional (default=None)","166","        If int, random_state is the seed used by the random number generator;","167","        If RandomState instance, random_state is the random number generator;","168","        If None, the random number generator is the RandomState instance used","169","        by `np.random`."],"delete":["165","    random_state : RandomState or an int seed, defaults to None.","166","        A random number generator instance."]}],"sklearn\/random_projection.py":[{"add":["156","    \"\"\"Generate a dense Gaussian random matrix.","172","    random_state : int, RandomState instance or None, optional (default=None)","173","        Control the pseudo random number generator used to generate the matrix","174","        at fit time.  If int, random_state is the seed used by the random","175","        number generator; If RandomState instance, random_state is the random","176","        number generator; If None, the random number generator is the","177","        RandomState instance used by `np.random`.","231","    random_state : int, RandomState instance or None, optional (default=None)","232","        Control the pseudo random number generator used to generate the matrix","233","        at fit time.  If int, random_state is the seed used by the random","234","        number generator; If RandomState instance, random_state is the random","235","        number generator; If None, the random number generator is the","236","        RandomState instance used by `np.random`.","454","    random_state : int, RandomState instance or None, optional (default=None)","455","        Control the pseudo random number generator used to generate the matrix","456","        at fit time.  If int, random_state is the seed used by the random","457","        number generator; If RandomState instance, random_state is the random","458","        number generator; If None, the random number generator is the","459","        RandomState instance used by `np.random`.","563","    random_state : int, RandomState instance or None, optional (default=None)","564","        Control the pseudo random number generator used to generate the matrix","565","        at fit time.  If int, random_state is the seed used by the random","566","        number generator; If RandomState instance, random_state is the random","567","        number generator; If None, the random number generator is the","568","        RandomState instance used by `np.random`."],"delete":["156","    \"\"\" Generate a dense Gaussian random matrix.","172","    random_state : int, RandomState instance or None (default=None)","173","        Control the pseudo random number generator used to generate the","174","        matrix at fit time.","228","    random_state : integer, RandomState instance or None (default=None)","229","        Control the pseudo random number generator used to generate the","230","        matrix at fit time.","448","    random_state : integer, RandomState instance or None (default=None)","449","        Control the pseudo random number generator used to generate the","450","        matrix at fit time.","554","    random_state : integer, RandomState instance or None (default=None)","555","        Control the pseudo random number generator used to generate the","556","        matrix at fit time."]}],"sklearn\/gaussian_process\/gpr.py":[{"add":["105","    random_state : int, RandomState instance or None, optional (default: None)","106","        The generator used to initialize the centers. If int, random_state is","107","        the seed used by the random number generator; If RandomState instance,","108","        random_state is the random number generator; If None, the random number","109","        generator is the RandomState instance used by `np.random`.","339","        random_state : int, RandomState instance or None, optional (default=0)","340","            If int, random_state is the seed used by the random number","341","            generator; If RandomState instance, random_state is the","342","            random number generator; If None, the random number","343","            generator is the RandomState instance used by `np.random`."],"delete":["105","    random_state : integer or numpy.RandomState, optional","106","        The generator used to initialize the centers. If an integer is","107","        given, it fixes the seed. Defaults to the global numpy random","108","        number generator.","338","        random_state : RandomState or an int seed (0 by default)","339","            A random number generator instance"]}],"sklearn\/svm\/classes.py":[{"add":["88","    random_state : int, RandomState instance or None, optional (default=None)","89","        The seed of the pseudo random number generator to use when shuffling","90","        the data.  If int, random_state is the seed used by the random number","91","        generator; If RandomState instance, random_state is the random number","92","        generator; If None, the random number generator is the RandomState","93","        instance used by `np.random`.","282","    random_state : int, RandomState instance or None, optional (default=None)","283","        The seed of the pseudo random number generator to use when shuffling","284","        the data.  If int, random_state is the seed used by the random number","285","        generator; If RandomState instance, random_state is the random number","286","        generator; If None, the random number generator is the RandomState","287","        instance used by `np.random`.","476","    random_state : int, RandomState instance or None, optional (default=None)","477","        The seed of the pseudo random number generator to use when shuffling","478","        the data.  If int, random_state is the seed used by the random number","479","        generator; If RandomState instance, random_state is the random number","480","        generator; If None, the random number generator is the RandomState","481","        instance used by `np.random`.","632","    random_state : int, RandomState instance or None, optional (default=None)","633","        The seed of the pseudo random number generator to use when shuffling","634","        the data.  If int, random_state is the seed used by the random number","635","        generator; If RandomState instance, random_state is the random number","636","        generator; If None, the random number generator is the RandomState","637","        instance used by `np.random`.","986","    random_state : int, RandomState instance or None, optional (default=None)","987","        The seed of the pseudo random number generator to use when shuffling","988","        the data.  If int, random_state is the seed used by the random number","989","        generator; If RandomState instance, random_state is the random number","990","        generator; If None, the random number generator is the RandomState","991","        instance used by `np.random`."],"delete":["88","    random_state : int seed, RandomState instance, or None (default=None)","89","        The seed of the pseudo random number generator to use when","90","        shuffling the data.","279","    random_state : int seed, RandomState instance, or None (default=None)","280","        The seed of the pseudo random number generator to use when","281","        shuffling the data.","470","    random_state : int seed, RandomState instance, or None (default)","471","        The seed of the pseudo random number generator to use when","472","        shuffling the data for probability estimation.","623","    random_state : int seed, RandomState instance, or None (default)","624","        The seed of the pseudo random number generator to use when","625","        shuffling the data for probability estimation.","974","    random_state : int seed, RandomState instance, or None (default)","975","        The seed of the pseudo random number generator to use when","976","        shuffling the data for probability estimation."]}],"sklearn\/metrics\/cluster\/unsupervised.py":[{"add":["64","    random_state : int, RandomState instance or None, optional (default=None)","65","        The generator used to randomly select a subset of samples.  If int,","66","        random_state is the seed used by the random number generator; If","67","        RandomState instance, random_state is the random number generator; If","68","        None, the random number generator is the RandomState instance used by","69","        `np.random`. Used when ``sample_size is not None``."],"delete":["64","    random_state : integer or numpy.RandomState, optional","65","        The generator used to randomly select a subset of samples if","66","        ``sample_size is not None``. If an integer is given, it fixes the seed.","67","        Defaults to the global numpy random number generator."]}],"sklearn\/ensemble\/base.py":[{"add":["31","    random_state : int, RandomState instance or None, optional (default=None)","32","        If int, random_state is the seed used by the random number generator;","33","        If RandomState instance, random_state is the random number generator;","34","        If None, the random number generator is the RandomState instance used","35","        by `np.random`."],"delete":["31","    random_state : numpy.RandomState or int, optional","32","        Random state used to generate integer values."]}],"sklearn\/linear_model\/passive_aggressive.py":[{"add":["30","    random_state : int, RandomState instance or None, optional, default=None","31","        The seed of the pseudo random number generator to use when shuffling","32","        the data.  If int, random_state is the seed used by the random number","33","        generator; If RandomState instance, random_state is the random number","34","        generator; If None, the random number generator is the RandomState","35","        instance used by `np.random`.","209","    random_state : int, RandomState instance or None, optional, default=None","210","        The seed of the pseudo random number generator to use when shuffling","211","        the data.  If int, random_state is the seed used by the random number","212","        generator; If RandomState instance, random_state is the random number","213","        generator; If None, the random number generator is the RandomState","214","        instance used by `np.random`."],"delete":["30","    random_state : int seed, RandomState instance, or None (default)","31","        The seed of the pseudo random number generator to use when","32","        shuffling the data.","206","    random_state : int seed, RandomState instance, or None (default)","207","        The seed of the pseudo random number generator to use when","208","        shuffling the data."]}],"sklearn\/model_selection\/_search.py":[{"add":["194","    random_state : int, RandomState instance or None, optional (default=None)","197","        If int, random_state is the seed used by the random number generator;","198","        If RandomState instance, random_state is the random number generator;","199","        If None, the random number generator is the RandomState instance used","200","        by `np.random`.","1060","    random_state : int, RandomState instance or None, optional, default=None","1063","        If int, random_state is the seed used by the random number generator;","1064","        If RandomState instance, random_state is the random number generator;","1065","        If None, the random number generator is the RandomState instance used","1066","        by `np.random`."],"delete":["194","    random_state : int or RandomState","1056","    random_state : int or RandomState"]}],"sklearn\/linear_model\/ransac.py":[{"add":["160","    random_state : int, RandomState instance or None, optional, default None","161","        The generator used to initialize the centers.  If int, random_state is","162","        the seed used by the random number generator; If RandomState instance,","163","        random_state is the random number generator; If None, the random number","164","        generator is the RandomState instance used by `np.random`."],"delete":["160","    random_state : integer or numpy.RandomState, optional","161","        The generator used to initialize the centers. If an integer is","162","        given, it fixes the seed. Defaults to the global numpy random","163","        number generator."]}],"sklearn\/mixture\/gmm.py":[{"add":["154","    random_state : int, RandomState instance or None, optional (default=None)","155","        If int, random_state is the seed used by the random number generator;","156","        If RandomState instance, random_state is the random number generator;","157","        If None, the random number generator is the RandomState instance used","158","        by `np.random`."],"delete":["154","    random_state : RandomState or an int seed (None by default)","155","        A random number generator instance"]}],"sklearn\/cross_validation.py":[{"add":["298","    random_state : int, RandomState instance or None, optional, default=None","299","        If int, random_state is the seed used by the random number","300","        generator; If RandomState instance, random_state is the random number","301","        generator; If None, the random number generator is the RandomState","302","        instance used by `np.random`. Used when ``shuffle`` == True.","502","    random_state : int, RandomState instance or None, optional, default=None","503","        If int, random_state is the seed used by the random number","504","        generator; If RandomState instance, random_state is the random number","505","        generator; If None, the random number generator is the RandomState","506","        instance used by `np.random`. Used when ``shuffle`` == True.","827","    random_state : int, RandomState instance or None, optional (default None)","828","        If int, random_state is the seed used by the random number generator;","829","        If RandomState instance, random_state is the random number generator;","830","        If None, the random number generator is the RandomState instance used","831","        by `np.random`.","1039","    random_state : int, RandomState instance or None, optional (default None)","1040","        If int, random_state is the seed used by the random number generator;","1041","        If RandomState instance, random_state is the random number generator;","1042","        If None, the random number generator is the RandomState instance used","1043","        by `np.random`.","1236","    random_state : int, RandomState instance or None, optional (default None)","1237","        If int, random_state is the seed used by the random number generator;","1238","        If RandomState instance, random_state is the random number generator;","1239","        If None, the random number generator is the RandomState instance used","1240","        by `np.random`.","1903","    random_state : int, RandomState instance or None, optional (default=0)","1904","        If int, random_state is the seed used by the random number generator;","1905","        If RandomState instance, random_state is the random number generator;","1906","        If None, the random number generator is the RandomState instance used","1907","        by `np.random`.","1993","    random_state : int, RandomState instance or None, optional (default=None)","1994","        If int, random_state is the seed used by the random number generator;","1995","        If RandomState instance, random_state is the random number generator;","1996","        If None, the random number generator is the RandomState instance used","1997","        by `np.random`."],"delete":["0","","299","    random_state : None, int or RandomState","300","        When shuffle=True, pseudo-random number generator state used for","301","        shuffling. If None, use default numpy RNG for shuffling.","501","    random_state : None, int or RandomState","502","        When shuffle=True, pseudo-random number generator state used for","503","        shuffling. If None, use default numpy RNG for shuffling.","824","    random_state : int or RandomState","825","        Pseudo-random number generator state used for random sampling.","1033","    random_state : int or RandomState","1034","        Pseudo-random number generator state used for random sampling.","1227","    random_state : int or RandomState","1228","        Pseudo-random number generator state used for random sampling.","1891","    random_state : RandomState or an int seed (0 by default)","1892","        A random number generator instance to define the state of the","1893","        random permutations generator.","1979","    random_state : int or RandomState","1980","        Pseudo-random number generator state used for random sampling."]}],"sklearn\/datasets\/olivetti_faces.py":[{"add":["73","    random_state : int, RandomState instance or None, optional (default=0)","74","        If int, random_state is the seed used by the random number generator;","75","        If RandomState instance, random_state is the random number generator;","76","        If None, the random number generator is the RandomState instance used","77","        by `np.random`."],"delete":["73","    random_state : optional, integer or RandomState object","74","        The seed or the random number generator used to shuffle the","75","        data."]}],"sklearn\/linear_model\/logistic.py":[{"add":["545","    random_state : int, RandomState instance or None, optional, default None","546","        The seed of the pseudo random number generator to use when shuffling","547","        the data.  If int, random_state is the seed used by the random number","548","        generator; If RandomState instance, random_state is the random number","549","        generator; If None, the random number generator is the RandomState","550","        instance used by `np.random`. Used when ``solver`` == 'sag' or","551","        'liblinear'.","866","    random_state : int, RandomState instance or None, optional, default None","867","        The seed of the pseudo random number generator to use when shuffling","868","        the data.  If int, random_state is the seed used by the random number","869","        generator; If RandomState instance, random_state is the random number","870","        generator; If None, the random number generator is the RandomState","871","        instance used by `np.random`. Used when ``solver`` == 'sag' and","872","        'liblinear'.","1034","    random_state : int, RandomState instance or None, optional, default: None","1035","        The seed of the pseudo random number generator to use when shuffling","1036","        the data.  If int, random_state is the seed used by the random number","1037","        generator; If RandomState instance, random_state is the random number","1038","        generator; If None, the random number generator is the RandomState","1039","        instance used by `np.random`. Used when ``solver`` == 'sag' or","1040","        'liblinear'.","1484","    random_state : int, RandomState instance or None, optional, default None","1485","        If int, random_state is the seed used by the random number generator;","1486","        If RandomState instance, random_state is the random number generator;","1487","        If None, the random number generator is the RandomState instance used","1488","        by `np.random`."],"delete":["545","    random_state : int seed, RandomState instance, or None (default)","546","        The seed of the pseudo random number generator to use when","547","        shuffling the data. Used only in solvers 'sag' and 'liblinear'.","862","    random_state : int seed, RandomState instance, or None (default)","863","        The seed of the pseudo random number generator to use when","864","        shuffling the data. Used only in solvers 'sag' and 'liblinear'.","1026","    random_state : int seed, RandomState instance, default: None","1027","        The seed of the pseudo random number generator to use when","1028","        shuffling the data. Used only in solvers 'sag' and 'liblinear'.","1472","    random_state : int seed, RandomState instance, or None (default)","1473","        The seed of the pseudo random number generator to use when","1474","        shuffling the data."]}],"sklearn\/utils\/__init__.py":[{"add":["177","    random_state : int, RandomState instance or None, optional (default=None)","178","        The seed of the pseudo random number generator to use when shuffling","179","        the data.  If int, random_state is the seed used by the random number","180","        generator; If RandomState instance, random_state is the random number","181","        generator; If None, the random number generator is the RandomState","182","        instance used by `np.random`.","277","    random_state : int, RandomState instance or None, optional (default=None)","278","        The seed of the pseudo random number generator to use when shuffling","279","        the data.  If int, random_state is the seed used by the random number","280","        generator; If RandomState instance, random_state is the random number","281","        generator; If None, the random number generator is the RandomState","282","        instance used by `np.random`."],"delete":["177","    random_state : int or RandomState instance","178","        Control the shuffling for reproducible behavior.","273","    random_state : int or RandomState instance","274","        Control the shuffling for reproducible behavior."]}],"sklearn\/cluster\/mean_shift_.py":[{"add":["49","    random_state : int, RandomState instance or None, optional (default=None)","50","        If int, random_state is the seed used by the random number generator;","51","        If RandomState instance, random_state is the random number generator;","52","        If None, the random number generator is the RandomState instance used","53","        by `np.random`."],"delete":["49","    random_state : int or RandomState","50","        Pseudo-random number generator state used for random sampling."]}],"sklearn\/model_selection\/_split.py":[{"add":["366","    random_state : int, RandomState instance or None, optional, default=None","367","        If int, random_state is the seed used by the random number generator;","368","        If RandomState instance, random_state is the random number generator;","369","        If None, the random number generator is the RandomState instance used","370","        by `np.random`. Used when ``shuffle`` == True.","535","    random_state : int, RandomState instance or None, optional, default=None","536","        If int, random_state is the seed used by the random number generator;","537","        If RandomState instance, random_state is the random number generator;","538","        If None, the random number generator is the RandomState instance used","539","        by `np.random`. Used when ``shuffle`` == True.","940","    random_state : int, RandomState instance or None, optional, default=None","941","        If int, random_state is the seed used by the random number generator;","942","        If RandomState instance, random_state is the random number generator;","943","        If None, the random number generator is the RandomState instance used","944","        by `np.random`.","1015","    random_state : int, RandomState instance or None, optional, default=None","1016","        If int, random_state is the seed used by the random number generator;","1017","        If RandomState instance, random_state is the random number generator;","1018","        If None, the random number generator is the RandomState instance used","1019","        by `np.random`.","1190","    random_state : int, RandomState instance or None, optional (default=None)","1191","        If int, random_state is the seed used by the random number generator;","1192","        If RandomState instance, random_state is the random number generator;","1193","        If None, the random number generator is the RandomState instance used","1194","        by `np.random`.","1275","    random_state : int, RandomState instance or None, optional (default=None)","1276","        If int, random_state is the seed used by the random number generator;","1277","        If RandomState instance, random_state is the random number generator;","1278","        If None, the random number generator is the RandomState instance used","1279","        by `np.random`.","1280","","1406","    random_state : int, RandomState instance or None, optional (default=None)","1407","        If int, random_state is the seed used by the random number generator;","1408","        If RandomState instance, random_state is the random number generator;","1409","        If None, the random number generator is the RandomState instance used","1410","        by `np.random`.","1411","","1816","    random_state : int, RandomState instance or None, optional (default=None)","1817","        If int, random_state is the seed used by the random number generator;","1818","        If RandomState instance, random_state is the random number generator;","1819","        If None, the random number generator is the RandomState instance used","1820","        by `np.random`."],"delete":["366","    random_state : None, int or RandomState","367","        When shuffle=True, pseudo-random number generator state used for","368","        shuffling. If None, use default numpy RNG for shuffling.","533","    random_state : None, int or RandomState","534","        When shuffle=True, pseudo-random number generator state used for","535","        shuffling. If None, use default numpy RNG for shuffling.","936","    random_state : None, int or RandomState, default=None","937","        Random state to be used to generate random state for each","938","        repetition.","1009","    random_state : None, int or RandomState, default=None","1010","        Random state to be used to generate random state for each","1011","        repetition.","1182","    random_state : int or RandomState","1183","        Pseudo-random number generator state used for random sampling.","1264","    random_state : int or RandomState","1265","        Pseudo-random number generator state used for random sampling.","1391","    random_state : int or RandomState","1392","        Pseudo-random number generator state used for random sampling.","1797","    random_state : int or RandomState","1798","        Pseudo-random number generator state used for random sampling."]}],"sklearn\/linear_model\/perceptron.py":[{"add":["32","    random_state : int, RandomState instance or None, optional, default None","33","        The seed of the pseudo random number generator to use when shuffling","34","        the data.  If int, random_state is the seed used by the random number","35","        generator; If RandomState instance, random_state is the random number","36","        generator; If None, the random number generator is the RandomState","37","        instance used by `np.random`."],"delete":["32","    random_state : int seed, RandomState instance, or None (default)","33","        The seed of the pseudo random number generator to use when","34","        shuffling the data."]}],"sklearn\/neighbors\/kde.py":[{"add":["186","        random_state : int, RandomState instance or None. default to None","187","            If int, random_state is the seed used by the random number","188","            generator; If RandomState instance, random_state is the random","189","            number generator; If None, the random number generator is the","190","            RandomState instance used by `np.random`."],"delete":["186","        random_state : RandomState or an int seed (0 by default)","187","            A random number generator instance."]}],"sklearn\/feature_selection\/mutual_info_.py":[{"add":["226","    random_state : int, RandomState instance or None, optional, default None","228","        to continuous variables in order to remove repeated values.  If int,","229","        random_state is the seed used by the random number generator; If","230","        RandomState instance, random_state is the random number generator; If","231","        None, the random number generator is the RandomState instance used by","232","        `np.random`.","333","    random_state : int, RandomState instance or None, optional, default None","336","        If int, random_state is the seed used by the random number generator;","337","        If RandomState instance, random_state is the random number generator;","338","        If None, the random number generator is the RandomState instance used","339","        by `np.random`.","412","    random_state : int, RandomState instance or None, optional, default None","414","        to continuous variables in order to remove repeated values.  If int,","415","        random_state is the seed used by the random number generator; If","416","        RandomState instance, random_state is the random number generator; If","417","        None, the random number generator is the RandomState instance used by","418","        `np.random`."],"delete":["226","    random_state : int seed, RandomState instance or None, default None","228","        to continuous variables in order to remove repeated values.","329","    random_state : int seed, RandomState instance or None, default None","404","    random_state : int seed, RandomState instance or None, default None","406","        to continuous variables in order to remove repeated values."]}],"sklearn\/svm\/base.py":[{"add":["806","    random_state : int, RandomState instance or None, optional (default=None)","807","        The seed of the pseudo random number generator to use when shuffling","808","        the data.  If int, random_state is the seed used by the random number","809","        generator; If RandomState instance, random_state is the random number","810","        generator; If None, the random number generator is the RandomState","811","        instance used by `np.random`.","812",""],"delete":["806","    random_state : int seed, RandomState instance, or None (default)","807","        The seed of the pseudo random number generator to use when","808","        shuffling the data."]}],"sklearn\/multiclass.py":[{"add":["642","    random_state : int, RandomState instance or None, optional, default: None","643","        The generator used to initialize the codebook.  If int, random_state is","644","        the seed used by the random number generator; If RandomState instance,","645","        random_state is the random number generator; If None, the random number","646","        generator is the RandomState instance used by `np.random`."],"delete":["642","    random_state : numpy.RandomState, optional","643","        The generator used to initialize the codebook. Defaults to","644","        numpy.random."]}],"sklearn\/linear_model\/randomized_l1.py":[{"add":["577","    random_state : int, RandomState instance or None, optional, default=None","578","        The generator used to randomize the design.  If int, random_state is","579","        the seed used by the random number generator; If RandomState instance,","580","        random_state is the random number generator; If None, the random number","581","        generator is the RandomState instance used by `np.random`."],"delete":["577","    random_state : integer or numpy.random.RandomState, optional","578","        The generator used to randomize the design."]}],"sklearn\/grid_search.py":[{"add":["202","    random_state : int, RandomState instance or None, optional (default=None)","205","        If int, random_state is the seed used by the random number generator;","206","        If RandomState instance, random_state is the random number generator;","207","        If None, the random number generator is the RandomState instance used","208","        by `np.random`.","960","    random_state : int, RandomState instance or None, optional, default=None","963","        If int, random_state is the seed used by the random number generator;","964","        If RandomState instance, random_state is the random number generator;","965","        If None, the random number generator is the RandomState instance used","966","        by `np.random`."],"delete":["202","    random_state : int or RandomState","956","    random_state : int or RandomState"]}],"sklearn\/mixture\/gaussian_mixture.py":[{"add":["502","    random_state : int, RandomState instance or None, optional (default=None)","503","        If int, random_state is the seed used by the random number generator;","504","        If RandomState instance, random_state is the random number generator;","505","        If None, the random number generator is the RandomState instance used","506","        by `np.random`."],"delete":["502","    random_state : RandomState or an int seed, defaults to None.","503","        A random number generator instance."]}],"sklearn\/decomposition\/factor_analysis.py":[{"add":["90","    random_state : int, RandomState instance or None, optional (default=0)","91","        If int, random_state is the seed used by the random number generator;","92","        If RandomState instance, random_state is the random number generator;","93","        If None, the random number generator is the RandomState instance used","94","        by `np.random`. Only used when ``svd_method`` equals 'randomized'."],"delete":["90","    random_state : int or RandomState","91","        Pseudo number generator state used for random sampling. Only used","92","        if ``svd_method`` equals 'randomized'"]}],"sklearn\/linear_model\/coordinate_descent.py":[{"add":["584","    random_state : int, RandomState instance or None, optional, default None","585","        The seed of the pseudo random number generator that selects a random","586","        feature to update.  If int, random_state is the seed used by the random","587","        number generator; If RandomState instance, random_state is the random","588","        number generator; If None, the random number generator is the","589","        RandomState instance used by `np.random`. Used when ``selection`` ==","834","    random_state : int, RandomState instance or None, optional, default None","835","        The seed of the pseudo random number generator that selects a random","836","        feature to update.  If int, random_state is the seed used by the random","837","        number generator; If RandomState instance, random_state is the random","838","        number generator; If None, the random number generator is the","839","        RandomState instance used by `np.random`. Used when ``selection`` ==","1274","    random_state : int, RandomState instance or None, optional, default None","1275","        The seed of the pseudo random number generator that selects a random","1276","        feature to update.  If int, random_state is the seed used by the random","1277","        number generator; If RandomState instance, random_state is the random","1278","        number generator; If None, the random number generator is the","1279","        RandomState instance used by `np.random`. Used when ``selection`` ==","1429","    random_state : int, RandomState instance or None, optional, default None","1430","        The seed of the pseudo random number generator that selects a random","1431","        feature to update.  If int, random_state is the seed used by the random","1432","        number generator; If RandomState instance, random_state is the random","1433","        number generator; If None, the random number generator is the","1434","        RandomState instance used by `np.random`. Used when ``selection`` ==","1602","    random_state : int, RandomState instance or None, optional, default None","1603","        The seed of the pseudo random number generator that selects a random","1604","        feature to update.  If int, random_state is the seed used by the random","1605","        number generator; If RandomState instance, random_state is the random","1606","        number generator; If None, the random number generator is the","1607","        RandomState instance used by `np.random`. Used when ``selection`` ==","1789","    random_state : int, RandomState instance or None, optional, default None","1790","        The seed of the pseudo random number generator that selects a random","1791","        feature to update.  If int, random_state is the seed used by the random","1792","        number generator; If RandomState instance, random_state is the random","1793","        number generator; If None, the random number generator is the","1794","        RandomState instance used by `np.random`. Used when ``selection`` ==","1945","    random_state : int, RandomState instance or None, optional, default None","1946","        The seed of the pseudo random number generator that selects a random","1947","        feature to update.  If int, random_state is the seed used by the random","1948","        number generator; If RandomState instance, random_state is the random","1949","        number generator; If None, the random number generator is the","1950","        RandomState instance used by `np.random`. Used when ``selection`` ==","2112","    random_state : int, RandomState instance or None, optional, default None","2113","        The seed of the pseudo random number generator that selects a random","2114","        feature to update.  If int, random_state is the seed used by the random","2115","        number generator; If RandomState instance, random_state is the random","2116","        number generator; If None, the random number generator is the","2117","        RandomState instance used by `np.random`. Used when ``selection`` ==","2118","        'random'\/"],"delete":["584","    random_state : int, RandomState instance, or None (default)","585","        The seed of the pseudo random number generator that selects","586","        a random feature to update. Useful only when selection is set to","831","    random_state : int, RandomState instance, or None (default)","832","        The seed of the pseudo random number generator that selects","833","        a random feature to update. Useful only when selection is set to","1268","    random_state : int, RandomState instance, or None (default)","1269","        The seed of the pseudo random number generator that selects","1270","        a random feature to update. Useful only when selection is set to","1420","    random_state : int, RandomState instance, or None (default)","1421","        The seed of the pseudo random number generator that selects","1422","        a random feature to update. Useful only when selection is set to","1590","    random_state : int, RandomState instance, or None (default)","1591","        The seed of the pseudo random number generator that selects","1592","        a random feature to update. Useful only when selection is set to","1774","    random_state : int, RandomState instance, or None (default)","1775","        The seed of the pseudo random number generator that selects","1776","        a random feature to update. Useful only when selection is set to","1927","    random_state : int, RandomState instance, or None (default)","1928","        The seed of the pseudo random number generator that selects","1929","        a random feature to update. Useful only when selection is set to","2091","    random_state : int, RandomState instance, or None (default)","2092","        The seed of the pseudo random number generator that selects","2093","        a random feature to update. Useful only when selection is set to","2094","        'random'."]}],"sklearn\/decomposition\/online_lda.py":[{"add":["221","    random_state : int, RandomState instance or None, optional (default=None)","222","        If int, random_state is the seed used by the random number generator;","223","        If RandomState instance, random_state is the random number generator;","224","        If None, the random number generator is the RandomState instance used","225","        by `np.random`."],"delete":["221","    random_state : int or RandomState instance or None, optional (default=None)","222","        Pseudo-random number generator seed control."]}],"sklearn\/manifold\/mds.py":[{"add":["21","    \"\"\"Computes multidimensional scaling using SMACOF algorithm","51","    random_state : int, RandomState instance or None, optional, default: None","52","        The generator used to initialize the centers.  If int, random_state is","53","        the seed used by the random number generator; If RandomState instance,","54","        random_state is the random number generator; If None, the random number","55","        generator is the RandomState instance used by `np.random`.","136","    \"\"\"Computes multidimensional scaling using the SMACOF algorithm.","199","    random_state : int, RandomState instance or None, optional, default: None","200","        The generator used to initialize the centers.  If int, random_state is","201","        the seed used by the random number generator; If RandomState instance,","202","        random_state is the random number generator; If None, the random number","203","        generator is the RandomState instance used by `np.random`.","316","    random_state : int, RandomState instance or None, optional, default: None","317","        The generator used to initialize the centers.  If int, random_state is","318","        the seed used by the random number generator; If RandomState instance,","319","        random_state is the random number generator; If None, the random number","320","        generator is the RandomState instance used by `np.random`."],"delete":["21","    \"\"\"","22","    Computes multidimensional scaling using SMACOF algorithm","52","    random_state : integer or numpy.RandomState, optional","53","        The generator used to initialize the centers. If an integer is","54","        given, it fixes the seed. Defaults to the global numpy random","55","        number generator.","136","    \"\"\"","137","    Computes multidimensional scaling using the SMACOF algorithm.","200","    random_state : integer or numpy.RandomState, optional, default: None","201","        The generator used to initialize the centers. If an integer is given,","202","        it fixes the seed. Defaults to the global numpy random number","203","        generator.","316","    random_state : integer or numpy.RandomState, optional, default: None","317","        The generator used to initialize the centers. If an integer is given,","318","        it fixes the seed. Defaults to the global numpy random number","319","        generator."]}],"sklearn\/linear_model\/sag.py":[{"add":["146","    random_state : int, RandomState instance or None, optional, default None","147","        The seed of the pseudo random number generator to use when shuffling","148","        the data.  If int, random_state is the seed used by the random number","149","        generator; If RandomState instance, random_state is the random number","150","        generator; If None, the random number generator is the RandomState","151","        instance used by `np.random`."],"delete":["146","    random_state : int seed, RandomState instance, or None (default)","147","        The seed of the pseudo random number generator to use when","148","        shuffling the data."]}],"sklearn\/cluster\/spectral.py":[{"add":["41","    random_state : int, RandomState instance or None, optional, default: None","42","        If int, random_state is the seed used by the random number generator;","43","        If RandomState instance, random_state is the random number generator;","44","        If None, the random number generator is the RandomState instance used","45","        by `np.random`.","198","    random_state : int, RandomState instance or None, optional, default: None","199","        A pseudo random number generator used for the initialization of the","200","        lobpcg eigen vectors decomposition when eigen_solver == 'amg' and by","201","        the K-Means initialization. If int, random_state is the seed used by","202","        the random number generator; If RandomState instance, random_state is","203","        the random number generator; If None, the random number generator is","204","        the RandomState instance used by `np.random`.","333","    random_state : int, RandomState instance or None, optional, default: None","334","        A pseudo random number generator used for the initialization of the","335","        lobpcg eigen vectors decomposition when eigen_solver == 'amg' and by","336","        the K-Means initialization.  If int, random_state is the seed used by","337","        the random number generator; If RandomState instance, random_state is","338","        the random number generator; If None, the random number generator is","339","        the RandomState instance used by `np.random`."],"delete":["41","    random_state : int seed, RandomState instance, or None (default)","42","        A pseudo random number generator used for the initialization of the","43","        of the rotation matrix","196","    random_state : int seed, RandomState instance, or None (default)","197","        A pseudo random number generator used for the initialization","198","        of the lobpcg eigen vectors decomposition when eigen_solver == 'amg'","199","        and by the K-Means initialization.","328","    random_state : int seed, RandomState instance, or None (default)","329","        A pseudo random number generator used for the initialization","330","        of the lobpcg eigen vectors decomposition when eigen_solver == 'amg'","331","        and by the K-Means initialization."]}],"sklearn\/decomposition\/nmf.py":[{"add":["270","    random_state : int, RandomState instance or None, optional, default: None","271","        If int, random_state is the seed used by the random number generator;","272","        If RandomState instance, random_state is the random number generator;","273","        If None, the random number generator is the RandomState instance used","274","        by `np.random`. Used when ``random`` == 'nndsvdar' or 'random'.","449","    random_state : int, RandomState instance or None, optional, default: None","450","        If int, random_state is the seed used by the random number generator;","451","        If RandomState instance, random_state is the random number generator;","452","        If None, the random number generator is the RandomState instance used","453","        by `np.random`.","917","    random_state : int, RandomState instance or None, optional, default: None","918","        If int, random_state is the seed used by the random number generator;","919","        If RandomState instance, random_state is the random number generator;","920","        If None, the random number generator is the RandomState instance used","921","        by `np.random`.","1107","    random_state : int, RandomState instance or None, optional, default: None","1108","        If int, random_state is the seed used by the random number generator;","1109","        If RandomState instance, random_state is the random number generator;","1110","        If None, the random number generator is the RandomState instance used","1111","        by `np.random`."],"delete":["270","    random_state : int seed, RandomState instance, or None (default)","271","        Random number generator seed control, used in 'nndsvdar' and","272","        'random' modes.","447","    random_state : integer seed, RandomState instance, or None (default)","448","        Random number generator seed control.","912","    random_state : integer seed, RandomState instance, or None (default)","913","        Random number generator seed control.","1099","    random_state : integer seed, RandomState instance, or None (default)","1100","        Random number generator seed control."]}],"sklearn\/covariance\/robust_covariance.py":[{"add":["57","    random_state : int, RandomState instance or None, optional (default=None)","58","        If int, random_state is the seed used by the random number generator;","59","        If RandomState instance, random_state is the random number generator;","60","        If None, the random number generator is the RandomState instance used","61","        by `np.random`.","218","    random_state : int, RandomState instance or None, optional (default=None)","219","        If int, random_state is the seed used by the random number generator;","220","        If RandomState instance, random_state is the random number generator;","221","        If None, the random number generator is the RandomState instance used","222","        by `np.random`.","317","    random_state : int, RandomState instance or None, optional (default=None)","318","        If int, random_state is the seed used by the random number generator;","319","        If RandomState instance, random_state is the random number generator;","320","        If None, the random number generator is the RandomState instance used","321","        by `np.random`.","538","    random_state : int, RandomState instance or None, optional (default=None)","539","        If int, random_state is the seed used by the random number generator;","540","        If RandomState instance, random_state is the random number generator;","541","        If None, the random number generator is the RandomState instance used","542","        by `np.random`."],"delete":["57","    random_state : integer or numpy.RandomState, optional","58","        The random generator used. If an integer is given, it fixes the","59","        seed. Defaults to the global numpy random number generator.","216","    random_state : integer or numpy.RandomState, default None","217","        The random generator used. If an integer is given, it fixes the","218","        seed. Defaults to the global numpy random number generator.","313","    random_state : integer or numpy.RandomState, optional","314","        The generator used to randomly subsample. If an integer is","315","        given, it fixes the seed. Defaults to the global numpy random","316","        number generator.","533","    random_state : integer or numpy.RandomState, optional","534","        The random generator used. If an integer is given, it fixes the","535","        seed. Defaults to the global numpy random number generator."]}],"sklearn\/decomposition\/fastica_.py":[{"add":["201","    random_state : int, RandomState instance or None, optional (default=None)","202","        If int, random_state is the seed used by the random number generator;","203","        If RandomState instance, random_state is the random number generator;","204","        If None, the random number generator is the RandomState instance used","205","        by `np.random`.","420","    random_state : int, RandomState instance or None, optional (default=None)","421","        If int, random_state is the seed used by the random number generator;","422","        If RandomState instance, random_state is the random number generator;","423","        If None, the random number generator is the RandomState instance used","424","        by `np.random`."],"delete":["201","    random_state : int or RandomState","202","        Pseudo number generator state used for random sampling.","417","    random_state : int or RandomState","418","        Pseudo number generator state used for random sampling."]}],"sklearn\/kernel_approximation.py":[{"add":["40","    random_state : int, RandomState instance or None, optional (default=None)","42","        If RandomState instance, random_state is the random number generator;","43","        If None, the random number generator is the RandomState instance used","44","        by `np.random`.","128","    random_state : int, RandomState instance or None, optional (default=None)","130","        If RandomState instance, random_state is the random number generator;","131","        If None, the random number generator is the RandomState instance used","132","        by `np.random`.","400","    random_state : int, RandomState instance or None, optional (default=None)","402","        If RandomState instance, random_state is the random number generator;","403","        If None, the random number generator is the RandomState instance used","404","        by `np.random`."],"delete":["40","    random_state : {int, RandomState}, optional","42","        if RandomState instance, random_state is the random number generator.","126","    random_state : {int, RandomState}, optional","128","        if RandomState instance, random_state is the random number generator.","396","    random_state : {int, RandomState}, optional","398","        if RandomState instance, random_state is the random number generator.","399",""]}],"sklearn\/manifold\/spectral_embedding_.py":[{"add":["168","    random_state : int, RandomState instance or None, optional, default: None","170","        lobpcg eigenvectors decomposition.  If int, random_state is the seed","171","        used by the random number generator; If RandomState instance,","172","        random_state is the random number generator; If None, the random number","173","        generator is the RandomState instance used by `np.random`. Used when","174","        ``solver`` == 'amg'.","350","    random_state : int, RandomState instance or None, optional, default: None","352","        lobpcg eigenvectors.  If int, random_state is the seed used by the","353","        random number generator; If RandomState instance, random_state is the","354","        random number generator; If None, the random number generator is the","355","        RandomState instance used by `np.random`. Used when ``solver`` ==","356","        'amg'."],"delete":["168","    random_state : int seed, RandomState instance, or None (default)","170","        lobpcg eigenvectors decomposition when eigen_solver == 'amg'.","171","        By default, arpack is used.","347","    random_state : int seed, RandomState instance, or None, default : None","349","        lobpcg eigenvectors decomposition when eigen_solver == 'amg'."]}],"sklearn\/utils\/extmath.py":[{"add":["217","    random_state : int, RandomState instance or None, optional (default=None)","218","        The seed of the pseudo random number generator to use when shuffling","219","        the data.  If int, random_state is the seed used by the random number","220","        generator; If RandomState instance, random_state is the random number","221","        generator; If None, the random number generator is the RandomState","222","        instance used by `np.random`.","326","    random_state : int, RandomState instance or None, optional (default=None)","327","        The seed of the pseudo random number generator to use when shuffling","328","        the data.  If int, random_state is the seed used by the random number","329","        generator; If RandomState instance, random_state is the random number","330","        generator; If None, the random number generator is the RandomState","331","        instance used by `np.random`."],"delete":["217","    random_state : RandomState or an int seed (0 by default)","218","        A random number generator instance","322","    random_state : RandomState or an int seed (0 by default)","323","        A random number generator instance to make behavior"]}],"sklearn\/feature_extraction\/image.py":[{"add":["321","    random_state : int, RandomState instance or None, optional (default=None)","323","        `max_patches` is not None.  If int, random_state is the seed used by","324","        the random number generator; If RandomState instance, random_state is","325","        the random number generator; If None, the random number generator is","326","        the RandomState instance used by `np.random`.","455","    random_state : int, RandomState instance or None, optional (default=None)","456","        If int, random_state is the seed used by the random number generator;","457","        If RandomState instance, random_state is the random number generator;","458","        If None, the random number generator is the RandomState instance used","459","        by `np.random`."],"delete":["321","    random_state : int or RandomState","323","        `max_patches` is not None.","452","    random_state : int or RandomState","453","        Pseudo number generator state used for random sampling."]}],"sklearn\/neural_network\/multilayer_perceptron.py":[{"add":["757","    random_state : int, RandomState instance or None, optional, default None","758","        If int, random_state is the seed used by the random number generator;","759","        If RandomState instance, random_state is the random number generator;","760","        If None, the random number generator is the RandomState instance used","761","        by `np.random`.","1131","    random_state : int, RandomState instance or None, optional, default None","1132","        If int, random_state is the seed used by the random number generator;","1133","        If RandomState instance, random_state is the random number generator;","1134","        If None, the random number generator is the RandomState instance used","1135","        by `np.random`."],"delete":["757","    random_state : int or RandomState, optional, default None","758","        State or seed for random number generator.","1128","    random_state : int or RandomState, optional, default None","1129","        State or seed for random number generator."]}],"sklearn\/decomposition\/pca.py":[{"add":["185","    random_state : int, RandomState instance or None, optional (default None)","186","        If int, random_state is the seed used by the random number generator;","187","        If RandomState instance, random_state is the random number generator;","188","        If None, the random number generator is the RandomState instance used","189","        by `np.random`. Used when ``svd_solver`` == 'arpack' or 'randomized'.","605","    random_state : int, RandomState instance or None, optional, default=None","606","        If int, random_state is the seed used by the random number generator;","607","        If RandomState instance, random_state is the random number generator;","608","        If None, the random number generator is the RandomState instance used","609","        by `np.random`."],"delete":["185","    random_state : int or RandomState instance or None (default None)","186","        Pseudo Random Number generator seed control. If None, use the","187","        numpy.random singleton. Used by svd_solver == 'arpack' or 'randomized'.","603","    random_state : int or RandomState instance or None (default)","604","        Pseudo Random Number generator seed control. If None, use the","605","        numpy.random singleton."]}],"sklearn\/linear_model\/ridge.py":[{"add":["277","    random_state : int, RandomState instance or None, optional, default None","278","        The seed of the pseudo random number generator to use when shuffling","279","        the data.  If int, random_state is the seed used by the random number","280","        generator; If RandomState instance, random_state is the random number","281","        generator; If None, the random number generator is the RandomState","282","        instance used by `np.random`. Used when ``solver`` == 'sag'.","585","    random_state : int, RandomState instance or None, optional, default None","586","        The seed of the pseudo random number generator to use when shuffling","587","        the data.  If int, random_state is the seed used by the random number","588","        generator; If RandomState instance, random_state is the random number","589","        generator; If None, the random number generator is the RandomState","590","        instance used by `np.random`. Used when ``solver`` == 'sag'.","736","    random_state : int, RandomState instance or None, optional, default None","737","        The seed of the pseudo random number generator to use when shuffling","738","        the data.  If int, random_state is the seed used by the random number","739","        generator; If RandomState instance, random_state is the random number","740","        generator; If None, the random number generator is the RandomState","741","        instance used by `np.random`. Used when ``solver`` == 'sag'."],"delete":["277","    random_state : int seed, RandomState instance, or None (default)","278","        The seed of the pseudo random number generator to use when","279","        shuffling the data. Used only in 'sag' solver.","582","    random_state : int seed, RandomState instance, or None (default)","583","        The seed of the pseudo random number generator to use when","584","        shuffling the data. Used only in 'sag' solver.","730","    random_state : int seed, RandomState instance, or None (default)","731","        The seed of the pseudo random number generator to use when","732","        shuffling the data. Used in 'sag' solver."]}],"sklearn\/linear_model\/theil_sen.py":[{"add":["245","    random_state : int, RandomState instance or None, optional, default None","246","        A random number generator instance to define the state of the random","247","        permutations generator.  If int, random_state is the seed used by the","248","        random number generator; If RandomState instance, random_state is the","249","        random number generator; If None, the random number generator is the","250","        RandomState instance used by `np.random`."],"delete":["245","    random_state : RandomState or an int seed, optional, default None","246","        A random number generator instance to define the state of the","247","        random permutations generator."]}],"sklearn\/decomposition\/truncated_svd.py":[{"add":["61","    random_state : int, RandomState instance or None, optional, default = None","62","        If int, random_state is the seed used by the random number generator;","63","        If RandomState instance, random_state is the random number generator;","64","        If None, the random number generator is the RandomState instance used","65","        by `np.random`."],"delete":["61","    random_state : int or RandomState, optional","62","        (Seed for) pseudo-random number generator. If not given, the","63","        numpy.random singleton is used."]}],"sklearn\/datasets\/samples_generator.py":[{"add":["1061","    random_state : int, RandomState instance or None, optional (default=None)","1062","        If int, random_state is the seed used by the random number generator;","1063","        If RandomState instance, random_state is the random number generator;","1064","        If None, the random number generator is the RandomState instance used","1065","        by `np.random`."],"delete":["1061","    random_state : int or RandomState instance, optional (default=None)","1062","        seed used by the pseudo random number generator"]}],"sklearn\/gaussian_process\/gaussian_process.py":[{"add":["171","    random_state : int, RandomState instance or None, optional (default=None)","173","        the Welch optimizer. If int, random_state is the seed used by the","174","        random number generator; If RandomState instance, random_state is the","175","        random number generator; If None, the random number generator is the","176","        RandomState instance used by `np.random`."],"delete":["171","    random_state : integer or numpy.RandomState, optional","173","        the Welch optimizer. If an integer is given, it fixes the seed.","174","        Defaults to the global numpy random number generator.","175",""]}],"sklearn\/cluster\/bicluster.py":[{"add":["238","    random_state : int, RandomState instance or None, optional, default: None","239","        If int, random_state is the seed used by the random number generator;","240","        If RandomState instance, random_state is the random number generator;","241","        If None, the random number generator is the RandomState instance used","242","        by `np.random`.","370","    random_state : int, RandomState instance or None, optional, default: None","371","        If int, random_state is the seed used by the random number generator;","372","        If RandomState instance, random_state is the random number generator;","373","        If None, the random number generator is the RandomState instance used","374","        by `np.random`."],"delete":["238","    random_state : int seed, RandomState instance, or None (default)","239","        A pseudo random number generator used by the K-Means","240","        initialization.","368","    random_state : int seed, RandomState instance, or None (default)","369","        A pseudo random number generator used by the K-Means","370","        initialization."]}],"sklearn\/linear_model\/stochastic_gradient.py":[{"add":["610","    random_state : int, RandomState instance or None, optional (default=None)","611","        The seed of the pseudo random number generator to use when shuffling","612","        the data.  If int, random_state is the seed used by the random number","613","        generator; If RandomState instance, random_state is the random number","614","        generator; If None, the random number generator is the RandomState","615","        instance used by `np.random`.","1139","    random_state : int, RandomState instance or None, optional (default=None)","1140","        The seed of the pseudo random number generator to use when shuffling","1141","        the data.  If int, random_state is the seed used by the random number","1142","        generator; If RandomState instance, random_state is the random number","1143","        generator; If None, the random number generator is the RandomState","1144","        instance used by `np.random`."],"delete":["610","    random_state : int seed, RandomState instance, or None (default)","611","        The seed of the pseudo random number generator to use when","612","        shuffling the data.","1136","    random_state : int seed, RandomState instance, or None (default)","1137","        The seed of the pseudo random number generator to use when","1138","        shuffling the data."]}],"sklearn\/model_selection\/_validation.py":[{"add":["582","    random_state : int, RandomState instance or None, optional (default=0)","583","        If int, random_state is the seed used by the random number generator;","584","        If RandomState instance, random_state is the random number generator;","585","        If None, the random number generator is the RandomState instance used","586","        by `np.random`.","747","    random_state : int, RandomState instance or None, optional (default=None)","748","        If int, random_state is the seed used by the random number generator;","749","        If RandomState instance, random_state is the random number generator;","750","        If None, the random number generator is the RandomState instance used","751","        by `np.random`. Used when ``shuffle`` == 'True'."],"delete":["582","    random_state : RandomState or an int seed (0 by default)","583","        A random number generator instance to define the state of the","584","        random permutations generator.","745","    random_state : None, int or RandomState","746","        When shuffle=True, pseudo-random number generator state used for","747","        shuffling. If None, use default numpy RNG for shuffling."]}],"sklearn\/decomposition\/dict_learning.py":[{"add":["330","    random_state : int, RandomState instance or None, optional (default=None)","331","        If int, random_state is the seed used by the random number generator;","332","        If RandomState instance, random_state is the random number generator;","333","        If None, the random number generator is the RandomState instance used","334","        by `np.random`.","439","    random_state : int, RandomState instance or None, optional (default=None)","440","        If int, random_state is the seed used by the random number generator;","441","        If RandomState instance, random_state is the random number generator;","442","        If None, the random number generator is the RandomState instance used","443","        by `np.random`.","624","    random_state : int, RandomState instance or None, optional (default=None)","625","        If int, random_state is the seed used by the random number generator;","626","        If RandomState instance, random_state is the random number generator;","627","        If None, the random number generator is the RandomState instance used","628","        by `np.random`.","1011","    random_state : int, RandomState instance or None, optional (default=None)","1012","        If int, random_state is the seed used by the random number generator;","1013","        If RandomState instance, random_state is the random number generator;","1014","        If None, the random number generator is the RandomState instance used","1015","        by `np.random`.","1174","    random_state : int, RandomState instance or None, optional (default=None)","1175","        If int, random_state is the seed used by the random number generator;","1176","        If RandomState instance, random_state is the random number generator;","1177","        If None, the random number generator is the RandomState instance used","1178","        by `np.random`."],"delete":["330","    random_state : int or RandomState","331","        Pseudo number generator state used for random sampling.","436","    random_state : int or RandomState","437","        Pseudo number generator state used for random sampling.","618","    random_state : int or RandomState","619","        Pseudo number generator state used for random sampling.","1002","    random_state : int or RandomState","1003","        Pseudo number generator state used for random sampling.","1162","    random_state : int or RandomState","1163","        Pseudo number generator state used for random sampling."]}],"sklearn\/ensemble\/gradient_boosting.py":[{"add":["1769","        previous solution."],"delete":["1769","        p","1770","revious solution."]}],"sklearn\/dummy.py":[{"add":["49","    random_state : int, RandomState instance or None, optional, default=None","50","        If int, random_state is the seed used by the random number generator;","51","        If RandomState instance, random_state is the random number generator;","52","        If None, the random number generator is the RandomState instance used","53","        by `np.random`."],"delete":["49","    random_state : int seed, RandomState instance, or None (default)","50","        The seed of the pseudo random number generator to use."]}],"sklearn\/manifold\/t_sne.py":[{"add":["584","    random_state : int, RandomState instance or None, optional (default: None)","585","        If int, random_state is the seed used by the random number generator;","586","        If RandomState instance, random_state is the random number generator;","587","        If None, the random number generator is the RandomState instance used","588","        by `np.random`.  Note that different initializations might result in","589","        different local minima of the cost function."],"delete":["584","    random_state : int or RandomState instance or None (default)","585","        Pseudo Random Number generator seed control. If None, use the","586","        numpy.random singleton. Note that different initializations","587","        might result in different local minima of the cost function."]}],"sklearn\/decomposition\/sparse_pca.py":[{"add":["62","    random_state : int, RandomState instance or None, optional (default=None)","63","        If int, random_state is the seed used by the random number generator;","64","        If RandomState instance, random_state is the random number generator;","65","        If None, the random number generator is the RandomState instance used","66","        by `np.random`.","233","    random_state : int, RandomState instance or None, optional (default=None)","234","        If int, random_state is the seed used by the random number generator;","235","        If RandomState instance, random_state is the random number generator;","236","        If None, the random number generator is the RandomState instance used","237","        by `np.random`."],"delete":["62","    random_state : int or RandomState","63","        Pseudo number generator state used for random sampling.","230","    random_state : int or RandomState","231","        Pseudo number generator state used for random sampling."]}],"sklearn\/manifold\/locally_linear.py":[{"add":["142","    random_state : int, RandomState instance or None, optional (default=None)","143","        If int, random_state is the seed used by the random number generator;","144","        If RandomState instance, random_state is the random number generator;","145","        If None, the random number generator is the RandomState instance used","146","        by `np.random`. Used when ``solver`` == 'arpack'.","249","    random_state : int, RandomState instance or None, optional (default=None)","250","        If int, random_state is the seed used by the random number generator;","251","        If RandomState instance, random_state is the random number generator;","252","        If None, the random number generator is the RandomState instance used","253","        by `np.random`. Used when ``solver`` == 'arpack'.","574","    random_state : int, RandomState instance or None, optional (default=None)","575","        If int, random_state is the seed used by the random number generator;","576","        If RandomState instance, random_state is the random number generator;","577","        If None, the random number generator is the RandomState instance used","578","        by `np.random`. Used when ``eigen_solver`` == 'arpack'."],"delete":["142","    random_state : numpy.RandomState or int, optional","143","        The generator or seed used to determine the starting vector for arpack","144","        iterations.  Defaults to numpy.random.","247","    random_state : numpy.RandomState or int, optional","248","        The generator or seed used to determine the starting vector for arpack","249","        iterations.  Defaults to numpy.random.","570","    random_state : numpy.RandomState or int, optional","571","        The generator or seed used to determine the starting vector for arpack","572","        iterations.  Defaults to numpy.random."]}],"sklearn\/decomposition\/kernel_pca.py":[{"add":["76","    random_state : int, RandomState instance or None, optional (default=None)","77","        If int, random_state is the seed used by the random number generator;","78","        If RandomState instance, random_state is the random number generator;","79","        If None, the random number generator is the RandomState instance used","80","        by `np.random`. Used when ``eigen_solver`` == 'arpack'."],"delete":["76","    random_state : int seed, RandomState instance, or None, default=None","77","        A pseudo random number generator used for the initialization of the","78","        residuals when eigen_solver == 'arpack'."]}],"sklearn\/cluster\/k_means_.py":[{"add":["232","    random_state : int, RandomState instance or None, optional, default: None","233","        If int, random_state is the seed used by the random number generator;","234","        If RandomState instance, random_state is the random number generator;","235","        If None, the random number generator is the RandomState instance used","236","        by `np.random`.","452","    random_state : int, RandomState instance or None, optional, default: None","453","        If int, random_state is the seed used by the random number generator;","454","        If RandomState instance, random_state is the random number generator;","455","        If None, the random number generator is the RandomState instance used","456","        by `np.random`.","642","    random_state : int, RandomState instance or None, optional, default: None","643","        If int, random_state is the seed used by the random number generator;","644","        If RandomState instance, random_state is the random number generator;","645","        If None, the random number generator is the RandomState instance used","646","        by `np.random`.","771","    random_state : int, RandomState instance or None, optional, default: None","772","        If int, random_state is the seed used by the random number generator;","773","        If RandomState instance, random_state is the random number generator;","774","        If None, the random number generator is the RandomState instance used","775","        by `np.random`.","1014","    random_state : int, RandomState instance or None, optional, default: None","1015","        If int, random_state is the seed used by the random number generator;","1016","        If RandomState instance, random_state is the random number generator;","1017","        If None, the random number generator is the RandomState instance used","1018","        by `np.random`.","1254","    random_state : int, RandomState instance or None, optional, default: None","1255","        If int, random_state is the seed used by the random number generator;","1256","        If RandomState instance, random_state is the random number generator;","1257","        If None, the random number generator is the RandomState instance used","1258","        by `np.random`."],"delete":["232","    random_state : integer or numpy.RandomState, optional","233","        The generator used to initialize the centers. If an integer is","234","        given, it fixes the seed. Defaults to the global numpy random","235","        number generator.","451","    random_state : integer or numpy.RandomState, optional","452","        The generator used to initialize the centers. If an integer is","453","        given, it fixes the seed. Defaults to the global numpy random","454","        number generator.","640","    random_state : integer or numpy.RandomState, optional","641","        The generator used to initialize the centers. If an integer is","642","        given, it fixes the seed. Defaults to the global numpy random","643","        number generator.","768","    random_state : integer or numpy.RandomState, optional","769","        The generator used to initialize the centers. If an integer is","770","        given, it fixes the seed. Defaults to the global numpy random","771","        number generator.","1010","    random_state : integer or numpy.RandomState, optional","1011","        The generator used to initialize the centers. If an integer is","1012","        given, it fixes the seed. Defaults to the global numpy random","1013","        number generator.","1249","    random_state : integer or numpy.RandomState, optional","1250","        The generator used to initialize the centers. If an integer is","1251","        given, it fixes the seed. Defaults to the global numpy random","1252","        number generator."]}]}},"a0db45db0e86243757b53294db2483228c917ce0":{"changes":{"sklearn\/utils\/class_weight.py":"MODIFY","sklearn\/utils\/tests\/test_class_weight.py":"MODIFY"},"diff":{"sklearn\/utils\/class_weight.py":[{"add":["68","                raise ValueError(\"Class label {} not present.\".format(c))"],"delete":["68","                raise ValueError(\"Class label %d not present.\" % c)"]}],"sklearn\/utils\/tests\/test_class_weight.py":[{"add":["33","    # Fix exception in error message formatting when missing label is a string","34","    # https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/8312","35","    assert_raise_message(ValueError,","36","                         'Class label label_not_present not present',","37","                         compute_class_weight,","38","                         {'label_not_present': 1.}, classes, y)"],"delete":[]}]}},"aea6462b81e42ee5ef582de6da353f8869642735":{"changes":{"doc\/whats_new.rst":"MODIFY","sklearn\/datasets\/samples_generator.py":"MODIFY","sklearn\/datasets\/tests\/test_samples_generator.py":"MODIFY"},"diff":{"doc\/whats_new.rst":[{"add":["142","   - Fixed a bug where :func:`sklearn.datasets.make_moons` gives an","143","     incorrect result when ``n_samples`` is odd.","144","     :issue:`8198` by :user:`Josh Levy <levy5674>`.","145",""],"delete":[]}],"sklearn\/datasets\/samples_generator.py":[{"add":["667","    y = np.hstack([np.zeros(n_samples_out, dtype=np.intp),","668","                   np.ones(n_samples_in, dtype=np.intp)])"],"delete":["667","    y = np.hstack([np.zeros(n_samples_in, dtype=np.intp),","668","                   np.ones(n_samples_out, dtype=np.intp)])"]}],"sklearn\/datasets\/tests\/test_samples_generator.py":[{"add":["26","from sklearn.datasets import make_moons","363","","364","","365","def test_make_moons():","366","    X, y = make_moons(3, shuffle=False)","367","    for x, label in zip(X, y):","368","        center = [0.0, 0.0] if label == 0 else [1.0, 0.5]","369","        dist_sqr = ((x - center) ** 2).sum()","370","        assert_almost_equal(dist_sqr, 1.0,","371","                            err_msg=\"Point is not on expected unit circle\")"],"delete":[]}]}},"899885640626f442b5ea8bbbccb2889c40f17738":{"changes":{"sklearn\/linear_model\/ridge.py":"MODIFY"},"diff":{"sklearn\/linear_model\/ridge.py":[{"add":["887",""],"delete":["887","        w[v == 0] = 0"]}]}},"a36c852e0253138b8bae1aa2dc0ea49e3da1d390":{"changes":{"sklearn\/metrics\/tests\/test_common.py":"MODIFY"},"diff":{"sklearn\/metrics\/tests\/test_common.py":[{"add":["1015","    # regression","1016","    y_true = random_state.random_sample(size=(n_samples,))","1017","    y_pred = random_state.random_sample(size=(n_samples,))","1018","    for name in ALL_METRICS:","1019","        if name not in REGRESSION_METRICS:","1020","            continue","1021","        if name in METRICS_WITHOUT_SAMPLE_WEIGHT:","1022","            continue","1023","        metric = ALL_METRICS[name]","1024","        yield _named_check(check_sample_weight_invariance, name), name,\\","1025","            metric, y_true, y_pred","1033","        if name in REGRESSION_METRICS:","1034","            continue","1052","        if name in REGRESSION_METRICS:","1053","            continue"],"delete":[]}]}},"719afba518e6c8f71a0d98faf2263a7312ac22a1":{"changes":{"examples\/model_selection\/plot_grid_search_digits.py":"ADD","\/dev\/null":"DELETE","examples\/exercises\/plot_digits_classification_exercise.py":"ADD","examples\/plot_missing_values.py":"ADD","examples\/applications\/plot_topics_extraction_with_nmf_lda.py":"ADD","examples\/applications\/plot_face_recognition.py":"ADD","examples\/feature_selection\/plot_feature_selection_pipeline.py":"ADD","examples\/bicluster\/plot_bicluster_newsgroups.py":"ADD","examples\/linear_model\/plot_lasso_dense_vs_sparse_data.py":"ADD","examples\/model_selection\/plot_randomized_search.py":"ADD","examples\/plot_feature_stacker.py":"ADD"},"diff":{"examples\/model_selection\/plot_grid_search_digits.py":[{"add":[],"delete":[]}],"\/dev\/null":[{"add":[],"delete":[]}],"examples\/exercises\/plot_digits_classification_exercise.py":[{"add":[],"delete":[]}],"examples\/plot_missing_values.py":[{"add":[],"delete":[]}],"examples\/applications\/plot_topics_extraction_with_nmf_lda.py":[{"add":[],"delete":[]}],"examples\/applications\/plot_face_recognition.py":[{"add":[],"delete":[]}],"examples\/feature_selection\/plot_feature_selection_pipeline.py":[{"add":[],"delete":[]}],"examples\/bicluster\/plot_bicluster_newsgroups.py":[{"add":[],"delete":[]}],"examples\/linear_model\/plot_lasso_dense_vs_sparse_data.py":[{"add":[],"delete":[]}],"examples\/model_selection\/plot_randomized_search.py":[{"add":[],"delete":[]}],"examples\/plot_feature_stacker.py":[{"add":[],"delete":[]}]}},"3dc8d2f83bfb94dbb5a7190bac39e046063db4e0":{"changes":{"examples\/text\/document_classification_20newsgroups.py":"MODIFY"},"diff":{"examples\/text\/document_classification_20newsgroups.py":[{"add":["36","from sklearn.feature_selection import SelectFromModel","262","    results.append(benchmark(LinearSVC(penalty=penalty, dual=False,","263","                                       tol=1e-3)))","291","  ('feature_selection', SelectFromModel(LinearSVC(penalty=\"l1\", dual=False,","292","                                                  tol=1e-3))),","293","  ('classification', LinearSVC(penalty=\"l2\"))])))"],"delete":["261","    results.append(benchmark(LinearSVC(loss='l2', penalty=penalty,","262","                                            dual=False, tol=1e-3)))","290","  ('feature_selection', LinearSVC(penalty=\"l1\", dual=False, tol=1e-3)),","291","  ('classification', LinearSVC())","292","])))"]}]}},"676f878e9815dee360e8ea6abb5d233cc82af025":{"changes":{"sklearn\/ensemble\/gradient_boosting.py":"MODIFY"},"diff":{"sklearn\/ensemble\/gradient_boosting.py":[{"add":["1435","                 min_impurity_split=None, init=None,","1842","                 min_impurity_split=None, init=None, random_state=None,"],"delete":["1435","                 min_impurity_split=1e-7, init=None,","1842","                 min_impurity_split=1e-7, init=None, random_state=None,"]}]}},"2a36ff1e483b2b772569029c884e1e9a60b577c5":{"changes":{"doc\/whats_new.rst":"MODIFY","sklearn\/decomposition\/pca.py":"MODIFY","sklearn\/decomposition\/incremental_pca.py":"MODIFY","sklearn\/decomposition\/tests\/test_pca.py":"MODIFY"},"diff":{"doc\/whats_new.rst":[{"add":["415","   - Fixed the implementation of `explained_variance_`","416","     in :class:`decomposition.PCA`,","417","     :class:`decomposition.RandomizedPCA` and","418","     :class:`decomposition.IncrementalPCA`.","419","     :issue:`9105` by `Hanmin Qin <https:\/\/github.com\/qinhanmin2014>`_. ","420",""],"delete":[]}],"sklearn\/decomposition\/pca.py":[{"add":["342","            U *= sqrt(X.shape[0] - 1)","412","        explained_variance_ = (S ** 2) \/ (n_samples - 1)","491","        self.explained_variance_ = (S ** 2) \/ (n_samples - 1)","492","        total_var = np.var(X, ddof=1, axis=0)","710","        self.explained_variance_ = exp_var = (S ** 2) \/ (n_samples - 1)","711","        full_var = np.var(X, ddof=1, axis=0).sum()"],"delete":["287","    Notes","288","    -----","289","    PCA uses the maximum likelihood estimate of the eigenvalues, which does not","290","    include the Bessel correction, though in practice this should rarely make a","291","    difference in a machine learning context.","292","","348","            U *= sqrt(X.shape[0])","418","        explained_variance_ = (S ** 2) \/ n_samples","497","        self.explained_variance_ = (S ** 2) \/ n_samples","498","        total_var = np.var(X, axis=0)","716","        self.explained_variance_ = exp_var = (S ** 2) \/ n_samples","717","        full_var = np.var(X, axis=0).sum()"]}],"sklearn\/decomposition\/incremental_pca.py":[{"add":["253","        explained_variance = S ** 2 \/ (n_total_samples - 1)"],"delete":["253","        explained_variance = S ** 2 \/ n_total_samples"]}],"sklearn\/decomposition\/tests\/test_pca.py":[{"add":["176","        assert_almost_equal(X_whitened.std(ddof=1, axis=0),","177","                            np.ones(n_components),","216","    expected_result = np.linalg.eig(np.cov(X, rowvar=False))[0]","217","    expected_result = sorted(expected_result, reverse=True)[:2]","218","","221","                              np.var(X_pca, ddof=1, axis=0))","222","    assert_array_almost_equal(pca.explained_variance_, expected_result)","226","                              np.var(X_pca, ddof=1, axis=0))","227","    assert_array_almost_equal(apca.explained_variance_, expected_result)","230","    assert_array_almost_equal(rpca.explained_variance_,","231","                              np.var(X_rpca, ddof=1, axis=0),","233","    assert_array_almost_equal(rpca.explained_variance_,","234","                              expected_result, decimal=1)"],"delete":["176","        assert_almost_equal(X_whitened.std(axis=0), np.ones(n_components),","217","                              np.var(X_pca, axis=0))","221","                              np.var(X_pca, axis=0))","224","    assert_array_almost_equal(rpca.explained_variance_, np.var(X_rpca, axis=0),"]}]}},"b4a12b18a8f9c274c637bf8f5a26282aa4249d85":{"changes":{"examples\/cross_decomposition\/plot_compare_cross_decomposition.py":"MODIFY"},"diff":{"examples\/cross_decomposition\/plot_compare_cross_decomposition.py":[{"add":["146","X_train_r, Y_train_r = cca.transform(X_train, Y_train)","147","X_test_r, Y_test_r = cca.transform(X_test, Y_test)"],"delete":["146","X_train_r, Y_train_r = plsca.transform(X_train, Y_train)","147","X_test_r, Y_test_r = plsca.transform(X_test, Y_test)"]}]}},"4e56b293d66f0f278080b40851770754880a048c":{"changes":{"sklearn\/_isotonic.pyx":"MODIFY","doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/isotonic.py":"MODIFY","sklearn\/tests\/test_isotonic.py":"MODIFY"},"diff":{"sklearn\/_isotonic.pyx":[{"add":["9","from cython cimport floating","15","def _inplace_contiguous_isotonic_regression(floating[::1] y, floating[::1] w):","18","        floating prev_y, sum_wy, sum_w","69","def _make_unique(np.ndarray[dtype=floating] X,","70","                 np.ndarray[dtype=floating] y,","71","                 np.ndarray[dtype=floating] sample_weights):","84","    cdef np.ndarray[dtype=floating] y_out = np.empty(unique_values,","85","                                                     dtype=X.dtype)","86","    cdef np.ndarray[dtype=floating] x_out = np.empty_like(y_out)","87","    cdef np.ndarray[dtype=floating] weights_out = np.empty_like(y_out)","88","","89","    cdef floating current_x = X[0]","90","    cdef floating current_y = 0","91","    cdef floating current_weight = 0","92","    cdef floating y_old = 0","96","    cdef floating x"],"delete":["9","","10","ctypedef np.float64_t DOUBLE","16","def _inplace_contiguous_isotonic_regression(DOUBLE[::1] y, DOUBLE[::1] w):","19","        DOUBLE prev_y, sum_wy, sum_w","70","def _make_unique(np.ndarray[dtype=np.float64_t] X,","71","                 np.ndarray[dtype=np.float64_t] y,","72","                 np.ndarray[dtype=np.float64_t] sample_weights):","84","    cdef np.ndarray[dtype=np.float64_t] y_out = np.empty(unique_values)","85","    cdef np.ndarray[dtype=np.float64_t] x_out = np.empty(unique_values)","86","    cdef np.ndarray[dtype=np.float64_t] weights_out = np.empty(unique_values)","88","    cdef np.float64_t current_x = X[0]","89","    cdef np.float64_t current_y = 0","90","    cdef np.float64_t current_weight = 0","91","    cdef np.float64_t y_old = 0","95","    cdef np.float64_t x"]}],"doc\/whats_new\/v0.21.rst":[{"add":["330",":mod:`sklearn.isotonic`","331",".......................","332","","333","- |Feature| Allow different dtypes (such as float32) in","334","  :class:`isotonic.IsotonicRegression` :issue:`8769` by :user:`Vlad Niculae <vene>`","335","","336",""],"delete":[]}],"sklearn\/isotonic.py":[{"add":["121","    y = as_float_array(y)","122","    y = np.array(y[order], dtype=y.dtype)","124","        sample_weight = np.ones(len(y), dtype=y.dtype)","126","        sample_weight = np.array(sample_weight[order], dtype=y.dtype)","242","        X = as_float_array(X)","243","        y = check_array(y, dtype=X.dtype, ensure_2d=False)","256","            sample_weight = check_array(sample_weight, ensure_2d=False,","257","                                        dtype=X.dtype)","261","            sample_weight = np.ones(len(y), dtype=X.dtype)","264","        X, y, sample_weight = [array[order] for array in [X, y, sample_weight]]","347","","348","        if hasattr(self, '_necessary_X_'):","349","            dtype = self._necessary_X_.dtype","350","        else:","351","            dtype = np.float64","352","","353","        T = check_array(T, dtype=dtype, ensure_2d=False)","354","","366","","367","        res = self.f_(T)","368","","369","        # on scipy 0.17, interp1d up-casts to float64, so we cast back","370","        res = res.astype(T.dtype)","371","","372","        return res"],"delete":["121","    y = np.array(y[order], dtype=np.float64)","123","        sample_weight = np.ones(len(y), dtype=np.float64)","125","        sample_weight = np.array(sample_weight[order], dtype=np.float64)","242","        X, y = [check_array(x, ensure_2d=False) for x in [X, y]]","243","","244","        y = as_float_array(y)","256","            sample_weight = check_array(sample_weight, ensure_2d=False)","260","            sample_weight = np.ones(len(y))","263","        X, y, sample_weight = [array[order].astype(np.float64, copy=False)","264","                               for array in [X, y, sample_weight]]","347","        T = as_float_array(T)","359","        return self.f_(T)"]}],"sklearn\/tests\/test_isotonic.py":[{"add":["6","                              IsotonicRegression, _make_unique)","8","from sklearn.utils.validation import as_float_array","462","","463","","464","def test_isotonic_dtype():","465","    y = [2, 1, 4, 3, 5]","466","    weights = np.array([.9, .9, .9, .9, .9], dtype=np.float64)","467","    reg = IsotonicRegression()","468","","469","    for dtype in (np.int32, np.int64, np.float32, np.float64):","470","        for sample_weight in (None, weights.astype(np.float32), weights):","471","            y_np = np.array(y, dtype=dtype)","472","            expected_dtype = as_float_array(y_np).dtype","473","","474","            res = isotonic_regression(y_np, sample_weight=sample_weight)","475","            assert_equal(res.dtype, expected_dtype)","476","","477","            X = np.arange(len(y)).astype(dtype)","478","            reg.fit(X, y_np, sample_weight=sample_weight)","479","            res = reg.predict(X)","480","            assert_equal(res.dtype, expected_dtype)","481","","482","","483","def test_make_unique_dtype():","484","    x_list = [2, 2, 2, 3, 5]","485","    for dtype in (np.float32, np.float64):","486","        x = np.array(x_list, dtype=dtype)","487","        y = x.copy()","488","        w = np.ones_like(x)","489","        x, y, w = _make_unique(x, y, w)","490","        assert_array_equal(x, [2, 3, 5])"],"delete":["6","                              IsotonicRegression)"]}]}},"ee82c3f357dbacb2e3d1ca6640b17fd73fd2ade1":{"changes":{"sklearn\/base.py":"MODIFY"},"diff":{"sklearn\/base.py":[{"add":["41","        If safe is false, clone will fall back to a deep copy on objects","261","            # Simple optimization to gain speed (inspect is slow)"],"delete":["41","        If safe is false, clone will fall back to a deepcopy on objects","261","            # Simple optimisation to gain speed (inspect is slow)"]}]}}}