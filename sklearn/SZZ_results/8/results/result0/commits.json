{"e548ea6d7396e3cd935a71e6e3379ce61fd4bf70":{"changes":{"doc\/themes\/scikit-learn\/layout.html":"MODIFY","doc\/themes\/scikit-learn\/static\/nature.css_t":"MODIFY"},"diff":{"doc\/themes\/scikit-learn\/layout.html":[{"add":["104","                <div class=\"gcse-search\" id=\"cse\" style=\"width: 100%;\"><\/div>","348","    <script>","349","      (function() {","350","        var cx = '016639176250731907682:tjtqbvtvij0';","351","        var gcse = document.createElement('script'); gcse.type = 'text\/javascript'; gcse.async = true;","352","        gcse.src = 'https:\/\/cse.google.com\/cse.js?cx=' + cx;","353","        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(gcse, s);","354","      })();"],"delete":["104","                <div id=\"cse\" style=\"width: 100%;\"><\/div>","348","","349","    <script src=\"http:\/\/www.google.com\/jsapi\" type=\"text\/javascript\"><\/script>","350","    <script type=\"text\/javascript\"> google.load('search', '1',","351","        {language : 'en'}); google.setOnLoadCallback(function() {","352","            var customSearchControl = new","353","            google.search.CustomSearchControl('016639176250731907682:tjtqbvtvij0');","354","            customSearchControl.setResultSetSize(google.search.Search.FILTERED_CSE_RESULTSET);","355","            var options = new google.search.DrawOptions();","356","            options.setAutoComplete(true);","357","            customSearchControl.draw('cse', options); }, true);"]}],"doc\/themes\/scikit-learn\/static\/nature.css_t":[{"add":["209","    margin-top: -40px;","215","    position: relative;","216","    top: -2px;","220","    display: none !important;","221","}","222","","223","form.gsc-search-box {","224","    padding: 0 !important;","225","}","226","","227","input.gsc-search-button {","228","    position: relative;","229","    top: -4px;","230","    border-radius: 5px !important;","231","    border-color: #FFFFFF !important;","232","    background-color: #ff9c34 !important;","233","}","234","","235","a.gs-title, a.gs-title > b{","236","    color: blue !important;","237","}","238","","239",".gsc-results .gsc-cursor-box .gsc-cursor-current-page {","240","    border-color: white !important;","241","    background-color: #ff9c34 !important;","242","    color: white !important;"],"delete":["209","    margin-top: -23px;","210","    \/*The min-height is added here, to prevent the element from shrinking","211","    too much, while the scripts are still loading the search-bar.","212","    Without it, layout glitches occur, as the element keeps dynamically","213","    changing its size while its loading-contents adjusts into position.*\/","222","    display: none;"]}]}},"964aedbebc40225f5845e807e1ed4068196c4a30":{"changes":{"sklearn\/neighbors\/classification.py":"MODIFY","sklearn\/neighbors\/regression.py":"MODIFY"},"diff":{"sklearn\/neighbors\/classification.py":[{"add":["63","    metric : string or callable, default 'minkowski'","270","    metric : string or callable, default 'minkowski'"],"delete":["63","    metric : string or DistanceMetric object (default = 'minkowski')","270","    metric : string or DistanceMetric object (default='minkowski')"]}],"sklearn\/neighbors\/regression.py":[{"add":["215","    metric : string or callable, default 'minkowski'"],"delete":["215","    metric : string or DistanceMetric object (default='minkowski')"]}]}},"d196ea3cc558723e25f089a4826924a6f4e1a62a":{"changes":{"sklearn\/linear_model\/coordinate_descent.py":"MODIFY","sklearn\/linear_model\/omp.py":"MODIFY","sklearn\/linear_model\/base.py":"MODIFY","sklearn\/linear_model\/bayes.py":"MODIFY","sklearn\/linear_model\/least_angle.py":"MODIFY","sklearn\/linear_model\/ridge.py":"MODIFY"},"diff":{"sklearn\/linear_model\/coordinate_descent.py":[{"add":["67","        This parameter is ignored when ``fit_intercept`` is set to False.","68","        If True, the regressors X will be normalized before regression by","69","        subtracting the mean and dividing by the l2-norm.","70","        If you wish to standardize, please use","71","        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``","72","        on an estimator with ``normalize=False``.","368","    See examples\/linear_model\/plot_lasso_coordinate_descent_path.py for an","369","    example.","531","        parameter.``alpha = 0`` is equivalent to an ordinary least square,","532","        solved by the :class:`LinearRegression` object. For numerical","547","        This parameter is ignored when ``fit_intercept`` is set to False.","548","        If True, the regressors X will be normalized before regression by","549","        subtracting the mean and dividing by the l2-norm.","550","        If you wish to standardize, please use","551","        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``","552","        on an estimator with ``normalize=False``.","793","        This parameter is ignored when ``fit_intercept`` is set to False.","794","        If True, the regressors X will be normalized before regression by","795","        subtracting the mean and dividing by the l2-norm.","796","        If you wish to standardize, please use","797","        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``","798","        on an estimator with ``normalize=False``.","1279","        This parameter is ignored when ``fit_intercept`` is set to False.","1280","        If True, the regressors X will be normalized before regression by","1281","        subtracting the mean and dividing by the l2-norm.","1282","        If you wish to standardize, please use","1283","        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``","1284","        on an estimator with ``normalize=False``.","1431","        This parameter is ignored when ``fit_intercept`` is set to False.","1432","        If True, the regressors X will be normalized before regression by","1433","        subtracting the mean and dividing by the l2-norm.","1434","        If you wish to standardize, please use","1435","        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``","1436","        on an estimator with ``normalize=False``.","1561","        This parameter is ignored when ``fit_intercept`` is set to False.","1562","        If True, the regressors X will be normalized before regression by","1563","        subtracting the mean and dividing by the l2-norm.","1564","        If you wish to standardize, please use","1565","        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``","1566","        on an estimator with ``normalize=False``.","1745","        This parameter is ignored when ``fit_intercept`` is set to False.","1746","        If True, the regressors X will be normalized before regression by","1747","        subtracting the mean and dividing by the l2-norm.","1748","        If you wish to standardize, please use","1749","        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``","1750","        on an estimator with ``normalize=False``.","1880","        This parameter is ignored when ``fit_intercept`` is set to False.","1881","        If True, the regressors X will be normalized before regression by","1882","        subtracting the mean and dividing by the l2-norm.","1883","        If you wish to standardize, please use","1884","        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``","1885","        on an estimator with ``normalize=False``.","2044","        This parameter is ignored when ``fit_intercept`` is set to False.","2045","        If True, the regressors X will be normalized before regression by","2046","        subtracting the mean and dividing by the l2-norm.","2047","        If you wish to standardize, please use","2048","        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``","2049","        on an estimator with ``normalize=False``."],"delete":["67","        If ``True``, the regressors X will be normalized before regression.","68","        This parameter is ignored when ``fit_intercept`` is set to ``False``.","69","        When the regressors are normalized, note that this makes the","70","        hyperparameters learnt more robust and almost independent of the number","71","        of samples. The same property is not valid for standardized data.","72","        However, if you wish to standardize, please use","73","        :class:`preprocessing.StandardScaler` before calling ``fit`` on an estimator","74","        with ``normalize=False``.","370","    See examples\/linear_model\/plot_lasso_coordinate_descent_path.py for an example.","532","        parameter.``alpha = 0`` is equivalent to an ordinary least square, solved","533","        by the :class:`LinearRegression` object. For numerical","548","        If ``True``, the regressors X will be normalized before regression.","549","        This parameter is ignored when ``fit_intercept`` is set to ``False``.","550","        When the regressors are normalized, note that this makes the","551","        hyperparameters learnt more robust and almost independent of the number","552","        of samples. The same property is not valid for standardized data.","553","        However, if you wish to standardize, please use","554","        :class:`preprocessing.StandardScaler` before calling ``fit`` on an estimator","555","        with ``normalize=False``.","796","        If ``True``, the regressors X will be normalized before regression.","797","        This parameter is ignored when ``fit_intercept`` is set to ``False``.","798","        When the regressors are normalized, note that this makes the","799","        hyperparameters learnt more robust and almost independent of the number","800","        of samples. The same property is not valid for standardized data.","801","        However, if you wish to standardize, please use","802","        :class:`preprocessing.StandardScaler` before calling ``fit`` on an estimator","803","        with ``normalize=False``.","1284","        If ``True``, the regressors X will be normalized before regression.","1285","        This parameter is ignored when ``fit_intercept`` is set to ``False``.","1286","        When the regressors are normalized, note that this makes the","1287","        hyperparameters learnt more robust and almost independent of the number","1288","        of samples. The same property is not valid for standardized data.","1289","        However, if you wish to standardize, please use","1290","        :class:`preprocessing.StandardScaler` before calling ``fit`` on an estimator","1291","        with ``normalize=False``.","1438","        If ``True``, the regressors X will be normalized before regression.","1439","        This parameter is ignored when ``fit_intercept`` is set to ``False``.","1440","        When the regressors are normalized, note that this makes the","1441","        hyperparameters learnt more robust and almost independent of the number","1442","        of samples. The same property is not valid for standardized data.","1443","        However, if you wish to standardize, please use","1444","        :class:`preprocessing.StandardScaler` before calling ``fit`` on an estimator","1445","        with ``normalize=False``.","1570","        If ``True``, the regressors X will be normalized before regression.","1571","        This parameter is ignored when ``fit_intercept`` is set to ``False``.","1572","        When the regressors are normalized, note that this makes the","1573","        hyperparameters learnt more robust and almost independent of the number","1574","        of samples. The same property is not valid for standardized data.","1575","        However, if you wish to standardize, please use","1576","        :class:`preprocessing.StandardScaler` before calling ``fit`` on an estimator","1577","        with ``normalize=False``.","1756","        If ``True``, the regressors X will be normalized before regression.","1757","        This parameter is ignored when ``fit_intercept`` is set to ``False``.","1758","        When the regressors are normalized, note that this makes the","1759","        hyperparameters learnt more robust and almost independent of the number","1760","        of samples. The same property is not valid for standardized data.","1761","        However, if you wish to standardize, please use","1762","        :class:`preprocessing.StandardScaler` before calling ``fit`` on an estimator","1763","        with ``normalize=False``.","1893","        If ``True``, the regressors X will be normalized before regression.","1894","        This parameter is ignored when ``fit_intercept`` is set to ``False``.","1895","        When the regressors are normalized, note that this makes the","1896","        hyperparameters learnt more robust and almost independent of the number","1897","        of samples. The same property is not valid for standardized data.","1898","        However, if you wish to standardize, please use","1899","        :class:`preprocessing.StandardScaler` before calling ``fit`` on an estimator","1900","        with ``normalize=False``.","2059","        If ``True``, the regressors X will be normalized before regression.","2060","        This parameter is ignored when ``fit_intercept`` is set to ``False``.","2061","        When the regressors are normalized, note that this makes the","2062","        hyperparameters learnt more robust and almost independent of the number","2063","        of samples. The same property is not valid for standardized data.","2064","        However, if you wish to standardize, please use","2065","        :class:`preprocessing.StandardScaler` before calling ``fit`` on an estimator","2066","        with ``normalize=False``."]}],"sklearn\/linear_model\/omp.py":[{"add":["560","        This parameter is ignored when ``fit_intercept`` is set to False.","561","        If True, the regressors X will be normalized before regression by","562","        subtracting the mean and dividing by the l2-norm.","563","        If you wish to standardize, please use","564","        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``","565","        on an estimator with ``normalize=False``.","696","        This parameter is ignored when ``fit_intercept`` is set to False.","697","        If True, the regressors X will be normalized before regression by","698","        subtracting the mean and dividing by the l2-norm.","699","        If you wish to standardize, please use","700","        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``","701","        on an estimator with ``normalize=False``.","761","        This parameter is ignored when ``fit_intercept`` is set to False.","762","        If True, the regressors X will be normalized before regression by","763","        subtracting the mean and dividing by the l2-norm.","764","        If you wish to standardize, please use","765","        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``","766","        on an estimator with ``normalize=False``."],"delete":["560","        If True, the regressors X will be normalized before regression.","561","        This parameter is ignored when `fit_intercept` is set to `False`.","562","        When the regressors are normalized, note that this makes the","563","        hyperparameters learnt more robust and almost independent of the number","564","        of samples. The same property is not valid for standardized data.","565","        However, if you wish to standardize, please use","566","        `preprocessing.StandardScaler` before calling `fit` on an estimator","567","        with `normalize=False`.","698","        If True, the regressors X will be normalized before regression.","699","        This parameter is ignored when `fit_intercept` is set to `False`.","700","        When the regressors are normalized, note that this makes the","701","        hyperparameters learnt more robust and almost independent of the number","702","        of samples. The same property is not valid for standardized data.","703","        However, if you wish to standardize, please use","704","        `preprocessing.StandardScaler` before calling `fit` on an estimator","705","        with `normalize=False`.","765","        If True, the regressors X will be normalized before regression.","766","        This parameter is ignored when `fit_intercept` is set to `False`.","767","        When the regressors are normalized, note that this makes the","768","        hyperparameters learnt more robust and almost independent of the number","769","        of samples. The same property is not valid for standardized data.","770","        However, if you wish to standardize, please use","771","        `preprocessing.StandardScaler` before calling `fit` on an estimator","772","        with `normalize=False`."]}],"sklearn\/linear_model\/base.py":[{"add":["412","        This parameter is ignored when ``fit_intercept`` is set to False.","413","        If True, the regressors X will be normalized before regression by","414","        subtracting the mean and dividing by the l2-norm.","415","        If you wish to standardize, please use","416","        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit`` on","417","        an estimator with ``normalize=False``."],"delete":["412","        If True, the regressors X will be normalized before regression.","413","        This parameter is ignored when `fit_intercept` is set to False.","414","        When the regressors are normalized, note that this makes the","415","        hyperparameters learnt more robust and almost independent of the number","416","        of samples. The same property is not valid for standardized data.","417","        However, if you wish to standardize, please use","418","        `preprocessing.StandardScaler` before calling `fit` on an estimator","419","        with `normalize=False`."]}],"sklearn\/linear_model\/bayes.py":[{"add":["66","        This parameter is ignored when ``fit_intercept`` is set to False.","67","        If True, the regressors X will be normalized before regression by","68","        subtracting the mean and dividing by the l2-norm.","69","        If you wish to standardize, please use","70","        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``","71","        on an estimator with ``normalize=False``.","329","        This parameter is ignored when ``fit_intercept`` is set to False.","330","        If True, the regressors X will be normalized before regression by","331","        subtracting the mean and dividing by the l2-norm.","332","        If you wish to standardize, please use","333","        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``","334","        on an estimator with ``normalize=False``."],"delete":["66","        If True, the regressors X will be normalized before regression.","67","        This parameter is ignored when `fit_intercept` is set to False.","68","        When the regressors are normalized, note that this makes the","69","        hyperparameters learnt more robust and almost independent of the number","70","        of samples. The same property is not valid for standardized data.","71","        However, if you wish to standardize, please use","72","        `preprocessing.StandardScaler` before calling `fit` on an estimator","73","        with `normalize=False`.","331","        If True, the regressors X will be normalized before regression.","332","        This parameter is ignored when `fit_intercept` is set to False.","333","        When the regressors are normalized, note that this makes the","334","        hyperparameters learnt more robust and almost independent of the number","335","        of samples. The same property is not valid for standardized data.","336","        However, if you wish to standardize, please use","337","        `preprocessing.StandardScaler` before calling `fit` on an estimator","338","        with `normalize=False`."]}],"sklearn\/linear_model\/least_angle.py":[{"add":["516","        This parameter is ignored when ``fit_intercept`` is set to False.","517","        If True, the regressors X will be normalized before regression by","518","        subtracting the mean and dividing by the l2-norm.","519","        If you wish to standardize, please use","520","        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``","521","        on an estimator with ``normalize=False``.","747","        This parameter is ignored when ``fit_intercept`` is set to False.","748","        If True, the regressors X will be normalized before regression by","749","        subtracting the mean and dividing by the l2-norm.","750","        If you wish to standardize, please use","751","        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``","752","        on an estimator with ``normalize=False``.","902","        This parameter is ignored when ``fit_intercept`` is set to False.","903","        If True, the regressors X will be normalized before regression by","904","        subtracting the mean and dividing by the l2-norm.","905","        If you wish to standardize, please use","906","        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``","907","        on an estimator with ``normalize=False``.","986","        This parameter is ignored when ``fit_intercept`` is set to False.","987","        If True, the regressors X will be normalized before regression by","988","        subtracting the mean and dividing by the l2-norm.","989","        If you wish to standardize, please use","990","        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``","991","        on an estimator with ``normalize=False``.","1198","        This parameter is ignored when ``fit_intercept`` is set to False.","1199","        If True, the regressors X will be normalized before regression by","1200","        subtracting the mean and dividing by the l2-norm.","1201","        If you wish to standardize, please use","1202","        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``","1203","        on an estimator with ``normalize=False``.","1331","        This parameter is ignored when ``fit_intercept`` is set to False.","1332","        If True, the regressors X will be normalized before regression by","1333","        subtracting the mean and dividing by the l2-norm.","1334","        If you wish to standardize, please use","1335","        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``","1336","        on an estimator with ``normalize=False``."],"delete":["516","        If True, the regressors X will be normalized before regression.","517","        This parameter is ignored when `fit_intercept` is set to False.","518","        When the regressors are normalized, note that this makes the","519","        hyperparameters learnt more robust and almost independent of the number","520","        of samples. The same property is not valid for standardized data.","521","        However, if you wish to standardize, please use","522","        `preprocessing.StandardScaler` before calling `fit` on an estimator","523","        with `normalize=False`.","749","        If True, the regressors X will be normalized before regression.","750","        This parameter is ignored when `fit_intercept` is set to False.","751","        When the regressors are normalized, note that this makes the","752","        hyperparameters learnt more robust and almost independent of the number","753","        of samples. The same property is not valid for standardized data.","754","        However, if you wish to standardize, please use","755","        `preprocessing.StandardScaler` before calling `fit` on an estimator","756","        with `normalize=False`.","906","        If True, the regressors X will be normalized before regression.","907","        This parameter is ignored when `fit_intercept` is set to False.","908","        When the regressors are normalized, note that this makes the","909","        hyperparameters learnt more robust and almost independent of the number","910","        of samples. The same property is not valid for standardized data.","911","        However, if you wish to standardize, please use","912","        `preprocessing.StandardScaler` before calling `fit` on an estimator","913","        with `normalize=False`.","992","        If True, the regressors X will be normalized before regression.","993","        This parameter is ignored when `fit_intercept` is set to False.","994","        When the regressors are normalized, note that this makes the","995","        hyperparameters learnt more robust and almost independent of the number","996","        of samples. The same property is not valid for standardized data.","997","        However, if you wish to standardize, please use","998","        `preprocessing.StandardScaler` before calling `fit` on an estimator","999","        with `normalize=False`.","1206","        If True, the regressors X will be normalized before regression.","1207","        This parameter is ignored when `fit_intercept` is set to False.","1208","        When the regressors are normalized, note that this makes the","1209","        hyperparameters learnt more robust and almost independent of the number","1210","        of samples. The same property is not valid for standardized data.","1211","        However, if you wish to standardize, please use","1212","        `preprocessing.StandardScaler` before calling `fit` on an estimator","1213","        with `normalize=False`.","1341","        If True, the regressors X will be normalized before regression.","1342","        This parameter is ignored when `fit_intercept` is set to False.","1343","        When the regressors are normalized, note that this makes the","1344","        hyperparameters learnt more robust and almost independent of the number","1345","        of samples. The same property is not valid for standardized data.","1346","        However, if you wish to standardize, please use","1347","        `preprocessing.StandardScaler` before calling `fit` on an estimator","1348","        with `normalize=False`."]}],"sklearn\/linear_model\/ridge.py":[{"add":["534","        This parameter is ignored when ``fit_intercept`` is set to False.","535","        If True, the regressors X will be normalized before regression by","536","        subtracting the mean and dividing by the l2-norm.","537","        If you wish to standardize, please use","538","        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``","539","        on an estimator with ``normalize=False``.","686","        This parameter is ignored when ``fit_intercept`` is set to False.","687","        If True, the regressors X will be normalized before regression by","688","        subtracting the mean and dividing by the l2-norm.","689","        If you wish to standardize, please use","690","        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``","691","        on an estimator with ``normalize=False``.","1139","        This parameter is ignored when ``fit_intercept`` is set to False.","1140","        If True, the regressors X will be normalized before regression by","1141","        subtracting the mean and dividing by the l2-norm.","1142","        If you wish to standardize, please use","1143","        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``","1144","        on an estimator with ``normalize=False``.","1240","        This parameter is ignored when ``fit_intercept`` is set to False.","1241","        If True, the regressors X will be normalized before regression by","1242","        subtracting the mean and dividing by the l2-norm.","1243","        If you wish to standardize, please use","1244","        :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``","1245","        on an estimator with ``normalize=False``."],"delete":["534","        If True, the regressors X will be normalized before regression.","535","        This parameter is ignored when `fit_intercept` is set to False.","536","        When the regressors are normalized, note that this makes the","537","        hyperparameters learnt more robust and almost independent of the number","538","        of samples. The same property is not valid for standardized data.","539","        However, if you wish to standardize, please use","540","        `preprocessing.StandardScaler` before calling `fit` on an estimator","541","        with `normalize=False`.","688","        If True, the regressors X will be normalized before regression.","689","        This parameter is ignored when `fit_intercept` is set to False.","690","        When the regressors are normalized, note that this makes the","691","        hyperparameters learnt more robust and almost independent of the number","692","        of samples. The same property is not valid for standardized data.","693","        However, if you wish to standardize, please use","694","        `preprocessing.StandardScaler` before calling `fit` on an estimator","695","        with `normalize=False`.","1143","        If True, the regressors X will be normalized before regression.","1144","        This parameter is ignored when `fit_intercept` is set to False.","1145","        When the regressors are normalized, note that this makes the","1146","        hyperparameters learnt more robust and almost independent of the number","1147","        of samples. The same property is not valid for standardized data.","1148","        However, if you wish to standardize, please use","1149","        `preprocessing.StandardScaler` before calling `fit` on an estimator","1150","        with `normalize=False`.","1246","        If True, the regressors X will be normalized before regression.","1247","        This parameter is ignored when `fit_intercept` is set to False.","1248","        When the regressors are normalized, note that this makes the","1249","        hyperparameters learnt more robust and almost independent of the number","1250","        of samples. The same property is not valid for standardized data.","1251","        However, if you wish to standardize, please use","1252","        `preprocessing.StandardScaler` before calling `fit` on an estimator","1253","        with `normalize=False`."]}]}},"38d59c7b5a8d0658a0747b5e0ea37d0768524f62":{"changes":{"doc\/whats_new.rst":"MODIFY","sklearn\/linear_model\/randomized_l1.py":"MODIFY"},"diff":{"doc\/whats_new.rst":[{"add":["152","   - Fixed a bug where :class:`sklearn.linear_model.RandomizedLasso` and","153","     :class:`sklearn.linear_model.RandomizedLogisticRegression` breaks for","154","     sparse input.","155","     :issue:`8259` by :user:`Aman Dalmia <dalmia>`.","156",""],"delete":[]}],"sklearn\/linear_model\/randomized_l1.py":[{"add":["18","from ..base import BaseEstimator","21","from ..feature_selection.base import SelectorMixin","22","from ..utils import (as_float_array, check_random_state, check_X_y, safe_mask)","61","                                                   SelectorMixin)):","89","               Returns an instance of self.","123","    def _get_support_mask(self):","124","        \"\"\"Get the boolean mask indicating which features are selected.","125","","126","        Returns","127","        -------","128","        support : boolean array of shape [# input features]","129","                  An element is True iff its corresponding feature is selected","130","                  for retention.","131","        \"\"\"","133","        return self.scores_ > self.selection_threshold"],"delete":["18","from ..base import BaseEstimator, TransformerMixin","21","from ..utils import (as_float_array, check_random_state, check_X_y,","22","                     check_array, safe_mask)","61","                                                   TransformerMixin)):","89","            Returns an instance of self.","123","    def get_support(self, indices=False):","124","        \"\"\"Return a mask, or list, of the features\/indices selected.\"\"\"","126","","127","        mask = self.scores_ > self.selection_threshold","128","        return mask if not indices else np.where(mask)[0]","129","","130","    # XXX: the two function below are copy\/pasted from feature_selection,","131","    # Should we add an intermediate base class?","132","    def transform(self, X):","133","        \"\"\"Transform a new matrix using the selected features\"\"\"","134","        mask = self.get_support()","135","        X = check_array(X)","136","        if len(mask) != X.shape[1]:","137","            raise ValueError(\"X has a different shape than during fitting.\")","138","        return check_array(X)[:, safe_mask(X, mask)]","139","","140","    def inverse_transform(self, X):","141","        \"\"\"Transform a new matrix using the selected features\"\"\"","142","        support = self.get_support()","143","        if X.ndim == 1:","144","            X = X[None, :]","145","        Xt = np.zeros((X.shape[0], support.size))","146","        Xt[:, support] = X","147","        return Xt"]}]}},"b43c79163d7e3f8477f973e8aa8bca7025f58481":{"changes":{"sklearn\/tests\/test_multiclass.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/multiclass.py":"MODIFY"},"diff":{"sklearn\/tests\/test_multiclass.py":[{"add":["253","        if hasattr(base_clf, 'decision_function'):","254","            dec = clf.decision_function(X)","255","            assert_equal(dec.shape, (5,))","529","    # first binary","530","    ovo_clf.fit(iris.data, iris.target == 0)","531","    decisions = ovo_clf.decision_function(iris.data)","532","    assert_equal(decisions.shape, (n_samples,))","533","","534","    # then multi-class"],"delete":[]}],"doc\/whats_new.rst":[{"add":["420","   - The ``decision_function`` output shape for binary classification in","421","     :class:`multi_class.OneVsRestClassifier` and","422","     :class:`multi_class.OneVsOneClassifier` is now ``(n_samples,)`` to conform","423","     to scikit-learn conventions. :issue:`9100` by `Andreas M¨¹ller`_.","424",""],"delete":[]}],"sklearn\/multiclass.py":[{"add":["370","        if len(self.estimators_) == 1:","371","            return self.estimators_[0].decision_function(X)","578","        if self.n_classes_ == 2:","579","            return self.classes_[(Y > 0).astype(np.int)]","612","        if self.n_classes_ == 2:","613","            return Y[:, 1]"],"delete":["608",""]}]}},"72d0529de742206f23be6a14670d21566e85835f":{"changes":{"sklearn\/tests\/test_discriminant_analysis.py":"MODIFY","sklearn\/discriminant_analysis.py":"MODIFY","doc\/whats_new.rst":"MODIFY"},"diff":{"sklearn\/tests\/test_discriminant_analysis.py":[{"add":["7","from sklearn.utils.testing import assert_false","11","from sklearn.utils.testing import assert_warns_message","227","def test_lda_store_covariance():","228","    # Test for slover 'lsqr' and 'eigen'","229","    # 'store_covariance' has no effect on 'lsqr' and 'eigen' solvers","230","    for solver in ('lsqr', 'eigen'):","231","        clf = LinearDiscriminantAnalysis(solver=solver).fit(X6, y6)","232","        assert_true(hasattr(clf, 'covariance_'))","233","","234","        # Test the actual attribute:","235","        clf = LinearDiscriminantAnalysis(solver=solver,","236","                                         store_covariance=True).fit(X6, y6)","237","        assert_true(hasattr(clf, 'covariance_'))","238","","239","        assert_array_almost_equal(","240","            clf.covariance_,","241","            np.array([[0.422222, 0.088889], [0.088889, 0.533333]])","242","        )","243","","244","    # Test for SVD slover, the default is to not set the covariances_ attribute","245","    clf = LinearDiscriminantAnalysis(solver='svd').fit(X6, y6)","246","    assert_false(hasattr(clf, 'covariance_'))","247","","248","    # Test the actual attribute:","249","    clf = LinearDiscriminantAnalysis(solver=solver,","250","                                     store_covariance=True).fit(X6, y6)","251","    assert_true(hasattr(clf, 'covariance_'))","252","","253","    assert_array_almost_equal(","254","        clf.covariance_,","255","        np.array([[0.422222, 0.088889], [0.088889, 0.533333]])","256","    )","257","","258","","298","def test_qda_store_covariance():","301","    assert_false(hasattr(clf, 'covariance_'))","304","    clf = QuadraticDiscriminantAnalysis(store_covariance=True).fit(X6, y6)","305","    assert_true(hasattr(clf, 'covariance_'))","308","        clf.covariance_[0],","313","        clf.covariance_[1],","318","def test_qda_deprecation():","319","    # Test the deprecation","320","    clf = QuadraticDiscriminantAnalysis(store_covariances=True)","321","    assert_warns_message(DeprecationWarning, \"'store_covariances' was renamed\"","322","                         \" to store_covariance in version 0.19 and will be \"","323","                         \"removed in 0.21.\", clf.fit, X, y)","324","","325","    # check that covariance_ (and covariances_ with warning) is stored","326","    assert_warns_message(DeprecationWarning, \"Attribute covariances_ was \"","327","                         \"deprecated in version 0.19 and will be removed \"","328","                         \"in 0.21. Use covariance_ instead\", getattr, clf,","329","                         'covariances_')","330","","331",""],"delete":["264","def test_qda_store_covariances():","267","    assert_true(not hasattr(clf, 'covariances_'))","270","    clf = QuadraticDiscriminantAnalysis(store_covariances=True).fit(X6, y6)","271","    assert_true(hasattr(clf, 'covariances_'))","274","        clf.covariances_[0],","279","        clf.covariances_[1],"]}],"sklearn\/discriminant_analysis.py":[{"add":["14","from .utils import deprecated","172","        Additionally compute class covariance matrix (default False), used","173","        only in 'svd' solver.","248","","558","    store_covariance : boolean","571","    covariance_ : list of array-like, shape = [n_features, n_features]","601","                                  store_covariance=False,","602","                                  store_covariances=None, tol=0.0001)","612","    def __init__(self, priors=None, reg_param=0., store_covariance=False,","613","                 tol=1.0e-4, store_covariances=None):","617","        self.store_covariance = store_covariance","620","    @property","621","    @deprecated(\"Attribute covariances_ was deprecated in version\"","622","                \" 0.19 and will be removed in 0.21. Use \"","623","                \"covariance_ instead\")","624","    def covariances_(self):","625","        return self.covariance_","626","","631","               ``store_covariances`` has been moved to main constructor as","632","               ``store_covariance``","635","               ``tol`` has been moved to main constructor.","659","        store_covariance = self.store_covariance or self.store_covariances","661","            warnings.warn(\"'store_covariances' was renamed to store_covariance\"","662","                          \" in version 0.19 and will be removed in 0.21.\",","663","                          DeprecationWarning)","664","        if store_covariance:","684","            if self.store_covariance or store_covariance:","689","        if self.store_covariance or store_covariance:","690","            self.covariance_ = cov"],"delete":["13","","172","        Additionally compute class covariance matrix (default False).","556","    store_covariances : boolean","569","    covariances_ : list of array-like, shape = [n_features, n_features]","599","                                  store_covariances=False, tol=0.0001)","609","    def __init__(self, priors=None, reg_param=0., store_covariances=False,","610","                 tol=1.0e-4):","620","               *store_covariance* has been moved to main constructor.","623","               *tol* has been moved to main constructor.","667","            if self.store_covariances:","672","        if self.store_covariances:","673","            self.covariances_ = cov"]}],"doc\/whats_new.rst":[{"add":["800","- The ``store_covariances`` and ``covariances_`` parameters of","801","  :class:`discriminant_analysis.QuadraticDiscriminantAnalysis`","802","  has been renamed to ``store_covariance`` and ``covariance_`` to be","803","  consistent with the corresponding parameter names of the","804","  :class:`discriminant_analysis.LinearDiscriminantAnalysis`. They will be","805","  removed in version 0.21. :issue:`7998` by :user:`Jiacheng <mrbeann>`","806",""],"delete":[]}]}},"fb65a0a4bae01daad7c751e3a942ce0126ca05ff":{"changes":{"doc\/whats_new.rst":"MODIFY","sklearn\/ensemble\/weight_boosting.py":"MODIFY","sklearn\/ensemble\/tests\/test_weight_boosting.py":"MODIFY"},"diff":{"doc\/whats_new.rst":[{"add":["155","   - Fixed a bug where :class:`sklearn.ensemble.AdaBoostClassifier` throws","156","     ``ZeroDivisionError`` while fitting data with single class labels.","157","     :issue:`7501` by :user:`Dominik Krzeminski <dokato>`.","158",""],"delete":["159",""]}],"sklearn\/ensemble\/weight_boosting.py":[{"add":["758","        if n_classes == 1:","759","            return np.ones((X.shape[0], 1))","760",""],"delete":[]}],"sklearn\/ensemble\/tests\/test_weight_boosting.py":[{"add":["76","def test_oneclass_adaboost_proba():","77","    # Test predict_proba robustness for one class label input.","78","    # In response to issue #7501","79","    # https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/7501","80","    y_t = np.ones(len(X))","81","    clf = AdaBoostClassifier().fit(X, y_t)","82","    assert_array_equal(clf.predict_proba(X), np.ones((len(X), 1)))","83","","84",""],"delete":[]}]}},"57415c461068f8a3d7cdca7e274dd8bc039d9efa":{"changes":{"sklearn\/manifold\/tests\/test_t_sne.py":"MODIFY","doc\/whats_new.rst":"MODIFY","sklearn\/manifold\/t_sne.py":"MODIFY"},"diff":{"sklearn\/manifold\/tests\/test_t_sne.py":[{"add":["14","from sklearn.utils.testing import skip_if_32bit","566","@skip_if_32bit","568","    # Use a dummy negative n_iter_without_progress and check output on stdout","571","    tsne = TSNE(n_iter_without_progress=-1, verbose=2,","572","                random_state=1, method='exact')","585","              \"last -1 episodes. Finished.\", out)"],"delete":["566","    # Make sure that the parameter n_iter_without_progress is used correctly","569","    tsne = TSNE(n_iter_without_progress=2, verbose=2,","570","                random_state=0, method='exact')","583","              \"last 2 episodes. Finished.\", out)"]}],"doc\/whats_new.rst":[{"add":["294","   - Fixed a bug in :class:`manifold.TSNE` affecting convergence of the","295","     gradient descent. :issue:`8768` by :user:`David DeTomaso <deto>`."],"delete":[]}],"sklearn\/manifold\/t_sne.py":[{"add":["390","        inc = update * grad < 0.0","392","        gains[inc] += 0.2","393","        gains[dec] *= 0.8","394","        np.clip(gains, min_gain, np.inf, out=gains)","633","    array([[ 0.00017619,  0.00004014],","634","           [ 0.00010268,  0.00020546],","635","           [ 0.00018298, -0.00008335],","636","           [ 0.00009501, -0.00001388]])"],"delete":["390","        inc = update * grad >= 0.0","392","        gains[inc] += 0.05","393","        gains[dec] *= 0.95","394","        np.clip(gains, min_gain, np.inf)","633","    array([[ 0.00017599,  0.00003993],","634","           [ 0.00009891,  0.00021913],","635","           [ 0.00018554, -0.00009357],","636","           [ 0.00009528, -0.00001407]])"]}]}},"1b2a9285affb385fc84057b3edf52fa6153cf76a":{"changes":{"sklearn\/feature_extraction\/text.py":"MODIFY"},"diff":{"sklearn\/feature_extraction\/text.py":[{"add":["161","        operating only inside word boundaries. n-grams at the edges","162","        of words are padded with space.\"\"\"","357","        word boundaries; n-grams at the edges of words are padded with space.","556","        word boundaries; n-grams at the edges of words are padded with space."],"delete":["161","        excluding any whitespace (operating only inside word boundaries)\"\"\"","356","        word boundaries.","555","        word boundaries."]}]}},"b57837188dd55d37e13ddcc8f9f548e9dbd3f0c1":{"changes":{"sklearn\/model_selection\/_split.py":"MODIFY","sklearn\/model_selection\/tests\/test_split.py":"MODIFY"},"diff":{"sklearn\/model_selection\/_split.py":[{"add":["999","    def get_n_splits(self, X=None, y=None, groups=None):","1000","        \"\"\"Returns the number of splitting iterations in the cross-validator","1001","","1002","        Parameters","1003","        ----------","1004","        X : object","1005","            Always ignored, exists for compatibility.","1006","            ``np.zeros(n_samples)`` may be used as a placeholder.","1007","","1008","        y : object","1009","            Always ignored, exists for compatibility.","1010","            ``np.zeros(n_samples)`` may be used as a placeholder.","1011","","1012","        groups : array-like, with shape (n_samples,), optional","1013","            Group labels for the samples used while splitting the dataset into","1014","            train\/test set.","1015","","1016","        Returns","1017","        -------","1018","        n_splits : int","1019","            Returns the number of splitting iterations in the cross-validator.","1020","        \"\"\"","1021","        rng = check_random_state(self.random_state)","1022","        cv = self.cv(random_state=rng, shuffle=True,","1023","                     **self.cvargs)","1024","        return cv.get_n_splits(X, y, groups) * self.n_repeats","1025",""],"delete":[]}],"sklearn\/model_selection\/tests\/test_split.py":[{"add":["846","def test_get_n_splits_for_repeated_kfold():","847","    n_splits = 3","848","    n_repeats = 4","849","    rkf = RepeatedKFold(n_splits, n_repeats)","850","    expected_n_splits = n_splits * n_repeats","851","    assert_equal(expected_n_splits, rkf.get_n_splits())","852","","853","","854","def test_get_n_splits_for_repeated_stratified_kfold():","855","    n_splits = 3","856","    n_repeats = 4","857","    rskf = RepeatedStratifiedKFold(n_splits, n_repeats)","858","    expected_n_splits = n_splits * n_repeats","859","    assert_equal(expected_n_splits, rskf.get_n_splits())","860","","861",""],"delete":[]}]}},"fa82eee82f2df40dae61df775d2f2c4652eec385":{"changes":{"sklearn\/gaussian_process\/gpr.py":"MODIFY","sklearn\/gaussian_process\/tests\/test_gpr.py":"MODIFY"},"diff":{"sklearn\/gaussian_process\/gpr.py":[{"add":["49","        Larger values correspond to increased noise level in the observations.","50","        This can also prevent a potential numerical issue during fitting, by","51","        ensuring that the calculated values form a positive definite matrix.","52","        If an array is passed, it must have the same number of entries as the","53","        data used for fitting and is used as datapoint-dependent noise level.","54","        Note that this is equivalent to adding a WhiteKernel with c=alpha.","55","        Allowing to specify the noise level directly as a parameter is mainly","56","        for convenience and for consistency with Ridge.","245","        try:","246","            self.L_ = cholesky(K, lower=True)  # Line 2","247","        except np.linalg.LinAlgError as exc:","248","            exc.args = (\"The kernel, %s, is not returning a \"","249","                        \"positive definite matrix. Try gradually \"","250","                        \"increasing the 'alpha' parameter of your \"","251","                        \"GaussianProcessRegressor estimator.\"","252","                        % self.kernel_,) + exc.args","253","            raise"],"delete":["49","        Larger values correspond to increased noise level in the observations","50","        and reduce potential numerical issue during fitting. If an array is","51","        passed, it must have the same number of entries as the data used for","52","        fitting and is used as datapoint-dependent noise level. Note that this","53","        is equivalent to adding a WhiteKernel with c=alpha. Allowing to specify","54","        the noise level directly as a parameter is mainly for convenience and","55","        for consistency with Ridge.","244","        self.L_ = cholesky(K, lower=True)  # Line 2","246",""]}],"sklearn\/gaussian_process\/tests\/test_gpr.py":[{"add":["12","from sklearn.gaussian_process.kernels import DotProduct","16","            assert_almost_equal, assert_equal, assert_raise_message)","293","def test_gpr_correct_error_message():","294","    X = np.arange(12).reshape(6, -1)","295","    y = np.ones(6)","296","    kernel = DotProduct()","297","    gpr = GaussianProcessRegressor(kernel=kernel, alpha=0.0)","298","    assert_raise_message(np.linalg.LinAlgError,","299","                         \"The kernel, %s, is not returning a \"","300","                         \"positive definite matrix. Try gradually increasing \"","301","                         \"the 'alpha' parameter of your \"","302","                         \"GaussianProcessRegressor estimator.\"","303","                         % kernel, gpr.fit, X, y)","304","","305",""],"delete":["15","            assert_almost_equal, assert_equal)"]}]}},"54e89511b82194f40df486573d4eb1d9c9031762":{"changes":{"sklearn\/tests\/test_base.py":"MODIFY"},"diff":{"sklearn\/tests\/test_base.py":[{"add":["16","from sklearn.utils.testing import ignore_warnings","443","@ignore_warnings(category=(UserWarning))"],"delete":[]}]}},"bbf43483c178e9ae53a0d2f9fa98028a3c9127f1":{"changes":{"doc\/README.md":"MODIFY"},"diff":{"doc\/README.md":[{"add":["0","# Documentation for scikit-learn","28","# Development documentation automated build"],"delete":["0","#Documentation for scikit-learn","28","#Development documentation automated build"]}]}},"2a36ff1e483b2b772569029c884e1e9a60b577c5":{"changes":{"doc\/whats_new.rst":"MODIFY","sklearn\/decomposition\/pca.py":"MODIFY","sklearn\/decomposition\/incremental_pca.py":"MODIFY","sklearn\/decomposition\/tests\/test_pca.py":"MODIFY"},"diff":{"doc\/whats_new.rst":[{"add":["415","   - Fixed the implementation of `explained_variance_`","416","     in :class:`decomposition.PCA`,","417","     :class:`decomposition.RandomizedPCA` and","418","     :class:`decomposition.IncrementalPCA`.","419","     :issue:`9105` by `Hanmin Qin <https:\/\/github.com\/qinhanmin2014>`_. ","420",""],"delete":[]}],"sklearn\/decomposition\/pca.py":[{"add":["342","            U *= sqrt(X.shape[0] - 1)","412","        explained_variance_ = (S ** 2) \/ (n_samples - 1)","491","        self.explained_variance_ = (S ** 2) \/ (n_samples - 1)","492","        total_var = np.var(X, ddof=1, axis=0)","710","        self.explained_variance_ = exp_var = (S ** 2) \/ (n_samples - 1)","711","        full_var = np.var(X, ddof=1, axis=0).sum()"],"delete":["287","    Notes","288","    -----","289","    PCA uses the maximum likelihood estimate of the eigenvalues, which does not","290","    include the Bessel correction, though in practice this should rarely make a","291","    difference in a machine learning context.","292","","348","            U *= sqrt(X.shape[0])","418","        explained_variance_ = (S ** 2) \/ n_samples","497","        self.explained_variance_ = (S ** 2) \/ n_samples","498","        total_var = np.var(X, axis=0)","716","        self.explained_variance_ = exp_var = (S ** 2) \/ n_samples","717","        full_var = np.var(X, axis=0).sum()"]}],"sklearn\/decomposition\/incremental_pca.py":[{"add":["253","        explained_variance = S ** 2 \/ (n_total_samples - 1)"],"delete":["253","        explained_variance = S ** 2 \/ n_total_samples"]}],"sklearn\/decomposition\/tests\/test_pca.py":[{"add":["176","        assert_almost_equal(X_whitened.std(ddof=1, axis=0),","177","                            np.ones(n_components),","216","    expected_result = np.linalg.eig(np.cov(X, rowvar=False))[0]","217","    expected_result = sorted(expected_result, reverse=True)[:2]","218","","221","                              np.var(X_pca, ddof=1, axis=0))","222","    assert_array_almost_equal(pca.explained_variance_, expected_result)","226","                              np.var(X_pca, ddof=1, axis=0))","227","    assert_array_almost_equal(apca.explained_variance_, expected_result)","230","    assert_array_almost_equal(rpca.explained_variance_,","231","                              np.var(X_rpca, ddof=1, axis=0),","233","    assert_array_almost_equal(rpca.explained_variance_,","234","                              expected_result, decimal=1)"],"delete":["176","        assert_almost_equal(X_whitened.std(axis=0), np.ones(n_components),","217","                              np.var(X_pca, axis=0))","221","                              np.var(X_pca, axis=0))","224","    assert_array_almost_equal(rpca.explained_variance_, np.var(X_rpca, axis=0),"]}]}},"daeba62b9ccbf515cf9bc419562cf5ae83e73d85":{"changes":{"sklearn\/utils\/validation.py":"MODIFY","sklearn\/utils\/tests\/test_validation.py":"MODIFY"},"diff":{"sklearn\/utils\/validation.py":[{"add":["94","    if hasattr(x, 'fit') and callable(x.fit):"],"delete":["94","    if hasattr(x, 'fit'):"]}],"sklearn\/utils\/tests\/test_validation.py":[{"add":["17","from sklearn.utils.testing import SkipTest","504","","505","","506","def test_check_dataframe_fit_attribute():","507","    # check pandas dataframe with 'fit' column does not raise error","508","    # https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/8415","509","    try:","510","        import pandas as pd","511","        X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])","512","        X_df = pd.DataFrame(X, columns=['a', 'b', 'fit'])","513","        check_consistent_length(X_df)","514","    except ImportError:","515","        raise SkipTest(\"Pandas not found\")"],"delete":[]}]}},"1ada91c341c984ed7bbf1a84ed22884925280d1d":{"changes":{"sklearn\/neighbors\/regression.py":"MODIFY"},"diff":{"sklearn\/neighbors\/regression.py":[{"add":["65","    metric : string or callable, default 'minkowski'"],"delete":["65","    metric : string or DistanceMetric object (default='minkowski')"]}]}},"d7e77ce0a32fc09840f74d8f3f07ff1c57ebd5f8":{"changes":{"doc\/themes\/scikit-learn\/layout.html":"MODIFY"},"diff":{"doc\/themes\/scikit-learn\/layout.html":[{"add":["277","    <p class=\"citing\">Please <b><a href=\"{{ pathto('about').replace('#', '') }}#citing-scikit-learn\" style=\"font-size: 110%;\">cite us <\/a><\/b>if you use the software.<\/p>"],"delete":["277","    <p class=\"citing\">Please <b><a href=\"about.html#citing-scikit-learn\" style=\"font-size: 110%;\">cite us <\/a><\/b>if you use the software.<\/p>"]}]}},"02c705e115cdcf03f76789774baf8fed84092924":{"changes":{"doc\/whats_new.rst":"MODIFY","sklearn\/utils\/estimator_checks.py":"MODIFY"},"diff":{"doc\/whats_new.rst":[{"add":["272","      ","273","   - Estimators with both methods ``decision_function`` and ``predict_proba`` ","274","     are now required to have a monotonic relation between them. The ","275","     method ``check_decision_proba_consistency`` has been added in ","276","     **sklearn.utils.estimator_checks** to check their consistency. ","277","     :issue:`7578` by :user:`Shubham Bhardwaj <shubham0704>`","278","      "],"delete":[]}],"sklearn\/utils\/estimator_checks.py":[{"add":["10","from scipy.stats import rankdata","115","    if (name not in","116","        [\"MultinomialNB\", \"LabelPropagation\", \"LabelSpreading\"] and","118","       name not in [\"DecisionTreeClassifier\", \"ExtraTreeClassifier\"]):","129","    # test if predict_proba is a monotonic transformation of decision_function","130","    yield check_decision_proba_consistency","273","    if (\"n_iter\" in params and estimator.__class__.__name__ != \"TSNE\"):","1115","                if (n_classes is 3 and not isinstance(classifier, BaseLibSVM)):","1576","                return (p.name != 'self' and","1577","                        p.kind != p.VAR_KEYWORD and","1578","                        p.kind != p.VAR_POSITIONAL)","1723","","1724","","1725","@ignore_warnings(category=DeprecationWarning)","1726","def check_decision_proba_consistency(name, Estimator):","1727","    # Check whether an estimator having both decision_function and","1728","    # predict_proba methods has outputs with perfect rank correlation.","1729","","1730","    centers = [(2, 2), (4, 4)]","1731","    X, y = make_blobs(n_samples=100, random_state=0, n_features=4,","1732","                      centers=centers, cluster_std=1.0, shuffle=True)","1733","    X_test = np.random.randn(20, 2) + 4","1734","    estimator = Estimator()","1735","","1736","    set_testing_parameters(estimator)","1737","","1738","    if (hasattr(estimator, \"decision_function\") and","1739","            hasattr(estimator, \"predict_proba\")):","1740","","1741","        estimator.fit(X, y)","1742","        a = estimator.predict_proba(X_test)[:, 1]","1743","        b = estimator.decision_function(X_test)","1744","        assert_array_equal(rankdata(a), rankdata(b))"],"delete":["8","","115","    if (name not in [\"MultinomialNB\", \"LabelPropagation\", \"LabelSpreading\"]","117","            and name not in [\"DecisionTreeClassifier\",","118","                             \"ExtraTreeClassifier\"]):","271","    if (\"n_iter\" in params","272","            and estimator.__class__.__name__ != \"TSNE\"):","1114","                if (n_classes is 3","1115","                        and not isinstance(classifier, BaseLibSVM)):","1576","                return (p.name != 'self'","1577","                        and p.kind != p.VAR_KEYWORD","1578","                        and p.kind != p.VAR_POSITIONAL)"]}]}},"24653e8721794d47173554ec723a358a26f7300c":{"changes":{"sklearn\/metrics\/classification.py":"MODIFY","sklearn\/metrics\/tests\/test_classification.py":"MODIFY","doc\/whats_new.rst":"MODIFY"},"diff":{"sklearn\/metrics\/classification.py":[{"add":["93","        if y_type == \"binary\":","94","            unique_values = np.union1d(y_true, y_pred)","95","            if len(unique_values) > 2:","96","                y_type = \"multiclass\""],"delete":[]}],"sklearn\/metrics\/tests\/test_classification.py":[{"add":["368","    y_true_inv2 = label_binarize(y_true, [\"a\", \"b\"])","369","    y_true_inv2 = np.where(y_true_inv2, 'a', 'b')","382","                               matthews_corrcoef, y_true, ['a'] * len(y_true))","1269","def test__check_targets_multiclass_with_both_y_true_and_y_pred_binary():","1270","    # https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/8098","1271","    y_true = [0, 1]","1272","    y_pred = [0, -1]","1273","    assert_equal(_check_targets(y_true, y_pred)[0], 'multiclass')","1274","","1275",""],"delete":["368","    y_true_inv2 = label_binarize(y_true, [\"a\", \"b\"]) * -1","381","                               matthews_corrcoef, y_true,","382","                               rng.randint(-100, 100) * np.ones(20, dtype=int))"]}],"doc\/whats_new.rst":[{"add":["262","   - Fix a bug in :func:`sklearn.metrics.classification._check_targets`","263","     which would return ``'binary'`` if ``y_true`` and ``y_pred`` were","264","     both ``'binary'`` but the union of ``y_true`` and ``y_pred`` was","265","     ``'multiclass'``. :issue:`8377` by `Loic Esteve`_."],"delete":[]}]}},"7877f3cfae54b20cfc5416eb296dc5d5ae78b231":{"changes":{"examples\/tree\/plot_unveil_tree_structure.py":"MODIFY"},"diff":{"examples\/tree\/plot_unveil_tree_structure.py":[{"add":["116","    print(\"decision id node %s : (X_test[%s, %s] (= %s) %s %s)\"","120","             X_test[sample_id, feature[node_id]],"],"delete":["116","    print(\"decision id node %s : (X[%s, %s] (= %s) %s %s)\"","120","             X_test[i, feature[node_id]],"]}]}},"8694278c027d1017670e67cd3298fc5fd627d4c9":{"changes":{"sklearn\/decomposition\/factor_analysis.py":"MODIFY","doc\/whats_new.rst":"MODIFY",".mailmap":"MODIFY","sklearn\/decomposition\/pca.py":"MODIFY","sklearn\/utils\/tests\/test_extmath.py":"MODIFY","sklearn\/decomposition\/base.py":"MODIFY"},"diff":{"sklearn\/decomposition\/factor_analysis.py":[{"add":["17","#         Denis A. Engemann <denis-alexander.engemann@inria.fr>"],"delete":["17","#         Denis A. Engemann <d.engemann@fz-juelich.de>"]}],"doc\/whats_new.rst":[{"add":["2306","     library by `Denis Engemann`_, and `Alexandre Gramfort`_.","2319","     significantly speedup computation by `Denis Engemann`_, and","2842","   - Reduce memory footprint of FastICA by `Denis Engemann`_ and","4988","","4989",".. _Denis Engemann: http:\/\/denis-engemann.de"],"delete":["2306","     library by :user:`Denis Engemann <dengemann>`, and `Alexandre Gramfort`_.","2319","     significantly speedup computation by :user:`Denis Engemann <dengemann>`, and","2842","   - Reduce memory footprint of FastICA by :user:`Denis Engemann <dengemann>` and"]}],".mailmap":[{"add":["25","Denis Engemann <denis-alexander.engemann@inria.fr>","26","Denis Engemann <denis-alexander.engemann@inria.fr> <denis.engemann@gmail.com>","27","Denis Engemann <denis-alexander.engemann@inria.fr> <dengemann@Deniss-MacBook-Pro.local>","28","Denis Engemann <denis-alexander.engemann@inria.fr> <dengemann <denis.engemann@gmail.com>","63","Jan Schl¨¹ter <scikit-learn@jan-schlueter.de>"],"delete":["25","Denis Engemann <d.engemann@fz-juelich.de>","26","Denis Engemann <d.engemann@fz-juelich.de> <denis.engemann@gmail.com>","27","Denis Engemann <d.engemann@fz-juelich.de> <dengemann@Deniss-MacBook-Pro.local>","28","Denis Engemann <d.engemann@fz-juelich.de> <dengemann <denis.engemann@gmail.com>","63","Jan Schlüter <scikit-learn@jan-schlueter.de>"]}],"sklearn\/decomposition\/pca.py":[{"add":["6","#         Denis A. Engemann <denis-alexander.engemann@inria.fr>"],"delete":["6","#         Denis A. Engemann <d.engemann@fz-juelich.de>"]}],"sklearn\/utils\/tests\/test_extmath.py":[{"add":["2","#          Denis Engemann <denis-alexander.engemann@inria.fr>"],"delete":["2","#          Denis Engemann <d.engemann@fz-juelich.de>"]}],"sklearn\/decomposition\/base.py":[{"add":["5","#         Denis A. Engemann <denis-alexander.engemann@inria.fr>"],"delete":["5","#         Denis A. Engemann <d.engemann@fz-juelich.de>"]}]}},"f38231e4c9cb43b8b49d789c21d2376def039c9c":{"changes":{"sklearn\/decomposition\/pca.py":"MODIFY","sklearn\/decomposition\/tests\/test_pca.py":"MODIFY"},"diff":{"sklearn\/decomposition\/pca.py":[{"add":["388","        else:","389","            raise ValueError(\"Unrecognized svd_solver='{0}'\"","390","                             \"\".format(svd_solver))"],"delete":[]}],"sklearn\/decomposition\/tests\/test_pca.py":[{"add":["586","","587","","588","def test_pca_bad_solver():","589","    X = np.random.RandomState(0).rand(5, 4)","590","    pca = PCA(n_components=3, svd_solver='bad_argument')","591","    assert_raises(ValueError, pca.fit, X)"],"delete":[]}]}},"4e56b293d66f0f278080b40851770754880a048c":{"changes":{"sklearn\/_isotonic.pyx":"MODIFY","doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/isotonic.py":"MODIFY","sklearn\/tests\/test_isotonic.py":"MODIFY"},"diff":{"sklearn\/_isotonic.pyx":[{"add":["9","from cython cimport floating","15","def _inplace_contiguous_isotonic_regression(floating[::1] y, floating[::1] w):","18","        floating prev_y, sum_wy, sum_w","69","def _make_unique(np.ndarray[dtype=floating] X,","70","                 np.ndarray[dtype=floating] y,","71","                 np.ndarray[dtype=floating] sample_weights):","84","    cdef np.ndarray[dtype=floating] y_out = np.empty(unique_values,","85","                                                     dtype=X.dtype)","86","    cdef np.ndarray[dtype=floating] x_out = np.empty_like(y_out)","87","    cdef np.ndarray[dtype=floating] weights_out = np.empty_like(y_out)","88","","89","    cdef floating current_x = X[0]","90","    cdef floating current_y = 0","91","    cdef floating current_weight = 0","92","    cdef floating y_old = 0","96","    cdef floating x"],"delete":["9","","10","ctypedef np.float64_t DOUBLE","16","def _inplace_contiguous_isotonic_regression(DOUBLE[::1] y, DOUBLE[::1] w):","19","        DOUBLE prev_y, sum_wy, sum_w","70","def _make_unique(np.ndarray[dtype=np.float64_t] X,","71","                 np.ndarray[dtype=np.float64_t] y,","72","                 np.ndarray[dtype=np.float64_t] sample_weights):","84","    cdef np.ndarray[dtype=np.float64_t] y_out = np.empty(unique_values)","85","    cdef np.ndarray[dtype=np.float64_t] x_out = np.empty(unique_values)","86","    cdef np.ndarray[dtype=np.float64_t] weights_out = np.empty(unique_values)","88","    cdef np.float64_t current_x = X[0]","89","    cdef np.float64_t current_y = 0","90","    cdef np.float64_t current_weight = 0","91","    cdef np.float64_t y_old = 0","95","    cdef np.float64_t x"]}],"doc\/whats_new\/v0.21.rst":[{"add":["330",":mod:`sklearn.isotonic`","331",".......................","332","","333","- |Feature| Allow different dtypes (such as float32) in","334","  :class:`isotonic.IsotonicRegression` :issue:`8769` by :user:`Vlad Niculae <vene>`","335","","336",""],"delete":[]}],"sklearn\/isotonic.py":[{"add":["121","    y = as_float_array(y)","122","    y = np.array(y[order], dtype=y.dtype)","124","        sample_weight = np.ones(len(y), dtype=y.dtype)","126","        sample_weight = np.array(sample_weight[order], dtype=y.dtype)","242","        X = as_float_array(X)","243","        y = check_array(y, dtype=X.dtype, ensure_2d=False)","256","            sample_weight = check_array(sample_weight, ensure_2d=False,","257","                                        dtype=X.dtype)","261","            sample_weight = np.ones(len(y), dtype=X.dtype)","264","        X, y, sample_weight = [array[order] for array in [X, y, sample_weight]]","347","","348","        if hasattr(self, '_necessary_X_'):","349","            dtype = self._necessary_X_.dtype","350","        else:","351","            dtype = np.float64","352","","353","        T = check_array(T, dtype=dtype, ensure_2d=False)","354","","366","","367","        res = self.f_(T)","368","","369","        # on scipy 0.17, interp1d up-casts to float64, so we cast back","370","        res = res.astype(T.dtype)","371","","372","        return res"],"delete":["121","    y = np.array(y[order], dtype=np.float64)","123","        sample_weight = np.ones(len(y), dtype=np.float64)","125","        sample_weight = np.array(sample_weight[order], dtype=np.float64)","242","        X, y = [check_array(x, ensure_2d=False) for x in [X, y]]","243","","244","        y = as_float_array(y)","256","            sample_weight = check_array(sample_weight, ensure_2d=False)","260","            sample_weight = np.ones(len(y))","263","        X, y, sample_weight = [array[order].astype(np.float64, copy=False)","264","                               for array in [X, y, sample_weight]]","347","        T = as_float_array(T)","359","        return self.f_(T)"]}],"sklearn\/tests\/test_isotonic.py":[{"add":["6","                              IsotonicRegression, _make_unique)","8","from sklearn.utils.validation import as_float_array","462","","463","","464","def test_isotonic_dtype():","465","    y = [2, 1, 4, 3, 5]","466","    weights = np.array([.9, .9, .9, .9, .9], dtype=np.float64)","467","    reg = IsotonicRegression()","468","","469","    for dtype in (np.int32, np.int64, np.float32, np.float64):","470","        for sample_weight in (None, weights.astype(np.float32), weights):","471","            y_np = np.array(y, dtype=dtype)","472","            expected_dtype = as_float_array(y_np).dtype","473","","474","            res = isotonic_regression(y_np, sample_weight=sample_weight)","475","            assert_equal(res.dtype, expected_dtype)","476","","477","            X = np.arange(len(y)).astype(dtype)","478","            reg.fit(X, y_np, sample_weight=sample_weight)","479","            res = reg.predict(X)","480","            assert_equal(res.dtype, expected_dtype)","481","","482","","483","def test_make_unique_dtype():","484","    x_list = [2, 2, 2, 3, 5]","485","    for dtype in (np.float32, np.float64):","486","        x = np.array(x_list, dtype=dtype)","487","        y = x.copy()","488","        w = np.ones_like(x)","489","        x, y, w = _make_unique(x, y, w)","490","        assert_array_equal(x, [2, 3, 5])"],"delete":["6","                              IsotonicRegression)"]}]}},"2cb7e472a7aabd18680dc748ca5a0b6e5283ccdf":{"changes":{"sklearn\/metrics\/cluster\/tests\/test_supervised.py":"MODIFY"},"diff":{"sklearn\/metrics\/cluster\/tests\/test_supervised.py":[{"add":["241","","242","","243","def test_fowlkes_mallows_score_properties():","244","    # handcrafted example","245","    labels_a = np.array([0, 0, 0, 1, 1, 2])","246","    labels_b = np.array([1, 1, 2, 2, 0, 0])","247","    expected = 1. \/ np.sqrt((1. + 3.) * (1. + 2.))","248","    # FMI = TP \/ sqrt((TP + FP) * (TP + FN))","249","","250","    score_original = fowlkes_mallows_score(labels_a, labels_b)","251","    assert_almost_equal(score_original, expected)","252","","253","    # symetric property","254","    score_symetric = fowlkes_mallows_score(labels_b, labels_a)","255","    assert_almost_equal(score_symetric, expected)","256","","257","    # permutation property","258","    score_permuted = fowlkes_mallows_score((labels_a + 1) % 3, labels_b)","259","    assert_almost_equal(score_permuted, expected)","260","","261","    # symetric and permutation(both together)","262","    score_both = fowlkes_mallows_score(labels_b, (labels_a + 2) % 3)","263","    assert_almost_equal(score_both, expected)"],"delete":[]}]}},"ee82c3f357dbacb2e3d1ca6640b17fd73fd2ade1":{"changes":{"sklearn\/base.py":"MODIFY"},"diff":{"sklearn\/base.py":[{"add":["41","        If safe is false, clone will fall back to a deep copy on objects","261","            # Simple optimization to gain speed (inspect is slow)"],"delete":["41","        If safe is false, clone will fall back to a deepcopy on objects","261","            # Simple optimisation to gain speed (inspect is slow)"]}]}}}