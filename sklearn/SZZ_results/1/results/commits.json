{"d2cd2540418d3ff66b324ec18566dbe0b5991b40":{"changes":{"sklearn\/neural_network\/tests\/test_mlp.py":"MODIFY","sklearn\/neural_network\/_multilayer_perceptron.py":"MODIFY","doc\/whats_new\/v0.24.rst":"MODIFY"},"diff":{"sklearn\/neural_network\/tests\/test_mlp.py":[{"add":["664","@pytest.mark.parametrize(\"MLPEstimator\", [MLPClassifier, MLPRegressor])","665","def test_warm_start_full_iteration(MLPEstimator):","666","    # Non-regression test for:","667","    # https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/16812","668","    # Check that the MLP estimator accomplish `max_iter` with a","669","    # warm started estimator.","670","    X, y = X_iris, y_iris","671","    max_iter = 3","672","    clf = MLPEstimator(","673","        hidden_layer_sizes=2, solver='sgd', warm_start=True, max_iter=max_iter","674","    )","675","    clf.fit(X, y)","676","    assert max_iter == clf.n_iter_","677","    clf.fit(X, y)","678","    assert 2 * max_iter == clf.n_iter_","679","","680",""],"delete":[]}],"sklearn\/neural_network\/_multilayer_perceptron.py":[{"add":["979","        # Matrix of actions to be taken under the possible combinations:","980","        # The case that incremental == True and classes_ not defined is","981","        # already checked by _check_partial_fit_first_call that is called","982","        # in _partial_fit below.","983","        # The cases are already grouped into the respective if blocks below.","984","        #","985","        # incremental warm_start classes_ def  action","986","        #    0            0         0        define classes_","987","        #    0            1         0        define classes_","988","        #    0            0         1        redefine classes_","989","        #","990","        #    0            1         1        check compat warm_start","991","        #    1            1         1        check compat warm_start","992","        #","993","        #    1            0         1        check compat last fit","994","        #","995","        # Note the reliance on short-circuiting here, so that the second","996","        # or part implies that classes_ is defined.","997","        if (","998","            (not hasattr(self, \"classes_\")) or","999","            (not self.warm_start and not incremental)","1000","        ):","1006","            if self.warm_start:","1007","                if set(classes) != set(self.classes_):","1008","                    raise ValueError(","1009","                        f\"warm_start can only be used where `y` has the same \"","1010","                        f\"classes as in the previous call to fit. Previously \"","1011","                        f\"got {self.classes_}, `y` has {classes}\"","1012","                    )","1013","            elif len(np.setdiff1d(classes, self.classes_, assume_unique=True)):","1014","                raise ValueError(","1015","                    f\"`y` has classes not in `self.classes_`. \"","1016","                    f\"`self.classes_` has {self.classes_}. 'y' has {classes}.\"","1017","                )"],"delete":["979","        if not incremental:","983","        elif self.warm_start:","984","            classes = unique_labels(y)","985","            if set(classes) != set(self.classes_):","986","                raise ValueError(\"warm_start can only be used where `y` has \"","987","                                 \"the same classes as in the previous \"","988","                                 \"call to fit. Previously got %s, `y` has %s\" %","989","                                 (self.classes_, classes))","992","            if len(np.setdiff1d(classes, self.classes_, assume_unique=True)):","993","                raise ValueError(\"`y` has classes not in `self.classes_`.\"","994","                                 \" `self.classes_` has %s. 'y' has %s.\" %","995","                                 (self.classes_, classes))","1023","    def fit(self, X, y):","1024","        \"\"\"Fit the model to data matrix X and target(s) y.","1025","","1026","        Parameters","1027","        ----------","1028","        X : ndarray or sparse matrix of shape (n_samples, n_features)","1029","            The input data.","1030","","1031","        y : ndarray of shape (n_samples,) or (n_samples, n_outputs)","1032","            The target values (class labels in classification, real numbers in","1033","            regression).","1034","","1035","        Returns","1036","        -------","1037","        self : returns a trained MLP model.","1038","        \"\"\"","1039","        return self._fit(X, y, incremental=(self.warm_start and","1040","                                            hasattr(self, \"classes_\")))","1041",""]}],"doc\/whats_new\/v0.24.rst":[{"add":["512","- |Fix| Fix method  :func:`fit` of :class:`neural_network.MLPClassifier`","513","  not iterating to ``max_iter`` if warm started.","514","  :pr:`18269` by :user:`Norbert Preining <norbusan>` and","515","  :user:`Guillaume Lemaitre <glemaitre>`.","516",""],"delete":[]}]}},"5f9555b3ea98e4ce8bb62b1fc6521772418d9144":{"changes":{"sklearn\/utils\/_pprint.py":"MODIFY","sklearn\/utils\/tests\/test_pprint.py":"MODIFY","doc\/whats_new\/v0.23.rst":"MODIFY"},"diff":{"sklearn\/utils\/_pprint.py":[{"add":["96","","98","        if (k not in init_params or (  # happens if k is part of a **kwargs","99","                repr(v) != repr(init_params[k]) and","100","                not (is_scalar_nan(init_params[k]) and is_scalar_nan(v)))):"],"delete":["97","        if (repr(v) != repr(init_params[k]) and","98","                not (is_scalar_nan(init_params[k]) and is_scalar_nan(v))):"]}],"sklearn\/utils\/tests\/test_pprint.py":[{"add":["10","from sklearn import set_config, config_context","540","","541","","542","def test_kwargs_in_init():","543","    # Make sure the changed_only=True mode is OK when an argument is passed as","544","    # kwargs.","545","    # Non-regression test for","546","    # https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/17206","547","","548","    class WithKWargs(BaseEstimator):","549","        # Estimator with a kwargs argument. These need to hack around","550","        # set_params and get_params. Here we mimic what LightGBM does.","551","        def __init__(self, a='willchange', b='unchanged', **kwargs):","552","            self.a = a","553","            self.b = b","554","            self._other_params = {}","555","            self.set_params(**kwargs)","556","","557","        def get_params(self, deep=True):","558","            params = super().get_params(deep=deep)","559","            params.update(self._other_params)","560","            return params","561","","562","        def set_params(self, **params):","563","            for key, value in params.items():","564","                setattr(self, key, value)","565","                self._other_params[key] = value","566","            return self","567","","568","    est = WithKWargs(a='something', c='abcd', d=None)","569","","570","    expected = \"WithKWargs(a='something', c='abcd', d=None)\"","571","    assert expected == est.__repr__()","572","","573","    with config_context(print_changed_only=False):","574","        expected = \"WithKWargs(a='something', b='unchanged', c='abcd', d=None)\"","575","        assert expected == est.__repr__()"],"delete":["10","from sklearn import set_config"]}],"doc\/whats_new\/v0.23.rst":[{"add":["18","  provided by the user were modified in place. :pr:`17204` by","21","Miscellaneous","22",".............","23","","24","- |Fix| Fixed a bug in the `repr` of third-party estimators that use a","25","  `**kwargs` parameter in their constructor, when `changed_only` is True","26","  which is now the default. :pr:`17205` by `Nicolas Hug`_.","27",""],"delete":["18","  provided by the user was modified in place. :pr:`17204` by"]}]}},"2992fe27f39eff397f6f046343033db860fb1dc0":{"changes":{"build_tools\/circle\/build_doc.sh":"MODIFY",".circleci\/config.yml":"MODIFY"},"diff":{"build_tools\/circle\/build_doc.sh":[{"add":["148","MINICONDA_PATH=$HOME\/miniconda"],"delete":[]}],".circleci\/config.yml":[{"add":[],"delete":["9","      - MINICONDA_PATH: ~\/miniconda","51","      - MINICONDA_PATH: ~\/miniconda"]}]}},"5d3d54da19ef5cefa7c14c89285559907df952f1":{"changes":{"doc\/developers\/contributing.rst":"MODIFY"},"diff":{"doc\/developers\/contributing.rst":[{"add":["265","","488",".. _stalled_pull_request:","489","","524","Stalled and Unclaimed Issues","525","^^^^^^^^^^^^^^^^^^^^^^^^^^^^","526","","527","Generally speaking, issues which are up for grabs will have a","528","`\"help wanted\" <https:\/\/github.com\/scikit-learn\/scikit-learn\/labels\/help%20wanted>`__ .","529","tag. However, not all issues which need contributors will have this tag,","530","as the \"help wanted\" tag is not always up-to-date with the state","531","of the issue. Contributors can find issues which are still up for grabs","532","using the following guidelines:","533","","534","* First, to **determine if an issue is claimed**:","535","","536","  * Check for linked pull requests","537","  * Check the conversation to see if anyone has said that they're working on","538","    creating a pull request","539","","540","* If a contributor comments on an issue to say they are working on it,","541","  a pull request is expected within 2 weeks (new contributor) or 4 weeks","542","  (contributor or core dev), unless an larger time frame is explicitly given.","543","  Beyond that time, another contributor can take the issue and make a","544","  pull request for it. We encourage contributors to comment directly on the","545","  stalled or unclaimed issue to let community members know that they will be","546","  working on it.","547","","548","* If the issue is linked to a :ref:`stalled pull request <stalled_pull_request>`,","549","  we recommend that contributors follow the procedure","550","  described in the :ref:`stalled_pull_request`","551","  section rather than working directly on the issue.","552",""],"delete":["265"," "]}]}},"3fb9e758a57455fc16847cc8d9452147f25d43a0":{"changes":{"sklearn\/impute\/tests\/test_impute.py":"MODIFY","sklearn\/impute\/_base.py":"MODIFY","doc\/whats_new\/v0.24.rst":"MODIFY"},"diff":{"sklearn\/impute\/tests\/test_impute.py":[{"add":["1345","    'strategy, expected',","1346","    [('most_frequent', 'b'), ('constant', 'missing_value')]","1347",")","1348","def test_simple_imputation_string_list(strategy, expected):","1349","    X = [['a', 'b'],","1350","         ['c', np.nan]]","1351","","1352","    X_true = np.array([","1353","        ['a', 'b'],","1354","        ['c', expected]","1355","    ], dtype=object)","1356","","1357","    imputer = SimpleImputer(strategy=strategy)","1358","    X_trans = imputer.fit_transform(X)","1359","","1360","    assert_array_equal(X_trans, X_true)","1361","","1362","","1363","@pytest.mark.parametrize("],"delete":[]}],"sklearn\/impute\/_base.py":[{"add":["230","            # If input is a list of strings, dtype = object.","231","            # Otherwise ValueError is raised in SimpleImputer","232","            # with strategy='most_frequent' or 'constant'","233","            # because the list is converted to Unicode numpy array","234","            if isinstance(X, list) and \\","235","               any(isinstance(elem, str) for row in X for elem in row):","236","                dtype = object","237","            else:","238","                dtype = None"],"delete":["230","            dtype = None"]}],"doc\/whats_new\/v0.24.rst":[{"add":["80","- |Feature| :class:`impute.SimpleImputer` now supports a list of strings","81","  when ``strategy='most_frequent'`` or ``strategy='constant'``.","82","  :pr:`17526` by :user:`Ayako YAGI <yagi-3>` and :user:`Juan Carlos Alfaro Jim¨¦nez <alfaro96>`.","83",""],"delete":[]}]}},"06d6f8a9f6e3d847b9db79a5fdebe64f78deef7f":{"changes":{"sklearn\/decomposition\/_dict_learning.py":"MODIFY","sklearn\/linear_model\/_omp.py":"MODIFY","sklearn\/utils\/fixes.py":"MODIFY","sklearn\/feature_selection\/_rfe.py":"MODIFY","sklearn\/covariance\/_graph_lasso.py":"MODIFY","sklearn\/neighbors\/_base.py":"MODIFY","sklearn\/linear_model\/_logistic.py":"MODIFY","sklearn\/calibration.py":"MODIFY","sklearn\/multioutput.py":"MODIFY","benchmarks\/bench_saga.py":"MODIFY","sklearn\/ensemble\/_forest.py":"MODIFY","sklearn\/utils\/tests\/test_parallel.py":"ADD","sklearn\/linear_model\/_least_angle.py":"MODIFY","sklearn\/linear_model\/_coordinate_descent.py":"MODIFY","sklearn\/model_selection\/_search.py":"MODIFY","sklearn\/decomposition\/_lda.py":"MODIFY","sklearn\/inspection\/_plot\/partial_dependence.py":"MODIFY","sklearn\/ensemble\/_stacking.py":"MODIFY","sklearn\/inspection\/_permutation_importance.py":"MODIFY","sklearn\/linear_model\/_stochastic_gradient.py":"MODIFY","sklearn\/linear_model\/_theil_sen.py":"MODIFY","sklearn\/manifold\/_mds.py":"MODIFY","sklearn\/model_selection\/_validation.py":"MODIFY","sklearn\/metrics\/pairwise.py":"MODIFY","sklearn\/ensemble\/_bagging.py":"MODIFY","sklearn\/ensemble\/_voting.py":"MODIFY","sklearn\/pipeline.py":"MODIFY","build_tools\/circle\/linting.sh":"MODIFY","sklearn\/multiclass.py":"MODIFY","sklearn\/linear_model\/_base.py":"MODIFY","sklearn\/cluster\/_mean_shift.py":"MODIFY","sklearn\/compose\/_column_transformer.py":"MODIFY"},"diff":{"sklearn\/decomposition\/_dict_learning.py":[{"add":["13","from joblib import Parallel, effective_n_jobs","21","from ..utils.fixes import delayed"],"delete":["13","from joblib import Parallel, delayed, effective_n_jobs"]}],"sklearn\/linear_model\/_omp.py":[{"add":["13","from joblib import Parallel","19","from ..utils.fixes import delayed"],"delete":["13","from joblib import Parallel, delayed"]}],"sklearn\/utils\/fixes.py":[{"add":["12","from functools import update_wrapper","14","import functools","22","from .._config import config_context, get_config","201","","202","","203","# remove when https:\/\/github.com\/joblib\/joblib\/issues\/1071 is fixed","204","def delayed(function):","205","    \"\"\"Decorator used to capture the arguments of a function.\"\"\"","206","    @functools.wraps(function)","207","    def delayed_function(*args, **kwargs):","208","        return _FuncWrapper(function), args, kwargs","209","    return delayed_function","210","","211","","212","class _FuncWrapper:","213","    \"\"\"\"Load the global configuration before calling the function.\"\"\"","214","    def __init__(self, function):","215","        self.function = function","216","        self.config = get_config()","217","        update_wrapper(self, self.function)","218","","219","    def __call__(self, *args, **kwargs):","220","        with config_context(**self.config):","221","            return self.function(*args, **kwargs)"],"delete":[]}],"sklearn\/feature_selection\/_rfe.py":[{"add":["10","from joblib import Parallel, effective_n_jobs","16","from ..utils.fixes import delayed"],"delete":["10","from joblib import Parallel, delayed, effective_n_jobs"]}],"sklearn\/covariance\/_graph_lasso.py":[{"add":["15","from joblib import Parallel","22","from ..utils.fixes import delayed"],"delete":["15","from joblib import Parallel, delayed"]}],"sklearn\/neighbors\/_base.py":[{"add":["17","from joblib import Parallel, effective_n_jobs","30","from ..utils.fixes import delayed","706","                delayed_query = delayed(_tree_query_parallel_helper)"],"delete":["17","from joblib import Parallel, delayed, effective_n_jobs","705","                check_pickle = False if old_joblib else None","706","                delayed_query = delayed(_tree_query_parallel_helper,","707","                                        check_pickle=check_pickle)"]}],"sklearn\/linear_model\/_logistic.py":[{"add":["18","from joblib import Parallel, effective_n_jobs","34","from ..utils.fixes import delayed"],"delete":["18","from joblib import Parallel, delayed, effective_n_jobs"]}],"sklearn\/calibration.py":[{"add":["15","from joblib import Parallel","26","from .utils.fixes import delayed"],"delete":["15","from joblib import delayed, Parallel"]}],"sklearn\/multioutput.py":[{"add":["18","from joblib import Parallel","29","from .utils.fixes import delayed"],"delete":["18","from joblib import Parallel, delayed"]}],"benchmarks\/bench_saga.py":[{"add":["9","from joblib import Parallel","10","from sklearn.utils.fixes import delayed"],"delete":["9","from joblib import delayed, Parallel"]}],"sklearn\/ensemble\/_forest.py":[{"add":["50","from joblib import Parallel","61","from ..utils.fixes import delayed"],"delete":["50","from joblib import Parallel, delayed"]}],"sklearn\/utils\/tests\/test_parallel.py":[{"add":[],"delete":[]}],"sklearn\/linear_model\/_least_angle.py":[{"add":["17","from joblib import Parallel","27","from ..utils.fixes import delayed"],"delete":["17","from joblib import Parallel, delayed"]}],"sklearn\/linear_model\/_coordinate_descent.py":[{"add":["14","from joblib import Parallel, effective_n_jobs","27","from ..utils.fixes import delayed"],"delete":["14","from joblib import Parallel, delayed, effective_n_jobs"]}],"sklearn\/model_selection\/_search.py":[{"add":["34","from joblib import Parallel","40","from ..utils.fixes import delayed"],"delete":["34","from joblib import Parallel, delayed"]}],"sklearn\/decomposition\/_lda.py":[{"add":["16","from joblib import Parallel, effective_n_jobs","23","from ..utils.fixes import delayed"],"delete":["16","from joblib import Parallel, delayed, effective_n_jobs"]}],"sklearn\/inspection\/_plot\/partial_dependence.py":[{"add":["8","from joblib import Parallel","16","from ...utils.fixes import delayed"],"delete":["8","from joblib import Parallel, delayed"]}],"sklearn\/ensemble\/_stacking.py":[{"add":["9","from joblib import Parallel","35","from ..utils.fixes import delayed"],"delete":["9","from joblib import Parallel, delayed"]}],"sklearn\/inspection\/_permutation_importance.py":[{"add":["9","from ..utils.fixes import delayed"],"delete":["3","from joblib import delayed"]}],"sklearn\/linear_model\/_stochastic_gradient.py":[{"add":["11","from joblib import Parallel","22","from ..utils.fixes import delayed"],"delete":["11","from joblib import Parallel, delayed"]}],"sklearn\/linear_model\/_theil_sen.py":[{"add":["17","from joblib import Parallel, effective_n_jobs","23","from ..utils.fixes import delayed"],"delete":["17","from joblib import Parallel, delayed, effective_n_jobs"]}],"sklearn\/manifold\/_mds.py":[{"add":["8","from joblib import Parallel, effective_n_jobs","17","from ..utils.fixes import delayed"],"delete":["8","from joblib import Parallel, delayed, effective_n_jobs"]}],"sklearn\/model_selection\/_validation.py":[{"add":["20","from joblib import Parallel, logger","27","from ..utils.fixes import delayed"],"delete":["20","from joblib import Parallel, delayed, logger"]}],"sklearn\/metrics\/pairwise.py":[{"add":["19","from joblib import Parallel, effective_n_jobs","31","from ..utils.fixes import delayed"],"delete":["19","from joblib import Parallel, delayed, effective_n_jobs"]}],"sklearn\/ensemble\/_bagging.py":[{"add":["12","from joblib import Parallel","25","from ..utils.fixes import delayed"],"delete":["12","from joblib import Parallel, delayed"]}],"sklearn\/ensemble\/_voting.py":[{"add":["19","from joblib import Parallel","35","from ..utils.fixes import delayed"],"delete":["19","from joblib import Parallel, delayed"]}],"sklearn\/pipeline.py":[{"add":["16","from joblib import Parallel","24","from .utils.fixes import delayed"],"delete":["16","from joblib import Parallel, delayed"]}],"build_tools\/circle\/linting.sh":[{"add":["174","","175","joblib_import=\"$(git grep -l -A 10 -E \"joblib import.+delayed\" -- \"*.py\" \":!sklearn\/utils\/_joblib.py\" \":!sklearn\/utils\/fixes.py\")\"","176","","177","if [ ! -z \"$joblib_import\" ]; then","178","    echo \"Use from sklearn.utils.fixes import delayed instead of joblib delayed. The following files contains imports to joblib.delayed:\"","179","    echo \"$joblib_import\"","180","    exit 1","181","fi"],"delete":[]}],"sklearn\/multiclass.py":[{"add":["55","from .utils.fixes import delayed","58","from joblib import Parallel"],"delete":["57","from joblib import Parallel, delayed"]}],"sklearn\/linear_model\/_base.py":[{"add":["25","from joblib import Parallel","39","from ..utils.fixes import delayed"],"delete":["25","from joblib import Parallel, delayed"]}],"sklearn\/cluster\/_mean_shift.py":[{"add":["18","from joblib import Parallel","22","from ..utils.fixes import delayed"],"delete":["18","from joblib import Parallel, delayed"]}],"sklearn\/compose\/_column_transformer.py":[{"add":["14","from joblib import Parallel","27","from ..utils.fixes import delayed"],"delete":["14","from joblib import Parallel, delayed"]}]}},"90d00daa76d1c6848a60554c7eba15f1e351ed46":{"changes":{"doc\/whats_new\/v0.23.rst":"MODIFY","sklearn\/cluster\/_k_means_lloyd.pyx":"MODIFY","sklearn\/cluster\/_k_means_elkan.pyx":"MODIFY"},"diff":{"doc\/whats_new\/v0.23.rst":[{"add":["21","- |Efficiency| :class:`cluster.KMeans` cannot spawn idle threads any more for","22","  very small datasets. :pr:`17210` by","23","  :user:`Jeremie du Boisberranger <jeremiedbb>`.","24",""],"delete":[]}],"sklearn\/cluster\/_k_means_lloyd.pyx":[{"add":["122","    # number of threads should not be bigger than number of chunks","123","    n_threads = min(n_threads, n_chunks)","124","","319","    # number of threads should not be bigger than number of chunks","320","    n_threads = min(n_threads, n_chunks)","321",""],"delete":[]}],"sklearn\/cluster\/_k_means_elkan.pyx":[{"add":["286","    # number of threads should not be bigger than number of chunks","287","    n_threads = min(n_threads, n_chunks)","288","","520","    # number of threads should not be bigger than number of chunks","521","    n_threads = min(n_threads, n_chunks)","522",""],"delete":[]}]}},"43c0efa1b9ec1b2af22849f231355fae4e1051e7":{"changes":{"sklearn\/ensemble\/_hist_gradient_boosting\/splitting.pyx":"MODIFY"},"diff":{"sklearn\/ensemble\/_hist_gradient_boosting\/splitting.pyx":[{"add":["136","        const signed char [::1] monotonic_cst","152","                 const signed char [::1] monotonic_cst,","412","            const signed char [::1] monotonic_cst = self.monotonic_cst","498","            signed char monotonic_cst,","612","            signed char monotonic_cst,","725","        signed char monotonic_cst,","822","    return value"],"delete":["136","        const char [::1] monotonic_cst","152","                 const char [::1] monotonic_cst,","412","            const char [::1] monotonic_cst = self.monotonic_cst","498","            char monotonic_cst,","612","            char monotonic_cst,","725","        char monotonic_cst,","822","    return value"]}]}},"547ead6c0c77633df5391cf3523f4f326fc26ea3":{"changes":{"sklearn\/ensemble\/_gradient_boosting.pyx":"MODIFY","sklearn\/utils\/validation.py":"MODIFY","sklearn\/ensemble\/_gb.py":"MODIFY","sklearn\/inspection\/_plot\/partial_dependence.py":"MODIFY","examples\/linear_model\/plot_tweedie_regression_insurance_claims.py":"MODIFY","sklearn\/preprocessing\/_encoders.py":"MODIFY","sklearn\/compose\/tests\/test_column_transformer.py":"MODIFY","sklearn\/inspection\/_partial_dependence.py":"MODIFY","sklearn\/neural_network\/tests\/test_rbm.py":"MODIFY","sklearn\/compose\/_column_transformer.py":"MODIFY","sklearn\/preprocessing\/tests\/test_encoders.py":"MODIFY","sklearn\/utils\/tests\/test_validation.py":"MODIFY"},"diff":{"sklearn\/ensemble\/_gradient_boosting.pyx":[{"add":["264","          np_zeros((n_total_samples,), dtype=bool)"],"delete":["34","from numpy import bool as np_bool","265","          np_zeros((n_total_samples,), dtype=np_bool)"]}],"sklearn\/utils\/validation.py":[{"add":["517","                dtypes_orig[i] = np.dtype(object)"],"delete":["517","                dtypes_orig[i] = np.dtype(np.object)"]}],"sklearn\/ensemble\/_gb.py":[{"add":["308","                                    dtype=object)","318","            self.estimators_ = np.empty((0, 0), dtype=object)"],"delete":["308","                                    dtype=np.object)","318","            self.estimators_ = np.empty((0, 0), dtype=np.object)"]}],"sklearn\/inspection\/_plot\/partial_dependence.py":[{"add":["250","        X = check_array(X, force_all_finite='allow-nan', dtype=object)","602","            self.axes_ = np.empty((n_rows, n_cols), dtype=object)","604","                self.lines_ = np.empty((n_rows, n_cols), dtype=object)","607","                                       dtype=object)","608","            self.contours_ = np.empty((n_rows, n_cols), dtype=object)","632","                self.lines_ = np.empty_like(ax, dtype=object)","635","                                       dtype=object)","636","            self.contours_ = np.empty_like(ax, dtype=object)","642","        self.deciles_vlines_ = np.empty_like(self.axes_, dtype=object)","643","        self.deciles_hlines_ = np.empty_like(self.axes_, dtype=object)"],"delete":["250","        X = check_array(X, force_all_finite='allow-nan', dtype=np.object)","602","            self.axes_ = np.empty((n_rows, n_cols), dtype=np.object)","604","                self.lines_ = np.empty((n_rows, n_cols), dtype=np.object)","607","                                       dtype=np.object)","608","            self.contours_ = np.empty((n_rows, n_cols), dtype=np.object)","632","                self.lines_ = np.empty_like(ax, dtype=np.object)","635","                                       dtype=np.object)","636","            self.contours_ = np.empty_like(ax, dtype=np.object)","642","        self.deciles_vlines_ = np.empty_like(self.axes_, dtype=np.object)","643","        self.deciles_hlines_ = np.empty_like(self.axes_, dtype=np.object)"]}],"examples\/linear_model\/plot_tweedie_regression_insurance_claims.py":[{"add":["88","    for column_name in df.columns[df.dtypes.values == object]:"],"delete":["88","    for column_name in df.columns[df.dtypes.values == np.object]:"]}],"sklearn\/preprocessing\/_encoders.py":[{"add":["45","                X = check_array(X, dtype=object)","324","                return np.zeros(len(self.categories_), dtype=object)","327","                                for cats in self.categories_], dtype=object)","363","                            dtype=object)"],"delete":["45","                X = check_array(X, dtype=np.object)","324","                return np.zeros(len(self.categories_), dtype=np.object)","327","                                for cats in self.categories_], dtype=np.object)","363","                            dtype=np.object)"]}],"sklearn\/compose\/tests\/test_column_transformer.py":[{"add":["1275","    (['col_str'], None, [object], None),","1276","    (['col_str'], None, object, None),","1284","    (['col_int', 'col_float', 'col_str'], None, [np.number, object], None),"],"delete":["1275","    (['col_str'], None, [np.object], None),","1276","    (['col_str'], None, np.object, None),","1284","    (['col_int', 'col_float', 'col_str'], None, [np.number, np.object], None),"]}],"sklearn\/inspection\/_partial_dependence.py":[{"add":["399","        X = check_array(X, force_all_finite='allow-nan', dtype=object)"],"delete":["399","        X = check_array(X, force_all_finite='allow-nan', dtype=np.object)"]}],"sklearn\/neural_network\/tests\/test_rbm.py":[{"add":["198","    (int, np.float64)])"],"delete":["198","    (np.int, np.float64)])"]}],"sklearn\/compose\/_column_transformer.py":[{"add":["650","    return check_array(X, force_all_finite='allow-nan', dtype=object)"],"delete":["650","    return check_array(X, force_all_finite='allow-nan', dtype=np.object)"]}],"sklearn\/preprocessing\/tests\/test_encoders.py":[{"add":["684","    assert ohe_test.drop_idx_.dtype == object"],"delete":["684","    assert ohe_test.drop_idx_.dtype == np.object"]}],"sklearn\/utils\/tests\/test_validation.py":[{"add":["213","     (np.array([[1, np.nan]], dtype=object),","385","    X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=object)"],"delete":["213","     (np.array([[1, np.nan]], dtype=np.object),","385","    X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=np.object)"]}]}},"936854579a3cc112f0686e397798d2aa73e5fab3":{"changes":{"sklearn\/metrics\/tests\/test_regression.py":"MODIFY","doc\/whats_new\/v0.24.rst":"MODIFY","sklearn\/metrics\/_regression.py":"MODIFY"},"diff":{"sklearn\/metrics\/tests\/test_regression.py":[{"add":["78","    assert_almost_equal(error, 0.454, decimal=2)","260","    assert_almost_equal(rmsew, 0.59, decimal=2)"],"delete":["78","    assert_almost_equal(error, 0.645, decimal=2)","260","    assert_almost_equal(rmsew, 0.62, decimal=2)"]}],"doc\/whats_new\/v0.24.rst":[{"add":["63",":mod:`sklearn.metrics`","64","......................","65","","66","- |Fix| Fixed a bug in :func:`metrics.mean_squared_error` where the","67","  average of multiple RMSE values was incorrectly calculated as the root of the","68","  average of multiple MSE values.","69","  :pr:`17309` by :user:`Swier Heeres <swierh>`","70",""],"delete":[]}],"sklearn\/metrics\/_regression.py":[{"add":["246","    >>> mean_squared_error(y_true, y_pred, squared=False)","247","    0.822...","259","","260","    if not squared:","261","        output_errors = np.sqrt(output_errors)","262","","265","            return output_errors","270","    return np.average(output_errors, weights=multioutput)"],"delete":["259","            return output_errors if squared else np.sqrt(output_errors)","264","    mse = np.average(output_errors, weights=multioutput)","265","    return mse if squared else np.sqrt(mse)"]}]}},"576775d6d15b7b69aebd2c464d932c28722eb4a0":{"changes":{"sklearn\/manifold\/tests\/test_mds.py":"MODIFY","doc\/whats_new\/v0.24.rst":"MODIFY","sklearn\/manifold\/_mds.py":"MODIFY"},"diff":{"sklearn\/manifold\/tests\/test_mds.py":[{"add":["64","","65","","66","@pytest.mark.parametrize(\"dissimilarity, expected_pairwise\", [","67","   (\"precomputed\", True),","68","   (\"euclidean\", False),","69","])","70","def test_MDS_pairwise(dissimilarity, expected_pairwise):","71","    # _pairwise attribute is set correctly","72","    mds_clf = mds.MDS(dissimilarity=dissimilarity)","73","    assert mds_clf._pairwise == expected_pairwise"],"delete":[]}],"doc\/whats_new\/v0.24.rst":[{"add":["289","- |Fix| :class:`manifold.MDS` now correctly sets its `_pairwise` attribute.","290","  :pr:`18278` by `Thomas Fan`_.","291","","369","  a estimator for multiclass wrappers.","380","- |Enhancement| :class:`multioutput.MultiOutputClassifier` and","384","  estimators) can be used as a estimator for multiclass wrappers."],"delete":["366","  a estimator for multiclass wrappers. ","377","- |Enhancement| :class:`multioutput.MultiOutputClassifier` and ","381","  estimators) can be used as a estimator for multiclass wrappers. "]}],"sklearn\/manifold\/_mds.py":[{"add":["388","        return self.dissimilarity == \"precomputed\""],"delete":["388","        return self.kernel == \"precomputed\""]}]}},"6b68144f179b9a56e05ae401da7527bc5da97f21":{"changes":{"sklearn\/tests\/test_multiclass.py":"MODIFY","sklearn\/multiclass.py":"MODIFY","doc\/whats_new\/v0.24.rst":"MODIFY"},"diff":{"sklearn\/tests\/test_multiclass.py":[{"add":["2","import pytest","12","from sklearn.utils._mocking import CheckingClassifier","18","from sklearn.utils import check_array","710","def test_ecoc_delegate_sparse_base_estimator():","711","    # Non-regression test for","712","    # https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/17218","713","    X, y = iris.data, iris.target","714","    X_sp = sp.csc_matrix(X)","715","","716","    # create an estimator that does not support sparse input","717","    base_estimator = CheckingClassifier(","718","        check_X=check_array,","719","        check_X_params={\"ensure_2d\": True, \"accept_sparse\": False},","720","    )","721","    ecoc = OutputCodeClassifier(base_estimator, random_state=0)","722","","723","    with pytest.raises(TypeError, match=\"A sparse matrix was passed\"):","724","        ecoc.fit(X_sp, y)","725","","726","    ecoc.fit(X, y)","727","    with pytest.raises(TypeError, match=\"A sparse matrix was passed\"):","728","        ecoc.predict(X_sp)","729","","730","    # smoke test to check when sparse input should be supported","731","    ecoc = OutputCodeClassifier(LinearSVC(random_state=0))","732","    ecoc.fit(X_sp, y).predict(X_sp)","733","    assert len(ecoc.estimators_) == 4","734","","735",""],"delete":[]}],"sklearn\/multiclass.py":[{"add":["804","        X, y = self._validate_data(X, y, accept_sparse=True)","852","        X = check_array(X, accept_sparse=True)"],"delete":["804","        X, y = self._validate_data(X, y)","852","        X = check_array(X)"]}],"doc\/whats_new\/v0.24.rst":[{"add":["115",":mod:`sklearn.multiclass`","116",".........................","117","","118","- |Fix| A fix to allow :class:`multiclass.OutputCodeClassifier` to accept","119","  sparse input data in its `fit` and `predict` methods. The check for","120","  validity of the input is now delegated to the base estimator.","121","  :pr:`17233` by :user:`Zolisa Bleki <zoj613>`.","122","","123",""],"delete":[]}]}},"324d3b59ec59f0d5415d561de28c40e4c054941e":{"changes":{"sklearn\/ensemble\/_stacking.py":"MODIFY","sklearn\/ensemble\/_weight_boosting.py":"MODIFY"},"diff":{"sklearn\/ensemble\/_stacking.py":[{"add":["270","        The default classifier is a","271","        :class:`~sklearn.linear_model.LogisticRegression`.","284","        either binary or multiclass,","285","        :class:`~sklearn.model_selection.StratifiedKFold` is used.","286","        In all other cases, :class:`~sklearn.model_selection.KFold` is used.","544","        The default regressor is a :class:`~sklearn.linear_model.RidgeCV`.","557","        either binary or multiclass,","558","        :class:`~sklearn.model_selection.StratifiedKFold` is used.","559","        In all other cases, :class:`~sklearn.model_selection.KFold` is used."],"delete":["270","        The default classifier is a `LogisticRegression`.","283","        either binary or multiclass, `StratifiedKFold` is used. In all other","284","        cases, `KFold` is used.","542","        The default regressor is a `RidgeCV`.","555","        either binary or multiclass, `StratifiedKFold` is used. In all other","556","        cases, `KFold` is used."]}],"sklearn\/ensemble\/_weight_boosting.py":[{"add":["7","- The `BaseWeightBoosting` base class implements a common ``fit`` method","11","- :class:`~sklearn.ensemble.AdaBoostClassifier` implements adaptive boosting","12","  (AdaBoost-SAMME) for classification problems.","14","- :class:`~sklearn.ensemble.AdaBoostRegressor` implements adaptive boosting","15","  (AdaBoost.R2) for regression problems.","305","        the base estimator is :class:`~sklearn.tree.DecisionTreeClassifier`","306","        initialized with `max_depth=1`.","891","        :class:`~sklearn.tree.DecisionTreeRegressor` initialized with","892","        `max_depth=3`."],"delete":["7","- The ``BaseWeightBoosting`` base class implements a common ``fit`` method","11","- ``AdaBoostClassifier`` implements adaptive boosting (AdaBoost-SAMME) for","12","  classification problems.","14","- ``AdaBoostRegressor`` implements adaptive boosting (AdaBoost.R2) for","15","  regression problems.","305","        the base estimator is ``DecisionTreeClassifier(max_depth=1)``.","890","        ``DecisionTreeRegressor(max_depth=3)``."]}]}},"f7a8dd2877c90249506ac8ad0a3eea7306636105":{"changes":{"sklearn\/multioutput.py":"MODIFY","doc\/whats_new\/v0.23.rst":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY"},"diff":{"sklearn\/multioutput.py":[{"add":["148","            .. versionadded:: 0.23","149","","240","        .. versionchanged:: 0.20","309","        .. versionchanged:: 0.20","352","            .. versionadded:: 0.23","353","","454","            .. versionadded:: 0.23","455","","826","            .. versionadded:: 0.23","827",""],"delete":["238","        .. versionchanged:: v0.20","307","        .. versionchanged:: v0.20"]}],"doc\/whats_new\/v0.23.rst":[{"add":["616","- |Feature| :func:`multioutput.MultiOutputRegressor.fit` and","617","  :func:`multioutput.MultiOutputClassifier.fit` now can accept `fit_params`","618","  to pass to the `estimator.fit` method of each step. :issue:`15953`","619","  :pr:`15959` by :user:`Ke Huang <huangk10>`.","620",""],"delete":[]}],"doc\/whats_new\/v0.22.rst":[{"add":[],"delete":["153",":mod:`sklearn.multioutput`","154","..........................","155","","156","- |Feature| :func:`multioutput.MultiOutputRegressor.fit` and","157","  :func:`multioutput.MultiOutputClassifier.fit` now can accept `fit_params`","158","  to pass to the `estimator.fit` method of each step. :issue:`15953`","159","  :pr:`15959` by :user:`Ke Huang <huangk10>`.","160",""]}]}},"58451568d3b44ba632d708add02f9c30f356570a":{"changes":{"sklearn\/decomposition\/tests\/test_truncated_svd.py":"MODIFY","sklearn\/decomposition\/_truncated_svd.py":"MODIFY","doc\/whats_new\/v0.24.rst":"MODIFY"},"diff":{"sklearn\/decomposition\/tests\/test_truncated_svd.py":[{"add":["193","","194","","195","@pytest.mark.parametrize(\"algorithm, tol\", [","196","    ('randomized', 0.), ('arpack', 1e-6), ('arpack', 0.)])","197","@pytest.mark.parametrize('kind', ('dense', 'sparse'))","198","def test_fit_transform(X_sparse, algorithm, tol, kind):","199","    # fit_transform(X) should equal fit(X).transform(X)","200","    X = X_sparse if kind == 'sparse' else X_sparse.toarray()","201","    svd = TruncatedSVD(n_components=5, n_iter=7, random_state=42,","202","                       algorithm=algorithm, tol=tol)","203","    X_transformed_1 = svd.fit_transform(X)","204","    X_transformed_2 = svd.fit(X).transform(X)","205","    assert_allclose(X_transformed_1, X_transformed_2)"],"delete":[]}],"sklearn\/decomposition\/_truncated_svd.py":[{"add":["130","        \"\"\"Fit model on training data X.","148","        \"\"\"Fit model to X and perform dimensionality reduction on X.","187","        # As a result of the SVD approximation error on X ~ U @ Sigma @ V.T,","188","        # X @ V is not the same as U @ Sigma","189","        if self.algorithm == \"randomized\" or \\","190","                (self.algorithm == \"arpack\" and self.tol > 0):","191","            X_transformed = safe_sparse_dot(X, self.components_.T)","192","        else:","193","            X_transformed = U * Sigma","194",""],"delete":["130","        \"\"\"Fit LSI model on training data X.","148","        \"\"\"Fit LSI model to X and perform dimensionality reduction on X.","188","        X_transformed = U * Sigma"]}],"doc\/whats_new\/v0.24.rst":[{"add":["182","- |Fix| :meth:`TruncatedSVD.fit_transform` consistently returns the same","183","  as :meth:`TruncatedSVD.fit` followed by :meth:`TruncatedSVD.transform`.","184","  :pr:`18528` by :user:`Albert Villanova del Moral <albertvillanova>` and","185","  :user:`Ruifeng Zheng <zhengruifeng>`.","186",""],"delete":[]}]}},"c2b31ac21b8780498e11a42744212231b3fefaa6":{"changes":{"sklearn\/tree\/_classes.py":"MODIFY"},"diff":{"sklearn\/tree\/_classes.py":[{"add":["1154","    >>> from sklearn.datasets import load_diabetes","1157","    >>> X, y = load_diabetes(return_X_y=True)","1162","    array([-0.39..., -0.46...,  0.02...,  0.06..., -0.50...,","1163","           0.16...,  0.11..., -0.73..., -0.30..., -0.00...])","1699","    >>> from sklearn.datasets import load_diabetes","1703","    >>> X, y = load_diabetes(return_X_y=True)","1710","    0.33..."],"delete":["1154","    >>> from sklearn.datasets import load_boston","1157","    >>> X, y = load_boston(return_X_y=True)","1162","    array([ 0.61..., 0.57..., -0.34..., 0.41..., 0.75...,","1163","            0.07..., 0.29..., 0.33..., -1.42..., -1.77...])","1699","    >>> from sklearn.datasets import load_boston","1703","    >>> X, y = load_boston(return_X_y=True)","1710","    0.7447..."]}]}},"0ce21d4133d70774e6a8e794d479d8bd3469c9a2":{"changes":{"doc\/themes\/scikit-learn-modern\/static\/css\/theme.css":"MODIFY"},"diff":{"doc\/themes\/scikit-learn-modern\/static\/css\/theme.css":[{"add":["73","  white-space: nowrap;"],"delete":[]}]}},"0ba19ad2d8943c7fb98fdefd523640c441bdb81c":{"changes":{"doc\/sphinxext\/github_link.py":"MODIFY"},"diff":{"doc\/sphinxext\/github_link.py":[{"add":["45","    # Unwrap the object to get the correct source","46","    # file in case that is wrapped by a decorator","47","    obj = inspect.unwrap(obj)","48",""],"delete":["42","    if type(class_name) != str:","43","        # Python 2 only","44","        class_name = class_name.encode('utf-8')"]}]}},"3cb3d4109e7acc497ad1e306013547e5f72ee5f4":{"changes":{"sklearn\/cluster\/_affinity_propagation.py":"MODIFY","sklearn\/cluster\/tests\/test_affinity_propagation.py":"MODIFY","doc\/whats_new\/v0.24.rst":"MODIFY"},"diff":{"sklearn\/cluster\/_affinity_propagation.py":[{"add":["164","    S += ((np.finfo(S.dtype).eps * S + np.finfo(S.dtype).tiny * 100) *"],"delete":["164","    S += ((np.finfo(np.double).eps * S + np.finfo(np.double).tiny * 100) *"]}],"sklearn\/cluster\/tests\/test_affinity_propagation.py":[{"add":["233","","234","","235","def test_affinity_propagation_float32():","236","    # Test to fix incorrect clusters due to dtype change","237","    # (non-regression test for issue #10832)","238","    X = np.array([[1, 0, 0, 0],","239","                  [0, 1, 1, 0],","240","                  [0, 1, 1, 0],","241","                  [0, 0, 0, 1]], dtype='float32')","242","    afp = AffinityPropagation(preference=1, affinity='precomputed',","243","                              random_state=0).fit(X)","244","    expected = np.array([0, 1, 1, 2])","245","    assert_array_equal(afp.labels_, expected)"],"delete":[]}],"doc\/whats_new\/v0.24.rst":[{"add":["72","- |Fix| Fixed a bug in :class:`cluster.AffinityPropagation`, that","73","  gives incorrect clusters when the array dtype is float32.","74","  :pr:`17995` by :user:`Thomaz Santana  <Wikilicious>` and :user:`Amanda Dsouza <amy12xx>`.","75",""],"delete":[]}]}},"0550793bd61b84beb60d3a92c3eb90cc788a27a8":{"changes":{"sklearn\/gaussian_process\/kernels.py":"MODIFY","examples\/gaussian_process\/plot_gpr_co2.py":"MODIFY","sklearn\/gaussian_process\/tests\/test_gpr.py":"MODIFY"},"diff":{"sklearn\/gaussian_process\/kernels.py":[{"add":["397","            if hyp.fixed:","398","                continue"],"delete":[]}],"examples\/gaussian_process\/plot_gpr_co2.py":[{"add":["131","                  noise_level_bounds=(1e-5, np.inf))  # noise terms"],"delete":["131","                  noise_level_bounds=(1e-3, np.inf))  # noise terms"]}],"sklearn\/gaussian_process\/tests\/test_gpr.py":[{"add":["16","from sklearn.gaussian_process.kernels import DotProduct, ExpSineSquared","527","","528","","529","def test_bound_check_fixed_hyperparameter():","530","    # Regression test for issue #17943","531","    # Check that having a hyperparameter with fixed bounds doesn't cause an","532","    # error","533","    k1 = 50.0**2 * RBF(length_scale=50.0)  # long term smooth rising trend","534","    k2 = ExpSineSquared(length_scale=1.0, periodicity=1.0,","535","                        periodicity_bounds=\"fixed\")  # seasonal component","536","    kernel = k1 + k2","537","    GaussianProcessRegressor(kernel=kernel).fit(X, y)"],"delete":["16","from sklearn.gaussian_process.kernels import DotProduct"]}]}},"41b18fea37e59a84bb9219c32c15c36432afc9ae":{"changes":{"doc\/whats_new\/v0.23.rst":"MODIFY","sklearn\/decomposition\/_sparse_pca.py":"MODIFY","sklearn\/decomposition\/tests\/test_sparse_pca.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.23.rst":[{"add":["217","- |Feature| Added `n_components_` attribute to :class:'decomposition.SparsePCA'","218","  and :class:'MiniBatchSparsePCA'. :pr:'16981' by :user:'Mateusz G¨®rski <Reksbril>'","219",""],"delete":[]}],"sklearn\/decomposition\/_sparse_pca.py":[{"add":["105","    n_components_ : int","106","        Estimated number of components.","107","","108","        .. versionadded:: 0.23","109","","204","        self.n_components_ = len(self.components_)","320","    n_components_ : int","321","        Estimated number of components.","322","","323","        .. versionadded:: 0.23","324","","416","        self.n_components_ = len(self.components_)"],"delete":[]}],"sklearn\/decomposition\/tests\/test_sparse_pca.py":[{"add":["209","","210","","211","@pytest.mark.parametrize(\"SPCA\", [SparsePCA, MiniBatchSparsePCA])","212","@pytest.mark.parametrize(\"n_components\", [None, 3])","213","def test_spca_n_components_(SPCA, n_components):","214","    rng = np.random.RandomState(0)","215","    n_samples, n_features = 12, 10","216","    X = rng.randn(n_samples, n_features)","217","","218","    model = SPCA(n_components=n_components).fit(X)","219","","220","    if n_components is not None:","221","        assert model.n_components_ == n_components","222","    else:","223","        assert model.n_components_ == n_features"],"delete":[]}]}},"a655de515e24515fff0007100dc9d49b5126b223":{"changes":{"doc\/whats_new\/v0.23.rst":"MODIFY","sklearn\/decomposition\/_pca.py":"MODIFY","sklearn\/decomposition\/tests\/test_pca.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.23.rst":[{"add":["141","","142","- |Fix| :class:`decomposition.PCA` with `n_components='mle'` now correctly","143","  handles small eigenvalues, and does not infer 0 as the correct number of","144","  components. :pr: `4441` by :user:`Lisa Schwetlick <lschwetlick>`, and","145","  :user:`Gelavizh Ahmadi <gelavizh1>` and :user:`Marija Vlajic Wheeler","146","  <marijavlajic>` and :pr:`16841` by `Nicolas Hug`_."],"delete":["141","- |Fix| :func:`decomposition._pca._assess_dimension` now correctly handles small","142","   eigenvalues. :pr: `4441` by :user:`Lisa Schwetlick <lschwetlick>`, and","143","   :user:`Gelavizh Ahmadi <gelavizh1>` and","144","   :user:`Marija Vlajic Wheeler <marijavlajic>`."]}],"sklearn\/decomposition\/_pca.py":[{"add":["30","def _assess_dimension(spectrum, rank, n_samples):","31","    \"\"\"Compute the log-likelihood of a rank ``rank`` dataset.","38","    spectrum : array of shape (n_features)","41","        Tested rank value. It should be strictly lower than n_features,","42","        otherwise the method isn't specified (division by zero in equation","43","        (31) from the paper).","58","    n_features = spectrum.shape[0]","59","    if not 1 <= rank < n_features:","60","        raise ValueError(\"the tested rank should be in [1, n_features - 1]\")","61","","62","    eps = 1e-15","63","","64","    if spectrum[rank - 1] < eps:","65","        # When the tested rank is associated with a small eigenvalue, there's","66","        # no point in computing the log-likelihood: it's going to be very","67","        # small and won't be the max anyway. Also, it can lead to numerical","68","        # issues below when computing pa, in particular in log((spectrum[i] -","69","        # spectrum[j]) because this will take the log of something very small.","70","        return -np.inf","73","    for i in range(1, rank + 1):","74","        pu += (gammaln((n_features - i + 1) \/ 2.) -","75","               log(np.pi) * (n_features - i + 1) \/ 2.)","80","    v = max(eps, np.sum(spectrum[rank:]) \/ (n_features - rank))","81","    pv = -np.log(v) * n_samples * (n_features - rank) \/ 2.","84","    pp = log(2. * np.pi) * (m + rank) \/ 2.","99","def _infer_dimension(spectrum, n_samples):","100","    \"\"\"Infers the dimension of a dataset with a given spectrum.","102","    The returned value will be in [1, n_features - 1].","104","    ll = np.empty_like(spectrum)","105","    ll[0] = -np.inf  # we don't want to return n_components = 0","106","    for rank in range(1, spectrum.shape[0]):","107","        ll[rank] = _assess_dimension(spectrum, rank, n_samples)","468","                _infer_dimension(explained_variance_, n_samples)"],"delete":["30","def _assess_dimension(spectrum, rank, n_samples, n_features):","31","    \"\"\"Compute the likelihood of a rank ``rank`` dataset.","38","    spectrum : array of shape (n)","41","        Tested rank value.","44","    n_features : int","45","        Number of features.","57","    if rank > len(spectrum):","58","        raise ValueError(\"The tested rank cannot exceed the rank of the\"","59","                         \" dataset\")","61","    spectrum_threshold = np.finfo(type(spectrum[0])).eps","64","    for i in range(rank):","65","        pu += (gammaln((n_features - i) \/ 2.) -","66","               log(np.pi) * (n_features - i) \/ 2.)","71","    if rank == n_features:","72","        # TODO: this line is never executed because _infer_dimension's","73","        # for loop is off by one","74","        pv = 0","75","        v = 1","76","    else:","77","        v = np.sum(spectrum[rank:]) \/ (n_features - rank)","78","        if spectrum_threshold > v:","79","            return -np.inf","80","        pv = -np.log(v) * n_samples * (n_features - rank) \/ 2.","83","    pp = log(2. * np.pi) * (m + rank + 1.) \/ 2.","89","        if spectrum_[i] < spectrum_threshold:","90","            # TODO: this line is never executed","91","            # (off by one in _infer_dimension)","92","            # this break only happens when rank == n_features and","93","            # spectrum_[i] < spectrum_threshold, otherwise the early return","94","            # above catches this case.","95","            break","105","def _infer_dimension(spectrum, n_samples, n_features):","106","    \"\"\"Infers the dimension of a dataset of shape (n_samples, n_features)","108","    The dataset is described by its spectrum `spectrum`.","110","    n_spectrum = len(spectrum)","111","    ll = np.empty(n_spectrum)","112","    for rank in range(n_spectrum):","113","        ll[rank] = _assess_dimension(spectrum, rank, n_samples, n_features)","474","                _infer_dimension(explained_variance_, n_samples, n_features)"]}],"sklearn\/decomposition\/tests\/test_pca.py":[{"add":["297","    assert pca.n_components_ == 1","335","    ll = np.array([_assess_dimension(spect, k, n) for k in range(1, p)])","350","    assert _infer_dimension(spect, n) > 1","363","    assert _infer_dimension(spect, n) > 2","572","def test_assess_dimension_bad_rank():","573","    # Test error when tested rank not in [1, n_features - 1]","576","    for rank in (0, 5):","577","        with pytest.raises(ValueError,","578","                           match=r\"should be in \\[1, n_features - 1\\]\"):","579","            _assess_dimension(spectrum, rank, n_samples)","582","def test_small_eigenvalues_mle():","583","    # Test rank associated with tiny eigenvalues are given a log-likelihood of","584","    # -inf. The inferred rank will be 1","586","","587","    assert _assess_dimension(spectrum, rank=1, n_samples=10) > -np.inf","588","","589","    for rank in (2, 3):","590","        assert _assess_dimension(spectrum, rank, 10) == -np.inf","591","","592","    assert _infer_dimension(spectrum, 10) == 1","595","def test_mle_redundant_data():","596","    # Test 'mle' with pathological X: only one relevant feature should give a","597","    # rank of 1","598","    X, _ = datasets.make_classification(n_features=20,","599","                                        n_informative=1, n_repeated=18,","603","    assert pca.n_components_ == 1","608","    # than the number of features during an mle fit","617","","618","","619","def test_mle_simple_case():","620","    # non-regression test for issue","621","    # https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/16730","622","    n_samples, n_dim = 1000, 10","623","    X = np.random.RandomState(0).randn(n_samples, n_dim)","624","    X[:, -1] = np.mean(X[:, :-1], axis=-1)  # true X dim is ndim - 1","625","    pca_skl = PCA('mle', svd_solver='full')","626","    pca_skl.fit(X)","627","    assert pca_skl.n_components_ == n_dim - 1","628","","629","","630","def test_assess_dimesion_rank_one():","631","    # Make sure assess_dimension works properly on a matrix of rank 1","632","    n_samples, n_features = 9, 6","633","    X = np.ones((n_samples, n_features))  # rank 1 matrix","634","    _, s, _ = np.linalg.svd(X, full_matrices=True)","635","    assert sum(s[1:]) == 0  # except for rank 1, all eigenvalues are 0","636","","637","    assert np.isfinite(_assess_dimension(s, rank=1, n_samples=n_samples))","638","    for rank in range(2, n_features):","639","        assert _assess_dimension(s, rank, n_samples) == -np.inf"],"delete":["297","    assert pca.n_components_ == 0","335","    ll = np.array([_assess_dimension(spect, k, n, p) for k in range(p)])","350","    assert _infer_dimension(spect, n, p) > 1","363","    assert _infer_dimension(spect, n, p) > 2","572","def test_infer_dim_bad_spec():","573","    # Test a spectrum that drops to near zero for PR #16224","576","    n_features = 5","577","    ret = _infer_dimension(spectrum, n_samples, n_features)","578","    assert ret == 0","581","def test_assess_dimension_error_rank_greater_than_features():","582","    # Test error when tested rank is greater than the number of features","583","    # for PR #16224","585","    n_samples = 10","586","    n_features = 4","587","    rank = 5","588","    with pytest.raises(ValueError, match=\"The tested rank cannot exceed \"","589","                                         \"the rank of the dataset\"):","590","        _assess_dimension(spectrum, rank, n_samples, n_features)","593","def test_assess_dimension_small_eigenvalues():","594","    # Test tiny eigenvalues appropriately when using 'mle'","595","    # for  PR #16224","596","    spectrum = np.array([1, 1e-30, 1e-30, 1e-30])","597","    n_samples = 10","598","    n_features = 5","599","    rank = 3","600","    ret = _assess_dimension(spectrum, rank, n_samples, n_features)","601","    assert ret == -np.inf","602","","603","","604","def test_infer_dim_mle():","605","    # Test small eigenvalues when 'mle' with pathological 'X' dataset","606","    # for PR #16224","607","    X, _ = datasets.make_classification(n_informative=1, n_repeated=18,","611","    assert pca.n_components_ == 0","616","    # than the number of features during an mle fit for PR #16224"]}]}},"d1f0d1917de1e6ecbefdf4173907cc03a4bfd4a8":{"changes":{"doc\/whats_new\/v0.23.rst":"MODIFY","sklearn\/ensemble\/_stacking.py":"MODIFY","sklearn\/ensemble\/tests\/test_stacking.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.23.rst":[{"add":["4",".. _changes_0_23_2:","5","","6","Version 0.23.2","7","==============","8","","9","Changelog","10","---------","11","","12",":mod:`sklearn.ensemble`","13",".......................","14","","15","- |Fix| Fixes :class:`ensemble.StackingClassifier` and","16","  :class:`ensemble.StackingRegressor` compatibility with estimators that","17","  do not define `n_features_in_`. :pr:`17357` by `Thomas Fan`_.","18",""],"delete":[]}],"sklearn\/ensemble\/_stacking.py":[{"add":["15","from ..exceptions import NotFittedError","199","    @property","200","    def n_features_in_(self):","201","        \"\"\"Number of features seen during :term:`fit`.\"\"\"","202","        try:","203","            check_is_fitted(self)","204","        except NotFittedError as nfe:","205","            raise AttributeError(","206","                f\"{self.__class__.__name__} object has no attribute \"","207","                f\"n_features_in_\") from nfe","208","        return self.estimators_[0].n_features_in_","209",""],"delete":["148","        self.n_features_in_ = self.estimators_[0].n_features_in_"]}],"sklearn\/ensemble\/tests\/test_stacking.py":[{"add":["19","from sklearn.datasets import make_regression","20","from sklearn.datasets import make_classification","495","","496","","497","@pytest.mark.parametrize(\"make_dataset, Stacking, Estimator\", [","498","    (make_classification, StackingClassifier, LogisticRegression),","499","    (make_regression, StackingRegressor, LinearRegression)","500","])","501","def test_stacking_without_n_features_in(make_dataset, Stacking, Estimator):","502","    # Stacking supports estimators without `n_features_in_`. Regression test","503","    # for #17353","504","","505","    class MyEstimator(Estimator):","506","        \"\"\"Estimator without n_features_in_\"\"\"","507","        def fit(self, X, y):","508","            super().fit(X, y)","509","            del self.n_features_in_","510","","511","    X, y = make_dataset(random_state=0, n_samples=100)","512","    stacker = Stacking(estimators=[('lr', MyEstimator())])","513","","514","    msg = f\"{Stacking.__name__} object has no attribute n_features_in_\"","515","    with pytest.raises(AttributeError, match=msg):","516","        stacker.n_features_in_","517","","518","    # Does not raise","519","    stacker.fit(X, y)","520","","521","    msg = \"'MyEstimator' object has no attribute 'n_features_in_'\"","522","    with pytest.raises(AttributeError, match=msg):","523","        stacker.n_features_in_"],"delete":[]}]}},"0cfe98b9f81463143675793e5b4b11268b2cf857":{"changes":{"sklearn\/ensemble\/_hist_gradient_boosting\/gradient_boosting.py":"MODIFY"},"diff":{"sklearn\/ensemble\/_hist_gradient_boosting\/gradient_boosting.py":[{"add":["998","class HistGradientBoostingClassifier(ClassifierMixin,","999","                                     BaseHistGradientBoosting):"],"delete":["998","class HistGradientBoostingClassifier(BaseHistGradientBoosting,","999","                                     ClassifierMixin):"]}]}},"8ab0064579a20b175400b05a33ededaa4f57d062":{"changes":{"sklearn\/neighbors\/_quad_tree.pyx":"MODIFY","sklearn\/tree\/_tree.pyx":"MODIFY"},"diff":{"sklearn\/neighbors\/_quad_tree.pyx":[{"add":["30","    int PyArray_SetBaseObject(np.ndarray arr, PyObject* obj)","576","        if PyArray_SetBaseObject(arr, <PyObject*> self) < 0:","577","            raise ValueError(\"Can't intialize array!\")"],"delete":["575","        arr.base = <PyObject*> self"]}],"sklearn\/tree\/_tree.pyx":[{"add":["46","    int PyArray_SetBaseObject(np.ndarray arr, PyObject* obj)","1100","        if PyArray_SetBaseObject(arr, <PyObject*> self) < 0:","1101","            raise ValueError(\"Can't initialize array.\")","1122","        if PyArray_SetBaseObject(arr, <PyObject*> self) < 0:","1123","            raise ValueError(\"Can't initialize array.\")"],"delete":["1099","        arr.base = <PyObject*> self","1120","        arr.base = <PyObject*> self"]}]}},"cf38502ec47ec1ff4609198573e746a6bfc25725":{"changes":{"sklearn\/inspection\/_plot\/partial_dependence.py":"MODIFY","sklearn\/inspection\/_partial_dependence.py":"MODIFY"},"diff":{"sklearn\/inspection\/_plot\/partial_dependence.py":[{"add":["91","    target : int, default=None","100","    response_method : {'auto', 'predict_proba', 'decision_function'}, \\","101","        default='auto'","110","    n_cols : int, default=3","114","    grid_resolution : int, default=100","118","    percentiles : tuple of float, default=(0.05, 0.95)","122","    method :{'auto', 'recursion', 'brute'}, default='auto'","146","    n_jobs : int, default=None","152","    verbose : int, default=0"],"delete":["91","    target : int, optional (default=None)","100","    response_method : 'auto', 'predict_proba' or 'decision_function', \\","101","            optional (default='auto')","110","    n_cols : int, optional (default=3)","114","    grid_resolution : int, optional (default=100)","118","    percentiles : tuple of float, optional (default=(0.05, 0.95))","122","    method : str, optional (default='auto')","146","    n_jobs : int, optional (default=None)","152","    verbose : int, optional (default=0)"]}],"sklearn\/inspection\/_partial_dependence.py":[{"add":["230","    response_method : {'auto', 'predict_proba', 'decision_function'}, \\","231","         default='auto'","240","    percentiles : tuple of float, default=(0.05, 0.95)","244","    grid_resolution : int, default=100","248","    method : {'auto', 'recursion', 'brute'}, default='auto'"],"delete":["230","    response_method : 'auto', 'predict_proba' or 'decision_function', \\","231","            optional (default='auto')","240","    percentiles : tuple of float, optional (default=(0.05, 0.95))","244","    grid_resolution : int, optional (default=100)","248","    method : str, optional (default='auto')"]}]}},"b229a663066f581706d47cfec85d4ba6358d99b4":{"changes":{"sklearn\/metrics\/_classification.py":"MODIFY","sklearn\/metrics\/tests\/test_classification.py":"MODIFY","doc\/whats_new\/v0.24.rst":"MODIFY"},"diff":{"sklearn\/metrics\/_classification.py":[{"add":["2348","    y_true_unique = np.unique(labels if labels is not None else y_true)"],"delete":["2348","    y_true_unique = np.unique(y_true)"]}],"sklearn\/metrics\/tests\/test_classification.py":[{"add":["2121","            dummy_hinge_loss)","2158","            dummy_hinge_loss)","2159","","2160","","2161","def test_hinge_loss_multiclass_missing_labels_only_two_unq_in_y_true():","2162","    # non-regression test for:","2163","    # https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/17630","2164","    # check that we can compute the hinge loss when providing an array","2165","    # with labels allowing to not have all labels in y_true","2166","    pred_decision = np.array([","2167","        [+0.36, -0.17, -0.58],","2168","        [-0.15, -0.58, -0.48],","2169","        [-1.45, -0.58, -0.38],","2170","        [-0.55, -0.78, -0.42],","2171","        [-1.45, -0.58, -0.38]","2172","    ])","2173","    y_true = np.array([0, 2, 2, 0, 2])","2174","    labels = np.array([0, 1, 2])","2175","    dummy_losses = np.array([","2176","        1 - pred_decision[0][0] + pred_decision[0][1],","2177","        1 - pred_decision[1][2] + pred_decision[1][0],","2178","        1 - pred_decision[2][2] + pred_decision[2][1],","2179","        1 - pred_decision[3][0] + pred_decision[3][2],","2180","        1 - pred_decision[4][2] + pred_decision[4][1]","2181","    ])","2182","    np.clip(dummy_losses, 0, None, out=dummy_losses)","2183","    dummy_hinge_loss = np.mean(dummy_losses)","2184","    assert_almost_equal(","2185","        hinge_loss(y_true, pred_decision, labels=labels),","2186","        dummy_hinge_loss","2187","    )"],"delete":["2121","                 dummy_hinge_loss)","2158","                 dummy_hinge_loss)"]}],"doc\/whats_new\/v0.24.rst":[{"add":["328","- |Fix| bug in :func:`metrics.hinge_loss` where error occurs when","329","  ``y_true`` is missing some labels that are provided explictly in the","330","  ``labels`` parameter.","331","  :pr:`17935` by :user:`Cary Goltermann <Ultramann>`.","332",""],"delete":[]}]}},"7536b25bd3793ec54a65114a839995cd352a1f66":{"changes":{"sklearn\/metrics\/_plot\/base.py":"MODIFY","sklearn\/metrics\/_plot\/roc_curve.py":"MODIFY","sklearn\/metrics\/_plot\/precision_recall_curve.py":"MODIFY"},"diff":{"sklearn\/metrics\/_plot\/base.py":[{"add":["0","def _check_classifier_response_method(estimator, response_method):"],"delete":["0","def _check_classifer_response_method(estimator, response_method):"]}],"sklearn\/metrics\/_plot\/roc_curve.py":[{"add":["3","from .base import _check_classifier_response_method","183","    prediction_method = _check_classifier_response_method(estimator,"],"delete":["3","from .base import _check_classifer_response_method","183","    prediction_method = _check_classifer_response_method(estimator,"]}],"sklearn\/metrics\/_plot\/precision_recall_curve.py":[{"add":["0","from .base import _check_classifier_response_method","176","    prediction_method = _check_classifier_response_method(estimator,"],"delete":["0","from .base import _check_classifer_response_method","176","    prediction_method = _check_classifer_response_method(estimator,"]}]}},"ad04857090542dfd1b6cc0cf3d5eb4cc2c5f1848":{"changes":{"sklearn\/neighbors\/_quad_tree.pxd":"MODIFY"},"diff":{"sklearn\/neighbors\/_quad_tree.pxd":[{"add":["84","                        float squared_theta=*, SIZE_t cell_id=*, long idx=*"],"delete":["84","                        float squared_theta=*, int cell_id=*, long idx=*"]}]}},"0111511ea38a97503091ce34a3bb3621eedb8fcf":{"changes":{"sklearn\/utils\/validation.py":"MODIFY","sklearn\/utils\/tests\/test_validation.py":"MODIFY","doc\/whats_new\/v0.24.rst":"MODIFY"},"diff":{"sklearn\/utils\/validation.py":[{"add":["570","        if array.dtype == np.dtype('object'):","571","            unique_dtypes = set(","572","                [dt.subtype.name for dt in array_orig.dtypes]","573","            )","574","            if len(unique_dtypes) > 1:","575","                raise ValueError(","576","                    \"Pandas DataFrame with mixed sparse extension arrays \"","577","                    \"generated a sparse matrix with object dtype which \"","578","                    \"can not be converted to a scipy sparse matrix.\"","579","                    \"Sparse extension arrays should all have the same \"","580","                    \"numeric type.\")"],"delete":[]}],"sklearn\/utils\/tests\/test_validation.py":[{"add":["47","from sklearn.utils.fixes import parse_version","1216","","1217","","1218","@pytest.mark.parametrize(","1219","    \"ntype1, ntype2\",","1220","    [","1221","        (\"longdouble\", \"float16\"),","1222","        (\"float16\", \"float32\"),","1223","        (\"float32\", \"double\"),","1224","        (\"int16\", \"int32\"),","1225","        (\"int32\", \"long\"),","1226","        (\"byte\", \"uint16\"),","1227","        (\"ushort\", \"uint32\"),","1228","        (\"uint32\", \"uint64\"),","1229","        (\"uint8\", \"int8\"),","1230","    ]","1231",")","1232","def test_check_pandas_sparse_invalid(ntype1, ntype2):","1233","    \"\"\"check that we raise an error with dataframe having","1234","    sparse extension arrays with unsupported mixed dtype","1235","    and pandas version below 1.1. pandas versions 1.1 and","1236","    above fixed this issue so no error will be raised.\"\"\"","1237","    pd = pytest.importorskip(\"pandas\", minversion=\"0.25.0\")","1238","    df = pd.DataFrame({'col1': pd.arrays.SparseArray([0, 1, 0],","1239","                                                     dtype=ntype1),","1240","                       'col2': pd.arrays.SparseArray([1, 0, 1],","1241","                                                     dtype=ntype2)})","1242","","1243","    if parse_version(pd.__version__) < parse_version('1.1'):","1244","        err_msg = \"Pandas DataFrame with mixed sparse extension arrays\"","1245","        with pytest.raises(ValueError, match=err_msg):","1246","            check_array(df, accept_sparse=['csr', 'csc'])","1247","    else:","1248","        # pandas fixed this issue at 1.1 so from here on,","1249","        # no error will be raised.","1250","        check_array(df, accept_sparse=['csr', 'csc'])","1251","","1252","","1253","@pytest.mark.parametrize(","1254","    \"ntype1, ntype2, expected_dtype\",","1255","    [","1256","        (\"longfloat\", \"longdouble\", \"float128\"),","1257","        (\"float16\", \"half\", \"float16\"),","1258","        (\"single\", \"float32\", \"float32\"),","1259","        (\"double\", \"float64\", \"float64\"),","1260","        (\"int8\", \"byte\", \"int8\"),","1261","        (\"short\", \"int16\", \"int16\"),","1262","        (\"intc\", \"int32\", \"int32\"),","1263","        (\"int0\", \"long\", \"int64\"),","1264","        (\"int\", \"long\", \"int64\"),","1265","        (\"int64\", \"longlong\", \"int64\"),","1266","        (\"int_\", \"intp\", \"int64\"),","1267","        (\"ubyte\", \"uint8\", \"uint8\"),","1268","        (\"uint16\", \"ushort\", \"uint16\"),","1269","        (\"uintc\", \"uint32\", \"uint32\"),","1270","        (\"uint\", \"uint64\", \"uint64\"),","1271","        (\"uintp\", \"ulonglong\", \"uint64\"),","1272","    ]","1273",")","1274","def test_check_pandas_sparse_valid(ntype1, ntype2, expected_dtype):","1275","    # check that we support the conversion of sparse dataframe with mixed","1276","    # type which can be converted safely.","1277","    pd = pytest.importorskip(\"pandas\", minversion=\"0.25.0\")","1278","    df = pd.DataFrame({'col1': pd.arrays.SparseArray([0, 1, 0],","1279","                                                     dtype=ntype1),","1280","                       'col2': pd.arrays.SparseArray([1, 0, 1],","1281","                                                     dtype=ntype2)})","1282","    arr = check_array(df, accept_sparse=['csr', 'csc'])","1283","    assert arr.dtype.name == expected_dtype"],"delete":[]}],"doc\/whats_new\/v0.24.rst":[{"add":["475",":mod:`sklearn.utils`","476","....................","477","","478","- |Fix| Raise ValueError with clear error message in :func:`check_array`","479","  for sparse DataFrames with mixed types.","480","  :pr:`17992` by :user:`Thomas J. Fan <thomasjpfan>` and","481","  :user:`Alex Shacked <alexshacked>`.","482",""],"delete":[]}]}},"acf195cc74b7082b35b5fb200e7c32eef1988362":{"changes":{"sklearn\/utils\/validation.py":"MODIFY","sklearn\/decomposition\/tests\/test_kernel_pca.py":"MODIFY","doc\/whats_new\/v0.24.rst":"MODIFY"},"diff":{"sklearn\/utils\/validation.py":[{"add":["1185","    small_pos_ratio = 1e-12 if is_double_precision else 1e-7"],"delete":["1185","    small_pos_ratio = 1e-12"]}],"sklearn\/decomposition\/tests\/test_kernel_pca.py":[{"add":["12","from sklearn.preprocessing import StandardScaler","298","","299","","300","def test_32_64_decomposition_shape():","301","    \"\"\" Test that the decomposition is similar for 32 and 64 bits data \"\"\"","302","    # see https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/18146","303","    X, y = make_blobs(","304","        n_samples=30,","305","        centers=[[0, 0, 0], [1, 1, 1]],","306","        random_state=0,","307","        cluster_std=0.1","308","    )","309","    X = StandardScaler().fit_transform(X)","310","    X -= X.min()","311","","312","    # Compare the shapes (corresponds to the number of non-zero eigenvalues)","313","    kpca = KernelPCA()","314","    assert (kpca.fit_transform(X).shape ==","315","            kpca.fit_transform(X.astype(np.float32)).shape)"],"delete":[]}],"doc\/whats_new\/v0.24.rst":[{"add":["24","- |Fix| :class:`decomposition.KernelPCA` behaviour is now more consistent","25","  between 32-bits and 64-bits data when the kernel has small positive","26","  eigenvalues.","119","- |Fix| :class:`decomposition.KernelPCA` behaviour is now more consistent","120","  between 32-bits and 64-bits data input when the kernel has small positive","121","  eigenvalues. Small positive eigenvalues were not correctly discarded for","122","  32-bits data.","123","  :pr:`18149` by :user:`Sylvain Mari¨¦ <smarie>`.","124",""],"delete":["24","- item","25","- item"]}]}},"a54cb47c74e54345001fbe2f9b5ed34e0fe9f9f8":{"changes":{"sklearn\/metrics\/_ranking.py":"MODIFY","sklearn\/metrics\/tests\/test_common.py":"MODIFY","sklearn\/metrics\/_classification.py":"MODIFY","sklearn\/metrics\/tests\/test_classification.py":"MODIFY","sklearn\/metrics\/tests\/test_ranking.py":"MODIFY"},"diff":{"sklearn\/metrics\/_ranking.py":[{"add":["210","        # Convert to Python primitive type to avoid NumPy type \/ Python str","211","        # comparison. See https:\/\/github.com\/numpy\/numpy\/issues\/6784","212","        present_labels = np.unique(y_true).tolist()","214","            raise ValueError(","215","                f\"pos_label={pos_label} is not a valid label. It should be \"","216","                f\"one of {present_labels}\"","217","            )"],"delete":["210","        present_labels = np.unique(y_true)","212","            raise ValueError(\"pos_label=%r is invalid. Set it to a label in \"","213","                             \"y_true.\" % pos_label)"]}],"sklearn\/metrics\/tests\/test_common.py":[{"add":["2","from inspect import signature","1415","","1416","","1417","@pytest.mark.parametrize(\"metric_name\", CLASSIFICATION_METRICS)","1418","def test_metrics_consistent_type_error(metric_name):","1419","    # check that an understable message is raised when the type between y_true","1420","    # and y_pred mismatch","1421","    rng = np.random.RandomState(42)","1422","    y1 = np.array([\"spam\"] * 3 + [\"eggs\"] * 2, dtype=object)","1423","    y2 = rng.randint(0, 2, size=y1.size)","1424","","1425","    err_msg = \"Labels in y_true and y_pred should be of the same type.\"","1426","    with pytest.raises(TypeError, match=err_msg):","1427","        CLASSIFICATION_METRICS[metric_name](y1, y2)","1428","","1429","","1430","@pytest.mark.parametrize(","1431","    \"metric, y_pred_threshold\",","1432","    [","1433","        (average_precision_score, True),","1434","        # FIXME: `brier_score_loss` does not follow this convention.","1435","        # See discussion in:","1436","        # https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/18307","1437","        pytest.param(","1438","            brier_score_loss, True, marks=pytest.mark.xfail(reason=\"#18307\")","1439","        ),","1440","        (f1_score, False),","1441","        (partial(fbeta_score, beta=1), False),","1442","        (jaccard_score, False),","1443","        (precision_recall_curve, True),","1444","        (precision_score, False),","1445","        (recall_score, False),","1446","        (roc_curve, True),","1447","    ],","1448",")","1449","@pytest.mark.parametrize(\"dtype_y_str\", [str, object])","1450","def test_metrics_pos_label_error_str(metric, y_pred_threshold, dtype_y_str):","1451","    # check that the error message if `pos_label` is not specified and the","1452","    # targets is made of strings.","1453","    rng = np.random.RandomState(42)","1454","    y1 = np.array([\"spam\"] * 3 + [\"eggs\"] * 2, dtype=dtype_y_str)","1455","    y2 = rng.randint(0, 2, size=y1.size)","1456","","1457","    if not y_pred_threshold:","1458","        y2 = np.array([\"spam\", \"eggs\"], dtype=dtype_y_str)[y2]","1459","","1460","    err_msg_pos_label_None = (","1461","        \"y_true takes value in {'eggs', 'spam'} and pos_label is not \"","1462","        \"specified: either make y_true take value in {0, 1} or {-1, 1} or \"","1463","        \"pass pos_label explicit\"","1464","    )","1465","    err_msg_pos_label_1 = (","1466","        r\"pos_label=1 is not a valid label. It should be one of \"","1467","        r\"\\['eggs', 'spam'\\]\"","1468","    )","1469","","1470","    pos_label_default = signature(metric).parameters[\"pos_label\"].default","1471","","1472","    err_msg = (","1473","        err_msg_pos_label_1","1474","        if pos_label_default == 1","1475","        else err_msg_pos_label_None","1476","    )","1477","    with pytest.raises(ValueError, match=err_msg):","1478","        metric(y1, y2)"],"delete":[]}],"sklearn\/metrics\/_classification.py":[{"add":["103","            try:","104","                unique_values = np.union1d(y_true, y_pred)","105","            except TypeError as e:","106","                # We expect y_true and y_pred to be of the same data type.","107","                # If `y_true` was provided to the classifier as strings,","108","                # `y_pred` given by the classifier will also be encoded with","109","                # strings. So we raise a meaningful error","110","                raise TypeError(","111","                    f\"Labels in y_true and y_pred should be of the same type. \"","112","                    f\"Got y_true={np.unique(y_true)} and \"","113","                    f\"y_pred={np.unique(y_pred)}. Make sure that the \"","114","                    f\"predictions provided by the classifier coincides with \"","115","                    f\"the true labels.\"","116","                ) from e","1272","    # Convert to Python primitive type to avoid NumPy type \/ Python str","1273","    # comparison. See https:\/\/github.com\/numpy\/numpy\/issues\/6784","1274","    present_labels = unique_labels(y_true, y_pred).tolist()","1279","                    raise ValueError(","1280","                        f\"pos_label={pos_label} is not a valid label. It \"","1281","                        f\"should be one of {present_labels}\"","1282","                    )"],"delete":["103","            unique_values = np.union1d(y_true, y_pred)","1259","    present_labels = unique_labels(y_true, y_pred)","1264","                    raise ValueError(\"pos_label=%r is not a valid label: \"","1265","                                     \"%r\" % (pos_label, present_labels))"]}],"sklearn\/metrics\/tests\/test_classification.py":[{"add":["1249","    err_msg = r\"pos_label=2 is not a valid label. It should be one of \\[0, 1\\]\""],"delete":["1249","    err_msg = r\"pos_label=2 is not a valid label: array\\(\\[0, 1\\]\\)\""]}],"sklearn\/metrics\/tests\/test_ranking.py":[{"add":["890","    err_msg = r\"pos_label=2 is not a valid label. It should be one of \\[0, 1\\]\"","891","    with pytest.raises(ValueError, match=err_msg):","897","    err_msg = (","898","        \"Parameter pos_label is fixed to 1 for multilabel-indicator y_true. \"","899","        \"Do not set pos_label or set pos_label to 1.\"","900","    )","901","    with pytest.raises(ValueError, match=err_msg):"],"delete":["890","    error_message = (\"pos_label=2 is invalid. Set it to a label in y_true.\")","891","    with pytest.raises(ValueError, match=error_message):","897","    error_message = (\"Parameter pos_label is fixed to 1 for multilabel\"","898","                     \"-indicator y_true. Do not set pos_label or set \"","899","                     \"pos_label to 1.\")","900","    with pytest.raises(ValueError, match=error_message):"]}]}},"fc6ee00b0accceeec48cc5b606e713514b481617":{"changes":{"sklearn\/ensemble\/tests\/test_forest.py":"MODIFY"},"diff":{"sklearn\/ensemble\/tests\/test_forest.py":[{"add":["165","    reg = ForestRegressor(n_estimators=5, criterion=criterion,","167","    reg.fit(boston.data, boston.target)","168","    score = reg.score(boston.data, boston.target)","172","    reg = ForestRegressor(n_estimators=5, criterion=criterion,","174","    reg.fit(boston.data, boston.target)","175","    score = reg.score(boston.data, boston.target)","684","    reg = ExtraTreesRegressor(n_estimators=n_trees, random_state=42).fit(X, y)","687","    for tree in reg.estimators_:","715","    reg = ExtraTreesRegressor(max_features=1, random_state=1).fit(X, y)","718","    for tree in reg.estimators_:","1067","    est_ws = None","1069","        if est_ws is None:","1070","            est_ws = ForestEstimator(n_estimators=n_estimators,","1074","            est_ws.set_params(n_estimators=n_estimators)","1075","        est_ws.fit(X, y)","1076","        assert len(est_ws) == n_estimators","1078","    est_no_ws = ForestEstimator(n_estimators=10, random_state=random_state,","1080","    est_no_ws.fit(X, y)","1082","    assert (set([tree.random_state for tree in est_ws]) ==","1083","            set([tree.random_state for tree in est_no_ws]))","1085","    assert_array_equal(est_ws.apply(X), est_no_ws.apply(X),","1098","    est = ForestEstimator(n_estimators=5, max_depth=1, warm_start=False,","1100","    est.fit(X, y)","1102","    est_2 = ForestEstimator(n_estimators=5, max_depth=1, warm_start=True,","1104","    est_2.fit(X, y)  # inits state","1105","    est_2.set_params(warm_start=False, random_state=1)","1106","    est_2.fit(X, y)  # clears old state and equals est","1108","    assert_array_almost_equal(est_2.apply(X), est.apply(X))","1120","    est = ForestEstimator(n_estimators=5, max_depth=1, warm_start=True)","1121","    est.fit(X, y)","1122","    est.set_params(n_estimators=4)","1123","    assert_raises(ValueError, est.fit, X, y)","1136","    est = ForestEstimator(n_estimators=5, max_depth=3, warm_start=True,","1138","    est.fit(X, y)","1140","    est_2 = ForestEstimator(n_estimators=5, max_depth=3, warm_start=True,","1142","    est_2.fit(X, y)","1143","    # Now est_2 equals est.","1145","    est_2.set_params(random_state=2)","1146","    assert_warns(UserWarning, est_2.fit, X, y)","1149","    assert_array_equal(est.apply(X), est_2.apply(X))","1162","    est = ForestEstimator(n_estimators=15, max_depth=3, warm_start=False,","1164","    est.fit(X, y)","1166","    est_2 = ForestEstimator(n_estimators=5, max_depth=3, warm_start=False,","1168","    est_2.fit(X, y)","1170","    est_2.set_params(warm_start=True, oob_score=True, n_estimators=15)","1171","    est_2.fit(X, y)","1173","    assert hasattr(est_2, 'oob_score_')","1174","    assert est.oob_score_ == est_2.oob_score_","1178","    est_3 = ForestEstimator(n_estimators=15, max_depth=3, warm_start=True,","1180","    est_3.fit(X, y)","1181","    assert not hasattr(est_3, 'oob_score_')","1183","    est_3.set_params(oob_score=True)","1184","    ignore_warnings(est_3.fit)(X, y)","1186","    assert est.oob_score_ == est_3.oob_score_"],"delete":["165","    clf = ForestRegressor(n_estimators=5, criterion=criterion,","167","    clf.fit(boston.data, boston.target)","168","    score = clf.score(boston.data, boston.target)","172","    clf = ForestRegressor(n_estimators=5, criterion=criterion,","174","    clf.fit(boston.data, boston.target)","175","    score = clf.score(boston.data, boston.target)","684","    clf = ExtraTreesRegressor(n_estimators=n_trees, random_state=42).fit(X, y)","687","    for tree in clf.estimators_:","715","    clf = ExtraTreesRegressor(max_features=1, random_state=1).fit(X, y)","718","    for tree in clf.estimators_:","1067","    clf_ws = None","1069","        if clf_ws is None:","1070","            clf_ws = ForestEstimator(n_estimators=n_estimators,","1074","            clf_ws.set_params(n_estimators=n_estimators)","1075","        clf_ws.fit(X, y)","1076","        assert len(clf_ws) == n_estimators","1078","    clf_no_ws = ForestEstimator(n_estimators=10, random_state=random_state,","1080","    clf_no_ws.fit(X, y)","1082","    assert (set([tree.random_state for tree in clf_ws]) ==","1083","                 set([tree.random_state for tree in clf_no_ws]))","1085","    assert_array_equal(clf_ws.apply(X), clf_no_ws.apply(X),","1098","    clf = ForestEstimator(n_estimators=5, max_depth=1, warm_start=False,","1100","    clf.fit(X, y)","1102","    clf_2 = ForestEstimator(n_estimators=5, max_depth=1, warm_start=True,","1104","    clf_2.fit(X, y)  # inits state","1105","    clf_2.set_params(warm_start=False, random_state=1)","1106","    clf_2.fit(X, y)  # clears old state and equals clf","1108","    assert_array_almost_equal(clf_2.apply(X), clf.apply(X))","1120","    clf = ForestEstimator(n_estimators=5, max_depth=1, warm_start=True)","1121","    clf.fit(X, y)","1122","    clf.set_params(n_estimators=4)","1123","    assert_raises(ValueError, clf.fit, X, y)","1136","    clf = ForestEstimator(n_estimators=5, max_depth=3, warm_start=True,","1138","    clf.fit(X, y)","1140","    clf_2 = ForestEstimator(n_estimators=5, max_depth=3, warm_start=True,","1142","    clf_2.fit(X, y)","1143","    # Now clf_2 equals clf.","1145","    clf_2.set_params(random_state=2)","1146","    assert_warns(UserWarning, clf_2.fit, X, y)","1149","    assert_array_equal(clf.apply(X), clf_2.apply(X))","1162","    clf = ForestEstimator(n_estimators=15, max_depth=3, warm_start=False,","1164","    clf.fit(X, y)","1166","    clf_2 = ForestEstimator(n_estimators=5, max_depth=3, warm_start=False,","1168","    clf_2.fit(X, y)","1170","    clf_2.set_params(warm_start=True, oob_score=True, n_estimators=15)","1171","    clf_2.fit(X, y)","1173","    assert hasattr(clf_2, 'oob_score_')","1174","    assert clf.oob_score_ == clf_2.oob_score_","1178","    clf_3 = ForestEstimator(n_estimators=15, max_depth=3, warm_start=True,","1180","    clf_3.fit(X, y)","1181","    assert not hasattr(clf_3, 'oob_score_')","1183","    clf_3.set_params(oob_score=True)","1184","    ignore_warnings(clf_3.fit)(X, y)","1186","    assert clf.oob_score_ == clf_3.oob_score_"]}]}},"5d04910476d3b50a06dfb15ec9faacd410a26541":{"changes":{"sklearn\/model_selection\/tests\/test_search.py":"MODIFY","doc\/whats_new\/v0.24.rst":"MODIFY","sklearn\/model_selection\/_search.py":"MODIFY"},"diff":{"sklearn\/model_selection\/tests\/test_search.py":[{"add":["58","from sklearn.neighbors import LocalOutlierFactor","79","","164","                set((\"bar\", x, \"foo\", y)","165","                    for x, y in product(params2[\"bar\"], params2[\"foo\"])))","694","    def check_X(x): return x.shape[1:] == (5, 3, 2)","695","    def check_y(x): return x.shape[1:] == (7, 11)","1101","@pytest.mark.parametrize('search_cv', [","1102","    RandomizedSearchCV(estimator=DecisionTreeClassifier(),","1103","                       param_distributions={'max_depth': [5, 10]}),","1104","    GridSearchCV(estimator=DecisionTreeClassifier(),","1105","                 param_grid={'max_depth': [5, 10]})","1106","])","1107","def test_search_cv_score_samples_error(search_cv):","1108","    X, y = make_blobs(n_samples=100, n_features=4, random_state=42)","1109","    search_cv.fit(X, y)","1110","","1111","    # Make sure to error out when underlying estimator does not implement","1112","    # the method `score_samples`","1113","    err_msg = (\"'DecisionTreeClassifier' object has no attribute \"","1114","               \"'score_samples'\")","1115","","1116","    with pytest.raises(AttributeError, match=err_msg):","1117","        search_cv.score_samples(X)","1118","","1119","","1120","@pytest.mark.parametrize('search_cv', [","1121","    RandomizedSearchCV(estimator=LocalOutlierFactor(novelty=True),","1122","                       param_distributions={'n_neighbors': [5, 10]},","1123","                       scoring=\"precision\"),","1124","    GridSearchCV(estimator=LocalOutlierFactor(novelty=True),","1125","                 param_grid={'n_neighbors': [5, 10]},","1126","                 scoring=\"precision\")","1127","])","1128","def test_search_cv_score_samples_method(search_cv):","1129","    # Set parameters","1130","    rng = np.random.RandomState(42)","1131","    n_samples = 300","1132","    outliers_fraction = 0.15","1133","    n_outliers = int(outliers_fraction * n_samples)","1134","    n_inliers = n_samples - n_outliers","1135","","1136","    # Create dataset","1137","    X = make_blobs(n_samples=n_inliers, n_features=2, centers=[[0, 0], [0, 0]],","1138","                   cluster_std=0.5, random_state=0)[0]","1139","    # Add some noisy points","1140","    X = np.concatenate([X, rng.uniform(low=-6, high=6,","1141","                                       size=(n_outliers, 2))], axis=0)","1142","","1143","    # Define labels to be able to score the estimator with `search_cv`","1144","    y_true = np.array([1] * n_samples)","1145","    y_true[-n_outliers:] = -1","1146","","1147","    # Fit on data","1148","    search_cv.fit(X, y_true)","1149","","1150","    # Verify that the stand alone estimator yields the same results","1151","    # as the ones obtained with *SearchCV","1152","    assert_allclose(search_cv.score_samples(X),","1153","                    search_cv.best_estimator_.score_samples(X))","1154","","1155",""],"delete":["162","                     set((\"bar\", x, \"foo\", y)","163","                         for x, y in product(params2[\"bar\"], params2[\"foo\"])))","692","    check_X = lambda x: x.shape[1:] == (5, 3, 2)","693","    check_y = lambda x: x.shape[1:] == (7, 11)"]}],"doc\/whats_new\/v0.24.rst":[{"add":["93","- |Feature| :class:`model_selection.RandomizedSearchCV` and","94","  :class:`model_selection.GridSearchCV` now have the method, ``score_samples``","95","  :pr:`17478` by :user:`Teon Brooks <teonbrooks>` and","96","  :user:`Mohamed Maskani <maskani-moh>`.","97","","129","  ``predict`` and related methods of :class:`svm.SVC`, :class:`svm.NuSVC`,"],"delete":["124","  ``predict`` and related methods of :class:`svm.SVC`, :class:`svm.NuSVC`, "]}],"sklearn\/model_selection\/_search.py":[{"add":["459","    @if_delegate_has_method(delegate=('best_estimator_', 'estimator'))","460","    def score_samples(self, X):","461","        \"\"\"Call score_samples on the estimator with the best found parameters.","462","","463","        Only available if ``refit=True`` and the underlying estimator supports","464","        ``score_samples``.","465","","466","        Parameters","467","        ----------","468","        X : iterable","469","            Data to predict on. Must fulfill input requirements","470","            of the underlying estimator.","471","","472","        Returns","473","        -------","474","        y_score : ndarray, shape (n_samples,)","475","        \"\"\"","476","        self._check_is_fitted('score_samples')","477","        return self.best_estimator_.score_samples(X)","478","","883","    It also implements \"score_samples\", \"predict\", \"predict_proba\",","884","    \"decision_function\", \"transform\" and \"inverse_transform\" if they are","885","    implemented in the estimator used.","1196","    It also implements \"score_samples\", \"predict\", \"predict_proba\",","1197","    \"decision_function\", \"transform\" and \"inverse_transform\" if they are","1198","    implemented in the estimator used."],"delete":["863","    It also implements \"predict\", \"predict_proba\", \"decision_function\",","864","    \"transform\" and \"inverse_transform\" if they are implemented in the","865","    estimator used.","1176","    It also implements \"predict\", \"predict_proba\", \"decision_function\",","1177","    \"transform\" and \"inverse_transform\" if they are implemented in the","1178","    estimator used."]}]}},"fc06baef499b8e0a6d677d4a19fa983f173ad06c":{"changes":{"sklearn\/cluster\/_kmeans.py":"MODIFY","sklearn\/cluster\/tests\/test_k_means.py":"MODIFY","doc\/whats_new\/v0.24.rst":"MODIFY"},"diff":{"sklearn\/cluster\/_kmeans.py":[{"add":["358","    labels_old = labels.copy()","378","    strict_convergence = False","379","","398","        if np.array_equal(labels, labels_old):","399","            # First check the labels for strict convergence.","401","                print(f\"Converged at iteration {i}: strict convergence.\")","402","            strict_convergence = True","404","        else:","405","            # No strict convergence, check for tol based convergence.","406","            center_shift_tot = (center_shift**2).sum()","407","            if center_shift_tot <= tol:","408","                if verbose:","409","                    print(f\"Converged at iteration {i}: center shift \"","410","                          f\"{center_shift_tot} within tolerance {tol}.\")","411","                break","413","        labels_old[:] = labels","414","","415","    if not strict_convergence:","486","    labels_old = labels.copy()","497","    strict_convergence = False","498","","512","            if np.array_equal(labels, labels_old):","513","                # First check the labels for strict convergence.","515","                    print(f\"Converged at iteration {i}: strict convergence.\")","516","                strict_convergence = True","518","            else:","519","                # No strict convergence, check for tol based convergence.","520","                center_shift_tot = (center_shift**2).sum()","521","                if center_shift_tot <= tol:","522","                    if verbose:","523","                        print(f\"Converged at iteration {i}: center shift \"","524","                              f\"{center_shift_tot} within tolerance {tol}.\")","525","                    break","527","            labels_old[:] = labels","528","","529","        if not strict_convergence:"],"delete":["228","        It's not advised to set `tol=0` since convergence might never be","229","        declared due to rounding errors. Use a very small number instead.","397","        center_shift_tot = (center_shift**2).sum()","398","        if center_shift_tot <= tol:","400","                print(f\"Converged at iteration {i}: center shift \"","401","                      f\"{center_shift_tot} within tolerance {tol}.\")","404","    if center_shift_tot > 0:","498","            center_shift_tot = (center_shift**2).sum()","499","            if center_shift_tot <= tol:","501","                    print(\"Converged at iteration {0}: \"","502","                          \"center shift {1} within tolerance {2}\"","503","                          .format(i, center_shift_tot, tol))","506","        if center_shift_tot > 0:","619","        It's not advised to set `tol=0` since convergence might never be","620","        declared due to rounding errors. Use a very small number instead."]}],"sklearn\/cluster\/tests\/test_k_means.py":[{"add":["1","import re","139","@pytest.mark.parametrize(\"tol\", [1e-2, 1e-8, 1e-100, 0])","168","    max_iter = 300","170","    km = KMeans(algorithm=algorithm, n_clusters=5, random_state=0,","171","                n_init=1, tol=0, max_iter=max_iter).fit(X)","173","    assert km.n_iter_ < max_iter","338","def test_minibatch_kmeans_verbose():","339","    # Check verbose mode of MiniBatchKMeans for better coverage.","340","    km = MiniBatchKMeans(n_clusters=n_clusters, random_state=42, verbose=1)","349","@pytest.mark.parametrize(\"algorithm\", [\"full\", \"elkan\"])","350","@pytest.mark.parametrize(\"tol\", [1e-2, 0])","351","def test_kmeans_verbose(algorithm, tol, capsys):","352","    # Check verbose mode of KMeans for better coverage.","353","    X = np.random.RandomState(0).normal(size=(5000, 10))","354","","355","    KMeans(algorithm=algorithm, n_clusters=n_clusters, random_state=42,","356","           init=\"random\", n_init=1, tol=tol, verbose=1).fit(X)","357","","358","    captured = capsys.readouterr()","359","","360","    assert re.search(r\"Initialization complete\", captured.out)","361","    assert re.search(r\"Iteration [0-9]+, inertia\", captured.out)","362","","363","    if tol == 0:","364","        assert re.search(r\"strict convergence\", captured.out)","365","    else:","366","        assert re.search(r\"center shift .* within tolerance\", captured.out)","367","","368",""],"delete":["138","@pytest.mark.parametrize(\"tol\", [1e-2, 1e-4, 1e-8])","165","    # We can only ensure that if the number of threads is not to large,","166","    # otherwise the roundings errors coming from the unpredictability of","167","    # the order in which chunks are processed make the convergence criterion","168","    # to never be exactly 0.","172","    with threadpool_limits(limits=1, user_api=\"openmp\"):","173","        km = KMeans(algorithm=algorithm, n_clusters=5, random_state=0,","174","                    n_init=1, tol=0, max_iter=300).fit(X)","176","    assert km.n_iter_ < 300","341","@pytest.mark.parametrize(\"Estimator\", [KMeans, MiniBatchKMeans])","342","def test_verbose(Estimator):","343","    # Check verbose mode of KMeans and MiniBatchKMeans for better coverage.","344","    km = Estimator(n_clusters=n_clusters, random_state=42, verbose=1)"]}],"doc\/whats_new\/v0.24.rst":[{"add":["93","- |Fix| Fixed a bug in :class:`cluster.KMeans` where rounding errors could","94","  prevent convergence to be declared when `tol=0`. :pr:`17959` by","95","  :user:`J¨¦r¨¦mie du Boisberranger <jeremiedbb>`."],"delete":[]}]}},"1523f395e952dd79d2427d8230056ed0cf47f7a3":{"changes":{"sklearn\/cluster\/_k_means_elkan.pyx":"MODIFY"},"diff":{"sklearn\/cluster\/_k_means_elkan.pyx":[{"add":["324","        free(centers_new_chunk)","325","        free(weight_in_clusters_chunk)","326","","558","        free(centers_new_chunk)","559","        free(weight_in_clusters_chunk)","560",""],"delete":[]}]}},"06bb4864e6a49366dd5bc4e60a6b384601c375a6":{"changes":{"sklearn\/feature_extraction\/text.py":"MODIFY","doc\/whats_new\/v0.23.rst":"MODIFY","sklearn\/feature_extraction\/tests\/test_text.py":"MODIFY"},"diff":{"sklearn\/feature_extraction\/text.py":[{"add":["1208","            if max_features is not None:","1209","                X = self._sort_features(X, vocabulary)","1214","            if max_features is None:","1215","                X = self._sort_features(X, vocabulary)"],"delete":["1212","","1213","            X = self._sort_features(X, vocabulary)","1214",""]}],"doc\/whats_new\/v0.23.rst":[{"add":["19",":mod:`sklearn.feature_extraction`","20",".................................","21","","22","- |Fix| Fixes bug in :class:`feature_extraction.text.CountVectorizer` where","23","  sample order invariance was broken when `max_features` was set and features","24","  had the same count. :pr:`18016` by `Thomas Fan`_, `Roman Yurchak`_, and","25","  `Joel Nothman`_.","26",""],"delete":[]}],"sklearn\/feature_extraction\/tests\/test_text.py":[{"add":["1341","","1342","","1343","def test_tie_breaking_sample_order_invariance():","1344","    # Checks the sample order invariance when setting max_features","1345","    # non-regression test for #17939","1346","    vec = CountVectorizer(max_features=1)","1347","    vocab1 = vec.fit(['hello', 'world']).vocabulary_","1348","    vocab2 = vec.fit(['world', 'hello']).vocabulary_","1349","    assert vocab1 == vocab2"],"delete":[]}]}},"430c2080e81dbf9aacc99dfd83d958030a7c4d07":{"changes":{"sklearn\/svm\/tests\/test_bounds.py":"MODIFY","sklearn\/svm\/src\/newrand\/newrand.h":"MODIFY","sklearn\/svm\/_newrand.pyx":"ADD","sklearn\/svm\/setup.py":"MODIFY"},"diff":{"sklearn\/svm\/tests\/test_bounds.py":[{"add":["2","from scipy import stats","9","from sklearn.svm._newrand import set_seed_wrap, bounded_rand_int_wrap","78","","79","","80","_MAX_UNSIGNED_INT = 4294967295","81","","82","","83","@pytest.mark.parametrize('seed, val',","84","                         [(None, 81),","85","                          (0, 54),","86","                          (_MAX_UNSIGNED_INT, 9)])","87","def test_newrand_set_seed(seed, val):","88","    \"\"\"Test that `set_seed` produces deterministic results\"\"\"","89","    if seed is not None:","90","        set_seed_wrap(seed)","91","    x = bounded_rand_int_wrap(100)","92","    assert x == val, f'Expected {val} but got {x} instead'","93","","94","","95","@pytest.mark.parametrize('seed',","96","                         [-1, _MAX_UNSIGNED_INT + 1])","97","def test_newrand_set_seed_overflow(seed):","98","    \"\"\"Test that `set_seed_wrap` is defined for unsigned 32bits ints\"\"\"","99","    with pytest.raises(OverflowError):","100","        set_seed_wrap(seed)","101","","102","","103","@pytest.mark.parametrize('range_, n_pts',","104","                         [(_MAX_UNSIGNED_INT, 10000), (100, 25)])","105","def test_newrand_bounded_rand_int(range_, n_pts):","106","    \"\"\"Test that `bounded_rand_int` follows a uniform distribution\"\"\"","107","    n_iter = 100","108","    ks_pvals = []","109","    uniform_dist = stats.uniform(loc=0, scale=range_)","110","    # perform multiple samplings to make chance of outlier sampling negligible","111","    for _ in range(n_iter):","112","        # Deterministic random sampling","113","        sample = [bounded_rand_int_wrap(range_) for _ in range(n_pts)]","114","        res = stats.kstest(sample, uniform_dist.cdf)","115","        ks_pvals.append(res.pvalue)","116","    # Null hypothesis = samples come from an uniform distribution.","117","    # Under the null hypothesis, p-values should be uniformly distributed","118","    # and not concentrated on low values","119","    # (this may seem counter-intuitive but is backed by multiple refs)","120","    # So we can do two checks:","121","","122","    # (1) check uniformity of p-values","123","    uniform_p_vals_dist = stats.uniform(loc=0, scale=1)","124","    res_pvals = stats.kstest(ks_pvals, uniform_p_vals_dist.cdf)","125","    assert res_pvals.pvalue > 0.05, (","126","        \"Null hypothesis rejected: generated random numbers are not uniform.\"","127","        \" Details: the (meta) p-value of the test of uniform distribution\"","128","        f\" of p-values is {res_pvals.pvalue} which is not > 0.05\")","129","","130","    # (2) (safety belt) check that 90% of p-values are above 0.05","131","    min_10pct_pval = np.percentile(ks_pvals, q=10)","132","    # lower 10th quantile pvalue <= 0.05 means that the test rejects the","133","    # null hypothesis that the sample came from the uniform distribution","134","    assert min_10pct_pval > 0.05, (","135","        \"Null hypothesis rejected: generated random numbers are not uniform. \"","136","        f\"Details: lower 10th quantile p-value of {min_10pct_pval} not > 0.05.\"","137","        )","138","","139","","140","@pytest.mark.parametrize('range_',","141","                         [-1, _MAX_UNSIGNED_INT + 1])","142","def test_newrand_bounded_rand_int_limits(range_):","143","    \"\"\"Test that `bounded_rand_int_wrap` is defined for unsigned 32bits ints\"\"\"","144","    with pytest.raises(OverflowError):","145","        bounded_rand_int_wrap(range_)"],"delete":[]}],"sklearn\/svm\/src\/newrand\/newrand.h":[{"add":["12","#include <random>  \/\/ needed for cython to generate a .cpp file from newrand.h","28","inline uint32_t bounded_rand_int(uint32_t range) {","29","    \/\/ \"LibSVM \/ LibLinear Original way\" - make a 31bit positive","31","    \/\/ return abs( (int)mt_rand()) % range;"],"delete":["7","","20","#if INT_MAX == 0x7FFFFFFF","22","#elif INT_MAX == 0x7FFFFFFFFFFFFFFF","23","std::mt19937_64 mt_rand(std::mt19937::default_seed);","24","#else","25","info(\"Random number generator is not fixed for this system. Please report issue. INT_MAX=%d\\n\", INT_MAX);","26","exit(1);","27","#endif","35","inline int bounded_rand_int(int orig_range) {","36","    \/\/ \"LibSVM \/ LibLinear Original way\" - make a 31bit or 63bit positive","38","    \/\/ return abs( (int)mt_rand()) % orig_range;","42","    \/\/ TODO how could we make this casting safer, raising an error if lost information?","43","    uint32_t range = uint32_t(orig_range);"]}],"sklearn\/svm\/_newrand.pyx":[{"add":[],"delete":[]}],"sklearn\/svm\/setup.py":[{"add":["12","    # newrand wrappers","13","    config.add_extension('_newrand',","14","                         sources=['_newrand.pyx'],","15","                         include_dirs=[numpy.get_include(),","16","                                       join('src', 'newrand')],","17","                         depends=[join('src', 'newrand', 'newrand.h')],","18","                         language='c++',","19","                         )","20",""],"delete":[]}]}},"98f0b832f8888ab2dd3a143ddd09525dd11f0479":{"changes":{"sklearn\/decomposition\/_dict_learning.py":"MODIFY","sklearn\/decomposition\/tests\/test_dict_learning.py":"MODIFY","doc\/whats_new\/v0.24.rst":"MODIFY"},"diff":{"sklearn\/decomposition\/_dict_learning.py":[{"add":["1492","            n_iter=1, method=self.fit_algorithm,","1506","        self.iter_offset_ = iter_offset + 1"],"delete":["1492","            n_iter=self.n_iter, method=self.fit_algorithm,","1506","        self.iter_offset_ = iter_offset + self.n_iter"]}],"sklearn\/decomposition\/tests\/test_dict_learning.py":[{"add":["393","def test_dict_learning_iter_offset():","394","    n_components = 12","395","    rng = np.random.RandomState(0)","396","    V = rng.randn(n_components, n_features)","397","    dict1 = MiniBatchDictionaryLearning(n_components, n_iter=10,","398","                                        dict_init=V, random_state=0,","399","                                        shuffle=False)","400","    dict2 = MiniBatchDictionaryLearning(n_components, n_iter=10,","401","                                        dict_init=V, random_state=0,","402","                                        shuffle=False)","403","    dict1.fit(X)","404","    for sample in X:","405","        dict2.partial_fit(sample[np.newaxis, :])","406","","407","    assert dict1.iter_offset_ == dict2.iter_offset_","408","","409",""],"delete":[]}],"doc\/whats_new\/v0.24.rst":[{"add":["46",":mod:`sklearn.decomposition`","47","............................","48","","49","- |Fix| Fixed a bug in","50","  :func:`decomposition.MiniBatchDictionaryLearning.partial_fit` which should","51","  update the dictionary by iterating only once over a mini-batch.","52","  :pr:`17433` by :user:`Chiara Marmo <cmarmo>`","53",""],"delete":[]}]}},"ac8cbb3799bdfa31360c87bf1097410326dba0bd":{"changes":{"sklearn\/tree\/_classes.py":"MODIFY","sklearn\/feature_extraction\/text.py":"MODIFY","sklearn\/model_selection\/tests\/test_search.py":"MODIFY","sklearn\/naive_bayes.py":"MODIFY","sklearn\/model_selection\/tests\/test_split.py":"MODIFY","sklearn\/ensemble\/tests\/test_voting.py":"MODIFY","sklearn\/linear_model\/tests\/test_ridge.py":"MODIFY","sklearn\/random_projection.py":"MODIFY","sklearn\/tests\/test_naive_bayes.py":"MODIFY","sklearn\/feature_extraction\/tests\/test_text.py":"MODIFY","sklearn\/gaussian_process\/kernels.py":"MODIFY","sklearn\/metrics\/_scorer.py":"MODIFY","sklearn\/multioutput.py":"MODIFY","sklearn\/feature_extraction\/image.py":"MODIFY","sklearn\/ensemble\/tests\/test_gradient_boosting.py":"MODIFY","sklearn\/manifold\/_isomap.py":"MODIFY","sklearn\/model_selection\/_search.py":"MODIFY","sklearn\/tree\/tests\/test_tree.py":"MODIFY","sklearn\/inspection\/_plot\/partial_dependence.py":"MODIFY","sklearn\/utils\/estimator_checks.py":"MODIFY","sklearn\/decomposition\/_truncated_svd.py":"MODIFY","sklearn\/decomposition\/tests\/test_sparse_pca.py":"MODIFY","sklearn\/tests\/test_multioutput.py":"MODIFY","sklearn\/ensemble\/tests\/test_iforest.py":"MODIFY","sklearn\/ensemble\/_iforest.py":"MODIFY","sklearn\/ensemble\/tests\/test_weight_boosting.py":"MODIFY","sklearn\/metrics\/tests\/test_score_objects.py":"MODIFY","sklearn\/model_selection\/_split.py":"MODIFY","sklearn\/ensemble\/tests\/test_bagging.py":"MODIFY","sklearn\/tests\/test_pipeline.py":"MODIFY","sklearn\/feature_extraction\/tests\/test_image.py":"MODIFY","sklearn\/tests\/test_random_projection.py":"MODIFY","sklearn\/ensemble\/_base.py":"MODIFY","sklearn\/ensemble\/_voting.py":"MODIFY","sklearn\/inspection\/_plot\/tests\/test_plot_partial_dependence.py":"MODIFY","sklearn\/tests\/test_dummy.py":"MODIFY","sklearn\/pipeline.py":"MODIFY","sklearn\/dummy.py":"MODIFY","sklearn\/decomposition\/_sparse_pca.py":"MODIFY","sklearn\/gaussian_process\/tests\/test_kernels.py":"MODIFY"},"diff":{"sklearn\/tree\/_classes.py":[{"add":[],"delete":["99","                 presort='deprecated',","113","        self.presort = presort","321","        if self.presort != 'deprecated':","322","            warnings.warn(\"The parameter 'presort' is deprecated and has no \"","323","                          \"effect. It will be removed in v0.24. You can \"","324","                          \"suppress this warning by not passing any value \"","325","                          \"to the 'presort' parameter.\",","326","                          FutureWarning)","327","","730","    presort : deprecated, default='deprecated'","731","        This parameter is deprecated and will be removed in v0.24.","732","","733","        .. deprecated:: 0.22","734","","833","                 presort='deprecated',","848","            presort=presort,","1094","    presort : deprecated, default='deprecated'","1095","        This parameter is deprecated and will be removed in v0.24.","1096","","1097","        .. deprecated:: 0.22","1098","","1187","                 presort='deprecated',","1201","            presort=presort,","1248","    @property","1249","    def classes_(self):","1250","        # TODO: Remove method in 0.24","1251","        msg = (\"the classes_ attribute is to be deprecated from version \"","1252","               \"0.22 and will be removed in 0.24.\")","1253","        warnings.warn(msg, FutureWarning)","1254","        return np.array([None] * self.n_outputs_)","1255","","1256","    @property","1257","    def n_classes_(self):","1258","        # TODO: Remove method in 0.24","1259","        msg = (\"the n_classes_ attribute is to be deprecated from version \"","1260","               \"0.22 and will be removed in 0.24.\")","1261","        warnings.warn(msg, FutureWarning)","1262","        return np.array([1] * self.n_outputs_, dtype=np.intp)","1263",""]}],"sklearn\/feature_extraction\/text.py":[{"add":["32","from ..utils import _IS_32BIT","1839","    def transform(self, raw_documents):"],"delete":["32","from ..utils import _IS_32BIT, deprecated","507","@deprecated(\"VectorizerMixin is deprecated in version \"","508","            \"0.22 and will be removed in version 0.24.\")","509","class VectorizerMixin(_VectorizerMixin):","510","    pass","511","","512","","1845","    def transform(self, raw_documents, copy=\"deprecated\"):","1856","        copy : bool, default=True","1857","            Whether to copy X and operate on the copy or perform in-place","1858","            operations.","1859","","1860","            .. deprecated:: 0.22","1861","               The `copy` parameter is unused and was deprecated in version","1862","               0.22 and will be removed in 0.24. This parameter will be","1863","               ignored.","1864","","1872","        # FIXME Remove copy parameter support in 0.24","1873","        if copy != \"deprecated\":","1874","            msg = (\"'copy' param is unused and has been deprecated since \"","1875","                   \"version 0.22. Backward compatibility for 'copy' will \"","1876","                   \"be removed in 0.24.\")","1877","            warnings.warn(msg, FutureWarning)"]}],"sklearn\/model_selection\/tests\/test_search.py":[{"add":["867","    search = GridSearchCV(SVC(), cv=n_splits, param_grid=params,","868","                          return_train_score=True)","869","    search.fit(X, y)","870","    cv_results = search.cv_results_","871","    # Check if score and timing are reasonable","872","    assert all(cv_results['rank_test_score'] >= 1)","873","    assert (all(cv_results[k] >= 0) for k in score_keys","874","            if k != 'rank_test_score')","875","    assert (all(cv_results[k] <= 1) for k in score_keys","876","            if 'time' not in k and","877","            k != 'rank_test_score')","878","    # Check cv_results structure","879","    check_cv_results_array_types(search, param_keys, score_keys)","880","    check_cv_results_keys(cv_results, param_keys, score_keys, n_candidates)","881","    # Check masking","882","    cv_results = search.cv_results_","883","    n_candidates = len(search.cv_results_['params'])","884","    assert all((cv_results['param_C'].mask[i] and","885","                cv_results['param_gamma'].mask[i] and","886","                not cv_results['param_degree'].mask[i])","887","               for i in range(n_candidates)","888","               if cv_results['param_kernel'][i] == 'linear')","889","    assert all((not cv_results['param_C'].mask[i] and","890","                not cv_results['param_gamma'].mask[i] and","891","                cv_results['param_degree'].mask[i])","892","               for i in range(n_candidates)","893","               if cv_results['param_kernel'][i] == 'rbf')","917","    search = RandomizedSearchCV(SVC(), n_iter=n_search_iter,","918","                                cv=n_splits,","919","                                param_distributions=params,","920","                                return_train_score=True)","921","    search.fit(X, y)","922","    cv_results = search.cv_results_","923","    # Check results structure","924","    check_cv_results_array_types(search, param_keys, score_keys)","925","    check_cv_results_keys(cv_results, param_keys, score_keys, n_cand)","926","    n_candidates = len(search.cv_results_['params'])","927","    assert all((cv_results['param_C'].mask[i] and","928","                cv_results['param_gamma'].mask[i] and","929","                not cv_results['param_degree'].mask[i])","930","               for i in range(n_candidates)","931","               if cv_results['param_kernel'][i] == 'linear')","932","    assert all((not cv_results['param_C'].mask[i] and","933","                not cv_results['param_gamma'].mask[i] and","934","                cv_results['param_degree'].mask[i])","935","               for i in range(n_candidates)","936","               if cv_results['param_kernel'][i] == 'rbf')","946","    # Test the IID parameter  TODO: Clearly this test does something else???","1000","    grid_searches = []","1001","    for scoring in ({'accuracy': make_scorer(accuracy_score),","1002","                     'recall': make_scorer(recall_score)},","1003","                    'accuracy', 'recall'):","1004","        grid_search = GridSearchCV(SVC(), cv=n_splits,","1005","                                   param_grid=params,","1006","                                   scoring=scoring, refit=False)","1007","        grid_search.fit(X, y)","1008","        grid_searches.append(grid_search)","1010","    compare_cv_results_multimetric_with_single(*grid_searches)","1022","    for refit in (True, False):","1023","        random_searches = []","1024","        for scoring in (('accuracy', 'recall'), 'accuracy', 'recall'):","1025","            # If True, for multi-metric pass refit='accuracy'","1026","            if refit:","1027","                probability = True","1028","                refit = 'accuracy' if isinstance(scoring, tuple) else refit","1029","            else:","1030","                probability = False","1031","            clf = SVC(probability=probability, random_state=42)","1032","            random_search = RandomizedSearchCV(clf, n_iter=n_search_iter,","1033","                                               cv=n_splits,","1034","                                               param_distributions=params,","1035","                                               scoring=scoring,","1036","                                               refit=refit, random_state=0)","1037","            random_search.fit(X, y)","1038","            random_searches.append(random_search)","1040","        compare_cv_results_multimetric_with_single(*random_searches)","1041","        compare_refit_methods_when_refit_with_acc(","1042","            random_searches[0], random_searches[1], refit)","1046","        search_multi, search_acc, search_rec):","1616","    for attr in dir(gscv):","1617","        if (attr[0].islower() and attr[-1:] == '_' and","1618","                attr not in {'cv_results_', 'best_estimator_',","1619","                             'refit_time_', 'classes_'}):","1620","            assert getattr(gscv, attr) == getattr(mycv, attr), \\","1621","                \"Attribute %s not equal\" % attr"],"delete":["37","from sklearn.model_selection import cross_val_score","847","@pytest.mark.filterwarnings(\"ignore:The parameter 'iid' is deprecated\")  # 0.24","869","    for iid in (False, True):","870","        search = GridSearchCV(SVC(), cv=n_splits, iid=iid,","871","                              param_grid=params, return_train_score=True)","872","        search.fit(X, y)","873","        assert iid == search.iid","874","        cv_results = search.cv_results_","875","        # Check if score and timing are reasonable","876","        assert all(cv_results['rank_test_score'] >= 1)","877","        assert (all(cv_results[k] >= 0) for k in score_keys","878","                if k != 'rank_test_score')","879","        assert (all(cv_results[k] <= 1) for k in score_keys","880","                if 'time' not in k and","881","                k != 'rank_test_score')","882","        # Check cv_results structure","883","        check_cv_results_array_types(search, param_keys, score_keys)","884","        check_cv_results_keys(cv_results, param_keys, score_keys, n_candidates)","885","        # Check masking","886","        cv_results = search.cv_results_","887","        n_candidates = len(search.cv_results_['params'])","888","        assert all((cv_results['param_C'].mask[i] and","889","                    cv_results['param_gamma'].mask[i] and","890","                    not cv_results['param_degree'].mask[i])","891","                   for i in range(n_candidates)","892","                   if cv_results['param_kernel'][i] == 'linear')","893","        assert all((not cv_results['param_C'].mask[i] and","894","                    not cv_results['param_gamma'].mask[i] and","895","                    cv_results['param_degree'].mask[i])","896","                   for i in range(n_candidates)","897","                   if cv_results['param_kernel'][i] == 'rbf')","900","@pytest.mark.filterwarnings(\"ignore:The parameter 'iid' is deprecated\")  # 0.24","922","    for iid in (False, True):","923","        search = RandomizedSearchCV(SVC(), n_iter=n_search_iter,","924","                                    cv=n_splits, iid=iid,","925","                                    param_distributions=params,","926","                                    return_train_score=True)","927","        search.fit(X, y)","928","        assert iid == search.iid","929","        cv_results = search.cv_results_","930","        # Check results structure","931","        check_cv_results_array_types(search, param_keys, score_keys)","932","        check_cv_results_keys(cv_results, param_keys, score_keys, n_cand)","933","        n_candidates = len(search.cv_results_['params'])","934","        assert all((cv_results['param_C'].mask[i] and","935","                    cv_results['param_gamma'].mask[i] and","936","                    not cv_results['param_degree'].mask[i])","937","                   for i in range(n_candidates)","938","                   if cv_results['param_kernel'][i] == 'linear')","939","        assert all((not cv_results['param_C'].mask[i] and","940","                    not cv_results['param_gamma'].mask[i] and","941","                    cv_results['param_degree'].mask[i])","942","                   for i in range(n_candidates)","943","                   if cv_results['param_kernel'][i] == 'rbf')","953","    # Test the IID parameter","1000","@pytest.mark.filterwarnings(\"ignore:The parameter 'iid' is deprecated\")  # 0.24","1001","def test_search_iid_param():","1002","    # Test the IID parameter","1003","    # noise-free simple 2d-data","1004","    X, y = make_blobs(centers=[[0, 0], [1, 0], [0, 1], [1, 1]], random_state=0,","1005","                      cluster_std=0.1, shuffle=False, n_samples=80)","1006","    # split dataset into two folds that are not iid","1007","    # first one contains data of all 4 blobs, second only from two.","1008","    mask = np.ones(X.shape[0], dtype=np.bool)","1009","    mask[np.where(y == 1)[0][::2]] = 0","1010","    mask[np.where(y == 2)[0][::2]] = 0","1011","    # this leads to perfect classification on one fold and a score of 1\/3 on","1012","    # the other","1013","    # create \"cv\" for splits","1014","    cv = [[mask, ~mask], [~mask, mask]]","1015","    # once with iid=True (default)","1016","    grid_search = GridSearchCV(SVC(gamma='auto'), param_grid={'C': [1, 10]},","1017","                               cv=cv, return_train_score=True, iid=True)","1018","    random_search = RandomizedSearchCV(SVC(gamma='auto'), n_iter=2,","1019","                                       param_distributions={'C': [1, 10]},","1020","                                       cv=cv, iid=True,","1021","                                       return_train_score=True)","1022","    for search in (grid_search, random_search):","1023","        search.fit(X, y)","1024","        assert search.iid or search.iid is None","1025","","1026","        test_cv_scores = np.array(list(search.cv_results_['split%d_test_score'","1027","                                                          % s_i][0]","1028","                                       for s_i in range(search.n_splits_)))","1029","        test_mean = search.cv_results_['mean_test_score'][0]","1030","        test_std = search.cv_results_['std_test_score'][0]","1031","","1032","        train_cv_scores = np.array(list(search.cv_results_['split%d_train_'","1033","                                                           'score' % s_i][0]","1034","                                        for s_i in range(search.n_splits_)))","1035","        train_mean = search.cv_results_['mean_train_score'][0]","1036","        train_std = search.cv_results_['std_train_score'][0]","1037","","1038","        # Test the first candidate","1039","        assert search.cv_results_['param_C'][0] == 1","1040","        assert_array_almost_equal(test_cv_scores, [1, 1. \/ 3.])","1041","        assert_array_almost_equal(train_cv_scores, [1, 1])","1042","","1043","        # for first split, 1\/4 of dataset is in test, for second 3\/4.","1044","        # take weighted average and weighted std","1045","        expected_test_mean = 1 * 1. \/ 4. + 1. \/ 3. * 3. \/ 4.","1046","        expected_test_std = np.sqrt(1. \/ 4 * (expected_test_mean - 1) ** 2 +","1047","                                    3. \/ 4 * (expected_test_mean - 1. \/ 3.) **","1048","                                    2)","1049","        assert_almost_equal(test_mean, expected_test_mean)","1050","        assert_almost_equal(test_std, expected_test_std)","1051","        assert_array_almost_equal(test_cv_scores,","1052","                                  cross_val_score(SVC(C=1, gamma='auto'), X,","1053","                                                  y, cv=cv))","1054","","1055","        # For the train scores, we do not take a weighted mean irrespective of","1056","        # i.i.d. or not","1057","        assert_almost_equal(train_mean, 1)","1058","        assert_almost_equal(train_std, 0)","1059","","1060","    # once with iid=False","1061","    grid_search = GridSearchCV(SVC(gamma='auto'),","1062","                               param_grid={'C': [1, 10]},","1063","                               cv=cv, iid=False, return_train_score=True)","1064","    random_search = RandomizedSearchCV(SVC(gamma='auto'), n_iter=2,","1065","                                       param_distributions={'C': [1, 10]},","1066","                                       cv=cv, iid=False,","1067","                                       return_train_score=True)","1068","","1069","    for search in (grid_search, random_search):","1070","        search.fit(X, y)","1071","        assert not search.iid","1072","","1073","        test_cv_scores = np.array(list(search.cv_results_['split%d_test_score'","1074","                                                          % s][0]","1075","                                       for s in range(search.n_splits_)))","1076","        test_mean = search.cv_results_['mean_test_score'][0]","1077","        test_std = search.cv_results_['std_test_score'][0]","1078","","1079","        train_cv_scores = np.array(list(search.cv_results_['split%d_train_'","1080","                                                           'score' % s][0]","1081","                                        for s in range(search.n_splits_)))","1082","        train_mean = search.cv_results_['mean_train_score'][0]","1083","        train_std = search.cv_results_['std_train_score'][0]","1084","","1085","        assert search.cv_results_['param_C'][0] == 1","1086","        # scores are the same as above","1087","        assert_array_almost_equal(test_cv_scores, [1, 1. \/ 3.])","1088","        # Unweighted mean\/std is used","1089","        assert_almost_equal(test_mean, np.mean(test_cv_scores))","1090","        assert_almost_equal(test_std, np.std(test_cv_scores))","1091","","1092","        # For the train scores, we do not take a weighted mean irrespective of","1093","        # i.i.d. or not","1094","        assert_almost_equal(train_mean, 1)","1095","        assert_almost_equal(train_std, 0)","1096","","1097","","1098","@pytest.mark.filterwarnings(\"ignore:The parameter 'iid' is deprecated\")  # 0.24","1106","    for iid in (False, True):","1107","        grid_searches = []","1108","        for scoring in ({'accuracy': make_scorer(accuracy_score),","1109","                         'recall': make_scorer(recall_score)},","1110","                        'accuracy', 'recall'):","1111","            grid_search = GridSearchCV(SVC(), cv=n_splits,","1112","                                       iid=iid, param_grid=params,","1113","                                       scoring=scoring, refit=False)","1114","            grid_search.fit(X, y)","1115","            assert grid_search.iid == iid","1116","            grid_searches.append(grid_search)","1118","        compare_cv_results_multimetric_with_single(*grid_searches, iid=iid)","1121","@pytest.mark.filterwarnings(\"ignore:The parameter 'iid' is deprecated\")  # 0.24","1131","    for iid in (True, False):","1132","        for refit in (True, False):","1133","            random_searches = []","1134","            for scoring in (('accuracy', 'recall'), 'accuracy', 'recall'):","1135","                # If True, for multi-metric pass refit='accuracy'","1136","                if refit:","1137","                    probability = True","1138","                    refit = 'accuracy' if isinstance(scoring, tuple) else refit","1139","                else:","1140","                    probability = False","1141","                clf = SVC(probability=probability, random_state=42)","1142","                random_search = RandomizedSearchCV(clf, n_iter=n_search_iter,","1143","                                                   cv=n_splits, iid=iid,","1144","                                                   param_distributions=params,","1145","                                                   scoring=scoring,","1146","                                                   refit=refit, random_state=0)","1147","                random_search.fit(X, y)","1148","                random_searches.append(random_search)","1150","            compare_cv_results_multimetric_with_single(*random_searches,","1151","                                                       iid=iid)","1152","            compare_refit_methods_when_refit_with_acc(","1153","                random_searches[0], random_searches[1], refit)","1156","@pytest.mark.filterwarnings(\"ignore:The parameter 'iid' is deprecated\")  # 0.24","1158","        search_multi, search_acc, search_rec, iid):","1162","    assert search_multi.iid == iid","1729","    # TODO: remove in v0.24, the deprecation goes away then.","1730","    with pytest.warns(FutureWarning,","1731","                      match=\"attribute is to be deprecated from version 0.22\"):","1732","        for attr in dir(gscv):","1733","            if (attr[0].islower() and attr[-1:] == '_' and","1734","                    attr not in {'cv_results_', 'best_estimator_',","1735","                                 'refit_time_',","1736","                                 }):","1737","                assert getattr(gscv, attr) == getattr(mycv, attr), \\","1738","                    \"Attribute %s not equal\" % attr","1762","@pytest.mark.parametrize(\"iid\", [False, True])","1763","def test_deprecated_grid_search_iid(iid):","1764","    # FIXME: remove in 0.24","1765","    depr_msg = \"The parameter 'iid' is deprecated in 0.22 and will be removed\"","1766","    X, y = make_blobs(n_samples=54, random_state=0, centers=2)","1767","    grid = GridSearchCV(","1768","        SVC(random_state=0), param_grid={'C': [10]}, cv=3, iid=iid","1769","    )","1770","    with pytest.warns(FutureWarning, match=depr_msg):","1771","        grid.fit(X, y)","1772","","1773",""]}],"sklearn\/naive_bayes.py":[{"add":["29","from .utils import check_X_y, check_array","54","    @abstractmethod"],"delete":["29","from .utils import check_X_y, check_array, deprecated","56","        # Note that this is not marked @abstractmethod as long as the","57","        # deprecated public alias sklearn.naive_bayes.BayesNB exists","58","        # (until 0.24) to preserve backward compat for 3rd party projects","59","        # with existing derived classes.","60","        return X","1227","","1228","","1229","# TODO: remove in 0.24","1230","@deprecated(\"BaseNB is deprecated in version \"","1231","            \"0.22 and will be removed in version 0.24.\")","1232","class BaseNB(_BaseNB):","1233","    pass","1234","","1235","","1236","# TODO: remove in 0.24","1237","@deprecated(\"BaseDiscreteNB is deprecated in version \"","1238","            \"0.22 and will be removed in version 0.24.\")","1239","class BaseDiscreteNB(_BaseDiscreteNB):","1240","    pass"]}],"sklearn\/model_selection\/tests\/test_split.py":[{"add":["1618","    with pytest.raises(ValueError,","1619","                       match='has no effect since shuffle is False'):"],"delete":["1618","    # TODO 0.24: raise a ValueError instead of a warning","1619","    with pytest.warns(FutureWarning,","1620","                      match='has no effect since shuffle is False'):"]}],"sklearn\/ensemble\/tests\/test_voting.py":[{"add":["360","def test_set_estimator_drop():","361","    # VotingClassifier set_params should be able to set estimators as drop","374","        eclf2.set_params(rf='drop').fit(X, y)","375","    assert not record","378","    assert dict(eclf2.estimators)[\"rf\"] == 'drop'","382","    assert eclf2.get_params()[\"rf\"] == 'drop'","387","    assert not record","393","            eclf2.set_params(lr='drop', rf='drop', nb='drop').fit(X, y)","394","    assert not record","407","        eclf2.set_params(rf='drop').fit(X1, y1)","408","    assert not record","478","def test_none_estimator_with_weights(X, y, voter):","484","    voter.set_params(lr='drop')","487","    assert not record"],"delete":["360","# TODO: Remove parametrization in 0.24 when None is removed in Voting*","361","@pytest.mark.parametrize(\"drop\", [None, 'drop'])","362","def test_set_estimator_none(drop):","363","    \"\"\"VotingClassifier set_params should be able to set estimators as None or","364","    drop\"\"\"","377","        eclf2.set_params(rf=drop).fit(X, y)","378","    assert record if drop is None else not record","381","    assert dict(eclf2.estimators)[\"rf\"] is drop","385","    assert eclf2.get_params()[\"rf\"] is drop","390","    assert record if drop is None else not record","396","            eclf2.set_params(lr=drop, rf=drop, nb=drop).fit(X, y)","397","    assert record if drop is None else not record","410","        eclf2.set_params(rf=drop).fit(X1, y1)","411","    assert record if drop is None else not record","472","# TODO: Remove drop=None in 0.24 when None is removed in Voting*","482","@pytest.mark.parametrize(\"drop\", [None, 'drop'])","483","def test_none_estimator_with_weights(X, y, voter, drop):","484","    # TODO: remove the parametrization on 'drop' when support for None is","485","    # removed.","491","    voter.set_params(lr=drop)","494","    assert record if drop is None else not record","557","","558","","559","# TODO: Remove in 0.24 when None is removed in Voting*","560","@pytest.mark.parametrize(","561","    \"Voter, BaseEstimator\",","562","    [(VotingClassifier, DecisionTreeClassifier),","563","     (VotingRegressor, DecisionTreeRegressor)]","564",")","565","def test_deprecate_none_transformer(Voter, BaseEstimator):","566","    est = Voter(estimators=[('lr', None),","567","                            ('tree', BaseEstimator(random_state=0))])","568","","569","    msg = (\"Using 'None' to drop an estimator from the ensemble is \"","570","           \"deprecated in 0.22 and support will be dropped in 0.24. \"","571","           \"Use the string 'drop' instead.\")","572","    with pytest.warns(FutureWarning, match=msg):","573","        est.fit(X, y)"]}],"sklearn\/linear_model\/tests\/test_ridge.py":[{"add":["533","    kfold.fit(X_tiled, y_tiled)"],"delete":["533","    # ignore warning from GridSearchCV: FutureWarning: The default","534","    # of the `iid` parameter will change from True to False in version 0.22","535","    # and will be removed in 0.24","536","    with ignore_warnings(category=FutureWarning):","537","        kfold.fit(X_tiled, y_tiled)"]}],"sklearn\/random_projection.py":[{"add":[],"delete":["43","from .utils import deprecated","155","# TODO: remove in 0.24","156","@deprecated(\"gaussian_random_matrix is deprecated in \"","157","            \"0.22 and will be removed in version 0.24.\")","158","def gaussian_random_matrix(n_components, n_features, random_state=None):","159","    return _gaussian_random_matrix(n_components, n_features, random_state)","160","","161","","202","# TODO: remove in 0.24","203","@deprecated(\"gaussian_random_matrix is deprecated in \"","204","            \"0.22 and will be removed in version 0.24.\")","205","def sparse_random_matrix(n_components, n_features, density='auto',","206","                         random_state=None):","207","    return _sparse_random_matrix(n_components, n_features, density,","208","                                 random_state)","209","","210",""]}],"sklearn\/tests\/test_naive_bayes.py":[{"add":[],"delete":["23","from sklearn.naive_bayes import BaseNB, BaseDiscreteNB","828","","829","","830","# TODO: remove in 0.24","831","def test_deprecations():","832","","833","    class A(BaseNB, GaussianNB):","834","        pass","835","","836","    class B(BaseDiscreteNB, CategoricalNB):","837","        pass","838","","839","    with pytest.warns(FutureWarning, match=\"is deprecated in version 0.22\"):","840","        A()","841","","842","    with pytest.warns(FutureWarning, match=\"is deprecated in version 0.22\"):","843","        B()"]}],"sklearn\/feature_extraction\/tests\/test_text.py":[{"add":[],"delete":["15","from sklearn.feature_extraction.text import VectorizerMixin","524","# FIXME Remove copy parameter support in 0.24","525","def test_tfidf_vectorizer_deprecationwarning():","526","    msg = (\"'copy' param is unused and has been deprecated since \"","527","           \"version 0.22. Backward compatibility for 'copy' will \"","528","           \"be removed in 0.24.\")","529","    with pytest.warns(FutureWarning, match=msg):","530","        tv = TfidfVectorizer()","531","        train_data = JUNK_FOOD_DOCS","532","        tv.fit(train_data)","533","        tv.transform(train_data, copy=True)","534","","535","","1354","","1355","","1356","# TODO: Remove in 0.24","1357","def test_vectorizermixin_is_deprecated():","1358","    class MyVectorizer(VectorizerMixin):","1359","        pass","1360","","1361","    msg = (\"VectorizerMixin is deprecated in version 0.22 and will be removed \"","1362","           \"in version 0.24.\")","1363","    with pytest.warns(FutureWarning, match=msg):","1364","        MyVectorizer()"]}],"sklearn\/gaussian_process\/kernels.py":[{"add":["160","            params[arg] = getattr(self, arg)","161",""],"delete":["25","import warnings","161","            try:","162","                value = getattr(self, arg)","163","            except AttributeError:","164","                warnings.warn('From version 0.24, get_params will raise an '","165","                              'AttributeError if a parameter cannot be '","166","                              'retrieved as an instance attribute. Previously '","167","                              'it would return None.',","168","                              FutureWarning)","169","                value = None","170","            params[arg] = value"]}],"sklearn\/metrics\/_scorer.py":[{"add":["347","            scorer = SCORERS[scoring]"],"delete":["23","import warnings","129","        # XXX After removing the deprecated scorers (v0.24) remove the","130","        # XXX deprecation_msg property again and remove __call__'s body again","131","        self._deprecation_msg = None","164","        if self._deprecation_msg is not None:","165","            warnings.warn(self._deprecation_msg,","166","                          category=FutureWarning,","167","                          stacklevel=2)","355","            if scoring == 'brier_score_loss':","356","                # deprecated","357","                scorer = brier_score_loss_scorer","358","            else:","359","                scorer = SCORERS[scoring]","670","deprecation_msg = ('Scoring method brier_score_loss was renamed to '","671","                   'neg_brier_score in version 0.22 and will '","672","                   'be removed in 0.24.')","673","brier_score_loss_scorer._deprecation_msg = deprecation_msg"]}],"sklearn\/multioutput.py":[{"add":[],"delete":["29","from .utils import deprecated","790","","791","","792","# TODO: remove in 0.24","793","@deprecated(\"MultiOutputEstimator is deprecated in version \"","794","            \"0.22 and will be removed in version 0.24.\")","795","class MultiOutputEstimator(_MultiOutputEstimator):","796","    pass"]}],"sklearn\/feature_extraction\/image.py":[{"add":["17","from ..utils import check_array, check_random_state"],"delete":["17","from ..utils import check_array, check_random_state, deprecated","307","@deprecated(\"The function feature_extraction.image.extract_patches has been \"","308","            \"deprecated in 0.22 and will be removed in 0.24.\")","309","def extract_patches(arr, patch_shape=8, extraction_step=1):","310","    \"\"\"Extracts patches of any n-dimensional array in place using strides.","311","","312","    Given an n-dimensional array it will return a 2n-dimensional array with","313","    the first n dimensions indexing patch position and the last n indexing","314","    the patch content. This operation is immediate (O(1)). A reshape","315","    performed on the first n dimensions will cause numpy to copy data, leading","316","    to a list of extracted patches.","317","","318","    Read more in the :ref:`User Guide <image_feature_extraction>`.","319","","320","    Parameters","321","    ----------","322","    arr : ndarray","323","        n-dimensional array of which patches are to be extracted","324","","325","    patch_shape : int or tuple of length arr.ndim, default=8","326","        Indicates the shape of the patches to be extracted. If an","327","        integer is given, the shape will be a hypercube of","328","        sidelength given by its value.","329","","330","    extraction_step : int or tuple of length arr.ndim, default=1","331","        Indicates step size at which extraction shall be performed.","332","        If integer is given, then the step is uniform in all dimensions.","333","","334","","335","    Returns","336","    -------","337","    patches : strided ndarray","338","        2n-dimensional array indexing patches on first n dimensions and","339","        containing patches on the last n dimensions. These dimensions","340","        are fake, but this way no data is copied. A simple reshape invokes","341","        a copying operation to obtain a list of patches:","342","        result.reshape([-1] + list(patch_shape))","343","    \"\"\"","344","    return _extract_patches(arr, patch_shape=patch_shape,","345","                            extraction_step=extraction_step)","346","","347",""]}],"sklearn\/ensemble\/tests\/test_gradient_boosting.py":[{"add":[],"delete":["34","from sklearn.utils._testing import ignore_warnings","1297","# TODO: Remove in 0.24 when DummyClassifier's `strategy` default updates","1298","@ignore_warnings(category=FutureWarning)"]}],"sklearn\/manifold\/_isomap.py":[{"add":[],"delete":["8","from ..utils.deprecation import deprecated","170","    # mypy error: Decorated property not supported","171","    @deprecated(  # type: ignore","172","        \"Attribute `training_data_` was deprecated in version 0.22 and\"","173","        \" will be removed in 0.24.\"","174","    )","175","    @property","176","    def training_data_(self):","177","        check_is_fitted(self)","178","        return self.nbrs_._fit_X","179",""]}],"sklearn\/model_selection\/_search.py":[{"add":["406","                 refit=True, cv=None, verbose=0,","848","                   weights=None)","1155","                 n_jobs=None, refit=True, cv=None,","1160","            n_jobs=n_jobs, refit=refit, cv=cv, verbose=verbose,","1485","                 scoring=None, n_jobs=None, refit=True,","1494","            n_jobs=n_jobs, refit=refit, cv=cv, verbose=verbose,"],"delete":["406","                 iid='deprecated', refit=True, cv=None, verbose=0,","413","        self.iid = iid","845","        if self.iid != 'deprecated':","846","            warnings.warn(","847","                \"The parameter 'iid' is deprecated in 0.22 and will be \"","848","                \"removed in 0.24.\", FutureWarning","849","            )","850","            iid = self.iid","851","        else:","852","            iid = False","853","","858","                   weights=test_sample_counts if iid else None)","936","    iid : bool, default=False","937","        If True, return the average score across folds, weighted by the number","938","        of samples in each test set. In this case, the data is assumed to be","939","        identically distributed across the folds, and the loss minimized is","940","        the total loss per sample, and not the mean loss across the folds.","941","","942","        .. deprecated:: 0.22","943","            Parameter ``iid`` is deprecated in 0.22 and will be removed in 0.24","944","","1174","                 n_jobs=None, iid='deprecated', refit=True, cv=None,","1179","            n_jobs=n_jobs, iid=iid, refit=refit, cv=cv, verbose=verbose,","1277","    iid : bool, default=False","1278","        If True, return the average score across folds, weighted by the number","1279","        of samples in each test set. In this case, the data is assumed to be","1280","        identically distributed across the folds, and the loss minimized is","1281","        the total loss per sample, and not the mean loss across the folds.","1282","","1283","        .. deprecated:: 0.22","1284","            Parameter ``iid`` is deprecated in 0.22 and will be removed in 0.24","1285","","1513","                 scoring=None, n_jobs=None, iid='deprecated', refit=True,","1522","            n_jobs=n_jobs, iid=iid, refit=refit, cv=cv, verbose=verbose,"]}],"sklearn\/tree\/tests\/test_tree.py":[{"add":[],"delete":["1609","@pytest.mark.parametrize('Cls',","1610","                         (DecisionTreeRegressor, DecisionTreeClassifier))","1611","@pytest.mark.parametrize('presort', ['auto', True, False])","1612","def test_presort_deprecated(Cls, presort):","1613","    # TODO: remove in v0.24","1614","    X = np.zeros((10, 10))","1615","    y = np.r_[[0] * 5, [1] * 5]","1616","    tree = Cls(presort=presort)","1617","    with pytest.warns(FutureWarning,","1618","                      match=\"The parameter 'presort' is deprecated \"):","1619","        tree.fit(X, y)","1620","","1621","","1933","def test_classes_deprecated():","1934","    X = [[0, 0], [2, 2], [4, 6], [10, 11]]","1935","    y = [0.5, 2.5, 3.5, 5.5]","1936","    clf = DecisionTreeRegressor()","1937","    clf = clf.fit(X, y)","1938","","1939","    match = (\"attribute is to be deprecated from version \"","1940","             \"0.22 and will be removed in 0.24.\")","1941","","1942","    with pytest.warns(FutureWarning, match=match):","1943","        n = len(clf.classes_)","1944","        assert n == clf.n_outputs_","1945","","1946","    with pytest.warns(FutureWarning, match=match):","1947","        assert len(clf.n_classes_) == clf.n_outputs_","1948","","1949",""]}],"sklearn\/inspection\/_plot\/partial_dependence.py":[{"add":["21","                            method='auto', n_jobs=None, verbose=0,"],"delete":["3","import warnings","22","                            method='auto', n_jobs=None, verbose=0, fig=None,","156","    fig : Matplotlib figure object, optional (default=None)","157","        A figure object onto which the plots will be drawn, after the figure","158","        has been cleared. By default, a new one is created.","159","","160","        .. deprecated:: 0.22","161","           ``fig`` will be removed in 0.24.","162","","319","    if fig is not None:","320","        warnings.warn(\"The fig parameter is deprecated in version \"","321","                      \"0.22 and will be removed in version 0.24\",","322","                      FutureWarning)","323","        fig.clear()","324","        ax = fig.gca()","325",""]}],"sklearn\/utils\/estimator_checks.py":[{"add":["575","    if name == 'DummyClassifier':","576","        # the default strategy prior would output constant predictions and fail","577","        # for check_classifiers_predictions","578","        estimator.set_params(strategy='stratified')","579",""],"delete":[]}],"sklearn\/decomposition\/_truncated_svd.py":[{"add":[],"delete":["90","    >>> from sklearn.random_projection import sparse_random_matrix"]}],"sklearn\/decomposition\/tests\/test_sparse_pca.py":[{"add":[],"delete":["191","@pytest.mark.parametrize(\"spca\", [SparsePCA, MiniBatchSparsePCA])","192","def test_spca_deprecation_warning(spca):","193","    rng = np.random.RandomState(0)","194","    Y, _, _ = generate_toy_data(3, 10, (8, 8), random_state=rng)","195","","196","    warn_msg = \"'normalize_components' has been deprecated in 0.22\"","197","    with pytest.warns(FutureWarning, match=warn_msg):","198","        spca(normalize_components=True).fit(Y)","199","","200","","201","@pytest.mark.parametrize(\"spca\", [SparsePCA, MiniBatchSparsePCA])","202","def test_spca_error_unormalized_components(spca):","203","    rng = np.random.RandomState(0)","204","    Y, _, _ = generate_toy_data(3, 10, (8, 8), random_state=rng)","205","","206","    err_msg = \"normalize_components=False is not supported starting \"","207","    with pytest.raises(NotImplementedError, match=err_msg):","208","        spca(normalize_components=False).fit(Y)","209","","210",""]}],"sklearn\/tests\/test_multioutput.py":[{"add":[],"delete":["28","from sklearn.multioutput import MultiOutputEstimator","558","# TODO: remove in 0.24","559","def test_deprecation():","560","    class A(MultiOutputEstimator, MultiOutputRegressor):","561","        pass","562","","563","    with pytest.warns(FutureWarning, match=\"is deprecated in version 0.22\"):","564","        A(SGDRegressor(random_state=0, max_iter=5))","565","","566",""]}],"sklearn\/ensemble\/tests\/test_iforest.py":[{"add":[],"delete":["126","    # test that behaviour='old' will raise an error","127","    msg = \"The old behaviour of IsolationForest is not implemented anymore.\"","128","    with pytest.raises(NotImplementedError, match=msg):","129","        IsolationForest(behaviour='old').fit(X)","130","","319","def test_iforest_deprecation():","320","    iforest = IsolationForest(behaviour='new')","321","    warn_msg = \"'behaviour' is deprecated in 0.22 and will be removed in 0.24\"","322","    with pytest.warns(FutureWarning, match=warn_msg):","323","        iforest.fit(iris.data)","324","","325",""]}],"sklearn\/ensemble\/_iforest.py":[{"add":[],"delete":["95","    behaviour : str, default='deprecated'","96","        This parameter has no effect, is deprecated, and will be removed.","97","","98","        .. versionadded:: 0.20","99","           ``behaviour`` is added in 0.20 for back-compatibility purpose.","100","","101","        .. deprecated:: 0.20","102","           ``behaviour='old'`` is deprecated in 0.20 and will not be possible","103","           in 0.22.","104","","105","        .. deprecated:: 0.22","106","           ``behaviour`` parameter is deprecated in 0.22 and removed in","107","           0.24.","108","","194","                 behaviour='deprecated',","214","        self.behaviour = behaviour","249","        if self.behaviour != 'deprecated':","250","            if self.behaviour == 'new':","251","                warn(","252","                    \"'behaviour' is deprecated in 0.22 and will be removed \"","253","                    \"in 0.24. You should not pass or set this parameter.\",","254","                    FutureWarning","255","                )","256","            else:","257","                raise NotImplementedError(","258","                    \"The old behaviour of IsolationForest is not implemented \"","259","                    \"anymore. Remove the 'behaviour' parameter.\"","260","                )","261",""]}],"sklearn\/ensemble\/tests\/test_weight_boosting.py":[{"add":[],"delete":["14","from sklearn.utils._testing import ignore_warnings","502","# TODO: Remove in 0.24 when DummyClassifier's `strategy` default changes","503","@ignore_warnings"]}],"sklearn\/metrics\/tests\/test_score_objects.py":[{"add":[],"delete":["555","def test_deprecated_scorer():","556","    X, y = make_blobs(random_state=0, centers=2)","557","    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)","558","    clf = DecisionTreeClassifier()","559","    clf.fit(X_train, y_train)","560","","561","    deprecated_scorer = get_scorer('brier_score_loss')","562","    with pytest.warns(FutureWarning):","563","        deprecated_scorer(clf, X_test, y_test)","564","","565",""]}],"sklearn\/model_selection\/_split.py":[{"add":["291","            raise ValueError(","293","                'False. You should leave '"],"delete":["291","            # TODO 0.24: raise a ValueError instead of a warning","292","            warnings.warn(","294","                'False. This will raise an error in 0.24. You should leave '","296","                FutureWarning"]}],"sklearn\/ensemble\/tests\/test_bagging.py":[{"add":[],"delete":["18","from sklearn.utils._testing import ignore_warnings","54","# TODO: Remove in 0.24 when DummyClassifier's `strategy` default updates","55","@ignore_warnings(category=FutureWarning)"]}],"sklearn\/tests\/test_pipeline.py":[{"add":["902","def test_set_feature_union_step_drop():","915","        ft.set_params(m2='drop')","919","    assert not record","922","        ft.set_params(m3='drop')","926","    assert not record","932","    assert not record","936","        ft = FeatureUnion([('m2', 'drop'), ('m3', mult3)])","940","    assert not record"],"delete":["902","# TODO: Remove parametrization in 0.24 when None is removed for FeatureUnion","903","@pytest.mark.parametrize('drop', ['drop', None])","904","def test_set_feature_union_step_drop(drop):","917","        ft.set_params(m2=drop)","921","    assert record if drop is None else not record","924","        ft.set_params(m3=drop)","928","    assert record if drop is None else not record","934","    assert record if drop is None else not record","938","        ft = FeatureUnion([('m2', drop), ('m3', mult3)])","942","    assert record if drop is None else not record","1229","","1230","","1231","# TODO: Remove in 0.24 when None is removed","1232","def test_feature_union_warns_with_none():","1233","    msg = (r\"Using None as a transformer is deprecated in version 0\\.22 and \"","1234","           r\"will be removed in version 0\\.24\\. Please use 'drop' instead\\.\")","1235","    with pytest.warns(FutureWarning, match=msg):","1236","        union = FeatureUnion([('multi1', None), ('multi2', Mult())])","1237","","1238","    X = [[1, 2, 3], [4, 5, 6]]","1239","","1240","    with pytest.warns(FutureWarning, match=msg):","1241","        union.fit_transform(X)"]}],"sklearn\/feature_extraction\/tests\/test_image.py":[{"add":["12","    reconstruct_from_patches_2d, PatchExtractor, _extract_patches)"],"delete":["12","    reconstruct_from_patches_2d, PatchExtractor, _extract_patches,","13","    extract_patches)","336","","337","","338","# TODO: Remove in 0.24","339","def test_extract_patches_deprecated():","340","    msg = (\"The function feature_extraction.image.extract_patches has been \"","341","           \"deprecated in 0.22 and will be removed in 0.24.\")","342","    with pytest.warns(FutureWarning, match=msg):","343","        extract_patches(downsampled_face)"]}],"sklearn\/tests\/test_random_projection.py":[{"add":[],"delete":["12","from sklearn.random_projection import gaussian_random_matrix","14","from sklearn.random_projection import sparse_random_matrix","356","","357","","358","# TODO remove in 0.24","359","def test_deprecations():","360","","361","    with pytest.warns(FutureWarning, match=\"deprecated in 0.22\"):","362","        gaussian_random_matrix(10, 100)","363","","364","    with pytest.warns(FutureWarning, match=\"deprecated in 0.22\"):","365","        sparse_random_matrix(10, 100)"]}],"sklearn\/ensemble\/_base.py":[{"add":["228","        has_estimator = any(est != 'drop' for est in estimators)","239","            if est != 'drop' and not is_estimator_type(est):"],"delete":["7","import warnings","229","        # FIXME: deprecate the usage of None to drop an estimator from the","230","        # ensemble. Remove in 0.24","231","        if any(est is None for est in estimators):","232","            warnings.warn(","233","                \"Using 'None' to drop an estimator from the ensemble is \"","234","                \"deprecated in 0.22 and support will be dropped in 0.24. \"","235","                \"Use the string 'drop' instead.\", FutureWarning","236","            )","237","","238","        has_estimator = any(est not in (None, 'drop') for est in estimators)","249","            if est not in (None, 'drop') and not is_estimator_type(est):"]}],"sklearn\/ensemble\/_voting.py":[{"add":["55","                if est[1] != 'drop']","80","                for idx, clf in enumerate(clfs) if clf != 'drop'","85","        # Uses 'drop' as placeholder for dropped estimators","88","            current_est = est if est == 'drop' else next(est_iter)","128","            ``'drop'`` is accepted. Using None was deprecated in 0.22 and","129","            support was removed in 0.24.","375","            ``'drop'`` is accepted. Using None was deprecated in 0.22 and","376","            support was removed in 0.24."],"delete":["55","                if est[1] not in (None, 'drop')]","80","                for idx, clf in enumerate(clfs) if clf not in (None, 'drop')","85","        # Uses None or 'drop' as placeholder for dropped estimators","88","            current_est = est if est in (None, 'drop') else next(est_iter)","128","            ``'drop'`` is accepted.","129","","130","        .. deprecated:: 0.22","131","           Using ``None`` to drop an estimator is deprecated in 0.22 and","132","           support will be dropped in 0.24. Use the string ``'drop'`` instead.","378","            ``'drop'`` is accepted.","379","","380","        .. deprecated:: 0.22","381","           Using ``None`` to drop an estimator is deprecated in 0.22 and","382","           support will be dropped in 0.24. Use the string ``'drop'`` instead."]}],"sklearn\/inspection\/_plot\/tests\/test_plot_partial_dependence.py":[{"add":[],"delete":["459","","460","","461","def test_plot_partial_dependence_fig_deprecated(pyplot):","462","    # Make sure fig object is correctly used if not None","463","    X, y = make_regression(n_samples=50, random_state=0)","464","    clf = LinearRegression()","465","    clf.fit(X, y)","466","","467","    fig = pyplot.figure()","468","    grid_resolution = 25","469","","470","    msg = (\"The fig parameter is deprecated in version 0.22 and will be \"","471","           \"removed in version 0.24\")","472","    with pytest.warns(FutureWarning, match=msg):","473","        plot_partial_dependence(","474","            clf, X, [0, 1], target=0, grid_resolution=grid_resolution, fig=fig)","475","","476","    assert pyplot.gcf() is fig"]}],"sklearn\/tests\/test_dummy.py":[{"add":[],"delete":["758","@pytest.mark.filterwarnings(\"ignore:The default value of strategy.*\")  # 0.24","767","","768","","769","@pytest.mark.parametrize(\"Dummy\", (DummyRegressor, DummyClassifier))","770","def test_outputs_2d_deprecation(Dummy):","771","    X = [[1, 2]]","772","    y = [0]","773","    with pytest.warns(FutureWarning,","774","                      match=\"will be removed in version 0.24\"):","775","        Dummy().fit(X, y).outputs_2d_","776","","777","","778","# TODO: Remove in 0.24 when DummyClassifier's `strategy` default updates","779","def test_strategy_stratified_deprecated_for_prior():","780","    X, y = [[1, 2]], [0]","781","","782","    msg = (\"The default value of strategy will change from \"","783","           \"stratified to prior in 0.24\")","784","    with pytest.warns(FutureWarning, match=msg):","785","        DummyClassifier().fit(X, y)"]}],"sklearn\/pipeline.py":[{"add":["781","        half of each tuple is the name of the transformer. The tranformer can","782","        be 'drop' for it to be ignored.","883","                if trans != 'drop')","1006","        self.transformer_list[:] = [(name, old if old == 'drop'"],"delete":["13","import warnings","782","        half of each tuple is the name of the transformer.","867","            # TODO: Remove in 0.24 when None is removed","868","            if t is None:","869","                warnings.warn(\"Using None as a transformer is deprecated \"","870","                              \"in version 0.22 and will be removed in \"","871","                              \"version 0.24. Please use 'drop' instead.\",","872","                              FutureWarning)","873","                continue","890","                if trans is not None and trans != 'drop')","1013","        self.transformer_list[:] = [(name, old if old is None or old == 'drop'"]}],"sklearn\/dummy.py":[{"add":["21","","35","    strategy : str, default=\"prior\"","49","          .. versionchanged:: 0.24","50","             The default value of `strategy` has changed to \"prior\" in version","51","             0.24.","96","    def __init__(self, *, strategy=\"prior\", random_state=None,","123","        if self.strategy not in allowed_strategies:","126","","127","        self._strategy = self.strategy"],"delete":["19","from .utils import deprecated","35","    strategy : str, default=\"stratified\"","49","          .. versionchanged:: 0.22","50","             The default value of `strategy` will change to \"prior\" in version","51","             0.24. Starting from version 0.22, a warning will be raised if","52","             `strategy` is not explicitly set.","53","","54","          .. versionadded:: 0.17","55","             Dummy Classifier now supports prior fitting strategy using","56","             parameter *prior*.","101","    def __init__(self, *, strategy=\"warn\", random_state=None,","128","        # TODO: Remove in 0.24","129","        if self.strategy == \"warn\":","130","            warnings.warn(\"The default value of strategy will change from \"","131","                          \"stratified to prior in 0.24.\", FutureWarning)","132","            self._strategy = \"stratified\"","133","        elif self.strategy not in allowed_strategies:","136","        else:","137","            self._strategy = self.strategy","397","    # mypy error: Decorated property not supported","398","    @deprecated(  # type: ignore","399","        \"The outputs_2d_ attribute is deprecated in version 0.22 \"","400","        \"and will be removed in version 0.24. It is equivalent to \"","401","        \"n_outputs_ > 1.\"","402","    )","403","    @property","404","    def outputs_2d_(self):","405","        return self.n_outputs_ != 1","406","","626","","627","    # mypy error: Decorated property not supported","628","    @deprecated(  # type: ignore","629","        \"The outputs_2d_ attribute is deprecated in version 0.22 \"","630","        \"and will be removed in version 0.24. It is equivalent to \"","631","        \"n_outputs_ > 1.\"","632","    )","633","    @property","634","    def outputs_2d_(self):","635","        return self.n_outputs_ != 1"]}],"sklearn\/decomposition\/_sparse_pca.py":[{"add":["114","                 U_init=None, V_init=None, verbose=False, random_state=None):","315","                 shuffle=True, n_jobs=None, method='lars', random_state=None):","319","            random_state=random_state)"],"delete":["4","import warnings","5","","16","# FIXME: remove in 0.24","17","def _check_normalize_components(normalize_components, estimator_name):","18","    if normalize_components != 'deprecated':","19","        if normalize_components:","20","            warnings.warn(","21","                \"'normalize_components' has been deprecated in 0.22 and \"","22","                \"will be removed in 0.24. Remove the parameter from the \"","23","                \" constructor.\", FutureWarning","24","            )","25","        else:","26","            raise NotImplementedError(","27","                \"normalize_components=False is not supported starting from \"","28","                \"0.22. Remove this parameter from the constructor.\"","29","            )","30","","31","","87","    normalize_components : 'deprecated'","88","        This parameter does not have any effect. The components are always","89","        normalized.","90","","91","        .. versionadded:: 0.20","92","","93","        .. deprecated:: 0.22","94","           ``normalize_components`` is deprecated in 0.22 and will be removed","95","           in 0.24.","96","","142","                 U_init=None, V_init=None, verbose=False, random_state=None,","143","                 normalize_components='deprecated'):","155","        self.normalize_components = normalize_components","176","        _check_normalize_components(","177","            self.normalize_components, self.__class__.__name__","178","        )","179","","306","    normalize_components : 'deprecated'","307","        This parameter does not have any effect. The components are always","308","        normalized.","309","","310","        .. versionadded:: 0.20","311","","312","        .. deprecated:: 0.22","313","           ``normalize_components`` is deprecated in 0.22 and will be removed","314","           in 0.24.","315","","359","                 shuffle=True, n_jobs=None, method='lars', random_state=None,","360","                 normalize_components='deprecated'):","364","            random_state=random_state,","365","            normalize_components=normalize_components)","390","        _check_normalize_components(","391","            self.normalize_components, self.__class__.__name__","392","        )","393",""]}],"sklearn\/gaussian_process\/tests\/test_kernels.py":[{"add":["16","            Exponentiation, CompoundKernel)"],"delete":["16","            Exponentiation, Kernel, CompoundKernel)","358","def test_warns_on_get_params_non_attribute():","359","    class MyKernel(Kernel):","360","        def __init__(self, param=5):","361","            pass","362","","363","        def __call__(self, X, Y=None, eval_gradient=False):","364","            return X","365","","366","        def diag(self, X):","367","            return np.ones(X.shape[0])","368","","369","        def is_stationary(self):","370","            return False","371","","372","    est = MyKernel()","373","    with pytest.warns(FutureWarning, match='AttributeError'):","374","        params = est.get_params()","375","","376","    assert params['param'] is None","377","","378",""]}]}},"e02e1bf7cbdffd0325269cc195298d62e0818180":{"changes":{"sklearn\/model_selection\/tests\/test_search.py":"MODIFY"},"diff":{"sklearn\/model_selection\/tests\/test_search.py":[{"add":["821","    param_distributions = {\"C\": uniform(0, 1)}","822","    sampler = ParameterSampler(param_distributions=param_distributions,","823","                               n_iter=10, random_state=0)","824","    assert [x for x in sampler] == [x for x in sampler]"],"delete":["15","from sklearn.utils.fixes import sp_version","822","    if sp_version >= (0, 16):","823","        param_distributions = {\"C\": uniform(0, 1)}","824","        sampler = ParameterSampler(param_distributions=param_distributions,","825","                                   n_iter=10, random_state=0)","826","        assert [x for x in sampler] == [x for x in sampler]"]}]}},"0e332293511a5b952039ad12defe626dfb023b67":{"changes":{"sklearn\/model_selection\/_validation.py":"MODIFY","sklearn\/model_selection\/tests\/test_validation.py":"MODIFY"},"diff":{"sklearn\/model_selection\/_validation.py":[{"add":["1541","    The aggregated output of _aggregate_score_dicts will be a list of dict","1561","    return {","1562","        key: np.asarray([score[key] for score in scores])","1563","        if isinstance(scores[0][key], numbers.Number)","1564","        else [score[key] for score in scores]","1565","        for key in scores[0]","1566","    }"],"delete":["1541","    The aggregated output of _fit_and_score will be a list of dict","1561","    return {key: np.asarray([score[key] for score in scores])","1562","            for key in scores[0]}"]}],"sklearn\/model_selection\/tests\/test_validation.py":[{"add":["358","def test_cross_validate_nested_estimator():","359","    # Non-regression test to ensure that nested","360","    # estimators are properly returned in a list","361","    # https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/17745","362","    (X, y) = load_iris(return_X_y=True)","363","    pipeline = Pipeline([","364","        (\"imputer\", SimpleImputer()),","365","        (\"classifier\", MockClassifier()),","366","    ])","367","","368","    results = cross_validate(pipeline, X, y, return_estimator=True)","369","    estimators = results[\"estimator\"]","370","","371","    assert isinstance(estimators, list)","372","    assert all(isinstance(estimator, Pipeline) for estimator in estimators)","373","","374",""],"delete":[]}]}},"c298cb771dfcc607d44c8219710f7cefaead39ea":{"changes":{"examples\/decomposition\/plot_ica_vs_pca.py":"MODIFY"},"diff":{"examples\/decomposition\/plot_ica_vs_pca.py":[{"add":["72","            plt.quiver((0, 0), (0, 0), x_axis, y_axis, zorder=11, width=0.01,","73","                       scale=6, color=color)"],"delete":["72","            plt.quiver(0, 0, x_axis, y_axis, zorder=11, width=0.01, scale=6,","73","                       color=color)"]}]}},"5297365ebec6c172060eb19462894816384b6674":{"changes":{"sklearn\/feature_selection\/_rfe.py":"MODIFY","sklearn\/feature_selection\/_from_model.py":"MODIFY","sklearn\/feature_selection\/_univariate_selection.py":"MODIFY","sklearn\/feature_selection\/_mutual_info.py":"MODIFY","sklearn\/feature_selection\/_variance_threshold.py":"MODIFY","sklearn\/feature_selection\/_base.py":"MODIFY"},"diff":{"sklearn\/feature_selection\/_rfe.py":[{"add":["59","    n_features_to_select : int or None, default=None","63","    step : int or float, default=1","69","    verbose : int, default=0","370","    step : int or float, default=1","378","    min_features_to_select : int, default=1","386","    cv : int, cross-validation generator or an iterable, default=None","406","    scoring : string, callable or None, default=None","411","    verbose : int, default=0","414","    n_jobs : int or None, default=None","422","    importance_getter : str or callable, default='auto'","526","        groups : array-like of shape (n_samples,) or None, default=None"],"delete":["59","    n_features_to_select : int or None (default=None)","63","    step : int or float, optional (default=1)","69","    verbose : int, (default=0)","370","    step : int or float, optional (default=1)","378","    min_features_to_select : int, (default=1)","386","    cv : int, cross-validation generator or an iterable, optional","406","    scoring : string, callable or None, optional, (default=None)","411","    verbose : int, (default=0)","414","    n_jobs : int or None, optional (default=None)","422","    importance_getter : str or callable, optional (default='auto')","526","        groups : array-like of shape (n_samples,) or None"]}],"sklearn\/feature_selection\/_from_model.py":[{"add":["75","    threshold : string or float, default=None","85","    prefit : bool, default=False","93","    norm_order : non-zero int, inf, -inf, default=1","98","    max_features : int, default=None","104","    importance_getter : str or callable, default='auto'","200","        y : array-like of shape (n_samples,), default=None","244","        y : array-like of shape (n_samples,), default=None"],"delete":["75","    threshold : string, float, optional default None","85","    prefit : bool, default False","93","    norm_order : non-zero int, inf, -inf, default 1","98","    max_features : int or None, optional","104","    importance_getter : str or callable, optional (default='auto')","200","        y : array-like, shape (n_samples,)","244","        y : array-like, shape (n_samples,)"]}],"sklearn\/feature_selection\/_univariate_selection.py":[{"add":["256","    center : bool, default=True","381","    score_func : callable, default=f_classif","389","    percentile : int, default=10","427","    GenericUnivariateSelect: Univariate feature selector with configurable","428","        mode.","467","    score_func : callable, default=f_classif","475","    k : int or \"all\", default=10","510","    SelectPercentile: Select features based on percentile of the highest","511","        scores.","515","    GenericUnivariateSelect: Univariate feature selector with configurable","516","        mode.","556","    score_func : callable, default=f_classif","562","    alpha : float, default=5e-2","591","    SelectPercentile: Select features based on percentile of the highest","592","        scores.","596","    GenericUnivariateSelect: Univariate feature selector with configurable","597","        mode.","620","    score_func : callable, default=f_classif","626","    alpha : float, default=5e-2","659","    SelectPercentile: Select features based on percentile of the highest","660","        scores.","664","    GenericUnivariateSelect: Univariate feature selector with configurable","665","        mode.","691","    score_func : callable, default=f_classif","697","    alpha : float, default=5e-2","724","    SelectPercentile: Select features based on percentile of the highest","725","        scores.","729","    GenericUnivariateSelect: Univariate feature selector with configurable","730","        mode.","756","    score_func : callable, default=f_classif","761","    mode : {'percentile', 'k_best', 'fpr', 'fdr', 'fwe'}, default='percentile'","764","    param : float or int depending on the feature selection mode, default=1e-5","794","    SelectPercentile: Select features based on percentile of the highest","795","        scores."],"delete":["256","    center : True, bool,","381","    score_func : callable","389","    percentile : int, optional, default=10","427","    GenericUnivariateSelect: Univariate feature selector with configurable mode.","466","    score_func : callable","474","    k : int or \"all\", optional, default=10","509","    SelectPercentile: Select features based on percentile of the highest scores.","513","    GenericUnivariateSelect: Univariate feature selector with configurable mode.","553","    score_func : callable","559","    alpha : float, optional","588","    SelectPercentile: Select features based on percentile of the highest scores.","592","    GenericUnivariateSelect: Univariate feature selector with configurable mode.","615","    score_func : callable","621","    alpha : float, optional","654","    SelectPercentile: Select features based on percentile of the highest scores.","658","    GenericUnivariateSelect: Univariate feature selector with configurable mode.","684","    score_func : callable","690","    alpha : float, optional","717","    SelectPercentile: Select features based on percentile of the highest scores.","721","    GenericUnivariateSelect: Univariate feature selector with configurable mode.","747","    score_func : callable","752","    mode : {'percentile', 'k_best', 'fpr', 'fdr', 'fwe'}","755","    param : float or int depending on the feature selection mode","785","    SelectPercentile: Select features based on percentile of the highest scores."]}],"sklearn\/feature_selection\/_mutual_info.py":[{"add":["173","    columns : iterable or None, default=None","204","    y : array-like of shape (n_samples,)","207","    discrete_features : {'auto', bool, array-like}, default='auto'","214","    discrete_target : bool, default=False","217","    n_neighbors : int, default=3","222","    copy : bool, default=True","226","    random_state : int, RandomState instance or None, default=None","315","    y : array-like of shape (n_samples,)","318","    discrete_features : {'auto', bool, array-like}, default='auto'","325","    n_neighbors : int, default=3","330","    copy : bool, default=True","334","    random_state : int, RandomState instance or None, default=None","352","       vice versa will usually give incorrect results, so be attentive about","353","       that.","359","    .. [1] `Mutual Information","360","           <https:\/\/en.wikipedia.org\/wiki\/Mutual_information>`_","395","    y : array-like of shape (n_samples,)","398","    discrete_features : {'auto', bool, array-like}, default='auto'","405","    n_neighbors : int, default=3","410","    copy : bool, default=True","414","    random_state : int, RandomState instance or None, default=None","432","       vice versa will usually give incorrect results, so be attentive about","433","       that.","439","    .. [1] `Mutual Information","440","           <https:\/\/en.wikipedia.org\/wiki\/Mutual_information>`_"],"delete":["173","    columns : iterable or None, default None","204","    y : array-like, shape (n_samples,)","207","    discrete_features : {'auto', bool, array-like}, default 'auto'","214","    discrete_target : bool, default False","217","    n_neighbors : int, default 3","222","    copy : bool, default True","226","    random_state : int, RandomState instance or None, optional, default None","315","    y : array-like, shape (n_samples,)","318","    discrete_features : {'auto', bool, array-like}, default 'auto'","325","    n_neighbors : int, default 3","330","    copy : bool, default True","334","    random_state : int, RandomState instance or None, optional, default None","352","       vice versa will usually give incorrect results, so be attentive about that.","358","    .. [1] `Mutual Information <https:\/\/en.wikipedia.org\/wiki\/Mutual_information>`_","393","    y : array-like, shape (n_samples,)","396","    discrete_features : {'auto', bool, array-like}, default 'auto'","403","    n_neighbors : int, default 3","408","    copy : bool, default True","412","    random_state : int, RandomState instance or None, optional, default None","430","       vice versa will usually give incorrect results, so be attentive about that.","436","    .. [1] `Mutual Information <https:\/\/en.wikipedia.org\/wiki\/Mutual_information>`_"]}],"sklearn\/feature_selection\/_variance_threshold.py":[{"add":["20","    threshold : float, default=0","58","        y : any, default=None"],"delete":["20","    threshold : float, optional","58","        y : any"]}],"sklearn\/feature_selection\/_base.py":[{"add":["34","        indices : bool, default=False"],"delete":["34","        indices : boolean (default False)"]}]}},"549514e4fdc59d925746b5dc9bd9383781923726":{"changes":{"examples\/feature_selection\/plot_feature_selection_pipeline.py":"MODIFY"},"diff":{"examples\/feature_selection\/plot_feature_selection_pipeline.py":[{"add":["13","from sklearn.feature_selection import SelectKBest, f_classif","29","anova_filter = SelectKBest(f_classif, k=3)"],"delete":["13","from sklearn.feature_selection import SelectKBest, f_regression","29","anova_filter = SelectKBest(f_regression, k=3)"]}]}},"e217b68fd00bb7c54b81a492ee6f9db6498517fa":{"changes":{"sklearn\/tree\/_tree.pyx":"MODIFY","doc\/whats_new\/v0.24.rst":"MODIFY"},"diff":{"sklearn\/tree\/_tree.pyx":[{"add":["654","","655","        if (node_ndarray.dtype != NODE_DTYPE):","656","            # possible mismatch of big\/little endian due to serialization","657","            # on a different architecture. Try swapping the byte order.  ","658","            node_ndarray = node_ndarray.byteswap().newbyteorder()","659","            if (node_ndarray.dtype != NODE_DTYPE):","660","                raise ValueError('Did not recognise loaded array dytpe')","661",""],"delete":["655","                node_ndarray.dtype != NODE_DTYPE or"]}],"doc\/whats_new\/v0.24.rst":[{"add":["393","- |Fix| Allow serialized tree based models to be unpickled on a machine","394","  with different endianness.","395","  :pr:`17644` by :user:`Qi Zhang <qzhang90>`.","396",""],"delete":[]}]}},"f2e873f41ee40b6028f19328cbdc73fc15cdb070":{"changes":{"sklearn\/utils\/_mocking.py":"MODIFY","sklearn\/utils\/tests\/test_mocking.py":"ADD"},"diff":{"sklearn\/utils\/_mocking.py":[{"add":["53","    Checks some property of `X` and `y`in fit \/ predict.","59","    check_y, check_X : callable, default=None","60","        The callable used to validate `X` and `y`. These callable should return","61","        a bool where `False` will trigger an `AssertionError`.","62","","63","    check_y_params, check_X_params : dict, default=None","64","        The optional parameters to pass to `check_X` and `check_y`.","65","","66","    foo_param : int, default=0","67","        A `foo` param. When `foo > 1`, the output of :meth:`score` will be 1","68","        otherwise it is 0.","69","","70","    expected_fit_params : list of str, default=None","71","        A list of the expected parameters given when calling `fit`.","75","    classes_ : int","76","        The classes seen during `fit`.","77","","78","    n_features_in_ : int","79","        The number of features seen during `fit`.","81","    def __init__(self, *, check_y=None, check_y_params=None,","82","                 check_X=None, check_X_params=None, foo_param=0,","85","        self.check_y_params = check_y_params","87","        self.check_X_params = check_X_params","92","        \"\"\"Fit classifier.","106","","107","        Returns","108","        -------","109","        self","111","        assert _num_samples(X) == _num_samples(y)","113","            params = {} if self.check_X_params is None else self.check_X_params","114","            assert self.check_X(X, **params)","116","            params = {} if self.check_y_params is None else self.check_y_params","118","        self.n_features_in_ = np.shape(X)[1]","119","        self.classes_ = np.unique(","120","            check_array(y, ensure_2d=False, allow_nd=True)","121","        )","124","            if missing:","125","                raise AssertionError(","126","                    f'Expected fit parameter(s) {list(missing)} not seen.'","127","                )","129","                if _num_samples(value) != _num_samples(X):","130","                    raise AssertionError(","131","                        f'Fit parameter {key} has length {_num_samples(value)}'","132","                        f'; expected {_num_samples(X)}.'","133","                    )","137","    def predict(self, X):","138","        \"\"\"Predict the first class seen in `classes_`.","139","","142","        X : array-like of shape (n_samples, n_features)","143","            The input data.","144","","145","        Returns","146","        -------","147","        preds : ndarray of shape (n_samples,)","148","            Predictions of the first class seens in `classes_`.","151","            params = {} if self.check_X_params is None else self.check_X_params","152","            assert self.check_X(X, **params)","153","        return self.classes_[np.zeros(_num_samples(X), dtype=np.int)]","154","","155","    def predict_proba(self, X):","156","        \"\"\"Predict probabilities for each class.","157","","158","        Here, the dummy classifier will provide a probability of 1 for the","159","        first class of `classes_` and 0 otherwise.","160","","161","        Parameters","162","        ----------","163","        X : array-like of shape (n_samples, n_features)","164","            The input data.","165","","166","        Returns","167","        -------","168","        proba : ndarray of shape (n_samples, n_classes)","169","            The probabilities for each sample and class.","170","        \"\"\"","171","        proba = np.zeros((_num_samples(X), len(self.classes_)))","172","        proba[:, 0] = 1","173","        return proba","174","","175","    def decision_function(self, X):","176","        \"\"\"Confidence score.","177","","178","        Parameters","179","        ----------","180","        X : array-like of shape (n_samples, n_features)","181","            The input data.","182","","183","        Returns","184","        -------","185","        decision : ndarray of shape (n_samples,) if n_classes == 2\\","186","                else (n_samples, n_classes)","187","            Confidence score.","188","        \"\"\"","189","        if len(self.classes_) == 2:","190","            # for binary classifier, the confidence score is related to","191","            # classes_[1] and therefore should be null.","192","            return np.zeros(_num_samples(X))","193","        else:","194","            return self.predict_proba(X)","197","        \"\"\"Fake score.","198","","205","        Y : array-like of shape (n_samples, n_output) or (n_samples,)","208","","209","        Returns","210","        -------","211","        score : float","212","            Either 0 or 1 depending of `foo_param` (i.e. `foo_param > 1 =>","213","            score=1` otherwise `score=0`)."],"delete":["53","    Checks some property of X and y in fit \/ predict.","59","    check_y","60","    check_X","61","    foo_param","62","    expected_fit_params","66","    classes_","68","    def __init__(self, check_y=None, check_X=None, foo_param=0,","76","        \"\"\"","77","        Fit classifier","92","        assert len(X) == len(y)","94","            assert self.check_X(X)","97","        self.n_features_in_ = len(X)","98","        self.classes_ = np.unique(check_array(y, ensure_2d=False,","99","                                              allow_nd=True))","102","            assert len(missing) == 0, 'Expected fit parameter(s) %s not ' \\","103","                                      'seen.' % list(missing)","105","                assert len(value) == len(X), (","106","                        'Fit parameter %s has length %d; '","107","                        'expected %d.'","108","                        % (key, len(value), len(X)))","112","    def predict(self, T):","113","        \"\"\"","116","        T : indexable, length n_samples","119","            assert self.check_X(T)","120","        return self.classes_[np.zeros(_num_samples(T), dtype=np.int)]","123","        \"\"\"","130","        Y : array-like of shape (n_samples, n_output) or (n_samples,), optional"]}],"sklearn\/utils\/tests\/test_mocking.py":[{"add":[],"delete":[]}]}},"28c1ed473f0738e28a6fd61c105da8ada4ab880d":{"changes":{"sklearn\/model_selection\/tests\/test_search.py":"MODIFY","doc\/whats_new\/v0.24.rst":"MODIFY","sklearn\/model_selection\/_search.py":"MODIFY"},"diff":{"sklearn\/model_selection\/tests\/test_search.py":[{"add":["1491","    assert len(ParameterSampler(params, n_iter=1000)) == 8"],"delete":[]}],"doc\/whats_new\/v0.24.rst":[{"add":["332","- |Fix| Fixed the `len` of :class:`model_selection.ParameterSampler` when","333","  all distributions are lists and `n_iter` is more than the number of unique","334","  parameter combinations. :pr:`18222` by `Nicolas Hug`_.","335",""],"delete":[]}],"sklearn\/model_selection\/_search.py":[{"add":["265","    def _is_all_lists(self):","266","        return all(","268","            for dist in self.param_distributions","269","        )","270","","271","    def __iter__(self):","274","        # if all distributions are given as lists, we want to sample without","275","        # replacement","276","        if self._is_all_lists():","308","        if self._is_all_lists():","309","            grid_size = len(ParameterGrid(self.param_distributions))","310","            return min(self.n_iter, grid_size)","311","        else:","312","            return self.n_iter"],"delete":["265","    def __iter__(self):","266","        # check if all distributions are given as lists","267","        # in this case we want to sample without replacement","268","        all_lists = all(","270","            for dist in self.param_distributions)","273","        if all_lists:","305","        return self.n_iter"]}]}},"2592edddb6eb326a65311f081e668d93ab044703":{"changes":{"sklearn\/exceptions.py":"MODIFY"},"diff":{"sklearn\/exceptions.py":[{"add":["100","    ...     print(w[-1].message)","101","    A column-vector y was passed when a 1d array was expected. Please change","102","    the shape of y to (n_samples, ), for example using ravel()."],"delete":["100","    ...     print(repr(w[-1].message))","101","    DataConversionWarning('A column-vector y was passed when a","102","    1d array was expected. Please change the shape of y to","103","    (n_samples, ), for example using ravel().')"]}]}},"ccf5c36503785a0330f460a9ae751ac973980443":{"changes":{"sklearn\/cluster\/_kmeans.py":"MODIFY","sklearn\/cluster\/tests\/test_k_means.py":"MODIFY","doc\/whats_new\/v0.23.rst":"MODIFY"},"diff":{"sklearn\/cluster\/_kmeans.py":[{"add":["179","        sample_weight = sample_weight * scale"],"delete":["179","        sample_weight *= scale"]}],"sklearn\/cluster\/tests\/test_k_means.py":[{"add":["1169","","1170","","1171","def test_sample_weight_unchanged():","1172","    # Check that sample_weight is not modified in place by KMeans (#17204)","1173","    X = np.array([[1], [2], [4]])","1174","    sample_weight = np.array([0.5, 0.2, 0.3])","1175","    KMeans(n_clusters=2, random_state=0).fit(X, sample_weight=sample_weight)","1176","","1177","    # internally, sample_weight is rescale to sum up to n_samples = 3","1178","    assert_array_equal(sample_weight, np.array([0.5, 0.2, 0.3]))"],"delete":[]}],"doc\/whats_new\/v0.23.rst":[{"add":["4",".. _changes_0_23_1:","5","","6","Version 0.23.1","7","==============","8","","9","**TBD**","10","","11","Changelog","12","---------","13","","14",":mod:`sklearn.cluster`","15","......................","16","","17","- |Fix| Fixed a bug in :class:`cluster.KMeans` where the sample weights","18","  provided by the user was modified in place. :pr:`17204` by","19","  :user:`Jeremie du Boisberranger <jeremiedbb>`.","20",""],"delete":[]}]}},"c11aaf5a66452217c4c9ac2811e34637979d7c85":{"changes":{"sklearn\/neighbors\/_quad_tree.pyx":"MODIFY","sklearn\/tree\/_tree.pyx":"MODIFY"},"diff":{"sklearn\/neighbors\/_quad_tree.pyx":[{"add":["32","# Build the corresponding numpy dtype for Cell.","33","# This works by casting `dummy` to an array of Cell of length 1, which numpy","34","# can construct a `dtype`-object for. See https:\/\/stackoverflow.com\/q\/62448946","35","# for a more detailed explanation.","36","cdef Cell dummy;","37","CELL_DTYPE = np.asarray(<Cell[:1]>(&dummy)).dtype"],"delete":["32","","33","# Repeat struct definition for numpy","34","CELL_DTYPE = np.dtype({","35","    'names': ['parent', 'children', 'cell_id', 'point_index', 'is_leaf',","36","              'max_width', 'depth', 'cumulative_size', 'center', 'barycenter',","37","              'min_bounds', 'max_bounds'],","38","    'formats': [np.intp, (np.intp, 8), np.intp, np.intp, np.int32, np.float32,","39","                np.intp, np.intp, (np.float32, 3), (np.float32, 3),","40","                (np.float32, 3), (np.float32, 3)],","41","    'offsets': [","42","        <Py_ssize_t> &(<Cell*> NULL).parent,","43","        <Py_ssize_t> &(<Cell*> NULL).children,","44","        <Py_ssize_t> &(<Cell*> NULL).cell_id,","45","        <Py_ssize_t> &(<Cell*> NULL).point_index,","46","        <Py_ssize_t> &(<Cell*> NULL).is_leaf,","47","        <Py_ssize_t> &(<Cell*> NULL).squared_max_width,","48","        <Py_ssize_t> &(<Cell*> NULL).depth,","49","        <Py_ssize_t> &(<Cell*> NULL).cumulative_size,","50","        <Py_ssize_t> &(<Cell*> NULL).center,","51","        <Py_ssize_t> &(<Cell*> NULL).barycenter,","52","        <Py_ssize_t> &(<Cell*> NULL).min_bounds,","53","        <Py_ssize_t> &(<Cell*> NULL).max_bounds,","54","    ]","55","})"]}],"sklearn\/tree\/_tree.pyx":[{"add":["70","# Build the corresponding numpy dtype for Node.","71","# This works by casting `dummy` to an array of Node of length 1, which numpy","72","# can construct a `dtype`-object for. See https:\/\/stackoverflow.com\/q\/62448946","73","# for a more detailed explanation.","74","cdef Node dummy;","75","NODE_DTYPE = np.asarray(<Node[:1]>(&dummy)).dtype"],"delete":["70","# Repeat struct definition for numpy","71","NODE_DTYPE = np.dtype({","72","    'names': ['left_child', 'right_child', 'feature', 'threshold', 'impurity',","73","              'n_node_samples', 'weighted_n_node_samples'],","74","    'formats': [np.intp, np.intp, np.intp, np.float64, np.float64, np.intp,","75","                np.float64],","76","    'offsets': [","77","        <Py_ssize_t> &(<Node*> NULL).left_child,","78","        <Py_ssize_t> &(<Node*> NULL).right_child,","79","        <Py_ssize_t> &(<Node*> NULL).feature,","80","        <Py_ssize_t> &(<Node*> NULL).threshold,","81","        <Py_ssize_t> &(<Node*> NULL).impurity,","82","        <Py_ssize_t> &(<Node*> NULL).n_node_samples,","83","        <Py_ssize_t> &(<Node*> NULL).weighted_n_node_samples","84","    ]","85","})"]}]}},"120f07bfb6df6ae82951c39b78d86f2abeb85ee4":{"changes":{"sklearn\/cluster\/_kmeans.py":"MODIFY"},"diff":{"sklearn\/cluster\/_kmeans.py":[{"add":["328","    \"\"\"A single run of k-means elkan, assumes preparation completed prior."],"delete":["328","    \"\"\"A single run of k-means lloyd, assumes preparation completed prior."]}]}},"df61e9ed98b0777cc0962be6e2d161f4c30110fd":{"changes":{"sklearn\/ensemble\/_hist_gradient_boosting\/gradient_boosting.py":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_gradient_boosting.py":"MODIFY","doc\/whats_new\/v0.24.rst":"MODIFY"},"diff":{"sklearn\/ensemble\/_hist_gradient_boosting\/gradient_boosting.py":[{"add":["647","        is_binned = getattr(self, '_in_fit', False)","648","        dtype = X_BINNED_DTYPE if is_binned else X_DTYPE","649","        X = check_array(X, dtype=dtype, force_all_finite=False)"],"delete":["647","        X = check_array(X, dtype=[X_DTYPE, X_BINNED_DTYPE],","648","                        force_all_finite=False)","655","        is_binned = getattr(self, '_in_fit', False)"]}],"sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_gradient_boosting.py":[{"add":["796","","797","","798","@pytest.mark.parametrize('Est', (HistGradientBoostingClassifier,","799","                                 HistGradientBoostingRegressor))","800","def test_uint8_predict(Est):","801","    # Non regression test for","802","    # https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/18408","803","    # Make sure X can be of dtype uint8 (i.e. X_BINNED_DTYPE) in predict. It","804","    # will be converted to X_DTYPE.","805","","806","    rng = np.random.RandomState(0)","807","","808","    X = rng.randint(0, 100, size=(10, 2)).astype(np.uint8)","809","    y = rng.randint(0, 2, size=10).astype(np.uint8)","810","    est = Est()","811","    est.fit(X, y)","812","    est.predict(X)"],"delete":[]}],"doc\/whats_new\/v0.24.rst":[{"add":["220","- |Fix|: Fixed a bug in","221","  :class:`ensemble.HistGradientBoostingRegressor` and","222","  :class:`ensemble.HistGradientBoostingClassifier` which can now accept data","223","  with `uint8` dtype in `predict`. :pr:`18410` by `Nicolas Hug`_.","224",""],"delete":[]}]}},"138dd7b88f1634447f838bc58088e594ffaf5549":{"changes":{"sklearn\/utils\/tests\/test_validation.py":"MODIFY"},"diff":{"sklearn\/utils\/tests\/test_validation.py":[{"add":["1254","    \"ntype1, ntype2, expected_subtype\",","1256","        (\"longfloat\", \"longdouble\", np.floating),","1257","        (\"float16\", \"half\", np.floating),","1258","        (\"single\", \"float32\", np.floating),","1259","        (\"double\", \"float64\", np.floating),","1260","        (\"int8\", \"byte\", np.integer),","1261","        (\"short\", \"int16\", np.integer),","1262","        (\"intc\", \"int32\", np.integer),","1263","        (\"int0\", \"long\", np.integer),","1264","        (\"int\", \"long\", np.integer),","1265","        (\"int64\", \"longlong\", np.integer),","1266","        (\"int_\", \"intp\", np.integer),","1267","        (\"ubyte\", \"uint8\", np.unsignedinteger),","1268","        (\"uint16\", \"ushort\", np.unsignedinteger),","1269","        (\"uintc\", \"uint32\", np.unsignedinteger),","1270","        (\"uint\", \"uint64\", np.unsignedinteger),","1271","        (\"uintp\", \"ulonglong\", np.unsignedinteger)","1274","def test_check_pandas_sparse_valid(ntype1, ntype2, expected_subtype):","1283","    assert np.issubdtype(arr.dtype, expected_subtype)"],"delete":["1254","    \"ntype1, ntype2, expected_dtype\",","1256","        (\"longfloat\", \"longdouble\", \"float128\"),","1257","        (\"float16\", \"half\", \"float16\"),","1258","        (\"single\", \"float32\", \"float32\"),","1259","        (\"double\", \"float64\", \"float64\"),","1260","        (\"int8\", \"byte\", \"int8\"),","1261","        (\"short\", \"int16\", \"int16\"),","1262","        (\"intc\", \"int32\", \"int32\"),","1263","        (\"int0\", \"long\", \"int64\"),","1264","        (\"int\", \"long\", \"int64\"),","1265","        (\"int64\", \"longlong\", \"int64\"),","1266","        (\"int_\", \"intp\", \"int64\"),","1267","        (\"ubyte\", \"uint8\", \"uint8\"),","1268","        (\"uint16\", \"ushort\", \"uint16\"),","1269","        (\"uintc\", \"uint32\", \"uint32\"),","1270","        (\"uint\", \"uint64\", \"uint64\"),","1271","        (\"uintp\", \"ulonglong\", \"uint64\"),","1274","def test_check_pandas_sparse_valid(ntype1, ntype2, expected_dtype):","1283","    assert arr.dtype.name == expected_dtype"]}]}},"82748208a162930f877edb687ab01c7703283fd3":{"changes":{"sklearn\/__init__.py":"MODIFY"},"diff":{"sklearn\/__init__.py":[{"add":["17","import random","18","","101",""],"delete":["99","    import os","101","    import random"]}]}},"ca0065d3bf6f7577c64a10c0ef5a18f4c2e74cf5":{"changes":{"sklearn\/utils\/extmath.py":"MODIFY","sklearn\/utils\/tests\/test_extmath.py":"MODIFY"},"diff":{"sklearn\/utils\/extmath.py":[{"add":["762","    new_mean = np.average(X_0,","763","                          weights=sample_weight, axis=0).astype(np.float64)"],"delete":["762","    new_mean = \\","763","        _safe_accumulator_op(np.average, X_0, weights=sample_weight, axis=0)"]}],"sklearn\/utils\/tests\/test_extmath.py":[{"add":["461","@pytest.mark.parametrize(\"dtype\", [np.float32, np.float64])","462","def test_incremental_weighted_mean_and_variance_simple(rng, dtype):","464","    X = rng.rand(1000, 20).astype(dtype)*mult","516","@pytest.mark.parametrize(\"dtype\", [np.float32, np.float64])","517","def test_incremental_weighted_mean_and_variance_ignore_nan(dtype):","526","                  [300, 300, 300, 300]]).astype(dtype)","531","                      [300, 300, 300, np.nan]]).astype(dtype)"],"delete":["461","def test_incremental_weighted_mean_and_variance_simple(rng):","463","    X = rng.rand(1000, 20)*mult","515","def test_incremental_weighted_mean_and_variance_ignore_nan():","524","                  [300, 300, 300, 300]])","529","                      [300, 300, 300, np.nan]])"]}]}},"854f6493075ff9cdab0c96522282f5c3979bf16b":{"changes":{"sklearn\/metrics\/_classification.py":"MODIFY","doc\/modules\/model_evaluation.rst":"MODIFY"},"diff":{"sklearn\/metrics\/_classification.py":[{"add":["2386","    The Brier score measures the mean squared difference between the predicted","2387","    probability and the actual outcome. The Brier score always","2391","    of only 0 and 1). It can be decomposed is the sum of refinement loss and","2393","","2399","    label is controlled via the parameter `pos_label`, which defaults to","2400","    the greater label unless `y_true` is all 0 or all -1, in which case","2401","    `pos_label` defaults to 1.","2402","","2403","    Read more in the :ref:`User Guide <brier_score_loss>`."],"delete":["2386","    Across all items in a set N predictions, the Brier score measures the","2387","    mean squared difference between (1) the predicted probability assigned","2388","    to the possible outcomes for item i, and (2) the actual outcome.","2389","    Therefore, the lower the Brier score is for a set of predictions, the","2390","    better the predictions are calibrated. Note that the Brier score always","2394","    of only 0 and 1). The Brier loss is composed of refinement loss and","2401","    label is controlled via the parameter pos_label, which defaults to 1.","2402","    Read more in the :ref:`User Guide <calibration>`."]}],"doc\/modules\/model_evaluation.rst":[{"add":["1507","the actual outcome can be a value between 0 and 1 [Brier1950]_.","1509","The Brier score loss is also between 0 to 1 and the lower the score (the mean","1538","The Brier score can be used to assess how well a classifier is calibrated","1539","however, a lower Brier score does not always mean a better calibration. This is","1540","because the Brier score can be decomposed as the sum of calibration loss and","1541","refinement loss [Bella2012]_. Calibration loss is defined as the mean squared","1542","deviation from empirical probabilities derived from the slope of ROC segments.","1543","Refinement loss can be defined as the expected optimal loss as measured by the","1544","area under the optimal cost curve. Refinement loss can change independently","1545","from calibration loss, thus a lower Brier score does not necessarily mean a","1546","better calibrated model. \"Only when refinement loss remains the same does a","1547","lower Brier score always mean better calibration\" [Bella2012]_, [Flach2008]_.","1557","  .. [Brier1950] G. Brier, `Verification of forecasts expressed in terms of","1558","    probability","1562","  .. [Bella2012] Bella, Ferri, Hern¨¢ndez-Orallo, and Ram¨ªrez-Quintana","1563","    `\"Calibration of Machine Learning Models\"","1564","    <http:\/\/dmip.webs.upv.es\/papers\/BFHRHandbook2010.pdf>`_","1565","    in Khosrow-Pour, M. \"Machine learning: concepts, methodologies, tools","1566","    and applications.\" Hershey, PA: Information Science Reference (2012).","1567","","1568","  .. [Flach2008] Flach, Peter, and Edson Matsubara. `\"On classification, ranking,","1569","    and probability estimation.\" <https:\/\/drops.dagstuhl.de\/opus\/volltexte\/2008\/1382\/>`_","1570","    Dagstuhl Seminar Proceedings. Schloss Dagstuhl-Leibniz-Zentrum fr Informatik (2008).","1571",""],"delete":["1507","the actual outcome can be a value between 0 and 1.","1509","The brier score loss is also between 0 to 1 and the lower the score (the mean","1547","  * G. Brier, `Verification of forecasts expressed in terms of probability"]}]}},"9901d8df131e06d8f6ba1677e10330cabfdeb245":{"changes":{"sklearn\/utils\/validation.py":"MODIFY","sklearn\/utils\/tests\/test_validation.py":"MODIFY"},"diff":{"sklearn\/utils\/validation.py":[{"add":["1303","        kwargs.update({k: arg for k, arg in zip(sig.parameters, args)})"],"delete":["1303","        kwargs.update({k: arg for k, arg in zip(all_args, args)})"]}],"sklearn\/utils\/tests\/test_validation.py":[{"add":["1099","    # The * is place before a keyword only argument without a default value","1100","    @_deprecate_positional_args","1101","    def f3(a, *, b, c=1, d=1):","1102","        pass","1103","","1104","    with pytest.warns(FutureWarning,","1105","                      match=r\"Pass b=2 as keyword args\"):","1106","        f3(1, 2)","1107",""],"delete":[]}]}},"b0c03d128007bd264e0e2b82e14c123a51f22308":{"changes":{"sklearn\/decomposition\/_dict_learning.py":"MODIFY","sklearn\/decomposition\/tests\/test_dict_learning.py":"MODIFY","sklearn\/datasets\/tests\/test_base.py":"MODIFY"},"diff":{"sklearn\/decomposition\/_dict_learning.py":[{"add":["16","from ..utils import deprecated","882","class _BaseSparseCoding(TransformerMixin):","883","    \"\"\"Base class from SparseCoder and DictionaryLearning algorithms.\"\"\"","884","    def __init__(self, transform_algorithm, transform_n_nonzero_coefs,","885","                 transform_alpha, split_sign, n_jobs, positive_code,","886","                 transform_max_iter):","895","    def _transform(self, X, dictionary):","896","        \"\"\"Private method allowing to accomodate both DictionaryLearning and","897","        SparseCoder.\"\"\"","898","        X = check_array(X)","899","","900","        code = sparse_encode(","901","            X, dictionary, algorithm=self.transform_algorithm,","902","            n_nonzero_coefs=self.transform_n_nonzero_coefs,","903","            alpha=self.transform_alpha, max_iter=self.transform_max_iter,","904","            n_jobs=self.n_jobs, positive=self.positive_code)","905","","906","        if self.split_sign:","907","            # feature vector is split into a positive and negative side","908","            n_samples, n_features = code.shape","909","            split_code = np.empty((n_samples, 2 * n_features))","910","            split_code[:, :n_features] = np.maximum(code, 0)","911","            split_code[:, n_features:] = -np.minimum(code, 0)","912","            code = split_code","913","","914","        return code","915","","934","        return self._transform(X, self.components_)","937","class SparseCoder(_BaseSparseCoding, BaseEstimator):","957","            'threshold'}, default='omp'","969","    transform_n_nonzero_coefs : int, default=0.1 * n_features","988","    n_jobs : int, default=None","1008","        The unchanged dictionary atoms.","1009","","1010","        .. deprecated:: 0.24","1011","           This attribute is deprecated in 0.24 and will be removed in 0.26.","1012","           Use `dictionary` instead.","1050","        super().__init__(","1051","            transform_algorithm, transform_n_nonzero_coefs,","1052","            transform_alpha, split_sign, n_jobs, positive_code,","1053","            transform_max_iter","1054","        )","1055","        self.dictionary = dictionary","1058","        \"\"\"Do nothing and return the estimator unchanged.","1075","    @deprecated(\"The attribute 'components_' is deprecated \"  # type: ignore","1076","                \"in 0.24 and will be removed in 0.26. Use the \"","1077","                \"'dictionary' instead.\")","1078","    @property","1079","    def components_(self):","1080","        return self.dictionary","1081","","1082","    def transform(self, X, y=None):","1083","        \"\"\"Encode the data as a sparse combination of the dictionary atoms.","1084","","1085","        Coding method is determined by the object parameter","1086","        `transform_algorithm`.","1087","","1088","        Parameters","1089","        ----------","1090","        X : array of shape (n_samples, n_features)","1091","            Test data to be transformed, must have the same number of","1092","            features as the data used to train the model.","1093","","1094","        Returns","1095","        -------","1096","        X_new : array of shape (n_samples, n_components)","1097","            Transformed data","1098","        \"\"\"","1099","        return super()._transform(X, self.dictionary)","1100","","1101","    def _more_tags(self):","1102","        return {\"requires_fit\": False}","1103","","1104","    @property","1105","    def n_components_(self):","1106","        return self.dictionary.shape[0]","1107","","1110","        return self.dictionary.shape[1]","1113","class DictionaryLearning(_BaseSparseCoding, BaseEstimator):","1284","        super().__init__(","1285","            transform_algorithm, transform_n_nonzero_coefs,","1286","            transform_alpha, split_sign, n_jobs, positive_code,","1287","            transform_max_iter","1288","        )","1289","        self.n_components = n_components","1341","class MiniBatchDictionaryLearning(_BaseSparseCoding, BaseEstimator):","1519","        super().__init__(","1520","            transform_algorithm, transform_n_nonzero_coefs, transform_alpha,","1521","            split_sign, n_jobs, positive_code, transform_max_iter","1522","        )","1523","        self.n_components = n_components"],"delete":["881","class SparseCodingMixin(TransformerMixin):","882","    \"\"\"Sparse coding mixin\"\"\"","883","","884","    def _set_sparse_coding_params(self, n_components,","885","                                  transform_algorithm='omp',","886","                                  transform_n_nonzero_coefs=None,","887","                                  transform_alpha=None, split_sign=False,","888","                                  n_jobs=None, positive_code=False,","889","                                  transform_max_iter=1000):","890","        self.n_components = n_components","915","","918","","919","        X = check_array(X)","920","","921","        code = sparse_encode(","922","            X, self.components_, algorithm=self.transform_algorithm,","923","            n_nonzero_coefs=self.transform_n_nonzero_coefs,","924","            alpha=self.transform_alpha, max_iter=self.transform_max_iter,","925","            n_jobs=self.n_jobs, positive=self.positive_code)","926","","927","        if self.split_sign:","928","            # feature vector is split into a positive and negative side","929","            n_samples, n_features = code.shape","930","            split_code = np.empty((n_samples, 2 * n_features))","931","            split_code[:, :n_features] = np.maximum(code, 0)","932","            split_code[:, n_features:] = -np.minimum(code, 0)","933","            code = split_code","934","","935","        return code","938","class SparseCoder(SparseCodingMixin, BaseEstimator):","958","    'threshold'}, default='omp'","970","    transform_n_nonzero_coefs : int, default=0.1*n_features","989","    n_jobs : int or None, default=None","1009","        The unchanged dictionary atoms","1047","        self._set_sparse_coding_params(dictionary.shape[0],","1048","                                       transform_algorithm,","1049","                                       transform_n_nonzero_coefs,","1050","                                       transform_alpha, split_sign, n_jobs,","1051","                                       positive_code, transform_max_iter)","1052","        self.components_ = dictionary","1055","        \"\"\"Do nothing and return the estimator unchanged","1069","            Returns the object itself","1075","        return self.components_.shape[1]","1078","class DictionaryLearning(SparseCodingMixin, BaseEstimator):","1249","        self._set_sparse_coding_params(n_components, transform_algorithm,","1250","                                       transform_n_nonzero_coefs,","1251","                                       transform_alpha, split_sign, n_jobs,","1252","                                       positive_code, transform_max_iter)","1304","class MiniBatchDictionaryLearning(SparseCodingMixin, BaseEstimator):","1482","        self._set_sparse_coding_params(n_components, transform_algorithm,","1483","                                       transform_n_nonzero_coefs,","1484","                                       transform_alpha, split_sign, n_jobs,","1485","                                       positive_code, transform_max_iter)"]}],"sklearn\/decomposition\/tests\/test_dict_learning.py":[{"add":["3","from functools import partial","6","from sklearn.base import clone","7","","23","from sklearn.utils.estimator_checks import check_transformer_data_not_an_array","24","from sklearn.utils.estimator_checks import check_transformer_general","25","from sklearn.utils.estimator_checks import check_transformers_unfitted","500","    coder = SparseCoder(dictionary=V, transform_algorithm='lasso_lars',","501","                        transform_alpha=0.001).transform(X)","502","    assert not np.all(coder == 0)","503","    assert np.sqrt(np.sum((np.dot(coder, V) - X) ** 2)) < 0.1","504","","505","","506","def test_sparse_coder_estimator_clone():","507","    n_components = 12","508","    rng = np.random.RandomState(0)","509","    V = rng.randn(n_components, n_features)  # random init","510","    V \/= np.sum(V ** 2, axis=1)[:, np.newaxis]","511","    coder = SparseCoder(dictionary=V, transform_algorithm='lasso_lars',","512","                        transform_alpha=0.001)","513","    cloned = clone(coder)","514","    assert id(cloned) != id(coder)","515","    np.testing.assert_allclose(cloned.dictionary, coder.dictionary)","516","    assert id(cloned.dictionary) != id(coder.dictionary)","517","    assert cloned.n_components_ == coder.n_components_","518","    assert cloned.n_features_in_ == coder.n_features_in_","519","    data = np.random.rand(n_samples, n_features).astype(np.float32)","520","    np.testing.assert_allclose(cloned.transform(data),","521","                               coder.transform(data))","543","def test_sparse_coder_common_transformer():","544","    rng = np.random.RandomState(777)","545","    n_components, n_features = 40, 3","546","    init_dict = rng.rand(n_components, n_features)","547","","548","    sc = SparseCoder(init_dict)","549","","550","    check_transformer_data_not_an_array(sc.__class__.__name__, sc)","551","    check_transformer_general(sc.__class__.__name__, sc)","552","    check_transformer_general_memmap = partial(","553","        check_transformer_general, readonly_memmap=True","554","    )","555","    check_transformer_general_memmap(sc.__class__.__name__, sc)","556","    check_transformers_unfitted(sc.__class__.__name__, sc)","557","","558","","559","# TODO: remove in 0.26","560","def test_sparse_coder_deprecation():","561","    # check that we raise a deprecation warning when accessing `components_`","562","    rng = np.random.RandomState(777)","563","    n_components, n_features = 40, 64","564","    init_dict = rng.rand(n_components, n_features)","565","    sc = SparseCoder(init_dict)","566","","567","    with pytest.warns(FutureWarning, match=\"'components_' is deprecated\"):","568","        sc.components_","569","","570",""],"delete":["494","    code = SparseCoder(dictionary=V, transform_algorithm='lasso_lars',","495","                       transform_alpha=0.001).transform(X)","496","    assert not np.all(code == 0)","497","    assert np.sqrt(np.sum((np.dot(code, V) - X) ** 2)) < 0.1"]}],"sklearn\/datasets\/tests\/test_base.py":[{"add":[],"delete":["4","import numpy"]}]}},"068c4e6e00da008dbd25dc279360bde01f7c0760":{"changes":{"sklearn\/ensemble\/_gb_losses.py":"MODIFY","sklearn\/ensemble\/tests\/test_gradient_boosting_loss_functions.py":"MODIFY","doc\/whats_new\/v0.24.rst":"MODIFY"},"diff":{"sklearn\/ensemble\/_gb_losses.py":[{"add":["712","        return np.average(","713","            -1 * (Y * raw_predictions).sum(axis=1) +","714","            logsumexp(raw_predictions, axis=1),","715","            weights=sample_weight","716","        )"],"delete":["712","        if sample_weight is None:","713","            return np.sum(-1 * (Y * raw_predictions).sum(axis=1) +","714","                          logsumexp(raw_predictions, axis=1))","715","        else:","716","            return np.sum(","717","                -1 * sample_weight * (Y * raw_predictions).sum(axis=1) +","718","                logsumexp(raw_predictions, axis=1))"]}],"sklearn\/ensemble\/tests\/test_gradient_boosting_loss_functions.py":[{"add":["28","            bd(np.array([1.0]), np.array([0.0])))","37","    def alt_dev(y, pred):","38","        return np.mean(np.logaddexp(0.0, -2.0 * (2.0 * y - 1) * pred))","39","","51","    def alt_ng(y, pred):","52","        return (2 * y - 1) \/ (1 + np.exp(2 * (2 * y - 1) * pred))","53","","150","@pytest.mark.parametrize(","151","    'n_classes, n_samples', [(3, 100), (5, 57), (7, 13)]","152",")","153","def test_multinomial_deviance(n_classes, n_samples):","154","    # Check multinomial deviance with and without sample weights.","155","    rng = np.random.RandomState(13)","156","    sample_weight = np.ones(n_samples)","157","    y_true = rng.randint(0, n_classes, size=n_samples)","158","    y_pred = np.zeros((n_samples, n_classes), dtype=np.float64)","159","    for klass in range(y_pred.shape[1]):","160","        y_pred[:, klass] = y_true == klass","161","","162","    loss = MultinomialDeviance(n_classes)","163","    loss_wo_sw = loss(y_true, y_pred)","164","    assert loss_wo_sw > 0","165","    loss_w_sw = loss(y_true, y_pred, sample_weight=sample_weight)","166","    assert loss_wo_sw == pytest.approx(loss_w_sw)","167","","168","    # Multinomial deviance uses weighted average loss rather than","169","    # weighted sum loss, so we make sure that the value remains the same","170","    # when we device the weight by 2.","171","    loss_w_sw = loss(y_true, y_pred, sample_weight=0.5 * sample_weight)","172","    assert loss_wo_sw == pytest.approx(loss_w_sw)","173","","174","","175","def test_mdl_computation_weighted():","176","    raw_predictions = np.array([[1., -1., -.1], [-2., 1., 2.]])","177","    y_true = np.array([0, 1])","178","    weights = np.array([1, 3])","179","    expected_loss = 1.0909323","180","    # MultinomialDeviance loss computation with weights.","181","    loss = MultinomialDeviance(3)","182","    assert (loss(y_true, raw_predictions, weights)","183","            == pytest.approx(expected_loss))","184","","185","","186","@pytest.mark.parametrize('n', [0, 1, 2])","187","def test_mdl_exception(n):","188","    # Check that MultinomialDeviance throws an exception when n_classes <= 2","189","    err_msg = 'MultinomialDeviance requires more than 2 classes.'","190","    with pytest.raises(ValueError, match=err_msg):","191","        MultinomialDeviance(n)","192","","193",""],"delete":["28","                 bd(np.array([1.0]), np.array([0.0])))","37","    alt_dev = lambda y, pred: np.mean(np.logaddexp(0.0, -2.0 *","38","                                                   (2.0 * y - 1) * pred))","50","    alt_ng = lambda y, pred: (2 * y - 1) \/ (1 + np.exp(2 * (2 * y - 1) * pred))"]}],"doc\/whats_new\/v0.24.rst":[{"add":["82","- |Fix| Fixed bug in :class:`ensemble.MultinomialDeviance` where the","83","  average of logloss was incorrectly calculated as sum of logloss.","84","  :pr:`17694` by :user:`Markus Rempfler <rempfler>` and","85","  :user:`Tsutomu Kusanagi <t-kusanagi2>`.","86",""],"delete":[]}]}},"2b655efaf26f7802e4e41f2e64e1b9abdcaa6cd2":{"changes":{"sklearn\/compose\/tests\/test_column_transformer.py":"MODIFY","sklearn\/compose\/_column_transformer.py":"MODIFY","doc\/whats_new\/v0.24.rst":"MODIFY"},"diff":{"sklearn\/compose\/tests\/test_column_transformer.py":[{"add":["1383","","1384","","1385","@pytest.mark.parametrize('remainder', [\"passthrough\", StandardScaler()])","1386","def test_sk_visual_block_remainder(remainder):","1387","    # remainder='passthrough' or an estimator will be shown in repr_html","1388","    ohe = OneHotEncoder()","1389","    ct = ColumnTransformer(transformers=[('ohe', ohe, [\"col1\", \"col2\"])],","1390","                           remainder=remainder)","1391","    visual_block = ct._sk_visual_block_()","1392","    assert visual_block.names == ('ohe', 'remainder')","1393","    assert visual_block.name_details == (['col1', 'col2'], '')","1394","    assert visual_block.estimators == (ohe, remainder)","1395","","1396","","1397","def test_sk_visual_block_remainder_drop():","1398","    # remainder='drop' is not shown in repr_html","1399","    ohe = OneHotEncoder()","1400","    ct = ColumnTransformer(transformers=[('ohe', ohe, [\"col1\", \"col2\"])])","1401","    visual_block = ct._sk_visual_block_()","1402","    assert visual_block.names == ('ohe',)","1403","    assert visual_block.name_details == (['col1', 'col2'],)","1404","    assert visual_block.estimators == (ohe,)","1405","","1406","","1407","@pytest.mark.parametrize('remainder', [\"passthrough\", StandardScaler()])","1408","def test_sk_visual_block_remainder_fitted_pandas(remainder):","1409","    # Remainder shows the columns after fitting","1410","    pd = pytest.importorskip('pandas')","1411","    ohe = OneHotEncoder()","1412","    ct = ColumnTransformer(transformers=[('ohe', ohe, [\"col1\", \"col2\"])],","1413","                           remainder=remainder)","1414","    df = pd.DataFrame({\"col1\": [\"a\", \"b\", \"c\"], \"col2\": [\"z\", \"z\", \"z\"],","1415","                       \"col3\": [1, 2, 3], \"col4\": [3, 4, 5]})","1416","    ct.fit(df)","1417","    visual_block = ct._sk_visual_block_()","1418","    assert visual_block.names == ('ohe', 'remainder')","1419","    assert visual_block.name_details == (['col1', 'col2'], ['col3', 'col4'])","1420","    assert visual_block.estimators == (ohe, remainder)","1421","","1422","","1423","@pytest.mark.parametrize('remainder', [\"passthrough\", StandardScaler()])","1424","def test_sk_visual_block_remainder_fitted_numpy(remainder):","1425","    # Remainder shows the indices after fitting","1426","    X = np.array([[1, 2, 3], [4, 5, 6]], dtype=float)","1427","    scaler = StandardScaler()","1428","    ct = ColumnTransformer(transformers=[('scale', scaler, [0, 2])],","1429","                           remainder=remainder)","1430","    ct.fit(X)","1431","    visual_block = ct._sk_visual_block_()","1432","    assert visual_block.names == ('scale', 'remainder')","1433","    assert visual_block.name_details == ([0, 2], [1])","1434","    assert visual_block.estimators == (scaler, remainder)"],"delete":[]}],"sklearn\/compose\/_column_transformer.py":[{"add":["641","        if isinstance(self.remainder, str) and self.remainder == 'drop':","642","            transformers = self.transformers","643","        elif hasattr(self, \"_remainder\"):","644","            remainder_columns = self._remainder[2]","645","            if hasattr(self, '_df_columns'):","646","                remainder_columns = (","647","                    self._df_columns[remainder_columns].tolist()","648","                )","649","            transformers = chain(self.transformers,","650","                                 [('remainder', self.remainder,","651","                                   remainder_columns)])","652","        else:","653","            transformers = chain(self.transformers,","654","                                 [('remainder', self.remainder, '')])","655","","656","        names, transformers, name_details = zip(*transformers)"],"delete":["641","        names, transformers, name_details = zip(*self.transformers)"]}],"doc\/whats_new\/v0.24.rst":[{"add":["88","- |FIX| :class:`compose.ColumnTransformer` now displays the remainder in the","89","  diagram display. :pr:`18167` by `Thomas Fan`_.","90",""],"delete":[]}]}},"ac6caa050bb7ce04a2b648e86e31f3de0af7dd35":{"changes":{"sklearn\/tests\/test_discriminant_analysis.py":"MODIFY","sklearn\/discriminant_analysis.py":"MODIFY","sklearn\/cluster\/tests\/test_k_means.py":"MODIFY","sklearn\/neighbors\/_base.py":"MODIFY","sklearn\/cluster\/tests\/test_dbscan.py":"MODIFY"},"diff":{"sklearn\/tests\/test_discriminant_analysis.py":[{"add":["76","                        rtol=1e-6, atol=1e-6, err_msg='solver %s' % solver)"],"delete":["76","                        rtol=1e-6, err_msg='solver %s' % solver)"]}],"sklearn\/discriminant_analysis.py":[{"add":["539","        prediction = self.predict_proba(X)","540","        prediction[prediction == 0.0] += np.finfo(prediction.dtype).tiny","541","        return np.log(prediction)"],"delete":["539","        return np.log(self.predict_proba(X))"]}],"sklearn\/cluster\/tests\/test_k_means.py":[{"add":["468","    n_init = 10 if type(init) is str else 1","470","                                 random_state=42, n_init=n_init)","675","    n_init = 10 if type(init) is str else 1","677","                        n_init=n_init, random_state=0).fit(data)","695","    n_init = 10 if type(init) is str else 1","697","                                 n_init=n_init, random_state=0).fit(X_csr)","951","                         random_state=42, n_init=1),"],"delete":["469","                                 random_state=42, n_init=10)","675","                        n_init=10, random_state=0).fit(data)","694","                                 n_init=10, random_state=0).fit(X_csr)","948","                         random_state=42),"]}],"sklearn\/neighbors\/_base.py":[{"add":["338","            if self.p is not None:","339","                warnings.warn(\"Parameter p is found in metric_params. \"","340","                              \"The corresponding parameter from __init__ \"","341","                              \"is ignored.\", SyntaxWarning, stacklevel=3)"],"delete":["338","            warnings.warn(\"Parameter p is found in metric_params. \"","339","                          \"The corresponding parameter from __init__ \"","340","                          \"is ignored.\", SyntaxWarning, stacklevel=3)"]}],"sklearn\/cluster\/tests\/test_dbscan.py":[{"add":["8","import warnings","9","","176","","177","    with warnings.catch_warnings(record=True) as warns:","178","        db = DBSCAN(","179","            metric='minkowski', metric_params={'p': p}, eps=eps,","180","            p=None, min_samples=min_samples, algorithm='ball_tree'","181","            ).fit(X)","182","    assert not warns","201","    with pytest.warns(","202","        SyntaxWarning,","203","        match=\"Parameter p is found in metric_params. \"","204","              \"The corresponding parameter from __init__ \"","205","              \"is ignored.\"):","206","        # Test that checks p is ignored in favor of metric_params={'p': <val>}","207","        db = DBSCAN(metric='minkowski', metric_params={'p': p}, eps=eps, p=p+1,","208","                    min_samples=min_samples, algorithm='ball_tree').fit(X)","209","        core_sample_4, labels_4 = db.core_sample_indices_, db.labels_","210","","211","    assert_array_equal(core_sample_1, core_sample_4)","212","    assert_array_equal(labels_1, labels_4)","213",""],"delete":["174","    db = DBSCAN(metric='minkowski', metric_params={'p': p}, eps=eps,","175","                min_samples=min_samples, algorithm='ball_tree').fit(X)"]}]}},"f734e11a53d3f9b983529fc22b877ef041e35cda":{"changes":{"sklearn\/decomposition\/tests\/test_nmf.py":"MODIFY","doc\/modules\/decomposition.rst":"MODIFY","sklearn\/decomposition\/_nmf.py":"MODIFY","doc\/whats_new\/v0.24.rst":"MODIFY"},"diff":{"sklearn\/decomposition\/tests\/test_nmf.py":[{"add":["22","@pytest.mark.parametrize('regularization',","23","                         [None, 'both', 'components', 'transformation'])","24","def test_convergence_warning(solver, regularization):","29","        NMF(solver=solver, regularization=regularization, max_iter=1).fit(A)","48","    msg = \"Invalid regularization parameter: got 'spam' instead of one of\"","49","    assert_raise_message(ValueError, msg, NMF(regularization=name).fit, A)","103","@pytest.mark.parametrize('solver', ('cd', 'mu'))","104","@pytest.mark.parametrize('init',","105","                         (None, 'nndsvd', 'nndsvda', 'nndsvdar', 'random'))","106","@pytest.mark.parametrize('regularization',","107","                         (None, 'both', 'components', 'transformation'))","108","def test_nmf_fit_nn_output(solver, init, regularization):","112","    model = NMF(n_components=2, solver=solver, init=init,","113","                regularization=regularization, random_state=0)","114","    transf = model.fit_transform(A)","115","    assert not((model.components_ < 0).any() or","116","               (transf < 0).any())","120","@pytest.mark.parametrize('regularization',","121","                         (None, 'both', 'components', 'transformation'))","122","def test_nmf_fit_close(solver, regularization):","126","               regularization=regularization, max_iter=600)","132","@pytest.mark.parametrize('regularization',","133","                         (None, 'both', 'components', 'transformation'))","134","def test_nmf_transform(solver, regularization):","139","            regularization=regularization, random_state=0, tol=1e-5)","161","@pytest.mark.parametrize('regularization',","162","                         (None, 'both', 'components', 'transformation'))","163","def test_nmf_inverse_transform(solver, regularization):","168","            regularization=regularization, max_iter=1000)","182","@pytest.mark.parametrize('regularization',","183","                         [None, 'both', 'components', 'transformation'])","184","def test_nmf_sparse_input(solver, regularization):","194","               regularization=regularization, random_state=0,","195","               tol=1e-2)","222","@pytest.mark.parametrize('init', ['random', 'nndsvd'])","223","@pytest.mark.parametrize('solver', ('cd', 'mu'))","224","@pytest.mark.parametrize('regularization',","225","                         (None, 'both', 'components', 'transformation'))","226","def test_non_negative_factorization_consistency(init, solver, regularization):","233","    W_nmf, H, _ = non_negative_factorization(","234","        A, init=init, solver=solver,","235","        regularization=regularization, random_state=1, tol=1e-2)","236","    W_nmf_2, _, _ = non_negative_factorization(","237","        A, H=H, update_H=False, init=init, solver=solver,","238","        regularization=regularization, random_state=1, tol=1e-2)","240","    model_class = NMF(init=init, solver=solver,","241","                      regularization=regularization,","242","                      random_state=1, tol=1e-2)","243","    W_cls = model_class.fit_transform(A)","244","    W_cls_2 = model_class.transform(A)","246","    assert_array_almost_equal(W_nmf, W_cls, decimal=10)","247","    assert_array_almost_equal(W_nmf_2, W_cls_2, decimal=10)","537","@pytest.mark.parametrize(\"regularization\",","538","                         (None, \"both\", \"components\", \"transformation\"))","539","def test_nmf_dtype_match(dtype_in, dtype_out, solver, regularization):","543","    nmf = NMF(solver=solver, regularization=regularization)","551","@pytest.mark.parametrize(\"regularization\",","552","                         (None, \"both\", \"components\", \"transformation\"))","553","def test_nmf_float32_float64_consistency(solver, regularization):","557","    nmf32 = NMF(solver=solver, regularization=regularization, random_state=0)","559","    nmf64 = NMF(solver=solver, regularization=regularization, random_state=0)"],"delete":["22","def test_convergence_warning(solver):","27","        NMF(solver=solver, max_iter=1).fit(A)","99","def test_nmf_fit_nn_output():","103","    for solver in ('cd', 'mu'):","104","        for init in (None, 'nndsvd', 'nndsvda', 'nndsvdar', 'random'):","105","            model = NMF(n_components=2, solver=solver, init=init,","106","                        random_state=0)","107","            transf = model.fit_transform(A)","108","            assert not((model.components_ < 0).any() or","109","                       (transf < 0).any())","113","def test_nmf_fit_close(solver):","117","               max_iter=600)","123","def test_nmf_transform(solver):","128","            random_state=0, tol=1e-5)","150","def test_nmf_inverse_transform(solver):","155","            max_iter=1000)","169","def test_nmf_sparse_input(solver):","179","               random_state=0, tol=1e-2)","206","def test_non_negative_factorization_consistency():","213","    for init in ['random', 'nndsvd']:","214","        for solver in ('cd', 'mu'):","215","            W_nmf, H, _ = non_negative_factorization(","216","                A, init=init, solver=solver, random_state=1, tol=1e-2)","217","            W_nmf_2, _, _ = non_negative_factorization(","218","                A, H=H, update_H=False, init=init, solver=solver,","219","                random_state=1, tol=1e-2)","221","            model_class = NMF(init=init, solver=solver, random_state=1,","222","                              tol=1e-2)","223","            W_cls = model_class.fit_transform(A)","224","            W_cls_2 = model_class.transform(A)","226","            assert_array_almost_equal(W_nmf, W_cls, decimal=10)","227","            assert_array_almost_equal(W_nmf_2, W_cls_2, decimal=10)","517","def test_nmf_dtype_match(dtype_in, dtype_out, solver):","521","    nmf = NMF(solver=solver)","529","def test_nmf_float32_float64_consistency(solver):","533","    nmf32 = NMF(solver=solver, random_state=0)","535","    nmf64 = NMF(solver=solver, random_state=0)"]}],"doc\/modules\/decomposition.rst":[{"add":["761",":class:`NMF` regularizes both W and H by default. The :attr:`regularization`","762","parameter allows for finer control, with which only W, only H,","763","or both can be regularized."],"delete":["761",":class:`NMF` regularizes both W and H. The public function","762",":func:`non_negative_factorization` allows a finer control through the","763",":attr:`regularization` attribute, and may regularize only W, only H, or both."]}],"sklearn\/decomposition\/_nmf.py":[{"add":["1083","    \"\"\"Non-Negative Matrix Factorization (NMF)","1099","        ||A||_Fro^2 = \\\\sum_{i,j} A_{ij}^2 (Frobenius norm)","1100","        ||vec(A)||_1 = \\\\sum_{i,j} abs(A_{ij}) (Elementwise L1 norm)","1200","    regularization : {'both', 'components', 'transformation', None}, \\","1201","                     default='both'","1202","        Select whether the regularization affects the components (H), the","1203","        transformation (W), both or none of them.","1204","","1205","        .. versionadded:: 0.24","1206","","1248","                 shuffle=False, regularization='both'):","1260","        self.regularization = regularization","1295","            l1_ratio=self.l1_ratio, regularization=self.regularization,","1344","            alpha=self.alpha, l1_ratio=self.l1_ratio,","1345","            regularization=self.regularization,","1346","            random_state=self.random_state,","1347","            verbose=self.verbose, shuffle=self.shuffle)"],"delete":["1083","    r\"\"\"Non-Negative Matrix Factorization (NMF)","1099","        ||A||_Fro^2 = \\sum_{i,j} A_{ij}^2 (Frobenius norm)","1100","        ||vec(A)||_1 = \\sum_{i,j} abs(A_{ij}) (Elementwise L1 norm)","1241","                 shuffle=False):","1287","            l1_ratio=self.l1_ratio, regularization='both',","1336","            alpha=self.alpha, l1_ratio=self.l1_ratio, regularization='both',","1337","            random_state=self.random_state, verbose=self.verbose,","1338","            shuffle=self.shuffle)"]}],"doc\/whats_new\/v0.24.rst":[{"add":["116","- |Enhancement| :class:`decomposition.NMF` now supports the optional parameter","117","  `regularization`, which can take the values `None`, `components`,","118","  `transformation` or `both`, in accordance with","119","  :func:`decomposition.NMF.non_negative_factorization`.","120","  :pr:`17414` by :user:`Bharat Raghunathan <Bharat123rox>`.","121",""],"delete":[]}]}},"174f9351495159fcb8d80680309e234f74feb9e8":{"changes":{"sklearn\/tests\/test_pipeline.py":"MODIFY","sklearn\/pipeline.py":"MODIFY","doc\/whats_new\/v0.24.rst":"MODIFY"},"diff":{"sklearn\/tests\/test_pipeline.py":[{"add":["560","@pytest.mark.parametrize(\"start, end\", [(0, 1), (0, 2), (1, 2), (1, 3),","561","                                        (None, 1), (1, None), (None, None)])","562","def test_pipeline_slice(start, end):","563","    pipe = Pipeline(","564","        [(\"transf1\", Transf()), (\"transf2\", Transf()), (\"clf\", FitParamT())],","565","        memory=\"123\",","566","        verbose=True,","567","    )","568","    pipe_slice = pipe[start:end]","569","    # Test class","570","    assert isinstance(pipe_slice, Pipeline)","571","    # Test steps","572","    assert pipe_slice.steps == pipe.steps[start:end]","573","    # Test named_steps attribute","574","    assert list(pipe_slice.named_steps.items()) == list(","575","        pipe.named_steps.items())[start:end]","576","    # Test the rest of the parameters","577","    pipe_params = pipe.get_params(deep=False)","578","    pipe_slice_params = pipe_slice.get_params(deep=False)","579","    del pipe_params[\"steps\"]","580","    del pipe_slice_params[\"steps\"]","581","    assert pipe_params == pipe_slice_params","582","    # Test exception","583","    msg = \"Pipeline slicing only supports a step of 1\"","584","    with pytest.raises(ValueError, match=msg):","585","        pipe[start:end:-1]"],"delete":["560","def test_pipeline_slice():","561","    pipe = Pipeline([('transf1', Transf()),","562","                     ('transf2', Transf()),","563","                     ('clf', FitParamT())])","564","    pipe2 = pipe[:-1]","565","    assert isinstance(pipe2, Pipeline)","566","    assert pipe2.steps == pipe.steps[:-1]","567","    assert 2 == len(pipe2.named_steps)","568","    assert_raises(ValueError, lambda: pipe[::-1])"]}],"sklearn\/pipeline.py":[{"add":["209","                raise ValueError(\"Pipeline slicing only supports a step of 1\")","210","            return self.__class__(","211","                self.steps[ind], memory=self.memory, verbose=self.verbose","212","            )"],"delete":["209","                raise ValueError('Pipeline slicing only supports a step of 1')","210","            return self.__class__(self.steps[ind])"]}],"doc\/whats_new\/v0.24.rst":[{"add":["532","- |Fix| A slice of a :class:`pipeline.Pipeline` now inherits the parameters of","533","  the original pipeline (`memory` and `verbose`).","534","  :pr:`18429` by :user:`Albert Villanova del Moral <albertvillanova>` and","535","  :user:`Pawe? Biernat <pwl>`.","536",""],"delete":[]}]}},"4e8dc9db5cbdc6fd327d3cf8e4ff78bb15140b92":{"changes":{"examples\/model_selection\/plot_grid_search_digits.py":"MODIFY","examples\/cluster\/plot_optics.py":"MODIFY","examples\/impute\/plot_missing_values.py":"MODIFY","examples\/covariance\/plot_covariance_estimation.py":"MODIFY","examples\/compose\/plot_column_transformer_mixed_types.py":"MODIFY","examples\/linear_model\/plot_sgd_early_stopping.py":"MODIFY","examples\/linear_model\/plot_sgd_loss_functions.py":"MODIFY","examples\/miscellaneous\/plot_kernel_approximation.py":"MODIFY","examples\/impute\/plot_iterative_imputer_variants_comparison.py":"MODIFY","examples\/miscellaneous\/plot_anomaly_comparison.py":"MODIFY","examples\/neighbors\/plot_kde_1d.py":"MODIFY","examples\/miscellaneous\/plot_multilabel.py":"MODIFY"},"diff":{"examples\/model_selection\/plot_grid_search_digits.py":[{"add":["6","which is done using the :class:`~sklearn.model_selection.GridSearchCV` object"],"delete":["6","which is done using the :class:`sklearn.model_selection.GridSearchCV` object"]}],"examples\/cluster\/plot_optics.py":[{"add":["4","","5",".. currentmodule:: sklearn","6","","10","The :class:`~cluster.OPTICS` is first used with its Xi cluster detection","12","corresponds to :class:`~cluster.DBSCAN`. We can see that the different"],"delete":["7","The :class:`sklearn.cluster.OPTICS` is first used with its Xi cluster detection","9","corresponds to :class:`sklearn.cluster.DBSCAN`. We can see that the different"]}],"examples\/impute\/plot_missing_values.py":[{"add":["6","value using the basic :class:`~sklearn.impute.SimpleImputer`.","180","# :class:`~sklearn.impute.KNNImputer` imputes missing values using the weighted","217","# Another option is the :class:`~sklearn.impute.IterativeImputer`. This uses"],"delete":["6","value using the basic :class:`sklearn.impute.SimpleImputer`.","180","# :class:`sklearn.impute.KNNImputer` imputes missing values using the weighted","217","# Another option is the :class:`sklearn.impute.IterativeImputer`. This uses"]}],"examples\/covariance\/plot_covariance_estimation.py":[{"add":["7",":class:`~sklearn.covariance.EmpiricalCovariance`. It is unbiased, i.e. it","23","  criterion), yielding the :class:`~sklearn.covariance.LedoitWolf`","27","  :class:`~sklearn.covariance.OAS`, proposed by Chen et al. Its"],"delete":["7",":class:`sklearn.covariance.EmpiricalCovariance`. It is unbiased, i.e. it","23","  criterion), yielding the :class:`sklearn.covariance.LedoitWolf`","27","  :class:`sklearn.covariance.OAS`, proposed by Chen et al. Its"]}],"examples\/compose\/plot_column_transformer_mixed_types.py":[{"add":["5",".. currentmodule:: sklearn","6","","9",":class:`~compose.ColumnTransformer`. This is particularly handy for the","21","using :class:`~pipeline.Pipeline`, together with a simple classification","168","# :class:`~sklearn.model_selection.GridSearchCV`."],"delete":["7",":class:`sklearn.compose.ColumnTransformer`. This is particularly handy for the","19","using :class:`sklearn.pipeline.Pipeline`, together with a simple classification","166","# :class:`sklearn.model_selection.GridSearchCV`."]}],"examples\/linear_model\/plot_sgd_early_stopping.py":[{"add":["31",":class:`~sklearn.linear_model.SGDClassifier` model to achieve almost the same"],"delete":["31",":class:`sklearn.linear_model.SGDClassifier` model to achieve almost the same"]}],"examples\/linear_model\/plot_sgd_loss_functions.py":[{"add":["6",":class:`~sklearn.linear_model.SGDClassifier` ."],"delete":["6",":class:`sklearn.linear_model.SGDClassifier` ."]}],"examples\/miscellaneous\/plot_kernel_approximation.py":[{"add":["26","stochastic gradient descent via :class:`~sklearn.linear_model.SGDClassifier`."],"delete":["26","stochastic gradient descent via :class:`sklearn.linear_model.SGDClassifier`."]}],"examples\/impute\/plot_iterative_imputer_variants_comparison.py":[{"add":["5",".. currentmodule:: sklearn","6","","7","The :class:`~impute.IterativeImputer` class is very flexible - it can be","12","imputation with :class:`~impute.IterativeImputer`:","14","* :class:`~linear_model.BayesianRidge`: regularized linear regression","15","* :class:`~tree.DecisionTreeRegressor`: non-linear regression","16","* :class:`~ensemble.ExtraTreesRegressor`: similar to missForest in R","17","* :class:`~neighbors.KNeighborsRegressor`: comparable to other KNN","21",":class:`~impute.IterativeImputer` to mimic the behavior of missForest, a","23",":class:`~ensemble.ExtraTreesRegressor` instead of","24",":class:`~ensemble.RandomForestRegressor` (as in missForest) due to its","27","Note that :class:`~neighbors.KNeighborsRegressor` is different from KNN","32",":class:`~impute.IterativeImputer` when using a","33",":class:`~linear_model.BayesianRidge` estimator on the California housing","37",":class:`~ensemble.ExtraTreesRegressor` and","38",":class:`~linear_model.BayesianRidge` give the best results."],"delete":["5","The :class:`sklearn.impute.IterativeImputer` class is very flexible - it can be","10","imputation with :class:`sklearn.impute.IterativeImputer`:","12","* :class:`~sklearn.linear_model.BayesianRidge`: regularized linear regression","13","* :class:`~sklearn.tree.DecisionTreeRegressor`: non-linear regression","14","* :class:`~sklearn.ensemble.ExtraTreesRegressor`: similar to missForest in R","15","* :class:`~sklearn.neighbors.KNeighborsRegressor`: comparable to other KNN","19",":class:`sklearn.impute.IterativeImputer` to mimic the behavior of missForest, a","21",":class:`sklearn.ensemble.ExtraTreesRegressor` instead of","22",":class:`sklearn.ensemble.RandomForestRegressor` (as in missForest) due to its","25","Note that :class:`sklearn.neighbors.KNeighborsRegressor` is different from KNN","30",":class:`sklearn.impute.IterativeImputer` when using a","31",":class:`sklearn.linear_model.BayesianRidge` estimator on the California housing","35",":class:`sklearn.ensemble.ExtraTreesRegressor` and","36",":class:`sklearn.linear_model.BayesianRidge` give the best results."]}],"examples\/miscellaneous\/plot_anomaly_comparison.py":[{"add":["16","The :class:`~sklearn.svm.OneClassSVM` is known to be sensitive to outliers and","24",":class:`~sklearn.covariance.EllipticEnvelope` assumes the data is Gaussian and","28",":class:`~sklearn.ensemble.IsolationForest` and","29",":class:`~sklearn.neighbors.LocalOutlierFactor` seem to perform reasonably well","31",":class:`~sklearn.neighbors.LocalOutlierFactor` over the other estimators is","39","hypercube. Except for the :class:`~sklearn.svm.OneClassSVM` which overfits a"],"delete":["16","The :class:`sklearn.svm.OneClassSVM` is known to be sensitive to outliers and","24",":class:`sklearn.covariance.EllipticEnvelope` assumes the data is Gaussian and","28",":class:`sklearn.ensemble.IsolationForest` and","29",":class:`sklearn.neighbors.LocalOutlierFactor` seem to perform reasonably well","31",":class:`sklearn.neighbors.LocalOutlierFactor` over the other estimators is","39","hypercube. Except for the :class:`sklearn.svm.OneClassSVM` which overfits a"]}],"examples\/neighbors\/plot_kde_1d.py":[{"add":["4","This example uses the :class:`~sklearn.neighbors.KernelDensity` class to","20",":class:`~sklearn.neighbors.KernelDensity` estimator.  The available kernels"],"delete":["4","This example uses the :class:`sklearn.neighbors.KernelDensity` class to","20",":class:`sklearn.neighbors.KernelDensity` estimator.  The available kernels"]}],"examples\/miscellaneous\/plot_multilabel.py":[{"add":["22","the :class:`~sklearn.multiclass.OneVsRestClassifier` metaclassifier using two"],"delete":["22","the :class:`sklearn.multiclass.OneVsRestClassifier` metaclassifier using two"]}]}},"4f496868c6aa7f50db99229847285efbe50040c2":{"changes":{"sklearn\/cluster\/tests\/test_k_means.py":"MODIFY"},"diff":{"sklearn\/cluster\/tests\/test_k_means.py":[{"add":["474","def test_minibatch_kmeans_init_size():","515","@pytest.mark.parametrize(\"array_constr\", [np.array, sp.csr_matrix],","516","                         ids=[\"dense\", \"sparse\"])","517","@pytest.mark.parametrize(\"dtype\", [np.float32, np.float64])","518","@pytest.mark.parametrize(\"init\", [\"random\", \"k-means++\"])","519","@pytest.mark.parametrize(\"Estimator, algorithm\", [","520","    (KMeans, \"full\"),","521","    (KMeans, \"elkan\"),","522","    (MiniBatchKMeans, None)","523","])","524","def test_predict(Estimator, algorithm, init, dtype, array_constr):","525","    # Check the predict method and the equivalence between fit.predict and","526","    # fit_predict.","528","    # There's a very small chance of failure with elkan on unstructured dataset","529","    # because predict method uses fast euclidean distances computation which","530","    # may cause small numerical instabilities.","531","    if sys.platform == \"darwin\":","532","        pytest.xfail(","533","            \"Known failures on MacOS, See \"","534","            \"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/12644\")","536","    X, _ = make_blobs(n_samples=500, n_features=10, centers=10, random_state=0)","537","    X = array_constr(X)","538","","539","    # With n_init = 1","540","    km = Estimator(n_clusters=10, init=init, n_init=1, random_state=0)","541","    if algorithm is not None:","542","        km.set_params(algorithm=algorithm)","543","    km.fit(X)","544","    labels = km.labels_","545","","546","    # re-predict labels for training set using predict","547","    pred = km.predict(X)","548","    assert_array_equal(pred, labels)","551","    pred = km.fit_predict(X)","552","    assert_array_equal(pred, labels)","553","","554","    # predict centroid labels","555","    pred = km.predict(km.cluster_centers_)","556","    assert_array_equal(pred, np.arange(10))","557","","558","    # With n_init > 1","559","    # Due to randomness in the order in which chunks of data are processed when","560","    # using more than one thread, there might be different rounding errors for","561","    # the computation of the inertia between 2 runs. This might result in a","562","    # different ranking of 2 inits, hence a different labeling, even if they","563","    # give the same clustering. We only check the labels up to a permutation.","564","","565","    km = Estimator(n_clusters=10, init=init, n_init=10, random_state=0)","566","    if algorithm is not None:","567","        km.set_params(algorithm=algorithm)","568","    km.fit(X)","569","    labels = km.labels_","570","","571","    # re-predict labels for training set using predict","572","    pred = km.predict(X)","573","    assert_allclose(v_measure_score(pred, labels), 1)","574","","575","    # re-predict labels for training set using fit_predict","576","    pred = km.fit_predict(X)","577","    assert_allclose(v_measure_score(pred, labels), 1)","578","","579","    # predict centroid labels","580","    pred = km.predict(km.cluster_centers_)","581","    assert_allclose(v_measure_score(pred, np.arange(10)), 1)","584","@pytest.mark.parametrize(\"init\", [\"random\", \"k-means++\", centers],","585","                         ids=[\"random\", \"k-means++\", \"ndarray\"])","586","@pytest.mark.parametrize(\"Estimator\", [KMeans, MiniBatchKMeans])","587","def test_predict_dense_sparse(Estimator, init):","589","    # predict time and vice versa.","591","    km = Estimator(n_clusters=n_clusters, init=init, n_init=n_init,","592","                   random_state=0)","594","    km.fit(X_csr)","595","    assert_array_equal(km.predict(X), km.labels_)","596","","597","    km.fit(X)","598","    assert_array_equal(km.predict(X_csr), km.labels_)","660","    previous_inertia = np.inf","661","    for n_init in [1, 5, 10]:","662","        # set max_iter=1 to avoid finding the global minimum and get the same","663","        # inertia each time","664","        km = KMeans(n_clusters=n_clusters, init=\"random\", n_init=n_init,","665","                    random_state=0, max_iter=1).fit(X)","666","        assert km.inertia_ <= previous_inertia","727","    km = Estimator(init=centers_new_type, n_clusters=n_clusters, n_init=1)","734","def test_kmeans_init_fitted_centers(data):","735","    # Check that starting fitting from a local optimum shouldn't change the","736","    # solution","737","    km1 = KMeans(n_clusters=n_clusters).fit(data)","738","    km2 = KMeans(n_clusters=n_clusters, init=km1.cluster_centers_,","739","                 n_init=1).fit(data)","741","    assert_allclose(km1.cluster_centers_, km2.cluster_centers_)","814","def test_kmeans_elkan_iter_attribute():","817","    km = KMeans(algorithm=\"elkan\", max_iter=1).fit(X)","818","    assert km.n_iter_ == 1","821","@pytest.mark.parametrize(\"array_constr\", [np.array, sp.csr_matrix],","822","                         ids=[\"dense\", \"sparse\"])","823","def test_kmeans_empty_cluster_relocated(array_constr):","826","    X = array_constr([[-1], [1]])","892","    # Check warning messages specific to KMeans","896","        KMeans(n_clusters=1, algorithm=\"elkan\").fit(X)","899","@pytest.mark.parametrize(\"array_constr\", [np.array, sp.csr_matrix],","900","                         ids=[\"dense\", \"sparse\"])","901","@pytest.mark.parametrize(\"algo\", [\"full\", \"elkan\"])","931","    # Check that the _euclidean_(dense\/sparse)_dense helpers produce correct","932","    # results","973","@pytest.mark.parametrize(\"Estimator\", [KMeans, MiniBatchKMeans])","974","def test_sample_weight_unchanged(Estimator):","978","    Estimator(n_clusters=2, random_state=0).fit(X, sample_weight=sample_weight)"],"delete":["34","from sklearn.metrics.cluster import homogeneity_score","475","def test_sparse_mb_k_means_callable_init():","476","","477","    def test_init(X, k, random_state):","478","        return centers","479","","480","    mb_k_means = MiniBatchKMeans(n_clusters=3, init=test_init,","481","                                 random_state=42).fit(X_csr)","482","    _check_fitted_model(mb_k_means)","483","","484","","485","def test_mini_batch_k_means_random_init_partial_fit():","486","    km = MiniBatchKMeans(n_clusters=n_clusters, init=\"random\", random_state=42)","487","","488","    # use the partial_fit API for online learning","489","    for X_minibatch in np.array_split(X, 10):","490","        km.partial_fit(X_minibatch)","491","","492","    # compute the labeling on the complete dataset","493","    labels = km.predict(X)","494","    assert v_measure_score(true_labels, labels) == 1.0","495","","496","","497","def test_minibatch_kmeans_default_init_size():","514","def test_minibatch_tol():","515","    mb_k_means = MiniBatchKMeans(n_clusters=n_clusters, batch_size=10,","516","                                 random_state=42, tol=.01).fit(X)","517","    _check_fitted_model(mb_k_means)","518","","519","","520","def test_minibatch_set_init_size():","521","    mb_k_means = MiniBatchKMeans(init=centers.copy(), n_clusters=n_clusters,","522","                                 init_size=666, random_state=42,","523","                                 n_init=1).fit(X)","524","    assert mb_k_means.init_size == 666","525","    assert mb_k_means._init_size == n_samples","526","    _check_fitted_model(mb_k_means)","527","","528","","553","@pytest.mark.parametrize('Estimator', [KMeans, MiniBatchKMeans])","554","@pytest.mark.parametrize('data', [X, X_csr], ids=['dense', 'sparse'])","555","@pytest.mark.parametrize('init', ['random', 'k-means++', centers.copy()])","556","def test_predict(Estimator, data, init):","557","    n_init = 10 if type(init) is str else 1","558","    k_means = Estimator(n_clusters=n_clusters, init=init,","559","                        n_init=n_init, random_state=0).fit(data)","561","    # sanity check: re-predict labeling for training set samples","562","    assert_array_equal(k_means.predict(data), k_means.labels_)","564","    # sanity check: predict centroid labels","565","    pred = k_means.predict(k_means.cluster_centers_)","566","    assert_array_equal(pred, np.arange(n_clusters))","569","    pred = k_means.fit_predict(data)","570","    assert_array_equal(pred, k_means.labels_)","573","@pytest.mark.parametrize('init', ['random', 'k-means++', centers.copy()])","574","def test_predict_minibatch_dense_sparse(init):","576","    # predict time","578","    mb_k_means = MiniBatchKMeans(n_clusters=n_clusters, init=init,","579","                                 n_init=n_init, random_state=0).fit(X_csr)","581","    assert_array_equal(mb_k_means.predict(X), mb_k_means.labels_)","641","@pytest.mark.parametrize('algo', ['full', 'elkan'])","642","def test_predict_equal_labels(algo):","643","    km = KMeans(random_state=13, n_init=1, max_iter=1,","644","                algorithm=algo)","645","    km.fit(X)","646","    assert_array_equal(km.predict(X), km.labels_)","647","","648","","649","def test_full_vs_elkan():","650","    km1 = KMeans(algorithm='full', random_state=13).fit(X)","651","    km2 = KMeans(algorithm='elkan', random_state=13).fit(X)","652","","653","    assert homogeneity_score(km1.predict(X), km2.predict(X)) == 1.0","654","","655","","658","    n_runs = 5","659","    n_init_range = [1, 5, 10]","660","    inertia = np.zeros((len(n_init_range), n_runs))","661","    for i, n_init in enumerate(n_init_range):","662","        for j in range(n_runs):","663","            km = KMeans(n_clusters=n_clusters, init=\"random\", n_init=n_init,","664","                        random_state=j).fit(X)","665","            inertia[i, j] = km.inertia_","666","","667","    inertia = inertia.mean(axis=1)","668","    failure_msg = (\"Inertia %r should be decreasing\"","669","                   \" when n_init is increasing.\") % list(inertia)","670","    for i in range(len(n_init_range) - 1):","671","        assert inertia[i] >= inertia[i + 1], failure_msg","732","    km = Estimator(init=centers, n_clusters=n_clusters, n_init=1)","739","def test_k_means_init_fitted_centers(data):","740","    # Get a local optimum","741","    centers = KMeans(n_clusters=3).fit(X).cluster_centers_","743","    # Fit starting from a local optimum shouldn't change the solution","744","    new_centers = KMeans(n_clusters=3, init=centers,","745","                         n_init=1).fit(X).cluster_centers_","746","    assert_array_almost_equal(centers, new_centers)","819","def test_iter_attribute():","822","    estimator = KMeans(algorithm=\"elkan\", max_iter=1)","823","    estimator.fit(np.random.rand(10, 10))","824","    assert estimator.n_iter_ == 1","827","def test_k_means_empty_cluster_relocated():","830","    X = np.array([[-1], [1]])","896","    X, _ = make_blobs(n_samples=10, n_features=2, centers=1, random_state=0)","897","    kmeans = KMeans(n_clusters=1, n_init=1, init='random', random_state=0,","898","                    algorithm='elkan')","899","","903","        kmeans.fit(X)","906","@pytest.mark.parametrize(\"array_constr\",","907","                         [np.array, sp.csr_matrix],","908","                         ids=['dense', 'sparse'])","909","@pytest.mark.parametrize(\"algo\", ['full', 'elkan'])","979","def test_sample_weight_unchanged():","983","    KMeans(n_clusters=2, random_state=0).fit(X, sample_weight=sample_weight)"]}]}},"0fa32639fed0775375c720984dd10d2931245b8a":{"changes":{"sklearn\/manifold\/_locally_linear.py":"MODIFY","doc\/whats_new\/v0.24.rst":"MODIFY"},"diff":{"sklearn\/manifold\/_locally_linear.py":[{"add":["20","def barycenter_weights(X, Y, indices, reg=1e-3):","23","    We estimate the weights to assign to each point in Y[indices] to recover","30","    Y : array-like, shape (n_samples, n_dim)","31","","32","    indices : array-like, shape (n_samples, n_dim)","33","            Indices of the points in Y used to compute the barycenter","48","    Y = check_array(Y, dtype=FLOAT_DTYPES)","49","    indices = check_array(indices, dtype=int)","51","    n_samples, n_neighbors = indices.shape","52","    assert X.shape[0] == n_samples","53","","59","    for i, ind in enumerate(indices):","60","        A = Y[ind]","61","        C = A - X[i]  # broadcasting","68","        G.flat[::n_neighbors + 1] += R","110","    data = barycenter_weights(X, X, ind, reg=reg)","731","        weights = barycenter_weights(X, self.nbrs_._fit_X, ind, reg=self.reg)"],"delete":["20","def barycenter_weights(X, Z, reg=1e-3):","23","    We estimate the weights to assign to each point in Y[i] to recover","30","    Z : array-like, shape (n_samples, n_neighbors, n_dim)","45","    Z = check_array(Z, dtype=FLOAT_DTYPES, allow_nd=True)","47","    n_samples, n_neighbors = X.shape[0], Z.shape[1]","53","    for i, A in enumerate(Z.transpose(0, 2, 1)):","54","        C = A.T - X[i]  # broadcasting","61","        G.flat[::Z.shape[1] + 1] += R","66","","104","    data = barycenter_weights(X, X[ind], reg=reg)","725","        weights = barycenter_weights(X, self.nbrs_._fit_X[ind],","726","                                     reg=self.reg)"]}],"doc\/whats_new\/v0.24.rst":[{"add":["214","- |Efficiency| Fixed :issue:`10493`. Improve Local Linear Embedding (LLE) ","215","  that raised `MemoryError` exception when used with large inputs.","216","  :pr:`17997` by :user:`Bertrand Maisonneuve <bmaisonn>`.","217",""],"delete":[]}]}},"e54cd3c0617e3485baa19e2c69332da55b363636":{"changes":{"sklearn\/metrics\/_ranking.py":"MODIFY","sklearn\/preprocessing\/_discretization.py":"MODIFY","sklearn\/preprocessing\/tests\/test_label.py":"MODIFY","sklearn\/calibration.py":"MODIFY","sklearn\/metrics\/tests\/test_classification.py":"MODIFY","sklearn\/preprocessing\/_encoders.py":"MODIFY","sklearn\/preprocessing\/_data.py":"MODIFY","sklearn\/preprocessing\/_function_transformer.py":"MODIFY","sklearn\/preprocessing\/tests\/test_data.py":"MODIFY","sklearn\/preprocessing\/_label.py":"MODIFY"},"diff":{"sklearn\/metrics\/_ranking.py":[{"add":["385","        y_true = label_binarize(y_true, classes=labels)[:, 0]","491","        y_true_multilabel = label_binarize(y_true, classes=classes)"],"delete":["385","        y_true = label_binarize(y_true, labels)[:, 0]","491","        y_true_multilabel = label_binarize(y_true, classes)"]}],"sklearn\/preprocessing\/_discretization.py":[{"add":["18","from ..utils.validation import _deprecate_positional_args","118","    @_deprecate_positional_args","119","    def __init__(self, n_bins=5, *, encode='onehot', strategy='quantile'):"],"delete":["117","    def __init__(self, n_bins=5, encode='onehot', strategy='quantile'):"]}],"sklearn\/preprocessing\/tests\/test_label.py":[{"add":["180","        label_binarize(np.array([[1, 3], [2, 1]]), classes=[1, 2, 3])","511","                label_binarize(y, classes=classes, neg_label=neg_label,","517","        binarized = label_binarize(y, classes=classes, neg_label=neg_label,","578","        label_binarize(y, classes=classes, neg_label=-1, pos_label=pos_label,","597","        label_binarize(y, classes=classes, neg_label=-1, pos_label=pos_label,"],"delete":["180","        label_binarize(np.array([[1, 3], [2, 1]]), [1, 2, 3])","511","                label_binarize(y, classes, neg_label=neg_label,","517","        binarized = label_binarize(y, classes, neg_label=neg_label,","578","        label_binarize(y, classes, neg_label=-1, pos_label=pos_label,","597","        label_binarize(y, classes, neg_label=-1, pos_label=pos_label,"]}],"sklearn\/calibration.py":[{"add":["331","        Y = label_binarize(y, classes=self.classes_)","576","    y_true = label_binarize(y_true, classes=labels)[:, 0]"],"delete":["331","        Y = label_binarize(y, self.classes_)","576","    y_true = label_binarize(y_true, labels)[:, 0]"]}],"sklearn\/metrics\/tests\/test_classification.py":[{"add":["656","    y_true_inv2 = label_binarize(y_true, classes=[\"a\", \"b\"])"],"delete":["656","    y_true_inv2 = label_binarize(y_true, [\"a\", \"b\"])"]}],"sklearn\/preprocessing\/_encoders.py":[{"add":["10","from ..utils.validation import _deprecate_positional_args","295","    @_deprecate_positional_args","296","    def __init__(self, *, categories='auto', drop=None, sparse=True,","657","    @_deprecate_positional_args","658","    def __init__(self, *, categories='auto', dtype=np.float64):"],"delete":["294","    def __init__(self, categories='auto', drop=None, sparse=True,","655","    def __init__(self, categories='auto', dtype=np.float64):"]}],"sklearn\/preprocessing\/_data.py":[{"add":["31","                                FLOAT_DTYPES, _deprecate_positional_args)","80","@_deprecate_positional_args","81","def scale(X, *, axis=0, with_mean=True, with_std=True, copy=True):","294","    @_deprecate_positional_args","295","    def __init__(self, feature_range=(0, 1), *, copy=True):","439","@_deprecate_positional_args","440","def minmax_scale(X, feature_range=(0, 1), *, axis=0, copy=True):","631","    @_deprecate_positional_args","632","    def __init__(self, *, copy=True, with_mean=True, with_std=True):","914","    @_deprecate_positional_args","915","    def __init__(self, *, copy=True):","1031","@_deprecate_positional_args","1032","def maxabs_scale(X, *, axis=0, copy=True):","1180","    @_deprecate_positional_args","1181","    def __init__(self, *, with_centering=True, with_scaling=True,","1290","@_deprecate_positional_args","1291","def robust_scale(X, *, axis=0, with_centering=True, with_scaling=True,","1442","    @_deprecate_positional_args","1443","    def __init__(self, degree=2, *, interaction_only=False, include_bias=True,","1648","@_deprecate_positional_args","1649","def normalize(X, norm='l2', *, axis=1, copy=True, return_norm=False):","1808","    @_deprecate_positional_args","1809","    def __init__(self, norm='l2', *, copy=True):","1845","@_deprecate_positional_args","1846","def binarize(X, *, threshold=0.0, copy=True):","1944","    @_deprecate_positional_args","1945","    def __init__(self, *, threshold=0.0, copy=True):","2242","    @_deprecate_positional_args","2243","    def __init__(self, *, n_quantiles=1000, output_distribution='uniform',","2575","@_deprecate_positional_args","2576","def quantile_transform(X, *, axis=0, n_quantiles=1000,","2780","    @_deprecate_positional_args","2781","    def __init__(self, method='yeo-johnson', *, standardize=True, copy=True):","3051","@_deprecate_positional_args","3052","def power_transform(X, method='yeo-johnson', *, standardize=True, copy=True):"],"delete":["31","                                FLOAT_DTYPES)","80","def scale(X, axis=0, with_mean=True, with_std=True, copy=True):","293","    def __init__(self, feature_range=(0, 1), copy=True):","437","def minmax_scale(X, feature_range=(0, 1), axis=0, copy=True):","628","    def __init__(self, copy=True, with_mean=True, with_std=True):","910","    def __init__(self, copy=True):","1026","def maxabs_scale(X, axis=0, copy=True):","1174","","1175","    def __init__(self, with_centering=True, with_scaling=True,","1284","def robust_scale(X, axis=0, with_centering=True, with_scaling=True,","1435","    def __init__(self, degree=2, interaction_only=False, include_bias=True,","1640","def normalize(X, norm='l2', axis=1, copy=True, return_norm=False):","1799","    def __init__(self, norm='l2', copy=True):","1835","def binarize(X, threshold=0.0, copy=True):","1933","    def __init__(self, threshold=0.0, copy=True):","2230","    def __init__(self, n_quantiles=1000, output_distribution='uniform',","2562","def quantile_transform(X, axis=0, n_quantiles=1000,","2766","    def __init__(self, method='yeo-johnson', standardize=True, copy=True):","3036","def power_transform(X, method='yeo-johnson', standardize=True, copy=True):"]}],"sklearn\/preprocessing\/_function_transformer.py":[{"add":["4","from ..utils.validation import _deprecate_positional_args","81","","82","    @_deprecate_positional_args","83","    def __init__(self, func=None, inverse_func=None, *, validate=False,"],"delete":["80","    def __init__(self, func=None, inverse_func=None, validate=False,"]}],"sklearn\/preprocessing\/tests\/test_data.py":[{"add":["2297","        power_transform(X_with_negatives, method='box-cox')","2306","        power_transform(np.zeros(X_2d.shape), method='box-cox')","2434","    pt = PowerTransformer(method, standardize=standardize)","2451","    pt = PowerTransformer(method, standardize=standardize, copy=True)","2479","    pt = PowerTransformer(method, standardize=standardize, copy=False)"],"delete":["2297","        power_transform(X_with_negatives, 'box-cox')","2306","        power_transform(np.zeros(X_2d.shape), 'box-cox')","2434","    pt = PowerTransformer(method, standardize)","2451","    pt = PowerTransformer(method, standardize, copy=True)","2479","    pt = PowerTransformer(method, standardize, copy=False)"]}],"sklearn\/preprocessing\/_label.py":[{"add":["23","from ..utils.validation import _deprecate_positional_args","399","    @_deprecate_positional_args","400","    def __init__(self, *, neg_label=0, pos_label=1, sparse_output=False):","487","        return label_binarize(y, classes=self.classes_,","545","@_deprecate_positional_args","546","def label_binarize(y, *, classes, neg_label=0, pos_label=1,","547","                   sparse_output=False):","857","    @_deprecate_positional_args","858","    def __init__(self, *, classes=None, sparse_output=False):"],"delete":["398","    def __init__(self, neg_label=0, pos_label=1, sparse_output=False):","485","        return label_binarize(y, self.classes_,","543","def label_binarize(y, classes, neg_label=0, pos_label=1, sparse_output=False):","853","    def __init__(self, classes=None, sparse_output=False):"]}]}},"afeae82ec419ff992d528a7af563361173cafec1":{"changes":{"build_tools\/azure\/posix.yml":"MODIFY",".travis.yml":"MODIFY","sklearn\/conftest.py":"MODIFY","build_tools\/azure\/posix-32.yml":"MODIFY"},"diff":{"build_tools\/azure\/posix.yml":[{"add":["19","    OMP_NUM_THREADS: '2'","20","    OPENBLAS_NUM_THREADS: '2'"],"delete":["19","    OMP_NUM_THREADS: '4'","20","    OPENBLAS_NUM_THREADS: '4'"]}],".travis.yml":[{"add":["15","    - OMP_NUM_THREADS=2","16","    - OPENBLAS_NUM_THREADS=2"],"delete":["15","    - OMP_NUM_THREADS=4","16","    - OPENBLAS_NUM_THREADS=4"]}],"sklearn\/conftest.py":[{"add":["0","import os","1","","3","from threadpoolctl import threadpool_limits","4","","5","from sklearn.utils._openmp_helpers import _openmp_effective_n_threads","26","","27","","28","def pytest_runtest_setup(item):","29","    \"\"\"Set the number of openmp threads based on the number of workers","30","    xdist is using to prevent oversubscription.","31","","32","    Parameters","33","    ----------","34","    item : pytest item","35","        item to be processed","36","    \"\"\"","37","    try:","38","        xdist_worker_count = int(os.environ['PYTEST_XDIST_WORKER_COUNT'])","39","    except KeyError:","40","        # raises when pytest-xdist is not installed","41","        return","42","","43","    openmp_threads = _openmp_effective_n_threads()","44","    threads_per_worker = max(openmp_threads \/\/ xdist_worker_count, 1)","45","    threadpool_limits(threads_per_worker, user_api='openmp')"],"delete":[]}],"build_tools\/azure\/posix-32.yml":[{"add":["16","    OMP_NUM_THREADS: '2'","19","    OPENBLAS_NUM_THREADS: '2'"],"delete":["16","    OMP_NUM_THREADS: '4'","19","    OPENBLAS_NUM_THREADS: '4'"]}]}},"8cce5bfcf0f001d91deca145f878f474a0b8e894":{"changes":{"doc\/whats_new\/v0.24.rst":"MODIFY"},"diff":{"doc\/whats_new\/v0.24.rst":[{"add":["54",":mod:`sklearn.feature_selection`","55","................................","56","","57","- |Feature| A new parameter `importance_getter` was added to","58","  :class:`feature_selection.RFE`, :class:`feature_selection.RFECV` and","59","  :class:`feature_selection.SelectFromModel`, allowing the user to specify an","60","  attribute name\/path or a `callable` for extracting feature importance from","61","  the estimator.  :pr:`15361` by :user:`Venkatachalam N <venkyyuvy>`","62","","80","- |Enhancement| Add `sample_weight` parameter to","81","  :class:`metrics.median_absolute_error`. :pr:`17225` by","82","  :user:`Lucy Liu <lucyleeow>`.","83",""],"delete":["80","- |Feature| A new parameter `importance_getter` was added to","81","  :class:`feature_selection.RFE`, :class:`feature_selection.RFECV` and","82","  :class:`feature_selection.SelectFromModel`, allowing the user to specify an","83","  attribute name\/path or a `callable` for extracting feature importance from","84","  the estimator.  :pr:`15361` by :user:`Venkatachalam N <venkyyuvy>`","85","","86",":mod:`sklearn.metrics`","87","......................","88","","89","- |Enhancement| Add `sample_weight` parameter to","90","  :class:`metrics.median_absolute_error`.","91","  :pr:`17225` by :user:`Lucy Liu <lucyleeow>`.","92",""]}]}},"c29092d6994a43ea19a82e91c0331e8b7d6e7d36":{"changes":{"examples\/decomposition\/plot_sparse_coding.py":"MODIFY","sklearn\/linear_model\/tests\/test_coordinate_descent.py":"MODIFY","sklearn\/utils\/fixes.py":"MODIFY","examples\/cluster\/plot_coin_segmentation.py":"MODIFY","sklearn\/ensemble\/tests\/test_forest.py":"MODIFY","sklearn\/neighbors\/_base.py":"MODIFY","examples\/miscellaneous\/plot_johnson_lindenstrauss_bound.py":"MODIFY","examples\/cluster\/plot_coin_ward_segmentation.py":"MODIFY","sklearn\/linear_model\/tests\/test_base.py":"MODIFY","sklearn\/neighbors\/tests\/test_dist_metrics.py":"MODIFY","sklearn\/utils\/__init__.py":"MODIFY","examples\/compose\/plot_transformed_target.py":"MODIFY","sklearn\/utils\/tests\/test_estimator_checks.py":"MODIFY","sklearn\/datasets\/_lfw.py":"MODIFY","sklearn\/tests\/test_pipeline.py":"MODIFY","sklearn\/linear_model\/tests\/test_least_angle.py":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_loss.py":"MODIFY","sklearn\/utils\/validation.py":"MODIFY","sklearn\/linear_model\/tests\/test_sgd.py":"MODIFY","sklearn\/utils\/tests\/test_multiclass.py":"MODIFY","examples\/neighbors\/plot_kde_1d.py":"MODIFY","conftest.py":"MODIFY"},"diff":{"examples\/decomposition\/plot_sparse_coding.py":[{"add":["22","from sklearn.utils.fixes import np_version, parse_version","69","lstsq_rcond = None if np_version >= parse_version('1.14') else -1"],"delete":["18","from distutils.version import LooseVersion","19","","70","lstsq_rcond = None if LooseVersion(np.__version__) >= '1.14' else -1"]}],"sklearn\/linear_model\/tests\/test_coordinate_descent.py":[{"add":["24","from sklearn.utils.fixes import parse_version","1036","    if (parse_version(joblib.__version__) < parse_version('0.12')","1037","            and backend == 'loky'):"],"delete":["10","from distutils.version import LooseVersion","1036","    if joblib.__version__ < LooseVersion('0.12') and backend == 'loky':"]}],"sklearn\/utils\/fixes.py":[{"add":["23","try:","24","    from pkg_resources import parse_version  # type: ignore","25","except ImportError:","26","    # setuptools not installed","27","    parse_version = LooseVersion  # type: ignore","30","np_version = parse_version(np.__version__)","31","sp_version = parse_version(scipy.__version__)","34","if sp_version >= parse_version('1.4'):","38","    # once support for sp_version < parse_version('1.4') is dropped","53","    if sp_version >= parse_version('1.1') or not sp.issparse(X):","82","    if parse_version(joblib.__version__) >= parse_version('0.12'):","167","    if np_version > parse_version('1.14'):"],"delete":["23","","24","def _parse_version(version_string):","25","    version = []","26","    for x in version_string.split('.'):","27","        try:","28","            version.append(int(x))","29","        except ValueError:","30","            # x may be of the form dev-1ea1592","31","            version.append(x)","32","    return tuple(version)","35","np_version = _parse_version(np.__version__)","36","sp_version = _parse_version(scipy.__version__)","39","if sp_version >= (1, 4):","43","    # once support for sp_version < (1, 4) is dropped","58","    if sp_version >= (1, 1) or not sp.issparse(X):","87","    if joblib.__version__ >= LooseVersion('0.12'):","172","    if np_version > (1, 14):"]}],"examples\/cluster\/plot_coin_segmentation.py":[{"add":["35","from sklearn.utils.fixes import parse_version","38","if parse_version(skimage.__version__) >= parse_version('0.14'):"],"delete":["27","from distutils.version import LooseVersion","38","if LooseVersion(skimage.__version__) >= '0.14':"]}],"sklearn\/ensemble\/tests\/test_forest.py":[{"add":["36","from sklearn.utils.fixes import parse_version","1276","@pytest.mark.skipif(parse_version(joblib.__version__) < parse_version('0.12'),"],"delete":["13","from distutils.version import LooseVersion","1276","@pytest.mark.skipif(joblib.__version__ < LooseVersion('0.12'),"]}],"sklearn\/neighbors\/_base.py":[{"add":["29","from ..utils.fixes import parse_version","652","                    parse_version(joblib.__version__) < parse_version('0.12'))","959","            if parse_version(joblib.__version__) < parse_version('0.12'):"],"delete":["9","from distutils.version import LooseVersion","652","                    LooseVersion(joblib.__version__) < LooseVersion('0.12'))","959","            if LooseVersion(joblib.__version__) < LooseVersion('0.12'):"]}],"examples\/miscellaneous\/plot_johnson_lindenstrauss_bound.py":[{"add":["26","from sklearn.utils.fixes import parse_version","29","if parse_version(matplotlib.__version__) >= parse_version('2.1'):"],"delete":["21","from distutils.version import LooseVersion","29","if LooseVersion(matplotlib.__version__) >= '2.1':"]}],"examples\/cluster\/plot_coin_ward_segmentation.py":[{"add":["29","from sklearn.utils.fixes import parse_version","32","if parse_version(skimage.__version__) >= parse_version('0.14'):"],"delete":["19","from distutils.version import LooseVersion","32","if LooseVersion(skimage.__version__) >= '0.14':"]}],"sklearn\/linear_model\/tests\/test_base.py":[{"add":["15","from sklearn.utils.fixes import parse_version","211","    if parse_version(pd.__version__) < parse_version('0.24.0'):"],"delete":["7","from distutils.version import LooseVersion","8","","212","    if LooseVersion(pd.__version__) < '0.24.0':"]}],"sklearn\/neighbors\/tests\/test_dist_metrics.py":[{"add":["13","from sklearn.utils.fixes import sp_version, parse_version","107","    if metric == 'jaccard' and sp_version < parse_version('1.2.0'):"],"delete":["8","from distutils.version import LooseVersion","9","from scipy import __version__ as scipy_version","108","    if metric == 'jaccard' and LooseVersion(scipy_version) < '1.2.0':"]}],"sklearn\/utils\/__init__.py":[{"add":["26","from .fixes import np_version, parse_version","173","    if np_version < parse_version('1.12') or issparse(array):"],"delete":["26","from .fixes import np_version","173","    if np_version < (1, 12) or issparse(array):"]}],"examples\/compose\/plot_transformed_target.py":[{"add":["27","from sklearn.utils.fixes import parse_version","34","if parse_version(matplotlib.__version__) >= parse_version('2.1'):"],"delete":["21","from distutils.version import LooseVersion","34","if LooseVersion(matplotlib.__version__) >= '2.1':"]}],"sklearn\/utils\/tests\/test_estimator_checks.py":[{"add":["27","from sklearn.utils.fixes import np_version, parse_version","337","    if np_version < parse_version('1.17'):"],"delete":["27","from sklearn.utils.fixes import _parse_version","337","    np_version = _parse_version(np.__version__)","338","    if np_version < (1, 17):"]}],"sklearn\/datasets\/_lfw.py":[{"add":["22","from ..utils.fixes import parse_version","306","    if parse_version(joblib.__version__) < parse_version('0.12'):","479","    if parse_version(joblib.__version__) < parse_version('0.12'):"],"delete":["14","from distutils.version import LooseVersion","306","    if LooseVersion(joblib.__version__) < LooseVersion('0.12'):","479","    if LooseVersion(joblib.__version__) < LooseVersion('0.12'):"]}],"sklearn\/tests\/test_pipeline.py":[{"add":["21","from sklearn.utils.fixes import parse_version","1022","        if parse_version(joblib.__version__) < parse_version('0.12'):","1084","    if parse_version(joblib.__version__) < parse_version('0.12'):"],"delete":["3","from distutils.version import LooseVersion","1022","        if LooseVersion(joblib.__version__) < LooseVersion('0.12'):","1084","    if LooseVersion(joblib.__version__) < LooseVersion('0.12'):"]}],"sklearn\/linear_model\/tests\/test_least_angle.py":[{"add":["14","from sklearn.utils.fixes import np_version, parse_version","124","    rcond = None if np_version >= parse_version('1.14') else -1"],"delete":["2","from distutils.version import LooseVersion","3","","125","    rcond = None if LooseVersion(np.__version__) >= '1.14' else -1"]}],"sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_loss.py":[{"add":["6","from sklearn.utils.fixes import sp_version, parse_version","62","@pytest.mark.skipif(sp_version == parse_version('1.2.0'),"],"delete":["6","from sklearn.utils.fixes import sp_version","62","@pytest.mark.skipif(sp_version == (1, 2, 0),"]}],"sklearn\/utils\/validation.py":[{"add":["25","from .fixes import _object_dtype_isnan, parse_version","230","        if parse_version(joblib.__version__) < parse_version('0.12'):"],"delete":["17","from distutils.version import LooseVersion","26","from .fixes import _object_dtype_isnan","231","        if LooseVersion(joblib.__version__) < '0.12':"]}],"sklearn\/linear_model\/tests\/test_sgd.py":[{"add":["14","from sklearn.utils.fixes import parse_version","1596","    if (parse_version(joblib.__version__) < parse_version('0.12')","1597","            and backend == 'loky'):"],"delete":["0","from distutils.version import LooseVersion","1596","    if joblib.__version__ < LooseVersion('0.12') and backend == 'loky':"]}],"sklearn\/utils\/tests\/test_multiclass.py":[{"add":["17","from sklearn.utils.fixes import parse_version","309","    if parse_version(pd.__version__) >= parse_version('0.25'):"],"delete":["5","from distutils.version import LooseVersion","309","    if LooseVersion(pd.__version__) >= '0.25':"]}],"examples\/neighbors\/plot_kde_1d.py":[{"add":["35","from sklearn.utils.fixes import parse_version","38","if parse_version(matplotlib.__version__) >= parse_version('2.1'):"],"delete":["33","from distutils.version import LooseVersion","38","if LooseVersion(matplotlib.__version__) >= '2.1':"]}],"conftest.py":[{"add":["16","from sklearn.utils.fixes import np_version, parse_version","20","if parse_version(pytest.__version__) < parse_version(PYTEST_MIN_VERSION):","54","        if np_version < parse_version('1.14'):"],"delete":["9","from distutils.version import LooseVersion","20","if LooseVersion(pytest.__version__) < PYTEST_MIN_VERSION:","54","        import numpy as np","55","        if LooseVersion(np.__version__) < LooseVersion('1.14'):"]}]}},"dba4514aa2ba2ea6f15f6cab6f090f4f952ad89a":{"changes":{"build_tools\/azure\/install.sh":"MODIFY"},"diff":{"build_tools\/azure\/install.sh":[{"add":["99","","100","    # TODO: Remove pin when https:\/\/github.com\/python-pillow\/Pillow\/issues\/4518 gets fixed","101","    python -m pip install \"pillow>=4.3.0,!=7.1.0,!=7.1.1\"","102",""],"delete":[]}]}},"831a00a55e72131634afd3565e7d8c8d8470afbe":{"changes":{"sklearn\/utils\/validation.py":"MODIFY"},"diff":{"sklearn\/utils\/validation.py":[{"add":["137","    force_all_finite : boolean or 'allow-nan', default=True","322","    force_all_finite : boolean or 'allow-nan', default=True","415","    accept_sparse : string, boolean or list\/tuple of strings, default=False","422","    accept_large_sparse : bool, default=True","429","    dtype : string, type, list of types or None, default=\"numeric\"","435","    order : 'F', 'C' or None, default=None","442","    copy : boolean, default=False","446","    force_all_finite : boolean or 'allow-nan', default=True","461","    ensure_2d : boolean, default=True","464","    allow_nd : boolean, default=False","467","    ensure_min_samples : int, default=1","471","    ensure_min_features : int, default=1","478","    estimator : str or estimator instance, default=None","712","    accept_sparse : string, boolean or list of string, default=False","719","    accept_large_sparse : bool, default=True","726","    dtype : string, type, list of types or None, default=\"numeric\"","732","    order : 'F', 'C' or None, default=None","735","    copy : boolean, default=False","739","    force_all_finite : boolean or 'allow-nan', default=True","755","    ensure_2d : boolean, default=True","758","    allow_nd : boolean, default=False","761","    multi_output : boolean, default=False","766","    ensure_min_samples : int, default=1","770","    ensure_min_features : int, default=1","777","    y_numeric : boolean, default=False","782","    estimator : str or estimator instance, default=None","915","    raise_warning : boolean, default=True","917","    raise_exception : boolean, default=False","990","    all_or_any : callable, {all, any}, default=all","1064","    min_val : float or int, default=None","1068","    max_val : float or int, default=None"],"delete":["137","    force_all_finite : boolean or 'allow-nan', (default=True)","322","    force_all_finite : boolean or 'allow-nan', (default=True)","415","    accept_sparse : string, boolean or list\/tuple of strings (default=False)","422","    accept_large_sparse : bool (default=True)","429","    dtype : string, type, list of types or None (default=\"numeric\")","435","    order : 'F', 'C' or None (default=None)","442","    copy : boolean (default=False)","446","    force_all_finite : boolean or 'allow-nan', (default=True)","461","    ensure_2d : boolean (default=True)","464","    allow_nd : boolean (default=False)","467","    ensure_min_samples : int (default=1)","471","    ensure_min_features : int (default=1)","478","    estimator : str or estimator instance (default=None)","712","    accept_sparse : string, boolean or list of string (default=False)","719","    accept_large_sparse : bool (default=True)","726","    dtype : string, type, list of types or None (default=\"numeric\")","732","    order : 'F', 'C' or None (default=None)","735","    copy : boolean (default=False)","739","    force_all_finite : boolean or 'allow-nan', (default=True)","755","    ensure_2d : boolean (default=True)","758","    allow_nd : boolean (default=False)","761","    multi_output : boolean (default=False)","766","    ensure_min_samples : int (default=1)","770","    ensure_min_features : int (default=1)","777","    y_numeric : boolean (default=False)","782","    estimator : str or estimator instance (default=None)","915","    raise_warning : boolean (default=True)","917","    raise_exception : boolean (default=False)","990","    all_or_any : callable, {all, any}, default all","1064","    min_val : float or int, optional (default=None)","1068","    max_val : float or int, optional (default=None)"]}]}},"f72d9711f5242cd73f91551b75399dcfb88a5861":{"changes":{"sklearn\/tests\/test_docstring_parameters.py":"MODIFY","sklearn\/cluster\/_affinity_propagation.py":"MODIFY","sklearn\/cluster\/tests\/test_affinity_propagation.py":"MODIFY","doc\/whats_new\/v0.23.rst":"MODIFY"},"diff":{"sklearn\/tests\/test_docstring_parameters.py":[{"add":["200","    # TO BE REMOVED for v0.25 (avoid FutureWarning)","201","    if Estimator.__name__ == 'AffinityPropagation':","202","        est.random_state = 63","203",""],"delete":[]}],"sklearn\/cluster\/_affinity_propagation.py":[{"add":["12","from ..utils import as_float_array, check_array, check_random_state","34","                         return_n_iter=False, random_state='warn'):","74","    random_state : int or np.random.RandomStateInstance, default: 0","75","        Pseudo-random number generator to control the starting state.","76","        Use an int for reproducible results across function calls.","77","        See the :term:`Glossary <random_state>`.","78","","79","        .. versionadded:: 0.23","80","            this parameter was previously hardcoded as 0.","81","","143","    if random_state == 'warn':","144","        warnings.warn((\"'random_state' has been introduced in 0.23. \"","145","                       \"It will be set to None starting from 0.25 which \"","146","                       \"means that results will differ at every function \"","147","                       \"call. Set 'random_state' to None to silence this \"","148","                       \"warning, or to 0 to keep the behavior of versions \"","149","                       \"<0.23.\"),","150","                      FutureWarning)","151","        random_state = 0","152","    random_state = check_random_state(random_state)","293","    random_state : int or np.random.RandomStateInstance, default: 0","294","        Pseudo-random number generator to control the starting state.","295","        Use an int for reproducible results across function calls.","296","        See the :term:`Glossary <random_state>`.","297","","298","        .. versionadded:: 0.23","299","            this parameter was previously hardcoded as 0.","342","","343","    Examples","344","    --------","345","    >>> from sklearn.cluster import AffinityPropagation","346","    >>> import numpy as np","347","    >>> X = np.array([[1, 2], [1, 4], [1, 0],","348","    ...               [4, 2], [4, 4], [4, 0]])","349","    >>> clustering = AffinityPropagation(random_state=5).fit(X)","350","    >>> clustering","351","    AffinityPropagation(random_state=5)","352","    >>> clustering.labels_","353","    array([0, 0, 0, 1, 1, 1])","354","    >>> clustering.predict([[0, 0], [4, 4]])","355","    array([0, 1])","356","    >>> clustering.cluster_centers_","357","    array([[1, 2],","358","           [4, 2]])","363","                 verbose=False, random_state='warn'):","372","        self.random_state = random_state","415","                copy=self.copy, verbose=self.verbose, return_n_iter=True,","416","                random_state=self.random_state)"],"delete":["12","from ..utils import as_float_array, check_array","34","                         return_n_iter=False):","135","    random_state = np.random.RandomState(0)","294","    Examples","295","    --------","296","    >>> from sklearn.cluster import AffinityPropagation","297","    >>> import numpy as np","298","    >>> X = np.array([[1, 2], [1, 4], [1, 0],","299","    ...               [4, 2], [4, 4], [4, 0]])","300","    >>> clustering = AffinityPropagation().fit(X)","301","    >>> clustering","302","    AffinityPropagation()","303","    >>> clustering.labels_","304","    array([0, 0, 0, 1, 1, 1])","305","    >>> clustering.predict([[0, 0], [4, 4]])","306","    array([0, 1])","307","    >>> clustering.cluster_centers_","308","    array([[1, 2],","309","           [4, 2]])","310","","339","                 verbose=False):","390","                copy=self.copy, verbose=self.verbose, return_n_iter=True)"]}],"sklearn\/cluster\/tests\/test_affinity_propagation.py":[{"add":["35","        S, preference=preference, random_state=39)","41","    af = AffinityPropagation(preference=preference, affinity=\"precomputed\",","42","                             random_state=28)","45","    af = AffinityPropagation(preference=preference, verbose=True,","46","                             random_state=37)","59","                                             copy=False, random_state=74)","67","    af = AffinityPropagation(affinity=\"unknown\", random_state=78)","70","    af_2 = AffinityPropagation(affinity='precomputed', random_state=21)","76","    af = AffinityPropagation(affinity=\"euclidean\", random_state=63)","91","    af = AffinityPropagation(affinity=\"precomputed\", random_state=57)","104","    af = AffinityPropagation(preference=-10, max_iter=1, random_state=82)","133","        affinity_propagation, S, preference=[-20, -10], random_state=37)","147","                      AffinityPropagation(preference=-10,","148","                                          max_iter=1, random_state=75).fit, X)","161","    af = AffinityPropagation(affinity='euclidean',","162","                             max_iter=2, random_state=34).fit(X)","187","def test_affinity_propagation_random_state():","188","    # Significance of random_state parameter","189","    # Generate sample data","190","    centers = [[1, 1], [-1, -1], [1, -1]]","191","    X, labels_true = make_blobs(n_samples=300, centers=centers,","192","                                cluster_std=0.5, random_state=0)","193","    # random_state = 0","194","    ap = AffinityPropagation(convergence_iter=1, max_iter=2, random_state=0)","195","    ap.fit(X)","196","    centers0 = ap.cluster_centers_","197","","198","    # random_state = 76","199","    ap = AffinityPropagation(convergence_iter=1, max_iter=2, random_state=76)","200","    ap.fit(X)","201","    centers76 = ap.cluster_centers_","202","","203","    assert np.mean((centers0 - centers76) ** 2) > 1","204","","205","","206","# FIXME: to be removed in 0.25","207","def test_affinity_propagation_random_state_warning():","208","    # test that a warning is raised when random_state is not defined.","209","    X = np.array([[0, 0], [1, 1], [-2, -2]])","210","    match = (\"'random_state' has been introduced in 0.23. \"","211","             \"It will be set to None starting from 0.25 which \"","212","             \"means that results will differ at every function \"","213","             \"call. Set 'random_state' to None to silence this \"","214","             \"warning, or to 0 to keep the behavior of versions \"","215","             \"<0.23.\")","216","    with pytest.warns(FutureWarning, match=match):","217","        AffinityPropagation().fit(X)","218","","226","    ap = AffinityPropagation(random_state=46)"],"delete":["35","        S, preference=preference)","41","    af = AffinityPropagation(preference=preference, affinity=\"precomputed\")","44","    af = AffinityPropagation(preference=preference, verbose=True)","57","                                             copy=False)","65","    af = AffinityPropagation(affinity=\"unknown\")","68","    af_2 = AffinityPropagation(affinity='precomputed')","74","    af = AffinityPropagation(affinity=\"euclidean\")","89","    af = AffinityPropagation(affinity=\"precomputed\")","102","    af = AffinityPropagation(preference=-10, max_iter=1)","131","        affinity_propagation, S, preference=[-20, -10])","145","                      AffinityPropagation(preference=-10, max_iter=1).fit, X)","158","    af = AffinityPropagation(affinity='euclidean', max_iter=2).fit(X)","190","    ap = AffinityPropagation()"]}],"doc\/whats_new\/v0.23.rst":[{"add":["145","- |API| The ``random_state`` parameter has been added to ","146","  :class:`cluster.AffinityPropagation`. :pr:`16801` by :user:`rcwoolston`","147","  and :user:`Chiara Marmo <cmarmo>`.","148",""],"delete":[]}]}},"94a9f9a0b04703ae98d61a7e9a98a4dcaac548e8":{"changes":{"doc\/developers\/advanced_installation.rst":"MODIFY","build_tools\/azure\/install.sh":"MODIFY"},"diff":{"doc\/developers\/advanced_installation.rst":[{"add":["243","        \"conda-forge::compilers>=1.0.4,!=1.1.0\" conda-forge::llvm-openmp"],"delete":["243","        \"conda-forge::compilers>=1.0.4\" conda-forge::llvm-openmp"]}],"build_tools\/azure\/install.sh":[{"add":["40","            # TODO: Remove !=1.1.0 when the following is fixed:","41","            # sklearn\/svm\/_libsvm.cpython-38-darwin.so,","42","            # 2): Symbol not found: _svm_check_parameter error","43","            TO_INSTALL=\"$TO_INSTALL conda-forge::compilers>=1.0.4,!=1.1.0 \\"],"delete":["40","            TO_INSTALL=\"$TO_INSTALL conda-forge::compilers>=1.0.4 \\"]}]}},"9fc7bceca6f6c61d8d09f1e82b40d268972ec2d4":{"changes":{"sklearn\/linear_model\/tests\/test_least_angle.py":"MODIFY","doc\/whats_new\/v0.23.rst":"MODIFY","sklearn\/linear_model\/_least_angle.py":"MODIFY"},"diff":{"sklearn\/linear_model\/tests\/test_least_angle.py":[{"add":["762","","763","","764","def test_copy_X_with_auto_gram():","765","    # Non-regression test for #17789, `copy_X=True` and Gram='auto' does not","766","    # overwrite X","767","    rng = np.random.RandomState(42)","768","    X = rng.rand(6, 6)","769","    y = rng.rand(6)","770","","771","    X_before = X.copy()","772","    linear_model.lars_path(X, y, Gram='auto', copy_X=True, method='lasso')","773","    # X did not change","774","    assert_allclose(X, X_before)"],"delete":[]}],"doc\/whats_new\/v0.23.rst":[{"add":["19",":mod:`sklearn.linear_model`","20","...........................","21","","22","- |Fix| :func:`linear_model.lars_path` does not overwrite `X` when","23","  `X_copy=True` and `Gram='auto'`. :pr:`17914` by `Thomas Fan`_.","24",""],"delete":[]}],"sklearn\/linear_model\/_least_angle.py":[{"add":["428","","429","    if copy_X and X is not None and Gram is None:","430","        # force copy. setting the array to be fortran-ordered","431","        # speeds up the calculation of the (partial) Gram matrix","432","        # and allows to easily swap columns","433","        X = X.copy('F')","434",""],"delete":["413","        if copy_X:","414","            # force copy. setting the array to be fortran-ordered","415","            # speeds up the calculation of the (partial) Gram matrix","416","            # and allows to easily swap columns","417","            X = X.copy('F')","418",""]}]}},"673c31a8801effabc207a6247c18a3c28dd502f4":{"changes":{"azure-pipelines.yml":"MODIFY","build_tools\/azure\/posix.yml":"MODIFY","build_tools\/azure\/test_script.cmd":"MODIFY","build_tools\/azure\/posix-32.yml":"MODIFY","build_tools\/azure\/test_script.sh":"MODIFY","build_tools\/azure\/install.cmd":"MODIFY","build_tools\/azure\/install.sh":"MODIFY","build_tools\/azure\/windows.yml":"MODIFY"},"diff":{"azure-pipelines.yml":[{"add":["101","        PYTEST_XDIST: 'false'","120","        PYTEST_XDIST: 'false'"],"delete":[]}],"build_tools\/azure\/posix.yml":[{"add":["18","    PYTEST_XDIST: 'true'"],"delete":[]}],"build_tools\/azure\/test_script.cmd":[{"add":["10","if \"%PYTEST_XDIST%\" == \"true\" (","11","    set PYTEST_ARGS=%PYTEST_ARGS% -n2","12",")","13",""],"delete":["5","    set PYTEST_ARGS=%PYTEST_ARGS% -n2"]}],"build_tools\/azure\/posix-32.yml":[{"add":["18","    PYTEST_XDIST: 'true'","43","        -e PYTEST_XDIST=$PYTEST_XDIST"],"delete":[]}],"build_tools\/azure\/test_script.sh":[{"add":["34","if [[ \"$PYTEST_XDIST\" == \"true\" ]]; then"],"delete":["34","if [[ \"$PYTHON_VERSION\" == \"*\" ]]; then"]}],"build_tools\/azure\/install.cmd":[{"add":["27","","28","IF \"%PYTEST_XDIST%\" == \"true\" (","29","    pip install pytest-xdist","30",")","31",""],"delete":["24","    pip install pytest-xdist"]}],"build_tools\/azure\/install.sh":[{"add":["94","    python -m pip install pytest==$PYTEST_VERSION pytest-cov","102","    python -m pip install pytest==$PYTEST_VERSION pytest-cov","116","if [[ \"$PYTEST_XDIST\" == \"true\" ]]; then","117","    python -m pip install pytest-xdist","118","fi","119",""],"delete":["75","    if [[ \"$PYTHON_VERSION\" == \"*\" ]]; then","76","        python -m pip install pytest-xdist","77","    fi","78","","98","    python -m pip install pytest==$PYTEST_VERSION pytest-cov pytest-xdist","106","    python -m pip install pytest==$PYTEST_VERSION pytest-cov pytest-xdist"]}],"build_tools\/azure\/windows.yml":[{"add":["19","    PYTEST_XDIST: 'true'"],"delete":[]}]}},"d8be25f65c1fdf5aadabac745b8ae6cd8920a0ad":{"changes":{"sklearn\/mixture\/_bayesian_mixture.py":"MODIFY","sklearn\/mixture\/_base.py":"MODIFY","sklearn\/mixture\/_gaussian_mixture.py":"MODIFY"},"diff":{"sklearn\/mixture\/_bayesian_mixture.py":[{"add":["25","    dirichlet_concentration : array-like of shape (n_samples,)","42","    degrees_of_freedom : array-like of shape (n_components,)","46","    log_det_precision_chol : array-like of shape (n_components,)","54","    log_wishart_norm : array-like of shape (n_components,)","84","    n_components : int, default=1","100","    tol : float, default=1e-3","105","    reg_covar : float, default=1e-6","109","    max_iter : int, default=100","112","    n_init : int, default=1","116","    init_params : {'kmeans', 'random'}, default='kmeans'","124","    weight_concentration_prior_type : str, default='dirichlet_process'","165","    random_state : int, RandomState instance or None, default=None","173","    warm_start : bool, default=False","179","    verbose : int, default=0","185","    verbose_interval : int, default=10","190","    weights_ : array-like of shape (n_components,)","193","    means_ : array-like of shape (n_components, n_features)","257","    weight_concentration_ : array-like of shape (n_components,)","268","    mean_precision_ : array-like of shape (n_components,)","271","    mean_prior_ : array-like of shape (n_features,)","278","    degrees_of_freedom_ : array-like of shape (n_components,)","350","        X : array-like of shape (n_samples, n_features)","388","        X : array-like of shape (n_samples, n_features)","414","        X : array-like of shape (n_samples, n_features)","432","        X : array-like of shape (n_samples, n_features)","473","        X : array-like of shape (n_samples, n_features)","475","        resp : array-like of shape (n_samples, n_components)","489","        nk : array-like of shape (n_components,)","507","        nk : array-like of shape (n_components,)","509","        xk : array-like of shape (n_components, n_features)","521","        nk : array-like of shape (n_components,)","523","        xk : array-like of shape (n_components, n_features)","546","        X : array-like of shape (n_samples, n_features)","548","        nk : array-like of shape (n_components,)","550","        xk : array-like of shape (n_components, n_features)","552","        sk : array-like of shape (n_components, n_features, n_features)","580","        X : array-like of shape (n_samples, n_features)","582","        nk : array-like of shape (n_components,)","584","        xk : array-like of shape (n_components, n_features)","586","        sk : array-like of shape (n_features, n_features)","610","        X : array-like of shape (n_samples, n_features)","612","        nk : array-like of shape (n_components,)","614","        xk : array-like of shape (n_components, n_features)","616","        sk : array-like of shape (n_components, n_features)","639","        X : array-like of shape (n_samples, n_features)","641","        nk : array-like of shape (n_components,)","643","        xk : array-like of shape (n_components, n_features)","645","        sk : array-like of shape (n_components,)","668","        X : array-like of shape (n_samples, n_features)","670","        log_resp : array-like of shape (n_samples, n_components)","719","        X : array-like of shape (n_samples, n_features)"],"delete":["25","    dirichlet_concentration : array-like, shape (n_samples,)","42","    degrees_of_freedom : array-like, shape (n_components,)","46","    log_det_precision_chol : array-like, shape (n_components,)","54","    log_wishart_norm : array-like, shape (n_components,)","84","    n_components : int, defaults=1.","100","    tol : float, defaults=1e-3.","105","    reg_covar : float, defaults=1e-6.","109","    max_iter : int, defaults=100.","112","    n_init : int, defaults=1.","116","    init_params : {'kmeans', 'random'}, defaults='kmeans'.","124","    weight_concentration_prior_type : str, defaults='dirichlet_process'.","165","    random_state : int, RandomState instance or None, default=None.","173","    warm_start : bool, default=False.","179","    verbose : int, default=0.","185","    verbose_interval : int, default=10.","190","    weights_ : array-like, shape (n_components,)","193","    means_ : array-like, shape (n_components, n_features)","257","    weight_concentration_ : array-like, shape (n_components,)","268","    mean_precision_ : array-like, shape (n_components,)","271","    mean_prior_ : array-like, shape (n_features,)","278","    degrees_of_freedom_ : array-like, shape (n_components,)","350","        X : array-like, shape (n_samples, n_features)","388","        X : array-like, shape (n_samples, n_features)","414","        X : array-like, shape (n_samples, n_features)","432","        X : array-like, shape (n_samples, n_features)","473","        X : array-like, shape (n_samples, n_features)","475","        resp : array-like, shape (n_samples, n_components)","489","        nk : array-like, shape (n_components,)","507","        nk : array-like, shape (n_components,)","509","        xk : array-like, shape (n_components, n_features)","521","        nk : array-like, shape (n_components,)","523","        xk : array-like, shape (n_components, n_features)","546","        X : array-like, shape (n_samples, n_features)","548","        nk : array-like, shape (n_components,)","550","        xk : array-like, shape (n_components, n_features)","552","        sk : array-like, shape (n_components, n_features, n_features)","580","        X : array-like, shape (n_samples, n_features)","582","        nk : array-like, shape (n_components,)","584","        xk : array-like, shape (n_components, n_features)","586","        sk : array-like, shape (n_features, n_features)","610","        X : array-like, shape (n_samples, n_features)","612","        nk : array-like, shape (n_components,)","614","        xk : array-like, shape (n_components, n_features)","616","        sk : array-like, shape (n_components, n_features)","639","        X : array-like, shape (n_samples, n_features)","641","        nk : array-like, shape (n_components,)","643","        xk : array-like, shape (n_components, n_features)","645","        sk : array-like, shape (n_components,)","668","        X : array-like, shape (n_samples, n_features)","670","        log_resp : array-like, shape (n_samples, n_components)","719","        X : array-like, shape (n_samples, n_features)"]}],"sklearn\/mixture\/_base.py":[{"add":["43","    X : array-like of shape (n_samples, n_features)","90","        X : array-like of shape (n_samples, n_features)","127","        X : array-like of shape  (n_samples, n_features)","136","        X : array-like of shape  (n_samples, n_features)","164","        X : array-like of shape  (n_samples, n_features)","166","        resp : array-like of shape (n_samples, n_components)","184","        X : array-like of shape (n_samples, n_features)","210","        X : array-like of shape (n_samples, n_features)","286","        X : array-like of shape (n_samples, n_features)","306","        X : array-like of shape (n_samples, n_features)","308","        log_resp : array-like of shape (n_samples, n_components)","327","        X : array-like of shape (n_samples, n_features)","346","        X : array-like of shape (n_samples, n_dimensions)","362","        X : array-like of shape (n_samples, n_features)","380","        X : array-like of shape (n_samples, n_features)","400","        n_samples : int, default=1","401","            Number of samples to generate.","449","        X : array-like of shape (n_samples, n_features)","475","        X : array-like of shape (n_samples, n_features)","492","        X : array-like of shape (n_samples, n_features)"],"delete":["43","    X : array-like, shape (n_samples, n_features)","90","        X : array-like, shape (n_samples, n_features)","127","        X : array-like, shape  (n_samples, n_features)","136","        X : array-like, shape  (n_samples, n_features)","164","        X : array-like, shape  (n_samples, n_features)","166","        resp : array-like, shape (n_samples, n_components)","184","        X : array-like, shape (n_samples, n_features)","210","        X : array-like, shape (n_samples, n_features)","286","        X : array-like, shape (n_samples, n_features)","306","        X : array-like, shape (n_samples, n_features)","308","        log_resp : array-like, shape (n_samples, n_components)","327","        X : array-like, shape (n_samples, n_features)","346","        X : array-like, shape (n_samples, n_dimensions)","362","        X : array-like, shape (n_samples, n_features)","380","        X : array-like, shape (n_samples, n_features)","400","        n_samples : int, optional","401","            Number of samples to generate. Defaults to 1.","449","        X : array-like, shape (n_samples, n_features)","475","        X : array-like, shape (n_samples, n_features)","492","        X : array-like, shape (n_samples, n_features)"]}],"sklearn\/mixture\/_gaussian_mixture.py":[{"add":["24","    weights : array-like of shape (n_components,)","57","    means : array-like of shape (n_components, n_features)","146","    resp : array-like of shape (n_samples, n_components)","148","    X : array-like of shape (n_samples, n_features)","150","    nk : array-like of shape (n_components,)","152","    means : array-like of shape (n_components, n_features)","175","    resp : array-like of shape (n_samples, n_components)","177","    X : array-like of shape (n_samples, n_features)","179","    nk : array-like of shape (n_components,)","181","    means : array-like of shape (n_components, n_features)","203","    responsibilities : array-like of shape (n_samples, n_components)","205","    X : array-like of shape (n_samples, n_features)","207","    nk : array-like of shape (n_components,)","209","    means : array-like of shape (n_components, n_features)","229","    responsibilities : array-like of shape (n_samples, n_components)","231","    X : array-like of shape (n_samples, n_features)","233","    nk : array-like of shape (n_components,)","235","    means : array-like of shape (n_components, n_features)","253","    X : array-like of shape (n_samples, n_features)","256","    resp : array-like of shape (n_samples, n_components)","267","    nk : array-like of shape (n_components,)","270","    means : array-like of shape (n_components, n_features)","358","    log_det_precision_chol : array-like of shape (n_components,)","384","    X : array-like of shape (n_samples, n_features)","386","    means : array-like of shape (n_components, n_features)","446","    n_components : int, default=1","449","    covariance_type : {'full', 'tied', 'diag', 'spherical'}, default='full'","462","    tol : float, default=1e-3","466","    reg_covar : float, default=1e-6","470","    max_iter : int, default=100","473","    n_init : int, default=1","476","    init_params : {'kmeans', 'random'}, default='kmeans'","484","    weights_init : array-like of shape (n_components, ), default=None","485","        The user-provided initial weights.","488","    means_init : array-like of shape (n_components, n_features), default=None","489","        The user-provided initial means,","492","    precisions_init : array-like, default=None","494","        matrices).","503","    random_state : int, RandomState instance or None, default=None","511","    warm_start : bool, default=False","519","    verbose : int, default=0","525","    verbose_interval : int, default=10","530","    weights_ : array-like of shape (n_components,)","533","    means_ : array-like of shape (n_components, n_features)","645","        X : array-like of shape (n_samples, n_features)","647","        resp : array-like of shape (n_samples, n_components)","678","        X : array-like of shape (n_samples, n_features)","680","        log_resp : array-like of shape (n_samples, n_components)"],"delete":["24","    weights : array-like, shape (n_components,)","57","    means : array-like, shape (n_components, n_features)","146","    resp : array-like, shape (n_samples, n_components)","148","    X : array-like, shape (n_samples, n_features)","150","    nk : array-like, shape (n_components,)","152","    means : array-like, shape (n_components, n_features)","175","    resp : array-like, shape (n_samples, n_components)","177","    X : array-like, shape (n_samples, n_features)","179","    nk : array-like, shape (n_components,)","181","    means : array-like, shape (n_components, n_features)","203","    responsibilities : array-like, shape (n_samples, n_components)","205","    X : array-like, shape (n_samples, n_features)","207","    nk : array-like, shape (n_components,)","209","    means : array-like, shape (n_components, n_features)","229","    responsibilities : array-like, shape (n_samples, n_components)","231","    X : array-like, shape (n_samples, n_features)","233","    nk : array-like, shape (n_components,)","235","    means : array-like, shape (n_components, n_features)","253","    X : array-like, shape (n_samples, n_features)","256","    resp : array-like, shape (n_samples, n_components)","267","    nk : array-like, shape (n_components,)","270","    means : array-like, shape (n_components, n_features)","358","    log_det_precision_chol : array-like, shape (n_components,)","384","    X : array-like, shape (n_samples, n_features)","386","    means : array-like, shape (n_components, n_features)","446","    n_components : int, defaults to 1.","449","    covariance_type : {'full' (default), 'tied', 'diag', 'spherical'}","462","    tol : float, defaults to 1e-3.","466","    reg_covar : float, defaults to 1e-6.","470","    max_iter : int, defaults to 100.","473","    n_init : int, defaults to 1.","476","    init_params : {'kmeans', 'random'}, defaults to 'kmeans'.","484","    weights_init : array-like, shape (n_components, ), optional","485","        The user-provided initial weights, defaults to None.","488","    means_init : array-like, shape (n_components, n_features), optional","489","        The user-provided initial means, defaults to None,","492","    precisions_init : array-like, optional.","494","        matrices), defaults to None.","503","    random_state : int, RandomState instance or None, optional (default=None)","511","    warm_start : bool, default to False.","519","    verbose : int, default to 0.","525","    verbose_interval : int, default to 10.","530","    weights_ : array-like, shape (n_components,)","533","    means_ : array-like, shape (n_components, n_features)","645","        X : array-like, shape (n_samples, n_features)","647","        resp : array-like, shape (n_samples, n_components)","678","        X : array-like, shape (n_samples, n_features)","680","        log_resp : array-like, shape (n_samples, n_components)"]}]}},"76f2479e2fd3ece249cb67a5324776098c50bdce":{"changes":{"sklearn\/mixture\/_gaussian_mixture.py":"MODIFY"},"diff":{"sklearn\/mixture\/_gaussian_mixture.py":[{"add":["486","        If it is None, weights are initialized using the `init_params` method.","490","        If it is None, means are initialized using the `init_params` method.","495","        If it is None, precisions are initialized using the 'init_params'","496","        method."],"delete":["486","        If it None, weights are initialized using the `init_params` method.","490","        If it None, means are initialized using the `init_params` method.","495","        If it None, precisions are initialized using the 'init_params' method."]}]}},"e770715c434739647ddbb645ff0fcd40c64ba1fd":{"changes":{"sklearn\/decomposition\/_dict_learning.py":"MODIFY","sklearn\/decomposition\/tests\/test_dict_learning.py":"MODIFY","sklearn\/linear_model\/tests\/test_logistic.py":"MODIFY","sklearn\/isotonic.py":"MODIFY","sklearn\/cluster\/_dbscan.py":"MODIFY","sklearn\/utils\/class_weight.py":"MODIFY","sklearn\/covariance\/_graph_lasso.py":"MODIFY","sklearn\/cluster\/tests\/test_spectral.py":"MODIFY","sklearn\/linear_model\/_logistic.py":"MODIFY","sklearn\/random_projection.py":"MODIFY","sklearn\/decomposition\/_fastica.py":"MODIFY","sklearn\/utils\/graph.py":"MODIFY","sklearn\/utils\/tests\/test_estimator_checks.py":"MODIFY","sklearn\/covariance\/_empirical_covariance.py":"MODIFY","sklearn\/calibration.py":"MODIFY","sklearn\/covariance\/_shrunk_covariance.py":"MODIFY","sklearn\/cluster\/_kmeans.py":"MODIFY","sklearn\/svm\/_base.py":"MODIFY","sklearn\/utils\/extmath.py":"MODIFY","sklearn\/feature_extraction\/image.py":"MODIFY","sklearn\/cluster\/_agglomerative.py":"MODIFY","sklearn\/svm\/tests\/test_svm.py":"MODIFY","sklearn\/utils\/tests\/test_sparsefuncs.py":"MODIFY","sklearn\/utils\/tests\/test_extmath.py":"MODIFY","examples\/linear_model\/plot_lasso_coordinate_descent_path.py":"MODIFY","sklearn\/linear_model\/_coordinate_descent.py":"MODIFY","sklearn\/cluster\/_affinity_propagation.py":"MODIFY","sklearn\/cluster\/_spectral.py":"MODIFY","sklearn\/feature_selection\/_univariate_selection.py":"MODIFY","sklearn\/utils\/__init__.py":"MODIFY","sklearn\/decomposition\/_nmf.py":"MODIFY","sklearn\/linear_model\/_stochastic_gradient.py":"MODIFY","sklearn\/decomposition\/tests\/test_nmf.py":"MODIFY","sklearn\/feature_extraction\/tests\/test_image.py":"MODIFY","sklearn\/utils\/sparsefuncs.py":"MODIFY","sklearn\/utils\/tests\/test_class_weight.py":"MODIFY","sklearn\/linear_model\/tests\/test_sgd.py":"MODIFY","sklearn\/decomposition\/_sparse_pca.py":"MODIFY","sklearn\/cluster\/_optics.py":"MODIFY","sklearn\/feature_selection\/_mutual_info.py":"MODIFY","sklearn\/cluster\/_mean_shift.py":"MODIFY","sklearn\/linear_model\/tests\/test_sag.py":"MODIFY","sklearn\/cluster\/tests\/test_hierarchical.py":"MODIFY","sklearn\/cluster\/tests\/test_mean_shift.py":"MODIFY"},"diff":{"sklearn\/decomposition\/_dict_learning.py":[{"add":["187","@_deprecate_positional_args","188","def sparse_encode(X, dictionary, *, gram=None, cov=None,","189","                  algorithm='lasso_lars', n_nonzero_coefs=None, alpha=None,","190","                  copy_cov=True, init=None, max_iter=1000, n_jobs=None,","191","                  check_input=True, verbose=0, positive=False):","424","@_deprecate_positional_args","425","def dict_learning(X, n_components, *, alpha, max_iter=100, tol=1e-8,","619","@_deprecate_positional_args","620","def dict_learning_online(X, n_components=2, *, alpha=1, n_iter=100,","1235","            X, n_components, alpha=self.alpha,","1439","            X, self.n_components, alpha=self.alpha,","1491","            X, self.n_components, alpha=self.alpha,"],"delete":["187","def sparse_encode(X, dictionary, gram=None, cov=None, algorithm='lasso_lars',","188","                  n_nonzero_coefs=None, alpha=None, copy_cov=True, init=None,","189","                  max_iter=1000, n_jobs=None, check_input=True, verbose=0,","190","                  positive=False):","423","def dict_learning(X, n_components, alpha, max_iter=100, tol=1e-8,","617","def dict_learning_online(X, n_components=2, alpha=1, n_iter=100,","1232","            X, n_components, self.alpha,","1436","            X, self.n_components, self.alpha,","1488","            X, self.n_components, self.alpha,"]}],"sklearn\/decomposition\/tests\/test_dict_learning.py":[{"add":["109","        dict_learning(X, n_components, alpha=alpha, positive_code=True)","249","        dict_learning_online(X, alpha=alpha, positive_code=True)"],"delete":["109","        dict_learning(X, n_components, alpha, positive_code=True)","249","        dict_learning_online(X, alpha, positive_code=True)"]}],"sklearn\/linear_model\/tests\/test_logistic.py":[{"add":["885","    class_weight = compute_class_weight(\"balanced\", classes=classes, y=y)"],"delete":["885","    class_weight = compute_class_weight(\"balanced\", classes, y)"]}],"sklearn\/isotonic.py":[{"add":["78","@_deprecate_positional_args","79","def isotonic_regression(y, *, sample_weight=None, y_min=None, y_max=None,","256","        y = isotonic_regression(unique_y, sample_weight=unique_sample_weight,","257","                                y_min=self.y_min, y_max=self.y_max,"],"delete":["78","def isotonic_regression(y, sample_weight=None, y_min=None, y_max=None,","255","        y = isotonic_regression(unique_y, unique_sample_weight,","256","                                self.y_min, self.y_max,"]}],"sklearn\/cluster\/_dbscan.py":[{"add":["22","@_deprecate_positional_args","23","def dbscan(X, eps=0.5, *, min_samples=5, metric='minkowski',","24","           metric_params=None, algorithm='auto', leaf_size=30, p=2,","25","           sample_weight=None, n_jobs=None):"],"delete":["22","def dbscan(X, eps=0.5, min_samples=5, metric='minkowski', metric_params=None,","23","           algorithm='auto', leaf_size=30, p=2, sample_weight=None,","24","           n_jobs=None):"]}],"sklearn\/utils\/class_weight.py":[{"add":["9","@_deprecate_positional_args","10","def compute_class_weight(class_weight, *, classes, y):","156","                                                    classes=classes_subsample,","157","                                                    y=y_subsample),","165","                                            classes=classes_full,","166","                                            y=y_full)"],"delete":["9","def compute_class_weight(class_weight, classes, y):","155","                                                    classes_subsample,","156","                                                    y_subsample),","164","                                            classes_full,","165","                                            y_full)"]}],"sklearn\/covariance\/_graph_lasso.py":[{"add":["77","@_deprecate_positional_args","78","def graphical_lasso(emp_cov, alpha, *, cov_init=None, mode='cd', tol=1e-4,"],"delete":["77","","78","def graphical_lasso(emp_cov, alpha, cov_init=None, mode='cd', tol=1e-4,"]}],"sklearn\/cluster\/tests\/test_spectral.py":[{"add":["189","        y_pred = discretize(y_true_noisy, random_state=random_state)"],"delete":["189","        y_pred = discretize(y_true_noisy, random_state)"]}],"sklearn\/linear_model\/_logistic.py":[{"add":["664","        class_weight_ = compute_class_weight(class_weight,","665","                                             classes=classes, y=y)","679","            class_weight_ = compute_class_weight(class_weight,","680","                                                 classes=mask_classes,","681","                                                 y=y_bin)","1871","            class_weight = compute_class_weight(","1872","                class_weight, classes=np.arange(len(self.classes_)), y=y)"],"delete":["664","        class_weight_ = compute_class_weight(class_weight, classes, y)","678","            class_weight_ = compute_class_weight(class_weight, mask_classes,","679","                                                 y_bin)","1869","            class_weight = compute_class_weight(class_weight,","1870","                                                np.arange(len(self.classes_)),","1871","                                                y)"]}],"sklearn\/random_projection.py":[{"add":["51","@_deprecate_positional_args","52","def johnson_lindenstrauss_min_dim(n_samples, *, eps=0.1):"],"delete":["51","def johnson_lindenstrauss_min_dim(n_samples, eps=0.1):"]}],"sklearn\/decomposition\/_fastica.py":[{"add":["149","@_deprecate_positional_args","150","def fastica(X, n_components=None, *, algorithm=\"parallel\", whiten=True,"],"delete":["149","def fastica(X, n_components=None, algorithm=\"parallel\", whiten=True,"]}],"sklearn\/utils\/graph.py":[{"add":["15","from .validation import _deprecate_positional_args","21","@_deprecate_positional_args","22","def single_source_shortest_path_length(graph, source, *, cutoff=None):"],"delete":["20","","21","def single_source_shortest_path_length(graph, source, cutoff=None):"]}],"sklearn\/utils\/tests\/test_estimator_checks.py":[{"add":["212","        class_weight = compute_class_weight(self.class_weight, classes=classes,","213","                                            y=y)"],"delete":["212","        class_weight = compute_class_weight(self.class_weight, classes, y)"]}],"sklearn\/covariance\/_empirical_covariance.py":[{"add":["50","@_deprecate_positional_args","51","def empirical_covariance(X, *, assume_centered=False):"],"delete":["50","def empirical_covariance(X, assume_centered=False):"]}],"sklearn\/calibration.py":[{"add":["508","@_deprecate_positional_args","509","def calibration_curve(y_true, y_prob, *, normalize=False, n_bins=5,"],"delete":["508","def calibration_curve(y_true, y_prob, normalize=False, n_bins=5,"]}],"sklearn\/covariance\/_shrunk_covariance.py":[{"add":["255","@_deprecate_positional_args","256","def ledoit_wolf(X, *, assume_centered=False, block_size=1000):","433","@_deprecate_positional_args","434","def oas(X, *, assume_centered=False):"],"delete":["255","def ledoit_wolf(X, assume_centered=False, block_size=1000):","432","","433","def oas(X, assume_centered=False):"]}],"sklearn\/cluster\/_kmeans.py":[{"add":["184","@_deprecate_positional_args","185","def k_means(X, n_clusters, *, sample_weight=None, init='k-means++',"],"delete":["184","def k_means(X, n_clusters, sample_weight=None, init='k-means++',"]}],"sklearn\/svm\/_base.py":[{"add":["545","        self.class_weight_ = compute_class_weight(self.class_weight,","546","                                                  classes=cls, y=y_)","929","        class_weight_ = compute_class_weight(class_weight, classes=classes_,","930","                                             y=y)"],"delete":["545","        self.class_weight_ = compute_class_weight(self.class_weight, cls, y_)","928","        class_weight_ = compute_class_weight(class_weight, classes_, y)"]}],"sklearn\/utils\/extmath.py":[{"add":["22","from .validation import _deprecate_positional_args","118","@_deprecate_positional_args","119","def safe_sparse_dot(a, b, *, dense_output=False):","160","@_deprecate_positional_args","161","def randomized_range_finder(A, *, size, n_iter,","245","@_deprecate_positional_args","246","def randomized_svd(M, n_components, *, n_oversamples=10, n_iter='auto',","348","    Q = randomized_range_finder(","349","        M, size=n_random, n_iter=n_iter,","350","        power_iteration_normalizer=power_iteration_normalizer,","351","        random_state=random_state)","377","@_deprecate_positional_args","378","def weighted_mode(a, w, *, axis=0):"],"delete":["117","def safe_sparse_dot(a, b, dense_output=False):","158","def randomized_range_finder(A, size, n_iter,","242","def randomized_svd(M, n_components, n_oversamples=10, n_iter='auto',","344","    Q = randomized_range_finder(M, n_random, n_iter,","345","                                power_iteration_normalizer, random_state)","371","def weighted_mode(a, w, axis=0):"]}],"sklearn\/feature_extraction\/image.py":[{"add":["132","@_deprecate_positional_args","133","def img_to_graph(img, *, mask=None, return_as=sparse.coo_matrix, dtype=None):","169","@_deprecate_positional_args","170","def grid_to_graph(n_x, n_y, n_z=1, *, mask=None, return_as=sparse.coo_matrix,","348","@_deprecate_positional_args","349","def extract_patches_2d(image, patch_size, *, max_patches=None,","350","                       random_state=None):","585","                image, patch_size, max_patches=self.max_patches,","586","                random_state=self.random_state)"],"delete":["132","def img_to_graph(img, mask=None, return_as=sparse.coo_matrix, dtype=None):","168","def grid_to_graph(n_x, n_y, n_z=1, mask=None, return_as=sparse.coo_matrix,","346","def extract_patches_2d(image, patch_size, max_patches=None, random_state=None):","581","                image, patch_size, self.max_patches, self.random_state)"]}],"sklearn\/cluster\/_agglomerative.py":[{"add":["137","@_deprecate_positional_args","138","def ward_tree(X, *, connectivity=None, n_clusters=None, return_distance=False):","879","        out = memory.cache(tree_builder)(X, connectivity=connectivity,"],"delete":["137","def ward_tree(X, connectivity=None, n_clusters=None, return_distance=False):","878","        out = memory.cache(tree_builder)(X, connectivity,"]}],"sklearn\/svm\/tests\/test_svm.py":[{"add":["601","    class_weights = compute_class_weight('balanced', classes=classes,","602","                                         y=y[unbalanced])"],"delete":["601","    class_weights = compute_class_weight('balanced', classes, y[unbalanced])"]}],"sklearn\/utils\/tests\/test_sparsefuncs.py":[{"add":["107","            incr_mean_variance_axis(X=axis, axis=last_mean, last_mean=last_var,","108","                                    last_var=last_n)","110","            incr_mean_variance_axis(X_lil, axis=axis, last_mean=last_mean,","111","                                    last_var=last_var, last_n=last_n)","116","            incr_mean_variance_axis(X_csr, axis=axis, last_mean=last_mean,","117","                                    last_var=last_var, last_n=last_n)","147","                    incr_mean_variance_axis(X_sparse, axis=axis,","148","                                            last_mean=last_mean,","149","                                            last_var=last_var,","150","                                            last_n=last_n)","178","        X1, axis=axis, last_mean=last_mean, last_var=last_var, last_n=last_n","181","        X2, axis=axis, last_mean=updated_mean, last_var=updated_var,","182","        last_n=updated_n","198","        X1, axis=axis, last_mean=last_mean, last_var=last_var, last_n=last_n","202","        X2, axis=axis, last_mean=last_mean, last_var=last_var, last_n=last_n","235","        X, axis=axis, last_mean=old_means.copy(),","236","        last_var=old_variances.copy(), last_n=old_sample_count.copy())","238","        X_nan, axis=axis, last_mean=old_means.copy(),","239","        last_var=old_variances.copy(), last_n=old_sample_count.copy())"],"delete":["107","            incr_mean_variance_axis(axis, last_mean, last_var, last_n)","109","            incr_mean_variance_axis(X_lil, axis, last_mean, last_var, last_n)","114","            incr_mean_variance_axis(X_csr, axis, last_mean, last_var, last_n)","144","                    incr_mean_variance_axis(X_sparse, axis, last_mean,","145","                                            last_var, last_n)","173","        X1, axis, last_mean, last_var, last_n","176","        X2, axis, updated_mean, updated_var, updated_n","192","        X1, axis, last_mean, last_var, last_n","196","        X2, axis, last_mean, last_var, last_n","229","        X, axis, old_means.copy(), old_variances.copy(),","230","        old_sample_count.copy())","232","        X_nan, axis, old_means.copy(), old_variances.copy(),","233","        old_sample_count.copy())"]}],"sklearn\/utils\/tests\/test_extmath.py":[{"add":["61","        mode2, score2 = weighted_mode(x, weights, axis=axis)"],"delete":["61","        mode2, score2 = weighted_mode(x, weights, axis)"]}],"examples\/linear_model\/plot_lasso_coordinate_descent_path.py":[{"add":["33","alphas_lasso, coefs_lasso, _ = lasso_path(X, y, eps=eps, fit_intercept=False)","37","    X, y, eps=eps, positive=True, fit_intercept=False)"],"delete":["33","alphas_lasso, coefs_lasso, _ = lasso_path(X, y, eps, fit_intercept=False)","37","    X, y, eps, positive=True, fit_intercept=False)"]}],"sklearn\/linear_model\/_coordinate_descent.py":[{"add":["170","@_deprecate_positional_args","171","def lasso_path(X, y, *, eps=1e-3, n_alphas=100, alphas=None,","316","@_deprecate_positional_args","317","def enet_path(X, y, *, l1_ratio=0.5, eps=1e-3, n_alphas=100, alphas=None,"],"delete":["170","def lasso_path(X, y, eps=1e-3, n_alphas=100, alphas=None,","315","def enet_path(X, y, l1_ratio=0.5, eps=1e-3, n_alphas=100, alphas=None,"]}],"sklearn\/cluster\/_affinity_propagation.py":[{"add":["32","@_deprecate_positional_args","33","def affinity_propagation(S, *, preference=None, convergence_iter=15,","34","                         max_iter=200, damping=0.5, copy=True, verbose=False,","414","                self.affinity_matrix_, preference=self.preference,","415","                max_iter=self.max_iter,"],"delete":["32","def affinity_propagation(S, preference=None, convergence_iter=15, max_iter=200,","33","                         damping=0.5, copy=True, verbose=False,","413","                self.affinity_matrix_, self.preference, max_iter=self.max_iter,"]}],"sklearn\/cluster\/_spectral.py":[{"add":["20","@_deprecate_positional_args","21","def discretize(vectors, *, copy=True, max_svd_restarts=30, n_iter_max=20,","159","@_deprecate_positional_args","160","def spectral_clustering(affinity, *, n_clusters=8, n_components=None,"],"delete":["20","def discretize(vectors, copy=True, max_svd_restarts=30, n_iter_max=20,","158","def spectral_clustering(affinity, n_clusters=8, n_components=None,"]}],"sklearn\/feature_selection\/_univariate_selection.py":[{"add":["231","@_deprecate_positional_args","232","def f_regression(X, y, *, center=True):"],"delete":["231","def f_regression(X, y, center=True):"]}],"sklearn\/utils\/__init__.py":[{"add":["275","def safe_indexing(X, indices, *, axis=0):"],"delete":["275","def safe_indexing(X, indices, axis=0):"]}],"sklearn\/decomposition\/_nmf.py":[{"add":["843","@_deprecate_positional_args","844","def non_negative_factorization(X, W=None, H=None, n_components=None, *,"],"delete":["843","def non_negative_factorization(X, W=None, H=None, n_components=None,"]}],"sklearn\/linear_model\/_stochastic_gradient.py":[{"add":["499","        self._expanded_class_weight = compute_class_weight(","500","            self.class_weight, classes=self.classes_, y=y)","683","                             \" use compute_class_weight('{0}', \"","684","                             \"classes=classes, y=y). \""],"delete":["499","        self._expanded_class_weight = compute_class_weight(self.class_weight,","500","                                                           self.classes_, y)","683","                             \" use compute_class_weight('{0}', classes, y). \""]}],"sklearn\/decomposition\/tests\/test_nmf.py":[{"add":["236","    assert_raise_message(ValueError, msg, nnmf, A, A, A, 1.5, init='random')","239","    assert_raise_message(ValueError, msg, nnmf, A, A, A, '2', init='random')","241","    assert_raise_message(ValueError, msg, nnmf, A, A, -A, 2, init='custom')","243","    assert_raise_message(ValueError, msg, nnmf, A, -A, A, 2, init='custom')","245","    assert_raise_message(ValueError, msg, nnmf, A, A, 0 * A, 2, init='custom')","247","    assert_raise_message(ValueError, msg, nnmf, A, A, 0 * A, 2, init='custom',","248","                         regularization='spam')"],"delete":["236","    assert_raise_message(ValueError, msg, nnmf, A, A, A, 1.5, 'random')","239","    assert_raise_message(ValueError, msg, nnmf, A, A, A, '2', 'random')","241","    assert_raise_message(ValueError, msg, nnmf, A, A, -A, 2, 'custom')","243","    assert_raise_message(ValueError, msg, nnmf, A, -A, A, 2, 'custom')","245","    assert_raise_message(ValueError, msg, nnmf, A, A, 0 * A, 2, 'custom')","247","    assert_raise_message(ValueError, msg, nnmf, A, A, 0 * A, 2, 'custom', True,","248","                         'cd', 2., 1e-4, 200, 0., 0., 'spam')"]}],"sklearn\/feature_extraction\/tests\/test_image.py":[{"add":["70","        graph = img_to_graph(face, mask=mask)"],"delete":["70","        graph = img_to_graph(face, mask)"]}],"sklearn\/utils\/sparsefuncs.py":[{"add":["7","from .validation import _deprecate_positional_args","101","@_deprecate_positional_args","102","def incr_mean_variance_axis(X, *, axis, last_mean, last_var, last_n):"],"delete":["100","def incr_mean_variance_axis(X, axis, last_mean, last_var, last_n):"]}],"sklearn\/utils\/tests\/test_class_weight.py":[{"add":["17","    cw = compute_class_weight(\"balanced\", classes=classes, y=y)","29","        compute_class_weight(\"balanced\", classes=classes, y=y)","34","        compute_class_weight({\"label_not_present\": 1.}, classes=classes, y=y)","38","        compute_class_weight(\"balanced\", classes=classes, y=y)","40","        compute_class_weight({0: 1., 1: 2.}, classes=classes, y=y)","47","    cw = compute_class_weight(class_weights, classes=classes, y=y)","58","        compute_class_weight(class_weights, classes=classes, y=y)","63","        compute_class_weight(class_weights, classes=classes, y=y)","100","    cw = compute_class_weight(\"balanced\", classes=classes, y=y)","107","    cw = compute_class_weight(\"balanced\", classes=classes, y=y)","119","    cw = compute_class_weight(\"balanced\", classes=classes, y=y)","133","    cw = compute_class_weight(None, classes=classes, y=y)","138","    cw = compute_class_weight({2: 1.5}, classes=classes, y=y)","142","    cw = compute_class_weight({2: 1.5, 4: 0.5}, classes=classes, y=y)"],"delete":["17","    cw = compute_class_weight(\"balanced\", classes, y)","29","        compute_class_weight(\"balanced\", classes, y)","34","        compute_class_weight({\"label_not_present\": 1.}, classes, y)","38","        compute_class_weight(\"balanced\", classes, y)","40","        compute_class_weight({0: 1., 1: 2.}, classes, y)","47","    cw = compute_class_weight(class_weights, classes, y)","58","        compute_class_weight(class_weights, classes, y)","63","        compute_class_weight(class_weights, classes, y)","100","    cw = compute_class_weight(\"balanced\", classes, y)","107","    cw = compute_class_weight(\"balanced\", classes, y)","119","    cw = compute_class_weight(\"balanced\", classes, y)","133","    cw = compute_class_weight(None, classes, y)","138","    cw = compute_class_weight({2: 1.5}, classes, y)","142","    cw = compute_class_weight({2: 1.5, 4: 0.5}, classes, y)"]}],"sklearn\/linear_model\/tests\/test_sgd.py":[{"add":["575","             r\"use compute_class_weight\\('balanced', classes=classes, y=y\\). \""],"delete":["575","             r\"use compute_class_weight\\('balanced', classes, y\\). \""]}],"sklearn\/decomposition\/_sparse_pca.py":[{"add":["189","        Vt, _, E, self.n_iter_ = dict_learning(X.T, n_components,","190","                                               alpha=self.alpha,"],"delete":["189","        Vt, _, E, self.n_iter_ = dict_learning(X.T, n_components, self.alpha,"]}],"sklearn\/cluster\/_optics.py":[{"add":["263","                reachability=self.reachability_,","264","                predecessor=self.predecessor_,","265","                ordering=self.ordering_,","266","                min_samples=self.min_samples,","267","                min_cluster_size=self.min_cluster_size,","268","                xi=self.xi,","269","                predecessor_correction=self.predecessor_correction)","281","            labels_ = cluster_optics_dbscan(","282","                reachability=self.reachability_,","283","                core_distances=self.core_distances_,","284","                ordering=self.ordering_, eps=eps)","341","@_deprecate_positional_args","342","def compute_optics_graph(X, *, min_samples, max_eps, metric, p, metric_params,","541","@_deprecate_positional_args","542","def cluster_optics_dbscan(*, reachability, core_distances, ordering, eps):","581","def cluster_optics_xi(*, reachability, predecessor, ordering, min_samples,"],"delete":["263","                self.reachability_,","264","                self.predecessor_,","265","                self.ordering_,","266","                self.min_samples,","267","                self.min_cluster_size,","268","                self.xi,","269","                self.predecessor_correction)","281","            labels_ = cluster_optics_dbscan(self.reachability_,","282","                                            self.core_distances_,","283","                                            self.ordering_,","284","                                            eps)","341","def compute_optics_graph(X, min_samples, max_eps, metric, p, metric_params,","540","def cluster_optics_dbscan(reachability, core_distances, ordering, eps):","579","def cluster_optics_xi(reachability, predecessor, ordering, min_samples,"]}],"sklearn\/feature_selection\/_mutual_info.py":[{"add":["13","from ..utils.validation import _deprecate_positional_args","293","@_deprecate_positional_args","294","def mutual_info_regression(X, y, *, discrete_features='auto', n_neighbors=3,","371","@_deprecate_positional_args","372","def mutual_info_classif(X, y, *, discrete_features='auto', n_neighbors=3,"],"delete":["292","def mutual_info_regression(X, y, discrete_features='auto', n_neighbors=3,","369","def mutual_info_classif(X, y, discrete_features='auto', n_neighbors=3,"]}],"sklearn\/cluster\/_mean_shift.py":[{"add":["28","@_deprecate_positional_args","29","def estimate_bandwidth(X, *, quantile=0.3, n_samples=None, random_state=0,","109","@_deprecate_positional_args","110","def mean_shift(X, *, bandwidth=None, seeds=None, bin_seeding=False,"],"delete":["28","def estimate_bandwidth(X, quantile=0.3, n_samples=None, random_state=0,","108","def mean_shift(X, bandwidth=None, seeds=None, bin_seeding=False,"]}],"sklearn\/linear_model\/tests\/test_sag.py":[{"add":["646","    class_weight_ = compute_class_weight(class_weight, classes=np.unique(y),","647","                                         y=y)","693","    class_weight_ = compute_class_weight(class_weight, classes=np.unique(y),","694","                                         y=y)"],"delete":["646","    class_weight_ = compute_class_weight(class_weight, np.unique(y), y)","692","    class_weight_ = compute_class_weight(class_weight, np.unique(y), y)"]}],"sklearn\/cluster\/tests\/test_hierarchical.py":[{"add":["76","            tree_builder(X.T, connectivity=connectivity)","82","            tree_builder(X.T, connectivity=np.ones((4, 4)))","85","            tree_builder(X.T[:0], connectivity=connectivity)","118","        children, n_nodes, n_leaves, parent = linkage_func(","119","            X.T, connectivity=connectivity)","301","            children, _, n_leaves, _ = _TREE_BUILDERS[linkage](","302","                X, connectivity=connectivity)"],"delete":["76","            tree_builder(X.T, connectivity)","82","            tree_builder(X.T, np.ones((4, 4)))","85","            tree_builder(X.T[:0], connectivity)","118","        children, n_nodes, n_leaves, parent = linkage_func(X.T, connectivity)","300","            children, _, n_leaves, _ = _TREE_BUILDERS[linkage](X, connectivity)"]}],"sklearn\/cluster\/tests\/test_mean_shift.py":[{"add":["73","    assert_raise_message(TypeError, msg, estimate_bandwidth, X)"],"delete":["73","    assert_raise_message(TypeError, msg, estimate_bandwidth, X, 200)"]}]}},"7bbd0d56530ad3758841595d81ab8deed069879d":{"changes":{"asv_benchmarks\/benchmarks\/datasets.py":"MODIFY"},"diff":{"asv_benchmarks\/benchmarks\/datasets.py":[{"add":["57","    X, y = fetch_openml('mnist_784', version=1, return_X_y=True,","58","                        as_frame=False)"],"delete":["57","    X, y = fetch_openml('mnist_784', version=1, return_X_y=True)"]}]}},"bbab854ff4338ba02319c3ac2568cc2fb42cc976":{"changes":{"doc\/themes\/scikit-learn-modern\/nav.html":"MODIFY"},"diff":{"doc\/themes\/scikit-learn-modern\/nav.html":[{"add":["11","  (\"What's new\", pathto('whats_new\/v' + version)),"],"delete":["11","  (\"What's new\", 'whats_new\/v' + version + '.html'),"]}]}},"c4c0fe2ce4e61ba5bf8f6b3f03ac6ea8f99cf688":{"changes":{"sklearn\/svm\/setup.py":"MODIFY"},"diff":{"sklearn\/svm\/setup.py":[{"add":["19","                         # Use C++11 random number generator fix","20","                         extra_compile_args=['-std=c++11']"],"delete":[]}]}},"dad615ae7a53e0b19cd6a3db11c3362fd58f99ad":{"changes":{"doc\/conf.py":"MODIFY","doc\/sphinxext\/custom_autosummary_new_suffix.py":"MODIFY"},"diff":{"doc\/conf.py":[{"add":["416","# `sklearn.cluster.dbscan` overlapping with `sklearn.cluster.DBSCAN`  on"],"delete":["416","# `sklearn.cluster.dbscan` overlapping with `klearn.cluster.DBSCAN`  on"]}],"doc\/sphinxext\/custom_autosummary_new_suffix.py":[{"add":["21","import sphinx","71","    if sphinx.version_info[0] <= 2:","72","        raise ModuleNotFoundError(\"Please install Sphinx >= 3.0 in order \"","73","                                  \"to build docs\")","75","    # Find listener id for process_generate_options added by","76","    process_generate_options_id = None","77","    builder_inited_listeners = app.events.listeners[\"builder-inited\"]","78","    for event_listener in builder_inited_listeners:","79","        func = event_listener.handler","80","        if func.__name__ == \"process_generate_options\":","81","            process_generate_options_id = event_listener.id","83","    assert process_generate_options_id is not None","84","","85","    # Override process_generate_options added by sphinx.ext.autosummary","86","    app.disconnect(process_generate_options_id)","87","    app.connect(\"builder-inited\", process_generate_options_custom_files)"],"delete":["21","import inspect","50","","72","    # Override process_generate_options added by sphinx.ext.autosummary","73","    builder_inited_listeners = app.events.listeners[\"builder-inited\"]","75","    for listener_id, obj in builder_inited_listeners.items():","76","        if (inspect.isfunction(obj)","77","                and obj.__name__ == \"process_generate_options\"):","78","            builder_inited_listeners[listener_id] = \\","79","                process_generate_options_custom_files"]}]}},"af2abc2e72bab89f05436c964c86fd7b17bfb6b8":{"changes":{"sklearn\/externals\/_pilutil.py":"MODIFY"},"diff":{"sklearn\/externals\/_pilutil.py":[{"add":["371","            image = Image.frombytes(mode, shape, data32.tobytes())","376","            image = Image.frombytes('L', shape, bytedata.tobytes())","378","                image.putpalette(asarray(pal, dtype=uint8).tobytes())","383","                image.putpalette(asarray(pal, dtype=uint8).tobytes())","387","            image = Image.frombytes('1', shape, bytedata.tobytes())","396","            image = Image.frombytes(mode, shape, data32.tobytes())","421","        strdata = bytedata.tobytes()","424","        strdata = transpose(bytedata, (0, 2, 1)).tobytes()","427","        strdata = transpose(bytedata, (1, 2, 0)).tobytes()"],"delete":["371","            image = Image.frombytes(mode, shape, data32.tostring())","376","            image = Image.frombytes('L', shape, bytedata.tostring())","378","                image.putpalette(asarray(pal, dtype=uint8).tostring())","383","                image.putpalette(asarray(pal, dtype=uint8).tostring())","387","            image = Image.frombytes('1', shape, bytedata.tostring())","396","            image = Image.frombytes(mode, shape, data32.tostring())","421","        strdata = bytedata.tostring()","424","        strdata = transpose(bytedata, (0, 2, 1)).tostring()","427","        strdata = transpose(bytedata, (1, 2, 0)).tostring()"]}]}},"5abd22f58f152a0a899f33bb22609cc085fbfdec":{"changes":{"sklearn\/utils\/estimator_checks.py":"MODIFY"},"diff":{"sklearn\/utils\/estimator_checks.py":[{"add":["367","            estimator = _construct_instance(estimator)"],"delete":["367","            estimator = _construct_instance(estimator),"]}]}},"8010cadf6ca9d319321ac72ff5e604b941f0d40c":{"changes":{"sklearn\/svm\/tests\/test_bounds.py":"MODIFY","sklearn\/svm\/_classes.py":"MODIFY","sklearn\/svm\/_bounds.py":"MODIFY"},"diff":{"sklearn\/svm\/tests\/test_bounds.py":[{"add":["39","                         l1_min_c, dense_X, Y1, loss=\"l2\")","43","    min_c = l1_min_c(X, y, loss=loss, fit_intercept=fit_intercept,","44","                     intercept_scaling=intercept_scaling)","75","        l1_min_c(dense_X, Y1, loss='l1')"],"delete":["39","                         l1_min_c, dense_X, Y1, \"l2\")","43","    min_c = l1_min_c(X, y, loss, fit_intercept, intercept_scaling)","74","        l1_min_c(dense_X, Y1, 'l1')"]}],"sklearn\/svm\/_classes.py":[{"add":["7","from ..utils.validation import _deprecate_positional_args","180","    @_deprecate_positional_args","181","    def __init__(self, penalty='l2', loss='squared_hinge', *, dual=True,","182","                 tol=1e-4, C=1.0, multi_class='ovr', fit_intercept=True,","367","    @_deprecate_positional_args","368","    def __init__(self, *, epsilon=0.0, tol=1e-4, C=1.0,","631","    @_deprecate_positional_args","632","    def __init__(self, *, C=1.0, kernel='rbf', degree=3, gamma='scale',","843","    @_deprecate_positional_args","844","    def __init__(self, *, nu=0.5, kernel='rbf', degree=3, gamma='scale',","998","    @_deprecate_positional_args","999","    def __init__(self, *, kernel='rbf', degree=3, gamma='scale',","1144","    @_deprecate_positional_args","1145","    def __init__(self, *, nu=0.5, C=1.0, kernel='rbf', degree=3,","1259","    @_deprecate_positional_args","1260","    def __init__(self, *, kernel='rbf', degree=3, gamma='scale',"],"delete":["179","","180","    def __init__(self, penalty='l2', loss='squared_hinge', dual=True, tol=1e-4,","181","                 C=1.0, multi_class='ovr', fit_intercept=True,","366","    def __init__(self, epsilon=0.0, tol=1e-4, C=1.0,","629","    def __init__(self, C=1.0, kernel='rbf', degree=3, gamma='scale',","840","    def __init__(self, nu=0.5, kernel='rbf', degree=3, gamma='scale',","994","    def __init__(self, kernel='rbf', degree=3, gamma='scale',","1139","    def __init__(self, nu=0.5, C=1.0, kernel='rbf', degree=3,","1253","    def __init__(self, kernel='rbf', degree=3, gamma='scale',"]}],"sklearn\/svm\/_bounds.py":[{"add":["8","from ..utils.validation import _deprecate_positional_args","12","@_deprecate_positional_args","13","def l1_min_c(X, y, *, loss='squared_hinge', fit_intercept=True,"],"delete":["11","def l1_min_c(X, y, loss='squared_hinge', fit_intercept=True,"]}]}},"b9956dbb469855a4e2328e1605feb194c7deb7b5":{"changes":{"azure-pipelines.yml":"MODIFY"},"diff":{"azure-pipelines.yml":[{"add":["23","        if [[ $BUILD_REASON == \"PullRequest\" ]]; then","24","          # By default pull requests use refs\/pull\/PULL_ID\/merge as the source branch","25","          # which has a \"Merge ID into ID\" as a commit message. The latest commit","26","          # message is the second to last commit","27","          COMMIT_ID=$(echo $BUILD_SOURCEVERSIONMESSAGE | awk '{print $2}')","28","          COMMIT_MESSAGE=$(git log $COMMIT_ID -1 --pretty=%B)","29","        else","30","          COMMIT_MESSAGE=$BUILD_SOURCEVERSIONMESSAGE","31","        fi","32","        echo \"##vso[task.setvariable variable=COMMIT_MESSAGE]$COMMIT_MESSAGE\"","33","      displayName: Get source version message","34","    - bash: |","35","        set -ex","36","        if [[ \"$COMMIT_MESSAGE\" =~ \"[lint skip]\" ]]; then","46","        if [[ \"$COMMIT_MESSAGE\" =~ \"[lint skip]\" ]]; then","55","        if [[ \"$COMMIT_MESSAGE\" =~ \"[scipy-dev]\" ]] || [[ $BUILD_REASON == \"Schedule\" ]]; then","56","          echo \"Running scipy-dev\""],"delete":["23","        if [[ $BUILD_SOURCEVERSIONMESSAGE =~ \\[lint\\ skip\\] ]]; then","33","        if [[ $BUILD_SOURCEVERSIONMESSAGE =~ \\[lint\\ skip\\] ]]; then","42","        if [[ $BUILD_SOURCEVERSIONMESSAGE =~ \\[scipy-dev\\] ]] || \\","43","           [[ $BUILD_REASON == \"Schedule\" ]]; then"]}]}},"f656c37d19308298ce5bd95eb2f0477c95a15bb7":{"changes":{"sklearn\/multioutput.py":"MODIFY","sklearn\/tests\/test_multioutput.py":"MODIFY","doc\/whats_new\/v0.24.rst":"MODIFY"},"diff":{"sklearn\/multioutput.py":[{"add":["455","        if isinstance(self.order_, tuple):","456","            self.order_ = np.array(self.order_)","457",""],"delete":[]}],"sklearn\/tests\/test_multioutput.py":[{"add":["603","","604","","605","@pytest.mark.parametrize(\"order_type\", [list, np.array, tuple])","606","def test_classifier_chain_tuple_order(order_type):","607","    X = [[1, 2, 3], [4, 5, 6], [1.5, 2.5, 3.5]]","608","    y = [[3, 2], [2, 3], [3, 2]]","609","    order = order_type([1, 0])","610","","611","    chain = ClassifierChain(RandomForestClassifier(), order=order)","612","","613","    chain.fit(X, y)","614","    X_test = [[1.5, 2.5, 3.5]]","615","    y_test = [[3, 2]]","616","    assert_array_almost_equal(chain.predict(X_test), y_test)","617","","618","","619","def test_classifier_chain_tuple_invalid_order():","620","    X = [[1, 2, 3], [4, 5, 6], [1.5, 2.5, 3.5]]","621","    y = [[3, 2], [2, 3], [3, 2]]","622","    order = tuple([1, 2])","623","","624","    chain = ClassifierChain(RandomForestClassifier(), order=order)","625","","626","    with pytest.raises(ValueError, match='invalid order'):","627","        chain.fit(X, y)"],"delete":[]}],"doc\/whats_new\/v0.24.rst":[{"add":["262","- |Efficiency| Fixed :issue:`10493`. Improve Local Linear Embedding (LLE)","325",":mod:`sklearn.multioutput`","326","..........................","327","","328","- |Fix| A fix to accept tuples for the ``order`` parameter ","329","  in :class:`multioutput.ClassifierChain`.","330","  :pr:`18124` by :user:`Gus Brocchini <boldloop>` and","331","  :user:`Amanda Dsouza <amy12xx>`.","332",""],"delete":["262","- |Efficiency| Fixed :issue:`10493`. Improve Local Linear Embedding (LLE) "]}]}},"018de223237bb396d488b51c8d84fdf3954c1550":{"changes":{"sklearn\/model_selection\/_validation.py":"MODIFY","doc\/modules\/cross_validation.rst":"MODIFY"},"diff":{"sklearn\/model_selection\/_validation.py":[{"add":["1055","    Read more in the :ref:`User Guide <permutation_test_score>`.","1056",""],"delete":[]}],"doc\/modules\/cross_validation.rst":[{"add":["858","","859",".. _permutation_test_score:","860","","861","Permutation test score","862","======================","863","","864",":func:`~sklearn.model_selection.permutation_test_score` offers another way","865","to evaluate the performance of classifiers. It provides a permutation-based","866","p-value, which represents how likely an observed performance of the","867","classifier would be obtained by chance. The null hypothesis in this test is","868","that the classifier fails to leverage any statistical dependency between the","869","features and the labels to make correct predictions on left out data.","870",":func:`~sklearn.model_selection.permutation_test_score` generates a null","871","distribution by calculating `n_permutations` different permutations of the","872","data. In each permutation the labels are randomly shuffled, thereby removing","873","any dependency between the features and the labels. The p-value output","874","is the fraction of permutations for which the average cross-validation score","875","obtained by the model is better than the cross-validation score obtained by","876","the model using the original data. For reliable results ``n_permutations``","877","should typically be larger than 100 and ``cv`` between 3-10 folds.","878","","879","A low p-value provides evidence that the dataset contains real dependency","880","between features and labels and the classifier was able to utilize this","881","to obtain good results. A high p-value could be due to a lack of dependency","882","between features and labels (there is no difference in feature values between","883","the classes) or because the classifier was not able to use the dependency in","884","the data. In the latter case, using a more appropriate classifier that","885","is able to utilize the structure in the data, would result in a low","886","p-value.","887","","888","Cross-validation provides information about how well a classifier generalizes,","889","specifically the range of expected errors of the classifier. However, a","890","classifier trained on a high dimensional dataset with no structure may still","891","perform better than expected on cross-validation, just by chance.","892","This can typically happen with small datasets with less than a few hundred","893","samples.","894",":func:`~sklearn.model_selection.permutation_test_score` provides information","895","on whether the classifier has found a real class structure and can help in","896","evaluating the performance of the classifier.","897","","898","It is important to note that this test has been shown to produce low","899","p-values even if there is only weak structure in the data because in the","900","corresponding permutated datasets there is absolutely no structure. This","901","test is therefore only able to show when the model reliably outperforms","902","random guessing.","903","","904","Finally, :func:`~sklearn.model_selection.permutation_test_score` is computed","905","using brute force and interally fits ``(n_permutations + 1) * n_cv`` models.","906","It is therefore only tractable with small datasets for which fitting an","907","individual model is very fast.","908","","909",".. topic:: Examples","910","","911","    * :ref:`sphx_glr_auto_examples_feature_selection_plot_permutation_test_for_classification.py`","912","","913",".. topic:: References:","914","","915"," * Ojala and Garriga. `Permutation Tests for Studying Classifier Performance","916","   <http:\/\/www.jmlr.org\/papers\/volume11\/ojala10a\/ojala10a.pdf>`_.","917","   J. Mach. Learn. Res. 2010."],"delete":[]}]}},"fb8a497a144e9a0fc01a37c6d47c13884581fecd":{"changes":{"doc\/support.rst":"MODIFY"},"diff":{"doc\/support.rst":[{"add":["41","Please describe the nature of your data and how you preprocessed it:","64","  - observed outcome or Python (or gdb) tracebacks","68","optionally a minimalistic subsample of your dataset (for instance, exported","71","Note: Gists are Git cloneable repositories and thus you can use Git to"],"delete":["41","Please describe the nature of your data and the how you preprocessed it:","64","  - observed outcome or python (or gdb) tracebacks","68","optionally a minimalistic subsample of your dataset (for instance exported","71","Note: gists are git cloneable repositories and thus you can use git to"]}]}},"c115ed715a32a628c77440253a6e3af502e1098d":{"changes":{"sklearn\/manifold\/_locally_linear.py":"MODIFY","sklearn\/manifold\/_isomap.py":"MODIFY"},"diff":{"sklearn\/manifold\/_locally_linear.py":[{"add":["32","    reg : float, default=1e-3","79","    reg : float, default=1e-3","84","    n_jobs : int or None, default=None","120","    k : int","123","    k_skip : int, default=1","126","    eigen_solver : {'auto', 'arpack', 'dense'}, default='arpack'","138","    tol : float, default=1e-6","142","    max_iter : int, default=100","201","    n_neighbors : int","204","    n_components : int","207","    reg : float, default=1e-3","211","    eigen_solver : {'auto', 'arpack', 'dense'}, default='auto'","225","    tol : float, default=1e-6","229","    max_iter : int, default=100","232","    method : {'standard', 'hessian', 'modified', 'ltsa'}, default='standard'","243","    hessian_tol : float, default=1e-4","247","    modified_tol : float, default=1e-12","256","    n_jobs : int or None, default=None","527","    n_neighbors : int, default=5","530","    n_components : int, default=2","533","    reg : float, default=1e-3","537","    eigen_solver : {'auto', 'arpack', 'dense'}, default='auto'","551","    tol : float, default=1e-6","555","    max_iter : int, default=100","559","    method : {'standard', 'hessian', 'modified', 'ltsa'}, default='standard'","570","    hessian_tol : float, default=1e-4","574","    modified_tol : float, default=1e-12","578","    neighbors_algorithm : {'auto', 'brute', 'kd_tree', 'ball_tree'}, \\","579","                          default='auto'","588","    n_jobs : int or None, default=None"],"delete":["32","    reg : float, optional","79","    reg : float, optional","84","    n_jobs : int or None, optional (default=None)","120","    k : integer","123","    k_skip : integer, optional","126","    eigen_solver : string, {'auto', 'arpack', 'dense'}","138","    tol : float, optional","142","    max_iter : int","201","    n_neighbors : integer","204","    n_components : integer","207","    reg : float","211","    eigen_solver : string, {'auto', 'arpack', 'dense'}","225","    tol : float, optional","229","    max_iter : integer","232","    method : {'standard', 'hessian', 'modified', 'ltsa'}","243","    hessian_tol : float, optional","247","    modified_tol : float, optional","256","    n_jobs : int or None, optional (default=None)","527","    n_neighbors : integer","530","    n_components : integer","533","    reg : float","537","    eigen_solver : string, {'auto', 'arpack', 'dense'}","551","    tol : float, optional","555","    max_iter : integer","559","    method : string ('standard', 'hessian', 'modified' or 'ltsa')","570","    hessian_tol : float, optional","574","    modified_tol : float, optional","578","    neighbors_algorithm : string ['auto'|'brute'|'kd_tree'|'ball_tree']","587","    n_jobs : int or None, optional (default=None)"]}],"sklearn\/manifold\/_isomap.py":[{"add":["24","    n_neighbors : int, default=5","27","    n_components : int, default=2","30","    eigen_solver : {'auto', 'arpack', 'dense'}, default='auto'","40","    tol : float, default=0","44","    max_iter : int, default=None","48","    path_method : {'auto', 'FW', 'D'}, default='auto'","57","    neighbors_algorithm : {'auto', 'brute', 'kd_tree', 'ball_tree'}, \\","58","                          default='auto'"],"delete":["24","    n_neighbors : integer","27","    n_components : integer","30","    eigen_solver : ['auto'|'arpack'|'dense']","40","    tol : float","44","    max_iter : integer","48","    path_method : string ['auto'|'FW'|'D']","57","    neighbors_algorithm : string ['auto'|'brute'|'kd_tree'|'ball_tree']"]}]}},"591146d9329660334f52708dcabb466a7e0d78ce":{"changes":{"sklearn\/cluster\/_mean_shift.py":"MODIFY","doc\/whats_new\/v0.24.rst":"MODIFY","sklearn\/cluster\/tests\/test_mean_shift.py":"MODIFY"},"diff":{"sklearn\/cluster\/_mean_shift.py":[{"add":["220","    if bin_size == 0:","221","        return X"],"delete":[]}],"doc\/whats_new\/v0.24.rst":[{"add":["56",".......................","57","","58","- |Fix| Fixed a bug in :class:`cluster.MeanShift` with `bin_seeding=True`. When","59","  the estimated bandwidth is 0, the behavior is equivalent to","60","  `bin_seeding=False`.","61","  :pr:`17742` by :user:`Jeremie du Boisberranger <jeremiedbb>`."],"delete":["56","........................."]}],"sklearn\/cluster\/tests\/test_mean_shift.py":[{"add":["14","from sklearn.utils._testing import assert_allclose","21","from sklearn.metrics import v_measure_score","172","","173","","174","def test_mean_shift_zero_bandwidth():","175","    # Check that mean shift works when the estimated bandwidth is 0.","176","    X = np.array([1, 1, 1, 2, 2, 2, 3, 3]).reshape(-1, 1)","177","","178","    # estimate_bandwidth with default args returns 0 on this dataset","179","    bandwidth = estimate_bandwidth(X)","180","    assert bandwidth == 0","181","","182","    # get_bin_seeds with a 0 bin_size should return the dataset itself","183","    assert get_bin_seeds(X, bin_size=bandwidth) is X","184","","185","    # MeanShift with binning and a 0 estimated bandwidth should be equivalent","186","    # to no binning.","187","    ms_binning = MeanShift(bin_seeding=True, bandwidth=None).fit(X)","188","    ms_nobinning = MeanShift(bin_seeding=False).fit(X)","189","    expected_labels = np.array([0, 0, 0, 1, 1, 1, 2, 2])","190","","191","    assert v_measure_score(ms_binning.labels_, expected_labels) == 1","192","    assert v_measure_score(ms_nobinning.labels_, expected_labels) == 1","193","    assert_allclose(ms_binning.cluster_centers_, ms_nobinning.cluster_centers_)"],"delete":[]}]}},"557218c021d58c7455b26820f331a9c388c9cb4d":{"changes":{"sklearn\/decomposition\/_truncated_svd.py":"MODIFY"},"diff":{"sklearn\/decomposition\/_truncated_svd.py":[{"add":["17","from ..utils.validation import check_is_fitted","215","        check_is_fitted(self)"],"delete":[]}]}},"946fddec7b62215191524c3f950a41fe944d014c":{"changes":{"\/dev\/null":"DELETE","sklearn\/utils\/fixes.py":"MODIFY","sklearn\/linear_model\/tests\/test_bayes.py":"MODIFY","doc\/whats_new\/v0.23.rst":"MODIFY","sklearn\/linear_model\/_bayes.py":"MODIFY"},"diff":{"\/dev\/null":[{"add":[],"delete":[]}],"sklearn\/utils\/fixes.py":[{"add":[],"delete":["44","if sp_version >= (1, 3):","45","    # Preserves earlier default choice of pinvh cutoff `cond` value.","46","    # Can be removed once issue #14055 is fully addressed.","47","    from ..externals._scipy_linalg import pinvh","48","else:","49","    # mypy error: Name 'pinvh' already defined (possibly by an import)","50","    from scipy.linalg import pinvh  # type: ignore  # noqa","51",""]}],"sklearn\/linear_model\/tests\/test_bayes.py":[{"add":["9","import pytest","10","","163","    n_samples = 10","185","    # With the inputs above, ARDRegression prunes both of the two coefficients","186","    # in the first iteration. Hence, the expected shape of `sigma_` is (0, 0).","187","    assert clf.sigma_.shape == (0, 0)","204","@pytest.mark.parametrize('seed', range(100))","205","@pytest.mark.parametrize('n_samples, n_features', ((10, 100), (100, 10)))","206","def test_ard_accuracy_on_easy_problem(seed, n_samples, n_features):","212","    regressor = ARDRegression()","216","    assert abs_coef_error < 1e-10","249","","250","","251","@pytest.mark.parametrize('seed', range(10))","252","def test_update_sigma(seed):","253","    # make sure the two update_sigma() helpers are equivalent. The woodbury","254","    # formula is used when n_samples < n_features, and the other one is used","255","    # otherwise.","256","","257","    rng = np.random.RandomState(seed)","258","","259","    # set n_samples == n_features to avoid instability issues when inverting","260","    # the matrices. Using the woodbury formula would be unstable when","261","    # n_samples > n_features","262","    n_samples = n_features = 10","263","    X = rng.randn(n_samples, n_features)","264","    alpha = 1","265","    lmbda = np.arange(1, n_features + 1)","266","    keep_lambda = np.array([True] * n_features)","267","","268","    reg = ARDRegression()","269","","270","    sigma = reg._update_sigma(X, alpha, lmbda, keep_lambda)","271","    sigma_woodbury = reg._update_sigma_woodbury(X, alpha, lmbda, keep_lambda)","272","","273","    np.testing.assert_allclose(sigma, sigma_woodbury)"],"delete":["161","    n_samples = 4","183","    # With the inputs above, ARDRegression prunes one of the two coefficients","184","    # in the first iteration. Hence, the expected shape of `sigma_` is (1, 1).","185","    assert clf.sigma_.shape == (1, 1)","202","def test_ard_accuracy_on_easy_problem():","205","    # This particular seed seems to converge poorly in the failure-case","206","    # (scipy==1.3.0, sklearn==0.21.2)","207","    seed = 45","211","    regressor = ARDRegression(n_iter=600)","215","    # Expect an accuracy of better than 1E-4 in most cases -","216","    # Failure-case produces 0.16!","217","    assert abs_coef_error < 0.01"]}],"doc\/whats_new\/v0.23.rst":[{"add":["305","- |Fix| |Efficiency| :class:`linear_model.ARDRegression` is more stable and","306","  much faster when `n_samples > n_features`. It can now scale to hundreds of","307","  thousands of samples. The stability fix might imply changes in the number","308","  of non-zero coefficients and in the predicted output. :pr:`16849` by","309","  `Nicolas Hug`_.","310",""],"delete":[]}],"sklearn\/linear_model\/_bayes.py":[{"add":["14","from scipy.linalg import pinvh","561","        update_sigma = (self._update_sigma if n_samples >= n_features","562","                        else self._update_sigma_woodbury)","565","            sigma_ = update_sigma(X, alpha_, lambda_, keep_lambda)","597","            if not keep_lambda.any():","598","                break","599","","600","        if keep_lambda.any():","601","            # update sigma and mu using updated params from the last iteration","602","            sigma_ = update_sigma(X, alpha_, lambda_, keep_lambda)","603","            coef_ = update_coeff(X, y, coef_, alpha_, keep_lambda, sigma_)","604","        else:","605","            sigma_ = np.array([]).reshape(0, 0)","614","    def _update_sigma_woodbury(self, X, alpha_, lambda_, keep_lambda):","615","        # See slides as referenced in the docstring note","616","        # this function is used when n_samples < n_features and will invert","617","        # a matrix of shape (n_samples, n_samples) making use of the","618","        # woodbury formula:","619","        # https:\/\/en.wikipedia.org\/wiki\/Woodbury_matrix_identity","620","        n_samples = X.shape[0]","621","        X_keep = X[:, keep_lambda]","622","        inv_lambda = 1 \/ lambda_[keep_lambda].reshape(1, -1)","623","        sigma_ = pinvh(","624","            np.eye(n_samples) \/ alpha_ + np.dot(X_keep * inv_lambda, X_keep.T)","625","        )","626","        sigma_ = np.dot(sigma_, X_keep * inv_lambda)","627","        sigma_ = - np.dot(inv_lambda.reshape(-1, 1) * X_keep.T, sigma_)","628","        sigma_[np.diag_indices(sigma_.shape[1])] += 1. \/ lambda_[keep_lambda]","629","        return sigma_","630","","631","    def _update_sigma(self, X, alpha_, lambda_, keep_lambda):","632","        # See slides as referenced in the docstring note","633","        # this function is used when n_samples >= n_features and will","634","        # invert a matrix of shape (n_features, n_features)","635","        X_keep = X[:, keep_lambda]","636","        gram = np.dot(X_keep.T, X_keep)","637","        eye = np.eye(gram.shape[0])","638","        sigma_inv = lambda_[keep_lambda] * eye + alpha_ * gram","639","        sigma_ = pinvh(sigma_inv)","640","        return sigma_","641",""],"delete":["14","from ..utils.fixes import pinvh","556","        # Compute sigma and mu (using Woodbury matrix identity)","557","        def update_sigma(X, alpha_, lambda_, keep_lambda, n_samples):","558","            sigma_ = pinvh(np.eye(n_samples) \/ alpha_ +","559","                           np.dot(X[:, keep_lambda] *","560","                           np.reshape(1. \/ lambda_[keep_lambda], [1, -1]),","561","                           X[:, keep_lambda].T))","562","            sigma_ = np.dot(sigma_, X[:, keep_lambda] *","563","                            np.reshape(1. \/ lambda_[keep_lambda], [1, -1]))","564","            sigma_ = - np.dot(np.reshape(1. \/ lambda_[keep_lambda], [-1, 1]) *","565","                              X[:, keep_lambda].T, sigma_)","566","            sigma_.flat[::(sigma_.shape[1] + 1)] += 1. \/ lambda_[keep_lambda]","567","            return sigma_","568","","576","            sigma_ = update_sigma(X, alpha_, lambda_, keep_lambda, n_samples)","608","        # update sigma and mu using updated parameters from the last iteration","609","        sigma_ = update_sigma(X, alpha_, lambda_, keep_lambda, n_samples)","610","        coef_ = update_coeff(X, y, coef_, alpha_, keep_lambda, sigma_)"]}]}},"d6ed0d0cae852e59db2d81239f59b34434762c6c":{"changes":{"sklearn\/multiclass.py":"MODIFY"},"diff":{"sklearn\/multiclass.py":[{"add":["162","        The number of jobs to use for the computation: the `n_classes`","163","        one-vs-rest problems are computed in parallel.","164","","508","        The number of jobs to use for the computation: the `n_classes * (","509","        n_classes - 1) \/ 2` OVO problems are computed in parallel.","510","","737","        The number of jobs to use for the computation: the multiclass problems","738","        are computed in parallel.","739",""],"delete":["162","        The number of jobs to use for the computation.","506","        The number of jobs to use for the computation.","733","        The number of jobs to use for the computation."]}]}},"bf6023fc77fe04d7f8d43135f36a70b832a9f662":{"changes":{"sklearn\/datasets\/tests\/test_kddcup99.py":"MODIFY"},"diff":{"sklearn\/datasets\/tests\/test_kddcup99.py":[{"add":["22","    data = fetch_kddcup99_fxt(subset='SA')","26","    data = fetch_kddcup99_fxt(subset='SF')","30","    data = fetch_kddcup99_fxt(subset='http')","34","    data = fetch_kddcup99_fxt(subset='smtp')","38","    fetch_func = partial(fetch_kddcup99_fxt, subset='smtp')"],"delete":["22","    data = fetch_kddcup99_fxt('SA')","26","    data = fetch_kddcup99_fxt('SF')","30","    data = fetch_kddcup99_fxt('http')","34","    data = fetch_kddcup99_fxt('smtp')","38","    fetch_func = partial(fetch_kddcup99_fxt, 'smtp')"]}]}},"2538489f4ad39d6e32ef4c2fa6297263a26e16ce":{"changes":{"sklearn\/neighbors\/_nearest_centroid.py":"MODIFY","sklearn\/neighbors\/tests\/test_nearest_centroid.py":"MODIFY","doc\/whats_new\/v0.24.rst":"MODIFY"},"diff":{"sklearn\/neighbors\/_nearest_centroid.py":[{"add":["156","            if np.all(np.ptp(X, axis=0) == 0):","157","                raise ValueError(\"All features have zero variance. \"","158","                                 \"Division by zero.\")"],"delete":[]}],"sklearn\/neighbors\/tests\/test_nearest_centroid.py":[{"add":["148","","149","","150","def test_features_zero_var():","151","    # Test that features with 0 variance throw error","152","","153","    X = np.empty((10, 2))","154","    X[:, 0] = -0.13725701","155","    X[:, 1] = -0.9853293","156","    y = np.zeros((10))","157","    y[0] = 1","158","","159","    clf = NearestCentroid(shrink_threshold=0.1)","160","    with assert_raises(ValueError):","161","        clf.fit(X, y)"],"delete":[]}],"doc\/whats_new\/v0.24.rst":[{"add":["508","- |Fix| :class:`neighbors.NearestCentroid` with a numerical `shrink_threshold`","509","  will raise a `ValueError` when fitting on data with all constant features.","510","  :pr:`18370` by :user:`Trevor Waite <trewaite>`.","511",""],"delete":[]}]}},"b2bc40ca8b63e7e8efe7831a08286c2b539078eb":{"changes":{"sklearn\/decomposition\/_truncated_svd.py":"MODIFY"},"diff":{"sklearn\/decomposition\/_truncated_svd.py":[{"add":["214","        X = check_array(X, accept_sparse=['csr', 'csc'])"],"delete":["214","        X = check_array(X, accept_sparse='csr')"]}]}},"8611b9a9af10b46cbf68bb7fc71a44a49ed6ec1f":{"changes":{"examples\/svm\/plot_linearsvc_support_vectors.py":"MODIFY"},"diff":{"examples\/svm\/plot_linearsvc_support_vectors.py":[{"add":["26","    # The support vectors are the samples that lie within the margin","27","    # boundaries, whose size is conventionally constrained to 1","28","    support_vector_indices = np.where(","29","        np.abs(decision_function) <= 1 + 1e-15)[0]"],"delete":["26","    support_vector_indices = np.where((2 * y - 1) * decision_function <= 1)[0]"]}]}}}