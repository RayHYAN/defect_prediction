{"673c31a8801effabc207a6247c18a3c28dd502f4":{"changes":{"azure-pipelines.yml":"MODIFY","build_tools\/azure\/posix.yml":"MODIFY","build_tools\/azure\/test_script.cmd":"MODIFY","build_tools\/azure\/posix-32.yml":"MODIFY","build_tools\/azure\/test_script.sh":"MODIFY","build_tools\/azure\/install.cmd":"MODIFY","build_tools\/azure\/install.sh":"MODIFY","build_tools\/azure\/windows.yml":"MODIFY"},"diff":{"azure-pipelines.yml":[{"add":["101","        PYTEST_XDIST: 'false'","120","        PYTEST_XDIST: 'false'"],"delete":[]}],"build_tools\/azure\/posix.yml":[{"add":["18","    PYTEST_XDIST: 'true'"],"delete":[]}],"build_tools\/azure\/test_script.cmd":[{"add":["10","if \"%PYTEST_XDIST%\" == \"true\" (","11","    set PYTEST_ARGS=%PYTEST_ARGS% -n2","12",")","13",""],"delete":["5","    set PYTEST_ARGS=%PYTEST_ARGS% -n2"]}],"build_tools\/azure\/posix-32.yml":[{"add":["18","    PYTEST_XDIST: 'true'","43","        -e PYTEST_XDIST=$PYTEST_XDIST"],"delete":[]}],"build_tools\/azure\/test_script.sh":[{"add":["34","if [[ \"$PYTEST_XDIST\" == \"true\" ]]; then"],"delete":["34","if [[ \"$PYTHON_VERSION\" == \"*\" ]]; then"]}],"build_tools\/azure\/install.cmd":[{"add":["27","","28","IF \"%PYTEST_XDIST%\" == \"true\" (","29","    pip install pytest-xdist","30",")","31",""],"delete":["24","    pip install pytest-xdist"]}],"build_tools\/azure\/install.sh":[{"add":["94","    python -m pip install pytest==$PYTEST_VERSION pytest-cov","102","    python -m pip install pytest==$PYTEST_VERSION pytest-cov","116","if [[ \"$PYTEST_XDIST\" == \"true\" ]]; then","117","    python -m pip install pytest-xdist","118","fi","119",""],"delete":["75","    if [[ \"$PYTHON_VERSION\" == \"*\" ]]; then","76","        python -m pip install pytest-xdist","77","    fi","78","","98","    python -m pip install pytest==$PYTEST_VERSION pytest-cov pytest-xdist","106","    python -m pip install pytest==$PYTEST_VERSION pytest-cov pytest-xdist"]}],"build_tools\/azure\/windows.yml":[{"add":["19","    PYTEST_XDIST: 'true'"],"delete":[]}]}},"e770715c434739647ddbb645ff0fcd40c64ba1fd":{"changes":{"sklearn\/decomposition\/_dict_learning.py":"MODIFY","sklearn\/decomposition\/tests\/test_dict_learning.py":"MODIFY","sklearn\/linear_model\/tests\/test_logistic.py":"MODIFY","sklearn\/isotonic.py":"MODIFY","sklearn\/cluster\/_dbscan.py":"MODIFY","sklearn\/utils\/class_weight.py":"MODIFY","sklearn\/covariance\/_graph_lasso.py":"MODIFY","sklearn\/cluster\/tests\/test_spectral.py":"MODIFY","sklearn\/linear_model\/_logistic.py":"MODIFY","sklearn\/random_projection.py":"MODIFY","sklearn\/decomposition\/_fastica.py":"MODIFY","sklearn\/utils\/graph.py":"MODIFY","sklearn\/utils\/tests\/test_estimator_checks.py":"MODIFY","sklearn\/covariance\/_empirical_covariance.py":"MODIFY","sklearn\/calibration.py":"MODIFY","sklearn\/covariance\/_shrunk_covariance.py":"MODIFY","sklearn\/cluster\/_kmeans.py":"MODIFY","sklearn\/svm\/_base.py":"MODIFY","sklearn\/utils\/extmath.py":"MODIFY","sklearn\/feature_extraction\/image.py":"MODIFY","sklearn\/cluster\/_agglomerative.py":"MODIFY","sklearn\/svm\/tests\/test_svm.py":"MODIFY","sklearn\/utils\/tests\/test_sparsefuncs.py":"MODIFY","sklearn\/utils\/tests\/test_extmath.py":"MODIFY","examples\/linear_model\/plot_lasso_coordinate_descent_path.py":"MODIFY","sklearn\/linear_model\/_coordinate_descent.py":"MODIFY","sklearn\/cluster\/_affinity_propagation.py":"MODIFY","sklearn\/cluster\/_spectral.py":"MODIFY","sklearn\/feature_selection\/_univariate_selection.py":"MODIFY","sklearn\/utils\/__init__.py":"MODIFY","sklearn\/decomposition\/_nmf.py":"MODIFY","sklearn\/linear_model\/_stochastic_gradient.py":"MODIFY","sklearn\/decomposition\/tests\/test_nmf.py":"MODIFY","sklearn\/feature_extraction\/tests\/test_image.py":"MODIFY","sklearn\/utils\/sparsefuncs.py":"MODIFY","sklearn\/utils\/tests\/test_class_weight.py":"MODIFY","sklearn\/linear_model\/tests\/test_sgd.py":"MODIFY","sklearn\/decomposition\/_sparse_pca.py":"MODIFY","sklearn\/cluster\/_optics.py":"MODIFY","sklearn\/feature_selection\/_mutual_info.py":"MODIFY","sklearn\/cluster\/_mean_shift.py":"MODIFY","sklearn\/linear_model\/tests\/test_sag.py":"MODIFY","sklearn\/cluster\/tests\/test_hierarchical.py":"MODIFY","sklearn\/cluster\/tests\/test_mean_shift.py":"MODIFY"},"diff":{"sklearn\/decomposition\/_dict_learning.py":[{"add":["187","@_deprecate_positional_args","188","def sparse_encode(X, dictionary, *, gram=None, cov=None,","189","                  algorithm='lasso_lars', n_nonzero_coefs=None, alpha=None,","190","                  copy_cov=True, init=None, max_iter=1000, n_jobs=None,","191","                  check_input=True, verbose=0, positive=False):","424","@_deprecate_positional_args","425","def dict_learning(X, n_components, *, alpha, max_iter=100, tol=1e-8,","619","@_deprecate_positional_args","620","def dict_learning_online(X, n_components=2, *, alpha=1, n_iter=100,","1235","            X, n_components, alpha=self.alpha,","1439","            X, self.n_components, alpha=self.alpha,","1491","            X, self.n_components, alpha=self.alpha,"],"delete":["187","def sparse_encode(X, dictionary, gram=None, cov=None, algorithm='lasso_lars',","188","                  n_nonzero_coefs=None, alpha=None, copy_cov=True, init=None,","189","                  max_iter=1000, n_jobs=None, check_input=True, verbose=0,","190","                  positive=False):","423","def dict_learning(X, n_components, alpha, max_iter=100, tol=1e-8,","617","def dict_learning_online(X, n_components=2, alpha=1, n_iter=100,","1232","            X, n_components, self.alpha,","1436","            X, self.n_components, self.alpha,","1488","            X, self.n_components, self.alpha,"]}],"sklearn\/decomposition\/tests\/test_dict_learning.py":[{"add":["109","        dict_learning(X, n_components, alpha=alpha, positive_code=True)","249","        dict_learning_online(X, alpha=alpha, positive_code=True)"],"delete":["109","        dict_learning(X, n_components, alpha, positive_code=True)","249","        dict_learning_online(X, alpha, positive_code=True)"]}],"sklearn\/linear_model\/tests\/test_logistic.py":[{"add":["885","    class_weight = compute_class_weight(\"balanced\", classes=classes, y=y)"],"delete":["885","    class_weight = compute_class_weight(\"balanced\", classes, y)"]}],"sklearn\/isotonic.py":[{"add":["78","@_deprecate_positional_args","79","def isotonic_regression(y, *, sample_weight=None, y_min=None, y_max=None,","256","        y = isotonic_regression(unique_y, sample_weight=unique_sample_weight,","257","                                y_min=self.y_min, y_max=self.y_max,"],"delete":["78","def isotonic_regression(y, sample_weight=None, y_min=None, y_max=None,","255","        y = isotonic_regression(unique_y, unique_sample_weight,","256","                                self.y_min, self.y_max,"]}],"sklearn\/cluster\/_dbscan.py":[{"add":["22","@_deprecate_positional_args","23","def dbscan(X, eps=0.5, *, min_samples=5, metric='minkowski',","24","           metric_params=None, algorithm='auto', leaf_size=30, p=2,","25","           sample_weight=None, n_jobs=None):"],"delete":["22","def dbscan(X, eps=0.5, min_samples=5, metric='minkowski', metric_params=None,","23","           algorithm='auto', leaf_size=30, p=2, sample_weight=None,","24","           n_jobs=None):"]}],"sklearn\/utils\/class_weight.py":[{"add":["9","@_deprecate_positional_args","10","def compute_class_weight(class_weight, *, classes, y):","156","                                                    classes=classes_subsample,","157","                                                    y=y_subsample),","165","                                            classes=classes_full,","166","                                            y=y_full)"],"delete":["9","def compute_class_weight(class_weight, classes, y):","155","                                                    classes_subsample,","156","                                                    y_subsample),","164","                                            classes_full,","165","                                            y_full)"]}],"sklearn\/covariance\/_graph_lasso.py":[{"add":["77","@_deprecate_positional_args","78","def graphical_lasso(emp_cov, alpha, *, cov_init=None, mode='cd', tol=1e-4,"],"delete":["77","","78","def graphical_lasso(emp_cov, alpha, cov_init=None, mode='cd', tol=1e-4,"]}],"sklearn\/cluster\/tests\/test_spectral.py":[{"add":["189","        y_pred = discretize(y_true_noisy, random_state=random_state)"],"delete":["189","        y_pred = discretize(y_true_noisy, random_state)"]}],"sklearn\/linear_model\/_logistic.py":[{"add":["664","        class_weight_ = compute_class_weight(class_weight,","665","                                             classes=classes, y=y)","679","            class_weight_ = compute_class_weight(class_weight,","680","                                                 classes=mask_classes,","681","                                                 y=y_bin)","1871","            class_weight = compute_class_weight(","1872","                class_weight, classes=np.arange(len(self.classes_)), y=y)"],"delete":["664","        class_weight_ = compute_class_weight(class_weight, classes, y)","678","            class_weight_ = compute_class_weight(class_weight, mask_classes,","679","                                                 y_bin)","1869","            class_weight = compute_class_weight(class_weight,","1870","                                                np.arange(len(self.classes_)),","1871","                                                y)"]}],"sklearn\/random_projection.py":[{"add":["51","@_deprecate_positional_args","52","def johnson_lindenstrauss_min_dim(n_samples, *, eps=0.1):"],"delete":["51","def johnson_lindenstrauss_min_dim(n_samples, eps=0.1):"]}],"sklearn\/decomposition\/_fastica.py":[{"add":["149","@_deprecate_positional_args","150","def fastica(X, n_components=None, *, algorithm=\"parallel\", whiten=True,"],"delete":["149","def fastica(X, n_components=None, algorithm=\"parallel\", whiten=True,"]}],"sklearn\/utils\/graph.py":[{"add":["15","from .validation import _deprecate_positional_args","21","@_deprecate_positional_args","22","def single_source_shortest_path_length(graph, source, *, cutoff=None):"],"delete":["20","","21","def single_source_shortest_path_length(graph, source, cutoff=None):"]}],"sklearn\/utils\/tests\/test_estimator_checks.py":[{"add":["212","        class_weight = compute_class_weight(self.class_weight, classes=classes,","213","                                            y=y)"],"delete":["212","        class_weight = compute_class_weight(self.class_weight, classes, y)"]}],"sklearn\/covariance\/_empirical_covariance.py":[{"add":["50","@_deprecate_positional_args","51","def empirical_covariance(X, *, assume_centered=False):"],"delete":["50","def empirical_covariance(X, assume_centered=False):"]}],"sklearn\/calibration.py":[{"add":["508","@_deprecate_positional_args","509","def calibration_curve(y_true, y_prob, *, normalize=False, n_bins=5,"],"delete":["508","def calibration_curve(y_true, y_prob, normalize=False, n_bins=5,"]}],"sklearn\/covariance\/_shrunk_covariance.py":[{"add":["255","@_deprecate_positional_args","256","def ledoit_wolf(X, *, assume_centered=False, block_size=1000):","433","@_deprecate_positional_args","434","def oas(X, *, assume_centered=False):"],"delete":["255","def ledoit_wolf(X, assume_centered=False, block_size=1000):","432","","433","def oas(X, assume_centered=False):"]}],"sklearn\/cluster\/_kmeans.py":[{"add":["184","@_deprecate_positional_args","185","def k_means(X, n_clusters, *, sample_weight=None, init='k-means++',"],"delete":["184","def k_means(X, n_clusters, sample_weight=None, init='k-means++',"]}],"sklearn\/svm\/_base.py":[{"add":["545","        self.class_weight_ = compute_class_weight(self.class_weight,","546","                                                  classes=cls, y=y_)","929","        class_weight_ = compute_class_weight(class_weight, classes=classes_,","930","                                             y=y)"],"delete":["545","        self.class_weight_ = compute_class_weight(self.class_weight, cls, y_)","928","        class_weight_ = compute_class_weight(class_weight, classes_, y)"]}],"sklearn\/utils\/extmath.py":[{"add":["22","from .validation import _deprecate_positional_args","118","@_deprecate_positional_args","119","def safe_sparse_dot(a, b, *, dense_output=False):","160","@_deprecate_positional_args","161","def randomized_range_finder(A, *, size, n_iter,","245","@_deprecate_positional_args","246","def randomized_svd(M, n_components, *, n_oversamples=10, n_iter='auto',","348","    Q = randomized_range_finder(","349","        M, size=n_random, n_iter=n_iter,","350","        power_iteration_normalizer=power_iteration_normalizer,","351","        random_state=random_state)","377","@_deprecate_positional_args","378","def weighted_mode(a, w, *, axis=0):"],"delete":["117","def safe_sparse_dot(a, b, dense_output=False):","158","def randomized_range_finder(A, size, n_iter,","242","def randomized_svd(M, n_components, n_oversamples=10, n_iter='auto',","344","    Q = randomized_range_finder(M, n_random, n_iter,","345","                                power_iteration_normalizer, random_state)","371","def weighted_mode(a, w, axis=0):"]}],"sklearn\/feature_extraction\/image.py":[{"add":["132","@_deprecate_positional_args","133","def img_to_graph(img, *, mask=None, return_as=sparse.coo_matrix, dtype=None):","169","@_deprecate_positional_args","170","def grid_to_graph(n_x, n_y, n_z=1, *, mask=None, return_as=sparse.coo_matrix,","348","@_deprecate_positional_args","349","def extract_patches_2d(image, patch_size, *, max_patches=None,","350","                       random_state=None):","585","                image, patch_size, max_patches=self.max_patches,","586","                random_state=self.random_state)"],"delete":["132","def img_to_graph(img, mask=None, return_as=sparse.coo_matrix, dtype=None):","168","def grid_to_graph(n_x, n_y, n_z=1, mask=None, return_as=sparse.coo_matrix,","346","def extract_patches_2d(image, patch_size, max_patches=None, random_state=None):","581","                image, patch_size, self.max_patches, self.random_state)"]}],"sklearn\/cluster\/_agglomerative.py":[{"add":["137","@_deprecate_positional_args","138","def ward_tree(X, *, connectivity=None, n_clusters=None, return_distance=False):","879","        out = memory.cache(tree_builder)(X, connectivity=connectivity,"],"delete":["137","def ward_tree(X, connectivity=None, n_clusters=None, return_distance=False):","878","        out = memory.cache(tree_builder)(X, connectivity,"]}],"sklearn\/svm\/tests\/test_svm.py":[{"add":["601","    class_weights = compute_class_weight('balanced', classes=classes,","602","                                         y=y[unbalanced])"],"delete":["601","    class_weights = compute_class_weight('balanced', classes, y[unbalanced])"]}],"sklearn\/utils\/tests\/test_sparsefuncs.py":[{"add":["107","            incr_mean_variance_axis(X=axis, axis=last_mean, last_mean=last_var,","108","                                    last_var=last_n)","110","            incr_mean_variance_axis(X_lil, axis=axis, last_mean=last_mean,","111","                                    last_var=last_var, last_n=last_n)","116","            incr_mean_variance_axis(X_csr, axis=axis, last_mean=last_mean,","117","                                    last_var=last_var, last_n=last_n)","147","                    incr_mean_variance_axis(X_sparse, axis=axis,","148","                                            last_mean=last_mean,","149","                                            last_var=last_var,","150","                                            last_n=last_n)","178","        X1, axis=axis, last_mean=last_mean, last_var=last_var, last_n=last_n","181","        X2, axis=axis, last_mean=updated_mean, last_var=updated_var,","182","        last_n=updated_n","198","        X1, axis=axis, last_mean=last_mean, last_var=last_var, last_n=last_n","202","        X2, axis=axis, last_mean=last_mean, last_var=last_var, last_n=last_n","235","        X, axis=axis, last_mean=old_means.copy(),","236","        last_var=old_variances.copy(), last_n=old_sample_count.copy())","238","        X_nan, axis=axis, last_mean=old_means.copy(),","239","        last_var=old_variances.copy(), last_n=old_sample_count.copy())"],"delete":["107","            incr_mean_variance_axis(axis, last_mean, last_var, last_n)","109","            incr_mean_variance_axis(X_lil, axis, last_mean, last_var, last_n)","114","            incr_mean_variance_axis(X_csr, axis, last_mean, last_var, last_n)","144","                    incr_mean_variance_axis(X_sparse, axis, last_mean,","145","                                            last_var, last_n)","173","        X1, axis, last_mean, last_var, last_n","176","        X2, axis, updated_mean, updated_var, updated_n","192","        X1, axis, last_mean, last_var, last_n","196","        X2, axis, last_mean, last_var, last_n","229","        X, axis, old_means.copy(), old_variances.copy(),","230","        old_sample_count.copy())","232","        X_nan, axis, old_means.copy(), old_variances.copy(),","233","        old_sample_count.copy())"]}],"sklearn\/utils\/tests\/test_extmath.py":[{"add":["61","        mode2, score2 = weighted_mode(x, weights, axis=axis)"],"delete":["61","        mode2, score2 = weighted_mode(x, weights, axis)"]}],"examples\/linear_model\/plot_lasso_coordinate_descent_path.py":[{"add":["33","alphas_lasso, coefs_lasso, _ = lasso_path(X, y, eps=eps, fit_intercept=False)","37","    X, y, eps=eps, positive=True, fit_intercept=False)"],"delete":["33","alphas_lasso, coefs_lasso, _ = lasso_path(X, y, eps, fit_intercept=False)","37","    X, y, eps, positive=True, fit_intercept=False)"]}],"sklearn\/linear_model\/_coordinate_descent.py":[{"add":["170","@_deprecate_positional_args","171","def lasso_path(X, y, *, eps=1e-3, n_alphas=100, alphas=None,","316","@_deprecate_positional_args","317","def enet_path(X, y, *, l1_ratio=0.5, eps=1e-3, n_alphas=100, alphas=None,"],"delete":["170","def lasso_path(X, y, eps=1e-3, n_alphas=100, alphas=None,","315","def enet_path(X, y, l1_ratio=0.5, eps=1e-3, n_alphas=100, alphas=None,"]}],"sklearn\/cluster\/_affinity_propagation.py":[{"add":["32","@_deprecate_positional_args","33","def affinity_propagation(S, *, preference=None, convergence_iter=15,","34","                         max_iter=200, damping=0.5, copy=True, verbose=False,","414","                self.affinity_matrix_, preference=self.preference,","415","                max_iter=self.max_iter,"],"delete":["32","def affinity_propagation(S, preference=None, convergence_iter=15, max_iter=200,","33","                         damping=0.5, copy=True, verbose=False,","413","                self.affinity_matrix_, self.preference, max_iter=self.max_iter,"]}],"sklearn\/cluster\/_spectral.py":[{"add":["20","@_deprecate_positional_args","21","def discretize(vectors, *, copy=True, max_svd_restarts=30, n_iter_max=20,","159","@_deprecate_positional_args","160","def spectral_clustering(affinity, *, n_clusters=8, n_components=None,"],"delete":["20","def discretize(vectors, copy=True, max_svd_restarts=30, n_iter_max=20,","158","def spectral_clustering(affinity, n_clusters=8, n_components=None,"]}],"sklearn\/feature_selection\/_univariate_selection.py":[{"add":["231","@_deprecate_positional_args","232","def f_regression(X, y, *, center=True):"],"delete":["231","def f_regression(X, y, center=True):"]}],"sklearn\/utils\/__init__.py":[{"add":["275","def safe_indexing(X, indices, *, axis=0):"],"delete":["275","def safe_indexing(X, indices, axis=0):"]}],"sklearn\/decomposition\/_nmf.py":[{"add":["843","@_deprecate_positional_args","844","def non_negative_factorization(X, W=None, H=None, n_components=None, *,"],"delete":["843","def non_negative_factorization(X, W=None, H=None, n_components=None,"]}],"sklearn\/linear_model\/_stochastic_gradient.py":[{"add":["499","        self._expanded_class_weight = compute_class_weight(","500","            self.class_weight, classes=self.classes_, y=y)","683","                             \" use compute_class_weight('{0}', \"","684","                             \"classes=classes, y=y). \""],"delete":["499","        self._expanded_class_weight = compute_class_weight(self.class_weight,","500","                                                           self.classes_, y)","683","                             \" use compute_class_weight('{0}', classes, y). \""]}],"sklearn\/decomposition\/tests\/test_nmf.py":[{"add":["236","    assert_raise_message(ValueError, msg, nnmf, A, A, A, 1.5, init='random')","239","    assert_raise_message(ValueError, msg, nnmf, A, A, A, '2', init='random')","241","    assert_raise_message(ValueError, msg, nnmf, A, A, -A, 2, init='custom')","243","    assert_raise_message(ValueError, msg, nnmf, A, -A, A, 2, init='custom')","245","    assert_raise_message(ValueError, msg, nnmf, A, A, 0 * A, 2, init='custom')","247","    assert_raise_message(ValueError, msg, nnmf, A, A, 0 * A, 2, init='custom',","248","                         regularization='spam')"],"delete":["236","    assert_raise_message(ValueError, msg, nnmf, A, A, A, 1.5, 'random')","239","    assert_raise_message(ValueError, msg, nnmf, A, A, A, '2', 'random')","241","    assert_raise_message(ValueError, msg, nnmf, A, A, -A, 2, 'custom')","243","    assert_raise_message(ValueError, msg, nnmf, A, -A, A, 2, 'custom')","245","    assert_raise_message(ValueError, msg, nnmf, A, A, 0 * A, 2, 'custom')","247","    assert_raise_message(ValueError, msg, nnmf, A, A, 0 * A, 2, 'custom', True,","248","                         'cd', 2., 1e-4, 200, 0., 0., 'spam')"]}],"sklearn\/feature_extraction\/tests\/test_image.py":[{"add":["70","        graph = img_to_graph(face, mask=mask)"],"delete":["70","        graph = img_to_graph(face, mask)"]}],"sklearn\/utils\/sparsefuncs.py":[{"add":["7","from .validation import _deprecate_positional_args","101","@_deprecate_positional_args","102","def incr_mean_variance_axis(X, *, axis, last_mean, last_var, last_n):"],"delete":["100","def incr_mean_variance_axis(X, axis, last_mean, last_var, last_n):"]}],"sklearn\/utils\/tests\/test_class_weight.py":[{"add":["17","    cw = compute_class_weight(\"balanced\", classes=classes, y=y)","29","        compute_class_weight(\"balanced\", classes=classes, y=y)","34","        compute_class_weight({\"label_not_present\": 1.}, classes=classes, y=y)","38","        compute_class_weight(\"balanced\", classes=classes, y=y)","40","        compute_class_weight({0: 1., 1: 2.}, classes=classes, y=y)","47","    cw = compute_class_weight(class_weights, classes=classes, y=y)","58","        compute_class_weight(class_weights, classes=classes, y=y)","63","        compute_class_weight(class_weights, classes=classes, y=y)","100","    cw = compute_class_weight(\"balanced\", classes=classes, y=y)","107","    cw = compute_class_weight(\"balanced\", classes=classes, y=y)","119","    cw = compute_class_weight(\"balanced\", classes=classes, y=y)","133","    cw = compute_class_weight(None, classes=classes, y=y)","138","    cw = compute_class_weight({2: 1.5}, classes=classes, y=y)","142","    cw = compute_class_weight({2: 1.5, 4: 0.5}, classes=classes, y=y)"],"delete":["17","    cw = compute_class_weight(\"balanced\", classes, y)","29","        compute_class_weight(\"balanced\", classes, y)","34","        compute_class_weight({\"label_not_present\": 1.}, classes, y)","38","        compute_class_weight(\"balanced\", classes, y)","40","        compute_class_weight({0: 1., 1: 2.}, classes, y)","47","    cw = compute_class_weight(class_weights, classes, y)","58","        compute_class_weight(class_weights, classes, y)","63","        compute_class_weight(class_weights, classes, y)","100","    cw = compute_class_weight(\"balanced\", classes, y)","107","    cw = compute_class_weight(\"balanced\", classes, y)","119","    cw = compute_class_weight(\"balanced\", classes, y)","133","    cw = compute_class_weight(None, classes, y)","138","    cw = compute_class_weight({2: 1.5}, classes, y)","142","    cw = compute_class_weight({2: 1.5, 4: 0.5}, classes, y)"]}],"sklearn\/linear_model\/tests\/test_sgd.py":[{"add":["575","             r\"use compute_class_weight\\('balanced', classes=classes, y=y\\). \""],"delete":["575","             r\"use compute_class_weight\\('balanced', classes, y\\). \""]}],"sklearn\/decomposition\/_sparse_pca.py":[{"add":["189","        Vt, _, E, self.n_iter_ = dict_learning(X.T, n_components,","190","                                               alpha=self.alpha,"],"delete":["189","        Vt, _, E, self.n_iter_ = dict_learning(X.T, n_components, self.alpha,"]}],"sklearn\/cluster\/_optics.py":[{"add":["263","                reachability=self.reachability_,","264","                predecessor=self.predecessor_,","265","                ordering=self.ordering_,","266","                min_samples=self.min_samples,","267","                min_cluster_size=self.min_cluster_size,","268","                xi=self.xi,","269","                predecessor_correction=self.predecessor_correction)","281","            labels_ = cluster_optics_dbscan(","282","                reachability=self.reachability_,","283","                core_distances=self.core_distances_,","284","                ordering=self.ordering_, eps=eps)","341","@_deprecate_positional_args","342","def compute_optics_graph(X, *, min_samples, max_eps, metric, p, metric_params,","541","@_deprecate_positional_args","542","def cluster_optics_dbscan(*, reachability, core_distances, ordering, eps):","581","def cluster_optics_xi(*, reachability, predecessor, ordering, min_samples,"],"delete":["263","                self.reachability_,","264","                self.predecessor_,","265","                self.ordering_,","266","                self.min_samples,","267","                self.min_cluster_size,","268","                self.xi,","269","                self.predecessor_correction)","281","            labels_ = cluster_optics_dbscan(self.reachability_,","282","                                            self.core_distances_,","283","                                            self.ordering_,","284","                                            eps)","341","def compute_optics_graph(X, min_samples, max_eps, metric, p, metric_params,","540","def cluster_optics_dbscan(reachability, core_distances, ordering, eps):","579","def cluster_optics_xi(reachability, predecessor, ordering, min_samples,"]}],"sklearn\/feature_selection\/_mutual_info.py":[{"add":["13","from ..utils.validation import _deprecate_positional_args","293","@_deprecate_positional_args","294","def mutual_info_regression(X, y, *, discrete_features='auto', n_neighbors=3,","371","@_deprecate_positional_args","372","def mutual_info_classif(X, y, *, discrete_features='auto', n_neighbors=3,"],"delete":["292","def mutual_info_regression(X, y, discrete_features='auto', n_neighbors=3,","369","def mutual_info_classif(X, y, discrete_features='auto', n_neighbors=3,"]}],"sklearn\/cluster\/_mean_shift.py":[{"add":["28","@_deprecate_positional_args","29","def estimate_bandwidth(X, *, quantile=0.3, n_samples=None, random_state=0,","109","@_deprecate_positional_args","110","def mean_shift(X, *, bandwidth=None, seeds=None, bin_seeding=False,"],"delete":["28","def estimate_bandwidth(X, quantile=0.3, n_samples=None, random_state=0,","108","def mean_shift(X, bandwidth=None, seeds=None, bin_seeding=False,"]}],"sklearn\/linear_model\/tests\/test_sag.py":[{"add":["646","    class_weight_ = compute_class_weight(class_weight, classes=np.unique(y),","647","                                         y=y)","693","    class_weight_ = compute_class_weight(class_weight, classes=np.unique(y),","694","                                         y=y)"],"delete":["646","    class_weight_ = compute_class_weight(class_weight, np.unique(y), y)","692","    class_weight_ = compute_class_weight(class_weight, np.unique(y), y)"]}],"sklearn\/cluster\/tests\/test_hierarchical.py":[{"add":["76","            tree_builder(X.T, connectivity=connectivity)","82","            tree_builder(X.T, connectivity=np.ones((4, 4)))","85","            tree_builder(X.T[:0], connectivity=connectivity)","118","        children, n_nodes, n_leaves, parent = linkage_func(","119","            X.T, connectivity=connectivity)","301","            children, _, n_leaves, _ = _TREE_BUILDERS[linkage](","302","                X, connectivity=connectivity)"],"delete":["76","            tree_builder(X.T, connectivity)","82","            tree_builder(X.T, np.ones((4, 4)))","85","            tree_builder(X.T[:0], connectivity)","118","        children, n_nodes, n_leaves, parent = linkage_func(X.T, connectivity)","300","            children, _, n_leaves, _ = _TREE_BUILDERS[linkage](X, connectivity)"]}],"sklearn\/cluster\/tests\/test_mean_shift.py":[{"add":["73","    assert_raise_message(TypeError, msg, estimate_bandwidth, X)"],"delete":["73","    assert_raise_message(TypeError, msg, estimate_bandwidth, X, 200)"]}]}},"7bbd0d56530ad3758841595d81ab8deed069879d":{"changes":{"asv_benchmarks\/benchmarks\/datasets.py":"MODIFY"},"diff":{"asv_benchmarks\/benchmarks\/datasets.py":[{"add":["57","    X, y = fetch_openml('mnist_784', version=1, return_X_y=True,","58","                        as_frame=False)"],"delete":["57","    X, y = fetch_openml('mnist_784', version=1, return_X_y=True)"]}]}},"854f6493075ff9cdab0c96522282f5c3979bf16b":{"changes":{"sklearn\/metrics\/_classification.py":"MODIFY","doc\/modules\/model_evaluation.rst":"MODIFY"},"diff":{"sklearn\/metrics\/_classification.py":[{"add":["2386","    The Brier score measures the mean squared difference between the predicted","2387","    probability and the actual outcome. The Brier score always","2391","    of only 0 and 1). It can be decomposed is the sum of refinement loss and","2393","","2399","    label is controlled via the parameter `pos_label`, which defaults to","2400","    the greater label unless `y_true` is all 0 or all -1, in which case","2401","    `pos_label` defaults to 1.","2402","","2403","    Read more in the :ref:`User Guide <brier_score_loss>`."],"delete":["2386","    Across all items in a set N predictions, the Brier score measures the","2387","    mean squared difference between (1) the predicted probability assigned","2388","    to the possible outcomes for item i, and (2) the actual outcome.","2389","    Therefore, the lower the Brier score is for a set of predictions, the","2390","    better the predictions are calibrated. Note that the Brier score always","2394","    of only 0 and 1). The Brier loss is composed of refinement loss and","2401","    label is controlled via the parameter pos_label, which defaults to 1.","2402","    Read more in the :ref:`User Guide <calibration>`."]}],"doc\/modules\/model_evaluation.rst":[{"add":["1507","the actual outcome can be a value between 0 and 1 [Brier1950]_.","1509","The Brier score loss is also between 0 to 1 and the lower the score (the mean","1538","The Brier score can be used to assess how well a classifier is calibrated","1539","however, a lower Brier score does not always mean a better calibration. This is","1540","because the Brier score can be decomposed as the sum of calibration loss and","1541","refinement loss [Bella2012]_. Calibration loss is defined as the mean squared","1542","deviation from empirical probabilities derived from the slope of ROC segments.","1543","Refinement loss can be defined as the expected optimal loss as measured by the","1544","area under the optimal cost curve. Refinement loss can change independently","1545","from calibration loss, thus a lower Brier score does not necessarily mean a","1546","better calibrated model. \"Only when refinement loss remains the same does a","1547","lower Brier score always mean better calibration\" [Bella2012]_, [Flach2008]_.","1557","  .. [Brier1950] G. Brier, `Verification of forecasts expressed in terms of","1558","    probability","1562","  .. [Bella2012] Bella, Ferri, Hern¨¢ndez-Orallo, and Ram¨ªrez-Quintana","1563","    `\"Calibration of Machine Learning Models\"","1564","    <http:\/\/dmip.webs.upv.es\/papers\/BFHRHandbook2010.pdf>`_","1565","    in Khosrow-Pour, M. \"Machine learning: concepts, methodologies, tools","1566","    and applications.\" Hershey, PA: Information Science Reference (2012).","1567","","1568","  .. [Flach2008] Flach, Peter, and Edson Matsubara. `\"On classification, ranking,","1569","    and probability estimation.\" <https:\/\/drops.dagstuhl.de\/opus\/volltexte\/2008\/1382\/>`_","1570","    Dagstuhl Seminar Proceedings. Schloss Dagstuhl-Leibniz-Zentrum fr Informatik (2008).","1571",""],"delete":["1507","the actual outcome can be a value between 0 and 1.","1509","The brier score loss is also between 0 to 1 and the lower the score (the mean","1547","  * G. Brier, `Verification of forecasts expressed in terms of probability"]}]}},"a54cb47c74e54345001fbe2f9b5ed34e0fe9f9f8":{"changes":{"sklearn\/metrics\/_ranking.py":"MODIFY","sklearn\/metrics\/tests\/test_common.py":"MODIFY","sklearn\/metrics\/_classification.py":"MODIFY","sklearn\/metrics\/tests\/test_classification.py":"MODIFY","sklearn\/metrics\/tests\/test_ranking.py":"MODIFY"},"diff":{"sklearn\/metrics\/_ranking.py":[{"add":["210","        # Convert to Python primitive type to avoid NumPy type \/ Python str","211","        # comparison. See https:\/\/github.com\/numpy\/numpy\/issues\/6784","212","        present_labels = np.unique(y_true).tolist()","214","            raise ValueError(","215","                f\"pos_label={pos_label} is not a valid label. It should be \"","216","                f\"one of {present_labels}\"","217","            )"],"delete":["210","        present_labels = np.unique(y_true)","212","            raise ValueError(\"pos_label=%r is invalid. Set it to a label in \"","213","                             \"y_true.\" % pos_label)"]}],"sklearn\/metrics\/tests\/test_common.py":[{"add":["2","from inspect import signature","1415","","1416","","1417","@pytest.mark.parametrize(\"metric_name\", CLASSIFICATION_METRICS)","1418","def test_metrics_consistent_type_error(metric_name):","1419","    # check that an understable message is raised when the type between y_true","1420","    # and y_pred mismatch","1421","    rng = np.random.RandomState(42)","1422","    y1 = np.array([\"spam\"] * 3 + [\"eggs\"] * 2, dtype=object)","1423","    y2 = rng.randint(0, 2, size=y1.size)","1424","","1425","    err_msg = \"Labels in y_true and y_pred should be of the same type.\"","1426","    with pytest.raises(TypeError, match=err_msg):","1427","        CLASSIFICATION_METRICS[metric_name](y1, y2)","1428","","1429","","1430","@pytest.mark.parametrize(","1431","    \"metric, y_pred_threshold\",","1432","    [","1433","        (average_precision_score, True),","1434","        # FIXME: `brier_score_loss` does not follow this convention.","1435","        # See discussion in:","1436","        # https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/18307","1437","        pytest.param(","1438","            brier_score_loss, True, marks=pytest.mark.xfail(reason=\"#18307\")","1439","        ),","1440","        (f1_score, False),","1441","        (partial(fbeta_score, beta=1), False),","1442","        (jaccard_score, False),","1443","        (precision_recall_curve, True),","1444","        (precision_score, False),","1445","        (recall_score, False),","1446","        (roc_curve, True),","1447","    ],","1448",")","1449","@pytest.mark.parametrize(\"dtype_y_str\", [str, object])","1450","def test_metrics_pos_label_error_str(metric, y_pred_threshold, dtype_y_str):","1451","    # check that the error message if `pos_label` is not specified and the","1452","    # targets is made of strings.","1453","    rng = np.random.RandomState(42)","1454","    y1 = np.array([\"spam\"] * 3 + [\"eggs\"] * 2, dtype=dtype_y_str)","1455","    y2 = rng.randint(0, 2, size=y1.size)","1456","","1457","    if not y_pred_threshold:","1458","        y2 = np.array([\"spam\", \"eggs\"], dtype=dtype_y_str)[y2]","1459","","1460","    err_msg_pos_label_None = (","1461","        \"y_true takes value in {'eggs', 'spam'} and pos_label is not \"","1462","        \"specified: either make y_true take value in {0, 1} or {-1, 1} or \"","1463","        \"pass pos_label explicit\"","1464","    )","1465","    err_msg_pos_label_1 = (","1466","        r\"pos_label=1 is not a valid label. It should be one of \"","1467","        r\"\\['eggs', 'spam'\\]\"","1468","    )","1469","","1470","    pos_label_default = signature(metric).parameters[\"pos_label\"].default","1471","","1472","    err_msg = (","1473","        err_msg_pos_label_1","1474","        if pos_label_default == 1","1475","        else err_msg_pos_label_None","1476","    )","1477","    with pytest.raises(ValueError, match=err_msg):","1478","        metric(y1, y2)"],"delete":[]}],"sklearn\/metrics\/_classification.py":[{"add":["103","            try:","104","                unique_values = np.union1d(y_true, y_pred)","105","            except TypeError as e:","106","                # We expect y_true and y_pred to be of the same data type.","107","                # If `y_true` was provided to the classifier as strings,","108","                # `y_pred` given by the classifier will also be encoded with","109","                # strings. So we raise a meaningful error","110","                raise TypeError(","111","                    f\"Labels in y_true and y_pred should be of the same type. \"","112","                    f\"Got y_true={np.unique(y_true)} and \"","113","                    f\"y_pred={np.unique(y_pred)}. Make sure that the \"","114","                    f\"predictions provided by the classifier coincides with \"","115","                    f\"the true labels.\"","116","                ) from e","1272","    # Convert to Python primitive type to avoid NumPy type \/ Python str","1273","    # comparison. See https:\/\/github.com\/numpy\/numpy\/issues\/6784","1274","    present_labels = unique_labels(y_true, y_pred).tolist()","1279","                    raise ValueError(","1280","                        f\"pos_label={pos_label} is not a valid label. It \"","1281","                        f\"should be one of {present_labels}\"","1282","                    )"],"delete":["103","            unique_values = np.union1d(y_true, y_pred)","1259","    present_labels = unique_labels(y_true, y_pred)","1264","                    raise ValueError(\"pos_label=%r is not a valid label: \"","1265","                                     \"%r\" % (pos_label, present_labels))"]}],"sklearn\/metrics\/tests\/test_classification.py":[{"add":["1249","    err_msg = r\"pos_label=2 is not a valid label. It should be one of \\[0, 1\\]\""],"delete":["1249","    err_msg = r\"pos_label=2 is not a valid label: array\\(\\[0, 1\\]\\)\""]}],"sklearn\/metrics\/tests\/test_ranking.py":[{"add":["890","    err_msg = r\"pos_label=2 is not a valid label. It should be one of \\[0, 1\\]\"","891","    with pytest.raises(ValueError, match=err_msg):","897","    err_msg = (","898","        \"Parameter pos_label is fixed to 1 for multilabel-indicator y_true. \"","899","        \"Do not set pos_label or set pos_label to 1.\"","900","    )","901","    with pytest.raises(ValueError, match=err_msg):"],"delete":["890","    error_message = (\"pos_label=2 is invalid. Set it to a label in y_true.\")","891","    with pytest.raises(ValueError, match=error_message):","897","    error_message = (\"Parameter pos_label is fixed to 1 for multilabel\"","898","                     \"-indicator y_true. Do not set pos_label or set \"","899","                     \"pos_label to 1.\")","900","    with pytest.raises(ValueError, match=error_message):"]}]}},"b0c03d128007bd264e0e2b82e14c123a51f22308":{"changes":{"sklearn\/decomposition\/_dict_learning.py":"MODIFY","sklearn\/decomposition\/tests\/test_dict_learning.py":"MODIFY","sklearn\/datasets\/tests\/test_base.py":"MODIFY"},"diff":{"sklearn\/decomposition\/_dict_learning.py":[{"add":["16","from ..utils import deprecated","882","class _BaseSparseCoding(TransformerMixin):","883","    \"\"\"Base class from SparseCoder and DictionaryLearning algorithms.\"\"\"","884","    def __init__(self, transform_algorithm, transform_n_nonzero_coefs,","885","                 transform_alpha, split_sign, n_jobs, positive_code,","886","                 transform_max_iter):","895","    def _transform(self, X, dictionary):","896","        \"\"\"Private method allowing to accomodate both DictionaryLearning and","897","        SparseCoder.\"\"\"","898","        X = check_array(X)","899","","900","        code = sparse_encode(","901","            X, dictionary, algorithm=self.transform_algorithm,","902","            n_nonzero_coefs=self.transform_n_nonzero_coefs,","903","            alpha=self.transform_alpha, max_iter=self.transform_max_iter,","904","            n_jobs=self.n_jobs, positive=self.positive_code)","905","","906","        if self.split_sign:","907","            # feature vector is split into a positive and negative side","908","            n_samples, n_features = code.shape","909","            split_code = np.empty((n_samples, 2 * n_features))","910","            split_code[:, :n_features] = np.maximum(code, 0)","911","            split_code[:, n_features:] = -np.minimum(code, 0)","912","            code = split_code","913","","914","        return code","915","","934","        return self._transform(X, self.components_)","937","class SparseCoder(_BaseSparseCoding, BaseEstimator):","957","            'threshold'}, default='omp'","969","    transform_n_nonzero_coefs : int, default=0.1 * n_features","988","    n_jobs : int, default=None","1008","        The unchanged dictionary atoms.","1009","","1010","        .. deprecated:: 0.24","1011","           This attribute is deprecated in 0.24 and will be removed in 0.26.","1012","           Use `dictionary` instead.","1050","        super().__init__(","1051","            transform_algorithm, transform_n_nonzero_coefs,","1052","            transform_alpha, split_sign, n_jobs, positive_code,","1053","            transform_max_iter","1054","        )","1055","        self.dictionary = dictionary","1058","        \"\"\"Do nothing and return the estimator unchanged.","1075","    @deprecated(\"The attribute 'components_' is deprecated \"  # type: ignore","1076","                \"in 0.24 and will be removed in 0.26. Use the \"","1077","                \"'dictionary' instead.\")","1078","    @property","1079","    def components_(self):","1080","        return self.dictionary","1081","","1082","    def transform(self, X, y=None):","1083","        \"\"\"Encode the data as a sparse combination of the dictionary atoms.","1084","","1085","        Coding method is determined by the object parameter","1086","        `transform_algorithm`.","1087","","1088","        Parameters","1089","        ----------","1090","        X : array of shape (n_samples, n_features)","1091","            Test data to be transformed, must have the same number of","1092","            features as the data used to train the model.","1093","","1094","        Returns","1095","        -------","1096","        X_new : array of shape (n_samples, n_components)","1097","            Transformed data","1098","        \"\"\"","1099","        return super()._transform(X, self.dictionary)","1100","","1101","    def _more_tags(self):","1102","        return {\"requires_fit\": False}","1103","","1104","    @property","1105","    def n_components_(self):","1106","        return self.dictionary.shape[0]","1107","","1110","        return self.dictionary.shape[1]","1113","class DictionaryLearning(_BaseSparseCoding, BaseEstimator):","1284","        super().__init__(","1285","            transform_algorithm, transform_n_nonzero_coefs,","1286","            transform_alpha, split_sign, n_jobs, positive_code,","1287","            transform_max_iter","1288","        )","1289","        self.n_components = n_components","1341","class MiniBatchDictionaryLearning(_BaseSparseCoding, BaseEstimator):","1519","        super().__init__(","1520","            transform_algorithm, transform_n_nonzero_coefs, transform_alpha,","1521","            split_sign, n_jobs, positive_code, transform_max_iter","1522","        )","1523","        self.n_components = n_components"],"delete":["881","class SparseCodingMixin(TransformerMixin):","882","    \"\"\"Sparse coding mixin\"\"\"","883","","884","    def _set_sparse_coding_params(self, n_components,","885","                                  transform_algorithm='omp',","886","                                  transform_n_nonzero_coefs=None,","887","                                  transform_alpha=None, split_sign=False,","888","                                  n_jobs=None, positive_code=False,","889","                                  transform_max_iter=1000):","890","        self.n_components = n_components","915","","918","","919","        X = check_array(X)","920","","921","        code = sparse_encode(","922","            X, self.components_, algorithm=self.transform_algorithm,","923","            n_nonzero_coefs=self.transform_n_nonzero_coefs,","924","            alpha=self.transform_alpha, max_iter=self.transform_max_iter,","925","            n_jobs=self.n_jobs, positive=self.positive_code)","926","","927","        if self.split_sign:","928","            # feature vector is split into a positive and negative side","929","            n_samples, n_features = code.shape","930","            split_code = np.empty((n_samples, 2 * n_features))","931","            split_code[:, :n_features] = np.maximum(code, 0)","932","            split_code[:, n_features:] = -np.minimum(code, 0)","933","            code = split_code","934","","935","        return code","938","class SparseCoder(SparseCodingMixin, BaseEstimator):","958","    'threshold'}, default='omp'","970","    transform_n_nonzero_coefs : int, default=0.1*n_features","989","    n_jobs : int or None, default=None","1009","        The unchanged dictionary atoms","1047","        self._set_sparse_coding_params(dictionary.shape[0],","1048","                                       transform_algorithm,","1049","                                       transform_n_nonzero_coefs,","1050","                                       transform_alpha, split_sign, n_jobs,","1051","                                       positive_code, transform_max_iter)","1052","        self.components_ = dictionary","1055","        \"\"\"Do nothing and return the estimator unchanged","1069","            Returns the object itself","1075","        return self.components_.shape[1]","1078","class DictionaryLearning(SparseCodingMixin, BaseEstimator):","1249","        self._set_sparse_coding_params(n_components, transform_algorithm,","1250","                                       transform_n_nonzero_coefs,","1251","                                       transform_alpha, split_sign, n_jobs,","1252","                                       positive_code, transform_max_iter)","1304","class MiniBatchDictionaryLearning(SparseCodingMixin, BaseEstimator):","1482","        self._set_sparse_coding_params(n_components, transform_algorithm,","1483","                                       transform_n_nonzero_coefs,","1484","                                       transform_alpha, split_sign, n_jobs,","1485","                                       positive_code, transform_max_iter)"]}],"sklearn\/decomposition\/tests\/test_dict_learning.py":[{"add":["3","from functools import partial","6","from sklearn.base import clone","7","","23","from sklearn.utils.estimator_checks import check_transformer_data_not_an_array","24","from sklearn.utils.estimator_checks import check_transformer_general","25","from sklearn.utils.estimator_checks import check_transformers_unfitted","500","    coder = SparseCoder(dictionary=V, transform_algorithm='lasso_lars',","501","                        transform_alpha=0.001).transform(X)","502","    assert not np.all(coder == 0)","503","    assert np.sqrt(np.sum((np.dot(coder, V) - X) ** 2)) < 0.1","504","","505","","506","def test_sparse_coder_estimator_clone():","507","    n_components = 12","508","    rng = np.random.RandomState(0)","509","    V = rng.randn(n_components, n_features)  # random init","510","    V \/= np.sum(V ** 2, axis=1)[:, np.newaxis]","511","    coder = SparseCoder(dictionary=V, transform_algorithm='lasso_lars',","512","                        transform_alpha=0.001)","513","    cloned = clone(coder)","514","    assert id(cloned) != id(coder)","515","    np.testing.assert_allclose(cloned.dictionary, coder.dictionary)","516","    assert id(cloned.dictionary) != id(coder.dictionary)","517","    assert cloned.n_components_ == coder.n_components_","518","    assert cloned.n_features_in_ == coder.n_features_in_","519","    data = np.random.rand(n_samples, n_features).astype(np.float32)","520","    np.testing.assert_allclose(cloned.transform(data),","521","                               coder.transform(data))","543","def test_sparse_coder_common_transformer():","544","    rng = np.random.RandomState(777)","545","    n_components, n_features = 40, 3","546","    init_dict = rng.rand(n_components, n_features)","547","","548","    sc = SparseCoder(init_dict)","549","","550","    check_transformer_data_not_an_array(sc.__class__.__name__, sc)","551","    check_transformer_general(sc.__class__.__name__, sc)","552","    check_transformer_general_memmap = partial(","553","        check_transformer_general, readonly_memmap=True","554","    )","555","    check_transformer_general_memmap(sc.__class__.__name__, sc)","556","    check_transformers_unfitted(sc.__class__.__name__, sc)","557","","558","","559","# TODO: remove in 0.26","560","def test_sparse_coder_deprecation():","561","    # check that we raise a deprecation warning when accessing `components_`","562","    rng = np.random.RandomState(777)","563","    n_components, n_features = 40, 64","564","    init_dict = rng.rand(n_components, n_features)","565","    sc = SparseCoder(init_dict)","566","","567","    with pytest.warns(FutureWarning, match=\"'components_' is deprecated\"):","568","        sc.components_","569","","570",""],"delete":["494","    code = SparseCoder(dictionary=V, transform_algorithm='lasso_lars',","495","                       transform_alpha=0.001).transform(X)","496","    assert not np.all(code == 0)","497","    assert np.sqrt(np.sum((np.dot(code, V) - X) ** 2)) < 0.1"]}],"sklearn\/datasets\/tests\/test_base.py":[{"add":[],"delete":["4","import numpy"]}]}},"547ead6c0c77633df5391cf3523f4f326fc26ea3":{"changes":{"sklearn\/ensemble\/_gradient_boosting.pyx":"MODIFY","sklearn\/utils\/validation.py":"MODIFY","sklearn\/ensemble\/_gb.py":"MODIFY","sklearn\/inspection\/_plot\/partial_dependence.py":"MODIFY","examples\/linear_model\/plot_tweedie_regression_insurance_claims.py":"MODIFY","sklearn\/preprocessing\/_encoders.py":"MODIFY","sklearn\/compose\/tests\/test_column_transformer.py":"MODIFY","sklearn\/inspection\/_partial_dependence.py":"MODIFY","sklearn\/neural_network\/tests\/test_rbm.py":"MODIFY","sklearn\/compose\/_column_transformer.py":"MODIFY","sklearn\/preprocessing\/tests\/test_encoders.py":"MODIFY","sklearn\/utils\/tests\/test_validation.py":"MODIFY"},"diff":{"sklearn\/ensemble\/_gradient_boosting.pyx":[{"add":["264","          np_zeros((n_total_samples,), dtype=bool)"],"delete":["34","from numpy import bool as np_bool","265","          np_zeros((n_total_samples,), dtype=np_bool)"]}],"sklearn\/utils\/validation.py":[{"add":["517","                dtypes_orig[i] = np.dtype(object)"],"delete":["517","                dtypes_orig[i] = np.dtype(np.object)"]}],"sklearn\/ensemble\/_gb.py":[{"add":["308","                                    dtype=object)","318","            self.estimators_ = np.empty((0, 0), dtype=object)"],"delete":["308","                                    dtype=np.object)","318","            self.estimators_ = np.empty((0, 0), dtype=np.object)"]}],"sklearn\/inspection\/_plot\/partial_dependence.py":[{"add":["250","        X = check_array(X, force_all_finite='allow-nan', dtype=object)","602","            self.axes_ = np.empty((n_rows, n_cols), dtype=object)","604","                self.lines_ = np.empty((n_rows, n_cols), dtype=object)","607","                                       dtype=object)","608","            self.contours_ = np.empty((n_rows, n_cols), dtype=object)","632","                self.lines_ = np.empty_like(ax, dtype=object)","635","                                       dtype=object)","636","            self.contours_ = np.empty_like(ax, dtype=object)","642","        self.deciles_vlines_ = np.empty_like(self.axes_, dtype=object)","643","        self.deciles_hlines_ = np.empty_like(self.axes_, dtype=object)"],"delete":["250","        X = check_array(X, force_all_finite='allow-nan', dtype=np.object)","602","            self.axes_ = np.empty((n_rows, n_cols), dtype=np.object)","604","                self.lines_ = np.empty((n_rows, n_cols), dtype=np.object)","607","                                       dtype=np.object)","608","            self.contours_ = np.empty((n_rows, n_cols), dtype=np.object)","632","                self.lines_ = np.empty_like(ax, dtype=np.object)","635","                                       dtype=np.object)","636","            self.contours_ = np.empty_like(ax, dtype=np.object)","642","        self.deciles_vlines_ = np.empty_like(self.axes_, dtype=np.object)","643","        self.deciles_hlines_ = np.empty_like(self.axes_, dtype=np.object)"]}],"examples\/linear_model\/plot_tweedie_regression_insurance_claims.py":[{"add":["88","    for column_name in df.columns[df.dtypes.values == object]:"],"delete":["88","    for column_name in df.columns[df.dtypes.values == np.object]:"]}],"sklearn\/preprocessing\/_encoders.py":[{"add":["45","                X = check_array(X, dtype=object)","324","                return np.zeros(len(self.categories_), dtype=object)","327","                                for cats in self.categories_], dtype=object)","363","                            dtype=object)"],"delete":["45","                X = check_array(X, dtype=np.object)","324","                return np.zeros(len(self.categories_), dtype=np.object)","327","                                for cats in self.categories_], dtype=np.object)","363","                            dtype=np.object)"]}],"sklearn\/compose\/tests\/test_column_transformer.py":[{"add":["1275","    (['col_str'], None, [object], None),","1276","    (['col_str'], None, object, None),","1284","    (['col_int', 'col_float', 'col_str'], None, [np.number, object], None),"],"delete":["1275","    (['col_str'], None, [np.object], None),","1276","    (['col_str'], None, np.object, None),","1284","    (['col_int', 'col_float', 'col_str'], None, [np.number, np.object], None),"]}],"sklearn\/inspection\/_partial_dependence.py":[{"add":["399","        X = check_array(X, force_all_finite='allow-nan', dtype=object)"],"delete":["399","        X = check_array(X, force_all_finite='allow-nan', dtype=np.object)"]}],"sklearn\/neural_network\/tests\/test_rbm.py":[{"add":["198","    (int, np.float64)])"],"delete":["198","    (np.int, np.float64)])"]}],"sklearn\/compose\/_column_transformer.py":[{"add":["650","    return check_array(X, force_all_finite='allow-nan', dtype=object)"],"delete":["650","    return check_array(X, force_all_finite='allow-nan', dtype=np.object)"]}],"sklearn\/preprocessing\/tests\/test_encoders.py":[{"add":["684","    assert ohe_test.drop_idx_.dtype == object"],"delete":["684","    assert ohe_test.drop_idx_.dtype == np.object"]}],"sklearn\/utils\/tests\/test_validation.py":[{"add":["213","     (np.array([[1, np.nan]], dtype=object),","385","    X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=object)"],"delete":["213","     (np.array([[1, np.nan]], dtype=np.object),","385","    X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=np.object)"]}]}},"fc06baef499b8e0a6d677d4a19fa983f173ad06c":{"changes":{"sklearn\/cluster\/_kmeans.py":"MODIFY","sklearn\/cluster\/tests\/test_k_means.py":"MODIFY","doc\/whats_new\/v0.24.rst":"MODIFY"},"diff":{"sklearn\/cluster\/_kmeans.py":[{"add":["358","    labels_old = labels.copy()","378","    strict_convergence = False","379","","398","        if np.array_equal(labels, labels_old):","399","            # First check the labels for strict convergence.","401","                print(f\"Converged at iteration {i}: strict convergence.\")","402","            strict_convergence = True","404","        else:","405","            # No strict convergence, check for tol based convergence.","406","            center_shift_tot = (center_shift**2).sum()","407","            if center_shift_tot <= tol:","408","                if verbose:","409","                    print(f\"Converged at iteration {i}: center shift \"","410","                          f\"{center_shift_tot} within tolerance {tol}.\")","411","                break","413","        labels_old[:] = labels","414","","415","    if not strict_convergence:","486","    labels_old = labels.copy()","497","    strict_convergence = False","498","","512","            if np.array_equal(labels, labels_old):","513","                # First check the labels for strict convergence.","515","                    print(f\"Converged at iteration {i}: strict convergence.\")","516","                strict_convergence = True","518","            else:","519","                # No strict convergence, check for tol based convergence.","520","                center_shift_tot = (center_shift**2).sum()","521","                if center_shift_tot <= tol:","522","                    if verbose:","523","                        print(f\"Converged at iteration {i}: center shift \"","524","                              f\"{center_shift_tot} within tolerance {tol}.\")","525","                    break","527","            labels_old[:] = labels","528","","529","        if not strict_convergence:"],"delete":["228","        It's not advised to set `tol=0` since convergence might never be","229","        declared due to rounding errors. Use a very small number instead.","397","        center_shift_tot = (center_shift**2).sum()","398","        if center_shift_tot <= tol:","400","                print(f\"Converged at iteration {i}: center shift \"","401","                      f\"{center_shift_tot} within tolerance {tol}.\")","404","    if center_shift_tot > 0:","498","            center_shift_tot = (center_shift**2).sum()","499","            if center_shift_tot <= tol:","501","                    print(\"Converged at iteration {0}: \"","502","                          \"center shift {1} within tolerance {2}\"","503","                          .format(i, center_shift_tot, tol))","506","        if center_shift_tot > 0:","619","        It's not advised to set `tol=0` since convergence might never be","620","        declared due to rounding errors. Use a very small number instead."]}],"sklearn\/cluster\/tests\/test_k_means.py":[{"add":["1","import re","139","@pytest.mark.parametrize(\"tol\", [1e-2, 1e-8, 1e-100, 0])","168","    max_iter = 300","170","    km = KMeans(algorithm=algorithm, n_clusters=5, random_state=0,","171","                n_init=1, tol=0, max_iter=max_iter).fit(X)","173","    assert km.n_iter_ < max_iter","338","def test_minibatch_kmeans_verbose():","339","    # Check verbose mode of MiniBatchKMeans for better coverage.","340","    km = MiniBatchKMeans(n_clusters=n_clusters, random_state=42, verbose=1)","349","@pytest.mark.parametrize(\"algorithm\", [\"full\", \"elkan\"])","350","@pytest.mark.parametrize(\"tol\", [1e-2, 0])","351","def test_kmeans_verbose(algorithm, tol, capsys):","352","    # Check verbose mode of KMeans for better coverage.","353","    X = np.random.RandomState(0).normal(size=(5000, 10))","354","","355","    KMeans(algorithm=algorithm, n_clusters=n_clusters, random_state=42,","356","           init=\"random\", n_init=1, tol=tol, verbose=1).fit(X)","357","","358","    captured = capsys.readouterr()","359","","360","    assert re.search(r\"Initialization complete\", captured.out)","361","    assert re.search(r\"Iteration [0-9]+, inertia\", captured.out)","362","","363","    if tol == 0:","364","        assert re.search(r\"strict convergence\", captured.out)","365","    else:","366","        assert re.search(r\"center shift .* within tolerance\", captured.out)","367","","368",""],"delete":["138","@pytest.mark.parametrize(\"tol\", [1e-2, 1e-4, 1e-8])","165","    # We can only ensure that if the number of threads is not to large,","166","    # otherwise the roundings errors coming from the unpredictability of","167","    # the order in which chunks are processed make the convergence criterion","168","    # to never be exactly 0.","172","    with threadpool_limits(limits=1, user_api=\"openmp\"):","173","        km = KMeans(algorithm=algorithm, n_clusters=5, random_state=0,","174","                    n_init=1, tol=0, max_iter=300).fit(X)","176","    assert km.n_iter_ < 300","341","@pytest.mark.parametrize(\"Estimator\", [KMeans, MiniBatchKMeans])","342","def test_verbose(Estimator):","343","    # Check verbose mode of KMeans and MiniBatchKMeans for better coverage.","344","    km = Estimator(n_clusters=n_clusters, random_state=42, verbose=1)"]}],"doc\/whats_new\/v0.24.rst":[{"add":["93","- |Fix| Fixed a bug in :class:`cluster.KMeans` where rounding errors could","94","  prevent convergence to be declared when `tol=0`. :pr:`17959` by","95","  :user:`J¨¦r¨¦mie du Boisberranger <jeremiedbb>`."],"delete":[]}]}},"1523f395e952dd79d2427d8230056ed0cf47f7a3":{"changes":{"sklearn\/cluster\/_k_means_elkan.pyx":"MODIFY"},"diff":{"sklearn\/cluster\/_k_means_elkan.pyx":[{"add":["324","        free(centers_new_chunk)","325","        free(weight_in_clusters_chunk)","326","","558","        free(centers_new_chunk)","559","        free(weight_in_clusters_chunk)","560",""],"delete":[]}]}},"936854579a3cc112f0686e397798d2aa73e5fab3":{"changes":{"sklearn\/metrics\/tests\/test_regression.py":"MODIFY","doc\/whats_new\/v0.24.rst":"MODIFY","sklearn\/metrics\/_regression.py":"MODIFY"},"diff":{"sklearn\/metrics\/tests\/test_regression.py":[{"add":["78","    assert_almost_equal(error, 0.454, decimal=2)","260","    assert_almost_equal(rmsew, 0.59, decimal=2)"],"delete":["78","    assert_almost_equal(error, 0.645, decimal=2)","260","    assert_almost_equal(rmsew, 0.62, decimal=2)"]}],"doc\/whats_new\/v0.24.rst":[{"add":["63",":mod:`sklearn.metrics`","64","......................","65","","66","- |Fix| Fixed a bug in :func:`metrics.mean_squared_error` where the","67","  average of multiple RMSE values was incorrectly calculated as the root of the","68","  average of multiple MSE values.","69","  :pr:`17309` by :user:`Swier Heeres <swierh>`","70",""],"delete":[]}],"sklearn\/metrics\/_regression.py":[{"add":["246","    >>> mean_squared_error(y_true, y_pred, squared=False)","247","    0.822...","259","","260","    if not squared:","261","        output_errors = np.sqrt(output_errors)","262","","265","            return output_errors","270","    return np.average(output_errors, weights=multioutput)"],"delete":["259","            return output_errors if squared else np.sqrt(output_errors)","264","    mse = np.average(output_errors, weights=multioutput)","265","    return mse if squared else np.sqrt(mse)"]}]}},"8010cadf6ca9d319321ac72ff5e604b941f0d40c":{"changes":{"sklearn\/svm\/tests\/test_bounds.py":"MODIFY","sklearn\/svm\/_classes.py":"MODIFY","sklearn\/svm\/_bounds.py":"MODIFY"},"diff":{"sklearn\/svm\/tests\/test_bounds.py":[{"add":["39","                         l1_min_c, dense_X, Y1, loss=\"l2\")","43","    min_c = l1_min_c(X, y, loss=loss, fit_intercept=fit_intercept,","44","                     intercept_scaling=intercept_scaling)","75","        l1_min_c(dense_X, Y1, loss='l1')"],"delete":["39","                         l1_min_c, dense_X, Y1, \"l2\")","43","    min_c = l1_min_c(X, y, loss, fit_intercept, intercept_scaling)","74","        l1_min_c(dense_X, Y1, 'l1')"]}],"sklearn\/svm\/_classes.py":[{"add":["7","from ..utils.validation import _deprecate_positional_args","180","    @_deprecate_positional_args","181","    def __init__(self, penalty='l2', loss='squared_hinge', *, dual=True,","182","                 tol=1e-4, C=1.0, multi_class='ovr', fit_intercept=True,","367","    @_deprecate_positional_args","368","    def __init__(self, *, epsilon=0.0, tol=1e-4, C=1.0,","631","    @_deprecate_positional_args","632","    def __init__(self, *, C=1.0, kernel='rbf', degree=3, gamma='scale',","843","    @_deprecate_positional_args","844","    def __init__(self, *, nu=0.5, kernel='rbf', degree=3, gamma='scale',","998","    @_deprecate_positional_args","999","    def __init__(self, *, kernel='rbf', degree=3, gamma='scale',","1144","    @_deprecate_positional_args","1145","    def __init__(self, *, nu=0.5, C=1.0, kernel='rbf', degree=3,","1259","    @_deprecate_positional_args","1260","    def __init__(self, *, kernel='rbf', degree=3, gamma='scale',"],"delete":["179","","180","    def __init__(self, penalty='l2', loss='squared_hinge', dual=True, tol=1e-4,","181","                 C=1.0, multi_class='ovr', fit_intercept=True,","366","    def __init__(self, epsilon=0.0, tol=1e-4, C=1.0,","629","    def __init__(self, C=1.0, kernel='rbf', degree=3, gamma='scale',","840","    def __init__(self, nu=0.5, kernel='rbf', degree=3, gamma='scale',","994","    def __init__(self, kernel='rbf', degree=3, gamma='scale',","1139","    def __init__(self, nu=0.5, C=1.0, kernel='rbf', degree=3,","1253","    def __init__(self, kernel='rbf', degree=3, gamma='scale',"]}],"sklearn\/svm\/_bounds.py":[{"add":["8","from ..utils.validation import _deprecate_positional_args","12","@_deprecate_positional_args","13","def l1_min_c(X, y, *, loss='squared_hinge', fit_intercept=True,"],"delete":["11","def l1_min_c(X, y, loss='squared_hinge', fit_intercept=True,"]}]}},"f656c37d19308298ce5bd95eb2f0477c95a15bb7":{"changes":{"sklearn\/multioutput.py":"MODIFY","sklearn\/tests\/test_multioutput.py":"MODIFY","doc\/whats_new\/v0.24.rst":"MODIFY"},"diff":{"sklearn\/multioutput.py":[{"add":["455","        if isinstance(self.order_, tuple):","456","            self.order_ = np.array(self.order_)","457",""],"delete":[]}],"sklearn\/tests\/test_multioutput.py":[{"add":["603","","604","","605","@pytest.mark.parametrize(\"order_type\", [list, np.array, tuple])","606","def test_classifier_chain_tuple_order(order_type):","607","    X = [[1, 2, 3], [4, 5, 6], [1.5, 2.5, 3.5]]","608","    y = [[3, 2], [2, 3], [3, 2]]","609","    order = order_type([1, 0])","610","","611","    chain = ClassifierChain(RandomForestClassifier(), order=order)","612","","613","    chain.fit(X, y)","614","    X_test = [[1.5, 2.5, 3.5]]","615","    y_test = [[3, 2]]","616","    assert_array_almost_equal(chain.predict(X_test), y_test)","617","","618","","619","def test_classifier_chain_tuple_invalid_order():","620","    X = [[1, 2, 3], [4, 5, 6], [1.5, 2.5, 3.5]]","621","    y = [[3, 2], [2, 3], [3, 2]]","622","    order = tuple([1, 2])","623","","624","    chain = ClassifierChain(RandomForestClassifier(), order=order)","625","","626","    with pytest.raises(ValueError, match='invalid order'):","627","        chain.fit(X, y)"],"delete":[]}],"doc\/whats_new\/v0.24.rst":[{"add":["262","- |Efficiency| Fixed :issue:`10493`. Improve Local Linear Embedding (LLE)","325",":mod:`sklearn.multioutput`","326","..........................","327","","328","- |Fix| A fix to accept tuples for the ``order`` parameter ","329","  in :class:`multioutput.ClassifierChain`.","330","  :pr:`18124` by :user:`Gus Brocchini <boldloop>` and","331","  :user:`Amanda Dsouza <amy12xx>`.","332",""],"delete":["262","- |Efficiency| Fixed :issue:`10493`. Improve Local Linear Embedding (LLE) "]}]}},"430c2080e81dbf9aacc99dfd83d958030a7c4d07":{"changes":{"sklearn\/svm\/tests\/test_bounds.py":"MODIFY","sklearn\/svm\/src\/newrand\/newrand.h":"MODIFY","sklearn\/svm\/_newrand.pyx":"ADD","sklearn\/svm\/setup.py":"MODIFY"},"diff":{"sklearn\/svm\/tests\/test_bounds.py":[{"add":["2","from scipy import stats","9","from sklearn.svm._newrand import set_seed_wrap, bounded_rand_int_wrap","78","","79","","80","_MAX_UNSIGNED_INT = 4294967295","81","","82","","83","@pytest.mark.parametrize('seed, val',","84","                         [(None, 81),","85","                          (0, 54),","86","                          (_MAX_UNSIGNED_INT, 9)])","87","def test_newrand_set_seed(seed, val):","88","    \"\"\"Test that `set_seed` produces deterministic results\"\"\"","89","    if seed is not None:","90","        set_seed_wrap(seed)","91","    x = bounded_rand_int_wrap(100)","92","    assert x == val, f'Expected {val} but got {x} instead'","93","","94","","95","@pytest.mark.parametrize('seed',","96","                         [-1, _MAX_UNSIGNED_INT + 1])","97","def test_newrand_set_seed_overflow(seed):","98","    \"\"\"Test that `set_seed_wrap` is defined for unsigned 32bits ints\"\"\"","99","    with pytest.raises(OverflowError):","100","        set_seed_wrap(seed)","101","","102","","103","@pytest.mark.parametrize('range_, n_pts',","104","                         [(_MAX_UNSIGNED_INT, 10000), (100, 25)])","105","def test_newrand_bounded_rand_int(range_, n_pts):","106","    \"\"\"Test that `bounded_rand_int` follows a uniform distribution\"\"\"","107","    n_iter = 100","108","    ks_pvals = []","109","    uniform_dist = stats.uniform(loc=0, scale=range_)","110","    # perform multiple samplings to make chance of outlier sampling negligible","111","    for _ in range(n_iter):","112","        # Deterministic random sampling","113","        sample = [bounded_rand_int_wrap(range_) for _ in range(n_pts)]","114","        res = stats.kstest(sample, uniform_dist.cdf)","115","        ks_pvals.append(res.pvalue)","116","    # Null hypothesis = samples come from an uniform distribution.","117","    # Under the null hypothesis, p-values should be uniformly distributed","118","    # and not concentrated on low values","119","    # (this may seem counter-intuitive but is backed by multiple refs)","120","    # So we can do two checks:","121","","122","    # (1) check uniformity of p-values","123","    uniform_p_vals_dist = stats.uniform(loc=0, scale=1)","124","    res_pvals = stats.kstest(ks_pvals, uniform_p_vals_dist.cdf)","125","    assert res_pvals.pvalue > 0.05, (","126","        \"Null hypothesis rejected: generated random numbers are not uniform.\"","127","        \" Details: the (meta) p-value of the test of uniform distribution\"","128","        f\" of p-values is {res_pvals.pvalue} which is not > 0.05\")","129","","130","    # (2) (safety belt) check that 90% of p-values are above 0.05","131","    min_10pct_pval = np.percentile(ks_pvals, q=10)","132","    # lower 10th quantile pvalue <= 0.05 means that the test rejects the","133","    # null hypothesis that the sample came from the uniform distribution","134","    assert min_10pct_pval > 0.05, (","135","        \"Null hypothesis rejected: generated random numbers are not uniform. \"","136","        f\"Details: lower 10th quantile p-value of {min_10pct_pval} not > 0.05.\"","137","        )","138","","139","","140","@pytest.mark.parametrize('range_',","141","                         [-1, _MAX_UNSIGNED_INT + 1])","142","def test_newrand_bounded_rand_int_limits(range_):","143","    \"\"\"Test that `bounded_rand_int_wrap` is defined for unsigned 32bits ints\"\"\"","144","    with pytest.raises(OverflowError):","145","        bounded_rand_int_wrap(range_)"],"delete":[]}],"sklearn\/svm\/src\/newrand\/newrand.h":[{"add":["12","#include <random>  \/\/ needed for cython to generate a .cpp file from newrand.h","28","inline uint32_t bounded_rand_int(uint32_t range) {","29","    \/\/ \"LibSVM \/ LibLinear Original way\" - make a 31bit positive","31","    \/\/ return abs( (int)mt_rand()) % range;"],"delete":["7","","20","#if INT_MAX == 0x7FFFFFFF","22","#elif INT_MAX == 0x7FFFFFFFFFFFFFFF","23","std::mt19937_64 mt_rand(std::mt19937::default_seed);","24","#else","25","info(\"Random number generator is not fixed for this system. Please report issue. INT_MAX=%d\\n\", INT_MAX);","26","exit(1);","27","#endif","35","inline int bounded_rand_int(int orig_range) {","36","    \/\/ \"LibSVM \/ LibLinear Original way\" - make a 31bit or 63bit positive","38","    \/\/ return abs( (int)mt_rand()) % orig_range;","42","    \/\/ TODO how could we make this casting safer, raising an error if lost information?","43","    uint32_t range = uint32_t(orig_range);"]}],"sklearn\/svm\/_newrand.pyx":[{"add":[],"delete":[]}],"sklearn\/svm\/setup.py":[{"add":["12","    # newrand wrappers","13","    config.add_extension('_newrand',","14","                         sources=['_newrand.pyx'],","15","                         include_dirs=[numpy.get_include(),","16","                                       join('src', 'newrand')],","17","                         depends=[join('src', 'newrand', 'newrand.h')],","18","                         language='c++',","19","                         )","20",""],"delete":[]}]}},"98f0b832f8888ab2dd3a143ddd09525dd11f0479":{"changes":{"sklearn\/decomposition\/_dict_learning.py":"MODIFY","sklearn\/decomposition\/tests\/test_dict_learning.py":"MODIFY","doc\/whats_new\/v0.24.rst":"MODIFY"},"diff":{"sklearn\/decomposition\/_dict_learning.py":[{"add":["1492","            n_iter=1, method=self.fit_algorithm,","1506","        self.iter_offset_ = iter_offset + 1"],"delete":["1492","            n_iter=self.n_iter, method=self.fit_algorithm,","1506","        self.iter_offset_ = iter_offset + self.n_iter"]}],"sklearn\/decomposition\/tests\/test_dict_learning.py":[{"add":["393","def test_dict_learning_iter_offset():","394","    n_components = 12","395","    rng = np.random.RandomState(0)","396","    V = rng.randn(n_components, n_features)","397","    dict1 = MiniBatchDictionaryLearning(n_components, n_iter=10,","398","                                        dict_init=V, random_state=0,","399","                                        shuffle=False)","400","    dict2 = MiniBatchDictionaryLearning(n_components, n_iter=10,","401","                                        dict_init=V, random_state=0,","402","                                        shuffle=False)","403","    dict1.fit(X)","404","    for sample in X:","405","        dict2.partial_fit(sample[np.newaxis, :])","406","","407","    assert dict1.iter_offset_ == dict2.iter_offset_","408","","409",""],"delete":[]}],"doc\/whats_new\/v0.24.rst":[{"add":["46",":mod:`sklearn.decomposition`","47","............................","48","","49","- |Fix| Fixed a bug in","50","  :func:`decomposition.MiniBatchDictionaryLearning.partial_fit` which should","51","  update the dictionary by iterating only once over a mini-batch.","52","  :pr:`17433` by :user:`Chiara Marmo <cmarmo>`","53",""],"delete":[]}]}},"018de223237bb396d488b51c8d84fdf3954c1550":{"changes":{"sklearn\/model_selection\/_validation.py":"MODIFY","doc\/modules\/cross_validation.rst":"MODIFY"},"diff":{"sklearn\/model_selection\/_validation.py":[{"add":["1055","    Read more in the :ref:`User Guide <permutation_test_score>`.","1056",""],"delete":[]}],"doc\/modules\/cross_validation.rst":[{"add":["858","","859",".. _permutation_test_score:","860","","861","Permutation test score","862","======================","863","","864",":func:`~sklearn.model_selection.permutation_test_score` offers another way","865","to evaluate the performance of classifiers. It provides a permutation-based","866","p-value, which represents how likely an observed performance of the","867","classifier would be obtained by chance. The null hypothesis in this test is","868","that the classifier fails to leverage any statistical dependency between the","869","features and the labels to make correct predictions on left out data.","870",":func:`~sklearn.model_selection.permutation_test_score` generates a null","871","distribution by calculating `n_permutations` different permutations of the","872","data. In each permutation the labels are randomly shuffled, thereby removing","873","any dependency between the features and the labels. The p-value output","874","is the fraction of permutations for which the average cross-validation score","875","obtained by the model is better than the cross-validation score obtained by","876","the model using the original data. For reliable results ``n_permutations``","877","should typically be larger than 100 and ``cv`` between 3-10 folds.","878","","879","A low p-value provides evidence that the dataset contains real dependency","880","between features and labels and the classifier was able to utilize this","881","to obtain good results. A high p-value could be due to a lack of dependency","882","between features and labels (there is no difference in feature values between","883","the classes) or because the classifier was not able to use the dependency in","884","the data. In the latter case, using a more appropriate classifier that","885","is able to utilize the structure in the data, would result in a low","886","p-value.","887","","888","Cross-validation provides information about how well a classifier generalizes,","889","specifically the range of expected errors of the classifier. However, a","890","classifier trained on a high dimensional dataset with no structure may still","891","perform better than expected on cross-validation, just by chance.","892","This can typically happen with small datasets with less than a few hundred","893","samples.","894",":func:`~sklearn.model_selection.permutation_test_score` provides information","895","on whether the classifier has found a real class structure and can help in","896","evaluating the performance of the classifier.","897","","898","It is important to note that this test has been shown to produce low","899","p-values even if there is only weak structure in the data because in the","900","corresponding permutated datasets there is absolutely no structure. This","901","test is therefore only able to show when the model reliably outperforms","902","random guessing.","903","","904","Finally, :func:`~sklearn.model_selection.permutation_test_score` is computed","905","using brute force and interally fits ``(n_permutations + 1) * n_cv`` models.","906","It is therefore only tractable with small datasets for which fitting an","907","individual model is very fast.","908","","909",".. topic:: Examples","910","","911","    * :ref:`sphx_glr_auto_examples_feature_selection_plot_permutation_test_for_classification.py`","912","","913",".. topic:: References:","914","","915"," * Ojala and Garriga. `Permutation Tests for Studying Classifier Performance","916","   <http:\/\/www.jmlr.org\/papers\/volume11\/ojala10a\/ojala10a.pdf>`_.","917","   J. Mach. Learn. Res. 2010."],"delete":[]}]}},"324d3b59ec59f0d5415d561de28c40e4c054941e":{"changes":{"sklearn\/ensemble\/_stacking.py":"MODIFY","sklearn\/ensemble\/_weight_boosting.py":"MODIFY"},"diff":{"sklearn\/ensemble\/_stacking.py":[{"add":["270","        The default classifier is a","271","        :class:`~sklearn.linear_model.LogisticRegression`.","284","        either binary or multiclass,","285","        :class:`~sklearn.model_selection.StratifiedKFold` is used.","286","        In all other cases, :class:`~sklearn.model_selection.KFold` is used.","544","        The default regressor is a :class:`~sklearn.linear_model.RidgeCV`.","557","        either binary or multiclass,","558","        :class:`~sklearn.model_selection.StratifiedKFold` is used.","559","        In all other cases, :class:`~sklearn.model_selection.KFold` is used."],"delete":["270","        The default classifier is a `LogisticRegression`.","283","        either binary or multiclass, `StratifiedKFold` is used. In all other","284","        cases, `KFold` is used.","542","        The default regressor is a `RidgeCV`.","555","        either binary or multiclass, `StratifiedKFold` is used. In all other","556","        cases, `KFold` is used."]}],"sklearn\/ensemble\/_weight_boosting.py":[{"add":["7","- The `BaseWeightBoosting` base class implements a common ``fit`` method","11","- :class:`~sklearn.ensemble.AdaBoostClassifier` implements adaptive boosting","12","  (AdaBoost-SAMME) for classification problems.","14","- :class:`~sklearn.ensemble.AdaBoostRegressor` implements adaptive boosting","15","  (AdaBoost.R2) for regression problems.","305","        the base estimator is :class:`~sklearn.tree.DecisionTreeClassifier`","306","        initialized with `max_depth=1`.","891","        :class:`~sklearn.tree.DecisionTreeRegressor` initialized with","892","        `max_depth=3`."],"delete":["7","- The ``BaseWeightBoosting`` base class implements a common ``fit`` method","11","- ``AdaBoostClassifier`` implements adaptive boosting (AdaBoost-SAMME) for","12","  classification problems.","14","- ``AdaBoostRegressor`` implements adaptive boosting (AdaBoost.R2) for","15","  regression problems.","305","        the base estimator is ``DecisionTreeClassifier(max_depth=1)``.","890","        ``DecisionTreeRegressor(max_depth=3)``."]}]}},"c115ed715a32a628c77440253a6e3af502e1098d":{"changes":{"sklearn\/manifold\/_locally_linear.py":"MODIFY","sklearn\/manifold\/_isomap.py":"MODIFY"},"diff":{"sklearn\/manifold\/_locally_linear.py":[{"add":["32","    reg : float, default=1e-3","79","    reg : float, default=1e-3","84","    n_jobs : int or None, default=None","120","    k : int","123","    k_skip : int, default=1","126","    eigen_solver : {'auto', 'arpack', 'dense'}, default='arpack'","138","    tol : float, default=1e-6","142","    max_iter : int, default=100","201","    n_neighbors : int","204","    n_components : int","207","    reg : float, default=1e-3","211","    eigen_solver : {'auto', 'arpack', 'dense'}, default='auto'","225","    tol : float, default=1e-6","229","    max_iter : int, default=100","232","    method : {'standard', 'hessian', 'modified', 'ltsa'}, default='standard'","243","    hessian_tol : float, default=1e-4","247","    modified_tol : float, default=1e-12","256","    n_jobs : int or None, default=None","527","    n_neighbors : int, default=5","530","    n_components : int, default=2","533","    reg : float, default=1e-3","537","    eigen_solver : {'auto', 'arpack', 'dense'}, default='auto'","551","    tol : float, default=1e-6","555","    max_iter : int, default=100","559","    method : {'standard', 'hessian', 'modified', 'ltsa'}, default='standard'","570","    hessian_tol : float, default=1e-4","574","    modified_tol : float, default=1e-12","578","    neighbors_algorithm : {'auto', 'brute', 'kd_tree', 'ball_tree'}, \\","579","                          default='auto'","588","    n_jobs : int or None, default=None"],"delete":["32","    reg : float, optional","79","    reg : float, optional","84","    n_jobs : int or None, optional (default=None)","120","    k : integer","123","    k_skip : integer, optional","126","    eigen_solver : string, {'auto', 'arpack', 'dense'}","138","    tol : float, optional","142","    max_iter : int","201","    n_neighbors : integer","204","    n_components : integer","207","    reg : float","211","    eigen_solver : string, {'auto', 'arpack', 'dense'}","225","    tol : float, optional","229","    max_iter : integer","232","    method : {'standard', 'hessian', 'modified', 'ltsa'}","243","    hessian_tol : float, optional","247","    modified_tol : float, optional","256","    n_jobs : int or None, optional (default=None)","527","    n_neighbors : integer","530","    n_components : integer","533","    reg : float","537","    eigen_solver : string, {'auto', 'arpack', 'dense'}","551","    tol : float, optional","555","    max_iter : integer","559","    method : string ('standard', 'hessian', 'modified' or 'ltsa')","570","    hessian_tol : float, optional","574","    modified_tol : float, optional","578","    neighbors_algorithm : string ['auto'|'brute'|'kd_tree'|'ball_tree']","587","    n_jobs : int or None, optional (default=None)"]}],"sklearn\/manifold\/_isomap.py":[{"add":["24","    n_neighbors : int, default=5","27","    n_components : int, default=2","30","    eigen_solver : {'auto', 'arpack', 'dense'}, default='auto'","40","    tol : float, default=0","44","    max_iter : int, default=None","48","    path_method : {'auto', 'FW', 'D'}, default='auto'","57","    neighbors_algorithm : {'auto', 'brute', 'kd_tree', 'ball_tree'}, \\","58","                          default='auto'"],"delete":["24","    n_neighbors : integer","27","    n_components : integer","30","    eigen_solver : ['auto'|'arpack'|'dense']","40","    tol : float","44","    max_iter : integer","48","    path_method : string ['auto'|'FW'|'D']","57","    neighbors_algorithm : string ['auto'|'brute'|'kd_tree'|'ball_tree']"]}]}},"afeae82ec419ff992d528a7af563361173cafec1":{"changes":{"build_tools\/azure\/posix.yml":"MODIFY",".travis.yml":"MODIFY","sklearn\/conftest.py":"MODIFY","build_tools\/azure\/posix-32.yml":"MODIFY"},"diff":{"build_tools\/azure\/posix.yml":[{"add":["19","    OMP_NUM_THREADS: '2'","20","    OPENBLAS_NUM_THREADS: '2'"],"delete":["19","    OMP_NUM_THREADS: '4'","20","    OPENBLAS_NUM_THREADS: '4'"]}],".travis.yml":[{"add":["15","    - OMP_NUM_THREADS=2","16","    - OPENBLAS_NUM_THREADS=2"],"delete":["15","    - OMP_NUM_THREADS=4","16","    - OPENBLAS_NUM_THREADS=4"]}],"sklearn\/conftest.py":[{"add":["0","import os","1","","3","from threadpoolctl import threadpool_limits","4","","5","from sklearn.utils._openmp_helpers import _openmp_effective_n_threads","26","","27","","28","def pytest_runtest_setup(item):","29","    \"\"\"Set the number of openmp threads based on the number of workers","30","    xdist is using to prevent oversubscription.","31","","32","    Parameters","33","    ----------","34","    item : pytest item","35","        item to be processed","36","    \"\"\"","37","    try:","38","        xdist_worker_count = int(os.environ['PYTEST_XDIST_WORKER_COUNT'])","39","    except KeyError:","40","        # raises when pytest-xdist is not installed","41","        return","42","","43","    openmp_threads = _openmp_effective_n_threads()","44","    threads_per_worker = max(openmp_threads \/\/ xdist_worker_count, 1)","45","    threadpool_limits(threads_per_worker, user_api='openmp')"],"delete":[]}],"build_tools\/azure\/posix-32.yml":[{"add":["16","    OMP_NUM_THREADS: '2'","19","    OPENBLAS_NUM_THREADS: '2'"],"delete":["16","    OMP_NUM_THREADS: '4'","19","    OPENBLAS_NUM_THREADS: '4'"]}]}},"8cce5bfcf0f001d91deca145f878f474a0b8e894":{"changes":{"doc\/whats_new\/v0.24.rst":"MODIFY"},"diff":{"doc\/whats_new\/v0.24.rst":[{"add":["54",":mod:`sklearn.feature_selection`","55","................................","56","","57","- |Feature| A new parameter `importance_getter` was added to","58","  :class:`feature_selection.RFE`, :class:`feature_selection.RFECV` and","59","  :class:`feature_selection.SelectFromModel`, allowing the user to specify an","60","  attribute name\/path or a `callable` for extracting feature importance from","61","  the estimator.  :pr:`15361` by :user:`Venkatachalam N <venkyyuvy>`","62","","80","- |Enhancement| Add `sample_weight` parameter to","81","  :class:`metrics.median_absolute_error`. :pr:`17225` by","82","  :user:`Lucy Liu <lucyleeow>`.","83",""],"delete":["80","- |Feature| A new parameter `importance_getter` was added to","81","  :class:`feature_selection.RFE`, :class:`feature_selection.RFECV` and","82","  :class:`feature_selection.SelectFromModel`, allowing the user to specify an","83","  attribute name\/path or a `callable` for extracting feature importance from","84","  the estimator.  :pr:`15361` by :user:`Venkatachalam N <venkyyuvy>`","85","","86",":mod:`sklearn.metrics`","87","......................","88","","89","- |Enhancement| Add `sample_weight` parameter to","90","  :class:`metrics.median_absolute_error`.","91","  :pr:`17225` by :user:`Lucy Liu <lucyleeow>`.","92",""]}]}},"d6ed0d0cae852e59db2d81239f59b34434762c6c":{"changes":{"sklearn\/multiclass.py":"MODIFY"},"diff":{"sklearn\/multiclass.py":[{"add":["162","        The number of jobs to use for the computation: the `n_classes`","163","        one-vs-rest problems are computed in parallel.","164","","508","        The number of jobs to use for the computation: the `n_classes * (","509","        n_classes - 1) \/ 2` OVO problems are computed in parallel.","510","","737","        The number of jobs to use for the computation: the multiclass problems","738","        are computed in parallel.","739",""],"delete":["162","        The number of jobs to use for the computation.","506","        The number of jobs to use for the computation.","733","        The number of jobs to use for the computation."]}]}},"58451568d3b44ba632d708add02f9c30f356570a":{"changes":{"sklearn\/decomposition\/tests\/test_truncated_svd.py":"MODIFY","sklearn\/decomposition\/_truncated_svd.py":"MODIFY","doc\/whats_new\/v0.24.rst":"MODIFY"},"diff":{"sklearn\/decomposition\/tests\/test_truncated_svd.py":[{"add":["193","","194","","195","@pytest.mark.parametrize(\"algorithm, tol\", [","196","    ('randomized', 0.), ('arpack', 1e-6), ('arpack', 0.)])","197","@pytest.mark.parametrize('kind', ('dense', 'sparse'))","198","def test_fit_transform(X_sparse, algorithm, tol, kind):","199","    # fit_transform(X) should equal fit(X).transform(X)","200","    X = X_sparse if kind == 'sparse' else X_sparse.toarray()","201","    svd = TruncatedSVD(n_components=5, n_iter=7, random_state=42,","202","                       algorithm=algorithm, tol=tol)","203","    X_transformed_1 = svd.fit_transform(X)","204","    X_transformed_2 = svd.fit(X).transform(X)","205","    assert_allclose(X_transformed_1, X_transformed_2)"],"delete":[]}],"sklearn\/decomposition\/_truncated_svd.py":[{"add":["130","        \"\"\"Fit model on training data X.","148","        \"\"\"Fit model to X and perform dimensionality reduction on X.","187","        # As a result of the SVD approximation error on X ~ U @ Sigma @ V.T,","188","        # X @ V is not the same as U @ Sigma","189","        if self.algorithm == \"randomized\" or \\","190","                (self.algorithm == \"arpack\" and self.tol > 0):","191","            X_transformed = safe_sparse_dot(X, self.components_.T)","192","        else:","193","            X_transformed = U * Sigma","194",""],"delete":["130","        \"\"\"Fit LSI model on training data X.","148","        \"\"\"Fit LSI model to X and perform dimensionality reduction on X.","188","        X_transformed = U * Sigma"]}],"doc\/whats_new\/v0.24.rst":[{"add":["182","- |Fix| :meth:`TruncatedSVD.fit_transform` consistently returns the same","183","  as :meth:`TruncatedSVD.fit` followed by :meth:`TruncatedSVD.transform`.","184","  :pr:`18528` by :user:`Albert Villanova del Moral <albertvillanova>` and","185","  :user:`Ruifeng Zheng <zhengruifeng>`.","186",""],"delete":[]}]}},"0ba19ad2d8943c7fb98fdefd523640c441bdb81c":{"changes":{"doc\/sphinxext\/github_link.py":"MODIFY"},"diff":{"doc\/sphinxext\/github_link.py":[{"add":["45","    # Unwrap the object to get the correct source","46","    # file in case that is wrapped by a decorator","47","    obj = inspect.unwrap(obj)","48",""],"delete":["42","    if type(class_name) != str:","43","        # Python 2 only","44","        class_name = class_name.encode('utf-8')"]}]}},"c29092d6994a43ea19a82e91c0331e8b7d6e7d36":{"changes":{"examples\/decomposition\/plot_sparse_coding.py":"MODIFY","sklearn\/linear_model\/tests\/test_coordinate_descent.py":"MODIFY","sklearn\/utils\/fixes.py":"MODIFY","examples\/cluster\/plot_coin_segmentation.py":"MODIFY","sklearn\/ensemble\/tests\/test_forest.py":"MODIFY","sklearn\/neighbors\/_base.py":"MODIFY","examples\/miscellaneous\/plot_johnson_lindenstrauss_bound.py":"MODIFY","examples\/cluster\/plot_coin_ward_segmentation.py":"MODIFY","sklearn\/linear_model\/tests\/test_base.py":"MODIFY","sklearn\/neighbors\/tests\/test_dist_metrics.py":"MODIFY","sklearn\/utils\/__init__.py":"MODIFY","examples\/compose\/plot_transformed_target.py":"MODIFY","sklearn\/utils\/tests\/test_estimator_checks.py":"MODIFY","sklearn\/datasets\/_lfw.py":"MODIFY","sklearn\/tests\/test_pipeline.py":"MODIFY","sklearn\/linear_model\/tests\/test_least_angle.py":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_loss.py":"MODIFY","sklearn\/utils\/validation.py":"MODIFY","sklearn\/linear_model\/tests\/test_sgd.py":"MODIFY","sklearn\/utils\/tests\/test_multiclass.py":"MODIFY","examples\/neighbors\/plot_kde_1d.py":"MODIFY","conftest.py":"MODIFY"},"diff":{"examples\/decomposition\/plot_sparse_coding.py":[{"add":["22","from sklearn.utils.fixes import np_version, parse_version","69","lstsq_rcond = None if np_version >= parse_version('1.14') else -1"],"delete":["18","from distutils.version import LooseVersion","19","","70","lstsq_rcond = None if LooseVersion(np.__version__) >= '1.14' else -1"]}],"sklearn\/linear_model\/tests\/test_coordinate_descent.py":[{"add":["24","from sklearn.utils.fixes import parse_version","1036","    if (parse_version(joblib.__version__) < parse_version('0.12')","1037","            and backend == 'loky'):"],"delete":["10","from distutils.version import LooseVersion","1036","    if joblib.__version__ < LooseVersion('0.12') and backend == 'loky':"]}],"sklearn\/utils\/fixes.py":[{"add":["23","try:","24","    from pkg_resources import parse_version  # type: ignore","25","except ImportError:","26","    # setuptools not installed","27","    parse_version = LooseVersion  # type: ignore","30","np_version = parse_version(np.__version__)","31","sp_version = parse_version(scipy.__version__)","34","if sp_version >= parse_version('1.4'):","38","    # once support for sp_version < parse_version('1.4') is dropped","53","    if sp_version >= parse_version('1.1') or not sp.issparse(X):","82","    if parse_version(joblib.__version__) >= parse_version('0.12'):","167","    if np_version > parse_version('1.14'):"],"delete":["23","","24","def _parse_version(version_string):","25","    version = []","26","    for x in version_string.split('.'):","27","        try:","28","            version.append(int(x))","29","        except ValueError:","30","            # x may be of the form dev-1ea1592","31","            version.append(x)","32","    return tuple(version)","35","np_version = _parse_version(np.__version__)","36","sp_version = _parse_version(scipy.__version__)","39","if sp_version >= (1, 4):","43","    # once support for sp_version < (1, 4) is dropped","58","    if sp_version >= (1, 1) or not sp.issparse(X):","87","    if joblib.__version__ >= LooseVersion('0.12'):","172","    if np_version > (1, 14):"]}],"examples\/cluster\/plot_coin_segmentation.py":[{"add":["35","from sklearn.utils.fixes import parse_version","38","if parse_version(skimage.__version__) >= parse_version('0.14'):"],"delete":["27","from distutils.version import LooseVersion","38","if LooseVersion(skimage.__version__) >= '0.14':"]}],"sklearn\/ensemble\/tests\/test_forest.py":[{"add":["36","from sklearn.utils.fixes import parse_version","1276","@pytest.mark.skipif(parse_version(joblib.__version__) < parse_version('0.12'),"],"delete":["13","from distutils.version import LooseVersion","1276","@pytest.mark.skipif(joblib.__version__ < LooseVersion('0.12'),"]}],"sklearn\/neighbors\/_base.py":[{"add":["29","from ..utils.fixes import parse_version","652","                    parse_version(joblib.__version__) < parse_version('0.12'))","959","            if parse_version(joblib.__version__) < parse_version('0.12'):"],"delete":["9","from distutils.version import LooseVersion","652","                    LooseVersion(joblib.__version__) < LooseVersion('0.12'))","959","            if LooseVersion(joblib.__version__) < LooseVersion('0.12'):"]}],"examples\/miscellaneous\/plot_johnson_lindenstrauss_bound.py":[{"add":["26","from sklearn.utils.fixes import parse_version","29","if parse_version(matplotlib.__version__) >= parse_version('2.1'):"],"delete":["21","from distutils.version import LooseVersion","29","if LooseVersion(matplotlib.__version__) >= '2.1':"]}],"examples\/cluster\/plot_coin_ward_segmentation.py":[{"add":["29","from sklearn.utils.fixes import parse_version","32","if parse_version(skimage.__version__) >= parse_version('0.14'):"],"delete":["19","from distutils.version import LooseVersion","32","if LooseVersion(skimage.__version__) >= '0.14':"]}],"sklearn\/linear_model\/tests\/test_base.py":[{"add":["15","from sklearn.utils.fixes import parse_version","211","    if parse_version(pd.__version__) < parse_version('0.24.0'):"],"delete":["7","from distutils.version import LooseVersion","8","","212","    if LooseVersion(pd.__version__) < '0.24.0':"]}],"sklearn\/neighbors\/tests\/test_dist_metrics.py":[{"add":["13","from sklearn.utils.fixes import sp_version, parse_version","107","    if metric == 'jaccard' and sp_version < parse_version('1.2.0'):"],"delete":["8","from distutils.version import LooseVersion","9","from scipy import __version__ as scipy_version","108","    if metric == 'jaccard' and LooseVersion(scipy_version) < '1.2.0':"]}],"sklearn\/utils\/__init__.py":[{"add":["26","from .fixes import np_version, parse_version","173","    if np_version < parse_version('1.12') or issparse(array):"],"delete":["26","from .fixes import np_version","173","    if np_version < (1, 12) or issparse(array):"]}],"examples\/compose\/plot_transformed_target.py":[{"add":["27","from sklearn.utils.fixes import parse_version","34","if parse_version(matplotlib.__version__) >= parse_version('2.1'):"],"delete":["21","from distutils.version import LooseVersion","34","if LooseVersion(matplotlib.__version__) >= '2.1':"]}],"sklearn\/utils\/tests\/test_estimator_checks.py":[{"add":["27","from sklearn.utils.fixes import np_version, parse_version","337","    if np_version < parse_version('1.17'):"],"delete":["27","from sklearn.utils.fixes import _parse_version","337","    np_version = _parse_version(np.__version__)","338","    if np_version < (1, 17):"]}],"sklearn\/datasets\/_lfw.py":[{"add":["22","from ..utils.fixes import parse_version","306","    if parse_version(joblib.__version__) < parse_version('0.12'):","479","    if parse_version(joblib.__version__) < parse_version('0.12'):"],"delete":["14","from distutils.version import LooseVersion","306","    if LooseVersion(joblib.__version__) < LooseVersion('0.12'):","479","    if LooseVersion(joblib.__version__) < LooseVersion('0.12'):"]}],"sklearn\/tests\/test_pipeline.py":[{"add":["21","from sklearn.utils.fixes import parse_version","1022","        if parse_version(joblib.__version__) < parse_version('0.12'):","1084","    if parse_version(joblib.__version__) < parse_version('0.12'):"],"delete":["3","from distutils.version import LooseVersion","1022","        if LooseVersion(joblib.__version__) < LooseVersion('0.12'):","1084","    if LooseVersion(joblib.__version__) < LooseVersion('0.12'):"]}],"sklearn\/linear_model\/tests\/test_least_angle.py":[{"add":["14","from sklearn.utils.fixes import np_version, parse_version","124","    rcond = None if np_version >= parse_version('1.14') else -1"],"delete":["2","from distutils.version import LooseVersion","3","","125","    rcond = None if LooseVersion(np.__version__) >= '1.14' else -1"]}],"sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_loss.py":[{"add":["6","from sklearn.utils.fixes import sp_version, parse_version","62","@pytest.mark.skipif(sp_version == parse_version('1.2.0'),"],"delete":["6","from sklearn.utils.fixes import sp_version","62","@pytest.mark.skipif(sp_version == (1, 2, 0),"]}],"sklearn\/utils\/validation.py":[{"add":["25","from .fixes import _object_dtype_isnan, parse_version","230","        if parse_version(joblib.__version__) < parse_version('0.12'):"],"delete":["17","from distutils.version import LooseVersion","26","from .fixes import _object_dtype_isnan","231","        if LooseVersion(joblib.__version__) < '0.12':"]}],"sklearn\/linear_model\/tests\/test_sgd.py":[{"add":["14","from sklearn.utils.fixes import parse_version","1596","    if (parse_version(joblib.__version__) < parse_version('0.12')","1597","            and backend == 'loky'):"],"delete":["0","from distutils.version import LooseVersion","1596","    if joblib.__version__ < LooseVersion('0.12') and backend == 'loky':"]}],"sklearn\/utils\/tests\/test_multiclass.py":[{"add":["17","from sklearn.utils.fixes import parse_version","309","    if parse_version(pd.__version__) >= parse_version('0.25'):"],"delete":["5","from distutils.version import LooseVersion","309","    if LooseVersion(pd.__version__) >= '0.25':"]}],"examples\/neighbors\/plot_kde_1d.py":[{"add":["35","from sklearn.utils.fixes import parse_version","38","if parse_version(matplotlib.__version__) >= parse_version('2.1'):"],"delete":["33","from distutils.version import LooseVersion","38","if LooseVersion(matplotlib.__version__) >= '2.1':"]}],"conftest.py":[{"add":["16","from sklearn.utils.fixes import np_version, parse_version","20","if parse_version(pytest.__version__) < parse_version(PYTEST_MIN_VERSION):","54","        if np_version < parse_version('1.14'):"],"delete":["9","from distutils.version import LooseVersion","20","if LooseVersion(pytest.__version__) < PYTEST_MIN_VERSION:","54","        import numpy as np","55","        if LooseVersion(np.__version__) < LooseVersion('1.14'):"]}]}},"0550793bd61b84beb60d3a92c3eb90cc788a27a8":{"changes":{"sklearn\/gaussian_process\/kernels.py":"MODIFY","examples\/gaussian_process\/plot_gpr_co2.py":"MODIFY","sklearn\/gaussian_process\/tests\/test_gpr.py":"MODIFY"},"diff":{"sklearn\/gaussian_process\/kernels.py":[{"add":["397","            if hyp.fixed:","398","                continue"],"delete":[]}],"examples\/gaussian_process\/plot_gpr_co2.py":[{"add":["131","                  noise_level_bounds=(1e-5, np.inf))  # noise terms"],"delete":["131","                  noise_level_bounds=(1e-3, np.inf))  # noise terms"]}],"sklearn\/gaussian_process\/tests\/test_gpr.py":[{"add":["16","from sklearn.gaussian_process.kernels import DotProduct, ExpSineSquared","527","","528","","529","def test_bound_check_fixed_hyperparameter():","530","    # Regression test for issue #17943","531","    # Check that having a hyperparameter with fixed bounds doesn't cause an","532","    # error","533","    k1 = 50.0**2 * RBF(length_scale=50.0)  # long term smooth rising trend","534","    k2 = ExpSineSquared(length_scale=1.0, periodicity=1.0,","535","                        periodicity_bounds=\"fixed\")  # seasonal component","536","    kernel = k1 + k2","537","    GaussianProcessRegressor(kernel=kernel).fit(X, y)"],"delete":["16","from sklearn.gaussian_process.kernels import DotProduct"]}]}},"41b18fea37e59a84bb9219c32c15c36432afc9ae":{"changes":{"doc\/whats_new\/v0.23.rst":"MODIFY","sklearn\/decomposition\/_sparse_pca.py":"MODIFY","sklearn\/decomposition\/tests\/test_sparse_pca.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.23.rst":[{"add":["217","- |Feature| Added `n_components_` attribute to :class:'decomposition.SparsePCA'","218","  and :class:'MiniBatchSparsePCA'. :pr:'16981' by :user:'Mateusz G¨®rski <Reksbril>'","219",""],"delete":[]}],"sklearn\/decomposition\/_sparse_pca.py":[{"add":["105","    n_components_ : int","106","        Estimated number of components.","107","","108","        .. versionadded:: 0.23","109","","204","        self.n_components_ = len(self.components_)","320","    n_components_ : int","321","        Estimated number of components.","322","","323","        .. versionadded:: 0.23","324","","416","        self.n_components_ = len(self.components_)"],"delete":[]}],"sklearn\/decomposition\/tests\/test_sparse_pca.py":[{"add":["209","","210","","211","@pytest.mark.parametrize(\"SPCA\", [SparsePCA, MiniBatchSparsePCA])","212","@pytest.mark.parametrize(\"n_components\", [None, 3])","213","def test_spca_n_components_(SPCA, n_components):","214","    rng = np.random.RandomState(0)","215","    n_samples, n_features = 12, 10","216","    X = rng.randn(n_samples, n_features)","217","","218","    model = SPCA(n_components=n_components).fit(X)","219","","220","    if n_components is not None:","221","        assert model.n_components_ == n_components","222","    else:","223","        assert model.n_components_ == n_features"],"delete":[]}]}}}