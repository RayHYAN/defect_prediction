{"94a9f9a0b04703ae98d61a7e9a98a4dcaac548e8":{"changes":{"doc\/developers\/advanced_installation.rst":"MODIFY","build_tools\/azure\/install.sh":"MODIFY"},"diff":{"doc\/developers\/advanced_installation.rst":[{"add":["243","        \"conda-forge::compilers>=1.0.4,!=1.1.0\" conda-forge::llvm-openmp"],"delete":["243","        \"conda-forge::compilers>=1.0.4\" conda-forge::llvm-openmp"]}],"build_tools\/azure\/install.sh":[{"add":["40","            # TODO: Remove !=1.1.0 when the following is fixed:","41","            # sklearn\/svm\/_libsvm.cpython-38-darwin.so,","42","            # 2): Symbol not found: _svm_check_parameter error","43","            TO_INSTALL=\"$TO_INSTALL conda-forge::compilers>=1.0.4,!=1.1.0 \\"],"delete":["40","            TO_INSTALL=\"$TO_INSTALL conda-forge::compilers>=1.0.4 \\"]}]}},"d1f0d1917de1e6ecbefdf4173907cc03a4bfd4a8":{"changes":{"doc\/whats_new\/v0.23.rst":"MODIFY","sklearn\/ensemble\/_stacking.py":"MODIFY","sklearn\/ensemble\/tests\/test_stacking.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.23.rst":[{"add":["4",".. _changes_0_23_2:","5","","6","Version 0.23.2","7","==============","8","","9","Changelog","10","---------","11","","12",":mod:`sklearn.ensemble`","13",".......................","14","","15","- |Fix| Fixes :class:`ensemble.StackingClassifier` and","16","  :class:`ensemble.StackingRegressor` compatibility with estimators that","17","  do not define `n_features_in_`. :pr:`17357` by `Thomas Fan`_.","18",""],"delete":[]}],"sklearn\/ensemble\/_stacking.py":[{"add":["15","from ..exceptions import NotFittedError","199","    @property","200","    def n_features_in_(self):","201","        \"\"\"Number of features seen during :term:`fit`.\"\"\"","202","        try:","203","            check_is_fitted(self)","204","        except NotFittedError as nfe:","205","            raise AttributeError(","206","                f\"{self.__class__.__name__} object has no attribute \"","207","                f\"n_features_in_\") from nfe","208","        return self.estimators_[0].n_features_in_","209",""],"delete":["148","        self.n_features_in_ = self.estimators_[0].n_features_in_"]}],"sklearn\/ensemble\/tests\/test_stacking.py":[{"add":["19","from sklearn.datasets import make_regression","20","from sklearn.datasets import make_classification","495","","496","","497","@pytest.mark.parametrize(\"make_dataset, Stacking, Estimator\", [","498","    (make_classification, StackingClassifier, LogisticRegression),","499","    (make_regression, StackingRegressor, LinearRegression)","500","])","501","def test_stacking_without_n_features_in(make_dataset, Stacking, Estimator):","502","    # Stacking supports estimators without `n_features_in_`. Regression test","503","    # for #17353","504","","505","    class MyEstimator(Estimator):","506","        \"\"\"Estimator without n_features_in_\"\"\"","507","        def fit(self, X, y):","508","            super().fit(X, y)","509","            del self.n_features_in_","510","","511","    X, y = make_dataset(random_state=0, n_samples=100)","512","    stacker = Stacking(estimators=[('lr', MyEstimator())])","513","","514","    msg = f\"{Stacking.__name__} object has no attribute n_features_in_\"","515","    with pytest.raises(AttributeError, match=msg):","516","        stacker.n_features_in_","517","","518","    # Does not raise","519","    stacker.fit(X, y)","520","","521","    msg = \"'MyEstimator' object has no attribute 'n_features_in_'\"","522","    with pytest.raises(AttributeError, match=msg):","523","        stacker.n_features_in_"],"delete":[]}]}},"9fc7bceca6f6c61d8d09f1e82b40d268972ec2d4":{"changes":{"sklearn\/linear_model\/tests\/test_least_angle.py":"MODIFY","doc\/whats_new\/v0.23.rst":"MODIFY","sklearn\/linear_model\/_least_angle.py":"MODIFY"},"diff":{"sklearn\/linear_model\/tests\/test_least_angle.py":[{"add":["762","","763","","764","def test_copy_X_with_auto_gram():","765","    # Non-regression test for #17789, `copy_X=True` and Gram='auto' does not","766","    # overwrite X","767","    rng = np.random.RandomState(42)","768","    X = rng.rand(6, 6)","769","    y = rng.rand(6)","770","","771","    X_before = X.copy()","772","    linear_model.lars_path(X, y, Gram='auto', copy_X=True, method='lasso')","773","    # X did not change","774","    assert_allclose(X, X_before)"],"delete":[]}],"doc\/whats_new\/v0.23.rst":[{"add":["19",":mod:`sklearn.linear_model`","20","...........................","21","","22","- |Fix| :func:`linear_model.lars_path` does not overwrite `X` when","23","  `X_copy=True` and `Gram='auto'`. :pr:`17914` by `Thomas Fan`_.","24",""],"delete":[]}],"sklearn\/linear_model\/_least_angle.py":[{"add":["428","","429","    if copy_X and X is not None and Gram is None:","430","        # force copy. setting the array to be fortran-ordered","431","        # speeds up the calculation of the (partial) Gram matrix","432","        # and allows to easily swap columns","433","        X = X.copy('F')","434",""],"delete":["413","        if copy_X:","414","            # force copy. setting the array to be fortran-ordered","415","            # speeds up the calculation of the (partial) Gram matrix","416","            # and allows to easily swap columns","417","            X = X.copy('F')","418",""]}]}},"8ab0064579a20b175400b05a33ededaa4f57d062":{"changes":{"sklearn\/neighbors\/_quad_tree.pyx":"MODIFY","sklearn\/tree\/_tree.pyx":"MODIFY"},"diff":{"sklearn\/neighbors\/_quad_tree.pyx":[{"add":["30","    int PyArray_SetBaseObject(np.ndarray arr, PyObject* obj)","576","        if PyArray_SetBaseObject(arr, <PyObject*> self) < 0:","577","            raise ValueError(\"Can't intialize array!\")"],"delete":["575","        arr.base = <PyObject*> self"]}],"sklearn\/tree\/_tree.pyx":[{"add":["46","    int PyArray_SetBaseObject(np.ndarray arr, PyObject* obj)","1100","        if PyArray_SetBaseObject(arr, <PyObject*> self) < 0:","1101","            raise ValueError(\"Can't initialize array.\")","1122","        if PyArray_SetBaseObject(arr, <PyObject*> self) < 0:","1123","            raise ValueError(\"Can't initialize array.\")"],"delete":["1099","        arr.base = <PyObject*> self","1120","        arr.base = <PyObject*> self"]}]}},"c11aaf5a66452217c4c9ac2811e34637979d7c85":{"changes":{"sklearn\/neighbors\/_quad_tree.pyx":"MODIFY","sklearn\/tree\/_tree.pyx":"MODIFY"},"diff":{"sklearn\/neighbors\/_quad_tree.pyx":[{"add":["32","# Build the corresponding numpy dtype for Cell.","33","# This works by casting `dummy` to an array of Cell of length 1, which numpy","34","# can construct a `dtype`-object for. See https:\/\/stackoverflow.com\/q\/62448946","35","# for a more detailed explanation.","36","cdef Cell dummy;","37","CELL_DTYPE = np.asarray(<Cell[:1]>(&dummy)).dtype"],"delete":["32","","33","# Repeat struct definition for numpy","34","CELL_DTYPE = np.dtype({","35","    'names': ['parent', 'children', 'cell_id', 'point_index', 'is_leaf',","36","              'max_width', 'depth', 'cumulative_size', 'center', 'barycenter',","37","              'min_bounds', 'max_bounds'],","38","    'formats': [np.intp, (np.intp, 8), np.intp, np.intp, np.int32, np.float32,","39","                np.intp, np.intp, (np.float32, 3), (np.float32, 3),","40","                (np.float32, 3), (np.float32, 3)],","41","    'offsets': [","42","        <Py_ssize_t> &(<Cell*> NULL).parent,","43","        <Py_ssize_t> &(<Cell*> NULL).children,","44","        <Py_ssize_t> &(<Cell*> NULL).cell_id,","45","        <Py_ssize_t> &(<Cell*> NULL).point_index,","46","        <Py_ssize_t> &(<Cell*> NULL).is_leaf,","47","        <Py_ssize_t> &(<Cell*> NULL).squared_max_width,","48","        <Py_ssize_t> &(<Cell*> NULL).depth,","49","        <Py_ssize_t> &(<Cell*> NULL).cumulative_size,","50","        <Py_ssize_t> &(<Cell*> NULL).center,","51","        <Py_ssize_t> &(<Cell*> NULL).barycenter,","52","        <Py_ssize_t> &(<Cell*> NULL).min_bounds,","53","        <Py_ssize_t> &(<Cell*> NULL).max_bounds,","54","    ]","55","})"]}],"sklearn\/tree\/_tree.pyx":[{"add":["70","# Build the corresponding numpy dtype for Node.","71","# This works by casting `dummy` to an array of Node of length 1, which numpy","72","# can construct a `dtype`-object for. See https:\/\/stackoverflow.com\/q\/62448946","73","# for a more detailed explanation.","74","cdef Node dummy;","75","NODE_DTYPE = np.asarray(<Node[:1]>(&dummy)).dtype"],"delete":["70","# Repeat struct definition for numpy","71","NODE_DTYPE = np.dtype({","72","    'names': ['left_child', 'right_child', 'feature', 'threshold', 'impurity',","73","              'n_node_samples', 'weighted_n_node_samples'],","74","    'formats': [np.intp, np.intp, np.intp, np.float64, np.float64, np.intp,","75","                np.float64],","76","    'offsets': [","77","        <Py_ssize_t> &(<Node*> NULL).left_child,","78","        <Py_ssize_t> &(<Node*> NULL).right_child,","79","        <Py_ssize_t> &(<Node*> NULL).feature,","80","        <Py_ssize_t> &(<Node*> NULL).threshold,","81","        <Py_ssize_t> &(<Node*> NULL).impurity,","82","        <Py_ssize_t> &(<Node*> NULL).n_node_samples,","83","        <Py_ssize_t> &(<Node*> NULL).weighted_n_node_samples","84","    ]","85","})"]}]}},"d8be25f65c1fdf5aadabac745b8ae6cd8920a0ad":{"changes":{"sklearn\/mixture\/_bayesian_mixture.py":"MODIFY","sklearn\/mixture\/_base.py":"MODIFY","sklearn\/mixture\/_gaussian_mixture.py":"MODIFY"},"diff":{"sklearn\/mixture\/_bayesian_mixture.py":[{"add":["25","    dirichlet_concentration : array-like of shape (n_samples,)","42","    degrees_of_freedom : array-like of shape (n_components,)","46","    log_det_precision_chol : array-like of shape (n_components,)","54","    log_wishart_norm : array-like of shape (n_components,)","84","    n_components : int, default=1","100","    tol : float, default=1e-3","105","    reg_covar : float, default=1e-6","109","    max_iter : int, default=100","112","    n_init : int, default=1","116","    init_params : {'kmeans', 'random'}, default='kmeans'","124","    weight_concentration_prior_type : str, default='dirichlet_process'","165","    random_state : int, RandomState instance or None, default=None","173","    warm_start : bool, default=False","179","    verbose : int, default=0","185","    verbose_interval : int, default=10","190","    weights_ : array-like of shape (n_components,)","193","    means_ : array-like of shape (n_components, n_features)","257","    weight_concentration_ : array-like of shape (n_components,)","268","    mean_precision_ : array-like of shape (n_components,)","271","    mean_prior_ : array-like of shape (n_features,)","278","    degrees_of_freedom_ : array-like of shape (n_components,)","350","        X : array-like of shape (n_samples, n_features)","388","        X : array-like of shape (n_samples, n_features)","414","        X : array-like of shape (n_samples, n_features)","432","        X : array-like of shape (n_samples, n_features)","473","        X : array-like of shape (n_samples, n_features)","475","        resp : array-like of shape (n_samples, n_components)","489","        nk : array-like of shape (n_components,)","507","        nk : array-like of shape (n_components,)","509","        xk : array-like of shape (n_components, n_features)","521","        nk : array-like of shape (n_components,)","523","        xk : array-like of shape (n_components, n_features)","546","        X : array-like of shape (n_samples, n_features)","548","        nk : array-like of shape (n_components,)","550","        xk : array-like of shape (n_components, n_features)","552","        sk : array-like of shape (n_components, n_features, n_features)","580","        X : array-like of shape (n_samples, n_features)","582","        nk : array-like of shape (n_components,)","584","        xk : array-like of shape (n_components, n_features)","586","        sk : array-like of shape (n_features, n_features)","610","        X : array-like of shape (n_samples, n_features)","612","        nk : array-like of shape (n_components,)","614","        xk : array-like of shape (n_components, n_features)","616","        sk : array-like of shape (n_components, n_features)","639","        X : array-like of shape (n_samples, n_features)","641","        nk : array-like of shape (n_components,)","643","        xk : array-like of shape (n_components, n_features)","645","        sk : array-like of shape (n_components,)","668","        X : array-like of shape (n_samples, n_features)","670","        log_resp : array-like of shape (n_samples, n_components)","719","        X : array-like of shape (n_samples, n_features)"],"delete":["25","    dirichlet_concentration : array-like, shape (n_samples,)","42","    degrees_of_freedom : array-like, shape (n_components,)","46","    log_det_precision_chol : array-like, shape (n_components,)","54","    log_wishart_norm : array-like, shape (n_components,)","84","    n_components : int, defaults=1.","100","    tol : float, defaults=1e-3.","105","    reg_covar : float, defaults=1e-6.","109","    max_iter : int, defaults=100.","112","    n_init : int, defaults=1.","116","    init_params : {'kmeans', 'random'}, defaults='kmeans'.","124","    weight_concentration_prior_type : str, defaults='dirichlet_process'.","165","    random_state : int, RandomState instance or None, default=None.","173","    warm_start : bool, default=False.","179","    verbose : int, default=0.","185","    verbose_interval : int, default=10.","190","    weights_ : array-like, shape (n_components,)","193","    means_ : array-like, shape (n_components, n_features)","257","    weight_concentration_ : array-like, shape (n_components,)","268","    mean_precision_ : array-like, shape (n_components,)","271","    mean_prior_ : array-like, shape (n_features,)","278","    degrees_of_freedom_ : array-like, shape (n_components,)","350","        X : array-like, shape (n_samples, n_features)","388","        X : array-like, shape (n_samples, n_features)","414","        X : array-like, shape (n_samples, n_features)","432","        X : array-like, shape (n_samples, n_features)","473","        X : array-like, shape (n_samples, n_features)","475","        resp : array-like, shape (n_samples, n_components)","489","        nk : array-like, shape (n_components,)","507","        nk : array-like, shape (n_components,)","509","        xk : array-like, shape (n_components, n_features)","521","        nk : array-like, shape (n_components,)","523","        xk : array-like, shape (n_components, n_features)","546","        X : array-like, shape (n_samples, n_features)","548","        nk : array-like, shape (n_components,)","550","        xk : array-like, shape (n_components, n_features)","552","        sk : array-like, shape (n_components, n_features, n_features)","580","        X : array-like, shape (n_samples, n_features)","582","        nk : array-like, shape (n_components,)","584","        xk : array-like, shape (n_components, n_features)","586","        sk : array-like, shape (n_features, n_features)","610","        X : array-like, shape (n_samples, n_features)","612","        nk : array-like, shape (n_components,)","614","        xk : array-like, shape (n_components, n_features)","616","        sk : array-like, shape (n_components, n_features)","639","        X : array-like, shape (n_samples, n_features)","641","        nk : array-like, shape (n_components,)","643","        xk : array-like, shape (n_components, n_features)","645","        sk : array-like, shape (n_components,)","668","        X : array-like, shape (n_samples, n_features)","670","        log_resp : array-like, shape (n_samples, n_components)","719","        X : array-like, shape (n_samples, n_features)"]}],"sklearn\/mixture\/_base.py":[{"add":["43","    X : array-like of shape (n_samples, n_features)","90","        X : array-like of shape (n_samples, n_features)","127","        X : array-like of shape  (n_samples, n_features)","136","        X : array-like of shape  (n_samples, n_features)","164","        X : array-like of shape  (n_samples, n_features)","166","        resp : array-like of shape (n_samples, n_components)","184","        X : array-like of shape (n_samples, n_features)","210","        X : array-like of shape (n_samples, n_features)","286","        X : array-like of shape (n_samples, n_features)","306","        X : array-like of shape (n_samples, n_features)","308","        log_resp : array-like of shape (n_samples, n_components)","327","        X : array-like of shape (n_samples, n_features)","346","        X : array-like of shape (n_samples, n_dimensions)","362","        X : array-like of shape (n_samples, n_features)","380","        X : array-like of shape (n_samples, n_features)","400","        n_samples : int, default=1","401","            Number of samples to generate.","449","        X : array-like of shape (n_samples, n_features)","475","        X : array-like of shape (n_samples, n_features)","492","        X : array-like of shape (n_samples, n_features)"],"delete":["43","    X : array-like, shape (n_samples, n_features)","90","        X : array-like, shape (n_samples, n_features)","127","        X : array-like, shape  (n_samples, n_features)","136","        X : array-like, shape  (n_samples, n_features)","164","        X : array-like, shape  (n_samples, n_features)","166","        resp : array-like, shape (n_samples, n_components)","184","        X : array-like, shape (n_samples, n_features)","210","        X : array-like, shape (n_samples, n_features)","286","        X : array-like, shape (n_samples, n_features)","306","        X : array-like, shape (n_samples, n_features)","308","        log_resp : array-like, shape (n_samples, n_components)","327","        X : array-like, shape (n_samples, n_features)","346","        X : array-like, shape (n_samples, n_dimensions)","362","        X : array-like, shape (n_samples, n_features)","380","        X : array-like, shape (n_samples, n_features)","400","        n_samples : int, optional","401","            Number of samples to generate. Defaults to 1.","449","        X : array-like, shape (n_samples, n_features)","475","        X : array-like, shape (n_samples, n_features)","492","        X : array-like, shape (n_samples, n_features)"]}],"sklearn\/mixture\/_gaussian_mixture.py":[{"add":["24","    weights : array-like of shape (n_components,)","57","    means : array-like of shape (n_components, n_features)","146","    resp : array-like of shape (n_samples, n_components)","148","    X : array-like of shape (n_samples, n_features)","150","    nk : array-like of shape (n_components,)","152","    means : array-like of shape (n_components, n_features)","175","    resp : array-like of shape (n_samples, n_components)","177","    X : array-like of shape (n_samples, n_features)","179","    nk : array-like of shape (n_components,)","181","    means : array-like of shape (n_components, n_features)","203","    responsibilities : array-like of shape (n_samples, n_components)","205","    X : array-like of shape (n_samples, n_features)","207","    nk : array-like of shape (n_components,)","209","    means : array-like of shape (n_components, n_features)","229","    responsibilities : array-like of shape (n_samples, n_components)","231","    X : array-like of shape (n_samples, n_features)","233","    nk : array-like of shape (n_components,)","235","    means : array-like of shape (n_components, n_features)","253","    X : array-like of shape (n_samples, n_features)","256","    resp : array-like of shape (n_samples, n_components)","267","    nk : array-like of shape (n_components,)","270","    means : array-like of shape (n_components, n_features)","358","    log_det_precision_chol : array-like of shape (n_components,)","384","    X : array-like of shape (n_samples, n_features)","386","    means : array-like of shape (n_components, n_features)","446","    n_components : int, default=1","449","    covariance_type : {'full', 'tied', 'diag', 'spherical'}, default='full'","462","    tol : float, default=1e-3","466","    reg_covar : float, default=1e-6","470","    max_iter : int, default=100","473","    n_init : int, default=1","476","    init_params : {'kmeans', 'random'}, default='kmeans'","484","    weights_init : array-like of shape (n_components, ), default=None","485","        The user-provided initial weights.","488","    means_init : array-like of shape (n_components, n_features), default=None","489","        The user-provided initial means,","492","    precisions_init : array-like, default=None","494","        matrices).","503","    random_state : int, RandomState instance or None, default=None","511","    warm_start : bool, default=False","519","    verbose : int, default=0","525","    verbose_interval : int, default=10","530","    weights_ : array-like of shape (n_components,)","533","    means_ : array-like of shape (n_components, n_features)","645","        X : array-like of shape (n_samples, n_features)","647","        resp : array-like of shape (n_samples, n_components)","678","        X : array-like of shape (n_samples, n_features)","680","        log_resp : array-like of shape (n_samples, n_components)"],"delete":["24","    weights : array-like, shape (n_components,)","57","    means : array-like, shape (n_components, n_features)","146","    resp : array-like, shape (n_samples, n_components)","148","    X : array-like, shape (n_samples, n_features)","150","    nk : array-like, shape (n_components,)","152","    means : array-like, shape (n_components, n_features)","175","    resp : array-like, shape (n_samples, n_components)","177","    X : array-like, shape (n_samples, n_features)","179","    nk : array-like, shape (n_components,)","181","    means : array-like, shape (n_components, n_features)","203","    responsibilities : array-like, shape (n_samples, n_components)","205","    X : array-like, shape (n_samples, n_features)","207","    nk : array-like, shape (n_components,)","209","    means : array-like, shape (n_components, n_features)","229","    responsibilities : array-like, shape (n_samples, n_components)","231","    X : array-like, shape (n_samples, n_features)","233","    nk : array-like, shape (n_components,)","235","    means : array-like, shape (n_components, n_features)","253","    X : array-like, shape (n_samples, n_features)","256","    resp : array-like, shape (n_samples, n_components)","267","    nk : array-like, shape (n_components,)","270","    means : array-like, shape (n_components, n_features)","358","    log_det_precision_chol : array-like, shape (n_components,)","384","    X : array-like, shape (n_samples, n_features)","386","    means : array-like, shape (n_components, n_features)","446","    n_components : int, defaults to 1.","449","    covariance_type : {'full' (default), 'tied', 'diag', 'spherical'}","462","    tol : float, defaults to 1e-3.","466","    reg_covar : float, defaults to 1e-6.","470","    max_iter : int, defaults to 100.","473","    n_init : int, defaults to 1.","476","    init_params : {'kmeans', 'random'}, defaults to 'kmeans'.","484","    weights_init : array-like, shape (n_components, ), optional","485","        The user-provided initial weights, defaults to None.","488","    means_init : array-like, shape (n_components, n_features), optional","489","        The user-provided initial means, defaults to None,","492","    precisions_init : array-like, optional.","494","        matrices), defaults to None.","503","    random_state : int, RandomState instance or None, optional (default=None)","511","    warm_start : bool, default to False.","519","    verbose : int, default to 0.","525","    verbose_interval : int, default to 10.","530","    weights_ : array-like, shape (n_components,)","533","    means_ : array-like, shape (n_components, n_features)","645","        X : array-like, shape (n_samples, n_features)","647","        resp : array-like, shape (n_samples, n_components)","678","        X : array-like, shape (n_samples, n_features)","680","        log_resp : array-like, shape (n_samples, n_components)"]}]}},"120f07bfb6df6ae82951c39b78d86f2abeb85ee4":{"changes":{"sklearn\/cluster\/_kmeans.py":"MODIFY"},"diff":{"sklearn\/cluster\/_kmeans.py":[{"add":["328","    \"\"\"A single run of k-means elkan, assumes preparation completed prior."],"delete":["328","    \"\"\"A single run of k-means lloyd, assumes preparation completed prior."]}]}},"0111511ea38a97503091ce34a3bb3621eedb8fcf":{"changes":{"sklearn\/utils\/validation.py":"MODIFY","sklearn\/utils\/tests\/test_validation.py":"MODIFY","doc\/whats_new\/v0.24.rst":"MODIFY"},"diff":{"sklearn\/utils\/validation.py":[{"add":["570","        if array.dtype == np.dtype('object'):","571","            unique_dtypes = set(","572","                [dt.subtype.name for dt in array_orig.dtypes]","573","            )","574","            if len(unique_dtypes) > 1:","575","                raise ValueError(","576","                    \"Pandas DataFrame with mixed sparse extension arrays \"","577","                    \"generated a sparse matrix with object dtype which \"","578","                    \"can not be converted to a scipy sparse matrix.\"","579","                    \"Sparse extension arrays should all have the same \"","580","                    \"numeric type.\")"],"delete":[]}],"sklearn\/utils\/tests\/test_validation.py":[{"add":["47","from sklearn.utils.fixes import parse_version","1216","","1217","","1218","@pytest.mark.parametrize(","1219","    \"ntype1, ntype2\",","1220","    [","1221","        (\"longdouble\", \"float16\"),","1222","        (\"float16\", \"float32\"),","1223","        (\"float32\", \"double\"),","1224","        (\"int16\", \"int32\"),","1225","        (\"int32\", \"long\"),","1226","        (\"byte\", \"uint16\"),","1227","        (\"ushort\", \"uint32\"),","1228","        (\"uint32\", \"uint64\"),","1229","        (\"uint8\", \"int8\"),","1230","    ]","1231",")","1232","def test_check_pandas_sparse_invalid(ntype1, ntype2):","1233","    \"\"\"check that we raise an error with dataframe having","1234","    sparse extension arrays with unsupported mixed dtype","1235","    and pandas version below 1.1. pandas versions 1.1 and","1236","    above fixed this issue so no error will be raised.\"\"\"","1237","    pd = pytest.importorskip(\"pandas\", minversion=\"0.25.0\")","1238","    df = pd.DataFrame({'col1': pd.arrays.SparseArray([0, 1, 0],","1239","                                                     dtype=ntype1),","1240","                       'col2': pd.arrays.SparseArray([1, 0, 1],","1241","                                                     dtype=ntype2)})","1242","","1243","    if parse_version(pd.__version__) < parse_version('1.1'):","1244","        err_msg = \"Pandas DataFrame with mixed sparse extension arrays\"","1245","        with pytest.raises(ValueError, match=err_msg):","1246","            check_array(df, accept_sparse=['csr', 'csc'])","1247","    else:","1248","        # pandas fixed this issue at 1.1 so from here on,","1249","        # no error will be raised.","1250","        check_array(df, accept_sparse=['csr', 'csc'])","1251","","1252","","1253","@pytest.mark.parametrize(","1254","    \"ntype1, ntype2, expected_dtype\",","1255","    [","1256","        (\"longfloat\", \"longdouble\", \"float128\"),","1257","        (\"float16\", \"half\", \"float16\"),","1258","        (\"single\", \"float32\", \"float32\"),","1259","        (\"double\", \"float64\", \"float64\"),","1260","        (\"int8\", \"byte\", \"int8\"),","1261","        (\"short\", \"int16\", \"int16\"),","1262","        (\"intc\", \"int32\", \"int32\"),","1263","        (\"int0\", \"long\", \"int64\"),","1264","        (\"int\", \"long\", \"int64\"),","1265","        (\"int64\", \"longlong\", \"int64\"),","1266","        (\"int_\", \"intp\", \"int64\"),","1267","        (\"ubyte\", \"uint8\", \"uint8\"),","1268","        (\"uint16\", \"ushort\", \"uint16\"),","1269","        (\"uintc\", \"uint32\", \"uint32\"),","1270","        (\"uint\", \"uint64\", \"uint64\"),","1271","        (\"uintp\", \"ulonglong\", \"uint64\"),","1272","    ]","1273",")","1274","def test_check_pandas_sparse_valid(ntype1, ntype2, expected_dtype):","1275","    # check that we support the conversion of sparse dataframe with mixed","1276","    # type which can be converted safely.","1277","    pd = pytest.importorskip(\"pandas\", minversion=\"0.25.0\")","1278","    df = pd.DataFrame({'col1': pd.arrays.SparseArray([0, 1, 0],","1279","                                                     dtype=ntype1),","1280","                       'col2': pd.arrays.SparseArray([1, 0, 1],","1281","                                                     dtype=ntype2)})","1282","    arr = check_array(df, accept_sparse=['csr', 'csc'])","1283","    assert arr.dtype.name == expected_dtype"],"delete":[]}],"doc\/whats_new\/v0.24.rst":[{"add":["475",":mod:`sklearn.utils`","476","....................","477","","478","- |Fix| Raise ValueError with clear error message in :func:`check_array`","479","  for sparse DataFrames with mixed types.","480","  :pr:`17992` by :user:`Thomas J. Fan <thomasjpfan>` and","481","  :user:`Alex Shacked <alexshacked>`.","482",""],"delete":[]}]}},"76f2479e2fd3ece249cb67a5324776098c50bdce":{"changes":{"sklearn\/mixture\/_gaussian_mixture.py":"MODIFY"},"diff":{"sklearn\/mixture\/_gaussian_mixture.py":[{"add":["486","        If it is None, weights are initialized using the `init_params` method.","490","        If it is None, means are initialized using the `init_params` method.","495","        If it is None, precisions are initialized using the 'init_params'","496","        method."],"delete":["486","        If it None, weights are initialized using the `init_params` method.","490","        If it None, means are initialized using the `init_params` method.","495","        If it None, precisions are initialized using the 'init_params' method."]}]}},"5d3d54da19ef5cefa7c14c89285559907df952f1":{"changes":{"doc\/developers\/contributing.rst":"MODIFY"},"diff":{"doc\/developers\/contributing.rst":[{"add":["265","","488",".. _stalled_pull_request:","489","","524","Stalled and Unclaimed Issues","525","^^^^^^^^^^^^^^^^^^^^^^^^^^^^","526","","527","Generally speaking, issues which are up for grabs will have a","528","`\"help wanted\" <https:\/\/github.com\/scikit-learn\/scikit-learn\/labels\/help%20wanted>`__ .","529","tag. However, not all issues which need contributors will have this tag,","530","as the \"help wanted\" tag is not always up-to-date with the state","531","of the issue. Contributors can find issues which are still up for grabs","532","using the following guidelines:","533","","534","* First, to **determine if an issue is claimed**:","535","","536","  * Check for linked pull requests","537","  * Check the conversation to see if anyone has said that they're working on","538","    creating a pull request","539","","540","* If a contributor comments on an issue to say they are working on it,","541","  a pull request is expected within 2 weeks (new contributor) or 4 weeks","542","  (contributor or core dev), unless an larger time frame is explicitly given.","543","  Beyond that time, another contributor can take the issue and make a","544","  pull request for it. We encourage contributors to comment directly on the","545","  stalled or unclaimed issue to let community members know that they will be","546","  working on it.","547","","548","* If the issue is linked to a :ref:`stalled pull request <stalled_pull_request>`,","549","  we recommend that contributors follow the procedure","550","  described in the :ref:`stalled_pull_request`","551","  section rather than working directly on the issue.","552",""],"delete":["265"," "]}]}},"138dd7b88f1634447f838bc58088e594ffaf5549":{"changes":{"sklearn\/utils\/tests\/test_validation.py":"MODIFY"},"diff":{"sklearn\/utils\/tests\/test_validation.py":[{"add":["1254","    \"ntype1, ntype2, expected_subtype\",","1256","        (\"longfloat\", \"longdouble\", np.floating),","1257","        (\"float16\", \"half\", np.floating),","1258","        (\"single\", \"float32\", np.floating),","1259","        (\"double\", \"float64\", np.floating),","1260","        (\"int8\", \"byte\", np.integer),","1261","        (\"short\", \"int16\", np.integer),","1262","        (\"intc\", \"int32\", np.integer),","1263","        (\"int0\", \"long\", np.integer),","1264","        (\"int\", \"long\", np.integer),","1265","        (\"int64\", \"longlong\", np.integer),","1266","        (\"int_\", \"intp\", np.integer),","1267","        (\"ubyte\", \"uint8\", np.unsignedinteger),","1268","        (\"uint16\", \"ushort\", np.unsignedinteger),","1269","        (\"uintc\", \"uint32\", np.unsignedinteger),","1270","        (\"uint\", \"uint64\", np.unsignedinteger),","1271","        (\"uintp\", \"ulonglong\", np.unsignedinteger)","1274","def test_check_pandas_sparse_valid(ntype1, ntype2, expected_subtype):","1283","    assert np.issubdtype(arr.dtype, expected_subtype)"],"delete":["1254","    \"ntype1, ntype2, expected_dtype\",","1256","        (\"longfloat\", \"longdouble\", \"float128\"),","1257","        (\"float16\", \"half\", \"float16\"),","1258","        (\"single\", \"float32\", \"float32\"),","1259","        (\"double\", \"float64\", \"float64\"),","1260","        (\"int8\", \"byte\", \"int8\"),","1261","        (\"short\", \"int16\", \"int16\"),","1262","        (\"intc\", \"int32\", \"int32\"),","1263","        (\"int0\", \"long\", \"int64\"),","1264","        (\"int\", \"long\", \"int64\"),","1265","        (\"int64\", \"longlong\", \"int64\"),","1266","        (\"int_\", \"intp\", \"int64\"),","1267","        (\"ubyte\", \"uint8\", \"uint8\"),","1268","        (\"uint16\", \"ushort\", \"uint16\"),","1269","        (\"uintc\", \"uint32\", \"uint32\"),","1270","        (\"uint\", \"uint64\", \"uint64\"),","1271","        (\"uintp\", \"ulonglong\", \"uint64\"),","1274","def test_check_pandas_sparse_valid(ntype1, ntype2, expected_dtype):","1283","    assert arr.dtype.name == expected_dtype"]}]}},"82748208a162930f877edb687ab01c7703283fd3":{"changes":{"sklearn\/__init__.py":"MODIFY"},"diff":{"sklearn\/__init__.py":[{"add":["17","import random","18","","101",""],"delete":["99","    import os","101","    import random"]}]}},"06d6f8a9f6e3d847b9db79a5fdebe64f78deef7f":{"changes":{"sklearn\/decomposition\/_dict_learning.py":"MODIFY","sklearn\/linear_model\/_omp.py":"MODIFY","sklearn\/utils\/fixes.py":"MODIFY","sklearn\/feature_selection\/_rfe.py":"MODIFY","sklearn\/covariance\/_graph_lasso.py":"MODIFY","sklearn\/neighbors\/_base.py":"MODIFY","sklearn\/linear_model\/_logistic.py":"MODIFY","sklearn\/calibration.py":"MODIFY","sklearn\/multioutput.py":"MODIFY","benchmarks\/bench_saga.py":"MODIFY","sklearn\/ensemble\/_forest.py":"MODIFY","sklearn\/utils\/tests\/test_parallel.py":"ADD","sklearn\/linear_model\/_least_angle.py":"MODIFY","sklearn\/linear_model\/_coordinate_descent.py":"MODIFY","sklearn\/model_selection\/_search.py":"MODIFY","sklearn\/decomposition\/_lda.py":"MODIFY","sklearn\/inspection\/_plot\/partial_dependence.py":"MODIFY","sklearn\/ensemble\/_stacking.py":"MODIFY","sklearn\/inspection\/_permutation_importance.py":"MODIFY","sklearn\/linear_model\/_stochastic_gradient.py":"MODIFY","sklearn\/linear_model\/_theil_sen.py":"MODIFY","sklearn\/manifold\/_mds.py":"MODIFY","sklearn\/model_selection\/_validation.py":"MODIFY","sklearn\/metrics\/pairwise.py":"MODIFY","sklearn\/ensemble\/_bagging.py":"MODIFY","sklearn\/ensemble\/_voting.py":"MODIFY","sklearn\/pipeline.py":"MODIFY","build_tools\/circle\/linting.sh":"MODIFY","sklearn\/multiclass.py":"MODIFY","sklearn\/linear_model\/_base.py":"MODIFY","sklearn\/cluster\/_mean_shift.py":"MODIFY","sklearn\/compose\/_column_transformer.py":"MODIFY"},"diff":{"sklearn\/decomposition\/_dict_learning.py":[{"add":["13","from joblib import Parallel, effective_n_jobs","21","from ..utils.fixes import delayed"],"delete":["13","from joblib import Parallel, delayed, effective_n_jobs"]}],"sklearn\/linear_model\/_omp.py":[{"add":["13","from joblib import Parallel","19","from ..utils.fixes import delayed"],"delete":["13","from joblib import Parallel, delayed"]}],"sklearn\/utils\/fixes.py":[{"add":["12","from functools import update_wrapper","14","import functools","22","from .._config import config_context, get_config","201","","202","","203","# remove when https:\/\/github.com\/joblib\/joblib\/issues\/1071 is fixed","204","def delayed(function):","205","    \"\"\"Decorator used to capture the arguments of a function.\"\"\"","206","    @functools.wraps(function)","207","    def delayed_function(*args, **kwargs):","208","        return _FuncWrapper(function), args, kwargs","209","    return delayed_function","210","","211","","212","class _FuncWrapper:","213","    \"\"\"\"Load the global configuration before calling the function.\"\"\"","214","    def __init__(self, function):","215","        self.function = function","216","        self.config = get_config()","217","        update_wrapper(self, self.function)","218","","219","    def __call__(self, *args, **kwargs):","220","        with config_context(**self.config):","221","            return self.function(*args, **kwargs)"],"delete":[]}],"sklearn\/feature_selection\/_rfe.py":[{"add":["10","from joblib import Parallel, effective_n_jobs","16","from ..utils.fixes import delayed"],"delete":["10","from joblib import Parallel, delayed, effective_n_jobs"]}],"sklearn\/covariance\/_graph_lasso.py":[{"add":["15","from joblib import Parallel","22","from ..utils.fixes import delayed"],"delete":["15","from joblib import Parallel, delayed"]}],"sklearn\/neighbors\/_base.py":[{"add":["17","from joblib import Parallel, effective_n_jobs","30","from ..utils.fixes import delayed","706","                delayed_query = delayed(_tree_query_parallel_helper)"],"delete":["17","from joblib import Parallel, delayed, effective_n_jobs","705","                check_pickle = False if old_joblib else None","706","                delayed_query = delayed(_tree_query_parallel_helper,","707","                                        check_pickle=check_pickle)"]}],"sklearn\/linear_model\/_logistic.py":[{"add":["18","from joblib import Parallel, effective_n_jobs","34","from ..utils.fixes import delayed"],"delete":["18","from joblib import Parallel, delayed, effective_n_jobs"]}],"sklearn\/calibration.py":[{"add":["15","from joblib import Parallel","26","from .utils.fixes import delayed"],"delete":["15","from joblib import delayed, Parallel"]}],"sklearn\/multioutput.py":[{"add":["18","from joblib import Parallel","29","from .utils.fixes import delayed"],"delete":["18","from joblib import Parallel, delayed"]}],"benchmarks\/bench_saga.py":[{"add":["9","from joblib import Parallel","10","from sklearn.utils.fixes import delayed"],"delete":["9","from joblib import delayed, Parallel"]}],"sklearn\/ensemble\/_forest.py":[{"add":["50","from joblib import Parallel","61","from ..utils.fixes import delayed"],"delete":["50","from joblib import Parallel, delayed"]}],"sklearn\/utils\/tests\/test_parallel.py":[{"add":[],"delete":[]}],"sklearn\/linear_model\/_least_angle.py":[{"add":["17","from joblib import Parallel","27","from ..utils.fixes import delayed"],"delete":["17","from joblib import Parallel, delayed"]}],"sklearn\/linear_model\/_coordinate_descent.py":[{"add":["14","from joblib import Parallel, effective_n_jobs","27","from ..utils.fixes import delayed"],"delete":["14","from joblib import Parallel, delayed, effective_n_jobs"]}],"sklearn\/model_selection\/_search.py":[{"add":["34","from joblib import Parallel","40","from ..utils.fixes import delayed"],"delete":["34","from joblib import Parallel, delayed"]}],"sklearn\/decomposition\/_lda.py":[{"add":["16","from joblib import Parallel, effective_n_jobs","23","from ..utils.fixes import delayed"],"delete":["16","from joblib import Parallel, delayed, effective_n_jobs"]}],"sklearn\/inspection\/_plot\/partial_dependence.py":[{"add":["8","from joblib import Parallel","16","from ...utils.fixes import delayed"],"delete":["8","from joblib import Parallel, delayed"]}],"sklearn\/ensemble\/_stacking.py":[{"add":["9","from joblib import Parallel","35","from ..utils.fixes import delayed"],"delete":["9","from joblib import Parallel, delayed"]}],"sklearn\/inspection\/_permutation_importance.py":[{"add":["9","from ..utils.fixes import delayed"],"delete":["3","from joblib import delayed"]}],"sklearn\/linear_model\/_stochastic_gradient.py":[{"add":["11","from joblib import Parallel","22","from ..utils.fixes import delayed"],"delete":["11","from joblib import Parallel, delayed"]}],"sklearn\/linear_model\/_theil_sen.py":[{"add":["17","from joblib import Parallel, effective_n_jobs","23","from ..utils.fixes import delayed"],"delete":["17","from joblib import Parallel, delayed, effective_n_jobs"]}],"sklearn\/manifold\/_mds.py":[{"add":["8","from joblib import Parallel, effective_n_jobs","17","from ..utils.fixes import delayed"],"delete":["8","from joblib import Parallel, delayed, effective_n_jobs"]}],"sklearn\/model_selection\/_validation.py":[{"add":["20","from joblib import Parallel, logger","27","from ..utils.fixes import delayed"],"delete":["20","from joblib import Parallel, delayed, logger"]}],"sklearn\/metrics\/pairwise.py":[{"add":["19","from joblib import Parallel, effective_n_jobs","31","from ..utils.fixes import delayed"],"delete":["19","from joblib import Parallel, delayed, effective_n_jobs"]}],"sklearn\/ensemble\/_bagging.py":[{"add":["12","from joblib import Parallel","25","from ..utils.fixes import delayed"],"delete":["12","from joblib import Parallel, delayed"]}],"sklearn\/ensemble\/_voting.py":[{"add":["19","from joblib import Parallel","35","from ..utils.fixes import delayed"],"delete":["19","from joblib import Parallel, delayed"]}],"sklearn\/pipeline.py":[{"add":["16","from joblib import Parallel","24","from .utils.fixes import delayed"],"delete":["16","from joblib import Parallel, delayed"]}],"build_tools\/circle\/linting.sh":[{"add":["174","","175","joblib_import=\"$(git grep -l -A 10 -E \"joblib import.+delayed\" -- \"*.py\" \":!sklearn\/utils\/_joblib.py\" \":!sklearn\/utils\/fixes.py\")\"","176","","177","if [ ! -z \"$joblib_import\" ]; then","178","    echo \"Use from sklearn.utils.fixes import delayed instead of joblib delayed. The following files contains imports to joblib.delayed:\"","179","    echo \"$joblib_import\"","180","    exit 1","181","fi"],"delete":[]}],"sklearn\/multiclass.py":[{"add":["55","from .utils.fixes import delayed","58","from joblib import Parallel"],"delete":["57","from joblib import Parallel, delayed"]}],"sklearn\/linear_model\/_base.py":[{"add":["25","from joblib import Parallel","39","from ..utils.fixes import delayed"],"delete":["25","from joblib import Parallel, delayed"]}],"sklearn\/cluster\/_mean_shift.py":[{"add":["18","from joblib import Parallel","22","from ..utils.fixes import delayed"],"delete":["18","from joblib import Parallel, delayed"]}],"sklearn\/compose\/_column_transformer.py":[{"add":["14","from joblib import Parallel","27","from ..utils.fixes import delayed"],"delete":["14","from joblib import Parallel, delayed"]}]}},"2b655efaf26f7802e4e41f2e64e1b9abdcaa6cd2":{"changes":{"sklearn\/compose\/tests\/test_column_transformer.py":"MODIFY","sklearn\/compose\/_column_transformer.py":"MODIFY","doc\/whats_new\/v0.24.rst":"MODIFY"},"diff":{"sklearn\/compose\/tests\/test_column_transformer.py":[{"add":["1383","","1384","","1385","@pytest.mark.parametrize('remainder', [\"passthrough\", StandardScaler()])","1386","def test_sk_visual_block_remainder(remainder):","1387","    # remainder='passthrough' or an estimator will be shown in repr_html","1388","    ohe = OneHotEncoder()","1389","    ct = ColumnTransformer(transformers=[('ohe', ohe, [\"col1\", \"col2\"])],","1390","                           remainder=remainder)","1391","    visual_block = ct._sk_visual_block_()","1392","    assert visual_block.names == ('ohe', 'remainder')","1393","    assert visual_block.name_details == (['col1', 'col2'], '')","1394","    assert visual_block.estimators == (ohe, remainder)","1395","","1396","","1397","def test_sk_visual_block_remainder_drop():","1398","    # remainder='drop' is not shown in repr_html","1399","    ohe = OneHotEncoder()","1400","    ct = ColumnTransformer(transformers=[('ohe', ohe, [\"col1\", \"col2\"])])","1401","    visual_block = ct._sk_visual_block_()","1402","    assert visual_block.names == ('ohe',)","1403","    assert visual_block.name_details == (['col1', 'col2'],)","1404","    assert visual_block.estimators == (ohe,)","1405","","1406","","1407","@pytest.mark.parametrize('remainder', [\"passthrough\", StandardScaler()])","1408","def test_sk_visual_block_remainder_fitted_pandas(remainder):","1409","    # Remainder shows the columns after fitting","1410","    pd = pytest.importorskip('pandas')","1411","    ohe = OneHotEncoder()","1412","    ct = ColumnTransformer(transformers=[('ohe', ohe, [\"col1\", \"col2\"])],","1413","                           remainder=remainder)","1414","    df = pd.DataFrame({\"col1\": [\"a\", \"b\", \"c\"], \"col2\": [\"z\", \"z\", \"z\"],","1415","                       \"col3\": [1, 2, 3], \"col4\": [3, 4, 5]})","1416","    ct.fit(df)","1417","    visual_block = ct._sk_visual_block_()","1418","    assert visual_block.names == ('ohe', 'remainder')","1419","    assert visual_block.name_details == (['col1', 'col2'], ['col3', 'col4'])","1420","    assert visual_block.estimators == (ohe, remainder)","1421","","1422","","1423","@pytest.mark.parametrize('remainder', [\"passthrough\", StandardScaler()])","1424","def test_sk_visual_block_remainder_fitted_numpy(remainder):","1425","    # Remainder shows the indices after fitting","1426","    X = np.array([[1, 2, 3], [4, 5, 6]], dtype=float)","1427","    scaler = StandardScaler()","1428","    ct = ColumnTransformer(transformers=[('scale', scaler, [0, 2])],","1429","                           remainder=remainder)","1430","    ct.fit(X)","1431","    visual_block = ct._sk_visual_block_()","1432","    assert visual_block.names == ('scale', 'remainder')","1433","    assert visual_block.name_details == ([0, 2], [1])","1434","    assert visual_block.estimators == (scaler, remainder)"],"delete":[]}],"sklearn\/compose\/_column_transformer.py":[{"add":["641","        if isinstance(self.remainder, str) and self.remainder == 'drop':","642","            transformers = self.transformers","643","        elif hasattr(self, \"_remainder\"):","644","            remainder_columns = self._remainder[2]","645","            if hasattr(self, '_df_columns'):","646","                remainder_columns = (","647","                    self._df_columns[remainder_columns].tolist()","648","                )","649","            transformers = chain(self.transformers,","650","                                 [('remainder', self.remainder,","651","                                   remainder_columns)])","652","        else:","653","            transformers = chain(self.transformers,","654","                                 [('remainder', self.remainder, '')])","655","","656","        names, transformers, name_details = zip(*transformers)"],"delete":["641","        names, transformers, name_details = zip(*self.transformers)"]}],"doc\/whats_new\/v0.24.rst":[{"add":["88","- |FIX| :class:`compose.ColumnTransformer` now displays the remainder in the","89","  diagram display. :pr:`18167` by `Thomas Fan`_.","90",""],"delete":[]}]}},"5d04910476d3b50a06dfb15ec9faacd410a26541":{"changes":{"sklearn\/model_selection\/tests\/test_search.py":"MODIFY","doc\/whats_new\/v0.24.rst":"MODIFY","sklearn\/model_selection\/_search.py":"MODIFY"},"diff":{"sklearn\/model_selection\/tests\/test_search.py":[{"add":["58","from sklearn.neighbors import LocalOutlierFactor","79","","164","                set((\"bar\", x, \"foo\", y)","165","                    for x, y in product(params2[\"bar\"], params2[\"foo\"])))","694","    def check_X(x): return x.shape[1:] == (5, 3, 2)","695","    def check_y(x): return x.shape[1:] == (7, 11)","1101","@pytest.mark.parametrize('search_cv', [","1102","    RandomizedSearchCV(estimator=DecisionTreeClassifier(),","1103","                       param_distributions={'max_depth': [5, 10]}),","1104","    GridSearchCV(estimator=DecisionTreeClassifier(),","1105","                 param_grid={'max_depth': [5, 10]})","1106","])","1107","def test_search_cv_score_samples_error(search_cv):","1108","    X, y = make_blobs(n_samples=100, n_features=4, random_state=42)","1109","    search_cv.fit(X, y)","1110","","1111","    # Make sure to error out when underlying estimator does not implement","1112","    # the method `score_samples`","1113","    err_msg = (\"'DecisionTreeClassifier' object has no attribute \"","1114","               \"'score_samples'\")","1115","","1116","    with pytest.raises(AttributeError, match=err_msg):","1117","        search_cv.score_samples(X)","1118","","1119","","1120","@pytest.mark.parametrize('search_cv', [","1121","    RandomizedSearchCV(estimator=LocalOutlierFactor(novelty=True),","1122","                       param_distributions={'n_neighbors': [5, 10]},","1123","                       scoring=\"precision\"),","1124","    GridSearchCV(estimator=LocalOutlierFactor(novelty=True),","1125","                 param_grid={'n_neighbors': [5, 10]},","1126","                 scoring=\"precision\")","1127","])","1128","def test_search_cv_score_samples_method(search_cv):","1129","    # Set parameters","1130","    rng = np.random.RandomState(42)","1131","    n_samples = 300","1132","    outliers_fraction = 0.15","1133","    n_outliers = int(outliers_fraction * n_samples)","1134","    n_inliers = n_samples - n_outliers","1135","","1136","    # Create dataset","1137","    X = make_blobs(n_samples=n_inliers, n_features=2, centers=[[0, 0], [0, 0]],","1138","                   cluster_std=0.5, random_state=0)[0]","1139","    # Add some noisy points","1140","    X = np.concatenate([X, rng.uniform(low=-6, high=6,","1141","                                       size=(n_outliers, 2))], axis=0)","1142","","1143","    # Define labels to be able to score the estimator with `search_cv`","1144","    y_true = np.array([1] * n_samples)","1145","    y_true[-n_outliers:] = -1","1146","","1147","    # Fit on data","1148","    search_cv.fit(X, y_true)","1149","","1150","    # Verify that the stand alone estimator yields the same results","1151","    # as the ones obtained with *SearchCV","1152","    assert_allclose(search_cv.score_samples(X),","1153","                    search_cv.best_estimator_.score_samples(X))","1154","","1155",""],"delete":["162","                     set((\"bar\", x, \"foo\", y)","163","                         for x, y in product(params2[\"bar\"], params2[\"foo\"])))","692","    check_X = lambda x: x.shape[1:] == (5, 3, 2)","693","    check_y = lambda x: x.shape[1:] == (7, 11)"]}],"doc\/whats_new\/v0.24.rst":[{"add":["93","- |Feature| :class:`model_selection.RandomizedSearchCV` and","94","  :class:`model_selection.GridSearchCV` now have the method, ``score_samples``","95","  :pr:`17478` by :user:`Teon Brooks <teonbrooks>` and","96","  :user:`Mohamed Maskani <maskani-moh>`.","97","","129","  ``predict`` and related methods of :class:`svm.SVC`, :class:`svm.NuSVC`,"],"delete":["124","  ``predict`` and related methods of :class:`svm.SVC`, :class:`svm.NuSVC`, "]}],"sklearn\/model_selection\/_search.py":[{"add":["459","    @if_delegate_has_method(delegate=('best_estimator_', 'estimator'))","460","    def score_samples(self, X):","461","        \"\"\"Call score_samples on the estimator with the best found parameters.","462","","463","        Only available if ``refit=True`` and the underlying estimator supports","464","        ``score_samples``.","465","","466","        Parameters","467","        ----------","468","        X : iterable","469","            Data to predict on. Must fulfill input requirements","470","            of the underlying estimator.","471","","472","        Returns","473","        -------","474","        y_score : ndarray, shape (n_samples,)","475","        \"\"\"","476","        self._check_is_fitted('score_samples')","477","        return self.best_estimator_.score_samples(X)","478","","883","    It also implements \"score_samples\", \"predict\", \"predict_proba\",","884","    \"decision_function\", \"transform\" and \"inverse_transform\" if they are","885","    implemented in the estimator used.","1196","    It also implements \"score_samples\", \"predict\", \"predict_proba\",","1197","    \"decision_function\", \"transform\" and \"inverse_transform\" if they are","1198","    implemented in the estimator used."],"delete":["863","    It also implements \"predict\", \"predict_proba\", \"decision_function\",","864","    \"transform\" and \"inverse_transform\" if they are implemented in the","865","    estimator used.","1176","    It also implements \"predict\", \"predict_proba\", \"decision_function\",","1177","    \"transform\" and \"inverse_transform\" if they are implemented in the","1178","    estimator used."]}]}},"ac6caa050bb7ce04a2b648e86e31f3de0af7dd35":{"changes":{"sklearn\/tests\/test_discriminant_analysis.py":"MODIFY","sklearn\/discriminant_analysis.py":"MODIFY","sklearn\/cluster\/tests\/test_k_means.py":"MODIFY","sklearn\/neighbors\/_base.py":"MODIFY","sklearn\/cluster\/tests\/test_dbscan.py":"MODIFY"},"diff":{"sklearn\/tests\/test_discriminant_analysis.py":[{"add":["76","                        rtol=1e-6, atol=1e-6, err_msg='solver %s' % solver)"],"delete":["76","                        rtol=1e-6, err_msg='solver %s' % solver)"]}],"sklearn\/discriminant_analysis.py":[{"add":["539","        prediction = self.predict_proba(X)","540","        prediction[prediction == 0.0] += np.finfo(prediction.dtype).tiny","541","        return np.log(prediction)"],"delete":["539","        return np.log(self.predict_proba(X))"]}],"sklearn\/cluster\/tests\/test_k_means.py":[{"add":["468","    n_init = 10 if type(init) is str else 1","470","                                 random_state=42, n_init=n_init)","675","    n_init = 10 if type(init) is str else 1","677","                        n_init=n_init, random_state=0).fit(data)","695","    n_init = 10 if type(init) is str else 1","697","                                 n_init=n_init, random_state=0).fit(X_csr)","951","                         random_state=42, n_init=1),"],"delete":["469","                                 random_state=42, n_init=10)","675","                        n_init=10, random_state=0).fit(data)","694","                                 n_init=10, random_state=0).fit(X_csr)","948","                         random_state=42),"]}],"sklearn\/neighbors\/_base.py":[{"add":["338","            if self.p is not None:","339","                warnings.warn(\"Parameter p is found in metric_params. \"","340","                              \"The corresponding parameter from __init__ \"","341","                              \"is ignored.\", SyntaxWarning, stacklevel=3)"],"delete":["338","            warnings.warn(\"Parameter p is found in metric_params. \"","339","                          \"The corresponding parameter from __init__ \"","340","                          \"is ignored.\", SyntaxWarning, stacklevel=3)"]}],"sklearn\/cluster\/tests\/test_dbscan.py":[{"add":["8","import warnings","9","","176","","177","    with warnings.catch_warnings(record=True) as warns:","178","        db = DBSCAN(","179","            metric='minkowski', metric_params={'p': p}, eps=eps,","180","            p=None, min_samples=min_samples, algorithm='ball_tree'","181","            ).fit(X)","182","    assert not warns","201","    with pytest.warns(","202","        SyntaxWarning,","203","        match=\"Parameter p is found in metric_params. \"","204","              \"The corresponding parameter from __init__ \"","205","              \"is ignored.\"):","206","        # Test that checks p is ignored in favor of metric_params={'p': <val>}","207","        db = DBSCAN(metric='minkowski', metric_params={'p': p}, eps=eps, p=p+1,","208","                    min_samples=min_samples, algorithm='ball_tree').fit(X)","209","        core_sample_4, labels_4 = db.core_sample_indices_, db.labels_","210","","211","    assert_array_equal(core_sample_1, core_sample_4)","212","    assert_array_equal(labels_1, labels_4)","213",""],"delete":["174","    db = DBSCAN(metric='minkowski', metric_params={'p': p}, eps=eps,","175","                min_samples=min_samples, algorithm='ball_tree').fit(X)"]}]}},"4e8dc9db5cbdc6fd327d3cf8e4ff78bb15140b92":{"changes":{"examples\/model_selection\/plot_grid_search_digits.py":"MODIFY","examples\/cluster\/plot_optics.py":"MODIFY","examples\/impute\/plot_missing_values.py":"MODIFY","examples\/covariance\/plot_covariance_estimation.py":"MODIFY","examples\/compose\/plot_column_transformer_mixed_types.py":"MODIFY","examples\/linear_model\/plot_sgd_early_stopping.py":"MODIFY","examples\/linear_model\/plot_sgd_loss_functions.py":"MODIFY","examples\/miscellaneous\/plot_kernel_approximation.py":"MODIFY","examples\/impute\/plot_iterative_imputer_variants_comparison.py":"MODIFY","examples\/miscellaneous\/plot_anomaly_comparison.py":"MODIFY","examples\/neighbors\/plot_kde_1d.py":"MODIFY","examples\/miscellaneous\/plot_multilabel.py":"MODIFY"},"diff":{"examples\/model_selection\/plot_grid_search_digits.py":[{"add":["6","which is done using the :class:`~sklearn.model_selection.GridSearchCV` object"],"delete":["6","which is done using the :class:`sklearn.model_selection.GridSearchCV` object"]}],"examples\/cluster\/plot_optics.py":[{"add":["4","","5",".. currentmodule:: sklearn","6","","10","The :class:`~cluster.OPTICS` is first used with its Xi cluster detection","12","corresponds to :class:`~cluster.DBSCAN`. We can see that the different"],"delete":["7","The :class:`sklearn.cluster.OPTICS` is first used with its Xi cluster detection","9","corresponds to :class:`sklearn.cluster.DBSCAN`. We can see that the different"]}],"examples\/impute\/plot_missing_values.py":[{"add":["6","value using the basic :class:`~sklearn.impute.SimpleImputer`.","180","# :class:`~sklearn.impute.KNNImputer` imputes missing values using the weighted","217","# Another option is the :class:`~sklearn.impute.IterativeImputer`. This uses"],"delete":["6","value using the basic :class:`sklearn.impute.SimpleImputer`.","180","# :class:`sklearn.impute.KNNImputer` imputes missing values using the weighted","217","# Another option is the :class:`sklearn.impute.IterativeImputer`. This uses"]}],"examples\/covariance\/plot_covariance_estimation.py":[{"add":["7",":class:`~sklearn.covariance.EmpiricalCovariance`. It is unbiased, i.e. it","23","  criterion), yielding the :class:`~sklearn.covariance.LedoitWolf`","27","  :class:`~sklearn.covariance.OAS`, proposed by Chen et al. Its"],"delete":["7",":class:`sklearn.covariance.EmpiricalCovariance`. It is unbiased, i.e. it","23","  criterion), yielding the :class:`sklearn.covariance.LedoitWolf`","27","  :class:`sklearn.covariance.OAS`, proposed by Chen et al. Its"]}],"examples\/compose\/plot_column_transformer_mixed_types.py":[{"add":["5",".. currentmodule:: sklearn","6","","9",":class:`~compose.ColumnTransformer`. This is particularly handy for the","21","using :class:`~pipeline.Pipeline`, together with a simple classification","168","# :class:`~sklearn.model_selection.GridSearchCV`."],"delete":["7",":class:`sklearn.compose.ColumnTransformer`. This is particularly handy for the","19","using :class:`sklearn.pipeline.Pipeline`, together with a simple classification","166","# :class:`sklearn.model_selection.GridSearchCV`."]}],"examples\/linear_model\/plot_sgd_early_stopping.py":[{"add":["31",":class:`~sklearn.linear_model.SGDClassifier` model to achieve almost the same"],"delete":["31",":class:`sklearn.linear_model.SGDClassifier` model to achieve almost the same"]}],"examples\/linear_model\/plot_sgd_loss_functions.py":[{"add":["6",":class:`~sklearn.linear_model.SGDClassifier` ."],"delete":["6",":class:`sklearn.linear_model.SGDClassifier` ."]}],"examples\/miscellaneous\/plot_kernel_approximation.py":[{"add":["26","stochastic gradient descent via :class:`~sklearn.linear_model.SGDClassifier`."],"delete":["26","stochastic gradient descent via :class:`sklearn.linear_model.SGDClassifier`."]}],"examples\/impute\/plot_iterative_imputer_variants_comparison.py":[{"add":["5",".. currentmodule:: sklearn","6","","7","The :class:`~impute.IterativeImputer` class is very flexible - it can be","12","imputation with :class:`~impute.IterativeImputer`:","14","* :class:`~linear_model.BayesianRidge`: regularized linear regression","15","* :class:`~tree.DecisionTreeRegressor`: non-linear regression","16","* :class:`~ensemble.ExtraTreesRegressor`: similar to missForest in R","17","* :class:`~neighbors.KNeighborsRegressor`: comparable to other KNN","21",":class:`~impute.IterativeImputer` to mimic the behavior of missForest, a","23",":class:`~ensemble.ExtraTreesRegressor` instead of","24",":class:`~ensemble.RandomForestRegressor` (as in missForest) due to its","27","Note that :class:`~neighbors.KNeighborsRegressor` is different from KNN","32",":class:`~impute.IterativeImputer` when using a","33",":class:`~linear_model.BayesianRidge` estimator on the California housing","37",":class:`~ensemble.ExtraTreesRegressor` and","38",":class:`~linear_model.BayesianRidge` give the best results."],"delete":["5","The :class:`sklearn.impute.IterativeImputer` class is very flexible - it can be","10","imputation with :class:`sklearn.impute.IterativeImputer`:","12","* :class:`~sklearn.linear_model.BayesianRidge`: regularized linear regression","13","* :class:`~sklearn.tree.DecisionTreeRegressor`: non-linear regression","14","* :class:`~sklearn.ensemble.ExtraTreesRegressor`: similar to missForest in R","15","* :class:`~sklearn.neighbors.KNeighborsRegressor`: comparable to other KNN","19",":class:`sklearn.impute.IterativeImputer` to mimic the behavior of missForest, a","21",":class:`sklearn.ensemble.ExtraTreesRegressor` instead of","22",":class:`sklearn.ensemble.RandomForestRegressor` (as in missForest) due to its","25","Note that :class:`sklearn.neighbors.KNeighborsRegressor` is different from KNN","30",":class:`sklearn.impute.IterativeImputer` when using a","31",":class:`sklearn.linear_model.BayesianRidge` estimator on the California housing","35",":class:`sklearn.ensemble.ExtraTreesRegressor` and","36",":class:`sklearn.linear_model.BayesianRidge` give the best results."]}],"examples\/miscellaneous\/plot_anomaly_comparison.py":[{"add":["16","The :class:`~sklearn.svm.OneClassSVM` is known to be sensitive to outliers and","24",":class:`~sklearn.covariance.EllipticEnvelope` assumes the data is Gaussian and","28",":class:`~sklearn.ensemble.IsolationForest` and","29",":class:`~sklearn.neighbors.LocalOutlierFactor` seem to perform reasonably well","31",":class:`~sklearn.neighbors.LocalOutlierFactor` over the other estimators is","39","hypercube. Except for the :class:`~sklearn.svm.OneClassSVM` which overfits a"],"delete":["16","The :class:`sklearn.svm.OneClassSVM` is known to be sensitive to outliers and","24",":class:`sklearn.covariance.EllipticEnvelope` assumes the data is Gaussian and","28",":class:`sklearn.ensemble.IsolationForest` and","29",":class:`sklearn.neighbors.LocalOutlierFactor` seem to perform reasonably well","31",":class:`sklearn.neighbors.LocalOutlierFactor` over the other estimators is","39","hypercube. Except for the :class:`sklearn.svm.OneClassSVM` which overfits a"]}],"examples\/neighbors\/plot_kde_1d.py":[{"add":["4","This example uses the :class:`~sklearn.neighbors.KernelDensity` class to","20",":class:`~sklearn.neighbors.KernelDensity` estimator.  The available kernels"],"delete":["4","This example uses the :class:`sklearn.neighbors.KernelDensity` class to","20",":class:`sklearn.neighbors.KernelDensity` estimator.  The available kernels"]}],"examples\/miscellaneous\/plot_multilabel.py":[{"add":["22","the :class:`~sklearn.multiclass.OneVsRestClassifier` metaclassifier using two"],"delete":["22","the :class:`sklearn.multiclass.OneVsRestClassifier` metaclassifier using two"]}]}},"6b68144f179b9a56e05ae401da7527bc5da97f21":{"changes":{"sklearn\/tests\/test_multiclass.py":"MODIFY","sklearn\/multiclass.py":"MODIFY","doc\/whats_new\/v0.24.rst":"MODIFY"},"diff":{"sklearn\/tests\/test_multiclass.py":[{"add":["2","import pytest","12","from sklearn.utils._mocking import CheckingClassifier","18","from sklearn.utils import check_array","710","def test_ecoc_delegate_sparse_base_estimator():","711","    # Non-regression test for","712","    # https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/17218","713","    X, y = iris.data, iris.target","714","    X_sp = sp.csc_matrix(X)","715","","716","    # create an estimator that does not support sparse input","717","    base_estimator = CheckingClassifier(","718","        check_X=check_array,","719","        check_X_params={\"ensure_2d\": True, \"accept_sparse\": False},","720","    )","721","    ecoc = OutputCodeClassifier(base_estimator, random_state=0)","722","","723","    with pytest.raises(TypeError, match=\"A sparse matrix was passed\"):","724","        ecoc.fit(X_sp, y)","725","","726","    ecoc.fit(X, y)","727","    with pytest.raises(TypeError, match=\"A sparse matrix was passed\"):","728","        ecoc.predict(X_sp)","729","","730","    # smoke test to check when sparse input should be supported","731","    ecoc = OutputCodeClassifier(LinearSVC(random_state=0))","732","    ecoc.fit(X_sp, y).predict(X_sp)","733","    assert len(ecoc.estimators_) == 4","734","","735",""],"delete":[]}],"sklearn\/multiclass.py":[{"add":["804","        X, y = self._validate_data(X, y, accept_sparse=True)","852","        X = check_array(X, accept_sparse=True)"],"delete":["804","        X, y = self._validate_data(X, y)","852","        X = check_array(X)"]}],"doc\/whats_new\/v0.24.rst":[{"add":["115",":mod:`sklearn.multiclass`","116",".........................","117","","118","- |Fix| A fix to allow :class:`multiclass.OutputCodeClassifier` to accept","119","  sparse input data in its `fit` and `predict` methods. The check for","120","  validity of the input is now delegated to the base estimator.","121","  :pr:`17233` by :user:`Zolisa Bleki <zoj613>`.","122","","123",""],"delete":[]}]}},"0fa32639fed0775375c720984dd10d2931245b8a":{"changes":{"sklearn\/manifold\/_locally_linear.py":"MODIFY","doc\/whats_new\/v0.24.rst":"MODIFY"},"diff":{"sklearn\/manifold\/_locally_linear.py":[{"add":["20","def barycenter_weights(X, Y, indices, reg=1e-3):","23","    We estimate the weights to assign to each point in Y[indices] to recover","30","    Y : array-like, shape (n_samples, n_dim)","31","","32","    indices : array-like, shape (n_samples, n_dim)","33","            Indices of the points in Y used to compute the barycenter","48","    Y = check_array(Y, dtype=FLOAT_DTYPES)","49","    indices = check_array(indices, dtype=int)","51","    n_samples, n_neighbors = indices.shape","52","    assert X.shape[0] == n_samples","53","","59","    for i, ind in enumerate(indices):","60","        A = Y[ind]","61","        C = A - X[i]  # broadcasting","68","        G.flat[::n_neighbors + 1] += R","110","    data = barycenter_weights(X, X, ind, reg=reg)","731","        weights = barycenter_weights(X, self.nbrs_._fit_X, ind, reg=self.reg)"],"delete":["20","def barycenter_weights(X, Z, reg=1e-3):","23","    We estimate the weights to assign to each point in Y[i] to recover","30","    Z : array-like, shape (n_samples, n_neighbors, n_dim)","45","    Z = check_array(Z, dtype=FLOAT_DTYPES, allow_nd=True)","47","    n_samples, n_neighbors = X.shape[0], Z.shape[1]","53","    for i, A in enumerate(Z.transpose(0, 2, 1)):","54","        C = A.T - X[i]  # broadcasting","61","        G.flat[::Z.shape[1] + 1] += R","66","","104","    data = barycenter_weights(X, X[ind], reg=reg)","725","        weights = barycenter_weights(X, self.nbrs_._fit_X[ind],","726","                                     reg=self.reg)"]}],"doc\/whats_new\/v0.24.rst":[{"add":["214","- |Efficiency| Fixed :issue:`10493`. Improve Local Linear Embedding (LLE) ","215","  that raised `MemoryError` exception when used with large inputs.","216","  :pr:`17997` by :user:`Bertrand Maisonneuve <bmaisonn>`.","217",""],"delete":[]}]}},"0e332293511a5b952039ad12defe626dfb023b67":{"changes":{"sklearn\/model_selection\/_validation.py":"MODIFY","sklearn\/model_selection\/tests\/test_validation.py":"MODIFY"},"diff":{"sklearn\/model_selection\/_validation.py":[{"add":["1541","    The aggregated output of _aggregate_score_dicts will be a list of dict","1561","    return {","1562","        key: np.asarray([score[key] for score in scores])","1563","        if isinstance(scores[0][key], numbers.Number)","1564","        else [score[key] for score in scores]","1565","        for key in scores[0]","1566","    }"],"delete":["1541","    The aggregated output of _fit_and_score will be a list of dict","1561","    return {key: np.asarray([score[key] for score in scores])","1562","            for key in scores[0]}"]}],"sklearn\/model_selection\/tests\/test_validation.py":[{"add":["358","def test_cross_validate_nested_estimator():","359","    # Non-regression test to ensure that nested","360","    # estimators are properly returned in a list","361","    # https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/17745","362","    (X, y) = load_iris(return_X_y=True)","363","    pipeline = Pipeline([","364","        (\"imputer\", SimpleImputer()),","365","        (\"classifier\", MockClassifier()),","366","    ])","367","","368","    results = cross_validate(pipeline, X, y, return_estimator=True)","369","    estimators = results[\"estimator\"]","370","","371","    assert isinstance(estimators, list)","372","    assert all(isinstance(estimator, Pipeline) for estimator in estimators)","373","","374",""],"delete":[]}]}},"c298cb771dfcc607d44c8219710f7cefaead39ea":{"changes":{"examples\/decomposition\/plot_ica_vs_pca.py":"MODIFY"},"diff":{"examples\/decomposition\/plot_ica_vs_pca.py":[{"add":["72","            plt.quiver((0, 0), (0, 0), x_axis, y_axis, zorder=11, width=0.01,","73","                       scale=6, color=color)"],"delete":["72","            plt.quiver(0, 0, x_axis, y_axis, zorder=11, width=0.01, scale=6,","73","                       color=color)"]}]}},"557218c021d58c7455b26820f331a9c388c9cb4d":{"changes":{"sklearn\/decomposition\/_truncated_svd.py":"MODIFY"},"diff":{"sklearn\/decomposition\/_truncated_svd.py":[{"add":["17","from ..utils.validation import check_is_fitted","215","        check_is_fitted(self)"],"delete":[]}]}},"c2b31ac21b8780498e11a42744212231b3fefaa6":{"changes":{"sklearn\/tree\/_classes.py":"MODIFY"},"diff":{"sklearn\/tree\/_classes.py":[{"add":["1154","    >>> from sklearn.datasets import load_diabetes","1157","    >>> X, y = load_diabetes(return_X_y=True)","1162","    array([-0.39..., -0.46...,  0.02...,  0.06..., -0.50...,","1163","           0.16...,  0.11..., -0.73..., -0.30..., -0.00...])","1699","    >>> from sklearn.datasets import load_diabetes","1703","    >>> X, y = load_diabetes(return_X_y=True)","1710","    0.33..."],"delete":["1154","    >>> from sklearn.datasets import load_boston","1157","    >>> X, y = load_boston(return_X_y=True)","1162","    array([ 0.61..., 0.57..., -0.34..., 0.41..., 0.75...,","1163","            0.07..., 0.29..., 0.33..., -1.42..., -1.77...])","1699","    >>> from sklearn.datasets import load_boston","1703","    >>> X, y = load_boston(return_X_y=True)","1710","    0.7447..."]}]}},"0ce21d4133d70774e6a8e794d479d8bd3469c9a2":{"changes":{"doc\/themes\/scikit-learn-modern\/static\/css\/theme.css":"MODIFY"},"diff":{"doc\/themes\/scikit-learn-modern\/static\/css\/theme.css":[{"add":["73","  white-space: nowrap;"],"delete":[]}]}},"3cb3d4109e7acc497ad1e306013547e5f72ee5f4":{"changes":{"sklearn\/cluster\/_affinity_propagation.py":"MODIFY","sklearn\/cluster\/tests\/test_affinity_propagation.py":"MODIFY","doc\/whats_new\/v0.24.rst":"MODIFY"},"diff":{"sklearn\/cluster\/_affinity_propagation.py":[{"add":["164","    S += ((np.finfo(S.dtype).eps * S + np.finfo(S.dtype).tiny * 100) *"],"delete":["164","    S += ((np.finfo(np.double).eps * S + np.finfo(np.double).tiny * 100) *"]}],"sklearn\/cluster\/tests\/test_affinity_propagation.py":[{"add":["233","","234","","235","def test_affinity_propagation_float32():","236","    # Test to fix incorrect clusters due to dtype change","237","    # (non-regression test for issue #10832)","238","    X = np.array([[1, 0, 0, 0],","239","                  [0, 1, 1, 0],","240","                  [0, 1, 1, 0],","241","                  [0, 0, 0, 1]], dtype='float32')","242","    afp = AffinityPropagation(preference=1, affinity='precomputed',","243","                              random_state=0).fit(X)","244","    expected = np.array([0, 1, 1, 2])","245","    assert_array_equal(afp.labels_, expected)"],"delete":[]}],"doc\/whats_new\/v0.24.rst":[{"add":["72","- |Fix| Fixed a bug in :class:`cluster.AffinityPropagation`, that","73","  gives incorrect clusters when the array dtype is float32.","74","  :pr:`17995` by :user:`Thomaz Santana  <Wikilicious>` and :user:`Amanda Dsouza <amy12xx>`.","75",""],"delete":[]}]}}}