{"0cfe98b9f81463143675793e5b4b11268b2cf857":{"changes":{"sklearn\/ensemble\/_hist_gradient_boosting\/gradient_boosting.py":"MODIFY"},"diff":{"sklearn\/ensemble\/_hist_gradient_boosting\/gradient_boosting.py":[{"add":["998","class HistGradientBoostingClassifier(ClassifierMixin,","999","                                     BaseHistGradientBoosting):"],"delete":["998","class HistGradientBoostingClassifier(BaseHistGradientBoosting,","999","                                     ClassifierMixin):"]}]}},"2592edddb6eb326a65311f081e668d93ab044703":{"changes":{"sklearn\/exceptions.py":"MODIFY"},"diff":{"sklearn\/exceptions.py":[{"add":["100","    ...     print(w[-1].message)","101","    A column-vector y was passed when a 1d array was expected. Please change","102","    the shape of y to (n_samples, ), for example using ravel()."],"delete":["100","    ...     print(repr(w[-1].message))","101","    DataConversionWarning('A column-vector y was passed when a","102","    1d array was expected. Please change the shape of y to","103","    (n_samples, ), for example using ravel().')"]}]}},"cf38502ec47ec1ff4609198573e746a6bfc25725":{"changes":{"sklearn\/inspection\/_plot\/partial_dependence.py":"MODIFY","sklearn\/inspection\/_partial_dependence.py":"MODIFY"},"diff":{"sklearn\/inspection\/_plot\/partial_dependence.py":[{"add":["91","    target : int, default=None","100","    response_method : {'auto', 'predict_proba', 'decision_function'}, \\","101","        default='auto'","110","    n_cols : int, default=3","114","    grid_resolution : int, default=100","118","    percentiles : tuple of float, default=(0.05, 0.95)","122","    method :{'auto', 'recursion', 'brute'}, default='auto'","146","    n_jobs : int, default=None","152","    verbose : int, default=0"],"delete":["91","    target : int, optional (default=None)","100","    response_method : 'auto', 'predict_proba' or 'decision_function', \\","101","            optional (default='auto')","110","    n_cols : int, optional (default=3)","114","    grid_resolution : int, optional (default=100)","118","    percentiles : tuple of float, optional (default=(0.05, 0.95))","122","    method : str, optional (default='auto')","146","    n_jobs : int, optional (default=None)","152","    verbose : int, optional (default=0)"]}],"sklearn\/inspection\/_partial_dependence.py":[{"add":["230","    response_method : {'auto', 'predict_proba', 'decision_function'}, \\","231","         default='auto'","240","    percentiles : tuple of float, default=(0.05, 0.95)","244","    grid_resolution : int, default=100","248","    method : {'auto', 'recursion', 'brute'}, default='auto'"],"delete":["230","    response_method : 'auto', 'predict_proba' or 'decision_function', \\","231","            optional (default='auto')","240","    percentiles : tuple of float, optional (default=(0.05, 0.95))","244","    grid_resolution : int, optional (default=100)","248","    method : str, optional (default='auto')"]}]}},"b229a663066f581706d47cfec85d4ba6358d99b4":{"changes":{"sklearn\/metrics\/_classification.py":"MODIFY","sklearn\/metrics\/tests\/test_classification.py":"MODIFY","doc\/whats_new\/v0.24.rst":"MODIFY"},"diff":{"sklearn\/metrics\/_classification.py":[{"add":["2348","    y_true_unique = np.unique(labels if labels is not None else y_true)"],"delete":["2348","    y_true_unique = np.unique(y_true)"]}],"sklearn\/metrics\/tests\/test_classification.py":[{"add":["2121","            dummy_hinge_loss)","2158","            dummy_hinge_loss)","2159","","2160","","2161","def test_hinge_loss_multiclass_missing_labels_only_two_unq_in_y_true():","2162","    # non-regression test for:","2163","    # https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/17630","2164","    # check that we can compute the hinge loss when providing an array","2165","    # with labels allowing to not have all labels in y_true","2166","    pred_decision = np.array([","2167","        [+0.36, -0.17, -0.58],","2168","        [-0.15, -0.58, -0.48],","2169","        [-1.45, -0.58, -0.38],","2170","        [-0.55, -0.78, -0.42],","2171","        [-1.45, -0.58, -0.38]","2172","    ])","2173","    y_true = np.array([0, 2, 2, 0, 2])","2174","    labels = np.array([0, 1, 2])","2175","    dummy_losses = np.array([","2176","        1 - pred_decision[0][0] + pred_decision[0][1],","2177","        1 - pred_decision[1][2] + pred_decision[1][0],","2178","        1 - pred_decision[2][2] + pred_decision[2][1],","2179","        1 - pred_decision[3][0] + pred_decision[3][2],","2180","        1 - pred_decision[4][2] + pred_decision[4][1]","2181","    ])","2182","    np.clip(dummy_losses, 0, None, out=dummy_losses)","2183","    dummy_hinge_loss = np.mean(dummy_losses)","2184","    assert_almost_equal(","2185","        hinge_loss(y_true, pred_decision, labels=labels),","2186","        dummy_hinge_loss","2187","    )"],"delete":["2121","                 dummy_hinge_loss)","2158","                 dummy_hinge_loss)"]}],"doc\/whats_new\/v0.24.rst":[{"add":["328","- |Fix| bug in :func:`metrics.hinge_loss` where error occurs when","329","  ``y_true`` is missing some labels that are provided explictly in the","330","  ``labels`` parameter.","331","  :pr:`17935` by :user:`Cary Goltermann <Ultramann>`.","332",""],"delete":[]}]}},"7536b25bd3793ec54a65114a839995cd352a1f66":{"changes":{"sklearn\/metrics\/_plot\/base.py":"MODIFY","sklearn\/metrics\/_plot\/roc_curve.py":"MODIFY","sklearn\/metrics\/_plot\/precision_recall_curve.py":"MODIFY"},"diff":{"sklearn\/metrics\/_plot\/base.py":[{"add":["0","def _check_classifier_response_method(estimator, response_method):"],"delete":["0","def _check_classifer_response_method(estimator, response_method):"]}],"sklearn\/metrics\/_plot\/roc_curve.py":[{"add":["3","from .base import _check_classifier_response_method","183","    prediction_method = _check_classifier_response_method(estimator,"],"delete":["3","from .base import _check_classifer_response_method","183","    prediction_method = _check_classifer_response_method(estimator,"]}],"sklearn\/metrics\/_plot\/precision_recall_curve.py":[{"add":["0","from .base import _check_classifier_response_method","176","    prediction_method = _check_classifier_response_method(estimator,"],"delete":["0","from .base import _check_classifer_response_method","176","    prediction_method = _check_classifer_response_method(estimator,"]}]}},"ad04857090542dfd1b6cc0cf3d5eb4cc2c5f1848":{"changes":{"sklearn\/neighbors\/_quad_tree.pxd":"MODIFY"},"diff":{"sklearn\/neighbors\/_quad_tree.pxd":[{"add":["84","                        float squared_theta=*, SIZE_t cell_id=*, long idx=*"],"delete":["84","                        float squared_theta=*, int cell_id=*, long idx=*"]}]}},"3fb9e758a57455fc16847cc8d9452147f25d43a0":{"changes":{"sklearn\/impute\/tests\/test_impute.py":"MODIFY","sklearn\/impute\/_base.py":"MODIFY","doc\/whats_new\/v0.24.rst":"MODIFY"},"diff":{"sklearn\/impute\/tests\/test_impute.py":[{"add":["1345","    'strategy, expected',","1346","    [('most_frequent', 'b'), ('constant', 'missing_value')]","1347",")","1348","def test_simple_imputation_string_list(strategy, expected):","1349","    X = [['a', 'b'],","1350","         ['c', np.nan]]","1351","","1352","    X_true = np.array([","1353","        ['a', 'b'],","1354","        ['c', expected]","1355","    ], dtype=object)","1356","","1357","    imputer = SimpleImputer(strategy=strategy)","1358","    X_trans = imputer.fit_transform(X)","1359","","1360","    assert_array_equal(X_trans, X_true)","1361","","1362","","1363","@pytest.mark.parametrize("],"delete":[]}],"sklearn\/impute\/_base.py":[{"add":["230","            # If input is a list of strings, dtype = object.","231","            # Otherwise ValueError is raised in SimpleImputer","232","            # with strategy='most_frequent' or 'constant'","233","            # because the list is converted to Unicode numpy array","234","            if isinstance(X, list) and \\","235","               any(isinstance(elem, str) for row in X for elem in row):","236","                dtype = object","237","            else:","238","                dtype = None"],"delete":["230","            dtype = None"]}],"doc\/whats_new\/v0.24.rst":[{"add":["80","- |Feature| :class:`impute.SimpleImputer` now supports a list of strings","81","  when ``strategy='most_frequent'`` or ``strategy='constant'``.","82","  :pr:`17526` by :user:`Ayako YAGI <yagi-3>` and :user:`Juan Carlos Alfaro Jim¨¦nez <alfaro96>`.","83",""],"delete":[]}]}},"bbab854ff4338ba02319c3ac2568cc2fb42cc976":{"changes":{"doc\/themes\/scikit-learn-modern\/nav.html":"MODIFY"},"diff":{"doc\/themes\/scikit-learn-modern\/nav.html":[{"add":["11","  (\"What's new\", pathto('whats_new\/v' + version)),"],"delete":["11","  (\"What's new\", 'whats_new\/v' + version + '.html'),"]}]}},"c4c0fe2ce4e61ba5bf8f6b3f03ac6ea8f99cf688":{"changes":{"sklearn\/svm\/setup.py":"MODIFY"},"diff":{"sklearn\/svm\/setup.py":[{"add":["19","                         # Use C++11 random number generator fix","20","                         extra_compile_args=['-std=c++11']"],"delete":[]}]}},"5abd22f58f152a0a899f33bb22609cc085fbfdec":{"changes":{"sklearn\/utils\/estimator_checks.py":"MODIFY"},"diff":{"sklearn\/utils\/estimator_checks.py":[{"add":["367","            estimator = _construct_instance(estimator)"],"delete":["367","            estimator = _construct_instance(estimator),"]}]}},"576775d6d15b7b69aebd2c464d932c28722eb4a0":{"changes":{"sklearn\/manifold\/tests\/test_mds.py":"MODIFY","doc\/whats_new\/v0.24.rst":"MODIFY","sklearn\/manifold\/_mds.py":"MODIFY"},"diff":{"sklearn\/manifold\/tests\/test_mds.py":[{"add":["64","","65","","66","@pytest.mark.parametrize(\"dissimilarity, expected_pairwise\", [","67","   (\"precomputed\", True),","68","   (\"euclidean\", False),","69","])","70","def test_MDS_pairwise(dissimilarity, expected_pairwise):","71","    # _pairwise attribute is set correctly","72","    mds_clf = mds.MDS(dissimilarity=dissimilarity)","73","    assert mds_clf._pairwise == expected_pairwise"],"delete":[]}],"doc\/whats_new\/v0.24.rst":[{"add":["289","- |Fix| :class:`manifold.MDS` now correctly sets its `_pairwise` attribute.","290","  :pr:`18278` by `Thomas Fan`_.","291","","369","  a estimator for multiclass wrappers.","380","- |Enhancement| :class:`multioutput.MultiOutputClassifier` and","384","  estimators) can be used as a estimator for multiclass wrappers."],"delete":["366","  a estimator for multiclass wrappers. ","377","- |Enhancement| :class:`multioutput.MultiOutputClassifier` and ","381","  estimators) can be used as a estimator for multiclass wrappers. "]}],"sklearn\/manifold\/_mds.py":[{"add":["388","        return self.dissimilarity == \"precomputed\""],"delete":["388","        return self.kernel == \"precomputed\""]}]}},"f734e11a53d3f9b983529fc22b877ef041e35cda":{"changes":{"sklearn\/decomposition\/tests\/test_nmf.py":"MODIFY","doc\/modules\/decomposition.rst":"MODIFY","sklearn\/decomposition\/_nmf.py":"MODIFY","doc\/whats_new\/v0.24.rst":"MODIFY"},"diff":{"sklearn\/decomposition\/tests\/test_nmf.py":[{"add":["22","@pytest.mark.parametrize('regularization',","23","                         [None, 'both', 'components', 'transformation'])","24","def test_convergence_warning(solver, regularization):","29","        NMF(solver=solver, regularization=regularization, max_iter=1).fit(A)","48","    msg = \"Invalid regularization parameter: got 'spam' instead of one of\"","49","    assert_raise_message(ValueError, msg, NMF(regularization=name).fit, A)","103","@pytest.mark.parametrize('solver', ('cd', 'mu'))","104","@pytest.mark.parametrize('init',","105","                         (None, 'nndsvd', 'nndsvda', 'nndsvdar', 'random'))","106","@pytest.mark.parametrize('regularization',","107","                         (None, 'both', 'components', 'transformation'))","108","def test_nmf_fit_nn_output(solver, init, regularization):","112","    model = NMF(n_components=2, solver=solver, init=init,","113","                regularization=regularization, random_state=0)","114","    transf = model.fit_transform(A)","115","    assert not((model.components_ < 0).any() or","116","               (transf < 0).any())","120","@pytest.mark.parametrize('regularization',","121","                         (None, 'both', 'components', 'transformation'))","122","def test_nmf_fit_close(solver, regularization):","126","               regularization=regularization, max_iter=600)","132","@pytest.mark.parametrize('regularization',","133","                         (None, 'both', 'components', 'transformation'))","134","def test_nmf_transform(solver, regularization):","139","            regularization=regularization, random_state=0, tol=1e-5)","161","@pytest.mark.parametrize('regularization',","162","                         (None, 'both', 'components', 'transformation'))","163","def test_nmf_inverse_transform(solver, regularization):","168","            regularization=regularization, max_iter=1000)","182","@pytest.mark.parametrize('regularization',","183","                         [None, 'both', 'components', 'transformation'])","184","def test_nmf_sparse_input(solver, regularization):","194","               regularization=regularization, random_state=0,","195","               tol=1e-2)","222","@pytest.mark.parametrize('init', ['random', 'nndsvd'])","223","@pytest.mark.parametrize('solver', ('cd', 'mu'))","224","@pytest.mark.parametrize('regularization',","225","                         (None, 'both', 'components', 'transformation'))","226","def test_non_negative_factorization_consistency(init, solver, regularization):","233","    W_nmf, H, _ = non_negative_factorization(","234","        A, init=init, solver=solver,","235","        regularization=regularization, random_state=1, tol=1e-2)","236","    W_nmf_2, _, _ = non_negative_factorization(","237","        A, H=H, update_H=False, init=init, solver=solver,","238","        regularization=regularization, random_state=1, tol=1e-2)","240","    model_class = NMF(init=init, solver=solver,","241","                      regularization=regularization,","242","                      random_state=1, tol=1e-2)","243","    W_cls = model_class.fit_transform(A)","244","    W_cls_2 = model_class.transform(A)","246","    assert_array_almost_equal(W_nmf, W_cls, decimal=10)","247","    assert_array_almost_equal(W_nmf_2, W_cls_2, decimal=10)","537","@pytest.mark.parametrize(\"regularization\",","538","                         (None, \"both\", \"components\", \"transformation\"))","539","def test_nmf_dtype_match(dtype_in, dtype_out, solver, regularization):","543","    nmf = NMF(solver=solver, regularization=regularization)","551","@pytest.mark.parametrize(\"regularization\",","552","                         (None, \"both\", \"components\", \"transformation\"))","553","def test_nmf_float32_float64_consistency(solver, regularization):","557","    nmf32 = NMF(solver=solver, regularization=regularization, random_state=0)","559","    nmf64 = NMF(solver=solver, regularization=regularization, random_state=0)"],"delete":["22","def test_convergence_warning(solver):","27","        NMF(solver=solver, max_iter=1).fit(A)","99","def test_nmf_fit_nn_output():","103","    for solver in ('cd', 'mu'):","104","        for init in (None, 'nndsvd', 'nndsvda', 'nndsvdar', 'random'):","105","            model = NMF(n_components=2, solver=solver, init=init,","106","                        random_state=0)","107","            transf = model.fit_transform(A)","108","            assert not((model.components_ < 0).any() or","109","                       (transf < 0).any())","113","def test_nmf_fit_close(solver):","117","               max_iter=600)","123","def test_nmf_transform(solver):","128","            random_state=0, tol=1e-5)","150","def test_nmf_inverse_transform(solver):","155","            max_iter=1000)","169","def test_nmf_sparse_input(solver):","179","               random_state=0, tol=1e-2)","206","def test_non_negative_factorization_consistency():","213","    for init in ['random', 'nndsvd']:","214","        for solver in ('cd', 'mu'):","215","            W_nmf, H, _ = non_negative_factorization(","216","                A, init=init, solver=solver, random_state=1, tol=1e-2)","217","            W_nmf_2, _, _ = non_negative_factorization(","218","                A, H=H, update_H=False, init=init, solver=solver,","219","                random_state=1, tol=1e-2)","221","            model_class = NMF(init=init, solver=solver, random_state=1,","222","                              tol=1e-2)","223","            W_cls = model_class.fit_transform(A)","224","            W_cls_2 = model_class.transform(A)","226","            assert_array_almost_equal(W_nmf, W_cls, decimal=10)","227","            assert_array_almost_equal(W_nmf_2, W_cls_2, decimal=10)","517","def test_nmf_dtype_match(dtype_in, dtype_out, solver):","521","    nmf = NMF(solver=solver)","529","def test_nmf_float32_float64_consistency(solver):","533","    nmf32 = NMF(solver=solver, random_state=0)","535","    nmf64 = NMF(solver=solver, random_state=0)"]}],"doc\/modules\/decomposition.rst":[{"add":["761",":class:`NMF` regularizes both W and H by default. The :attr:`regularization`","762","parameter allows for finer control, with which only W, only H,","763","or both can be regularized."],"delete":["761",":class:`NMF` regularizes both W and H. The public function","762",":func:`non_negative_factorization` allows a finer control through the","763",":attr:`regularization` attribute, and may regularize only W, only H, or both."]}],"sklearn\/decomposition\/_nmf.py":[{"add":["1083","    \"\"\"Non-Negative Matrix Factorization (NMF)","1099","        ||A||_Fro^2 = \\\\sum_{i,j} A_{ij}^2 (Frobenius norm)","1100","        ||vec(A)||_1 = \\\\sum_{i,j} abs(A_{ij}) (Elementwise L1 norm)","1200","    regularization : {'both', 'components', 'transformation', None}, \\","1201","                     default='both'","1202","        Select whether the regularization affects the components (H), the","1203","        transformation (W), both or none of them.","1204","","1205","        .. versionadded:: 0.24","1206","","1248","                 shuffle=False, regularization='both'):","1260","        self.regularization = regularization","1295","            l1_ratio=self.l1_ratio, regularization=self.regularization,","1344","            alpha=self.alpha, l1_ratio=self.l1_ratio,","1345","            regularization=self.regularization,","1346","            random_state=self.random_state,","1347","            verbose=self.verbose, shuffle=self.shuffle)"],"delete":["1083","    r\"\"\"Non-Negative Matrix Factorization (NMF)","1099","        ||A||_Fro^2 = \\sum_{i,j} A_{ij}^2 (Frobenius norm)","1100","        ||vec(A)||_1 = \\sum_{i,j} abs(A_{ij}) (Elementwise L1 norm)","1241","                 shuffle=False):","1287","            l1_ratio=self.l1_ratio, regularization='both',","1336","            alpha=self.alpha, l1_ratio=self.l1_ratio, regularization='both',","1337","            random_state=self.random_state, verbose=self.verbose,","1338","            shuffle=self.shuffle)"]}],"doc\/whats_new\/v0.24.rst":[{"add":["116","- |Enhancement| :class:`decomposition.NMF` now supports the optional parameter","117","  `regularization`, which can take the values `None`, `components`,","118","  `transformation` or `both`, in accordance with","119","  :func:`decomposition.NMF.non_negative_factorization`.","120","  :pr:`17414` by :user:`Bharat Raghunathan <Bharat123rox>`.","121",""],"delete":[]}]}},"f7a8dd2877c90249506ac8ad0a3eea7306636105":{"changes":{"sklearn\/multioutput.py":"MODIFY","doc\/whats_new\/v0.23.rst":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY"},"diff":{"sklearn\/multioutput.py":[{"add":["148","            .. versionadded:: 0.23","149","","240","        .. versionchanged:: 0.20","309","        .. versionchanged:: 0.20","352","            .. versionadded:: 0.23","353","","454","            .. versionadded:: 0.23","455","","826","            .. versionadded:: 0.23","827",""],"delete":["238","        .. versionchanged:: v0.20","307","        .. versionchanged:: v0.20"]}],"doc\/whats_new\/v0.23.rst":[{"add":["616","- |Feature| :func:`multioutput.MultiOutputRegressor.fit` and","617","  :func:`multioutput.MultiOutputClassifier.fit` now can accept `fit_params`","618","  to pass to the `estimator.fit` method of each step. :issue:`15953`","619","  :pr:`15959` by :user:`Ke Huang <huangk10>`.","620",""],"delete":[]}],"doc\/whats_new\/v0.22.rst":[{"add":[],"delete":["153",":mod:`sklearn.multioutput`","154","..........................","155","","156","- |Feature| :func:`multioutput.MultiOutputRegressor.fit` and","157","  :func:`multioutput.MultiOutputClassifier.fit` now can accept `fit_params`","158","  to pass to the `estimator.fit` method of each step. :issue:`15953`","159","  :pr:`15959` by :user:`Ke Huang <huangk10>`.","160",""]}]}},"591146d9329660334f52708dcabb466a7e0d78ce":{"changes":{"sklearn\/cluster\/_mean_shift.py":"MODIFY","doc\/whats_new\/v0.24.rst":"MODIFY","sklearn\/cluster\/tests\/test_mean_shift.py":"MODIFY"},"diff":{"sklearn\/cluster\/_mean_shift.py":[{"add":["220","    if bin_size == 0:","221","        return X"],"delete":[]}],"doc\/whats_new\/v0.24.rst":[{"add":["56",".......................","57","","58","- |Fix| Fixed a bug in :class:`cluster.MeanShift` with `bin_seeding=True`. When","59","  the estimated bandwidth is 0, the behavior is equivalent to","60","  `bin_seeding=False`.","61","  :pr:`17742` by :user:`Jeremie du Boisberranger <jeremiedbb>`."],"delete":["56","........................."]}],"sklearn\/cluster\/tests\/test_mean_shift.py":[{"add":["14","from sklearn.utils._testing import assert_allclose","21","from sklearn.metrics import v_measure_score","172","","173","","174","def test_mean_shift_zero_bandwidth():","175","    # Check that mean shift works when the estimated bandwidth is 0.","176","    X = np.array([1, 1, 1, 2, 2, 2, 3, 3]).reshape(-1, 1)","177","","178","    # estimate_bandwidth with default args returns 0 on this dataset","179","    bandwidth = estimate_bandwidth(X)","180","    assert bandwidth == 0","181","","182","    # get_bin_seeds with a 0 bin_size should return the dataset itself","183","    assert get_bin_seeds(X, bin_size=bandwidth) is X","184","","185","    # MeanShift with binning and a 0 estimated bandwidth should be equivalent","186","    # to no binning.","187","    ms_binning = MeanShift(bin_seeding=True, bandwidth=None).fit(X)","188","    ms_nobinning = MeanShift(bin_seeding=False).fit(X)","189","    expected_labels = np.array([0, 0, 0, 1, 1, 1, 2, 2])","190","","191","    assert v_measure_score(ms_binning.labels_, expected_labels) == 1","192","    assert v_measure_score(ms_nobinning.labels_, expected_labels) == 1","193","    assert_allclose(ms_binning.cluster_centers_, ms_nobinning.cluster_centers_)"],"delete":[]}]}},"5297365ebec6c172060eb19462894816384b6674":{"changes":{"sklearn\/feature_selection\/_rfe.py":"MODIFY","sklearn\/feature_selection\/_from_model.py":"MODIFY","sklearn\/feature_selection\/_univariate_selection.py":"MODIFY","sklearn\/feature_selection\/_mutual_info.py":"MODIFY","sklearn\/feature_selection\/_variance_threshold.py":"MODIFY","sklearn\/feature_selection\/_base.py":"MODIFY"},"diff":{"sklearn\/feature_selection\/_rfe.py":[{"add":["59","    n_features_to_select : int or None, default=None","63","    step : int or float, default=1","69","    verbose : int, default=0","370","    step : int or float, default=1","378","    min_features_to_select : int, default=1","386","    cv : int, cross-validation generator or an iterable, default=None","406","    scoring : string, callable or None, default=None","411","    verbose : int, default=0","414","    n_jobs : int or None, default=None","422","    importance_getter : str or callable, default='auto'","526","        groups : array-like of shape (n_samples,) or None, default=None"],"delete":["59","    n_features_to_select : int or None (default=None)","63","    step : int or float, optional (default=1)","69","    verbose : int, (default=0)","370","    step : int or float, optional (default=1)","378","    min_features_to_select : int, (default=1)","386","    cv : int, cross-validation generator or an iterable, optional","406","    scoring : string, callable or None, optional, (default=None)","411","    verbose : int, (default=0)","414","    n_jobs : int or None, optional (default=None)","422","    importance_getter : str or callable, optional (default='auto')","526","        groups : array-like of shape (n_samples,) or None"]}],"sklearn\/feature_selection\/_from_model.py":[{"add":["75","    threshold : string or float, default=None","85","    prefit : bool, default=False","93","    norm_order : non-zero int, inf, -inf, default=1","98","    max_features : int, default=None","104","    importance_getter : str or callable, default='auto'","200","        y : array-like of shape (n_samples,), default=None","244","        y : array-like of shape (n_samples,), default=None"],"delete":["75","    threshold : string, float, optional default None","85","    prefit : bool, default False","93","    norm_order : non-zero int, inf, -inf, default 1","98","    max_features : int or None, optional","104","    importance_getter : str or callable, optional (default='auto')","200","        y : array-like, shape (n_samples,)","244","        y : array-like, shape (n_samples,)"]}],"sklearn\/feature_selection\/_univariate_selection.py":[{"add":["256","    center : bool, default=True","381","    score_func : callable, default=f_classif","389","    percentile : int, default=10","427","    GenericUnivariateSelect: Univariate feature selector with configurable","428","        mode.","467","    score_func : callable, default=f_classif","475","    k : int or \"all\", default=10","510","    SelectPercentile: Select features based on percentile of the highest","511","        scores.","515","    GenericUnivariateSelect: Univariate feature selector with configurable","516","        mode.","556","    score_func : callable, default=f_classif","562","    alpha : float, default=5e-2","591","    SelectPercentile: Select features based on percentile of the highest","592","        scores.","596","    GenericUnivariateSelect: Univariate feature selector with configurable","597","        mode.","620","    score_func : callable, default=f_classif","626","    alpha : float, default=5e-2","659","    SelectPercentile: Select features based on percentile of the highest","660","        scores.","664","    GenericUnivariateSelect: Univariate feature selector with configurable","665","        mode.","691","    score_func : callable, default=f_classif","697","    alpha : float, default=5e-2","724","    SelectPercentile: Select features based on percentile of the highest","725","        scores.","729","    GenericUnivariateSelect: Univariate feature selector with configurable","730","        mode.","756","    score_func : callable, default=f_classif","761","    mode : {'percentile', 'k_best', 'fpr', 'fdr', 'fwe'}, default='percentile'","764","    param : float or int depending on the feature selection mode, default=1e-5","794","    SelectPercentile: Select features based on percentile of the highest","795","        scores."],"delete":["256","    center : True, bool,","381","    score_func : callable","389","    percentile : int, optional, default=10","427","    GenericUnivariateSelect: Univariate feature selector with configurable mode.","466","    score_func : callable","474","    k : int or \"all\", optional, default=10","509","    SelectPercentile: Select features based on percentile of the highest scores.","513","    GenericUnivariateSelect: Univariate feature selector with configurable mode.","553","    score_func : callable","559","    alpha : float, optional","588","    SelectPercentile: Select features based on percentile of the highest scores.","592","    GenericUnivariateSelect: Univariate feature selector with configurable mode.","615","    score_func : callable","621","    alpha : float, optional","654","    SelectPercentile: Select features based on percentile of the highest scores.","658","    GenericUnivariateSelect: Univariate feature selector with configurable mode.","684","    score_func : callable","690","    alpha : float, optional","717","    SelectPercentile: Select features based on percentile of the highest scores.","721","    GenericUnivariateSelect: Univariate feature selector with configurable mode.","747","    score_func : callable","752","    mode : {'percentile', 'k_best', 'fpr', 'fdr', 'fwe'}","755","    param : float or int depending on the feature selection mode","785","    SelectPercentile: Select features based on percentile of the highest scores."]}],"sklearn\/feature_selection\/_mutual_info.py":[{"add":["173","    columns : iterable or None, default=None","204","    y : array-like of shape (n_samples,)","207","    discrete_features : {'auto', bool, array-like}, default='auto'","214","    discrete_target : bool, default=False","217","    n_neighbors : int, default=3","222","    copy : bool, default=True","226","    random_state : int, RandomState instance or None, default=None","315","    y : array-like of shape (n_samples,)","318","    discrete_features : {'auto', bool, array-like}, default='auto'","325","    n_neighbors : int, default=3","330","    copy : bool, default=True","334","    random_state : int, RandomState instance or None, default=None","352","       vice versa will usually give incorrect results, so be attentive about","353","       that.","359","    .. [1] `Mutual Information","360","           <https:\/\/en.wikipedia.org\/wiki\/Mutual_information>`_","395","    y : array-like of shape (n_samples,)","398","    discrete_features : {'auto', bool, array-like}, default='auto'","405","    n_neighbors : int, default=3","410","    copy : bool, default=True","414","    random_state : int, RandomState instance or None, default=None","432","       vice versa will usually give incorrect results, so be attentive about","433","       that.","439","    .. [1] `Mutual Information","440","           <https:\/\/en.wikipedia.org\/wiki\/Mutual_information>`_"],"delete":["173","    columns : iterable or None, default None","204","    y : array-like, shape (n_samples,)","207","    discrete_features : {'auto', bool, array-like}, default 'auto'","214","    discrete_target : bool, default False","217","    n_neighbors : int, default 3","222","    copy : bool, default True","226","    random_state : int, RandomState instance or None, optional, default None","315","    y : array-like, shape (n_samples,)","318","    discrete_features : {'auto', bool, array-like}, default 'auto'","325","    n_neighbors : int, default 3","330","    copy : bool, default True","334","    random_state : int, RandomState instance or None, optional, default None","352","       vice versa will usually give incorrect results, so be attentive about that.","358","    .. [1] `Mutual Information <https:\/\/en.wikipedia.org\/wiki\/Mutual_information>`_","393","    y : array-like, shape (n_samples,)","396","    discrete_features : {'auto', bool, array-like}, default 'auto'","403","    n_neighbors : int, default 3","408","    copy : bool, default True","412","    random_state : int, RandomState instance or None, optional, default None","430","       vice versa will usually give incorrect results, so be attentive about that.","436","    .. [1] `Mutual Information <https:\/\/en.wikipedia.org\/wiki\/Mutual_information>`_"]}],"sklearn\/feature_selection\/_variance_threshold.py":[{"add":["20","    threshold : float, default=0","58","        y : any, default=None"],"delete":["20","    threshold : float, optional","58","        y : any"]}],"sklearn\/feature_selection\/_base.py":[{"add":["34","        indices : bool, default=False"],"delete":["34","        indices : boolean (default False)"]}]}},"946fddec7b62215191524c3f950a41fe944d014c":{"changes":{"\/dev\/null":"DELETE","sklearn\/utils\/fixes.py":"MODIFY","sklearn\/linear_model\/tests\/test_bayes.py":"MODIFY","doc\/whats_new\/v0.23.rst":"MODIFY","sklearn\/linear_model\/_bayes.py":"MODIFY"},"diff":{"\/dev\/null":[{"add":[],"delete":[]}],"sklearn\/utils\/fixes.py":[{"add":[],"delete":["44","if sp_version >= (1, 3):","45","    # Preserves earlier default choice of pinvh cutoff `cond` value.","46","    # Can be removed once issue #14055 is fully addressed.","47","    from ..externals._scipy_linalg import pinvh","48","else:","49","    # mypy error: Name 'pinvh' already defined (possibly by an import)","50","    from scipy.linalg import pinvh  # type: ignore  # noqa","51",""]}],"sklearn\/linear_model\/tests\/test_bayes.py":[{"add":["9","import pytest","10","","163","    n_samples = 10","185","    # With the inputs above, ARDRegression prunes both of the two coefficients","186","    # in the first iteration. Hence, the expected shape of `sigma_` is (0, 0).","187","    assert clf.sigma_.shape == (0, 0)","204","@pytest.mark.parametrize('seed', range(100))","205","@pytest.mark.parametrize('n_samples, n_features', ((10, 100), (100, 10)))","206","def test_ard_accuracy_on_easy_problem(seed, n_samples, n_features):","212","    regressor = ARDRegression()","216","    assert abs_coef_error < 1e-10","249","","250","","251","@pytest.mark.parametrize('seed', range(10))","252","def test_update_sigma(seed):","253","    # make sure the two update_sigma() helpers are equivalent. The woodbury","254","    # formula is used when n_samples < n_features, and the other one is used","255","    # otherwise.","256","","257","    rng = np.random.RandomState(seed)","258","","259","    # set n_samples == n_features to avoid instability issues when inverting","260","    # the matrices. Using the woodbury formula would be unstable when","261","    # n_samples > n_features","262","    n_samples = n_features = 10","263","    X = rng.randn(n_samples, n_features)","264","    alpha = 1","265","    lmbda = np.arange(1, n_features + 1)","266","    keep_lambda = np.array([True] * n_features)","267","","268","    reg = ARDRegression()","269","","270","    sigma = reg._update_sigma(X, alpha, lmbda, keep_lambda)","271","    sigma_woodbury = reg._update_sigma_woodbury(X, alpha, lmbda, keep_lambda)","272","","273","    np.testing.assert_allclose(sigma, sigma_woodbury)"],"delete":["161","    n_samples = 4","183","    # With the inputs above, ARDRegression prunes one of the two coefficients","184","    # in the first iteration. Hence, the expected shape of `sigma_` is (1, 1).","185","    assert clf.sigma_.shape == (1, 1)","202","def test_ard_accuracy_on_easy_problem():","205","    # This particular seed seems to converge poorly in the failure-case","206","    # (scipy==1.3.0, sklearn==0.21.2)","207","    seed = 45","211","    regressor = ARDRegression(n_iter=600)","215","    # Expect an accuracy of better than 1E-4 in most cases -","216","    # Failure-case produces 0.16!","217","    assert abs_coef_error < 0.01"]}],"doc\/whats_new\/v0.23.rst":[{"add":["305","- |Fix| |Efficiency| :class:`linear_model.ARDRegression` is more stable and","306","  much faster when `n_samples > n_features`. It can now scale to hundreds of","307","  thousands of samples. The stability fix might imply changes in the number","308","  of non-zero coefficients and in the predicted output. :pr:`16849` by","309","  `Nicolas Hug`_.","310",""],"delete":[]}],"sklearn\/linear_model\/_bayes.py":[{"add":["14","from scipy.linalg import pinvh","561","        update_sigma = (self._update_sigma if n_samples >= n_features","562","                        else self._update_sigma_woodbury)","565","            sigma_ = update_sigma(X, alpha_, lambda_, keep_lambda)","597","            if not keep_lambda.any():","598","                break","599","","600","        if keep_lambda.any():","601","            # update sigma and mu using updated params from the last iteration","602","            sigma_ = update_sigma(X, alpha_, lambda_, keep_lambda)","603","            coef_ = update_coeff(X, y, coef_, alpha_, keep_lambda, sigma_)","604","        else:","605","            sigma_ = np.array([]).reshape(0, 0)","614","    def _update_sigma_woodbury(self, X, alpha_, lambda_, keep_lambda):","615","        # See slides as referenced in the docstring note","616","        # this function is used when n_samples < n_features and will invert","617","        # a matrix of shape (n_samples, n_samples) making use of the","618","        # woodbury formula:","619","        # https:\/\/en.wikipedia.org\/wiki\/Woodbury_matrix_identity","620","        n_samples = X.shape[0]","621","        X_keep = X[:, keep_lambda]","622","        inv_lambda = 1 \/ lambda_[keep_lambda].reshape(1, -1)","623","        sigma_ = pinvh(","624","            np.eye(n_samples) \/ alpha_ + np.dot(X_keep * inv_lambda, X_keep.T)","625","        )","626","        sigma_ = np.dot(sigma_, X_keep * inv_lambda)","627","        sigma_ = - np.dot(inv_lambda.reshape(-1, 1) * X_keep.T, sigma_)","628","        sigma_[np.diag_indices(sigma_.shape[1])] += 1. \/ lambda_[keep_lambda]","629","        return sigma_","630","","631","    def _update_sigma(self, X, alpha_, lambda_, keep_lambda):","632","        # See slides as referenced in the docstring note","633","        # this function is used when n_samples >= n_features and will","634","        # invert a matrix of shape (n_features, n_features)","635","        X_keep = X[:, keep_lambda]","636","        gram = np.dot(X_keep.T, X_keep)","637","        eye = np.eye(gram.shape[0])","638","        sigma_inv = lambda_[keep_lambda] * eye + alpha_ * gram","639","        sigma_ = pinvh(sigma_inv)","640","        return sigma_","641",""],"delete":["14","from ..utils.fixes import pinvh","556","        # Compute sigma and mu (using Woodbury matrix identity)","557","        def update_sigma(X, alpha_, lambda_, keep_lambda, n_samples):","558","            sigma_ = pinvh(np.eye(n_samples) \/ alpha_ +","559","                           np.dot(X[:, keep_lambda] *","560","                           np.reshape(1. \/ lambda_[keep_lambda], [1, -1]),","561","                           X[:, keep_lambda].T))","562","            sigma_ = np.dot(sigma_, X[:, keep_lambda] *","563","                            np.reshape(1. \/ lambda_[keep_lambda], [1, -1]))","564","            sigma_ = - np.dot(np.reshape(1. \/ lambda_[keep_lambda], [-1, 1]) *","565","                              X[:, keep_lambda].T, sigma_)","566","            sigma_.flat[::(sigma_.shape[1] + 1)] += 1. \/ lambda_[keep_lambda]","567","            return sigma_","568","","576","            sigma_ = update_sigma(X, alpha_, lambda_, keep_lambda, n_samples)","608","        # update sigma and mu using updated parameters from the last iteration","609","        sigma_ = update_sigma(X, alpha_, lambda_, keep_lambda, n_samples)","610","        coef_ = update_coeff(X, y, coef_, alpha_, keep_lambda, sigma_)"]}]}},"549514e4fdc59d925746b5dc9bd9383781923726":{"changes":{"examples\/feature_selection\/plot_feature_selection_pipeline.py":"MODIFY"},"diff":{"examples\/feature_selection\/plot_feature_selection_pipeline.py":[{"add":["13","from sklearn.feature_selection import SelectKBest, f_classif","29","anova_filter = SelectKBest(f_classif, k=3)"],"delete":["13","from sklearn.feature_selection import SelectKBest, f_regression","29","anova_filter = SelectKBest(f_regression, k=3)"]}]}},"e217b68fd00bb7c54b81a492ee6f9db6498517fa":{"changes":{"sklearn\/tree\/_tree.pyx":"MODIFY","doc\/whats_new\/v0.24.rst":"MODIFY"},"diff":{"sklearn\/tree\/_tree.pyx":[{"add":["654","","655","        if (node_ndarray.dtype != NODE_DTYPE):","656","            # possible mismatch of big\/little endian due to serialization","657","            # on a different architecture. Try swapping the byte order.  ","658","            node_ndarray = node_ndarray.byteswap().newbyteorder()","659","            if (node_ndarray.dtype != NODE_DTYPE):","660","                raise ValueError('Did not recognise loaded array dytpe')","661",""],"delete":["655","                node_ndarray.dtype != NODE_DTYPE or"]}],"doc\/whats_new\/v0.24.rst":[{"add":["393","- |Fix| Allow serialized tree based models to be unpickled on a machine","394","  with different endianness.","395","  :pr:`17644` by :user:`Qi Zhang <qzhang90>`.","396",""],"delete":[]}]}},"f2e873f41ee40b6028f19328cbdc73fc15cdb070":{"changes":{"sklearn\/utils\/_mocking.py":"MODIFY","sklearn\/utils\/tests\/test_mocking.py":"ADD"},"diff":{"sklearn\/utils\/_mocking.py":[{"add":["53","    Checks some property of `X` and `y`in fit \/ predict.","59","    check_y, check_X : callable, default=None","60","        The callable used to validate `X` and `y`. These callable should return","61","        a bool where `False` will trigger an `AssertionError`.","62","","63","    check_y_params, check_X_params : dict, default=None","64","        The optional parameters to pass to `check_X` and `check_y`.","65","","66","    foo_param : int, default=0","67","        A `foo` param. When `foo > 1`, the output of :meth:`score` will be 1","68","        otherwise it is 0.","69","","70","    expected_fit_params : list of str, default=None","71","        A list of the expected parameters given when calling `fit`.","75","    classes_ : int","76","        The classes seen during `fit`.","77","","78","    n_features_in_ : int","79","        The number of features seen during `fit`.","81","    def __init__(self, *, check_y=None, check_y_params=None,","82","                 check_X=None, check_X_params=None, foo_param=0,","85","        self.check_y_params = check_y_params","87","        self.check_X_params = check_X_params","92","        \"\"\"Fit classifier.","106","","107","        Returns","108","        -------","109","        self","111","        assert _num_samples(X) == _num_samples(y)","113","            params = {} if self.check_X_params is None else self.check_X_params","114","            assert self.check_X(X, **params)","116","            params = {} if self.check_y_params is None else self.check_y_params","118","        self.n_features_in_ = np.shape(X)[1]","119","        self.classes_ = np.unique(","120","            check_array(y, ensure_2d=False, allow_nd=True)","121","        )","124","            if missing:","125","                raise AssertionError(","126","                    f'Expected fit parameter(s) {list(missing)} not seen.'","127","                )","129","                if _num_samples(value) != _num_samples(X):","130","                    raise AssertionError(","131","                        f'Fit parameter {key} has length {_num_samples(value)}'","132","                        f'; expected {_num_samples(X)}.'","133","                    )","137","    def predict(self, X):","138","        \"\"\"Predict the first class seen in `classes_`.","139","","142","        X : array-like of shape (n_samples, n_features)","143","            The input data.","144","","145","        Returns","146","        -------","147","        preds : ndarray of shape (n_samples,)","148","            Predictions of the first class seens in `classes_`.","151","            params = {} if self.check_X_params is None else self.check_X_params","152","            assert self.check_X(X, **params)","153","        return self.classes_[np.zeros(_num_samples(X), dtype=np.int)]","154","","155","    def predict_proba(self, X):","156","        \"\"\"Predict probabilities for each class.","157","","158","        Here, the dummy classifier will provide a probability of 1 for the","159","        first class of `classes_` and 0 otherwise.","160","","161","        Parameters","162","        ----------","163","        X : array-like of shape (n_samples, n_features)","164","            The input data.","165","","166","        Returns","167","        -------","168","        proba : ndarray of shape (n_samples, n_classes)","169","            The probabilities for each sample and class.","170","        \"\"\"","171","        proba = np.zeros((_num_samples(X), len(self.classes_)))","172","        proba[:, 0] = 1","173","        return proba","174","","175","    def decision_function(self, X):","176","        \"\"\"Confidence score.","177","","178","        Parameters","179","        ----------","180","        X : array-like of shape (n_samples, n_features)","181","            The input data.","182","","183","        Returns","184","        -------","185","        decision : ndarray of shape (n_samples,) if n_classes == 2\\","186","                else (n_samples, n_classes)","187","            Confidence score.","188","        \"\"\"","189","        if len(self.classes_) == 2:","190","            # for binary classifier, the confidence score is related to","191","            # classes_[1] and therefore should be null.","192","            return np.zeros(_num_samples(X))","193","        else:","194","            return self.predict_proba(X)","197","        \"\"\"Fake score.","198","","205","        Y : array-like of shape (n_samples, n_output) or (n_samples,)","208","","209","        Returns","210","        -------","211","        score : float","212","            Either 0 or 1 depending of `foo_param` (i.e. `foo_param > 1 =>","213","            score=1` otherwise `score=0`)."],"delete":["53","    Checks some property of X and y in fit \/ predict.","59","    check_y","60","    check_X","61","    foo_param","62","    expected_fit_params","66","    classes_","68","    def __init__(self, check_y=None, check_X=None, foo_param=0,","76","        \"\"\"","77","        Fit classifier","92","        assert len(X) == len(y)","94","            assert self.check_X(X)","97","        self.n_features_in_ = len(X)","98","        self.classes_ = np.unique(check_array(y, ensure_2d=False,","99","                                              allow_nd=True))","102","            assert len(missing) == 0, 'Expected fit parameter(s) %s not ' \\","103","                                      'seen.' % list(missing)","105","                assert len(value) == len(X), (","106","                        'Fit parameter %s has length %d; '","107","                        'expected %d.'","108","                        % (key, len(value), len(X)))","112","    def predict(self, T):","113","        \"\"\"","116","        T : indexable, length n_samples","119","            assert self.check_X(T)","120","        return self.classes_[np.zeros(_num_samples(T), dtype=np.int)]","123","        \"\"\"","130","        Y : array-like of shape (n_samples, n_output) or (n_samples,), optional"]}],"sklearn\/utils\/tests\/test_mocking.py":[{"add":[],"delete":[]}]}},"28c1ed473f0738e28a6fd61c105da8ada4ab880d":{"changes":{"sklearn\/model_selection\/tests\/test_search.py":"MODIFY","doc\/whats_new\/v0.24.rst":"MODIFY","sklearn\/model_selection\/_search.py":"MODIFY"},"diff":{"sklearn\/model_selection\/tests\/test_search.py":[{"add":["1491","    assert len(ParameterSampler(params, n_iter=1000)) == 8"],"delete":[]}],"doc\/whats_new\/v0.24.rst":[{"add":["332","- |Fix| Fixed the `len` of :class:`model_selection.ParameterSampler` when","333","  all distributions are lists and `n_iter` is more than the number of unique","334","  parameter combinations. :pr:`18222` by `Nicolas Hug`_.","335",""],"delete":[]}],"sklearn\/model_selection\/_search.py":[{"add":["265","    def _is_all_lists(self):","266","        return all(","268","            for dist in self.param_distributions","269","        )","270","","271","    def __iter__(self):","274","        # if all distributions are given as lists, we want to sample without","275","        # replacement","276","        if self._is_all_lists():","308","        if self._is_all_lists():","309","            grid_size = len(ParameterGrid(self.param_distributions))","310","            return min(self.n_iter, grid_size)","311","        else:","312","            return self.n_iter"],"delete":["265","    def __iter__(self):","266","        # check if all distributions are given as lists","267","        # in this case we want to sample without replacement","268","        all_lists = all(","270","            for dist in self.param_distributions)","273","        if all_lists:","305","        return self.n_iter"]}]}},"b2bc40ca8b63e7e8efe7831a08286c2b539078eb":{"changes":{"sklearn\/decomposition\/_truncated_svd.py":"MODIFY"},"diff":{"sklearn\/decomposition\/_truncated_svd.py":[{"add":["214","        X = check_array(X, accept_sparse=['csr', 'csc'])"],"delete":["214","        X = check_array(X, accept_sparse='csr')"]}]}},"dba4514aa2ba2ea6f15f6cab6f090f4f952ad89a":{"changes":{"build_tools\/azure\/install.sh":"MODIFY"},"diff":{"build_tools\/azure\/install.sh":[{"add":["99","","100","    # TODO: Remove pin when https:\/\/github.com\/python-pillow\/Pillow\/issues\/4518 gets fixed","101","    python -m pip install \"pillow>=4.3.0,!=7.1.0,!=7.1.1\"","102",""],"delete":[]}]}},"831a00a55e72131634afd3565e7d8c8d8470afbe":{"changes":{"sklearn\/utils\/validation.py":"MODIFY"},"diff":{"sklearn\/utils\/validation.py":[{"add":["137","    force_all_finite : boolean or 'allow-nan', default=True","322","    force_all_finite : boolean or 'allow-nan', default=True","415","    accept_sparse : string, boolean or list\/tuple of strings, default=False","422","    accept_large_sparse : bool, default=True","429","    dtype : string, type, list of types or None, default=\"numeric\"","435","    order : 'F', 'C' or None, default=None","442","    copy : boolean, default=False","446","    force_all_finite : boolean or 'allow-nan', default=True","461","    ensure_2d : boolean, default=True","464","    allow_nd : boolean, default=False","467","    ensure_min_samples : int, default=1","471","    ensure_min_features : int, default=1","478","    estimator : str or estimator instance, default=None","712","    accept_sparse : string, boolean or list of string, default=False","719","    accept_large_sparse : bool, default=True","726","    dtype : string, type, list of types or None, default=\"numeric\"","732","    order : 'F', 'C' or None, default=None","735","    copy : boolean, default=False","739","    force_all_finite : boolean or 'allow-nan', default=True","755","    ensure_2d : boolean, default=True","758","    allow_nd : boolean, default=False","761","    multi_output : boolean, default=False","766","    ensure_min_samples : int, default=1","770","    ensure_min_features : int, default=1","777","    y_numeric : boolean, default=False","782","    estimator : str or estimator instance, default=None","915","    raise_warning : boolean, default=True","917","    raise_exception : boolean, default=False","990","    all_or_any : callable, {all, any}, default=all","1064","    min_val : float or int, default=None","1068","    max_val : float or int, default=None"],"delete":["137","    force_all_finite : boolean or 'allow-nan', (default=True)","322","    force_all_finite : boolean or 'allow-nan', (default=True)","415","    accept_sparse : string, boolean or list\/tuple of strings (default=False)","422","    accept_large_sparse : bool (default=True)","429","    dtype : string, type, list of types or None (default=\"numeric\")","435","    order : 'F', 'C' or None (default=None)","442","    copy : boolean (default=False)","446","    force_all_finite : boolean or 'allow-nan', (default=True)","461","    ensure_2d : boolean (default=True)","464","    allow_nd : boolean (default=False)","467","    ensure_min_samples : int (default=1)","471","    ensure_min_features : int (default=1)","478","    estimator : str or estimator instance (default=None)","712","    accept_sparse : string, boolean or list of string (default=False)","719","    accept_large_sparse : bool (default=True)","726","    dtype : string, type, list of types or None (default=\"numeric\")","732","    order : 'F', 'C' or None (default=None)","735","    copy : boolean (default=False)","739","    force_all_finite : boolean or 'allow-nan', (default=True)","755","    ensure_2d : boolean (default=True)","758","    allow_nd : boolean (default=False)","761","    multi_output : boolean (default=False)","766","    ensure_min_samples : int (default=1)","770","    ensure_min_features : int (default=1)","777","    y_numeric : boolean (default=False)","782","    estimator : str or estimator instance (default=None)","915","    raise_warning : boolean (default=True)","917","    raise_exception : boolean (default=False)","990","    all_or_any : callable, {all, any}, default all","1064","    min_val : float or int, optional (default=None)","1068","    max_val : float or int, optional (default=None)"]}]}},"8611b9a9af10b46cbf68bb7fc71a44a49ed6ec1f":{"changes":{"examples\/svm\/plot_linearsvc_support_vectors.py":"MODIFY"},"diff":{"examples\/svm\/plot_linearsvc_support_vectors.py":[{"add":["26","    # The support vectors are the samples that lie within the margin","27","    # boundaries, whose size is conventionally constrained to 1","28","    support_vector_indices = np.where(","29","        np.abs(decision_function) <= 1 + 1e-15)[0]"],"delete":["26","    support_vector_indices = np.where((2 * y - 1) * decision_function <= 1)[0]"]}]}},"a655de515e24515fff0007100dc9d49b5126b223":{"changes":{"doc\/whats_new\/v0.23.rst":"MODIFY","sklearn\/decomposition\/_pca.py":"MODIFY","sklearn\/decomposition\/tests\/test_pca.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.23.rst":[{"add":["141","","142","- |Fix| :class:`decomposition.PCA` with `n_components='mle'` now correctly","143","  handles small eigenvalues, and does not infer 0 as the correct number of","144","  components. :pr: `4441` by :user:`Lisa Schwetlick <lschwetlick>`, and","145","  :user:`Gelavizh Ahmadi <gelavizh1>` and :user:`Marija Vlajic Wheeler","146","  <marijavlajic>` and :pr:`16841` by `Nicolas Hug`_."],"delete":["141","- |Fix| :func:`decomposition._pca._assess_dimension` now correctly handles small","142","   eigenvalues. :pr: `4441` by :user:`Lisa Schwetlick <lschwetlick>`, and","143","   :user:`Gelavizh Ahmadi <gelavizh1>` and","144","   :user:`Marija Vlajic Wheeler <marijavlajic>`."]}],"sklearn\/decomposition\/_pca.py":[{"add":["30","def _assess_dimension(spectrum, rank, n_samples):","31","    \"\"\"Compute the log-likelihood of a rank ``rank`` dataset.","38","    spectrum : array of shape (n_features)","41","        Tested rank value. It should be strictly lower than n_features,","42","        otherwise the method isn't specified (division by zero in equation","43","        (31) from the paper).","58","    n_features = spectrum.shape[0]","59","    if not 1 <= rank < n_features:","60","        raise ValueError(\"the tested rank should be in [1, n_features - 1]\")","61","","62","    eps = 1e-15","63","","64","    if spectrum[rank - 1] < eps:","65","        # When the tested rank is associated with a small eigenvalue, there's","66","        # no point in computing the log-likelihood: it's going to be very","67","        # small and won't be the max anyway. Also, it can lead to numerical","68","        # issues below when computing pa, in particular in log((spectrum[i] -","69","        # spectrum[j]) because this will take the log of something very small.","70","        return -np.inf","73","    for i in range(1, rank + 1):","74","        pu += (gammaln((n_features - i + 1) \/ 2.) -","75","               log(np.pi) * (n_features - i + 1) \/ 2.)","80","    v = max(eps, np.sum(spectrum[rank:]) \/ (n_features - rank))","81","    pv = -np.log(v) * n_samples * (n_features - rank) \/ 2.","84","    pp = log(2. * np.pi) * (m + rank) \/ 2.","99","def _infer_dimension(spectrum, n_samples):","100","    \"\"\"Infers the dimension of a dataset with a given spectrum.","102","    The returned value will be in [1, n_features - 1].","104","    ll = np.empty_like(spectrum)","105","    ll[0] = -np.inf  # we don't want to return n_components = 0","106","    for rank in range(1, spectrum.shape[0]):","107","        ll[rank] = _assess_dimension(spectrum, rank, n_samples)","468","                _infer_dimension(explained_variance_, n_samples)"],"delete":["30","def _assess_dimension(spectrum, rank, n_samples, n_features):","31","    \"\"\"Compute the likelihood of a rank ``rank`` dataset.","38","    spectrum : array of shape (n)","41","        Tested rank value.","44","    n_features : int","45","        Number of features.","57","    if rank > len(spectrum):","58","        raise ValueError(\"The tested rank cannot exceed the rank of the\"","59","                         \" dataset\")","61","    spectrum_threshold = np.finfo(type(spectrum[0])).eps","64","    for i in range(rank):","65","        pu += (gammaln((n_features - i) \/ 2.) -","66","               log(np.pi) * (n_features - i) \/ 2.)","71","    if rank == n_features:","72","        # TODO: this line is never executed because _infer_dimension's","73","        # for loop is off by one","74","        pv = 0","75","        v = 1","76","    else:","77","        v = np.sum(spectrum[rank:]) \/ (n_features - rank)","78","        if spectrum_threshold > v:","79","            return -np.inf","80","        pv = -np.log(v) * n_samples * (n_features - rank) \/ 2.","83","    pp = log(2. * np.pi) * (m + rank + 1.) \/ 2.","89","        if spectrum_[i] < spectrum_threshold:","90","            # TODO: this line is never executed","91","            # (off by one in _infer_dimension)","92","            # this break only happens when rank == n_features and","93","            # spectrum_[i] < spectrum_threshold, otherwise the early return","94","            # above catches this case.","95","            break","105","def _infer_dimension(spectrum, n_samples, n_features):","106","    \"\"\"Infers the dimension of a dataset of shape (n_samples, n_features)","108","    The dataset is described by its spectrum `spectrum`.","110","    n_spectrum = len(spectrum)","111","    ll = np.empty(n_spectrum)","112","    for rank in range(n_spectrum):","113","        ll[rank] = _assess_dimension(spectrum, rank, n_samples, n_features)","474","                _infer_dimension(explained_variance_, n_samples, n_features)"]}],"sklearn\/decomposition\/tests\/test_pca.py":[{"add":["297","    assert pca.n_components_ == 1","335","    ll = np.array([_assess_dimension(spect, k, n) for k in range(1, p)])","350","    assert _infer_dimension(spect, n) > 1","363","    assert _infer_dimension(spect, n) > 2","572","def test_assess_dimension_bad_rank():","573","    # Test error when tested rank not in [1, n_features - 1]","576","    for rank in (0, 5):","577","        with pytest.raises(ValueError,","578","                           match=r\"should be in \\[1, n_features - 1\\]\"):","579","            _assess_dimension(spectrum, rank, n_samples)","582","def test_small_eigenvalues_mle():","583","    # Test rank associated with tiny eigenvalues are given a log-likelihood of","584","    # -inf. The inferred rank will be 1","586","","587","    assert _assess_dimension(spectrum, rank=1, n_samples=10) > -np.inf","588","","589","    for rank in (2, 3):","590","        assert _assess_dimension(spectrum, rank, 10) == -np.inf","591","","592","    assert _infer_dimension(spectrum, 10) == 1","595","def test_mle_redundant_data():","596","    # Test 'mle' with pathological X: only one relevant feature should give a","597","    # rank of 1","598","    X, _ = datasets.make_classification(n_features=20,","599","                                        n_informative=1, n_repeated=18,","603","    assert pca.n_components_ == 1","608","    # than the number of features during an mle fit","617","","618","","619","def test_mle_simple_case():","620","    # non-regression test for issue","621","    # https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/16730","622","    n_samples, n_dim = 1000, 10","623","    X = np.random.RandomState(0).randn(n_samples, n_dim)","624","    X[:, -1] = np.mean(X[:, :-1], axis=-1)  # true X dim is ndim - 1","625","    pca_skl = PCA('mle', svd_solver='full')","626","    pca_skl.fit(X)","627","    assert pca_skl.n_components_ == n_dim - 1","628","","629","","630","def test_assess_dimesion_rank_one():","631","    # Make sure assess_dimension works properly on a matrix of rank 1","632","    n_samples, n_features = 9, 6","633","    X = np.ones((n_samples, n_features))  # rank 1 matrix","634","    _, s, _ = np.linalg.svd(X, full_matrices=True)","635","    assert sum(s[1:]) == 0  # except for rank 1, all eigenvalues are 0","636","","637","    assert np.isfinite(_assess_dimension(s, rank=1, n_samples=n_samples))","638","    for rank in range(2, n_features):","639","        assert _assess_dimension(s, rank, n_samples) == -np.inf"],"delete":["297","    assert pca.n_components_ == 0","335","    ll = np.array([_assess_dimension(spect, k, n, p) for k in range(p)])","350","    assert _infer_dimension(spect, n, p) > 1","363","    assert _infer_dimension(spect, n, p) > 2","572","def test_infer_dim_bad_spec():","573","    # Test a spectrum that drops to near zero for PR #16224","576","    n_features = 5","577","    ret = _infer_dimension(spectrum, n_samples, n_features)","578","    assert ret == 0","581","def test_assess_dimension_error_rank_greater_than_features():","582","    # Test error when tested rank is greater than the number of features","583","    # for PR #16224","585","    n_samples = 10","586","    n_features = 4","587","    rank = 5","588","    with pytest.raises(ValueError, match=\"The tested rank cannot exceed \"","589","                                         \"the rank of the dataset\"):","590","        _assess_dimension(spectrum, rank, n_samples, n_features)","593","def test_assess_dimension_small_eigenvalues():","594","    # Test tiny eigenvalues appropriately when using 'mle'","595","    # for  PR #16224","596","    spectrum = np.array([1, 1e-30, 1e-30, 1e-30])","597","    n_samples = 10","598","    n_features = 5","599","    rank = 3","600","    ret = _assess_dimension(spectrum, rank, n_samples, n_features)","601","    assert ret == -np.inf","602","","603","","604","def test_infer_dim_mle():","605","    # Test small eigenvalues when 'mle' with pathological 'X' dataset","606","    # for PR #16224","607","    X, _ = datasets.make_classification(n_informative=1, n_repeated=18,","611","    assert pca.n_components_ == 0","616","    # than the number of features during an mle fit for PR #16224"]}]}}}