{"f72d9711f5242cd73f91551b75399dcfb88a5861":{"changes":{"sklearn\/tests\/test_docstring_parameters.py":"MODIFY","sklearn\/cluster\/_affinity_propagation.py":"MODIFY","sklearn\/cluster\/tests\/test_affinity_propagation.py":"MODIFY","doc\/whats_new\/v0.23.rst":"MODIFY"},"diff":{"sklearn\/tests\/test_docstring_parameters.py":[{"add":["200","    # TO BE REMOVED for v0.25 (avoid FutureWarning)","201","    if Estimator.__name__ == 'AffinityPropagation':","202","        est.random_state = 63","203",""],"delete":[]}],"sklearn\/cluster\/_affinity_propagation.py":[{"add":["12","from ..utils import as_float_array, check_array, check_random_state","34","                         return_n_iter=False, random_state='warn'):","74","    random_state : int or np.random.RandomStateInstance, default: 0","75","        Pseudo-random number generator to control the starting state.","76","        Use an int for reproducible results across function calls.","77","        See the :term:`Glossary <random_state>`.","78","","79","        .. versionadded:: 0.23","80","            this parameter was previously hardcoded as 0.","81","","143","    if random_state == 'warn':","144","        warnings.warn((\"'random_state' has been introduced in 0.23. \"","145","                       \"It will be set to None starting from 0.25 which \"","146","                       \"means that results will differ at every function \"","147","                       \"call. Set 'random_state' to None to silence this \"","148","                       \"warning, or to 0 to keep the behavior of versions \"","149","                       \"<0.23.\"),","150","                      FutureWarning)","151","        random_state = 0","152","    random_state = check_random_state(random_state)","293","    random_state : int or np.random.RandomStateInstance, default: 0","294","        Pseudo-random number generator to control the starting state.","295","        Use an int for reproducible results across function calls.","296","        See the :term:`Glossary <random_state>`.","297","","298","        .. versionadded:: 0.23","299","            this parameter was previously hardcoded as 0.","342","","343","    Examples","344","    --------","345","    >>> from sklearn.cluster import AffinityPropagation","346","    >>> import numpy as np","347","    >>> X = np.array([[1, 2], [1, 4], [1, 0],","348","    ...               [4, 2], [4, 4], [4, 0]])","349","    >>> clustering = AffinityPropagation(random_state=5).fit(X)","350","    >>> clustering","351","    AffinityPropagation(random_state=5)","352","    >>> clustering.labels_","353","    array([0, 0, 0, 1, 1, 1])","354","    >>> clustering.predict([[0, 0], [4, 4]])","355","    array([0, 1])","356","    >>> clustering.cluster_centers_","357","    array([[1, 2],","358","           [4, 2]])","363","                 verbose=False, random_state='warn'):","372","        self.random_state = random_state","415","                copy=self.copy, verbose=self.verbose, return_n_iter=True,","416","                random_state=self.random_state)"],"delete":["12","from ..utils import as_float_array, check_array","34","                         return_n_iter=False):","135","    random_state = np.random.RandomState(0)","294","    Examples","295","    --------","296","    >>> from sklearn.cluster import AffinityPropagation","297","    >>> import numpy as np","298","    >>> X = np.array([[1, 2], [1, 4], [1, 0],","299","    ...               [4, 2], [4, 4], [4, 0]])","300","    >>> clustering = AffinityPropagation().fit(X)","301","    >>> clustering","302","    AffinityPropagation()","303","    >>> clustering.labels_","304","    array([0, 0, 0, 1, 1, 1])","305","    >>> clustering.predict([[0, 0], [4, 4]])","306","    array([0, 1])","307","    >>> clustering.cluster_centers_","308","    array([[1, 2],","309","           [4, 2]])","310","","339","                 verbose=False):","390","                copy=self.copy, verbose=self.verbose, return_n_iter=True)"]}],"sklearn\/cluster\/tests\/test_affinity_propagation.py":[{"add":["35","        S, preference=preference, random_state=39)","41","    af = AffinityPropagation(preference=preference, affinity=\"precomputed\",","42","                             random_state=28)","45","    af = AffinityPropagation(preference=preference, verbose=True,","46","                             random_state=37)","59","                                             copy=False, random_state=74)","67","    af = AffinityPropagation(affinity=\"unknown\", random_state=78)","70","    af_2 = AffinityPropagation(affinity='precomputed', random_state=21)","76","    af = AffinityPropagation(affinity=\"euclidean\", random_state=63)","91","    af = AffinityPropagation(affinity=\"precomputed\", random_state=57)","104","    af = AffinityPropagation(preference=-10, max_iter=1, random_state=82)","133","        affinity_propagation, S, preference=[-20, -10], random_state=37)","147","                      AffinityPropagation(preference=-10,","148","                                          max_iter=1, random_state=75).fit, X)","161","    af = AffinityPropagation(affinity='euclidean',","162","                             max_iter=2, random_state=34).fit(X)","187","def test_affinity_propagation_random_state():","188","    # Significance of random_state parameter","189","    # Generate sample data","190","    centers = [[1, 1], [-1, -1], [1, -1]]","191","    X, labels_true = make_blobs(n_samples=300, centers=centers,","192","                                cluster_std=0.5, random_state=0)","193","    # random_state = 0","194","    ap = AffinityPropagation(convergence_iter=1, max_iter=2, random_state=0)","195","    ap.fit(X)","196","    centers0 = ap.cluster_centers_","197","","198","    # random_state = 76","199","    ap = AffinityPropagation(convergence_iter=1, max_iter=2, random_state=76)","200","    ap.fit(X)","201","    centers76 = ap.cluster_centers_","202","","203","    assert np.mean((centers0 - centers76) ** 2) > 1","204","","205","","206","# FIXME: to be removed in 0.25","207","def test_affinity_propagation_random_state_warning():","208","    # test that a warning is raised when random_state is not defined.","209","    X = np.array([[0, 0], [1, 1], [-2, -2]])","210","    match = (\"'random_state' has been introduced in 0.23. \"","211","             \"It will be set to None starting from 0.25 which \"","212","             \"means that results will differ at every function \"","213","             \"call. Set 'random_state' to None to silence this \"","214","             \"warning, or to 0 to keep the behavior of versions \"","215","             \"<0.23.\")","216","    with pytest.warns(FutureWarning, match=match):","217","        AffinityPropagation().fit(X)","218","","226","    ap = AffinityPropagation(random_state=46)"],"delete":["35","        S, preference=preference)","41","    af = AffinityPropagation(preference=preference, affinity=\"precomputed\")","44","    af = AffinityPropagation(preference=preference, verbose=True)","57","                                             copy=False)","65","    af = AffinityPropagation(affinity=\"unknown\")","68","    af_2 = AffinityPropagation(affinity='precomputed')","74","    af = AffinityPropagation(affinity=\"euclidean\")","89","    af = AffinityPropagation(affinity=\"precomputed\")","102","    af = AffinityPropagation(preference=-10, max_iter=1)","131","        affinity_propagation, S, preference=[-20, -10])","145","                      AffinityPropagation(preference=-10, max_iter=1).fit, X)","158","    af = AffinityPropagation(affinity='euclidean', max_iter=2).fit(X)","190","    ap = AffinityPropagation()"]}],"doc\/whats_new\/v0.23.rst":[{"add":["145","- |API| The ``random_state`` parameter has been added to ","146","  :class:`cluster.AffinityPropagation`. :pr:`16801` by :user:`rcwoolston`","147","  and :user:`Chiara Marmo <cmarmo>`.","148",""],"delete":[]}]}},"d2cd2540418d3ff66b324ec18566dbe0b5991b40":{"changes":{"sklearn\/neural_network\/tests\/test_mlp.py":"MODIFY","sklearn\/neural_network\/_multilayer_perceptron.py":"MODIFY","doc\/whats_new\/v0.24.rst":"MODIFY"},"diff":{"sklearn\/neural_network\/tests\/test_mlp.py":[{"add":["664","@pytest.mark.parametrize(\"MLPEstimator\", [MLPClassifier, MLPRegressor])","665","def test_warm_start_full_iteration(MLPEstimator):","666","    # Non-regression test for:","667","    # https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/16812","668","    # Check that the MLP estimator accomplish `max_iter` with a","669","    # warm started estimator.","670","    X, y = X_iris, y_iris","671","    max_iter = 3","672","    clf = MLPEstimator(","673","        hidden_layer_sizes=2, solver='sgd', warm_start=True, max_iter=max_iter","674","    )","675","    clf.fit(X, y)","676","    assert max_iter == clf.n_iter_","677","    clf.fit(X, y)","678","    assert 2 * max_iter == clf.n_iter_","679","","680",""],"delete":[]}],"sklearn\/neural_network\/_multilayer_perceptron.py":[{"add":["979","        # Matrix of actions to be taken under the possible combinations:","980","        # The case that incremental == True and classes_ not defined is","981","        # already checked by _check_partial_fit_first_call that is called","982","        # in _partial_fit below.","983","        # The cases are already grouped into the respective if blocks below.","984","        #","985","        # incremental warm_start classes_ def  action","986","        #    0            0         0        define classes_","987","        #    0            1         0        define classes_","988","        #    0            0         1        redefine classes_","989","        #","990","        #    0            1         1        check compat warm_start","991","        #    1            1         1        check compat warm_start","992","        #","993","        #    1            0         1        check compat last fit","994","        #","995","        # Note the reliance on short-circuiting here, so that the second","996","        # or part implies that classes_ is defined.","997","        if (","998","            (not hasattr(self, \"classes_\")) or","999","            (not self.warm_start and not incremental)","1000","        ):","1006","            if self.warm_start:","1007","                if set(classes) != set(self.classes_):","1008","                    raise ValueError(","1009","                        f\"warm_start can only be used where `y` has the same \"","1010","                        f\"classes as in the previous call to fit. Previously \"","1011","                        f\"got {self.classes_}, `y` has {classes}\"","1012","                    )","1013","            elif len(np.setdiff1d(classes, self.classes_, assume_unique=True)):","1014","                raise ValueError(","1015","                    f\"`y` has classes not in `self.classes_`. \"","1016","                    f\"`self.classes_` has {self.classes_}. 'y' has {classes}.\"","1017","                )"],"delete":["979","        if not incremental:","983","        elif self.warm_start:","984","            classes = unique_labels(y)","985","            if set(classes) != set(self.classes_):","986","                raise ValueError(\"warm_start can only be used where `y` has \"","987","                                 \"the same classes as in the previous \"","988","                                 \"call to fit. Previously got %s, `y` has %s\" %","989","                                 (self.classes_, classes))","992","            if len(np.setdiff1d(classes, self.classes_, assume_unique=True)):","993","                raise ValueError(\"`y` has classes not in `self.classes_`.\"","994","                                 \" `self.classes_` has %s. 'y' has %s.\" %","995","                                 (self.classes_, classes))","1023","    def fit(self, X, y):","1024","        \"\"\"Fit the model to data matrix X and target(s) y.","1025","","1026","        Parameters","1027","        ----------","1028","        X : ndarray or sparse matrix of shape (n_samples, n_features)","1029","            The input data.","1030","","1031","        y : ndarray of shape (n_samples,) or (n_samples, n_outputs)","1032","            The target values (class labels in classification, real numbers in","1033","            regression).","1034","","1035","        Returns","1036","        -------","1037","        self : returns a trained MLP model.","1038","        \"\"\"","1039","        return self._fit(X, y, incremental=(self.warm_start and","1040","                                            hasattr(self, \"classes_\")))","1041",""]}],"doc\/whats_new\/v0.24.rst":[{"add":["512","- |Fix| Fix method  :func:`fit` of :class:`neural_network.MLPClassifier`","513","  not iterating to ``max_iter`` if warm started.","514","  :pr:`18269` by :user:`Norbert Preining <norbusan>` and","515","  :user:`Guillaume Lemaitre <glemaitre>`.","516",""],"delete":[]}]}},"ccf5c36503785a0330f460a9ae751ac973980443":{"changes":{"sklearn\/cluster\/_kmeans.py":"MODIFY","sklearn\/cluster\/tests\/test_k_means.py":"MODIFY","doc\/whats_new\/v0.23.rst":"MODIFY"},"diff":{"sklearn\/cluster\/_kmeans.py":[{"add":["179","        sample_weight = sample_weight * scale"],"delete":["179","        sample_weight *= scale"]}],"sklearn\/cluster\/tests\/test_k_means.py":[{"add":["1169","","1170","","1171","def test_sample_weight_unchanged():","1172","    # Check that sample_weight is not modified in place by KMeans (#17204)","1173","    X = np.array([[1], [2], [4]])","1174","    sample_weight = np.array([0.5, 0.2, 0.3])","1175","    KMeans(n_clusters=2, random_state=0).fit(X, sample_weight=sample_weight)","1176","","1177","    # internally, sample_weight is rescale to sum up to n_samples = 3","1178","    assert_array_equal(sample_weight, np.array([0.5, 0.2, 0.3]))"],"delete":[]}],"doc\/whats_new\/v0.23.rst":[{"add":["4",".. _changes_0_23_1:","5","","6","Version 0.23.1","7","==============","8","","9","**TBD**","10","","11","Changelog","12","---------","13","","14",":mod:`sklearn.cluster`","15","......................","16","","17","- |Fix| Fixed a bug in :class:`cluster.KMeans` where the sample weights","18","  provided by the user was modified in place. :pr:`17204` by","19","  :user:`Jeremie du Boisberranger <jeremiedbb>`.","20",""],"delete":[]}]}},"5f9555b3ea98e4ce8bb62b1fc6521772418d9144":{"changes":{"sklearn\/utils\/_pprint.py":"MODIFY","sklearn\/utils\/tests\/test_pprint.py":"MODIFY","doc\/whats_new\/v0.23.rst":"MODIFY"},"diff":{"sklearn\/utils\/_pprint.py":[{"add":["96","","98","        if (k not in init_params or (  # happens if k is part of a **kwargs","99","                repr(v) != repr(init_params[k]) and","100","                not (is_scalar_nan(init_params[k]) and is_scalar_nan(v)))):"],"delete":["97","        if (repr(v) != repr(init_params[k]) and","98","                not (is_scalar_nan(init_params[k]) and is_scalar_nan(v))):"]}],"sklearn\/utils\/tests\/test_pprint.py":[{"add":["10","from sklearn import set_config, config_context","540","","541","","542","def test_kwargs_in_init():","543","    # Make sure the changed_only=True mode is OK when an argument is passed as","544","    # kwargs.","545","    # Non-regression test for","546","    # https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/17206","547","","548","    class WithKWargs(BaseEstimator):","549","        # Estimator with a kwargs argument. These need to hack around","550","        # set_params and get_params. Here we mimic what LightGBM does.","551","        def __init__(self, a='willchange', b='unchanged', **kwargs):","552","            self.a = a","553","            self.b = b","554","            self._other_params = {}","555","            self.set_params(**kwargs)","556","","557","        def get_params(self, deep=True):","558","            params = super().get_params(deep=deep)","559","            params.update(self._other_params)","560","            return params","561","","562","        def set_params(self, **params):","563","            for key, value in params.items():","564","                setattr(self, key, value)","565","                self._other_params[key] = value","566","            return self","567","","568","    est = WithKWargs(a='something', c='abcd', d=None)","569","","570","    expected = \"WithKWargs(a='something', c='abcd', d=None)\"","571","    assert expected == est.__repr__()","572","","573","    with config_context(print_changed_only=False):","574","        expected = \"WithKWargs(a='something', b='unchanged', c='abcd', d=None)\"","575","        assert expected == est.__repr__()"],"delete":["10","from sklearn import set_config"]}],"doc\/whats_new\/v0.23.rst":[{"add":["18","  provided by the user were modified in place. :pr:`17204` by","21","Miscellaneous","22",".............","23","","24","- |Fix| Fixed a bug in the `repr` of third-party estimators that use a","25","  `**kwargs` parameter in their constructor, when `changed_only` is True","26","  which is now the default. :pr:`17205` by `Nicolas Hug`_.","27",""],"delete":["18","  provided by the user was modified in place. :pr:`17204` by"]}]}},"2992fe27f39eff397f6f046343033db860fb1dc0":{"changes":{"build_tools\/circle\/build_doc.sh":"MODIFY",".circleci\/config.yml":"MODIFY"},"diff":{"build_tools\/circle\/build_doc.sh":[{"add":["148","MINICONDA_PATH=$HOME\/miniconda"],"delete":[]}],".circleci\/config.yml":[{"add":[],"delete":["9","      - MINICONDA_PATH: ~\/miniconda","51","      - MINICONDA_PATH: ~\/miniconda"]}]}},"df61e9ed98b0777cc0962be6e2d161f4c30110fd":{"changes":{"sklearn\/ensemble\/_hist_gradient_boosting\/gradient_boosting.py":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_gradient_boosting.py":"MODIFY","doc\/whats_new\/v0.24.rst":"MODIFY"},"diff":{"sklearn\/ensemble\/_hist_gradient_boosting\/gradient_boosting.py":[{"add":["647","        is_binned = getattr(self, '_in_fit', False)","648","        dtype = X_BINNED_DTYPE if is_binned else X_DTYPE","649","        X = check_array(X, dtype=dtype, force_all_finite=False)"],"delete":["647","        X = check_array(X, dtype=[X_DTYPE, X_BINNED_DTYPE],","648","                        force_all_finite=False)","655","        is_binned = getattr(self, '_in_fit', False)"]}],"sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_gradient_boosting.py":[{"add":["796","","797","","798","@pytest.mark.parametrize('Est', (HistGradientBoostingClassifier,","799","                                 HistGradientBoostingRegressor))","800","def test_uint8_predict(Est):","801","    # Non regression test for","802","    # https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/18408","803","    # Make sure X can be of dtype uint8 (i.e. X_BINNED_DTYPE) in predict. It","804","    # will be converted to X_DTYPE.","805","","806","    rng = np.random.RandomState(0)","807","","808","    X = rng.randint(0, 100, size=(10, 2)).astype(np.uint8)","809","    y = rng.randint(0, 2, size=10).astype(np.uint8)","810","    est = Est()","811","    est.fit(X, y)","812","    est.predict(X)"],"delete":[]}],"doc\/whats_new\/v0.24.rst":[{"add":["220","- |Fix|: Fixed a bug in","221","  :class:`ensemble.HistGradientBoostingRegressor` and","222","  :class:`ensemble.HistGradientBoostingClassifier` which can now accept data","223","  with `uint8` dtype in `predict`. :pr:`18410` by `Nicolas Hug`_.","224",""],"delete":[]}]}},"ca0065d3bf6f7577c64a10c0ef5a18f4c2e74cf5":{"changes":{"sklearn\/utils\/extmath.py":"MODIFY","sklearn\/utils\/tests\/test_extmath.py":"MODIFY"},"diff":{"sklearn\/utils\/extmath.py":[{"add":["762","    new_mean = np.average(X_0,","763","                          weights=sample_weight, axis=0).astype(np.float64)"],"delete":["762","    new_mean = \\","763","        _safe_accumulator_op(np.average, X_0, weights=sample_weight, axis=0)"]}],"sklearn\/utils\/tests\/test_extmath.py":[{"add":["461","@pytest.mark.parametrize(\"dtype\", [np.float32, np.float64])","462","def test_incremental_weighted_mean_and_variance_simple(rng, dtype):","464","    X = rng.rand(1000, 20).astype(dtype)*mult","516","@pytest.mark.parametrize(\"dtype\", [np.float32, np.float64])","517","def test_incremental_weighted_mean_and_variance_ignore_nan(dtype):","526","                  [300, 300, 300, 300]]).astype(dtype)","531","                      [300, 300, 300, np.nan]]).astype(dtype)"],"delete":["461","def test_incremental_weighted_mean_and_variance_simple(rng):","463","    X = rng.rand(1000, 20)*mult","515","def test_incremental_weighted_mean_and_variance_ignore_nan():","524","                  [300, 300, 300, 300]])","529","                      [300, 300, 300, np.nan]])"]}]}},"9901d8df131e06d8f6ba1677e10330cabfdeb245":{"changes":{"sklearn\/utils\/validation.py":"MODIFY","sklearn\/utils\/tests\/test_validation.py":"MODIFY"},"diff":{"sklearn\/utils\/validation.py":[{"add":["1303","        kwargs.update({k: arg for k, arg in zip(sig.parameters, args)})"],"delete":["1303","        kwargs.update({k: arg for k, arg in zip(all_args, args)})"]}],"sklearn\/utils\/tests\/test_validation.py":[{"add":["1099","    # The * is place before a keyword only argument without a default value","1100","    @_deprecate_positional_args","1101","    def f3(a, *, b, c=1, d=1):","1102","        pass","1103","","1104","    with pytest.warns(FutureWarning,","1105","                      match=r\"Pass b=2 as keyword args\"):","1106","        f3(1, 2)","1107",""],"delete":[]}]}},"acf195cc74b7082b35b5fb200e7c32eef1988362":{"changes":{"sklearn\/utils\/validation.py":"MODIFY","sklearn\/decomposition\/tests\/test_kernel_pca.py":"MODIFY","doc\/whats_new\/v0.24.rst":"MODIFY"},"diff":{"sklearn\/utils\/validation.py":[{"add":["1185","    small_pos_ratio = 1e-12 if is_double_precision else 1e-7"],"delete":["1185","    small_pos_ratio = 1e-12"]}],"sklearn\/decomposition\/tests\/test_kernel_pca.py":[{"add":["12","from sklearn.preprocessing import StandardScaler","298","","299","","300","def test_32_64_decomposition_shape():","301","    \"\"\" Test that the decomposition is similar for 32 and 64 bits data \"\"\"","302","    # see https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/18146","303","    X, y = make_blobs(","304","        n_samples=30,","305","        centers=[[0, 0, 0], [1, 1, 1]],","306","        random_state=0,","307","        cluster_std=0.1","308","    )","309","    X = StandardScaler().fit_transform(X)","310","    X -= X.min()","311","","312","    # Compare the shapes (corresponds to the number of non-zero eigenvalues)","313","    kpca = KernelPCA()","314","    assert (kpca.fit_transform(X).shape ==","315","            kpca.fit_transform(X.astype(np.float32)).shape)"],"delete":[]}],"doc\/whats_new\/v0.24.rst":[{"add":["24","- |Fix| :class:`decomposition.KernelPCA` behaviour is now more consistent","25","  between 32-bits and 64-bits data when the kernel has small positive","26","  eigenvalues.","119","- |Fix| :class:`decomposition.KernelPCA` behaviour is now more consistent","120","  between 32-bits and 64-bits data input when the kernel has small positive","121","  eigenvalues. Small positive eigenvalues were not correctly discarded for","122","  32-bits data.","123","  :pr:`18149` by :user:`Sylvain Mari¨¦ <smarie>`.","124",""],"delete":["24","- item","25","- item"]}]}},"068c4e6e00da008dbd25dc279360bde01f7c0760":{"changes":{"sklearn\/ensemble\/_gb_losses.py":"MODIFY","sklearn\/ensemble\/tests\/test_gradient_boosting_loss_functions.py":"MODIFY","doc\/whats_new\/v0.24.rst":"MODIFY"},"diff":{"sklearn\/ensemble\/_gb_losses.py":[{"add":["712","        return np.average(","713","            -1 * (Y * raw_predictions).sum(axis=1) +","714","            logsumexp(raw_predictions, axis=1),","715","            weights=sample_weight","716","        )"],"delete":["712","        if sample_weight is None:","713","            return np.sum(-1 * (Y * raw_predictions).sum(axis=1) +","714","                          logsumexp(raw_predictions, axis=1))","715","        else:","716","            return np.sum(","717","                -1 * sample_weight * (Y * raw_predictions).sum(axis=1) +","718","                logsumexp(raw_predictions, axis=1))"]}],"sklearn\/ensemble\/tests\/test_gradient_boosting_loss_functions.py":[{"add":["28","            bd(np.array([1.0]), np.array([0.0])))","37","    def alt_dev(y, pred):","38","        return np.mean(np.logaddexp(0.0, -2.0 * (2.0 * y - 1) * pred))","39","","51","    def alt_ng(y, pred):","52","        return (2 * y - 1) \/ (1 + np.exp(2 * (2 * y - 1) * pred))","53","","150","@pytest.mark.parametrize(","151","    'n_classes, n_samples', [(3, 100), (5, 57), (7, 13)]","152",")","153","def test_multinomial_deviance(n_classes, n_samples):","154","    # Check multinomial deviance with and without sample weights.","155","    rng = np.random.RandomState(13)","156","    sample_weight = np.ones(n_samples)","157","    y_true = rng.randint(0, n_classes, size=n_samples)","158","    y_pred = np.zeros((n_samples, n_classes), dtype=np.float64)","159","    for klass in range(y_pred.shape[1]):","160","        y_pred[:, klass] = y_true == klass","161","","162","    loss = MultinomialDeviance(n_classes)","163","    loss_wo_sw = loss(y_true, y_pred)","164","    assert loss_wo_sw > 0","165","    loss_w_sw = loss(y_true, y_pred, sample_weight=sample_weight)","166","    assert loss_wo_sw == pytest.approx(loss_w_sw)","167","","168","    # Multinomial deviance uses weighted average loss rather than","169","    # weighted sum loss, so we make sure that the value remains the same","170","    # when we device the weight by 2.","171","    loss_w_sw = loss(y_true, y_pred, sample_weight=0.5 * sample_weight)","172","    assert loss_wo_sw == pytest.approx(loss_w_sw)","173","","174","","175","def test_mdl_computation_weighted():","176","    raw_predictions = np.array([[1., -1., -.1], [-2., 1., 2.]])","177","    y_true = np.array([0, 1])","178","    weights = np.array([1, 3])","179","    expected_loss = 1.0909323","180","    # MultinomialDeviance loss computation with weights.","181","    loss = MultinomialDeviance(3)","182","    assert (loss(y_true, raw_predictions, weights)","183","            == pytest.approx(expected_loss))","184","","185","","186","@pytest.mark.parametrize('n', [0, 1, 2])","187","def test_mdl_exception(n):","188","    # Check that MultinomialDeviance throws an exception when n_classes <= 2","189","    err_msg = 'MultinomialDeviance requires more than 2 classes.'","190","    with pytest.raises(ValueError, match=err_msg):","191","        MultinomialDeviance(n)","192","","193",""],"delete":["28","                 bd(np.array([1.0]), np.array([0.0])))","37","    alt_dev = lambda y, pred: np.mean(np.logaddexp(0.0, -2.0 *","38","                                                   (2.0 * y - 1) * pred))","50","    alt_ng = lambda y, pred: (2 * y - 1) \/ (1 + np.exp(2 * (2 * y - 1) * pred))"]}],"doc\/whats_new\/v0.24.rst":[{"add":["82","- |Fix| Fixed bug in :class:`ensemble.MultinomialDeviance` where the","83","  average of logloss was incorrectly calculated as sum of logloss.","84","  :pr:`17694` by :user:`Markus Rempfler <rempfler>` and","85","  :user:`Tsutomu Kusanagi <t-kusanagi2>`.","86",""],"delete":[]}]}},"fc6ee00b0accceeec48cc5b606e713514b481617":{"changes":{"sklearn\/ensemble\/tests\/test_forest.py":"MODIFY"},"diff":{"sklearn\/ensemble\/tests\/test_forest.py":[{"add":["165","    reg = ForestRegressor(n_estimators=5, criterion=criterion,","167","    reg.fit(boston.data, boston.target)","168","    score = reg.score(boston.data, boston.target)","172","    reg = ForestRegressor(n_estimators=5, criterion=criterion,","174","    reg.fit(boston.data, boston.target)","175","    score = reg.score(boston.data, boston.target)","684","    reg = ExtraTreesRegressor(n_estimators=n_trees, random_state=42).fit(X, y)","687","    for tree in reg.estimators_:","715","    reg = ExtraTreesRegressor(max_features=1, random_state=1).fit(X, y)","718","    for tree in reg.estimators_:","1067","    est_ws = None","1069","        if est_ws is None:","1070","            est_ws = ForestEstimator(n_estimators=n_estimators,","1074","            est_ws.set_params(n_estimators=n_estimators)","1075","        est_ws.fit(X, y)","1076","        assert len(est_ws) == n_estimators","1078","    est_no_ws = ForestEstimator(n_estimators=10, random_state=random_state,","1080","    est_no_ws.fit(X, y)","1082","    assert (set([tree.random_state for tree in est_ws]) ==","1083","            set([tree.random_state for tree in est_no_ws]))","1085","    assert_array_equal(est_ws.apply(X), est_no_ws.apply(X),","1098","    est = ForestEstimator(n_estimators=5, max_depth=1, warm_start=False,","1100","    est.fit(X, y)","1102","    est_2 = ForestEstimator(n_estimators=5, max_depth=1, warm_start=True,","1104","    est_2.fit(X, y)  # inits state","1105","    est_2.set_params(warm_start=False, random_state=1)","1106","    est_2.fit(X, y)  # clears old state and equals est","1108","    assert_array_almost_equal(est_2.apply(X), est.apply(X))","1120","    est = ForestEstimator(n_estimators=5, max_depth=1, warm_start=True)","1121","    est.fit(X, y)","1122","    est.set_params(n_estimators=4)","1123","    assert_raises(ValueError, est.fit, X, y)","1136","    est = ForestEstimator(n_estimators=5, max_depth=3, warm_start=True,","1138","    est.fit(X, y)","1140","    est_2 = ForestEstimator(n_estimators=5, max_depth=3, warm_start=True,","1142","    est_2.fit(X, y)","1143","    # Now est_2 equals est.","1145","    est_2.set_params(random_state=2)","1146","    assert_warns(UserWarning, est_2.fit, X, y)","1149","    assert_array_equal(est.apply(X), est_2.apply(X))","1162","    est = ForestEstimator(n_estimators=15, max_depth=3, warm_start=False,","1164","    est.fit(X, y)","1166","    est_2 = ForestEstimator(n_estimators=5, max_depth=3, warm_start=False,","1168","    est_2.fit(X, y)","1170","    est_2.set_params(warm_start=True, oob_score=True, n_estimators=15)","1171","    est_2.fit(X, y)","1173","    assert hasattr(est_2, 'oob_score_')","1174","    assert est.oob_score_ == est_2.oob_score_","1178","    est_3 = ForestEstimator(n_estimators=15, max_depth=3, warm_start=True,","1180","    est_3.fit(X, y)","1181","    assert not hasattr(est_3, 'oob_score_')","1183","    est_3.set_params(oob_score=True)","1184","    ignore_warnings(est_3.fit)(X, y)","1186","    assert est.oob_score_ == est_3.oob_score_"],"delete":["165","    clf = ForestRegressor(n_estimators=5, criterion=criterion,","167","    clf.fit(boston.data, boston.target)","168","    score = clf.score(boston.data, boston.target)","172","    clf = ForestRegressor(n_estimators=5, criterion=criterion,","174","    clf.fit(boston.data, boston.target)","175","    score = clf.score(boston.data, boston.target)","684","    clf = ExtraTreesRegressor(n_estimators=n_trees, random_state=42).fit(X, y)","687","    for tree in clf.estimators_:","715","    clf = ExtraTreesRegressor(max_features=1, random_state=1).fit(X, y)","718","    for tree in clf.estimators_:","1067","    clf_ws = None","1069","        if clf_ws is None:","1070","            clf_ws = ForestEstimator(n_estimators=n_estimators,","1074","            clf_ws.set_params(n_estimators=n_estimators)","1075","        clf_ws.fit(X, y)","1076","        assert len(clf_ws) == n_estimators","1078","    clf_no_ws = ForestEstimator(n_estimators=10, random_state=random_state,","1080","    clf_no_ws.fit(X, y)","1082","    assert (set([tree.random_state for tree in clf_ws]) ==","1083","                 set([tree.random_state for tree in clf_no_ws]))","1085","    assert_array_equal(clf_ws.apply(X), clf_no_ws.apply(X),","1098","    clf = ForestEstimator(n_estimators=5, max_depth=1, warm_start=False,","1100","    clf.fit(X, y)","1102","    clf_2 = ForestEstimator(n_estimators=5, max_depth=1, warm_start=True,","1104","    clf_2.fit(X, y)  # inits state","1105","    clf_2.set_params(warm_start=False, random_state=1)","1106","    clf_2.fit(X, y)  # clears old state and equals clf","1108","    assert_array_almost_equal(clf_2.apply(X), clf.apply(X))","1120","    clf = ForestEstimator(n_estimators=5, max_depth=1, warm_start=True)","1121","    clf.fit(X, y)","1122","    clf.set_params(n_estimators=4)","1123","    assert_raises(ValueError, clf.fit, X, y)","1136","    clf = ForestEstimator(n_estimators=5, max_depth=3, warm_start=True,","1138","    clf.fit(X, y)","1140","    clf_2 = ForestEstimator(n_estimators=5, max_depth=3, warm_start=True,","1142","    clf_2.fit(X, y)","1143","    # Now clf_2 equals clf.","1145","    clf_2.set_params(random_state=2)","1146","    assert_warns(UserWarning, clf_2.fit, X, y)","1149","    assert_array_equal(clf.apply(X), clf_2.apply(X))","1162","    clf = ForestEstimator(n_estimators=15, max_depth=3, warm_start=False,","1164","    clf.fit(X, y)","1166","    clf_2 = ForestEstimator(n_estimators=5, max_depth=3, warm_start=False,","1168","    clf_2.fit(X, y)","1170","    clf_2.set_params(warm_start=True, oob_score=True, n_estimators=15)","1171","    clf_2.fit(X, y)","1173","    assert hasattr(clf_2, 'oob_score_')","1174","    assert clf.oob_score_ == clf_2.oob_score_","1178","    clf_3 = ForestEstimator(n_estimators=15, max_depth=3, warm_start=True,","1180","    clf_3.fit(X, y)","1181","    assert not hasattr(clf_3, 'oob_score_')","1183","    clf_3.set_params(oob_score=True)","1184","    ignore_warnings(clf_3.fit)(X, y)","1186","    assert clf.oob_score_ == clf_3.oob_score_"]}]}},"90d00daa76d1c6848a60554c7eba15f1e351ed46":{"changes":{"doc\/whats_new\/v0.23.rst":"MODIFY","sklearn\/cluster\/_k_means_lloyd.pyx":"MODIFY","sklearn\/cluster\/_k_means_elkan.pyx":"MODIFY"},"diff":{"doc\/whats_new\/v0.23.rst":[{"add":["21","- |Efficiency| :class:`cluster.KMeans` cannot spawn idle threads any more for","22","  very small datasets. :pr:`17210` by","23","  :user:`Jeremie du Boisberranger <jeremiedbb>`.","24",""],"delete":[]}],"sklearn\/cluster\/_k_means_lloyd.pyx":[{"add":["122","    # number of threads should not be bigger than number of chunks","123","    n_threads = min(n_threads, n_chunks)","124","","319","    # number of threads should not be bigger than number of chunks","320","    n_threads = min(n_threads, n_chunks)","321",""],"delete":[]}],"sklearn\/cluster\/_k_means_elkan.pyx":[{"add":["286","    # number of threads should not be bigger than number of chunks","287","    n_threads = min(n_threads, n_chunks)","288","","520","    # number of threads should not be bigger than number of chunks","521","    n_threads = min(n_threads, n_chunks)","522",""],"delete":[]}]}},"43c0efa1b9ec1b2af22849f231355fae4e1051e7":{"changes":{"sklearn\/ensemble\/_hist_gradient_boosting\/splitting.pyx":"MODIFY"},"diff":{"sklearn\/ensemble\/_hist_gradient_boosting\/splitting.pyx":[{"add":["136","        const signed char [::1] monotonic_cst","152","                 const signed char [::1] monotonic_cst,","412","            const signed char [::1] monotonic_cst = self.monotonic_cst","498","            signed char monotonic_cst,","612","            signed char monotonic_cst,","725","        signed char monotonic_cst,","822","    return value"],"delete":["136","        const char [::1] monotonic_cst","152","                 const char [::1] monotonic_cst,","412","            const char [::1] monotonic_cst = self.monotonic_cst","498","            char monotonic_cst,","612","            char monotonic_cst,","725","        char monotonic_cst,","822","    return value"]}]}},"dad615ae7a53e0b19cd6a3db11c3362fd58f99ad":{"changes":{"doc\/conf.py":"MODIFY","doc\/sphinxext\/custom_autosummary_new_suffix.py":"MODIFY"},"diff":{"doc\/conf.py":[{"add":["416","# `sklearn.cluster.dbscan` overlapping with `sklearn.cluster.DBSCAN`  on"],"delete":["416","# `sklearn.cluster.dbscan` overlapping with `klearn.cluster.DBSCAN`  on"]}],"doc\/sphinxext\/custom_autosummary_new_suffix.py":[{"add":["21","import sphinx","71","    if sphinx.version_info[0] <= 2:","72","        raise ModuleNotFoundError(\"Please install Sphinx >= 3.0 in order \"","73","                                  \"to build docs\")","75","    # Find listener id for process_generate_options added by","76","    process_generate_options_id = None","77","    builder_inited_listeners = app.events.listeners[\"builder-inited\"]","78","    for event_listener in builder_inited_listeners:","79","        func = event_listener.handler","80","        if func.__name__ == \"process_generate_options\":","81","            process_generate_options_id = event_listener.id","83","    assert process_generate_options_id is not None","84","","85","    # Override process_generate_options added by sphinx.ext.autosummary","86","    app.disconnect(process_generate_options_id)","87","    app.connect(\"builder-inited\", process_generate_options_custom_files)"],"delete":["21","import inspect","50","","72","    # Override process_generate_options added by sphinx.ext.autosummary","73","    builder_inited_listeners = app.events.listeners[\"builder-inited\"]","75","    for listener_id, obj in builder_inited_listeners.items():","76","        if (inspect.isfunction(obj)","77","                and obj.__name__ == \"process_generate_options\"):","78","            builder_inited_listeners[listener_id] = \\","79","                process_generate_options_custom_files"]}]}},"af2abc2e72bab89f05436c964c86fd7b17bfb6b8":{"changes":{"sklearn\/externals\/_pilutil.py":"MODIFY"},"diff":{"sklearn\/externals\/_pilutil.py":[{"add":["371","            image = Image.frombytes(mode, shape, data32.tobytes())","376","            image = Image.frombytes('L', shape, bytedata.tobytes())","378","                image.putpalette(asarray(pal, dtype=uint8).tobytes())","383","                image.putpalette(asarray(pal, dtype=uint8).tobytes())","387","            image = Image.frombytes('1', shape, bytedata.tobytes())","396","            image = Image.frombytes(mode, shape, data32.tobytes())","421","        strdata = bytedata.tobytes()","424","        strdata = transpose(bytedata, (0, 2, 1)).tobytes()","427","        strdata = transpose(bytedata, (1, 2, 0)).tobytes()"],"delete":["371","            image = Image.frombytes(mode, shape, data32.tostring())","376","            image = Image.frombytes('L', shape, bytedata.tostring())","378","                image.putpalette(asarray(pal, dtype=uint8).tostring())","383","                image.putpalette(asarray(pal, dtype=uint8).tostring())","387","            image = Image.frombytes('1', shape, bytedata.tostring())","396","            image = Image.frombytes(mode, shape, data32.tostring())","421","        strdata = bytedata.tostring()","424","        strdata = transpose(bytedata, (0, 2, 1)).tostring()","427","        strdata = transpose(bytedata, (1, 2, 0)).tostring()"]}]}},"06bb4864e6a49366dd5bc4e60a6b384601c375a6":{"changes":{"sklearn\/feature_extraction\/text.py":"MODIFY","doc\/whats_new\/v0.23.rst":"MODIFY","sklearn\/feature_extraction\/tests\/test_text.py":"MODIFY"},"diff":{"sklearn\/feature_extraction\/text.py":[{"add":["1208","            if max_features is not None:","1209","                X = self._sort_features(X, vocabulary)","1214","            if max_features is None:","1215","                X = self._sort_features(X, vocabulary)"],"delete":["1212","","1213","            X = self._sort_features(X, vocabulary)","1214",""]}],"doc\/whats_new\/v0.23.rst":[{"add":["19",":mod:`sklearn.feature_extraction`","20",".................................","21","","22","- |Fix| Fixes bug in :class:`feature_extraction.text.CountVectorizer` where","23","  sample order invariance was broken when `max_features` was set and features","24","  had the same count. :pr:`18016` by `Thomas Fan`_, `Roman Yurchak`_, and","25","  `Joel Nothman`_.","26",""],"delete":[]}],"sklearn\/feature_extraction\/tests\/test_text.py":[{"add":["1341","","1342","","1343","def test_tie_breaking_sample_order_invariance():","1344","    # Checks the sample order invariance when setting max_features","1345","    # non-regression test for #17939","1346","    vec = CountVectorizer(max_features=1)","1347","    vocab1 = vec.fit(['hello', 'world']).vocabulary_","1348","    vocab2 = vec.fit(['world', 'hello']).vocabulary_","1349","    assert vocab1 == vocab2"],"delete":[]}]}},"b9956dbb469855a4e2328e1605feb194c7deb7b5":{"changes":{"azure-pipelines.yml":"MODIFY"},"diff":{"azure-pipelines.yml":[{"add":["23","        if [[ $BUILD_REASON == \"PullRequest\" ]]; then","24","          # By default pull requests use refs\/pull\/PULL_ID\/merge as the source branch","25","          # which has a \"Merge ID into ID\" as a commit message. The latest commit","26","          # message is the second to last commit","27","          COMMIT_ID=$(echo $BUILD_SOURCEVERSIONMESSAGE | awk '{print $2}')","28","          COMMIT_MESSAGE=$(git log $COMMIT_ID -1 --pretty=%B)","29","        else","30","          COMMIT_MESSAGE=$BUILD_SOURCEVERSIONMESSAGE","31","        fi","32","        echo \"##vso[task.setvariable variable=COMMIT_MESSAGE]$COMMIT_MESSAGE\"","33","      displayName: Get source version message","34","    - bash: |","35","        set -ex","36","        if [[ \"$COMMIT_MESSAGE\" =~ \"[lint skip]\" ]]; then","46","        if [[ \"$COMMIT_MESSAGE\" =~ \"[lint skip]\" ]]; then","55","        if [[ \"$COMMIT_MESSAGE\" =~ \"[scipy-dev]\" ]] || [[ $BUILD_REASON == \"Schedule\" ]]; then","56","          echo \"Running scipy-dev\""],"delete":["23","        if [[ $BUILD_SOURCEVERSIONMESSAGE =~ \\[lint\\ skip\\] ]]; then","33","        if [[ $BUILD_SOURCEVERSIONMESSAGE =~ \\[lint\\ skip\\] ]]; then","42","        if [[ $BUILD_SOURCEVERSIONMESSAGE =~ \\[scipy-dev\\] ]] || \\","43","           [[ $BUILD_REASON == \"Schedule\" ]]; then"]}]}},"174f9351495159fcb8d80680309e234f74feb9e8":{"changes":{"sklearn\/tests\/test_pipeline.py":"MODIFY","sklearn\/pipeline.py":"MODIFY","doc\/whats_new\/v0.24.rst":"MODIFY"},"diff":{"sklearn\/tests\/test_pipeline.py":[{"add":["560","@pytest.mark.parametrize(\"start, end\", [(0, 1), (0, 2), (1, 2), (1, 3),","561","                                        (None, 1), (1, None), (None, None)])","562","def test_pipeline_slice(start, end):","563","    pipe = Pipeline(","564","        [(\"transf1\", Transf()), (\"transf2\", Transf()), (\"clf\", FitParamT())],","565","        memory=\"123\",","566","        verbose=True,","567","    )","568","    pipe_slice = pipe[start:end]","569","    # Test class","570","    assert isinstance(pipe_slice, Pipeline)","571","    # Test steps","572","    assert pipe_slice.steps == pipe.steps[start:end]","573","    # Test named_steps attribute","574","    assert list(pipe_slice.named_steps.items()) == list(","575","        pipe.named_steps.items())[start:end]","576","    # Test the rest of the parameters","577","    pipe_params = pipe.get_params(deep=False)","578","    pipe_slice_params = pipe_slice.get_params(deep=False)","579","    del pipe_params[\"steps\"]","580","    del pipe_slice_params[\"steps\"]","581","    assert pipe_params == pipe_slice_params","582","    # Test exception","583","    msg = \"Pipeline slicing only supports a step of 1\"","584","    with pytest.raises(ValueError, match=msg):","585","        pipe[start:end:-1]"],"delete":["560","def test_pipeline_slice():","561","    pipe = Pipeline([('transf1', Transf()),","562","                     ('transf2', Transf()),","563","                     ('clf', FitParamT())])","564","    pipe2 = pipe[:-1]","565","    assert isinstance(pipe2, Pipeline)","566","    assert pipe2.steps == pipe.steps[:-1]","567","    assert 2 == len(pipe2.named_steps)","568","    assert_raises(ValueError, lambda: pipe[::-1])"]}],"sklearn\/pipeline.py":[{"add":["209","                raise ValueError(\"Pipeline slicing only supports a step of 1\")","210","            return self.__class__(","211","                self.steps[ind], memory=self.memory, verbose=self.verbose","212","            )"],"delete":["209","                raise ValueError('Pipeline slicing only supports a step of 1')","210","            return self.__class__(self.steps[ind])"]}],"doc\/whats_new\/v0.24.rst":[{"add":["532","- |Fix| A slice of a :class:`pipeline.Pipeline` now inherits the parameters of","533","  the original pipeline (`memory` and `verbose`).","534","  :pr:`18429` by :user:`Albert Villanova del Moral <albertvillanova>` and","535","  :user:`Pawe? Biernat <pwl>`.","536",""],"delete":[]}]}},"fb8a497a144e9a0fc01a37c6d47c13884581fecd":{"changes":{"doc\/support.rst":"MODIFY"},"diff":{"doc\/support.rst":[{"add":["41","Please describe the nature of your data and how you preprocessed it:","64","  - observed outcome or Python (or gdb) tracebacks","68","optionally a minimalistic subsample of your dataset (for instance, exported","71","Note: Gists are Git cloneable repositories and thus you can use Git to"],"delete":["41","Please describe the nature of your data and the how you preprocessed it:","64","  - observed outcome or python (or gdb) tracebacks","68","optionally a minimalistic subsample of your dataset (for instance exported","71","Note: gists are git cloneable repositories and thus you can use git to"]}]}},"ac8cbb3799bdfa31360c87bf1097410326dba0bd":{"changes":{"sklearn\/tree\/_classes.py":"MODIFY","sklearn\/feature_extraction\/text.py":"MODIFY","sklearn\/model_selection\/tests\/test_search.py":"MODIFY","sklearn\/naive_bayes.py":"MODIFY","sklearn\/model_selection\/tests\/test_split.py":"MODIFY","sklearn\/ensemble\/tests\/test_voting.py":"MODIFY","sklearn\/linear_model\/tests\/test_ridge.py":"MODIFY","sklearn\/random_projection.py":"MODIFY","sklearn\/tests\/test_naive_bayes.py":"MODIFY","sklearn\/feature_extraction\/tests\/test_text.py":"MODIFY","sklearn\/gaussian_process\/kernels.py":"MODIFY","sklearn\/metrics\/_scorer.py":"MODIFY","sklearn\/multioutput.py":"MODIFY","sklearn\/feature_extraction\/image.py":"MODIFY","sklearn\/ensemble\/tests\/test_gradient_boosting.py":"MODIFY","sklearn\/manifold\/_isomap.py":"MODIFY","sklearn\/model_selection\/_search.py":"MODIFY","sklearn\/tree\/tests\/test_tree.py":"MODIFY","sklearn\/inspection\/_plot\/partial_dependence.py":"MODIFY","sklearn\/utils\/estimator_checks.py":"MODIFY","sklearn\/decomposition\/_truncated_svd.py":"MODIFY","sklearn\/decomposition\/tests\/test_sparse_pca.py":"MODIFY","sklearn\/tests\/test_multioutput.py":"MODIFY","sklearn\/ensemble\/tests\/test_iforest.py":"MODIFY","sklearn\/ensemble\/_iforest.py":"MODIFY","sklearn\/ensemble\/tests\/test_weight_boosting.py":"MODIFY","sklearn\/metrics\/tests\/test_score_objects.py":"MODIFY","sklearn\/model_selection\/_split.py":"MODIFY","sklearn\/ensemble\/tests\/test_bagging.py":"MODIFY","sklearn\/tests\/test_pipeline.py":"MODIFY","sklearn\/feature_extraction\/tests\/test_image.py":"MODIFY","sklearn\/tests\/test_random_projection.py":"MODIFY","sklearn\/ensemble\/_base.py":"MODIFY","sklearn\/ensemble\/_voting.py":"MODIFY","sklearn\/inspection\/_plot\/tests\/test_plot_partial_dependence.py":"MODIFY","sklearn\/tests\/test_dummy.py":"MODIFY","sklearn\/pipeline.py":"MODIFY","sklearn\/dummy.py":"MODIFY","sklearn\/decomposition\/_sparse_pca.py":"MODIFY","sklearn\/gaussian_process\/tests\/test_kernels.py":"MODIFY"},"diff":{"sklearn\/tree\/_classes.py":[{"add":[],"delete":["99","                 presort='deprecated',","113","        self.presort = presort","321","        if self.presort != 'deprecated':","322","            warnings.warn(\"The parameter 'presort' is deprecated and has no \"","323","                          \"effect. It will be removed in v0.24. You can \"","324","                          \"suppress this warning by not passing any value \"","325","                          \"to the 'presort' parameter.\",","326","                          FutureWarning)","327","","730","    presort : deprecated, default='deprecated'","731","        This parameter is deprecated and will be removed in v0.24.","732","","733","        .. deprecated:: 0.22","734","","833","                 presort='deprecated',","848","            presort=presort,","1094","    presort : deprecated, default='deprecated'","1095","        This parameter is deprecated and will be removed in v0.24.","1096","","1097","        .. deprecated:: 0.22","1098","","1187","                 presort='deprecated',","1201","            presort=presort,","1248","    @property","1249","    def classes_(self):","1250","        # TODO: Remove method in 0.24","1251","        msg = (\"the classes_ attribute is to be deprecated from version \"","1252","               \"0.22 and will be removed in 0.24.\")","1253","        warnings.warn(msg, FutureWarning)","1254","        return np.array([None] * self.n_outputs_)","1255","","1256","    @property","1257","    def n_classes_(self):","1258","        # TODO: Remove method in 0.24","1259","        msg = (\"the n_classes_ attribute is to be deprecated from version \"","1260","               \"0.22 and will be removed in 0.24.\")","1261","        warnings.warn(msg, FutureWarning)","1262","        return np.array([1] * self.n_outputs_, dtype=np.intp)","1263",""]}],"sklearn\/feature_extraction\/text.py":[{"add":["32","from ..utils import _IS_32BIT","1839","    def transform(self, raw_documents):"],"delete":["32","from ..utils import _IS_32BIT, deprecated","507","@deprecated(\"VectorizerMixin is deprecated in version \"","508","            \"0.22 and will be removed in version 0.24.\")","509","class VectorizerMixin(_VectorizerMixin):","510","    pass","511","","512","","1845","    def transform(self, raw_documents, copy=\"deprecated\"):","1856","        copy : bool, default=True","1857","            Whether to copy X and operate on the copy or perform in-place","1858","            operations.","1859","","1860","            .. deprecated:: 0.22","1861","               The `copy` parameter is unused and was deprecated in version","1862","               0.22 and will be removed in 0.24. This parameter will be","1863","               ignored.","1864","","1872","        # FIXME Remove copy parameter support in 0.24","1873","        if copy != \"deprecated\":","1874","            msg = (\"'copy' param is unused and has been deprecated since \"","1875","                   \"version 0.22. Backward compatibility for 'copy' will \"","1876","                   \"be removed in 0.24.\")","1877","            warnings.warn(msg, FutureWarning)"]}],"sklearn\/model_selection\/tests\/test_search.py":[{"add":["867","    search = GridSearchCV(SVC(), cv=n_splits, param_grid=params,","868","                          return_train_score=True)","869","    search.fit(X, y)","870","    cv_results = search.cv_results_","871","    # Check if score and timing are reasonable","872","    assert all(cv_results['rank_test_score'] >= 1)","873","    assert (all(cv_results[k] >= 0) for k in score_keys","874","            if k != 'rank_test_score')","875","    assert (all(cv_results[k] <= 1) for k in score_keys","876","            if 'time' not in k and","877","            k != 'rank_test_score')","878","    # Check cv_results structure","879","    check_cv_results_array_types(search, param_keys, score_keys)","880","    check_cv_results_keys(cv_results, param_keys, score_keys, n_candidates)","881","    # Check masking","882","    cv_results = search.cv_results_","883","    n_candidates = len(search.cv_results_['params'])","884","    assert all((cv_results['param_C'].mask[i] and","885","                cv_results['param_gamma'].mask[i] and","886","                not cv_results['param_degree'].mask[i])","887","               for i in range(n_candidates)","888","               if cv_results['param_kernel'][i] == 'linear')","889","    assert all((not cv_results['param_C'].mask[i] and","890","                not cv_results['param_gamma'].mask[i] and","891","                cv_results['param_degree'].mask[i])","892","               for i in range(n_candidates)","893","               if cv_results['param_kernel'][i] == 'rbf')","917","    search = RandomizedSearchCV(SVC(), n_iter=n_search_iter,","918","                                cv=n_splits,","919","                                param_distributions=params,","920","                                return_train_score=True)","921","    search.fit(X, y)","922","    cv_results = search.cv_results_","923","    # Check results structure","924","    check_cv_results_array_types(search, param_keys, score_keys)","925","    check_cv_results_keys(cv_results, param_keys, score_keys, n_cand)","926","    n_candidates = len(search.cv_results_['params'])","927","    assert all((cv_results['param_C'].mask[i] and","928","                cv_results['param_gamma'].mask[i] and","929","                not cv_results['param_degree'].mask[i])","930","               for i in range(n_candidates)","931","               if cv_results['param_kernel'][i] == 'linear')","932","    assert all((not cv_results['param_C'].mask[i] and","933","                not cv_results['param_gamma'].mask[i] and","934","                cv_results['param_degree'].mask[i])","935","               for i in range(n_candidates)","936","               if cv_results['param_kernel'][i] == 'rbf')","946","    # Test the IID parameter  TODO: Clearly this test does something else???","1000","    grid_searches = []","1001","    for scoring in ({'accuracy': make_scorer(accuracy_score),","1002","                     'recall': make_scorer(recall_score)},","1003","                    'accuracy', 'recall'):","1004","        grid_search = GridSearchCV(SVC(), cv=n_splits,","1005","                                   param_grid=params,","1006","                                   scoring=scoring, refit=False)","1007","        grid_search.fit(X, y)","1008","        grid_searches.append(grid_search)","1010","    compare_cv_results_multimetric_with_single(*grid_searches)","1022","    for refit in (True, False):","1023","        random_searches = []","1024","        for scoring in (('accuracy', 'recall'), 'accuracy', 'recall'):","1025","            # If True, for multi-metric pass refit='accuracy'","1026","            if refit:","1027","                probability = True","1028","                refit = 'accuracy' if isinstance(scoring, tuple) else refit","1029","            else:","1030","                probability = False","1031","            clf = SVC(probability=probability, random_state=42)","1032","            random_search = RandomizedSearchCV(clf, n_iter=n_search_iter,","1033","                                               cv=n_splits,","1034","                                               param_distributions=params,","1035","                                               scoring=scoring,","1036","                                               refit=refit, random_state=0)","1037","            random_search.fit(X, y)","1038","            random_searches.append(random_search)","1040","        compare_cv_results_multimetric_with_single(*random_searches)","1041","        compare_refit_methods_when_refit_with_acc(","1042","            random_searches[0], random_searches[1], refit)","1046","        search_multi, search_acc, search_rec):","1616","    for attr in dir(gscv):","1617","        if (attr[0].islower() and attr[-1:] == '_' and","1618","                attr not in {'cv_results_', 'best_estimator_',","1619","                             'refit_time_', 'classes_'}):","1620","            assert getattr(gscv, attr) == getattr(mycv, attr), \\","1621","                \"Attribute %s not equal\" % attr"],"delete":["37","from sklearn.model_selection import cross_val_score","847","@pytest.mark.filterwarnings(\"ignore:The parameter 'iid' is deprecated\")  # 0.24","869","    for iid in (False, True):","870","        search = GridSearchCV(SVC(), cv=n_splits, iid=iid,","871","                              param_grid=params, return_train_score=True)","872","        search.fit(X, y)","873","        assert iid == search.iid","874","        cv_results = search.cv_results_","875","        # Check if score and timing are reasonable","876","        assert all(cv_results['rank_test_score'] >= 1)","877","        assert (all(cv_results[k] >= 0) for k in score_keys","878","                if k != 'rank_test_score')","879","        assert (all(cv_results[k] <= 1) for k in score_keys","880","                if 'time' not in k and","881","                k != 'rank_test_score')","882","        # Check cv_results structure","883","        check_cv_results_array_types(search, param_keys, score_keys)","884","        check_cv_results_keys(cv_results, param_keys, score_keys, n_candidates)","885","        # Check masking","886","        cv_results = search.cv_results_","887","        n_candidates = len(search.cv_results_['params'])","888","        assert all((cv_results['param_C'].mask[i] and","889","                    cv_results['param_gamma'].mask[i] and","890","                    not cv_results['param_degree'].mask[i])","891","                   for i in range(n_candidates)","892","                   if cv_results['param_kernel'][i] == 'linear')","893","        assert all((not cv_results['param_C'].mask[i] and","894","                    not cv_results['param_gamma'].mask[i] and","895","                    cv_results['param_degree'].mask[i])","896","                   for i in range(n_candidates)","897","                   if cv_results['param_kernel'][i] == 'rbf')","900","@pytest.mark.filterwarnings(\"ignore:The parameter 'iid' is deprecated\")  # 0.24","922","    for iid in (False, True):","923","        search = RandomizedSearchCV(SVC(), n_iter=n_search_iter,","924","                                    cv=n_splits, iid=iid,","925","                                    param_distributions=params,","926","                                    return_train_score=True)","927","        search.fit(X, y)","928","        assert iid == search.iid","929","        cv_results = search.cv_results_","930","        # Check results structure","931","        check_cv_results_array_types(search, param_keys, score_keys)","932","        check_cv_results_keys(cv_results, param_keys, score_keys, n_cand)","933","        n_candidates = len(search.cv_results_['params'])","934","        assert all((cv_results['param_C'].mask[i] and","935","                    cv_results['param_gamma'].mask[i] and","936","                    not cv_results['param_degree'].mask[i])","937","                   for i in range(n_candidates)","938","                   if cv_results['param_kernel'][i] == 'linear')","939","        assert all((not cv_results['param_C'].mask[i] and","940","                    not cv_results['param_gamma'].mask[i] and","941","                    cv_results['param_degree'].mask[i])","942","                   for i in range(n_candidates)","943","                   if cv_results['param_kernel'][i] == 'rbf')","953","    # Test the IID parameter","1000","@pytest.mark.filterwarnings(\"ignore:The parameter 'iid' is deprecated\")  # 0.24","1001","def test_search_iid_param():","1002","    # Test the IID parameter","1003","    # noise-free simple 2d-data","1004","    X, y = make_blobs(centers=[[0, 0], [1, 0], [0, 1], [1, 1]], random_state=0,","1005","                      cluster_std=0.1, shuffle=False, n_samples=80)","1006","    # split dataset into two folds that are not iid","1007","    # first one contains data of all 4 blobs, second only from two.","1008","    mask = np.ones(X.shape[0], dtype=np.bool)","1009","    mask[np.where(y == 1)[0][::2]] = 0","1010","    mask[np.where(y == 2)[0][::2]] = 0","1011","    # this leads to perfect classification on one fold and a score of 1\/3 on","1012","    # the other","1013","    # create \"cv\" for splits","1014","    cv = [[mask, ~mask], [~mask, mask]]","1015","    # once with iid=True (default)","1016","    grid_search = GridSearchCV(SVC(gamma='auto'), param_grid={'C': [1, 10]},","1017","                               cv=cv, return_train_score=True, iid=True)","1018","    random_search = RandomizedSearchCV(SVC(gamma='auto'), n_iter=2,","1019","                                       param_distributions={'C': [1, 10]},","1020","                                       cv=cv, iid=True,","1021","                                       return_train_score=True)","1022","    for search in (grid_search, random_search):","1023","        search.fit(X, y)","1024","        assert search.iid or search.iid is None","1025","","1026","        test_cv_scores = np.array(list(search.cv_results_['split%d_test_score'","1027","                                                          % s_i][0]","1028","                                       for s_i in range(search.n_splits_)))","1029","        test_mean = search.cv_results_['mean_test_score'][0]","1030","        test_std = search.cv_results_['std_test_score'][0]","1031","","1032","        train_cv_scores = np.array(list(search.cv_results_['split%d_train_'","1033","                                                           'score' % s_i][0]","1034","                                        for s_i in range(search.n_splits_)))","1035","        train_mean = search.cv_results_['mean_train_score'][0]","1036","        train_std = search.cv_results_['std_train_score'][0]","1037","","1038","        # Test the first candidate","1039","        assert search.cv_results_['param_C'][0] == 1","1040","        assert_array_almost_equal(test_cv_scores, [1, 1. \/ 3.])","1041","        assert_array_almost_equal(train_cv_scores, [1, 1])","1042","","1043","        # for first split, 1\/4 of dataset is in test, for second 3\/4.","1044","        # take weighted average and weighted std","1045","        expected_test_mean = 1 * 1. \/ 4. + 1. \/ 3. * 3. \/ 4.","1046","        expected_test_std = np.sqrt(1. \/ 4 * (expected_test_mean - 1) ** 2 +","1047","                                    3. \/ 4 * (expected_test_mean - 1. \/ 3.) **","1048","                                    2)","1049","        assert_almost_equal(test_mean, expected_test_mean)","1050","        assert_almost_equal(test_std, expected_test_std)","1051","        assert_array_almost_equal(test_cv_scores,","1052","                                  cross_val_score(SVC(C=1, gamma='auto'), X,","1053","                                                  y, cv=cv))","1054","","1055","        # For the train scores, we do not take a weighted mean irrespective of","1056","        # i.i.d. or not","1057","        assert_almost_equal(train_mean, 1)","1058","        assert_almost_equal(train_std, 0)","1059","","1060","    # once with iid=False","1061","    grid_search = GridSearchCV(SVC(gamma='auto'),","1062","                               param_grid={'C': [1, 10]},","1063","                               cv=cv, iid=False, return_train_score=True)","1064","    random_search = RandomizedSearchCV(SVC(gamma='auto'), n_iter=2,","1065","                                       param_distributions={'C': [1, 10]},","1066","                                       cv=cv, iid=False,","1067","                                       return_train_score=True)","1068","","1069","    for search in (grid_search, random_search):","1070","        search.fit(X, y)","1071","        assert not search.iid","1072","","1073","        test_cv_scores = np.array(list(search.cv_results_['split%d_test_score'","1074","                                                          % s][0]","1075","                                       for s in range(search.n_splits_)))","1076","        test_mean = search.cv_results_['mean_test_score'][0]","1077","        test_std = search.cv_results_['std_test_score'][0]","1078","","1079","        train_cv_scores = np.array(list(search.cv_results_['split%d_train_'","1080","                                                           'score' % s][0]","1081","                                        for s in range(search.n_splits_)))","1082","        train_mean = search.cv_results_['mean_train_score'][0]","1083","        train_std = search.cv_results_['std_train_score'][0]","1084","","1085","        assert search.cv_results_['param_C'][0] == 1","1086","        # scores are the same as above","1087","        assert_array_almost_equal(test_cv_scores, [1, 1. \/ 3.])","1088","        # Unweighted mean\/std is used","1089","        assert_almost_equal(test_mean, np.mean(test_cv_scores))","1090","        assert_almost_equal(test_std, np.std(test_cv_scores))","1091","","1092","        # For the train scores, we do not take a weighted mean irrespective of","1093","        # i.i.d. or not","1094","        assert_almost_equal(train_mean, 1)","1095","        assert_almost_equal(train_std, 0)","1096","","1097","","1098","@pytest.mark.filterwarnings(\"ignore:The parameter 'iid' is deprecated\")  # 0.24","1106","    for iid in (False, True):","1107","        grid_searches = []","1108","        for scoring in ({'accuracy': make_scorer(accuracy_score),","1109","                         'recall': make_scorer(recall_score)},","1110","                        'accuracy', 'recall'):","1111","            grid_search = GridSearchCV(SVC(), cv=n_splits,","1112","                                       iid=iid, param_grid=params,","1113","                                       scoring=scoring, refit=False)","1114","            grid_search.fit(X, y)","1115","            assert grid_search.iid == iid","1116","            grid_searches.append(grid_search)","1118","        compare_cv_results_multimetric_with_single(*grid_searches, iid=iid)","1121","@pytest.mark.filterwarnings(\"ignore:The parameter 'iid' is deprecated\")  # 0.24","1131","    for iid in (True, False):","1132","        for refit in (True, False):","1133","            random_searches = []","1134","            for scoring in (('accuracy', 'recall'), 'accuracy', 'recall'):","1135","                # If True, for multi-metric pass refit='accuracy'","1136","                if refit:","1137","                    probability = True","1138","                    refit = 'accuracy' if isinstance(scoring, tuple) else refit","1139","                else:","1140","                    probability = False","1141","                clf = SVC(probability=probability, random_state=42)","1142","                random_search = RandomizedSearchCV(clf, n_iter=n_search_iter,","1143","                                                   cv=n_splits, iid=iid,","1144","                                                   param_distributions=params,","1145","                                                   scoring=scoring,","1146","                                                   refit=refit, random_state=0)","1147","                random_search.fit(X, y)","1148","                random_searches.append(random_search)","1150","            compare_cv_results_multimetric_with_single(*random_searches,","1151","                                                       iid=iid)","1152","            compare_refit_methods_when_refit_with_acc(","1153","                random_searches[0], random_searches[1], refit)","1156","@pytest.mark.filterwarnings(\"ignore:The parameter 'iid' is deprecated\")  # 0.24","1158","        search_multi, search_acc, search_rec, iid):","1162","    assert search_multi.iid == iid","1729","    # TODO: remove in v0.24, the deprecation goes away then.","1730","    with pytest.warns(FutureWarning,","1731","                      match=\"attribute is to be deprecated from version 0.22\"):","1732","        for attr in dir(gscv):","1733","            if (attr[0].islower() and attr[-1:] == '_' and","1734","                    attr not in {'cv_results_', 'best_estimator_',","1735","                                 'refit_time_',","1736","                                 }):","1737","                assert getattr(gscv, attr) == getattr(mycv, attr), \\","1738","                    \"Attribute %s not equal\" % attr","1762","@pytest.mark.parametrize(\"iid\", [False, True])","1763","def test_deprecated_grid_search_iid(iid):","1764","    # FIXME: remove in 0.24","1765","    depr_msg = \"The parameter 'iid' is deprecated in 0.22 and will be removed\"","1766","    X, y = make_blobs(n_samples=54, random_state=0, centers=2)","1767","    grid = GridSearchCV(","1768","        SVC(random_state=0), param_grid={'C': [10]}, cv=3, iid=iid","1769","    )","1770","    with pytest.warns(FutureWarning, match=depr_msg):","1771","        grid.fit(X, y)","1772","","1773",""]}],"sklearn\/naive_bayes.py":[{"add":["29","from .utils import check_X_y, check_array","54","    @abstractmethod"],"delete":["29","from .utils import check_X_y, check_array, deprecated","56","        # Note that this is not marked @abstractmethod as long as the","57","        # deprecated public alias sklearn.naive_bayes.BayesNB exists","58","        # (until 0.24) to preserve backward compat for 3rd party projects","59","        # with existing derived classes.","60","        return X","1227","","1228","","1229","# TODO: remove in 0.24","1230","@deprecated(\"BaseNB is deprecated in version \"","1231","            \"0.22 and will be removed in version 0.24.\")","1232","class BaseNB(_BaseNB):","1233","    pass","1234","","1235","","1236","# TODO: remove in 0.24","1237","@deprecated(\"BaseDiscreteNB is deprecated in version \"","1238","            \"0.22 and will be removed in version 0.24.\")","1239","class BaseDiscreteNB(_BaseDiscreteNB):","1240","    pass"]}],"sklearn\/model_selection\/tests\/test_split.py":[{"add":["1618","    with pytest.raises(ValueError,","1619","                       match='has no effect since shuffle is False'):"],"delete":["1618","    # TODO 0.24: raise a ValueError instead of a warning","1619","    with pytest.warns(FutureWarning,","1620","                      match='has no effect since shuffle is False'):"]}],"sklearn\/ensemble\/tests\/test_voting.py":[{"add":["360","def test_set_estimator_drop():","361","    # VotingClassifier set_params should be able to set estimators as drop","374","        eclf2.set_params(rf='drop').fit(X, y)","375","    assert not record","378","    assert dict(eclf2.estimators)[\"rf\"] == 'drop'","382","    assert eclf2.get_params()[\"rf\"] == 'drop'","387","    assert not record","393","            eclf2.set_params(lr='drop', rf='drop', nb='drop').fit(X, y)","394","    assert not record","407","        eclf2.set_params(rf='drop').fit(X1, y1)","408","    assert not record","478","def test_none_estimator_with_weights(X, y, voter):","484","    voter.set_params(lr='drop')","487","    assert not record"],"delete":["360","# TODO: Remove parametrization in 0.24 when None is removed in Voting*","361","@pytest.mark.parametrize(\"drop\", [None, 'drop'])","362","def test_set_estimator_none(drop):","363","    \"\"\"VotingClassifier set_params should be able to set estimators as None or","364","    drop\"\"\"","377","        eclf2.set_params(rf=drop).fit(X, y)","378","    assert record if drop is None else not record","381","    assert dict(eclf2.estimators)[\"rf\"] is drop","385","    assert eclf2.get_params()[\"rf\"] is drop","390","    assert record if drop is None else not record","396","            eclf2.set_params(lr=drop, rf=drop, nb=drop).fit(X, y)","397","    assert record if drop is None else not record","410","        eclf2.set_params(rf=drop).fit(X1, y1)","411","    assert record if drop is None else not record","472","# TODO: Remove drop=None in 0.24 when None is removed in Voting*","482","@pytest.mark.parametrize(\"drop\", [None, 'drop'])","483","def test_none_estimator_with_weights(X, y, voter, drop):","484","    # TODO: remove the parametrization on 'drop' when support for None is","485","    # removed.","491","    voter.set_params(lr=drop)","494","    assert record if drop is None else not record","557","","558","","559","# TODO: Remove in 0.24 when None is removed in Voting*","560","@pytest.mark.parametrize(","561","    \"Voter, BaseEstimator\",","562","    [(VotingClassifier, DecisionTreeClassifier),","563","     (VotingRegressor, DecisionTreeRegressor)]","564",")","565","def test_deprecate_none_transformer(Voter, BaseEstimator):","566","    est = Voter(estimators=[('lr', None),","567","                            ('tree', BaseEstimator(random_state=0))])","568","","569","    msg = (\"Using 'None' to drop an estimator from the ensemble is \"","570","           \"deprecated in 0.22 and support will be dropped in 0.24. \"","571","           \"Use the string 'drop' instead.\")","572","    with pytest.warns(FutureWarning, match=msg):","573","        est.fit(X, y)"]}],"sklearn\/linear_model\/tests\/test_ridge.py":[{"add":["533","    kfold.fit(X_tiled, y_tiled)"],"delete":["533","    # ignore warning from GridSearchCV: FutureWarning: The default","534","    # of the `iid` parameter will change from True to False in version 0.22","535","    # and will be removed in 0.24","536","    with ignore_warnings(category=FutureWarning):","537","        kfold.fit(X_tiled, y_tiled)"]}],"sklearn\/random_projection.py":[{"add":[],"delete":["43","from .utils import deprecated","155","# TODO: remove in 0.24","156","@deprecated(\"gaussian_random_matrix is deprecated in \"","157","            \"0.22 and will be removed in version 0.24.\")","158","def gaussian_random_matrix(n_components, n_features, random_state=None):","159","    return _gaussian_random_matrix(n_components, n_features, random_state)","160","","161","","202","# TODO: remove in 0.24","203","@deprecated(\"gaussian_random_matrix is deprecated in \"","204","            \"0.22 and will be removed in version 0.24.\")","205","def sparse_random_matrix(n_components, n_features, density='auto',","206","                         random_state=None):","207","    return _sparse_random_matrix(n_components, n_features, density,","208","                                 random_state)","209","","210",""]}],"sklearn\/tests\/test_naive_bayes.py":[{"add":[],"delete":["23","from sklearn.naive_bayes import BaseNB, BaseDiscreteNB","828","","829","","830","# TODO: remove in 0.24","831","def test_deprecations():","832","","833","    class A(BaseNB, GaussianNB):","834","        pass","835","","836","    class B(BaseDiscreteNB, CategoricalNB):","837","        pass","838","","839","    with pytest.warns(FutureWarning, match=\"is deprecated in version 0.22\"):","840","        A()","841","","842","    with pytest.warns(FutureWarning, match=\"is deprecated in version 0.22\"):","843","        B()"]}],"sklearn\/feature_extraction\/tests\/test_text.py":[{"add":[],"delete":["15","from sklearn.feature_extraction.text import VectorizerMixin","524","# FIXME Remove copy parameter support in 0.24","525","def test_tfidf_vectorizer_deprecationwarning():","526","    msg = (\"'copy' param is unused and has been deprecated since \"","527","           \"version 0.22. Backward compatibility for 'copy' will \"","528","           \"be removed in 0.24.\")","529","    with pytest.warns(FutureWarning, match=msg):","530","        tv = TfidfVectorizer()","531","        train_data = JUNK_FOOD_DOCS","532","        tv.fit(train_data)","533","        tv.transform(train_data, copy=True)","534","","535","","1354","","1355","","1356","# TODO: Remove in 0.24","1357","def test_vectorizermixin_is_deprecated():","1358","    class MyVectorizer(VectorizerMixin):","1359","        pass","1360","","1361","    msg = (\"VectorizerMixin is deprecated in version 0.22 and will be removed \"","1362","           \"in version 0.24.\")","1363","    with pytest.warns(FutureWarning, match=msg):","1364","        MyVectorizer()"]}],"sklearn\/gaussian_process\/kernels.py":[{"add":["160","            params[arg] = getattr(self, arg)","161",""],"delete":["25","import warnings","161","            try:","162","                value = getattr(self, arg)","163","            except AttributeError:","164","                warnings.warn('From version 0.24, get_params will raise an '","165","                              'AttributeError if a parameter cannot be '","166","                              'retrieved as an instance attribute. Previously '","167","                              'it would return None.',","168","                              FutureWarning)","169","                value = None","170","            params[arg] = value"]}],"sklearn\/metrics\/_scorer.py":[{"add":["347","            scorer = SCORERS[scoring]"],"delete":["23","import warnings","129","        # XXX After removing the deprecated scorers (v0.24) remove the","130","        # XXX deprecation_msg property again and remove __call__'s body again","131","        self._deprecation_msg = None","164","        if self._deprecation_msg is not None:","165","            warnings.warn(self._deprecation_msg,","166","                          category=FutureWarning,","167","                          stacklevel=2)","355","            if scoring == 'brier_score_loss':","356","                # deprecated","357","                scorer = brier_score_loss_scorer","358","            else:","359","                scorer = SCORERS[scoring]","670","deprecation_msg = ('Scoring method brier_score_loss was renamed to '","671","                   'neg_brier_score in version 0.22 and will '","672","                   'be removed in 0.24.')","673","brier_score_loss_scorer._deprecation_msg = deprecation_msg"]}],"sklearn\/multioutput.py":[{"add":[],"delete":["29","from .utils import deprecated","790","","791","","792","# TODO: remove in 0.24","793","@deprecated(\"MultiOutputEstimator is deprecated in version \"","794","            \"0.22 and will be removed in version 0.24.\")","795","class MultiOutputEstimator(_MultiOutputEstimator):","796","    pass"]}],"sklearn\/feature_extraction\/image.py":[{"add":["17","from ..utils import check_array, check_random_state"],"delete":["17","from ..utils import check_array, check_random_state, deprecated","307","@deprecated(\"The function feature_extraction.image.extract_patches has been \"","308","            \"deprecated in 0.22 and will be removed in 0.24.\")","309","def extract_patches(arr, patch_shape=8, extraction_step=1):","310","    \"\"\"Extracts patches of any n-dimensional array in place using strides.","311","","312","    Given an n-dimensional array it will return a 2n-dimensional array with","313","    the first n dimensions indexing patch position and the last n indexing","314","    the patch content. This operation is immediate (O(1)). A reshape","315","    performed on the first n dimensions will cause numpy to copy data, leading","316","    to a list of extracted patches.","317","","318","    Read more in the :ref:`User Guide <image_feature_extraction>`.","319","","320","    Parameters","321","    ----------","322","    arr : ndarray","323","        n-dimensional array of which patches are to be extracted","324","","325","    patch_shape : int or tuple of length arr.ndim, default=8","326","        Indicates the shape of the patches to be extracted. If an","327","        integer is given, the shape will be a hypercube of","328","        sidelength given by its value.","329","","330","    extraction_step : int or tuple of length arr.ndim, default=1","331","        Indicates step size at which extraction shall be performed.","332","        If integer is given, then the step is uniform in all dimensions.","333","","334","","335","    Returns","336","    -------","337","    patches : strided ndarray","338","        2n-dimensional array indexing patches on first n dimensions and","339","        containing patches on the last n dimensions. These dimensions","340","        are fake, but this way no data is copied. A simple reshape invokes","341","        a copying operation to obtain a list of patches:","342","        result.reshape([-1] + list(patch_shape))","343","    \"\"\"","344","    return _extract_patches(arr, patch_shape=patch_shape,","345","                            extraction_step=extraction_step)","346","","347",""]}],"sklearn\/ensemble\/tests\/test_gradient_boosting.py":[{"add":[],"delete":["34","from sklearn.utils._testing import ignore_warnings","1297","# TODO: Remove in 0.24 when DummyClassifier's `strategy` default updates","1298","@ignore_warnings(category=FutureWarning)"]}],"sklearn\/manifold\/_isomap.py":[{"add":[],"delete":["8","from ..utils.deprecation import deprecated","170","    # mypy error: Decorated property not supported","171","    @deprecated(  # type: ignore","172","        \"Attribute `training_data_` was deprecated in version 0.22 and\"","173","        \" will be removed in 0.24.\"","174","    )","175","    @property","176","    def training_data_(self):","177","        check_is_fitted(self)","178","        return self.nbrs_._fit_X","179",""]}],"sklearn\/model_selection\/_search.py":[{"add":["406","                 refit=True, cv=None, verbose=0,","848","                   weights=None)","1155","                 n_jobs=None, refit=True, cv=None,","1160","            n_jobs=n_jobs, refit=refit, cv=cv, verbose=verbose,","1485","                 scoring=None, n_jobs=None, refit=True,","1494","            n_jobs=n_jobs, refit=refit, cv=cv, verbose=verbose,"],"delete":["406","                 iid='deprecated', refit=True, cv=None, verbose=0,","413","        self.iid = iid","845","        if self.iid != 'deprecated':","846","            warnings.warn(","847","                \"The parameter 'iid' is deprecated in 0.22 and will be \"","848","                \"removed in 0.24.\", FutureWarning","849","            )","850","            iid = self.iid","851","        else:","852","            iid = False","853","","858","                   weights=test_sample_counts if iid else None)","936","    iid : bool, default=False","937","        If True, return the average score across folds, weighted by the number","938","        of samples in each test set. In this case, the data is assumed to be","939","        identically distributed across the folds, and the loss minimized is","940","        the total loss per sample, and not the mean loss across the folds.","941","","942","        .. deprecated:: 0.22","943","            Parameter ``iid`` is deprecated in 0.22 and will be removed in 0.24","944","","1174","                 n_jobs=None, iid='deprecated', refit=True, cv=None,","1179","            n_jobs=n_jobs, iid=iid, refit=refit, cv=cv, verbose=verbose,","1277","    iid : bool, default=False","1278","        If True, return the average score across folds, weighted by the number","1279","        of samples in each test set. In this case, the data is assumed to be","1280","        identically distributed across the folds, and the loss minimized is","1281","        the total loss per sample, and not the mean loss across the folds.","1282","","1283","        .. deprecated:: 0.22","1284","            Parameter ``iid`` is deprecated in 0.22 and will be removed in 0.24","1285","","1513","                 scoring=None, n_jobs=None, iid='deprecated', refit=True,","1522","            n_jobs=n_jobs, iid=iid, refit=refit, cv=cv, verbose=verbose,"]}],"sklearn\/tree\/tests\/test_tree.py":[{"add":[],"delete":["1609","@pytest.mark.parametrize('Cls',","1610","                         (DecisionTreeRegressor, DecisionTreeClassifier))","1611","@pytest.mark.parametrize('presort', ['auto', True, False])","1612","def test_presort_deprecated(Cls, presort):","1613","    # TODO: remove in v0.24","1614","    X = np.zeros((10, 10))","1615","    y = np.r_[[0] * 5, [1] * 5]","1616","    tree = Cls(presort=presort)","1617","    with pytest.warns(FutureWarning,","1618","                      match=\"The parameter 'presort' is deprecated \"):","1619","        tree.fit(X, y)","1620","","1621","","1933","def test_classes_deprecated():","1934","    X = [[0, 0], [2, 2], [4, 6], [10, 11]]","1935","    y = [0.5, 2.5, 3.5, 5.5]","1936","    clf = DecisionTreeRegressor()","1937","    clf = clf.fit(X, y)","1938","","1939","    match = (\"attribute is to be deprecated from version \"","1940","             \"0.22 and will be removed in 0.24.\")","1941","","1942","    with pytest.warns(FutureWarning, match=match):","1943","        n = len(clf.classes_)","1944","        assert n == clf.n_outputs_","1945","","1946","    with pytest.warns(FutureWarning, match=match):","1947","        assert len(clf.n_classes_) == clf.n_outputs_","1948","","1949",""]}],"sklearn\/inspection\/_plot\/partial_dependence.py":[{"add":["21","                            method='auto', n_jobs=None, verbose=0,"],"delete":["3","import warnings","22","                            method='auto', n_jobs=None, verbose=0, fig=None,","156","    fig : Matplotlib figure object, optional (default=None)","157","        A figure object onto which the plots will be drawn, after the figure","158","        has been cleared. By default, a new one is created.","159","","160","        .. deprecated:: 0.22","161","           ``fig`` will be removed in 0.24.","162","","319","    if fig is not None:","320","        warnings.warn(\"The fig parameter is deprecated in version \"","321","                      \"0.22 and will be removed in version 0.24\",","322","                      FutureWarning)","323","        fig.clear()","324","        ax = fig.gca()","325",""]}],"sklearn\/utils\/estimator_checks.py":[{"add":["575","    if name == 'DummyClassifier':","576","        # the default strategy prior would output constant predictions and fail","577","        # for check_classifiers_predictions","578","        estimator.set_params(strategy='stratified')","579",""],"delete":[]}],"sklearn\/decomposition\/_truncated_svd.py":[{"add":[],"delete":["90","    >>> from sklearn.random_projection import sparse_random_matrix"]}],"sklearn\/decomposition\/tests\/test_sparse_pca.py":[{"add":[],"delete":["191","@pytest.mark.parametrize(\"spca\", [SparsePCA, MiniBatchSparsePCA])","192","def test_spca_deprecation_warning(spca):","193","    rng = np.random.RandomState(0)","194","    Y, _, _ = generate_toy_data(3, 10, (8, 8), random_state=rng)","195","","196","    warn_msg = \"'normalize_components' has been deprecated in 0.22\"","197","    with pytest.warns(FutureWarning, match=warn_msg):","198","        spca(normalize_components=True).fit(Y)","199","","200","","201","@pytest.mark.parametrize(\"spca\", [SparsePCA, MiniBatchSparsePCA])","202","def test_spca_error_unormalized_components(spca):","203","    rng = np.random.RandomState(0)","204","    Y, _, _ = generate_toy_data(3, 10, (8, 8), random_state=rng)","205","","206","    err_msg = \"normalize_components=False is not supported starting \"","207","    with pytest.raises(NotImplementedError, match=err_msg):","208","        spca(normalize_components=False).fit(Y)","209","","210",""]}],"sklearn\/tests\/test_multioutput.py":[{"add":[],"delete":["28","from sklearn.multioutput import MultiOutputEstimator","558","# TODO: remove in 0.24","559","def test_deprecation():","560","    class A(MultiOutputEstimator, MultiOutputRegressor):","561","        pass","562","","563","    with pytest.warns(FutureWarning, match=\"is deprecated in version 0.22\"):","564","        A(SGDRegressor(random_state=0, max_iter=5))","565","","566",""]}],"sklearn\/ensemble\/tests\/test_iforest.py":[{"add":[],"delete":["126","    # test that behaviour='old' will raise an error","127","    msg = \"The old behaviour of IsolationForest is not implemented anymore.\"","128","    with pytest.raises(NotImplementedError, match=msg):","129","        IsolationForest(behaviour='old').fit(X)","130","","319","def test_iforest_deprecation():","320","    iforest = IsolationForest(behaviour='new')","321","    warn_msg = \"'behaviour' is deprecated in 0.22 and will be removed in 0.24\"","322","    with pytest.warns(FutureWarning, match=warn_msg):","323","        iforest.fit(iris.data)","324","","325",""]}],"sklearn\/ensemble\/_iforest.py":[{"add":[],"delete":["95","    behaviour : str, default='deprecated'","96","        This parameter has no effect, is deprecated, and will be removed.","97","","98","        .. versionadded:: 0.20","99","           ``behaviour`` is added in 0.20 for back-compatibility purpose.","100","","101","        .. deprecated:: 0.20","102","           ``behaviour='old'`` is deprecated in 0.20 and will not be possible","103","           in 0.22.","104","","105","        .. deprecated:: 0.22","106","           ``behaviour`` parameter is deprecated in 0.22 and removed in","107","           0.24.","108","","194","                 behaviour='deprecated',","214","        self.behaviour = behaviour","249","        if self.behaviour != 'deprecated':","250","            if self.behaviour == 'new':","251","                warn(","252","                    \"'behaviour' is deprecated in 0.22 and will be removed \"","253","                    \"in 0.24. You should not pass or set this parameter.\",","254","                    FutureWarning","255","                )","256","            else:","257","                raise NotImplementedError(","258","                    \"The old behaviour of IsolationForest is not implemented \"","259","                    \"anymore. Remove the 'behaviour' parameter.\"","260","                )","261",""]}],"sklearn\/ensemble\/tests\/test_weight_boosting.py":[{"add":[],"delete":["14","from sklearn.utils._testing import ignore_warnings","502","# TODO: Remove in 0.24 when DummyClassifier's `strategy` default changes","503","@ignore_warnings"]}],"sklearn\/metrics\/tests\/test_score_objects.py":[{"add":[],"delete":["555","def test_deprecated_scorer():","556","    X, y = make_blobs(random_state=0, centers=2)","557","    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)","558","    clf = DecisionTreeClassifier()","559","    clf.fit(X_train, y_train)","560","","561","    deprecated_scorer = get_scorer('brier_score_loss')","562","    with pytest.warns(FutureWarning):","563","        deprecated_scorer(clf, X_test, y_test)","564","","565",""]}],"sklearn\/model_selection\/_split.py":[{"add":["291","            raise ValueError(","293","                'False. You should leave '"],"delete":["291","            # TODO 0.24: raise a ValueError instead of a warning","292","            warnings.warn(","294","                'False. This will raise an error in 0.24. You should leave '","296","                FutureWarning"]}],"sklearn\/ensemble\/tests\/test_bagging.py":[{"add":[],"delete":["18","from sklearn.utils._testing import ignore_warnings","54","# TODO: Remove in 0.24 when DummyClassifier's `strategy` default updates","55","@ignore_warnings(category=FutureWarning)"]}],"sklearn\/tests\/test_pipeline.py":[{"add":["902","def test_set_feature_union_step_drop():","915","        ft.set_params(m2='drop')","919","    assert not record","922","        ft.set_params(m3='drop')","926","    assert not record","932","    assert not record","936","        ft = FeatureUnion([('m2', 'drop'), ('m3', mult3)])","940","    assert not record"],"delete":["902","# TODO: Remove parametrization in 0.24 when None is removed for FeatureUnion","903","@pytest.mark.parametrize('drop', ['drop', None])","904","def test_set_feature_union_step_drop(drop):","917","        ft.set_params(m2=drop)","921","    assert record if drop is None else not record","924","        ft.set_params(m3=drop)","928","    assert record if drop is None else not record","934","    assert record if drop is None else not record","938","        ft = FeatureUnion([('m2', drop), ('m3', mult3)])","942","    assert record if drop is None else not record","1229","","1230","","1231","# TODO: Remove in 0.24 when None is removed","1232","def test_feature_union_warns_with_none():","1233","    msg = (r\"Using None as a transformer is deprecated in version 0\\.22 and \"","1234","           r\"will be removed in version 0\\.24\\. Please use 'drop' instead\\.\")","1235","    with pytest.warns(FutureWarning, match=msg):","1236","        union = FeatureUnion([('multi1', None), ('multi2', Mult())])","1237","","1238","    X = [[1, 2, 3], [4, 5, 6]]","1239","","1240","    with pytest.warns(FutureWarning, match=msg):","1241","        union.fit_transform(X)"]}],"sklearn\/feature_extraction\/tests\/test_image.py":[{"add":["12","    reconstruct_from_patches_2d, PatchExtractor, _extract_patches)"],"delete":["12","    reconstruct_from_patches_2d, PatchExtractor, _extract_patches,","13","    extract_patches)","336","","337","","338","# TODO: Remove in 0.24","339","def test_extract_patches_deprecated():","340","    msg = (\"The function feature_extraction.image.extract_patches has been \"","341","           \"deprecated in 0.22 and will be removed in 0.24.\")","342","    with pytest.warns(FutureWarning, match=msg):","343","        extract_patches(downsampled_face)"]}],"sklearn\/tests\/test_random_projection.py":[{"add":[],"delete":["12","from sklearn.random_projection import gaussian_random_matrix","14","from sklearn.random_projection import sparse_random_matrix","356","","357","","358","# TODO remove in 0.24","359","def test_deprecations():","360","","361","    with pytest.warns(FutureWarning, match=\"deprecated in 0.22\"):","362","        gaussian_random_matrix(10, 100)","363","","364","    with pytest.warns(FutureWarning, match=\"deprecated in 0.22\"):","365","        sparse_random_matrix(10, 100)"]}],"sklearn\/ensemble\/_base.py":[{"add":["228","        has_estimator = any(est != 'drop' for est in estimators)","239","            if est != 'drop' and not is_estimator_type(est):"],"delete":["7","import warnings","229","        # FIXME: deprecate the usage of None to drop an estimator from the","230","        # ensemble. Remove in 0.24","231","        if any(est is None for est in estimators):","232","            warnings.warn(","233","                \"Using 'None' to drop an estimator from the ensemble is \"","234","                \"deprecated in 0.22 and support will be dropped in 0.24. \"","235","                \"Use the string 'drop' instead.\", FutureWarning","236","            )","237","","238","        has_estimator = any(est not in (None, 'drop') for est in estimators)","249","            if est not in (None, 'drop') and not is_estimator_type(est):"]}],"sklearn\/ensemble\/_voting.py":[{"add":["55","                if est[1] != 'drop']","80","                for idx, clf in enumerate(clfs) if clf != 'drop'","85","        # Uses 'drop' as placeholder for dropped estimators","88","            current_est = est if est == 'drop' else next(est_iter)","128","            ``'drop'`` is accepted. Using None was deprecated in 0.22 and","129","            support was removed in 0.24.","375","            ``'drop'`` is accepted. Using None was deprecated in 0.22 and","376","            support was removed in 0.24."],"delete":["55","                if est[1] not in (None, 'drop')]","80","                for idx, clf in enumerate(clfs) if clf not in (None, 'drop')","85","        # Uses None or 'drop' as placeholder for dropped estimators","88","            current_est = est if est in (None, 'drop') else next(est_iter)","128","            ``'drop'`` is accepted.","129","","130","        .. deprecated:: 0.22","131","           Using ``None`` to drop an estimator is deprecated in 0.22 and","132","           support will be dropped in 0.24. Use the string ``'drop'`` instead.","378","            ``'drop'`` is accepted.","379","","380","        .. deprecated:: 0.22","381","           Using ``None`` to drop an estimator is deprecated in 0.22 and","382","           support will be dropped in 0.24. Use the string ``'drop'`` instead."]}],"sklearn\/inspection\/_plot\/tests\/test_plot_partial_dependence.py":[{"add":[],"delete":["459","","460","","461","def test_plot_partial_dependence_fig_deprecated(pyplot):","462","    # Make sure fig object is correctly used if not None","463","    X, y = make_regression(n_samples=50, random_state=0)","464","    clf = LinearRegression()","465","    clf.fit(X, y)","466","","467","    fig = pyplot.figure()","468","    grid_resolution = 25","469","","470","    msg = (\"The fig parameter is deprecated in version 0.22 and will be \"","471","           \"removed in version 0.24\")","472","    with pytest.warns(FutureWarning, match=msg):","473","        plot_partial_dependence(","474","            clf, X, [0, 1], target=0, grid_resolution=grid_resolution, fig=fig)","475","","476","    assert pyplot.gcf() is fig"]}],"sklearn\/tests\/test_dummy.py":[{"add":[],"delete":["758","@pytest.mark.filterwarnings(\"ignore:The default value of strategy.*\")  # 0.24","767","","768","","769","@pytest.mark.parametrize(\"Dummy\", (DummyRegressor, DummyClassifier))","770","def test_outputs_2d_deprecation(Dummy):","771","    X = [[1, 2]]","772","    y = [0]","773","    with pytest.warns(FutureWarning,","774","                      match=\"will be removed in version 0.24\"):","775","        Dummy().fit(X, y).outputs_2d_","776","","777","","778","# TODO: Remove in 0.24 when DummyClassifier's `strategy` default updates","779","def test_strategy_stratified_deprecated_for_prior():","780","    X, y = [[1, 2]], [0]","781","","782","    msg = (\"The default value of strategy will change from \"","783","           \"stratified to prior in 0.24\")","784","    with pytest.warns(FutureWarning, match=msg):","785","        DummyClassifier().fit(X, y)"]}],"sklearn\/pipeline.py":[{"add":["781","        half of each tuple is the name of the transformer. The tranformer can","782","        be 'drop' for it to be ignored.","883","                if trans != 'drop')","1006","        self.transformer_list[:] = [(name, old if old == 'drop'"],"delete":["13","import warnings","782","        half of each tuple is the name of the transformer.","867","            # TODO: Remove in 0.24 when None is removed","868","            if t is None:","869","                warnings.warn(\"Using None as a transformer is deprecated \"","870","                              \"in version 0.22 and will be removed in \"","871","                              \"version 0.24. Please use 'drop' instead.\",","872","                              FutureWarning)","873","                continue","890","                if trans is not None and trans != 'drop')","1013","        self.transformer_list[:] = [(name, old if old is None or old == 'drop'"]}],"sklearn\/dummy.py":[{"add":["21","","35","    strategy : str, default=\"prior\"","49","          .. versionchanged:: 0.24","50","             The default value of `strategy` has changed to \"prior\" in version","51","             0.24.","96","    def __init__(self, *, strategy=\"prior\", random_state=None,","123","        if self.strategy not in allowed_strategies:","126","","127","        self._strategy = self.strategy"],"delete":["19","from .utils import deprecated","35","    strategy : str, default=\"stratified\"","49","          .. versionchanged:: 0.22","50","             The default value of `strategy` will change to \"prior\" in version","51","             0.24. Starting from version 0.22, a warning will be raised if","52","             `strategy` is not explicitly set.","53","","54","          .. versionadded:: 0.17","55","             Dummy Classifier now supports prior fitting strategy using","56","             parameter *prior*.","101","    def __init__(self, *, strategy=\"warn\", random_state=None,","128","        # TODO: Remove in 0.24","129","        if self.strategy == \"warn\":","130","            warnings.warn(\"The default value of strategy will change from \"","131","                          \"stratified to prior in 0.24.\", FutureWarning)","132","            self._strategy = \"stratified\"","133","        elif self.strategy not in allowed_strategies:","136","        else:","137","            self._strategy = self.strategy","397","    # mypy error: Decorated property not supported","398","    @deprecated(  # type: ignore","399","        \"The outputs_2d_ attribute is deprecated in version 0.22 \"","400","        \"and will be removed in version 0.24. It is equivalent to \"","401","        \"n_outputs_ > 1.\"","402","    )","403","    @property","404","    def outputs_2d_(self):","405","        return self.n_outputs_ != 1","406","","626","","627","    # mypy error: Decorated property not supported","628","    @deprecated(  # type: ignore","629","        \"The outputs_2d_ attribute is deprecated in version 0.22 \"","630","        \"and will be removed in version 0.24. It is equivalent to \"","631","        \"n_outputs_ > 1.\"","632","    )","633","    @property","634","    def outputs_2d_(self):","635","        return self.n_outputs_ != 1"]}],"sklearn\/decomposition\/_sparse_pca.py":[{"add":["114","                 U_init=None, V_init=None, verbose=False, random_state=None):","315","                 shuffle=True, n_jobs=None, method='lars', random_state=None):","319","            random_state=random_state)"],"delete":["4","import warnings","5","","16","# FIXME: remove in 0.24","17","def _check_normalize_components(normalize_components, estimator_name):","18","    if normalize_components != 'deprecated':","19","        if normalize_components:","20","            warnings.warn(","21","                \"'normalize_components' has been deprecated in 0.22 and \"","22","                \"will be removed in 0.24. Remove the parameter from the \"","23","                \" constructor.\", FutureWarning","24","            )","25","        else:","26","            raise NotImplementedError(","27","                \"normalize_components=False is not supported starting from \"","28","                \"0.22. Remove this parameter from the constructor.\"","29","            )","30","","31","","87","    normalize_components : 'deprecated'","88","        This parameter does not have any effect. The components are always","89","        normalized.","90","","91","        .. versionadded:: 0.20","92","","93","        .. deprecated:: 0.22","94","           ``normalize_components`` is deprecated in 0.22 and will be removed","95","           in 0.24.","96","","142","                 U_init=None, V_init=None, verbose=False, random_state=None,","143","                 normalize_components='deprecated'):","155","        self.normalize_components = normalize_components","176","        _check_normalize_components(","177","            self.normalize_components, self.__class__.__name__","178","        )","179","","306","    normalize_components : 'deprecated'","307","        This parameter does not have any effect. The components are always","308","        normalized.","309","","310","        .. versionadded:: 0.20","311","","312","        .. deprecated:: 0.22","313","           ``normalize_components`` is deprecated in 0.22 and will be removed","314","           in 0.24.","315","","359","                 shuffle=True, n_jobs=None, method='lars', random_state=None,","360","                 normalize_components='deprecated'):","364","            random_state=random_state,","365","            normalize_components=normalize_components)","390","        _check_normalize_components(","391","            self.normalize_components, self.__class__.__name__","392","        )","393",""]}],"sklearn\/gaussian_process\/tests\/test_kernels.py":[{"add":["16","            Exponentiation, CompoundKernel)"],"delete":["16","            Exponentiation, Kernel, CompoundKernel)","358","def test_warns_on_get_params_non_attribute():","359","    class MyKernel(Kernel):","360","        def __init__(self, param=5):","361","            pass","362","","363","        def __call__(self, X, Y=None, eval_gradient=False):","364","            return X","365","","366","        def diag(self, X):","367","            return np.ones(X.shape[0])","368","","369","        def is_stationary(self):","370","            return False","371","","372","    est = MyKernel()","373","    with pytest.warns(FutureWarning, match='AttributeError'):","374","        params = est.get_params()","375","","376","    assert params['param'] is None","377","","378",""]}]}},"e02e1bf7cbdffd0325269cc195298d62e0818180":{"changes":{"sklearn\/model_selection\/tests\/test_search.py":"MODIFY"},"diff":{"sklearn\/model_selection\/tests\/test_search.py":[{"add":["821","    param_distributions = {\"C\": uniform(0, 1)}","822","    sampler = ParameterSampler(param_distributions=param_distributions,","823","                               n_iter=10, random_state=0)","824","    assert [x for x in sampler] == [x for x in sampler]"],"delete":["15","from sklearn.utils.fixes import sp_version","822","    if sp_version >= (0, 16):","823","        param_distributions = {\"C\": uniform(0, 1)}","824","        sampler = ParameterSampler(param_distributions=param_distributions,","825","                                   n_iter=10, random_state=0)","826","        assert [x for x in sampler] == [x for x in sampler]"]}]}},"4f496868c6aa7f50db99229847285efbe50040c2":{"changes":{"sklearn\/cluster\/tests\/test_k_means.py":"MODIFY"},"diff":{"sklearn\/cluster\/tests\/test_k_means.py":[{"add":["474","def test_minibatch_kmeans_init_size():","515","@pytest.mark.parametrize(\"array_constr\", [np.array, sp.csr_matrix],","516","                         ids=[\"dense\", \"sparse\"])","517","@pytest.mark.parametrize(\"dtype\", [np.float32, np.float64])","518","@pytest.mark.parametrize(\"init\", [\"random\", \"k-means++\"])","519","@pytest.mark.parametrize(\"Estimator, algorithm\", [","520","    (KMeans, \"full\"),","521","    (KMeans, \"elkan\"),","522","    (MiniBatchKMeans, None)","523","])","524","def test_predict(Estimator, algorithm, init, dtype, array_constr):","525","    # Check the predict method and the equivalence between fit.predict and","526","    # fit_predict.","528","    # There's a very small chance of failure with elkan on unstructured dataset","529","    # because predict method uses fast euclidean distances computation which","530","    # may cause small numerical instabilities.","531","    if sys.platform == \"darwin\":","532","        pytest.xfail(","533","            \"Known failures on MacOS, See \"","534","            \"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/12644\")","536","    X, _ = make_blobs(n_samples=500, n_features=10, centers=10, random_state=0)","537","    X = array_constr(X)","538","","539","    # With n_init = 1","540","    km = Estimator(n_clusters=10, init=init, n_init=1, random_state=0)","541","    if algorithm is not None:","542","        km.set_params(algorithm=algorithm)","543","    km.fit(X)","544","    labels = km.labels_","545","","546","    # re-predict labels for training set using predict","547","    pred = km.predict(X)","548","    assert_array_equal(pred, labels)","551","    pred = km.fit_predict(X)","552","    assert_array_equal(pred, labels)","553","","554","    # predict centroid labels","555","    pred = km.predict(km.cluster_centers_)","556","    assert_array_equal(pred, np.arange(10))","557","","558","    # With n_init > 1","559","    # Due to randomness in the order in which chunks of data are processed when","560","    # using more than one thread, there might be different rounding errors for","561","    # the computation of the inertia between 2 runs. This might result in a","562","    # different ranking of 2 inits, hence a different labeling, even if they","563","    # give the same clustering. We only check the labels up to a permutation.","564","","565","    km = Estimator(n_clusters=10, init=init, n_init=10, random_state=0)","566","    if algorithm is not None:","567","        km.set_params(algorithm=algorithm)","568","    km.fit(X)","569","    labels = km.labels_","570","","571","    # re-predict labels for training set using predict","572","    pred = km.predict(X)","573","    assert_allclose(v_measure_score(pred, labels), 1)","574","","575","    # re-predict labels for training set using fit_predict","576","    pred = km.fit_predict(X)","577","    assert_allclose(v_measure_score(pred, labels), 1)","578","","579","    # predict centroid labels","580","    pred = km.predict(km.cluster_centers_)","581","    assert_allclose(v_measure_score(pred, np.arange(10)), 1)","584","@pytest.mark.parametrize(\"init\", [\"random\", \"k-means++\", centers],","585","                         ids=[\"random\", \"k-means++\", \"ndarray\"])","586","@pytest.mark.parametrize(\"Estimator\", [KMeans, MiniBatchKMeans])","587","def test_predict_dense_sparse(Estimator, init):","589","    # predict time and vice versa.","591","    km = Estimator(n_clusters=n_clusters, init=init, n_init=n_init,","592","                   random_state=0)","594","    km.fit(X_csr)","595","    assert_array_equal(km.predict(X), km.labels_)","596","","597","    km.fit(X)","598","    assert_array_equal(km.predict(X_csr), km.labels_)","660","    previous_inertia = np.inf","661","    for n_init in [1, 5, 10]:","662","        # set max_iter=1 to avoid finding the global minimum and get the same","663","        # inertia each time","664","        km = KMeans(n_clusters=n_clusters, init=\"random\", n_init=n_init,","665","                    random_state=0, max_iter=1).fit(X)","666","        assert km.inertia_ <= previous_inertia","727","    km = Estimator(init=centers_new_type, n_clusters=n_clusters, n_init=1)","734","def test_kmeans_init_fitted_centers(data):","735","    # Check that starting fitting from a local optimum shouldn't change the","736","    # solution","737","    km1 = KMeans(n_clusters=n_clusters).fit(data)","738","    km2 = KMeans(n_clusters=n_clusters, init=km1.cluster_centers_,","739","                 n_init=1).fit(data)","741","    assert_allclose(km1.cluster_centers_, km2.cluster_centers_)","814","def test_kmeans_elkan_iter_attribute():","817","    km = KMeans(algorithm=\"elkan\", max_iter=1).fit(X)","818","    assert km.n_iter_ == 1","821","@pytest.mark.parametrize(\"array_constr\", [np.array, sp.csr_matrix],","822","                         ids=[\"dense\", \"sparse\"])","823","def test_kmeans_empty_cluster_relocated(array_constr):","826","    X = array_constr([[-1], [1]])","892","    # Check warning messages specific to KMeans","896","        KMeans(n_clusters=1, algorithm=\"elkan\").fit(X)","899","@pytest.mark.parametrize(\"array_constr\", [np.array, sp.csr_matrix],","900","                         ids=[\"dense\", \"sparse\"])","901","@pytest.mark.parametrize(\"algo\", [\"full\", \"elkan\"])","931","    # Check that the _euclidean_(dense\/sparse)_dense helpers produce correct","932","    # results","973","@pytest.mark.parametrize(\"Estimator\", [KMeans, MiniBatchKMeans])","974","def test_sample_weight_unchanged(Estimator):","978","    Estimator(n_clusters=2, random_state=0).fit(X, sample_weight=sample_weight)"],"delete":["34","from sklearn.metrics.cluster import homogeneity_score","475","def test_sparse_mb_k_means_callable_init():","476","","477","    def test_init(X, k, random_state):","478","        return centers","479","","480","    mb_k_means = MiniBatchKMeans(n_clusters=3, init=test_init,","481","                                 random_state=42).fit(X_csr)","482","    _check_fitted_model(mb_k_means)","483","","484","","485","def test_mini_batch_k_means_random_init_partial_fit():","486","    km = MiniBatchKMeans(n_clusters=n_clusters, init=\"random\", random_state=42)","487","","488","    # use the partial_fit API for online learning","489","    for X_minibatch in np.array_split(X, 10):","490","        km.partial_fit(X_minibatch)","491","","492","    # compute the labeling on the complete dataset","493","    labels = km.predict(X)","494","    assert v_measure_score(true_labels, labels) == 1.0","495","","496","","497","def test_minibatch_kmeans_default_init_size():","514","def test_minibatch_tol():","515","    mb_k_means = MiniBatchKMeans(n_clusters=n_clusters, batch_size=10,","516","                                 random_state=42, tol=.01).fit(X)","517","    _check_fitted_model(mb_k_means)","518","","519","","520","def test_minibatch_set_init_size():","521","    mb_k_means = MiniBatchKMeans(init=centers.copy(), n_clusters=n_clusters,","522","                                 init_size=666, random_state=42,","523","                                 n_init=1).fit(X)","524","    assert mb_k_means.init_size == 666","525","    assert mb_k_means._init_size == n_samples","526","    _check_fitted_model(mb_k_means)","527","","528","","553","@pytest.mark.parametrize('Estimator', [KMeans, MiniBatchKMeans])","554","@pytest.mark.parametrize('data', [X, X_csr], ids=['dense', 'sparse'])","555","@pytest.mark.parametrize('init', ['random', 'k-means++', centers.copy()])","556","def test_predict(Estimator, data, init):","557","    n_init = 10 if type(init) is str else 1","558","    k_means = Estimator(n_clusters=n_clusters, init=init,","559","                        n_init=n_init, random_state=0).fit(data)","561","    # sanity check: re-predict labeling for training set samples","562","    assert_array_equal(k_means.predict(data), k_means.labels_)","564","    # sanity check: predict centroid labels","565","    pred = k_means.predict(k_means.cluster_centers_)","566","    assert_array_equal(pred, np.arange(n_clusters))","569","    pred = k_means.fit_predict(data)","570","    assert_array_equal(pred, k_means.labels_)","573","@pytest.mark.parametrize('init', ['random', 'k-means++', centers.copy()])","574","def test_predict_minibatch_dense_sparse(init):","576","    # predict time","578","    mb_k_means = MiniBatchKMeans(n_clusters=n_clusters, init=init,","579","                                 n_init=n_init, random_state=0).fit(X_csr)","581","    assert_array_equal(mb_k_means.predict(X), mb_k_means.labels_)","641","@pytest.mark.parametrize('algo', ['full', 'elkan'])","642","def test_predict_equal_labels(algo):","643","    km = KMeans(random_state=13, n_init=1, max_iter=1,","644","                algorithm=algo)","645","    km.fit(X)","646","    assert_array_equal(km.predict(X), km.labels_)","647","","648","","649","def test_full_vs_elkan():","650","    km1 = KMeans(algorithm='full', random_state=13).fit(X)","651","    km2 = KMeans(algorithm='elkan', random_state=13).fit(X)","652","","653","    assert homogeneity_score(km1.predict(X), km2.predict(X)) == 1.0","654","","655","","658","    n_runs = 5","659","    n_init_range = [1, 5, 10]","660","    inertia = np.zeros((len(n_init_range), n_runs))","661","    for i, n_init in enumerate(n_init_range):","662","        for j in range(n_runs):","663","            km = KMeans(n_clusters=n_clusters, init=\"random\", n_init=n_init,","664","                        random_state=j).fit(X)","665","            inertia[i, j] = km.inertia_","666","","667","    inertia = inertia.mean(axis=1)","668","    failure_msg = (\"Inertia %r should be decreasing\"","669","                   \" when n_init is increasing.\") % list(inertia)","670","    for i in range(len(n_init_range) - 1):","671","        assert inertia[i] >= inertia[i + 1], failure_msg","732","    km = Estimator(init=centers, n_clusters=n_clusters, n_init=1)","739","def test_k_means_init_fitted_centers(data):","740","    # Get a local optimum","741","    centers = KMeans(n_clusters=3).fit(X).cluster_centers_","743","    # Fit starting from a local optimum shouldn't change the solution","744","    new_centers = KMeans(n_clusters=3, init=centers,","745","                         n_init=1).fit(X).cluster_centers_","746","    assert_array_almost_equal(centers, new_centers)","819","def test_iter_attribute():","822","    estimator = KMeans(algorithm=\"elkan\", max_iter=1)","823","    estimator.fit(np.random.rand(10, 10))","824","    assert estimator.n_iter_ == 1","827","def test_k_means_empty_cluster_relocated():","830","    X = np.array([[-1], [1]])","896","    X, _ = make_blobs(n_samples=10, n_features=2, centers=1, random_state=0)","897","    kmeans = KMeans(n_clusters=1, n_init=1, init='random', random_state=0,","898","                    algorithm='elkan')","899","","903","        kmeans.fit(X)","906","@pytest.mark.parametrize(\"array_constr\",","907","                         [np.array, sp.csr_matrix],","908","                         ids=['dense', 'sparse'])","909","@pytest.mark.parametrize(\"algo\", ['full', 'elkan'])","979","def test_sample_weight_unchanged():","983","    KMeans(n_clusters=2, random_state=0).fit(X, sample_weight=sample_weight)"]}]}},"e54cd3c0617e3485baa19e2c69332da55b363636":{"changes":{"sklearn\/metrics\/_ranking.py":"MODIFY","sklearn\/preprocessing\/_discretization.py":"MODIFY","sklearn\/preprocessing\/tests\/test_label.py":"MODIFY","sklearn\/calibration.py":"MODIFY","sklearn\/metrics\/tests\/test_classification.py":"MODIFY","sklearn\/preprocessing\/_encoders.py":"MODIFY","sklearn\/preprocessing\/_data.py":"MODIFY","sklearn\/preprocessing\/_function_transformer.py":"MODIFY","sklearn\/preprocessing\/tests\/test_data.py":"MODIFY","sklearn\/preprocessing\/_label.py":"MODIFY"},"diff":{"sklearn\/metrics\/_ranking.py":[{"add":["385","        y_true = label_binarize(y_true, classes=labels)[:, 0]","491","        y_true_multilabel = label_binarize(y_true, classes=classes)"],"delete":["385","        y_true = label_binarize(y_true, labels)[:, 0]","491","        y_true_multilabel = label_binarize(y_true, classes)"]}],"sklearn\/preprocessing\/_discretization.py":[{"add":["18","from ..utils.validation import _deprecate_positional_args","118","    @_deprecate_positional_args","119","    def __init__(self, n_bins=5, *, encode='onehot', strategy='quantile'):"],"delete":["117","    def __init__(self, n_bins=5, encode='onehot', strategy='quantile'):"]}],"sklearn\/preprocessing\/tests\/test_label.py":[{"add":["180","        label_binarize(np.array([[1, 3], [2, 1]]), classes=[1, 2, 3])","511","                label_binarize(y, classes=classes, neg_label=neg_label,","517","        binarized = label_binarize(y, classes=classes, neg_label=neg_label,","578","        label_binarize(y, classes=classes, neg_label=-1, pos_label=pos_label,","597","        label_binarize(y, classes=classes, neg_label=-1, pos_label=pos_label,"],"delete":["180","        label_binarize(np.array([[1, 3], [2, 1]]), [1, 2, 3])","511","                label_binarize(y, classes, neg_label=neg_label,","517","        binarized = label_binarize(y, classes, neg_label=neg_label,","578","        label_binarize(y, classes, neg_label=-1, pos_label=pos_label,","597","        label_binarize(y, classes, neg_label=-1, pos_label=pos_label,"]}],"sklearn\/calibration.py":[{"add":["331","        Y = label_binarize(y, classes=self.classes_)","576","    y_true = label_binarize(y_true, classes=labels)[:, 0]"],"delete":["331","        Y = label_binarize(y, self.classes_)","576","    y_true = label_binarize(y_true, labels)[:, 0]"]}],"sklearn\/metrics\/tests\/test_classification.py":[{"add":["656","    y_true_inv2 = label_binarize(y_true, classes=[\"a\", \"b\"])"],"delete":["656","    y_true_inv2 = label_binarize(y_true, [\"a\", \"b\"])"]}],"sklearn\/preprocessing\/_encoders.py":[{"add":["10","from ..utils.validation import _deprecate_positional_args","295","    @_deprecate_positional_args","296","    def __init__(self, *, categories='auto', drop=None, sparse=True,","657","    @_deprecate_positional_args","658","    def __init__(self, *, categories='auto', dtype=np.float64):"],"delete":["294","    def __init__(self, categories='auto', drop=None, sparse=True,","655","    def __init__(self, categories='auto', dtype=np.float64):"]}],"sklearn\/preprocessing\/_data.py":[{"add":["31","                                FLOAT_DTYPES, _deprecate_positional_args)","80","@_deprecate_positional_args","81","def scale(X, *, axis=0, with_mean=True, with_std=True, copy=True):","294","    @_deprecate_positional_args","295","    def __init__(self, feature_range=(0, 1), *, copy=True):","439","@_deprecate_positional_args","440","def minmax_scale(X, feature_range=(0, 1), *, axis=0, copy=True):","631","    @_deprecate_positional_args","632","    def __init__(self, *, copy=True, with_mean=True, with_std=True):","914","    @_deprecate_positional_args","915","    def __init__(self, *, copy=True):","1031","@_deprecate_positional_args","1032","def maxabs_scale(X, *, axis=0, copy=True):","1180","    @_deprecate_positional_args","1181","    def __init__(self, *, with_centering=True, with_scaling=True,","1290","@_deprecate_positional_args","1291","def robust_scale(X, *, axis=0, with_centering=True, with_scaling=True,","1442","    @_deprecate_positional_args","1443","    def __init__(self, degree=2, *, interaction_only=False, include_bias=True,","1648","@_deprecate_positional_args","1649","def normalize(X, norm='l2', *, axis=1, copy=True, return_norm=False):","1808","    @_deprecate_positional_args","1809","    def __init__(self, norm='l2', *, copy=True):","1845","@_deprecate_positional_args","1846","def binarize(X, *, threshold=0.0, copy=True):","1944","    @_deprecate_positional_args","1945","    def __init__(self, *, threshold=0.0, copy=True):","2242","    @_deprecate_positional_args","2243","    def __init__(self, *, n_quantiles=1000, output_distribution='uniform',","2575","@_deprecate_positional_args","2576","def quantile_transform(X, *, axis=0, n_quantiles=1000,","2780","    @_deprecate_positional_args","2781","    def __init__(self, method='yeo-johnson', *, standardize=True, copy=True):","3051","@_deprecate_positional_args","3052","def power_transform(X, method='yeo-johnson', *, standardize=True, copy=True):"],"delete":["31","                                FLOAT_DTYPES)","80","def scale(X, axis=0, with_mean=True, with_std=True, copy=True):","293","    def __init__(self, feature_range=(0, 1), copy=True):","437","def minmax_scale(X, feature_range=(0, 1), axis=0, copy=True):","628","    def __init__(self, copy=True, with_mean=True, with_std=True):","910","    def __init__(self, copy=True):","1026","def maxabs_scale(X, axis=0, copy=True):","1174","","1175","    def __init__(self, with_centering=True, with_scaling=True,","1284","def robust_scale(X, axis=0, with_centering=True, with_scaling=True,","1435","    def __init__(self, degree=2, interaction_only=False, include_bias=True,","1640","def normalize(X, norm='l2', axis=1, copy=True, return_norm=False):","1799","    def __init__(self, norm='l2', copy=True):","1835","def binarize(X, threshold=0.0, copy=True):","1933","    def __init__(self, threshold=0.0, copy=True):","2230","    def __init__(self, n_quantiles=1000, output_distribution='uniform',","2562","def quantile_transform(X, axis=0, n_quantiles=1000,","2766","    def __init__(self, method='yeo-johnson', standardize=True, copy=True):","3036","def power_transform(X, method='yeo-johnson', standardize=True, copy=True):"]}],"sklearn\/preprocessing\/_function_transformer.py":[{"add":["4","from ..utils.validation import _deprecate_positional_args","81","","82","    @_deprecate_positional_args","83","    def __init__(self, func=None, inverse_func=None, *, validate=False,"],"delete":["80","    def __init__(self, func=None, inverse_func=None, validate=False,"]}],"sklearn\/preprocessing\/tests\/test_data.py":[{"add":["2297","        power_transform(X_with_negatives, method='box-cox')","2306","        power_transform(np.zeros(X_2d.shape), method='box-cox')","2434","    pt = PowerTransformer(method, standardize=standardize)","2451","    pt = PowerTransformer(method, standardize=standardize, copy=True)","2479","    pt = PowerTransformer(method, standardize=standardize, copy=False)"],"delete":["2297","        power_transform(X_with_negatives, 'box-cox')","2306","        power_transform(np.zeros(X_2d.shape), 'box-cox')","2434","    pt = PowerTransformer(method, standardize)","2451","    pt = PowerTransformer(method, standardize, copy=True)","2479","    pt = PowerTransformer(method, standardize, copy=False)"]}],"sklearn\/preprocessing\/_label.py":[{"add":["23","from ..utils.validation import _deprecate_positional_args","399","    @_deprecate_positional_args","400","    def __init__(self, *, neg_label=0, pos_label=1, sparse_output=False):","487","        return label_binarize(y, classes=self.classes_,","545","@_deprecate_positional_args","546","def label_binarize(y, *, classes, neg_label=0, pos_label=1,","547","                   sparse_output=False):","857","    @_deprecate_positional_args","858","    def __init__(self, *, classes=None, sparse_output=False):"],"delete":["398","    def __init__(self, neg_label=0, pos_label=1, sparse_output=False):","485","        return label_binarize(y, self.classes_,","543","def label_binarize(y, classes, neg_label=0, pos_label=1, sparse_output=False):","853","    def __init__(self, classes=None, sparse_output=False):"]}]}},"bf6023fc77fe04d7f8d43135f36a70b832a9f662":{"changes":{"sklearn\/datasets\/tests\/test_kddcup99.py":"MODIFY"},"diff":{"sklearn\/datasets\/tests\/test_kddcup99.py":[{"add":["22","    data = fetch_kddcup99_fxt(subset='SA')","26","    data = fetch_kddcup99_fxt(subset='SF')","30","    data = fetch_kddcup99_fxt(subset='http')","34","    data = fetch_kddcup99_fxt(subset='smtp')","38","    fetch_func = partial(fetch_kddcup99_fxt, subset='smtp')"],"delete":["22","    data = fetch_kddcup99_fxt('SA')","26","    data = fetch_kddcup99_fxt('SF')","30","    data = fetch_kddcup99_fxt('http')","34","    data = fetch_kddcup99_fxt('smtp')","38","    fetch_func = partial(fetch_kddcup99_fxt, 'smtp')"]}]}},"2538489f4ad39d6e32ef4c2fa6297263a26e16ce":{"changes":{"sklearn\/neighbors\/_nearest_centroid.py":"MODIFY","sklearn\/neighbors\/tests\/test_nearest_centroid.py":"MODIFY","doc\/whats_new\/v0.24.rst":"MODIFY"},"diff":{"sklearn\/neighbors\/_nearest_centroid.py":[{"add":["156","            if np.all(np.ptp(X, axis=0) == 0):","157","                raise ValueError(\"All features have zero variance. \"","158","                                 \"Division by zero.\")"],"delete":[]}],"sklearn\/neighbors\/tests\/test_nearest_centroid.py":[{"add":["148","","149","","150","def test_features_zero_var():","151","    # Test that features with 0 variance throw error","152","","153","    X = np.empty((10, 2))","154","    X[:, 0] = -0.13725701","155","    X[:, 1] = -0.9853293","156","    y = np.zeros((10))","157","    y[0] = 1","158","","159","    clf = NearestCentroid(shrink_threshold=0.1)","160","    with assert_raises(ValueError):","161","        clf.fit(X, y)"],"delete":[]}],"doc\/whats_new\/v0.24.rst":[{"add":["508","- |Fix| :class:`neighbors.NearestCentroid` with a numerical `shrink_threshold`","509","  will raise a `ValueError` when fitting on data with all constant features.","510","  :pr:`18370` by :user:`Trevor Waite <trewaite>`.","511",""],"delete":[]}]}}}