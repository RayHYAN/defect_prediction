{"5674122c9755c7b6acf2c30ee0759f7087879a95":{"changes":{"sklearn\/impute\/tests\/test_impute.py":"MODIFY","sklearn\/gaussian_process\/tests\/test_gpc.py":"MODIFY","sklearn\/mixture\/tests\/test_gaussian_mixture.py":"MODIFY","sklearn\/metrics\/tests\/test_classification.py":"MODIFY","sklearn\/model_selection\/tests\/test_search.py":"MODIFY","sklearn\/manifold\/tests\/test_t_sne.py":"MODIFY","sklearn\/model_selection\/tests\/test_split.py":"MODIFY","sklearn\/feature_selection\/tests\/test_feature_select.py":"MODIFY","sklearn\/feature_extraction\/tests\/test_image.py":"MODIFY","sklearn\/metrics\/tests\/test_common.py":"MODIFY","sklearn\/model_selection\/tests\/test_validation.py":"MODIFY","sklearn\/metrics\/tests\/test_pairwise.py":"MODIFY","sklearn\/mixture\/tests\/test_bayesian_mixture.py":"MODIFY","sklearn\/inspection\/tests\/test_partial_dependence.py":"MODIFY"},"diff":{"sklearn\/impute\/tests\/test_impute.py":[{"add":["798","    d = 50","810","    assert_allclose(X_filled, X, atol=0.02)","819","    n = 70","820","    d = 70","834","    imputer = IterativeImputer(max_iter=5,","892","                               tol=1e-2,"],"delete":["798","    d = 100","810","    assert_allclose(X_filled, X, atol=0.01)","819","    n = 100","820","    d = 100","834","    imputer = IterativeImputer(max_iter=10,","892","                               tol=1e-3,"]}],"sklearn\/gaussian_process\/tests\/test_gpc.py":[{"add":["115","    # Define a dummy optimizer that simply tests 10 random hyperparameters","120","        for _ in range(10):"],"delete":["115","    # Define a dummy optimizer that simply tests 50 random hyperparameters","120","        for _ in range(50):"]}],"sklearn\/mixture\/tests\/test_gaussian_mixture.py":[{"add":["71","    def __init__(self, rng, n_samples=200, n_components=2, n_features=2,","654","            assert_allclose(ecov.error_norm(prec_pred[k]), 0, atol=0.15)","1033","    for random_state in range(15):","1034","        rand_data = RandomData(np.random.RandomState(random_state),","1035","                               n_samples=50, scale=1)"],"delete":["71","    def __init__(self, rng, n_samples=500, n_components=2, n_features=2,","654","            assert_allclose(ecov.error_norm(prec_pred[k]), 0, atol=0.1)","1033","    for random_state in range(25):","1034","        rand_data = RandomData(np.random.RandomState(random_state), scale=1)"]}],"sklearn\/metrics\/tests\/test_classification.py":[{"add":["680","@pytest.mark.parametrize('n_points', [100, 10000])"],"delete":["680","@pytest.mark.parametrize('n_points', [100, 10000, 1000000])"]}],"sklearn\/model_selection\/tests\/test_search.py":[{"add":["739","    X, y = make_blobs(n_samples=50, random_state=0)","740","    km = KMeans(random_state=0, init=\"random\", n_init=1)","1101","    params = dict(C=np.logspace(-4, 1, 3),","1102","                  gamma=np.logspace(-5, 0, 3, base=0.1))","1109","                    probability = True","1111","                else:","1112","                    probability = False","1113","                clf = SVC(probability=probability, random_state=42)"],"delete":["739","    X, y = make_blobs(random_state=0)","740","    km = KMeans(random_state=0)","1101","    params = dict(C=np.logspace(-10, 1), gamma=np.logspace(-5, 0, base=0.1))","1109","                clf = SVC(probability=True, random_state=42)"]}],"sklearn\/manifold\/tests\/test_t_sne.py":[{"add":["128","    n_samples = 200","241","@pytest.mark.parametrize(\"method\", ['exact', 'barnes_hut'])","242","@pytest.mark.parametrize(\"init\", ('random', 'pca'))","243","def test_preserve_trustworthiness_approximately(method, init):","248","    tsne = TSNE(n_components=n_components, init=init, random_state=0,","249","                method=method, n_iter=700)","250","    X_embedded = tsne.fit_transform(X)","251","    t = trustworthiness(X, X_embedded, n_neighbors=1)","252","    assert t > 0.85","272","    X = random_state.randn(50, 2)","273","    X[(np.random.randint(0, 50, 25), np.random.randint(0, 2, 25))] = 0.0","276","                random_state=0, method='exact', n_iter=500)","286","        X = random_state.randn(80, 2)","290","                    random_state=i, verbose=0, n_iter=500)","419","                    method=method, early_exaggeration=1.0, n_iter=250)","423","                    method=method, early_exaggeration=10.0, n_iter=250)","585","    X = random_state.randn(10, 2).astype(dt, copy=False)","587","                random_state=0, method=method, verbose=0,","588","                n_iter=300)","605","                random_state=0, method=method, verbose=0, n_iter=503)","722","    X = random_state.randn(50, 2)","724","                random_state=0, method='exact',","725","                n_iter=500)","747","@pytest.mark.parametrize('method', ['barnes_hut', 'exact'])","748","def test_uniform_grid(method):","760","    seeds = [0, 1, 2]","761","    n_iter = 500","828","            random_state=0, n_iter=300).fit_transform(X)","831","            random_state=0, n_iter=300).fit_transform(dist_func(X))"],"delete":["128","    n_samples = 500","241","def test_preserve_trustworthiness_approximately():","245","    methods = ['exact', 'barnes_hut']","247","    for init in ('random', 'pca'):","248","        for method in methods:","249","            tsne = TSNE(n_components=n_components, init=init, random_state=0,","250","                        method=method)","251","            X_embedded = tsne.fit_transform(X)","252","            t = trustworthiness(X, X_embedded, n_neighbors=1)","253","            assert_greater(t, 0.85, msg='Trustworthiness={:0.3f} < 0.85 '","254","                                        'for method={} and '","255","                                        'init={}'.format(t, method, init))","275","    X = random_state.randn(100, 2)","276","    X[(np.random.randint(0, 100, 50), np.random.randint(0, 2, 50))] = 0.0","279","                random_state=0, method='exact')","289","        X = random_state.randn(100, 2)","293","                    random_state=i, verbose=0)","422","                    method=method, early_exaggeration=1.0)","426","                    method=method, early_exaggeration=10.0)","588","    X = random_state.randn(50, 2).astype(dt, copy=False)","590","                random_state=0, method=method, verbose=0)","607","                random_state=0, method=method, verbose=0, n_iter=1003)","724","    X = random_state.randn(100, 2)","726","                random_state=0, method='exact')","748","def check_uniform_grid(method, seeds=[0, 1, 2], n_iter=1000):","793","@pytest.mark.parametrize('method', ['barnes_hut', 'exact'])","794","def test_uniform_grid(method):","795","    check_uniform_grid(method)","796","","797","","831","            random_state=0).fit_transform(X)","834","            random_state=0).fit_transform(dist_func(X))"]}],"sklearn\/model_selection\/tests\/test_split.py":[{"add":["1479","        gs = GridSearchCV(Ridge(solver=\"eigen\"), param_grid={'alpha': [1, .1]},"],"delete":["1479","        gs = GridSearchCV(Ridge(), param_grid={'alpha': [1, .1]},"]}],"sklearn\/feature_selection\/tests\/test_feature_select.py":[{"add":["8","import pytest","9","","410","@pytest.mark.parametrize(\"alpha\", [0.001, 0.01, 0.1])","411","@pytest.mark.parametrize(\"n_informative\", [1, 5, 10])","412","def test_select_fdr_regression(alpha, n_informative):","438","    # As per Benjamini-Hochberg, the expected false discovery rate","439","    # should be lower than alpha:","440","    # FDR = E(FP \/ (TP + FP)) <= alpha","441","    false_discovery_rate = np.mean([single_fdr(alpha, n_informative,","442","                                               random_state) for","443","                                    random_state in range(100)])","444","    assert alpha >= false_discovery_rate","446","    # Make sure that the empirical false discovery rate increases","447","    # with alpha:","448","    if false_discovery_rate != 0:","449","        assert false_discovery_rate > alpha \/ 10"],"delete":["18","from sklearn.utils.testing import assert_greater","19","from sklearn.utils.testing import assert_greater_equal","410","def test_select_fdr_regression():","436","    for alpha in [0.001, 0.01, 0.1]:","437","        for n_informative in [1, 5, 10]:","438","            # As per Benjamini-Hochberg, the expected false discovery rate","439","            # should be lower than alpha:","440","            # FDR = E(FP \/ (TP + FP)) <= alpha","441","            false_discovery_rate = np.mean([single_fdr(alpha, n_informative,","442","                                                       random_state) for","443","                                            random_state in range(100)])","444","            assert_greater_equal(alpha, false_discovery_rate)","446","            # Make sure that the empirical false discovery rate increases","447","            # with alpha:","448","            if false_discovery_rate != 0:","449","                assert_greater(false_discovery_rate, alpha \/ 10)"]}],"sklearn\/feature_extraction\/tests\/test_image.py":[{"add":["65","    # subsample by 4 to reduce run time","66","    face = face[::4, ::4]","81","","82","    # subsample by 4 to reduce run time","83","    face = face[::4, ::4]","84",""],"delete":[]}],"sklearn\/metrics\/tests\/test_common.py":[{"add":["1199","    _, ya = make_multilabel_classification(n_features=1, n_classes=10,","1200","                                           random_state=0, n_samples=50,","1202","    _, yb = make_multilabel_classification(n_features=1, n_classes=10,","1203","                                           random_state=1, n_samples=50,"],"delete":["1199","    _, ya = make_multilabel_classification(n_features=1, n_classes=20,","1200","                                           random_state=0, n_samples=100,","1202","    _, yb = make_multilabel_classification(n_features=1, n_classes=20,","1203","                                           random_state=1, n_samples=100,"]}],"sklearn\/model_selection\/tests\/test_validation.py":[{"add":["814","                         cross_val_predict,","815","                         LogisticRegression(solver=\"liblinear\"),","822","    preds = cross_val_predict(LogisticRegression(solver=\"liblinear\"), X, y,","828","    preds = cross_val_predict(LogisticRegression(solver=\"liblinear\"), X, y,","869","    preds = cross_val_predict(LogisticRegression(solver=\"liblinear\"), X, y,","875","    preds = cross_val_predict(LogisticRegression(solver=\"liblinear\"), X, y,","883","    preds = cross_val_predict(LogisticRegression(solver=\"liblinear\"), X, y,","889","    preds = cross_val_predict(LogisticRegression(solver=\"liblinear\"), X, y,","926","    predictions = cross_val_predict(LogisticRegression(solver=\"liblinear\"),","927","                                    X.tolist(),","929","    predictions = cross_val_predict(LogisticRegression(solver=\"liblinear\"),","930","                                    X,","966","    clf = LogisticRegression(random_state=1, solver=\"liblinear\")","1397","    X, y = make_classification(n_classes=2,  random_state=0)","1411","    check_cross_val_predict_with_method_binary(","1412","            LogisticRegression(solver=\"liblinear\"))","1413","    check_cross_val_predict_with_method_multiclass(","1414","            LogisticRegression(solver=\"liblinear\"))","1433","    est = GridSearchCV(LogisticRegression(random_state=42, solver=\"liblinear\"),","1449","    est = OneVsRestClassifier(LogisticRegression(solver=\"liblinear\",","1450","                                                 random_state=0))","1490","    est = LogisticRegression(solver=\"liblinear\")","1548","        est = LogisticRegression(solver=\"liblinear\")"],"delete":["814","                         cross_val_predict, LogisticRegression(),","821","    preds = cross_val_predict(LogisticRegression(), X, y,","827","    preds = cross_val_predict(LogisticRegression(), X, y,","868","    preds = cross_val_predict(LogisticRegression(), X, y,","874","    preds = cross_val_predict(LogisticRegression(), X, y,","882","    preds = cross_val_predict(LogisticRegression(), X, y,","888","    preds = cross_val_predict(LogisticRegression(), X, y,","925","    predictions = cross_val_predict(LogisticRegression(), X.tolist(),","927","    predictions = cross_val_predict(LogisticRegression(), X,","963","    clf = LogisticRegression(random_state=1)","1394","    X, y = make_classification(n_classes=2, random_state=0)","1408","    check_cross_val_predict_with_method_binary(LogisticRegression())","1409","    check_cross_val_predict_with_method_multiclass(LogisticRegression())","1428","    est = GridSearchCV(LogisticRegression(random_state=42),","1444","    est = OneVsRestClassifier(LogisticRegression(random_state=0))","1484","    est = LogisticRegression()","1542","        est = LogisticRegression()"]}],"sklearn\/metrics\/tests\/test_pairwise.py":[{"add":["560","    X = rng.random_sample((200, 4))","571","    Y = rng.random_sample((100, 4))","1105","    with config_context(working_memory=0.1):  # to have more than 1 chunk","1107","        X = rng.random_sample((100, 10))","1117","            Y = rng.random_sample((100, 10))"],"delete":["560","    X = rng.random_sample((400, 4))","571","    Y = rng.random_sample((200, 4))","1105","    with config_context(working_memory=1):  # to have more than 1 chunk","1107","        X = rng.random_sample((1000, 10))","1117","            Y = rng.random_sample((1000, 10))"]}],"sklearn\/mixture\/tests\/test_bayesian_mixture.py":[{"add":["298","                warm_start=True, max_iter=1, random_state=rng, tol=1e-3)","437","    rand_data = RandomData(rng, n_samples=50, scale=7)","455","    X = np.random.RandomState(0).randn(50, 5)"],"delete":["298","                warm_start=True, max_iter=1, random_state=rng, tol=1e-4)","437","    rand_data = RandomData(rng, scale=7)","455","    X = np.random.RandomState(0).randn(1000, 5)"]}],"sklearn\/inspection\/tests\/test_partial_dependence.py":[{"add":["39","binary_classification_data = (make_classification(n_samples=50,","40","                                                  random_state=0), 1)","41","multiclass_classification_data = (make_classification(n_samples=50,","42","                                                      n_classes=3,","45","regression_data = (make_regression(n_samples=50, random_state=0), 1)","46","multioutput_regression_data = (make_regression(n_samples=50, n_targets=2,","47","                                               random_state=0), 2)"],"delete":["39","binary_classification_data = (make_classification(random_state=0), 1)","40","multiclass_classification_data = (make_classification(n_classes=3,","43","regression_data = (make_regression(random_state=0), 1)","44","multioutput_regression_data = (make_regression(n_targets=2, random_state=0), 2)"]}]}},"24d4b2c2f3bae149407db57182643405426f275c":{"changes":{"examples\/neural_networks\/plot_mlp_training_curves.py":"MODIFY"},"diff":{"examples\/neural_networks\/plot_mlp_training_curves.py":[{"add":["16","","17","import warnings","18","","20","","24","from sklearn.exceptions import ConvergenceWarning","59","","72","","73","        # some parameter combinations will not converge as can be seen on the","74","        # plots so they are ignored here","75","        with warnings.catch_warnings():","76","            warnings.filterwarnings(\"ignore\", category=ConvergenceWarning,","77","                                    module=\"sklearn\")","78","            mlp.fit(X, y)","79","","84","        ax.plot(mlp.loss_curve_, label=label, **args)"],"delete":["66","        mlp.fit(X, y)","71","            ax.plot(mlp.loss_curve_, label=label, **args)"]}]}},"14dd3cb38487a78a28a6d794a80fd012a92c6656":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/cluster\/tests\/test_optics.py":"MODIFY","sklearn\/cluster\/optics_.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["35","- |Fix| Fixed a bug in :class:`cluster.OPTICS` where users were unable to pass","36","  float `min_samples` and `min_cluster_size`. :pr:`14496` by","37","  :user:`Fabian Klopfer <someusername1>`","38","  and :user:`Hanmin Qin <qinhanmin2014>`.","39",""],"delete":[]}],"sklearn\/cluster\/tests\/test_optics.py":[{"add":["103","    # check float min_samples and min_cluster_size","104","    clust = OPTICS(min_samples=0.1, min_cluster_size=0.08,","105","                   max_eps=20, cluster_method='xi',","106","                   xi=0.4).fit(X)","107","    assert_array_equal(clust.labels_, expected_labels)","108",""],"delete":[]}],"sklearn\/cluster\/optics_.py":[{"add":["46","    min_samples : int > 1 or float between 0 and 1 (default=5)","343","    min_samples : int > 1 or float between 0 and 1","439","        min_samples = max(2, int(min_samples * n_samples))","584","    min_samples : int > 1 or float between 0 and 1","621","        min_samples = max(2, int(min_samples * n_samples))","626","        min_cluster_size = max(2, int(min_cluster_size * n_samples))","755","    min_samples : int > 1","759","    min_cluster_size : int > 1","760","        Minimum number of samples in an OPTICS cluster."],"delete":["46","    min_samples : int > 1 or float between 0 and 1 (default=None)","343","    min_samples : int (default=5)","439","        min_samples = max(2, min_samples * n_samples)","584","    min_samples : int > 1 or float between 0 and 1 (default=None)","621","        min_samples = max(2, min_samples * n_samples)","626","        min_cluster_size = max(2, min_cluster_size * n_samples)","755","    min_samples : int > 1 or float between 0 and 1 (default=None)","758","        Expressed as an absolute number or a fraction of the number of samples","759","        (rounded to be at least 2).","761","    min_cluster_size : int > 1 or float between 0 and 1","762","        Minimum number of samples in an OPTICS cluster, expressed as an","763","        absolute number or a fraction of the number of samples (rounded","764","        to be at least 2)."]}]}},"9e3216ecf99612b8b733e13a154a6c92e9856798":{"changes":{"sklearn\/metrics\/_plot\/roc_curve.py":"MODIFY","sklearn\/metrics\/_plot\/tests\/test_plot_roc_curve.py":"MODIFY"},"diff":{"sklearn\/metrics\/_plot\/roc_curve.py":[{"add":["152","                            sample_weight=sample_weight,"],"delete":[]}],"sklearn\/metrics\/_plot\/tests\/test_plot_roc_curve.py":[{"add":["2","import numpy as np","54","@pytest.mark.parametrize(\"with_sample_weight\", [True, False])","55","@pytest.mark.parametrize(\"drop_intermediate\", [True, False])","56","def test_plot_roc_curve(pyplot, response_method, data_binary,","57","                        with_sample_weight, drop_intermediate):","59","    if with_sample_weight:","60","        rng = np.random.RandomState(42)","61","        sample_weight = rng.randint(1, 4, size=(X.shape[0]))","62","    else:","63","        sample_weight = None","68","    viz = plot_roc_curve(lr, X, y, alpha=0.8, sample_weight=sample_weight,","69","                         drop_intermediate=drop_intermediate)","75","    fpr, tpr, _ = roc_curve(y, y_pred, sample_weight=sample_weight,","76","                            drop_intermediate=drop_intermediate)","93","    assert viz.ax_.get_ylabel() == \"True Positive Rate\"","94","    assert viz.ax_.get_xlabel() == \"False Positive Rate\""],"delete":["53","def test_plot_roc_curve(pyplot, response_method, data_binary):","59","    viz = plot_roc_curve(lr, X, y, alpha=0.8)","65","    fpr, tpr, _ = roc_curve(y, y_pred)"]}]}},"1a14920464acd9c16fcc0187f89cbbd738068ebc":{"changes":{"sklearn\/metrics\/base.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY","sklearn\/metrics\/tests\/test_ranking.py":"MODIFY"},"diff":{"sklearn\/metrics\/base.py":[{"add":["123","        if average_weight is not None:","124","            # Scores with 0 weights are forced to be 0, preventing the average","125","            # score from being affected by 0-weighted NaN elements.","126","            average_weight = np.asarray(average_weight)","127","            score[average_weight == 0] = 0"],"delete":[]}],"doc\/whats_new\/v0.22.rst":[{"add":["235","- |Enhancement| Allow computing averaged metrics in the case of no true positives.","236","  :pr:`14595` by `Andreas M¨¹ller`_.","237",""],"delete":[]}],"sklearn\/metrics\/tests\/test_ranking.py":[{"add":["336","    tpr, fpr, _ = assert_warns(UndefinedMetricWarning, roc_curve, y_true,","337","                               y_score)","345","    tpr, fpr, _ = assert_warns(UndefinedMetricWarning, roc_curve, y_true,","346","                               y_score)","664","","812","    with np.errstate(all=\"ignore\"):","813","        # if one class is never present weighted should not be NaN","814","        y_true = np.array([[0, 0], [0, 1]])","815","        y_score = np.array([[0, 0], [0, 1]])","816","        assert_almost_equal(average_precision_score(y_true, y_score,","817","                            average=\"weighted\"), 1)","818",""],"delete":["336","    tpr, fpr, _ = assert_warns(UndefinedMetricWarning, roc_curve, y_true, y_score)","344","    tpr, fpr, _ = assert_warns(UndefinedMetricWarning, roc_curve, y_true, y_score)"]}]}},"aada3ead7ece17f48da00c10d6eb13dc71d4cb84":{"changes":{"sklearn\/feature_extraction\/text.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY","sklearn\/feature_extraction\/tests\/test_text.py":"MODIFY"},"diff":{"sklearn\/feature_extraction\/text.py":[{"add":["131","    try:","132","        # If `s` is ASCII-compatible, then it does not contain any accented","133","        # characters and we can avoid an expensive list comprehension","134","        s.encode(\"ASCII\", errors=\"strict\")","136","    except UnicodeEncodeError:","137","        normalized = unicodedata.normalize('NFKD', s)"],"delete":["131","    normalized = unicodedata.normalize('NFKD', s)","132","    if normalized == s:","134","    else:"]}],"doc\/whats_new\/v0.22.rst":[{"add":["257","- |Fix| :func:`feature_extraction.text.strip_accents_unicode` now correctly","258","  removes accents from strings that are in NFKD normalized form. :pr:`15100` by","259","  :user:`Daniel Grady <DGrady>`.","260",""],"delete":[]}],"sklearn\/feature_extraction\/tests\/test_text.py":[{"add":["99","    # strings that are already decomposed","100","    a = \"o\\u0308\"  # o with diaresis","101","    expected = \"o\"","102","    assert strip_accents_unicode(a) == expected","103","","104","    # combining marks by themselves","105","    a = \"\\u0300\\u0301\\u0302\\u0303\"","106","    expected = \"\"","107","    assert strip_accents_unicode(a) == expected","108","","109","    # Multiple combining marks on one character","110","    a = \"o\\u0308\\u0304\"","111","    expected = \"o\"","112","    assert strip_accents_unicode(a) == expected","113",""],"delete":[]}]}},"283a61384e094165ed06186230f9f002b81296da":{"changes":{"doc\/developers\/advanced_installation.rst":"MODIFY","sklearn\/_build_utils\/openmp_helpers.py":"MODIFY","build_tools\/azure\/install.sh":"MODIFY"},"diff":{"doc\/developers\/advanced_installation.rst":[{"add":["138","    export LDFLAGS=\"$LDFLAGS -Wl,-rpath,\/usr\/local\/opt\/libomp\/lib -L\/usr\/local\/opt\/libomp\/lib -lomp\"","157","    export LDFLAGS=\"$LDFLAGS -Wl,-rpath,\/usr\/local\/lib -L\/usr\/local\/lib -lomp\""],"delete":["138","    export LDFLAGS=\"$LDFLAGS -L\/usr\/local\/opt\/libomp\/lib -lomp\"","139","    export DYLD_LIBRARY_PATH=\/usr\/local\/opt\/libomp\/lib","158","    export LDFLAGS=\"$LDFLAGS -L\/usr\/local\/lib -lomp\"","159","    export DYLD_LIBRARY_PATH=\/usr\/local\/lib"]}],"sklearn\/_build_utils\/openmp_helpers.py":[{"add":["52","        # export LDFLAGS=\"$LDFLAGS -Wl,-rpath,\/usr\/local\/opt\/libomp\/lib","53","        #                          -L\/usr\/local\/opt\/libomp\/lib -lomp\"","88","                extra_preargs = extra_preargs.strip().split(\" \")","89","                extra_preargs = [","90","                    flag for flag in extra_preargs","91","                    if flag.startswith(('-L', '-Wl,-rpath', '-l'))]"],"delete":["52","        # export LDFLAGS=\"$LDFLAGS -L\/usr\/local\/opt\/libomp\/lib -lomp\"","53","        # export DYLD_LIBRARY_PATH=\/usr\/local\/opt\/libomp\/lib","88","                extra_preargs = extra_preargs.split(\" \")","89","            else:","90","                extra_preargs = []"]}],"build_tools\/azure\/install.sh":[{"add":["16","    export LDFLAGS=\"$LDFLAGS -Wl,-rpath,\/usr\/local\/opt\/libomp\/lib -L\/usr\/local\/opt\/libomp\/lib -lomp\""],"delete":["16","    export LDFLAGS=\"$LDFLAGS -L\/usr\/local\/opt\/libomp\/lib -lomp\"","17","    export DYLD_LIBRARY_PATH=\/usr\/local\/opt\/libomp\/lib"]}]}},"42706ebf2d4cac551dc8348dd0d049461cf2c3f8":{"changes":{"azure-pipelines.yml":"MODIFY","doc\/developers\/advanced_installation.rst":"MODIFY","sklearn\/utils\/fixes.py":"MODIFY","doc\/tutorial\/machine_learning_map\/pyparsing.py":"MODIFY",".circleci\/config.yml":"MODIFY","README.rst":"MODIFY","build_tools\/azure\/posix-32.yml":"MODIFY","setup.py":"MODIFY","doc\/templates\/index.html":"MODIFY","sklearn\/utils\/_mask.py":"MODIFY","build_tools\/circle\/build_test_pypy.sh":"MODIFY","sklearn\/gaussian_process\/tests\/test_gpr.py":"MODIFY","sklearn\/feature_extraction\/tests\/test_text.py":"MODIFY","doc\/install.rst":"MODIFY","build_tools\/azure\/install.sh":"MODIFY","sklearn\/utils\/_testing.py":"MODIFY"},"diff":{"azure-pipelines.yml":[{"add":["5","    vmImage: ubuntu-18.04","29","    vmImage: ubuntu-18.04","46","    vmImage: ubuntu-18.04","50","      # versions of numpy, scipy with ATLAS that comes with Ubuntu Bionic 18.04","51","      # i.e. numpy 1.13.3 and scipy 0.19","52","      py36_ubuntu_atlas:","54","        PYTHON_VERSION: '3.6'","56","      # Linux + Python 3.6 build with OpenBLAS and without SITE_JOBLIB","57","      py36_conda_openblas:","59","        PYTHON_VERSION: '3.6'","61","        NUMPY_VERSION: '1.13.3'","62","        SCIPY_VERSION: '0.19.1'","67","        PILLOW_VERSION: '4.2.1'","68","        MATPLOTLIB_VERSION: '2.1.1'","69","        # latest version of joblib available in conda for Python 3.6","70","        JOBLIB_VERSION: '0.13.2'","86","    vmImage: ubuntu-18.04","89","      py36_ubuntu_atlas_32bit:","91","        PYTHON_VERSION: '3.6'","92","        JOBLIB_VERSION: '0.13'","137","      py36_pip_openblas_32bit:","138","        PYTHON_VERSION: '3.6'"],"delete":["5","    vmImage: ubuntu-16.04","29","    vmImage: ubuntu-16.04","46","    vmImage: ubuntu-16.04","50","      # versions of numpy, scipy with ATLAS that comes with Ubuntu Xenial 16.04","51","      # i.e. numpy 1.11 and scipy 0.17","52","      py35_ubuntu_atlas:","54","        PYTHON_VERSION: '3.5'","56","      # Linux + Python 3.5 build with OpenBLAS and without SITE_JOBLIB","57","      py35_conda_openblas:","59","        PYTHON_VERSION: '3.5'","61","        NUMPY_VERSION: '1.11.0'","62","        SCIPY_VERSION: '0.17.0'","67","        PILLOW_VERSION: '4.0.0'","68","        MATPLOTLIB_VERSION: '1.5.1'","69","        # later version of joblib are not packaged in conda for Python 3.5","70","        JOBLIB_VERSION: '0.12.3'","86","    vmImage: ubuntu-16.04","89","      py35_ubuntu_atlas_32bit:","91","        PYTHON_VERSION: '3.5'","92","        JOBLIB_VERSION: '0.11'","137","      py35_pip_openblas_32bit:","138","        PYTHON_VERSION: '3.5'"]}],"doc\/developers\/advanced_installation.rst":[{"add":["85","- Python (>= 3.6),","86","- NumPy (>= 1.13.3),","87","- SciPy (>= 0.19),"],"delete":["85","- Python (>= 3.5),","86","- NumPy (>= 1.11),","87","- SciPy (>= 0.17),"]}],"sklearn\/utils\/fixes.py":[{"add":["175","def _object_dtype_isnan(X):","176","    return X != X"],"delete":["175","# Fix for behavior inconsistency on numpy.equal for object dtypes.","176","# For numpy versions < 1.13, numpy.equal tests element-wise identity of objects","177","# instead of equality. This fix returns the mask of NaNs in an array of","178","# numerical or object values for all numpy versions.","179","if np_version < (1, 13):","180","    def _object_dtype_isnan(X):","181","        return np.frompyfunc(lambda x: x != x, 1, 1)(X).astype(bool)","182","else:","183","    def _object_dtype_isnan(X):","184","        return X != X"]}],"doc\/tutorial\/machine_learning_map\/pyparsing.py":[{"add":["1022","    def extract_stack(limit=0):\r","1023","        offset = -2\r","1024","        frame_summary = traceback.extract_stack(limit=-offset+limit-1)[offset]\r","1025","        return [(frame_summary.filename, frame_summary.lineno)]\r","1026","    def extract_tb(tb, limit=0):\r","1027","        frames = traceback.extract_tb(tb, limit=limit)\r","1028","        frame_summary = frames[-1]\r","1029","        return [(frame_summary.filename, frame_summary.lineno)]\r"],"delete":["1022","    # traceback return data structure changed in Py3.5 - normalize back to plain tuples\r","1023","    if system_version[:2] >= (3,5):\r","1024","        def extract_stack(limit=0):\r","1025","            # special handling for Python 3.5.0 - extra deep call stack by 1\r","1026","            offset = -3 if system_version == (3,5,0) else -2\r","1027","            frame_summary = traceback.extract_stack(limit=-offset+limit-1)[offset]\r","1028","            return [(frame_summary.filename, frame_summary.lineno)]\r","1029","        def extract_tb(tb, limit=0):\r","1030","            frames = traceback.extract_tb(tb, limit=limit)\r","1031","            frame_summary = frames[-1]\r","1032","            return [(frame_summary.filename, frame_summary.lineno)]\r","1033","    else:\r","1034","        extract_stack = traceback.extract_stack\r","1035","        extract_tb = traceback.extract_tb\r"]}],".circleci\/config.yml":[{"add":["11","      - PYTHON_VERSION: 3.6","12","      - NUMPY_VERSION: 1.13.3","13","      - SCIPY_VERSION: 0.19.1","14","      - MATPLOTLIB_VERSION: 2.1.1","21","      - SCIKIT_IMAGE_VERSION: 0.13"],"delete":["11","      - PYTHON_VERSION: 3.5","12","      - NUMPY_VERSION: 1.11.0","13","      - SCIPY_VERSION: 0.17.0","14","      - MATPLOTLIB_VERSION: 1.5.1","21","      - SCIKIT_IMAGE_VERSION: 0.12.3"]}],"README.rst":[{"add":["49","- Python (>= 3.6)","50","- NumPy (>= 1.13.3)","51","- SciPy (>= 0.19.1)","55","scikit-learn 0.23 and later require Python 3.6 or newer.","58","and classes end with \"Display\") require Matplotlib (>= 2.1.1). For running the","59","examples Matplotlib >= 2.1.1 is required. A few examples require","60","scikit-image >= 0.13, a few examples require pandas >= 0.18.0."],"delete":["49","- Python (>= 3.5)","50","- NumPy (>= 1.11.0)","51","- SciPy (>= 0.17.0)","55","scikit-learn 0.21 and later require Python 3.5 or newer.","58","and classes end with \"Display\") require Matplotlib (>= 1.5.1). For running the","59","examples Matplotlib >= 1.5.1 is required. A few examples require","60","scikit-image >= 0.12.3, a few examples require pandas >= 0.18.0."]}],"build_tools\/azure\/posix-32.yml":[{"add":["42","        i386\/ubuntu:18.04"],"delete":["42","        i386\/ubuntu:16.04"]}],"setup.py":[{"add":["17","    # Python 2 compat: just to be able to declare that Python >=3.6 is needed.","54","    SCIPY_MIN_VERSION = '0.19.1'","55","    NUMPY_MIN_VERSION = '1.13.3'","141","    # that python 3.6 is required.","255","                    python_requires=\">=3.6\",","283","        if sys.version_info < (3, 6):","285","                \"Scikit-learn requires Python 3.6 or later. The current\""],"delete":["17","    # Python 2 compat: just to be able to declare that Python >=3.5 is needed.","54","    SCIPY_MIN_VERSION = '0.17.0'","55","    NUMPY_MIN_VERSION = '1.11.0'","141","    # that python 3.5 is required.","246","                                 'Programming Language :: Python :: 3.5',","256","                    python_requires=\">=3.5\",","284","        if sys.version_info < (3, 5):","286","                \"Scikit-learn requires Python 3.5 or later. The current\""]}],"doc\/templates\/index.html":[{"add":["158","        <li><strong>Scikit-learn from 0.23 requires Python 3.6 or greater.<\/strong>","159","        <\/li>"],"delete":[]}],"sklearn\/utils\/_mask.py":[{"add":["7","    \"\"\"Compute the boolean mask X == value_to_mask.\"\"\"","18","        return X == value_to_mask"],"delete":["7","    \"\"\"Compute the boolean mask X == missing_values.\"\"\"","18","        # X == value_to_mask with object dtypes does not always perform","19","        # element-wise for old versions of numpy","20","        return np.equal(X, value_to_mask)"]}],"build_tools\/circle\/build_test_pypy.sh":[{"add":["5","apt-get -yq install libatlas-base-dev liblapack-dev gfortran ccache libopenblas-dev"],"delete":["5","apt-get -yq install libatlas-dev libatlas-base-dev liblapack-dev gfortran ccache libopenblas-dev"]}],"sklearn\/gaussian_process\/tests\/test_gpr.py":[{"add":["5","import sys","49","    if sys.maxsize <= 2 ** 32 and sys.version_info[:2] == (3, 6):","50","        pytest.xfail(\"This test may fail on 32bit Py3.6\")","51","","76","    if sys.maxsize <= 2 ** 32 and sys.version_info[:2] == (3, 6):","77","        pytest.xfail(\"This test may fail on 32bit Py3.6\")","78","","184","    if sys.maxsize <= 2 ** 32 and sys.version_info[:2] == (3, 6):","185","        pytest.xfail(\"This test may fail on 32bit Py3.6\")","186",""],"delete":[]}],"sklearn\/feature_extraction\/tests\/test_text.py":[{"add":[],"delete":["34","                                   clean_warning_registry,","390","    clean_warning_registry()"]}],"doc\/install.rst":[{"add":["134","and classes end with \"Display\") require Matplotlib (>= 2.1.1). For running the","135","examples Matplotlib >= 2.1.1 is required. A few examples require","136","scikit-image >= 0.13, a few examples require pandas >= 0.18.0.","141","    Scikit-learn 0.21 supported Python 3.5-3.7.","142","    Scikit-learn 0.22 supported Python 3.5-3.8.","143","    Scikit-learn now requires Python 3.6 or newer."],"delete":["134","and classes end with \"Display\") require Matplotlib (>= 1.5.1). For running the","135","examples Matplotlib >= 1.5.1 is required. A few examples require","136","scikit-image >= 0.12.3, a few examples require pandas >= 0.18.0.","141","    Scikit-learn now requires Python 3.5 or newer."]}],"build_tools\/azure\/install.sh":[{"add":["3","set -x","76","    sudo apt-get install python3-scipy python3-matplotlib libatlas3-base libatlas-base-dev python3-virtualenv","82","    apt-get install -y python3-dev python3-scipy python3-matplotlib libatlas3-base libatlas-base-dev python3-virtualenv"],"delete":["75","    sudo apt-get install python3-scipy python3-matplotlib libatlas3-base libatlas-base-dev libatlas-dev python3-virtualenv","81","    apt-get install -y python3-dev python3-scipy python3-matplotlib libatlas3-base libatlas-base-dev libatlas-dev python3-virtualenv"]}],"sklearn\/utils\/_testing.py":[{"add":[],"delete":["567","def clean_warning_registry():","568","    \"\"\"Clean Python warning registry for easier testing of warning messages.","569","","570","    When changing warning filters this function is not necessary with","571","    Python3.5+, as __warningregistry__ will be re-set internally.","572","    See https:\/\/bugs.python.org\/issue4180 and","573","    https:\/\/bugs.python.org\/issue21724 for more details.","574","","575","    \"\"\"","576","    for mod in sys.modules.values():","577","        registry = getattr(mod, \"__warningregistry__\", None)","578","        if registry is not None:","579","            registry.clear()","580","","581",""]}]}},"b580ad5dfd286c77fb06cf5803fad27ea2d57c0d":{"changes":{"sklearn\/utils\/estimator_checks.py":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_gradient_boosting.py":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/grower.py":"MODIFY"},"diff":{"sklearn\/utils\/estimator_checks.py":[{"add":["2403","        # Since the link function from decision_function() to predict_proba()","2404","        # is sometimes not precise enough (typically expit), we round to the","2405","        # 10th decimal to avoid numerical issues.","2406","        a = estimator.predict_proba(X_test)[:, 1].round(decimals=10)","2407","        b = estimator.decision_function(X_test).round(decimals=10)"],"delete":["2403","        a = estimator.predict_proba(X_test)[:, 1]","2404","        b = estimator.decision_function(X_test)"]}],"sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_gradient_boosting.py":[{"add":["174","","175","","176","@pytest.mark.parametrize('data', [","177","    make_classification(random_state=0, n_classes=2),","178","    make_classification(random_state=0, n_classes=3, n_informative=3)","179","], ids=['binary_crossentropy', 'categorical_crossentropy'])","180","def test_zero_division_hessians(data):","181","    # non regression test for issue #14018","182","    # make sure we avoid zero division errors when computing the leaves values.","183","","184","    # If the learning rate is too high, the raw predictions are bad and will","185","    # saturate the softmax (or sigmoid in binary classif). This leads to","186","    # probabilities being exactly 0 or 1, gradients being constant, and","187","    # hessians being zero.","188","    X, y = data","189","    gb = HistGradientBoostingClassifier(learning_rate=100, max_iter=10)","190","    gb.fit(X, y)"],"delete":[]}],"sklearn\/ensemble\/_hist_gradient_boosting\/grower.py":[{"add":["18","from .types import Y_DTYPE","19","","20","","21","EPS = np.finfo(Y_DTYPE).eps  # to avoid zero division errors","404","            node.sum_hessians + self.splitter.l2_regularization + EPS)"],"delete":["400","            node.sum_hessians + self.splitter.l2_regularization)"]}]}},"a89462b59d1bb1733203bbfcce95ba77d99ba762":{"changes":{"sklearn\/svm\/base.py":"MODIFY","sklearn\/svm\/tests\/test_svm.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY"},"diff":{"sklearn\/svm\/base.py":[{"add":["289","        if not n_SV:","290","            self.dual_coef_ = sp.csr_matrix([])","291","        else:","292","            dual_coef_indptr = np.arange(0, dual_coef_indices.size + 1,","293","                                         dual_coef_indices.size \/ n_class)","294","            self.dual_coef_ = sp.csr_matrix(","295","                (dual_coef_data, dual_coef_indices, dual_coef_indptr),","296","                (n_class, n_SV))"],"delete":["289","        dual_coef_indptr = np.arange(0, dual_coef_indices.size + 1,","290","                                     dual_coef_indices.size \/ n_class)","291","        self.dual_coef_ = sp.csr_matrix(","292","            (dual_coef_data, dual_coef_indices, dual_coef_indptr),","293","            (n_class, n_SV))"]}],"sklearn\/svm\/tests\/test_svm.py":[{"add":["692","def test_sparse_fit_support_vectors_empty():","693","    # Regression test for #14893","694","    X_train = sparse.csr_matrix([[0, 1, 0, 0],","695","                                 [0, 0, 0, 1],","696","                                 [0, 0, 1, 0],","697","                                 [0, 0, 0, 1]])","698","    y_train = np.array([0.04, 0.04, 0.10, 0.16])","699","    model = svm.SVR(kernel='linear')","700","    model.fit(X_train, y_train)","701","    assert not model.support_vectors_.data.size","702","    assert not model.dual_coef_.data.size","703","","704",""],"delete":[]}],"doc\/whats_new\/v0.22.rst":[{"add":["574","- |Fix| fixed a bug in :class:`BaseLibSVM._sparse_fit` where n_SV=0 raised a","575","  ZeroDivisionError. :pr:`14894` by :user:`Danna Naser <danna-naser>`.","576",""],"delete":[]}]}},"32d5a763acdd34a295187c45a49a5feb7aca6f4a":{"changes":{"sklearn\/utils\/weight_vector.pyx":"MODIFY","sklearn\/utils\/weight_vector.pxd":"MODIFY","sklearn\/linear_model\/sgd_fast.pyx":"MODIFY"},"diff":{"sklearn\/utils\/weight_vector.pyx":[{"add":["43","    def __cinit__(self, double [::1] w, double [::1] aw):","49","        self.sq_norm = _dot(<int>w.shape[0], &w[0], 1, &w[0], 1)","51","        self.w_data_ptr = &w[0]","52","        if aw is not None:","53","            self.aw_data_ptr = &aw[0]","172","        if self.aw_data_ptr != NULL:","173","            _axpy(self.n_features, self.average_a,","174","                  self.w_data_ptr, 1, self.aw_data_ptr, 1)","175","            _scal(self.n_features, 1.0 \/ self.average_b, self.aw_data_ptr, 1)","179","        _scal(self.n_features, self.wscale, self.w_data_ptr, 1)"],"delete":["36","    w_data_ptr : double*","37","        A pointer to the data of the numpy array.","45","","46","    def __cinit__(self,","47","                  np.ndarray[double, ndim=1, mode='c'] w,","48","                  np.ndarray[double, ndim=1, mode='c'] aw):","49","        cdef double *wdata = <double *>w.data","50","","54","        self.w = w","55","        self.w_data_ptr = wdata","58","        self.sq_norm = _dot(<int>w.shape[0], wdata, 1, wdata, 1)","60","        self.aw = aw","61","        if self.aw is not None:","62","            self.aw_data_ptr = <double *>aw.data","181","        if self.aw is not None:","182","            _axpy(<int>self.aw.shape[0], self.average_a,","183","                  <double *>self.w.data, 1, <double *>self.aw.data, 1)","184","            _scal(<int>self.aw.shape[0], 1.0 \/ self.average_b,","185","                  <double *>self.aw.data, 1)","189","        _scal(<int>self.w.shape[0], self.wscale, <double *>self.w.data, 1)"]}],"sklearn\/utils\/weight_vector.pxd":[{"add":[],"delete":["10","    cdef np.ndarray w","11","    cdef np.ndarray aw"]}],"sklearn\/linear_model\/sgd_fast.pyx":[{"add":["835","        if wscale * z > 0.0:","839","        elif wscale * z < 0.0:"],"delete":["835","        if wscale * w_data_ptr[idx] > 0.0:","839","        elif wscale * w_data_ptr[idx] < 0.0:"]}]}},"e94f5de906c70cda43854f47925879fcf02204f4":{"changes":{"doc\/whats_new\/v0.22.rst":"MODIFY","sklearn\/decomposition\/sparse_pca.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.22.rst":[{"add":["19","- :class:`decomposition.SparsePCA` where `normalize_components` has no effect","20","  due to deprecation."],"delete":["19","..","20","    TO FILL IN AS WE GO"]}],"sklearn\/decomposition\/sparse_pca.py":[{"add":["226","        X = X - self.mean_"],"delete":["226","","227","        if self.normalize_components:","228","            X = X - self.mean_","233","        if not self.normalize_components:","234","            s = np.sqrt((U ** 2).sum(axis=0))","235","            s[s == 0] = 1","236","            U \/= s","237",""]}]}},"e55e37c4ac2096ad039523d441c160da6c79fef5":{"changes":{"sklearn\/kernel_approximation.py":"MODIFY","sklearn\/tests\/test_kernel_approximation.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY"},"diff":{"sklearn\/kernel_approximation.py":[{"add":["520","","603","        if not callable(self.kernel) and self.kernel != 'precomputed':","612","                                 \"Nystroem if using a callable \"","613","                                 \"or precomputed kernel\")"],"delete":["602","        if not callable(self.kernel):","611","                                 \"Nystroem if using a callable kernel.\")"]}],"sklearn\/tests\/test_kernel_approximation.py":[{"add":["256","","257","","258","def test_nystroem_precomputed_kernel():","259","    # Non-regression: test Nystroem on precomputed kernel.","260","    # PR - 14706","261","    rnd = np.random.RandomState(12)","262","    X = rnd.uniform(size=(10, 4))","263","","264","    K = polynomial_kernel(X, degree=2, coef0=.1)","265","    nystroem = Nystroem(kernel='precomputed', n_components=X.shape[0])","266","    X_transformed = nystroem.fit_transform(K)","267","    assert_array_almost_equal(np.dot(X_transformed, X_transformed.T), K)","268","","269","    # if degree, gamma or coef0 is passed, we raise a ValueError","270","    msg = \"Don't pass gamma, coef0 or degree to Nystroem\"","271","    params = ({'gamma': 1}, {'coef0': 1}, {'degree': 2})","272","    for param in params:","273","        ny = Nystroem(kernel='precomputed', n_components=X.shape[0],","274","                      **param)","275","        with pytest.raises(ValueError, match=msg):","276","            ny.fit(K)"],"delete":[]}],"doc\/whats_new\/v0.22.rst":[{"add":["202",":mod:`sklearn.kernel_approximation`","203","...................................","204","","205","-|FIX| Fixed a bug where :class:`kernel_approximation.Nystroem` raised a","206"," `KeyError` when using `kernel=\"precomputed\"`.","207"," :pr:`14706` by :user:`Venkatachalam N <venkyyuvy>`. ","208",""],"delete":[]}]}},"847b3468c7c60c7d89c21f5e854b6c492f7261d5":{"changes":{"sklearn\/utils\/fixes.py":"MODIFY","sklearn\/multioutput.py":"MODIFY","sklearn\/ensemble\/forest.py":"MODIFY"},"diff":{"sklearn\/utils\/fixes.py":[{"add":[],"delete":["157","def parallel_helper(obj, methodname, *args, **kwargs):","158","    \"\"\"Workaround for Python 2 limitations of pickling instance methods","159","","160","    Parameters","161","    ----------","162","    obj","163","    methodname","164","    *args","165","    **kwargs","166","","167","    \"\"\"","168","    return getattr(obj, methodname)(*args, **kwargs)","169","","170",""]}],"sklearn\/multioutput.py":[{"add":["194","            delayed(e.predict)(X)"],"delete":["25","from .utils.fixes import parallel_helper","195","            delayed(parallel_helper)(e, 'predict', X)"]}],"sklearn\/ensemble\/forest.py":[{"add":["61","from ..utils.fixes import _joblib_parallel_args","220","            delayed(tree.apply)(X, check_input=False)","251","            delayed(tree.decision_path)(X,"],"delete":["61","from ..utils.fixes import parallel_helper, _joblib_parallel_args","220","            delayed(parallel_helper)(tree, 'apply', X, check_input=False)","251","            delayed(parallel_helper)(tree, 'decision_path', X,"]}]}},"68044b061d7abc0c16f632890939438033306161":{"changes":{"build_tools\/circle\/build_test_pypy.sh":"MODIFY"},"diff":{"build_tools\/circle\/build_test_pypy.sh":[{"add":["5","apt-get -yq install libatlas-dev libatlas-base-dev liblapack-dev gfortran ccache libopenblas-dev"],"delete":["5","apt-get -yq install libatlas-dev libatlas-base-dev liblapack-dev gfortran ccache"]}]}},"b8ab2149679aa4bde0affe93406f2e038ca8cb83":{"changes":{"sklearn\/utils\/fixes.py":"MODIFY","sklearn\/manifold\/spectral_embedding_.py":"MODIFY","sklearn\/externals\/_lobpcg.py":"ADD","doc\/whats_new\/v0.22.rst":"MODIFY"},"diff":{"sklearn\/utils\/fixes.py":[{"add":["40","if sp_version >= (1, 3):","41","    from scipy.sparse.linalg import lobpcg","42","else:","43","    # Backport of lobpcg functionality from scipy 1.3.0, can be removed","44","    # once support for sp_version < (1, 3) is dropped","45","    from ..externals._lobpcg import lobpcg  # noqa"],"delete":[]}],"sklearn\/manifold\/spectral_embedding_.py":[{"add":["12","from scipy.sparse.linalg import eigsh","19","from ..utils.fixes import lobpcg"],"delete":["12","from scipy.sparse.linalg import eigsh, lobpcg"]}],"sklearn\/externals\/_lobpcg.py":[{"add":[],"delete":[]}],"doc\/whats_new\/v0.22.rst":[{"add":["131","- |Fix| Port `lobpcg` from SciPy which implement some bug fixes but only","132","  available in 1.3+.","133","  :pr:`14195` by :user:`Guillaume Lemaitre <glemaitre>`.","134",""],"delete":[]}]}},"3de368d40548fae71f6288f68c639abda05a3d7d":{"changes":{"sklearn\/model_selection\/_split.py":"MODIFY","sklearn\/model_selection\/tests\/test_search.py":"MODIFY","sklearn\/model_selection\/tests\/test_split.py":"MODIFY","sklearn\/model_selection\/tests\/test_validation.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY"},"diff":{"sklearn\/model_selection\/_split.py":[{"add":["603","    The implementation is designed to:","604","","605","    * Generate test sets such that all contain the same distribution of","606","      classes, or as close as possible.","607","    * Be invariant to class label: relabelling ``y = [\"Happy\", \"Sad\"]`` to","608","      ``y = [1, 0]`` should not change the indices generated.","609","    * Preserve order dependencies in the dataset ordering, when","610","      ``shuffle=False``: all samples from class k in some test set were","611","      contiguous in y, or separated in y by samples from classes other than k.","612","    * Generate test sets where the smallest and largest differ by at most one","613","      sample.","614","","615","    .. versionchanged:: 0.22","616","        The previous implementation did not follow the last constraint.","637","","638","        _, y_idx, y_inv = np.unique(y, return_index=True, return_inverse=True)","639","        # y_inv encodes y according to lexicographic order. We invert y_idx to","640","        # map the classes so that they are encoded by order of appearance:","641","        # 0 represents the first label appearing in y, 1 the second, etc.","642","        _, class_perm = np.unique(y_idx, return_inverse=True)","643","        y_encoded = class_perm[y_inv]","644","","645","        n_classes = len(y_idx)","646","        y_counts = np.bincount(y_encoded)","654","                           \" members, which is less than n_splits=%d.\"","655","                           % (min_groups, self.n_splits)), UserWarning)","657","        # Determine the optimal number of samples from each class in each fold,","658","        # using round robin over the sorted y. (This can be done direct from","659","        # counts, but that code is unreadable.)","660","        y_order = np.sort(y_encoded)","661","        allocation = np.asarray(","662","            [np.bincount(y_order[i::self.n_splits], minlength=n_classes)","663","             for i in range(self.n_splits)])","665","        # To maintain the data order dependencies as best as possible within","666","        # the stratification constraint, we assign samples from each class in","667","        # blocks (and then mess that up when shuffle=True).","668","        test_folds = np.empty(len(y), dtype='i')","669","        for k in range(n_classes):","670","            # since the kth column of allocation stores the number of samples","671","            # of class k in each test set, this generates blocks of fold","672","            # indices corresponding to the allocation for class k.","673","            folds_for_class = np.arange(self.n_splits).repeat(allocation[:, k])","674","            if self.shuffle:","675","                rng.shuffle(folds_for_class)","676","            test_folds[y_encoded == k] = folds_for_class"],"delete":["603","    Train and test sizes may be different in each fold, with a difference of at","604","    most ``n_classes``.","625","        n_samples = y.shape[0]","626","        unique_y, y_inversed = np.unique(y, return_inverse=True)","627","        y_counts = np.bincount(y_inversed)","635","                           \" members, which is too few. The minimum\"","636","                           \" number of members in any class cannot\"","637","                           \" be less than n_splits=%d.\"","638","                           % (min_groups, self.n_splits)), Warning)","640","        # pre-assign each sample to a test fold index using individual KFold","641","        # splitting strategies for each class so as to respect the balance of","642","        # classes","643","        # NOTE: Passing the data corresponding to ith class say X[y==class_i]","644","        # will break when the data is not 100% stratifiable for all classes.","645","        # So we pass np.zeroes(max(c, n_splits)) as data to the KFold","646","        per_cls_cvs = [","647","            KFold(self.n_splits, shuffle=self.shuffle,","648","                  random_state=rng).split(np.zeros(max(count, self.n_splits)))","649","            for count in y_counts]","651","        test_folds = np.zeros(n_samples, dtype=np.int)","652","        for test_fold_indices, per_cls_splits in enumerate(zip(*per_cls_cvs)):","653","            for cls, (_, test_split) in zip(unique_y, per_cls_splits):","654","                cls_test_folds = test_folds[y == cls]","655","                # the test split can be too big because we used","656","                # KFold(...).split(X[:max(c, n_splits)]) when data is not 100%","657","                # stratifiable for all the classes","658","                # (we use a warning instead of raising an exception)","659","                # If this is the case, let's trim it:","660","                test_split = test_split[test_split < len(cls_test_folds)]","661","                cls_test_folds[test_split] = test_fold_indices","662","                test_folds[y == cls] = cls_test_folds","663",""]}],"sklearn\/model_selection\/tests\/test_search.py":[{"add":["212","                         \"Fit parameter spam has length 1; expected\","],"delete":["212","                         \"Fit parameter spam has length 1; expected 4.\","]}],"sklearn\/model_selection\/tests\/test_split.py":[{"add":["8","from itertools import permutations","10","from sklearn.utils.testing import assert_allclose","371","    # Check equivalence to KFold","372","    y = [0, 1, 0, 1, 0, 1, 0, 1]","373","    X = np.ones_like(y)","374","    np.testing.assert_equal(","375","        list(StratifiedKFold(3).split(X, y)),","376","        list(KFold(3).split(X, y)))","378","","379","@pytest.mark.parametrize('shuffle', [False, True])","380","@pytest.mark.parametrize('k', [4, 5, 6, 7, 8, 9, 10])","381","def test_stratified_kfold_ratios(k, shuffle):","389","    distr = np.bincount(y) \/ len(y)","391","    test_sizes = []","392","    skf = StratifiedKFold(k, random_state=0, shuffle=shuffle)","393","    for train, test in skf.split(X, y):","394","        assert_allclose(np.bincount(y[train]) \/ len(train), distr, atol=0.02)","395","        assert_allclose(np.bincount(y[test]) \/ len(test), distr, atol=0.02)","396","        test_sizes.append(len(test))","397","    assert np.ptp(test_sizes) <= 1","398","","399","","400","@pytest.mark.parametrize('shuffle', [False, True])","401","@pytest.mark.parametrize('k', [4, 6, 7])","402","def test_stratified_kfold_label_invariance(k, shuffle):","403","    # Check that stratified kfold gives the same indices regardless of labels","404","    n_samples = 100","405","    y = np.array([2] * int(0.10 * n_samples) +","406","                 [0] * int(0.89 * n_samples) +","407","                 [1] * int(0.01 * n_samples))","408","    X = np.ones(len(y))","409","","410","    def get_splits(y):","411","        return [(list(train), list(test))","412","                for train, test","413","                in StratifiedKFold(k, random_state=0,","414","                                   shuffle=shuffle).split(X, y)]","415","","416","    splits_base = get_splits(y)","417","    for perm in permutations([0, 1, 2]):","418","        y_perm = np.take(perm, y)","419","        splits_perm = get_splits(y_perm)","420","        assert splits_perm == splits_base","571","    assert 0.94 > mean_score"],"delete":["9","from sklearn.utils.testing import assert_almost_equal","371","def test_stratified_kfold_ratios():","380","    for shuffle in (False, True):","381","        for train, test in StratifiedKFold(5, shuffle=shuffle).split(X, y):","382","            assert_almost_equal(np.sum(y[train] == 4) \/ len(train), 0.10, 2)","383","            assert_almost_equal(np.sum(y[train] == 0) \/ len(train), 0.89, 2)","384","            assert_almost_equal(np.sum(y[train] == 1) \/ len(train), 0.01, 2)","385","            assert_almost_equal(np.sum(y[test] == 4) \/ len(test), 0.10, 2)","386","            assert_almost_equal(np.sum(y[test] == 0) \/ len(test), 0.89, 2)","387","            assert_almost_equal(np.sum(y[test] == 1) \/ len(test), 0.01, 2)","538","    assert 0.93 > mean_score"]}],"sklearn\/model_selection\/tests\/test_validation.py":[{"add":["963","    cv = StratifiedKFold(n_splits=2)"],"delete":["963","    cv = StratifiedKFold(n_splits=2, random_state=1)"]}],"doc\/whats_new\/v0.22.rst":[{"add":["19","- :class:`cluster.KMeans` when `n_jobs=1`. |Fix|","29","- :class:`linear_model.Ridge` when `X` is sparse. |Fix|","30","- :class:`model_selection.StratifiedKFold` and any use of `cv=int` with a","31","  classifier. |Fix|","279","- |Fix| Reimplemented :class:`model_selection.StratifiedKFold` to fix an issue","280","  where one test set could be `n_classes` larger than another. Test sets should","281","  now be near-equally sized. :pr:`14704` by `Joel Nothman`_.","282",""],"delete":["25","- :class:`linear_model.Ridge` when `X` is sparse. |Fix|","26","- :class:`cluster.KMeans` when `n_jobs=1`. |Fix|"]}]}},"197f448eedb13fd267e3cc0c2a3b98c87706106d":{"changes":{"sklearn\/neighbors\/nca.py":"MODIFY","sklearn\/neighbors\/tests\/test_nca.py":"MODIFY"},"diff":{"sklearn\/neighbors\/nca.py":[{"add":["15","import numbers","302","            check_scalar(","303","                self.n_components, 'n_components', numbers.Integral, 1)","322","        check_scalar(self.max_iter, 'max_iter', numbers.Integral, 1)","323","        check_scalar(self.tol, 'tol', numbers.Real, 0.)","324","        check_scalar(self.verbose, 'verbose', numbers.Integral, 0)"],"delete":["301","            check_scalar(self.n_components, 'n_components', int, 1)","320","        check_scalar(self.max_iter, 'max_iter', int, 1)","321","        check_scalar(self.tol, 'tol', float, 0.)","322","        check_scalar(self.verbose, 'verbose', int, 0)"]}],"sklearn\/neighbors\/tests\/test_nca.py":[{"add":["131","    assert_raises(TypeError, NCA(tol='1').fit, X, y)","520","","521","","522","@pytest.mark.parametrize('param, value', [('n_components', np.int32(3)),","523","                                          ('max_iter', np.int32(100)),","524","                                          ('tol', np.float32(0.0001))])","525","def test_parameters_valid_types(param, value):","526","    # check that no error is raised when parameters have numpy integer or","527","    # floating types.","528","    nca = NeighborhoodComponentsAnalysis(**{param: value})","529","","530","    X = iris_data","531","    y = iris_target","532","","533","    nca.fit(X, y)"],"delete":["131","    assert_raises(TypeError, NCA(tol=1).fit, X, y)"]}]}},"21fc1d97452d4e3a6d744d0eef95ecaf7e87859c":{"changes":{"doc\/themes\/scikit-learn-modern\/static\/css\/theme.css":"MODIFY"},"diff":{"doc\/themes\/scikit-learn-modern\/static\/css\/theme.css":[{"add":["317","div#carouselExampleSlidesOnly {","318","  min-height: 200px;","319","}","320",""],"delete":[]}]}},"7e8405c89ce72675d82169b116e865f73ced5e19":{"changes":{"sklearn\/metrics\/cluster\/tests\/test_unsupervised.py":"MODIFY","sklearn\/metrics\/cluster\/tests\/test_supervised.py":"MODIFY"},"diff":{"sklearn\/metrics\/cluster\/tests\/test_unsupervised.py":[{"add":["137","    err_msg = (r'Number of labels is %d\\. Valid values are 2 '","138","               r'to n_samples - 1 \\(inclusive\\)' % len(np.unique(y)))","139","    with pytest.raises(ValueError, match=err_msg):","140","        silhouette_score(X, y)","144","    err_msg = (r'Number of labels is %d\\. Valid values are 2 '","145","               r'to n_samples - 1 \\(inclusive\\)' % len(np.unique(y)))","146","    with pytest.raises(ValueError, match=err_msg):","147","        silhouette_score(X, y)","191","    with pytest.raises(ValueError, match=\"Number of labels is\"):","192","        func(rng.rand(10, 2), np.zeros(10))","198","    with pytest.raises(ValueError, match=\"Number of labels is\"):","199","        func(rng.rand(10, 2), np.arange(10))"],"delete":["7","from sklearn.utils.testing import assert_raises_regexp","8","from sklearn.utils.testing import assert_raise_message","139","    assert_raises_regexp(ValueError,","140","                         r'Number of labels is %d\\. Valid values are 2 '","141","                         r'to n_samples - 1 \\(inclusive\\)' % len(np.unique(y)),","142","                         silhouette_score, X, y)","146","    assert_raises_regexp(ValueError,","147","                         r'Number of labels is %d\\. Valid values are 2 '","148","                         r'to n_samples - 1 \\(inclusive\\)' % len(np.unique(y)),","149","                         silhouette_score, X, y)","193","    assert_raise_message(ValueError, \"Number of labels is\",","194","                         func,","195","                         rng.rand(10, 2), np.zeros(10))","201","    assert_raise_message(ValueError, \"Number of labels is\",","202","                         func,","203","                         rng.rand(10, 2), np.arange(10))"]}],"sklearn\/metrics\/cluster\/tests\/test_supervised.py":[{"add":["1","import pytest","19","        assert_almost_equal, ignore_warnings)","38","        with pytest.raises(ValueError, match=expected):","39","            score_func([0, 1], [1, 1, 1])","41","        expected = r\"labels_true must be 1D: shape is \\(2\"","42","        with pytest.raises(ValueError, match=expected):","43","            score_func([[0, 1], [1, 0]], [1, 1, 1])","45","        expected = r\"labels_pred must be 1D: shape is \\(2\"","46","        with pytest.raises(ValueError, match=expected):","47","            score_func([0, 1, 0], [[1, 1], [0, 0]])","264","    with pytest.raises(ValueError, match=\"Cannot set 'eps' when sparse=True\"):","265","        contingency_matrix(labels_a, labels_b, eps=1e-10, sparse=True)"],"delete":["18","        assert_almost_equal, assert_raise_message, ignore_warnings)","37","        assert_raise_message(ValueError, expected, score_func,","38","                             [0, 1], [1, 1, 1])","40","        expected = \"labels_true must be 1D: shape is (2\"","41","        assert_raise_message(ValueError, expected, score_func,","42","                             [[0, 1], [1, 0]], [1, 1, 1])","44","        expected = \"labels_pred must be 1D: shape is (2\"","45","        assert_raise_message(ValueError, expected, score_func,","46","                             [0, 1, 0], [[1, 1], [0, 0]])","263","    C_sparse = assert_raise_message(ValueError,","264","                                    \"Cannot set 'eps' when sparse=True\",","265","                                    contingency_matrix, labels_a, labels_b,","266","                                    eps=1e-10, sparse=True)"]}]}},"247ea83065bbbb2817a13b70d6178705f5c0ad0a":{"changes":{"sklearn\/linear_model\/least_angle.py":"MODIFY"},"diff":{"sklearn\/linear_model\/least_angle.py":[{"add":["1023","    copy_X : bool, default=True"],"delete":["1023","    copy_X : bool default=True"]}]}},"76d5f061a8068ef5dce469b52c7f8322c518e2a2":{"changes":{"sklearn\/tests\/test_pipeline.py":"MODIFY","sklearn\/pipeline.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY"},"diff":{"sklearn\/tests\/test_pipeline.py":[{"add":["23","from sklearn.base import clone, BaseEstimator, TransformerMixin","37","iris = load_iris()","1152","","1153","","1154","def test_feature_union_fit_params():","1155","    # Regression test for issue: #15117","1156","    class Dummy(TransformerMixin, BaseEstimator):","1157","        def fit(self, X, y=None, **fit_params):","1158","            if fit_params != {'a': 0}:","1159","                raise ValueError","1160","            return self","1161","","1162","        def transform(self, X, y=None):","1163","            return X","1164","","1165","    X, y = iris.data, iris.target","1166","    t = FeatureUnion([('dummy0', Dummy()), ('dummy1', Dummy())])","1167","    with pytest.raises(ValueError):","1168","        t.fit(X, y)","1169","","1170","    with pytest.raises(ValueError):","1171","        t.fit_transform(X, y)","1172","","1173","    t.fit(X, y, a=0)","1174","    t.fit_transform(X, y, a=0)"],"delete":["23","from sklearn.base import clone, BaseEstimator","242","    iris = load_iris()","321","    iris = load_iris()","336","    iris = load_iris()","367","    iris = load_iris()","400","    iris = load_iris()","458","    iris = load_iris()","532","    iris = load_iris()","551","    iris = load_iris()","773","    iris = load_iris()","867","    iris = load_iris()","989","    iris = load_iris()","1024","    iris = load_iris()"]}],"sklearn\/pipeline.py":[{"add":["878","    def fit(self, X, y=None, **fit_params):","894","        transformers = self._parallel_func(X, y, fit_params, _fit_one)"],"delete":["878","    def fit(self, X, y=None):","894","        transformers = self._parallel_func(X, y, {}, _fit_one)"]}],"doc\/whats_new\/v0.22.rst":[{"add":["506","- |Fix| The `fit` in :class:`~pipeline.FeatureUnion` now accepts `fit_params`","507","  to pass to the underlying transformers. :pr:`15119` by `Adrin Jalali`_.","508",""],"delete":[]}]}},"939fa3cccefe708db7a81c5248db32a1d600bf8d":{"changes":{"sklearn\/ensemble\/_base.py":"MODIFY","sklearn\/ensemble\/_voting.py":"MODIFY","sklearn\/ensemble\/tests\/test_voting.py":"MODIFY"},"diff":{"sklearn\/ensemble\/_base.py":[{"add":["9","import warnings","226","        # FIXME: deprecate the usage of None to drop an estimator from the","227","        # ensemble. Remove in 0.24","228","        if any(est is None for est in estimators):","229","            warnings.warn(","230","                \"Using 'None' to drop an estimator from the ensemble is \"","231","                \"deprecated in 0.22 and support will be dropped in 0.24. \"","232","                \"Use the string 'drop' instead.\", DeprecationWarning","233","            )","234","","248","                    \"The estimator {} should be a {}.\".format("],"delete":["238","                    \"The estimator {} should be a {}.\"","239","                    .format("]}],"sklearn\/ensemble\/_voting.py":[{"add":["90","        ``self.estimators_``. An estimator can be set to ``'drop'``","93","        .. deprecated:: 0.22","94","           Using ``None`` to drop an estimator is deprecated in 0.22 and","95","           support will be dropped in 0.24. Use the string ``'drop'`` instead.","96","","125","        that are not 'drop'.","328","        ``self.estimators_``. An estimator can be set to ``'drop'`` using","329","        ``set_params``.","330","","331","        .. deprecated:: 0.22","332","           Using ``None`` to drop an estimator is deprecated in 0.22 and","333","           support will be dropped in 0.24. Use the string ``'drop'`` instead.","349","        that are not 'drop'.","354","        .. versionadded:: 0.20","355",""],"delete":["90","        ``self.estimators_``. An estimator can be set to ``None`` or ``'drop'``","121","        that are not `None`.","324","        ``self.estimators_``. An estimator can be set to ``None`` or ``'drop'``","325","        using ``set_params``.","341","        that are not `None`."]}],"sklearn\/ensemble\/tests\/test_voting.py":[{"add":["26","from sklearn.base import BaseEstimator, ClassifierMixin, clone","391","# TODO: Remove parametrization in 0.24 when None is removed in Voting*","407","    with pytest.warns(None) as record:","408","        eclf2.set_params(rf=drop).fit(X, y)","409","    assert record if drop is None else not record","419","    with pytest.warns(None) as record:","420","        eclf2.set_params(voting='soft').fit(X, y)","421","    assert record if drop is None else not record","425","    with pytest.warns(None) as record:","426","        with pytest.raises(ValueError, match=msg):","427","            eclf2.set_params(lr=drop, rf=drop, nb=drop).fit(X, y)","428","    assert record if drop is None else not record","440","    with pytest.warns(None) as record:","441","        eclf2.set_params(rf=drop).fit(X1, y1)","442","    assert record if drop is None else not record","503","# TODO: Remove drop=None in 0.24 when None is removed in Voting*","515","    # TODO: remove the parametrization on 'drop' when support for None is","516","    # removed.","517","    # check that an estimator can be set to 'drop' and passing some weight","520","    voter = clone(voter)","523","    with pytest.warns(None) as record:","524","        voter.fit(X, y, sample_weight=np.ones(y.shape))","525","    assert record if drop is None else not record","545","","546","","547","# TODO: Remove in 0.24 when None is removed in Voting*","548","@pytest.mark.parametrize(","549","    \"Voter, BaseEstimator\",","550","    [(VotingClassifier, DecisionTreeClassifier),","551","     (VotingRegressor, DecisionTreeRegressor)]","552",")","553","def test_deprecate_none_transformer(Voter, BaseEstimator):","554","    est = Voter(estimators=[('lr', None),","555","                            ('tree', BaseEstimator(random_state=0))])","556","","557","    msg = (\"Using 'None' to drop an estimator from the ensemble is \"","558","           \"deprecated in 0.22 and support will be dropped in 0.24. \"","559","           \"Use the string 'drop' instead.\")","560","    with pytest.warns(DeprecationWarning, match=msg):","561","        est.fit(X, y)"],"delete":["26","from sklearn.base import BaseEstimator, ClassifierMixin","406","    eclf2.set_params(rf=drop).fit(X, y)","416","    eclf2.set_params(voting='soft').fit(X, y)","420","    assert_raise_message(","421","        ValueError, msg, eclf2.set_params(lr=drop, rf=drop, nb=drop).fit, X, y)","433","    eclf2.set_params(rf=drop).fit(X1, y1)","505","    # check that an estimator can be set to None and passing some weight","510","    voter.fit(X, y, sample_weight=np.ones(y.shape))"]}]}},"62aee0666e8803f20ecf0f6214621367e50f3961":{"changes":{"sklearn\/neighbors\/_nearest_centroid.py":"ADD",".gitignore":"MODIFY","sklearn\/neighbors\/tests\/test_neighbors.py":"MODIFY","sklearn\/neighbors\/_lof.py":"ADD","sklearn\/neighbors\/_base.py":"ADD","sklearn\/neighbors\/_quad_tree.pxd":"ADD","sklearn\/neighbors\/tests\/test_dist_metrics.py":"MODIFY","sklearn\/neighbors\/tests\/test_kde.py":"MODIFY","sklearn\/tree\/_utils.pxd":"MODIFY","sklearn\/neighbors\/_kde.py":"ADD","sklearn\/neighbors\/_dist_metrics.pyx":"ADD","sklearn\/neighbors\/_typedefs.pxd":"ADD","doc\/modules\/neighbors.rst":"MODIFY","sklearn\/neighbors\/_unsupervised.py":"ADD","sklearn\/neighbors\/_quad_tree.pyx":"ADD","sklearn\/manifold\/_barnes_hut_tsne.pyx":"MODIFY","sklearn\/neighbors\/tests\/test_nca.py":"MODIFY","sklearn\/neighbors\/_binary_tree.pxi":"ADD","sklearn\/neighbors\/setup.py":"MODIFY","sklearn\/neighbors\/__init__.py":"MODIFY","sklearn\/neighbors\/_regression.py":"ADD","sklearn\/impute\/_knn.py":"MODIFY","sklearn\/neighbors\/_nca.py":"ADD","sklearn\/_build_utils\/deprecated_modules.py":"MODIFY","sklearn\/neighbors\/_kd_tree.pyx":"ADD","sklearn\/neighbors\/_typedefs.pyx":"ADD","sklearn\/neighbors\/tests\/test_kd_tree.py":"MODIFY","sklearn\/neighbors\/tests\/test_graph.py":"MODIFY","sklearn\/neighbors\/_dist_metrics.pxd":"ADD","sklearn\/neighbors\/_graph.py":"ADD","sklearn\/neighbors\/_classification.py":"ADD","\/dev\/null":"DELETE","sklearn\/neighbors\/_ball_tree.pyx":"ADD","doc\/modules\/density.rst":"MODIFY","sklearn\/neighbors\/tests\/test_neighbors_tree.py":"MODIFY","sklearn\/neighbors\/tests\/test_quad_tree.py":"MODIFY","sklearn\/cluster\/tests\/test_hierarchical.py":"MODIFY","sklearn\/neighbors\/tests\/test_ball_tree.py":"MODIFY"},"diff":{"sklearn\/neighbors\/_nearest_centroid.py":[{"add":[],"delete":[]}],".gitignore":[{"add":["132","sklearn\/neighbors\/ball_tree.py","133","sklearn\/neighbors\/base.py","134","sklearn\/neighbors\/classification.py","135","sklearn\/neighbors\/dist_metrics.py","136","sklearn\/neighbors\/graph.py","137","sklearn\/neighbors\/kd_tree.py","138","sklearn\/neighbors\/kde.py","139","sklearn\/neighbors\/lof.py","140","sklearn\/neighbors\/nca.py","141","sklearn\/neighbors\/nearest_centroid.py","142","sklearn\/neighbors\/quad_tree.py","143","sklearn\/neighbors\/regression.py","144","sklearn\/neighbors\/typedefs.py","145","sklearn\/neighbors\/unsupervised.py","146",""],"delete":[]}],"sklearn\/neighbors\/tests\/test_neighbors.py":[{"add":["16","from sklearn.neighbors import VALID_METRICS_SPARSE, VALID_METRICS","17","from sklearn.neighbors._base import _is_sorted_by_data, _check_precomputed","1584","    # 'ball_tree': uses sklearn.neighbors._dist_metrics"],"delete":["16","from sklearn.neighbors.base import VALID_METRICS_SPARSE, VALID_METRICS","17","from sklearn.neighbors.base import _is_sorted_by_data, _check_precomputed","1584","    # 'ball_tree': uses sklearn.neighbors.dist_metrics"]}],"sklearn\/neighbors\/_lof.py":[{"add":[],"delete":[]}],"sklearn\/neighbors\/_base.py":[{"add":[],"delete":[]}],"sklearn\/neighbors\/_quad_tree.pxd":[{"add":[],"delete":[]}],"sklearn\/neighbors\/tests\/test_dist_metrics.py":[{"add":["11","from sklearn.neighbors import DistanceMetric"],"delete":["11","from sklearn.neighbors.dist_metrics import DistanceMetric"]}],"sklearn\/neighbors\/tests\/test_kde.py":[{"add":["6","from sklearn.neighbors._ball_tree import kernel_norm"],"delete":["6","from sklearn.neighbors.ball_tree import kernel_norm"]}],"sklearn\/tree\/_utils.pxd":[{"add":["13","from ..neighbors._quad_tree cimport Cell"],"delete":["13","from ..neighbors.quad_tree cimport Cell"]}],"sklearn\/neighbors\/_kde.py":[{"add":[],"delete":[]}],"sklearn\/neighbors\/_dist_metrics.pyx":[{"add":[],"delete":[]}],"sklearn\/neighbors\/_typedefs.pxd":[{"add":[],"delete":[]}],"doc\/modules\/neighbors.rst":[{"add":["471","    >>> from sklearn.neighbors import NearestCentroid"],"delete":["471","    >>> from sklearn.neighbors.nearest_centroid import NearestCentroid"]}],"sklearn\/neighbors\/_unsupervised.py":[{"add":[],"delete":[]}],"sklearn\/neighbors\/_quad_tree.pyx":[{"add":[],"delete":[]}],"sklearn\/manifold\/_barnes_hut_tsne.pyx":[{"add":["17","from ..neighbors._quad_tree cimport _QuadTree"],"delete":["17","from ..neighbors.quad_tree cimport _QuadTree"]}],"sklearn\/neighbors\/tests\/test_nca.py":[{"add":["20","from sklearn.neighbors import NeighborhoodComponentsAnalysis"],"delete":["20","from sklearn.neighbors.nca import NeighborhoodComponentsAnalysis"]}],"sklearn\/neighbors\/_binary_tree.pxi":[{"add":[],"delete":[]}],"sklearn\/neighbors\/setup.py":[{"add":["12","    config.add_extension('_ball_tree',","13","                         sources=['_ball_tree.pyx'],","17","    config.add_extension('_kd_tree',","18","                         sources=['_kd_tree.pyx'],","22","    config.add_extension('_dist_metrics',","23","                         sources=['_dist_metrics.pyx'],","29","    config.add_extension('_typedefs',","30","                         sources=['_typedefs.pyx'],","33","    config.add_extension(\"_quad_tree\",","34","                         sources=[\"_quad_tree.pyx\"],"],"delete":["12","    config.add_extension('ball_tree',","13","                         sources=['ball_tree.pyx'],","17","    config.add_extension('kd_tree',","18","                         sources=['kd_tree.pyx'],","22","    config.add_extension('dist_metrics',","23","                         sources=['dist_metrics.pyx'],","29","    config.add_extension('typedefs',","30","                         sources=['typedefs.pyx'],","33","    config.add_extension(\"quad_tree\",","34","                         sources=[\"quad_tree.pyx\"],"]}],"sklearn\/neighbors\/__init__.py":[{"add":["5","from ._ball_tree import BallTree","6","from ._kd_tree import KDTree","7","from ._dist_metrics import DistanceMetric","8","from ._graph import kneighbors_graph, radius_neighbors_graph","9","from ._graph import KNeighborsTransformer, RadiusNeighborsTransformer","10","from ._unsupervised import NearestNeighbors","11","from ._classification import KNeighborsClassifier, RadiusNeighborsClassifier","12","from ._regression import KNeighborsRegressor, RadiusNeighborsRegressor","13","from ._nearest_centroid import NearestCentroid","14","from ._kde import KernelDensity","15","from ._lof import LocalOutlierFactor","16","from ._nca import NeighborhoodComponentsAnalysis","17","from ._base import VALID_METRICS, VALID_METRICS_SPARSE"],"delete":["5","from .ball_tree import BallTree","6","from .kd_tree import KDTree","7","from .dist_metrics import DistanceMetric","8","from .graph import kneighbors_graph, radius_neighbors_graph","9","from .graph import KNeighborsTransformer, RadiusNeighborsTransformer","10","from .unsupervised import NearestNeighbors","11","from .classification import KNeighborsClassifier, RadiusNeighborsClassifier","12","from .regression import KNeighborsRegressor, RadiusNeighborsRegressor","13","from .nearest_centroid import NearestCentroid","14","from .kde import KernelDensity","15","from .lof import LocalOutlierFactor","16","from .nca import NeighborhoodComponentsAnalysis","17","from .base import VALID_METRICS, VALID_METRICS_SPARSE"]}],"sklearn\/neighbors\/_regression.py":[{"add":[],"delete":[]}],"sklearn\/impute\/_knn.py":[{"add":["6","from ..neighbors._base import _get_weights","7","from ..neighbors._base import _check_weights"],"delete":["6","from ..neighbors.base import _get_weights","7","from ..neighbors.base import _check_weights"]}],"sklearn\/neighbors\/_nca.py":[{"add":[],"delete":[]}],"sklearn\/_build_utils\/deprecated_modules.py":[{"add":["88","    ('_ball_tree', 'sklearn.neighbors.ball_tree', 'sklearn.neighbors',","89","     'BallTree'),","90","    ('_base', 'sklearn.neighbors.base', 'sklearn.neighbors',","91","     'VALID_METRICS'),","92","    ('_classification', 'sklearn.neighbors.classification',","93","     'sklearn.neighbors', 'KNeighborsClassifier'),","94","    ('_dist_metrics', 'sklearn.neighbors.dist_metrics', 'sklearn.neighbors',","95","     'DistanceMetric'),","96","    ('_graph', 'sklearn.neighbors.graph', 'sklearn.neighbors',","97","     'KNeighborsTransformer'),","98","    ('_kd_tree', 'sklearn.neighbors.kd_tree', 'sklearn.neighbors',","99","     'KDTree'),","100","    ('_kde', 'sklearn.neighbors.kde', 'sklearn.neighbors',","101","     'KernelDensity'),","102","    ('_lof', 'sklearn.neighbors.lof', 'sklearn.neighbors',","103","     'LocalOutlierFactor'),","104","    ('_nca', 'sklearn.neighbors.nca', 'sklearn.neighbors',","105","     'NeighborhoodComponentsAnalysis'),","106","    ('_nearest_centroid', 'sklearn.neighbors.nearest_centroid',","107","     'sklearn.neighbors', 'NearestCentroid'),","108","    ('_quad_tree', 'sklearn.neighbors.quad_tree', 'sklearn.neighbors',","109","     'CELL_DTYPE'),","110","    ('_regression', 'sklearn.neighbors.regression', 'sklearn.neighbors',","111","     'KNeighborsRegressor'),","112","    ('_typedefs', 'sklearn.neighbors.typedefs', 'sklearn.neighbors',","113","     'DTYPE'),","114","    ('_unsupervised', 'sklearn.neighbors.unsupervised', 'sklearn.neighbors',","115","     'NearestNeighbors'),","116",""],"delete":[]}],"sklearn\/neighbors\/_kd_tree.pyx":[{"add":[],"delete":[]}],"sklearn\/neighbors\/_typedefs.pyx":[{"add":[],"delete":[]}],"sklearn\/neighbors\/tests\/test_kd_tree.py":[{"add":["5","from sklearn.neighbors._kd_tree import (KDTree, NeighborsHeap,","6","                                        simultaneous_sort, kernel_norm,","7","                                        nodeheap_sort, DTYPE, ITYPE)","8","from sklearn.neighbors import DistanceMetric"],"delete":["5","from sklearn.neighbors.kd_tree import (KDTree, NeighborsHeap,","6","                                       simultaneous_sort, kernel_norm,","7","                                       nodeheap_sort, DTYPE, ITYPE)","8","from sklearn.neighbors.dist_metrics import DistanceMetric"]}],"sklearn\/neighbors\/tests\/test_graph.py":[{"add":["4","from sklearn.neighbors._base import _is_sorted_by_data"],"delete":["4","from sklearn.neighbors.base import _is_sorted_by_data"]}],"sklearn\/neighbors\/_dist_metrics.pxd":[{"add":[],"delete":[]}],"sklearn\/neighbors\/_graph.py":[{"add":[],"delete":[]}],"sklearn\/neighbors\/_classification.py":[{"add":[],"delete":[]}],"\/dev\/null":[{"add":[],"delete":[]}],"sklearn\/neighbors\/_ball_tree.pyx":[{"add":[],"delete":[]}],"doc\/modules\/density.rst":[{"add":["80","   >>> from sklearn.neighbors import KernelDensity"],"delete":["80","   >>> from sklearn.neighbors.kde import KernelDensity"]}],"sklearn\/neighbors\/tests\/test_neighbors_tree.py":[{"add":["8","from sklearn.neighbors import DistanceMetric","9","from sklearn.neighbors._ball_tree import BallTree","10","from sklearn.neighbors._kd_tree import KDTree"],"delete":["8","from sklearn.neighbors.dist_metrics import DistanceMetric","9","from sklearn.neighbors.ball_tree import BallTree","10","from sklearn.neighbors.kd_tree import KDTree"]}],"sklearn\/neighbors\/tests\/test_quad_tree.py":[{"add":["5","from sklearn.neighbors._quad_tree import _QuadTree"],"delete":["5","from sklearn.neighbors.quad_tree import _QuadTree"]}],"sklearn\/cluster\/tests\/test_hierarchical.py":[{"add":["30","from sklearn.neighbors import kneighbors_graph"],"delete":["30","from sklearn.neighbors.graph import kneighbors_graph"]}],"sklearn\/neighbors\/tests\/test_ball_tree.py":[{"add":["5","from sklearn.neighbors._ball_tree import (BallTree, NeighborsHeap,","6","                                          simultaneous_sort, kernel_norm,","7","                                          nodeheap_sort, DTYPE, ITYPE)","8","from sklearn.neighbors import DistanceMetric"],"delete":["5","from sklearn.neighbors.ball_tree import (BallTree, NeighborsHeap,","6","                                         simultaneous_sort, kernel_norm,","7","                                         nodeheap_sort, DTYPE, ITYPE)","8","from sklearn.neighbors.dist_metrics import DistanceMetric"]}]}},"ec2ea1b5ae1adb30aa17356d8f0b115206cab627":{"changes":{"doc\/modules\/grid_search.rst":"MODIFY","doc\/modules\/compose.rst":"MODIFY"},"diff":{"doc\/modules\/grid_search.rst":[{"add":["45","possibly by reading the enclosed reference to the literature.","194",".. _composite_grid_search:","195","","198","`GridSearchCV` and `RandomizedSearchCV` allow searching over parameters of","199","composite or nested estimators such as `pipeline.Pipeline`,","200","`ColumnTransformer`, `VotingClassifier` or `CalibratedClassifierCV`","201","using a dedicated ``<estimator>__<parameter>`` syntax::","203","  >>> from sklearn.model_selection import GridSearchCV","204","  >>> from sklearn.calibration import CalibratedClassifierCV","205","  >>> from sklearn.ensemble import RandomForestClassifier","206","  >>> from sklearn.datasets import make_moons","207","  >>> X, y = make_moons()","208","  >>> calibrated_forest = CalibratedClassifierCV(","209","  ...    base_estimator=RandomForestClassifier(n_estimators=10))","210","  >>> param_grid = {","211","  ...    'base_estimator__max_depth': [2, 4, 6, 8]}","212","  >>> search = GridSearchCV(calibrated_forest, param_grid, cv=5)","213","  >>> search.fit(X, y)","214","  GridSearchCV(cv=5,","215","               estimator=CalibratedClassifierCV(...),","216","               param_grid={'base_estimator__max_depth': [2, 4, 6, 8]})","217","","218","Here, ``<estimator>`` is the parameter name of the nested estimator,","219","in this case ``base_estimator``.","220","If the meta-estimator is constructed as a collection of estimators as in","221","`pipeline.Pipeline`, then ``<estimator>`` refers to the name of the estimator,","222","see :ref:`pipeline_nested_parameters`.  In practice, there can be several","223","levels of nesting::","224","","225","  >>> from sklearn.pipeline import Pipeline","226","  >>> from sklearn.feature_selection import SelectKBest","227","  >>> pipe = Pipeline([","228","  ...    ('select', SelectKBest()),","229","  ...    ('model', calibrated_forest)])","230","  >>> param_grid = {","231","  ...    'select__k': [1, 2],","232","  ...    'model__base_estimator__max_depth': [2, 4, 6, 8]}","233","  >>> search = GridSearchCV(pipe, param_grid, cv=5).fit(X, y)","234",""],"delete":["45","possibly by reading the enclosed reference to the literature.  ","197",":ref:`pipeline` describes building composite estimators whose","198","parameter space can be searched with these tools."]}],"doc\/modules\/compose.rst":[{"add":["103","","104",".. _pipeline_nested_parameters:","105","","133","    >>> pipe[0]","152"," * :ref:`composite_grid_search`","374","    FeatureUnion(transformer_list=[('linear_pca', PCA()),","425","<sklearn.preprocessing.OneHotEncoder>` but apply a"],"delete":["130","    >>> pipe[0] ","149"," * :ref:`grid_search`","371","    FeatureUnion(transformer_list=[('linear_pca', PCA()), ","422","<sklearn.preprocessing.OneHotEncoder>` but apply a "]}]}}}