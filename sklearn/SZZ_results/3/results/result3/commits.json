{"abfb6fd11e97cefd1947078646399cecec5bbe9c":{"changes":{"sklearn\/linear_model\/tests\/test_least_angle.py":"MODIFY","doc\/whats_new\/v0.23.rst":"MODIFY","sklearn\/linear_model\/_least_angle.py":"MODIFY"},"diff":{"sklearn\/linear_model\/tests\/test_least_angle.py":[{"add":["8","from sklearn.base import clone","20","from sklearn.linear_model import Lars, LassoLars","737","@pytest.mark.parametrize('est', (LassoLars(alpha=1e-3), Lars()))","738","def test_lars_with_jitter(est):","739","    # Test that a small amount of jitter helps stability,","740","    # using example provided in issue #2746","741","","742","    X = np.array([[0.0, 0.0, 0.0, -1.0, 0.0],","743","                  [0.0, -1.0, 0.0, 0.0, 0.0]])","744","    y = [-2.5, -2.5]","745","    expected_coef = [0, 2.5, 0, 2.5, 0]","746","","747","    # set to fit_intercept to False since target is constant and we want check","748","    # the value of coef. coef would be all zeros otherwise.","749","    est.set_params(fit_intercept=False)","750","    est_jitter = clone(est).set_params(jitter=10e-8, random_state=0)","751","","752","    est.fit(X, y)","753","    est_jitter.fit(X, y)","754","","755","    assert np.mean((est.coef_ - est_jitter.coef_)**2) > .1","756","    np.testing.assert_allclose(est_jitter.coef_, expected_coef, rtol=1e-3)","757","","758",""],"delete":[]}],"doc\/whats_new\/v0.23.rst":[{"add":["300","- |Enhancement| :class:`linear_model.LassoLars` and","301","  :class:`linear_model.Lars` now support a `jitter` parameter that adds","302","  random noise to the target. This might help with stability in some edge","303","  cases. :pr:`15179` by :user:`angelaambroz`.","304",""],"delete":[]}],"sklearn\/linear_model\/_least_angle.py":[{"add":["23","from ..utils import check_random_state","803","    jitter : float, default=None","804","        Upper bound on a uniform noise parameter to be added to the","805","        `y` values, to satisfy the model's assumption of","806","        one-at-a-time computations. Might help with stability.","807","","808","    random_state : int, RandomState instance or None (default)","809","        Determines random number generation for jittering. Pass an int","810","        for reproducible output across multiple function calls.","811","        See :term:`Glossary <random_state>`. Ignored if `jitter` is None.","812","","859","                 eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,","860","                 jitter=None, random_state=None):","869","        self.jitter = jitter","870","        self.random_state = random_state","970","        if self.jitter is not None:","971","            rng = check_random_state(self.random_state)","972","","973","            noise = rng.uniform(high=self.jitter, size=len(y))","974","            y = y + noise","975","","1053","    jitter : float, default=None","1054","        Upper bound on a uniform noise parameter to be added to the","1055","        `y` values, to satisfy the model's assumption of","1056","        one-at-a-time computations. Might help with stability.","1057","","1058","    random_state : int, RandomState instance or None (default)","1059","        Determines random number generation for jittering. Pass an int","1060","        for reproducible output across multiple function calls.","1061","        See :term:`Glossary <random_state>`. Ignored if `jitter` is None.","1062","","1115","                 positive=False, jitter=None, random_state=None):","1126","        self.jitter = jitter","1127","        self.random_state = random_state"],"delete":["848","                 eps=np.finfo(np.float).eps, copy_X=True, fit_path=True):","1085","                 positive=False):"]}]}},"b3030f046f7a39be8dbdf44a010ab1b888c4a785":{"changes":{"sklearn\/mixture\/gaussian_mixture.py":"MODIFY"},"diff":{"sklearn\/mixture\/gaussian_mixture.py":[{"add":["311","    if covariance_type == 'full':"],"delete":["311","    if covariance_type in 'full':"]}]}},"da6611148fa2b7a73277cde20b7251da6a551bf0":{"changes":{"sklearn\/datasets\/base.py":"MODIFY","sklearn\/datasets\/species_distributions.py":"MODIFY","doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/datasets\/california_housing.py":"MODIFY","sklearn\/datasets\/covtype.py":"MODIFY","sklearn\/datasets\/olivetti_faces.py":"MODIFY","sklearn\/datasets\/kddcup99.py":"MODIFY","sklearn\/datasets\/rcv1.py":"MODIFY","sklearn\/datasets\/tests\/test_base.py":"MODIFY"},"diff":{"sklearn\/datasets\/base.py":[{"add":["12","import warnings","922","","923","","924","def _refresh_cache(files, compress):","925","    # TODO: REMOVE in v0.23","926","    import joblib","927","    msg = \"sklearn.externals.joblib is deprecated in 0.21\"","928","    with warnings.catch_warnings(record=True) as warns:","929","        data = tuple([joblib.load(f) for f in files])","930","","931","    refresh_needed = any([str(x.message).startswith(msg) for x in warns])","932","","933","    other_warns = [w for w in warns if not str(w.message).startswith(msg)]","934","    for w in other_warns:","935","        warnings.warn(message=w.message, category=w.category)","936","","937","    if refresh_needed:","938","        try:","939","            for value, path in zip(data, files):","940","                joblib.dump(value, path, compress=compress)","941","        except IOError:","942","            message = (\"This dataset will stop being loadable in scikit-learn \"","943","                       \"version 0.23 because it references a deprecated \"","944","                       \"import path. Consider removing the following files \"","945","                       \"and allowing it to be cached anew:\\n%s\"","946","                       % (\"\\n\".join(files)))","947","            warnings.warn(message=message, category=DeprecationWarning)","948","","949","    return data[0] if len(data) == 1 else data"],"delete":[]}],"sklearn\/datasets\/species_distributions.py":[{"add":["53","from .base import _refresh_cache","262","        bunch = _refresh_cache([archive_path], 9)","263","        # TODO: Revert to the following line in v0.23","264","        # bunch = joblib.load(archive_path)"],"delete":["261","        bunch = joblib.load(archive_path)"]}],"doc\/whats_new\/v0.21.rst":[{"add":["14",":mod:`sklearn.datasets`","15",".......................","16","","17","- |Fix| :func:`datasets.fetch_california_housing`,","18","  :func:`datasets.fetch_covtype`,","19","  :func:`datasets.fetch_kddcup99`, :func:`datasets.fetch_olivetti_faces`,","20","  :func:`datasets.fetch_rcv1`, and :func:`datasets.fetch_species_distributions`","21","  try to persist the previously cache using the new ``joblib`` if the cahced","22","  data was persisted using the deprecated ``sklearn.externals.joblib``. This","23","  behavior is set to be deprecated and removed in v0.23.","24","  :pr:`14197` by `Adrin Jalali`_.","25",""],"delete":[]}],"sklearn\/datasets\/california_housing.py":[{"add":["36","from .base import _refresh_cache","132","        cal_housing = _refresh_cache([filepath], 6)","133","        # TODO: Revert to the following line in v0.23","134","        # cal_housing = joblib.load(filepath)"],"delete":["131","        cal_housing = joblib.load(filepath)"]}],"sklearn\/datasets\/covtype.py":[{"add":["27","from .base import _refresh_cache","128","        X, y = _refresh_cache([samples_path, targets_path], 9)","129","        # TODO: Revert to the following two lines in v0.23","130","        # X = joblib.load(samples_path)","131","        # y = joblib.load(targets_path)"],"delete":["127","        X = joblib.load(samples_path)","128","        y = joblib.load(targets_path)"]}],"sklearn\/datasets\/olivetti_faces.py":[{"add":["26","from .base import _refresh_cache","110","        faces = _refresh_cache([filepath], 6)","111","        # TODO: Revert to the following line in v0.23","112","        # faces = joblib.load(filepath)"],"delete":["109","        faces = joblib.load(filepath)"]}],"sklearn\/datasets\/kddcup99.py":[{"add":["22","from .base import _refresh_cache","295","        X, y = _refresh_cache([samples_path, targets_path], 0)","296","        # TODO: Revert to the following two lines in v0.23","297","        # X = joblib.load(samples_path)","298","        # y = joblib.load(targets_path)"],"delete":["294","        X = joblib.load(samples_path)","295","        y = joblib.load(targets_path)"]}],"sklearn\/datasets\/rcv1.py":[{"add":["24","from .base import _refresh_cache","192","        X, sample_id = _refresh_cache([samples_path, sample_id_path], 9)","193","        # TODO: Revert to the following two lines in v0.23","194","        # X = joblib.load(samples_path)","195","        # sample_id = joblib.load(sample_id_path)","248","        y, categories = _refresh_cache([sample_topics_path, topics_path], 9)","249","        # TODO: Revert to the following two lines in v0.23","250","        # y = joblib.load(sample_topics_path)","251","        # categories = joblib.load(topics_path)"],"delete":["191","        X = joblib.load(samples_path)","192","        sample_id = joblib.load(sample_id_path)","245","        y = joblib.load(sample_topics_path)","246","        categories = joblib.load(topics_path)"]}],"sklearn\/datasets\/tests\/test_base.py":[{"add":["10","import joblib","26","from sklearn.datasets.base import _refresh_cache","280","","281","","282","def test_refresh_cache(monkeypatch):","283","    # uses pytests monkeypatch fixture","284","    # https:\/\/docs.pytest.org\/en\/latest\/monkeypatch.html","285","","286","    def _load_warn(*args, **kwargs):","287","        # raise the warning from \"externals.joblib.__init__.py\"","288","        # this is raised when a file persisted by the old joblib is loaded now","289","        msg = (\"sklearn.externals.joblib is deprecated in 0.21 and will be \"","290","               \"removed in 0.23. Please import this functionality directly \"","291","               \"from joblib, which can be installed with: pip install joblib. \"","292","               \"If this warning is raised when loading pickled models, you \"","293","               \"may need to re-serialize those models with scikit-learn \"","294","               \"0.21+.\")","295","        warnings.warn(msg, DeprecationWarning)","296","        return 0","297","","298","    def _load_warn_unrelated(*args, **kwargs):","299","        warnings.warn(\"unrelated warning\", DeprecationWarning)","300","        return 0","301","","302","    def _dump_safe(*args, **kwargs):","303","        pass","304","","305","    def _dump_raise(*args, **kwargs):","306","        # this happens if the file is read-only and joblib.dump fails to write","307","        # on it.","308","        raise IOError()","309","","310","    # test if the dataset spesific warning is raised if load raises the joblib","311","    # warning, and dump fails to dump with new joblib","312","    monkeypatch.setattr(joblib, \"load\", _load_warn)","313","    monkeypatch.setattr(joblib, \"dump\", _dump_raise)","314","    msg = \"This dataset will stop being loadable in scikit-learn\"","315","    with pytest.warns(DeprecationWarning, match=msg):","316","        _refresh_cache('test', 0)","317","","318","    # make sure no warning is raised if load raises the warning, but dump","319","    # manages to dump the new data","320","    monkeypatch.setattr(joblib, \"load\", _load_warn)","321","    monkeypatch.setattr(joblib, \"dump\", _dump_safe)","322","    with pytest.warns(None) as warns:","323","        _refresh_cache('test', 0)","324","    assert len(warns) == 0","325","","326","    # test if an unrelated warning is still passed through and not suppressed","327","    # by _refresh_cache","328","    monkeypatch.setattr(joblib, \"load\", _load_warn_unrelated)","329","    monkeypatch.setattr(joblib, \"dump\", _dump_safe)","330","    with pytest.warns(DeprecationWarning, match=\"unrelated warning\"):","331","        _refresh_cache('test', 0)"],"delete":[]}]}},"36b688eb04de0172dace761ca63616f18d615542":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/linear_model\/tests\/test_logistic.py":"MODIFY","sklearn\/linear_model\/logistic.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["14",":mod:`sklearn.linear_model`","15","...........................","16","- |Fix| Fixed a bug in :class:`linear_model.LogisticRegressionCV` where","17","  ``refit=False`` would fail depending on the ``'multiclass'`` and","18","  ``'penalty'`` parameters (regression introduced in 0.21). :pr:`14087` by","19","  `Nicolas Hug`_."],"delete":[]}],"sklearn\/linear_model\/tests\/test_logistic.py":[{"add":["1534","@pytest.mark.parametrize('penalty', ('l2', 'elasticnet'))","1535","@pytest.mark.parametrize('multi_class', ('ovr', 'multinomial', 'auto'))","1536","def test_LogisticRegressionCV_no_refit(penalty, multi_class):","1546","    if penalty == 'elasticnet':","1547","        l1_ratios = np.linspace(0, 1, 2)","1548","    else:","1549","        l1_ratios = None","1551","    lrcv = LogisticRegressionCV(penalty=penalty, Cs=Cs, solver='saga',"],"delete":["1534","@pytest.mark.parametrize('multi_class', ('ovr', 'multinomial'))","1535","def test_LogisticRegressionCV_no_refit(multi_class):","1545","    l1_ratios = np.linspace(0, 1, 2)","1547","    lrcv = LogisticRegressionCV(penalty='elasticnet', Cs=Cs, solver='saga',"]}],"sklearn\/linear_model\/logistic.py":[{"add":["2172","                if multi_class == 'ovr':","2182","                if self.penalty == 'elasticnet':","2183","                    best_indices_l1 = best_indices \/\/ len(self.Cs_)","2184","                    self.l1_ratio_.append(np.mean(l1_ratios_[best_indices_l1]))","2185","                else:","2186","                    self.l1_ratio_.append(None)"],"delete":["2172","                if self.multi_class == 'ovr':","2182","                best_indices_l1 = best_indices \/\/ len(self.Cs_)","2183","                self.l1_ratio_.append(np.mean(l1_ratios_[best_indices_l1]))"]}]}},"1efbe186ebc13990756eaf822fce60a931e3c87b":{"changes":{"sklearn\/neighbors\/tests\/test_neighbors.py":"MODIFY"},"diff":{"sklearn\/neighbors\/tests\/test_neighbors.py":[{"add":["973","        if (issubclass(cls, neighbors.KNeighborsClassifier) or","974","                issubclass(cls, neighbors.KNeighborsRegressor)):","1217","            assert_array_equal(graph.A, [[0, 1], [1, 0]])","1218","            assert_array_equal(graph.data, [1, 1])","1219","            assert_array_equal(graph.indices, [1, 0])"],"delete":["973","        if (isinstance(cls, neighbors.KNeighborsClassifier) or","974","                isinstance(cls, neighbors.KNeighborsRegressor)):","1217","            assert_array_equal(rng.A, [[0, 1], [1, 0]])","1218","            assert_array_equal(rng.data, [1, 1])","1219","            assert_array_equal(rng.indices, [1, 0])"]}]}},"581b0e1d73414f47ef6cde6cd282667b7e767a36":{"changes":{"sklearn\/neighbors\/quad_tree.pyx":"MODIFY","sklearn\/tree\/_utils.pyx":"MODIFY","sklearn\/utils\/graph_shortest_path.pyx":"MODIFY","sklearn\/utils\/sparsefuncs_fast.pyx":"MODIFY","sklearn\/tree\/_tree.pyx":"MODIFY"},"diff":{"sklearn\/neighbors\/quad_tree.pyx":[{"add":["8","from cpython cimport Py_INCREF, PyObject, PyTypeObject","25","    object PyArray_NewFromDescr(PyTypeObject* subtype, np.dtype descr,","574","        arr = PyArray_NewFromDescr(<PyTypeObject *> np.ndarray,","575","                                   CELL_DTYPE, 1, shape,"],"delete":["8","from cpython cimport Py_INCREF, PyObject","25","    object PyArray_NewFromDescr(object subtype, np.dtype descr,","574","        arr = PyArray_NewFromDescr(np.ndarray, CELL_DTYPE, 1, shape,"]}],"sklearn\/tree\/_utils.pyx":[{"add":["513","        cdef DOUBLE_t original_median = 0.0","570","        cdef DOUBLE_t original_median = 0.0","585","        cdef double original_median = 0.0"],"delete":["513","        cdef DOUBLE_t original_median","570","        cdef DOUBLE_t original_median","585","        cdef double original_median"]}],"sklearn\/utils\/graph_shortest_path.pyx":[{"add":["118","    cdef unsigned int N = graph.shape[0]","500","    cdef unsigned int i_N","501","    cdef ITYPE_t i","507","    for i_N in range(0, N):","508","        initialize_node(&nodes[i_N], i_N)","566","    cdef unsigned int i_N","567","    cdef ITYPE_t i","576","    for i_N in range(0, N):","577","        nodes[i_N].state = 0  # 0 -> NOT_IN_HEAP","578","        nodes[i_N].val = 0"],"delete":["118","    cdef int N = graph.shape[0]","500","    cdef unsigned int i","506","    for i from 0 <= i < N:","507","        initialize_node(&nodes[i], i)","565","    cdef unsigned int i","574","    for i from 0 <= i < N:","575","        nodes[i].state = 0  # 0 -> NOT_IN_HEAP","576","        nodes[i].val = 0"]}],"sklearn\/utils\/sparsefuncs_fast.pyx":[{"add":[],"delete":["173","        unsigned long long endptr"]}],"sklearn\/tree\/_tree.pyx":[{"add":["18","from cpython cimport Py_INCREF, PyObject, PyTypeObject","41","    object PyArray_NewFromDescr(PyTypeObject* subtype, np.dtype descr,","1119","        arr = PyArray_NewFromDescr(<PyTypeObject *> np.ndarray,","1120","                                   <np.dtype> NODE_DTYPE, 1, shape,"],"delete":["18","from cpython cimport Py_INCREF, PyObject","41","    object PyArray_NewFromDescr(object subtype, np.dtype descr,","1119","        arr = PyArray_NewFromDescr(np.ndarray, <np.dtype> NODE_DTYPE, 1, shape,"]}]}},"03ea20db0f9585fa0d44f4d3cae4b4c4a7c7f235":{"changes":{"sklearn\/gaussian_process\/gpc.py":"MODIFY","sklearn\/compose\/tests\/test_target.py":"MODIFY","sklearn\/covariance\/elliptic_envelope.py":"MODIFY","sklearn\/naive_bayes.py":"MODIFY","sklearn\/impute\/_iterative.py":"MODIFY","sklearn\/preprocessing\/_encoders.py":"MODIFY","sklearn\/ensemble\/tests\/test_voting.py":"MODIFY","sklearn\/random_projection.py":"MODIFY","sklearn\/gaussian_process\/gpr.py":"MODIFY","sklearn\/utils\/mocking.py":"MODIFY","sklearn\/multioutput.py":"MODIFY","sklearn\/neural_network\/rbm.py":"MODIFY","sklearn\/tests\/test_base.py":"MODIFY","sklearn\/svm\/classes.py":"MODIFY","examples\/compose\/plot_column_transformer.py":"MODIFY","sklearn\/semi_supervised\/label_propagation.py":"MODIFY","sklearn\/ensemble\/base.py":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/binning.py":"MODIFY","sklearn\/feature_extraction\/dict_vectorizer.py":"MODIFY","sklearn\/linear_model\/bayes.py":"MODIFY","sklearn\/model_selection\/_search.py":"MODIFY","sklearn\/linear_model\/ransac.py":"MODIFY","sklearn\/cluster\/hierarchical.py":"MODIFY","sklearn\/tree\/tree.py":"MODIFY","sklearn\/discriminant_analysis.py":"MODIFY","sklearn\/neighbors\/nearest_centroid.py":"MODIFY","sklearn\/linear_model\/omp.py":"MODIFY","sklearn\/compose\/_target.py":"MODIFY","sklearn\/feature_selection\/univariate_selection.py":"MODIFY","sklearn\/cluster\/mean_shift_.py":"MODIFY","sklearn\/ensemble\/forest.py":"MODIFY","sklearn\/feature_selection\/variance_threshold.py":"MODIFY","sklearn\/ensemble\/iforest.py":"MODIFY","sklearn\/neighbors\/base.py":"MODIFY","sklearn\/pipeline.py":"MODIFY","sklearn\/svm\/base.py":"MODIFY","sklearn\/multiclass.py":"MODIFY","sklearn\/cluster\/dbscan_.py":"MODIFY","sklearn\/ensemble\/weight_boosting.py":"MODIFY","sklearn\/compose\/_column_transformer.py":"MODIFY","sklearn\/kernel_ridge.py":"MODIFY","sklearn\/feature_extraction\/text.py":"MODIFY","sklearn\/base.py":"MODIFY","sklearn\/isotonic.py":"MODIFY","sklearn\/decomposition\/factor_analysis.py":"MODIFY","sklearn\/linear_model\/coordinate_descent.py":"MODIFY","sklearn\/neighbors\/nca.py":"MODIFY","sklearn\/decomposition\/online_lda.py":"MODIFY","sklearn\/impute\/_base.py":"MODIFY","sklearn\/cluster\/spectral.py":"MODIFY","sklearn\/decomposition\/nmf.py":"MODIFY","sklearn\/utils\/tests\/test_estimator_checks.py":"MODIFY","sklearn\/utils\/tests\/test_pprint.py":"MODIFY","sklearn\/calibration.py":"MODIFY","sklearn\/decomposition\/fastica_.py":"MODIFY","sklearn\/kernel_approximation.py":"MODIFY","sklearn\/neural_network\/multilayer_perceptron.py":"MODIFY","sklearn\/feature_selection\/from_model.py":"MODIFY","sklearn\/cluster\/optics_.py":"MODIFY","sklearn\/decomposition\/base.py":"MODIFY","sklearn\/ensemble\/voting.py":"MODIFY","sklearn\/linear_model\/ridge.py":"MODIFY","sklearn\/preprocessing\/_discretization.py":"MODIFY","sklearn\/feature_extraction\/hashing.py":"MODIFY","sklearn\/feature_selection\/rfe.py":"MODIFY","sklearn\/linear_model\/theil_sen.py":"MODIFY","sklearn\/cluster\/affinity_propagation_.py":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/gradient_boosting.py":"MODIFY","sklearn\/decomposition\/truncated_svd.py":"MODIFY","sklearn\/linear_model\/base.py":"MODIFY","sklearn\/preprocessing\/data.py":"MODIFY","sklearn\/cluster\/tests\/test_bicluster.py":"MODIFY","sklearn\/preprocessing\/label.py":"MODIFY","sklearn\/cluster\/bicluster.py":"MODIFY","sklearn\/ensemble\/bagging.py":"MODIFY","sklearn\/manifold\/isomap.py":"MODIFY","sklearn\/linear_model\/stochastic_gradient.py":"MODIFY","sklearn\/decomposition\/dict_learning.py":"MODIFY","sklearn\/ensemble\/gradient_boosting.py":"MODIFY","sklearn\/dummy.py":"MODIFY","sklearn\/decomposition\/sparse_pca.py":"MODIFY","sklearn\/manifold\/locally_linear.py":"MODIFY","sklearn\/preprocessing\/_function_transformer.py":"MODIFY","sklearn\/cross_decomposition\/cca_.py":"MODIFY","sklearn\/cross_decomposition\/pls_.py":"MODIFY","sklearn\/decomposition\/kernel_pca.py":"MODIFY","sklearn\/cluster\/birch.py":"MODIFY","sklearn\/cluster\/k_means_.py":"MODIFY","sklearn\/inspection\/tests\/test_partial_dependence.py":"MODIFY","sklearn\/linear_model\/least_angle.py":"MODIFY"},"diff":{"sklearn\/gaussian_process\/gpc.py":[{"add":["451","class GaussianProcessClassifier(ClassifierMixin, BaseEstimator):"],"delete":["451","class GaussianProcessClassifier(BaseEstimator, ClassifierMixin):"]}],"sklearn\/compose\/tests\/test_target.py":[{"add":["228","class DummyCheckerArrayTransformer(TransformerMixin, BaseEstimator):","270","class DummyTransformer(TransformerMixin, BaseEstimator):"],"delete":["228","class DummyCheckerArrayTransformer(BaseEstimator, TransformerMixin):","270","class DummyTransformer(BaseEstimator, TransformerMixin):"]}],"sklearn\/covariance\/elliptic_envelope.py":[{"add":["11","class EllipticEnvelope(OutlierMixin, MinCovDet):"],"delete":["11","class EllipticEnvelope(MinCovDet, OutlierMixin):"]}],"sklearn\/naive_bayes.py":[{"add":["37","class BaseNB(ClassifierMixin, BaseEstimator, metaclass=ABCMeta):"],"delete":["37","class BaseNB(BaseEstimator, ClassifierMixin, metaclass=ABCMeta):"]}],"sklearn\/impute\/_iterative.py":[{"add":["27","class IterativeImputer(TransformerMixin, BaseEstimator):"],"delete":["27","class IterativeImputer(BaseEstimator, TransformerMixin):"]}],"sklearn\/preprocessing\/_encoders.py":[{"add":["21","class _BaseEncoder(TransformerMixin, BaseEstimator):"],"delete":["21","class _BaseEncoder(BaseEstimator, TransformerMixin):"]}],"sklearn\/ensemble\/tests\/test_voting.py":[{"add":["335","    class ClassifierErrorFit(ClassifierMixin, BaseEstimator):","345","    class MockClassifier(ClassifierMixin, BaseEstimator):"],"delete":["335","    class ClassifierErrorFit(BaseEstimator, ClassifierMixin):","345","    class MockClassifier(BaseEstimator, ClassifierMixin):"]}],"sklearn\/random_projection.py":[{"add":["291","class BaseRandomProjection(TransformerMixin, BaseEstimator, metaclass=ABCMeta):"],"delete":["291","class BaseRandomProjection(BaseEstimator, TransformerMixin, metaclass=ABCMeta):"]}],"sklearn\/gaussian_process\/gpr.py":[{"add":["21","class GaussianProcessRegressor(MultiOutputMixin,","22","                               RegressorMixin, BaseEstimator):"],"delete":["21","class GaussianProcessRegressor(BaseEstimator, RegressorMixin,","22","                               MultiOutputMixin):"]}],"sklearn\/utils\/mocking.py":[{"add":["50","class CheckingClassifier(ClassifierMixin, BaseEstimator):"],"delete":["50","class CheckingClassifier(BaseEstimator, ClassifierMixin):"]}],"sklearn\/multioutput.py":[{"add":["204","class MultiOutputRegressor(RegressorMixin, MultiOutputEstimator):","299","class MultiOutputClassifier(ClassifierMixin, MultiOutputEstimator):","517","class ClassifierChain(MetaEstimatorMixin, ClassifierMixin, _BaseChain):","677","class RegressorChain(MetaEstimatorMixin, RegressorMixin, _BaseChain):"],"delete":["204","class MultiOutputRegressor(MultiOutputEstimator, RegressorMixin):","299","class MultiOutputClassifier(MultiOutputEstimator, ClassifierMixin):","517","class ClassifierChain(_BaseChain, ClassifierMixin, MetaEstimatorMixin):","677","class RegressorChain(_BaseChain, RegressorMixin, MetaEstimatorMixin):"]}],"sklearn\/neural_network\/rbm.py":[{"add":["25","class BernoulliRBM(TransformerMixin, BaseEstimator):"],"delete":["25","class BernoulliRBM(BaseEstimator, TransformerMixin):"]}],"sklearn\/tests\/test_base.py":[{"add":["65","    def _more_tags(self):","66","        return dict()","67","","68","","69","class InheritDiamondOverwriteTag(DiamondOverwriteTag):","300","    class DummyEstimator(TransformerMixin, BaseEstimator):","415","class MultiInheritanceEstimator(DontPickleAttributeMixin, BaseEstimator):","482","    redefine_tags_est = OverrideTag()","483","    assert not redefine_tags_est._get_tags()['allow_nan']","486","    assert diamond_tag_est._get_tags()['allow_nan']","487","","488","    inherit_diamond_tag_est = InheritDiamondOverwriteTag()","489","    assert inherit_diamond_tag_est._get_tags()['allow_nan']"],"delete":["295","    class DummyEstimator(BaseEstimator, TransformerMixin):","410","class MultiInheritanceEstimator(BaseEstimator, DontPickleAttributeMixin):","477","    invalid_tags_est = OverrideTag()","478","    with pytest.raises(TypeError, match=\"Inconsistent values for tag\"):","479","        invalid_tags_est._get_tags()","482","    with pytest.raises(TypeError, match=\"Inconsistent values for tag\"):","483","        diamond_tag_est._get_tags()"]}],"sklearn\/svm\/classes.py":[{"add":["253","class LinearSVR(RegressorMixin, LinearModel):","826","class SVR(RegressorMixin, BaseLibSVM):","958","class NuSVR(RegressorMixin, BaseLibSVM):","1083","class OneClassSVM(OutlierMixin, BaseLibSVM):"],"delete":["253","class LinearSVR(LinearModel, RegressorMixin):","826","class SVR(BaseLibSVM, RegressorMixin):","958","class NuSVR(BaseLibSVM, RegressorMixin):","1083","class OneClassSVM(BaseLibSVM, OutlierMixin):"]}],"examples\/compose\/plot_column_transformer.py":[{"add":["44","class TextStats(TransformerMixin, BaseEstimator):","56","class SubjectBodyExtractor(TransformerMixin, BaseEstimator):"],"delete":["44","class TextStats(BaseEstimator, TransformerMixin):","56","class SubjectBodyExtractor(BaseEstimator, TransformerMixin):"]}],"sklearn\/semi_supervised\/label_propagation.py":[{"add":["73","class BaseLabelPropagation(ClassifierMixin, BaseEstimator, metaclass=ABCMeta):"],"delete":["73","class BaseLabelPropagation(BaseEstimator, ClassifierMixin, metaclass=ABCMeta):"]}],"sklearn\/ensemble\/base.py":[{"add":["60","class BaseEnsemble(MetaEstimatorMixin, BaseEstimator, metaclass=ABCMeta):"],"delete":["60","class BaseEnsemble(BaseEstimator, MetaEstimatorMixin, metaclass=ABCMeta):"]}],"sklearn\/ensemble\/_hist_gradient_boosting\/binning.py":[{"add":["85","class _BinMapper(TransformerMixin, BaseEstimator):"],"delete":["85","class _BinMapper(BaseEstimator, TransformerMixin):"]}],"sklearn\/feature_extraction\/dict_vectorizer.py":[{"add":["23","class DictVectorizer(TransformerMixin, BaseEstimator):"],"delete":["23","class DictVectorizer(BaseEstimator, TransformerMixin):"]}],"sklearn\/linear_model\/bayes.py":[{"add":["21","class BayesianRidge(RegressorMixin, LinearModel):","377","class ARDRegression(RegressorMixin, LinearModel):"],"delete":["21","class BayesianRidge(LinearModel, RegressorMixin):","377","class ARDRegression(LinearModel, RegressorMixin):"]}],"sklearn\/model_selection\/_search.py":[{"add":["399","class BaseSearchCV(MetaEstimatorMixin, BaseEstimator, metaclass=ABCMeta):"],"delete":["399","class BaseSearchCV(BaseEstimator, MetaEstimatorMixin, metaclass=ABCMeta):"]}],"sklearn\/linear_model\/ransac.py":[{"add":["55","class RANSACRegressor(MetaEstimatorMixin, RegressorMixin,","56","                      MultiOutputMixin, BaseEstimator):"],"delete":["55","class RANSACRegressor(BaseEstimator, MetaEstimatorMixin, RegressorMixin,","56","                      MultiOutputMixin):"]}],"sklearn\/cluster\/hierarchical.py":[{"add":["654","class AgglomerativeClustering(ClusterMixin, BaseEstimator):"],"delete":["654","class AgglomerativeClustering(BaseEstimator, ClusterMixin):"]}],"sklearn\/tree\/tree.py":[{"add":["75","class BaseDecisionTree(MultiOutputMixin, BaseEstimator, metaclass=ABCMeta):","599","class DecisionTreeClassifier(ClassifierMixin, BaseDecisionTree):","974","class DecisionTreeRegressor(RegressorMixin, BaseDecisionTree):"],"delete":["75","class BaseDecisionTree(BaseEstimator, MultiOutputMixin, metaclass=ABCMeta):","599","class DecisionTreeClassifier(BaseDecisionTree, ClassifierMixin):","974","class DecisionTreeRegressor(BaseDecisionTree, RegressorMixin):"]}],"sklearn\/discriminant_analysis.py":[{"add":["555","class QuadraticDiscriminantAnalysis(ClassifierMixin, BaseEstimator):"],"delete":["555","class QuadraticDiscriminantAnalysis(BaseEstimator, ClassifierMixin):"]}],"sklearn\/neighbors\/nearest_centroid.py":[{"add":["22","class NearestCentroid(ClassifierMixin, BaseEstimator):"],"delete":["22","class NearestCentroid(BaseEstimator, ClassifierMixin):"]}],"sklearn\/linear_model\/omp.py":[{"add":["541","class OrthogonalMatchingPursuit(MultiOutputMixin, RegressorMixin, LinearModel):","755","class OrthogonalMatchingPursuitCV(RegressorMixin, LinearModel):"],"delete":["541","class OrthogonalMatchingPursuit(LinearModel, RegressorMixin, MultiOutputMixin):","755","class OrthogonalMatchingPursuitCV(LinearModel, RegressorMixin):"]}],"sklearn\/compose\/_target.py":[{"add":["16","class TransformedTargetRegressor(RegressorMixin, BaseEstimator):"],"delete":["16","class TransformedTargetRegressor(BaseEstimator, RegressorMixin):"]}],"sklearn\/feature_selection\/univariate_selection.py":[{"add":["311","class _BaseFilter(SelectorMixin, BaseEstimator):"],"delete":["311","class _BaseFilter(BaseEstimator, SelectorMixin):"]}],"sklearn\/cluster\/mean_shift_.py":[{"add":["295","class MeanShift(ClusterMixin, BaseEstimator):"],"delete":["295","class MeanShift(BaseEstimator, ClusterMixin):"]}],"sklearn\/ensemble\/forest.py":[{"add":["124","class BaseForest(MultiOutputMixin, BaseEnsemble, metaclass=ABCMeta):","394","class ForestClassifier(ClassifierMixin, BaseForest, metaclass=ABCMeta):","635","class ForestRegressor(RegressorMixin, BaseForest, metaclass=ABCMeta):"],"delete":["124","class BaseForest(BaseEnsemble, MultiOutputMixin, metaclass=ABCMeta):","394","class ForestClassifier(BaseForest, ClassifierMixin, metaclass=ABCMeta):","635","class ForestRegressor(BaseForest, RegressorMixin, metaclass=ABCMeta):"]}],"sklearn\/feature_selection\/variance_threshold.py":[{"add":["11","class VarianceThreshold(SelectorMixin, BaseEstimator):"],"delete":["11","class VarianceThreshold(BaseEstimator, SelectorMixin):"]}],"sklearn\/ensemble\/iforest.py":[{"add":["26","class IsolationForest(OutlierMixin, BaseBagging):"],"delete":["26","class IsolationForest(BaseBagging, OutlierMixin):"]}],"sklearn\/neighbors\/base.py":[{"add":["105","class NeighborsBase(MultiOutputMixin, BaseEstimator, metaclass=ABCMeta):"],"delete":["105","class NeighborsBase(BaseEstimator, MultiOutputMixin, metaclass=ABCMeta):"]}],"sklearn\/pipeline.py":[{"add":["746","class FeatureUnion(TransformerMixin, _BaseComposition):"],"delete":["746","class FeatureUnion(_BaseComposition, TransformerMixin):"]}],"sklearn\/svm\/base.py":[{"add":["487","class BaseSVC(ClassifierMixin, BaseLibSVM, metaclass=ABCMeta):"],"delete":["487","class BaseSVC(BaseLibSVM, ClassifierMixin, metaclass=ABCMeta):"]}],"sklearn\/multiclass.py":[{"add":["131","class OneVsRestClassifier(MultiOutputMixin, ClassifierMixin,","132","                          MetaEstimatorMixin, BaseEstimator):","437","class OneVsOneClassifier(MetaEstimatorMixin, ClassifierMixin, BaseEstimator):","564","                                             (combinations)))","636","class OutputCodeClassifier(MetaEstimatorMixin, ClassifierMixin, BaseEstimator):"],"delete":["131","class OneVsRestClassifier(BaseEstimator, ClassifierMixin, MetaEstimatorMixin,","132","                          MultiOutputMixin):","437","class OneVsOneClassifier(BaseEstimator, ClassifierMixin, MetaEstimatorMixin):","564","                                              (combinations)))","636","class OutputCodeClassifier(BaseEstimator, ClassifierMixin, MetaEstimatorMixin):"]}],"sklearn\/cluster\/dbscan_.py":[{"add":["192","class DBSCAN(ClusterMixin, BaseEstimator):"],"delete":["192","class DBSCAN(BaseEstimator, ClusterMixin):"]}],"sklearn\/ensemble\/weight_boosting.py":[{"add":["292","class AdaBoostClassifier(ClassifierMixin, BaseWeightBoosting):","856","class AdaBoostRegressor(RegressorMixin, BaseWeightBoosting):"],"delete":["292","class AdaBoostClassifier(BaseWeightBoosting, ClassifierMixin):","856","class AdaBoostRegressor(BaseWeightBoosting, RegressorMixin):"]}],"sklearn\/compose\/_column_transformer.py":[{"add":["35","class ColumnTransformer(TransformerMixin, _BaseComposition):"],"delete":["35","class ColumnTransformer(_BaseComposition, TransformerMixin):"]}],"sklearn\/kernel_ridge.py":[{"add":["15","class KernelRidge(MultiOutputMixin, RegressorMixin, BaseEstimator):"],"delete":["15","class KernelRidge(BaseEstimator, RegressorMixin, MultiOutputMixin):"]}],"sklearn\/feature_extraction\/text.py":[{"add":["471","class HashingVectorizer(TransformerMixin, VectorizerMixin, BaseEstimator):","755","class CountVectorizer(VectorizerMixin, BaseEstimator):","1231","class TfidfTransformer(TransformerMixin, BaseEstimator):"],"delete":["471","class HashingVectorizer(BaseEstimator, VectorizerMixin, TransformerMixin):","755","class CountVectorizer(BaseEstimator, VectorizerMixin):","1231","class TfidfTransformer(BaseEstimator, TransformerMixin):"]}],"sklearn\/base.py":[{"add":["311","    def _more_tags(self):","312","        return _DEFAULT_TAGS","313","","316","        for base_class in reversed(inspect.getmro(self.__class__)):","317","            if hasattr(base_class, '_more_tags'):","318","                # need the if because mixins might not have _more_tags","319","                # but might do redundant work in estimators","320","                # (i.e. calling more tags on BaseEstimator multiple times)","322","                collected_tags.update(more_tags)","323","        return collected_tags"],"delete":["131","def _update_if_consistent(dict1, dict2):","132","    common_keys = set(dict1.keys()).intersection(dict2.keys())","133","    for key in common_keys:","134","        if dict1[key] != dict2[key]:","135","            raise TypeError(\"Inconsistent values for tag {}: {} != {}\".format(","136","                key, dict1[key], dict2[key]","137","            ))","138","    dict1.update(dict2)","139","    return dict1","140","","141","","324","        for base_class in inspect.getmro(self.__class__):","325","            if (hasattr(base_class, '_more_tags')","326","                    and base_class != self.__class__):","328","                collected_tags = _update_if_consistent(collected_tags,","329","                                                       more_tags)","330","        if hasattr(self, '_more_tags'):","331","            more_tags = self._more_tags()","332","            collected_tags = _update_if_consistent(collected_tags, more_tags)","333","        tags = _DEFAULT_TAGS.copy()","334","        tags.update(collected_tags)","335","        return tags"]}],"sklearn\/isotonic.py":[{"add":["139","class IsotonicRegression(RegressorMixin, TransformerMixin, BaseEstimator):"],"delete":["139","class IsotonicRegression(BaseEstimator, TransformerMixin, RegressorMixin):"]}],"sklearn\/decomposition\/factor_analysis.py":[{"add":["34","class FactorAnalysis(TransformerMixin, BaseEstimator):"],"delete":["34","class FactorAnalysis(BaseEstimator, TransformerMixin):"]}],"sklearn\/linear_model\/coordinate_descent.py":[{"add":["500","class ElasticNet(MultiOutputMixin, RegressorMixin, LinearModel):","1037","class LinearModelCV(MultiOutputMixin, LinearModel, metaclass=ABCMeta):","1233","class LassoCV(RegressorMixin, LinearModelCV):","1399","class ElasticNetCV(RegressorMixin, LinearModelCV):","1919","class MultiTaskElasticNetCV(RegressorMixin, LinearModelCV):","2107","class MultiTaskLassoCV(RegressorMixin, LinearModelCV):"],"delete":["500","class ElasticNet(LinearModel, RegressorMixin, MultiOutputMixin):","1037","class LinearModelCV(LinearModel, MultiOutputMixin, metaclass=ABCMeta):","1233","class LassoCV(LinearModelCV, RegressorMixin):","1399","class ElasticNetCV(LinearModelCV, RegressorMixin):","1919","class MultiTaskElasticNetCV(LinearModelCV, RegressorMixin):","2107","class MultiTaskLassoCV(LinearModelCV, RegressorMixin):"]}],"sklearn\/neighbors\/nca.py":[{"add":["29","class NeighborhoodComponentsAnalysis(TransformerMixin, BaseEstimator):"],"delete":["29","class NeighborhoodComponentsAnalysis(BaseEstimator, TransformerMixin):"]}],"sklearn\/decomposition\/online_lda.py":[{"add":["134","class LatentDirichletAllocation(TransformerMixin, BaseEstimator):"],"delete":["134","class LatentDirichletAllocation(BaseEstimator, TransformerMixin):"]}],"sklearn\/impute\/_base.py":[{"add":["66","class SimpleImputer(TransformerMixin, BaseEstimator):","418","class MissingIndicator(TransformerMixin, BaseEstimator):"],"delete":["66","class SimpleImputer(BaseEstimator, TransformerMixin):","418","class MissingIndicator(BaseEstimator, TransformerMixin):"]}],"sklearn\/cluster\/spectral.py":[{"add":["274","class SpectralClustering(ClusterMixin, BaseEstimator):"],"delete":["274","class SpectralClustering(BaseEstimator, ClusterMixin):"]}],"sklearn\/decomposition\/nmf.py":[{"add":["1070","class NMF(TransformerMixin, BaseEstimator):"],"delete":["1070","class NMF(BaseEstimator, TransformerMixin):"]}],"sklearn\/utils\/tests\/test_estimator_checks.py":[{"add":["45","class BaseBadClassifier(ClassifierMixin, BaseEstimator):"],"delete":["45","class BaseBadClassifier(BaseEstimator, ClassifierMixin):"]}],"sklearn\/utils\/tests\/test_pprint.py":[{"add":["43","class StandardScaler(TransformerMixin, BaseEstimator):"],"delete":["43","class StandardScaler(BaseEstimator, TransformerMixin):"]}],"sklearn\/calibration.py":[{"add":["467","class _SigmoidCalibration(RegressorMixin, BaseEstimator):"],"delete":["467","class _SigmoidCalibration(BaseEstimator, RegressorMixin):"]}],"sklearn\/decomposition\/fastica_.py":[{"add":["382","class FastICA(TransformerMixin, BaseEstimator):"],"delete":["382","class FastICA(BaseEstimator, TransformerMixin):"]}],"sklearn\/kernel_approximation.py":[{"add":["23","class RBFSampler(TransformerMixin, BaseEstimator):","127","class SkewedChi2Sampler(TransformerMixin, BaseEstimator):","241","class AdditiveChi2Sampler(TransformerMixin, BaseEstimator):","431","class Nystroem(TransformerMixin, BaseEstimator):"],"delete":["23","class RBFSampler(BaseEstimator, TransformerMixin):","127","class SkewedChi2Sampler(BaseEstimator, TransformerMixin):","241","class AdditiveChi2Sampler(BaseEstimator, TransformerMixin):","431","class Nystroem(BaseEstimator, TransformerMixin):"]}],"sklearn\/neural_network\/multilayer_perceptron.py":[{"add":["690","class MLPClassifier(ClassifierMixin, BaseMultilayerPerceptron):","1082","class MLPRegressor(RegressorMixin, BaseMultilayerPerceptron):"],"delete":["690","class MLPClassifier(BaseMultilayerPerceptron, ClassifierMixin):","1082","class MLPRegressor(BaseMultilayerPerceptron, RegressorMixin):"]}],"sklearn\/feature_selection\/from_model.py":[{"add":["80","class SelectFromModel(MetaEstimatorMixin, SelectorMixin, BaseEstimator):"],"delete":["80","class SelectFromModel(BaseEstimator, SelectorMixin, MetaEstimatorMixin):"]}],"sklearn\/cluster\/optics_.py":[{"add":["23","class OPTICS(ClusterMixin, BaseEstimator):"],"delete":["23","class OPTICS(BaseEstimator, ClusterMixin):"]}],"sklearn\/decomposition\/base.py":[{"add":["19","class _BasePCA(TransformerMixin, BaseEstimator, metaclass=ABCMeta):"],"delete":["19","class _BasePCA(BaseEstimator, TransformerMixin, metaclass=ABCMeta):"]}],"sklearn\/ensemble\/voting.py":[{"add":["50","class _BaseVoting(TransformerMixin, _BaseComposition):","147","class VotingClassifier(ClassifierMixin, _BaseVoting):","377","class VotingRegressor(RegressorMixin, _BaseVoting):"],"delete":["50","class _BaseVoting(_BaseComposition, TransformerMixin):","147","class VotingClassifier(_BaseVoting, ClassifierMixin):","377","class VotingRegressor(_BaseVoting, RegressorMixin):"]}],"sklearn\/linear_model\/ridge.py":[{"add":["523","class _BaseRidge(MultiOutputMixin, LinearModel, metaclass=ABCMeta):","604","class Ridge(RegressorMixin, _BaseRidge):","1507","class _BaseRidgeCV(MultiOutputMixin, LinearModel):","1579","class RidgeCV(RegressorMixin, _BaseRidgeCV):"],"delete":["523","class _BaseRidge(LinearModel, MultiOutputMixin, metaclass=ABCMeta):","604","class Ridge(_BaseRidge, RegressorMixin):","1507","class _BaseRidgeCV(LinearModel, MultiOutputMixin):","1579","class RidgeCV(_BaseRidgeCV, RegressorMixin):"]}],"sklearn\/preprocessing\/_discretization.py":[{"add":["20","class KBinsDiscretizer(TransformerMixin, BaseEstimator):"],"delete":["20","class KBinsDiscretizer(BaseEstimator, TransformerMixin):"]}],"sklearn\/feature_extraction\/hashing.py":[{"add":["26","class FeatureHasher(TransformerMixin, BaseEstimator):"],"delete":["26","class FeatureHasher(BaseEstimator, TransformerMixin):"]}],"sklearn\/feature_selection\/rfe.py":[{"add":["36","class RFE(SelectorMixin, MetaEstimatorMixin, BaseEstimator):"],"delete":["36","class RFE(BaseEstimator, MetaEstimatorMixin, SelectorMixin):"]}],"sklearn\/linear_model\/theil_sen.py":[{"add":["195","class TheilSenRegressor(RegressorMixin, LinearModel):"],"delete":["195","class TheilSenRegressor(LinearModel, RegressorMixin):"]}],"sklearn\/cluster\/affinity_propagation_.py":[{"add":["235","class AffinityPropagation(ClusterMixin, BaseEstimator):"],"delete":["235","class AffinityPropagation(BaseEstimator, ClusterMixin):"]}],"sklearn\/ensemble\/_hist_gradient_boosting\/gradient_boosting.py":[{"add":["641","class HistGradientBoostingRegressor(RegressorMixin, BaseHistGradientBoosting):"],"delete":["641","class HistGradientBoostingRegressor(BaseHistGradientBoosting, RegressorMixin):"]}],"sklearn\/decomposition\/truncated_svd.py":[{"add":["20","class TruncatedSVD(TransformerMixin, BaseEstimator):"],"delete":["20","class TruncatedSVD(BaseEstimator, TransformerMixin):"]}],"sklearn\/linear_model\/base.py":[{"add":["364","class LinearRegression(MultiOutputMixin, RegressorMixin, LinearModel):"],"delete":["364","class LinearRegression(LinearModel, RegressorMixin, MultiOutputMixin):"]}],"sklearn\/preprocessing\/data.py":[{"add":["198","class MinMaxScaler(TransformerMixin, BaseEstimator):","495","class StandardScaler(TransformerMixin, BaseEstimator):","823","class MaxAbsScaler(TransformerMixin, BaseEstimator):","1052","class RobustScaler(TransformerMixin, BaseEstimator):","1330","class PolynomialFeatures(TransformerMixin, BaseEstimator):","1703","class Normalizer(TransformerMixin, BaseEstimator):","1841","class Binarizer(TransformerMixin, BaseEstimator):","1932","class KernelCenterer(TransformerMixin, BaseEstimator):","2093","class QuantileTransformer(TransformerMixin, BaseEstimator):","2637","class PowerTransformer(TransformerMixin, BaseEstimator):"],"delete":["198","class MinMaxScaler(BaseEstimator, TransformerMixin):","495","class StandardScaler(BaseEstimator, TransformerMixin):","823","class MaxAbsScaler(BaseEstimator, TransformerMixin):","1052","class RobustScaler(BaseEstimator, TransformerMixin):","1330","class PolynomialFeatures(BaseEstimator, TransformerMixin):","1703","class Normalizer(BaseEstimator, TransformerMixin):","1841","class Binarizer(BaseEstimator, TransformerMixin):","1932","class KernelCenterer(BaseEstimator, TransformerMixin):","2093","class QuantileTransformer(BaseEstimator, TransformerMixin):","2637","class PowerTransformer(BaseEstimator, TransformerMixin):"]}],"sklearn\/cluster\/tests\/test_bicluster.py":[{"add":["26","class MockBiclustering(BiclusterMixin, BaseEstimator):"],"delete":["26","class MockBiclustering(BaseEstimator, BiclusterMixin):"]}],"sklearn\/preprocessing\/label.py":[{"add":["170","class LabelEncoder(TransformerMixin, BaseEstimator):","302","class LabelBinarizer(TransformerMixin, BaseEstimator):","783","class MultiLabelBinarizer(TransformerMixin, BaseEstimator):"],"delete":["170","class LabelEncoder(BaseEstimator, TransformerMixin):","302","class LabelBinarizer(BaseEstimator, TransformerMixin):","783","class MultiLabelBinarizer(BaseEstimator, TransformerMixin):"]}],"sklearn\/cluster\/bicluster.py":[{"add":["86","class BaseSpectral(BiclusterMixin, BaseEstimator, metaclass=ABCMeta):"],"delete":["86","class BaseSpectral(BaseEstimator, BiclusterMixin, metaclass=ABCMeta):"]}],"sklearn\/ensemble\/bagging.py":[{"add":["431","class BaggingClassifier(ClassifierMixin, BaseBagging):","818","class BaggingRegressor(RegressorMixin, BaseBagging):"],"delete":["431","class BaggingClassifier(BaseBagging, ClassifierMixin):","818","class BaggingRegressor(BaseBagging, RegressorMixin):"]}],"sklearn\/manifold\/isomap.py":[{"add":["14","class Isomap(TransformerMixin, BaseEstimator):"],"delete":["14","class Isomap(BaseEstimator, TransformerMixin):"]}],"sklearn\/linear_model\/stochastic_gradient.py":[{"add":["67","class BaseSGD(SparseCoefMixin, BaseEstimator, metaclass=ABCMeta):","422","class BaseSGDClassifier(LinearClassifierMixin, BaseSGD, metaclass=ABCMeta):","1052","class BaseSGDRegressor(RegressorMixin, BaseSGD):"],"delete":["67","class BaseSGD(BaseEstimator, SparseCoefMixin, metaclass=ABCMeta):","422","class BaseSGDClassifier(BaseSGD, LinearClassifierMixin, metaclass=ABCMeta):","1052","class BaseSGDRegressor(BaseSGD, RegressorMixin):"]}],"sklearn\/decomposition\/dict_learning.py":[{"add":["934","class SparseCoder(SparseCodingMixin, BaseEstimator):","1047","class DictionaryLearning(SparseCodingMixin, BaseEstimator):","1243","class MiniBatchDictionaryLearning(SparseCodingMixin, BaseEstimator):"],"delete":["934","class SparseCoder(BaseEstimator, SparseCodingMixin):","1047","class DictionaryLearning(BaseEstimator, SparseCodingMixin):","1243","class MiniBatchDictionaryLearning(BaseEstimator, SparseCodingMixin):"]}],"sklearn\/ensemble\/gradient_boosting.py":[{"add":["1812","class GradientBoostingClassifier(ClassifierMixin, BaseGradientBoosting):","2288","class GradientBoostingRegressor(RegressorMixin, BaseGradientBoosting):"],"delete":["1812","class GradientBoostingClassifier(BaseGradientBoosting, ClassifierMixin):","2288","class GradientBoostingRegressor(BaseGradientBoosting, RegressorMixin):"]}],"sklearn\/dummy.py":[{"add":["21","class DummyClassifier(MultiOutputMixin, ClassifierMixin, BaseEstimator):","355","class DummyRegressor(MultiOutputMixin, RegressorMixin, BaseEstimator):"],"delete":["21","class DummyClassifier(BaseEstimator, ClassifierMixin, MultiOutputMixin):","355","class DummyRegressor(BaseEstimator, RegressorMixin, MultiOutputMixin):"]}],"sklearn\/decomposition\/sparse_pca.py":[{"add":["31","class SparsePCA(TransformerMixin, BaseEstimator):"],"delete":["31","class SparsePCA(BaseEstimator, TransformerMixin):"]}],"sklearn\/manifold\/locally_linear.py":[{"add":["520","class LocallyLinearEmbedding(TransformerMixin,","521","                             _UnstableArchMixin, BaseEstimator):"],"delete":["520","class LocallyLinearEmbedding(BaseEstimator, TransformerMixin,","521","                             _UnstableArchMixin):"]}],"sklearn\/preprocessing\/_function_transformer.py":[{"add":["13","class FunctionTransformer(TransformerMixin, BaseEstimator):"],"delete":["13","class FunctionTransformer(BaseEstimator, TransformerMixin):"]}],"sklearn\/cross_decomposition\/cca_.py":[{"add":["6","class CCA(_UnstableArchMixin, _PLS):"],"delete":["6","class CCA(_PLS, _UnstableArchMixin):"]}],"sklearn\/cross_decomposition\/pls_.py":[{"add":["123","class _PLS(TransformerMixin, RegressorMixin, MultiOutputMixin, BaseEstimator,","752","class PLSSVD(TransformerMixin, BaseEstimator):"],"delete":["123","class _PLS(BaseEstimator, TransformerMixin, RegressorMixin, MultiOutputMixin,","752","class PLSSVD(BaseEstimator, TransformerMixin):"]}],"sklearn\/decomposition\/kernel_pca.py":[{"add":["18","class KernelPCA(TransformerMixin, BaseEstimator):"],"delete":["18","class KernelPCA(BaseEstimator, TransformerMixin):"]}],"sklearn\/cluster\/birch.py":[{"add":["321","class Birch(ClusterMixin, TransformerMixin, BaseEstimator):"],"delete":["321","class Birch(BaseEstimator, TransformerMixin, ClusterMixin):"]}],"sklearn\/cluster\/k_means_.py":[{"add":["763","class KMeans(TransformerMixin, ClusterMixin, BaseEstimator):"],"delete":["763","class KMeans(BaseEstimator, ClusterMixin, TransformerMixin):"]}],"sklearn\/inspection\/tests\/test_partial_dependence.py":[{"add":["289","class NoPredictProbaNoDecisionFunction(ClassifierMixin, BaseEstimator):"],"delete":["289","class NoPredictProbaNoDecisionFunction(BaseEstimator, ClassifierMixin):"]}],"sklearn\/linear_model\/least_angle.py":[{"add":["763","class Lars(MultiOutputMixin, RegressorMixin, LinearModel):"],"delete":["763","class Lars(LinearModel, RegressorMixin, MultiOutputMixin):"]}]}},"992ed41ecc693f93a94ffd8cca52117af16e096e":{"changes":{"sklearn\/utils\/tests\/test_utils.py":"MODIFY","sklearn\/compose\/tests\/test_column_transformer.py":"MODIFY","sklearn\/utils\/__init__.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY"},"diff":{"sklearn\/utils\/tests\/test_utils.py":[{"add":["12","                                   assert_allclose_dense_sparse,","15","from sklearn.utils import _array_indexing","369","@pytest.mark.parametrize(\"array_type\", ['array', 'sparse', 'dataframe'])","370","def test_safe_indexing_mask_axis_1(array_type):","371","    # regression test for #14510","372","    # check that boolean array-like and boolean array lead to the same indexing","373","    # even in NumPy < 1.12","374","    if array_type == 'array':","375","        array_constructor = np.asarray","376","    elif array_type == 'sparse':","377","        array_constructor = sp.csr_matrix","378","    elif array_type == 'dataframe':","379","        pd = pytest.importorskip('pandas')","380","        array_constructor = pd.DataFrame","381","","382","    X = array_constructor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])","383","    mask = [True, False, True]","384","    mask_array = np.array(mask)","385","    X_masked = safe_indexing(X, mask, axis=1)","386","    X_masked_array = safe_indexing(X, mask_array, axis=1)","387","    assert_allclose_dense_sparse(X_masked, X_masked_array)","388","","389","","390","def test_array_indexing_array_error():","391","    X = np.array([[0, 1], [2, 3]])","392","    mask = [True, False]","393","    with pytest.raises(ValueError, match=\"'axis' should be either 0\"):","394","        _array_indexing(X, mask, axis=3)","395","","396",""],"delete":[]}],"sklearn\/compose\/tests\/test_column_transformer.py":[{"add":["18","from sklearn.preprocessing import FunctionTransformer","1111","","1112","","1113","@pytest.mark.parametrize(\"array_type\", [np.asarray, sparse.csr_matrix])","1114","def test_column_transformer_mask_indexing(array_type):","1115","    # Regression test for #14510","1116","    # Boolean array-like does not behave as boolean array with NumPy < 1.12","1117","    # and sparse matrices as well","1118","    X = np.transpose([[1, 2, 3], [4, 5, 6], [5, 6, 7], [8, 9, 10]])","1119","    X = array_type(X)","1120","    column_transformer = ColumnTransformer(","1121","        [('identity', FunctionTransformer(), [False, True, False, True])]","1122","    )","1123","    X_trans = column_transformer.fit_transform(X)","1124","    assert X_trans.shape == (3, 2)"],"delete":[]}],"sklearn\/utils\/__init__.py":[{"add":["20","from .fixes import np_version","228","def _array_indexing(array, key, axis=0):","229","    \"\"\"Index an array consistently across NumPy version.\"\"\"","230","    if axis not in (0, 1):","231","        raise ValueError(","232","            \"'axis' should be either 0 (to index rows) or 1 (to index \"","233","            \" column). Got {} instead.\".format(axis)","234","        )","235","    if np_version < (1, 12) or issparse(array):","236","        # check if we have an boolean array-likes to make the proper indexing","237","        key_array = np.asarray(key)","238","        if np.issubdtype(key_array.dtype, np.bool_):","239","            key = key_array","240","    return array[key] if axis == 0 else array[:, key]","241","","242","","284","            return _array_indexing(X, indices, axis=0)","374","            return _array_indexing(X, key, axis=1)","389","            idx = safe_indexing(np.arange(n_columns), key)"],"delete":["268","            return X[indices]","358","            return X[:, key]","373","            idx = np.arange(n_columns)[key]"]}],"doc\/whats_new\/v0.22.rst":[{"add":["63",":mod:`sklearn.compose`","64","......................","65","","66","- |Fix| Fixed a bug in :class:`compose.ColumnTransformer` which failed to","67","  select the proper columns when using a boolean list, with NumPy older than","68","  1.12.","69","  :pr:`14510` by :user:`Guillaume Lemaitre <glemaitre>`.","70",""],"delete":[]}]}},"801cca8e73215d4946f05379319d97156be659d6":{"changes":{"doc\/conf.py":"MODIFY"},"diff":{"doc\/conf.py":[{"add":[],"delete":["305","                        module=\"matplotlib\","]}]}},"4a325353ef1d9f10bf65f9f89763284ecf0de9e2":{"changes":{"README.rst":"MODIFY"},"diff":{"README.rst":[{"add":["29","SciPy and is distributed under the 3-Clause BSD license.","54","**Scikit-learn 0.20 was the last version to support Python 2.7.**","55","scikit-learn 0.21 and later require Python 3.5 or newer.","71","the easiest way to install scikit-learn is using ``pip``   ::"],"delete":["29","SciPy and distributed under the 3-Clause BSD license.","54","**Scikit-learn 0.20 was the last version to support Python2.7.**","55","Scikit-learn 0.21 and later require Python 3.5 or newer.","71","the easiest way to install scikit-learn is using ``pip`` ::"]}]}},"220e146a7fe2bb4f3d45d30cab5505bc22f2db85":{"changes":{"sklearn\/linear_model\/tests\/test_logistic.py":"MODIFY","sklearn\/linear_model\/logistic.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY"},"diff":{"sklearn\/linear_model\/tests\/test_logistic.py":[{"add":["15","from sklearn.model_selection import cross_val_score","1763","","1764","","1765","def test_scores_attribute_layout_elasticnet():","1766","    # Non regression test for issue #14955.","1767","    # when penalty is elastic net the scores_ attribute has shape","1768","    # (n_classes, n_Cs, n_l1_ratios)","1769","    # We here make sure that the second dimension indeed corresponds to Cs and","1770","    # the third dimension corresponds to l1_ratios.","1771","","1772","    X, y = make_classification(n_samples=1000, random_state=0)","1773","    cv = StratifiedKFold(n_splits=5, shuffle=False)","1774","","1775","    l1_ratios = [.1, .9]","1776","    Cs = [.1, 1, 10]","1777","","1778","    lrcv = LogisticRegressionCV(penalty='elasticnet', solver='saga',","1779","                                l1_ratios=l1_ratios, Cs=Cs, cv=cv,","1780","                                random_state=0)","1781","    lrcv.fit(X, y)","1782","","1783","    avg_scores_lrcv = lrcv.scores_[1].mean(axis=0)  # average over folds","1784","","1785","    for i, C in enumerate(Cs):","1786","        for j, l1_ratio in enumerate(l1_ratios):","1787","","1788","            lr = LogisticRegression(penalty='elasticnet', solver='saga', C=C,","1789","                                    l1_ratio=l1_ratio, random_state=0)","1790","","1791","            avg_score_lr = cross_val_score(lr, X, y, cv=cv).mean()","1792","            assert avg_scores_lrcv[i, j] == pytest.approx(avg_score_lr)"],"delete":[]}],"sklearn\/linear_model\/logistic.py":[{"add":["2203","            # with n_cs=2 and n_l1_ratios=3","2204","            # the layout of scores is","2205","            # [c1, c2, c1, c2, c1, c2]","2206","            #   l1_1 ,  l1_2 ,  l1_3","2207","            # To get a 2d array with the following layout","2208","            #      l1_1, l1_2, l1_3","2209","            # c1 [[ .  ,  .  ,  .  ],","2210","            # c2  [ .  ,  .  ,  .  ]]","2211","            # We need to first reshape and then transpose.","2212","            # The same goes for the other arrays","2215","                    (len(folds), self.l1_ratios_.size, self.Cs_.size, -1))","2216","                self.coefs_paths_[cls] = np.transpose(self.coefs_paths_[cls],","2217","                                                      (0, 2, 1, 3))","2220","                    (len(folds), self.l1_ratios_.size, self.Cs_.size))","2221","                self.scores_[cls] = np.transpose(self.scores_[cls], (0, 2, 1))","2222","","2224","                (-1, len(folds), self.l1_ratios_.size, self.Cs_.size))","2225","            self.n_iter_ = np.transpose(self.n_iter_, (0, 1, 3, 2))"],"delete":["2205","                    (len(folds), self.Cs_.size, self.l1_ratios_.size, -1))","2208","                    (len(folds), self.Cs_.size, self.l1_ratios_.size))","2210","                (-1, len(folds), self.Cs_.size, self.l1_ratios_.size))"]}],"doc\/whats_new\/v0.22.rst":[{"add":["345","- |FIX| Fixed a bug in :class:`linear_model.LogisticRegressionCV` where the","346","  ``scores_``, ``n_iter_`` and ``coefs_paths_`` attribute would have a wrong","347","  ordering with ``penalty='elastic-net'``. :pr:`15044` by `Nicolas Hug`_","348",""],"delete":[]}]}},"c0c53137cec61a4d6cd72d8a43bbe0321476e440":{"changes":{"sklearn\/ensemble\/weight_boosting.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY","sklearn\/ensemble\/tests\/test_weight_boosting.py":"MODIFY"},"diff":{"sklearn\/ensemble\/weight_boosting.py":[{"add":["36","from ..utils.extmath import softmax","751","    @staticmethod","752","    def _compute_proba_from_decision(decision, n_classes):","753","        \"\"\"Compute probabilities from the decision function.","754","","755","        This is based eq. (4) of [1] where:","756","            p(y=c|X) = exp((1 \/ K-1) f_c(X)) \/ sum_k(exp((1 \/ K-1) f_k(X)))","757","                     = softmax((1 \/ K-1) * f(X))","758","","759","        References","760","        ----------","761","        .. [1] J. Zhu, H. Zou, S. Rosset, T. Hastie, \"Multi-class AdaBoost\",","762","               2009.","763","        \"\"\"","764","        if n_classes == 2:","765","            decision = np.vstack([-decision, decision]).T \/ 2","766","        else:","767","            decision \/= (n_classes - 1)","768","        return softmax(decision, copy=False)","769","","797","        decision = self.decision_function(X)","798","        return self._compute_proba_from_decision(decision, n_classes)","828","        for decision in self.staged_decision_function(X):","829","            yield self._compute_proba_from_decision(decision, n_classes)"],"delete":["777","        if self.algorithm == 'SAMME.R':","778","            # The weights are all 1. for SAMME.R","779","            proba = sum(_samme_proba(estimator, n_classes, X)","780","                        for estimator in self.estimators_)","781","        else:  # self.algorithm == \"SAMME\"","782","            proba = sum(estimator.predict_proba(X) * w","783","                        for estimator, w in zip(self.estimators_,","784","                                                self.estimator_weights_))","785","","786","        proba \/= self.estimator_weights_.sum()","787","        proba = np.exp((1. \/ (n_classes - 1)) * proba)","788","        normalizer = proba.sum(axis=1)[:, np.newaxis]","789","        normalizer[normalizer == 0.0] = 1.0","790","        proba \/= normalizer","791","","792","        return proba","821","        proba = None","822","        norm = 0.","824","        for weight, estimator in zip(self.estimator_weights_,","825","                                     self.estimators_):","826","            norm += weight","827","","828","            if self.algorithm == 'SAMME.R':","829","                # The weights are all 1. for SAMME.R","830","                current_proba = _samme_proba(estimator, n_classes, X)","831","            else:  # elif self.algorithm == \"SAMME\":","832","                current_proba = estimator.predict_proba(X) * weight","833","","834","            if proba is None:","835","                proba = current_proba","836","            else:","837","                proba += current_proba","838","","839","            real_proba = np.exp((1. \/ (n_classes - 1)) * (proba \/ norm))","840","            normalizer = real_proba.sum(axis=1)[:, np.newaxis]","841","            normalizer[normalizer == 0.0] = 1.0","842","            real_proba \/= normalizer","843","","844","            yield real_proba"]}],"doc\/whats_new\/v0.22.rst":[{"add":["103","- |Fix| :class:`ensemble.AdaBoostClassifier` computes probabilities based on","104","  the decision function as in the literature. Thus, `predict` and","105","  `predict_proba` give consistent results.","106","  :pr:`14114` by :user:`Guillaume Lemaitre <glemaitre>`.","107",""],"delete":[]}],"sklearn\/ensemble\/tests\/test_weight_boosting.py":[{"add":["3","import pytest","86","@pytest.mark.parametrize(\"algorithm\", [\"SAMME\", \"SAMME.R\"])","87","def test_classification_toy(algorithm):","89","    clf = AdaBoostClassifier(algorithm=algorithm, random_state=0)","90","    clf.fit(X, y_class)","91","    assert_array_equal(clf.predict(T), y_t_class)","92","    assert_array_equal(np.unique(np.asarray(y_t_class)), clf.classes_)","93","    assert clf.predict_proba(T).shape == (len(T), 2)","94","    assert clf.decision_function(T).shape == (len(T),)","153","@pytest.mark.parametrize(\"algorithm\", [\"SAMME\", \"SAMME.R\"])","154","def test_staged_predict(algorithm):","160","    clf = AdaBoostClassifier(algorithm=algorithm, n_estimators=10)","161","    clf.fit(iris.data, iris.target, sample_weight=iris_weights)","163","    predictions = clf.predict(iris.data)","164","    staged_predictions = [p for p in clf.staged_predict(iris.data)]","165","    proba = clf.predict_proba(iris.data)","166","    staged_probas = [p for p in clf.staged_predict_proba(iris.data)]","167","    score = clf.score(iris.data, iris.target, sample_weight=iris_weights)","168","    staged_scores = [","169","        s for s in clf.staged_score(","170","            iris.data, iris.target, sample_weight=iris_weights)]","172","    assert len(staged_predictions) == 10","173","    assert_array_almost_equal(predictions, staged_predictions[-1])","174","    assert len(staged_probas) == 10","175","    assert_array_almost_equal(proba, staged_probas[-1])","176","    assert len(staged_scores) == 10","177","    assert_array_almost_equal(score, staged_scores[-1])","505","","506","","507","@pytest.mark.parametrize(\"algorithm\", [\"SAMME\", \"SAMME.R\"])","508","def test_adaboost_consistent_predict(algorithm):","509","    # check that predict_proba and predict give consistent results","510","    # regression test for:","511","    # https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/14084","512","    X_train, X_test, y_train, y_test = train_test_split(","513","        *datasets.load_digits(return_X_y=True), random_state=42","514","    )","515","    model = AdaBoostClassifier(algorithm=algorithm, random_state=42)","516","    model.fit(X_train, y_train)","517","","518","    assert_array_equal(","519","        np.argmax(model.predict_proba(X_test), axis=1),","520","        model.predict(X_test)","521","    )"],"delete":["85","def test_classification_toy():","87","    for alg in ['SAMME', 'SAMME.R']:","88","        clf = AdaBoostClassifier(algorithm=alg, random_state=0)","89","        clf.fit(X, y_class)","90","        assert_array_equal(clf.predict(T), y_t_class)","91","        assert_array_equal(np.unique(np.asarray(y_t_class)), clf.classes_)","92","        assert clf.predict_proba(T).shape == (len(T), 2)","93","        assert clf.decision_function(T).shape == (len(T),)","152","def test_staged_predict():","158","    # AdaBoost classification","159","    for alg in ['SAMME', 'SAMME.R']:","160","        clf = AdaBoostClassifier(algorithm=alg, n_estimators=10)","161","        clf.fit(iris.data, iris.target, sample_weight=iris_weights)","163","        predictions = clf.predict(iris.data)","164","        staged_predictions = [p for p in clf.staged_predict(iris.data)]","165","        proba = clf.predict_proba(iris.data)","166","        staged_probas = [p for p in clf.staged_predict_proba(iris.data)]","167","        score = clf.score(iris.data, iris.target, sample_weight=iris_weights)","168","        staged_scores = [","169","            s for s in clf.staged_score(","170","                iris.data, iris.target, sample_weight=iris_weights)]","172","        assert len(staged_predictions) == 10","173","        assert_array_almost_equal(predictions, staged_predictions[-1])","174","        assert len(staged_probas) == 10","175","        assert_array_almost_equal(proba, staged_probas[-1])","176","        assert len(staged_scores) == 10","177","        assert_array_almost_equal(score, staged_scores[-1])"]}]}},"9115ab0ed3fcee50d750585b670003d3957a4897":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/compose\/tests\/test_column_transformer.py":"MODIFY","sklearn\/compose\/_column_transformer.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["76",":mod:`sklearn.compose`","77",".....................","78","","79","- |Fix| Fixed an issue in :class:`compose.ColumnTransformer` where using","80","  DataFrames whose column order differs between :func:``fit`` and","81","  :func:``transform`` could lead to silently passing incorrect columns to the","82","  ``remainder`` transformer.","83","  :pr:`14237` by `Andreas Schuderer <schuderer>`.","84",""],"delete":[]}],"sklearn\/compose\/tests\/test_column_transformer.py":[{"add":["494","    # transformed n_features does not match fitted n_features","495","    col = [0, 1]","496","    ct = ColumnTransformer([('trans', Trans(), col)], remainder=remainder)","497","    ct.fit(X_array)","498","    X_array_more = np.array([[0, 1, 2], [2, 4, 6], [3, 6, 9]]).T","499","    ct.transform(X_array_more)  # Should accept added columns","500","    X_array_fewer = np.array([[0, 1, 2], ]).T","501","    err_msg = 'Number of features'","502","    with pytest.raises(ValueError, match=err_msg):","503","        ct.transform(X_array_fewer)","504","","1073","","1074","","1075","@pytest.mark.parametrize(\"explicit_colname\", ['first', 'second'])","1076","def test_column_transformer_reordered_column_names_remainder(explicit_colname):","1077","    \"\"\"Regression test for issue #14223: 'Named col indexing fails with","1078","       ColumnTransformer remainder on changing DataFrame column ordering'","1079","","1080","       Should raise error on changed order combined with remainder.","1081","       Should allow for added columns in `transform` input DataFrame","1082","       as long as all preceding columns match.","1083","    \"\"\"","1084","    pd = pytest.importorskip('pandas')","1085","","1086","    X_fit_array = np.array([[0, 1, 2], [2, 4, 6]]).T","1087","    X_fit_df = pd.DataFrame(X_fit_array, columns=['first', 'second'])","1088","","1089","    X_trans_array = np.array([[2, 4, 6], [0, 1, 2]]).T","1090","    X_trans_df = pd.DataFrame(X_trans_array, columns=['second', 'first'])","1091","","1092","    tf = ColumnTransformer([('bycol', Trans(), explicit_colname)],","1093","                           remainder=Trans())","1094","","1095","    tf.fit(X_fit_df)","1096","    err_msg = 'Column ordering must be equal'","1097","    with pytest.raises(ValueError, match=err_msg):","1098","        tf.transform(X_trans_df)","1099","","1100","    # No error for added columns if ordering is identical","1101","    X_extended_df = X_fit_df.copy()","1102","    X_extended_df['third'] = [3, 6, 9]","1103","    tf.transform(X_extended_df)  # No error should be raised","1104","","1105","    # No 'columns' AttributeError when transform input is a numpy array","1106","    X_array = X_fit_array.copy()","1107","    err_msg = 'Specifying the columns'","1108","    with pytest.raises(ValueError, match=err_msg):","1109","        tf.transform(X_array)"],"delete":[]}],"sklearn\/compose\/_column_transformer.py":[{"add":["21","from ..utils import _check_key_type","83","        Note that using this feature requires that the DataFrame columns","84","        input at :term:`fit` and :term:`transform` have identical order.","308","        # Make it possible to check for reordered named columns on transform","309","        if (hasattr(X, 'columns') and","310","                any(_check_key_type(cols, str) for cols in self._columns)):","311","            self._df_columns = X.columns","312","","313","        self._n_features = X.shape[1]","317","        remaining_idx = list(set(range(self._n_features)) - set(cols))","318","        remaining_idx = sorted(remaining_idx) or None","520","","521","        if self._n_features > X.shape[1]:","522","            raise ValueError('Number of features of the input must be equal '","523","                             'to or greater than that of the fitted '","524","                             'transformer. Transformer n_features is {0} '","525","                             'and input n_features is {1}.'","526","                             .format(self._n_features, X.shape[1]))","527","","528","        # No column reordering allowed for named cols combined with remainder","529","        if (self._remainder[2] is not None and","530","                hasattr(self, '_df_columns') and","531","                hasattr(X, 'columns')):","532","            n_cols_fit = len(self._df_columns)","533","            n_cols_transform = len(X.columns)","534","            if (n_cols_transform >= n_cols_fit and","535","                    any(X.columns[:n_cols_fit] != self._df_columns)):","536","                raise ValueError('Column ordering must be equal for fit '","537","                                 'and for transform when using the '","538","                                 'remainder keyword')","539",""],"delete":["305","        n_columns = X.shape[1]","309","        remaining_idx = sorted(list(set(range(n_columns)) - set(cols))) or None","510",""]}]}},"8632775c23a306661cf043bebd9ea4852edecada":{"changes":{"sklearn\/svm\/src\/liblinear\/liblinear_helper.c":"MODIFY"},"diff":{"sklearn\/svm\/src\/liblinear\/liblinear_helper.c":[{"add":["89","            for (j=0; j<i; ++j)","90","                free(sparse[j]);","91","            free(sparse);","92","            return NULL;"],"delete":["89","            int l;","90","            for (l=0; l<i; l++)","91","                free(sparse[l]);","92","            break;"]}]}},"9f7d3f92721cca58613a7200f9a1a6465237a326":{"changes":{"sklearn\/impute\/tests\/test_impute.py":"MODIFY","sklearn\/impute\/_iterative.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY"},"diff":{"sklearn\/impute\/tests\/test_impute.py":[{"add":["459","@pytest.mark.parametrize(\"X\", [[[1], [2]], [[1], [np.nan]]])","460","def test_iterative_imputer_one_feature(X):","461","    # check we exit early when there is a single feature","462","    imputer = IterativeImputer().fit(X)","463","    assert imputer.n_iter_ == 0","464","    imputer = IterativeImputer()","465","    imputer.fit([[1], [2]])","466","    assert imputer.n_iter_ == 0","467","    imputer.fit([[1], [np.nan]])","468","    assert imputer.n_iter_ == 0","469","","470","","601","                               skip_complete=True,","967","    \"skip_complete\", [True, False]","968",")","969","def test_iterative_imputer_skip_non_missing(skip_complete):","970","    # check the imputing strategy when missing data are present in the","971","    # testing set only.","972","    # taken from: https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/14383","973","    rng = np.random.RandomState(0)","974","    X_train = np.array([","975","        [5, 2, 2, 1],","976","        [10, 1, 2, 7],","977","        [3, 1, 1, 1],","978","        [8, 4, 2, 2]","979","    ])","980","    X_test = np.array([","981","        [np.nan, 2, 4, 5],","982","        [np.nan, 4, 1, 2],","983","        [np.nan, 1, 10, 1]","984","    ])","985","    imputer = IterativeImputer(","986","        initial_strategy='mean', skip_complete=skip_complete, random_state=rng","987","    )","988","    X_test_est = imputer.fit(X_train).transform(X_test)","989","    if skip_complete:","990","        # impute with the initial strategy: 'mean'","991","        assert_allclose(X_test_est[:, 0], np.mean(X_train[:, 0]))","992","    else:","993","        assert_allclose(X_test_est[:, 0], [11, 7, 12], rtol=1e-4)","994","","995","","996","@pytest.mark.parametrize("],"delete":[]}],"sklearn\/impute\/_iterative.py":[{"add":["103","    skip_complete : boolean, optional (default=False)","104","        If ``True`` then features with missing values during ``transform``","105","        which did not have any missing values during ``fit`` will be imputed","106","        with the initial imputation method only. Set to ``True`` if you have","107","        many features with no missing values at both ``fit`` and ``transform``","108","        time to save compute.","109","","162","    random_state_ : RandomState instance","163","        RandomState instance that is generated either from a seed, the random","164","        number generator or by `np.random`.","165","","201","                 skip_complete=False,","216","        self.skip_complete = skip_complete","276","        missing_row_mask = mask_missing_values[:, feat_idx]","284","        # if no missing values, don't predict","285","        if np.sum(missing_row_mask) == 0:","286","            return X_filled, estimator","287","","288","        # get posterior samples if there is at least one missing value","294","            # two types of problems: (1) non-positive sigmas","295","            # (2) mus outside legal range of min_value and max_value","296","            # (results in inf sample)","394","        if self.skip_complete:","395","            missing_values_idx = np.flatnonzero(frac_of_missing_values)","396","        else:","397","            missing_values_idx = np.arange(np.shape(frac_of_missing_values)[0])","563","        # Edge case: a single feature. We return the initial ...","564","        if Xt.shape[1] == 1:","565","            self.n_iter_ = 0","566","            return Xt","567",""],"delete":["168","    Features with missing values during ``transform`` which did not have any","169","    missing values during ``fit`` will be imputed with the initial imputation","170","    method only.","171","","260","","261","        # if nothing is missing, just return the default","262","        # (should not happen at fit time because feat_ids would be excluded)","263","        missing_row_mask = mask_missing_values[:, feat_idx]","264","        if not np.any(missing_row_mask):","265","            return X_filled, estimator","266","","281","        # get posterior samples","287","            # two types of problems: (1) non-positive sigmas, (2) mus outside","288","            # legal range of min_value and max_value (results in inf sample)","386","        missing_values_idx = np.nonzero(frac_of_missing_values)[0]","548",""]}],"doc\/whats_new\/v0.22.rst":[{"add":["32","- :class:`impute.IterativeImputer` when `X` has features with no missing","33","  values. |Feature|","211","- |Fix| :class:`impute.IterativeImputer` now works when there is only one feature.","212","  By :user:`Sergey Feldman <sergeyf>`.","213","","214","- |Feature| :class:`impute.IterativeImputer` has new `skip_compute` flag that","215","  is False by default, which, when True, will skip computation on features that","216","  have no missing values during the fit phase. :issue:`13773` by","217","  :user:`Sergey Feldman <sergeyf>`.","218",""],"delete":[]}]}},"38f9195c7b5afcbc34afef03beeb57e53e1d99df":{"changes":{".gitignore":"MODIFY"},"diff":{".gitignore":[{"add":["61","!\/**\/src\/**\/*.c","62","!\/**\/src\/**\/*.cpp"],"delete":["61","!*\/src\/*.c","62","!*\/src\/*.cpp"]}]}},"9e08b142e0ad60b6bbc1c4a1a3f6e583faf25f3e":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","doc\/whats_new\/changelog_legend.inc":"ADD","doc\/whats_new.rst":"MODIFY","doc\/templates\/index.html":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["9",".. include:: changelog_legend.inc","10",""],"delete":[]}],"doc\/whats_new\/changelog_legend.inc":[{"add":[],"delete":[]}],"doc\/whats_new.rst":[{"add":["6","Release notes for all scikit-learn releases are linked in this this page.","14","    Version 0.22 <whats_new\/v0.22.rst>","15","    Version 0.21 <whats_new\/v0.21.rst>"],"delete":["1",".. include:: includes\/big_toc_css.rst","7","Release notes for current and recent releases are detailed on this page, with","8",":ref:`previous releases <previous_releases_whats_new>` linked below.","13","Legend for changelogs","14","---------------------","15","","16","- |MajorFeature|: something big that you couldn't do before.","17","- |Feature|: something that you couldn't do before.","18","- |Efficiency|: an existing feature now may not require as much computation or","19","  memory.","20","- |Enhancement|: a miscellaneous minor improvement.","21","- |Fix|: something that previously didn't work as documentated -- or according","22","  to reasonable expectations -- should now work.","23","- |API|: you will need to change your code to have the same effect in the","24","  future; or a feature will be removed in the future.","25","","26",".. include:: whats_new\/v0.22.rst","27",".. include:: whats_new\/v0.21.rst","28","","29",".. _previous_releases_whats_new:","30","","31","Previous Releases","32","================="]}],"doc\/templates\/index.html":[{"add":["160","        <li><strong>July 2019.<\/strong> scikit-learn 0.21.3 (<a href=\"whats_new\/v0.21.html#version-0-21-3\">Changelog<\/a>) and 0.20.4 (<a href=\"whats_new\/v0.20.html#version-0-20-4\">Changelog<\/a>) are available for download.","162","        <li><strong>May 2019.<\/strong> scikit-learn 0.21.0 to 0.21.2 are available for download (<a href=\"whats_new\/v0.21.html#version-0-21-2\">Changelog<\/a>).","164","        <li><strong>March 2019.<\/strong> scikit-learn 0.20.3 is available for download (<a href=\"whats_new\/v0.20.html#version-0-20-3\">Changelog<\/a>).","166","        <li><strong>September 2018.<\/strong> scikit-learn 0.20.0 is available for download (<a href=\"whats_new\/v0.20.html#version-0-20-0\">Changelog<\/a>).","168","        <li><strong>July 2018.<\/strong> scikit-learn 0.19.2 is available for download (<a href=\"whats_new\/v0.19.html#version-0-19-2\">Changelog<\/a>)."],"delete":["160","        <li><strong>July 2019.<\/strong> scikit-learn 0.21.3 (<a href=\"whats_new.html#version-0-21-3\">Changelog<\/a>) and 0.20.4 (<a href=\"whats_new.html#version-0-20-4\">Changelog<\/a>) are available for download.","162","        <li><strong>May 2019.<\/strong> scikit-learn 0.21.0 to 0.21.2 are available for download (<a href=\"whats_new.html#version-0-21\">Changelog<\/a>).","164","        <li><strong>March 2019.<\/strong> scikit-learn 0.20.3 is available for download (<a href=\"whats_new.html#version-0-20-3\">Changelog<\/a>).","166","        <li><strong>September 2018.<\/strong> scikit-learn 0.20.0 is available for download (<a href=\"whats_new.html#version-0-20-0\">Changelog<\/a>).","168","        <li><strong>July 2018.<\/strong> scikit-learn 0.19.2 is available for download (<a href=\"whats_new.html#version-0-19\">Changelog<\/a>)."]}],"doc\/whats_new\/v0.22.rst":[{"add":["15",".. include:: changelog_legend.inc"],"delete":[]}]}},"16f4ac90f0732988e3b7efe0c937eaff70e99692":{"changes":{"sklearn\/tests\/test_pipeline.py":"MODIFY","sklearn\/pipeline.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY"},"diff":{"sklearn\/tests\/test_pipeline.py":[{"add":["900","# TODO: Remove parametrization in 0.24 when None is removed for FeatureUnion","914","    with pytest.warns(None) as record:","915","        ft.set_params(m2=drop)","916","        assert_array_equal([[3]], ft.fit(X).transform(X))","917","        assert_array_equal([[3]], ft.fit_transform(X))","919","    assert record if drop is None else not record","921","    with pytest.warns(None) as record:","922","        ft.set_params(m3=drop)","923","        assert_array_equal([[]], ft.fit(X).transform(X))","924","        assert_array_equal([[]], ft.fit_transform(X))","926","    assert record if drop is None else not record","928","    with pytest.warns(None) as record:","929","        # check we can change back","930","        ft.set_params(m3=mult3)","931","        assert_array_equal([[3]], ft.fit(X).transform(X))","932","    assert record if drop is None else not record","934","    with pytest.warns(None) as record:","935","        # Check 'drop' step at construction time","936","        ft = FeatureUnion([('m2', drop), ('m3', mult3)])","937","        assert_array_equal([[3]], ft.fit(X).transform(X))","938","        assert_array_equal([[3]], ft.fit_transform(X))","940","    assert record if drop is None else not record","1138","     (FeatureUnion([('mult1', 'drop'), ('mult2', Mult()), ('mult3', 'drop')]),","1184","","1185","","1186","# TODO: Remove in 0.24 when None is removed","1187","def test_feature_union_warns_with_none():","1188","    msg = (r\"Using None as a transformer is deprecated in version 0\\.22 and \"","1189","           r\"will be removed in version 0\\.24\\. Please use 'drop' instead\\.\")","1190","    with pytest.warns(DeprecationWarning, match=msg):","1191","        union = FeatureUnion([('multi1', None), ('multi2', Mult())])","1192","","1193","    X = [[1, 2, 3], [4, 5, 6]]","1194","","1195","    with pytest.warns(DeprecationWarning, match=msg):","1196","        union.fit_transform(X)"],"delete":["913","    ft.set_params(m2=drop)","914","    assert_array_equal([[3]], ft.fit(X).transform(X))","915","    assert_array_equal([[3]], ft.fit_transform(X))","918","    ft.set_params(m3=drop)","919","    assert_array_equal([[]], ft.fit(X).transform(X))","920","    assert_array_equal([[]], ft.fit_transform(X))","923","    # check we can change back","924","    ft.set_params(m3=mult3)","925","    assert_array_equal([[3]], ft.fit(X).transform(X))","927","    # Check 'drop' step at construction time","928","    ft = FeatureUnion([('m2', drop), ('m3', mult3)])","929","    assert_array_equal([[3]], ft.fit(X).transform(X))","930","    assert_array_equal([[3]], ft.fit_transform(X))","1129","     (FeatureUnion([('mult1', None), ('mult2', Mult()), ('mult3', None)]),"]}],"sklearn\/pipeline.py":[{"add":["13","import warnings","757","    or removed by setting to 'drop'.","767","        .. versionchanged:: 0.22","768","           Deprecated `None` as a transformer in favor of 'drop'.","769","","846","            # TODO: Remove in 0.24 when None is removed","847","            if t is None:","848","                warnings.warn(\"Using None as a transformer is deprecated \"","849","                              \"in version 0.22 and will be removed in \"","850","                              \"version 0.24. Please use 'drop' instead.\",","851","                              DeprecationWarning)","852","                continue","853","            if t == 'drop':"],"delete":["756","    or removed by setting to 'drop' or ``None``.","842","            if t is None or t == 'drop':"]}],"doc\/whats_new\/v0.22.rst":[{"add":["530","- |API| `None` as a transformer is now deprecated in","531","  :class:`pipeline.FeatureUnion`. Please use `'drop'` instead. :pr:`15053` by","532","  `Thomas Fan`_.","533",""],"delete":[]}]}},"77a9d32dd813f4ecb100cc6c9ac40a0ac5d8d365":{"changes":{"doc\/themes\/scikit-learn-modern\/static\/css\/theme.css":"MODIFY"},"diff":{"doc\/themes\/scikit-learn-modern\/static\/css\/theme.css":[{"add":["25","p {","26","  word-break: break-word;","27","  hyphens: auto;","28","}","29","","666","  padding-left: 0.25rem;","897","table.docutils p {","900","  word-break: initial;"],"delete":["661","  padding: 4px 0 0 4px;","892","table.docutils td:last-child p {"]}]}},"de3040439546362a28c9861ac4b568bc5ffd68e4":{"changes":{"sklearn\/impute\/tests\/test_impute.py":"MODIFY","sklearn\/impute\/_iterative.py":"MODIFY"},"diff":{"sklearn\/impute\/tests\/test_impute.py":[{"add":["18","from sklearn.datasets import load_boston","927","def test_iterative_imputer_catch_warning():","928","    # check that we catch a RuntimeWarning due to a division by zero when a","929","    # feature is constant in the dataset","930","    X, y = load_boston(return_X_y=True)","931","    n_samples, n_features = X.shape","932","","933","    # simulate that a feature only contain one category during fit","934","    X[:, 3] = 1","935","","936","    # add some missing values","937","    rng = np.random.RandomState(0)","938","    missing_rate = 0.15","939","    for feat in range(n_features):","940","        sample_idx = rng.choice(","941","            np.arange(n_samples), size=int(n_samples * missing_rate),","942","            replace=False","943","        )","944","        X[sample_idx, feat] = np.nan","945","","946","    imputer = IterativeImputer(n_nearest_features=5, sample_posterior=True)","947","    with pytest.warns(None) as record:","948","        X_fill = imputer.fit_transform(X, y)","949","    assert not record.list","950","    assert not np.any(np.isnan(X_fill))","951","","952",""],"delete":[]}],"sklearn\/impute\/_iterative.py":[{"add":["432","        with np.errstate(invalid='ignore'):","433","            # if a feature in the neighboorhood has only a single value","434","            # (e.g., categorical feature), the std. dev. will be null and","435","            # np.corrcoef will raise a warning due to a division by zero","436","            abs_corr_mat = np.abs(np.corrcoef(X_filled.T))"],"delete":["432","        abs_corr_mat = np.abs(np.corrcoef(X_filled.T))"]}]}},"e424ab17bb73472a829faca3dfdc599a9d6df56b":{"changes":{"doc\/whats_new\/v0.22.rst":"MODIFY","sklearn\/cluster\/_k_means_elkan.pyx":"MODIFY"},"diff":{"doc\/whats_new\/v0.22.rst":[{"add":["87","- |Fix| Fixed a bug where `elkan` algorithm in :class:`cluster.KMeans` was","88","  producing Segmentation Fault on large arrays due to integer index overflow.","89","  :pr:`15057` by :user:`Vladimir Korolev <balodja>`.","90",""],"delete":[]}],"sklearn\/cluster\/_k_means_elkan.pyx":[{"add":["32","        Py_ssize_t n_samples, int n_features, int n_clusters):","71","    n_samples : Py_ssize_t","85","    cdef int c_x, j","86","    cdef Py_ssize_t sample","147","    cdef Py_ssize_t point_index","148","    cdef int center_index, label"],"delete":["32","        int n_samples, int n_features, int n_clusters):","71","    n_samples : int","85","    cdef int c_x, j, sample","146","    cdef int point_index, center_index, label"]}]}},"56edce84caf6f3c8ef96376e1b684c45311130f3":{"changes":{"\/dev\/null":"DELETE","sklearn\/_build_utils\/deprecated_modules.py":"MODIFY",".gitignore":"MODIFY","sklearn\/cluster\/_birch.py":"MODIFY","sklearn\/utils\/deprecation.py":"MODIFY","conftest.py":"MODIFY","sklearn\/cluster\/tests\/test_hierarchical.py":"MODIFY"},"diff":{"\/dev\/null":[{"add":[],"delete":[]}],"sklearn\/_build_utils\/deprecated_modules.py":[{"add":["22","    ('_rbm', 'sklearn.neural_network.rbm', 'sklearn.neural_network'),","23","    ('_multilayer_perceptron',","24","     'sklearn.neural_network.multilayer_perceptron', 'sklearn.neural_network'),","25",""],"delete":[]}],".gitignore":[{"add":["95","sklearn\/neural_network\/rbm.py","96","sklearn\/neural_network\/multilayer_perceptron.py","97",""],"delete":[]}],"sklearn\/cluster\/_birch.py":[{"add":["16","from . import AgglomerativeClustering"],"delete":["16","from ._hierarchical import AgglomerativeClustering"]}],"sklearn\/utils\/deprecation.py":[{"add":["142","    warnings.warn(message, DeprecationWarning)"],"delete":["130","    # We don't want to raise a dep warning if we are in a pytest session else","131","    # the CIs with -Werror::DeprecationWarning would fail. The deprecations are","132","    # still properly tested in sklearn\/tests\/test_import_deprecations.py","145","    if not getattr(sys, '_is_pytest_session', False):","146","        warnings.warn(message, DeprecationWarning)"]}],"conftest.py":[{"add":["9","import os","17","from sklearn._build_utils.deprecated_modules import _DEPRECATED_MODULES","100","","101","","102","# TODO: Remove when modules are deprecated in 0.24","103","# Configures pytest to ignore deprecated modules.","104","collect_ignore_glob = [","105","    os.path.join(*deprecated_path.split(\".\")) + \".py\"","106","    for _, deprecated_path, _ in _DEPRECATED_MODULES]"],"delete":[]}],"sklearn\/cluster\/tests\/test_hierarchical.py":[{"add":["25","                                           linkage_tree, _fix_connectivity)"],"delete":["25","                                           _fix_connectivity)","26","from sklearn.cluster.hierarchical import linkage_tree"]}]}},"c52b6e128708fabbdfb25ffe6d44893ad553c1fb":{"changes":{"sklearn\/manifold\/tests\/test_spectral_embedding.py":"MODIFY","sklearn\/manifold\/spectral_embedding_.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY"},"diff":{"sklearn\/manifold\/tests\/test_spectral_embedding.py":[{"add":["177","    assert _check_with_col_sign_flipping(embed_amg, embed_arpack, 0.1e-4)","178","","179","    # same with special case in which amg is not actually used","180","    # regression test for #10715","181","    # affinity between nodes","182","    row = [0, 0, 1, 2, 3, 3, 4]","183","    col = [1, 2, 2, 3, 4, 5, 5]","184","    val = [100, 100, 100, 1, 100, 100, 100]","185","","186","    affinity = sparse.coo_matrix((val + val, (row + col, col + row)),","187","                                 shape=(6, 6)).toarray()","188","    se_amg.affinity = \"precomputed\"","189","    se_arpack.affinity = \"precomputed\"","190","    embed_amg = se_amg.fit_transform(affinity)","191","    embed_arpack = se_arpack.fit_transform(affinity)","192","    assert _check_with_col_sign_flipping(embed_amg, embed_arpack, 0.1e-4)"],"delete":["177","    assert _check_with_col_sign_flipping(embed_amg, embed_arpack, 0.05)"]}],"sklearn\/manifold\/spectral_embedding_.py":[{"add":["269","            _, diffusion_map = eigsh(","270","                laplacian, k=n_components, sigma=1.0, which='LM',","271","                tol=eigen_tol, v0=v0)","282","    elif eigen_solver == 'amg':","295","        _, diffusion_map = lobpcg(laplacian, X, M=M, tol=1.e-12,","296","                                  largest=False)","303","    if eigen_solver == \"lobpcg\":","313","            _, diffusion_map = eigh(laplacian)","323","            _, diffusion_map = lobpcg(laplacian, X, tol=1e-15,","324","                                      largest=False, maxiter=2000)"],"delete":["269","            lambdas, diffusion_map = eigsh(laplacian, k=n_components,","270","                                           sigma=1.0, which='LM',","271","                                           tol=eigen_tol, v0=v0)","282","    if eigen_solver == 'amg':","295","        lambdas, diffusion_map = lobpcg(laplacian, X, M=M, tol=1.e-12,","296","                                        largest=False)","303","    elif eigen_solver == \"lobpcg\":","313","            lambdas, diffusion_map = eigh(laplacian)","323","            lambdas, diffusion_map = lobpcg(laplacian, X, tol=1e-15,","324","                                            largest=False, maxiter=2000)"]}],"doc\/whats_new\/v0.22.rst":[{"add":["233","","234",":mod:`sklearn.manifold`","235",".......................","236","- |Fix| Fixed a bug where :func:`manifold.spectral_embedding` (and therefore","237","  :class:`manifold.SpectralEmedding` and `clustering.SpectralClustering`)","238","  computed wrong eigenvalues with ``solver='amg'`` when","239","  ``n_samples < 5 * n_components``. :pr:`14647` by `Andreas Mller`_.","240","  "],"delete":["268",":mod:`sklearn.metrics`","269","......................","270",""]}]}},"e6a4dc97c9a62420ca00766366352da665566584":{"changes":{"sklearn\/utils\/validation.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY","sklearn\/utils\/tests\/test_validation.py":"MODIFY"},"diff":{"sklearn\/utils\/validation.py":[{"add":["34","def _assert_all_finite(X, allow_nan=False, msg_dtype=None):","54","            raise ValueError(","55","                    msg_err.format","56","                    (type_err,","57","                     msg_dtype if msg_dtype is not None else X.dtype)","58","            )","500","                if dtype is not None and np.dtype(dtype).kind in 'iu':","501","                    # Conversion float -> int should not contain NaN or","502","                    # inf (numpy#14412). We cannot use casting='safe' because","503","                    # then conversion float -> int would be disallowed.","504","                    array = np.asarray(array, order=order)","505","                    if array.dtype.kind == 'f':","506","                        _assert_all_finite(array, allow_nan=False,","507","                                           msg_dtype=dtype)","508","                    array = array.astype(dtype, casting=\"unsafe\", copy=False)","509","                else:","510","                    array = np.asarray(array, order=order, dtype=dtype)"],"delete":["34","def _assert_all_finite(X, allow_nan=False):","54","            raise ValueError(msg_err.format(type_err, X.dtype))","496","                array = np.asarray(array, dtype=dtype, order=order)"]}],"doc\/whats_new\/v0.22.rst":[{"add":["556","- |Fix| :func:`utils.check_array` is now raising an error instead of casting","557","  NaN to integer.","558","  :pr:`14872` by `Roman Yurchak`_.","559",""],"delete":[]}],"sklearn\/utils\/tests\/test_validation.py":[{"add":["204","@pytest.mark.parametrize(","205","    \"X, err_msg\",","206","    [(np.array([[1, np.nan]]),","207","      \"Input contains NaN, infinity or a value too large for.*int\"),","208","     (np.array([[1, np.nan]]),","209","      \"Input contains NaN, infinity or a value too large for.*int\"),","210","     (np.array([[1, np.inf]]),","211","      \"Input contains NaN, infinity or a value too large for.*int\"),","212","     (np.array([[1, np.nan]], dtype=np.object),","213","      \"cannot convert float NaN to integer\")]","214",")","215","@pytest.mark.parametrize(\"force_all_finite\", [True, False])","216","def test_check_array_force_all_finite_object_unsafe_casting(","217","        X, err_msg, force_all_finite):","218","    # casting a float array containing NaN or inf to int dtype should","219","    # raise an error irrespective of the force_all_finite parameter.","220","    with pytest.raises(ValueError, match=err_msg):","221","        check_array(X, dtype=np.int, force_all_finite=force_all_finite)","222","","223",""],"delete":[]}]}},"cc64397d0c1bbf58d44a862d5fafbf6b6cfb619d":{"changes":{"sklearn\/utils\/multiclass.py":"MODIFY","sklearn\/utils\/tests\/test_multiclass.py":"MODIFY"},"diff":{"sklearn\/utils\/multiclass.py":[{"add":["242","    sparse_pandas = (y.__class__.__name__ in ['SparseSeries', 'SparseArray'])","243","    if sparse_pandas:","244","        raise ValueError(\"y cannot be class 'SparseSeries' or 'SparseArray'\")"],"delete":["242","    sparseseries = (y.__class__.__name__ == 'SparseSeries')","243","    if sparseseries:","244","        raise ValueError(\"y cannot be class 'SparseSeries'.\")"]}],"sklearn\/utils\/tests\/test_multiclass.py":[{"add":["4","import pytest","296","def test_type_of_target_pandas_sparse():","297","    pd = pytest.importorskip(\"pandas\")","298","","299","    y = pd.SparseArray([1, np.nan, np.nan, 1, np.nan])","300","    msg = \"y cannot be class 'SparseSeries' or 'SparseArray'\"","301","    with pytest.raises(ValueError, match=msg):","302","        type_of_target(y)"],"delete":["4","","295","    try:","296","        from pandas import SparseSeries","297","    except ImportError:","298","        raise SkipTest(\"Pandas not found\")","300","    y = SparseSeries([1, 0, 0, 1, 0])","301","    msg = \"y cannot be class 'SparseSeries'.\"","302","    assert_raises_regex(ValueError, msg, type_of_target, y)"]}]}}}