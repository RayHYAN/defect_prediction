{"abfb6fd11e97cefd1947078646399cecec5bbe9c":{"changes":{"sklearn\/linear_model\/tests\/test_least_angle.py":"MODIFY","doc\/whats_new\/v0.23.rst":"MODIFY","sklearn\/linear_model\/_least_angle.py":"MODIFY"},"diff":{"sklearn\/linear_model\/tests\/test_least_angle.py":[{"add":["8","from sklearn.base import clone","20","from sklearn.linear_model import Lars, LassoLars","737","@pytest.mark.parametrize('est', (LassoLars(alpha=1e-3), Lars()))","738","def test_lars_with_jitter(est):","739","    # Test that a small amount of jitter helps stability,","740","    # using example provided in issue #2746","741","","742","    X = np.array([[0.0, 0.0, 0.0, -1.0, 0.0],","743","                  [0.0, -1.0, 0.0, 0.0, 0.0]])","744","    y = [-2.5, -2.5]","745","    expected_coef = [0, 2.5, 0, 2.5, 0]","746","","747","    # set to fit_intercept to False since target is constant and we want check","748","    # the value of coef. coef would be all zeros otherwise.","749","    est.set_params(fit_intercept=False)","750","    est_jitter = clone(est).set_params(jitter=10e-8, random_state=0)","751","","752","    est.fit(X, y)","753","    est_jitter.fit(X, y)","754","","755","    assert np.mean((est.coef_ - est_jitter.coef_)**2) > .1","756","    np.testing.assert_allclose(est_jitter.coef_, expected_coef, rtol=1e-3)","757","","758",""],"delete":[]}],"doc\/whats_new\/v0.23.rst":[{"add":["300","- |Enhancement| :class:`linear_model.LassoLars` and","301","  :class:`linear_model.Lars` now support a `jitter` parameter that adds","302","  random noise to the target. This might help with stability in some edge","303","  cases. :pr:`15179` by :user:`angelaambroz`.","304",""],"delete":[]}],"sklearn\/linear_model\/_least_angle.py":[{"add":["23","from ..utils import check_random_state","803","    jitter : float, default=None","804","        Upper bound on a uniform noise parameter to be added to the","805","        `y` values, to satisfy the model's assumption of","806","        one-at-a-time computations. Might help with stability.","807","","808","    random_state : int, RandomState instance or None (default)","809","        Determines random number generation for jittering. Pass an int","810","        for reproducible output across multiple function calls.","811","        See :term:`Glossary <random_state>`. Ignored if `jitter` is None.","812","","859","                 eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,","860","                 jitter=None, random_state=None):","869","        self.jitter = jitter","870","        self.random_state = random_state","970","        if self.jitter is not None:","971","            rng = check_random_state(self.random_state)","972","","973","            noise = rng.uniform(high=self.jitter, size=len(y))","974","            y = y + noise","975","","1053","    jitter : float, default=None","1054","        Upper bound on a uniform noise parameter to be added to the","1055","        `y` values, to satisfy the model's assumption of","1056","        one-at-a-time computations. Might help with stability.","1057","","1058","    random_state : int, RandomState instance or None (default)","1059","        Determines random number generation for jittering. Pass an int","1060","        for reproducible output across multiple function calls.","1061","        See :term:`Glossary <random_state>`. Ignored if `jitter` is None.","1062","","1115","                 positive=False, jitter=None, random_state=None):","1126","        self.jitter = jitter","1127","        self.random_state = random_state"],"delete":["848","                 eps=np.finfo(np.float).eps, copy_X=True, fit_path=True):","1085","                 positive=False):"]}]}},"da6611148fa2b7a73277cde20b7251da6a551bf0":{"changes":{"sklearn\/datasets\/base.py":"MODIFY","sklearn\/datasets\/species_distributions.py":"MODIFY","doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/datasets\/california_housing.py":"MODIFY","sklearn\/datasets\/covtype.py":"MODIFY","sklearn\/datasets\/olivetti_faces.py":"MODIFY","sklearn\/datasets\/kddcup99.py":"MODIFY","sklearn\/datasets\/rcv1.py":"MODIFY","sklearn\/datasets\/tests\/test_base.py":"MODIFY"},"diff":{"sklearn\/datasets\/base.py":[{"add":["12","import warnings","922","","923","","924","def _refresh_cache(files, compress):","925","    # TODO: REMOVE in v0.23","926","    import joblib","927","    msg = \"sklearn.externals.joblib is deprecated in 0.21\"","928","    with warnings.catch_warnings(record=True) as warns:","929","        data = tuple([joblib.load(f) for f in files])","930","","931","    refresh_needed = any([str(x.message).startswith(msg) for x in warns])","932","","933","    other_warns = [w for w in warns if not str(w.message).startswith(msg)]","934","    for w in other_warns:","935","        warnings.warn(message=w.message, category=w.category)","936","","937","    if refresh_needed:","938","        try:","939","            for value, path in zip(data, files):","940","                joblib.dump(value, path, compress=compress)","941","        except IOError:","942","            message = (\"This dataset will stop being loadable in scikit-learn \"","943","                       \"version 0.23 because it references a deprecated \"","944","                       \"import path. Consider removing the following files \"","945","                       \"and allowing it to be cached anew:\\n%s\"","946","                       % (\"\\n\".join(files)))","947","            warnings.warn(message=message, category=DeprecationWarning)","948","","949","    return data[0] if len(data) == 1 else data"],"delete":[]}],"sklearn\/datasets\/species_distributions.py":[{"add":["53","from .base import _refresh_cache","262","        bunch = _refresh_cache([archive_path], 9)","263","        # TODO: Revert to the following line in v0.23","264","        # bunch = joblib.load(archive_path)"],"delete":["261","        bunch = joblib.load(archive_path)"]}],"doc\/whats_new\/v0.21.rst":[{"add":["14",":mod:`sklearn.datasets`","15",".......................","16","","17","- |Fix| :func:`datasets.fetch_california_housing`,","18","  :func:`datasets.fetch_covtype`,","19","  :func:`datasets.fetch_kddcup99`, :func:`datasets.fetch_olivetti_faces`,","20","  :func:`datasets.fetch_rcv1`, and :func:`datasets.fetch_species_distributions`","21","  try to persist the previously cache using the new ``joblib`` if the cahced","22","  data was persisted using the deprecated ``sklearn.externals.joblib``. This","23","  behavior is set to be deprecated and removed in v0.23.","24","  :pr:`14197` by `Adrin Jalali`_.","25",""],"delete":[]}],"sklearn\/datasets\/california_housing.py":[{"add":["36","from .base import _refresh_cache","132","        cal_housing = _refresh_cache([filepath], 6)","133","        # TODO: Revert to the following line in v0.23","134","        # cal_housing = joblib.load(filepath)"],"delete":["131","        cal_housing = joblib.load(filepath)"]}],"sklearn\/datasets\/covtype.py":[{"add":["27","from .base import _refresh_cache","128","        X, y = _refresh_cache([samples_path, targets_path], 9)","129","        # TODO: Revert to the following two lines in v0.23","130","        # X = joblib.load(samples_path)","131","        # y = joblib.load(targets_path)"],"delete":["127","        X = joblib.load(samples_path)","128","        y = joblib.load(targets_path)"]}],"sklearn\/datasets\/olivetti_faces.py":[{"add":["26","from .base import _refresh_cache","110","        faces = _refresh_cache([filepath], 6)","111","        # TODO: Revert to the following line in v0.23","112","        # faces = joblib.load(filepath)"],"delete":["109","        faces = joblib.load(filepath)"]}],"sklearn\/datasets\/kddcup99.py":[{"add":["22","from .base import _refresh_cache","295","        X, y = _refresh_cache([samples_path, targets_path], 0)","296","        # TODO: Revert to the following two lines in v0.23","297","        # X = joblib.load(samples_path)","298","        # y = joblib.load(targets_path)"],"delete":["294","        X = joblib.load(samples_path)","295","        y = joblib.load(targets_path)"]}],"sklearn\/datasets\/rcv1.py":[{"add":["24","from .base import _refresh_cache","192","        X, sample_id = _refresh_cache([samples_path, sample_id_path], 9)","193","        # TODO: Revert to the following two lines in v0.23","194","        # X = joblib.load(samples_path)","195","        # sample_id = joblib.load(sample_id_path)","248","        y, categories = _refresh_cache([sample_topics_path, topics_path], 9)","249","        # TODO: Revert to the following two lines in v0.23","250","        # y = joblib.load(sample_topics_path)","251","        # categories = joblib.load(topics_path)"],"delete":["191","        X = joblib.load(samples_path)","192","        sample_id = joblib.load(sample_id_path)","245","        y = joblib.load(sample_topics_path)","246","        categories = joblib.load(topics_path)"]}],"sklearn\/datasets\/tests\/test_base.py":[{"add":["10","import joblib","26","from sklearn.datasets.base import _refresh_cache","280","","281","","282","def test_refresh_cache(monkeypatch):","283","    # uses pytests monkeypatch fixture","284","    # https:\/\/docs.pytest.org\/en\/latest\/monkeypatch.html","285","","286","    def _load_warn(*args, **kwargs):","287","        # raise the warning from \"externals.joblib.__init__.py\"","288","        # this is raised when a file persisted by the old joblib is loaded now","289","        msg = (\"sklearn.externals.joblib is deprecated in 0.21 and will be \"","290","               \"removed in 0.23. Please import this functionality directly \"","291","               \"from joblib, which can be installed with: pip install joblib. \"","292","               \"If this warning is raised when loading pickled models, you \"","293","               \"may need to re-serialize those models with scikit-learn \"","294","               \"0.21+.\")","295","        warnings.warn(msg, DeprecationWarning)","296","        return 0","297","","298","    def _load_warn_unrelated(*args, **kwargs):","299","        warnings.warn(\"unrelated warning\", DeprecationWarning)","300","        return 0","301","","302","    def _dump_safe(*args, **kwargs):","303","        pass","304","","305","    def _dump_raise(*args, **kwargs):","306","        # this happens if the file is read-only and joblib.dump fails to write","307","        # on it.","308","        raise IOError()","309","","310","    # test if the dataset spesific warning is raised if load raises the joblib","311","    # warning, and dump fails to dump with new joblib","312","    monkeypatch.setattr(joblib, \"load\", _load_warn)","313","    monkeypatch.setattr(joblib, \"dump\", _dump_raise)","314","    msg = \"This dataset will stop being loadable in scikit-learn\"","315","    with pytest.warns(DeprecationWarning, match=msg):","316","        _refresh_cache('test', 0)","317","","318","    # make sure no warning is raised if load raises the warning, but dump","319","    # manages to dump the new data","320","    monkeypatch.setattr(joblib, \"load\", _load_warn)","321","    monkeypatch.setattr(joblib, \"dump\", _dump_safe)","322","    with pytest.warns(None) as warns:","323","        _refresh_cache('test', 0)","324","    assert len(warns) == 0","325","","326","    # test if an unrelated warning is still passed through and not suppressed","327","    # by _refresh_cache","328","    monkeypatch.setattr(joblib, \"load\", _load_warn_unrelated)","329","    monkeypatch.setattr(joblib, \"dump\", _dump_safe)","330","    with pytest.warns(DeprecationWarning, match=\"unrelated warning\"):","331","        _refresh_cache('test', 0)"],"delete":[]}]}},"581b0e1d73414f47ef6cde6cd282667b7e767a36":{"changes":{"sklearn\/neighbors\/quad_tree.pyx":"MODIFY","sklearn\/tree\/_utils.pyx":"MODIFY","sklearn\/utils\/graph_shortest_path.pyx":"MODIFY","sklearn\/utils\/sparsefuncs_fast.pyx":"MODIFY","sklearn\/tree\/_tree.pyx":"MODIFY"},"diff":{"sklearn\/neighbors\/quad_tree.pyx":[{"add":["8","from cpython cimport Py_INCREF, PyObject, PyTypeObject","25","    object PyArray_NewFromDescr(PyTypeObject* subtype, np.dtype descr,","574","        arr = PyArray_NewFromDescr(<PyTypeObject *> np.ndarray,","575","                                   CELL_DTYPE, 1, shape,"],"delete":["8","from cpython cimport Py_INCREF, PyObject","25","    object PyArray_NewFromDescr(object subtype, np.dtype descr,","574","        arr = PyArray_NewFromDescr(np.ndarray, CELL_DTYPE, 1, shape,"]}],"sklearn\/tree\/_utils.pyx":[{"add":["513","        cdef DOUBLE_t original_median = 0.0","570","        cdef DOUBLE_t original_median = 0.0","585","        cdef double original_median = 0.0"],"delete":["513","        cdef DOUBLE_t original_median","570","        cdef DOUBLE_t original_median","585","        cdef double original_median"]}],"sklearn\/utils\/graph_shortest_path.pyx":[{"add":["118","    cdef unsigned int N = graph.shape[0]","500","    cdef unsigned int i_N","501","    cdef ITYPE_t i","507","    for i_N in range(0, N):","508","        initialize_node(&nodes[i_N], i_N)","566","    cdef unsigned int i_N","567","    cdef ITYPE_t i","576","    for i_N in range(0, N):","577","        nodes[i_N].state = 0  # 0 -> NOT_IN_HEAP","578","        nodes[i_N].val = 0"],"delete":["118","    cdef int N = graph.shape[0]","500","    cdef unsigned int i","506","    for i from 0 <= i < N:","507","        initialize_node(&nodes[i], i)","565","    cdef unsigned int i","574","    for i from 0 <= i < N:","575","        nodes[i].state = 0  # 0 -> NOT_IN_HEAP","576","        nodes[i].val = 0"]}],"sklearn\/utils\/sparsefuncs_fast.pyx":[{"add":[],"delete":["173","        unsigned long long endptr"]}],"sklearn\/tree\/_tree.pyx":[{"add":["18","from cpython cimport Py_INCREF, PyObject, PyTypeObject","41","    object PyArray_NewFromDescr(PyTypeObject* subtype, np.dtype descr,","1119","        arr = PyArray_NewFromDescr(<PyTypeObject *> np.ndarray,","1120","                                   <np.dtype> NODE_DTYPE, 1, shape,"],"delete":["18","from cpython cimport Py_INCREF, PyObject","41","    object PyArray_NewFromDescr(object subtype, np.dtype descr,","1119","        arr = PyArray_NewFromDescr(np.ndarray, <np.dtype> NODE_DTYPE, 1, shape,"]}]}},"8fe89ea5241d0301fa0ee5f9f02b3f3db2ee89d4":{"changes":{"sklearn\/impute\/_base.py":"MODIFY","sklearn\/utils\/estimator_checks.py":"MODIFY"},"diff":{"sklearn\/impute\/_base.py":[{"add":["670","                'X_types': ['2darray', 'string']}"],"delete":["670","                'X_types': ['2darray', 'str']}"]}],"sklearn\/utils\/estimator_checks.py":[{"add":["665","    if 'string' not in tags['X_types']:"],"delete":["665","    if 'str' not in tags['X_types']:"]}]}},"14dd3cb38487a78a28a6d794a80fd012a92c6656":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/cluster\/tests\/test_optics.py":"MODIFY","sklearn\/cluster\/optics_.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["35","- |Fix| Fixed a bug in :class:`cluster.OPTICS` where users were unable to pass","36","  float `min_samples` and `min_cluster_size`. :pr:`14496` by","37","  :user:`Fabian Klopfer <someusername1>`","38","  and :user:`Hanmin Qin <qinhanmin2014>`.","39",""],"delete":[]}],"sklearn\/cluster\/tests\/test_optics.py":[{"add":["103","    # check float min_samples and min_cluster_size","104","    clust = OPTICS(min_samples=0.1, min_cluster_size=0.08,","105","                   max_eps=20, cluster_method='xi',","106","                   xi=0.4).fit(X)","107","    assert_array_equal(clust.labels_, expected_labels)","108",""],"delete":[]}],"sklearn\/cluster\/optics_.py":[{"add":["46","    min_samples : int > 1 or float between 0 and 1 (default=5)","343","    min_samples : int > 1 or float between 0 and 1","439","        min_samples = max(2, int(min_samples * n_samples))","584","    min_samples : int > 1 or float between 0 and 1","621","        min_samples = max(2, int(min_samples * n_samples))","626","        min_cluster_size = max(2, int(min_cluster_size * n_samples))","755","    min_samples : int > 1","759","    min_cluster_size : int > 1","760","        Minimum number of samples in an OPTICS cluster."],"delete":["46","    min_samples : int > 1 or float between 0 and 1 (default=None)","343","    min_samples : int (default=5)","439","        min_samples = max(2, min_samples * n_samples)","584","    min_samples : int > 1 or float between 0 and 1 (default=None)","621","        min_samples = max(2, min_samples * n_samples)","626","        min_cluster_size = max(2, min_cluster_size * n_samples)","755","    min_samples : int > 1 or float between 0 and 1 (default=None)","758","        Expressed as an absolute number or a fraction of the number of samples","759","        (rounded to be at least 2).","761","    min_cluster_size : int > 1 or float between 0 and 1","762","        Minimum number of samples in an OPTICS cluster, expressed as an","763","        absolute number or a fraction of the number of samples (rounded","764","        to be at least 2)."]}]}},"ff3af8deef29ee61f94a5c3229f9cfe0e6118e7e":{"changes":{"doc\/whats_new\/v0.22.rst":"MODIFY","sklearn\/decomposition\/nmf.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.22.rst":[{"add":["165","- |Efficiency| :class:`decomposition.NMF(solver='mu')` fitted on sparse input","166","  matrices now uses batching to avoid briefly allocating an array with size","167","  (#non-zero elements, n_components). :pr:`15257` by `Mart Willocx <Maocx>`_.","168",""],"delete":[]}],"sklearn\/decomposition\/nmf.py":[{"add":["11","import time","12","import warnings","13","from math import sqrt","15","from .cdnmf_fast import _update_cdnmf_fast","17","from ..exceptions import ConvergenceWarning","171","        n_vals = ii.shape[0]","172","        dot_vals = np.empty(n_vals)","173","        n_components = W.shape[1]","174","","175","        batch_size = max(n_components, n_vals \/\/ n_components)","176","        for start in range(0, n_vals, batch_size):","177","            batch = slice(start, start + batch_size)","178","            dot_vals[batch] = np.multiply(W[ii[batch], :],","179","                                          H.T[jj[batch], :]).sum(axis=1)","180",""],"delete":["8","from math import sqrt","9","import warnings","11","import time","12","","20","from ..exceptions import ConvergenceWarning","21","from .cdnmf_fast import _update_cdnmf_fast","172","        dot_vals = np.multiply(W[ii, :], H.T[jj, :]).sum(axis=1)"]}]}},"bd2cc1064829d9531558ae4e9c211ea85a9e628e":{"changes":{"sklearn\/cluster\/tests\/test_optics.py":"MODIFY"},"diff":{"sklearn\/cluster\/tests\/test_optics.py":[{"add":["111","                   xi=0.3).fit(X)"],"delete":["111","                   xi=0.1).fit(X)"]}]}},"d624bb1771e97f861c71d70735437233f4b2516f":{"changes":{"build_tools\/azure\/posix.yml":"MODIFY"},"diff":{"build_tools\/azure\/posix.yml":[{"add":["25","    - bash: sudo chown -R $USER $CONDA","26","      displayName: Take ownership of conda installation","27","      condition: eq(variables['DISTRIB'], 'conda')"],"delete":[]}]}},"96c1a5b973936175ce6bd1ef4e0758a77da027c1":{"changes":{"sklearn\/metrics\/tests\/test_score_objects.py":"MODIFY","sklearn\/metrics\/_scorer.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY"},"diff":{"sklearn\/metrics\/tests\/test_score_objects.py":[{"add":["6","from functools import partial","32","from sklearn.linear_model import Ridge, LogisticRegression, Perceptron","673","","674","","675","@pytest.mark.parametrize('scorer_name, metric', [","676","    ('roc_auc_ovr', partial(roc_auc_score, multi_class='ovr')),","677","    ('roc_auc_ovo', partial(roc_auc_score, multi_class='ovo')),","678","    ('roc_auc_ovr_weighted', partial(roc_auc_score, multi_class='ovr',","679","                                     average='weighted')),","680","    ('roc_auc_ovo_weighted', partial(roc_auc_score, multi_class='ovo',","681","                                     average='weighted'))])","682","def test_multiclass_roc_proba_scorer(scorer_name, metric):","683","    scorer = get_scorer(scorer_name)","684","    X, y = make_classification(n_classes=3, n_informative=3, n_samples=20,","685","                               random_state=0)","686","    lr = LogisticRegression(multi_class=\"multinomial\").fit(X, y)","687","    y_proba = lr.predict_proba(X)","688","    expected_score = metric(y, y_proba)","689","","690","    assert scorer(lr, X, y) == pytest.approx(expected_score)","691","","692","","693","def test_multiclass_roc_proba_scorer_label():","694","    scorer = make_scorer(roc_auc_score, multi_class='ovo',","695","                         labels=[0, 1, 2], needs_proba=True)","696","    X, y = make_classification(n_classes=3, n_informative=3, n_samples=20,","697","                               random_state=0)","698","    lr = LogisticRegression(multi_class=\"multinomial\").fit(X, y)","699","    y_proba = lr.predict_proba(X)","700","","701","    y_binary = y == 0","702","    expected_score = roc_auc_score(y_binary, y_proba,","703","                                   multi_class='ovo',","704","                                   labels=[0, 1, 2])","705","","706","    assert scorer(lr, X, y_binary) == pytest.approx(expected_score)","707","","708","","709","@pytest.mark.parametrize('scorer_name', [","710","    'roc_auc_ovr', 'roc_auc_ovo',","711","    'roc_auc_ovr_weighted', 'roc_auc_ovo_weighted'])","712","def test_multiclass_roc_no_proba_scorer_errors(scorer_name):","713","    # Perceptron has no predict_proba","714","    scorer = get_scorer(scorer_name)","715","    X, y = make_classification(n_classes=3, n_informative=3, n_samples=20,","716","                               random_state=0)","717","    lr = Perceptron().fit(X, y)","718","    msg = \"'Perceptron' object has no attribute 'predict_proba'\"","719","    with pytest.raises(AttributeError, match=msg):","720","        scorer(lr, X, y)"],"delete":["31","from sklearn.linear_model import Ridge, LogisticRegression"]}],"sklearn\/metrics\/_scorer.py":[{"add":["249","            elif y_pred.shape[1] == 1:  # not multiclass","647","roc_auc_ovo_scorer = make_scorer(roc_auc_score, needs_proba=True,","649","roc_auc_ovo_weighted_scorer = make_scorer(roc_auc_score, needs_proba=True,","652","roc_auc_ovr_scorer = make_scorer(roc_auc_score, needs_proba=True,","654","roc_auc_ovr_weighted_scorer = make_scorer(roc_auc_score, needs_proba=True,"],"delete":["249","            else:","647","roc_auc_ovo_scorer = make_scorer(roc_auc_score, needs_threshold=True,","649","roc_auc_ovo_weighted_scorer = make_scorer(roc_auc_score, needs_threshold=True,","652","roc_auc_ovr_scorer = make_scorer(roc_auc_score, needs_threshold=True,","654","roc_auc_ovr_weighted_scorer = make_scorer(roc_auc_score, needs_threshold=True,"]}],"doc\/whats_new\/v0.22.rst":[{"add":["472","- |Feature| Added multiclass support to :func:`metrics.roc_auc_score` with","473","  corresponding scorers 'roc_auc_ovr', 'roc_auc_ovo', 'roc_auc_ovr_weighted', ","474","  and 'roc_auc_ovo_weighted'. :pr:`12789` and :pr:`15274` by ","475","  :user:`Kathy Chen <kathyxchen>`, :user:`Mohamed Maskani <maskani-moh>`, and","476","  `Thomas Fan`_."],"delete":["472","- |Feature| Added multiclass support to :func:`metrics.roc_auc_score`.","473","  :issue:`12789` by :user:`Kathy Chen <kathyxchen>`,","474","  :user:`Mohamed Maskani <maskani-moh>`, and :user:`Thomas Fan <thomasjpfan>`."]}]}},"b580ad5dfd286c77fb06cf5803fad27ea2d57c0d":{"changes":{"sklearn\/utils\/estimator_checks.py":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_gradient_boosting.py":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/grower.py":"MODIFY"},"diff":{"sklearn\/utils\/estimator_checks.py":[{"add":["2403","        # Since the link function from decision_function() to predict_proba()","2404","        # is sometimes not precise enough (typically expit), we round to the","2405","        # 10th decimal to avoid numerical issues.","2406","        a = estimator.predict_proba(X_test)[:, 1].round(decimals=10)","2407","        b = estimator.decision_function(X_test).round(decimals=10)"],"delete":["2403","        a = estimator.predict_proba(X_test)[:, 1]","2404","        b = estimator.decision_function(X_test)"]}],"sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_gradient_boosting.py":[{"add":["174","","175","","176","@pytest.mark.parametrize('data', [","177","    make_classification(random_state=0, n_classes=2),","178","    make_classification(random_state=0, n_classes=3, n_informative=3)","179","], ids=['binary_crossentropy', 'categorical_crossentropy'])","180","def test_zero_division_hessians(data):","181","    # non regression test for issue #14018","182","    # make sure we avoid zero division errors when computing the leaves values.","183","","184","    # If the learning rate is too high, the raw predictions are bad and will","185","    # saturate the softmax (or sigmoid in binary classif). This leads to","186","    # probabilities being exactly 0 or 1, gradients being constant, and","187","    # hessians being zero.","188","    X, y = data","189","    gb = HistGradientBoostingClassifier(learning_rate=100, max_iter=10)","190","    gb.fit(X, y)"],"delete":[]}],"sklearn\/ensemble\/_hist_gradient_boosting\/grower.py":[{"add":["18","from .types import Y_DTYPE","19","","20","","21","EPS = np.finfo(Y_DTYPE).eps  # to avoid zero division errors","404","            node.sum_hessians + self.splitter.l2_regularization + EPS)"],"delete":["400","            node.sum_hessians + self.splitter.l2_regularization)"]}]}},"32d5a763acdd34a295187c45a49a5feb7aca6f4a":{"changes":{"sklearn\/utils\/weight_vector.pyx":"MODIFY","sklearn\/utils\/weight_vector.pxd":"MODIFY","sklearn\/linear_model\/sgd_fast.pyx":"MODIFY"},"diff":{"sklearn\/utils\/weight_vector.pyx":[{"add":["43","    def __cinit__(self, double [::1] w, double [::1] aw):","49","        self.sq_norm = _dot(<int>w.shape[0], &w[0], 1, &w[0], 1)","51","        self.w_data_ptr = &w[0]","52","        if aw is not None:","53","            self.aw_data_ptr = &aw[0]","172","        if self.aw_data_ptr != NULL:","173","            _axpy(self.n_features, self.average_a,","174","                  self.w_data_ptr, 1, self.aw_data_ptr, 1)","175","            _scal(self.n_features, 1.0 \/ self.average_b, self.aw_data_ptr, 1)","179","        _scal(self.n_features, self.wscale, self.w_data_ptr, 1)"],"delete":["36","    w_data_ptr : double*","37","        A pointer to the data of the numpy array.","45","","46","    def __cinit__(self,","47","                  np.ndarray[double, ndim=1, mode='c'] w,","48","                  np.ndarray[double, ndim=1, mode='c'] aw):","49","        cdef double *wdata = <double *>w.data","50","","54","        self.w = w","55","        self.w_data_ptr = wdata","58","        self.sq_norm = _dot(<int>w.shape[0], wdata, 1, wdata, 1)","60","        self.aw = aw","61","        if self.aw is not None:","62","            self.aw_data_ptr = <double *>aw.data","181","        if self.aw is not None:","182","            _axpy(<int>self.aw.shape[0], self.average_a,","183","                  <double *>self.w.data, 1, <double *>self.aw.data, 1)","184","            _scal(<int>self.aw.shape[0], 1.0 \/ self.average_b,","185","                  <double *>self.aw.data, 1)","189","        _scal(<int>self.w.shape[0], self.wscale, <double *>self.w.data, 1)"]}],"sklearn\/utils\/weight_vector.pxd":[{"add":[],"delete":["10","    cdef np.ndarray w","11","    cdef np.ndarray aw"]}],"sklearn\/linear_model\/sgd_fast.pyx":[{"add":["835","        if wscale * z > 0.0:","839","        elif wscale * z < 0.0:"],"delete":["835","        if wscale * w_data_ptr[idx] > 0.0:","839","        elif wscale * w_data_ptr[idx] < 0.0:"]}]}},"38f9195c7b5afcbc34afef03beeb57e53e1d99df":{"changes":{".gitignore":"MODIFY"},"diff":{".gitignore":[{"add":["61","!\/**\/src\/**\/*.c","62","!\/**\/src\/**\/*.cpp"],"delete":["61","!*\/src\/*.c","62","!*\/src\/*.cpp"]}]}},"9e08b142e0ad60b6bbc1c4a1a3f6e583faf25f3e":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","doc\/whats_new\/changelog_legend.inc":"ADD","doc\/whats_new.rst":"MODIFY","doc\/templates\/index.html":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["9",".. include:: changelog_legend.inc","10",""],"delete":[]}],"doc\/whats_new\/changelog_legend.inc":[{"add":[],"delete":[]}],"doc\/whats_new.rst":[{"add":["6","Release notes for all scikit-learn releases are linked in this this page.","14","    Version 0.22 <whats_new\/v0.22.rst>","15","    Version 0.21 <whats_new\/v0.21.rst>"],"delete":["1",".. include:: includes\/big_toc_css.rst","7","Release notes for current and recent releases are detailed on this page, with","8",":ref:`previous releases <previous_releases_whats_new>` linked below.","13","Legend for changelogs","14","---------------------","15","","16","- |MajorFeature|: something big that you couldn't do before.","17","- |Feature|: something that you couldn't do before.","18","- |Efficiency|: an existing feature now may not require as much computation or","19","  memory.","20","- |Enhancement|: a miscellaneous minor improvement.","21","- |Fix|: something that previously didn't work as documentated -- or according","22","  to reasonable expectations -- should now work.","23","- |API|: you will need to change your code to have the same effect in the","24","  future; or a feature will be removed in the future.","25","","26",".. include:: whats_new\/v0.22.rst","27",".. include:: whats_new\/v0.21.rst","28","","29",".. _previous_releases_whats_new:","30","","31","Previous Releases","32","================="]}],"doc\/templates\/index.html":[{"add":["160","        <li><strong>July 2019.<\/strong> scikit-learn 0.21.3 (<a href=\"whats_new\/v0.21.html#version-0-21-3\">Changelog<\/a>) and 0.20.4 (<a href=\"whats_new\/v0.20.html#version-0-20-4\">Changelog<\/a>) are available for download.","162","        <li><strong>May 2019.<\/strong> scikit-learn 0.21.0 to 0.21.2 are available for download (<a href=\"whats_new\/v0.21.html#version-0-21-2\">Changelog<\/a>).","164","        <li><strong>March 2019.<\/strong> scikit-learn 0.20.3 is available for download (<a href=\"whats_new\/v0.20.html#version-0-20-3\">Changelog<\/a>).","166","        <li><strong>September 2018.<\/strong> scikit-learn 0.20.0 is available for download (<a href=\"whats_new\/v0.20.html#version-0-20-0\">Changelog<\/a>).","168","        <li><strong>July 2018.<\/strong> scikit-learn 0.19.2 is available for download (<a href=\"whats_new\/v0.19.html#version-0-19-2\">Changelog<\/a>)."],"delete":["160","        <li><strong>July 2019.<\/strong> scikit-learn 0.21.3 (<a href=\"whats_new.html#version-0-21-3\">Changelog<\/a>) and 0.20.4 (<a href=\"whats_new.html#version-0-20-4\">Changelog<\/a>) are available for download.","162","        <li><strong>May 2019.<\/strong> scikit-learn 0.21.0 to 0.21.2 are available for download (<a href=\"whats_new.html#version-0-21\">Changelog<\/a>).","164","        <li><strong>March 2019.<\/strong> scikit-learn 0.20.3 is available for download (<a href=\"whats_new.html#version-0-20-3\">Changelog<\/a>).","166","        <li><strong>September 2018.<\/strong> scikit-learn 0.20.0 is available for download (<a href=\"whats_new.html#version-0-20-0\">Changelog<\/a>).","168","        <li><strong>July 2018.<\/strong> scikit-learn 0.19.2 is available for download (<a href=\"whats_new.html#version-0-19\">Changelog<\/a>)."]}],"doc\/whats_new\/v0.22.rst":[{"add":["15",".. include:: changelog_legend.inc"],"delete":[]}]}},"77a9d32dd813f4ecb100cc6c9ac40a0ac5d8d365":{"changes":{"doc\/themes\/scikit-learn-modern\/static\/css\/theme.css":"MODIFY"},"diff":{"doc\/themes\/scikit-learn-modern\/static\/css\/theme.css":[{"add":["25","p {","26","  word-break: break-word;","27","  hyphens: auto;","28","}","29","","666","  padding-left: 0.25rem;","897","table.docutils p {","900","  word-break: initial;"],"delete":["661","  padding: 4px 0 0 4px;","892","table.docutils td:last-child p {"]}]}},"76d5f061a8068ef5dce469b52c7f8322c518e2a2":{"changes":{"sklearn\/tests\/test_pipeline.py":"MODIFY","sklearn\/pipeline.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY"},"diff":{"sklearn\/tests\/test_pipeline.py":[{"add":["23","from sklearn.base import clone, BaseEstimator, TransformerMixin","37","iris = load_iris()","1152","","1153","","1154","def test_feature_union_fit_params():","1155","    # Regression test for issue: #15117","1156","    class Dummy(TransformerMixin, BaseEstimator):","1157","        def fit(self, X, y=None, **fit_params):","1158","            if fit_params != {'a': 0}:","1159","                raise ValueError","1160","            return self","1161","","1162","        def transform(self, X, y=None):","1163","            return X","1164","","1165","    X, y = iris.data, iris.target","1166","    t = FeatureUnion([('dummy0', Dummy()), ('dummy1', Dummy())])","1167","    with pytest.raises(ValueError):","1168","        t.fit(X, y)","1169","","1170","    with pytest.raises(ValueError):","1171","        t.fit_transform(X, y)","1172","","1173","    t.fit(X, y, a=0)","1174","    t.fit_transform(X, y, a=0)"],"delete":["23","from sklearn.base import clone, BaseEstimator","242","    iris = load_iris()","321","    iris = load_iris()","336","    iris = load_iris()","367","    iris = load_iris()","400","    iris = load_iris()","458","    iris = load_iris()","532","    iris = load_iris()","551","    iris = load_iris()","773","    iris = load_iris()","867","    iris = load_iris()","989","    iris = load_iris()","1024","    iris = load_iris()"]}],"sklearn\/pipeline.py":[{"add":["878","    def fit(self, X, y=None, **fit_params):","894","        transformers = self._parallel_func(X, y, fit_params, _fit_one)"],"delete":["878","    def fit(self, X, y=None):","894","        transformers = self._parallel_func(X, y, {}, _fit_one)"]}],"doc\/whats_new\/v0.22.rst":[{"add":["506","- |Fix| The `fit` in :class:`~pipeline.FeatureUnion` now accepts `fit_params`","507","  to pass to the underlying transformers. :pr:`15119` by `Adrin Jalali`_.","508",""],"delete":[]}]}},"e424ab17bb73472a829faca3dfdc599a9d6df56b":{"changes":{"doc\/whats_new\/v0.22.rst":"MODIFY","sklearn\/cluster\/_k_means_elkan.pyx":"MODIFY"},"diff":{"doc\/whats_new\/v0.22.rst":[{"add":["87","- |Fix| Fixed a bug where `elkan` algorithm in :class:`cluster.KMeans` was","88","  producing Segmentation Fault on large arrays due to integer index overflow.","89","  :pr:`15057` by :user:`Vladimir Korolev <balodja>`.","90",""],"delete":[]}],"sklearn\/cluster\/_k_means_elkan.pyx":[{"add":["32","        Py_ssize_t n_samples, int n_features, int n_clusters):","71","    n_samples : Py_ssize_t","85","    cdef int c_x, j","86","    cdef Py_ssize_t sample","147","    cdef Py_ssize_t point_index","148","    cdef int center_index, label"],"delete":["32","        int n_samples, int n_features, int n_clusters):","71","    n_samples : int","85","    cdef int c_x, j, sample","146","    cdef int point_index, center_index, label"]}]}},"56edce84caf6f3c8ef96376e1b684c45311130f3":{"changes":{"\/dev\/null":"DELETE","sklearn\/_build_utils\/deprecated_modules.py":"MODIFY",".gitignore":"MODIFY","sklearn\/cluster\/_birch.py":"MODIFY","sklearn\/utils\/deprecation.py":"MODIFY","conftest.py":"MODIFY","sklearn\/cluster\/tests\/test_hierarchical.py":"MODIFY"},"diff":{"\/dev\/null":[{"add":[],"delete":[]}],"sklearn\/_build_utils\/deprecated_modules.py":[{"add":["22","    ('_rbm', 'sklearn.neural_network.rbm', 'sklearn.neural_network'),","23","    ('_multilayer_perceptron',","24","     'sklearn.neural_network.multilayer_perceptron', 'sklearn.neural_network'),","25",""],"delete":[]}],".gitignore":[{"add":["95","sklearn\/neural_network\/rbm.py","96","sklearn\/neural_network\/multilayer_perceptron.py","97",""],"delete":[]}],"sklearn\/cluster\/_birch.py":[{"add":["16","from . import AgglomerativeClustering"],"delete":["16","from ._hierarchical import AgglomerativeClustering"]}],"sklearn\/utils\/deprecation.py":[{"add":["142","    warnings.warn(message, DeprecationWarning)"],"delete":["130","    # We don't want to raise a dep warning if we are in a pytest session else","131","    # the CIs with -Werror::DeprecationWarning would fail. The deprecations are","132","    # still properly tested in sklearn\/tests\/test_import_deprecations.py","145","    if not getattr(sys, '_is_pytest_session', False):","146","        warnings.warn(message, DeprecationWarning)"]}],"conftest.py":[{"add":["9","import os","17","from sklearn._build_utils.deprecated_modules import _DEPRECATED_MODULES","100","","101","","102","# TODO: Remove when modules are deprecated in 0.24","103","# Configures pytest to ignore deprecated modules.","104","collect_ignore_glob = [","105","    os.path.join(*deprecated_path.split(\".\")) + \".py\"","106","    for _, deprecated_path, _ in _DEPRECATED_MODULES]"],"delete":[]}],"sklearn\/cluster\/tests\/test_hierarchical.py":[{"add":["25","                                           linkage_tree, _fix_connectivity)"],"delete":["25","                                           _fix_connectivity)","26","from sklearn.cluster.hierarchical import linkage_tree"]}]}},"e6a4dc97c9a62420ca00766366352da665566584":{"changes":{"sklearn\/utils\/validation.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY","sklearn\/utils\/tests\/test_validation.py":"MODIFY"},"diff":{"sklearn\/utils\/validation.py":[{"add":["34","def _assert_all_finite(X, allow_nan=False, msg_dtype=None):","54","            raise ValueError(","55","                    msg_err.format","56","                    (type_err,","57","                     msg_dtype if msg_dtype is not None else X.dtype)","58","            )","500","                if dtype is not None and np.dtype(dtype).kind in 'iu':","501","                    # Conversion float -> int should not contain NaN or","502","                    # inf (numpy#14412). We cannot use casting='safe' because","503","                    # then conversion float -> int would be disallowed.","504","                    array = np.asarray(array, order=order)","505","                    if array.dtype.kind == 'f':","506","                        _assert_all_finite(array, allow_nan=False,","507","                                           msg_dtype=dtype)","508","                    array = array.astype(dtype, casting=\"unsafe\", copy=False)","509","                else:","510","                    array = np.asarray(array, order=order, dtype=dtype)"],"delete":["34","def _assert_all_finite(X, allow_nan=False):","54","            raise ValueError(msg_err.format(type_err, X.dtype))","496","                array = np.asarray(array, dtype=dtype, order=order)"]}],"doc\/whats_new\/v0.22.rst":[{"add":["556","- |Fix| :func:`utils.check_array` is now raising an error instead of casting","557","  NaN to integer.","558","  :pr:`14872` by `Roman Yurchak`_.","559",""],"delete":[]}],"sklearn\/utils\/tests\/test_validation.py":[{"add":["204","@pytest.mark.parametrize(","205","    \"X, err_msg\",","206","    [(np.array([[1, np.nan]]),","207","      \"Input contains NaN, infinity or a value too large for.*int\"),","208","     (np.array([[1, np.nan]]),","209","      \"Input contains NaN, infinity or a value too large for.*int\"),","210","     (np.array([[1, np.inf]]),","211","      \"Input contains NaN, infinity or a value too large for.*int\"),","212","     (np.array([[1, np.nan]], dtype=np.object),","213","      \"cannot convert float NaN to integer\")]","214",")","215","@pytest.mark.parametrize(\"force_all_finite\", [True, False])","216","def test_check_array_force_all_finite_object_unsafe_casting(","217","        X, err_msg, force_all_finite):","218","    # casting a float array containing NaN or inf to int dtype should","219","    # raise an error irrespective of the force_all_finite parameter.","220","    with pytest.raises(ValueError, match=err_msg):","221","        check_array(X, dtype=np.int, force_all_finite=force_all_finite)","222","","223",""],"delete":[]}]}},"11d2539444744f1aad91f9f07202be09e5cc52d9":{"changes":{"build_tools\/circle\/build_doc.sh":"MODIFY",".circleci\/config.yml":"MODIFY"},"diff":{"build_tools\/circle\/build_doc.sh":[{"add":["103","    latexmk gsfonts ccache","114","export PATH=\"\/usr\/lib\/ccache:$MINICONDA_PATH\/bin:$PATH\"","115","","116","ccache -M 512M","117","export CCACHE_COMPRESS=1","134","export OMP_NUM_THREADS=1","135",""],"delete":["103","    latexmk gsfonts","114","export PATH=\"$MINICONDA_PATH\/bin:$PATH\""]}],".circleci\/config.yml":[{"add":["20","      - restore_cache:","21","          keys:","22","            - doc-min-deps-ccache-{{ .Branch }}","23","            - doc-min-deps-ccache","26","          key: doc-min-deps-ccache-{{ .Branch }}-{{ .BuildNum }}","27","          paths:","28","            - ~\/.ccache","29","            - ~\/.cache\/pip","30","      - save_cache:","53","      - restore_cache:","54","          keys:","55","            - doc-ccache-{{ .Branch }}","56","            - doc-ccache","59","          key: doc-ccache-{{ .Branch }}-{{ .BuildNum }}","60","          paths:","61","            - ~\/.ccache","62","            - ~\/.cache\/pip","63","      - save_cache:"],"delete":[]}]}},"62aee0666e8803f20ecf0f6214621367e50f3961":{"changes":{"sklearn\/neighbors\/_nearest_centroid.py":"ADD",".gitignore":"MODIFY","sklearn\/neighbors\/tests\/test_neighbors.py":"MODIFY","sklearn\/neighbors\/_lof.py":"ADD","sklearn\/neighbors\/_base.py":"ADD","sklearn\/neighbors\/_quad_tree.pxd":"ADD","sklearn\/neighbors\/tests\/test_dist_metrics.py":"MODIFY","sklearn\/neighbors\/tests\/test_kde.py":"MODIFY","sklearn\/tree\/_utils.pxd":"MODIFY","sklearn\/neighbors\/_kde.py":"ADD","sklearn\/neighbors\/_dist_metrics.pyx":"ADD","sklearn\/neighbors\/_typedefs.pxd":"ADD","doc\/modules\/neighbors.rst":"MODIFY","sklearn\/neighbors\/_unsupervised.py":"ADD","sklearn\/neighbors\/_quad_tree.pyx":"ADD","sklearn\/manifold\/_barnes_hut_tsne.pyx":"MODIFY","sklearn\/neighbors\/tests\/test_nca.py":"MODIFY","sklearn\/neighbors\/_binary_tree.pxi":"ADD","sklearn\/neighbors\/setup.py":"MODIFY","sklearn\/neighbors\/__init__.py":"MODIFY","sklearn\/neighbors\/_regression.py":"ADD","sklearn\/impute\/_knn.py":"MODIFY","sklearn\/neighbors\/_nca.py":"ADD","sklearn\/_build_utils\/deprecated_modules.py":"MODIFY","sklearn\/neighbors\/_kd_tree.pyx":"ADD","sklearn\/neighbors\/_typedefs.pyx":"ADD","sklearn\/neighbors\/tests\/test_kd_tree.py":"MODIFY","sklearn\/neighbors\/tests\/test_graph.py":"MODIFY","sklearn\/neighbors\/_dist_metrics.pxd":"ADD","sklearn\/neighbors\/_graph.py":"ADD","sklearn\/neighbors\/_classification.py":"ADD","\/dev\/null":"DELETE","sklearn\/neighbors\/_ball_tree.pyx":"ADD","doc\/modules\/density.rst":"MODIFY","sklearn\/neighbors\/tests\/test_neighbors_tree.py":"MODIFY","sklearn\/neighbors\/tests\/test_quad_tree.py":"MODIFY","sklearn\/cluster\/tests\/test_hierarchical.py":"MODIFY","sklearn\/neighbors\/tests\/test_ball_tree.py":"MODIFY"},"diff":{"sklearn\/neighbors\/_nearest_centroid.py":[{"add":[],"delete":[]}],".gitignore":[{"add":["132","sklearn\/neighbors\/ball_tree.py","133","sklearn\/neighbors\/base.py","134","sklearn\/neighbors\/classification.py","135","sklearn\/neighbors\/dist_metrics.py","136","sklearn\/neighbors\/graph.py","137","sklearn\/neighbors\/kd_tree.py","138","sklearn\/neighbors\/kde.py","139","sklearn\/neighbors\/lof.py","140","sklearn\/neighbors\/nca.py","141","sklearn\/neighbors\/nearest_centroid.py","142","sklearn\/neighbors\/quad_tree.py","143","sklearn\/neighbors\/regression.py","144","sklearn\/neighbors\/typedefs.py","145","sklearn\/neighbors\/unsupervised.py","146",""],"delete":[]}],"sklearn\/neighbors\/tests\/test_neighbors.py":[{"add":["16","from sklearn.neighbors import VALID_METRICS_SPARSE, VALID_METRICS","17","from sklearn.neighbors._base import _is_sorted_by_data, _check_precomputed","1584","    # 'ball_tree': uses sklearn.neighbors._dist_metrics"],"delete":["16","from sklearn.neighbors.base import VALID_METRICS_SPARSE, VALID_METRICS","17","from sklearn.neighbors.base import _is_sorted_by_data, _check_precomputed","1584","    # 'ball_tree': uses sklearn.neighbors.dist_metrics"]}],"sklearn\/neighbors\/_lof.py":[{"add":[],"delete":[]}],"sklearn\/neighbors\/_base.py":[{"add":[],"delete":[]}],"sklearn\/neighbors\/_quad_tree.pxd":[{"add":[],"delete":[]}],"sklearn\/neighbors\/tests\/test_dist_metrics.py":[{"add":["11","from sklearn.neighbors import DistanceMetric"],"delete":["11","from sklearn.neighbors.dist_metrics import DistanceMetric"]}],"sklearn\/neighbors\/tests\/test_kde.py":[{"add":["6","from sklearn.neighbors._ball_tree import kernel_norm"],"delete":["6","from sklearn.neighbors.ball_tree import kernel_norm"]}],"sklearn\/tree\/_utils.pxd":[{"add":["13","from ..neighbors._quad_tree cimport Cell"],"delete":["13","from ..neighbors.quad_tree cimport Cell"]}],"sklearn\/neighbors\/_kde.py":[{"add":[],"delete":[]}],"sklearn\/neighbors\/_dist_metrics.pyx":[{"add":[],"delete":[]}],"sklearn\/neighbors\/_typedefs.pxd":[{"add":[],"delete":[]}],"doc\/modules\/neighbors.rst":[{"add":["471","    >>> from sklearn.neighbors import NearestCentroid"],"delete":["471","    >>> from sklearn.neighbors.nearest_centroid import NearestCentroid"]}],"sklearn\/neighbors\/_unsupervised.py":[{"add":[],"delete":[]}],"sklearn\/neighbors\/_quad_tree.pyx":[{"add":[],"delete":[]}],"sklearn\/manifold\/_barnes_hut_tsne.pyx":[{"add":["17","from ..neighbors._quad_tree cimport _QuadTree"],"delete":["17","from ..neighbors.quad_tree cimport _QuadTree"]}],"sklearn\/neighbors\/tests\/test_nca.py":[{"add":["20","from sklearn.neighbors import NeighborhoodComponentsAnalysis"],"delete":["20","from sklearn.neighbors.nca import NeighborhoodComponentsAnalysis"]}],"sklearn\/neighbors\/_binary_tree.pxi":[{"add":[],"delete":[]}],"sklearn\/neighbors\/setup.py":[{"add":["12","    config.add_extension('_ball_tree',","13","                         sources=['_ball_tree.pyx'],","17","    config.add_extension('_kd_tree',","18","                         sources=['_kd_tree.pyx'],","22","    config.add_extension('_dist_metrics',","23","                         sources=['_dist_metrics.pyx'],","29","    config.add_extension('_typedefs',","30","                         sources=['_typedefs.pyx'],","33","    config.add_extension(\"_quad_tree\",","34","                         sources=[\"_quad_tree.pyx\"],"],"delete":["12","    config.add_extension('ball_tree',","13","                         sources=['ball_tree.pyx'],","17","    config.add_extension('kd_tree',","18","                         sources=['kd_tree.pyx'],","22","    config.add_extension('dist_metrics',","23","                         sources=['dist_metrics.pyx'],","29","    config.add_extension('typedefs',","30","                         sources=['typedefs.pyx'],","33","    config.add_extension(\"quad_tree\",","34","                         sources=[\"quad_tree.pyx\"],"]}],"sklearn\/neighbors\/__init__.py":[{"add":["5","from ._ball_tree import BallTree","6","from ._kd_tree import KDTree","7","from ._dist_metrics import DistanceMetric","8","from ._graph import kneighbors_graph, radius_neighbors_graph","9","from ._graph import KNeighborsTransformer, RadiusNeighborsTransformer","10","from ._unsupervised import NearestNeighbors","11","from ._classification import KNeighborsClassifier, RadiusNeighborsClassifier","12","from ._regression import KNeighborsRegressor, RadiusNeighborsRegressor","13","from ._nearest_centroid import NearestCentroid","14","from ._kde import KernelDensity","15","from ._lof import LocalOutlierFactor","16","from ._nca import NeighborhoodComponentsAnalysis","17","from ._base import VALID_METRICS, VALID_METRICS_SPARSE"],"delete":["5","from .ball_tree import BallTree","6","from .kd_tree import KDTree","7","from .dist_metrics import DistanceMetric","8","from .graph import kneighbors_graph, radius_neighbors_graph","9","from .graph import KNeighborsTransformer, RadiusNeighborsTransformer","10","from .unsupervised import NearestNeighbors","11","from .classification import KNeighborsClassifier, RadiusNeighborsClassifier","12","from .regression import KNeighborsRegressor, RadiusNeighborsRegressor","13","from .nearest_centroid import NearestCentroid","14","from .kde import KernelDensity","15","from .lof import LocalOutlierFactor","16","from .nca import NeighborhoodComponentsAnalysis","17","from .base import VALID_METRICS, VALID_METRICS_SPARSE"]}],"sklearn\/neighbors\/_regression.py":[{"add":[],"delete":[]}],"sklearn\/impute\/_knn.py":[{"add":["6","from ..neighbors._base import _get_weights","7","from ..neighbors._base import _check_weights"],"delete":["6","from ..neighbors.base import _get_weights","7","from ..neighbors.base import _check_weights"]}],"sklearn\/neighbors\/_nca.py":[{"add":[],"delete":[]}],"sklearn\/_build_utils\/deprecated_modules.py":[{"add":["88","    ('_ball_tree', 'sklearn.neighbors.ball_tree', 'sklearn.neighbors',","89","     'BallTree'),","90","    ('_base', 'sklearn.neighbors.base', 'sklearn.neighbors',","91","     'VALID_METRICS'),","92","    ('_classification', 'sklearn.neighbors.classification',","93","     'sklearn.neighbors', 'KNeighborsClassifier'),","94","    ('_dist_metrics', 'sklearn.neighbors.dist_metrics', 'sklearn.neighbors',","95","     'DistanceMetric'),","96","    ('_graph', 'sklearn.neighbors.graph', 'sklearn.neighbors',","97","     'KNeighborsTransformer'),","98","    ('_kd_tree', 'sklearn.neighbors.kd_tree', 'sklearn.neighbors',","99","     'KDTree'),","100","    ('_kde', 'sklearn.neighbors.kde', 'sklearn.neighbors',","101","     'KernelDensity'),","102","    ('_lof', 'sklearn.neighbors.lof', 'sklearn.neighbors',","103","     'LocalOutlierFactor'),","104","    ('_nca', 'sklearn.neighbors.nca', 'sklearn.neighbors',","105","     'NeighborhoodComponentsAnalysis'),","106","    ('_nearest_centroid', 'sklearn.neighbors.nearest_centroid',","107","     'sklearn.neighbors', 'NearestCentroid'),","108","    ('_quad_tree', 'sklearn.neighbors.quad_tree', 'sklearn.neighbors',","109","     'CELL_DTYPE'),","110","    ('_regression', 'sklearn.neighbors.regression', 'sklearn.neighbors',","111","     'KNeighborsRegressor'),","112","    ('_typedefs', 'sklearn.neighbors.typedefs', 'sklearn.neighbors',","113","     'DTYPE'),","114","    ('_unsupervised', 'sklearn.neighbors.unsupervised', 'sklearn.neighbors',","115","     'NearestNeighbors'),","116",""],"delete":[]}],"sklearn\/neighbors\/_kd_tree.pyx":[{"add":[],"delete":[]}],"sklearn\/neighbors\/_typedefs.pyx":[{"add":[],"delete":[]}],"sklearn\/neighbors\/tests\/test_kd_tree.py":[{"add":["5","from sklearn.neighbors._kd_tree import (KDTree, NeighborsHeap,","6","                                        simultaneous_sort, kernel_norm,","7","                                        nodeheap_sort, DTYPE, ITYPE)","8","from sklearn.neighbors import DistanceMetric"],"delete":["5","from sklearn.neighbors.kd_tree import (KDTree, NeighborsHeap,","6","                                       simultaneous_sort, kernel_norm,","7","                                       nodeheap_sort, DTYPE, ITYPE)","8","from sklearn.neighbors.dist_metrics import DistanceMetric"]}],"sklearn\/neighbors\/tests\/test_graph.py":[{"add":["4","from sklearn.neighbors._base import _is_sorted_by_data"],"delete":["4","from sklearn.neighbors.base import _is_sorted_by_data"]}],"sklearn\/neighbors\/_dist_metrics.pxd":[{"add":[],"delete":[]}],"sklearn\/neighbors\/_graph.py":[{"add":[],"delete":[]}],"sklearn\/neighbors\/_classification.py":[{"add":[],"delete":[]}],"\/dev\/null":[{"add":[],"delete":[]}],"sklearn\/neighbors\/_ball_tree.pyx":[{"add":[],"delete":[]}],"doc\/modules\/density.rst":[{"add":["80","   >>> from sklearn.neighbors import KernelDensity"],"delete":["80","   >>> from sklearn.neighbors.kde import KernelDensity"]}],"sklearn\/neighbors\/tests\/test_neighbors_tree.py":[{"add":["8","from sklearn.neighbors import DistanceMetric","9","from sklearn.neighbors._ball_tree import BallTree","10","from sklearn.neighbors._kd_tree import KDTree"],"delete":["8","from sklearn.neighbors.dist_metrics import DistanceMetric","9","from sklearn.neighbors.ball_tree import BallTree","10","from sklearn.neighbors.kd_tree import KDTree"]}],"sklearn\/neighbors\/tests\/test_quad_tree.py":[{"add":["5","from sklearn.neighbors._quad_tree import _QuadTree"],"delete":["5","from sklearn.neighbors.quad_tree import _QuadTree"]}],"sklearn\/cluster\/tests\/test_hierarchical.py":[{"add":["30","from sklearn.neighbors import kneighbors_graph"],"delete":["30","from sklearn.neighbors.graph import kneighbors_graph"]}],"sklearn\/neighbors\/tests\/test_ball_tree.py":[{"add":["5","from sklearn.neighbors._ball_tree import (BallTree, NeighborsHeap,","6","                                          simultaneous_sort, kernel_norm,","7","                                          nodeheap_sort, DTYPE, ITYPE)","8","from sklearn.neighbors import DistanceMetric"],"delete":["5","from sklearn.neighbors.ball_tree import (BallTree, NeighborsHeap,","6","                                         simultaneous_sort, kernel_norm,","7","                                         nodeheap_sort, DTYPE, ITYPE)","8","from sklearn.neighbors.dist_metrics import DistanceMetric"]}]}},"bb208d3bf6e77a4408c00b5fb6a6b7ce674b8912":{"changes":{"doc\/whats_new\/v0.22.rst":"MODIFY","sklearn\/cross_decomposition\/pls_.py":"MODIFY","sklearn\/cross_decomposition\/tests\/test_pls.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.22.rst":[{"add":["389",":mod:`sklearn.cross_decomposition`","390","..................................","391","","392","- |Fix| Fixed a bug where :class:`cross_decomposition.PLSCanonical` and","393","  :class:`cross_decomposition.PLSRegression` were raising an error when fitted","394","  with a target matrix `Y` in which the first column was constant.","395","  :issue:`13609` by :user:`Camila Williamson <camilaagw>`.","406","  :pr:`13609` by :user:`Guillaume Lemaitre <glemaitre>`."],"delete":["399","  :pr:`14195` by :user:`Guillaume Lemaitre <glemaitre>`."]}],"sklearn\/cross_decomposition\/pls_.py":[{"add":["33","    for col in Y.T:","34","        if np.any(np.abs(col) > np.finfo(np.double).eps):","35","            y_score = col.reshape(len(col), 1)","36","            break","37",""],"delete":["33","    y_score = Y[:, [0]]"]}],"sklearn\/cross_decomposition\/tests\/test_pls.py":[{"add":["263","    # 4) Another \"Non regression test\" of PLS Regression (PLS2):","264","    #    Checking behavior when the first column of Y is constant","265","    # ===============================================","266","    # The results were compared against a modified version of plsreg2","267","    # from the R-package plsdepot","268","    X = d.data","269","    Y = d.target","270","    Y[:, 0] = 1","271","    pls_2 = pls_.PLSRegression(n_components=X.shape[1])","272","    pls_2.fit(X, Y)","273","","274","    x_weights = np.array(","275","        [[-0.6273573, 0.007081799, 0.7786994],","276","         [-0.7493417, -0.277612681, -0.6011807],","277","         [-0.2119194, 0.960666981, -0.1794690]])","278","    x_weights_sign_flip = pls_2.x_weights_ \/ x_weights","279","","280","    x_loadings = np.array(","281","        [[-0.6273512, -0.22464538, 0.7786994],","282","         [-0.6643156, -0.09871193, -0.6011807],","283","         [-0.5125877, 1.01407380, -0.1794690]])","284","    x_loadings_sign_flip = pls_2.x_loadings_ \/ x_loadings","285","","286","    y_loadings = np.array(","287","        [[0.0000000, 0.0000000, 0.0000000],","288","         [0.4357300, 0.5828479, 0.2174802],","289","         [-0.1353739, -0.2486423, -0.1810386]])","290","","291","    # R\/python sign flip should be the same in x_weight and x_rotation","292","    assert_array_almost_equal(x_loadings_sign_flip, x_weights_sign_flip, 4)","293","","294","    # This test that R \/ python give the same result up to column","295","    # sign indeterminacy","296","    assert_array_almost_equal(np.abs(x_loadings_sign_flip), 1, 4)","297","    assert_array_almost_equal(np.abs(x_weights_sign_flip), 1, 4)","298","","299","    # For the PLSRegression with default parameters, it holds that","300","    # y_loadings==y_weights. In this case we only test that R\/python","301","    # give the same result for the y_loadings irrespective of the sign","302","    assert_array_almost_equal(np.abs(pls_2.y_loadings_), np.abs(y_loadings), 4)","303",""],"delete":[]}]}},"ac72a4844ad7b9c170af546050eb9147ed8318bf":{"changes":{"doc\/whats_new\/v0.22.rst":"MODIFY","sklearn\/cluster\/mean_shift_.py":"MODIFY","sklearn\/cluster\/tests\/test_mean_shift.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.22.rst":[{"add":["91","- |Fix| :class:`~cluster.MeanShift` now accepts a :term:`max_iter` with a","92","  default value of 300 instead of always using the default 300. It also now","93","  exposes an ``n_iter_`` indicating the maximum number of iterations performed","94","  on each seed. :pr:`15120` by `Adrin Jalali`_.","95",""],"delete":[]}],"sklearn\/cluster\/mean_shift_.py":[{"add":["103","            break","105","    return tuple(my_mean), len(points_within), completed_iterations","181","    model = MeanShift(bandwidth=bandwidth, seeds=seeds,","182","                      min_bin_freq=min_bin_freq,","183","                      bin_seeding=bin_seeding,","184","                      cluster_all=cluster_all, n_jobs=n_jobs,","185","                      max_iter=max_iter).fit(X)","186","    return model.cluster_centers_, model.labels_","290","    max_iter : int, default=300","291","        Maximum number of iterations, per seed point before the clustering","292","        operation terminates (for that seed point), if has not converged yet.","293","","294","        .. versionadded:: 0.22","295","","304","    n_iter_ : int","305","        Maximum number of iterations performed on each seed.","306","","307","        .. versionadded:: 0.22","308","","349","                 min_bin_freq=1, cluster_all=True, n_jobs=None, max_iter=300):","356","        self.max_iter = max_iter","370","        bandwidth = self.bandwidth","371","        if bandwidth is None:","372","            bandwidth = estimate_bandwidth(X, n_jobs=self.n_jobs)","373","        elif bandwidth <= 0:","374","            raise ValueError(\"bandwidth needs to be greater than zero or None,\"","375","                             \" got %f\" % bandwidth)","376","","377","        seeds = self.seeds","378","        if seeds is None:","379","            if self.bin_seeding:","380","                seeds = get_bin_seeds(X, bandwidth, self.min_bin_freq)","381","            else:","382","                seeds = X","383","        n_samples, n_features = X.shape","384","        center_intensity_dict = {}","385","","386","        # We use n_jobs=1 because this will be used in nested calls under","387","        # parallel calls to _mean_shift_single_seed so there is no need for","388","        # for further parallelism.","389","        nbrs = NearestNeighbors(radius=bandwidth, n_jobs=1).fit(X)","390","","391","        # execute iterations on all seeds in parallel","392","        all_res = Parallel(n_jobs=self.n_jobs)(","393","            delayed(_mean_shift_single_seed)","394","            (seed, X, nbrs, self.max_iter) for seed in seeds)","395","        # copy results in a dictionary","396","        for i in range(len(seeds)):","397","            if all_res[i][1]:  # i.e. len(points_within) > 0","398","                center_intensity_dict[all_res[i][0]] = all_res[i][1]","399","","400","        self.n_iter_ = max([x[2] for x in all_res])","401","","402","        if not center_intensity_dict:","403","            # nothing near seeds","404","            raise ValueError(\"No point was within bandwidth=%f of any seed.\"","405","                             \" Try a different seeding strategy \\","406","                             or increase the bandwidth.\"","407","                             % bandwidth)","408","","409","        # POST PROCESSING: remove near duplicate points","410","        # If the distance between two kernels is less than the bandwidth,","411","        # then we have to remove one because it is a duplicate. Remove the","412","        # one with fewer points.","413","","414","        sorted_by_intensity = sorted(center_intensity_dict.items(),","415","                                     key=lambda tup: (tup[1], tup[0]),","416","                                     reverse=True)","417","        sorted_centers = np.array([tup[0] for tup in sorted_by_intensity])","418","        unique = np.ones(len(sorted_centers), dtype=np.bool)","419","        nbrs = NearestNeighbors(radius=bandwidth,","420","                                n_jobs=self.n_jobs).fit(sorted_centers)","421","        for i, center in enumerate(sorted_centers):","422","            if unique[i]:","423","                neighbor_idxs = nbrs.radius_neighbors([center],","424","                                                      return_distance=False)[0]","425","                unique[neighbor_idxs] = 0","426","                unique[i] = 1  # leave the current point as unique","427","        cluster_centers = sorted_centers[unique]","428","","429","        # ASSIGN LABELS: a point belongs to the cluster that it is closest to","430","        nbrs = NearestNeighbors(n_neighbors=1,","431","                                n_jobs=self.n_jobs).fit(cluster_centers)","432","        labels = np.zeros(n_samples, dtype=np.int)","433","        distances, idxs = nbrs.kneighbors(X)","434","        if self.cluster_all:","435","            labels = idxs.flatten()","436","        else:","437","            labels.fill(-1)","438","            bool_selector = distances.flatten() <= bandwidth","439","            labels[bool_selector] = idxs.flatten()[bool_selector]","440","","441","        self.cluster_centers_, self.labels_ = cluster_centers, labels"],"delete":["103","            return tuple(my_mean), len(points_within)","180","","181","    if bandwidth is None:","182","        bandwidth = estimate_bandwidth(X, n_jobs=n_jobs)","183","    elif bandwidth <= 0:","184","        raise ValueError(\"bandwidth needs to be greater than zero or None,\"","185","                         \" got %f\" % bandwidth)","186","    if seeds is None:","187","        if bin_seeding:","188","            seeds = get_bin_seeds(X, bandwidth, min_bin_freq)","189","        else:","190","            seeds = X","191","    n_samples, n_features = X.shape","192","    center_intensity_dict = {}","193","","194","    # We use n_jobs=1 because this will be used in nested calls under","195","    # parallel calls to _mean_shift_single_seed so there is no need for","196","    # for further parallelism.","197","    nbrs = NearestNeighbors(radius=bandwidth, n_jobs=1).fit(X)","198","","199","    # execute iterations on all seeds in parallel","200","    all_res = Parallel(n_jobs=n_jobs)(","201","        delayed(_mean_shift_single_seed)","202","        (seed, X, nbrs, max_iter) for seed in seeds)","203","    # copy results in a dictionary","204","    for i in range(len(seeds)):","205","        if all_res[i] is not None:","206","            center_intensity_dict[all_res[i][0]] = all_res[i][1]","207","","208","    if not center_intensity_dict:","209","        # nothing near seeds","210","        raise ValueError(\"No point was within bandwidth=%f of any seed.\"","211","                         \" Try a different seeding strategy \\","212","                         or increase the bandwidth.\"","213","                         % bandwidth)","214","","215","    # POST PROCESSING: remove near duplicate points","216","    # If the distance between two kernels is less than the bandwidth,","217","    # then we have to remove one because it is a duplicate. Remove the","218","    # one with fewer points.","219","","220","    sorted_by_intensity = sorted(center_intensity_dict.items(),","221","                                 key=lambda tup: (tup[1], tup[0]),","222","                                 reverse=True)","223","    sorted_centers = np.array([tup[0] for tup in sorted_by_intensity])","224","    unique = np.ones(len(sorted_centers), dtype=np.bool)","225","    nbrs = NearestNeighbors(radius=bandwidth,","226","                            n_jobs=n_jobs).fit(sorted_centers)","227","    for i, center in enumerate(sorted_centers):","228","        if unique[i]:","229","            neighbor_idxs = nbrs.radius_neighbors([center],","230","                                                  return_distance=False)[0]","231","            unique[neighbor_idxs] = 0","232","            unique[i] = 1  # leave the current point as unique","233","    cluster_centers = sorted_centers[unique]","234","","235","    # ASSIGN LABELS: a point belongs to the cluster that it is closest to","236","    nbrs = NearestNeighbors(n_neighbors=1, n_jobs=n_jobs).fit(cluster_centers)","237","    labels = np.zeros(n_samples, dtype=np.int)","238","    distances, idxs = nbrs.kneighbors(X)","239","    if cluster_all:","240","        labels = idxs.flatten()","241","    else:","242","        labels.fill(-1)","243","        bool_selector = distances.flatten() <= bandwidth","244","        labels[bool_selector] = idxs.flatten()[bool_selector]","245","    return cluster_centers, labels","397","                 min_bin_freq=1, cluster_all=True, n_jobs=None):","417","        self.cluster_centers_, self.labels_ = \\","418","            mean_shift(X, bandwidth=self.bandwidth, seeds=self.seeds,","419","                       min_bin_freq=self.min_bin_freq,","420","                       bin_seeding=self.bin_seeding,","421","                       cluster_all=self.cluster_all, n_jobs=self.n_jobs)"]}],"sklearn\/cluster\/tests\/test_mean_shift.py":[{"add":["157","","158","","159","@pytest.mark.parametrize('max_iter', [1, 100])","160","def test_max_iter(max_iter):","161","    clusters1, _ = mean_shift(X, max_iter=max_iter)","162","    ms = MeanShift(max_iter=max_iter).fit(X)","163","    clusters2 = ms.cluster_centers_","164","","165","    assert ms.n_iter_ <= ms.max_iter","166","    assert len(clusters1) == len(clusters2)","167","","168","    for c1, c2 in zip(clusters1, clusters2):","169","        assert np.allclose(c1, c2)"],"delete":[]}]}},"03ea20db0f9585fa0d44f4d3cae4b4c4a7c7f235":{"changes":{"sklearn\/gaussian_process\/gpc.py":"MODIFY","sklearn\/compose\/tests\/test_target.py":"MODIFY","sklearn\/covariance\/elliptic_envelope.py":"MODIFY","sklearn\/naive_bayes.py":"MODIFY","sklearn\/impute\/_iterative.py":"MODIFY","sklearn\/preprocessing\/_encoders.py":"MODIFY","sklearn\/ensemble\/tests\/test_voting.py":"MODIFY","sklearn\/random_projection.py":"MODIFY","sklearn\/gaussian_process\/gpr.py":"MODIFY","sklearn\/utils\/mocking.py":"MODIFY","sklearn\/multioutput.py":"MODIFY","sklearn\/neural_network\/rbm.py":"MODIFY","sklearn\/tests\/test_base.py":"MODIFY","sklearn\/svm\/classes.py":"MODIFY","examples\/compose\/plot_column_transformer.py":"MODIFY","sklearn\/semi_supervised\/label_propagation.py":"MODIFY","sklearn\/ensemble\/base.py":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/binning.py":"MODIFY","sklearn\/feature_extraction\/dict_vectorizer.py":"MODIFY","sklearn\/linear_model\/bayes.py":"MODIFY","sklearn\/model_selection\/_search.py":"MODIFY","sklearn\/linear_model\/ransac.py":"MODIFY","sklearn\/cluster\/hierarchical.py":"MODIFY","sklearn\/tree\/tree.py":"MODIFY","sklearn\/discriminant_analysis.py":"MODIFY","sklearn\/neighbors\/nearest_centroid.py":"MODIFY","sklearn\/linear_model\/omp.py":"MODIFY","sklearn\/compose\/_target.py":"MODIFY","sklearn\/feature_selection\/univariate_selection.py":"MODIFY","sklearn\/cluster\/mean_shift_.py":"MODIFY","sklearn\/ensemble\/forest.py":"MODIFY","sklearn\/feature_selection\/variance_threshold.py":"MODIFY","sklearn\/ensemble\/iforest.py":"MODIFY","sklearn\/neighbors\/base.py":"MODIFY","sklearn\/pipeline.py":"MODIFY","sklearn\/svm\/base.py":"MODIFY","sklearn\/multiclass.py":"MODIFY","sklearn\/cluster\/dbscan_.py":"MODIFY","sklearn\/ensemble\/weight_boosting.py":"MODIFY","sklearn\/compose\/_column_transformer.py":"MODIFY","sklearn\/kernel_ridge.py":"MODIFY","sklearn\/feature_extraction\/text.py":"MODIFY","sklearn\/base.py":"MODIFY","sklearn\/isotonic.py":"MODIFY","sklearn\/decomposition\/factor_analysis.py":"MODIFY","sklearn\/linear_model\/coordinate_descent.py":"MODIFY","sklearn\/neighbors\/nca.py":"MODIFY","sklearn\/decomposition\/online_lda.py":"MODIFY","sklearn\/impute\/_base.py":"MODIFY","sklearn\/cluster\/spectral.py":"MODIFY","sklearn\/decomposition\/nmf.py":"MODIFY","sklearn\/utils\/tests\/test_estimator_checks.py":"MODIFY","sklearn\/utils\/tests\/test_pprint.py":"MODIFY","sklearn\/calibration.py":"MODIFY","sklearn\/decomposition\/fastica_.py":"MODIFY","sklearn\/kernel_approximation.py":"MODIFY","sklearn\/neural_network\/multilayer_perceptron.py":"MODIFY","sklearn\/feature_selection\/from_model.py":"MODIFY","sklearn\/cluster\/optics_.py":"MODIFY","sklearn\/decomposition\/base.py":"MODIFY","sklearn\/ensemble\/voting.py":"MODIFY","sklearn\/linear_model\/ridge.py":"MODIFY","sklearn\/preprocessing\/_discretization.py":"MODIFY","sklearn\/feature_extraction\/hashing.py":"MODIFY","sklearn\/feature_selection\/rfe.py":"MODIFY","sklearn\/linear_model\/theil_sen.py":"MODIFY","sklearn\/cluster\/affinity_propagation_.py":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/gradient_boosting.py":"MODIFY","sklearn\/decomposition\/truncated_svd.py":"MODIFY","sklearn\/linear_model\/base.py":"MODIFY","sklearn\/preprocessing\/data.py":"MODIFY","sklearn\/cluster\/tests\/test_bicluster.py":"MODIFY","sklearn\/preprocessing\/label.py":"MODIFY","sklearn\/cluster\/bicluster.py":"MODIFY","sklearn\/ensemble\/bagging.py":"MODIFY","sklearn\/manifold\/isomap.py":"MODIFY","sklearn\/linear_model\/stochastic_gradient.py":"MODIFY","sklearn\/decomposition\/dict_learning.py":"MODIFY","sklearn\/ensemble\/gradient_boosting.py":"MODIFY","sklearn\/dummy.py":"MODIFY","sklearn\/decomposition\/sparse_pca.py":"MODIFY","sklearn\/manifold\/locally_linear.py":"MODIFY","sklearn\/preprocessing\/_function_transformer.py":"MODIFY","sklearn\/cross_decomposition\/cca_.py":"MODIFY","sklearn\/cross_decomposition\/pls_.py":"MODIFY","sklearn\/decomposition\/kernel_pca.py":"MODIFY","sklearn\/cluster\/birch.py":"MODIFY","sklearn\/cluster\/k_means_.py":"MODIFY","sklearn\/inspection\/tests\/test_partial_dependence.py":"MODIFY","sklearn\/linear_model\/least_angle.py":"MODIFY"},"diff":{"sklearn\/gaussian_process\/gpc.py":[{"add":["451","class GaussianProcessClassifier(ClassifierMixin, BaseEstimator):"],"delete":["451","class GaussianProcessClassifier(BaseEstimator, ClassifierMixin):"]}],"sklearn\/compose\/tests\/test_target.py":[{"add":["228","class DummyCheckerArrayTransformer(TransformerMixin, BaseEstimator):","270","class DummyTransformer(TransformerMixin, BaseEstimator):"],"delete":["228","class DummyCheckerArrayTransformer(BaseEstimator, TransformerMixin):","270","class DummyTransformer(BaseEstimator, TransformerMixin):"]}],"sklearn\/covariance\/elliptic_envelope.py":[{"add":["11","class EllipticEnvelope(OutlierMixin, MinCovDet):"],"delete":["11","class EllipticEnvelope(MinCovDet, OutlierMixin):"]}],"sklearn\/naive_bayes.py":[{"add":["37","class BaseNB(ClassifierMixin, BaseEstimator, metaclass=ABCMeta):"],"delete":["37","class BaseNB(BaseEstimator, ClassifierMixin, metaclass=ABCMeta):"]}],"sklearn\/impute\/_iterative.py":[{"add":["27","class IterativeImputer(TransformerMixin, BaseEstimator):"],"delete":["27","class IterativeImputer(BaseEstimator, TransformerMixin):"]}],"sklearn\/preprocessing\/_encoders.py":[{"add":["21","class _BaseEncoder(TransformerMixin, BaseEstimator):"],"delete":["21","class _BaseEncoder(BaseEstimator, TransformerMixin):"]}],"sklearn\/ensemble\/tests\/test_voting.py":[{"add":["335","    class ClassifierErrorFit(ClassifierMixin, BaseEstimator):","345","    class MockClassifier(ClassifierMixin, BaseEstimator):"],"delete":["335","    class ClassifierErrorFit(BaseEstimator, ClassifierMixin):","345","    class MockClassifier(BaseEstimator, ClassifierMixin):"]}],"sklearn\/random_projection.py":[{"add":["291","class BaseRandomProjection(TransformerMixin, BaseEstimator, metaclass=ABCMeta):"],"delete":["291","class BaseRandomProjection(BaseEstimator, TransformerMixin, metaclass=ABCMeta):"]}],"sklearn\/gaussian_process\/gpr.py":[{"add":["21","class GaussianProcessRegressor(MultiOutputMixin,","22","                               RegressorMixin, BaseEstimator):"],"delete":["21","class GaussianProcessRegressor(BaseEstimator, RegressorMixin,","22","                               MultiOutputMixin):"]}],"sklearn\/utils\/mocking.py":[{"add":["50","class CheckingClassifier(ClassifierMixin, BaseEstimator):"],"delete":["50","class CheckingClassifier(BaseEstimator, ClassifierMixin):"]}],"sklearn\/multioutput.py":[{"add":["204","class MultiOutputRegressor(RegressorMixin, MultiOutputEstimator):","299","class MultiOutputClassifier(ClassifierMixin, MultiOutputEstimator):","517","class ClassifierChain(MetaEstimatorMixin, ClassifierMixin, _BaseChain):","677","class RegressorChain(MetaEstimatorMixin, RegressorMixin, _BaseChain):"],"delete":["204","class MultiOutputRegressor(MultiOutputEstimator, RegressorMixin):","299","class MultiOutputClassifier(MultiOutputEstimator, ClassifierMixin):","517","class ClassifierChain(_BaseChain, ClassifierMixin, MetaEstimatorMixin):","677","class RegressorChain(_BaseChain, RegressorMixin, MetaEstimatorMixin):"]}],"sklearn\/neural_network\/rbm.py":[{"add":["25","class BernoulliRBM(TransformerMixin, BaseEstimator):"],"delete":["25","class BernoulliRBM(BaseEstimator, TransformerMixin):"]}],"sklearn\/tests\/test_base.py":[{"add":["65","    def _more_tags(self):","66","        return dict()","67","","68","","69","class InheritDiamondOverwriteTag(DiamondOverwriteTag):","300","    class DummyEstimator(TransformerMixin, BaseEstimator):","415","class MultiInheritanceEstimator(DontPickleAttributeMixin, BaseEstimator):","482","    redefine_tags_est = OverrideTag()","483","    assert not redefine_tags_est._get_tags()['allow_nan']","486","    assert diamond_tag_est._get_tags()['allow_nan']","487","","488","    inherit_diamond_tag_est = InheritDiamondOverwriteTag()","489","    assert inherit_diamond_tag_est._get_tags()['allow_nan']"],"delete":["295","    class DummyEstimator(BaseEstimator, TransformerMixin):","410","class MultiInheritanceEstimator(BaseEstimator, DontPickleAttributeMixin):","477","    invalid_tags_est = OverrideTag()","478","    with pytest.raises(TypeError, match=\"Inconsistent values for tag\"):","479","        invalid_tags_est._get_tags()","482","    with pytest.raises(TypeError, match=\"Inconsistent values for tag\"):","483","        diamond_tag_est._get_tags()"]}],"sklearn\/svm\/classes.py":[{"add":["253","class LinearSVR(RegressorMixin, LinearModel):","826","class SVR(RegressorMixin, BaseLibSVM):","958","class NuSVR(RegressorMixin, BaseLibSVM):","1083","class OneClassSVM(OutlierMixin, BaseLibSVM):"],"delete":["253","class LinearSVR(LinearModel, RegressorMixin):","826","class SVR(BaseLibSVM, RegressorMixin):","958","class NuSVR(BaseLibSVM, RegressorMixin):","1083","class OneClassSVM(BaseLibSVM, OutlierMixin):"]}],"examples\/compose\/plot_column_transformer.py":[{"add":["44","class TextStats(TransformerMixin, BaseEstimator):","56","class SubjectBodyExtractor(TransformerMixin, BaseEstimator):"],"delete":["44","class TextStats(BaseEstimator, TransformerMixin):","56","class SubjectBodyExtractor(BaseEstimator, TransformerMixin):"]}],"sklearn\/semi_supervised\/label_propagation.py":[{"add":["73","class BaseLabelPropagation(ClassifierMixin, BaseEstimator, metaclass=ABCMeta):"],"delete":["73","class BaseLabelPropagation(BaseEstimator, ClassifierMixin, metaclass=ABCMeta):"]}],"sklearn\/ensemble\/base.py":[{"add":["60","class BaseEnsemble(MetaEstimatorMixin, BaseEstimator, metaclass=ABCMeta):"],"delete":["60","class BaseEnsemble(BaseEstimator, MetaEstimatorMixin, metaclass=ABCMeta):"]}],"sklearn\/ensemble\/_hist_gradient_boosting\/binning.py":[{"add":["85","class _BinMapper(TransformerMixin, BaseEstimator):"],"delete":["85","class _BinMapper(BaseEstimator, TransformerMixin):"]}],"sklearn\/feature_extraction\/dict_vectorizer.py":[{"add":["23","class DictVectorizer(TransformerMixin, BaseEstimator):"],"delete":["23","class DictVectorizer(BaseEstimator, TransformerMixin):"]}],"sklearn\/linear_model\/bayes.py":[{"add":["21","class BayesianRidge(RegressorMixin, LinearModel):","377","class ARDRegression(RegressorMixin, LinearModel):"],"delete":["21","class BayesianRidge(LinearModel, RegressorMixin):","377","class ARDRegression(LinearModel, RegressorMixin):"]}],"sklearn\/model_selection\/_search.py":[{"add":["399","class BaseSearchCV(MetaEstimatorMixin, BaseEstimator, metaclass=ABCMeta):"],"delete":["399","class BaseSearchCV(BaseEstimator, MetaEstimatorMixin, metaclass=ABCMeta):"]}],"sklearn\/linear_model\/ransac.py":[{"add":["55","class RANSACRegressor(MetaEstimatorMixin, RegressorMixin,","56","                      MultiOutputMixin, BaseEstimator):"],"delete":["55","class RANSACRegressor(BaseEstimator, MetaEstimatorMixin, RegressorMixin,","56","                      MultiOutputMixin):"]}],"sklearn\/cluster\/hierarchical.py":[{"add":["654","class AgglomerativeClustering(ClusterMixin, BaseEstimator):"],"delete":["654","class AgglomerativeClustering(BaseEstimator, ClusterMixin):"]}],"sklearn\/tree\/tree.py":[{"add":["75","class BaseDecisionTree(MultiOutputMixin, BaseEstimator, metaclass=ABCMeta):","599","class DecisionTreeClassifier(ClassifierMixin, BaseDecisionTree):","974","class DecisionTreeRegressor(RegressorMixin, BaseDecisionTree):"],"delete":["75","class BaseDecisionTree(BaseEstimator, MultiOutputMixin, metaclass=ABCMeta):","599","class DecisionTreeClassifier(BaseDecisionTree, ClassifierMixin):","974","class DecisionTreeRegressor(BaseDecisionTree, RegressorMixin):"]}],"sklearn\/discriminant_analysis.py":[{"add":["555","class QuadraticDiscriminantAnalysis(ClassifierMixin, BaseEstimator):"],"delete":["555","class QuadraticDiscriminantAnalysis(BaseEstimator, ClassifierMixin):"]}],"sklearn\/neighbors\/nearest_centroid.py":[{"add":["22","class NearestCentroid(ClassifierMixin, BaseEstimator):"],"delete":["22","class NearestCentroid(BaseEstimator, ClassifierMixin):"]}],"sklearn\/linear_model\/omp.py":[{"add":["541","class OrthogonalMatchingPursuit(MultiOutputMixin, RegressorMixin, LinearModel):","755","class OrthogonalMatchingPursuitCV(RegressorMixin, LinearModel):"],"delete":["541","class OrthogonalMatchingPursuit(LinearModel, RegressorMixin, MultiOutputMixin):","755","class OrthogonalMatchingPursuitCV(LinearModel, RegressorMixin):"]}],"sklearn\/compose\/_target.py":[{"add":["16","class TransformedTargetRegressor(RegressorMixin, BaseEstimator):"],"delete":["16","class TransformedTargetRegressor(BaseEstimator, RegressorMixin):"]}],"sklearn\/feature_selection\/univariate_selection.py":[{"add":["311","class _BaseFilter(SelectorMixin, BaseEstimator):"],"delete":["311","class _BaseFilter(BaseEstimator, SelectorMixin):"]}],"sklearn\/cluster\/mean_shift_.py":[{"add":["295","class MeanShift(ClusterMixin, BaseEstimator):"],"delete":["295","class MeanShift(BaseEstimator, ClusterMixin):"]}],"sklearn\/ensemble\/forest.py":[{"add":["124","class BaseForest(MultiOutputMixin, BaseEnsemble, metaclass=ABCMeta):","394","class ForestClassifier(ClassifierMixin, BaseForest, metaclass=ABCMeta):","635","class ForestRegressor(RegressorMixin, BaseForest, metaclass=ABCMeta):"],"delete":["124","class BaseForest(BaseEnsemble, MultiOutputMixin, metaclass=ABCMeta):","394","class ForestClassifier(BaseForest, ClassifierMixin, metaclass=ABCMeta):","635","class ForestRegressor(BaseForest, RegressorMixin, metaclass=ABCMeta):"]}],"sklearn\/feature_selection\/variance_threshold.py":[{"add":["11","class VarianceThreshold(SelectorMixin, BaseEstimator):"],"delete":["11","class VarianceThreshold(BaseEstimator, SelectorMixin):"]}],"sklearn\/ensemble\/iforest.py":[{"add":["26","class IsolationForest(OutlierMixin, BaseBagging):"],"delete":["26","class IsolationForest(BaseBagging, OutlierMixin):"]}],"sklearn\/neighbors\/base.py":[{"add":["105","class NeighborsBase(MultiOutputMixin, BaseEstimator, metaclass=ABCMeta):"],"delete":["105","class NeighborsBase(BaseEstimator, MultiOutputMixin, metaclass=ABCMeta):"]}],"sklearn\/pipeline.py":[{"add":["746","class FeatureUnion(TransformerMixin, _BaseComposition):"],"delete":["746","class FeatureUnion(_BaseComposition, TransformerMixin):"]}],"sklearn\/svm\/base.py":[{"add":["487","class BaseSVC(ClassifierMixin, BaseLibSVM, metaclass=ABCMeta):"],"delete":["487","class BaseSVC(BaseLibSVM, ClassifierMixin, metaclass=ABCMeta):"]}],"sklearn\/multiclass.py":[{"add":["131","class OneVsRestClassifier(MultiOutputMixin, ClassifierMixin,","132","                          MetaEstimatorMixin, BaseEstimator):","437","class OneVsOneClassifier(MetaEstimatorMixin, ClassifierMixin, BaseEstimator):","564","                                             (combinations)))","636","class OutputCodeClassifier(MetaEstimatorMixin, ClassifierMixin, BaseEstimator):"],"delete":["131","class OneVsRestClassifier(BaseEstimator, ClassifierMixin, MetaEstimatorMixin,","132","                          MultiOutputMixin):","437","class OneVsOneClassifier(BaseEstimator, ClassifierMixin, MetaEstimatorMixin):","564","                                              (combinations)))","636","class OutputCodeClassifier(BaseEstimator, ClassifierMixin, MetaEstimatorMixin):"]}],"sklearn\/cluster\/dbscan_.py":[{"add":["192","class DBSCAN(ClusterMixin, BaseEstimator):"],"delete":["192","class DBSCAN(BaseEstimator, ClusterMixin):"]}],"sklearn\/ensemble\/weight_boosting.py":[{"add":["292","class AdaBoostClassifier(ClassifierMixin, BaseWeightBoosting):","856","class AdaBoostRegressor(RegressorMixin, BaseWeightBoosting):"],"delete":["292","class AdaBoostClassifier(BaseWeightBoosting, ClassifierMixin):","856","class AdaBoostRegressor(BaseWeightBoosting, RegressorMixin):"]}],"sklearn\/compose\/_column_transformer.py":[{"add":["35","class ColumnTransformer(TransformerMixin, _BaseComposition):"],"delete":["35","class ColumnTransformer(_BaseComposition, TransformerMixin):"]}],"sklearn\/kernel_ridge.py":[{"add":["15","class KernelRidge(MultiOutputMixin, RegressorMixin, BaseEstimator):"],"delete":["15","class KernelRidge(BaseEstimator, RegressorMixin, MultiOutputMixin):"]}],"sklearn\/feature_extraction\/text.py":[{"add":["471","class HashingVectorizer(TransformerMixin, VectorizerMixin, BaseEstimator):","755","class CountVectorizer(VectorizerMixin, BaseEstimator):","1231","class TfidfTransformer(TransformerMixin, BaseEstimator):"],"delete":["471","class HashingVectorizer(BaseEstimator, VectorizerMixin, TransformerMixin):","755","class CountVectorizer(BaseEstimator, VectorizerMixin):","1231","class TfidfTransformer(BaseEstimator, TransformerMixin):"]}],"sklearn\/base.py":[{"add":["311","    def _more_tags(self):","312","        return _DEFAULT_TAGS","313","","316","        for base_class in reversed(inspect.getmro(self.__class__)):","317","            if hasattr(base_class, '_more_tags'):","318","                # need the if because mixins might not have _more_tags","319","                # but might do redundant work in estimators","320","                # (i.e. calling more tags on BaseEstimator multiple times)","322","                collected_tags.update(more_tags)","323","        return collected_tags"],"delete":["131","def _update_if_consistent(dict1, dict2):","132","    common_keys = set(dict1.keys()).intersection(dict2.keys())","133","    for key in common_keys:","134","        if dict1[key] != dict2[key]:","135","            raise TypeError(\"Inconsistent values for tag {}: {} != {}\".format(","136","                key, dict1[key], dict2[key]","137","            ))","138","    dict1.update(dict2)","139","    return dict1","140","","141","","324","        for base_class in inspect.getmro(self.__class__):","325","            if (hasattr(base_class, '_more_tags')","326","                    and base_class != self.__class__):","328","                collected_tags = _update_if_consistent(collected_tags,","329","                                                       more_tags)","330","        if hasattr(self, '_more_tags'):","331","            more_tags = self._more_tags()","332","            collected_tags = _update_if_consistent(collected_tags, more_tags)","333","        tags = _DEFAULT_TAGS.copy()","334","        tags.update(collected_tags)","335","        return tags"]}],"sklearn\/isotonic.py":[{"add":["139","class IsotonicRegression(RegressorMixin, TransformerMixin, BaseEstimator):"],"delete":["139","class IsotonicRegression(BaseEstimator, TransformerMixin, RegressorMixin):"]}],"sklearn\/decomposition\/factor_analysis.py":[{"add":["34","class FactorAnalysis(TransformerMixin, BaseEstimator):"],"delete":["34","class FactorAnalysis(BaseEstimator, TransformerMixin):"]}],"sklearn\/linear_model\/coordinate_descent.py":[{"add":["500","class ElasticNet(MultiOutputMixin, RegressorMixin, LinearModel):","1037","class LinearModelCV(MultiOutputMixin, LinearModel, metaclass=ABCMeta):","1233","class LassoCV(RegressorMixin, LinearModelCV):","1399","class ElasticNetCV(RegressorMixin, LinearModelCV):","1919","class MultiTaskElasticNetCV(RegressorMixin, LinearModelCV):","2107","class MultiTaskLassoCV(RegressorMixin, LinearModelCV):"],"delete":["500","class ElasticNet(LinearModel, RegressorMixin, MultiOutputMixin):","1037","class LinearModelCV(LinearModel, MultiOutputMixin, metaclass=ABCMeta):","1233","class LassoCV(LinearModelCV, RegressorMixin):","1399","class ElasticNetCV(LinearModelCV, RegressorMixin):","1919","class MultiTaskElasticNetCV(LinearModelCV, RegressorMixin):","2107","class MultiTaskLassoCV(LinearModelCV, RegressorMixin):"]}],"sklearn\/neighbors\/nca.py":[{"add":["29","class NeighborhoodComponentsAnalysis(TransformerMixin, BaseEstimator):"],"delete":["29","class NeighborhoodComponentsAnalysis(BaseEstimator, TransformerMixin):"]}],"sklearn\/decomposition\/online_lda.py":[{"add":["134","class LatentDirichletAllocation(TransformerMixin, BaseEstimator):"],"delete":["134","class LatentDirichletAllocation(BaseEstimator, TransformerMixin):"]}],"sklearn\/impute\/_base.py":[{"add":["66","class SimpleImputer(TransformerMixin, BaseEstimator):","418","class MissingIndicator(TransformerMixin, BaseEstimator):"],"delete":["66","class SimpleImputer(BaseEstimator, TransformerMixin):","418","class MissingIndicator(BaseEstimator, TransformerMixin):"]}],"sklearn\/cluster\/spectral.py":[{"add":["274","class SpectralClustering(ClusterMixin, BaseEstimator):"],"delete":["274","class SpectralClustering(BaseEstimator, ClusterMixin):"]}],"sklearn\/decomposition\/nmf.py":[{"add":["1070","class NMF(TransformerMixin, BaseEstimator):"],"delete":["1070","class NMF(BaseEstimator, TransformerMixin):"]}],"sklearn\/utils\/tests\/test_estimator_checks.py":[{"add":["45","class BaseBadClassifier(ClassifierMixin, BaseEstimator):"],"delete":["45","class BaseBadClassifier(BaseEstimator, ClassifierMixin):"]}],"sklearn\/utils\/tests\/test_pprint.py":[{"add":["43","class StandardScaler(TransformerMixin, BaseEstimator):"],"delete":["43","class StandardScaler(BaseEstimator, TransformerMixin):"]}],"sklearn\/calibration.py":[{"add":["467","class _SigmoidCalibration(RegressorMixin, BaseEstimator):"],"delete":["467","class _SigmoidCalibration(BaseEstimator, RegressorMixin):"]}],"sklearn\/decomposition\/fastica_.py":[{"add":["382","class FastICA(TransformerMixin, BaseEstimator):"],"delete":["382","class FastICA(BaseEstimator, TransformerMixin):"]}],"sklearn\/kernel_approximation.py":[{"add":["23","class RBFSampler(TransformerMixin, BaseEstimator):","127","class SkewedChi2Sampler(TransformerMixin, BaseEstimator):","241","class AdditiveChi2Sampler(TransformerMixin, BaseEstimator):","431","class Nystroem(TransformerMixin, BaseEstimator):"],"delete":["23","class RBFSampler(BaseEstimator, TransformerMixin):","127","class SkewedChi2Sampler(BaseEstimator, TransformerMixin):","241","class AdditiveChi2Sampler(BaseEstimator, TransformerMixin):","431","class Nystroem(BaseEstimator, TransformerMixin):"]}],"sklearn\/neural_network\/multilayer_perceptron.py":[{"add":["690","class MLPClassifier(ClassifierMixin, BaseMultilayerPerceptron):","1082","class MLPRegressor(RegressorMixin, BaseMultilayerPerceptron):"],"delete":["690","class MLPClassifier(BaseMultilayerPerceptron, ClassifierMixin):","1082","class MLPRegressor(BaseMultilayerPerceptron, RegressorMixin):"]}],"sklearn\/feature_selection\/from_model.py":[{"add":["80","class SelectFromModel(MetaEstimatorMixin, SelectorMixin, BaseEstimator):"],"delete":["80","class SelectFromModel(BaseEstimator, SelectorMixin, MetaEstimatorMixin):"]}],"sklearn\/cluster\/optics_.py":[{"add":["23","class OPTICS(ClusterMixin, BaseEstimator):"],"delete":["23","class OPTICS(BaseEstimator, ClusterMixin):"]}],"sklearn\/decomposition\/base.py":[{"add":["19","class _BasePCA(TransformerMixin, BaseEstimator, metaclass=ABCMeta):"],"delete":["19","class _BasePCA(BaseEstimator, TransformerMixin, metaclass=ABCMeta):"]}],"sklearn\/ensemble\/voting.py":[{"add":["50","class _BaseVoting(TransformerMixin, _BaseComposition):","147","class VotingClassifier(ClassifierMixin, _BaseVoting):","377","class VotingRegressor(RegressorMixin, _BaseVoting):"],"delete":["50","class _BaseVoting(_BaseComposition, TransformerMixin):","147","class VotingClassifier(_BaseVoting, ClassifierMixin):","377","class VotingRegressor(_BaseVoting, RegressorMixin):"]}],"sklearn\/linear_model\/ridge.py":[{"add":["523","class _BaseRidge(MultiOutputMixin, LinearModel, metaclass=ABCMeta):","604","class Ridge(RegressorMixin, _BaseRidge):","1507","class _BaseRidgeCV(MultiOutputMixin, LinearModel):","1579","class RidgeCV(RegressorMixin, _BaseRidgeCV):"],"delete":["523","class _BaseRidge(LinearModel, MultiOutputMixin, metaclass=ABCMeta):","604","class Ridge(_BaseRidge, RegressorMixin):","1507","class _BaseRidgeCV(LinearModel, MultiOutputMixin):","1579","class RidgeCV(_BaseRidgeCV, RegressorMixin):"]}],"sklearn\/preprocessing\/_discretization.py":[{"add":["20","class KBinsDiscretizer(TransformerMixin, BaseEstimator):"],"delete":["20","class KBinsDiscretizer(BaseEstimator, TransformerMixin):"]}],"sklearn\/feature_extraction\/hashing.py":[{"add":["26","class FeatureHasher(TransformerMixin, BaseEstimator):"],"delete":["26","class FeatureHasher(BaseEstimator, TransformerMixin):"]}],"sklearn\/feature_selection\/rfe.py":[{"add":["36","class RFE(SelectorMixin, MetaEstimatorMixin, BaseEstimator):"],"delete":["36","class RFE(BaseEstimator, MetaEstimatorMixin, SelectorMixin):"]}],"sklearn\/linear_model\/theil_sen.py":[{"add":["195","class TheilSenRegressor(RegressorMixin, LinearModel):"],"delete":["195","class TheilSenRegressor(LinearModel, RegressorMixin):"]}],"sklearn\/cluster\/affinity_propagation_.py":[{"add":["235","class AffinityPropagation(ClusterMixin, BaseEstimator):"],"delete":["235","class AffinityPropagation(BaseEstimator, ClusterMixin):"]}],"sklearn\/ensemble\/_hist_gradient_boosting\/gradient_boosting.py":[{"add":["641","class HistGradientBoostingRegressor(RegressorMixin, BaseHistGradientBoosting):"],"delete":["641","class HistGradientBoostingRegressor(BaseHistGradientBoosting, RegressorMixin):"]}],"sklearn\/decomposition\/truncated_svd.py":[{"add":["20","class TruncatedSVD(TransformerMixin, BaseEstimator):"],"delete":["20","class TruncatedSVD(BaseEstimator, TransformerMixin):"]}],"sklearn\/linear_model\/base.py":[{"add":["364","class LinearRegression(MultiOutputMixin, RegressorMixin, LinearModel):"],"delete":["364","class LinearRegression(LinearModel, RegressorMixin, MultiOutputMixin):"]}],"sklearn\/preprocessing\/data.py":[{"add":["198","class MinMaxScaler(TransformerMixin, BaseEstimator):","495","class StandardScaler(TransformerMixin, BaseEstimator):","823","class MaxAbsScaler(TransformerMixin, BaseEstimator):","1052","class RobustScaler(TransformerMixin, BaseEstimator):","1330","class PolynomialFeatures(TransformerMixin, BaseEstimator):","1703","class Normalizer(TransformerMixin, BaseEstimator):","1841","class Binarizer(TransformerMixin, BaseEstimator):","1932","class KernelCenterer(TransformerMixin, BaseEstimator):","2093","class QuantileTransformer(TransformerMixin, BaseEstimator):","2637","class PowerTransformer(TransformerMixin, BaseEstimator):"],"delete":["198","class MinMaxScaler(BaseEstimator, TransformerMixin):","495","class StandardScaler(BaseEstimator, TransformerMixin):","823","class MaxAbsScaler(BaseEstimator, TransformerMixin):","1052","class RobustScaler(BaseEstimator, TransformerMixin):","1330","class PolynomialFeatures(BaseEstimator, TransformerMixin):","1703","class Normalizer(BaseEstimator, TransformerMixin):","1841","class Binarizer(BaseEstimator, TransformerMixin):","1932","class KernelCenterer(BaseEstimator, TransformerMixin):","2093","class QuantileTransformer(BaseEstimator, TransformerMixin):","2637","class PowerTransformer(BaseEstimator, TransformerMixin):"]}],"sklearn\/cluster\/tests\/test_bicluster.py":[{"add":["26","class MockBiclustering(BiclusterMixin, BaseEstimator):"],"delete":["26","class MockBiclustering(BaseEstimator, BiclusterMixin):"]}],"sklearn\/preprocessing\/label.py":[{"add":["170","class LabelEncoder(TransformerMixin, BaseEstimator):","302","class LabelBinarizer(TransformerMixin, BaseEstimator):","783","class MultiLabelBinarizer(TransformerMixin, BaseEstimator):"],"delete":["170","class LabelEncoder(BaseEstimator, TransformerMixin):","302","class LabelBinarizer(BaseEstimator, TransformerMixin):","783","class MultiLabelBinarizer(BaseEstimator, TransformerMixin):"]}],"sklearn\/cluster\/bicluster.py":[{"add":["86","class BaseSpectral(BiclusterMixin, BaseEstimator, metaclass=ABCMeta):"],"delete":["86","class BaseSpectral(BaseEstimator, BiclusterMixin, metaclass=ABCMeta):"]}],"sklearn\/ensemble\/bagging.py":[{"add":["431","class BaggingClassifier(ClassifierMixin, BaseBagging):","818","class BaggingRegressor(RegressorMixin, BaseBagging):"],"delete":["431","class BaggingClassifier(BaseBagging, ClassifierMixin):","818","class BaggingRegressor(BaseBagging, RegressorMixin):"]}],"sklearn\/manifold\/isomap.py":[{"add":["14","class Isomap(TransformerMixin, BaseEstimator):"],"delete":["14","class Isomap(BaseEstimator, TransformerMixin):"]}],"sklearn\/linear_model\/stochastic_gradient.py":[{"add":["67","class BaseSGD(SparseCoefMixin, BaseEstimator, metaclass=ABCMeta):","422","class BaseSGDClassifier(LinearClassifierMixin, BaseSGD, metaclass=ABCMeta):","1052","class BaseSGDRegressor(RegressorMixin, BaseSGD):"],"delete":["67","class BaseSGD(BaseEstimator, SparseCoefMixin, metaclass=ABCMeta):","422","class BaseSGDClassifier(BaseSGD, LinearClassifierMixin, metaclass=ABCMeta):","1052","class BaseSGDRegressor(BaseSGD, RegressorMixin):"]}],"sklearn\/decomposition\/dict_learning.py":[{"add":["934","class SparseCoder(SparseCodingMixin, BaseEstimator):","1047","class DictionaryLearning(SparseCodingMixin, BaseEstimator):","1243","class MiniBatchDictionaryLearning(SparseCodingMixin, BaseEstimator):"],"delete":["934","class SparseCoder(BaseEstimator, SparseCodingMixin):","1047","class DictionaryLearning(BaseEstimator, SparseCodingMixin):","1243","class MiniBatchDictionaryLearning(BaseEstimator, SparseCodingMixin):"]}],"sklearn\/ensemble\/gradient_boosting.py":[{"add":["1812","class GradientBoostingClassifier(ClassifierMixin, BaseGradientBoosting):","2288","class GradientBoostingRegressor(RegressorMixin, BaseGradientBoosting):"],"delete":["1812","class GradientBoostingClassifier(BaseGradientBoosting, ClassifierMixin):","2288","class GradientBoostingRegressor(BaseGradientBoosting, RegressorMixin):"]}],"sklearn\/dummy.py":[{"add":["21","class DummyClassifier(MultiOutputMixin, ClassifierMixin, BaseEstimator):","355","class DummyRegressor(MultiOutputMixin, RegressorMixin, BaseEstimator):"],"delete":["21","class DummyClassifier(BaseEstimator, ClassifierMixin, MultiOutputMixin):","355","class DummyRegressor(BaseEstimator, RegressorMixin, MultiOutputMixin):"]}],"sklearn\/decomposition\/sparse_pca.py":[{"add":["31","class SparsePCA(TransformerMixin, BaseEstimator):"],"delete":["31","class SparsePCA(BaseEstimator, TransformerMixin):"]}],"sklearn\/manifold\/locally_linear.py":[{"add":["520","class LocallyLinearEmbedding(TransformerMixin,","521","                             _UnstableArchMixin, BaseEstimator):"],"delete":["520","class LocallyLinearEmbedding(BaseEstimator, TransformerMixin,","521","                             _UnstableArchMixin):"]}],"sklearn\/preprocessing\/_function_transformer.py":[{"add":["13","class FunctionTransformer(TransformerMixin, BaseEstimator):"],"delete":["13","class FunctionTransformer(BaseEstimator, TransformerMixin):"]}],"sklearn\/cross_decomposition\/cca_.py":[{"add":["6","class CCA(_UnstableArchMixin, _PLS):"],"delete":["6","class CCA(_PLS, _UnstableArchMixin):"]}],"sklearn\/cross_decomposition\/pls_.py":[{"add":["123","class _PLS(TransformerMixin, RegressorMixin, MultiOutputMixin, BaseEstimator,","752","class PLSSVD(TransformerMixin, BaseEstimator):"],"delete":["123","class _PLS(BaseEstimator, TransformerMixin, RegressorMixin, MultiOutputMixin,","752","class PLSSVD(BaseEstimator, TransformerMixin):"]}],"sklearn\/decomposition\/kernel_pca.py":[{"add":["18","class KernelPCA(TransformerMixin, BaseEstimator):"],"delete":["18","class KernelPCA(BaseEstimator, TransformerMixin):"]}],"sklearn\/cluster\/birch.py":[{"add":["321","class Birch(ClusterMixin, TransformerMixin, BaseEstimator):"],"delete":["321","class Birch(BaseEstimator, TransformerMixin, ClusterMixin):"]}],"sklearn\/cluster\/k_means_.py":[{"add":["763","class KMeans(TransformerMixin, ClusterMixin, BaseEstimator):"],"delete":["763","class KMeans(BaseEstimator, ClusterMixin, TransformerMixin):"]}],"sklearn\/inspection\/tests\/test_partial_dependence.py":[{"add":["289","class NoPredictProbaNoDecisionFunction(ClassifierMixin, BaseEstimator):"],"delete":["289","class NoPredictProbaNoDecisionFunction(BaseEstimator, ClassifierMixin):"]}],"sklearn\/linear_model\/least_angle.py":[{"add":["763","class Lars(MultiOutputMixin, RegressorMixin, LinearModel):"],"delete":["763","class Lars(LinearModel, RegressorMixin, MultiOutputMixin):"]}]}},"9e3216ecf99612b8b733e13a154a6c92e9856798":{"changes":{"sklearn\/metrics\/_plot\/roc_curve.py":"MODIFY","sklearn\/metrics\/_plot\/tests\/test_plot_roc_curve.py":"MODIFY"},"diff":{"sklearn\/metrics\/_plot\/roc_curve.py":[{"add":["152","                            sample_weight=sample_weight,"],"delete":[]}],"sklearn\/metrics\/_plot\/tests\/test_plot_roc_curve.py":[{"add":["2","import numpy as np","54","@pytest.mark.parametrize(\"with_sample_weight\", [True, False])","55","@pytest.mark.parametrize(\"drop_intermediate\", [True, False])","56","def test_plot_roc_curve(pyplot, response_method, data_binary,","57","                        with_sample_weight, drop_intermediate):","59","    if with_sample_weight:","60","        rng = np.random.RandomState(42)","61","        sample_weight = rng.randint(1, 4, size=(X.shape[0]))","62","    else:","63","        sample_weight = None","68","    viz = plot_roc_curve(lr, X, y, alpha=0.8, sample_weight=sample_weight,","69","                         drop_intermediate=drop_intermediate)","75","    fpr, tpr, _ = roc_curve(y, y_pred, sample_weight=sample_weight,","76","                            drop_intermediate=drop_intermediate)","93","    assert viz.ax_.get_ylabel() == \"True Positive Rate\"","94","    assert viz.ax_.get_xlabel() == \"False Positive Rate\""],"delete":["53","def test_plot_roc_curve(pyplot, response_method, data_binary):","59","    viz = plot_roc_curve(lr, X, y, alpha=0.8)","65","    fpr, tpr, _ = roc_curve(y, y_pred)"]}]}},"a2968c2e4f0cbd5e957b0ba1d63ce2cb16d8008f":{"changes":{"sklearn\/feature_selection\/tests\/test_base.py":"MODIFY","sklearn\/feature_selection\/tests\/test_chi2.py":"MODIFY","sklearn\/feature_selection\/tests\/test_mutual_info.py":"MODIFY","sklearn\/feature_selection\/tests\/test_variance_threshold.py":"MODIFY","sklearn\/feature_selection\/tests\/test_from_model.py":"MODIFY","sklearn\/feature_selection\/tests\/test_feature_select.py":"MODIFY"},"diff":{"sklearn\/feature_selection\/tests\/test_base.py":[{"add":["1","import pytest","56","    with pytest.raises(ValueError):","57","        sel.transform(np.array([[1], [2]]))","73","    with pytest.raises(ValueError):","74","        sel.transform(np.array([[1], [2]]))","93","    with pytest.raises(ValueError):","94","        sel.inverse_transform(np.array([[1], [2]]))","110","    with pytest.raises(ValueError):","111","        sel.inverse_transform(np.array([[1], [2]]))"],"delete":["8","from sklearn.utils.testing import assert_raises","56","    assert_raises(ValueError, sel.transform, np.array([[1], [2]]))","72","    assert_raises(ValueError, sel.transform, np.array([[1], [2]]))","91","    assert_raises(ValueError, sel.inverse_transform, np.array([[1], [2]]))","107","    assert_raises(ValueError, sel.inverse_transform, np.array([[1], [2]]))"]}],"sklearn\/feature_selection\/tests\/test_chi2.py":[{"add":["8","import pytest","67","        with pytest.raises(ValueError):","68","            chi2(X, y)"],"delete":["13","from sklearn.utils.testing import assert_raises","67","        assert_raises(ValueError, chi2, X, y)"]}],"sklearn\/feature_selection\/tests\/test_mutual_info.py":[{"add":["2","import pytest","6","from sklearn.utils.testing import assert_array_equal, assert_almost_equal","184","        with pytest.raises(ValueError):","185","            mutual_info(X_csr, y, discrete_features=False)","186","        with pytest.raises(ValueError):","187","            mutual_info(X, y, discrete_features='manual')","188","        with pytest.raises(ValueError):","189","            mutual_info(X_csr, y, discrete_features=[True, False, True])","190","        with pytest.raises(IndexError):","191","            mutual_info(X, y, discrete_features=[True, False, True, False])","192","        with pytest.raises(IndexError):","193","            mutual_info(X, y, discrete_features=[1, 4])"],"delete":["5","from sklearn.utils.testing import (assert_array_equal, assert_almost_equal,","6","                                   assert_raises)","184","        assert_raises(ValueError, mutual_info, X_csr, y,","185","                      discrete_features=False)","186","        assert_raises(ValueError, mutual_info, X, y,","187","                      discrete_features='manual')","188","        assert_raises(ValueError, mutual_info, X_csr, y,","189","                      discrete_features=[True, False, True])","190","        assert_raises(IndexError, mutual_info, X, y,","191","                      discrete_features=[True, False, True, False])","192","        assert_raises(IndexError, mutual_info, X, y, discrete_features=[1, 4])"]}],"sklearn\/feature_selection\/tests\/test_variance_threshold.py":[{"add":["3","from sklearn.utils.testing import assert_array_equal","21","    with pytest.raises(ValueError):","22","        VarianceThreshold().fit([[0, 1, 2, 3]])","23","    with pytest.raises(ValueError):","24","        VarianceThreshold().fit([[0, 1], [0, 1]])"],"delete":["3","from sklearn.utils.testing import assert_array_equal, assert_raises","21","    assert_raises(ValueError, VarianceThreshold().fit, [[0, 1, 2, 3]])","22","    assert_raises(ValueError, VarianceThreshold().fit, [[0, 1], [0, 1]])"]}],"sklearn\/feature_selection\/tests\/test_from_model.py":[{"add":["29","        with pytest.raises(ValueError):","30","            model.transform(data)","292","    with pytest.raises(ValueError):","293","        model.fit(data, y)"],"delete":["6","from sklearn.utils.testing import assert_raises","30","        assert_raises(ValueError, model.transform, data)","292","    assert_raises(ValueError, model.fit, data, y)"]}],"sklearn\/feature_selection\/tests\/test_feature_select.py":[{"add":["325","    with pytest.raises(ValueError):","326","        SelectPercentile(percentile=-1).fit(X, y)","327","    with pytest.raises(ValueError):","328","        SelectPercentile(percentile=101).fit(X, y)","329","    with pytest.raises(ValueError):","330","        GenericUnivariateSelect(mode='percentile', param=-1).fit(X, y)","331","    with pytest.raises(ValueError):","332","        GenericUnivariateSelect(mode='percentile', param=101).fit(X, y)","567","        with pytest.raises(TypeError):","568","            SelectFeatures(score_func=10).fit(X, y)","575","    with pytest.raises(ValueError):","576","        SelectKBest(k=-1).fit(X, y)","577","    with pytest.raises(ValueError):","578","        SelectKBest(k=4).fit(X, y)","579","    with pytest.raises(ValueError):","580","        GenericUnivariateSelect(mode='k_best', param=-1).fit(X, y)","581","    with pytest.raises(ValueError):","582","        GenericUnivariateSelect(mode='k_best', param=4).fit(X, y)"],"delete":["11","from sklearn.utils.testing import assert_raises","326","    assert_raises(ValueError, SelectPercentile(percentile=-1).fit, X, y)","327","    assert_raises(ValueError, SelectPercentile(percentile=101).fit, X, y)","328","    assert_raises(ValueError, GenericUnivariateSelect(mode='percentile',","329","                                                      param=-1).fit, X, y)","330","    assert_raises(ValueError, GenericUnivariateSelect(mode='percentile',","331","                                                      param=101).fit, X, y)","566","        assert_raises(TypeError, SelectFeatures(score_func=10).fit, X, y)","573","    assert_raises(ValueError, SelectKBest(k=-1).fit, X, y)","574","    assert_raises(ValueError, SelectKBest(k=4).fit, X, y)","575","    assert_raises(ValueError,","576","                  GenericUnivariateSelect(mode='k_best', param=-1).fit, X, y)","577","    assert_raises(ValueError,","578","                  GenericUnivariateSelect(mode='k_best', param=4).fit, X, y)"]}]}},"e74e03294af942a6c82a7b482f517567e330dee5":{"changes":{"sklearn\/tree\/tests\/test_export.py":"MODIFY","sklearn\/tree\/tests\/test_tree.py":"MODIFY"},"diff":{"sklearn\/tree\/tests\/test_export.py":[{"add":["7","import pytest","217","    with pytest.raises(NotFittedError):","218","        export_graphviz(clf, out)","226","    with pytest.raises(ValueError, match=message):","227","        export_graphviz(clf, None, feature_names=[\"a\"])","231","    with pytest.raises(ValueError, match=message):","232","        export_graphviz(clf, None, feature_names=[\"a\", \"b\", \"c\"])","236","    with pytest.raises(TypeError, match=message):","237","        export_graphviz(clf.fit(X, y).tree_)","241","    with pytest.raises(IndexError):","242","        export_graphviz(clf, out, class_names=[])","246","    with pytest.raises(ValueError, match=\"should be greater or equal\"):","247","        export_graphviz(clf, out, precision=-1)","248","    with pytest.raises(ValueError, match=\"should be an integer\"):","249","        export_graphviz(clf, out, precision=\"1\")","316","    err_msg = \"max_depth bust be >= 0, given -1\"","317","    with pytest.raises(ValueError, match=err_msg):","318","        export_text(clf, max_depth=-1)","319","    err_msg = \"feature_names must contain 2 elements, got 1\"","320","    with pytest.raises(ValueError, match=err_msg):","321","        export_text(clf, feature_names=['a'])","322","    err_msg = \"decimals must be >= 0, given -1\"","323","    with pytest.raises(ValueError, match=err_msg):","324","        export_text(clf, decimals=-1)","325","    err_msg = \"spacing must be > 0, given 0\"","326","    with pytest.raises(ValueError, match=err_msg):","327","        export_text(clf, spacing=0)"],"delete":["13","from sklearn.utils.testing import (assert_raises,","14","                                   assert_raises_regex,","15","                                   assert_raise_message)","219","    assert_raises(NotFittedError, export_graphviz, clf, out)","227","    assert_raise_message(ValueError, message, export_graphviz, clf, None,","228","                         feature_names=[\"a\"])","232","    assert_raise_message(ValueError, message, export_graphviz, clf, None,","233","                         feature_names=[\"a\", \"b\", \"c\"])","237","    assert_raise_message(TypeError, message,","238","                         export_graphviz, clf.fit(X, y).tree_)","242","    assert_raises(IndexError, export_graphviz, clf, out, class_names=[])","246","    assert_raises_regex(ValueError, \"should be greater or equal\",","247","                        export_graphviz, clf, out, precision=-1)","248","    assert_raises_regex(ValueError, \"should be an integer\",","249","                        export_graphviz, clf, out, precision=\"1\")","316","    assert_raise_message(ValueError,","317","                         \"max_depth bust be >= 0, given -1\",","318","                         export_text, clf, max_depth=-1)","319","    assert_raise_message(ValueError,","320","                         \"feature_names must contain 2 elements, got 1\",","321","                         export_text, clf, feature_names=['a'])","322","    assert_raise_message(ValueError,","323","                         \"decimals must be >= 0, given -1\",","324","                         export_text, clf, decimals=-1)","325","    assert_raise_message(ValueError,","326","                         \"spacing must be > 0, given 0\",","327","                         export_text, clf, spacing=0)"]}],"sklearn\/tree\/tests\/test_tree.py":[{"add":["393","    with pytest.raises(ValueError):","394","        getattr(clf, 'feature_importances_')","473","        with pytest.raises(ValueError):","474","            est.fit(X, y)","477","        with pytest.raises(ValueError):","478","            est.fit(X, y)","481","        with pytest.raises(ValueError):","482","            est.fit(X, y)","485","        with pytest.raises(ValueError):","486","            est.fit(X, y)","489","        with pytest.raises(ValueError):","490","            est.fit(X, y)","498","        with pytest.raises(NotFittedError):","499","            est.predict_proba(X)","503","        with pytest.raises(ValueError):","504","            est.predict_proba(X2)","507","        with pytest.raises(ValueError):","508","            TreeEstimator(min_samples_leaf=-1).fit(X, y)","509","        with pytest.raises(ValueError):","510","            TreeEstimator(min_samples_leaf=.6).fit(X, y)","511","        with pytest.raises(ValueError):","512","            TreeEstimator(min_samples_leaf=0.).fit(X, y)","513","        with pytest.raises(ValueError):","514","            TreeEstimator(min_samples_leaf=3.).fit(X, y)","515","        with pytest.raises(ValueError):","516","            TreeEstimator(min_weight_fraction_leaf=-1).fit(X, y)","517","        with pytest.raises(ValueError):","518","            TreeEstimator(min_weight_fraction_leaf=0.51).fit(X, y)","519","        with pytest.raises(ValueError):","520","            TreeEstimator(min_samples_split=-1).fit(X, y)","521","        with pytest.raises(ValueError):","522","            TreeEstimator(min_samples_split=0.0).fit(X, y)","523","        with pytest.raises(ValueError):","524","            TreeEstimator(min_samples_split=1.1).fit(X, y)","525","        with pytest.raises(ValueError):","526","            TreeEstimator(min_samples_split=2.5).fit(X, y)","527","        with pytest.raises(ValueError):","528","            TreeEstimator(max_depth=-1).fit(X, y)","529","        with pytest.raises(ValueError):","530","            TreeEstimator(max_features=42).fit(X, y)","533","            with pytest.raises(ValueError):","534","                TreeEstimator(min_impurity_split=-1.0).fit(X, y)","535","        with pytest.raises(ValueError):","536","            TreeEstimator(min_impurity_decrease=-1.0).fit(X, y)","541","        with pytest.raises(ValueError):","542","            est.fit(X, y2)","552","        with pytest.raises(NotFittedError):","553","            est.predict(T)","558","        with pytest.raises(ValueError):","559","            est.predict(t[:, 1:])","566","        with pytest.raises(ValueError):","567","            est.predict(X)","568","        with pytest.raises(ValueError):","569","            est.apply(X)","573","        with pytest.raises(ValueError):","574","            clf.predict(Xt)","575","        with pytest.raises(ValueError):","576","            clf.apply(Xt)","580","        with pytest.raises(NotFittedError):","581","            est.apply(T)","1124","    with pytest.raises(ValueError):","1125","        clf.fit(X, y, sample_weight=sample_weight)","1128","    with pytest.raises(ValueError):","1129","        clf.fit(X, y, sample_weight=sample_weight)","1132","    with pytest.raises(ValueError):","1133","        clf.fit(X, y, sample_weight=sample_weight)","1136","    with pytest.raises(ValueError):","1137","        clf.fit(X, y, sample_weight=sample_weight)","1195","    with pytest.raises(ValueError):","1196","        clf.fit(X, y)","1197","    with pytest.raises(ValueError):","1198","        clf.fit(X, _y)","1202","    with pytest.raises(ValueError):","1203","        clf.fit(X, _y)","1207","    with pytest.raises(ValueError):","1208","        clf.fit(X, _y)","1226","        with pytest.raises(ValueError):","1227","            est.fit(X, y)","1229","        with pytest.raises(ValueError):","1230","            est.fit(X, y)","1232","        with pytest.raises(ValueError):","1233","            est.fit(X, y)","1310","    with pytest.raises(MemoryError):","1311","        _realloc_test()","1324","    with pytest.raises(Exception):","1325","        clf.fit(X, y)","1331","    with pytest.raises(MemoryError):","1332","        clf.fit(X, y)","1552","    with pytest.raises(ValueError):","1553","        TreeEstimator(random_state=0).fit(X, y)","1557","    with pytest.raises(ValueError):","1558","        est.predict([X])","1624","    with pytest.raises(ValueError):","1625","        est.fit(X, y)","1651","    with pytest.raises(ValueError,","1652","                       match=msg.replace('(', r'\\(').replace(')', r'\\)')):","1653","        est.fit(X, y)","1700","    with pytest.raises(TypeError):","1701","        TreeEstimator(random_state=0).fit(X, y)"],"delete":["24","from sklearn.utils.testing import assert_raises","28","from sklearn.utils.testing import assert_raise_message","395","    assert_raises(ValueError, getattr, clf, 'feature_importances_')","474","        assert_raises(ValueError, est.fit, X, y)","477","        assert_raises(ValueError, est.fit, X, y)","480","        assert_raises(ValueError, est.fit, X, y)","483","        assert_raises(ValueError, est.fit, X, y)","486","        assert_raises(ValueError, est.fit, X, y)","494","        assert_raises(NotFittedError, est.predict_proba, X)","498","        assert_raises(ValueError, est.predict_proba, X2)","501","        assert_raises(ValueError, TreeEstimator(min_samples_leaf=-1).fit, X, y)","502","        assert_raises(ValueError, TreeEstimator(min_samples_leaf=.6).fit, X, y)","503","        assert_raises(ValueError, TreeEstimator(min_samples_leaf=0.).fit, X, y)","504","        assert_raises(ValueError, TreeEstimator(min_samples_leaf=3.).fit, X, y)","505","        assert_raises(ValueError,","506","                      TreeEstimator(min_weight_fraction_leaf=-1).fit,","507","                      X, y)","508","        assert_raises(ValueError,","509","                      TreeEstimator(min_weight_fraction_leaf=0.51).fit,","510","                      X, y)","511","        assert_raises(ValueError, TreeEstimator(min_samples_split=-1).fit,","512","                      X, y)","513","        assert_raises(ValueError, TreeEstimator(min_samples_split=0.0).fit,","514","                      X, y)","515","        assert_raises(ValueError, TreeEstimator(min_samples_split=1.1).fit,","516","                      X, y)","517","        assert_raises(ValueError, TreeEstimator(min_samples_split=2.5).fit,","518","                      X, y)","519","        assert_raises(ValueError, TreeEstimator(max_depth=-1).fit, X, y)","520","        assert_raises(ValueError, TreeEstimator(max_features=42).fit, X, y)","523","            assert_raises(ValueError,","524","                          TreeEstimator(min_impurity_split=-1.0).fit, X, y)","525","        assert_raises(ValueError,","526","                      TreeEstimator(min_impurity_decrease=-1.0).fit, X, y)","531","        assert_raises(ValueError, est.fit, X, y2)","541","        assert_raises(NotFittedError, est.predict, T)","546","        assert_raises(ValueError, est.predict, t[:, 1:])","553","        assert_raises(ValueError, est.predict, X)","554","        assert_raises(ValueError, est.apply, X)","558","        assert_raises(ValueError, clf.predict, Xt)","559","        assert_raises(ValueError, clf.apply, Xt)","563","        assert_raises(NotFittedError, est.apply, T)","1106","    assert_raises(ValueError, clf.fit, X, y, sample_weight=sample_weight)","1109","    assert_raises(ValueError, clf.fit, X, y, sample_weight=sample_weight)","1112","    assert_raises(ValueError, clf.fit, X, y, sample_weight=sample_weight)","1115","    assert_raises(ValueError, clf.fit, X, y, sample_weight=sample_weight)","1173","    assert_raises(ValueError, clf.fit, X, y)","1174","    assert_raises(ValueError, clf.fit, X, _y)","1178","    assert_raises(ValueError, clf.fit, X, _y)","1182","    assert_raises(ValueError, clf.fit, X, _y)","1200","        assert_raises(ValueError, est.fit, X, y)","1202","        assert_raises(ValueError, est.fit, X, y)","1204","        assert_raises(ValueError, est.fit, X, y)","1281","    assert_raises(MemoryError, _realloc_test)","1294","    assert_raises(Exception, clf.fit, X, y)","1300","    assert_raises(MemoryError, clf.fit, X, y)","1520","    assert_raises(ValueError, TreeEstimator(random_state=0).fit, X, y)","1524","    assert_raises(ValueError, est.predict, [X])","1590","    assert_raises(ValueError, est.fit, X, y)","1616","    assert_raise_message(ValueError, msg, est.fit, X, y)","1663","    assert_raises(TypeError, TreeEstimator(random_state=0).fit, X, y)"]}]}},"a0cfcef679ee9bb7fd05e146767b510663f870a2":{"changes":{"sklearn\/linear_model\/tests\/test_omp.py":"MODIFY"},"diff":{"sklearn\/linear_model\/tests\/test_omp.py":[{"add":["19","n_samples, n_features, n_nonzero_coefs, n_targets = 25, 35, 5, 3"],"delete":["19","n_samples, n_features, n_nonzero_coefs, n_targets = 20, 30, 5, 3"]}]}},"4a325353ef1d9f10bf65f9f89763284ecf0de9e2":{"changes":{"README.rst":"MODIFY"},"diff":{"README.rst":[{"add":["29","SciPy and is distributed under the 3-Clause BSD license.","54","**Scikit-learn 0.20 was the last version to support Python 2.7.**","55","scikit-learn 0.21 and later require Python 3.5 or newer.","71","the easiest way to install scikit-learn is using ``pip``   ::"],"delete":["29","SciPy and distributed under the 3-Clause BSD license.","54","**Scikit-learn 0.20 was the last version to support Python2.7.**","55","Scikit-learn 0.21 and later require Python 3.5 or newer.","71","the easiest way to install scikit-learn is using ``pip`` ::"]}]}},"2e7b554293d430ef6ce7dfca194b78a6c039f7b5":{"changes":{"sklearn\/linear_model\/tests\/test_ridge.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY","sklearn\/linear_model\/ridge.py":"MODIFY"},"diff":{"sklearn\/linear_model\/tests\/test_ridge.py":[{"add":["468","def test_ridge_loo_cv_asym_scoring():","469","    # checking on asymmetric scoring","470","    scoring = 'explained_variance'","471","    n_samples, n_features = 10, 5","472","    n_targets = 1","473","    X, y = _make_sparse_offset_regression(","474","        n_samples=n_samples, n_features=n_features, n_targets=n_targets,","475","        random_state=0, shuffle=False, noise=1, n_informative=5","476","    )","477","","478","    alphas = [1e-3, .1, 1., 10., 1e3]","479","    loo_ridge = RidgeCV(cv=n_samples, fit_intercept=True,","480","                        alphas=alphas, scoring=scoring,","481","                        normalize=True)","482","","483","    gcv_ridge = RidgeCV(fit_intercept=True,","484","                        alphas=alphas, scoring=scoring,","485","                        normalize=True)","486","","487","    loo_ridge.fit(X, y)","488","    gcv_ridge.fit(X, y)","489","","490","    assert gcv_ridge.alpha_ == pytest.approx(loo_ridge.alpha_)","491","    assert_allclose(gcv_ridge.coef_, loo_ridge.coef_, rtol=1e-3)","492","    assert_allclose(gcv_ridge.intercept_, loo_ridge.intercept_, rtol=1e-3)","493","","494","","1073",""],"delete":[]}],"doc\/whats_new\/v0.22.rst":[{"add":["251","- |FIX| :class:`linear_model.RidgeCV` and :class:`linear_model.RidgeClassifierCV`  ","252","  now correctly scores when `cv=None`.","253","  :pr:`14864` by :user:`Venkatachalam N <venkyyuvy>`.","254",""],"delete":[]}],"sklearn\/linear_model\/ridge.py":[{"add":["1486","            # signature of scorer is (estimator, X, y)","1487","            out = [scorer(identity_estimator, cv_values[:, i], y.ravel())"],"delete":["1486","            out = [scorer(identity_estimator, y.ravel(), cv_values[:, i])"]}]}},"c997e9b3eaf00339bda796dbb34f4f64ca0fba85":{"changes":{"sklearn\/datasets\/lfw.py":"MODIFY"},"diff":{"sklearn\/datasets\/lfw.py":[{"add":["153","        if img.ndim == 0:"],"delete":["153","        if img.ndim is 0:"]}]}},"a89462b59d1bb1733203bbfcce95ba77d99ba762":{"changes":{"sklearn\/svm\/base.py":"MODIFY","sklearn\/svm\/tests\/test_svm.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY"},"diff":{"sklearn\/svm\/base.py":[{"add":["289","        if not n_SV:","290","            self.dual_coef_ = sp.csr_matrix([])","291","        else:","292","            dual_coef_indptr = np.arange(0, dual_coef_indices.size + 1,","293","                                         dual_coef_indices.size \/ n_class)","294","            self.dual_coef_ = sp.csr_matrix(","295","                (dual_coef_data, dual_coef_indices, dual_coef_indptr),","296","                (n_class, n_SV))"],"delete":["289","        dual_coef_indptr = np.arange(0, dual_coef_indices.size + 1,","290","                                     dual_coef_indices.size \/ n_class)","291","        self.dual_coef_ = sp.csr_matrix(","292","            (dual_coef_data, dual_coef_indices, dual_coef_indptr),","293","            (n_class, n_SV))"]}],"sklearn\/svm\/tests\/test_svm.py":[{"add":["692","def test_sparse_fit_support_vectors_empty():","693","    # Regression test for #14893","694","    X_train = sparse.csr_matrix([[0, 1, 0, 0],","695","                                 [0, 0, 0, 1],","696","                                 [0, 0, 1, 0],","697","                                 [0, 0, 0, 1]])","698","    y_train = np.array([0.04, 0.04, 0.10, 0.16])","699","    model = svm.SVR(kernel='linear')","700","    model.fit(X_train, y_train)","701","    assert not model.support_vectors_.data.size","702","    assert not model.dual_coef_.data.size","703","","704",""],"delete":[]}],"doc\/whats_new\/v0.22.rst":[{"add":["574","- |Fix| fixed a bug in :class:`BaseLibSVM._sparse_fit` where n_SV=0 raised a","575","  ZeroDivisionError. :pr:`14894` by :user:`Danna Naser <danna-naser>`.","576",""],"delete":[]}]}},"e55e37c4ac2096ad039523d441c160da6c79fef5":{"changes":{"sklearn\/kernel_approximation.py":"MODIFY","sklearn\/tests\/test_kernel_approximation.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY"},"diff":{"sklearn\/kernel_approximation.py":[{"add":["520","","603","        if not callable(self.kernel) and self.kernel != 'precomputed':","612","                                 \"Nystroem if using a callable \"","613","                                 \"or precomputed kernel\")"],"delete":["602","        if not callable(self.kernel):","611","                                 \"Nystroem if using a callable kernel.\")"]}],"sklearn\/tests\/test_kernel_approximation.py":[{"add":["256","","257","","258","def test_nystroem_precomputed_kernel():","259","    # Non-regression: test Nystroem on precomputed kernel.","260","    # PR - 14706","261","    rnd = np.random.RandomState(12)","262","    X = rnd.uniform(size=(10, 4))","263","","264","    K = polynomial_kernel(X, degree=2, coef0=.1)","265","    nystroem = Nystroem(kernel='precomputed', n_components=X.shape[0])","266","    X_transformed = nystroem.fit_transform(K)","267","    assert_array_almost_equal(np.dot(X_transformed, X_transformed.T), K)","268","","269","    # if degree, gamma or coef0 is passed, we raise a ValueError","270","    msg = \"Don't pass gamma, coef0 or degree to Nystroem\"","271","    params = ({'gamma': 1}, {'coef0': 1}, {'degree': 2})","272","    for param in params:","273","        ny = Nystroem(kernel='precomputed', n_components=X.shape[0],","274","                      **param)","275","        with pytest.raises(ValueError, match=msg):","276","            ny.fit(K)"],"delete":[]}],"doc\/whats_new\/v0.22.rst":[{"add":["202",":mod:`sklearn.kernel_approximation`","203","...................................","204","","205","-|FIX| Fixed a bug where :class:`kernel_approximation.Nystroem` raised a","206"," `KeyError` when using `kernel=\"precomputed\"`.","207"," :pr:`14706` by :user:`Venkatachalam N <venkyyuvy>`. ","208",""],"delete":[]}]}},"a3fad528f99755df44d787d505681d66af9eaa5c":{"changes":{"doc\/tutorial\/statistical_inference\/unsupervised_learning.rst":"MODIFY","doc\/conftest.py":"MODIFY"},"diff":{"doc\/tutorial\/statistical_inference\/unsupervised_learning.rst":[{"add":["23","algorithms. The simplest clustering algorithm is :ref:`k_means`.","26","   :target: ..\/..\/auto_examples\/cluster\/plot_cluster_iris.html","27","   :scale: 70","28","   :align: center","172","also referred to as connected components) when clustering an image.","175","   :target: ..\/..\/auto_examples\/cluster\/plot_coin_ward_segmentation.html","176","   :scale: 40","177","   :align: center","179","::","181","    >>> from skimage.data import coins","182","    >>> from scipy.ndimage.filters import gaussian_filter","183","    >>> from skimage.transform import rescale","184","    >>> rescaled_coins = rescale(","185","    ...     gaussian_filter(coins(), sigma=2),","186","    ...     0.2, mode='reflect', anti_aliasing=False, multichannel=False","187","    ... )","188","    >>> X = np.reshape(rescaled_coins, (-1, 1))","190","We need a vectorized version of the image. `'rescaled_coins'` is a down-scaled","191","version of the coins image to speed up the process::","192","","193","    >>> from sklearn.feature_extraction import grid_to_graph","194","    >>> connectivity = grid_to_graph(*rescaled_coins.shape)","195","","196","Define the graph structure of the data. Pixels connected to their neighbors::","197","","198","    >>> n_clusters = 27  # number of regions","199","","200","    >>> from sklearn.cluster import AgglomerativeClustering","201","    >>> ward = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward',","202","    ...                                connectivity=connectivity)","203","    >>> ward.fit(X)","204","    AgglomerativeClustering(connectivity=..., n_clusters=27)","205","    >>> label = np.reshape(ward.labels_, rescaled_coins.shape)","218","   :target: ..\/..\/auto_examples\/cluster\/plot_digits_agglomeration.html","219","   :align: center","220","   :scale: 57"],"delete":["23","algorithms. The simplest clustering algorithm is","24",":ref:`k_means`.","27","    :target: ..\/..\/auto_examples\/cluster\/plot_cluster_iris.html","28","    :scale: 70","29","    :align: right","30","","174","also referred to as connected components) when","175","clustering an image:","178","    :target: ..\/..\/auto_examples\/cluster\/plot_coin_ward_segmentation.html","179","    :scale: 40","180","    :align: right","182",".. literalinclude:: ..\/..\/auto_examples\/cluster\/plot_coin_ward_segmentation.py","183","    :lines: 21-45","185","..","186","    >>> from sklearn.feature_extraction.image import grid_to_graph","187","    >>> connectivity = grid_to_graph(*face.shape)","201","    :target: ..\/..\/auto_examples\/cluster\/plot_digits_agglomeration.html","202","    :align: right","203","    :scale: 57"]}],"doc\/conftest.py":[{"add":["60","    try:","61","        import skimage  # noqa","62","    except ImportError:","63","        raise SkipTest(\"Skipping unsupervised_learning.rst, scikit-image \"","64","                       \"not installed\")"],"delete":[]}]}},"73caac258c5df478779336e0f9336d4ad321fd1d":{"changes":{"lgtm.yml":"MODIFY"},"diff":{"lgtm.yml":[{"add":["3","      - pip3 install numpy==1.16.3","4","      - pip3 install --no-deps scipy Cython"],"delete":["3","      - pip3 install numpy scipy Cython"]}]}},"847b3468c7c60c7d89c21f5e854b6c492f7261d5":{"changes":{"sklearn\/utils\/fixes.py":"MODIFY","sklearn\/multioutput.py":"MODIFY","sklearn\/ensemble\/forest.py":"MODIFY"},"diff":{"sklearn\/utils\/fixes.py":[{"add":[],"delete":["157","def parallel_helper(obj, methodname, *args, **kwargs):","158","    \"\"\"Workaround for Python 2 limitations of pickling instance methods","159","","160","    Parameters","161","    ----------","162","    obj","163","    methodname","164","    *args","165","    **kwargs","166","","167","    \"\"\"","168","    return getattr(obj, methodname)(*args, **kwargs)","169","","170",""]}],"sklearn\/multioutput.py":[{"add":["194","            delayed(e.predict)(X)"],"delete":["25","from .utils.fixes import parallel_helper","195","            delayed(parallel_helper)(e, 'predict', X)"]}],"sklearn\/ensemble\/forest.py":[{"add":["61","from ..utils.fixes import _joblib_parallel_args","220","            delayed(tree.apply)(X, check_input=False)","251","            delayed(tree.decision_path)(X,"],"delete":["61","from ..utils.fixes import parallel_helper, _joblib_parallel_args","220","            delayed(parallel_helper)(tree, 'apply', X, check_input=False)","251","            delayed(parallel_helper)(tree, 'decision_path', X,"]}]}},"8632775c23a306661cf043bebd9ea4852edecada":{"changes":{"sklearn\/svm\/src\/liblinear\/liblinear_helper.c":"MODIFY"},"diff":{"sklearn\/svm\/src\/liblinear\/liblinear_helper.c":[{"add":["89","            for (j=0; j<i; ++j)","90","                free(sparse[j]);","91","            free(sparse);","92","            return NULL;"],"delete":["89","            int l;","90","            for (l=0; l<i; l++)","91","                free(sparse[l]);","92","            break;"]}]}},"68044b061d7abc0c16f632890939438033306161":{"changes":{"build_tools\/circle\/build_test_pypy.sh":"MODIFY"},"diff":{"build_tools\/circle\/build_test_pypy.sh":[{"add":["5","apt-get -yq install libatlas-dev libatlas-base-dev liblapack-dev gfortran ccache libopenblas-dev"],"delete":["5","apt-get -yq install libatlas-dev libatlas-base-dev liblapack-dev gfortran ccache"]}]}},"947dffcc9c07bbb01739ad4d34a2f33b8e1120ed":{"changes":{"sklearn\/ensemble\/_hist_gradient_boosting\/gradient_boosting.py":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_gradient_boosting.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY"},"diff":{"sklearn\/ensemble\/_hist_gradient_boosting\/gradient_boosting.py":[{"add":["428","        if is_classifier(self):","429","            y_small_train = self.classes_[y_small_train.astype(int)]","435","            if is_classifier(self):","436","                y_val = self.classes_[y_val.astype(int)]"],"delete":[]}],"sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_gradient_boosting.py":[{"add":["417","","418","","419","@pytest.mark.parametrize(\"scoring\", [None, 'loss'])","420","def test_string_target_early_stopping(scoring):","421","    # Regression tests for #14709 where the targets need to be encoded before","422","    # to compute the score","423","    rng = np.random.RandomState(42)","424","    X = rng.randn(100, 10)","425","    y = np.array(['x'] * 50 + ['y'] * 50, dtype=object)","426","    gbrt = HistGradientBoostingClassifier(n_iter_no_change=10, scoring=scoring)","427","    gbrt.fit(X, y)"],"delete":[]}],"doc\/whats_new\/v0.22.rst":[{"add":["134","  - |Fix| Fixed a bug where early stopping would break with string targets.","135","    :pr:`14710` by :user:`Guillaume Lemaitre <glemaitre>`."],"delete":[]}]}},"8b002f2714e9787ad7101ca276daeb4dc4eed2a8":{"changes":{"sklearn\/manifold\/tests\/test_t_sne.py":"MODIFY"},"diff":{"sklearn\/manifold\/tests\/test_t_sne.py":[{"add":["3","from numpy.testing import assert_allclose","277","                random_state=0, method='exact', n_iter=750)","279","    assert_allclose(trustworthiness(X_csr, X_embedded, n_neighbors=1),","280","                    1.0, rtol=1.1e-1)"],"delete":["276","                random_state=0, method='exact', n_iter=500)","278","    assert_almost_equal(trustworthiness(X_csr, X_embedded, n_neighbors=1), 1.0,","279","                        decimal=1)"]}]}},"b7c41636907defd0ca210ed2e8e17fd4735567a0":{"changes":{"sklearn\/svm\/src\/liblinear\/liblinear_helper.c":"MODIFY"},"diff":{"sklearn\/svm\/src\/liblinear\/liblinear_helper.c":[{"add":["34","            for (j=0; j<i; j++)"],"delete":["34","            for (j=0; j<i-1; j++)"]}]}},"3de368d40548fae71f6288f68c639abda05a3d7d":{"changes":{"sklearn\/model_selection\/_split.py":"MODIFY","sklearn\/model_selection\/tests\/test_search.py":"MODIFY","sklearn\/model_selection\/tests\/test_split.py":"MODIFY","sklearn\/model_selection\/tests\/test_validation.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY"},"diff":{"sklearn\/model_selection\/_split.py":[{"add":["603","    The implementation is designed to:","604","","605","    * Generate test sets such that all contain the same distribution of","606","      classes, or as close as possible.","607","    * Be invariant to class label: relabelling ``y = [\"Happy\", \"Sad\"]`` to","608","      ``y = [1, 0]`` should not change the indices generated.","609","    * Preserve order dependencies in the dataset ordering, when","610","      ``shuffle=False``: all samples from class k in some test set were","611","      contiguous in y, or separated in y by samples from classes other than k.","612","    * Generate test sets where the smallest and largest differ by at most one","613","      sample.","614","","615","    .. versionchanged:: 0.22","616","        The previous implementation did not follow the last constraint.","637","","638","        _, y_idx, y_inv = np.unique(y, return_index=True, return_inverse=True)","639","        # y_inv encodes y according to lexicographic order. We invert y_idx to","640","        # map the classes so that they are encoded by order of appearance:","641","        # 0 represents the first label appearing in y, 1 the second, etc.","642","        _, class_perm = np.unique(y_idx, return_inverse=True)","643","        y_encoded = class_perm[y_inv]","644","","645","        n_classes = len(y_idx)","646","        y_counts = np.bincount(y_encoded)","654","                           \" members, which is less than n_splits=%d.\"","655","                           % (min_groups, self.n_splits)), UserWarning)","657","        # Determine the optimal number of samples from each class in each fold,","658","        # using round robin over the sorted y. (This can be done direct from","659","        # counts, but that code is unreadable.)","660","        y_order = np.sort(y_encoded)","661","        allocation = np.asarray(","662","            [np.bincount(y_order[i::self.n_splits], minlength=n_classes)","663","             for i in range(self.n_splits)])","665","        # To maintain the data order dependencies as best as possible within","666","        # the stratification constraint, we assign samples from each class in","667","        # blocks (and then mess that up when shuffle=True).","668","        test_folds = np.empty(len(y), dtype='i')","669","        for k in range(n_classes):","670","            # since the kth column of allocation stores the number of samples","671","            # of class k in each test set, this generates blocks of fold","672","            # indices corresponding to the allocation for class k.","673","            folds_for_class = np.arange(self.n_splits).repeat(allocation[:, k])","674","            if self.shuffle:","675","                rng.shuffle(folds_for_class)","676","            test_folds[y_encoded == k] = folds_for_class"],"delete":["603","    Train and test sizes may be different in each fold, with a difference of at","604","    most ``n_classes``.","625","        n_samples = y.shape[0]","626","        unique_y, y_inversed = np.unique(y, return_inverse=True)","627","        y_counts = np.bincount(y_inversed)","635","                           \" members, which is too few. The minimum\"","636","                           \" number of members in any class cannot\"","637","                           \" be less than n_splits=%d.\"","638","                           % (min_groups, self.n_splits)), Warning)","640","        # pre-assign each sample to a test fold index using individual KFold","641","        # splitting strategies for each class so as to respect the balance of","642","        # classes","643","        # NOTE: Passing the data corresponding to ith class say X[y==class_i]","644","        # will break when the data is not 100% stratifiable for all classes.","645","        # So we pass np.zeroes(max(c, n_splits)) as data to the KFold","646","        per_cls_cvs = [","647","            KFold(self.n_splits, shuffle=self.shuffle,","648","                  random_state=rng).split(np.zeros(max(count, self.n_splits)))","649","            for count in y_counts]","651","        test_folds = np.zeros(n_samples, dtype=np.int)","652","        for test_fold_indices, per_cls_splits in enumerate(zip(*per_cls_cvs)):","653","            for cls, (_, test_split) in zip(unique_y, per_cls_splits):","654","                cls_test_folds = test_folds[y == cls]","655","                # the test split can be too big because we used","656","                # KFold(...).split(X[:max(c, n_splits)]) when data is not 100%","657","                # stratifiable for all the classes","658","                # (we use a warning instead of raising an exception)","659","                # If this is the case, let's trim it:","660","                test_split = test_split[test_split < len(cls_test_folds)]","661","                cls_test_folds[test_split] = test_fold_indices","662","                test_folds[y == cls] = cls_test_folds","663",""]}],"sklearn\/model_selection\/tests\/test_search.py":[{"add":["212","                         \"Fit parameter spam has length 1; expected\","],"delete":["212","                         \"Fit parameter spam has length 1; expected 4.\","]}],"sklearn\/model_selection\/tests\/test_split.py":[{"add":["8","from itertools import permutations","10","from sklearn.utils.testing import assert_allclose","371","    # Check equivalence to KFold","372","    y = [0, 1, 0, 1, 0, 1, 0, 1]","373","    X = np.ones_like(y)","374","    np.testing.assert_equal(","375","        list(StratifiedKFold(3).split(X, y)),","376","        list(KFold(3).split(X, y)))","378","","379","@pytest.mark.parametrize('shuffle', [False, True])","380","@pytest.mark.parametrize('k', [4, 5, 6, 7, 8, 9, 10])","381","def test_stratified_kfold_ratios(k, shuffle):","389","    distr = np.bincount(y) \/ len(y)","391","    test_sizes = []","392","    skf = StratifiedKFold(k, random_state=0, shuffle=shuffle)","393","    for train, test in skf.split(X, y):","394","        assert_allclose(np.bincount(y[train]) \/ len(train), distr, atol=0.02)","395","        assert_allclose(np.bincount(y[test]) \/ len(test), distr, atol=0.02)","396","        test_sizes.append(len(test))","397","    assert np.ptp(test_sizes) <= 1","398","","399","","400","@pytest.mark.parametrize('shuffle', [False, True])","401","@pytest.mark.parametrize('k', [4, 6, 7])","402","def test_stratified_kfold_label_invariance(k, shuffle):","403","    # Check that stratified kfold gives the same indices regardless of labels","404","    n_samples = 100","405","    y = np.array([2] * int(0.10 * n_samples) +","406","                 [0] * int(0.89 * n_samples) +","407","                 [1] * int(0.01 * n_samples))","408","    X = np.ones(len(y))","409","","410","    def get_splits(y):","411","        return [(list(train), list(test))","412","                for train, test","413","                in StratifiedKFold(k, random_state=0,","414","                                   shuffle=shuffle).split(X, y)]","415","","416","    splits_base = get_splits(y)","417","    for perm in permutations([0, 1, 2]):","418","        y_perm = np.take(perm, y)","419","        splits_perm = get_splits(y_perm)","420","        assert splits_perm == splits_base","571","    assert 0.94 > mean_score"],"delete":["9","from sklearn.utils.testing import assert_almost_equal","371","def test_stratified_kfold_ratios():","380","    for shuffle in (False, True):","381","        for train, test in StratifiedKFold(5, shuffle=shuffle).split(X, y):","382","            assert_almost_equal(np.sum(y[train] == 4) \/ len(train), 0.10, 2)","383","            assert_almost_equal(np.sum(y[train] == 0) \/ len(train), 0.89, 2)","384","            assert_almost_equal(np.sum(y[train] == 1) \/ len(train), 0.01, 2)","385","            assert_almost_equal(np.sum(y[test] == 4) \/ len(test), 0.10, 2)","386","            assert_almost_equal(np.sum(y[test] == 0) \/ len(test), 0.89, 2)","387","            assert_almost_equal(np.sum(y[test] == 1) \/ len(test), 0.01, 2)","538","    assert 0.93 > mean_score"]}],"sklearn\/model_selection\/tests\/test_validation.py":[{"add":["963","    cv = StratifiedKFold(n_splits=2)"],"delete":["963","    cv = StratifiedKFold(n_splits=2, random_state=1)"]}],"doc\/whats_new\/v0.22.rst":[{"add":["19","- :class:`cluster.KMeans` when `n_jobs=1`. |Fix|","29","- :class:`linear_model.Ridge` when `X` is sparse. |Fix|","30","- :class:`model_selection.StratifiedKFold` and any use of `cv=int` with a","31","  classifier. |Fix|","279","- |Fix| Reimplemented :class:`model_selection.StratifiedKFold` to fix an issue","280","  where one test set could be `n_classes` larger than another. Test sets should","281","  now be near-equally sized. :pr:`14704` by `Joel Nothman`_.","282",""],"delete":["25","- :class:`linear_model.Ridge` when `X` is sparse. |Fix|","26","- :class:`cluster.KMeans` when `n_jobs=1`. |Fix|"]}]}},"7aff4c3513cffce53a4f17737ba19e4f44c36a93":{"changes":{"sklearn\/metrics\/classification.py":"MODIFY","doc\/modules\/partial_dependence.rst":"MODIFY","doc\/about.rst":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY","doc\/modules\/computing.rst":"MODIFY"},"diff":{"sklearn\/metrics\/classification.py":[{"add":["1398",""],"delete":[]}],"doc\/modules\/partial_dependence.rst":[{"add":["123","    T. Hastie, R. Tibshirani and J. Friedman, `The Elements of"],"delete":["123"," .. [HTF2009] T. Hastie, R. Tibshirani and J. Friedman, `The Elements of"]}],"doc\/about.rst":[{"add":["171","   |                    |","177","   |                    |"],"delete":["171","   |      .......       |","177","   |      ........      |"]}],"doc\/whats_new\/v0.22.rst":[{"add":["664","  unnecessarily. :pr:`15094` by `Andreas Mller`_."],"delete":["664","  unnecessarily. :pre:`15094` by `Andreas Mller`_."]}],"doc\/modules\/computing.rst":[{"add":["636","- Manually setting one of the environment variables (``OMP_NUM_THREADS``,","637","  ``MKL_NUM_THREADS``, ``OPENBLAS_NUM_THREADS``, or ``BLIS_NUM_THREADS``)","638","  will take precedence over what joblib tries to do. The total number of","639","  threads will be ``n_jobs * <LIB>_NUM_THREADS``. Note that setting this","640","  limit will also impact your computations in the main process, which will","641","  only use ``<LIB>_NUM_THREADS``. Joblib exposes a context manager for","642","  finer control over the number of threads in its workers (see joblib docs","643","  linked below)."],"delete":["636"," - Manually setting one of the environment variables (``OMP_NUM_THREADS``,","637","   ``MKL_NUM_THREADS``, ``OPENBLAS_NUM_THREADS``, or ``BLIS_NUM_THREADS``)","638","   will take precedence over what joblib tries to do. The total number of","639","   threads will be ``n_jobs * <LIB>_NUM_THREADS``. Note that setting this","640","   limit will also impact your computations in the main process, which will","641","   only use ``<LIB>_NUM_THREADS``. Joblib exposes a context manager for","642","   finer control over the number of threads in its workers (see joblib docs","643","   linked below)."]}]}},"21fc1d97452d4e3a6d744d0eef95ecaf7e87859c":{"changes":{"doc\/themes\/scikit-learn-modern\/static\/css\/theme.css":"MODIFY"},"diff":{"doc\/themes\/scikit-learn-modern\/static\/css\/theme.css":[{"add":["317","div#carouselExampleSlidesOnly {","318","  min-height: 200px;","319","}","320",""],"delete":[]}]}},"247ea83065bbbb2817a13b70d6178705f5c0ad0a":{"changes":{"sklearn\/linear_model\/least_angle.py":"MODIFY"},"diff":{"sklearn\/linear_model\/least_angle.py":[{"add":["1023","    copy_X : bool, default=True"],"delete":["1023","    copy_X : bool default=True"]}]}},"0ea324406174f046877c98bfe758c16122ad8ba7":{"changes":{"sklearn\/metrics\/tests\/test_score_objects.py":"MODIFY","sklearn\/metrics\/_scorer.py":"MODIFY","sklearn\/metrics\/tests\/test_common.py":"MODIFY","doc\/modules\/classes.rst":"MODIFY","doc\/whats_new\/_contributors.rst":"MODIFY","doc\/modules\/model_evaluation.rst":"MODIFY","sklearn\/metrics\/__init__.py":"MODIFY","sklearn\/metrics\/tests\/test_regression.py":"MODIFY","doc\/whats_new\/v0.24.rst":"MODIFY","sklearn\/metrics\/_regression.py":"MODIFY"},"diff":{"sklearn\/metrics\/tests\/test_score_objects.py":[{"add":["45","                      'neg_mean_absolute_percentage_error',","50","                      'mean_absolute_percentage_error',"],"delete":[]}],"sklearn\/metrics\/_scorer.py":[{"add":["32","               brier_score_loss, jaccard_score, mean_absolute_percentage_error)","616","neg_mean_absolute_percentage_error_scorer = make_scorer(","617","    mean_absolute_percentage_error, greater_is_better=False","618",")","679","               neg_mean_absolute_percentage_error=neg_mean_absolute_percentage_error_scorer,  # noqa"],"delete":["32","               brier_score_loss, jaccard_score)"]}],"sklearn\/metrics\/tests\/test_common.py":[{"add":["43","from sklearn.metrics import mean_absolute_percentage_error","101","    \"mean_absolute_percentage_error\": mean_absolute_percentage_error,","429","    \"r2_score\", \"explained_variance_score\", \"mean_absolute_percentage_error\"","476","    \"mean_compound_poisson_deviance\", \"mean_absolute_percentage_error\"","1375","        if metric == mean_absolute_percentage_error:","1376","            assert np.isfinite(current_score)","1377","            assert current_score > 1e6","1378","            # Here we are not comparing the values in case of MAPE because","1379","            # whenever y_true value is exactly zero, the MAPE value doesn't","1380","            # signify anything. Thus, in this case we are just expecting","1381","            # very large finite value.","1382","        else:","1383","            assert_almost_equal(score, current_score)"],"delete":["427","    \"r2_score\", \"explained_variance_score\"","474","    \"mean_compound_poisson_deviance\"","1373","        assert_almost_equal(score, current_score)"]}],"doc\/modules\/classes.rst":[{"add":["902","","983","   metrics.mean_absolute_percentage_error"],"delete":["902","\t"]}],"doc\/whats_new\/_contributors.rst":[{"add":["178",".. _Guillaume Lemaitre: https:\/\/github.com\/glemaitre"],"delete":["178",".. _Guillaume Lemaitre: https:\/\/github.com\/glemaitre"]}],"doc\/modules\/model_evaluation.rst":[{"add":["56","====================================   ==============================================     ==================================","57","Scoring                                Function                                           Comment","58","====================================   ==============================================     ==================================","60","'accuracy'                             :func:`metrics.accuracy_score`","61","'balanced_accuracy'                    :func:`metrics.balanced_accuracy_score`","62","'average_precision'                    :func:`metrics.average_precision_score`","63","'neg_brier_score'                      :func:`metrics.brier_score_loss`","64","'f1'                                   :func:`metrics.f1_score`                           for binary targets","65","'f1_micro'                             :func:`metrics.f1_score`                           micro-averaged","66","'f1_macro'                             :func:`metrics.f1_score`                           macro-averaged","67","'f1_weighted'                          :func:`metrics.f1_score`                           weighted average","68","'f1_samples'                           :func:`metrics.f1_score`                           by multilabel sample","69","'neg_log_loss'                         :func:`metrics.log_loss`                           requires ``predict_proba`` support","70","'precision' etc.                       :func:`metrics.precision_score`                    suffixes apply as with 'f1'","71","'recall' etc.                          :func:`metrics.recall_score`                       suffixes apply as with 'f1'","72","'jaccard' etc.                         :func:`metrics.jaccard_score`                      suffixes apply as with 'f1'","73","'roc_auc'                              :func:`metrics.roc_auc_score`","74","'roc_auc_ovr'                          :func:`metrics.roc_auc_score`","75","'roc_auc_ovo'                          :func:`metrics.roc_auc_score`","76","'roc_auc_ovr_weighted'                 :func:`metrics.roc_auc_score`","77","'roc_auc_ovo_weighted'                 :func:`metrics.roc_auc_score`","80","'adjusted_mutual_info_score'           :func:`metrics.adjusted_mutual_info_score`","81","'adjusted_rand_score'                  :func:`metrics.adjusted_rand_score`","82","'completeness_score'                   :func:`metrics.completeness_score`","83","'fowlkes_mallows_score'                :func:`metrics.fowlkes_mallows_score`","84","'homogeneity_score'                    :func:`metrics.homogeneity_score`","85","'mutual_info_score'                    :func:`metrics.mutual_info_score`","86","'normalized_mutual_info_score'         :func:`metrics.normalized_mutual_info_score`","87","'v_measure_score'                      :func:`metrics.v_measure_score`","90","'explained_variance'                   :func:`metrics.explained_variance_score`","91","'max_error'                            :func:`metrics.max_error`","92","'neg_mean_absolute_error'              :func:`metrics.mean_absolute_error`","93","'neg_mean_squared_error'               :func:`metrics.mean_squared_error`","94","'neg_root_mean_squared_error'          :func:`metrics.mean_squared_error`","95","'neg_mean_squared_log_error'           :func:`metrics.mean_squared_log_error`","96","'neg_median_absolute_error'            :func:`metrics.median_absolute_error`","97","'r2'                                   :func:`metrics.r2_score`","98","'neg_mean_poisson_deviance'            :func:`metrics.mean_poisson_deviance`","99","'neg_mean_gamma_deviance'              :func:`metrics.mean_gamma_deviance`","100","'neg_mean_absolute_percentage_error'   :func:`metrics.mean_absolute_percentage_error`","101","====================================   ==============================================     ==================================","1966",".. _mean_absolute_percentage_error:","1967","","1968","Mean absolute percentage error","1969","------------------------------","1970","The :func:`mean_absolute_percentage_error` (MAPE), also known as mean absolute","1971","percentage deviation (MAPD), is an evaluation metric for regression problems.","1972","The idea of this metric is to be sensitive to relative errors. It is for example","1973","not changed by a global scaling of the target variable.","1974","","1975","If :math:`\\hat{y}_i` is the predicted value of the :math:`i`-th sample","1976","and :math:`y_i` is the corresponding true value, then the mean absolute percentage","1977","error (MAPE) estimated over :math:`n_{\\text{samples}}` is defined as","1978","","1979",".. math::","1980","","1981","  \\text{MAPE}(y, \\hat{y}) = \\frac{1}{n_{\\text{samples}}} \\sum_{i=0}^{n_{\\text{samples}}-1} \\frac{{}\\left| y_i - \\hat{y}_i \\right|}{max(\\epsilon, \\left| y_i \\right|)}","1982","","1983","where :math:`\\epsilon` is an arbitrary small yet strictly positive number to","1984","avoid undefined results when y is zero.","1985","","1986","The :func:`mean_absolute_percentage_error` function supports multioutput.","1987","","1988","Here is a small example of usage of the :func:`mean_absolute_percentage_error`","1989","function::","1990","","1991","  >>> from sklearn.metrics import mean_absolute_percentage_error","1992","  >>> y_true = [1, 10, 1e6]","1993","  >>> y_pred = [0.9, 15, 1.2e6]","1994","  >>> mean_absolute_percentage_error(y_true, y_pred)","1995","  0.2666...","1996","","1997","In above example, if we had used `mean_absolute_error`, it would have ignored","1998","the small magnitude values and only reflected the error in prediction of highest","1999","magnitude value. But that problem is resolved in case of MAPE because it calculates","2000","relative percentage error with respect to actual output.","2001",""],"delete":["56","==============================    =============================================     ==================================","57","Scoring                           Function                                          Comment","58","==============================    =============================================     ==================================","60","'accuracy'                        :func:`metrics.accuracy_score`","61","'balanced_accuracy'               :func:`metrics.balanced_accuracy_score`","62","'average_precision'               :func:`metrics.average_precision_score`","63","'neg_brier_score'                 :func:`metrics.brier_score_loss`","64","'f1'                              :func:`metrics.f1_score`                          for binary targets","65","'f1_micro'                        :func:`metrics.f1_score`                          micro-averaged","66","'f1_macro'                        :func:`metrics.f1_score`                          macro-averaged","67","'f1_weighted'                     :func:`metrics.f1_score`                          weighted average","68","'f1_samples'                      :func:`metrics.f1_score`                          by multilabel sample","69","'neg_log_loss'                    :func:`metrics.log_loss`                          requires ``predict_proba`` support","70","'precision' etc.                  :func:`metrics.precision_score`                   suffixes apply as with 'f1'","71","'recall' etc.                     :func:`metrics.recall_score`                      suffixes apply as with 'f1'","72","'jaccard' etc.                    :func:`metrics.jaccard_score`                     suffixes apply as with 'f1'","73","'roc_auc'                         :func:`metrics.roc_auc_score`","74","'roc_auc_ovr'                     :func:`metrics.roc_auc_score`","75","'roc_auc_ovo'                     :func:`metrics.roc_auc_score`","76","'roc_auc_ovr_weighted'            :func:`metrics.roc_auc_score`","77","'roc_auc_ovo_weighted'            :func:`metrics.roc_auc_score`","80","'adjusted_mutual_info_score'      :func:`metrics.adjusted_mutual_info_score`","81","'adjusted_rand_score'             :func:`metrics.adjusted_rand_score`","82","'completeness_score'              :func:`metrics.completeness_score`","83","'fowlkes_mallows_score'           :func:`metrics.fowlkes_mallows_score`","84","'homogeneity_score'               :func:`metrics.homogeneity_score`","85","'mutual_info_score'               :func:`metrics.mutual_info_score`","86","'normalized_mutual_info_score'    :func:`metrics.normalized_mutual_info_score`","87","'v_measure_score'                 :func:`metrics.v_measure_score`","90","'explained_variance'              :func:`metrics.explained_variance_score`","91","'max_error'                       :func:`metrics.max_error`","92","'neg_mean_absolute_error'         :func:`metrics.mean_absolute_error`","93","'neg_mean_squared_error'          :func:`metrics.mean_squared_error`","94","'neg_root_mean_squared_error'     :func:`metrics.mean_squared_error`","95","'neg_mean_squared_log_error'      :func:`metrics.mean_squared_log_error`","96","'neg_median_absolute_error'       :func:`metrics.median_absolute_error`","97","'r2'                              :func:`metrics.r2_score`","98","'neg_mean_poisson_deviance'       :func:`metrics.mean_poisson_deviance`","99","'neg_mean_gamma_deviance'         :func:`metrics.mean_gamma_deviance`","100","==============================    =============================================     =================================="]}],"sklearn\/metrics\/__init__.py":[{"add":["66","from ._regression import mean_absolute_percentage_error","131","    'mean_absolute_percentage_error',"],"delete":[]}],"sklearn\/metrics\/tests\/test_regression.py":[{"add":["15","from sklearn.metrics import mean_absolute_percentage_error","35","    mape = mean_absolute_percentage_error(y_true, y_pred)","36","    assert np.isfinite(mape)","37","    assert mape > 1e6","92","    error = np.around(mean_absolute_percentage_error(y_true, y_pred),","93","                      decimals=2)","94","    assert np.isfinite(error)","95","    assert error > 1e6","110","    assert_almost_equal(mean_absolute_percentage_error([0.], [0.]), 0.00, 2)","209","    mape = mean_absolute_percentage_error(y_true, y_pred,","210","                                          multioutput='raw_values')","216","    assert_array_almost_equal(mape, [0.0778, 0.2262], decimal=2)","268","    mapew = mean_absolute_percentage_error(y_true, y_pred,","269","                                           multioutput=[0.4, 0.6])","276","    assert_almost_equal(mapew, 0.1668, decimal=2)","325","","326","","327","def test_mean_absolute_percentage_error():","328","    random_number_generator = np.random.RandomState(42)","329","    y_true = random_number_generator.exponential(size=100)","330","    y_pred = 1.2 * y_true","331","    assert mean_absolute_percentage_error(y_true, y_pred) == pytest.approx(0.2)"],"delete":[]}],"doc\/whats_new\/v0.24.rst":[{"add":["152","- |Feature| Added :func:`metrics.mean_absolute_percentage_error` metric and","153","  the associated scorer for regression problems. :issue:`10708` fixed with the","154","  PR :pr:`15007` by :user:`Ashutosh Hathidara <ashutosh1919>`. The scorer and","155","  some practical test cases were taken from PR :pr:`10711` by","156","  :user:`Mohamed Ali Jamaoui <mohamed-ali>`.","157",""],"delete":[]}],"sklearn\/metrics\/_regression.py":[{"add":["22","#          Ashutosh Hathidara <ashutoshhathidara98@gmail.com>","44","    \"mean_absolute_percentage_error\",","196","def mean_absolute_percentage_error(y_true, y_pred,","197","                                   sample_weight=None,","198","                                   multioutput='uniform_average'):","199","    \"\"\"Mean absolute percentage error regression loss","200","","201","    Note here that we do not represent the output as a percentage in range","202","    [0, 100]. Instead, we represent it in range [0, 1\/eps]. Read more in the","203","    :ref:`User Guide <mean_absolute_percentage_error>`.","204","","205","    Parameters","206","    ----------","207","    y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)","208","        Ground truth (correct) target values.","209","","210","    y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)","211","        Estimated target values.","212","","213","    sample_weight : array-like of shape (n_samples,), default=None","214","        Sample weights.","215","","216","    multioutput : {'raw_values', 'uniform_average'} or array-like","217","        Defines aggregating of multiple output values.","218","        Array-like value defines weights used to average errors.","219","        If input is list then the shape must be (n_outputs,).","220","","221","        'raw_values' :","222","            Returns a full set of errors in case of multioutput input.","223","","224","        'uniform_average' :","225","            Errors of all outputs are averaged with uniform weight.","226","","227","    Returns","228","    -------","229","    loss : float or ndarray of floats in the range [0, 1\/eps]","230","        If multioutput is 'raw_values', then mean absolute percentage error","231","        is returned for each output separately.","232","        If multioutput is 'uniform_average' or an ndarray of weights, then the","233","        weighted average of all output errors is returned.","234","","235","        MAPE output is non-negative floating point. The best value is 0.0.","236","        But note the fact that bad predictions can lead to arbitarily large","237","        MAPE values, especially if some y_true values are very close to zero.","238","        Note that we return a large value instead of `inf` when y_true is zero.","239","","240","    Examples","241","    --------","242","    >>> from sklearn.metrics import mean_absolute_percentage_error","243","    >>> y_true = [3, -0.5, 2, 7]","244","    >>> y_pred = [2.5, 0.0, 2, 8]","245","    >>> mean_absolute_percentage_error(y_true, y_pred)","246","    0.3273...","247","    >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]","248","    >>> y_pred = [[0, 2], [-1, 2], [8, -5]]","249","    >>> mean_absolute_percentage_error(y_true, y_pred)","250","    0.5515...","251","    >>> mean_absolute_percentage_error(y_true, y_pred, multioutput=[0.3, 0.7])","252","    0.6198...","253","    \"\"\"","254","    y_type, y_true, y_pred, multioutput = _check_reg_targets(","255","        y_true, y_pred, multioutput)","256","    check_consistent_length(y_true, y_pred, sample_weight)","257","    epsilon = np.finfo(np.float64).eps","258","    mape = np.abs(y_pred - y_true) \/ np.maximum(np.abs(y_true), epsilon)","259","    output_errors = np.average(mape,","260","                               weights=sample_weight, axis=0)","261","    if isinstance(multioutput, str):","262","        if multioutput == 'raw_values':","263","            return output_errors","264","        elif multioutput == 'uniform_average':","265","            # pass None as weights to np.average: uniform mean","266","            multioutput = None","267","","268","    return np.average(output_errors, weights=multioutput)","269","","270",""],"delete":[]}]}},"c52b6e128708fabbdfb25ffe6d44893ad553c1fb":{"changes":{"sklearn\/manifold\/tests\/test_spectral_embedding.py":"MODIFY","sklearn\/manifold\/spectral_embedding_.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY"},"diff":{"sklearn\/manifold\/tests\/test_spectral_embedding.py":[{"add":["177","    assert _check_with_col_sign_flipping(embed_amg, embed_arpack, 0.1e-4)","178","","179","    # same with special case in which amg is not actually used","180","    # regression test for #10715","181","    # affinity between nodes","182","    row = [0, 0, 1, 2, 3, 3, 4]","183","    col = [1, 2, 2, 3, 4, 5, 5]","184","    val = [100, 100, 100, 1, 100, 100, 100]","185","","186","    affinity = sparse.coo_matrix((val + val, (row + col, col + row)),","187","                                 shape=(6, 6)).toarray()","188","    se_amg.affinity = \"precomputed\"","189","    se_arpack.affinity = \"precomputed\"","190","    embed_amg = se_amg.fit_transform(affinity)","191","    embed_arpack = se_arpack.fit_transform(affinity)","192","    assert _check_with_col_sign_flipping(embed_amg, embed_arpack, 0.1e-4)"],"delete":["177","    assert _check_with_col_sign_flipping(embed_amg, embed_arpack, 0.05)"]}],"sklearn\/manifold\/spectral_embedding_.py":[{"add":["269","            _, diffusion_map = eigsh(","270","                laplacian, k=n_components, sigma=1.0, which='LM',","271","                tol=eigen_tol, v0=v0)","282","    elif eigen_solver == 'amg':","295","        _, diffusion_map = lobpcg(laplacian, X, M=M, tol=1.e-12,","296","                                  largest=False)","303","    if eigen_solver == \"lobpcg\":","313","            _, diffusion_map = eigh(laplacian)","323","            _, diffusion_map = lobpcg(laplacian, X, tol=1e-15,","324","                                      largest=False, maxiter=2000)"],"delete":["269","            lambdas, diffusion_map = eigsh(laplacian, k=n_components,","270","                                           sigma=1.0, which='LM',","271","                                           tol=eigen_tol, v0=v0)","282","    if eigen_solver == 'amg':","295","        lambdas, diffusion_map = lobpcg(laplacian, X, M=M, tol=1.e-12,","296","                                        largest=False)","303","    elif eigen_solver == \"lobpcg\":","313","            lambdas, diffusion_map = eigh(laplacian)","323","            lambdas, diffusion_map = lobpcg(laplacian, X, tol=1e-15,","324","                                            largest=False, maxiter=2000)"]}],"doc\/whats_new\/v0.22.rst":[{"add":["233","","234",":mod:`sklearn.manifold`","235",".......................","236","- |Fix| Fixed a bug where :func:`manifold.spectral_embedding` (and therefore","237","  :class:`manifold.SpectralEmedding` and `clustering.SpectralClustering`)","238","  computed wrong eigenvalues with ``solver='amg'`` when","239","  ``n_samples < 5 * n_components``. :pr:`14647` by `Andreas Mller`_.","240","  "],"delete":["268",":mod:`sklearn.metrics`","269","......................","270",""]}]}},"53f76d1a24ef42eb8c620fc1116c53db11dd07d9":{"changes":{"sklearn\/feature_extraction\/text.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY","sklearn\/feature_extraction\/tests\/test_text.py":"MODIFY"},"diff":{"sklearn\/feature_extraction\/text.py":[{"add":["17","from functools import partial","47","def _preprocess(doc, accent_function=None, lower=False):","48","    \"\"\"Chain together an optional series of text preprocessing steps to","49","    apply to a document.","50","","51","    Parameters","52","    ----------","53","    doc: str","54","        The string to preprocess","55","    accent_function: callable","56","        Function for handling accented characters. Common strategies include","57","        normalizing and removing.","58","    lower: bool","59","        Whether to use str.lower to lowercase all fo the text","60","","61","    Returns","62","    -------","63","    doc: str","64","        preprocessed string","65","    \"\"\"","66","    if lower:","67","        doc = doc.lower()","68","    if accent_function is not None:","69","        doc = accent_function(doc)","70","    return doc","71","","72","","73","def _analyze(doc, analyzer=None, tokenizer=None, ngrams=None,","74","             preprocessor=None, decoder=None, stop_words=None):","75","    \"\"\"Chain together an optional series of text processing steps to go from","76","    a single document to ngrams, with or without tokenizing or preprocessing.","77","","78","    If analyzer is used, only the decoder argument is used, as the analyzer is","79","    intended to replace the preprocessor, tokenizer, and ngrams steps.","80","","81","    Parameters","82","    ----------","83","    analyzer: callable","84","    tokenizer: callable","85","    ngrams: callable","86","    preprocessor: callable","87","    decoder: callable","88","    stop_words: list","89","","90","    Returns","91","    -------","92","    ngrams: list","93","        A sequence of tokens, possibly with pairs, triples, etc.","94","    \"\"\"","95","","96","    if decoder is not None:","97","        doc = decoder(doc)","98","    if analyzer is not None:","99","        doc = analyzer(doc)","100","    else:","101","        if preprocessor is not None:","102","            doc = preprocessor(doc)","103","        if tokenizer is not None:","104","            doc = tokenizer(doc)","105","        if ngrams is not None:","106","            if stop_words is not None:","107","                doc = ngrams(doc, stop_words)","108","            else:","109","                doc = ngrams(doc)","110","    return doc","111","","112","","303","            strip_accents = None","314","        return partial(","315","            _preprocess, accent_function=strip_accents, lower=self.lowercase","316","        )","323","        return token_pattern.findall","396","            return partial(","397","                _analyze, analyzer=self.analyzer, decoder=self.decode","398","            )","403","            return partial(_analyze, ngrams=self._char_ngrams,","404","                           preprocessor=preprocess, decoder=self.decode)","407","            return partial(_analyze, ngrams=self._char_wb_ngrams,","408","                           preprocessor=preprocess, decoder=self.decode)","415","            return partial(_analyze, ngrams=self._word_ngrams,","416","                           tokenizer=tokenize, preprocessor=preprocess,","417","                           decoder=self.decode, stop_words=stop_words)"],"delete":["234","        # unfortunately python functools package does not have an efficient","235","        # `compose` function that would have allowed us to chain a dynamic","236","        # number of functions. However the cost of a lambda call is a few","237","        # hundreds of nanoseconds which is negligible when compared to the","238","        # cost of tokenizing a string of 1000 chars for instance.","239","        noop = lambda x: x","240","","243","            strip_accents = noop","254","        if self.lowercase:","255","            return lambda x: strip_accents(x.lower())","256","        else:","257","            return strip_accents","264","        return lambda doc: token_pattern.findall(doc)","337","            return lambda doc: self.analyzer(self.decode(doc))","342","            return lambda doc: self._char_ngrams(preprocess(self.decode(doc)))","345","            return lambda doc: self._char_wb_ngrams(","346","                preprocess(self.decode(doc)))","353","            return lambda doc: self._word_ngrams(","354","                tokenize(preprocess(self.decode(doc))), stop_words)"]}],"doc\/whats_new\/v0.22.rst":[{"add":["136",":mod:`sklearn.feature_extraction`","137",".......................","138","","139","- |Fix| Functions created by build_preprocessor and build_analyzer of","140","  :class:`feature_extraction.text.VectorizerMixin` can now be pickled.","141","  :pr:`14430` by :user:`Dillon Niederhut <deniederhut>`.","142","","231","","240","","241","- |Fix| KernelCenterer now throws error when fit on non-square","273","  :pr:`14336` by :user:`Gregory Dexter <gdex1>`.","284",""],"delete":["224","  ","233"," ","234","- |Fix| KernelCenterer now throws error when fit on non-square ","266","  :pr:`14336` by :user:`Gregory Dexter <gdex1>`.  ","277","  "]}],"sklearn\/feature_extraction\/tests\/test_text.py":[{"add":["482","    processor = v3.build_preprocessor()","483","    text = (\"J'ai mang du kangourou  ce midi, \"","484","            \"c'tait pas trs bon.\")","485","    expected = strip_accents_ascii(text)","486","    result = processor(text)","487","    assert expected == result","891","@pytest.mark.parametrize('factory', [","892","    CountVectorizer.build_analyzer,","893","    CountVectorizer.build_preprocessor,","894","    CountVectorizer.build_tokenizer,","895","])","896","def test_pickling_built_processors(factory):","897","    \"\"\"Tokenizers cannot be pickled","898","    https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/12833","899","    \"\"\"","900","    vec = CountVectorizer()","901","    function = factory(vec)","902","    text = (\"J'ai mang du kangourou  ce midi, \"","903","            \"c'tait pas trs bon.\")","904","    roundtripped_function = pickle.loads(pickle.dumps(function))","905","    expected = function(text)","906","    result = roundtripped_function(text)","907","    assert result == expected","908","","909",""],"delete":["482","    assert v3.build_preprocessor() == strip_accents_ascii"]}]}},"36b688eb04de0172dace761ca63616f18d615542":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/linear_model\/tests\/test_logistic.py":"MODIFY","sklearn\/linear_model\/logistic.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["14",":mod:`sklearn.linear_model`","15","...........................","16","- |Fix| Fixed a bug in :class:`linear_model.LogisticRegressionCV` where","17","  ``refit=False`` would fail depending on the ``'multiclass'`` and","18","  ``'penalty'`` parameters (regression introduced in 0.21). :pr:`14087` by","19","  `Nicolas Hug`_."],"delete":[]}],"sklearn\/linear_model\/tests\/test_logistic.py":[{"add":["1534","@pytest.mark.parametrize('penalty', ('l2', 'elasticnet'))","1535","@pytest.mark.parametrize('multi_class', ('ovr', 'multinomial', 'auto'))","1536","def test_LogisticRegressionCV_no_refit(penalty, multi_class):","1546","    if penalty == 'elasticnet':","1547","        l1_ratios = np.linspace(0, 1, 2)","1548","    else:","1549","        l1_ratios = None","1551","    lrcv = LogisticRegressionCV(penalty=penalty, Cs=Cs, solver='saga',"],"delete":["1534","@pytest.mark.parametrize('multi_class', ('ovr', 'multinomial'))","1535","def test_LogisticRegressionCV_no_refit(multi_class):","1545","    l1_ratios = np.linspace(0, 1, 2)","1547","    lrcv = LogisticRegressionCV(penalty='elasticnet', Cs=Cs, solver='saga',"]}],"sklearn\/linear_model\/logistic.py":[{"add":["2172","                if multi_class == 'ovr':","2182","                if self.penalty == 'elasticnet':","2183","                    best_indices_l1 = best_indices \/\/ len(self.Cs_)","2184","                    self.l1_ratio_.append(np.mean(l1_ratios_[best_indices_l1]))","2185","                else:","2186","                    self.l1_ratio_.append(None)"],"delete":["2172","                if self.multi_class == 'ovr':","2182","                best_indices_l1 = best_indices \/\/ len(self.Cs_)","2183","                self.l1_ratio_.append(np.mean(l1_ratios_[best_indices_l1]))"]}]}},"e912fe7984127a5ca9dd35ffe316b69aa33d8cc9":{"changes":{"sklearn\/manifold\/tests\/test_mds.py":"MODIFY","sklearn\/manifold\/tests\/test_locally_linear.py":"MODIFY","sklearn\/manifold\/tests\/test_spectral_embedding.py":"MODIFY","sklearn\/manifold\/tests\/test_t_sne.py":"MODIFY"},"diff":{"sklearn\/manifold\/tests\/test_mds.py":[{"add":["2","import pytest","33","    with pytest.raises(ValueError):","34","        mds.smacof(sim)","41","    with pytest.raises(ValueError):","42","        mds.smacof(sim)","53","    with pytest.raises(ValueError):","54","        mds.smacof(sim, init=Z, n_init=1)"],"delete":["4","from sklearn.utils.testing import assert_raises","33","    assert_raises(ValueError, mds.smacof, sim)","40","    assert_raises(ValueError, mds.smacof, sim)","51","    assert_raises(ValueError, mds.smacof, sim, init=Z, n_init=1)"]}],"sklearn\/manifold\/tests\/test_locally_linear.py":[{"add":["5","import pytest","132","    with pytest.raises(ValueError):","133","        f(manifold.locally_linear_embedding(M, 2, 1,","134","                                            method='standard',","135","                                            eigen_solver='arpack'))"],"delete":["10","from sklearn.utils.testing import assert_raises","132","    assert_raises(ValueError, f(manifold.locally_linear_embedding),","133","                  M, 2, 1, method='standard', eigen_solver='arpack')"]}],"sklearn\/manifold\/tests\/test_spectral_embedding.py":[{"add":["204","    with pytest.raises(ValueError):","205","        se.fit(S)","212","    with pytest.raises(ValueError):","213","        se.fit(S)"],"delete":["19","from sklearn.utils.testing import assert_raises","205","    assert_raises(ValueError, se.fit, S)","212","    assert_raises(ValueError, se.fit, S)"]}],"sklearn\/manifold\/tests\/test_t_sne.py":[{"add":["304","    with pytest.raises(ValueError, match=\"early_exaggeration .*\"):","305","        tsne.fit_transform(np.array([[0.0], [0.0]]))","311","    with pytest.raises(ValueError, match=\"n_iter .*\"):","312","        tsne.fit_transform(np.array([[0.0], [0.0]]))","318","    with pytest.raises(ValueError, match=\".* square distance matrix\"):","319","        tsne.fit_transform(np.array([[0.0], [1.0]]))","327","        with pytest.raises(ValueError, match=\"All distances .*precomputed.*\"):","328","            tsne.fit_transform(bad_dist)","338","    with pytest.raises(ValueError, match=\"All distances .*metric given.*\"):","339","        tsne.fit_transform(X)","346","    with pytest.raises(ValueError, match=m):","347","        tsne.fit_transform(np.array([[0.0], [1.0]]))","367","    with pytest.raises(ValueError, match=\"Unknown metric not available.*\"):","368","        tsne.fit_transform(np.array([[0.0], [1.0]]))","371","    with pytest.raises(ValueError, match=\"Metric 'not available' not valid.*\"):","372","        tsne.fit_transform(np.array([[0.0], [1.0]]))","378","    with pytest.raises(ValueError, match=\"'method' must be 'barnes_hut' or \"):","379","        tsne.fit_transform(np.array([[0.0], [1.0]]))","386","        with pytest.raises(ValueError, match=\"'angle' must be between \"","387","                                             \"0.0 - 1.0\"):","388","            tsne.fit_transform(np.array([[0.0], [1.0]]))","394","    with pytest.raises(ValueError, match=\"The parameter init=\\\"pca\\\" cannot\"","395","                                         \" be used with\"","396","                                         \" metric=\\\"precomputed\\\".\"):","397","        tsne.fit_transform(np.array([[0.0], [1.0]]))","403","    with pytest.raises(ValueError, match=\"'n_components' should be .*\"):","404","        tsne.fit_transform(np.array([[0.0], [1.0]]))","572","    with pytest.raises(TypeError, match=\"A sparse matrix was.*\"):","573","        tsne.fit_transform(X_csr)"],"delete":["13","from sklearn.utils.testing import assert_raises_regexp","305","    assert_raises_regexp(ValueError, \"early_exaggeration .*\",","306","                         tsne.fit_transform, np.array([[0.0], [0.0]]))","312","    assert_raises_regexp(ValueError, \"n_iter .*\", tsne.fit_transform,","313","                         np.array([[0.0], [0.0]]))","319","    assert_raises_regexp(ValueError, \".* square distance matrix\",","320","                         tsne.fit_transform, np.array([[0.0], [1.0]]))","328","        assert_raises_regexp(ValueError, \"All distances .*precomputed.*\",","329","                             tsne.fit_transform, bad_dist)","339","    assert_raises_regexp(ValueError, \"All distances .*metric given.*\",","340","                         tsne.fit_transform, X)","347","    assert_raises_regexp(ValueError, m, tsne.fit_transform,","348","                         np.array([[0.0], [1.0]]))","368","    assert_raises_regexp(ValueError, \"Unknown metric not available.*\",","369","                         tsne.fit_transform, np.array([[0.0], [1.0]]))","372","    assert_raises_regexp(ValueError, \"Metric 'not available' not valid.*\",","373","                         tsne.fit_transform, np.array([[0.0], [1.0]]))","379","    assert_raises_regexp(ValueError, \"'method' must be 'barnes_hut' or \",","380","                         tsne.fit_transform, np.array([[0.0], [1.0]]))","387","        assert_raises_regexp(ValueError, \"'angle' must be between 0.0 - 1.0\",","388","                             tsne.fit_transform, np.array([[0.0], [1.0]]))","394","    assert_raises_regexp(ValueError, \"The parameter init=\\\"pca\\\" cannot be \"","395","                         \"used with metric=\\\"precomputed\\\".\",","396","                         tsne.fit_transform, np.array([[0.0], [1.0]]))","402","    assert_raises_regexp(ValueError, \"'n_components' should be .*\",","403","                         tsne.fit_transform, np.array([[0.0], [1.0]]))","571","    assert_raises_regexp(TypeError, \"A sparse matrix was.*\",","572","                         tsne.fit_transform, X_csr)"]}]}},"1efbe186ebc13990756eaf822fce60a931e3c87b":{"changes":{"sklearn\/neighbors\/tests\/test_neighbors.py":"MODIFY"},"diff":{"sklearn\/neighbors\/tests\/test_neighbors.py":[{"add":["973","        if (issubclass(cls, neighbors.KNeighborsClassifier) or","974","                issubclass(cls, neighbors.KNeighborsRegressor)):","1217","            assert_array_equal(graph.A, [[0, 1], [1, 0]])","1218","            assert_array_equal(graph.data, [1, 1])","1219","            assert_array_equal(graph.indices, [1, 0])"],"delete":["973","        if (isinstance(cls, neighbors.KNeighborsClassifier) or","974","                isinstance(cls, neighbors.KNeighborsRegressor)):","1217","            assert_array_equal(rng.A, [[0, 1], [1, 0]])","1218","            assert_array_equal(rng.data, [1, 1])","1219","            assert_array_equal(rng.indices, [1, 0])"]}]}},"af82929bfa2dbac2453c22641969d4d69dd324f1":{"changes":{"sklearn\/svm\/libsvm.pyx":"MODIFY","sklearn\/svm\/base.py":"MODIFY","sklearn\/svm\/tests\/test_svm.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY"},"diff":{"sklearn\/svm\/libsvm.pyx":[{"add":["220","    if svm_type == 0 or svm_type == 1:","221","        n_class_SV = np.empty(n_class, dtype=np.int32)","222","        copy_nSV(n_class_SV.data, model)","223","    else:","224","        # OneClass and SVR are considered to have 2 classes","225","        n_class_SV = np.array([SV_len, SV_len], dtype=np.int32)"],"delete":["219","    # TODO: do only in classification","221","    n_class_SV = np.empty(n_class, dtype=np.int32)","222","    copy_nSV(n_class_SV.data, model)"]}],"sklearn\/svm\/base.py":[{"add":["246","        self.support_, self.support_vectors_, self._n_support, \\","270","            self.intercept_, self._n_support, \\","330","            X, self.support_, self.support_vectors_, self._n_support,","356","            self.probability, self._n_support,","409","            X, self.support_, self.support_vectors_, self._n_support,","435","            self.probability, self._n_support,","486","    @property","487","    def n_support_(self):","488","        try:","489","            check_is_fitted(self)","490","        except NotFittedError:","491","            raise AttributeError","492","","493","        svm_type = LIBSVM_IMPL.index(self._impl)","494","        if svm_type in (0, 1):","495","            return self._n_support","496","        else:","497","            # SVR and OneClass","498","            # _n_support has size 2, we make it size 1","499","            return np.array([self._n_support[0]])","500","","685","            X, self.support_, self.support_vectors_, self._n_support,","712","            self.probability, self._n_support,","721","            coef = _one_vs_one_coef(self.dual_coef_, self._n_support,"],"delete":["246","        self.support_, self.support_vectors_, self.n_support_, \\","270","            self.intercept_, self.n_support_, \\","330","            X, self.support_, self.support_vectors_, self.n_support_,","356","            self.probability, self.n_support_,","409","            X, self.support_, self.support_vectors_, self.n_support_,","435","            self.probability, self.n_support_,","670","            X, self.support_, self.support_vectors_, self.n_support_,","697","            self.probability, self.n_support_,","706","            coef = _one_vs_one_coef(self.dual_coef_, self.n_support_,"]}],"sklearn\/svm\/tests\/test_svm.py":[{"add":["1190","","1191","","1192","def test_n_support_oneclass_svr():","1193","    # Make n_support is correct for oneclass and SVR (used to be","1194","    # non-initialized)","1195","    # this is a non regression test for issue #14774","1196","    X = np.array([[0], [0.44], [0.45], [0.46], [1]])","1197","    clf = svm.OneClassSVM()","1198","    assert not hasattr(clf, 'n_support_')","1199","    clf.fit(X)","1200","    assert clf.n_support_ == clf.support_vectors_.shape[0]","1201","    assert clf.n_support_.size == 1","1202","    assert clf.n_support_ == 3","1203","","1204","    y = np.arange(X.shape[0])","1205","    reg = svm.SVR().fit(X, y)","1206","    assert reg.n_support_ == reg.support_vectors_.shape[0]","1207","    assert reg.n_support_.size == 1","1208","    assert reg.n_support_ == 4"],"delete":[]}],"doc\/whats_new\/v0.22.rst":[{"add":["539","- |Fix| The `n_support_` attribute of :class:`svm.SVR` and","540","  :class:`svm.OneClassSVM` was previously non-initialized, and had size 2. It","541","  has now size 1 with the correct value. :pr:`15099` by `Nicolas Hug`_.","542",""],"delete":[]}]}},"e2b6bff0aca06f1276e2486c67169e3ca264d1f1":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/tree\/tests\/test_export.py":"MODIFY","sklearn\/tree\/export.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["21",":mod:`sklearn.tree`","22","...................","23","","24","- |Fix| Fixed bug in :func:`tree.export_text` when the tree has one feature and ","25","  a single feature name is passed in. :pr:`14053` by `Thomas Fan`","26",""],"delete":[]}],"sklearn\/tree\/tests\/test_export.py":[{"add":["398","    X_single = [[-2], [-1], [-1], [1], [1], [2]]","399","    reg = DecisionTreeRegressor(max_depth=2, random_state=0)","400","    reg.fit(X_single, y_mo)","401","","402","    expected_report = dedent(\"\"\"","403","    |--- first <= 0.0","404","    |   |--- value: [-1.0, -1.0]","405","    |--- first >  0.0","406","    |   |--- value: [1.0, 1.0]","407","    \"\"\").lstrip()","408","    assert export_text(reg, decimals=1,","409","                       feature_names=['first']) == expected_report","410","    assert export_text(reg, decimals=1, show_weights=True,","411","                       feature_names=['first']) == expected_report","412",""],"delete":[]}],"sklearn\/tree\/export.py":[{"add":["892","        feature_names_ = [feature_names[i] if i != _tree.TREE_UNDEFINED","893","                          else None for i in tree_.feature]"],"delete":["892","        feature_names_ = [feature_names[i] for i in tree_.feature]"]}]}},"992ed41ecc693f93a94ffd8cca52117af16e096e":{"changes":{"sklearn\/utils\/tests\/test_utils.py":"MODIFY","sklearn\/compose\/tests\/test_column_transformer.py":"MODIFY","sklearn\/utils\/__init__.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY"},"diff":{"sklearn\/utils\/tests\/test_utils.py":[{"add":["12","                                   assert_allclose_dense_sparse,","15","from sklearn.utils import _array_indexing","369","@pytest.mark.parametrize(\"array_type\", ['array', 'sparse', 'dataframe'])","370","def test_safe_indexing_mask_axis_1(array_type):","371","    # regression test for #14510","372","    # check that boolean array-like and boolean array lead to the same indexing","373","    # even in NumPy < 1.12","374","    if array_type == 'array':","375","        array_constructor = np.asarray","376","    elif array_type == 'sparse':","377","        array_constructor = sp.csr_matrix","378","    elif array_type == 'dataframe':","379","        pd = pytest.importorskip('pandas')","380","        array_constructor = pd.DataFrame","381","","382","    X = array_constructor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])","383","    mask = [True, False, True]","384","    mask_array = np.array(mask)","385","    X_masked = safe_indexing(X, mask, axis=1)","386","    X_masked_array = safe_indexing(X, mask_array, axis=1)","387","    assert_allclose_dense_sparse(X_masked, X_masked_array)","388","","389","","390","def test_array_indexing_array_error():","391","    X = np.array([[0, 1], [2, 3]])","392","    mask = [True, False]","393","    with pytest.raises(ValueError, match=\"'axis' should be either 0\"):","394","        _array_indexing(X, mask, axis=3)","395","","396",""],"delete":[]}],"sklearn\/compose\/tests\/test_column_transformer.py":[{"add":["18","from sklearn.preprocessing import FunctionTransformer","1111","","1112","","1113","@pytest.mark.parametrize(\"array_type\", [np.asarray, sparse.csr_matrix])","1114","def test_column_transformer_mask_indexing(array_type):","1115","    # Regression test for #14510","1116","    # Boolean array-like does not behave as boolean array with NumPy < 1.12","1117","    # and sparse matrices as well","1118","    X = np.transpose([[1, 2, 3], [4, 5, 6], [5, 6, 7], [8, 9, 10]])","1119","    X = array_type(X)","1120","    column_transformer = ColumnTransformer(","1121","        [('identity', FunctionTransformer(), [False, True, False, True])]","1122","    )","1123","    X_trans = column_transformer.fit_transform(X)","1124","    assert X_trans.shape == (3, 2)"],"delete":[]}],"sklearn\/utils\/__init__.py":[{"add":["20","from .fixes import np_version","228","def _array_indexing(array, key, axis=0):","229","    \"\"\"Index an array consistently across NumPy version.\"\"\"","230","    if axis not in (0, 1):","231","        raise ValueError(","232","            \"'axis' should be either 0 (to index rows) or 1 (to index \"","233","            \" column). Got {} instead.\".format(axis)","234","        )","235","    if np_version < (1, 12) or issparse(array):","236","        # check if we have an boolean array-likes to make the proper indexing","237","        key_array = np.asarray(key)","238","        if np.issubdtype(key_array.dtype, np.bool_):","239","            key = key_array","240","    return array[key] if axis == 0 else array[:, key]","241","","242","","284","            return _array_indexing(X, indices, axis=0)","374","            return _array_indexing(X, key, axis=1)","389","            idx = safe_indexing(np.arange(n_columns), key)"],"delete":["268","            return X[indices]","358","            return X[:, key]","373","            idx = np.arange(n_columns)[key]"]}],"doc\/whats_new\/v0.22.rst":[{"add":["63",":mod:`sklearn.compose`","64","......................","65","","66","- |Fix| Fixed a bug in :class:`compose.ColumnTransformer` which failed to","67","  select the proper columns when using a boolean list, with NumPy older than","68","  1.12.","69","  :pr:`14510` by :user:`Guillaume Lemaitre <glemaitre>`.","70",""],"delete":[]}]}},"df1e3fbe86a24dd01d0455ef434f7ea22d0ddba0":{"changes":{"sklearn\/linear_model\/tests\/test_ridge.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY","sklearn\/linear_model\/base.py":"MODIFY","sklearn\/linear_model\/ridge.py":"MODIFY"},"diff":{"sklearn\/linear_model\/tests\/test_ridge.py":[{"add":["1212","","1213","","1214","def test_ridge_sag_with_X_fortran():","1215","    # check that Fortran array are converted when using SAG solver","1216","    X, y = make_regression(random_state=42)","1217","    # for the order of X and y to not be C-ordered arrays","1218","    X = np.asfortranarray(X)","1219","    X = X[::2, :]","1220","    y = y[::2]","1221","    Ridge(solver='sag').fit(X, y)"],"delete":[]}],"doc\/whats_new\/v0.22.rst":[{"add":["117","- |Enhancement| :class:`linear_model.BayesianRidge` now accepts hyperparameters","129","  :pr:`14108`, :pr:`14170` by :user:`Alex Henrie <alexhenrie>`.","130","","131","- |Fix| :class:`linear_model.Ridge` with `solver='sag'` now accepts F-ordered","132","  and non-contiguous arrays and makes a conversion instead of failing.","133","  :pr:`14458` by :user:`Guillaume Lemaitre <glemaitre>`."],"delete":["117","- |Enhancement| :class:`linearmodel.BayesianRidge` now accepts hyperparameters","129","  :pr:`14108`, pr:`14170` by :user:`Alex Henrie <alexhenrie>`."]}],"sklearn\/linear_model\/base.py":[{"add":["93","        X = np.ascontiguousarray(X)"],"delete":[]}],"sklearn\/linear_model\/ridge.py":[{"add":["411","        y = check_array(y, dtype=X.dtype, ensure_2d=False, order=None)"],"delete":["411","        y = check_array(y, dtype=X.dtype, ensure_2d=False, order=\"C\")"]}]}},"38af35db9c654d2b2ff60f69861910413636b9f9":{"changes":{"sklearn\/ensemble\/_hist_gradient_boosting\/gradient_boosting.py":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_gradient_boosting.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY"},"diff":{"sklearn\/ensemble\/_hist_gradient_boosting\/gradient_boosting.py":[{"add":["1044","        if (self.loss == 'categorical_crossentropy' and","1045","                self.n_trees_per_iteration_ == 1):","1046","            raise ValueError(\"'categorical_crossentropy' is not suitable for \"","1047","                             \"a binary classification problem. Please use \"","1048","                             \"'auto' or 'binary_crossentropy' instead.\")","1049",""],"delete":[]}],"sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_gradient_boosting.py":[{"add":["428","def test_crossentropy_binary_problem():","429","    # categorical_crossentropy should only be used if there are more than two","430","    # classes present. PR #14869","431","    X = [[1], [0]]","432","    y = [0, 1]","433","    gbrt = HistGradientBoostingClassifier(loss='categorical_crossentropy')","434","    with pytest.raises(ValueError,","435","                       match=\"'categorical_crossentropy' is not suitable for\"):","436","        gbrt.fit(X, y)","437","","438",""],"delete":[]}],"doc\/whats_new\/v0.22.rst":[{"add":["194","  - |Fix| :class:`ensemble.HistGradientBoostingClassifier` now raises an error","195","    if ``categorical_crossentropy`` loss is given for a binary classification","196","    problem. :pr:`14869` by `Adrin Jalali`_."],"delete":[]}]}},"220e146a7fe2bb4f3d45d30cab5505bc22f2db85":{"changes":{"sklearn\/linear_model\/tests\/test_logistic.py":"MODIFY","sklearn\/linear_model\/logistic.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY"},"diff":{"sklearn\/linear_model\/tests\/test_logistic.py":[{"add":["15","from sklearn.model_selection import cross_val_score","1763","","1764","","1765","def test_scores_attribute_layout_elasticnet():","1766","    # Non regression test for issue #14955.","1767","    # when penalty is elastic net the scores_ attribute has shape","1768","    # (n_classes, n_Cs, n_l1_ratios)","1769","    # We here make sure that the second dimension indeed corresponds to Cs and","1770","    # the third dimension corresponds to l1_ratios.","1771","","1772","    X, y = make_classification(n_samples=1000, random_state=0)","1773","    cv = StratifiedKFold(n_splits=5, shuffle=False)","1774","","1775","    l1_ratios = [.1, .9]","1776","    Cs = [.1, 1, 10]","1777","","1778","    lrcv = LogisticRegressionCV(penalty='elasticnet', solver='saga',","1779","                                l1_ratios=l1_ratios, Cs=Cs, cv=cv,","1780","                                random_state=0)","1781","    lrcv.fit(X, y)","1782","","1783","    avg_scores_lrcv = lrcv.scores_[1].mean(axis=0)  # average over folds","1784","","1785","    for i, C in enumerate(Cs):","1786","        for j, l1_ratio in enumerate(l1_ratios):","1787","","1788","            lr = LogisticRegression(penalty='elasticnet', solver='saga', C=C,","1789","                                    l1_ratio=l1_ratio, random_state=0)","1790","","1791","            avg_score_lr = cross_val_score(lr, X, y, cv=cv).mean()","1792","            assert avg_scores_lrcv[i, j] == pytest.approx(avg_score_lr)"],"delete":[]}],"sklearn\/linear_model\/logistic.py":[{"add":["2203","            # with n_cs=2 and n_l1_ratios=3","2204","            # the layout of scores is","2205","            # [c1, c2, c1, c2, c1, c2]","2206","            #   l1_1 ,  l1_2 ,  l1_3","2207","            # To get a 2d array with the following layout","2208","            #      l1_1, l1_2, l1_3","2209","            # c1 [[ .  ,  .  ,  .  ],","2210","            # c2  [ .  ,  .  ,  .  ]]","2211","            # We need to first reshape and then transpose.","2212","            # The same goes for the other arrays","2215","                    (len(folds), self.l1_ratios_.size, self.Cs_.size, -1))","2216","                self.coefs_paths_[cls] = np.transpose(self.coefs_paths_[cls],","2217","                                                      (0, 2, 1, 3))","2220","                    (len(folds), self.l1_ratios_.size, self.Cs_.size))","2221","                self.scores_[cls] = np.transpose(self.scores_[cls], (0, 2, 1))","2222","","2224","                (-1, len(folds), self.l1_ratios_.size, self.Cs_.size))","2225","            self.n_iter_ = np.transpose(self.n_iter_, (0, 1, 3, 2))"],"delete":["2205","                    (len(folds), self.Cs_.size, self.l1_ratios_.size, -1))","2208","                    (len(folds), self.Cs_.size, self.l1_ratios_.size))","2210","                (-1, len(folds), self.Cs_.size, self.l1_ratios_.size))"]}],"doc\/whats_new\/v0.22.rst":[{"add":["345","- |FIX| Fixed a bug in :class:`linear_model.LogisticRegressionCV` where the","346","  ``scores_``, ``n_iter_`` and ``coefs_paths_`` attribute would have a wrong","347","  ordering with ``penalty='elastic-net'``. :pr:`15044` by `Nicolas Hug`_","348",""],"delete":[]}]}},"3d606cf8ba18949c3083e460e9ccda393c6c20b2":{"changes":{"sklearn\/model_selection\/_validation.py":"MODIFY","sklearn\/model_selection\/tests\/test_search.py":"MODIFY","sklearn\/model_selection\/_search.py":"MODIFY"},"diff":{"sklearn\/model_selection\/_validation.py":[{"add":["496","        # clone after setting parameters in case any parameters","497","        # are estimators (like pipeline steps)","498","        # because pipeline doesn't clone steps in fit","499","        cloned_parameters = {}","500","        for k, v in parameters.items():","501","            cloned_parameters[k] = clone(v, safe=False)","502","","503","        estimator = estimator.set_params(**cloned_parameters)"],"delete":["496","        estimator.set_params(**parameters)"]}],"sklearn\/model_selection\/tests\/test_search.py":[{"add":["65","from sklearn.linear_model import Ridge, SGDClassifier, LinearRegression","200","def test_grid_search_pipeline_steps():","201","    # check that parameters that are estimators are cloned before fitting","202","    pipe = Pipeline([('regressor', LinearRegression())])","203","    param_grid = {'regressor': [LinearRegression(), Ridge()]}","204","    grid_search = GridSearchCV(pipe, param_grid, cv=2)","205","    grid_search.fit(X, y)","206","    regressor_results = grid_search.cv_results_['param_regressor']","207","    assert isinstance(regressor_results[0], LinearRegression)","208","    assert isinstance(regressor_results[1], Ridge)","209","    assert not hasattr(regressor_results[0], 'coef_')","210","    assert not hasattr(regressor_results[1], 'coef_')","211","    assert regressor_results[0] is not grid_search.best_estimator_","212","    assert regressor_results[1] is not grid_search.best_estimator_","213","    # check that we didn't modify the parameter grid that was passed","214","    assert not hasattr(param_grid['regressor'][0], 'coef_')","215","    assert not hasattr(param_grid['regressor'][1], 'coef_')","216","","217",""],"delete":["65","from sklearn.linear_model import Ridge, SGDClassifier"]}],"sklearn\/model_selection\/_search.py":[{"add":["729","            # we clone again after setting params in case some","730","            # of the params are estimators as well.","731","            self.best_estimator_ = clone(clone(base_estimator).set_params(","732","                **self.best_params_))"],"delete":["729","            self.best_estimator_ = clone(base_estimator).set_params(","730","                **self.best_params_)"]}]}},"f6923a297130ce95a5fa1adf5c049d59800d7408":{"changes":{"sklearn\/feature_extraction\/image.py":"MODIFY"},"diff":{"sklearn\/feature_extraction\/image.py":[{"add":["324","        Determines the random number generator used for random sampling when","325","        `max_patches` is not None. Use an int to make the randomness","326","        deterministic.","327","        See :term:`Glossary <random_state>`.","456","        Determines the random number generator used for random sampling when","457","        `max_patches` is not None. Use an int to make the randomness","458","        deterministic.","459","        See :term:`Glossary <random_state>`.","460",""],"delete":["324","        Pseudo number generator state used for random sampling to use if","325","        `max_patches` is not None.  If int, random_state is the seed used by","326","        the random number generator; If RandomState instance, random_state is","327","        the random number generator; If None, the random number generator is","328","        the RandomState instance used by `np.random`.","457","        If int, random_state is the seed used by the random number generator;","458","        If RandomState instance, random_state is the random number generator;","459","        If None, the random number generator is the RandomState instance used","460","        by `np.random`."]}]}},"e94f5de906c70cda43854f47925879fcf02204f4":{"changes":{"doc\/whats_new\/v0.22.rst":"MODIFY","sklearn\/decomposition\/sparse_pca.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.22.rst":[{"add":["19","- :class:`decomposition.SparsePCA` where `normalize_components` has no effect","20","  due to deprecation."],"delete":["19","..","20","    TO FILL IN AS WE GO"]}],"sklearn\/decomposition\/sparse_pca.py":[{"add":["226","        X = X - self.mean_"],"delete":["226","","227","        if self.normalize_components:","228","            X = X - self.mean_","233","        if not self.normalize_components:","234","            s = np.sqrt((U ** 2).sum(axis=0))","235","            s[s == 0] = 1","236","            U \/= s","237",""]}]}},"1c116807ce1e925f73c87c34e586520d455682d6":{"changes":{"sklearn\/preprocessing\/tests\/test_label.py":"MODIFY","sklearn\/preprocessing\/tests\/test_base.py":"MODIFY","sklearn\/preprocessing\/tests\/test_data.py":"MODIFY","sklearn\/preprocessing\/tests\/test_discretization.py":"MODIFY","sklearn\/preprocessing\/tests\/test_encoders.py":"MODIFY"},"diff":{"sklearn\/preprocessing\/tests\/test_label.py":[{"add":["133","    with pytest.raises(ValueError):","134","        lb.transform(multi_label)","137","    with pytest.raises(ValueError):","138","        lb.transform([])","139","    with pytest.raises(ValueError):","140","        lb.inverse_transform([])","142","    with pytest.raises(ValueError):","143","        LabelBinarizer(neg_label=2, pos_label=1)","144","    with pytest.raises(ValueError):","145","        LabelBinarizer(neg_label=2, pos_label=2)","147","    with pytest.raises(ValueError):","148","        LabelBinarizer(neg_label=1, pos_label=2, sparse_output=True)","151","    with pytest.raises(ValueError):","152","        _inverse_binarize_thresholding(y=csr_matrix([[1, 2], [2, 1]]),","153","                                       output_type=\"foo\", classes=[1, 2],","154","                                       threshold=0)","158","    with pytest.raises(ValueError):","159","        LabelBinarizer().fit_transform(y_seq_of_seqs)","162","    with pytest.raises(ValueError):","163","        _inverse_binarize_thresholding(y=csr_matrix([[1, 2], [2, 1]]),","164","                                       output_type=\"foo\",","165","                                       classes=[1, 2, 3],","166","                                       threshold=0)","169","    with pytest.raises(ValueError):","170","        _inverse_binarize_thresholding(y=np.array([[1, 2, 3], [2, 1, 3]]),","171","                                       output_type=\"binary\",","172","                                       classes=[1, 2, 3],","173","                                       threshold=0)","176","    with pytest.raises(ValueError):","177","        LabelBinarizer().fit(np.array([[1, 3], [2, 1]]))","178","    with pytest.raises(ValueError):","179","        label_binarize(np.array([[1, 3], [2, 1]]), [1, 2, 3])","216","    with pytest.raises(ValueError):","217","        le.transform([0, 6])","225","    with pytest.raises(ValueError, match=msg):","226","        le.transform(\"apple\")","232","    with pytest.raises(ValueError):","233","        le.transform([])","234","    with pytest.raises(ValueError):","235","        le.inverse_transform([])","241","    with pytest.raises(ValueError, match=msg):","242","        le.inverse_transform([-2])","243","    with pytest.raises(ValueError, match=msg):","244","        le.inverse_transform([-2, -3, -4])","248","    with pytest.raises(ValueError, match=msg):","249","        le.inverse_transform(\"\")","307","    with pytest.raises(ValueError):","308","        mlb.inverse_transform(csr_matrix(np.array([[0, 1, 1],","309","                                                   [2, 0, 0],","310","                                                   [1, 1, 0]])))","395","    with pytest.raises(ValueError, match=err_msg):","396","        mlb.fit(inp)","459","    with pytest.raises(TypeError):","460","        mlb.fit_transform([({}), ({}, {'a': 'b'})])","475","    with pytest.raises(ValueError):","476","        mlb.inverse_transform(np.array([[1, 3]]))","483","    with pytest.raises(ValueError):","484","        mlb.inverse_transform(np.array([[1]]))","485","    with pytest.raises(ValueError):","486","        mlb.inverse_transform(np.array([[1, 1, 1]]))","510","            with pytest.raises(ValueError):","511","                label_binarize(y, classes, neg_label=neg_label,","512","                               pos_label=pos_label,","513","                               sparse_output=sparse_output)","577","    with pytest.raises(ValueError):","578","        label_binarize(y, classes, neg_label=-1, pos_label=pos_label,","579","                       sparse_output=True)","596","    with pytest.raises(ValueError):","597","        label_binarize(y, classes, neg_label=-1, pos_label=pos_label,","598","                       sparse_output=True)","602","    with pytest.raises(ValueError):","603","        label_binarize([0, 2], classes=[0, 2], pos_label=0, neg_label=1)"],"delete":["14","from sklearn.utils.testing import assert_raises","15","from sklearn.utils.testing import assert_raise_message","135","    assert_raises(ValueError, lb.transform, multi_label)","138","    assert_raises(ValueError, lb.transform, [])","139","    assert_raises(ValueError, lb.inverse_transform, [])","141","    assert_raises(ValueError, LabelBinarizer, neg_label=2, pos_label=1)","142","    assert_raises(ValueError, LabelBinarizer, neg_label=2, pos_label=2)","144","    assert_raises(ValueError, LabelBinarizer, neg_label=1, pos_label=2,","145","                  sparse_output=True)","148","    assert_raises(ValueError, _inverse_binarize_thresholding,","149","                  y=csr_matrix([[1, 2], [2, 1]]), output_type=\"foo\",","150","                  classes=[1, 2], threshold=0)","154","    assert_raises(ValueError, LabelBinarizer().fit_transform, y_seq_of_seqs)","157","    assert_raises(ValueError, _inverse_binarize_thresholding,","158","                  y=csr_matrix([[1, 2], [2, 1]]), output_type=\"foo\",","159","                  classes=[1, 2, 3], threshold=0)","162","    assert_raises(ValueError, _inverse_binarize_thresholding,","163","                  y=np.array([[1, 2, 3], [2, 1, 3]]), output_type=\"binary\",","164","                  classes=[1, 2, 3], threshold=0)","167","    assert_raises(ValueError, LabelBinarizer().fit, np.array([[1, 3], [2, 1]]))","168","    assert_raises(ValueError, label_binarize, np.array([[1, 3], [2, 1]]),","169","                  [1, 2, 3])","206","    assert_raises(ValueError, le.transform, [0, 6])","214","    assert_raise_message(ValueError, msg, le.transform, \"apple\")","220","    assert_raises(ValueError, le.transform, [])","221","    assert_raises(ValueError, le.inverse_transform, [])","227","    assert_raise_message(ValueError, msg, le.inverse_transform, [-2])","228","    assert_raise_message(ValueError, msg, le.inverse_transform, [-2, -3, -4])","232","    assert_raise_message(ValueError, msg, le.inverse_transform, \"\")","290","    assert_raises(ValueError, mlb.inverse_transform,","291","                  csr_matrix(np.array([[0, 1, 1],","292","                                       [2, 0, 0],","293","                                       [1, 1, 0]])))","378","    assert_raise_message(ValueError, err_msg, mlb.fit, inp)","441","    assert_raises(TypeError, mlb.fit_transform, [({}), ({}, {'a': 'b'})])","456","    assert_raises(ValueError, mlb.inverse_transform, np.array([[1, 3]]))","463","    assert_raises(ValueError, mlb.inverse_transform, np.array([[1]]))","464","    assert_raises(ValueError, mlb.inverse_transform, np.array([[1, 1, 1]]))","488","            assert_raises(ValueError, label_binarize, y, classes,","489","                          neg_label=neg_label, pos_label=pos_label,","490","                          sparse_output=sparse_output)","554","    assert_raises(ValueError, label_binarize, y, classes, neg_label=-1,","555","                  pos_label=pos_label, sparse_output=True)","572","    assert_raises(ValueError, label_binarize, y, classes, neg_label=-1,","573","                  pos_label=pos_label, sparse_output=True)","577","    assert_raises(ValueError, label_binarize, [0, 2], classes=[0, 2],","578","                  pos_label=0, neg_label=1)"]}],"sklearn\/preprocessing\/tests\/test_base.py":[{"add":["62","    err_msg = (\"The retain_order option can only be set to True \"","63","               \"for dense matrices.\")","64","    with pytest.raises(ValueError, match=err_msg):","65","        _transform_selected(sparse.csr_matrix(X), Binarizer().transform,","66","                            dtype=np.int, selected=[0], retain_order=True)","71","    err_msg = (\"The retain_order option can only be set to True \"","72","               \"if the dimensions of the input array match the \"","73","               \"dimensions of the transformed array.\")","74","    with pytest.raises(ValueError, match=err_msg):","75","        _transform_selected(X, transform, dtype=np.int,","76","                            selected=[0], retain_order=True)"],"delete":["5","from sklearn.utils.testing import assert_raise_message","63","    assert_raise_message(ValueError,","64","                         \"The retain_order option can only be set to True \"","65","                         \"for dense matrices.\",","66","                         _transform_selected, sparse.csr_matrix(X),","67","                         Binarizer().transform, dtype=np.int, selected=[0],","68","                         retain_order=True)","73","    assert_raise_message(ValueError,","74","                         \"The retain_order option can only be set to True \"","75","                         \"if the dimensions of the input array match the \"","76","                         \"dimensions of the transformed array.\",","77","                         _transform_selected, X, transform, dtype=np.int,","78","                         selected=[0], retain_order=True)"]}],"sklearn\/preprocessing\/tests\/test_data.py":[{"add":["692","    with pytest.raises(ValueError):","693","        scaler.fit(X)","791","    with pytest.raises(ValueError):","792","        StandardScaler().fit(X_csr)","793","    with pytest.raises(ValueError):","794","        StandardScaler().fit(X_csc)","1026","    with pytest.raises(ValueError):","1027","        scale(X_csr, with_mean=True)","1028","    with pytest.raises(ValueError):","1029","        StandardScaler(with_mean=True).fit(X_csr)","1031","    with pytest.raises(ValueError):","1032","        scale(X_csc, with_mean=True)","1033","    with pytest.raises(ValueError):","1034","        StandardScaler(with_mean=True).fit(X_csc)","1038","    with pytest.raises(ValueError):","1039","        scaler.transform(X_csr)","1040","    with pytest.raises(ValueError):","1041","        scaler.transform(X_csc)","1044","    with pytest.raises(ValueError):","1045","        scaler.inverse_transform(X_transformed_csr)","1048","    with pytest.raises(ValueError):","1049","        scaler.inverse_transform(X_transformed_csc)","1055","    with pytest.raises(ValueError, match=\"Input contains infinity \"","1056","                       \"or a value too large\"):","1057","        scale(X)","1211","    err_msg = \"Invalid value for 'n_quantiles': 0.\"","1212","    with pytest.raises(ValueError, match=err_msg):","1213","        QuantileTransformer(n_quantiles=0).fit(X)","1214","    err_msg = \"Invalid value for 'subsample': 0.\"","1215","    with pytest.raises(ValueError, match=err_msg):","1216","        QuantileTransformer(subsample=0).fit(X)","1217","    err_msg = (\"The number of quantiles cannot be greater than \"","1218","               \"the number of samples used. Got 1000 quantiles \"","1219","               \"and 10 samples.\")","1220","    with pytest.raises(ValueError, match=err_msg):","1221","        QuantileTransformer(subsample=10).fit(X)","1224","    err_msg = \"QuantileTransformer only accepts non-negative sparse matrices.\"","1225","    with pytest.raises(ValueError, match=err_msg):","1226","        transformer.fit(X_neg)","1228","    err_msg = \"QuantileTransformer only accepts non-negative sparse matrices.\"","1229","    with pytest.raises(ValueError, match=err_msg):","1230","        transformer.transform(X_neg)","1234","    err_msg = (\"X does not have the same number of features as the previously\"","1235","               \" fitted \" \"data. Got 2 instead of 3.\")","1236","    with pytest.raises(ValueError, match=err_msg):","1237","        transformer.transform(X_bad_feat)","1238","    err_msg = (\"X does not have the same number of features \"","1239","               \"as the previously fitted data. Got 2 instead of 3.\")","1240","    with pytest.raises(ValueError, match=err_msg):","1241","        transformer.inverse_transform(X_bad_feat)","1246","    err_msg = (\"'output_distribution' has to be either 'normal' or \"","1247","               \"'uniform'. Got 'rnd' instead.\")","1248","    with pytest.raises(ValueError, match=err_msg):","1249","        transformer.fit(X)","1255","    err_msg = (\"'output_distribution' has to be either 'normal' or 'uniform'.\"","1256","               \" Got 'rnd' instead.\")","1257","    with pytest.raises(ValueError, match=err_msg):","1258","        transformer.transform(X)","1260","    err_msg = (\"'output_distribution' has to be either 'normal' or 'uniform'.\"","1261","               \" Got 'rnd' instead.\")","1262","    with pytest.raises(ValueError, match=err_msg):","1263","        transformer.inverse_transform(X_tran)","1265","    with pytest.raises(ValueError,","1266","                       match='Expected 2D array, got scalar array instead'):","1267","        transformer.transform(10)","1557","        with pytest.raises(ValueError, match=r'Invalid quantile range: \\('):","1558","            scaler.fit(iris.data)","1578","    with pytest.raises(ValueError):","1579","        scale(X_csr, with_mean=False, axis=1)","1968","    with pytest.raises(ValueError):","1969","        normalize([[0]], axis=2)","1970","    with pytest.raises(ValueError):","1971","        normalize([[0]], norm='l3')","2006","        with pytest.raises(NotImplementedError):","2007","            normalize(X_sparse, norm=norm, return_norm=True)","2064","    with pytest.raises(ValueError):","2065","        binarizer.transform(sparse.csc_matrix(X))","2171","    with pytest.raises(ValueError, match=\"axis should be either equal \"","2172","                                         \"to 0 or 1. Got axis=2\"):","2173","        quantile_transform(X.T, axis=2)","2180","    with pytest.raises(NotFittedError):","2181","        pt.transform(X)","2182","    with pytest.raises(NotFittedError):","2183","        pt.inverse_transform(X)","2264","    with pytest.raises(ValueError, match=not_positive_message):","2265","        pt.transform(X_with_negatives)","2267","    with pytest.raises(ValueError, match=not_positive_message):","2268","        pt.fit(X_with_negatives)","2270","    with pytest.raises(ValueError, match=not_positive_message):","2271","        power_transform(X_with_negatives, 'box-cox')","2273","    with pytest.raises(ValueError, match=not_positive_message):","2274","        pt.transform(np.zeros(X_2d.shape))","2276","    with pytest.raises(ValueError, match=not_positive_message):","2277","        pt.fit(np.zeros(X_2d.shape))","2279","    with pytest.raises(ValueError, match=not_positive_message):","2280","        power_transform(np.zeros(X_2d.shape), 'box-cox')","2300","    with pytest.raises(ValueError, match=wrong_shape_message):","2301","        pt.transform(X[:, 0:1])","2303","    with pytest.raises(ValueError, match=wrong_shape_message):","2304","        pt.inverse_transform(X[:, 0:1])","2313","    with pytest.raises(ValueError, match=bad_method_message):","2314","        pt.fit(X)"],"delete":["18","from sklearn.utils.testing import assert_raise_message","23","from sklearn.utils.testing import assert_raises","24","from sklearn.utils.testing import assert_raises_regex","695","    assert_raises(ValueError, scaler.fit, X)","793","    assert_raises(ValueError, StandardScaler().fit, X_csr)","794","    assert_raises(ValueError, StandardScaler().fit, X_csc)","1026","    assert_raises(ValueError, scale, X_csr, with_mean=True)","1027","    assert_raises(ValueError, StandardScaler(with_mean=True).fit, X_csr)","1029","    assert_raises(ValueError, scale, X_csc, with_mean=True)","1030","    assert_raises(ValueError, StandardScaler(with_mean=True).fit, X_csc)","1034","    assert_raises(ValueError, scaler.transform, X_csr)","1035","    assert_raises(ValueError, scaler.transform, X_csc)","1038","    assert_raises(ValueError, scaler.inverse_transform, X_transformed_csr)","1041","    assert_raises(ValueError, scaler.inverse_transform, X_transformed_csc)","1047","    assert_raises_regex(ValueError,","1048","                        \"Input contains infinity or a value too large\",","1049","                        scale, X)","1203","    assert_raises_regex(ValueError, \"Invalid value for 'n_quantiles': 0.\",","1204","                        QuantileTransformer(n_quantiles=0).fit, X)","1205","    assert_raises_regex(ValueError, \"Invalid value for 'subsample': 0.\",","1206","                        QuantileTransformer(subsample=0).fit, X)","1207","    assert_raises_regex(ValueError, \"The number of quantiles cannot be\"","1208","                        \" greater than the number of samples used. Got\"","1209","                        \" 1000 quantiles and 10 samples.\",","1210","                        QuantileTransformer(subsample=10).fit, X)","1213","    assert_raises_regex(ValueError, \"QuantileTransformer only accepts \"","1214","                        \"non-negative sparse matrices.\",","1215","                        transformer.fit, X_neg)","1217","    assert_raises_regex(ValueError, \"QuantileTransformer only accepts \"","1218","                        \"non-negative sparse matrices.\",","1219","                        transformer.transform, X_neg)","1223","    assert_raises_regex(ValueError, \"X does not have the same number of \"","1224","                        \"features as the previously fitted data. Got 2\"","1225","                        \" instead of 3.\",","1226","                        transformer.transform, X_bad_feat)","1227","    assert_raises_regex(ValueError, \"X does not have the same number of \"","1228","                        \"features as the previously fitted data. Got 2\"","1229","                        \" instead of 3.\",","1230","                        transformer.inverse_transform, X_bad_feat)","1235","    assert_raises_regex(ValueError, \"'output_distribution' has to be either\"","1236","                        \" 'normal' or 'uniform'. Got 'rnd' instead.\",","1237","                        transformer.fit, X)","1243","    assert_raises_regex(ValueError, \"'output_distribution' has to be either\"","1244","                        \" 'normal' or 'uniform'. Got 'rnd' instead.\",","1245","                        transformer.transform, X)","1247","    assert_raises_regex(ValueError, \"'output_distribution' has to be either\"","1248","                        \" 'normal' or 'uniform'. Got 'rnd' instead.\",","1249","                        transformer.inverse_transform, X_tran)","1251","    assert_raise_message(ValueError,","1252","                         'Expected 2D array, got scalar array instead',","1253","                         transformer.transform, 10)","1543","        assert_raises_regex(ValueError, r'Invalid quantile range: \\(',","1544","                            scaler.fit, iris.data)","1564","    assert_raises(ValueError, scale, X_csr, with_mean=False, axis=1)","1953","    assert_raises(ValueError, normalize, [[0]], axis=2)","1954","    assert_raises(ValueError, normalize, [[0]], norm='l3')","1989","        assert_raises(NotImplementedError, normalize, X_sparse,","1990","                      norm=norm, return_norm=True)","2047","    assert_raises(ValueError, binarizer.transform, sparse.csc_matrix(X))","2153","    assert_raises_regex(ValueError, \"axis should be either equal to 0 or 1\"","2154","                        \". Got axis=2\", quantile_transform, X.T, axis=2)","2161","    assert_raises(NotFittedError, pt.transform, X)","2162","    assert_raises(NotFittedError, pt.inverse_transform, X)","2243","    assert_raise_message(ValueError, not_positive_message,","2244","                         pt.transform, X_with_negatives)","2246","    assert_raise_message(ValueError, not_positive_message,","2247","                         pt.fit, X_with_negatives)","2249","    assert_raise_message(ValueError, not_positive_message,","2250","                         power_transform, X_with_negatives, 'box-cox')","2252","    assert_raise_message(ValueError, not_positive_message,","2253","                         pt.transform, np.zeros(X_2d.shape))","2255","    assert_raise_message(ValueError, not_positive_message,","2256","                         pt.fit, np.zeros(X_2d.shape))","2258","    assert_raise_message(ValueError, not_positive_message,","2259","                         power_transform, np.zeros(X_2d.shape), 'box-cox')","2279","    assert_raise_message(ValueError, wrong_shape_message,","2280","                         pt.transform, X[:, 0:1])","2282","    assert_raise_message(ValueError, wrong_shape_message,","2283","                         pt.inverse_transform, X[:, 0:1])","2292","    assert_raise_message(ValueError, bad_method_message,","2293","                         pt.fit, X)"]}],"sklearn\/preprocessing\/tests\/test_discretization.py":[{"add":["39","    err_msg = (\"KBinsDiscretizer received an invalid \"","40","               \"number of bins. Received 1, expected at least 2.\")","41","    with pytest.raises(ValueError, match=err_msg):","42","        est.fit_transform(X)","45","    err_msg = (\"KBinsDiscretizer received an invalid \"","46","               \"n_bins type. Received float, expected int.\")","47","    with pytest.raises(ValueError, match=err_msg):","48","        est.fit_transform(X)","55","    err_msg = r\"n_bins must be a scalar or array of shape \\(n_features,\\).\"","56","    with pytest.raises(ValueError, match=err_msg):","57","        est.fit_transform(X)","62","    err_msg = r\"n_bins must be a scalar or array of shape \\(n_features,\\).\"","63","    with pytest.raises(ValueError, match=err_msg):","64","        est.fit_transform(X)","69","    err_msg = (\"KBinsDiscretizer received an invalid number of bins \"","70","               \"at indices 0, 3. Number of bins must be at least 2, \"","71","               \"and must be an int.\")","72","    with pytest.raises(ValueError, match=err_msg):","73","        est.fit_transform(X)","78","    err_msg = (\"KBinsDiscretizer received an invalid number of bins \"","79","               \"at indices 0, 2. Number of bins must be at least 2, \"","80","               \"and must be an int.\")","81","    with pytest.raises(ValueError, match=err_msg):","82","        est.fit_transform(X)","105","    err_msg = \"Incorrect number of features. Expecting 4, received 5\"","106","    with pytest.raises(ValueError, match=err_msg):","107","        est.transform(bad_X)","130","    with pytest.raises(ValueError):","131","        est.fit(X)","135","    with pytest.raises(ValueError):","136","        est.transform(X)","152","    err_msg = (r\"Valid options for 'encode' are \"","153","               r\"\\('onehot', 'onehot-dense', 'ordinal'\\). \"","154","               r\"Got encode='invalid-encode' instead.\")","155","    with pytest.raises(ValueError, match=err_msg):","156","        est.fit(X)","184","    err_msg = (r\"Valid options for 'strategy' are \"","185","               r\"\\('uniform', 'quantile', 'kmeans'\\). \"","186","               r\"Got strategy='invalid-strategy' instead.\")","187","    with pytest.raises(ValueError, match=err_msg):","188","        est.fit(X)"],"delete":["11","    assert_raises,","12","    assert_raise_message,","41","    assert_raise_message(ValueError, \"KBinsDiscretizer received an invalid \"","42","                         \"number of bins. Received 1, expected at least 2.\",","43","                         est.fit_transform, X)","46","    assert_raise_message(ValueError, \"KBinsDiscretizer received an invalid \"","47","                         \"n_bins type. Received float, expected int.\",","48","                         est.fit_transform, X)","55","    assert_raise_message(ValueError,","56","                         \"n_bins must be a scalar or array of shape \"","57","                         \"(n_features,).\", est.fit_transform, X)","62","    assert_raise_message(ValueError,","63","                         \"n_bins must be a scalar or array of shape \"","64","                         \"(n_features,).\", est.fit_transform, X)","69","    assert_raise_message(ValueError,","70","                         \"KBinsDiscretizer received an invalid number of bins \"","71","                         \"at indices 0, 3. Number of bins must be at least 2, \"","72","                         \"and must be an int.\",","73","                         est.fit_transform, X)","78","    assert_raise_message(ValueError,","79","                         \"KBinsDiscretizer received an invalid number of bins \"","80","                         \"at indices 0, 2. Number of bins must be at least 2, \"","81","                         \"and must be an int.\",","82","                         est.fit_transform, X)","105","    assert_raise_message(ValueError,","106","                         \"Incorrect number of features. Expecting 4, \"","107","                         \"received 5\", est.transform, bad_X)","130","    assert_raises(ValueError, est.fit, X)","134","    assert_raises(ValueError, est.transform, X)","150","    assert_raise_message(ValueError, \"Valid options for 'encode' are \"","151","                         \"('onehot', 'onehot-dense', 'ordinal'). \"","152","                         \"Got encode='invalid-encode' instead.\",","153","                         est.fit, X)","181","    assert_raise_message(ValueError, \"Valid options for 'strategy' are \"","182","                         \"('uniform', 'quantile', 'kmeans'). \"","183","                         \"Got strategy='invalid-strategy' instead.\",","184","                         est.fit, X)"]}],"sklearn\/preprocessing\/tests\/test_encoders.py":[{"add":["262","    with pytest.raises(ValueError, match=msg):","263","        enc.inverse_transform(X_tr)","522","    with pytest.raises(ValueError, match=msg):","523","        enc.inverse_transform(X_tr)","554","","647","    err_msg = \"`drop` should have length equal to the number\"","648","    with pytest.raises(ValueError, match=err_msg):","649","        enc.fit([['abc', 2, 55], ['def', 1, 55], ['def', 3, 59]])"],"delete":["10","from sklearn.utils.testing import assert_raises_regex","263","    assert_raises_regex(ValueError, msg, enc.inverse_transform, X_tr)","522","    assert_raises_regex(ValueError, msg, enc.inverse_transform, X_tr)","645","    assert_raises_regex(","646","        ValueError,","647","        \"`drop` should have length equal to the number\",","648","        enc.fit, [['abc', 2, 55], ['def', 1, 55], ['def', 3, 59]])"]}]}},"dd3b80e8c736dd73005c42d832b4a919a052eae8":{"changes":{"examples\/plot_kernel_approximation.py":"MODIFY"},"diff":{"examples\/plot_kernel_approximation.py":[{"add":["30","","31","###########################################################################","32","# Python package and dataset imports, load dataset","33","# ---------------------------------------------------","34","","40","print(__doc__)","41","","56","","57","##################################################################","58","# Timing and accuracy plots","59","# --------------------------------------------------","125","plt.figure(figsize=(16, 4))","126","accuracy = plt.subplot(121)","127","# second y axis for timings","128","timescale = plt.subplot(122)","163","plt.tight_layout()","164","plt.show()","165","","166","","167","############################################################################","168","# Decision Surfaces of RBF Kernel SVM and Linear SVM","169","# --------------------------------------------------------","170","# The second plot visualized the decision surfaces of the RBF kernel SVM and","171","# the linear SVM with approximate kernel maps.","172","# The plot shows decision surfaces of the classifiers projected onto","173","# the first two principal components of the data. This visualization should","174","# be taken with a grain of salt since it is just an interesting slice through","175","# the decision surface in 64 dimensions. In particular note that","176","# a datapoint (represented as a dot) does not necessarily be classified","177","# into the region it is lying in, since it will not lie on the plane","178","# that the first two principal components span.","179","# The usage of :class:`RBFSampler` and :class:`Nystroem` is described in detail","180","# in :ref:`kernel_approximation`.","205","plt.figure(figsize=(18, 7.5))","206","plt.rcParams.update({'font.size': 14})"],"delete":["29","The second plot visualized the decision surfaces of the RBF kernel SVM and","30","the linear SVM with approximate kernel maps.","31","The plot shows decision surfaces of the classifiers projected onto","32","the first two principal components of the data. This visualization should","33","be taken with a grain of salt since it is just an interesting slice through","34","the decision surface in 64 dimensions. In particular note that","35","a datapoint (represented as a dot) does not necessarily be classified","36","into the region it is lying in, since it will not lie on the plane","37","that the first two principal components span.","38","","39","The usage of :class:`RBFSampler` and :class:`Nystroem` is described in detail","40","in :ref:`kernel_approximation`.","41","","43","print(__doc__)","128","plt.figure(figsize=(8, 8))","129","accuracy = plt.subplot(211)","130","# second y axis for timeings","131","timescale = plt.subplot(212)","190","plt.tight_layout()","191","plt.figure(figsize=(12, 5))","192",""]}]}},"3aafaa97f3fb6ea6d57f357ff94f777f8ff6289f":{"changes":{"sklearn\/semi_supervised\/tests\/test_label_propagation.py":"MODIFY"},"diff":{"sklearn\/semi_supervised\/tests\/test_label_propagation.py":[{"add":["3","import pytest","121","        with pytest.raises(ValueError):","122","            label_propagation.LabelSpreading(alpha=alpha).fit(X, y)"],"delete":["5","from sklearn.utils.testing import assert_raises","121","        assert_raises(ValueError,","122","                      lambda **kwargs:","123","                      label_propagation.LabelSpreading(**kwargs).fit(X, y),","124","                      alpha=alpha)"]}]}},"16f4ac90f0732988e3b7efe0c937eaff70e99692":{"changes":{"sklearn\/tests\/test_pipeline.py":"MODIFY","sklearn\/pipeline.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY"},"diff":{"sklearn\/tests\/test_pipeline.py":[{"add":["900","# TODO: Remove parametrization in 0.24 when None is removed for FeatureUnion","914","    with pytest.warns(None) as record:","915","        ft.set_params(m2=drop)","916","        assert_array_equal([[3]], ft.fit(X).transform(X))","917","        assert_array_equal([[3]], ft.fit_transform(X))","919","    assert record if drop is None else not record","921","    with pytest.warns(None) as record:","922","        ft.set_params(m3=drop)","923","        assert_array_equal([[]], ft.fit(X).transform(X))","924","        assert_array_equal([[]], ft.fit_transform(X))","926","    assert record if drop is None else not record","928","    with pytest.warns(None) as record:","929","        # check we can change back","930","        ft.set_params(m3=mult3)","931","        assert_array_equal([[3]], ft.fit(X).transform(X))","932","    assert record if drop is None else not record","934","    with pytest.warns(None) as record:","935","        # Check 'drop' step at construction time","936","        ft = FeatureUnion([('m2', drop), ('m3', mult3)])","937","        assert_array_equal([[3]], ft.fit(X).transform(X))","938","        assert_array_equal([[3]], ft.fit_transform(X))","940","    assert record if drop is None else not record","1138","     (FeatureUnion([('mult1', 'drop'), ('mult2', Mult()), ('mult3', 'drop')]),","1184","","1185","","1186","# TODO: Remove in 0.24 when None is removed","1187","def test_feature_union_warns_with_none():","1188","    msg = (r\"Using None as a transformer is deprecated in version 0\\.22 and \"","1189","           r\"will be removed in version 0\\.24\\. Please use 'drop' instead\\.\")","1190","    with pytest.warns(DeprecationWarning, match=msg):","1191","        union = FeatureUnion([('multi1', None), ('multi2', Mult())])","1192","","1193","    X = [[1, 2, 3], [4, 5, 6]]","1194","","1195","    with pytest.warns(DeprecationWarning, match=msg):","1196","        union.fit_transform(X)"],"delete":["913","    ft.set_params(m2=drop)","914","    assert_array_equal([[3]], ft.fit(X).transform(X))","915","    assert_array_equal([[3]], ft.fit_transform(X))","918","    ft.set_params(m3=drop)","919","    assert_array_equal([[]], ft.fit(X).transform(X))","920","    assert_array_equal([[]], ft.fit_transform(X))","923","    # check we can change back","924","    ft.set_params(m3=mult3)","925","    assert_array_equal([[3]], ft.fit(X).transform(X))","927","    # Check 'drop' step at construction time","928","    ft = FeatureUnion([('m2', drop), ('m3', mult3)])","929","    assert_array_equal([[3]], ft.fit(X).transform(X))","930","    assert_array_equal([[3]], ft.fit_transform(X))","1129","     (FeatureUnion([('mult1', None), ('mult2', Mult()), ('mult3', None)]),"]}],"sklearn\/pipeline.py":[{"add":["13","import warnings","757","    or removed by setting to 'drop'.","767","        .. versionchanged:: 0.22","768","           Deprecated `None` as a transformer in favor of 'drop'.","769","","846","            # TODO: Remove in 0.24 when None is removed","847","            if t is None:","848","                warnings.warn(\"Using None as a transformer is deprecated \"","849","                              \"in version 0.22 and will be removed in \"","850","                              \"version 0.24. Please use 'drop' instead.\",","851","                              DeprecationWarning)","852","                continue","853","            if t == 'drop':"],"delete":["756","    or removed by setting to 'drop' or ``None``.","842","            if t is None or t == 'drop':"]}],"doc\/whats_new\/v0.22.rst":[{"add":["530","- |API| `None` as a transformer is now deprecated in","531","  :class:`pipeline.FeatureUnion`. Please use `'drop'` instead. :pr:`15053` by","532","  `Thomas Fan`_.","533",""],"delete":[]}]}},"98ca716429d3f59dbf4d622bd3b60c2173377e9c":{"changes":{"sklearn\/_build_utils\/__init__.py":"MODIFY"},"diff":{"sklearn\/_build_utils\/__init__.py":[{"add":["10","import contextlib","89","        n_jobs = 1","90","        with contextlib.suppress(ImportError):","91","            import joblib","92","            if LooseVersion(joblib.__version__) > LooseVersion(\"0.13.0\"):","93","                # earlier joblib versions don't account for CPU affinity","94","                # constraints, and may over-estimate the number of available","95","                # CPU particularly in CI (cf loky#114)","96","                n_jobs = joblib.effective_n_jobs()","97","","100","            nthreads=n_jobs,"],"delete":[]}]}},"9d2595f2be4db6b4af29d21aa88292d2e179ce12":{"changes":{"doc\/themes\/scikit-learn-modern\/static\/css\/theme.css":"MODIFY"},"diff":{"doc\/themes\/scikit-learn-modern\/static\/css\/theme.css":[{"add":["590","div.sk-page-content h1 code,","591","div.sk-page-content h2 code,","592","div.sk-page-content h3 code,","593","div.sk-page-content h4 code {","594","  white-space: normal;","595","}","596",""],"delete":[]}]}},"7e8405c89ce72675d82169b116e865f73ced5e19":{"changes":{"sklearn\/metrics\/cluster\/tests\/test_unsupervised.py":"MODIFY","sklearn\/metrics\/cluster\/tests\/test_supervised.py":"MODIFY"},"diff":{"sklearn\/metrics\/cluster\/tests\/test_unsupervised.py":[{"add":["137","    err_msg = (r'Number of labels is %d\\. Valid values are 2 '","138","               r'to n_samples - 1 \\(inclusive\\)' % len(np.unique(y)))","139","    with pytest.raises(ValueError, match=err_msg):","140","        silhouette_score(X, y)","144","    err_msg = (r'Number of labels is %d\\. Valid values are 2 '","145","               r'to n_samples - 1 \\(inclusive\\)' % len(np.unique(y)))","146","    with pytest.raises(ValueError, match=err_msg):","147","        silhouette_score(X, y)","191","    with pytest.raises(ValueError, match=\"Number of labels is\"):","192","        func(rng.rand(10, 2), np.zeros(10))","198","    with pytest.raises(ValueError, match=\"Number of labels is\"):","199","        func(rng.rand(10, 2), np.arange(10))"],"delete":["7","from sklearn.utils.testing import assert_raises_regexp","8","from sklearn.utils.testing import assert_raise_message","139","    assert_raises_regexp(ValueError,","140","                         r'Number of labels is %d\\. Valid values are 2 '","141","                         r'to n_samples - 1 \\(inclusive\\)' % len(np.unique(y)),","142","                         silhouette_score, X, y)","146","    assert_raises_regexp(ValueError,","147","                         r'Number of labels is %d\\. Valid values are 2 '","148","                         r'to n_samples - 1 \\(inclusive\\)' % len(np.unique(y)),","149","                         silhouette_score, X, y)","193","    assert_raise_message(ValueError, \"Number of labels is\",","194","                         func,","195","                         rng.rand(10, 2), np.zeros(10))","201","    assert_raise_message(ValueError, \"Number of labels is\",","202","                         func,","203","                         rng.rand(10, 2), np.arange(10))"]}],"sklearn\/metrics\/cluster\/tests\/test_supervised.py":[{"add":["1","import pytest","19","        assert_almost_equal, ignore_warnings)","38","        with pytest.raises(ValueError, match=expected):","39","            score_func([0, 1], [1, 1, 1])","41","        expected = r\"labels_true must be 1D: shape is \\(2\"","42","        with pytest.raises(ValueError, match=expected):","43","            score_func([[0, 1], [1, 0]], [1, 1, 1])","45","        expected = r\"labels_pred must be 1D: shape is \\(2\"","46","        with pytest.raises(ValueError, match=expected):","47","            score_func([0, 1, 0], [[1, 1], [0, 0]])","264","    with pytest.raises(ValueError, match=\"Cannot set 'eps' when sparse=True\"):","265","        contingency_matrix(labels_a, labels_b, eps=1e-10, sparse=True)"],"delete":["18","        assert_almost_equal, assert_raise_message, ignore_warnings)","37","        assert_raise_message(ValueError, expected, score_func,","38","                             [0, 1], [1, 1, 1])","40","        expected = \"labels_true must be 1D: shape is (2\"","41","        assert_raise_message(ValueError, expected, score_func,","42","                             [[0, 1], [1, 0]], [1, 1, 1])","44","        expected = \"labels_pred must be 1D: shape is (2\"","45","        assert_raise_message(ValueError, expected, score_func,","46","                             [0, 1, 0], [[1, 1], [0, 0]])","263","    C_sparse = assert_raise_message(ValueError,","264","                                    \"Cannot set 'eps' when sparse=True\",","265","                                    contingency_matrix, labels_a, labels_b,","266","                                    eps=1e-10, sparse=True)"]}]}},"e49b9d3d754b189062d23bf213847a6370158282":{"changes":{"sklearn\/datasets\/tests\/data\/openml\/62\/api-v1-json-data-qualities-62.json.gz":"ADD","sklearn\/datasets\/tests\/data\/openml\/62\/data-v1-download-52352.arff.gz":"ADD","sklearn\/datasets\/tests\/test_openml.py":"MODIFY","sklearn\/datasets\/tests\/data\/openml\/62\/api-v1-json-data-62.json.gz":"ADD","sklearn\/datasets\/tests\/data\/openml\/62\/api-v1-json-data-features-62.json.gz":"ADD","doc\/whats_new\/v0.22.rst":"MODIFY","sklearn\/datasets\/openml.py":"MODIFY"},"diff":{"sklearn\/datasets\/tests\/data\/openml\/62\/api-v1-json-data-qualities-62.json.gz":[{"add":[],"delete":[]}],"sklearn\/datasets\/tests\/data\/openml\/62\/data-v1-download-52352.arff.gz":[{"add":[],"delete":[]}],"sklearn\/datasets\/tests\/test_openml.py":[{"add":["1160","","1161","","1162","@pytest.mark.parametrize('gzip_response', [True, False])","1163","def test_fetch_openml_with_ignored_feature(monkeypatch, gzip_response):","1164","    # Regression test for #14340","1165","    # 62 is the ID of the ZOO dataset","1166","    data_id = 62","1167","    _monkey_patch_webbased_functions(monkeypatch, data_id, gzip_response)","1168","","1169","    dataset = sklearn.datasets.fetch_openml(data_id=data_id, cache=False)","1170","    assert dataset is not None","1171","    # The dataset has 17 features, including 1 ignored (animal),","1172","    # so we assert that we don't have the ignored feature in the final Bunch","1173","    assert dataset['data'].shape == (101, 16)","1174","    assert 'animal' not in dataset['feature_names']"],"delete":[]}],"sklearn\/datasets\/tests\/data\/openml\/62\/api-v1-json-data-62.json.gz":[{"add":[],"delete":[]}],"sklearn\/datasets\/tests\/data\/openml\/62\/api-v1-json-data-features-62.json.gz":[{"add":[],"delete":[]}],"doc\/whats_new\/v0.22.rst":[{"add":["82","- |Fix| Fixed a bug in :func:`datasets.fetch_openml`, which failed to load","83","  an OpenML dataset that contains an ignored feature.","84","  :pr:`14623` by :user:`Sarra Habchi <HabchiSarra>`.","85",""],"delete":[]}],"sklearn\/datasets\/openml.py":[{"add":["426","def _get_num_samples(data_qualities):","427","    \"\"\"Get the number of samples from data qualities.","428","","429","    Parameters","430","    ----------","431","    data_qualities : list of dict","432","        Used to retrieve the number of instances (samples) in the dataset.","433","","434","    Returns","435","    -------","436","    n_samples : int","437","        The number of samples in the dataset or -1 if data qualities are","438","        unavailable.","439","    \"\"\"","440","    # If the data qualities are unavailable, we return -1","441","    default_n_samples = -1","442","","444","        return default_n_samples","445","","447","    return int(float(qualities.get('NumberOfInstances', default_n_samples)))","721","        # The shape must include the ignored features to keep the right indexes","722","        # during the arff data conversion.","724","        shape = _get_num_samples(data_qualities), len(features_list)"],"delete":["426","def _get_data_shape(data_qualities):","427","    # Using the data_info dictionary from _get_data_info_by_name to extract","428","    # the number of samples \/ features","430","        return None","432","    try:","433","        return (int(float(qualities['NumberOfInstances'])),","434","                int(float(qualities['NumberOfFeatures'])))","435","    except AttributeError:","436","        return None","711","        shape = _get_data_shape(data_qualities)","712","        # if the data qualities were not available, we can still get the","713","        # n_features from the feature list, with the n_samples unknown","714","        if shape is None:","715","            shape = (-1, len(features_list))"]}]}},"939fa3cccefe708db7a81c5248db32a1d600bf8d":{"changes":{"sklearn\/ensemble\/_base.py":"MODIFY","sklearn\/ensemble\/_voting.py":"MODIFY","sklearn\/ensemble\/tests\/test_voting.py":"MODIFY"},"diff":{"sklearn\/ensemble\/_base.py":[{"add":["9","import warnings","226","        # FIXME: deprecate the usage of None to drop an estimator from the","227","        # ensemble. Remove in 0.24","228","        if any(est is None for est in estimators):","229","            warnings.warn(","230","                \"Using 'None' to drop an estimator from the ensemble is \"","231","                \"deprecated in 0.22 and support will be dropped in 0.24. \"","232","                \"Use the string 'drop' instead.\", DeprecationWarning","233","            )","234","","248","                    \"The estimator {} should be a {}.\".format("],"delete":["238","                    \"The estimator {} should be a {}.\"","239","                    .format("]}],"sklearn\/ensemble\/_voting.py":[{"add":["90","        ``self.estimators_``. An estimator can be set to ``'drop'``","93","        .. deprecated:: 0.22","94","           Using ``None`` to drop an estimator is deprecated in 0.22 and","95","           support will be dropped in 0.24. Use the string ``'drop'`` instead.","96","","125","        that are not 'drop'.","328","        ``self.estimators_``. An estimator can be set to ``'drop'`` using","329","        ``set_params``.","330","","331","        .. deprecated:: 0.22","332","           Using ``None`` to drop an estimator is deprecated in 0.22 and","333","           support will be dropped in 0.24. Use the string ``'drop'`` instead.","349","        that are not 'drop'.","354","        .. versionadded:: 0.20","355",""],"delete":["90","        ``self.estimators_``. An estimator can be set to ``None`` or ``'drop'``","121","        that are not `None`.","324","        ``self.estimators_``. An estimator can be set to ``None`` or ``'drop'``","325","        using ``set_params``.","341","        that are not `None`."]}],"sklearn\/ensemble\/tests\/test_voting.py":[{"add":["26","from sklearn.base import BaseEstimator, ClassifierMixin, clone","391","# TODO: Remove parametrization in 0.24 when None is removed in Voting*","407","    with pytest.warns(None) as record:","408","        eclf2.set_params(rf=drop).fit(X, y)","409","    assert record if drop is None else not record","419","    with pytest.warns(None) as record:","420","        eclf2.set_params(voting='soft').fit(X, y)","421","    assert record if drop is None else not record","425","    with pytest.warns(None) as record:","426","        with pytest.raises(ValueError, match=msg):","427","            eclf2.set_params(lr=drop, rf=drop, nb=drop).fit(X, y)","428","    assert record if drop is None else not record","440","    with pytest.warns(None) as record:","441","        eclf2.set_params(rf=drop).fit(X1, y1)","442","    assert record if drop is None else not record","503","# TODO: Remove drop=None in 0.24 when None is removed in Voting*","515","    # TODO: remove the parametrization on 'drop' when support for None is","516","    # removed.","517","    # check that an estimator can be set to 'drop' and passing some weight","520","    voter = clone(voter)","523","    with pytest.warns(None) as record:","524","        voter.fit(X, y, sample_weight=np.ones(y.shape))","525","    assert record if drop is None else not record","545","","546","","547","# TODO: Remove in 0.24 when None is removed in Voting*","548","@pytest.mark.parametrize(","549","    \"Voter, BaseEstimator\",","550","    [(VotingClassifier, DecisionTreeClassifier),","551","     (VotingRegressor, DecisionTreeRegressor)]","552",")","553","def test_deprecate_none_transformer(Voter, BaseEstimator):","554","    est = Voter(estimators=[('lr', None),","555","                            ('tree', BaseEstimator(random_state=0))])","556","","557","    msg = (\"Using 'None' to drop an estimator from the ensemble is \"","558","           \"deprecated in 0.22 and support will be dropped in 0.24. \"","559","           \"Use the string 'drop' instead.\")","560","    with pytest.warns(DeprecationWarning, match=msg):","561","        est.fit(X, y)"],"delete":["26","from sklearn.base import BaseEstimator, ClassifierMixin","406","    eclf2.set_params(rf=drop).fit(X, y)","416","    eclf2.set_params(voting='soft').fit(X, y)","420","    assert_raise_message(","421","        ValueError, msg, eclf2.set_params(lr=drop, rf=drop, nb=drop).fit, X, y)","433","    eclf2.set_params(rf=drop).fit(X1, y1)","505","    # check that an estimator can be set to None and passing some weight","510","    voter.fit(X, y, sample_weight=np.ones(y.shape))"]}]}},"42659232bd42cd38846ffbf3e579f72604772b96":{"changes":{"sklearn\/utils\/fixes.py":"MODIFY","doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/linear_model\/tests\/test_bayes.py":"MODIFY","sklearn\/externals\/_scipy_linalg.py":"ADD","sklearn\/linear_model\/bayes.py":"MODIFY"},"diff":{"sklearn\/utils\/fixes.py":[{"add":["47","if sp_version >= (1, 3):","48","    # Preserves earlier default choice of pinvh cutoff `cond` value.","49","    # Can be removed once issue #14055 is fully addressed.","50","    from ..externals._scipy_linalg import pinvh","51","else:","52","    from scipy.linalg import pinvh # noqa","53",""],"delete":[]}],"doc\/whats_new\/v0.21.rst":[{"add":["57","- |Fix| Fixed a bug in :class:`impute.SimpleImputer` and","58","  :class:`impute.IterativeImputer` so that no errors are thrown when there are","59","  missing values in training data. :pr:`13974` by `Frank Hoang <fhoang7>`.","67","- |Fix| Compatibility fix for :class:`linear_model.ARDRegression` and","68","  Scipy>=1.3.0. Adapts to upstream changes to the default `pinvh` cutoff","69","  threshold which otherwise results in poor accuracy in some cases.","70","  :pr:`14067` by :user:`Tim Staley <timstaley>`."],"delete":["57","- |Fix| Fixed a bug in :class:`SimpleImputer` and :class:`IterativeImputer`","58","  so that no errors are thrown when there are missing values in training data.","59","  :pr:`13974` by `Frank Hoang <fhoang7>`."]}],"sklearn\/linear_model\/tests\/test_bayes.py":[{"add":["202","def test_ard_accuracy_on_easy_problem():","203","    # Check that ARD converges with reasonable accuracy on an easy problem","204","    # (Github issue #14055)","205","    # This particular seed seems to converge poorly in the failure-case","206","    # (scipy==1.3.0, sklearn==0.21.2)","207","    seed = 45","208","    X = np.random.RandomState(seed=seed).normal(size=(250, 3))","209","    y = X[:, 1]","210","","211","    regressor = ARDRegression()","212","    regressor.fit(X, y)","213","","214","    abs_coef_error = np.abs(1 - regressor.coef_[1])","215","    # Expect an accuracy of better than 1E-4 in most cases -","216","    # Failure-case produces 0.16!","217","    assert abs_coef_error < 0.01","218","","219",""],"delete":[]}],"sklearn\/externals\/_scipy_linalg.py":[{"add":[],"delete":[]}],"sklearn\/linear_model\/bayes.py":[{"add":["15","from ..utils.fixes import pinvh"],"delete":["10","from scipy.linalg import pinvh"]}]}},"a05c8d89d977a7b4df768a83657eeded561160ad":{"changes":{"sklearn\/feature_extraction\/tests\/test_feature_hasher.py":"MODIFY","sklearn\/feature_extraction\/tests\/test_image.py":"MODIFY","sklearn\/feature_extraction\/tests\/test_text.py":"MODIFY"},"diff":{"sklearn\/feature_extraction\/tests\/test_feature_hasher.py":[{"add":["3","import pytest","6","from sklearn.utils.testing import (ignore_warnings,","89","    with pytest.raises(ValueError):","90","        FeatureHasher(input_type=\"gobbledygook\")","91","    with pytest.raises(ValueError):","92","        FeatureHasher(n_features=-1)","93","    with pytest.raises(ValueError):","94","        FeatureHasher(n_features=0)","95","    with pytest.raises(TypeError):","96","        FeatureHasher(n_features='ham')","99","    with pytest.raises(ValueError):","100","        h.transform([])","101","    with pytest.raises(Exception):","102","        h.transform([[5.5]])","103","    with pytest.raises(Exception):","104","        h.transform([[None]])","111","    with pytest.raises(TypeError):","112","        hasher.fit()"],"delete":["5","from sklearn.utils.testing import (assert_raises, ignore_warnings,","88","    assert_raises(ValueError, FeatureHasher, input_type=\"gobbledygook\")","89","    assert_raises(ValueError, FeatureHasher, n_features=-1)","90","    assert_raises(ValueError, FeatureHasher, n_features=0)","91","    assert_raises(TypeError, FeatureHasher, n_features='ham')","94","    assert_raises(ValueError, h.transform, [])","95","    assert_raises(Exception, h.transform, [[5.5]])","96","    assert_raises(Exception, h.transform, [[None]])","103","    assert_raises(TypeError, hasher.fit)"]}],"sklearn\/feature_extraction\/tests\/test_image.py":[{"add":["8","import pytest","13","from sklearn.utils.testing import ignore_warnings","175","    with pytest.raises(ValueError):","176","        extract_patches_2d(face, (p_h, p_w), max_patches=2.0)","177","    with pytest.raises(ValueError):","178","        extract_patches_2d(face, (p_h, p_w), max_patches=-1.0)","331","    with pytest.raises(ValueError):","332","        extract_patches_2d(x, (4, 1))","333","    with pytest.raises(ValueError):","334","        extract_patches_2d(x, (1, 4))"],"delete":["12","from sklearn.utils.testing import assert_raises, ignore_warnings","174","    assert_raises(ValueError, extract_patches_2d, face, (p_h, p_w),","175","                  max_patches=2.0)","176","    assert_raises(ValueError, extract_patches_2d, face, (p_h, p_w),","177","                  max_patches=-1.0)","330","    assert_raises(ValueError, extract_patches_2d, x, (4, 1))","331","    assert_raises(ValueError, extract_patches_2d, x, (1, 4))"]}],"sklearn\/feature_extraction\/tests\/test_text.py":[{"add":["35","                                   SkipTest, assert_no_warnings,","180","    with pytest.raises(UnicodeDecodeError):","181","        wa(text_bytes)","185","    with pytest.raises(UnicodeDecodeError):","186","        ca(text_bytes)","303","    with pytest.raises(ValueError):","304","        cv.get_stop_words()","306","    with pytest.raises(ValueError):","307","        cv.get_stop_words()","457","    with pytest.raises(ValueError):","458","        t3.transform(counts_train)","466","    with pytest.raises(ValueError):","467","        t3.transform(X_incompt)","488","    with pytest.raises(ValueError):","489","        v3.transform(train_data)","502","    with pytest.raises(ValueError):","503","        v3.build_preprocessor()","507","    with pytest.raises(ValueError):","508","        v3.build_analyzer()","579","    with pytest.raises(ValueError):","580","        cv.get_feature_names()","1026","    with pytest.raises(ValueError):","1027","        setattr(copy, 'idf_', invalid_idf)","1033","    with pytest.raises(ValueError):","1034","        vect.fit([])"],"delete":["35","                                   SkipTest, assert_raises, assert_no_warnings,","180","    assert_raises(UnicodeDecodeError, wa, text_bytes)","184","    assert_raises(UnicodeDecodeError, ca, text_bytes)","301","    assert_raises(ValueError, cv.get_stop_words)","303","    assert_raises(ValueError, cv.get_stop_words)","453","    assert_raises(ValueError, t3.transform, counts_train)","461","    assert_raises(ValueError, t3.transform, X_incompt)","482","    assert_raises(ValueError, v3.transform, train_data)","495","    assert_raises(ValueError, v3.build_preprocessor)","499","    assert_raises(ValueError, v3.build_analyzer)","570","    assert_raises(ValueError, cv.get_feature_names)","1016","    assert_raises(ValueError, setattr, copy, 'idf_', invalid_idf)","1022","    assert_raises(ValueError, vect.fit, [])"]}]}},"ec2ea1b5ae1adb30aa17356d8f0b115206cab627":{"changes":{"doc\/modules\/grid_search.rst":"MODIFY","doc\/modules\/compose.rst":"MODIFY"},"diff":{"doc\/modules\/grid_search.rst":[{"add":["45","possibly by reading the enclosed reference to the literature.","194",".. _composite_grid_search:","195","","198","`GridSearchCV` and `RandomizedSearchCV` allow searching over parameters of","199","composite or nested estimators such as `pipeline.Pipeline`,","200","`ColumnTransformer`, `VotingClassifier` or `CalibratedClassifierCV`","201","using a dedicated ``<estimator>__<parameter>`` syntax::","203","  >>> from sklearn.model_selection import GridSearchCV","204","  >>> from sklearn.calibration import CalibratedClassifierCV","205","  >>> from sklearn.ensemble import RandomForestClassifier","206","  >>> from sklearn.datasets import make_moons","207","  >>> X, y = make_moons()","208","  >>> calibrated_forest = CalibratedClassifierCV(","209","  ...    base_estimator=RandomForestClassifier(n_estimators=10))","210","  >>> param_grid = {","211","  ...    'base_estimator__max_depth': [2, 4, 6, 8]}","212","  >>> search = GridSearchCV(calibrated_forest, param_grid, cv=5)","213","  >>> search.fit(X, y)","214","  GridSearchCV(cv=5,","215","               estimator=CalibratedClassifierCV(...),","216","               param_grid={'base_estimator__max_depth': [2, 4, 6, 8]})","217","","218","Here, ``<estimator>`` is the parameter name of the nested estimator,","219","in this case ``base_estimator``.","220","If the meta-estimator is constructed as a collection of estimators as in","221","`pipeline.Pipeline`, then ``<estimator>`` refers to the name of the estimator,","222","see :ref:`pipeline_nested_parameters`.  In practice, there can be several","223","levels of nesting::","224","","225","  >>> from sklearn.pipeline import Pipeline","226","  >>> from sklearn.feature_selection import SelectKBest","227","  >>> pipe = Pipeline([","228","  ...    ('select', SelectKBest()),","229","  ...    ('model', calibrated_forest)])","230","  >>> param_grid = {","231","  ...    'select__k': [1, 2],","232","  ...    'model__base_estimator__max_depth': [2, 4, 6, 8]}","233","  >>> search = GridSearchCV(pipe, param_grid, cv=5).fit(X, y)","234",""],"delete":["45","possibly by reading the enclosed reference to the literature.  ","197",":ref:`pipeline` describes building composite estimators whose","198","parameter space can be searched with these tools."]}],"doc\/modules\/compose.rst":[{"add":["103","","104",".. _pipeline_nested_parameters:","105","","133","    >>> pipe[0]","152"," * :ref:`composite_grid_search`","374","    FeatureUnion(transformer_list=[('linear_pca', PCA()),","425","<sklearn.preprocessing.OneHotEncoder>` but apply a"],"delete":["130","    >>> pipe[0] ","149"," * :ref:`grid_search`","371","    FeatureUnion(transformer_list=[('linear_pca', PCA()), ","422","<sklearn.preprocessing.OneHotEncoder>` but apply a "]}]}},"19ad136223b62a631f8f331859f297730005e899":{"changes":{"sklearn\/__init__.py":"MODIFY","sklearn\/tree\/_classes.py":"MODIFY","sklearn\/utils\/tests\/test_testing.py":"MODIFY","sklearn\/metrics\/_classification.py":"MODIFY","sklearn\/ensemble\/tests\/test_voting.py":"MODIFY","sklearn\/utils\/deprecation.py":"MODIFY","sklearn\/inspection\/_partial_dependence.py":"MODIFY","sklearn\/feature_extraction\/tests\/test_feature_hasher.py":"MODIFY","sklearn\/utils\/tests\/test_utils.py":"MODIFY","doc\/developers\/tips.rst":"MODIFY","sklearn\/inspection\/tests\/test_plot_partial_dependence.py":"MODIFY","sklearn\/tests\/test_common.py":"MODIFY","sklearn\/linear_model\/_least_angle.py":"MODIFY","sklearn\/model_selection\/_search.py":"MODIFY","sklearn\/ensemble\/tests\/test_forest.py":"MODIFY","sklearn\/externals\/six.py":"MODIFY","sklearn\/utils\/__init__.py":"MODIFY","sklearn\/ensemble\/tests\/test_iforest.py":"MODIFY","sklearn\/ensemble\/_iforest.py":"MODIFY","sklearn\/utils\/tests\/test_linear_assignment.py":"MODIFY","sklearn\/metrics\/tests\/test_score_objects.py":"MODIFY","sklearn\/model_selection\/_split.py":"MODIFY","sklearn\/tests\/test_pipeline.py":"MODIFY","sklearn\/feature_extraction\/tests\/test_image.py":"MODIFY","sklearn\/metrics\/cluster\/tests\/test_unsupervised.py":"MODIFY","sklearn\/utils\/linear_assignment_.py":"MODIFY","sklearn\/utils\/validation.py":"MODIFY","sklearn\/ensemble\/_gb.py":"MODIFY","sklearn\/ensemble\/_base.py":"MODIFY","sklearn\/tests\/test_dummy.py":"MODIFY","sklearn\/pipeline.py":"MODIFY","sklearn\/utils\/tests\/test_deprecated_utils.py":"MODIFY","sklearn\/decomposition\/_sparse_pca.py":"MODIFY","sklearn\/compose\/_column_transformer.py":"MODIFY","sklearn\/utils\/tests\/test_validation.py":"MODIFY","sklearn\/datasets\/tests\/test_base.py":"MODIFY","sklearn\/tests\/test_docstring_parameters.py":"MODIFY","sklearn\/feature_extraction\/text.py":"MODIFY","sklearn\/linear_model\/tests\/test_logistic.py":"MODIFY","sklearn\/model_selection\/tests\/test_search.py":"MODIFY","sklearn\/model_selection\/tests\/test_split.py":"MODIFY","sklearn\/datasets\/_base.py":"MODIFY","sklearn\/externals\/joblib\/__init__.py":"MODIFY","sklearn\/svm\/_classes.py":"MODIFY","sklearn\/linear_model\/tests\/test_ridge.py":"MODIFY","sklearn\/tests\/test_calibration.py":"MODIFY","doc\/developers\/contributing.rst":"MODIFY","sklearn\/feature_extraction\/tests\/test_text.py":"MODIFY","sklearn\/utils\/tests\/test_estimator_checks.py":"MODIFY","sklearn\/metrics\/_scorer.py":"MODIFY","sklearn\/tests\/test_import_deprecations.py":"MODIFY","sklearn\/ensemble\/tests\/test_partial_dependence.py":"MODIFY","sklearn\/svm\/tests\/test_svm.py":"MODIFY","sklearn\/ensemble\/tests\/test_gradient_boosting.py":"MODIFY","sklearn\/utils\/tests\/test_extmath.py":"MODIFY","sklearn\/utils\/_testing.py":"MODIFY","sklearn\/tree\/tests\/test_tree.py":"MODIFY","sklearn\/metrics\/tests\/test_classification.py":"MODIFY","sklearn\/utils\/tests\/test_deprecation.py":"MODIFY","sklearn\/utils\/estimator_checks.py":"MODIFY","sklearn\/decomposition\/tests\/test_sparse_pca.py":"MODIFY","sklearn\/compose\/tests\/test_column_transformer.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY","sklearn\/cluster\/tests\/test_hierarchical.py":"MODIFY"},"diff":{"sklearn\/__init__.py":[{"add":[],"delete":["16","import warnings","27","# Make sure that DeprecationWarning within this package always gets printed","28","warnings.filterwarnings('always', category=DeprecationWarning,","29","                        module=r'^{0}\\.'.format(re.escape(__name__)))","30",""]}],"sklearn\/tree\/_classes.py":[{"add":["300","                          FutureWarning)","317","                          \"to the 'presort' parameter.\",","318","                          FutureWarning)","1231","        warnings.warn(msg, FutureWarning)","1239","        warnings.warn(msg, FutureWarning)"],"delete":["300","                          DeprecationWarning)","317","                          \"to the 'presort' parameter.\", DeprecationWarning)","1230","        warnings.warn(msg, DeprecationWarning)","1238","        warnings.warn(msg, DeprecationWarning)"]}],"sklearn\/utils\/tests\/test_testing.py":[{"add":["41","@pytest.mark.filterwarnings(\"ignore\",","42","                            category=FutureWarning)  # 0.24","48","@pytest.mark.filterwarnings(\"ignore\",","49","                            category=FutureWarning)  # 0.24","55","@pytest.mark.filterwarnings(\"ignore\",","56","                            category=FutureWarning)  # 0.24","63","@pytest.mark.filterwarnings(\"ignore\",","64","                            category=FutureWarning)  # 0.24","152","                                 category=FutureWarning))","259","            warnings.warn(\"yo\", FutureWarning)","674","    with pytest.warns(FutureWarning, match=msg):"],"delete":["41","@pytest.mark.filterwarnings(\"ignore\", category=DeprecationWarning)  # 0.24","47","@pytest.mark.filterwarnings(\"ignore\", category=DeprecationWarning)  # 0.24","53","@pytest.mark.filterwarnings(\"ignore\", category=DeprecationWarning)  # 0.24","60","@pytest.mark.filterwarnings(\"ignore\", category=DeprecationWarning)  # 0.24","148","                                 category=DeprecationWarning))","255","            warnings.warn(\"yo\", DeprecationWarning)","670","    with pytest.warns(DeprecationWarning, match=msg):"]}],"sklearn\/metrics\/_classification.py":[{"add":["643","                  'and multiclass classification tasks.',","644","                  FutureWarning)","2136","                      FutureWarning)"],"delete":["643","                  'and multiclass classification tasks.', DeprecationWarning)","2135","                      DeprecationWarning)"]}],"sklearn\/ensemble\/tests\/test_voting.py":[{"add":["560","    with pytest.warns(FutureWarning, match=msg):"],"delete":["560","    with pytest.warns(DeprecationWarning, match=msg):"]}],"sklearn\/utils\/deprecation.py":[{"add":["4","","67","            warnings.warn(msg, category=FutureWarning)","86","            warnings.warn(msg, category=FutureWarning)","101","            warnings.warn(msg, category=FutureWarning)","143","    warnings.warn(message, FutureWarning)"],"delete":["66","            warnings.warn(msg, category=DeprecationWarning)","85","            warnings.warn(msg, category=DeprecationWarning)","100","            warnings.warn(msg, category=DeprecationWarning)","142","    warnings.warn(message, DeprecationWarning)"]}],"sklearn\/inspection\/_partial_dependence.py":[{"add":["646","                      FutureWarning)"],"delete":["646","                      DeprecationWarning)"]}],"sklearn\/feature_extraction\/tests\/test_feature_hasher.py":[{"add":["146","@ignore_warnings(category=FutureWarning)"],"delete":["146","@ignore_warnings(category=DeprecationWarning)"]}],"sklearn\/utils\/tests\/test_utils.py":[{"add":["69","        assert issubclass(w[0].category, FutureWarning)","85","        assert issubclass(w[0].category, FutureWarning)","641","            FutureWarning, \"deprecated in version 0.20.1\",","642","            *args, **kw)"],"delete":["69","        assert issubclass(w[0].category, DeprecationWarning)","85","        assert issubclass(w[0].category, DeprecationWarning)","641","            DeprecationWarning, \"deprecated in version 0.20.1\", *args, **kw)"]}],"doc\/developers\/tips.rst":[{"add":["104","Since our continuous integration tests will error if","105","``FutureWarning`` isn't properly caught,","106","it is also recommended to run ``pytest`` along with the","107","``-Werror::FutureWarning`` flag."],"delete":["104","Since our continuous integration tests will error if ``DeprecationWarning``","105","or ``FutureWarning`` aren't properly caught, it is also recommended to run","106","``pytest`` along with the ``-Werror::DeprecationWarning`` and","107","``-Werror::FutureWarning`` flags."]}],"sklearn\/inspection\/tests\/test_plot_partial_dependence.py":[{"add":["375","    with pytest.warns(FutureWarning, match=msg):"],"delete":["375","    with pytest.warns(DeprecationWarning, match=msg):"]}],"sklearn\/tests\/test_common.py":[{"add":["93","    with ignore_warnings(category=(FutureWarning,","94","                                   ConvergenceWarning,","122","@ignore_warnings(category=(DeprecationWarning, FutureWarning))"],"delete":["93","    with ignore_warnings(category=(DeprecationWarning, ConvergenceWarning,","121","@ignore_warnings(category=DeprecationWarning)"]}],"sklearn\/linear_model\/_least_angle.py":[{"add":["159","                      FutureWarning)"],"delete":["159","                      DeprecationWarning)"]}],"sklearn\/model_selection\/_search.py":[{"add":["817","                \"removed in 0.24.\", FutureWarning"],"delete":["817","                \"removed in 0.24.\", DeprecationWarning"]}],"sklearn\/ensemble\/tests\/test_forest.py":[{"add":["1240","        est = assert_warns_message(FutureWarning,","1241","                                   \"min_impurity_decrease\","],"delete":["1240","        est = assert_warns_message(DeprecationWarning, \"min_impurity_decrease\","]}],"sklearn\/externals\/six.py":[{"add":["30","              \"(https:\/\/pypi.org\/project\/six\/).\", FutureWarning)"],"delete":["30","              \"(https:\/\/pypi.org\/project\/six\/).\", DeprecationWarning)"]}],"sklearn\/utils\/__init__.py":[{"add":["1183","        with ignore_warnings(category=FutureWarning):"],"delete":["1183","        with ignore_warnings(category=DeprecationWarning):"]}],"sklearn\/ensemble\/tests\/test_iforest.py":[{"add":["322","    with pytest.warns(FutureWarning, match=warn_msg):"],"delete":["322","    with pytest.warns(DeprecationWarning, match=warn_msg):"]}],"sklearn\/ensemble\/_iforest.py":[{"add":["235","                    FutureWarning"],"delete":["235","                    DeprecationWarning"]}],"sklearn\/utils\/tests\/test_linear_assignment.py":[{"add":["10","@pytest.mark.filterwarnings(","11","  \"ignore::FutureWarning\")"],"delete":["10","@pytest.mark.filterwarnings(\"ignore::DeprecationWarning\")"]}],"sklearn\/metrics\/tests\/test_score_objects.py":[{"add":["561","    with pytest.warns(FutureWarning):"],"delete":["561","    with pytest.warns(DeprecationWarning):"]}],"sklearn\/model_selection\/_split.py":[{"add":["295","                FutureWarning","2170","        warnings.simplefilter(\"always\", FutureWarning)","2176","            if len(w) and w[0].category == FutureWarning:"],"delete":["295","                DeprecationWarning","2170","        warnings.simplefilter(\"always\", DeprecationWarning)","2176","            if len(w) and w[0].category == DeprecationWarning:"]}],"sklearn\/tests\/test_pipeline.py":[{"add":["1190","    with pytest.warns(FutureWarning, match=msg):","1195","    with pytest.warns(FutureWarning, match=msg):"],"delete":["1190","    with pytest.warns(DeprecationWarning, match=msg):","1195","    with pytest.warns(DeprecationWarning, match=msg):"]}],"sklearn\/feature_extraction\/tests\/test_image.py":[{"add":["342","    with pytest.warns(FutureWarning, match=msg):"],"delete":["342","    with pytest.warns(DeprecationWarning, match=msg):"]}],"sklearn\/metrics\/cluster\/tests\/test_unsupervised.py":[{"add":["227","    assert_warns_message(FutureWarning, depr_message,"],"delete":["227","    assert_warns_message(DeprecationWarning, depr_message,"]}],"sklearn\/utils\/linear_assignment_.py":[{"add":["15","","21","    FutureWarning)","127","        FutureWarning)"],"delete":["20","    DeprecationWarning)","126","        DeprecationWarning)"]}],"sklearn\/utils\/validation.py":[{"add":["437","            FutureWarning, stacklevel=2)","931","                      \"argument is ignored.\", FutureWarning)","935","                      \"argument is ignored.\", FutureWarning)","1145","                          FutureWarning)"],"delete":["437","            DeprecationWarning, stacklevel=2)","931","                      \"argument is ignored.\", DeprecationWarning)","935","                      \"argument is ignored.\", DeprecationWarning)","1145","                          DeprecationWarning)"]}],"sklearn\/ensemble\/_gb.py":[{"add":["1341","                          FutureWarning)"],"delete":["1341","                          DeprecationWarning)"]}],"sklearn\/ensemble\/_base.py":[{"add":["232","                \"Use the string 'drop' instead.\", FutureWarning"],"delete":["232","                \"Use the string 'drop' instead.\", DeprecationWarning"]}],"sklearn\/tests\/test_dummy.py":[{"add":["763","    with pytest.warns(FutureWarning,"],"delete":["763","    with pytest.warns(DeprecationWarning,"]}],"sklearn\/pipeline.py":[{"add":["851","                              FutureWarning)"],"delete":["851","                              DeprecationWarning)"]}],"sklearn\/utils\/tests\/test_deprecated_utils.py":[{"add":["20","    with pytest.warns(FutureWarning,","21","                      match=\"removed in version 0.24\"):","27","    with pytest.warns(FutureWarning,","28","                      match=\"removed in version 0.24\"):","34","    with pytest.warns(FutureWarning,","35","                      match=\"removed in version 0.24\"):","41","    with pytest.warns(FutureWarning,","42","                      match=\"removed in version 0.24\"):","48","    with pytest.warns(FutureWarning,","49","                      match=\"removed in version 0.24\"):","55","    with pytest.warns(FutureWarning,","56","                      match=\"removed in version 0.24\"):","79","    with pytest.warns(FutureWarning,","80","                      match=\"removed in version 0.24\"):","86","    with pytest.warns(FutureWarning,","87","                      match=\"removed in version 0.24\"):","93","    with pytest.warns(FutureWarning,","94","                      match=\"removed in version 0.24\"):"],"delete":["20","    with pytest.warns(DeprecationWarning, match=\"removed in version 0.24\"):","26","    with pytest.warns(DeprecationWarning, match=\"removed in version 0.24\"):","32","    with pytest.warns(DeprecationWarning, match=\"removed in version 0.24\"):","38","    with pytest.warns(DeprecationWarning, match=\"removed in version 0.24\"):","44","    with pytest.warns(DeprecationWarning, match=\"removed in version 0.24\"):","50","    with pytest.warns(DeprecationWarning, match=\"removed in version 0.24\"):","73","    with pytest.warns(DeprecationWarning, match=\"removed in version 0.24\"):","79","    with pytest.warns(DeprecationWarning, match=\"removed in version 0.24\"):","85","    with pytest.warns(DeprecationWarning, match=\"removed in version 0.24\"):"]}],"sklearn\/decomposition\/_sparse_pca.py":[{"add":["22","                \" constructor.\", FutureWarning"],"delete":["22","                \" constructor.\", DeprecationWarning"]}],"sklearn\/compose\/_column_transformer.py":[{"add":["424","                          FutureWarning)"],"delete":["424","                          DeprecationWarning)"]}],"sklearn\/utils\/tests\/test_validation.py":[{"add":["452","            warnings.simplefilter(\"ignore\", FutureWarning)  # 0.23","487","    with pytest.warns(FutureWarning,","490","    with pytest.warns(FutureWarning,","727","        FutureWarning,","731","        FutureWarning,","801","        warnings.simplefilter(\"ignore\", FutureWarning)  # 0.23","820","        warnings.simplefilter(\"ignore\", FutureWarning)  # 0.23","1009","    with pytest.warns(FutureWarning,","1013","    with pytest.warns(FutureWarning,","1021","    with pytest.warns(FutureWarning,","1033","    with pytest.warns(FutureWarning,","1037","    with pytest.warns(FutureWarning,","1046","    with pytest.warns(FutureWarning,","1050","    with pytest.warns(FutureWarning,"],"delete":["452","            warnings.simplefilter(\"ignore\", DeprecationWarning)  # 0.23","487","    with pytest.warns(DeprecationWarning,","490","    with pytest.warns(DeprecationWarning,","727","        DeprecationWarning,","731","        DeprecationWarning,","801","        warnings.simplefilter(\"ignore\", DeprecationWarning)  # 0.23","820","        warnings.simplefilter(\"ignore\", DeprecationWarning)  # 0.23","1009","    with pytest.warns(DeprecationWarning,","1013","    with pytest.warns(DeprecationWarning,","1021","    with pytest.warns(DeprecationWarning,","1033","    with pytest.warns(DeprecationWarning,","1037","    with pytest.warns(DeprecationWarning,","1046","    with pytest.warns(DeprecationWarning,","1050","    with pytest.warns(DeprecationWarning,"]}],"sklearn\/datasets\/tests\/test_base.py":[{"add":["294","        warnings.warn(msg, FutureWarning)","298","        warnings.warn(\"unrelated warning\", FutureWarning)","314","    with pytest.warns(FutureWarning, match=msg):","329","    with pytest.warns(FutureWarning, match=\"unrelated warning\"):"],"delete":["294","        warnings.warn(msg, DeprecationWarning)","298","        warnings.warn(\"unrelated warning\", DeprecationWarning)","314","    with pytest.warns(DeprecationWarning, match=msg):","329","    with pytest.warns(DeprecationWarning, match=\"unrelated warning\"):"]}],"sklearn\/tests\/test_docstring_parameters.py":[{"add":["21","","22","# walk_packages() ignores DeprecationWarnings, now we need to ignore","23","# FutureWarnings","24","with warnings.catch_warnings():","25","    warnings.simplefilter('ignore', FutureWarning)","26","    PUBLIC_MODULES = set([","27","        pckg[1] for pckg in walk_packages(prefix='sklearn.',","28","                                          path=sklearn.__path__)","29","        if not (\"._\" in pckg[1] or \".tests.\" in pckg[1])","30","    ])","54","@pytest.mark.filterwarnings('ignore::FutureWarning')","135","@ignore_warnings(category=FutureWarning)"],"delete":["21","PUBLIC_MODULES = set([pckg[1] for pckg in walk_packages(prefix='sklearn.',","22","                                                        path=sklearn.__path__)","23","                      if not (\"._\" in pckg[1] or \".tests.\" in pckg[1])])","127","@ignore_warnings(category=DeprecationWarning)"]}],"sklearn\/feature_extraction\/text.py":[{"add":["1839","            warnings.warn(msg, FutureWarning)"],"delete":["1839","            warnings.warn(msg, DeprecationWarning)"]}],"sklearn\/linear_model\/tests\/test_logistic.py":[{"add":["1731","    assert_warns_message(FutureWarning,"],"delete":["1731","    assert_warns_message(DeprecationWarning,"]}],"sklearn\/model_selection\/tests\/test_search.py":[{"add":["1696","    with pytest.warns(FutureWarning,","1736","    with pytest.warns(FutureWarning, match=depr_msg):"],"delete":["1696","    with pytest.warns(DeprecationWarning,","1736","    with pytest.warns(DeprecationWarning, match=depr_msg):"]}],"sklearn\/model_selection\/tests\/test_split.py":[{"add":["1425","        warnings.simplefilter(\"ignore\", FutureWarning)","1592","    with pytest.warns(FutureWarning,"],"delete":["1425","        warnings.simplefilter(\"ignore\", DeprecationWarning)","1592","    with pytest.warns(DeprecationWarning,"]}],"sklearn\/datasets\/_base.py":[{"add":["934","            warnings.warn(message=message, category=FutureWarning)"],"delete":["934","            warnings.warn(message=message, category=DeprecationWarning)"]}],"sklearn\/externals\/joblib\/__init__.py":[{"add":["14","    warnings.warn(msg, category=FutureWarning)"],"delete":["14","    warnings.warn(msg, category=DeprecationWarning)"]}],"sklearn\/svm\/_classes.py":[{"add":["225","                          FutureWarning)","414","                          FutureWarning)"],"delete":["225","                          DeprecationWarning)","414","                          DeprecationWarning)"]}],"sklearn\/linear_model\/tests\/test_ridge.py":[{"add":["524","    # ignore warning from GridSearchCV: FutureWarning: The default","525","    # of the `iid` parameter will change from True to False in version 0.22","526","    # and will be removed in 0.24","527","    with ignore_warnings(category=FutureWarning):"],"delete":["524","    # ignore warning from GridSearchCV: DeprecationWarning: The default of the","525","    # `iid` parameter will change from True to False in version 0.22 and will","526","    # be removed in 0.24","527","    with ignore_warnings(category=DeprecationWarning):"]}],"sklearn\/tests\/test_calibration.py":[{"add":["322","@ignore_warnings(category=FutureWarning)"],"delete":["322","@ignore_warnings(category=(DeprecationWarning, FutureWarning))"]}],"doc\/developers\/contributing.rst":[{"add":["828","If a parameter has to be deprecated, a ``FutureWarning`` warning","829","must be raised too.","837","                          \"will be removed in 0.15.\",","838","                          FutureWarning)","853","                            \"will be removed in 0.15.\",","854","                            FutureWarning)"],"delete":["828","If a parameter has to be deprecated, use ``DeprecationWarning`` appropriately.","836","                          \"will be removed in 0.15.\", DeprecationWarning)","851","                            \"will be removed in 0.15.\", DeprecationWarning)"]}],"sklearn\/feature_extraction\/tests\/test_text.py":[{"add":["545","    with pytest.warns(FutureWarning, match=msg):","1370","    with pytest.warns(FutureWarning, match=msg):"],"delete":["545","    with pytest.warns(DeprecationWarning, match=msg):","1370","    with pytest.warns(DeprecationWarning, match=msg):"]}],"sklearn\/utils\/tests\/test_estimator_checks.py":[{"add":["465","        with ignore_warnings(category=FutureWarning):","475","        with ignore_warnings(category=FutureWarning):"],"delete":["465","        with ignore_warnings(category=(FutureWarning, DeprecationWarning)):","475","        with ignore_warnings(category=(FutureWarning, DeprecationWarning)):"]}],"sklearn\/metrics\/_scorer.py":[{"add":["165","                          category=FutureWarning,"],"delete":["165","                          category=DeprecationWarning,"]}],"sklearn\/tests\/test_import_deprecations.py":[{"add":["38","    with pytest.warns(FutureWarning,"],"delete":["38","    with pytest.warns(DeprecationWarning,"]}],"sklearn\/ensemble\/tests\/test_partial_dependence.py":[{"add":["29","@ignore_warnings(category=FutureWarning)","68","@ignore_warnings(category=FutureWarning)","84","@ignore_warnings(category=FutureWarning)","98","@ignore_warnings(category=FutureWarning)","124","@ignore_warnings(category=FutureWarning)","153","@ignore_warnings(category=FutureWarning)","189","@ignore_warnings(category=FutureWarning)","226","@ignore_warnings(category=FutureWarning)","275","    with pytest.warns(FutureWarning, match=warn_msg):"],"delete":["29","@ignore_warnings(category=DeprecationWarning)","68","@ignore_warnings(category=DeprecationWarning)","84","@ignore_warnings(category=DeprecationWarning)","98","@ignore_warnings(category=DeprecationWarning)","124","@ignore_warnings(category=DeprecationWarning)","153","@ignore_warnings(category=DeprecationWarning)","189","@ignore_warnings(category=DeprecationWarning)","226","@ignore_warnings(category=DeprecationWarning)","275","    with pytest.warns(DeprecationWarning, match=warn_msg):"]}],"sklearn\/svm\/tests\/test_svm.py":[{"add":["748","    assert_warns_message(FutureWarning,","753","    assert_warns_message(FutureWarning,","759","    assert_warns_message(FutureWarning,","765","    assert_warns_message(FutureWarning,"],"delete":["748","    assert_warns_message(DeprecationWarning,","753","    assert_warns_message(DeprecationWarning,","759","    assert_warns_message(DeprecationWarning,","765","    assert_warns_message(DeprecationWarning,"]}],"sklearn\/ensemble\/tests\/test_gradient_boosting.py":[{"add":["1089","    est = assert_warns_message(FutureWarning,","1090","                               \"min_impurity_decrease\",","1403","    with pytest.warns(FutureWarning,"],"delete":["1089","    est = assert_warns_message(DeprecationWarning, \"min_impurity_decrease\",","1402","    with pytest.warns(DeprecationWarning,"]}],"sklearn\/utils\/tests\/test_extmath.py":[{"add":["650","    with pytest.warns(FutureWarning, match=msg):"],"delete":["650","    with pytest.warns(DeprecationWarning, match=msg):"]}],"sklearn\/utils\/_testing.py":[{"add":["53","","124","        if hasattr(np, 'FutureWarning'):","170","        if hasattr(np, 'FutureWarning'):","246","        if hasattr(np, 'FutureWarning'):","496","                      FutureWarning)","501","                      FutureWarning)","506","                      FutureWarning)","519","        with ignore_warnings(category=FutureWarning):"],"delete":["123","        if hasattr(np, 'VisibleDeprecationWarning'):","169","        if hasattr(np, 'VisibleDeprecationWarning'):","245","        if hasattr(np, 'VisibleDeprecationWarning'):","495","                      DeprecationWarning)","500","                      DeprecationWarning)","505","                      DeprecationWarning)","518","        with ignore_warnings(category=DeprecationWarning):"]}],"sklearn\/tree\/tests\/test_tree.py":[{"add":["528","        with ignore_warnings(category=FutureWarning):","809","            assert_warns(FutureWarning, est.fit, X, y)","825","        assert_warns_message(FutureWarning,","1625","    with pytest.warns(FutureWarning,","1959","    with pytest.warns(FutureWarning, match=match):","1963","    with pytest.warns(FutureWarning, match=match):"],"delete":["528","        with ignore_warnings(category=DeprecationWarning):","809","            assert_warns(DeprecationWarning, est.fit, X, y)","825","        assert_warns_message(DeprecationWarning,","1625","    with pytest.warns(DeprecationWarning,","1959","    with pytest.warns(DeprecationWarning, match=match):","1963","    with pytest.warns(DeprecationWarning, match=match):"]}],"sklearn\/metrics\/tests\/test_classification.py":[{"add":["1143","    assert_warns_message(FutureWarning,","2214","    jss = partial(assert_warns, FutureWarning,","2215","                  jaccard_similarity_score)"],"delete":["1143","    assert_warns_message(DeprecationWarning,","2214","    jss = partial(assert_warns, DeprecationWarning, jaccard_similarity_score)"]}],"sklearn\/utils\/tests\/test_deprecation.py":[{"add":["38","    assert_warns_message(FutureWarning, 'qwerty', MockClass1)","39","    assert_warns_message(FutureWarning, 'mockclass2_method',","41","    assert_warns_message(FutureWarning, 'deprecated', MockClass3)","42","    val = assert_warns_message(FutureWarning, 'deprecated',","43","                               mock_function)"],"delete":["38","    assert_warns_message(DeprecationWarning, 'qwerty', MockClass1)","39","    assert_warns_message(DeprecationWarning, 'mockclass2_method',","41","    assert_warns_message(DeprecationWarning, 'deprecated', MockClass3)","42","    val = assert_warns_message(DeprecationWarning, 'deprecated', mock_function)"]}],"sklearn\/utils\/estimator_checks.py":[{"add":["146","@ignore_warnings(category=FutureWarning)","655","    with ignore_warnings(category=FutureWarning):","660","        with ignore_warnings(category=FutureWarning):","666","            with ignore_warnings(category=FutureWarning):","701","@ignore_warnings(category=FutureWarning)","728","@ignore_warnings(category=(FutureWarning))","745","@ignore_warnings(category=(FutureWarning))","765","@ignore_warnings(category=FutureWarning)","803","@ignore_warnings(category=(FutureWarning, UserWarning))","902","@ignore_warnings(category=FutureWarning)","957","@ignore_warnings(category=FutureWarning)","1007","@ignore_warnings(category=FutureWarning)","1138","@ignore_warnings(category=FutureWarning)","1152","@ignore_warnings(category=FutureWarning)","1168","@ignore_warnings(category=FutureWarning)","1344","@ignore_warnings(category=FutureWarning)","1367","@ignore_warnings(category=FutureWarning)","1387","        with ignore_warnings(category=FutureWarning):","1500","@ignore_warnings(category=FutureWarning)","1526","@ignore_warnings(category=FutureWarning)","1584","@ignore_warnings(category=FutureWarning)","1607","@ignore_warnings(category=FutureWarning)","1666","@ignore_warnings(category=FutureWarning)","1681","@ignore_warnings(category=FutureWarning)","1691","    with ignore_warnings(category=FutureWarning):","1934","@ignore_warnings(category=(FutureWarning))","1968","@ignore_warnings(category=FutureWarning)","2007","@ignore_warnings(category=FutureWarning)","2139","@ignore_warnings(category=FutureWarning)","2167","@ignore_warnings(category=FutureWarning)","2235","        assert_warns_message(FutureWarning, msg, func, X)","2238","@ignore_warnings(category=FutureWarning)","2290","@ignore_warnings(category=FutureWarning)","2310","@ignore_warnings(category=FutureWarning)","2349","@ignore_warnings(category=FutureWarning)","2388","@ignore_warnings(category=FutureWarning)","2422","@ignore_warnings(category=FutureWarning)","2445","@ignore_warnings(category=FutureWarning)","2455","@ignore_warnings(category=FutureWarning)","2463","@ignore_warnings(category=FutureWarning)","2490","    with ignore_warnings(category=FutureWarning):","2574","@ignore_warnings(category=FutureWarning)","2608","@ignore_warnings(category=FutureWarning)","2634","@ignore_warnings(category=FutureWarning)","2646","@ignore_warnings(category=FutureWarning)","2700","@ignore_warnings(category=FutureWarning)","2711","@ignore_warnings(category=FutureWarning)"],"delete":["146","@ignore_warnings(category=(DeprecationWarning, FutureWarning))","655","    with ignore_warnings(category=DeprecationWarning):","660","        with ignore_warnings(category=(DeprecationWarning, FutureWarning)):","666","            with ignore_warnings(category=(DeprecationWarning, FutureWarning)):","701","@ignore_warnings(category=(DeprecationWarning, FutureWarning))","728","@ignore_warnings(category=(DeprecationWarning, FutureWarning))","745","@ignore_warnings(category=(DeprecationWarning, FutureWarning))","765","@ignore_warnings(category=(DeprecationWarning, FutureWarning))","803","@ignore_warnings(category=(DeprecationWarning, FutureWarning, UserWarning))","902","@ignore_warnings(category=(DeprecationWarning, FutureWarning))","957","@ignore_warnings(category=(DeprecationWarning, FutureWarning))","1007","@ignore_warnings(category=(DeprecationWarning, FutureWarning))","1138","@ignore_warnings(category=(DeprecationWarning, FutureWarning))","1152","@ignore_warnings(category=(DeprecationWarning, FutureWarning))","1168","@ignore_warnings(category=(DeprecationWarning, FutureWarning))","1344","@ignore_warnings(category=(DeprecationWarning, FutureWarning))","1367","@ignore_warnings(category=DeprecationWarning)","1387","        with ignore_warnings(category=(DeprecationWarning, FutureWarning)):","1500","@ignore_warnings(category=(DeprecationWarning, FutureWarning))","1526","@ignore_warnings(category=(DeprecationWarning, FutureWarning))","1584","@ignore_warnings(category=(DeprecationWarning, FutureWarning))","1607","@ignore_warnings(category=(DeprecationWarning, FutureWarning))","1666","@ignore_warnings(category=DeprecationWarning)","1681","@ignore_warnings(category=DeprecationWarning)","1691","    with ignore_warnings(category=(DeprecationWarning, FutureWarning)):","1967","@ignore_warnings(category=(DeprecationWarning, FutureWarning))","2006","@ignore_warnings(category=(DeprecationWarning, FutureWarning))","2138","@ignore_warnings(category=(DeprecationWarning, FutureWarning))","2166","@ignore_warnings(category=(DeprecationWarning, FutureWarning))","2234","        assert_warns_message(DeprecationWarning, msg, func, X)","2237","@ignore_warnings(category=(DeprecationWarning, FutureWarning))","2289","@ignore_warnings(category=(DeprecationWarning, FutureWarning))","2309","@ignore_warnings(category=(DeprecationWarning, FutureWarning))","2348","@ignore_warnings(category=(DeprecationWarning, FutureWarning))","2387","@ignore_warnings(category=(DeprecationWarning, FutureWarning))","2421","@ignore_warnings(category=(DeprecationWarning, FutureWarning))","2444","@ignore_warnings(category=DeprecationWarning)","2454","@ignore_warnings(category=DeprecationWarning)","2462","@ignore_warnings(category=(DeprecationWarning, FutureWarning))","2489","    with ignore_warnings(category=(DeprecationWarning, FutureWarning)):","2573","@ignore_warnings(category=(DeprecationWarning, FutureWarning))","2607","@ignore_warnings(category=(DeprecationWarning, FutureWarning))","2633","@ignore_warnings(category=(DeprecationWarning, FutureWarning))","2645","@ignore_warnings(category=(DeprecationWarning, FutureWarning))","2699","@ignore_warnings(category=(DeprecationWarning, FutureWarning))","2710","@ignore_warnings(category=(DeprecationWarning, FutureWarning))"]}],"sklearn\/decomposition\/tests\/test_sparse_pca.py":[{"add":["197","    with pytest.warns(FutureWarning, match=warn_msg):"],"delete":["15","","198","    with pytest.warns(DeprecationWarning, match=warn_msg):"]}],"sklearn\/compose\/tests\/test_column_transformer.py":[{"add":["502","    with pytest.warns(FutureWarning, match=msg):","1110","    with pytest.warns(FutureWarning, match=warn_msg):","1135","    with pytest.warns(FutureWarning, match=msg):","1141","    with pytest.warns(FutureWarning, match=msg):","1151","    with pytest.warns(FutureWarning, match=msg):"],"delete":["502","    with pytest.warns(DeprecationWarning, match=msg):","1110","    with pytest.warns(DeprecationWarning, match=warn_msg):","1135","    with pytest.warns(DeprecationWarning, match=msg):","1141","    with pytest.warns(DeprecationWarning, match=msg):","1151","    with pytest.warns(DeprecationWarning, match=msg):"]}],"doc\/whats_new\/v0.22.rst":[{"add":["15","","16","Deprecations: using ``FutureWarning`` from now on","17","-------------------------------------------------","18","","19","When deprecating a feature, previous versions of scikit-learn used to raise","20","a ``DeprecationWarning``. Since the ``DeprecationWarnings`` aren't shown by","21","default by Python, scikit-learn needed to resort to a custom warning filter","22","that would always show the warnings.","23","","24","This filter is now removed, and starting from 0.22 scikit-learn will show","25","``FutureWarnings`` for deprecations. :pr:`15080` by `Nicolas Hug`_.","26","","27",""],"delete":[]}],"sklearn\/cluster\/tests\/test_hierarchical.py":[{"add":["736","    with pytest.warns(FutureWarning, match=match):"],"delete":["736","    with pytest.warns(DeprecationWarning, match=match):"]}]}},"5674122c9755c7b6acf2c30ee0759f7087879a95":{"changes":{"sklearn\/impute\/tests\/test_impute.py":"MODIFY","sklearn\/gaussian_process\/tests\/test_gpc.py":"MODIFY","sklearn\/mixture\/tests\/test_gaussian_mixture.py":"MODIFY","sklearn\/metrics\/tests\/test_classification.py":"MODIFY","sklearn\/model_selection\/tests\/test_search.py":"MODIFY","sklearn\/manifold\/tests\/test_t_sne.py":"MODIFY","sklearn\/model_selection\/tests\/test_split.py":"MODIFY","sklearn\/feature_selection\/tests\/test_feature_select.py":"MODIFY","sklearn\/feature_extraction\/tests\/test_image.py":"MODIFY","sklearn\/metrics\/tests\/test_common.py":"MODIFY","sklearn\/model_selection\/tests\/test_validation.py":"MODIFY","sklearn\/metrics\/tests\/test_pairwise.py":"MODIFY","sklearn\/mixture\/tests\/test_bayesian_mixture.py":"MODIFY","sklearn\/inspection\/tests\/test_partial_dependence.py":"MODIFY"},"diff":{"sklearn\/impute\/tests\/test_impute.py":[{"add":["798","    d = 50","810","    assert_allclose(X_filled, X, atol=0.02)","819","    n = 70","820","    d = 70","834","    imputer = IterativeImputer(max_iter=5,","892","                               tol=1e-2,"],"delete":["798","    d = 100","810","    assert_allclose(X_filled, X, atol=0.01)","819","    n = 100","820","    d = 100","834","    imputer = IterativeImputer(max_iter=10,","892","                               tol=1e-3,"]}],"sklearn\/gaussian_process\/tests\/test_gpc.py":[{"add":["115","    # Define a dummy optimizer that simply tests 10 random hyperparameters","120","        for _ in range(10):"],"delete":["115","    # Define a dummy optimizer that simply tests 50 random hyperparameters","120","        for _ in range(50):"]}],"sklearn\/mixture\/tests\/test_gaussian_mixture.py":[{"add":["71","    def __init__(self, rng, n_samples=200, n_components=2, n_features=2,","654","            assert_allclose(ecov.error_norm(prec_pred[k]), 0, atol=0.15)","1033","    for random_state in range(15):","1034","        rand_data = RandomData(np.random.RandomState(random_state),","1035","                               n_samples=50, scale=1)"],"delete":["71","    def __init__(self, rng, n_samples=500, n_components=2, n_features=2,","654","            assert_allclose(ecov.error_norm(prec_pred[k]), 0, atol=0.1)","1033","    for random_state in range(25):","1034","        rand_data = RandomData(np.random.RandomState(random_state), scale=1)"]}],"sklearn\/metrics\/tests\/test_classification.py":[{"add":["680","@pytest.mark.parametrize('n_points', [100, 10000])"],"delete":["680","@pytest.mark.parametrize('n_points', [100, 10000, 1000000])"]}],"sklearn\/model_selection\/tests\/test_search.py":[{"add":["739","    X, y = make_blobs(n_samples=50, random_state=0)","740","    km = KMeans(random_state=0, init=\"random\", n_init=1)","1101","    params = dict(C=np.logspace(-4, 1, 3),","1102","                  gamma=np.logspace(-5, 0, 3, base=0.1))","1109","                    probability = True","1111","                else:","1112","                    probability = False","1113","                clf = SVC(probability=probability, random_state=42)"],"delete":["739","    X, y = make_blobs(random_state=0)","740","    km = KMeans(random_state=0)","1101","    params = dict(C=np.logspace(-10, 1), gamma=np.logspace(-5, 0, base=0.1))","1109","                clf = SVC(probability=True, random_state=42)"]}],"sklearn\/manifold\/tests\/test_t_sne.py":[{"add":["128","    n_samples = 200","241","@pytest.mark.parametrize(\"method\", ['exact', 'barnes_hut'])","242","@pytest.mark.parametrize(\"init\", ('random', 'pca'))","243","def test_preserve_trustworthiness_approximately(method, init):","248","    tsne = TSNE(n_components=n_components, init=init, random_state=0,","249","                method=method, n_iter=700)","250","    X_embedded = tsne.fit_transform(X)","251","    t = trustworthiness(X, X_embedded, n_neighbors=1)","252","    assert t > 0.85","272","    X = random_state.randn(50, 2)","273","    X[(np.random.randint(0, 50, 25), np.random.randint(0, 2, 25))] = 0.0","276","                random_state=0, method='exact', n_iter=500)","286","        X = random_state.randn(80, 2)","290","                    random_state=i, verbose=0, n_iter=500)","419","                    method=method, early_exaggeration=1.0, n_iter=250)","423","                    method=method, early_exaggeration=10.0, n_iter=250)","585","    X = random_state.randn(10, 2).astype(dt, copy=False)","587","                random_state=0, method=method, verbose=0,","588","                n_iter=300)","605","                random_state=0, method=method, verbose=0, n_iter=503)","722","    X = random_state.randn(50, 2)","724","                random_state=0, method='exact',","725","                n_iter=500)","747","@pytest.mark.parametrize('method', ['barnes_hut', 'exact'])","748","def test_uniform_grid(method):","760","    seeds = [0, 1, 2]","761","    n_iter = 500","828","            random_state=0, n_iter=300).fit_transform(X)","831","            random_state=0, n_iter=300).fit_transform(dist_func(X))"],"delete":["128","    n_samples = 500","241","def test_preserve_trustworthiness_approximately():","245","    methods = ['exact', 'barnes_hut']","247","    for init in ('random', 'pca'):","248","        for method in methods:","249","            tsne = TSNE(n_components=n_components, init=init, random_state=0,","250","                        method=method)","251","            X_embedded = tsne.fit_transform(X)","252","            t = trustworthiness(X, X_embedded, n_neighbors=1)","253","            assert_greater(t, 0.85, msg='Trustworthiness={:0.3f} < 0.85 '","254","                                        'for method={} and '","255","                                        'init={}'.format(t, method, init))","275","    X = random_state.randn(100, 2)","276","    X[(np.random.randint(0, 100, 50), np.random.randint(0, 2, 50))] = 0.0","279","                random_state=0, method='exact')","289","        X = random_state.randn(100, 2)","293","                    random_state=i, verbose=0)","422","                    method=method, early_exaggeration=1.0)","426","                    method=method, early_exaggeration=10.0)","588","    X = random_state.randn(50, 2).astype(dt, copy=False)","590","                random_state=0, method=method, verbose=0)","607","                random_state=0, method=method, verbose=0, n_iter=1003)","724","    X = random_state.randn(100, 2)","726","                random_state=0, method='exact')","748","def check_uniform_grid(method, seeds=[0, 1, 2], n_iter=1000):","793","@pytest.mark.parametrize('method', ['barnes_hut', 'exact'])","794","def test_uniform_grid(method):","795","    check_uniform_grid(method)","796","","797","","831","            random_state=0).fit_transform(X)","834","            random_state=0).fit_transform(dist_func(X))"]}],"sklearn\/model_selection\/tests\/test_split.py":[{"add":["1479","        gs = GridSearchCV(Ridge(solver=\"eigen\"), param_grid={'alpha': [1, .1]},"],"delete":["1479","        gs = GridSearchCV(Ridge(), param_grid={'alpha': [1, .1]},"]}],"sklearn\/feature_selection\/tests\/test_feature_select.py":[{"add":["8","import pytest","9","","410","@pytest.mark.parametrize(\"alpha\", [0.001, 0.01, 0.1])","411","@pytest.mark.parametrize(\"n_informative\", [1, 5, 10])","412","def test_select_fdr_regression(alpha, n_informative):","438","    # As per Benjamini-Hochberg, the expected false discovery rate","439","    # should be lower than alpha:","440","    # FDR = E(FP \/ (TP + FP)) <= alpha","441","    false_discovery_rate = np.mean([single_fdr(alpha, n_informative,","442","                                               random_state) for","443","                                    random_state in range(100)])","444","    assert alpha >= false_discovery_rate","446","    # Make sure that the empirical false discovery rate increases","447","    # with alpha:","448","    if false_discovery_rate != 0:","449","        assert false_discovery_rate > alpha \/ 10"],"delete":["18","from sklearn.utils.testing import assert_greater","19","from sklearn.utils.testing import assert_greater_equal","410","def test_select_fdr_regression():","436","    for alpha in [0.001, 0.01, 0.1]:","437","        for n_informative in [1, 5, 10]:","438","            # As per Benjamini-Hochberg, the expected false discovery rate","439","            # should be lower than alpha:","440","            # FDR = E(FP \/ (TP + FP)) <= alpha","441","            false_discovery_rate = np.mean([single_fdr(alpha, n_informative,","442","                                                       random_state) for","443","                                            random_state in range(100)])","444","            assert_greater_equal(alpha, false_discovery_rate)","446","            # Make sure that the empirical false discovery rate increases","447","            # with alpha:","448","            if false_discovery_rate != 0:","449","                assert_greater(false_discovery_rate, alpha \/ 10)"]}],"sklearn\/feature_extraction\/tests\/test_image.py":[{"add":["65","    # subsample by 4 to reduce run time","66","    face = face[::4, ::4]","81","","82","    # subsample by 4 to reduce run time","83","    face = face[::4, ::4]","84",""],"delete":[]}],"sklearn\/metrics\/tests\/test_common.py":[{"add":["1199","    _, ya = make_multilabel_classification(n_features=1, n_classes=10,","1200","                                           random_state=0, n_samples=50,","1202","    _, yb = make_multilabel_classification(n_features=1, n_classes=10,","1203","                                           random_state=1, n_samples=50,"],"delete":["1199","    _, ya = make_multilabel_classification(n_features=1, n_classes=20,","1200","                                           random_state=0, n_samples=100,","1202","    _, yb = make_multilabel_classification(n_features=1, n_classes=20,","1203","                                           random_state=1, n_samples=100,"]}],"sklearn\/model_selection\/tests\/test_validation.py":[{"add":["814","                         cross_val_predict,","815","                         LogisticRegression(solver=\"liblinear\"),","822","    preds = cross_val_predict(LogisticRegression(solver=\"liblinear\"), X, y,","828","    preds = cross_val_predict(LogisticRegression(solver=\"liblinear\"), X, y,","869","    preds = cross_val_predict(LogisticRegression(solver=\"liblinear\"), X, y,","875","    preds = cross_val_predict(LogisticRegression(solver=\"liblinear\"), X, y,","883","    preds = cross_val_predict(LogisticRegression(solver=\"liblinear\"), X, y,","889","    preds = cross_val_predict(LogisticRegression(solver=\"liblinear\"), X, y,","926","    predictions = cross_val_predict(LogisticRegression(solver=\"liblinear\"),","927","                                    X.tolist(),","929","    predictions = cross_val_predict(LogisticRegression(solver=\"liblinear\"),","930","                                    X,","966","    clf = LogisticRegression(random_state=1, solver=\"liblinear\")","1397","    X, y = make_classification(n_classes=2,  random_state=0)","1411","    check_cross_val_predict_with_method_binary(","1412","            LogisticRegression(solver=\"liblinear\"))","1413","    check_cross_val_predict_with_method_multiclass(","1414","            LogisticRegression(solver=\"liblinear\"))","1433","    est = GridSearchCV(LogisticRegression(random_state=42, solver=\"liblinear\"),","1449","    est = OneVsRestClassifier(LogisticRegression(solver=\"liblinear\",","1450","                                                 random_state=0))","1490","    est = LogisticRegression(solver=\"liblinear\")","1548","        est = LogisticRegression(solver=\"liblinear\")"],"delete":["814","                         cross_val_predict, LogisticRegression(),","821","    preds = cross_val_predict(LogisticRegression(), X, y,","827","    preds = cross_val_predict(LogisticRegression(), X, y,","868","    preds = cross_val_predict(LogisticRegression(), X, y,","874","    preds = cross_val_predict(LogisticRegression(), X, y,","882","    preds = cross_val_predict(LogisticRegression(), X, y,","888","    preds = cross_val_predict(LogisticRegression(), X, y,","925","    predictions = cross_val_predict(LogisticRegression(), X.tolist(),","927","    predictions = cross_val_predict(LogisticRegression(), X,","963","    clf = LogisticRegression(random_state=1)","1394","    X, y = make_classification(n_classes=2, random_state=0)","1408","    check_cross_val_predict_with_method_binary(LogisticRegression())","1409","    check_cross_val_predict_with_method_multiclass(LogisticRegression())","1428","    est = GridSearchCV(LogisticRegression(random_state=42),","1444","    est = OneVsRestClassifier(LogisticRegression(random_state=0))","1484","    est = LogisticRegression()","1542","        est = LogisticRegression()"]}],"sklearn\/metrics\/tests\/test_pairwise.py":[{"add":["560","    X = rng.random_sample((200, 4))","571","    Y = rng.random_sample((100, 4))","1105","    with config_context(working_memory=0.1):  # to have more than 1 chunk","1107","        X = rng.random_sample((100, 10))","1117","            Y = rng.random_sample((100, 10))"],"delete":["560","    X = rng.random_sample((400, 4))","571","    Y = rng.random_sample((200, 4))","1105","    with config_context(working_memory=1):  # to have more than 1 chunk","1107","        X = rng.random_sample((1000, 10))","1117","            Y = rng.random_sample((1000, 10))"]}],"sklearn\/mixture\/tests\/test_bayesian_mixture.py":[{"add":["298","                warm_start=True, max_iter=1, random_state=rng, tol=1e-3)","437","    rand_data = RandomData(rng, n_samples=50, scale=7)","455","    X = np.random.RandomState(0).randn(50, 5)"],"delete":["298","                warm_start=True, max_iter=1, random_state=rng, tol=1e-4)","437","    rand_data = RandomData(rng, scale=7)","455","    X = np.random.RandomState(0).randn(1000, 5)"]}],"sklearn\/inspection\/tests\/test_partial_dependence.py":[{"add":["39","binary_classification_data = (make_classification(n_samples=50,","40","                                                  random_state=0), 1)","41","multiclass_classification_data = (make_classification(n_samples=50,","42","                                                      n_classes=3,","45","regression_data = (make_regression(n_samples=50, random_state=0), 1)","46","multioutput_regression_data = (make_regression(n_samples=50, n_targets=2,","47","                                               random_state=0), 2)"],"delete":["39","binary_classification_data = (make_classification(random_state=0), 1)","40","multiclass_classification_data = (make_classification(n_classes=3,","43","regression_data = (make_regression(random_state=0), 1)","44","multioutput_regression_data = (make_regression(n_targets=2, random_state=0), 2)"]}]}},"b3030f046f7a39be8dbdf44a010ab1b888c4a785":{"changes":{"sklearn\/mixture\/gaussian_mixture.py":"MODIFY"},"diff":{"sklearn\/mixture\/gaussian_mixture.py":[{"add":["311","    if covariance_type == 'full':"],"delete":["311","    if covariance_type in 'full':"]}]}},"24d4b2c2f3bae149407db57182643405426f275c":{"changes":{"examples\/neural_networks\/plot_mlp_training_curves.py":"MODIFY"},"diff":{"examples\/neural_networks\/plot_mlp_training_curves.py":[{"add":["16","","17","import warnings","18","","20","","24","from sklearn.exceptions import ConvergenceWarning","59","","72","","73","        # some parameter combinations will not converge as can be seen on the","74","        # plots so they are ignored here","75","        with warnings.catch_warnings():","76","            warnings.filterwarnings(\"ignore\", category=ConvergenceWarning,","77","                                    module=\"sklearn\")","78","            mlp.fit(X, y)","79","","84","        ax.plot(mlp.loss_curve_, label=label, **args)"],"delete":["66","        mlp.fit(X, y)","71","            ax.plot(mlp.loss_curve_, label=label, **args)"]}]}},"93c628eedea495d0fe3e284c066e5cebff5b758c":{"changes":{"sklearn\/isotonic.py":"MODIFY","sklearn\/tests\/test_isotonic.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY"},"diff":{"sklearn\/isotonic.py":[{"add":["326","        check_params = dict(accept_sparse=False, ensure_2d=False)","327","        X = check_array(X, dtype=[np.float64, np.float32], **check_params)","328","        y = check_array(y, dtype=X.dtype, **check_params)"],"delete":["326","        check_params = dict(accept_sparse=False, ensure_2d=False,","327","                            dtype=[np.float64, np.float32])","328","        X = check_array(X, **check_params)","329","        y = check_array(y, **check_params)"]}],"sklearn\/tests\/test_isotonic.py":[{"add":["5","import pytest","6","","389","    assert np.all(y >= 0)","390","    assert np.all(y <= 0.1)","395","    assert np.all(y >= 0)","396","    assert np.all(y <= 0.1)","401","    assert np.all(y >= 0)","489","@pytest.mark.parametrize(","490","    \"y_dtype\", [np.int32, np.int64, np.float32, np.float64]","491",")","492","def test_isotonic_mismatched_dtype(y_dtype):","493","    # regression test for #15004","494","    # check that data are converted when X and y dtype differ","495","    reg = IsotonicRegression()","496","    y = np.array([2, 1, 4, 3, 5], dtype=y_dtype)","497","    X = np.arange(len(y), dtype=np.float32)","498","    reg.fit(X, y)","499","    assert reg.predict(X).dtype == X.dtype","500","","501",""],"delete":["387","    assert(np.all(y >= 0))","388","    assert(np.all(y <= 0.1))","393","    assert(np.all(y >= 0))","394","    assert(np.all(y <= 0.1))","399","    assert(np.all(y >= 0))"]}],"doc\/whats_new\/v0.22.rst":[{"add":["574",":mod:`sklearn.isotonic`","575","..................................","576","","577","- |Fix| Fixed a bug where :class:`isotonic.IsotonicRegression.fit` raised error","578","  when `X.dtype == 'float32'` and `X.dtype != y.dtype`.","579","  :pr:`14902` by :user:`Lucas <lostcoaster>`.","580","","581",""],"delete":[]}]}},"1a14920464acd9c16fcc0187f89cbbd738068ebc":{"changes":{"sklearn\/metrics\/base.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY","sklearn\/metrics\/tests\/test_ranking.py":"MODIFY"},"diff":{"sklearn\/metrics\/base.py":[{"add":["123","        if average_weight is not None:","124","            # Scores with 0 weights are forced to be 0, preventing the average","125","            # score from being affected by 0-weighted NaN elements.","126","            average_weight = np.asarray(average_weight)","127","            score[average_weight == 0] = 0"],"delete":[]}],"doc\/whats_new\/v0.22.rst":[{"add":["235","- |Enhancement| Allow computing averaged metrics in the case of no true positives.","236","  :pr:`14595` by `Andreas Mller`_.","237",""],"delete":[]}],"sklearn\/metrics\/tests\/test_ranking.py":[{"add":["336","    tpr, fpr, _ = assert_warns(UndefinedMetricWarning, roc_curve, y_true,","337","                               y_score)","345","    tpr, fpr, _ = assert_warns(UndefinedMetricWarning, roc_curve, y_true,","346","                               y_score)","664","","812","    with np.errstate(all=\"ignore\"):","813","        # if one class is never present weighted should not be NaN","814","        y_true = np.array([[0, 0], [0, 1]])","815","        y_score = np.array([[0, 0], [0, 1]])","816","        assert_almost_equal(average_precision_score(y_true, y_score,","817","                            average=\"weighted\"), 1)","818",""],"delete":["336","    tpr, fpr, _ = assert_warns(UndefinedMetricWarning, roc_curve, y_true, y_score)","344","    tpr, fpr, _ = assert_warns(UndefinedMetricWarning, roc_curve, y_true, y_score)"]}]}},"aada3ead7ece17f48da00c10d6eb13dc71d4cb84":{"changes":{"sklearn\/feature_extraction\/text.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY","sklearn\/feature_extraction\/tests\/test_text.py":"MODIFY"},"diff":{"sklearn\/feature_extraction\/text.py":[{"add":["131","    try:","132","        # If `s` is ASCII-compatible, then it does not contain any accented","133","        # characters and we can avoid an expensive list comprehension","134","        s.encode(\"ASCII\", errors=\"strict\")","136","    except UnicodeEncodeError:","137","        normalized = unicodedata.normalize('NFKD', s)"],"delete":["131","    normalized = unicodedata.normalize('NFKD', s)","132","    if normalized == s:","134","    else:"]}],"doc\/whats_new\/v0.22.rst":[{"add":["257","- |Fix| :func:`feature_extraction.text.strip_accents_unicode` now correctly","258","  removes accents from strings that are in NFKD normalized form. :pr:`15100` by","259","  :user:`Daniel Grady <DGrady>`.","260",""],"delete":[]}],"sklearn\/feature_extraction\/tests\/test_text.py":[{"add":["99","    # strings that are already decomposed","100","    a = \"o\\u0308\"  # o with diaresis","101","    expected = \"o\"","102","    assert strip_accents_unicode(a) == expected","103","","104","    # combining marks by themselves","105","    a = \"\\u0300\\u0301\\u0302\\u0303\"","106","    expected = \"\"","107","    assert strip_accents_unicode(a) == expected","108","","109","    # Multiple combining marks on one character","110","    a = \"o\\u0308\\u0304\"","111","    expected = \"o\"","112","    assert strip_accents_unicode(a) == expected","113",""],"delete":[]}]}},"801cca8e73215d4946f05379319d97156be659d6":{"changes":{"doc\/conf.py":"MODIFY"},"diff":{"doc\/conf.py":[{"add":[],"delete":["305","                        module=\"matplotlib\","]}]}},"283a61384e094165ed06186230f9f002b81296da":{"changes":{"doc\/developers\/advanced_installation.rst":"MODIFY","sklearn\/_build_utils\/openmp_helpers.py":"MODIFY","build_tools\/azure\/install.sh":"MODIFY"},"diff":{"doc\/developers\/advanced_installation.rst":[{"add":["138","    export LDFLAGS=\"$LDFLAGS -Wl,-rpath,\/usr\/local\/opt\/libomp\/lib -L\/usr\/local\/opt\/libomp\/lib -lomp\"","157","    export LDFLAGS=\"$LDFLAGS -Wl,-rpath,\/usr\/local\/lib -L\/usr\/local\/lib -lomp\""],"delete":["138","    export LDFLAGS=\"$LDFLAGS -L\/usr\/local\/opt\/libomp\/lib -lomp\"","139","    export DYLD_LIBRARY_PATH=\/usr\/local\/opt\/libomp\/lib","158","    export LDFLAGS=\"$LDFLAGS -L\/usr\/local\/lib -lomp\"","159","    export DYLD_LIBRARY_PATH=\/usr\/local\/lib"]}],"sklearn\/_build_utils\/openmp_helpers.py":[{"add":["52","        # export LDFLAGS=\"$LDFLAGS -Wl,-rpath,\/usr\/local\/opt\/libomp\/lib","53","        #                          -L\/usr\/local\/opt\/libomp\/lib -lomp\"","88","                extra_preargs = extra_preargs.strip().split(\" \")","89","                extra_preargs = [","90","                    flag for flag in extra_preargs","91","                    if flag.startswith(('-L', '-Wl,-rpath', '-l'))]"],"delete":["52","        # export LDFLAGS=\"$LDFLAGS -L\/usr\/local\/opt\/libomp\/lib -lomp\"","53","        # export DYLD_LIBRARY_PATH=\/usr\/local\/opt\/libomp\/lib","88","                extra_preargs = extra_preargs.split(\" \")","89","            else:","90","                extra_preargs = []"]}],"build_tools\/azure\/install.sh":[{"add":["16","    export LDFLAGS=\"$LDFLAGS -Wl,-rpath,\/usr\/local\/opt\/libomp\/lib -L\/usr\/local\/opt\/libomp\/lib -lomp\""],"delete":["16","    export LDFLAGS=\"$LDFLAGS -L\/usr\/local\/opt\/libomp\/lib -lomp\"","17","    export DYLD_LIBRARY_PATH=\/usr\/local\/opt\/libomp\/lib"]}]}},"42706ebf2d4cac551dc8348dd0d049461cf2c3f8":{"changes":{"azure-pipelines.yml":"MODIFY","doc\/developers\/advanced_installation.rst":"MODIFY","sklearn\/utils\/fixes.py":"MODIFY","doc\/tutorial\/machine_learning_map\/pyparsing.py":"MODIFY",".circleci\/config.yml":"MODIFY","README.rst":"MODIFY","build_tools\/azure\/posix-32.yml":"MODIFY","setup.py":"MODIFY","doc\/templates\/index.html":"MODIFY","sklearn\/utils\/_mask.py":"MODIFY","build_tools\/circle\/build_test_pypy.sh":"MODIFY","sklearn\/gaussian_process\/tests\/test_gpr.py":"MODIFY","sklearn\/feature_extraction\/tests\/test_text.py":"MODIFY","doc\/install.rst":"MODIFY","build_tools\/azure\/install.sh":"MODIFY","sklearn\/utils\/_testing.py":"MODIFY"},"diff":{"azure-pipelines.yml":[{"add":["5","    vmImage: ubuntu-18.04","29","    vmImage: ubuntu-18.04","46","    vmImage: ubuntu-18.04","50","      # versions of numpy, scipy with ATLAS that comes with Ubuntu Bionic 18.04","51","      # i.e. numpy 1.13.3 and scipy 0.19","52","      py36_ubuntu_atlas:","54","        PYTHON_VERSION: '3.6'","56","      # Linux + Python 3.6 build with OpenBLAS and without SITE_JOBLIB","57","      py36_conda_openblas:","59","        PYTHON_VERSION: '3.6'","61","        NUMPY_VERSION: '1.13.3'","62","        SCIPY_VERSION: '0.19.1'","67","        PILLOW_VERSION: '4.2.1'","68","        MATPLOTLIB_VERSION: '2.1.1'","69","        # latest version of joblib available in conda for Python 3.6","70","        JOBLIB_VERSION: '0.13.2'","86","    vmImage: ubuntu-18.04","89","      py36_ubuntu_atlas_32bit:","91","        PYTHON_VERSION: '3.6'","92","        JOBLIB_VERSION: '0.13'","137","      py36_pip_openblas_32bit:","138","        PYTHON_VERSION: '3.6'"],"delete":["5","    vmImage: ubuntu-16.04","29","    vmImage: ubuntu-16.04","46","    vmImage: ubuntu-16.04","50","      # versions of numpy, scipy with ATLAS that comes with Ubuntu Xenial 16.04","51","      # i.e. numpy 1.11 and scipy 0.17","52","      py35_ubuntu_atlas:","54","        PYTHON_VERSION: '3.5'","56","      # Linux + Python 3.5 build with OpenBLAS and without SITE_JOBLIB","57","      py35_conda_openblas:","59","        PYTHON_VERSION: '3.5'","61","        NUMPY_VERSION: '1.11.0'","62","        SCIPY_VERSION: '0.17.0'","67","        PILLOW_VERSION: '4.0.0'","68","        MATPLOTLIB_VERSION: '1.5.1'","69","        # later version of joblib are not packaged in conda for Python 3.5","70","        JOBLIB_VERSION: '0.12.3'","86","    vmImage: ubuntu-16.04","89","      py35_ubuntu_atlas_32bit:","91","        PYTHON_VERSION: '3.5'","92","        JOBLIB_VERSION: '0.11'","137","      py35_pip_openblas_32bit:","138","        PYTHON_VERSION: '3.5'"]}],"doc\/developers\/advanced_installation.rst":[{"add":["85","- Python (>= 3.6),","86","- NumPy (>= 1.13.3),","87","- SciPy (>= 0.19),"],"delete":["85","- Python (>= 3.5),","86","- NumPy (>= 1.11),","87","- SciPy (>= 0.17),"]}],"sklearn\/utils\/fixes.py":[{"add":["175","def _object_dtype_isnan(X):","176","    return X != X"],"delete":["175","# Fix for behavior inconsistency on numpy.equal for object dtypes.","176","# For numpy versions < 1.13, numpy.equal tests element-wise identity of objects","177","# instead of equality. This fix returns the mask of NaNs in an array of","178","# numerical or object values for all numpy versions.","179","if np_version < (1, 13):","180","    def _object_dtype_isnan(X):","181","        return np.frompyfunc(lambda x: x != x, 1, 1)(X).astype(bool)","182","else:","183","    def _object_dtype_isnan(X):","184","        return X != X"]}],"doc\/tutorial\/machine_learning_map\/pyparsing.py":[{"add":["1022","    def extract_stack(limit=0):\r","1023","        offset = -2\r","1024","        frame_summary = traceback.extract_stack(limit=-offset+limit-1)[offset]\r","1025","        return [(frame_summary.filename, frame_summary.lineno)]\r","1026","    def extract_tb(tb, limit=0):\r","1027","        frames = traceback.extract_tb(tb, limit=limit)\r","1028","        frame_summary = frames[-1]\r","1029","        return [(frame_summary.filename, frame_summary.lineno)]\r"],"delete":["1022","    # traceback return data structure changed in Py3.5 - normalize back to plain tuples\r","1023","    if system_version[:2] >= (3,5):\r","1024","        def extract_stack(limit=0):\r","1025","            # special handling for Python 3.5.0 - extra deep call stack by 1\r","1026","            offset = -3 if system_version == (3,5,0) else -2\r","1027","            frame_summary = traceback.extract_stack(limit=-offset+limit-1)[offset]\r","1028","            return [(frame_summary.filename, frame_summary.lineno)]\r","1029","        def extract_tb(tb, limit=0):\r","1030","            frames = traceback.extract_tb(tb, limit=limit)\r","1031","            frame_summary = frames[-1]\r","1032","            return [(frame_summary.filename, frame_summary.lineno)]\r","1033","    else:\r","1034","        extract_stack = traceback.extract_stack\r","1035","        extract_tb = traceback.extract_tb\r"]}],".circleci\/config.yml":[{"add":["11","      - PYTHON_VERSION: 3.6","12","      - NUMPY_VERSION: 1.13.3","13","      - SCIPY_VERSION: 0.19.1","14","      - MATPLOTLIB_VERSION: 2.1.1","21","      - SCIKIT_IMAGE_VERSION: 0.13"],"delete":["11","      - PYTHON_VERSION: 3.5","12","      - NUMPY_VERSION: 1.11.0","13","      - SCIPY_VERSION: 0.17.0","14","      - MATPLOTLIB_VERSION: 1.5.1","21","      - SCIKIT_IMAGE_VERSION: 0.12.3"]}],"README.rst":[{"add":["49","- Python (>= 3.6)","50","- NumPy (>= 1.13.3)","51","- SciPy (>= 0.19.1)","55","scikit-learn 0.23 and later require Python 3.6 or newer.","58","and classes end with \"Display\") require Matplotlib (>= 2.1.1). For running the","59","examples Matplotlib >= 2.1.1 is required. A few examples require","60","scikit-image >= 0.13, a few examples require pandas >= 0.18.0."],"delete":["49","- Python (>= 3.5)","50","- NumPy (>= 1.11.0)","51","- SciPy (>= 0.17.0)","55","scikit-learn 0.21 and later require Python 3.5 or newer.","58","and classes end with \"Display\") require Matplotlib (>= 1.5.1). For running the","59","examples Matplotlib >= 1.5.1 is required. A few examples require","60","scikit-image >= 0.12.3, a few examples require pandas >= 0.18.0."]}],"build_tools\/azure\/posix-32.yml":[{"add":["42","        i386\/ubuntu:18.04"],"delete":["42","        i386\/ubuntu:16.04"]}],"setup.py":[{"add":["17","    # Python 2 compat: just to be able to declare that Python >=3.6 is needed.","54","    SCIPY_MIN_VERSION = '0.19.1'","55","    NUMPY_MIN_VERSION = '1.13.3'","141","    # that python 3.6 is required.","255","                    python_requires=\">=3.6\",","283","        if sys.version_info < (3, 6):","285","                \"Scikit-learn requires Python 3.6 or later. The current\""],"delete":["17","    # Python 2 compat: just to be able to declare that Python >=3.5 is needed.","54","    SCIPY_MIN_VERSION = '0.17.0'","55","    NUMPY_MIN_VERSION = '1.11.0'","141","    # that python 3.5 is required.","246","                                 'Programming Language :: Python :: 3.5',","256","                    python_requires=\">=3.5\",","284","        if sys.version_info < (3, 5):","286","                \"Scikit-learn requires Python 3.5 or later. The current\""]}],"doc\/templates\/index.html":[{"add":["158","        <li><strong>Scikit-learn from 0.23 requires Python 3.6 or greater.<\/strong>","159","        <\/li>"],"delete":[]}],"sklearn\/utils\/_mask.py":[{"add":["7","    \"\"\"Compute the boolean mask X == value_to_mask.\"\"\"","18","        return X == value_to_mask"],"delete":["7","    \"\"\"Compute the boolean mask X == missing_values.\"\"\"","18","        # X == value_to_mask with object dtypes does not always perform","19","        # element-wise for old versions of numpy","20","        return np.equal(X, value_to_mask)"]}],"build_tools\/circle\/build_test_pypy.sh":[{"add":["5","apt-get -yq install libatlas-base-dev liblapack-dev gfortran ccache libopenblas-dev"],"delete":["5","apt-get -yq install libatlas-dev libatlas-base-dev liblapack-dev gfortran ccache libopenblas-dev"]}],"sklearn\/gaussian_process\/tests\/test_gpr.py":[{"add":["5","import sys","49","    if sys.maxsize <= 2 ** 32 and sys.version_info[:2] == (3, 6):","50","        pytest.xfail(\"This test may fail on 32bit Py3.6\")","51","","76","    if sys.maxsize <= 2 ** 32 and sys.version_info[:2] == (3, 6):","77","        pytest.xfail(\"This test may fail on 32bit Py3.6\")","78","","184","    if sys.maxsize <= 2 ** 32 and sys.version_info[:2] == (3, 6):","185","        pytest.xfail(\"This test may fail on 32bit Py3.6\")","186",""],"delete":[]}],"sklearn\/feature_extraction\/tests\/test_text.py":[{"add":[],"delete":["34","                                   clean_warning_registry,","390","    clean_warning_registry()"]}],"doc\/install.rst":[{"add":["134","and classes end with \"Display\") require Matplotlib (>= 2.1.1). For running the","135","examples Matplotlib >= 2.1.1 is required. A few examples require","136","scikit-image >= 0.13, a few examples require pandas >= 0.18.0.","141","    Scikit-learn 0.21 supported Python 3.5-3.7.","142","    Scikit-learn 0.22 supported Python 3.5-3.8.","143","    Scikit-learn now requires Python 3.6 or newer."],"delete":["134","and classes end with \"Display\") require Matplotlib (>= 1.5.1). For running the","135","examples Matplotlib >= 1.5.1 is required. A few examples require","136","scikit-image >= 0.12.3, a few examples require pandas >= 0.18.0.","141","    Scikit-learn now requires Python 3.5 or newer."]}],"build_tools\/azure\/install.sh":[{"add":["3","set -x","76","    sudo apt-get install python3-scipy python3-matplotlib libatlas3-base libatlas-base-dev python3-virtualenv","82","    apt-get install -y python3-dev python3-scipy python3-matplotlib libatlas3-base libatlas-base-dev python3-virtualenv"],"delete":["75","    sudo apt-get install python3-scipy python3-matplotlib libatlas3-base libatlas-base-dev libatlas-dev python3-virtualenv","81","    apt-get install -y python3-dev python3-scipy python3-matplotlib libatlas3-base libatlas-base-dev libatlas-dev python3-virtualenv"]}],"sklearn\/utils\/_testing.py":[{"add":[],"delete":["567","def clean_warning_registry():","568","    \"\"\"Clean Python warning registry for easier testing of warning messages.","569","","570","    When changing warning filters this function is not necessary with","571","    Python3.5+, as __warningregistry__ will be re-set internally.","572","    See https:\/\/bugs.python.org\/issue4180 and","573","    https:\/\/bugs.python.org\/issue21724 for more details.","574","","575","    \"\"\"","576","    for mod in sys.modules.values():","577","        registry = getattr(mod, \"__warningregistry__\", None)","578","        if registry is not None:","579","            registry.clear()","580","","581",""]}]}},"d92f12a137827b0e6357a133db3de6a3c3f34bf7":{"changes":{"sklearn\/linear_model\/tests\/test_coordinate_descent.py":"MODIFY","sklearn\/linear_model\/coordinate_descent.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY"},"diff":{"sklearn\/linear_model\/tests\/test_coordinate_descent.py":[{"add":["890","","891","","892","def test_multi_task_lasso_cv_dtype():","893","    n_samples, n_features = 10, 3","894","    rng = np.random.RandomState(42)","895","    X = rng.binomial(1, .5, size=(n_samples, n_features))","896","    X = X.astype(int)  # make it explicit that X is int","897","    y = X[:, [0, 0]].copy()","898","    est = MultiTaskLassoCV(n_alphas=5, fit_intercept=True).fit(X, y)","899","    assert_array_almost_equal(est.coef_, [[1, 0, 0]] * 2, decimal=3)"],"delete":[]}],"sklearn\/linear_model\/coordinate_descent.py":[{"add":["1114","            X = check_array(X, 'csc', dtype=[np.float64, np.float32],","1115","                            copy=False)"],"delete":["1114","            X = check_array(X, 'csc', copy=False)"]}],"doc\/whats_new\/v0.22.rst":[{"add":["341","- |FIX| :class:`linear_model.MultiTaskLassoCV` and","342","  :class:`linear_model.MultiTaskElasticNetCV` with X of dtype int","343","  and `fit_intercept=True`.","344","  :pr:`15086` by :user:`Alex Gramfort <agramfort>`.","345",""],"delete":[]}]}},"db23c4ece02b55c51987d78420c7616917f82031":{"changes":{"sklearn\/compose\/tests\/test_target.py":"MODIFY","sklearn\/compose\/tests\/test_column_transformer.py":"MODIFY"},"diff":{"sklearn\/compose\/tests\/test_target.py":[{"add":["31","    with pytest.raises(ValueError,","32","                       match=\"'transformer' and functions\"","33","                       \" 'func'\/'inverse_func' cannot both be set.\"):","34","        regr.fit(X, y)","39","    with pytest.raises(TypeError, match=r\"fit\\(\\) got an unexpected \"","40","                       \"keyword argument 'sample_weight'\"):","41","        regr.fit(X, y, sample_weight=sample_weight)","44","    with pytest.raises(ValueError, match=\"When 'func' is provided, \"","45","                       \"'inverse_func' must also be provided\"):","46","        regr.fit(X, y)","264","    with pytest.raises(AssertionError):","265","        tt.fit(X, y.tolist())","266","    with pytest.raises(AssertionError):","267","        tt.predict(X)"],"delete":["9","from sklearn.utils.testing import assert_raises","10","from sklearn.utils.testing import assert_raises_regex","33","    assert_raises_regex(ValueError, \"'transformer' and functions\"","34","                        \" 'func'\/'inverse_func' cannot both be set.\",","35","                        regr.fit, X, y)","40","    assert_raises_regex(TypeError, r\"fit\\(\\) got an unexpected keyword \"","41","                        \"argument 'sample_weight'\", regr.fit, X, y,","42","                        sample_weight=sample_weight)","45","    assert_raises_regex(ValueError, \"When 'func' is provided, 'inverse_func'\"","46","                        \" must also be provided\", regr.fit, X, y)","264","    assert_raises(AssertionError, tt.fit, X, y.tolist())","265","    assert_raises(AssertionError, tt.predict, X)"]}],"sklearn\/compose\/tests\/test_column_transformer.py":[{"add":["649","    with pytest.raises(NotFittedError):","650","        ct.get_feature_names()"],"delete":["10","from sklearn.utils.testing import assert_raises","650","    assert_raises(NotFittedError, ct.get_feature_names)"]}]}},"7eded0f6470a500172e44656f7ee6110965a3c56":{"changes":{"doc\/modules\/ensemble.rst":"MODIFY"},"diff":{"doc\/modules\/ensemble.rst":[{"add":["927","of :math:`\\mathcal{O}(n_\\text{features} \\times n \\log(n))` where :math:`n`","928","is the number of samples at the node.","935",":math:`\\mathcal{O}(n_\\text{features} \\times n)` complexity, much smaller","936","than the previous one. In addition, instead of considering :math:`n` split","937","points, we here consider only ``max_bins`` split points, which is much","938","smaller."],"delete":["927","of :math:`\\mathcal{O}(\\text{n_features} * n \\log(n))` where :math:`n` is the","928","number of samples at the node.","935",":math:`\\mathcal{O}(\\text{n_features} * n)` complexity, much smaller than the","936","previous one. In addition, instead of considering :math:`n` split points, we","937","here consider only ``max_bins`` split points, which is much smaller."]}]}},"c0c53137cec61a4d6cd72d8a43bbe0321476e440":{"changes":{"sklearn\/ensemble\/weight_boosting.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY","sklearn\/ensemble\/tests\/test_weight_boosting.py":"MODIFY"},"diff":{"sklearn\/ensemble\/weight_boosting.py":[{"add":["36","from ..utils.extmath import softmax","751","    @staticmethod","752","    def _compute_proba_from_decision(decision, n_classes):","753","        \"\"\"Compute probabilities from the decision function.","754","","755","        This is based eq. (4) of [1] where:","756","            p(y=c|X) = exp((1 \/ K-1) f_c(X)) \/ sum_k(exp((1 \/ K-1) f_k(X)))","757","                     = softmax((1 \/ K-1) * f(X))","758","","759","        References","760","        ----------","761","        .. [1] J. Zhu, H. Zou, S. Rosset, T. Hastie, \"Multi-class AdaBoost\",","762","               2009.","763","        \"\"\"","764","        if n_classes == 2:","765","            decision = np.vstack([-decision, decision]).T \/ 2","766","        else:","767","            decision \/= (n_classes - 1)","768","        return softmax(decision, copy=False)","769","","797","        decision = self.decision_function(X)","798","        return self._compute_proba_from_decision(decision, n_classes)","828","        for decision in self.staged_decision_function(X):","829","            yield self._compute_proba_from_decision(decision, n_classes)"],"delete":["777","        if self.algorithm == 'SAMME.R':","778","            # The weights are all 1. for SAMME.R","779","            proba = sum(_samme_proba(estimator, n_classes, X)","780","                        for estimator in self.estimators_)","781","        else:  # self.algorithm == \"SAMME\"","782","            proba = sum(estimator.predict_proba(X) * w","783","                        for estimator, w in zip(self.estimators_,","784","                                                self.estimator_weights_))","785","","786","        proba \/= self.estimator_weights_.sum()","787","        proba = np.exp((1. \/ (n_classes - 1)) * proba)","788","        normalizer = proba.sum(axis=1)[:, np.newaxis]","789","        normalizer[normalizer == 0.0] = 1.0","790","        proba \/= normalizer","791","","792","        return proba","821","        proba = None","822","        norm = 0.","824","        for weight, estimator in zip(self.estimator_weights_,","825","                                     self.estimators_):","826","            norm += weight","827","","828","            if self.algorithm == 'SAMME.R':","829","                # The weights are all 1. for SAMME.R","830","                current_proba = _samme_proba(estimator, n_classes, X)","831","            else:  # elif self.algorithm == \"SAMME\":","832","                current_proba = estimator.predict_proba(X) * weight","833","","834","            if proba is None:","835","                proba = current_proba","836","            else:","837","                proba += current_proba","838","","839","            real_proba = np.exp((1. \/ (n_classes - 1)) * (proba \/ norm))","840","            normalizer = real_proba.sum(axis=1)[:, np.newaxis]","841","            normalizer[normalizer == 0.0] = 1.0","842","            real_proba \/= normalizer","843","","844","            yield real_proba"]}],"doc\/whats_new\/v0.22.rst":[{"add":["103","- |Fix| :class:`ensemble.AdaBoostClassifier` computes probabilities based on","104","  the decision function as in the literature. Thus, `predict` and","105","  `predict_proba` give consistent results.","106","  :pr:`14114` by :user:`Guillaume Lemaitre <glemaitre>`.","107",""],"delete":[]}],"sklearn\/ensemble\/tests\/test_weight_boosting.py":[{"add":["3","import pytest","86","@pytest.mark.parametrize(\"algorithm\", [\"SAMME\", \"SAMME.R\"])","87","def test_classification_toy(algorithm):","89","    clf = AdaBoostClassifier(algorithm=algorithm, random_state=0)","90","    clf.fit(X, y_class)","91","    assert_array_equal(clf.predict(T), y_t_class)","92","    assert_array_equal(np.unique(np.asarray(y_t_class)), clf.classes_)","93","    assert clf.predict_proba(T).shape == (len(T), 2)","94","    assert clf.decision_function(T).shape == (len(T),)","153","@pytest.mark.parametrize(\"algorithm\", [\"SAMME\", \"SAMME.R\"])","154","def test_staged_predict(algorithm):","160","    clf = AdaBoostClassifier(algorithm=algorithm, n_estimators=10)","161","    clf.fit(iris.data, iris.target, sample_weight=iris_weights)","163","    predictions = clf.predict(iris.data)","164","    staged_predictions = [p for p in clf.staged_predict(iris.data)]","165","    proba = clf.predict_proba(iris.data)","166","    staged_probas = [p for p in clf.staged_predict_proba(iris.data)]","167","    score = clf.score(iris.data, iris.target, sample_weight=iris_weights)","168","    staged_scores = [","169","        s for s in clf.staged_score(","170","            iris.data, iris.target, sample_weight=iris_weights)]","172","    assert len(staged_predictions) == 10","173","    assert_array_almost_equal(predictions, staged_predictions[-1])","174","    assert len(staged_probas) == 10","175","    assert_array_almost_equal(proba, staged_probas[-1])","176","    assert len(staged_scores) == 10","177","    assert_array_almost_equal(score, staged_scores[-1])","505","","506","","507","@pytest.mark.parametrize(\"algorithm\", [\"SAMME\", \"SAMME.R\"])","508","def test_adaboost_consistent_predict(algorithm):","509","    # check that predict_proba and predict give consistent results","510","    # regression test for:","511","    # https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/14084","512","    X_train, X_test, y_train, y_test = train_test_split(","513","        *datasets.load_digits(return_X_y=True), random_state=42","514","    )","515","    model = AdaBoostClassifier(algorithm=algorithm, random_state=42)","516","    model.fit(X_train, y_train)","517","","518","    assert_array_equal(","519","        np.argmax(model.predict_proba(X_test), axis=1),","520","        model.predict(X_test)","521","    )"],"delete":["85","def test_classification_toy():","87","    for alg in ['SAMME', 'SAMME.R']:","88","        clf = AdaBoostClassifier(algorithm=alg, random_state=0)","89","        clf.fit(X, y_class)","90","        assert_array_equal(clf.predict(T), y_t_class)","91","        assert_array_equal(np.unique(np.asarray(y_t_class)), clf.classes_)","92","        assert clf.predict_proba(T).shape == (len(T), 2)","93","        assert clf.decision_function(T).shape == (len(T),)","152","def test_staged_predict():","158","    # AdaBoost classification","159","    for alg in ['SAMME', 'SAMME.R']:","160","        clf = AdaBoostClassifier(algorithm=alg, n_estimators=10)","161","        clf.fit(iris.data, iris.target, sample_weight=iris_weights)","163","        predictions = clf.predict(iris.data)","164","        staged_predictions = [p for p in clf.staged_predict(iris.data)]","165","        proba = clf.predict_proba(iris.data)","166","        staged_probas = [p for p in clf.staged_predict_proba(iris.data)]","167","        score = clf.score(iris.data, iris.target, sample_weight=iris_weights)","168","        staged_scores = [","169","            s for s in clf.staged_score(","170","                iris.data, iris.target, sample_weight=iris_weights)]","172","        assert len(staged_predictions) == 10","173","        assert_array_almost_equal(predictions, staged_predictions[-1])","174","        assert len(staged_probas) == 10","175","        assert_array_almost_equal(proba, staged_probas[-1])","176","        assert len(staged_scores) == 10","177","        assert_array_almost_equal(score, staged_scores[-1])"]}]}},"9115ab0ed3fcee50d750585b670003d3957a4897":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/compose\/tests\/test_column_transformer.py":"MODIFY","sklearn\/compose\/_column_transformer.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["76",":mod:`sklearn.compose`","77",".....................","78","","79","- |Fix| Fixed an issue in :class:`compose.ColumnTransformer` where using","80","  DataFrames whose column order differs between :func:``fit`` and","81","  :func:``transform`` could lead to silently passing incorrect columns to the","82","  ``remainder`` transformer.","83","  :pr:`14237` by `Andreas Schuderer <schuderer>`.","84",""],"delete":[]}],"sklearn\/compose\/tests\/test_column_transformer.py":[{"add":["494","    # transformed n_features does not match fitted n_features","495","    col = [0, 1]","496","    ct = ColumnTransformer([('trans', Trans(), col)], remainder=remainder)","497","    ct.fit(X_array)","498","    X_array_more = np.array([[0, 1, 2], [2, 4, 6], [3, 6, 9]]).T","499","    ct.transform(X_array_more)  # Should accept added columns","500","    X_array_fewer = np.array([[0, 1, 2], ]).T","501","    err_msg = 'Number of features'","502","    with pytest.raises(ValueError, match=err_msg):","503","        ct.transform(X_array_fewer)","504","","1073","","1074","","1075","@pytest.mark.parametrize(\"explicit_colname\", ['first', 'second'])","1076","def test_column_transformer_reordered_column_names_remainder(explicit_colname):","1077","    \"\"\"Regression test for issue #14223: 'Named col indexing fails with","1078","       ColumnTransformer remainder on changing DataFrame column ordering'","1079","","1080","       Should raise error on changed order combined with remainder.","1081","       Should allow for added columns in `transform` input DataFrame","1082","       as long as all preceding columns match.","1083","    \"\"\"","1084","    pd = pytest.importorskip('pandas')","1085","","1086","    X_fit_array = np.array([[0, 1, 2], [2, 4, 6]]).T","1087","    X_fit_df = pd.DataFrame(X_fit_array, columns=['first', 'second'])","1088","","1089","    X_trans_array = np.array([[2, 4, 6], [0, 1, 2]]).T","1090","    X_trans_df = pd.DataFrame(X_trans_array, columns=['second', 'first'])","1091","","1092","    tf = ColumnTransformer([('bycol', Trans(), explicit_colname)],","1093","                           remainder=Trans())","1094","","1095","    tf.fit(X_fit_df)","1096","    err_msg = 'Column ordering must be equal'","1097","    with pytest.raises(ValueError, match=err_msg):","1098","        tf.transform(X_trans_df)","1099","","1100","    # No error for added columns if ordering is identical","1101","    X_extended_df = X_fit_df.copy()","1102","    X_extended_df['third'] = [3, 6, 9]","1103","    tf.transform(X_extended_df)  # No error should be raised","1104","","1105","    # No 'columns' AttributeError when transform input is a numpy array","1106","    X_array = X_fit_array.copy()","1107","    err_msg = 'Specifying the columns'","1108","    with pytest.raises(ValueError, match=err_msg):","1109","        tf.transform(X_array)"],"delete":[]}],"sklearn\/compose\/_column_transformer.py":[{"add":["21","from ..utils import _check_key_type","83","        Note that using this feature requires that the DataFrame columns","84","        input at :term:`fit` and :term:`transform` have identical order.","308","        # Make it possible to check for reordered named columns on transform","309","        if (hasattr(X, 'columns') and","310","                any(_check_key_type(cols, str) for cols in self._columns)):","311","            self._df_columns = X.columns","312","","313","        self._n_features = X.shape[1]","317","        remaining_idx = list(set(range(self._n_features)) - set(cols))","318","        remaining_idx = sorted(remaining_idx) or None","520","","521","        if self._n_features > X.shape[1]:","522","            raise ValueError('Number of features of the input must be equal '","523","                             'to or greater than that of the fitted '","524","                             'transformer. Transformer n_features is {0} '","525","                             'and input n_features is {1}.'","526","                             .format(self._n_features, X.shape[1]))","527","","528","        # No column reordering allowed for named cols combined with remainder","529","        if (self._remainder[2] is not None and","530","                hasattr(self, '_df_columns') and","531","                hasattr(X, 'columns')):","532","            n_cols_fit = len(self._df_columns)","533","            n_cols_transform = len(X.columns)","534","            if (n_cols_transform >= n_cols_fit and","535","                    any(X.columns[:n_cols_fit] != self._df_columns)):","536","                raise ValueError('Column ordering must be equal for fit '","537","                                 'and for transform when using the '","538","                                 'remainder keyword')","539",""],"delete":["305","        n_columns = X.shape[1]","309","        remaining_idx = sorted(list(set(range(n_columns)) - set(cols))) or None","510",""]}]}},"c3fb4bc8fce495f91343964cffecd504a28a2187":{"changes":{"sklearn\/cluster\/tests\/test_mean_shift.py":"MODIFY"},"diff":{"sklearn\/cluster\/tests\/test_mean_shift.py":[{"add":["38","    assert bandwidth == pytest.approx(0., abs=1e-5)"],"delete":["38","    assert bandwidth == 0."]}]}},"730d1e726b9fb59811639896c5f8c26bb2ee0628":{"changes":{"doc\/whats_new\/v0.23.rst":"MODIFY","sklearn\/linear_model\/tests\/test_ridge.py":"MODIFY","sklearn\/linear_model\/_ridge.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.23.rst":[{"add":["102","- |Fix| Fixed a bug in :class:`linear_model.RidgeClassifierCV` to pass a","103","  specific scoring strategy. Before the internal estimator outputs score","104","  instead of predictions.","105","  :pr:`14848` by :user:`Venkatachalam N <venkyyuvy>`.","106",""],"delete":[]}],"sklearn\/linear_model\/tests\/test_ridge.py":[{"add":["62","def _accuracy_callable(y_test, y_pred):","63","    return np.mean(y_test == y_pred)","64","","65","","66","def _mean_squared_error_callable(y_test, y_pred):","67","    return ((y_test - y_pred) ** 2).mean()","68","","69","","736","@pytest.mark.parametrize(\"scoring\", [None, \"accuracy\", _accuracy_callable])","737","@pytest.mark.parametrize(\"cv\", [None, KFold(5)])","738","@pytest.mark.parametrize(\"filter_\", [DENSE_FILTER, SPARSE_FILTER])","739","def test_ridge_classifier_with_scoring(filter_, scoring, cv):","740","    # non-regression test for #14672","741","    # check that RidgeClassifierCV works with all sort of scoring and","742","    # cross-validation","743","    scoring_ = make_scorer(scoring) if callable(scoring) else scoring","744","    clf = RidgeClassifierCV(scoring=scoring_, cv=cv)","745","    # Smoke test to check that fit\/predict does not raise error","746","    clf.fit(filter_(X_iris), y_iris).predict(filter_(X_iris))","747","","748","","749","@pytest.mark.parametrize(\"cv\", [None, KFold(5)])","750","@pytest.mark.parametrize(\"filter_\", [DENSE_FILTER, SPARSE_FILTER])","751","def test_ridge_regression_custom_scoring(filter_, cv):","752","    # check that custom scoring is working as expected","753","    # check the tie breaking strategy (keep the first alpha tried)","754","","755","    def _dummy_score(y_test, y_pred):","756","        return 0.42","757","","758","    alphas = np.logspace(-2, 2, num=5)","759","    clf = RidgeClassifierCV(","760","        alphas=alphas, scoring=make_scorer(_dummy_score), cv=cv","761","    )","762","    clf.fit(filter_(X_iris), y_iris)","763","    assert clf.best_score_ == pytest.approx(0.42)","764","    # In case of tie score, the first alphas will be kept","765","    assert clf.alpha_ == pytest.approx(alphas[0])","766","","767","","887","@pytest.mark.parametrize(","888","    \"scoring\", [None, 'neg_mean_squared_error', _mean_squared_error_callable]","889",")","899","    scoring_ = make_scorer(scoring) if callable(scoring) else scoring","900","","901","    r = RidgeCV(alphas=alphas, cv=None, store_cv_values=True, scoring=scoring_)","919","@pytest.mark.parametrize(\"scoring\", [None, 'accuracy', _accuracy_callable])","920","def test_ridge_classifier_cv_store_cv_values(scoring):","929","    scoring_ = make_scorer(scoring) if callable(scoring) else scoring","930","","931","    r = RidgeClassifierCV(","932","        alphas=alphas, cv=None, store_cv_values=True, scoring=scoring_","933","    )"],"delete":["847","@pytest.mark.parametrize(\"scoring\", [None, 'neg_mean_squared_error'])","857","    r = RidgeCV(alphas=alphas, cv=None, store_cv_values=True, scoring=scoring)","875","def test_ridge_classifier_cv_store_cv_values():","884","    r = RidgeClassifierCV(alphas=alphas, cv=None, store_cv_values=True)"]}],"sklearn\/linear_model\/_ridge.py":[{"add":["21","from ..base import RegressorMixin, MultiOutputMixin, is_classifier","1059","class _IdentityRegressor:","1060","    \"\"\"Fake regressor which will directly output the prediction.\"\"\"","1069","class _IdentityClassifier(LinearClassifierMixin):","1070","    \"\"\"Fake classifier which will directly output the prediction.","1071","","1072","    We inherit from LinearClassifierMixin to get the proper shape for the","1073","    output `y`.","1074","    \"\"\"","1075","    def __init__(self, classes):","1076","        self.classes_ = classes","1077","","1078","    def decision_function(self, y_predict):","1079","        return y_predict","1080","","1081","","1083","    \"\"\"Ridge regression with built-in Generalized Cross-Validation.","1128","                 gcv_mode=None, store_cv_values=False,","1129","                 is_clf=False):","1137","        self.is_clf = is_clf","1522","                if self.is_clf:","1523","                    identity_estimator = _IdentityClassifier(","1524","                        classes=np.arange(n_y)","1525","                    )","1526","                    predictions_, y_ = predictions, y.argmax(axis=1)","1527","                else:","1528","                    identity_estimator = _IdentityRegressor()","1529","                    predictions_, y_ = predictions.ravel(), y.ravel()","1530","","1531","                alpha_score = scorer(identity_estimator, predictions_, y_)","1532","","1602","                                  store_cv_values=self.store_cv_values,","1603","                                  is_clf=is_classifier(self))","1615","            model = RidgeClassifier if is_classifier(self) else Ridge","1616","            gs = GridSearchCV(model(fit_intercept=self.fit_intercept,","1903","        target = Y if self.cv is None else y","1904","        _BaseRidgeCV.fit(self, X, target, sample_weight=sample_weight)"],"delete":["21","from ..base import RegressorMixin, MultiOutputMixin","1059","class _IdentityEstimator:","1060","    \"\"\"Hack to call a scorer when we already have the predictions.\"\"\"","1070","    \"\"\"Ridge regression with built-in Generalized Cross-Validation","1115","                 gcv_mode=None, store_cv_values=False):","1504","                alpha_score = scorer(","1505","                    _IdentityEstimator(), predictions.ravel(), y.ravel())","1578","                                  store_cv_values=self.store_cv_values)","1590","            gs = GridSearchCV(Ridge(fit_intercept=self.fit_intercept,","1718","    pass","1878","        _BaseRidgeCV.fit(self, X, Y, sample_weight=sample_weight)"]}]}},"01ba635cfcd523d01604ae9eb37a13844df6d92e":{"changes":{"build_tools\/azure\/test_pytest_soft_dependency.sh":"MODIFY",".circleci\/config.yml":"MODIFY","build_tools\/azure\/install.sh":"MODIFY"},"diff":{"build_tools\/azure\/test_pytest_soft_dependency.sh":[{"add":["9","    # conda may remove coverage when uninstall pytest and py","10","    pip install coverage"],"delete":[]}],".circleci\/config.yml":[{"add":["51","      # TODO: Revert to 3 when wheel issue with conda installation of python 3.7.4","52","      - PYTHON_VERSION: 3.7.3"],"delete":["51","      - PYTHON_VERSION: 3"]}],"build_tools\/azure\/install.sh":[{"add":["26","version_ge() {","27","    # The two version numbers are seperated with a new line is piped to sort","28","    # -rV. The -V activates for version number sorting and -r sorts in","29","    # decending order. If the first argument is the top element of the sort, it","30","    # is greater than or equal to the second argument.","31","    test \"$(printf \"${1}\\n${2}\" | sort -rV | head -n 1)\" == \"$1\"","32","}","33","","35","","36","    # TODO","37","    # Remove when wheel issue is fixed with conda installations of python 3.7.4","38","    if [[ \"$PYTHON_VERSION\" == \"*\" ]]; then","39","        PINNED_PYTHON_VERSION=\"3.7.3\"","40","    else","41","        PINNED_PYTHON_VERSION=$PYTHON_VERSION","42","    fi","43","","44","    TO_INSTALL=\"python=$PINNED_PYTHON_VERSION pip pytest=$PYTEST_VERSION \\","45","                pytest-cov numpy=$NUMPY_VERSION scipy=$SCIPY_VERSION \\","70","    # Old packages coming from the 'free' conda channel have been removed but","71","    # we are using them for testing Python 3.5. See","72","    # https:\/\/www.anaconda.com\/why-we-removed-the-free-channel-in-conda-4-7\/","73","    # for more details. restore_free_channel is defined starting from conda 4.7","74","    conda_version=$(conda -V | awk '{print $2}')","75","    if version_ge \"$conda_version\" \"4.7.0\" && [[ \"$PYTHON_VERSION\" == \"3.5\" ]]; then","76","        conda config --set restore_free_channel true","77","    fi","78",""],"delete":["27","    TO_INSTALL=\"python=$PYTHON_VERSION pip pytest=$PYTEST_VERSION pytest-cov \\","28","                numpy=$NUMPY_VERSION scipy=$SCIPY_VERSION \\"]}]}},"b8ab2149679aa4bde0affe93406f2e038ca8cb83":{"changes":{"sklearn\/utils\/fixes.py":"MODIFY","sklearn\/manifold\/spectral_embedding_.py":"MODIFY","sklearn\/externals\/_lobpcg.py":"ADD","doc\/whats_new\/v0.22.rst":"MODIFY"},"diff":{"sklearn\/utils\/fixes.py":[{"add":["40","if sp_version >= (1, 3):","41","    from scipy.sparse.linalg import lobpcg","42","else:","43","    # Backport of lobpcg functionality from scipy 1.3.0, can be removed","44","    # once support for sp_version < (1, 3) is dropped","45","    from ..externals._lobpcg import lobpcg  # noqa"],"delete":[]}],"sklearn\/manifold\/spectral_embedding_.py":[{"add":["12","from scipy.sparse.linalg import eigsh","19","from ..utils.fixes import lobpcg"],"delete":["12","from scipy.sparse.linalg import eigsh, lobpcg"]}],"sklearn\/externals\/_lobpcg.py":[{"add":[],"delete":[]}],"doc\/whats_new\/v0.22.rst":[{"add":["131","- |Fix| Port `lobpcg` from SciPy which implement some bug fixes but only","132","  available in 1.3+.","133","  :pr:`14195` by :user:`Guillaume Lemaitre <glemaitre>`.","134",""],"delete":[]}]}},"70351e6efca51de0ec2a2834831d67f024a9525b":{"changes":{"doc\/themes\/scikit-learn-modern\/static\/css\/theme.css":"MODIFY"},"diff":{"doc\/themes\/scikit-learn-modern\/static\/css\/theme.css":[{"add":["493","dl.citation > dd > ol > li {","494","  display: inline;","495","}","496","","497","dl.citation > dd > ol {","498","  margin-bottom: 0;","499","}"],"delete":[]}]}},"9f7d3f92721cca58613a7200f9a1a6465237a326":{"changes":{"sklearn\/impute\/tests\/test_impute.py":"MODIFY","sklearn\/impute\/_iterative.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY"},"diff":{"sklearn\/impute\/tests\/test_impute.py":[{"add":["459","@pytest.mark.parametrize(\"X\", [[[1], [2]], [[1], [np.nan]]])","460","def test_iterative_imputer_one_feature(X):","461","    # check we exit early when there is a single feature","462","    imputer = IterativeImputer().fit(X)","463","    assert imputer.n_iter_ == 0","464","    imputer = IterativeImputer()","465","    imputer.fit([[1], [2]])","466","    assert imputer.n_iter_ == 0","467","    imputer.fit([[1], [np.nan]])","468","    assert imputer.n_iter_ == 0","469","","470","","601","                               skip_complete=True,","967","    \"skip_complete\", [True, False]","968",")","969","def test_iterative_imputer_skip_non_missing(skip_complete):","970","    # check the imputing strategy when missing data are present in the","971","    # testing set only.","972","    # taken from: https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/14383","973","    rng = np.random.RandomState(0)","974","    X_train = np.array([","975","        [5, 2, 2, 1],","976","        [10, 1, 2, 7],","977","        [3, 1, 1, 1],","978","        [8, 4, 2, 2]","979","    ])","980","    X_test = np.array([","981","        [np.nan, 2, 4, 5],","982","        [np.nan, 4, 1, 2],","983","        [np.nan, 1, 10, 1]","984","    ])","985","    imputer = IterativeImputer(","986","        initial_strategy='mean', skip_complete=skip_complete, random_state=rng","987","    )","988","    X_test_est = imputer.fit(X_train).transform(X_test)","989","    if skip_complete:","990","        # impute with the initial strategy: 'mean'","991","        assert_allclose(X_test_est[:, 0], np.mean(X_train[:, 0]))","992","    else:","993","        assert_allclose(X_test_est[:, 0], [11, 7, 12], rtol=1e-4)","994","","995","","996","@pytest.mark.parametrize("],"delete":[]}],"sklearn\/impute\/_iterative.py":[{"add":["103","    skip_complete : boolean, optional (default=False)","104","        If ``True`` then features with missing values during ``transform``","105","        which did not have any missing values during ``fit`` will be imputed","106","        with the initial imputation method only. Set to ``True`` if you have","107","        many features with no missing values at both ``fit`` and ``transform``","108","        time to save compute.","109","","162","    random_state_ : RandomState instance","163","        RandomState instance that is generated either from a seed, the random","164","        number generator or by `np.random`.","165","","201","                 skip_complete=False,","216","        self.skip_complete = skip_complete","276","        missing_row_mask = mask_missing_values[:, feat_idx]","284","        # if no missing values, don't predict","285","        if np.sum(missing_row_mask) == 0:","286","            return X_filled, estimator","287","","288","        # get posterior samples if there is at least one missing value","294","            # two types of problems: (1) non-positive sigmas","295","            # (2) mus outside legal range of min_value and max_value","296","            # (results in inf sample)","394","        if self.skip_complete:","395","            missing_values_idx = np.flatnonzero(frac_of_missing_values)","396","        else:","397","            missing_values_idx = np.arange(np.shape(frac_of_missing_values)[0])","563","        # Edge case: a single feature. We return the initial ...","564","        if Xt.shape[1] == 1:","565","            self.n_iter_ = 0","566","            return Xt","567",""],"delete":["168","    Features with missing values during ``transform`` which did not have any","169","    missing values during ``fit`` will be imputed with the initial imputation","170","    method only.","171","","260","","261","        # if nothing is missing, just return the default","262","        # (should not happen at fit time because feat_ids would be excluded)","263","        missing_row_mask = mask_missing_values[:, feat_idx]","264","        if not np.any(missing_row_mask):","265","            return X_filled, estimator","266","","281","        # get posterior samples","287","            # two types of problems: (1) non-positive sigmas, (2) mus outside","288","            # legal range of min_value and max_value (results in inf sample)","386","        missing_values_idx = np.nonzero(frac_of_missing_values)[0]","548",""]}],"doc\/whats_new\/v0.22.rst":[{"add":["32","- :class:`impute.IterativeImputer` when `X` has features with no missing","33","  values. |Feature|","211","- |Fix| :class:`impute.IterativeImputer` now works when there is only one feature.","212","  By :user:`Sergey Feldman <sergeyf>`.","213","","214","- |Feature| :class:`impute.IterativeImputer` has new `skip_compute` flag that","215","  is False by default, which, when True, will skip computation on features that","216","  have no missing values during the fit phase. :issue:`13773` by","217","  :user:`Sergey Feldman <sergeyf>`.","218",""],"delete":[]}]}},"ca9ceba5589d881e26b0f9bea06418083ba433b0":{"changes":{"sklearn\/model_selection\/_split.py":"MODIFY","sklearn\/model_selection\/tests\/test_split.py":"MODIFY"},"diff":{"sklearn\/model_selection\/_split.py":[{"add":["1165","    def __repr__(self):","1166","        return _build_repr(self)","1167","","2163","                if value is None and hasattr(self, 'cvargs'):","2164","                    value = self.cvargs.get(key, None)"],"delete":[]}],"sklearn\/model_selection\/tests\/test_split.py":[{"add":["982","@pytest.mark.parametrize(","983","    \"RepeatedCV\", [RepeatedKFold, RepeatedStratifiedKFold]","984",")","985","def test_repeated_cv_repr(RepeatedCV):","986","    n_splits, n_repeats = 2, 6","987","    repeated_cv = RepeatedCV(n_splits=n_splits, n_repeats=n_repeats)","988","    repeated_cv_repr = ('{}(n_repeats=6, n_splits=2, random_state=None)'","989","                        .format(repeated_cv.__class__.__name__))","990","    assert repeated_cv_repr == repr(repeated_cv)","991","","992",""],"delete":[]}]}},"197f448eedb13fd267e3cc0c2a3b98c87706106d":{"changes":{"sklearn\/neighbors\/nca.py":"MODIFY","sklearn\/neighbors\/tests\/test_nca.py":"MODIFY"},"diff":{"sklearn\/neighbors\/nca.py":[{"add":["15","import numbers","302","            check_scalar(","303","                self.n_components, 'n_components', numbers.Integral, 1)","322","        check_scalar(self.max_iter, 'max_iter', numbers.Integral, 1)","323","        check_scalar(self.tol, 'tol', numbers.Real, 0.)","324","        check_scalar(self.verbose, 'verbose', numbers.Integral, 0)"],"delete":["301","            check_scalar(self.n_components, 'n_components', int, 1)","320","        check_scalar(self.max_iter, 'max_iter', int, 1)","321","        check_scalar(self.tol, 'tol', float, 0.)","322","        check_scalar(self.verbose, 'verbose', int, 0)"]}],"sklearn\/neighbors\/tests\/test_nca.py":[{"add":["131","    assert_raises(TypeError, NCA(tol='1').fit, X, y)","520","","521","","522","@pytest.mark.parametrize('param, value', [('n_components', np.int32(3)),","523","                                          ('max_iter', np.int32(100)),","524","                                          ('tol', np.float32(0.0001))])","525","def test_parameters_valid_types(param, value):","526","    # check that no error is raised when parameters have numpy integer or","527","    # floating types.","528","    nca = NeighborhoodComponentsAnalysis(**{param: value})","529","","530","    X = iris_data","531","    y = iris_target","532","","533","    nca.fit(X, y)"],"delete":["131","    assert_raises(TypeError, NCA(tol=1).fit, X, y)"]}]}},"de3040439546362a28c9861ac4b568bc5ffd68e4":{"changes":{"sklearn\/impute\/tests\/test_impute.py":"MODIFY","sklearn\/impute\/_iterative.py":"MODIFY"},"diff":{"sklearn\/impute\/tests\/test_impute.py":[{"add":["18","from sklearn.datasets import load_boston","927","def test_iterative_imputer_catch_warning():","928","    # check that we catch a RuntimeWarning due to a division by zero when a","929","    # feature is constant in the dataset","930","    X, y = load_boston(return_X_y=True)","931","    n_samples, n_features = X.shape","932","","933","    # simulate that a feature only contain one category during fit","934","    X[:, 3] = 1","935","","936","    # add some missing values","937","    rng = np.random.RandomState(0)","938","    missing_rate = 0.15","939","    for feat in range(n_features):","940","        sample_idx = rng.choice(","941","            np.arange(n_samples), size=int(n_samples * missing_rate),","942","            replace=False","943","        )","944","        X[sample_idx, feat] = np.nan","945","","946","    imputer = IterativeImputer(n_nearest_features=5, sample_posterior=True)","947","    with pytest.warns(None) as record:","948","        X_fill = imputer.fit_transform(X, y)","949","    assert not record.list","950","    assert not np.any(np.isnan(X_fill))","951","","952",""],"delete":[]}],"sklearn\/impute\/_iterative.py":[{"add":["432","        with np.errstate(invalid='ignore'):","433","            # if a feature in the neighboorhood has only a single value","434","            # (e.g., categorical feature), the std. dev. will be null and","435","            # np.corrcoef will raise a warning due to a division by zero","436","            abs_corr_mat = np.abs(np.corrcoef(X_filled.T))"],"delete":["432","        abs_corr_mat = np.abs(np.corrcoef(X_filled.T))"]}]}},"8e8e60eaa33ceeb6e2e846c747813bcf57899148":{"changes":{"sklearn\/svm\/tests\/test_sparse.py":"MODIFY","sklearn\/svm\/tests\/test_bounds.py":"MODIFY","sklearn\/svm\/tests\/test_svm.py":"MODIFY"},"diff":{"sklearn\/svm\/tests\/test_sparse.py":[{"add":["11","from sklearn.utils.testing import (assert_warns,","191","    with pytest.raises(ValueError):","192","        svm.SVC(C=-1).fit(X, Y)","196","    with pytest.raises(ValueError):","197","        clf.fit(X_sp, Y)","200","    with pytest.raises(ValueError):","201","        clf.fit(X_sp, Y2)"],"delete":["11","from sklearn.utils.testing import (assert_raises, assert_warns,","191","    assert_raises(ValueError, svm.SVC(C=-1).fit, X, Y)","195","    assert_raises(ValueError, clf.fit, X_sp, Y)","198","    assert_raises(ValueError, clf.fit, X_sp, Y2)"]}],"sklearn\/svm\/tests\/test_bounds.py":[{"add":["68","    with pytest.raises(ValueError):","69","        l1_min_c(X, y)","73","    with pytest.raises(ValueError):","74","        l1_min_c(dense_X, Y1, 'l1')"],"delete":["9","from sklearn.utils.testing import assert_raises","69","    assert_raises(ValueError, l1_min_c, X, y)","73","    assert_raises(ValueError, l1_min_c, dense_X, Y1, 'l1')"]}],"sklearn\/svm\/tests\/test_svm.py":[{"add":["19","from sklearn.utils.testing import assert_warns","21","from sklearn.utils.testing import ignore_warnings","99","    with pytest.raises(ValueError):","100","        clf.predict(KT.T)","234","    with pytest.raises(ValueError):","235","        clf.predict(X)","250","    with pytest.raises(AttributeError):","251","        (lambda: clf.coef_)()","476","    with pytest.raises(ValueError):","477","        svm.SVC(C=-1).fit(X, Y)","481","    with pytest.raises(ValueError):","482","        clf.fit(X, Y)","485","    with pytest.raises(ValueError):","486","        clf.fit(X, Y2)","501","    with pytest.raises(ValueError):","502","        clf.fit(X, Y)","506","    with pytest.raises(ValueError):","507","        clf.fit(X, Y, sample_weight=range(len(X) - 1))","511","    with pytest.raises(ValueError):","512","        clf.predict(sparse.lil_matrix(X))","516","    with pytest.raises(ValueError):","517","        clf.predict(X)","521","    with pytest.raises(ValueError):","522","        clf.predict(Xt)","577","            with pytest.raises(ValueError, match=\"Unsupported set of \"","578","                               \"arguments.*penalty='%s.*loss='%s.*dual=%s\"","579","                               % (penalty, loss, dual)):","580","                clf.fit(X, y)","585","    with pytest.raises(ValueError, match=\".*loss='l3' is not supported.*\"):","586","        svm.LinearSVC(loss=\"l3\").fit(X, y)","808","        with pytest.raises(AttributeError):","809","            clf.__setattr__('coef_', np.arange(3))","810","        with pytest.raises((RuntimeError, ValueError)):","811","            clf.coef_.__setitem__((0, 0), 0)","858","    with pytest.raises(ValueError):","859","        svc.fit(X, Y)","872","    with pytest.raises(Exception, match=r\".*\\bSVC\\b.*\\bnot\\b.*\\bfitted\\b\"):","873","        clf.predict(X)","876","    with pytest.raises(Exception, match=r\".*\\bNuSVR\\b.*\\bnot\\b.*\\bfitted\\b\"):","877","        clf.predict(X)"],"delete":["19","from sklearn.utils.testing import assert_raises_regexp, assert_warns","21","from sklearn.utils.testing import ignore_warnings, assert_raises","99","    assert_raises(ValueError, clf.predict, KT.T)","233","    assert_raises(ValueError, clf.predict, X)","248","    assert_raises(AttributeError, lambda: clf.coef_)","473","    assert_raises(ValueError, svm.SVC(C=-1).fit, X, Y)","477","    assert_raises(ValueError, clf.fit, X, Y)","480","    assert_raises(ValueError, clf.fit, X, Y2)","495","    assert_raises(ValueError, clf.fit, X, Y)","499","    assert_raises(ValueError, clf.fit, X, Y, sample_weight=range(len(X) - 1))","503","    assert_raises(ValueError, clf.predict, sparse.lil_matrix(X))","507","    assert_raises(ValueError, clf.predict, X)","511","    assert_raises(ValueError, clf.predict, Xt)","566","            assert_raises_regexp(ValueError,","567","                                 \"Unsupported set of arguments.*penalty='%s.*\"","568","                                 \"loss='%s.*dual=%s\"","569","                                 % (penalty, loss, dual),","570","                                 clf.fit, X, y)","575","    assert_raises_regexp(ValueError, \".*loss='l3' is not supported.*\",","576","                         svm.LinearSVC(loss=\"l3\").fit, X, y)","798","        assert_raises(AttributeError, clf.__setattr__, 'coef_', np.arange(3))","799","        assert_raises((RuntimeError, ValueError),","800","                      clf.coef_.__setitem__, (0, 0), 0)","847","    assert_raises(ValueError, svc.fit, X, Y)","860","    assert_raises_regexp(Exception, r\".*\\bSVC\\b.*\\bnot\\b.*\\bfitted\\b\",","861","                         clf.predict, X)","864","    assert_raises_regexp(Exception, r\".*\\bNuSVR\\b.*\\bnot\\b.*\\bfitted\\b\",","865","                         clf.predict, X)"]}]}},"f82d966d7b6286aa34a11374304c0bca515f4331":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/inspection\/partial_dependence.py":"MODIFY","sklearn\/inspection\/tests\/test_partial_dependence.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["28","","29",":mod:`sklearn.inspection`","30",".....................","31","","32","- |Fix| Fixed a bug in :func:`inspection.plot_partial_dependence` where ","33","  ``target`` parameter was not being taken into account for multiclass problems.","34","  :pr:`14393` by :user:`Guillem G. Subies <guillemgsubies>`.","35",""],"delete":[]}],"sklearn\/inspection\/partial_dependence.py":[{"add":[],"delete":["588","    else:","589","        target_idx = 0"]}],"sklearn\/inspection\/tests\/test_partial_dependence.py":[{"add":["482","    clf = GradientBoostingClassifier(n_estimators=10, random_state=1)","483","    iris = load_iris()","484","","485","    # Test partial dependence plot function on multi-class input.","486","    clf.fit(iris.data, iris.target)","501","    fig2 = pyplot.gcf()","502","    axs2 = fig2.get_axes()","503","    assert len(axs2) == 2","504","    assert all(ax.has_data for ax in axs2)","505","","506","    # check that the pd plots are the same for 0 and \"setosa\"","507","    assert all(axs[0].lines[0]._y == axs2[0].lines[0]._y)","508","    # check that the pd plots are different for another target","509","    clf.fit(iris.data, iris.target)","510","    plot_partial_dependence(clf, iris.data, [0, 1],","511","                            target=1,","512","                            grid_resolution=grid_resolution)","513","    fig3 = pyplot.gcf()","514","    axs3 = fig3.get_axes()","515","    assert any(axs[0].lines[0]._y != axs3[0].lines[0]._y)"],"delete":["481","    # Test partial dependence plot function on multi-class input.","482","    iris = load_iris()","483","    clf = GradientBoostingClassifier(n_estimators=10, random_state=1)","484","    clf.fit(iris.data, iris.target)","485","","497","    clf = GradientBoostingClassifier(n_estimators=10, random_state=1)","499","","500","    grid_resolution = 25","504","    fig = pyplot.gcf()","505","    axs = fig.get_axes()","506","    assert len(axs) == 2","507","    assert all(ax.has_data for ax in axs)"]}]}},"3ab22f973fb1ff1643fb980075559316478f7bd4":{"changes":{"examples\/decomposition\/plot_ica_blind_source_separation.py":"MODIFY"},"diff":{"examples\/decomposition\/plot_ica_blind_source_separation.py":[{"add":["71","plt.tight_layout()"],"delete":["71","plt.subplots_adjust(0.09, 0.04, 0.94, 0.94, 0.26, 0.46)"]}]}},"125a54d9440567afe77ed45f57be0fa7edaa74c3":{"changes":{"azure-pipelines.yml":"MODIFY","sklearn\/__init__.py":"MODIFY"},"diff":{"azure-pipelines.yml":[{"add":["40","      pylatest_conda_mkl:","41","        DISTRIB: 'conda'","42","        PYTHON_VERSION: '*'","43","        INSTALL_MKL: 'true'","44","        NUMPY_VERSION: '*'","45","        SCIPY_VERSION: '*'","46","        CYTHON_VERSION: '*'","47","        PILLOW_VERSION: '*'","48","        PYTEST_VERSION: '*'","49","        JOBLIB_VERSION: '*'","50","        COVERAGE: 'true'"],"delete":[]}],"sklearn\/__init__.py":[{"add":["60","# Workaround issue discovered in intel-openmp 2019.5:","61","# https:\/\/github.com\/ContinuumIO\/anaconda-issues\/issues\/11294","62","os.environ.setdefault(\"KMP_INIT_AT_FORK\", \"FALSE\")"],"delete":[]}]}},"cc64397d0c1bbf58d44a862d5fafbf6b6cfb619d":{"changes":{"sklearn\/utils\/multiclass.py":"MODIFY","sklearn\/utils\/tests\/test_multiclass.py":"MODIFY"},"diff":{"sklearn\/utils\/multiclass.py":[{"add":["242","    sparse_pandas = (y.__class__.__name__ in ['SparseSeries', 'SparseArray'])","243","    if sparse_pandas:","244","        raise ValueError(\"y cannot be class 'SparseSeries' or 'SparseArray'\")"],"delete":["242","    sparseseries = (y.__class__.__name__ == 'SparseSeries')","243","    if sparseseries:","244","        raise ValueError(\"y cannot be class 'SparseSeries'.\")"]}],"sklearn\/utils\/tests\/test_multiclass.py":[{"add":["4","import pytest","296","def test_type_of_target_pandas_sparse():","297","    pd = pytest.importorskip(\"pandas\")","298","","299","    y = pd.SparseArray([1, np.nan, np.nan, 1, np.nan])","300","    msg = \"y cannot be class 'SparseSeries' or 'SparseArray'\"","301","    with pytest.raises(ValueError, match=msg):","302","        type_of_target(y)"],"delete":["4","","295","    try:","296","        from pandas import SparseSeries","297","    except ImportError:","298","        raise SkipTest(\"Pandas not found\")","300","    y = SparseSeries([1, 0, 0, 1, 0])","301","    msg = \"y cannot be class 'SparseSeries'.\"","302","    assert_raises_regex(ValueError, msg, type_of_target, y)"]}]}}}