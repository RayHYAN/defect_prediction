{"415fd83dbf089d48a2c5bb15002933f432810421":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/tests\/test_discriminant_analysis.py":"MODIFY","sklearn\/discriminant_analysis.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["95","- |Enhancement| :class:`discriminant_analysis.LinearDiscriminantAnalysis` now","96","  preserves ``float32`` and ``float64`` dtypes. :issues:`8769` and","97","  :issues:`11000` by :user:`Thibault Sejourne <thibsej>`","98",""],"delete":[]}],"sklearn\/tests\/test_discriminant_analysis.py":[{"add":["9","from sklearn.utils.testing import assert_allclose","299","@pytest.mark.parametrize(\"data_type, expected_type\", [","300","    (np.float32, np.float32),","301","    (np.float64, np.float64),","302","    (np.int32, np.float64),","303","    (np.int64, np.float64)","304","])","305","def test_lda_dtype_match(data_type, expected_type):","306","    for (solver, shrinkage) in solver_shrinkage:","307","        clf = LinearDiscriminantAnalysis(solver=solver, shrinkage=shrinkage)","308","        clf.fit(X.astype(data_type), y.astype(data_type))","309","        assert clf.coef_.dtype == expected_type","310","","311","","312","def test_lda_numeric_consistency_float32_float64():","313","    for (solver, shrinkage) in solver_shrinkage:","314","        clf_32 = LinearDiscriminantAnalysis(solver=solver, shrinkage=shrinkage)","315","        clf_32.fit(X.astype(np.float32), y.astype(np.float32))","316","        clf_64 = LinearDiscriminantAnalysis(solver=solver, shrinkage=shrinkage)","317","        clf_64.fit(X.astype(np.float64), y.astype(np.float64))","318","","319","        # Check value consistency between types","320","        rtol = 1e-6","321","        assert_allclose(clf_32.coef_, clf_64.coef_, rtol=rtol)","322","","323",""],"delete":[]}],"sklearn\/discriminant_analysis.py":[{"add":["429","        X, y = check_X_y(X, y, ensure_min_samples=2, estimator=self,","430","                         dtype=[np.float64, np.float32])","488","            self.coef_ = np.array(self.coef_[1, :] - self.coef_[0, :], ndmin=2,","489","                                  dtype=X.dtype)","491","                                       ndmin=1, dtype=X.dtype)"],"delete":["429","        X, y = check_X_y(X, y, ensure_min_samples=2, estimator=self)","487","            self.coef_ = np.array(self.coef_[1, :] - self.coef_[0, :], ndmin=2)","489","                                       ndmin=1)"]}]}},"d1c52f402c89586ff66c2ec6e86f794da7715a18":{"changes":{"examples\/inspection\/plot_permutation_importance.py":"ADD","sklearn\/inspection\/tests\/test_permutation_importance.py":"ADD","examples\/inspection\/plot_permutation_importance_multicollinear.py":"ADD","doc\/modules\/classes.rst":"MODIFY","doc\/modules\/permutation_importance.rst":"ADD","sklearn\/inspection\/permutation_importance.py":"ADD","sklearn\/inspection\/__init__.py":"MODIFY","doc\/inspection.rst":"MODIFY"},"diff":{"examples\/inspection\/plot_permutation_importance.py":[{"add":[],"delete":[]}],"sklearn\/inspection\/tests\/test_permutation_importance.py":[{"add":[],"delete":[]}],"examples\/inspection\/plot_permutation_importance_multicollinear.py":[{"add":[],"delete":[]}],"doc\/modules\/classes.rst":[{"add":["659","   inspection.permutation_importance"],"delete":["1259",""]}],"doc\/modules\/permutation_importance.rst":[{"add":[],"delete":[]}],"sklearn\/inspection\/permutation_importance.py":[{"add":[],"delete":[]}],"sklearn\/inspection\/__init__.py":[{"add":["3","from .permutation_importance import permutation_importance","8","    'permutation_importance'"],"delete":["3",""]}],"doc\/inspection.rst":[{"add":["7","Predictive performance is often the main goal of developing machine learning","8","models. Yet summarising performance with an evaluation metric is often","9","insufficient: it assumes that the evaluation metric and test dataset","10","perfectly reflect the target domain, which is rarely true. In certain domains,","11","a model needs a certain level of interpretability before it can be deployed.","12","A model that is exhibiting performance issues needs to be debugged for one to ","13","understand the model's underlying issue. The ","14",":mod:`sklearn.inspection` module provides tools to help understand the ","15","predictions from a model and what affects them. This can be used to ","16","evaluate assumptions and biases of a model, design a better model, or","17","to diagnose issues with model performance.","18","","22","    modules\/permutation_importance"],"delete":[]}]}},"2f35e9e79fdf6f8179c326722aa72f81871781fd":{"changes":{"sklearn\/datasets\/tests\/test_svmlight_format.py":"MODIFY"},"diff":{"sklearn\/datasets\/tests\/test_svmlight_format.py":[{"add":["228","                for dtype in [np.float32, np.float64, np.int32, np.int64]:","239","                    # Note: with dtype=np.int32 we are performing unsafe casts,","240","                    # where X.astype(dtype) overflows. The result is","241","                    # then platform dependent and X_dense.astype(dtype) may be","242","                    # different from X_sparse.astype(dtype).asarray().","243","                    X_input = X.astype(dtype)","244","","245","                    dump_svmlight_file(X_input, y, f, comment=\"test\",","265","                    if sp.issparse(X_input):","266","                        X_input_dense = X_input.toarray()","267","                    else:","268","                        X_input_dense = X_input","273","                            X_input_dense, X2_dense, 4)","279","                            X_input_dense, X2_dense, 15)"],"delete":["228","                for dtype in [np.float32, np.float64, np.int32]:","239","                    dump_svmlight_file(X.astype(dtype), y, f, comment=\"test\",","263","                            X_dense.astype(dtype), X2_dense, 4)","269","                            X_dense.astype(dtype), X2_dense, 15)"]}]}},"be03467b9cdcdcd6940350195d23310f992cb6d7":{"changes":{"sklearn\/feature_selection\/variance_threshold.py":"MODIFY","sklearn\/feature_selection\/tests\/test_variance_threshold.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY"},"diff":{"sklearn\/feature_selection\/variance_threshold.py":[{"add":["7","from ..utils.sparsefuncs import mean_variance_axis, min_max_axis","67","            if self.threshold == 0:","68","                mins, maxes = min_max_axis(X, axis=0)","69","                peak_to_peaks = maxes - mins","72","            if self.threshold == 0:","73","                peak_to_peaks = np.ptp(X, axis=0)","74","","75","        if self.threshold == 0:","76","            # Use peak-to-peak to avoid numeric precision issues","77","            # for constant features","78","            self.variances_ = np.minimum(self.variances_, peak_to_peaks)"],"delete":["7","from ..utils.sparsefuncs import mean_variance_axis"]}],"sklearn\/feature_selection\/tests\/test_variance_threshold.py":[{"add":["0","import numpy as np","1","import pytest","2","","31","","32","","33","def test_zero_variance_floating_point_error():","34","    # Test that VarianceThreshold(0.0).fit eliminates features that have","35","    # the same value in every sample, even when floating point errors","36","    # cause np.var not to be 0 for the feature.","37","    # See #13691","38","","39","    data = [[-0.13725701]] * 10","40","    assert np.var(data) != 0","41","    for X in [data, csr_matrix(data), csc_matrix(data), bsr_matrix(data)]:","42","        msg = \"No feature in X meets the variance threshold 0.00000\"","43","        with pytest.raises(ValueError, match=msg):","44","            VarianceThreshold().fit(X)"],"delete":[]}],"doc\/whats_new\/v0.22.rst":[{"add":["74","","75",":mod:`sklearn.feature_selection`","76","................................","77","- |Fix| Fixed a bug where :class:`VarianceThreshold` with `threshold=0` did not","78","  remove constant features due to numerical instability, by using range","79","  rather than variance in this case.","80","  :pr:`13704` by `Roddy MacSween <rlms>`."],"delete":["67","","76","\t"]}]}},"79b549cfd9f1c62769da6c7d75f561cc6093323c":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/linear_model\/tests\/test_least_angle.py":"MODIFY","sklearn\/linear_model\/least_angle.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["167","- |Fix| Fixed a bug in :class:`linear_model.LassoLarsIC`, where user input","168","   ``copy_X=False`` at instance creation would be overridden by default","169","   parameter value ``copy_X=True`` in ``fit``. ","170","   :issue:`12972` by :user:`Lucio Fernandez-Arjona <luk-f-a>`","171",""],"delete":[]}],"sklearn\/linear_model\/tests\/test_least_angle.py":[{"add":["20","from sklearn.linear_model.least_angle import _lars_path_residues, LassoLarsIC","688","","689","","690","@pytest.mark.parametrize('copy_X', [True, False])","691","def test_lasso_lars_copyX_behaviour(copy_X):","692","    \"\"\"","693","    Test that user input regarding copy_X is not being overridden (it was until","694","    at least version 0.21)","695","","696","    \"\"\"","697","    lasso_lars = LassoLarsIC(copy_X=copy_X, precompute=False)","698","    rng = np.random.RandomState(0)","699","    X = rng.normal(0, 1, (100, 5))","700","    X_copy = X.copy()","701","    y = X[:, 2]","702","    lasso_lars.fit(X, y)","703","    assert copy_X == np.array_equal(X, X_copy)","704","","705","","706","@pytest.mark.parametrize('copy_X', [True, False])","707","def test_lasso_lars_fit_copyX_behaviour(copy_X):","708","    \"\"\"","709","    Test that user input to .fit for copy_X overrides default __init__ value","710","","711","    \"\"\"","712","    lasso_lars = LassoLarsIC(precompute=False)","713","    rng = np.random.RandomState(0)","714","    X = rng.normal(0, 1, (100, 5))","715","    X_copy = X.copy()","716","    y = X[:, 2]","717","    lasso_lars.fit(X, y, copy_X=copy_X)","718","    assert copy_X == np.array_equal(X, X_copy)"],"delete":["20","from sklearn.linear_model.least_angle import _lars_path_residues"]}],"sklearn\/linear_model\/least_angle.py":[{"add":["1479","    def fit(self, X, y, copy_X=None):","1490","        copy_X : boolean, optional, default None","1491","            If provided, this parameter will override the choice","1492","            of copy_X made at instance creation.","1500","        if copy_X is None:","1501","            copy_X = self.copy_X","1505","            X, y, self.fit_intercept, self.normalize, copy_X)"],"delete":["1479","    def fit(self, X, y, copy_X=True):","1490","        copy_X : boolean, optional, default True","1501","            X, y, self.fit_intercept, self.normalize, self.copy_X)"]}]}},"896a76eb22fe498494ab1b73594a13a7f266b912":{"changes":{"sklearn\/cluster\/hierarchical.py":"MODIFY","sklearn\/cluster\/affinity_propagation_.py":"MODIFY","sklearn\/cluster\/dbscan_.py":"MODIFY","sklearn\/cluster\/spectral.py":"MODIFY"},"diff":{"sklearn\/cluster\/hierarchical.py":[{"add":["775","        \"\"\"Fit the hierarchical clustering from features, or distance matrix.","779","        X : array-like, shape (n_samples, n_features) or (n_samples, n_samples)","780","            Training instances to cluster, or distances between instances if","781","            ``affinity='precomputed'``.","784","            Not used, present here for API consistency by convention.","878","    def fit_predict(self, X, y=None):","879","        \"\"\"Fit the hierarchical clustering from features or distance matrix,","880","        and return cluster labels.","881","","882","        Parameters","883","        ----------","884","        X : array-like, shape (n_samples, n_features) or (n_samples, n_samples)","885","            Training instances to cluster, or distances between instances if","886","            ``affinity='precomputed'``.","887","","888","        y : Ignored","889","            Not used, present here for API consistency by convention.","890","","891","        Returns","892","        -------","893","        labels : ndarray, shape (n_samples,)","894","            Cluster labels.","895","        \"\"\"","896","        return super().fit_predict(X, y)","897",""],"delete":["775","        \"\"\"Fit the hierarchical clustering on the data","779","        X : array-like, shape = [n_samples, n_features]","780","            Training data. Shape [n_samples, n_features], or [n_samples,","781","            n_samples] if affinity=='precomputed'."]}],"sklearn\/cluster\/affinity_propagation_.py":[{"add":["353","        \"\"\"Fit the clustering from features, or affinity matrix.","357","        X : array-like or sparse matrix, shape (n_samples, n_features), or \\","358","            array-like, shape (n_samples, n_samples)","359","            Training instances to cluster, or similarities \/ affinities between","360","            instances if ``affinity='precomputed'``. If a sparse feature matrix","361","            is provided, it will be converted into a sparse ``csr_matrix``.","364","            Not used, present here for API consistency by convention.","365","","366","        Returns","367","        -------","368","        self","401","        X : array-like or sparse matrix, shape (n_samples, n_features)","402","            New data to predict. If a sparse matrix is provided, it will be","403","            converted into a sparse ``csr_matrix``.","407","        labels : ndarray, shape (n_samples,)","408","            Cluster labels.","422","","423","    def fit_predict(self, X, y=None):","424","        \"\"\"Fit the clustering from features or affinity matrix, and return","425","        cluster labels.","426","","427","        Parameters","428","        ----------","429","        X : array-like or sparse matrix, shape (n_samples, n_features), or \\","430","            array-like, shape (n_samples, n_samples)","431","            Training instances to cluster, or similarities \/ affinities between","432","            instances if ``affinity='precomputed'``. If a sparse feature matrix","433","            is provided, it will be converted into a sparse ``csr_matrix``.","434","","435","        y : Ignored","436","            Not used, present here for API consistency by convention.","437","","438","        Returns","439","        -------","440","        labels : ndarray, shape (n_samples,)","441","            Cluster labels.","442","        \"\"\"","443","        return super().fit_predict(X, y)"],"delete":["353","        \"\"\" Create affinity matrix from negative euclidean distances, then","354","        apply affinity propagation clustering.","358","","359","        X : array-like, shape (n_samples, n_features) or (n_samples, n_samples)","360","            Data matrix or, if affinity is ``precomputed``, matrix of","361","            similarities \/ affinities.","396","        X : {array-like, sparse matrix}, shape (n_samples, n_features)","397","            New data to predict.","401","        labels : array, shape (n_samples,)","402","            Index of the cluster each sample belongs to."]}],"sklearn\/cluster\/dbscan_.py":[{"add":["331","        \"\"\"Perform DBSCAN clustering from features, or distance matrix.","335","        X : array-like or sparse matrix, shape (n_samples, n_features), or \\","336","            (n_samples, n_samples)","337","            Training instances to cluster, or distances between instances if","338","            ``metric='precomputed'``. If a sparse matrix is provided, it will","339","            be converted into a sparse ``csr_matrix``.","340","","343","            ``min_samples`` is by itself a core sample; a sample with a","344","            negative weight may inhibit its eps-neighbor from being core.","348","            Not used, present here for API consistency by convention.","349","","350","        Returns","351","        -------","352","        self","368","        \"\"\"Perform DBSCAN clustering from features or distance matrix,","369","        and return cluster labels.","373","        X : array-like or sparse matrix, shape (n_samples, n_features), or \\","374","            (n_samples, n_samples)","375","            Training instances to cluster, or distances between instances if","376","            ``metric='precomputed'``. If a sparse matrix is provided, it will","377","            be converted into a sparse ``csr_matrix``.","378","","381","            ``min_samples`` is by itself a core sample; a sample with a","382","            negative weight may inhibit its eps-neighbor from being core.","386","            Not used, present here for API consistency by convention.","390","        labels : ndarray, shape (n_samples,)","391","            Cluster labels. Noisy samples are given the label -1."],"delete":["331","        \"\"\"Perform DBSCAN clustering from features or distance matrix.","335","        X : array or sparse (CSR) matrix of shape (n_samples, n_features), or \\","336","                array of shape (n_samples, n_samples)","337","            A feature array, or array of distances between samples if","338","            ``metric='precomputed'``.","341","            ``min_samples`` is by itself a core sample; a sample with negative","342","            weight may inhibit its eps-neighbor from being core.","361","        \"\"\"Performs clustering on X and returns cluster labels.","365","        X : array or sparse (CSR) matrix of shape (n_samples, n_features), or \\","366","                array of shape (n_samples, n_samples)","367","            A feature array, or array of distances between samples if","368","            ``metric='precomputed'``.","371","            ``min_samples`` is by itself a core sample; a sample with negative","372","            weight may inhibit its eps-neighbor from being core.","379","        y : ndarray, shape (n_samples,)","380","            cluster labels"]}],"sklearn\/cluster\/spectral.py":[{"add":["450","        \"\"\"Perform spectral clustering from features, or affinity matrix.","454","        X : array-like or sparse matrix, shape (n_samples, n_features), or \\","455","            array-like, shape (n_samples, n_samples)","456","            Training instances to cluster, or similarities \/ affinities between","457","            instances if ``affinity='precomputed'``. If a sparse matrix is","458","            provided in a format other than ``csr_matrix``, ``csc_matrix``,","459","            or ``coo_matrix``, it will be converted into a sparse","460","            ``csr_matrix``.","463","            Not used, present here for API consistency by convention.","464","","465","        Returns","466","        -------","467","        self","508","    def fit_predict(self, X, y=None):","509","        \"\"\"Perform spectral clustering from features, or affinity matrix,","510","        and return cluster labels.","511","","512","        Parameters","513","        ----------","514","        X : array-like or sparse matrix, shape (n_samples, n_features), or \\","515","            array-like, shape (n_samples, n_samples)","516","            Training instances to cluster, or similarities \/ affinities between","517","            instances if ``affinity='precomputed'``. If a sparse matrix is","518","            provided in a format other than ``csr_matrix``, ``csc_matrix``,","519","            or ``coo_matrix``, it will be converted into a sparse","520","            ``csr_matrix``.","521","","522","        y : Ignored","523","            Not used, present here for API consistency by convention.","524","","525","        Returns","526","        -------","527","        labels : ndarray, shape (n_samples,)","528","            Cluster labels.","529","        \"\"\"","530","        return super().fit_predict(X, y)","531",""],"delete":["450","        \"\"\"Creates an affinity matrix for X using the selected affinity,","451","        then applies spectral clustering to this affinity matrix.","455","        X : array-like or sparse matrix, shape (n_samples, n_features)","456","            OR, if affinity==`precomputed`, a precomputed affinity","457","            matrix of shape (n_samples, n_samples)"]}]}},"ea63c56638aa6e0b987afe742c77d1e18007b768":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/ensemble\/gradient_boosting.py":"MODIFY","sklearn\/ensemble\/_gb_losses.py":"ADD","sklearn\/ensemble\/tests\/test_gradient_boosting.py":"MODIFY","sklearn\/ensemble\/tests\/test_gradient_boosting_loss_functions.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["130","- |Fix| Fixed a bug in :class:`ensemble.GradientBoostingClassifier` and","131","  :class:`ensemble.GradientBoostingRegressor`, which didn't support","132","  scikit-learn estimators as the initial estimator. Also added support of","133","  initial estimator which does not support sample weights. :issue:`12436` by","134","  :user:`J¨¦r¨¦mie du Boisberranger <jeremiedbb>` and :issue:`12983` by","135","  :user:`Nicolas Hug<NicolasHug>`.","136","","146","- |Fix| Fixed a bug in :class:`ensemble.GradientBoostingClassifier` where","147","  the default initial prediction of a multiclass classifier would predict the","148","  classes priors instead of the log of the priors. :issue:`12983` by","149","  :user:`Nicolas Hug<NicolasHug>`.","150",""],"delete":[]}],"sklearn\/ensemble\/gradient_boosting.py":[{"add":["28","from ..base import BaseEstimator","29","from ..base import is_classifier","48","from . import _gb_losses","63","# FIXME: 0.23","64","# All the losses and corresponding init estimators have been moved to the","65","# _losses module in 0.21. We deprecate them and keep them here for now in case","66","# someone has imported them. None of these losses can be used as a parameter","67","# to a GBDT estimator anyway (loss param only accepts strings).","68","","69","@deprecated(\"QuantileEstimator is deprecated in version \"","70","            \"0.21 and will be removed in version 0.23.\")","124","@deprecated(\"MeanEstimator is deprecated in version \"","125","            \"0.21 and will be removed in version 0.23.\")","167","@deprecated(\"LogOddsEstimator is deprecated in version \"","168","            \"0.21 and will be removed in version 0.23.\")","219","@deprecated(\"ScaledLogOddsEstimator is deprecated in version \"","220","            \"0.21 and will be removed in version 0.23.\")","226","@deprecated(\"PriorProbablityEstimator is deprecated in version \"","227","            \"0.21 and will be removed in version 0.23.\")","271","@deprecated(\"Using ZeroEstimator is deprecated in version \"","272","            \"0.21 and will be removed in version 0.23.\")","274","    \"\"\"An estimator that simply predicts zero.","275","","276","    .. deprecated:: 0.21","277","        Using ``ZeroEstimator`` or ``init='zero'`` is deprecated in version","278","        0.21 and will be removed in version 0.23.","279","","280","    \"\"\"","324","    def predict_proba(self, X):","325","        return self.predict(X)","327","","328","@deprecated(\"All Losses in sklearn.ensemble.gradient_boosting are \"","329","            \"deprecated in version \"","330","            \"0.21 and will be removed in version 0.23.\")","438","@deprecated(\"All Losses in sklearn.ensemble.gradient_boosting are \"","439","            \"deprecated in version \"","440","            \"0.21 and will be removed in version 0.23.\")","456","@deprecated(\"All Losses in sklearn.ensemble.gradient_boosting are \"","457","            \"deprecated in version \"","458","            \"0.21 and will be removed in version 0.23.\")","542","@deprecated(\"All Losses in sklearn.ensemble.gradient_boosting are \"","543","            \"deprecated in version \"","544","            \"0.21 and will be removed in version 0.23.\")","601","@deprecated(\"All Losses in sklearn.ensemble.gradient_boosting are \"","602","            \"deprecated in version \"","603","            \"0.21 and will be removed in version 0.23.\")","707","@deprecated(\"All Losses in sklearn.ensemble.gradient_boosting are \"","708","            \"deprecated in version \"","709","            \"0.21 and will be removed in version 0.23.\")","787","@deprecated(\"All Losses in sklearn.ensemble.gradient_boosting are \"","788","            \"deprecated in version \"","789","            \"0.21 and will be removed in version 0.23.\")","808","@deprecated(\"All Losses in sklearn.ensemble.gradient_boosting are \"","809","            \"deprecated in version \"","810","            \"0.21 and will be removed in version 0.23.\")","902","@deprecated(\"All Losses in sklearn.ensemble.gradient_boosting are \"","903","            \"deprecated in version \"","904","            \"0.21 and will be removed in version 0.23.\")","1000","@deprecated(\"All Losses in sklearn.ensemble.gradient_boosting are \"","1001","            \"deprecated in version \"","1002","            \"0.21 and will be removed in version 0.23.\")","1090","class VerboseReporter(object):","1201","    def _fit_stage(self, i, X, y, raw_predictions, sample_weight, sample_mask,","1209","        # Need to pass a copy of raw_predictions to negative_gradient()","1210","        # because raw_predictions is partially updated at the end of the loop","1211","        # in update_terminal_regions(), and gradients need to be evaluated at","1213","        raw_predictions_copy = raw_predictions.copy()","1219","            residual = loss.negative_gradient(y, raw_predictions_copy, k=k,","1246","            loss.update_terminal_regions(","1247","                tree.tree_, X, y, residual, raw_predictions, sample_weight,","1248","                sample_mask, learning_rate=self.learning_rate, k=k)","1253","        return raw_predictions","1266","                or self.loss not in _gb_losses.LOSS_FUNCTIONS):","1270","            loss_class = (_gb_losses.MultinomialDeviance","1272","                          else _gb_losses.BinomialDeviance)","1274","            loss_class = _gb_losses.LOSS_FUNCTIONS[self.loss]","1286","            # init must be an estimator or 'zero'","1287","            if isinstance(self.init, BaseEstimator):","1288","                self.loss_.check_init_estimator(self.init)","1289","            elif not (isinstance(self.init, str) and self.init == 'zero'):","1290","                raise ValueError(","1291","                    \"The init parameter must be an estimator or 'zero'. \"","1292","                    \"Got init={}\".format(self.init)","1293","                )","1342","        self.init_ = self.init","1343","        if self.init_ is None:","1442","","1443","        sample_weight_is_none = sample_weight is None","1444","        if sample_weight_is_none:","1448","            sample_weight_is_none = False","1459","            if is_classifier(self):","1460","                if self.n_classes_ != np.unique(y).shape[0]:","1461","                    # We choose to error here. The problem is that the init","1462","                    # estimator would be trained on y, which has some missing","1463","                    # classes now, so its predictions would not have the","1464","                    # correct shape.","1465","                    raise ValueError(","1466","                        'The training data after the early stopping split '","1467","                        'is missing some classes. Try using another random '","1468","                        'seed.'","1469","                    )","1479","            # fit initial model and initialize raw predictions","1480","            if self.init_ == 'zero':","1481","                raw_predictions = np.zeros(shape=(X.shape[0], self.loss_.K),","1482","                                           dtype=np.float64)","1483","            else:","1484","                try:","1485","                    self.init_.fit(X, y, sample_weight=sample_weight)","1486","                except TypeError:","1487","                    if sample_weight_is_none:","1488","                        self.init_.fit(X, y)","1489","                    else:","1490","                        raise ValueError(","1491","                            \"The initial estimator {} does not support sample \"","1492","                            \"weights.\".format(self.init_.__class__.__name__))","1494","                raw_predictions = \\","1495","                    self.loss_.get_init_raw_predictions(X, self.init_)","1496","","1497","","1517","            raw_predictions = self._raw_predict(X)","1536","        n_stages = self._fit_stages(","1537","            X, y, raw_predictions, sample_weight, self._rng, X_val, y_val,","1538","            sample_weight_val, begin_at_stage, monitor, X_idx_sorted)","1550","    def _fit_stages(self, X, y, raw_predictions, sample_weight, random_state,","1584","            y_val_pred_iter = self._staged_raw_predict(X_val)","1596","                                      raw_predictions[~sample_mask],","1600","            raw_predictions = self._fit_stage(","1601","                i, X, y, raw_predictions, sample_weight, sample_mask,","1602","                random_state, X_idx_sorted, X_csc, X_csr)","1607","                                             raw_predictions[sample_mask],","1611","                                          raw_predictions[~sample_mask],","1615","                self.train_score_[i] = loss_(y, raw_predictions, sample_weight)","1646","    def _raw_predict_init(self, X):","1647","        \"\"\"Check input and compute raw predictions of the init estimtor.\"\"\"","1653","        if self.init_ == 'zero':","1654","            raw_predictions = np.zeros(shape=(X.shape[0], self.loss_.K),","1655","                                       dtype=np.float64)","1656","        else:","1657","            raw_predictions = self.loss_.get_init_raw_predictions(","1658","                X, self.init_).astype(np.float64)","1659","        return raw_predictions","1661","    def _raw_predict(self, X):","1662","        \"\"\"Return the sum of the trees raw predictions (+ init estimator).\"\"\"","1663","        raw_predictions = self._raw_predict_init(X)","1664","        predict_stages(self.estimators_, X, self.learning_rate,","1665","                       raw_predictions)","1666","        return raw_predictions","1668","    def _staged_raw_predict(self, X):","1669","        \"\"\"Compute raw predictions of ``X`` for each iteration.","1683","        raw_predictions : generator of array, shape (n_samples, k)","1684","            The raw predictions of the input samples. The order of the","1690","        raw_predictions = self._raw_predict_init(X)","1692","            predict_stage(self.estimators_, i, X, self.learning_rate,","1693","                          raw_predictions)","1694","            yield raw_predictions.copy()","1872","    init : estimator or 'zero', optional (default=None)","1873","        An estimator object that is used to compute the initial predictions.","1874","        ``init`` has to provide `fit` and `predict_proba`. If 'zero', the","1875","        initial raw predictions are set to zero. By default, a","1876","        ``DummyEstimator`` predicting the classes priors is used.","2064","            The decision function of the input samples, which corresponds to","2065","            the raw values predicted from the trees of the ensemble . The","2066","            order of the classes corresponds to that in the attribute","2067","            `classes_`. Regression and binary classification produce an","2068","            array of shape [n_samples].","2071","        raw_predictions = self._raw_predict(X)","2072","        if raw_predictions.shape[1] == 1:","2073","            return raw_predictions.ravel()","2074","        return raw_predictions","2092","            The decision function of the input samples, which corresponds to","2093","            the raw values predicted from the trees of the ensemble . The","2098","        yield from self._staged_raw_predict(X)","2115","        raw_predictions = self.decision_function(X)","2116","        encoded_labels = \\","2117","            self.loss_._raw_prediction_to_decision(raw_predictions)","2118","        return self.classes_.take(encoded_labels, axis=0)","2138","        for raw_predictions in self._staged_raw_predict(X):","2139","            encoded_labels = \\","2140","                self.loss_._raw_prediction_to_decision(raw_predictions)","2141","            yield self.classes_.take(encoded_labels, axis=0)","2164","        raw_predictions = self.decision_function(X)","2166","            return self.loss_._raw_prediction_to_proba(raw_predictions)","2216","            for raw_predictions in self._staged_raw_predict(X):","2217","                yield self.loss_._raw_prediction_to_proba(raw_predictions)","2335","    init : estimator or 'zero', optional (default=None)","2336","        An estimator object that is used to compute the initial predictions.","2337","        ``init`` has to provide `fit` and `predict`. If 'zero', the initial","2338","        raw predictions are set to zero. By default a ``DummyEstimator`` is","2339","        used, predicting either the average target value (for loss='ls'), or","2340","        a quantile for the other losses.","2512","        # In regression we can directly return the raw value from the trees.","2513","        return self._raw_predict(X).ravel()","2533","        for raw_predictions in self._staged_raw_predict(X):","2534","            yield raw_predictions.ravel()"],"delete":["253","    \"\"\"An estimator that simply predicts zero. \"\"\"","1030","LOSS_FUNCTIONS = {'ls': LeastSquaresError,","1031","                  'lad': LeastAbsoluteError,","1032","                  'huber': HuberLossFunction,","1033","                  'quantile': QuantileLossFunction,","1034","                  'deviance': None,    # for both, multinomial and binomial","1035","                  'exponential': ExponentialLoss,","1036","                  }","1037","","1038","","1039","INIT_ESTIMATORS = {'zero': ZeroEstimator}","1040","","1041","","1042","class VerboseReporter:","1153","    def _fit_stage(self, i, X, y, y_pred, sample_weight, sample_mask,","1161","        # Need to pass a copy of y_pred to negative_gradient() because y_pred","1162","        # is partially updated at the end of the loop in","1163","        # update_terminal_regions(), and gradients need to be evaluated at","1165","        y_pred_copy = y_pred.copy()","1171","            residual = loss.negative_gradient(y, y_pred_copy, k=k,","1198","            loss.update_terminal_regions(tree.tree_, X, y, residual, y_pred,","1199","                                         sample_weight, sample_mask,","1200","                                         learning_rate=self.learning_rate, k=k)","1205","        return y_pred","1218","                or self.loss not in LOSS_FUNCTIONS):","1222","            loss_class = (MultinomialDeviance","1224","                          else BinomialDeviance)","1226","            loss_class = LOSS_FUNCTIONS[self.loss]","1238","            if isinstance(self.init, str):","1239","                if self.init not in INIT_ESTIMATORS:","1240","                    raise ValueError('init=\"%s\" is not supported' % self.init)","1241","            else:","1242","                if (not hasattr(self.init, 'fit')","1243","                        or not hasattr(self.init, 'predict')):","1244","                    raise ValueError(\"init=%r must be valid BaseEstimator \"","1245","                                     \"and support both fit and \"","1246","                                     \"predict\" % self.init)","1295","        if self.init is None:","1297","        elif isinstance(self.init, str):","1298","            self.init_ = INIT_ESTIMATORS[self.init]()","1299","        else:","1300","            self.init_ = self.init","1398","        if sample_weight is None:","1421","            # fit initial model - FIXME make sample_weight optional","1422","            self.init_.fit(X, y, sample_weight)","1424","            # init predictions","1425","            y_pred = self.init_.predict(X)","1445","            y_pred = self._decision_function(X)","1464","        n_stages = self._fit_stages(X, y, y_pred, sample_weight, self._rng,","1465","                                    X_val, y_val, sample_weight_val,","1466","                                    begin_at_stage, monitor, X_idx_sorted)","1478","    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,","1512","            y_val_pred_iter = self._staged_decision_function(X_val)","1524","                                      y_pred[~sample_mask],","1528","            y_pred = self._fit_stage(i, X, y, y_pred, sample_weight,","1529","                                     sample_mask, random_state, X_idx_sorted,","1530","                                     X_csc, X_csr)","1535","                                             y_pred[sample_mask],","1539","                                          y_pred[~sample_mask],","1543","                self.train_score_[i] = loss_(y, y_pred, sample_weight)","1574","    def _init_decision_function(self, X):","1575","        \"\"\"Check input and compute prediction of ``init``. \"\"\"","1581","        score = self.init_.predict(X).astype(np.float64)","1582","        return score","1584","    def _decision_function(self, X):","1585","        # for use in inner loop, not raveling the output in single-class case,","1586","        # not doing input validation.","1587","        score = self._init_decision_function(X)","1588","        predict_stages(self.estimators_, X, self.learning_rate, score)","1589","        return score","1591","","1592","    def _staged_decision_function(self, X):","1593","        \"\"\"Compute decision function of ``X`` for each iteration.","1607","        score : generator of array, shape (n_samples, k)","1608","            The decision function of the input samples. The order of the","1614","        score = self._init_decision_function(X)","1616","            predict_stage(self.estimators_, i, X, self.learning_rate, score)","1617","            yield score.copy()","1795","    init : estimator, optional","1796","        An estimator object that is used to compute the initial","1797","        predictions. ``init`` has to provide ``fit`` and ``predict``.","1798","        If None it uses ``loss.init_estimator``.","1986","            The decision function of the input samples. The order of the","1987","            classes corresponds to that in the attribute `classes_`.","1988","            Regression and binary classification produce an array of shape","1989","            [n_samples].","1992","        score = self._decision_function(X)","1993","        if score.shape[1] == 1:","1994","            return score.ravel()","1995","        return score","2013","            The decision function of the input samples. The order of the","2018","        yield from self._staged_decision_function(X)","2035","        score = self.decision_function(X)","2036","        decisions = self.loss_._score_to_decision(score)","2037","        return self.classes_.take(decisions, axis=0)","2057","        for score in self._staged_decision_function(X):","2058","            decisions = self.loss_._score_to_decision(score)","2059","            yield self.classes_.take(decisions, axis=0)","2082","        score = self.decision_function(X)","2084","            return self.loss_._score_to_proba(score)","2134","            for score in self._staged_decision_function(X):","2135","                yield self.loss_._score_to_proba(score)","2253","    init : estimator, optional (default=None)","2254","        An estimator object that is used to compute the initial","2255","        predictions. ``init`` has to provide ``fit`` and ``predict``.","2256","        If None it uses ``loss.init_estimator``.","2428","        return self._decision_function(X).ravel()","2448","        for y in self._staged_decision_function(X):","2449","            yield y.ravel()"]}],"sklearn\/ensemble\/_gb_losses.py":[{"add":[],"delete":[]}],"sklearn\/ensemble\/tests\/test_gradient_boosting.py":[{"add":["15","from sklearn.base import BaseEstimator","16","from sklearn.datasets import (make_classification, fetch_california_housing,","17","                              make_regression)","22","from sklearn.preprocessing import OneHotEncoder","23","from sklearn.svm import LinearSVC","40","from sklearn.dummy import DummyClassifier, DummyRegressor","41","","1054","    # Test if init='zero' works for regression.","1069","    # Test if init='zero' works for classification.","1321","","1322","","1323","class _NoSampleWeightWrapper(BaseEstimator):","1324","    def __init__(self, est):","1325","        self.est = est","1326","","1327","    def fit(self, X, y):","1328","        self.est.fit(X, y)","1329","","1330","    def predict(self, X):","1331","        return self.est.predict(X)","1332","","1333","    def predict_proba(self, X):","1334","        return self.est.predict_proba(X)","1335","","1336","","1337","def _make_multiclass():","1338","    return make_classification(n_classes=3, n_clusters_per_class=1)","1339","","1340","","1341","@pytest.mark.parametrize(","1342","    \"gb, dataset_maker, init_estimator\",","1343","    [(GradientBoostingClassifier, make_classification, DummyClassifier),","1344","     (GradientBoostingClassifier, _make_multiclass, DummyClassifier),","1345","     (GradientBoostingRegressor, make_regression, DummyRegressor)],","1346","    ids=[\"binary classification\", \"multiclass classification\", \"regression\"])","1347","def test_gradient_boosting_with_init(gb, dataset_maker, init_estimator):","1348","    # Check that GradientBoostingRegressor works when init is a sklearn","1349","    # estimator.","1350","    # Check that an error is raised if trying to fit with sample weight but","1351","    # inital estimator does not support sample weight","1352","","1353","    X, y = dataset_maker()","1354","    sample_weight = np.random.RandomState(42).rand(100)","1355","","1356","    # init supports sample weights","1357","    init_est = init_estimator()","1358","    gb(init=init_est).fit(X, y, sample_weight=sample_weight)","1359","","1360","    # init does not support sample weights","1361","    init_est = _NoSampleWeightWrapper(init_estimator())","1362","    gb(init=init_est).fit(X, y)  # ok no sample weights","1363","    with pytest.raises(ValueError,","1364","                       match=\"estimator.*does not support sample weights\"):","1365","        gb(init=init_est).fit(X, y, sample_weight=sample_weight)","1366","","1367","","1368","@pytest.mark.parametrize('estimator, missing_method', [","1369","    (GradientBoostingClassifier(init=LinearSVC()), 'predict_proba'),","1370","    (GradientBoostingRegressor(init=OneHotEncoder()), 'predict')","1371","])","1372","def test_gradient_boosting_init_wrong_methods(estimator, missing_method):","1373","    # Make sure error is raised if init estimators don't have the required","1374","    # methods (fit, predict, predict_proba)","1375","","1376","    message = (\"The init parameter must be a valid estimator and support \"","1377","               \"both fit and \" + missing_method)","1378","    with pytest.raises(ValueError, match=message):","1379","        estimator.fit(X, y)","1380","","1381","","1382","def test_early_stopping_n_classes():","1383","    # when doing early stopping (_, y_train, _, _ = train_test_split(X, y))","1384","    # there might be classes in y that are missing in y_train. As the init","1385","    # estimator will be trained on y_train, we need to raise an error if this","1386","    # happens.","1387","","1388","    X = [[1, 2], [2, 3], [3, 4], [4, 5]]","1389","    y = [0, 1, 1, 1]","1390","    gb = GradientBoostingClassifier(n_iter_no_change=5, random_state=4)","1391","    with pytest.raises(","1392","                ValueError,","1393","                match='The training data after the early stopping split'):","1394","        gb.fit(X, y)","1395","","1396","    # No error with another random seed","1397","    gb = GradientBoostingClassifier(n_iter_no_change=5, random_state=0)","1398","    gb.fit(X, y)"],"delete":["15","from sklearn.datasets import make_classification, fetch_california_housing","1048","    # Test if ZeroEstimator works for regression.","1049","    est = GradientBoostingRegressor(n_estimators=20, max_depth=1,","1050","                                    random_state=1, init=ZeroEstimator())","1051","    est.fit(boston.data, boston.target)","1052","    y_pred = est.predict(boston.data)","1053","    mse = mean_squared_error(boston.target, y_pred)","1054","    assert_almost_equal(mse, 33.0, decimal=0)","1069","    # Test if ZeroEstimator works for classification.","1072","    est = GradientBoostingClassifier(n_estimators=20, max_depth=1,","1073","                                     random_state=1, init=ZeroEstimator())","1074","    est.fit(X, y)","1075","","1076","    assert_greater(est.score(X, y), 0.96)"]}],"sklearn\/ensemble\/tests\/test_gradient_boosting_loss_functions.py":[{"add":["6","from numpy.testing import assert_allclose","10","from sklearn.utils.stats import _weighted_percentile","11","from sklearn.ensemble._gb_losses import RegressionLossFunction","12","from sklearn.ensemble._gb_losses import LeastSquaresError","13","from sklearn.ensemble._gb_losses import LeastAbsoluteError","14","from sklearn.ensemble._gb_losses import HuberLossFunction","15","from sklearn.ensemble._gb_losses import QuantileLossFunction","16","from sklearn.ensemble._gb_losses import BinomialDeviance","17","from sklearn.ensemble._gb_losses import MultinomialDeviance","18","from sklearn.ensemble._gb_losses import ExponentialLoss","19","from sklearn.ensemble._gb_losses import LOSS_FUNCTIONS","93","        out = loss.get_init_raw_predictions(X, init_est)","98","        sw_out = loss.get_init_raw_predictions(X, sw_init_est)","102","        assert_allclose(out, sw_out, rtol=1e-2)","176","","177","","178","def test_init_raw_predictions_shapes():","179","    # Make sure get_init_raw_predictions returns float64 arrays with shape","180","    # (n_samples, K) where K is 1 for binary classification and regression, and","181","    # K = n_classes for multiclass classification","182","    rng = np.random.RandomState(0)","183","","184","    n_samples = 100","185","    X = rng.normal(size=(n_samples, 5))","186","    y = rng.normal(size=n_samples)","187","    for loss in (LeastSquaresError(n_classes=1),","188","                 LeastAbsoluteError(n_classes=1),","189","                 QuantileLossFunction(n_classes=1),","190","                 HuberLossFunction(n_classes=1)):","191","        init_estimator = loss.init_estimator().fit(X, y)","192","        raw_predictions = loss.get_init_raw_predictions(y, init_estimator)","193","        assert raw_predictions.shape == (n_samples, 1)","194","        assert raw_predictions.dtype == np.float64","195","","196","    y = rng.randint(0, 2, size=n_samples)","197","    for loss in (BinomialDeviance(n_classes=2),","198","                 ExponentialLoss(n_classes=2)):","199","        init_estimator = loss.init_estimator().fit(X, y)","200","        raw_predictions = loss.get_init_raw_predictions(y, init_estimator)","201","        assert raw_predictions.shape == (n_samples, 1)","202","        assert raw_predictions.dtype == np.float64","203","","204","    for n_classes in range(3, 5):","205","        y = rng.randint(0, n_classes, size=n_samples)","206","        loss = MultinomialDeviance(n_classes=n_classes)","207","        init_estimator = loss.init_estimator().fit(X, y)","208","        raw_predictions = loss.get_init_raw_predictions(y, init_estimator)","209","        assert raw_predictions.shape == (n_samples, n_classes)","210","        assert raw_predictions.dtype == np.float64","211","","212","","213","def test_init_raw_predictions_values():","214","    # Make sure the get_init_raw_predictions() returns the expected values for","215","    # each loss.","216","    rng = np.random.RandomState(0)","217","","218","    n_samples = 100","219","    X = rng.normal(size=(n_samples, 5))","220","    y = rng.normal(size=n_samples)","221","","222","    # Least squares loss","223","    loss = LeastSquaresError(n_classes=1)","224","    init_estimator = loss.init_estimator().fit(X, y)","225","    raw_predictions = loss.get_init_raw_predictions(y, init_estimator)","226","    # Make sure baseline prediction is the mean of all targets","227","    assert_almost_equal(raw_predictions, y.mean())","228","","229","    # Least absolute and huber loss","230","    for Loss in (LeastAbsoluteError, HuberLossFunction):","231","        loss = Loss(n_classes=1)","232","        init_estimator = loss.init_estimator().fit(X, y)","233","        raw_predictions = loss.get_init_raw_predictions(y, init_estimator)","234","        # Make sure baseline prediction is the median of all targets","235","        assert_almost_equal(raw_predictions, np.median(y))","236","","237","    # Quantile loss","238","    for alpha in (.1, .5, .9):","239","        loss = QuantileLossFunction(n_classes=1, alpha=alpha)","240","        init_estimator = loss.init_estimator().fit(X, y)","241","        raw_predictions = loss.get_init_raw_predictions(y, init_estimator)","242","        # Make sure baseline prediction is the alpha-quantile of all targets","243","        assert_almost_equal(raw_predictions, np.percentile(y, alpha * 100))","244","","245","    y = rng.randint(0, 2, size=n_samples)","246","","247","    # Binomial deviance","248","    loss = BinomialDeviance(n_classes=2)","249","    init_estimator = loss.init_estimator().fit(X, y)","250","    # Make sure baseline prediction is equal to link_function(p), where p","251","    # is the proba of the positive class. We want predict_proba() to return p,","252","    # and by definition","253","    # p = inverse_link_function(raw_prediction) = sigmoid(raw_prediction)","254","    # So we want raw_prediction = link_function(p) = log(p \/ (1 - p))","255","    raw_predictions = loss.get_init_raw_predictions(y, init_estimator)","256","    p = y.mean()","257","    assert_almost_equal(raw_predictions, np.log(p \/ (1 - p)))","258","","259","    # Exponential loss","260","    loss = ExponentialLoss(n_classes=2)","261","    init_estimator = loss.init_estimator().fit(X, y)","262","    raw_predictions = loss.get_init_raw_predictions(y, init_estimator)","263","    p = y.mean()","264","    assert_almost_equal(raw_predictions, .5 * np.log(p \/ (1 - p)))","265","","266","    # Multinomial deviance loss","267","    for n_classes in range(3, 5):","268","        y = rng.randint(0, n_classes, size=n_samples)","269","        loss = MultinomialDeviance(n_classes=n_classes)","270","        init_estimator = loss.init_estimator().fit(X, y)","271","        raw_predictions = loss.get_init_raw_predictions(y, init_estimator)","272","        for k in range(n_classes):","273","            p = (y == k).mean()","274","        assert_almost_equal(raw_predictions[:, k], np.log(p))"],"delete":["5","from numpy.testing import assert_array_equal","10","from sklearn.utils.testing import assert_raises","11","from sklearn.ensemble.gradient_boosting import BinomialDeviance","12","from sklearn.ensemble.gradient_boosting import LogOddsEstimator","13","from sklearn.ensemble.gradient_boosting import LeastSquaresError","14","from sklearn.ensemble.gradient_boosting import RegressionLossFunction","15","from sklearn.ensemble.gradient_boosting import LOSS_FUNCTIONS","16","from sklearn.ensemble.gradient_boosting import _weighted_percentile","17","from sklearn.ensemble.gradient_boosting import QuantileLossFunction","54","def test_log_odds_estimator():","55","    # Check log odds estimator.","56","    est = LogOddsEstimator()","57","    assert_raises(ValueError, est.fit, None, np.array([1]))","58","","59","    est.fit(None, np.array([1.0, 0.0]))","60","    assert_equal(est.prior, 0.0)","61","    assert_array_equal(est.predict(np.array([[1.0], [1.0]])),","62","                       np.array([[0.0], [0.0]]))","63","","64","","102","        out = init_est.predict(X)","107","        sw_out = init_est.predict(X)","111","        assert_array_equal(out, sw_out)","157","    X = rng.rand(100, 2)"]}]}},"8155d0e492b4fe5913eb1dc3d76fbf7d46365318":{"changes":{"sklearn\/isotonic.py":"MODIFY","sklearn\/tests\/test_isotonic.py":"MODIFY"},"diff":{"sklearn\/isotonic.py":[{"add":["9","from .utils import check_array, check_consistent_length","121","    y = check_array(y, ensure_2d=False, dtype=[np.float64, np.float32])","317","        check_params = dict(accept_sparse=False, ensure_2d=False,","318","                            dtype=[np.float64, np.float32])","319","        X = check_array(X, **check_params)","320","        y = check_array(y, **check_params)","321","        check_consistent_length(X, y, sample_weight)","322",""],"delete":["9","from .utils import as_float_array, check_array, check_consistent_length","121","    y = as_float_array(y)","242","        X = as_float_array(X)","243","        y = check_array(y, dtype=X.dtype, ensure_2d=False)","244","        check_consistent_length(X, y, sample_weight)"]}],"sklearn\/tests\/test_isotonic.py":[{"add":["8","from sklearn.utils.validation import check_array","17","","27","    y_transformed_s = \\","28","        ir.fit(x_s, y_s, sample_weight=sample_weight_s).transform(x)","433","    y_train = np.less(rng.rand(n_samples),","434","                      expit(X_train)).astype('int64').astype('float64')","475","            expected_dtype = \\","476","                check_array(y_np, dtype=[np.float64, np.float32],","477","                            ensure_2d=False).dtype"],"delete":["8","from sklearn.utils.validation import as_float_array","26","    y_transformed_s = ir.fit(x_s, y_s, sample_weight=sample_weight_s).transform(x)","431","    y_train = np.less(rng.rand(n_samples), expit(X_train)).astype('int64')","472","            expected_dtype = as_float_array(y_np).dtype"]}]}},"b34751b7ed02b2cfcc36037fb729d4360480a299":{"changes":{"azure-pipelines.yml":"MODIFY","sklearn\/tree\/tests\/test_export.py":"MODIFY","sklearn\/utils\/testing.py":"MODIFY","sklearn\/conftest.py":"ADD","sklearn\/ensemble\/tests\/test_partial_dependence.py":"MODIFY","sklearn\/tests\/test_common.py":"MODIFY","doc\/developers\/contributing.rst":"MODIFY","build_tools\/azure\/install.cmd":"MODIFY","sklearn\/inspection\/tests\/test_partial_dependence.py":"MODIFY"},"diff":{"azure-pipelines.yml":[{"add":["24","        MATPLOTLIB_VERSION: '1.5.1'"],"delete":[]}],"sklearn\/tree\/tests\/test_export.py":[{"add":["401","def test_plot_tree(pyplot):"],"delete":["401","def test_plot_tree():","403","    pytest.importorskip(\"matplotlib.pyplot\")"]}],"sklearn\/utils\/testing.py":[{"add":[],"delete":["716","def if_matplotlib(func):","717","    \"\"\"Test decorator that skips test if matplotlib not installed.","718","","719","    Parameters","720","    ----------","721","    func","722","    \"\"\"","723","    @wraps(func)","724","    def run_test(*args, **kwargs):","725","        try:","726","            import matplotlib","727","            matplotlib.use('Agg', warn=False)","728","            # this fails if no $DISPLAY specified","729","            import matplotlib.pyplot as plt","730","            plt.figure()","731","        except ImportError:","732","            raise SkipTest('Matplotlib not available.')","733","        else:","734","            return func(*args, **kwargs)","735","    return run_test","736","","737","","1026","","1027","","1028","def close_figure(fig=None):","1029","    \"\"\"Close a matplotlibt figure.","1030","","1031","    Parameters","1032","    ----------","1033","    fig : int or str or Figure, optional (default=None)","1034","        The figure, figure number or figure name to close. If ``None``, all","1035","        current figures are closed.","1036","    \"\"\"","1037","    from matplotlib.pyplot import get_fignums, close as _close  # noqa","1038","","1039","    if fig is None:","1040","        for fig in get_fignums():","1041","            _close(fig)","1042","    else:","1043","        _close(fig)"]}],"sklearn\/conftest.py":[{"add":[],"delete":[]}],"sklearn\/ensemble\/tests\/test_partial_dependence.py":[{"add":["156","def test_plot_partial_dependence(pyplot):","190","def test_plot_partial_dependence_input(pyplot):","227","def test_plot_partial_dependence_multiclass(pyplot):","262","@pytest.mark.parametrize(","263","    \"func, params\",","264","    [(partial_dependence, {'target_variables': [0], 'X': boston.data}),","265","     (plot_partial_dependence, {'X': boston.data, 'features': [0, 1, (0, 1)]})]","266",")","267","def test_raise_deprecation_warning(pyplot, func, params):","272","    warn_msg = \"The function ensemble.{} has been deprecated\".format(","273","        func.__name__","274","    )","275","    with pytest.warns(DeprecationWarning, match=warn_msg):","276","        func(clf, **params, grid_resolution=grid_resolution)"],"delete":["9","from sklearn.utils.testing import if_matplotlib","16","from sklearn.utils.testing import assert_warns_message","158","@if_matplotlib","159","def test_plot_partial_dependence():","192","@if_matplotlib","194","def test_plot_partial_dependence_input():","230","@if_matplotlib","232","def test_plot_partial_dependence_multiclass():","267","def test_warning_raised_partial_dependence():","268","    # Test that deprecation warning is raised","269","","274","    assert_warns_message(DeprecationWarning, \"The function \"","275","                         \"ensemble.partial_dependence has been deprecated \",","276","                         partial_dependence, clf, [0], X=boston.data,","277","                         grid_resolution=grid_resolution)","278","","279","","280","@if_matplotlib","281","def test_warning_raised_partial_dependence_plot():","282","    # Test that deprecation warning is raised","283","","284","    clf = GradientBoostingRegressor(n_estimators=10, random_state=1)","285","    clf.fit(boston.data, boston.target)","286","    grid_resolution = 25","287","","288","    assert_warns_message(DeprecationWarning, \"The function \"","289","                         \"ensemble.plot_partial_dependence has been \"","290","                         \"deprecated\",","291","                         plot_partial_dependence, clf, boston.data,","292","                         [0, 1, (0, 1)], grid_resolution=grid_resolution,","293","                         feature_names=boston.feature_names)"]}],"sklearn\/tests\/test_common.py":[{"add":["217","    EXCEPTIONS = ('utils', 'tests', 'base', 'setup', 'conftest')"],"delete":["217","    EXCEPTIONS = ('utils', 'tests', 'base', 'setup')"]}],"doc\/developers\/contributing.rst":[{"add":["628","-----------------------------------","646","Writing matplotlib related tests","647","................................","648","","649","Test fixtures ensure that a set of tests will be executing with the appropriate","650","initialization and cleanup. The scikit-learn test suite implements a fixture","651","which can be used with ``matplotlib``.","652","","653","``pyplot``","654","    The ``pyplot`` fixture should be used when a test function is dealing with","655","    ``matplotlib``. ``matplotlib`` is a soft dependency and is not required.","656","    This fixture is in charge of skipping the tests if ``matplotlib`` is not","657","    installed. In addition, figures created during the tests will be","658","    automatically closed once the test function has been executed.","659","","660","To use this fixture in a test function, one needs to pass it as an","661","argument::","662","","663","    def test_requiring_mpl_fixture(pyplot):","664","        # you can now safely use matplotlib","665","","666","Workflow to improve test coverage","667",".................................","668","","669","To test code coverage, you need to install the `coverage","670","<https:\/\/pypi.org\/project\/coverage\/>`_ package in addition to pytest.","671","","672","1. Run 'make test-coverage'. The output lists for each file the line","673","    numbers that are not tested.","674","","675","2. Find a low hanging fruit, looking at which lines are not tested,","676","    write or adapt a test specifically for these lines.","677","","678","3. Loop."],"delete":["628","------------------------------------","643",".. note:: **Workflow to improve test coverage**","644","","645","   To test code coverage, you need to install the `coverage","646","   <https:\/\/pypi.org\/project\/coverage\/>`_ package in addition to pytest.","647","","648","   1. Run 'make test-coverage'. The output lists for each file the line","649","      numbers that are not tested.","650","","651","   2. Find a low hanging fruit, looking at which lines are not tested,","652","      write or adapt a test specifically for these lines.","653","","654","   3. Loop.","655",""]}],"build_tools\/azure\/install.cmd":[{"add":["13","    conda create -n %VIRTUALENV% -q -y python=%PYTHON_VERSION% numpy scipy cython matplotlib pytest wheel pillow joblib"],"delete":["13","    conda create -n %VIRTUALENV% -q -y python=%PYTHON_VERSION% numpy scipy cython pytest wheel pillow joblib"]}],"sklearn\/inspection\/tests\/test_partial_dependence.py":[{"add":["397","def test_plot_partial_dependence(pyplot):","407","    fig = pyplot.gcf()","418","    fig = pyplot.gcf()","429","    fig = pyplot.gcf()","435","def test_plot_partial_dependence_multiclass(pyplot):","445","    fig = pyplot.gcf()","459","    fig = pyplot.gcf()","465","def test_plot_partial_dependence_multioutput(pyplot):","475","    fig = pyplot.gcf()","483","    fig = pyplot.gcf()","518","def test_plot_partial_dependence_error(pyplot, data, params, err_msg):","526","def test_plot_partial_dependence_fig(pyplot):","532","    fig = pyplot.figure()","537","    assert pyplot.gcf() is fig"],"delete":["29","from sklearn.utils.testing import if_matplotlib","398","@if_matplotlib","399","def test_plot_partial_dependence():","401","    import matplotlib.pyplot as plt  # noqa","402","","411","    fig = plt.gcf()","422","    fig = plt.gcf()","433","    fig = plt.gcf()","438","    plt.close('all')","440","","441","@if_matplotlib","442","def test_plot_partial_dependence_multiclass():","444","    import matplotlib.pyplot as plt  # noqa","453","    fig = plt.gcf()","467","    fig = plt.gcf()","472","    plt.close('all')","474","","475","@if_matplotlib","476","def test_plot_partial_dependence_multioutput():","478","    import matplotlib.pyplot as plt  # noqa","487","    fig = plt.gcf()","495","    fig = plt.gcf()","500","    plt.close('all')","502","","503","@if_matplotlib","533","def test_plot_partial_dependence_error(data, params, err_msg):","534","    import matplotlib.pyplot as plt  # noqa","541","    plt.close()","543","","544","@if_matplotlib","545","def test_plot_partial_dependence_fig():","547","","548","    import matplotlib.pyplot as plt","549","","554","    fig = plt.figure()","559","    assert plt.gcf() is fig","560","","561","    plt.close()"]}]}},"1f75ffa75ff91603bf19c55c9f6718eae47f39b7":{"changes":{"sklearn\/datasets\/tests\/data\/openml\/2\/api-v1-json-data-qualities-2.json.gz":"ADD","sklearn\/datasets\/tests\/data\/openml\/61\/api-v1-json-data-qualities-61.json.gz":"ADD","sklearn\/datasets\/tests\/test_openml.py":"MODIFY","sklearn\/datasets\/tests\/data\/openml\/561\/api-v1-json-data-qualities-561.json.gz":"ADD","sklearn\/datasets\/tests\/data\/openml\/1119\/api-v1-json-data-qualities-1119.json.gz":"ADD","sklearn\/datasets\/tests\/data\/openml\/3\/api-v1-json-data-qualities-3.json.gz":"ADD","sklearn\/datasets\/tests\/data\/openml\/40589\/api-v1-json-data-qualities-40589.json.gz":"ADD","sklearn\/datasets\/tests\/data\/openml\/40675\/api-v1-json-data-qualities-40675.json.gz":"ADD","sklearn\/datasets\/tests\/data\/openml\/1\/api-v1-json-data-qualities-1.json.gz":"ADD","sklearn\/datasets\/tests\/data\/openml\/40966\/api-v1-json-data-qualities-40966.json.gz":"ADD","sklearn\/datasets\/openml.py":"MODIFY"},"diff":{"sklearn\/datasets\/tests\/data\/openml\/2\/api-v1-json-data-qualities-2.json.gz":[{"add":[],"delete":[]}],"sklearn\/datasets\/tests\/data\/openml\/61\/api-v1-json-data-qualities-61.json.gz":[{"add":[],"delete":[]}],"sklearn\/datasets\/tests\/test_openml.py":[{"add":["57","    data_downloaded = np.array(list(data_arff['data']), dtype='O')"],"delete":["57","    data_downloaded = np.array(data_arff['data'], dtype='O')"]}],"sklearn\/datasets\/tests\/data\/openml\/561\/api-v1-json-data-qualities-561.json.gz":[{"add":[],"delete":[]}],"sklearn\/datasets\/tests\/data\/openml\/1119\/api-v1-json-data-qualities-1119.json.gz":[{"add":[],"delete":[]}],"sklearn\/datasets\/tests\/data\/openml\/3\/api-v1-json-data-qualities-3.json.gz":[{"add":[],"delete":[]}],"sklearn\/datasets\/tests\/data\/openml\/40589\/api-v1-json-data-qualities-40589.json.gz":[{"add":[],"delete":[]}],"sklearn\/datasets\/tests\/data\/openml\/40675\/api-v1-json-data-qualities-40675.json.gz":[{"add":[],"delete":[]}],"sklearn\/datasets\/tests\/data\/openml\/1\/api-v1-json-data-qualities-1.json.gz":[{"add":[],"delete":[]}],"sklearn\/datasets\/tests\/data\/openml\/40966\/api-v1-json-data-qualities-40966.json.gz":[{"add":[],"delete":[]}],"sklearn\/datasets\/openml.py":[{"add":["8","import itertools","9","from collections.abc import Generator","27","_DATA_QUALITIES = \"api\/v1\/json\/data\/qualities\/{}\"","215","def _convert_arff_data(arff_data, col_slice_x, col_slice_y, shape=None):","239","    if isinstance(arff_data, Generator):","240","        if shape[0] == -1:","241","            count = -1","242","        else:","243","            count = shape[0] * shape[1]","244","        data = np.fromiter(itertools.chain.from_iterable(arff_data),","245","                           dtype='float64', count=count)","246","        data = data.reshape(*shape)","247","        X = data[:, col_slice_x]","248","        y = data[:, col_slice_y]","344","def _get_data_qualities(data_id, data_home):","345","    # OpenML API function:","346","    # https:\/\/www.openml.org\/api_docs#!\/data\/get_data_qualities_id","347","    url = _DATA_QUALITIES.format(data_id)","348","    error_message = \"Dataset with data_id {} not found.\".format(data_id)","349","    json_data = _get_json_content_from_openml_api(url, error_message, True,","350","                                                  data_home)","351","    try:","352","        return json_data['data_qualities']['quality']","353","    except KeyError:","354","        # the qualities might not be available, but we still try to process","355","        # the data","356","        return None","357","","358","","359","def _get_data_shape(data_qualities):","360","    # Using the data_info dictionary from _get_data_info_by_name to extract","361","    # the number of samples \/ features","362","    if data_qualities is None:","363","        return None","364","    qualities = {d['name']: d['value'] for d in data_qualities}","365","    try:","366","        return (int(float(qualities['NumberOfInstances'])),","367","                int(float(qualities['NumberOfFeatures'])))","368","    except AttributeError:","369","        return None","370","","371","","385","                return_type = _arff.DENSE_GEN","547","        if version != \"active\":","623","    if not return_sparse:","624","        data_qualities = _get_data_qualities(data_id, data_home)","625","        shape = _get_data_shape(data_qualities)","626","        # if the data qualities were not available, we can still get the","627","        # n_features from the feature list, with the n_samples unknown","628","        if shape is None:","629","            shape = (-1, len(features_list))","630","    else:","631","        shape = None","632","","636","","644","    X, y = _convert_arff_data(arff['data'], col_slice_x, col_slice_y, shape)"],"delete":["212","def _convert_arff_data(arff_data, col_slice_x, col_slice_y):","236","    if isinstance(arff_data, list):","237","        data = np.array(arff_data, dtype=np.float64)","238","        X = np.array(data[:, col_slice_x], dtype=np.float64)","239","        y = np.array(data[:, col_slice_y], dtype=np.float64)","348","                return_type = _arff.DENSE","510","        if version is not \"active\":","589","    arff_data = arff['data']","597","    X, y = _convert_arff_data(arff_data, col_slice_x, col_slice_y)"]}]}},"66899eddd01ef7c694565bf71a8bc8a8b0adeae5":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/linear_model\/tests\/test_base.py":"MODIFY","sklearn\/linear_model\/base.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["202","- |Fix| Fixed a bug in :class:`linear_model.LinearRegression` that","203","  was not returning the same coeffecients and intercepts with","204","  ``fit_intercept=True`` in sparse and dense case.","205","  :issue:`13279` by `Alexandre Gramfort`_","206",""],"delete":[]}],"sklearn\/linear_model\/tests\/test_base.py":[{"add":["156","@pytest.mark.parametrize('normalize', [True, False])","157","@pytest.mark.parametrize('fit_intercept', [True, False])","158","def test_linear_regression_sparse_equal_dense(normalize, fit_intercept):","159","    # Test that linear regression agrees between sparse and dense","160","    rng = check_random_state(0)","161","    n_samples = 200","162","    n_features = 2","163","    X = rng.randn(n_samples, n_features)","164","    X[X < 0.1] = 0.","165","    Xcsr = sparse.csr_matrix(X)","166","    y = rng.rand(n_samples)","167","    params = dict(normalize=normalize, fit_intercept=fit_intercept)","168","    clf_dense = LinearRegression(**params)","169","    clf_sparse = LinearRegression(**params)","170","    clf_dense.fit(X, y)","171","    clf_sparse.fit(Xcsr, y)","172","    assert clf_dense.intercept_ == pytest.approx(clf_sparse.intercept_)","173","    assert_allclose(clf_dense.coef_, clf_sparse.coef_)","174","","175",""],"delete":[]}],"sklearn\/linear_model\/base.py":[{"add":["469","            copy=self.copy_X, sample_weight=sample_weight,","470","            return_mean=True)","477","            X_offset_scale = X_offset \/ X_scale","478","","479","            def matvec(b):","480","                return X.dot(b) - b.dot(X_offset_scale)","481","","482","            def rmatvec(b):","483","                return X.T.dot(b) - X_offset_scale * np.sum(b)","484","","485","            X_centered = sparse.linalg.LinearOperator(shape=X.shape,","486","                                                      matvec=matvec,","487","                                                      rmatvec=rmatvec)","488","","490","                out = sparse_lsqr(X_centered, y)","496","                    delayed(sparse_lsqr)(X_centered, y[:, j].ravel())"],"delete":["469","            copy=self.copy_X, sample_weight=sample_weight)","477","                out = sparse_lsqr(X, y)","483","                    delayed(sparse_lsqr)(X, y[:, j].ravel())"]}]}},"fc6d906c01848a24b6f247b8598ba5a37c0098fb":{"changes":{"sklearn\/linear_model\/tests\/test_coordinate_descent.py":"MODIFY","sklearn\/linear_model\/cd_fast.pyx":"MODIFY"},"diff":{"sklearn\/linear_model\/tests\/test_coordinate_descent.py":[{"add":["847","","848","","849","def test_convergence_warnings():","850","    random_state = np.random.RandomState(0)","851","    X = random_state.standard_normal((1000, 500))","852","    y = random_state.standard_normal((1000, 3))","853","","854","    # check that the model fails to converge","855","    with pytest.warns(ConvergenceWarning):","856","        MultiTaskElasticNet(max_iter=1, tol=0).fit(X, y)","857","","858","    # check that the model converges w\/o warnings","859","    with pytest.warns(None) as record:","860","        MultiTaskElasticNet(max_iter=1000).fit(X, y)","861","","862","    assert not record.list","863","","864","","865","def test_sparse_input_convergence_warning():","866","    X, y, _, _ = build_dataset(n_samples=1000, n_features=500)","867","","868","    with pytest.warns(ConvergenceWarning):","869","        ElasticNet(max_iter=1, tol=0).fit(","870","            sparse.csr_matrix(X, dtype=np.float32), y)","871","","872","    # check that the model converges w\/o warnings","873","    with pytest.warns(None) as record:","874","        Lasso(max_iter=1000).fit(sparse.csr_matrix(X, dtype=np.float32), y)","875","","876","    assert not record.list"],"delete":[]}],"sklearn\/linear_model\/cd_fast.pyx":[{"add":["156","        warnings.warn(\"Coordinate descent with no regularization may lead to \"","157","                      \"unexpected results and is discouraged.\")","250","            # for\/else, runs if for doesn't end with a `break`","252","                warnings.warn(\"Objective did not converge. You might want to \"","253","                              \"increase the number of iterations. Duality \"","254","                              \"gap: {}, tolerance: {}\".format(gap, tol),","255","                              ConvergenceWarning)","465","            # for\/else, runs if for doesn't end with a `break`","467","                warnings.warn(\"Objective did not converge. You might want to \"","468","                              \"increase the number of iterations. Duality \"","469","                              \"gap: {}, tolerance: {}\".format(gap, tol),","470","                              ConvergenceWarning)","617","        else:","618","            # for\/else, runs if for doesn't end with a `break`","619","            with gil:","620","                warnings.warn(\"Objective did not converge. You might want to \"","621","                              \"increase the number of iterations. Duality \"","622","                              \"gap: {}, tolerance: {}\".format(gap, tol),","623","                              ConvergenceWarning)","812","        else:","813","            # for\/else, runs if for doesn't end with a `break`","814","            with gil:","815","                warnings.warn(\"Objective did not converge. You might want to \"","816","                              \"increase the number of iterations. Duality \"","817","                              \"gap: {}, tolerance: {}\".format(gap, tol),","818","                              ConvergenceWarning)"],"delete":["156","        warnings.warn(\"Coordinate descent with no regularization may lead to unexpected\"","157","            \" results and is discouraged.\")","251","                warnings.warn(\"Objective did not converge.\"","252","                \" You might want to increase the number of iterations.\"","253","                \" Duality gap: {}, tolerance: {}\".format(gap, tol),","254","                ConvergenceWarning)","465","                warnings.warn(\"Objective did not converge.\"","466","                \" You might want to increase the number of iterations.\"","467","                \" Duality gap: {}, tolerance: {}\".format(gap, tol),","468","                ConvergenceWarning)","615","","616","        with gil:","617","            warnings.warn(\"Objective did not converge.\"","618","            \" You might want to increase the number of iterations.\"","619","            \" Duality gap: {}, tolerance: {}\".format(gap, tol),","620","            ConvergenceWarning)","809","                else:","810","                    with gil:","811","                        warnings.warn(\"Objective did not converge.\"","812","                        \" You might want to increase the number of iterations.\"","813","                        \" Duality gap: {}, tolerance: {}\".format(gap, tol),","814","                        ConvergenceWarning)"]}]}},"bf6949ba9d5fc019fd48eda247249d8fd8c420fa":{"changes":{"sklearn\/multioutput.py":"MODIFY"},"diff":{"sklearn\/multioutput.py":[{"add":["462","        check_is_fitted(self, 'estimators_')","639","        return {'_skip_test': True,","640","                'multioutput_only': True}","726","        return {'multioutput_only': True}"],"delete":["638","        return {'_skip_test': True}","724","        # FIXME","725","        return {'_skip_test': True}"]}]}},"37a9d187f9eb4864fb52556e9f293f8ff4f698b7":{"changes":{"sklearn\/metrics\/pairwise.py":"MODIFY","sklearn\/neighbors\/dist_metrics.pyx":"MODIFY"},"diff":{"sklearn\/metrics\/pairwise.py":[{"add":["452","       D(x, y) = 2\\\\arcsin[\\\\sqrt{\\\\sin^2((x1 - y1) \/ 2)","453","                                + \\\\cos(x1)\\\\cos(y1)\\\\sin^2((x2 - y2) \/ 2)}]"],"delete":["452","       D(x, y) = 2\\arcsin[\\\\sqrt{\\\\sin^2((x1 - y1) \/ 2)","453","                                + cos(x1)cos(y1)sin^2((x2 - y2) \/ 2)}]"]}],"sklearn\/neighbors\/dist_metrics.pyx":[{"add":["982","       D(x, y) = 2\\\\arcsin[\\\\sqrt{\\\\sin^2((x1 - y1) \/ 2)","983","                                + \\\\cos(x1)\\\\cos(y1)\\\\sin^2((x2 - y2) \/ 2)}]"],"delete":["982","       D(x, y) = 2\\arcsin[\\sqrt{\\sin^2((x1 - y1) \/ 2)","983","                                + cos(x1)cos(y1)sin^2((x2 - y2) \/ 2)}]"]}]}},"19c8af6d6a9528162fd41d8a3363b807e337d63d":{"changes":{"sklearn\/metrics\/scorer.py":"MODIFY","sklearn\/metrics\/tests\/test_score_objects.py":"MODIFY","doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/metrics\/tests\/test_common.py":"MODIFY","doc\/modules\/classes.rst":"MODIFY","sklearn\/metrics\/classification.py":"MODIFY","sklearn\/metrics\/tests\/test_classification.py":"MODIFY","examples\/multioutput\/plot_classifier_chain_yeast.py":"MODIFY","doc\/modules\/model_evaluation.rst":"MODIFY","sklearn\/metrics\/__init__.py":"MODIFY","sklearn\/tests\/test_multioutput.py":"MODIFY"},"diff":{"sklearn\/metrics\/scorer.py":[{"add":["30","               brier_score_loss, jaccard_score)","533","                     ('recall', recall_score), ('f1', f1_score),","534","                     ('jaccard', jaccard_score)]:","535","    SCORERS[name] = make_scorer(metric, average='binary')"],"delete":["30","               brier_score_loss)","533","                     ('recall', recall_score), ('f1', f1_score)]:","534","    SCORERS[name] = make_scorer(metric)"]}],"sklearn\/metrics\/tests\/test_score_objects.py":[{"add":["19","                             log_loss, precision_score, recall_score,","20","                             jaccard_score)","54","               'neg_log_loss', 'log_loss', 'brier_score_loss',","55","               'jaccard', 'jaccard_weighted', 'jaccard_macro',","56","               'jaccard_micro']","68","MULTILABEL_ONLY_SCORERS = ['precision_samples', 'recall_samples', 'f1_samples',","69","                           'jaccard_samples']","290","                           ('recall', recall_score),","291","                           ('jaccard', jaccard_score)]:"],"delete":["7","","20","                             log_loss, precision_score, recall_score)","54","               'neg_log_loss', 'log_loss', 'brier_score_loss']","66","MULTILABEL_ONLY_SCORERS = ['precision_samples', 'recall_samples', 'f1_samples']","287","                           ('recall', recall_score)]:"]}],"doc\/whats_new\/v0.21.rst":[{"add":["295","- |Feature| :func:`metrics.jaccard_score` has been added to calculate the","296","  Jaccard coefficient as an evaluation metric for binary, multilabel and","297","  multiclass tasks, with an interface analogous to :func:`metrics.f1_score`.","298","  :issue:`13151` by :user:`Gaurav Dhingra <gxyd>` and `Joel Nothman`_.","299","","325","- |API| :func:`metrics.jaccard_similarity_score` is deprecated in favour of","326","  the more consistent :func:`metrics.jaccard_score`. The former behavior for","327","  binary and multiclass targets is broken.","328","  :issue:`13151` by `Joel Nothman`_.","329",""],"delete":[]}],"sklearn\/metrics\/tests\/test_common.py":[{"add":["39","from sklearn.metrics import jaccard_score","128","    \"jaccard_score\": jaccard_score,","141","    \"weighted_jaccard_score\": partial(jaccard_score, average=\"weighted\"),","148","    \"micro_jaccard_score\": partial(jaccard_score, average=\"micro\"),","155","    \"macro_jaccard_score\": partial(jaccard_score, average=\"macro\"),","162","    \"samples_jaccard_score\": partial(jaccard_score, average=\"samples\"),","250","    \"samples_jaccard_score\",","272","    \"jaccard_score\",","273","","292","    \"precision_score\", \"recall_score\", \"f1_score\", \"f2_score\", \"f0.5_score\",","293","    \"jaccard_score\"","309","    \"jaccard_score\",","339","    \"jaccard_score\",","343","    \"weighted_jaccard_score\",","347","    \"micro_jaccard_score\",","351","    \"macro_jaccard_score\",","352","","388","    \"weighted_jaccard_score\",","392","    \"macro_jaccard_score\",","396","    \"micro_jaccard_score\",","397","","402","    \"samples_jaccard_score\",","418","    \"micro_jaccard_score\", \"macro_jaccard_score\",","419","    \"jaccard_score\",","420","    \"samples_jaccard_score\",","421","","449","    \"weighted_precision_score\", \"weighted_jaccard_score\",","450","    \"unnormalized_multilabel_confusion_matrix\",","471","    y_true_bin = random_state.randint(0, 2, size=(20, 25))","472","    y_pred_bin = random_state.randint(0, 2, size=(20, 25))","473","","487","        if name in METRIC_UNDEFINED_BINARY:","488","            if name in MULTILABELS_METRICS:","489","                assert_allclose(metric(y_true_bin, y_pred_bin),","490","                                metric(y_pred_bin, y_true_bin),","491","                                err_msg=\"%s is not symmetric\" % name)","492","            else:","493","                assert False, \"This case is currently unhandled\"","494","        else:","495","            assert_allclose(metric(y_true, y_pred),","496","                            metric(y_pred, y_true),","497","                            err_msg=\"%s is not symmetric\" % name)"],"delete":["39","from sklearn.metrics import jaccard_similarity_score","124","    \"jaccard_similarity_score\": jaccard_similarity_score,","125","    \"unnormalized_jaccard_similarity_score\":","126","    partial(jaccard_similarity_score, normalize=False),","127","","288","    \"precision_score\", \"recall_score\", \"f1_score\", \"f2_score\", \"f0.5_score\"","351","    \"jaccard_similarity_score\",","374","    \"jaccard_similarity_score\", \"unnormalized_jaccard_similarity_score\",","402","    \"jaccard_similarity_score\", \"unnormalized_jaccard_similarity_score\",","432","    \"weighted_precision_score\", \"unnormalized_multilabel_confusion_matrix\",","466","        assert_allclose(metric(y_true, y_pred), metric(y_pred, y_true),","467","                        err_msg=\"%s is not symmetric\" % name)"]}],"doc\/modules\/classes.rst":[{"add":["846","   metrics.jaccard_score","1507","   metrics.jaccard_similarity_score"],"delete":["846","   metrics.jaccard_similarity_score"]}],"sklearn\/metrics\/classification.py":[{"add":["150","    jaccard_score, hamming_loss, zero_one_loss","155","    to the ``jaccard_score`` function.","583","    .. deprecated:: 0.21","584","        This is deprecated to be removed in 0.23, since its handling of","585","        binary and multiclass inputs was broken. `jaccard_score` has an API","586","        that is consistent with precision_score, f_score, etc.","631","    warnings.warn('jaccard_similarity_score has been deprecated and replaced '","632","                  'with jaccard_score. It will be removed in version 0.23. '","633","                  'This implementation has surprising behavior for binary '","634","                  'and multiclass classification tasks.', DeprecationWarning)","652","def jaccard_score(y_true, y_pred, labels=None, pos_label=1,","653","                  average='binary', sample_weight=None):","654","    \"\"\"Jaccard similarity coefficient score","655","","656","    The Jaccard index [1], or Jaccard similarity coefficient, defined as","657","    the size of the intersection divided by the size of the union of two label","658","    sets, is used to compare set of predicted labels for a sample to the","659","    corresponding set of labels in ``y_true``.","660","","661","    Read more in the :ref:`User Guide <jaccard_score>`.","662","","663","    Parameters","664","    ----------","665","    y_true : 1d array-like, or label indicator array \/ sparse matrix","666","        Ground truth (correct) labels.","667","","668","    y_pred : 1d array-like, or label indicator array \/ sparse matrix","669","        Predicted labels, as returned by a classifier.","670","","671","    labels : list, optional","672","        The set of labels to include when ``average != 'binary'``, and their","673","        order if ``average is None``. Labels present in the data can be","674","        excluded, for example to calculate a multiclass average ignoring a","675","        majority negative class, while labels not present in the data will","676","        result in 0 components in a macro average. For multilabel targets,","677","        labels are column indices. By default, all labels in ``y_true`` and","678","        ``y_pred`` are used in sorted order.","679","","680","    pos_label : str or int, 1 by default","681","        The class to report if ``average='binary'`` and the data is binary.","682","        If the data are multiclass or multilabel, this will be ignored;","683","        setting ``labels=[pos_label]`` and ``average != 'binary'`` will report","684","        scores for that label only.","685","","686","    average : string, [None, 'binary' (default), 'micro', 'macro', 'samples', \\","687","                       'weighted']","688","        If ``None``, the scores for each class are returned. Otherwise, this","689","        determines the type of averaging performed on the data:","690","","691","        ``'binary'``:","692","            Only report results for the class specified by ``pos_label``.","693","            This is applicable only if targets (``y_{true,pred}``) are binary.","694","        ``'micro'``:","695","            Calculate metrics globally by counting the total true positives,","696","            false negatives and false positives.","697","        ``'macro'``:","698","            Calculate metrics for each label, and find their unweighted","699","            mean.  This does not take label imbalance into account.","700","        ``'weighted'``:","701","            Calculate metrics for each label, and find their average, weighted","702","            by support (the number of true instances for each label). This","703","            alters 'macro' to account for label imbalance.","704","        ``'samples'``:","705","            Calculate metrics for each instance, and find their average (only","706","            meaningful for multilabel classification).","707","","708","    sample_weight : array-like of shape = [n_samples], optional","709","        Sample weights.","710","","711","    Returns","712","    -------","713","    score : float (if average is not None) or array of floats, shape =\\","714","            [n_unique_labels]","715","","716","    See also","717","    --------","718","    accuracy_score, f_score, multilabel_confusion_matrix","719","","720","    Notes","721","    -----","722","    :func:`jaccard_score` may be a poor metric if there are no","723","    positives for some samples or classes. Jaccard is undefined if there are","724","    no true or predicted labels, and our implementation will return a score","725","    of 0 with a warning.","726","","727","    References","728","    ----------","729","    .. [1] `Wikipedia entry for the Jaccard index","730","           <https:\/\/en.wikipedia.org\/wiki\/Jaccard_index>`_","731","","732","    Examples","733","    --------","734","    >>> import numpy as np","735","    >>> from sklearn.metrics import jaccard_score","736","    >>> y_true = np.array([[0, 1, 1],","737","    ...                    [1, 1, 0]])","738","    >>> y_pred = np.array([[1, 1, 1],","739","    ...                    [1, 0, 0]])","740","","741","    In the binary case:","742","","743","    >>> jaccard_score(y_true[0], y_pred[0])  # doctest: +ELLIPSIS","744","    0.6666...","745","","746","    In the multilabel case:","747","","748","    >>> jaccard_score(y_true, y_pred, average='samples')","749","    0.5833...","750","    >>> jaccard_score(y_true, y_pred, average='macro')","751","    0.6666...","752","    >>> jaccard_score(y_true, y_pred, average=None)","753","    array([0.5, 0.5, 1. ])","754","","755","    In the multiclass case:","756","","757","    >>> y_pred = [0, 2, 1, 2]","758","    >>> y_true = [0, 1, 2, 2]","759","    >>> jaccard_score(y_true, y_pred, average=None)","760","    ... # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS","761","    array([1. , 0. , 0.33...])","762","    \"\"\"","763","    labels = _check_set_wise_labels(y_true, y_pred, average, labels,","764","                                    pos_label)","765","    samplewise = average == 'samples'","766","    MCM = multilabel_confusion_matrix(y_true, y_pred,","767","                                      sample_weight=sample_weight,","768","                                      labels=labels, samplewise=samplewise)","769","    numerator = MCM[:, 1, 1]","770","    denominator = MCM[:, 1, 1] + MCM[:, 0, 1] + MCM[:, 1, 0]","771","","772","    if average == 'micro':","773","        numerator = np.array([numerator.sum()])","774","        denominator = np.array([denominator.sum()])","775","","776","    jaccard = _prf_divide(numerator, denominator, 'jaccard',","777","                          'true or predicted', average, ('jaccard',))","778","    if average is None:","779","        return jaccard","780","    if average == 'weighted':","781","        weights = MCM[:, 1, 0] + MCM[:, 1, 1]","782","        if not np.any(weights):","783","            # numerator is 0, and warning should have already been issued","784","            weights = None","785","    elif average == 'samples' and sample_weight is not None:","786","        weights = sample_weight","787","    else:","788","        weights = None","789","    return np.average(jaccard, weights=weights)","790","","791","","917","    accuracy_score, hamming_loss, jaccard_score","1028","    fbeta_score, precision_recall_fscore_support, jaccard_score,","1195","    denominator = denominator.copy()","1196","    denominator[mask] = 1  # avoid infs\/nans","1197","    result = numerator \/ denominator","1228","def _check_set_wise_labels(y_true, y_pred, average, labels, pos_label):","1229","    \"\"\"Validation associated with set-wise metrics","1230","","1231","    Returns identified labels","1232","    \"\"\"","1233","    average_options = (None, 'micro', 'macro', 'weighted', 'samples')","1234","    if average not in average_options and average != 'binary':","1235","        raise ValueError('average has to be one of ' +","1236","                         str(average_options))","1237","","1238","    y_type, y_true, y_pred = _check_targets(y_true, y_pred)","1239","    present_labels = unique_labels(y_true, y_pred)","1240","    if average == 'binary':","1241","        if y_type == 'binary':","1242","            if pos_label not in present_labels:","1243","                if len(present_labels) >= 2:","1244","                    raise ValueError(\"pos_label=%r is not a valid label: \"","1245","                                     \"%r\" % (pos_label, present_labels))","1246","            labels = [pos_label]","1247","        else:","1248","            average_options = list(average_options)","1249","            if y_type == 'multiclass':","1250","                average_options.remove('samples')","1251","            raise ValueError(\"Target is %s but average='binary'. Please \"","1252","                             \"choose another average setting, one of %r.\"","1253","                             % (y_type, average_options))","1254","    elif pos_label not in (None, 1):","1255","        warnings.warn(\"Note that pos_label (set to %r) is ignored when \"","1256","                      \"average != 'binary' (got %r). You may use \"","1257","                      \"labels=[pos_label] to specify a single positive class.\"","1258","                      % (pos_label, average), UserWarning)","1259","    return labels","1260","","1261","","1413","    labels = _check_set_wise_labels(y_true, y_pred, average, labels,","1414","                                    pos_label)","1433","    # Divide, and on zero-division, set scores to 0 and warn:","1434","","1435","    precision = _prf_divide(tp_sum, pred_sum,","1436","                            'precision', 'predicted', average, warn_for)","1437","    recall = _prf_divide(tp_sum, true_sum,","1438","                         'recall', 'true', average, warn_for)","1439","    # Don't need to warn for F: either P or R warned, or tp == 0 where pos","1440","    # and true are nonzero, in which case, F is well-defined and zero","1441","    denom = beta2 * precision + recall","1442","    denom[denom == 0.] = 1  # avoid division by 0","1443","    f_score = (1 + beta2) * precision * recall \/ denom","1985","    accuracy_score, jaccard_score, zero_one_loss"],"delete":["150","    jaccard_similarity_score, hamming_loss, zero_one_loss","155","    to the ``jaccard_similarity_score`` function.","583","    The Jaccard index [1], or Jaccard similarity coefficient, defined as","584","    the size of the intersection divided by the size of the union of two label","585","    sets, is used to compare set of predicted labels for a sample to the","586","    corresponding set of labels in ``y_true``.","630","","631","","632","    Examples","633","    --------","634","    >>> from sklearn.metrics import jaccard_similarity_score","635","    >>> y_pred = [0, 2, 1, 3]","636","    >>> y_true = [0, 1, 2, 3]","637","    >>> jaccard_similarity_score(y_true, y_pred)","638","    0.5","639","    >>> jaccard_similarity_score(y_true, y_pred, normalize=False)","640","    2","641","","642","    In the multilabel case with binary label indicators:","643","","644","    >>> import numpy as np","645","    >>> jaccard_similarity_score(np.array([[0, 1], [1, 1]]),\\","646","        np.ones((2, 2)))","647","    0.75","791","    accuracy_score, hamming_loss, jaccard_similarity_score","902","    fbeta_score, precision_recall_fscore_support, jaccard_similarity_score,","1068","    result = numerator \/ denominator","1073","    # remove infs","1074","    result[mask] = 0.0","1075","","1252","    average_options = (None, 'micro', 'macro', 'weighted', 'samples')","1253","    if average not in average_options and average != 'binary':","1254","        raise ValueError('average has to be one of ' +","1255","                         str(average_options))","1258","","1259","    y_type, y_true, y_pred = _check_targets(y_true, y_pred)","1260","    check_consistent_length(y_true, y_pred, sample_weight)","1261","    present_labels = unique_labels(y_true, y_pred)","1262","","1263","    if average == 'binary':","1264","        if y_type == 'binary':","1265","            if pos_label not in present_labels and len(present_labels) >= 2:","1266","                raise ValueError(\"pos_label=%r is not a valid label: %r\" %","1267","                                 (pos_label, present_labels))","1268","            labels = [pos_label]","1269","        else:","1270","            raise ValueError(\"Target is %s but average='binary'. Please \"","1271","                             \"choose another average setting.\" % y_type)","1272","    elif pos_label not in (None, 1):","1273","        warnings.warn(\"Note that pos_label (set to %r) is ignored when \"","1274","                      \"average != 'binary' (got %r). You may use \"","1275","                      \"labels=[pos_label] to specify a single positive class.\"","1276","                      % (pos_label, average), UserWarning)","1294","    with np.errstate(divide='ignore', invalid='ignore'):","1295","        # Divide, and on zero-division, set scores to 0 and warn:","1297","        # Oddly, we may get an \"invalid\" rather than a \"divide\" error","1298","        # here.","1299","        precision = _prf_divide(tp_sum, pred_sum,","1300","                                'precision', 'predicted', average, warn_for)","1301","        recall = _prf_divide(tp_sum, true_sum,","1302","                             'recall', 'true', average, warn_for)","1303","        # Don't need to warn for F: either P or R warned, or tp == 0 where pos","1304","        # and true are nonzero, in which case, F is well-defined and zero","1305","        f_score = ((1 + beta2) * precision * recall \/","1306","                   (beta2 * precision + recall))","1307","        f_score[tp_sum == 0] = 0.0","1849","    accuracy_score, jaccard_similarity_score, zero_one_loss"]}],"sklearn\/metrics\/tests\/test_classification.py":[{"add":["13","from sklearn.preprocessing import label_binarize, LabelBinarizer","39","from sklearn.metrics import jaccard_score","1143","def test_jaccard_score_validation():","1144","    y_true = np.array([0, 1, 0, 1, 1])","1145","    y_pred = np.array([0, 1, 0, 1, 1])","1146","    assert_raise_message(ValueError, \"pos_label=2 is not a valid label: \"","1147","                         \"array([0, 1])\", jaccard_score, y_true,","1148","                         y_pred, average='binary', pos_label=2)","1149","","1150","    y_true = np.array([[0, 1, 1], [1, 0, 0]])","1151","    y_pred = np.array([[1, 1, 1], [1, 0, 1]])","1152","    msg1 = (\"Target is multilabel-indicator but average='binary'. \"","1153","            \"Please choose another average setting, one of [None, \"","1154","            \"'micro', 'macro', 'weighted', 'samples'].\")","1155","    assert_raise_message(ValueError, msg1, jaccard_score, y_true,","1156","                         y_pred, average='binary', pos_label=-1)","1157","","1158","    y_true = np.array([0, 1, 1, 0, 2])","1159","    y_pred = np.array([1, 1, 1, 1, 0])","1160","    msg2 = (\"Target is multiclass but average='binary'. Please choose \"","1161","            \"another average setting, one of [None, 'micro', 'macro', \"","1162","            \"'weighted'].\")","1163","    assert_raise_message(ValueError, msg2, jaccard_score, y_true,","1164","                         y_pred, average='binary')","1165","    msg3 = (\"Samplewise metrics are not available outside of multilabel \"","1166","            \"classification.\")","1167","    assert_raise_message(ValueError, msg3, jaccard_score, y_true,","1168","                         y_pred, average='samples')","1169","","1170","    assert_warns_message(UserWarning,","1171","                         \"Note that pos_label (set to 3) is ignored when \"","1172","                         \"average != 'binary' (got 'micro'). You may use \"","1173","                         \"labels=[pos_label] to specify a single positive \"","1174","                         \"class.\", jaccard_score, y_true, y_pred,","1175","                         average='micro', pos_label=3)","1176","","1177","","1178","def test_multilabel_jaccard_score(recwarn):","1186","    assert jaccard_score(y1, y2, average='samples') == 0.75","1187","    assert jaccard_score(y1, y1, average='samples') == 1","1188","    assert jaccard_score(y2, y2, average='samples') == 1","1189","    assert jaccard_score(y2, np.logical_not(y2), average='samples') == 0","1190","    assert jaccard_score(y1, np.logical_not(y1), average='samples') == 0","1191","    assert jaccard_score(y1, np.zeros(y1.shape), average='samples') == 0","1192","    assert jaccard_score(y2, np.zeros(y1.shape), average='samples') == 0","1193","","1194","    y_true = np.array([[0, 1, 1], [1, 0, 0]])","1195","    y_pred = np.array([[1, 1, 1], [1, 0, 1]])","1196","    # average='macro'","1197","    assert_almost_equal(jaccard_score(y_true, y_pred,","1198","                                      average='macro'), 2. \/ 3)","1199","    # average='micro'","1200","    assert_almost_equal(jaccard_score(y_true, y_pred,","1201","                                      average='micro'), 3. \/ 5)","1202","    # average='samples'","1203","    assert_almost_equal(jaccard_score(y_true, y_pred, average='samples'),","1204","                        7. \/ 12)","1205","    assert_almost_equal(jaccard_score(y_true, y_pred,","1206","                                      average='samples',","1207","                                      labels=[0, 2]), 1. \/ 2)","1208","    assert_almost_equal(jaccard_score(y_true, y_pred,","1209","                                      average='samples',","1210","                                      labels=[1, 2]), 1. \/ 2)","1211","    # average=None","1212","    assert_array_equal(jaccard_score(y_true, y_pred, average=None),","1213","                       np.array([1. \/ 2, 1., 1. \/ 2]))","1214","","1215","    y_true = np.array([[0, 1, 1], [1, 0, 1]])","1216","    y_pred = np.array([[1, 1, 1], [1, 0, 1]])","1217","    assert_almost_equal(jaccard_score(y_true, y_pred,","1218","                                      average='macro'), 5. \/ 6)","1219","    # average='weighted'","1220","    assert_almost_equal(jaccard_score(y_true, y_pred,","1221","                                      average='weighted'), 7. \/ 8)","1222","","1223","    msg2 = 'Got 4 > 2'","1224","    assert_raise_message(ValueError, msg2, jaccard_score, y_true,","1225","                         y_pred, labels=[4], average='macro')","1226","    msg3 = 'Got -1 < 0'","1227","    assert_raise_message(ValueError, msg3, jaccard_score, y_true,","1228","                         y_pred, labels=[-1], average='macro')","1229","","1230","    msg = ('Jaccard is ill-defined and being set to 0.0 in labels '","1231","           'with no true or predicted samples.')","1232","    assert assert_warns_message(UndefinedMetricWarning, msg,","1233","                                jaccard_score,","1234","                                np.array([[0, 1]]),","1235","                                np.array([[0, 1]]),","1236","                                average='macro') == 0.5","1237","","1238","    msg = ('Jaccard is ill-defined and being set to 0.0 in samples '","1239","           'with no true or predicted labels.')","1240","    assert assert_warns_message(UndefinedMetricWarning, msg,","1241","                                jaccard_score,","1242","                                np.array([[0, 0], [1, 1]]),","1243","                                np.array([[0, 0], [1, 1]]),","1244","                                average='samples') == 0.5","1245","","1246","    assert not list(recwarn)","1247","","1248","","1249","def test_multiclass_jaccard_score(recwarn):","1250","    y_true = ['ant', 'ant', 'cat', 'cat', 'ant', 'cat', 'bird', 'bird']","1251","    y_pred = ['cat', 'ant', 'cat', 'cat', 'ant', 'bird', 'bird', 'cat']","1252","    labels = ['ant', 'bird', 'cat']","1253","    lb = LabelBinarizer()","1254","    lb.fit(labels)","1255","    y_true_bin = lb.transform(y_true)","1256","    y_pred_bin = lb.transform(y_pred)","1257","    multi_jaccard_score = partial(jaccard_score, y_true,","1258","                                  y_pred)","1259","    bin_jaccard_score = partial(jaccard_score,","1260","                                y_true_bin, y_pred_bin)","1261","    multi_labels_list = [['ant', 'bird'], ['ant', 'cat'], ['cat', 'bird'],","1262","                         ['ant'], ['bird'], ['cat'], None]","1263","    bin_labels_list = [[0, 1], [0, 2], [2, 1], [0], [1], [2], None]","1264","","1265","    # other than average='samples'\/'none-samples', test everything else here","1266","    for average in ('macro', 'weighted', 'micro', None):","1267","        for m_label, b_label in zip(multi_labels_list, bin_labels_list):","1268","            assert_almost_equal(multi_jaccard_score(average=average,","1269","                                                    labels=m_label),","1270","                                bin_jaccard_score(average=average,","1271","                                                  labels=b_label))","1272","","1273","    y_true = np.array([[0, 0], [0, 0], [0, 0]])","1274","    y_pred = np.array([[0, 0], [0, 0], [0, 0]])","1275","    with ignore_warnings():","1276","        assert (jaccard_score(y_true, y_pred, average='weighted')","1277","                == 0)","1278","","1279","    assert not list(recwarn)","1280","","1281","","1282","def test_average_binary_jaccard_score(recwarn):","1283","    # tp=0, fp=0, fn=1, tn=0","1284","    assert jaccard_score([1], [0], average='binary') == 0.","1285","    # tp=0, fp=0, fn=0, tn=1","1286","    msg = ('Jaccard is ill-defined and being set to 0.0 due to '","1287","           'no true or predicted samples')","1288","    assert assert_warns_message(UndefinedMetricWarning,","1289","                                msg,","1290","                                jaccard_score,","1291","                                [0, 0], [0, 0],","1292","                                average='binary') == 0.","1293","    # tp=1, fp=0, fn=0, tn=0 (pos_label=0)","1294","    assert jaccard_score([0], [0], pos_label=0,","1295","                         average='binary') == 1.","1296","    y_true = np.array([1, 0, 1, 1, 0])","1297","    y_pred = np.array([1, 0, 1, 1, 1])","1298","    assert_almost_equal(jaccard_score(y_true, y_pred,","1299","                                      average='binary'), 3. \/ 4)","1300","    assert_almost_equal(jaccard_score(y_true, y_pred,","1301","                                      average='binary',","1302","                                      pos_label=0), 1. \/ 2)","1303","","1304","    assert not list(recwarn)","1684","    msg_mc = (\"Target is multiclass but average='binary'. Please \"","1685","              \"choose another average setting, one of [\"","1686","              \"None, 'micro', 'macro', 'weighted'].\")","1689","    msg_ind = (\"Target is multilabel-indicator but average='binary'. Please \"","1690","               \"choose another average setting, one of [\"","1691","               \"None, 'micro', 'macro', 'weighted', 'samples'].\")","1693","    for y_true, y_pred, msg in [","1694","        (y_true_mc, y_pred_mc, msg_mc),","1695","        (y_true_ind, y_pred_ind, msg_ind),","1699","            assert_raise_message(ValueError, msg,","2025","","2026","","2027","def test_multilabel_jaccard_similarity_score_deprecation():","2028","    # Dense label indicator matrix format","2029","    y1 = np.array([[0, 1, 1], [1, 0, 1]])","2030","    y2 = np.array([[0, 0, 1], [1, 0, 1]])","2031","","2032","    # size(y1 \\inter y2) = [1, 2]","2033","    # size(y1 \\union y2) = [2, 2]","2034","","2035","    jss = partial(assert_warns, DeprecationWarning, jaccard_similarity_score)","2036","    assert_equal(jss(y1, y2), 0.75)","2037","    assert_equal(jss(y1, y1), 1)","2038","    assert_equal(jss(y2, y2), 1)","2039","    assert_equal(jss(y2, np.logical_not(y2)), 0)","2040","    assert_equal(jss(y1, np.logical_not(y1)), 0)","2041","    assert_equal(jss(y1, np.zeros(y1.shape)), 0)","2042","    assert_equal(jss(y2, np.zeros(y1.shape)), 0)"],"delete":["13","from sklearn.preprocessing import label_binarize","1142","def test_multilabel_jaccard_similarity_score():","1150","    assert_equal(jaccard_similarity_score(y1, y2), 0.75)","1151","    assert_equal(jaccard_similarity_score(y1, y1), 1)","1152","    assert_equal(jaccard_similarity_score(y2, y2), 1)","1153","    assert_equal(jaccard_similarity_score(y2, np.logical_not(y2)), 0)","1154","    assert_equal(jaccard_similarity_score(y1, np.logical_not(y1)), 0)","1155","    assert_equal(jaccard_similarity_score(y1, np.zeros(y1.shape)), 0)","1156","    assert_equal(jaccard_similarity_score(y2, np.zeros(y1.shape)), 0)","1539","    for y_true, y_pred, y_type in [","1540","        (y_true_mc, y_pred_mc, 'multiclass'),","1541","        (y_true_ind, y_pred_ind, 'multilabel-indicator'),","1545","            assert_raise_message(ValueError,","1546","                                 \"Target is %s but average='binary'. Please \"","1547","                                 \"choose another average setting.\" % y_type,"]}],"examples\/multioutput\/plot_classifier_chain_yeast.py":[{"add":["12",":ref:`jaccard score <jaccard_score>` for each sample.","43","from sklearn.metrics import jaccard_score","60","ovr_jaccard_score = jaccard_score(Y_test, Y_pred_ovr, average='samples')","71","chain_jaccard_scores = [jaccard_score(Y_test, Y_pred_chain >= .5,","72","                                      average='samples')","76","ensemble_jaccard_score = jaccard_score(Y_test,","77","                                       Y_pred_ensemble >= .5,","78","                                       average='samples')"],"delete":["12",":ref:`jaccard similarity score <jaccard_similarity_score>`.","43","from sklearn.metrics import jaccard_similarity_score","60","ovr_jaccard_score = jaccard_similarity_score(Y_test, Y_pred_ovr)","71","chain_jaccard_scores = [jaccard_similarity_score(Y_test, Y_pred_chain >= .5)","75","ensemble_jaccard_score = jaccard_similarity_score(Y_test,","76","                                                  Y_pred_ensemble >= .5)"]}],"doc\/modules\/model_evaluation.rst":[{"add":["72","'jaccard' etc.                    :func:`metrics.jaccard_score`                     suffixes apply as with 'f1'","329","   jaccard_score","349",".. _average:","350","","923",".. _jaccard_similarity_score:","924","","925","Jaccard similarity coefficient score","926","-------------------------------------","927","","928","The :func:`jaccard_score` function computes the average of `Jaccard similarity","929","coefficients <https:\/\/en.wikipedia.org\/wiki\/Jaccard_index>`_, also called the","930","Jaccard index, between pairs of label sets.","931","","932","The Jaccard similarity coefficient of the :math:`i`-th samples,","933","with a ground truth label set :math:`y_i` and predicted label set","934",":math:`\\hat{y}_i`, is defined as","935","","936",".. math::","937","","938","    J(y_i, \\hat{y}_i) = \\frac{|y_i \\cap \\hat{y}_i|}{|y_i \\cup \\hat{y}_i|}.","939","","940",":func:`jaccard_score` works like :func:`precision_recall_fscore_support` as a","941","naively set-wise measure applying natively to binary targets, and extended to","942","apply to multilabel and multiclass through the use of `average` (see","943",":ref:`above <average>`).","944","","945","In the binary case: ::","946","","947","  >>> import numpy as np","948","  >>> from sklearn.metrics import jaccard_score","949","  >>> y_true = np.array([[0, 1, 1],","950","  ...                    [1, 1, 0]])","951","  >>> y_pred = np.array([[1, 1, 1],","952","  ...                    [1, 0, 0]])","953","  >>> jaccard_score(y_true[0], y_pred[0])  # doctest: +ELLIPSIS","954","  0.6666...","955","","956","In the multilabel case with binary label indicators: ::","957","","958","  >>> jaccard_score(y_true, y_pred, average='samples')  # doctest: +ELLIPSIS","959","  0.5833...","960","  >>> jaccard_score(y_true, y_pred, average='macro')  # doctest: +ELLIPSIS","961","  0.6666...","962","  >>> jaccard_score(y_true, y_pred, average=None)","963","  array([0.5, 0.5, 1. ])","964","","965","Multiclass problems are binarized and treated like the corresponding","966","multilabel problem: ::","967","","968","  >>> y_pred = [0, 2, 1, 2]","969","  >>> y_true = [0, 1, 2, 2]","970","  >>> jaccard_score(y_true, y_pred, average=None)","971","  ... # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS","972","  array([1. , 0. , 0.33...])","973","  >>> jaccard_score(y_true, y_pred, average='macro')","974","  0.44...","975","  >>> jaccard_score(y_true, y_pred, average='micro')","976","  0.33...","977",""],"delete":["328","   jaccard_similarity_score","357",".. _average:","358","","682",".. _jaccard_similarity_score:","683","","684","Jaccard similarity coefficient score","685","-------------------------------------","686","","687","The :func:`jaccard_similarity_score` function computes the average (default)","688","or sum of `Jaccard similarity coefficients","689","<https:\/\/en.wikipedia.org\/wiki\/Jaccard_index>`_, also called the Jaccard index,","690","between pairs of label sets.","691","","692","The Jaccard similarity coefficient of the :math:`i`-th samples,","693","with a ground truth label set :math:`y_i` and predicted label set","694",":math:`\\hat{y}_i`, is defined as","695","","696",".. math::","697","","698","    J(y_i, \\hat{y}_i) = \\frac{|y_i \\cap \\hat{y}_i|}{|y_i \\cup \\hat{y}_i|}.","699","","700","In binary and multiclass classification, the Jaccard similarity coefficient","701","score is equal to the classification accuracy.","702","","703","::","704","","705","  >>> import numpy as np","706","  >>> from sklearn.metrics import jaccard_similarity_score","707","  >>> y_pred = [0, 2, 1, 3]","708","  >>> y_true = [0, 1, 2, 3]","709","  >>> jaccard_similarity_score(y_true, y_pred)","710","  0.5","711","  >>> jaccard_similarity_score(y_true, y_pred, normalize=False)","712","  2","713","","714","In the multilabel case with binary label indicators: ::","715","","716","  >>> jaccard_similarity_score(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))","717","  0.75","718",""]}],"sklearn\/metrics\/__init__.py":[{"add":["25","from .classification import jaccard_score","101","    'jaccard_score',"],"delete":[]}],"sklearn\/tests\/test_multioutput.py":[{"add":["25","from sklearn.metrics import jaccard_score, mean_squared_error","433","    assert_greater(jaccard_score(Y_test, Y_pred_chain, average='samples'),","434","                   jaccard_score(Y_test, Y_pred_ovr, average='samples'))","512","            assert jaccard_score(Y, Y_pred_cv, average='samples') > .4"],"delete":["25","from sklearn.metrics import jaccard_similarity_score, mean_squared_error","433","    assert_greater(jaccard_similarity_score(Y_test, Y_pred_chain),","434","                   jaccard_similarity_score(Y_test, Y_pred_ovr))","512","            assert jaccard_similarity_score(Y, Y_pred_cv) > .4"]}]}},"9328581ec5213e743fca116db06e7beef9a9da00":{"changes":{"examples\/decomposition\/plot_faces_decomposition.py":"MODIFY"},"diff":{"examples\/decomposition\/plot_faces_decomposition.py":[{"add":["156","                                                  fit_algorithm='cd',","163","                                                  fit_algorithm='cd',"],"delete":["38",""]}]}},"eb011b8c2ee2d335df101d648753cb1dc94631a9":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/tests\/test_dummy.py":"MODIFY","sklearn\/dummy.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["101","  ","102",":mod:`sklearn.dummy`","103","....................","104","","105","- |Fix| Fixed a bug in :class:`dummy.DummyClassifier` where the","106","  ``predict_proba`` method was returning int32 array instead of","107","  float64 for the ``stratified`` strategy. :issue:`13266` by","108","  :user:`Christos Aridas<chkoar>`."],"delete":[]}],"sklearn\/tests\/test_dummy.py":[{"add":["711","","712","","713","@pytest.mark.parametrize(","714","    \"strategy\", [\"stratified\", \"most_frequent\", \"prior\", \"uniform\", \"constant\"]","715",")","716","def test_dtype_of_classifier_probas(strategy):","717","    y = [0, 2, 1, 1]","718","    X = np.zeros(4)","719","    model = DummyClassifier(strategy=strategy, random_state=0, constant=0)","720","    probas = model.fit(X, y).predict_proba(X)","721","","722","    assert probas.dtype == np.float64"],"delete":[]}],"sklearn\/dummy.py":[{"add":["279","                out = out.astype(np.float64)"],"delete":[]}]}},"3a471242059187097f33f1539163dca9fbb0b21c":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/mixture\/tests\/test_gaussian_mixture.py":"MODIFY","sklearn\/mixture\/base.py":"MODIFY","sklearn\/mixture\/tests\/test_bayesian_mixture.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["307",":mod:`sklearn.mixture`","308","......................","309","","310","- |Fix| Fixed a bug in :class:`mixture.BaseMixture` and therefore on estimators","311","  based on it, i.e. :class:`mixture.GaussianMixture` and","312","  :class:`mixture.BayesianGaussianMixture`, where ``fit_predict`` and","313","  ``fit.predict`` were not equivalent. :issue:`13142` by","314","  :user:`J¨¦r¨¦mie du Boisberranger <jeremiedbb>`.","315",""],"delete":[]}],"sklearn\/mixture\/tests\/test_gaussian_mixture.py":[{"add":["600","def test_gaussian_mixture_fit_predict_n_init():","601","    # Check that fit_predict is equivalent to fit.predict, when n_init > 1","602","    X = np.random.RandomState(0).randn(1000, 5)","603","    gm = GaussianMixture(n_components=5, n_init=5, random_state=0)","604","    y_pred1 = gm.fit_predict(X)","605","    y_pred2 = gm.predict(X)","606","    assert_array_equal(y_pred1, y_pred2)","607","","608",""],"delete":[]}],"sklearn\/mixture\/base.py":[{"add":["270","        # Always do a final e-step to guarantee that the labels returned by","271","        # fit_predict(X) are always consistent with fit(X).predict(X)","272","        # for any value of max_iter and tol (and any random_state).","273","        _, log_resp = self._e_step(X)","274",""],"delete":["259","        # Always do a final e-step to guarantee that the labels returned by","260","        # fit_predict(X) are always consistent with fit(X).predict(X)","261","        # for any value of max_iter and tol (and any random_state).","262","        _, log_resp = self._e_step(X)","263",""]}],"sklearn\/mixture\/tests\/test_bayesian_mixture.py":[{"add":["453","def test_bayesian_mixture_fit_predict_n_init():","454","    # Check that fit_predict is equivalent to fit.predict, when n_init > 1","455","    X = np.random.RandomState(0).randn(1000, 5)","456","    gm = BayesianGaussianMixture(n_components=5, n_init=10, random_state=0)","457","    y_pred1 = gm.fit_predict(X)","458","    y_pred2 = gm.predict(X)","459","    assert_array_equal(y_pred1, y_pred2)","460","","461",""],"delete":[]}]}},"f483a70dba3d094c8b612d0cb820338745351e18":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/tests\/test_dummy.py":"MODIFY","sklearn\/dummy.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["162","- |Fix| Fixed a bug in :class:`dummy.DummyClassifier` where it was throwing a","163","  dimension mismatch error in prediction time if a column vector ``y`` with","164","  ``shape=(n, 1)`` was given at ``fit`` time. :issue:`13545` by :user:`Nick","165","  Sorros <nsorros>` and `Adrin Jalali`_.","166",""],"delete":[]}],"sklearn\/tests\/test_dummy.py":[{"add":["104","def test_most_frequent_and_prior_strategy_with_2d_column_y():","105","    # non-regression test added in","106","    # https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/13545","107","    X = [[0], [0], [0], [0]]","108","    y_1d = [1, 2, 1, 1]","109","    y_2d = [[1], [2], [1], [1]]","110","","111","    for strategy in (\"most_frequent\", \"prior\"):","112","        clf_1d = DummyClassifier(strategy=strategy, random_state=0)","113","        clf_2d = DummyClassifier(strategy=strategy, random_state=0)","114","","115","        clf_1d.fit(X, y_1d)","116","        clf_2d.fit(X, y_2d)","117","        assert_array_equal(clf_1d.predict(X), clf_2d.predict(X))","118","","119",""],"delete":[]}],"sklearn\/dummy.py":[{"add":["122","        self.output_2d_ = y.ndim == 2 and y.shape[1] > 1","123","","428","        self.output_2d_ = y.ndim == 2 and y.shape[1] > 1","429",""],"delete":["122","        self.output_2d_ = y.ndim == 2","427","        self.output_2d_ = y.ndim == 2"]}]}},"adc1e590d4dc1e230b49a4c10b4cd7b672bb3d69":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/covariance\/graph_lasso_.py":"MODIFY","sklearn\/covariance\/tests\/test_graphical_lasso.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["31",":mod:`sklearn.covariance`","32","......................","33","","34","- |Fix| Fixed a regression in :func:`covariance.graphical_lasso` so that","35","  the case `n_features=2` is handled correctly. :issue:`13276` by","36","  :user:`Aur¨¦lien Bellet <bellet>`.","37",""],"delete":[]}],"sklearn\/covariance\/graph_lasso_.py":[{"add":["206","        sub_covariance = np.copy(covariance_[1:, 1:], order='C')"],"delete":["206","        sub_covariance = np.ascontiguousarray(covariance_[1:, 1:])"]}],"sklearn\/covariance\/tests\/test_graphical_lasso.py":[{"add":["87","def test_graph_lasso_2D():","88","    # Hard-coded solution from Python skggm package","89","    # obtained by calling `quic(emp_cov, lam=.1, tol=1e-8)`","90","    cov_skggm = np.array([[3.09550269, 1.186972],","91","                         [1.186972, 0.57713289]])","92","","93","    icov_skggm = np.array([[1.52836773, -3.14334831],","94","                          [-3.14334831,  8.19753385]])","95","    X = datasets.load_iris().data[:, 2:]","96","    emp_cov = empirical_covariance(X)","97","    for method in ('cd', 'lars'):","98","        cov, icov = graphical_lasso(emp_cov, alpha=.1, return_costs=False,","99","                                    mode=method)","100","        assert_array_almost_equal(cov, cov_skggm)","101","        assert_array_almost_equal(icov, icov_skggm)","102","","103",""],"delete":[]}]}},"404387c067934ac46bc0901e9a15ae14974541c5":{"changes":{"doc\/governance.rst":"MODIFY","doc\/modules\/clustering.rst":"MODIFY","doc\/whats_new\/v0.19.rst":"MODIFY","doc\/about.rst":"MODIFY","doc\/glossary.rst":"MODIFY","doc\/modules\/computing.rst":"MODIFY"},"diff":{"doc\/governance.rst":[{"add":["27","\u2013 not only code \u2013 as detailed in the :ref:`contributors guide <contributing>`."],"delete":["27","\u2013 not only code \u2013 as detailed in the `contributors guide <contributing>`_."]}],"doc\/modules\/clustering.rst":[{"add":["120","The :class:`KMeans` algorithm clusters data by trying to separate samples in n","121","groups of equal variance, minimizing a criterion known as the *inertia* or","122","within-cluster sum-of-squares (see below). This algorithm requires the number","123","of clusters to be specified. It scales well to large number of samples and has","124","been used across a large range of application areas in many different fields.","126","The k-means algorithm divides a set of :math:`N` samples :math:`X` into","127",":math:`K` disjoint clusters :math:`C`, each described by the mean :math:`\\mu_j`","128","of the samples in the cluster. The means are commonly called the cluster","129","\"centroids\"; note that they are not, in general, points from :math:`X`,","131","","132","The K-means algorithm aims to choose centroids that minimise the **inertia**,","133","or **within-cluster sum-of-squares criterion**:","137","Inertia can be recognized as a measure of how internally coherent clusters are.","148","  Running a dimensionality reduction algorithm such as :ref:`PCA` prior to","149","  k-means clustering can alleviate this problem and speed up the","150","  computations."],"delete":["120","The :class:`KMeans` algorithm clusters data by trying to separate samples","121","in n groups of equal variance, minimizing a criterion known as the","122","`inertia <inertia>`_ or within-cluster sum-of-squares.","123","This algorithm requires the number of clusters to be specified.","124","It scales well to large number of samples and has been used","125","across a large range of application areas in many different fields.","127","The k-means algorithm divides a set of :math:`N` samples :math:`X`","128","into :math:`K` disjoint clusters :math:`C`,","129","each described by the mean :math:`\\mu_j` of the samples in the cluster.","130","The means are commonly called the cluster \"centroids\";","131","note that they are not, in general, points from :math:`X`,","133","The K-means algorithm aims to choose centroids","134","that minimise the *inertia*, or within-cluster sum of squared criterion:","138","Inertia, or the within-cluster sum of squares criterion,","139","can be recognized as a measure of how internally coherent clusters are.","150","  Running a dimensionality reduction algorithm such as `PCA <PCA>`_","151","  prior to k-means clustering can alleviate this problem","152","  and speed up the computations."]}],"doc\/whats_new\/v0.19.rst":[{"add":["229","  classification. By :user:`Adam Kleczewski <adamklec>`."],"delete":["229","  classification. By `Adam Kleczewski <adamklec>`_."]}],"doc\/about.rst":[{"add":["58","out in the :ref:`governance document <governance>`."],"delete":["58","out in the `governance document <governance>`_."]}],"doc\/glossary.rst":[{"add":["172","        `categorical-encoding","173","        <https:\/\/contrib.scikit-learn.org\/categorical-encoding>`_","174","        package for tools related to encoding categorical features."],"delete":["172","        `https:\/\/contrib.scikit-learn.org\/categorical-encoding\/","173","        <category_encoders>`_ package for tools related to encoding","174","        categorical features."]}],"doc\/modules\/computing.rst":[{"add":["502","  - :ref:`scikit-learn developer performance documentation <performance-howto>`"],"delete":["502","  - `scikit-learn developer performance documentation <..\/developers\/performance.html>`_"]}]}},"fa383a4aca0dc503f798c9576e934b11bfc1a6a4":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/tree\/tests\/test_export.py":"MODIFY","sklearn\/tree\/export.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["46",":mod:`sklearn.tree`","47","................................","48","","49","- |Fix| Fixed an issue with :func:`plot_tree` where it display","50","  entropy calculations even for `gini` criterion in DecisionTreeClassifiers.","51","  :pr:`13947` by :user:`Frank Hoang <fhoang7>`.","52","","974","Frank Hoang, Fibinse Xavier`, Finn O'Shea, Gabriel Marzinotto, Gabriel Vacaliuc, ","975","Gabriele Calvo, Gael Varoquaux, GauravAhlawat, Giuseppe Vettigli, Greg Gandenberger,"],"delete":["967","Fibinse Xavier`, Finn O'Shea, Gabriel Marzinotto, Gabriel Vacaliuc, Gabriele","968","Calvo, Gael Varoquaux, GauravAhlawat, Giuseppe Vettigli, Greg Gandenberger,"]}],"sklearn\/tree\/tests\/test_export.py":[{"add":["401","def test_plot_tree_entropy(pyplot):","403","    # Check correctness of export_graphviz for criterion = entropy","404","    clf = DecisionTreeClassifier(max_depth=3,","405","                                 min_samples_split=2,","406","                                 criterion=\"entropy\",","407","                                 random_state=2)","408","    clf.fit(X, y)","409","","410","    # Test export code","411","    feature_names = ['first feat', 'sepal_width']","412","    nodes = plot_tree(clf, feature_names=feature_names)","413","    assert len(nodes) == 3","414","    assert nodes[0].get_text() == (\"first feat <= 0.0\\nentropy = 1.0\\n\"","415","                                   \"samples = 6\\nvalue = [3, 3]\")","416","    assert nodes[1].get_text() == \"entropy = 0.0\\nsamples = 3\\nvalue = [3, 0]\"","417","    assert nodes[2].get_text() == \"entropy = 0.0\\nsamples = 3\\nvalue = [0, 3]\"","418","","419","","420","def test_plot_tree_gini(pyplot):","421","    # mostly smoke tests","422","    # Check correctness of export_graphviz for criterion = gini","433","    assert nodes[0].get_text() == (\"first feat <= 0.0\\ngini = 0.5\\n\"","435","    assert nodes[1].get_text() == \"gini = 0.0\\nsamples = 3\\nvalue = [3, 0]\"","436","    assert nodes[2].get_text() == \"gini = 0.0\\nsamples = 3\\nvalue = [0, 3]\""],"delete":["401","def test_plot_tree(pyplot):","403","    # Check correctness of export_graphviz","414","    assert nodes[0].get_text() == (\"first feat <= 0.0\\nentropy = 0.5\\n\"","416","    assert nodes[1].get_text() == \"entropy = 0.0\\nsamples = 3\\nvalue = [3, 0]\"","417","    assert nodes[2].get_text() == \"entropy = 0.0\\nsamples = 3\\nvalue = [0, 3]\""]}],"sklearn\/tree\/export.py":[{"add":["549","    def _make_tree(self, node_id, et, criterion, depth=0):","552","        name = self.node_to_str(et, node_id, criterion=criterion)","556","                                        criterion, depth=depth + 1),","558","                                        criterion, depth=depth + 1)]","570","        my_tree = self._make_tree(0, decision_tree.tree_,","571","                                  decision_tree.criterion)"],"delete":["549","    def _make_tree(self, node_id, et, depth=0):","552","        name = self.node_to_str(et, node_id, criterion='entropy')","556","                                        depth=depth + 1),","558","                                        depth=depth + 1)]","570","        my_tree = self._make_tree(0, decision_tree.tree_)"]}]}},"1015caf54df347c00152c67adaf1851c5771f0a0":{"changes":{"sklearn\/neighbors\/tests\/test_neighbors.py":"MODIFY","sklearn\/tests\/test_site_joblib.py":"MODIFY","sklearn\/utils\/testing.py":"MODIFY","sklearn\/covariance\/graph_lasso_.py":"MODIFY","sklearn\/datasets\/rcv1.py":"MODIFY","sklearn\/multioutput.py":"MODIFY","sklearn\/ensemble\/base.py":"MODIFY","sklearn\/model_selection\/_search.py":"MODIFY","sklearn\/datasets\/species_distributions.py":"MODIFY","sklearn\/datasets\/california_housing.py":"MODIFY","sklearn\/datasets\/olivetti_faces.py":"MODIFY","sklearn\/linear_model\/logistic.py":"MODIFY","sklearn\/ensemble\/tests\/test_forest.py":"MODIFY","sklearn\/linear_model\/omp.py":"MODIFY","sklearn\/cluster\/mean_shift_.py":"MODIFY","sklearn\/ensemble\/forest.py":"MODIFY","sklearn\/metrics\/tests\/test_score_objects.py":"MODIFY","sklearn\/tests\/test_pipeline.py":"MODIFY","sklearn\/datasets\/covtype.py":"MODIFY","sklearn\/metrics\/pairwise.py":"MODIFY","sklearn\/utils\/validation.py":"MODIFY","sklearn\/neighbors\/base.py":"MODIFY","sklearn\/pipeline.py":"MODIFY","sklearn\/linear_model\/tests\/test_sgd.py":"MODIFY","sklearn\/multiclass.py":"MODIFY","sklearn\/compose\/_column_transformer.py":"MODIFY","sklearn\/datasets\/lfw.py":"MODIFY","sklearn\/utils\/fixes.py":"MODIFY","sklearn\/utils\/tests\/test_fixes.py":"MODIFY","sklearn\/linear_model\/coordinate_descent.py":"MODIFY","sklearn\/decomposition\/online_lda.py":"MODIFY","sklearn\/manifold\/mds.py":"MODIFY","sklearn\/datasets\/kddcup99.py":"MODIFY","sklearn\/neighbors\/tests\/test_kde.py":"MODIFY","sklearn\/utils\/tests\/test_estimator_checks.py":"MODIFY","sklearn\/ensemble\/partial_dependence.py":"MODIFY","sklearn\/ensemble\/voting.py":"MODIFY","sklearn\/feature_selection\/rfe.py":"MODIFY","sklearn\/linear_model\/theil_sen.py":"MODIFY","sklearn\/datasets\/twenty_newsgroups.py":"MODIFY","sklearn\/utils\/estimator_checks.py":"MODIFY","sklearn\/decomposition\/tests\/test_sparse_pca.py":"MODIFY","sklearn\/tests\/test_multioutput.py":"MODIFY","sklearn\/linear_model\/base.py":"MODIFY","sklearn\/ensemble\/bagging.py":"MODIFY","sklearn\/ensemble\/tests\/test_bagging.py":"MODIFY","sklearn\/linear_model\/stochastic_gradient.py":"MODIFY","sklearn\/model_selection\/_validation.py":"MODIFY","sklearn\/decomposition\/dict_learning.py":"MODIFY","sklearn\/cluster\/k_means_.py":"MODIFY","sklearn\/linear_model\/least_angle.py":"MODIFY"},"diff":{"sklearn\/neighbors\/tests\/test_neighbors.py":[{"add":["29","import joblib","1352","    with joblib.parallel_backend(backend):"],"delete":["29","from sklearn.utils._joblib import joblib","30","from sklearn.utils._joblib import parallel_backend","1353","    with parallel_backend(backend):"]}],"sklearn\/tests\/test_site_joblib.py":[{"add":[],"delete":["0","from sklearn.utils._joblib import Parallel, delayed  # noqa","1","from sklearn.utils._joblib import Memory, parallel_backend # noqa"]}],"sklearn\/utils\/testing.py":[{"add":["45","import joblib"],"delete":["50","from sklearn.utils._joblib import joblib"]}],"sklearn\/covariance\/graph_lasso_.py":[{"add":["15","from joblib import Parallel, delayed"],"delete":["24","from ..utils._joblib import Parallel, delayed"]}],"sklearn\/datasets\/rcv1.py":[{"add":["18","import joblib","183","        joblib.dump(X, samples_path, compress=9)","184","        joblib.dump(sample_id, sample_id_path, compress=9)","191","        X = joblib.load(samples_path)","192","        sample_id = joblib.load(sample_id_path)","242","        joblib.dump(y, sample_topics_path, compress=9)","243","        joblib.dump(categories, topics_path, compress=9)","245","        y = joblib.load(sample_topics_path)","246","        categories = joblib.load(topics_path)"],"delete":["23","from ..utils import _joblib","183","        _joblib.dump(X, samples_path, compress=9)","184","        _joblib.dump(sample_id, sample_id_path, compress=9)","191","        X = _joblib.load(samples_path)","192","        sample_id = _joblib.load(sample_id_path)","242","        _joblib.dump(y, sample_topics_path, compress=9)","243","        _joblib.dump(categories, topics_path, compress=9)","245","        y = _joblib.load(sample_topics_path)","246","        categories = _joblib.load(topics_path)"]}],"sklearn\/multioutput.py":[{"add":["18","from joblib import Parallel, delayed","19",""],"delete":["27","from .utils._joblib import Parallel, delayed"]}],"sklearn\/ensemble\/base.py":[{"add":["10","from joblib import effective_n_jobs","11",""],"delete":["14","from ..utils._joblib import effective_n_jobs"]}],"sklearn\/model_selection\/_search.py":[{"add":["31","from joblib import Parallel, delayed"],"delete":["31","from ..utils._joblib import Parallel, delayed"]}],"sklearn\/datasets\/species_distributions.py":[{"add":["46","import joblib","47","","259","        joblib.dump(bunch, archive_path, compress=9)","261","        bunch = joblib.load(archive_path)"],"delete":["43","","52","from ..utils import _joblib","259","        _joblib.dump(bunch, archive_path, compress=9)","261","        bunch = _joblib.load(archive_path)"]}],"sklearn\/datasets\/california_housing.py":[{"add":["30","import joblib","31","","127","            joblib.dump(cal_housing, filepath, compress=6)","131","        cal_housing = joblib.load(filepath)"],"delete":["35","from ..utils import _joblib","126","            _joblib.dump(cal_housing, filepath, compress=6)","130","        cal_housing = _joblib.load(filepath)"]}],"sklearn\/datasets\/olivetti_faces.py":[{"add":["20","import joblib","106","        joblib.dump(faces, filepath, compress=6)","109","        faces = joblib.load(filepath)"],"delete":["25","from ..utils import _joblib","106","        _joblib.dump(faces, filepath, compress=6)","109","        faces = _joblib.load(filepath)"]}],"sklearn\/linear_model\/logistic.py":[{"add":["18","from joblib import Parallel, delayed, effective_n_jobs"],"delete":["35","from ..utils._joblib import Parallel, delayed, effective_n_jobs"]}],"sklearn\/ensemble\/tests\/test_forest.py":[{"add":["25","import joblib","1281","joblib.register_parallel_backend('testing', MyBackend)","1284","@pytest.mark.skipif(joblib.__version__ < LooseVersion('0.12'),","1290","    with joblib.parallel_backend(\"testing\") as (ba, n_jobs):","1296","    with joblib.parallel_backend(\"testing\") as (ba, _):"],"delete":["25","from sklearn.utils._joblib import joblib","26","from sklearn.utils._joblib import parallel_backend","27","from sklearn.utils._joblib import register_parallel_backend","28","from sklearn.utils._joblib import __version__ as __joblib_version__","1284","register_parallel_backend('testing', MyBackend)","1287","@pytest.mark.skipif(__joblib_version__ < LooseVersion('0.12'),","1293","    with parallel_backend(\"testing\") as (ba, n_jobs):","1299","    with parallel_backend(\"testing\") as (ba, _):"]}],"sklearn\/linear_model\/omp.py":[{"add":["13","from joblib import Parallel, delayed"],"delete":["18","from ..utils._joblib import Parallel, delayed"]}],"sklearn\/cluster\/mean_shift_.py":[{"add":["18","from joblib import Parallel, delayed"],"delete":["25","from ..utils._joblib import Parallel","26","from ..utils._joblib import delayed"]}],"sklearn\/ensemble\/forest.py":[{"add":["49","from joblib import Parallel, delayed"],"delete":["51","from ..utils._joblib import Parallel, delayed"]}],"sklearn\/metrics\/tests\/test_score_objects.py":[{"add":["8","import joblib","101","    joblib.dump((X, y, y_ml), filename)","102","    X_mm, y_mm, y_ml_mm = joblib.load(filename, mmap_mode='r')"],"delete":["39","from sklearn.utils import _joblib","101","    _joblib.dump((X, y, y_ml), filename)","102","    X_mm, y_mm, y_ml_mm = _joblib.load(filename, mmap_mode='r')"]}],"sklearn\/tests\/test_pipeline.py":[{"add":["13","import joblib","1031","        if LooseVersion(joblib.__version__) < LooseVersion('0.12'):","1033","            memory = joblib.Memory(cachedir=cachedir, verbose=10)","1035","            memory = joblib.Memory(location=cachedir, verbose=10)","1093","    if LooseVersion(joblib.__version__) < LooseVersion('0.12'):","1095","        memory = joblib.Memory(cachedir=cachedir, verbose=10)","1097","        memory = joblib.Memory(location=cachedir, verbose=10)"],"delete":["37","from sklearn.utils._joblib import Memory","38","from sklearn.utils._joblib import __version__ as joblib_version","1032","        if LooseVersion(joblib_version) < LooseVersion('0.12'):","1034","            memory = Memory(cachedir=cachedir, verbose=10)","1036","            memory = Memory(location=cachedir, verbose=10)","1094","    if LooseVersion(joblib_version) < LooseVersion('0.12'):","1096","        memory = Memory(cachedir=cachedir, verbose=10)","1098","        memory = Memory(location=cachedir, verbose=10)"]}],"sklearn\/datasets\/covtype.py":[{"add":["22","import joblib","119","        joblib.dump(X, samples_path, compress=9)","120","        joblib.dump(y, targets_path, compress=9)","127","        X = joblib.load(samples_path)","128","        y = joblib.load(targets_path)"],"delete":["28","from ..utils import _joblib","119","        _joblib.dump(X, samples_path, compress=9)","120","        _joblib.dump(y, targets_path, compress=9)","127","        X = _joblib.load(samples_path)","128","        y = _joblib.load(targets_path)"]}],"sklearn\/metrics\/pairwise.py":[{"add":["19","from joblib import Parallel, delayed, effective_n_jobs"],"delete":["27","from ..utils._joblib import Parallel","28","from ..utils._joblib import delayed","29","from ..utils._joblib import effective_n_jobs"]}],"sklearn\/utils\/validation.py":[{"add":["19","import joblib","177","        if LooseVersion(joblib.__version__) < '0.12':","178","            memory = joblib.Memory(cachedir=memory, verbose=0)","180","            memory = joblib.Memory(location=memory, verbose=0)"],"delete":["25","from ._joblib import Memory","26","from ._joblib import __version__ as joblib_version","178","        if LooseVersion(joblib_version) < '0.12':","179","            memory = Memory(cachedir=memory, verbose=0)","181","            memory = Memory(location=memory, verbose=0)"]}],"sklearn\/neighbors\/base.py":[{"add":["17","import joblib","18","from joblib import Parallel, delayed, effective_n_jobs","441","            old_joblib = (","442","                    LooseVersion(joblib.__version__) < LooseVersion('0.12'))","738","            if LooseVersion(joblib.__version__) < LooseVersion('0.12'):"],"delete":["27","from ..utils._joblib import Parallel, delayed, effective_n_jobs","28","from ..utils._joblib import __version__ as joblib_version","441","            old_joblib = LooseVersion(joblib_version) < LooseVersion('0.12')","737","            if LooseVersion(joblib_version) < LooseVersion('0.12'):"]}],"sklearn\/pipeline.py":[{"add":["16","from joblib import Parallel, delayed"],"delete":["18","from .utils._joblib import Parallel, delayed"]}],"sklearn\/linear_model\/tests\/test_sgd.py":[{"add":["6","import joblib","1594","    if joblib.__version__ < LooseVersion('0.12') and backend == 'loky':","1615","    with joblib.parallel_backend(backend=backend):"],"delete":["27","from sklearn.utils import _joblib","28","from sklearn.utils._joblib import parallel_backend","29","","1596","    if _joblib.__version__ < LooseVersion('0.12') and backend == 'loky':","1617","    with parallel_backend(backend=backend):"]}],"sklearn\/multiclass.py":[{"add":["55","from joblib import Parallel, delayed"],"delete":["55","from .utils._joblib import Parallel","56","from .utils._joblib import delayed"]}],"sklearn\/compose\/_column_transformer.py":[{"add":["13","from joblib import Parallel, delayed"],"delete":["15","from ..utils._joblib import Parallel, delayed"]}],"sklearn\/datasets\/lfw.py":[{"add":["17","import joblib","18","from joblib import Memory","305","    if LooseVersion(joblib.__version__) < LooseVersion('0.12'):","476","    if LooseVersion(joblib.__version__) < LooseVersion('0.12'):"],"delete":["20","from ..utils._joblib import Memory","21","from ..utils import _joblib","305","    if LooseVersion(_joblib.__version__) < LooseVersion('0.12'):","476","    if LooseVersion(_joblib.__version__) < LooseVersion('0.12'):"]}],"sklearn\/utils\/fixes.py":[{"add":["220","    import joblib","222","    if joblib.__version__ >= LooseVersion('0.12'):","228","                                  % (list(extra_args), joblib.__version__))"],"delete":["220","    from . import _joblib","222","    if _joblib.__version__ >= LooseVersion('0.12'):","228","                                  % (list(extra_args), _joblib.__version__))"]}],"sklearn\/utils\/tests\/test_fixes.py":[{"add":["29","    import joblib","30","    monkeypatch.setattr(joblib, '__version__', joblib_version)"],"delete":["29","    import sklearn.utils._joblib","30","    monkeypatch.setattr(sklearn.utils._joblib, '__version__', joblib_version)"]}],"sklearn\/linear_model\/coordinate_descent.py":[{"add":["13","from joblib import Parallel, delayed, effective_n_jobs"],"delete":["20","from ..utils._joblib import Parallel, delayed, effective_n_jobs"]}],"sklearn\/decomposition\/online_lda.py":[{"add":["16","from joblib import Parallel, delayed, effective_n_jobs"],"delete":["23","from ..utils._joblib import Parallel, delayed, effective_n_jobs"]}],"sklearn\/manifold\/mds.py":[{"add":["8","from joblib import Parallel, delayed, effective_n_jobs"],"delete":["14","from ..utils._joblib import Parallel","15","from ..utils._joblib import delayed","16","from ..utils._joblib import effective_n_jobs"]}],"sklearn\/datasets\/kddcup99.py":[{"add":["17","import joblib","285","        joblib.dump(X, samples_path, compress=0)","286","        joblib.dump(y, targets_path, compress=0)","294","        X = joblib.load(samples_path)","295","        y = joblib.load(targets_path)"],"delete":["17","","23","from ..utils import _joblib","286","        _joblib.dump(X, samples_path, compress=0)","287","        _joblib.dump(y, targets_path, compress=0)","295","        X = _joblib.load(samples_path)","296","        y = _joblib.load(targets_path)"]}],"sklearn\/neighbors\/tests\/test_kde.py":[{"add":["12","import joblib","221","    joblib.dump(kde, file_path)","222","    kde = joblib.load(file_path)"],"delete":["12","from sklearn.utils import _joblib","221","    _joblib.dump(kde, file_path)","222","    kde = _joblib.load(file_path)"]}],"sklearn\/utils\/tests\/test_estimator_checks.py":[{"add":["5","import joblib","441","            old_hash = joblib.hash(est)","443","        assert_equal(old_hash, joblib.hash(est))","452","            old_hash = joblib.hash(est)","454","        assert_equal(old_hash, joblib.hash(est))"],"delete":["10","from sklearn.utils import _joblib","441","            old_hash = _joblib.hash(est)","443","        assert_equal(old_hash, _joblib.hash(est))","452","            old_hash = _joblib.hash(est)","454","        assert_equal(old_hash, _joblib.hash(est))"]}],"sklearn\/ensemble\/partial_dependence.py":[{"add":["14","from joblib import Parallel, delayed"],"delete":["16","from ..utils._joblib import Parallel, delayed"]}],"sklearn\/ensemble\/voting.py":[{"add":["18","from joblib import Parallel, delayed","19",""],"delete":["23","from ..utils._joblib import Parallel, delayed"]}],"sklearn\/feature_selection\/rfe.py":[{"add":["9","from joblib import Parallel, delayed, effective_n_jobs","10",""],"delete":["17","from ..utils._joblib import Parallel, delayed, effective_n_jobs"]}],"sklearn\/linear_model\/theil_sen.py":[{"add":["17","from joblib import Parallel, delayed, effective_n_jobs"],"delete":["22","from ..utils._joblib import Parallel, delayed, effective_n_jobs"]}],"sklearn\/datasets\/twenty_newsgroups.py":[{"add":["37","import joblib","400","        X_train, X_test = joblib.load(target_file)","405","        joblib.dump((X_train, X_test), target_file, compress=9)"],"delete":["45","from ..utils import _joblib","400","        X_train, X_test = _joblib.load(target_file)","405","        _joblib.dump((X_train, X_test), target_file, compress=9)"]}],"sklearn\/utils\/estimator_checks.py":[{"add":["12","import joblib","2058","        assert_equal(joblib.hash(new_value), joblib.hash(original_value),","2222","                           np.float64, types.FunctionType, joblib.Memory])"],"delete":["14","from . import _joblib","2058","        assert_equal(_joblib.hash(new_value), _joblib.hash(original_value),","2222","                           np.float64, types.FunctionType, _joblib.Memory])"]}],"sklearn\/decomposition\/tests\/test_sparse_pca.py":[{"add":["144","        import joblib","145","        _mp = joblib.parallel.multiprocessing","146","        joblib.parallel.multiprocessing = None","152","            joblib.parallel.multiprocessing = _mp"],"delete":["144","        import sklearn.utils._joblib.parallel as joblib_par","145","        _mp = joblib_par.multiprocessing","146","        joblib_par.multiprocessing = None","152","            joblib_par.multiprocessing = _mp"]}],"sklearn\/tests\/test_multioutput.py":[{"add":["4","from joblib import cpu_count"],"delete":["19","from sklearn.utils._joblib import cpu_count"]}],"sklearn\/linear_model\/base.py":[{"add":["24","from joblib import Parallel, delayed"],"delete":["25","from ..utils._joblib import Parallel, delayed"]}],"sklearn\/ensemble\/bagging.py":[{"add":["12","from joblib import Parallel, delayed","13",""],"delete":["14","from ..utils._joblib import Parallel, delayed"]}],"sklearn\/ensemble\/tests\/test_bagging.py":[{"add":["8","import joblib","225","        self.training_hash_ = joblib.hash(X)"],"delete":["34","from sklearn.utils import _joblib","225","        self.training_hash_ = _joblib.hash(X)"]}],"sklearn\/linear_model\/stochastic_gradient.py":[{"add":["11","from joblib import Parallel, delayed"],"delete":["11","from ..utils._joblib import Parallel, delayed"]}],"sklearn\/model_selection\/_validation.py":[{"add":["19","from joblib import Parallel, delayed"],"delete":["25","from ..utils._joblib import Parallel, delayed"]}],"sklearn\/decomposition\/dict_learning.py":[{"add":["13","from joblib import Parallel, delayed, effective_n_jobs"],"delete":["15","from ..utils._joblib import Parallel, delayed, effective_n_jobs"]}],"sklearn\/cluster\/k_means_.py":[{"add":["17","from joblib import Parallel, delayed, effective_n_jobs"],"delete":["30","from ..utils._joblib import Parallel","31","from ..utils._joblib import delayed","32","from ..utils._joblib import effective_n_jobs"]}],"sklearn\/linear_model\/least_angle.py":[{"add":["17","from joblib import Parallel, delayed"],"delete":["23","from ..utils._joblib import Parallel, delayed"]}]}},"372092cfac5186496490772caac626ed222b06c3":{"changes":{"doc\/modules\/clustering.rst":"MODIFY","sklearn\/manifold\/tests\/test_spectral_embedding.py":"MODIFY","sklearn\/manifold\/spectral_embedding_.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY"},"diff":{"doc\/modules\/clustering.rst":[{"add":["436",":class:`SpectralClustering` performs a low-dimension embedding of the","437","affinity matrix between samples, followed by clustering, e.g., by KMeans,","438","of the components of the eigenvectors in the low dimensional space.","439","It is especially computationally efficient if the affinity matrix is sparse","440","and the `amg` solver is used for the eigenvalue problem (Note, the `amg` solver","441","requires that the `pyamg <https:\/\/github.com\/pyamg\/pyamg>`_ module is installed.)","443","The present version of SpectralClustering requires the number of clusters","444","to be specified in advance. It works well for a small number of clusters,","445","but is not advised for many clusters.","446","","447","For two clusters, SpectralClustering solves a convex relaxation of the","448","`normalised cuts <https:\/\/people.eecs.berkeley.edu\/~malik\/papers\/SM-ncut.pdf>`_","449","problem on the similarity graph: cutting the graph in two so that the weight of","450","the edges cut is small compared to the weights of the edges inside each","451","cluster. This criteria is especially interesting when working on images, where","452","graph vertices are pixels, and weights of the edges of the similarity graph are","453","computed using a function of a gradient of the image.","500","``\"kmeans\"`` strategy can match finer details, but can be unstable.","501","In particular, unless you control the ``random_state``, it may not be","502","reproducible from run-to-run, as it depends on random initialization.","503","The alternative ``\"discretize\"`` strategy is 100% reproducible, but tends","504","to create parcels of fairly even and geometrical shape.","515","Spectral Clustering can also be used to partition graphs via their spectral","542"," * `\"Preconditioned Spectral Clustering for Stochastic","543","   Block Partition Streaming Graph Challenge\"","544","   <https:\/\/arxiv.org\/abs\/1708.07481>`_","545","   David Zhuzhunashvili, Andrew Knyazev"],"delete":["436",":class:`SpectralClustering` does a low-dimension embedding of the","437","affinity matrix between samples, followed by a KMeans in the low","438","dimensional space. It is especially efficient if the affinity matrix is","439","sparse and the `pyamg <https:\/\/github.com\/pyamg\/pyamg>`_ module is installed.","440","SpectralClustering requires the number of clusters to be specified. It","441","works well for a small number of clusters but is not advised when using","442","many clusters.","444","For two clusters, it solves a convex relaxation of the `normalised","445","cuts <https:\/\/people.eecs.berkeley.edu\/~malik\/papers\/SM-ncut.pdf>`_ problem on","446","the similarity graph: cutting the graph in two so that the weight of the","447","edges cut is small compared to the weights of the edges inside each","448","cluster. This criteria is especially interesting when working on images:","449","graph vertices are pixels, and edges of the similarity graph are a","450","function of the gradient of the image.","497","The ``\"kmeans\"`` strategy can match finer details of the data, but it can be","498","more unstable. In particular, unless you control the ``random_state``, it","499","may not be reproducible from run-to-run, as it depends on a random","500","initialization. On the other hand, the ``\"discretize\"`` strategy is 100%","501","reproducible, but it tends to create parcels of fairly even and","502","geometrical shape.","513","Spectral Clustering can also be used to cluster graphs by their spectral"]}],"sklearn\/manifold\/tests\/test_spectral_embedding.py":[{"add":["164","    pytest.importorskip('pyamg')","192","def test_spectral_embedding_amg_solver_failure(seed=36):","193","    # Test spectral embedding with amg solver failure, see issue #13393","194","    pytest.importorskip('pyamg')","195","","196","    # The generated graph below is NOT fully connected if n_neighbors=3","197","    n_samples = 200","198","    n_clusters = 3","199","    n_features = 3","200","    centers = np.eye(n_clusters, n_features)","201","    S, true_labels = make_blobs(n_samples=n_samples, centers=centers,","202","                                cluster_std=1., random_state=42)","203","","204","    se_amg0 = SpectralEmbedding(n_components=3, affinity=\"nearest_neighbors\",","205","                                eigen_solver=\"amg\", n_neighbors=3,","206","                                random_state=np.random.RandomState(seed))","207","    embed_amg0 = se_amg0.fit_transform(S)","208","","209","    for i in range(10):","210","        se_amg0.set_params(random_state=np.random.RandomState(seed + 1))","211","        embed_amg1 = se_amg0.fit_transform(S)","212","","213","        assert _check_with_col_sign_flipping(embed_amg0, embed_amg1, 0.05)","214","","215","","216","@pytest.mark.filterwarnings(\"ignore:the behavior of nmi will \"","217","                            \"change in version 0.22\")"],"delete":["164","    try:","165","        from pyamg import smoothed_aggregation_solver  # noqa","166","    except ImportError:","167","        raise SkipTest(\"pyamg not available.\")"]}],"sklearn\/manifold\/spectral_embedding_.py":[{"add":["291","","292","        # The Laplacian matrix is always singular, having at least one zero","293","        # eigenvalue, corresponding to the trivial eigenvector, which is a","294","        # constant. Using a singular matrix for preconditioning may result in","295","        # random failures in LOBPCG and is not supported by the existing","296","        # theory:","297","        #     see https:\/\/doi.org\/10.1007\/s10208-015-9297-1","298","        # Shift the Laplacian so its diagononal is not all ones. The shift","299","        # does change the eigenpairs however, so we'll feed the shifted","300","        # matrix to the solver and afterward set it back to the original.","301","        diag_shift = 1e-5 * sparse.eye(laplacian.shape[0])","302","        laplacian += diag_shift","304","        laplacian -= diag_shift","305","","309","        _, diffusion_map = lobpcg(laplacian, X, M=M, tol=1.e-5,","391","        to be installed. It can be faster on very large, sparse problems."],"delete":["295","        _, diffusion_map = lobpcg(laplacian, X, M=M, tol=1.e-12,","377","        to be installed. It can be faster on very large, sparse problems,","378","        but may also lead to instabilities."]}],"doc\/whats_new\/v0.22.rst":[{"add":["235","","238","  computed wrong eigenvalues with ``eigen_solver='amg'`` when","241","- |Fix| Fixed a bug in :func:`manifold.spectral_embedding`  used in","242","  :class:`manifold.SpectralEmbedding` and :class:`cluster.spectral.SpectralClustering`","243","  where ``eigen_solver=\"amg\"`` would sometimes result in a LinAlgError.","244","  :issue:`13393` by :user:`Andrew Knyazev <lobpcg>`","245","  :pr:`13707` by :user:`Scott White <whitews>`","246",""],"delete":["233","","238","  computed wrong eigenvalues with ``solver='amg'`` when"]}]}},"f16227ba44ea5c2c95228b09474d3f7e8bc22210":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/metrics\/pairwise.py":"MODIFY","sklearn\/metrics\/pairwise_fast.pyx":"MODIFY","sklearn\/metrics\/tests\/test_pairwise.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["545","  in version 0.21 and will be removed in version 0.23. :pr:`10580` by","546","  :user:`Reshama Shaikh <reshamas>` and :user:`Sandra Mitrovic <SandraMNE>`.","547","","548","- |Fix| The function :func:`euclidean_distances`, and therefore","549","  several estimators with ``metric='euclidean'``, suffered from numerical","550","  precision issues with ``float32`` features. Precision has been increased at the","551","  cost of a small drop of performance. :pr:`13554` by :user:`Celelibi` and","552","  :user:`J¨¦r¨¦mie du Boisberranger <jeremiedbb>`."],"delete":["545","  in version 0.21 and will be removed in version 0.23.","546","  :pr:`10580` by :user:`Reshama Shaikh <reshamas>` and :user:`Sandra","547","  Mitrovic <SandraMNE>`."]}],"sklearn\/metrics\/pairwise.py":[{"add":["195","        May be ignored in some cases, see the note below.","203","        May be ignored in some cases, see the note below.","204","","205","    Notes","206","    -----","207","    To achieve better accuracy, `X_norm_squared`?and `Y_norm_squared` may be","208","    unused if they are passed as ``float32``.","212","    distances : array, shape (n_samples_1, n_samples_2)","233","    # If norms are passed as float32, they are unused. If arrays are passed as","234","    # float32, norms needs to be recomputed on upcast chunks.","235","    # TODO: use a float64 accumulator in row_norms to avoid the latter.","243","        if XX.dtype == np.float32:","244","            XX = None","245","    elif X.dtype == np.float32:","246","        XX = None","250","    if X is Y and XX is not None:","251","        # shortcut in the common case euclidean_distances(X, X)","259","        if YY.dtype == np.float32:","260","            YY = None","261","    elif Y.dtype == np.float32:","262","        YY = None","266","    if X.dtype == np.float32:","267","        # To minimize precision issues with float32, we compute the distance","268","        # matrix on chunks of X and Y upcast to float64","269","        distances = _euclidean_distances_upcast(X, XX, Y, YY)","270","    else:","271","        # if dtype is already float64, no need to chunk and upcast","272","        distances = - 2 * safe_sparse_dot(X, Y.T, dense_output=True)","273","        distances += XX","274","        distances += YY","277","    # Ensure that distances between vectors and themselves are set to 0.0.","278","    # This may not be the case due to floating point rounding errors.","280","        np.fill_diagonal(distances, 0)","285","def _euclidean_distances_upcast(X, XX=None, Y=None, YY=None):","286","    \"\"\"Euclidean distances between X and Y","287","","288","    Assumes X and Y have float32 dtype.","289","    Assumes XX and YY have float64 dtype or are None.","290","","291","    X and Y are upcast to float64 by chunks, which size is chosen to limit","292","    memory increase by approximately 10% (at least 10MiB).","293","    \"\"\"","294","    n_samples_X = X.shape[0]","295","    n_samples_Y = Y.shape[0]","296","    n_features = X.shape[1]","297","","298","    distances = np.empty((n_samples_X, n_samples_Y), dtype=np.float32)","299","","300","    x_density = X.nnz \/ np.prod(X.shape) if issparse(X) else 1","301","    y_density = Y.nnz \/ np.prod(Y.shape) if issparse(Y) else 1","302","","303","    # Allow 10% more memory than X, Y and the distance matrix take (at least","304","    # 10MiB)","305","    maxmem = max(","306","        ((x_density * n_samples_X + y_density * n_samples_Y) * n_features","307","         + (x_density * n_samples_X * y_density * n_samples_Y)) \/ 10,","308","        10 * 2**17)","309","","310","    # The increase amount of memory in 8-byte blocks is:","311","    # - x_density * batch_size * n_features (copy of chunk of X)","312","    # - y_density * batch_size * n_features (copy of chunk of Y)","313","    # - batch_size * batch_size (chunk of distance matrix)","314","    # Hence x? + (xd+yd)kx = M, where x=batch_size, k=n_features, M=maxmem","315","    #                                 xd=x_density and yd=y_density","316","    tmp = (x_density + y_density) * n_features","317","    batch_size = (-tmp + np.sqrt(tmp**2 + 4 * maxmem)) \/ 2","318","    batch_size = max(int(batch_size), 1)","319","","320","    x_batches = gen_batches(X.shape[0], batch_size)","321","    y_batches = gen_batches(Y.shape[0], batch_size)","322","","323","    for i, x_slice in enumerate(x_batches):","324","        X_chunk = X[x_slice].astype(np.float64)","325","        if XX is None:","326","            XX_chunk = row_norms(X_chunk, squared=True)[:, np.newaxis]","327","        else:","328","            XX_chunk = XX[x_slice]","329","","330","        for j, y_slice in enumerate(y_batches):","331","            if X is Y and j < i:","332","                # when X is Y the distance matrix is symmetric so we only need","333","                # to compute half of it.","334","                d = distances[y_slice, x_slice].T","335","","336","            else:","337","                Y_chunk = Y[y_slice].astype(np.float64)","338","                if YY is None:","339","                    YY_chunk = row_norms(Y_chunk, squared=True)[np.newaxis, :]","340","                else:","341","                    YY_chunk = YY[:, y_slice]","342","","343","                d = -2 * safe_sparse_dot(X_chunk, Y_chunk.T, dense_output=True)","344","                d += XX_chunk","345","                d += YY_chunk","346","","347","            distances[x_slice, y_slice] = d.astype(np.float32, copy=False)","348","","349","    return distances","350","","351",""],"delete":["205","    distances : {array, sparse matrix}, shape (n_samples_1, n_samples_2)","236","    if X is Y:  # shortcut in the common case euclidean_distances(X, X)","247","    distances = safe_sparse_dot(X, Y.T, dense_output=True)","248","    distances *= -2","249","    distances += XX","250","    distances += YY","254","        # Ensure that distances between vectors and themselves are set to 0.0.","255","        # This may not be the case due to floating point rounding errors.","256","        distances.flat[::distances.shape[0] + 1] = 0.0"]}],"sklearn\/metrics\/pairwise_fast.pyx":[{"add":["12","from libc.string cimport memset"],"delete":["9","from libc.string cimport memset"]}],"sklearn\/metrics\/tests\/test_pairwise.py":[{"add":["586","@pytest.mark.parametrize(\"x_array_constr\", [np.array, csr_matrix],","587","                         ids=[\"dense\", \"sparse\"])","588","@pytest.mark.parametrize(\"y_array_constr\", [np.array, csr_matrix],","589","                         ids=[\"dense\", \"sparse\"])","590","def test_euclidean_distances_known_result(x_array_constr, y_array_constr):","591","    # Check the pairwise Euclidean distances computation on known result","592","    X = x_array_constr([[0]])","593","    Y = y_array_constr([[1], [2]])","595","    assert_allclose(D, [[1., 2.]])","598","@pytest.mark.parametrize(\"dtype\", [np.float32, np.float64])","599","@pytest.mark.parametrize(\"y_array_constr\", [np.array, csr_matrix],","600","                         ids=[\"dense\", \"sparse\"])","601","def test_euclidean_distances_with_norms(dtype, y_array_constr):","603","    # and that we get a wrong answer with wrong {X,Y}_norm_squared","604","    rng = np.random.RandomState(0)","605","    X = rng.random_sample((10, 10)).astype(dtype, copy=False)","606","    Y = rng.random_sample((20, 10)).astype(dtype, copy=False)","607","","608","    # norms will only be used if their dtype is float64","609","    X_norm_sq = (X.astype(np.float64) ** 2).sum(axis=1).reshape(1, -1)","610","    Y_norm_sq = (Y.astype(np.float64) ** 2).sum(axis=1).reshape(1, -1)","611","","612","    Y = y_array_constr(Y)","613","","619","    assert_allclose(D2, D1)","620","    assert_allclose(D3, D1)","621","    assert_allclose(D4, D1)","627","    with pytest.raises(AssertionError):","628","        assert_allclose(wrong_D, D1)","629","","630","","631","@pytest.mark.parametrize(\"dtype\", [np.float32, np.float64])","632","@pytest.mark.parametrize(\"x_array_constr\", [np.array, csr_matrix],","633","                         ids=[\"dense\", \"sparse\"])","634","@pytest.mark.parametrize(\"y_array_constr\", [np.array, csr_matrix],","635","                         ids=[\"dense\", \"sparse\"])","636","def test_euclidean_distances(dtype, x_array_constr, y_array_constr):","637","    # check that euclidean distances gives same result as scipy cdist","638","    # when X and Y != X are provided","639","    rng = np.random.RandomState(0)","640","    X = rng.random_sample((100, 10)).astype(dtype, copy=False)","641","    X[X < 0.8] = 0","642","    Y = rng.random_sample((10, 10)).astype(dtype, copy=False)","643","    Y[Y < 0.8] = 0","644","","645","    expected = cdist(X, Y)","646","","647","    X = x_array_constr(X)","648","    Y = y_array_constr(Y)","649","    distances = euclidean_distances(X, Y)","650","","651","    # the default rtol=1e-7 is too close to the float32 precision","652","    # and fails due too rounding errors.","653","    assert_allclose(distances, expected, rtol=1e-6)","654","    assert distances.dtype == dtype","655","","656","","657","@pytest.mark.parametrize(\"dtype\", [np.float32, np.float64])","658","@pytest.mark.parametrize(\"x_array_constr\", [np.array, csr_matrix],","659","                         ids=[\"dense\", \"sparse\"])","660","def test_euclidean_distances_sym(dtype, x_array_constr):","661","    # check that euclidean distances gives same result as scipy pdist","662","    # when only X is provided","663","    rng = np.random.RandomState(0)","664","    X = rng.random_sample((100, 10)).astype(dtype, copy=False)","665","    X[X < 0.8] = 0","666","","667","    expected = squareform(pdist(X))","668","","669","    X = x_array_constr(X)","670","    distances = euclidean_distances(X)","671","","672","    # the default rtol=1e-7 is too close to the float32 precision","673","    # and fails due too rounding errors.","674","    assert_allclose(distances, expected, rtol=1e-6)","675","    assert distances.dtype == dtype","676","","677","","678","@pytest.mark.parametrize(","679","    \"dtype, eps, rtol\",","680","    [(np.float32, 1e-4, 1e-5),","681","     pytest.param(","682","         np.float64, 1e-8, 0.99,","683","         marks=pytest.mark.xfail(reason='failing due to lack of precision'))])","684","@pytest.mark.parametrize(\"dim\", [1, 1000000])","685","def test_euclidean_distances_extreme_values(dtype, eps, rtol, dim):","686","    # check that euclidean distances is correct with float32 input thanks to","687","    # upcasting. On float64 there are still precision issues.","688","    X = np.array([[1.] * dim], dtype=dtype)","689","    Y = np.array([[1. + eps] * dim], dtype=dtype)","690","","691","    distances = euclidean_distances(X, Y)","692","    expected = cdist(X, Y)","693","","694","    assert_allclose(distances, expected, rtol=1e-5)"],"delete":["586","def test_euclidean_distances():","587","    # Check the pairwise Euclidean distances computation","588","    X = [[0]]","589","    Y = [[1], [2]]","591","    assert_array_almost_equal(D, [[1., 2.]])","593","    X = csr_matrix(X)","594","    Y = csr_matrix(Y)","595","    D = euclidean_distances(X, Y)","596","    assert_array_almost_equal(D, [[1., 2.]])","598","    rng = np.random.RandomState(0)","599","    X = rng.random_sample((10, 4))","600","    Y = rng.random_sample((20, 4))","601","    X_norm_sq = (X ** 2).sum(axis=1).reshape(1, -1)","602","    Y_norm_sq = (Y ** 2).sum(axis=1).reshape(1, -1)","603","","610","    assert_array_almost_equal(D2, D1)","611","    assert_array_almost_equal(D3, D1)","612","    assert_array_almost_equal(D4, D1)","615","    X_norm_sq *= 0.5","616","    Y_norm_sq *= 0.5","620","    assert_greater(np.max(np.abs(wrong_D - D1)), .01)"]}]}}}