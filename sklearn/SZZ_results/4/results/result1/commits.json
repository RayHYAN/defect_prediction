{"7500693ea22b05b06d9ab8789d156ed00df35b11":{"changes":{"sklearn\/preprocessing\/tests\/test_imputation.py":"MODIFY","sklearn\/model_selection\/_search.py":"MODIFY"},"diff":{"sklearn\/preprocessing\/tests\/test_imputation.py":[{"add":["280","    X = sparse_random_matrix(l, l, density=0.10, random_state=0)","281","    Y = sparse_random_matrix(l, 1, density=0.10, random_state=0).toarray()"],"delete":["280","    X = sparse_random_matrix(l, l, density=0.10)","281","    Y = sparse_random_matrix(l, 1, density=0.10).toarray()"]}],"sklearn\/model_selection\/_search.py":[{"add":["382","    def __init__(self, estimator, scoring=None, n_jobs=None, iid='warn',"],"delete":["382","    def __init__(self, estimator, scoring=None,","383","                 fit_params=None, n_jobs=None, iid='warn',","390","        self.fit_params = fit_params"]}]}},"d14276428ba122b8a2e47775599f1ca91553d80d":{"changes":{"sklearn\/utils\/_pprint.py":"MODIFY","sklearn\/utils\/tests\/test_pprint.py":"MODIFY"},"diff":{"sklearn\/utils\/_pprint.py":[{"add":["97","        if (repr(v) != repr(init_params[k]) and"],"delete":["97","        if (v != init_params[k] and"]}],"sklearn\/utils\/tests\/test_pprint.py":[{"add":["6","from sklearn.linear_model import LogisticRegressionCV","215","    # make sure array parameters don't throw error (see #13583)","216","    repr(LogisticRegressionCV(Cs=np.array([0.1, 1])))","217",""],"delete":[]}]}},"3cb0b3bc22b0a6f99f80c025fd378660896a2c68":{"changes":{"build_tools\/circle\/build_test_pypy.sh":"MODIFY"},"diff":{"build_tools\/circle\/build_test_pypy.sh":[{"add":["23","pip install --extra-index https:\/\/antocuni.github.io\/pypy-wheels\/ubuntu \"numpy==1.15.*\" Cython pytest"],"delete":["23","pip install --extra-index https:\/\/antocuni.github.io\/pypy-wheels\/ubuntu \"numpy==1.5.*\" Cython pytest"]}]}},"3ce62371f853492b58f3a4103cf03021971ebedd":{"changes":{"sklearn\/neural_network\/tests\/test_mlp.py":"MODIFY"},"diff":{"sklearn\/neural_network\/tests\/test_mlp.py":[{"add":["179","        random_state = np.random.RandomState(seed=42)","180","        X = random_state.rand(n_samples, n_features)"],"delete":["179","        X = np.random.random((n_samples, n_features))"]}]}},"fd93ea03b1e53af4ca544405c2a5b3abfa9389e8":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/ensemble\/gradient_boosting.py":"MODIFY","doc\/whats_new\/_contributors.rst":"MODIFY","sklearn\/ensemble\/tests\/test_gradient_boosting.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["201","  :class:`ensemble.ExtraTreesRegressor`,","202","  :class:`ensemble.RandomTreesEmbedding`,","203","  :class:`ensemble.GradientBoostingClassifier`, and","204","  :class:`ensemble.GradientBoostingRegressor`) now:","210","  :issue:`13636` and :issue:`13620` by `Adrin Jalali`_."],"delete":["201","  :class:`ensemble.ExtraTreesRegressor`, and","202","  :class:`ensemble.RandomTreesEmbedding`) now:","208","  :issue:`13636` by `Adrin Jalali`_."]}],"sklearn\/ensemble\/gradient_boosting.py":[{"add":["1711","            The values of this array sum to 1, unless all trees are single node","1712","            trees consisting of only the root node, in which case it will be an","1713","            array of zeros.","1717","        relevant_trees = [tree","1718","                          for stage in self.estimators_ for tree in stage","1719","                          if tree.tree_.node_count > 1]","1720","        if not relevant_trees:","1721","            # degenerate case where all trees have only one node","1722","            return np.zeros(shape=self.n_features_, dtype=np.float64)","1724","        relevant_feature_importances = [","1725","            tree.tree_.compute_feature_importances(normalize=False)","1726","            for tree in relevant_trees","1727","        ]","1728","        avg_feature_importances = np.mean(relevant_feature_importances,","1729","                                          axis=0, dtype=np.float64)","1730","        return avg_feature_importances \/ np.sum(avg_feature_importances)"],"delete":["1714","        total_sum = np.zeros((self.n_features_, ), dtype=np.float64)","1715","        for stage in self.estimators_:","1716","            stage_sum = sum(tree.tree_.compute_feature_importances(","1717","                normalize=False) for tree in stage) \/ len(stage)","1718","            total_sum += stage_sum","1720","        importances = total_sum \/ total_sum.sum()","1721","        return importances"]}],"doc\/whats_new\/_contributors.rst":[{"add":["176",".. _Nicolas Hug: https:\/\/github.com\/NicolasHug"],"delete":["176",".. _Nicolas Hug: https:\/\/github.com\/NicolasHug"]}],"sklearn\/ensemble\/tests\/test_gradient_boosting.py":[{"add":["1442","","1443","","1444","def test_gbr_degenerate_feature_importances():","1445","    # growing an ensemble of single node trees. See #13620","1446","    X = np.zeros((10, 10))","1447","    y = np.ones((10,))","1448","    gbr = GradientBoostingRegressor().fit(X, y)","1449","    assert_array_equal(gbr.feature_importances_,","1450","                       np.zeros(10, dtype=np.float64))"],"delete":[]}]}},"89e6c967d653710b48722dd6b162e692d3c42365":{"changes":{"sklearn\/linear_model\/tests\/test_ridge.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY","sklearn\/linear_model\/ridge.py":"MODIFY"},"diff":{"sklearn\/linear_model\/tests\/test_ridge.py":[{"add":["1010","@pytest.mark.parametrize('solver', ['sparse_cg', 'auto'])","1011","def test_ridge_fit_intercept_sparse(solver):","1012","    X, y = _make_sparse_offset_regression(n_features=20, random_state=0)","1015","    # for now only sparse_cg can correctly fit an intercept with sparse X with","1016","    # default tol and max_iter.","1017","    # sag is tested separately in test_ridge_fit_intercept_sparse_sag","1018","    # because it requires more iterations and should raise a warning if default","1019","    # max_iter is used.","1020","    # other solvers raise an exception, as checked in","1021","    # test_ridge_fit_intercept_sparse_error","1022","    #","1023","    # \"auto\" should switch to \"sparse_cg\" when X is sparse","1024","    # so the reference we use for both (\"auto\" and \"sparse_cg\") is","1025","    # Ridge(solver=\"sparse_cg\"), fitted using the dense representation (note","1026","    # that \"sparse_cg\" can fit sparse or dense data)","1027","    dense_ridge = Ridge(alpha=1., solver='sparse_cg', fit_intercept=True)","1028","    sparse_ridge = Ridge(alpha=1., solver=solver, fit_intercept=True)","1029","    dense_ridge.fit(X, y)","1030","    with pytest.warns(None) as record:","1031","        sparse_ridge.fit(X_csr, y)","1032","    assert len(record) == 0","1033","    assert np.allclose(dense_ridge.intercept_, sparse_ridge.intercept_)","1034","    assert np.allclose(dense_ridge.coef_, sparse_ridge.coef_)","1036","","1037","@pytest.mark.parametrize('solver', ['saga', 'lsqr', 'svd', 'cholesky'])","1038","def test_ridge_fit_intercept_sparse_error(solver):","1039","    X, y = _make_sparse_offset_regression(n_features=20, random_state=0)","1040","    X_csr = sp.csr_matrix(X)","1041","    sparse_ridge = Ridge(alpha=1., solver=solver, fit_intercept=True)","1042","    err_msg = \"solver='{}' does not support\".format(solver)","1043","    with pytest.raises(ValueError, match=err_msg):","1044","        sparse_ridge.fit(X_csr, y)","1045","","1046","","1047","def test_ridge_fit_intercept_sparse_sag():","1048","    X, y = _make_sparse_offset_regression(","1049","        n_features=5, n_samples=20, random_state=0, X_offset=5.)","1050","    X_csr = sp.csr_matrix(X)","1051","","1052","    params = dict(alpha=1., solver='sag', fit_intercept=True,","1053","                  tol=1e-10, max_iter=100000)","1054","    dense_ridge = Ridge(**params)","1055","    sparse_ridge = Ridge(**params)","1056","    dense_ridge.fit(X, y)","1057","    with pytest.warns(None) as record:","1058","        sparse_ridge.fit(X_csr, y)","1059","    assert len(record) == 0","1060","    assert np.allclose(dense_ridge.intercept_, sparse_ridge.intercept_,","1061","                       rtol=1e-4)","1062","    assert np.allclose(dense_ridge.coef_, sparse_ridge.coef_, rtol=1e-4)","1063","    with pytest.warns(UserWarning, match='\"sag\" solver requires.*'):","1064","        Ridge(solver='sag').fit(X_csr, y)"],"delete":["1010","def test_ridge_fit_intercept_sparse():","1011","    X, y = make_regression(n_samples=1000, n_features=2, n_informative=2,","1012","                           bias=10., random_state=42)","1013","","1016","    for solver in ['sag', 'sparse_cg']:","1017","        dense = Ridge(alpha=1., tol=1.e-15, solver=solver, fit_intercept=True)","1018","        sparse = Ridge(alpha=1., tol=1.e-15, solver=solver, fit_intercept=True)","1019","        dense.fit(X, y)","1020","        with pytest.warns(None) as record:","1021","            sparse.fit(X_csr, y)","1022","        assert len(record) == 0","1023","        assert_almost_equal(dense.intercept_, sparse.intercept_)","1024","        assert_array_almost_equal(dense.coef_, sparse.coef_)","1026","    # test the solver switch and the corresponding warning","1027","    for solver in ['saga', 'lsqr']:","1028","        sparse = Ridge(alpha=1., tol=1.e-15, solver=solver, fit_intercept=True)","1029","        assert_raises_regex(ValueError, \"In Ridge,\", sparse.fit, X_csr, y)"]}],"doc\/whats_new\/v0.22.rst":[{"add":["26","- :class:`linear_model.Ridge` when `X` is sparse. |Fix|","27","","118","- |Fix| :class:`linear_model.Ridge` now correctly fits an intercept when `X` is","119","  sparse, `solver=\"auto\"` and `fit_intercept=True`, because the default solver","120","  in this configuration has changed to `sparse_cg`, which can fit an intercept","121","  with sparse data. :pr:`13995` by :user:`J¨¦r?me Dock¨¨s <jeromedockes>`.","122",""],"delete":[]}],"sklearn\/linear_model\/ridge.py":[{"add":["547","        if sparse.issparse(X) and self.fit_intercept:","548","            if self.solver not in ['auto', 'sparse_cg', 'sag']:","549","                raise ValueError(","550","                    \"solver='{}' does not support fitting the intercept \"","551","                    \"on sparse data. Please set the solver to 'auto' or \"","552","                    \"'sparse_cg', 'sag', or set `fit_intercept=False`\"","553","                    .format(self.solver))","554","            if (self.solver == 'sag' and self.max_iter is None and","555","                    self.tol > 1e-4):","556","                warnings.warn(","557","                    '\"sag\" solver requires many iterations to fit '","558","                    'an intercept with sparse inputs. Either set the '","559","                    'solver to \"auto\" or \"sparse_cg\", or set a low '","560","                    '\"tol\" and a high \"max_iter\" (especially if inputs are '","561","                    'not standardized).')","562","                solver = 'sag'","563","            else:","564","                solver = 'sparse_cg'","565","        else:","566","            solver = self.solver","577","        if solver == 'sag' and sparse.issparse(X) and self.fit_intercept:","585","","587","            if sparse.issparse(X) and self.fit_intercept:","596","                max_iter=self.max_iter, tol=self.tol, solver=solver,","684","        'sparse_cg' supports sparse input when `fit_intercept` is True."],"delete":["557","        # temporary fix for fitting the intercept with sparse data using 'sag'","558","        if (sparse.issparse(X) and self.fit_intercept and","559","           self.solver != 'sparse_cg'):","568","            if sparse.issparse(X) and self.solver == 'sparse_cg':","577","                max_iter=self.max_iter, tol=self.tol, solver=self.solver,","665","        'sag' and 'sparse_cg' supports sparse input when `fit_intercept` is","666","        True."]}]}},"296ee0ca45319a95f8a836c4855a613eeb6c9041":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/calibration.py":"MODIFY","sklearn\/metrics\/classification.py":"MODIFY","sklearn\/metrics\/tests\/test_classification.py":"MODIFY","sklearn\/metrics\/ranking.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["482","- |Fix| Fixed a bug where :func:`metrics.brier_score_loss` will sometimes","483","  return incorrect result when there's only one class in ``y_true``.","484","  :pr:`13628` by :user:`Hanmin Qin <qinhanmin2014>`.","485","","486","- |Fix| Fixed a bug in :func:`metrics.label_ranking_average_precision_score`"],"delete":["482","- |Fix| Fixed a bug in :func:`metrics.label_ranking_average_precision_score` "]}],"sklearn\/calibration.py":[{"add":["573","    check_consistent_length(y_true, y_prob)","581","    labels = np.unique(y_true)","582","    if len(labels) > 2:","583","        raise ValueError(\"Only binary classification is supported. \"","584","                         \"Provided labels %s.\" % labels)","585","    y_true = label_binarize(y_true, labels)[:, 0]"],"delete":["27","from .metrics.classification import _check_binary_probabilistic_predictions","581","    y_true = _check_binary_probabilistic_predictions(y_true, y_prob)"]}],"sklearn\/metrics\/classification.py":[{"add":["30","from ..preprocessing import LabelBinarizer","2336","        Label of the positive class.","2337","        Defaults to the greater label unless y_true is all 0 or all -1","2338","        in which case pos_label defaults to 1.","2373","    labels = np.unique(y_true)","2374","    if len(labels) > 2:","2375","        raise ValueError(\"Only binary classification is supported. \"","2376","                         \"Labels in y_true: %s.\" % labels)","2377","    if y_prob.max() > 1:","2378","        raise ValueError(\"y_prob contains values greater than 1.\")","2379","    if y_prob.min() < 0:","2380","        raise ValueError(\"y_prob contains values less than 0.\")","2381","","2382","    # if pos_label=None, when y_true is in {-1, 1} or {0, 1},","2383","    # pos_labe is set to 1 (consistent with precision_recall_curve\/roc_curve),","2384","    # otherwise pos_label is set to the greater label","2385","    # (different from precision_recall_curve\/roc_curve,","2386","    # the purpose is to keep backward compatibility).","2388","        if (np.array_equal(labels, [0]) or","2389","                np.array_equal(labels, [-1])):","2390","            pos_label = 1","2391","        else:","2392","            pos_label = y_true.max()"],"delete":["30","from ..preprocessing import LabelBinarizer, label_binarize","2303","def _check_binary_probabilistic_predictions(y_true, y_prob):","2304","    \"\"\"Check that y_true is binary and y_prob contains valid probabilities\"\"\"","2305","    check_consistent_length(y_true, y_prob)","2306","","2307","    labels = np.unique(y_true)","2308","","2309","    if len(labels) > 2:","2310","        raise ValueError(\"Only binary classification is supported. \"","2311","                         \"Provided labels %s.\" % labels)","2312","","2313","    if y_prob.max() > 1:","2314","        raise ValueError(\"y_prob contains values greater than 1.\")","2315","","2316","    if y_prob.min() < 0:","2317","        raise ValueError(\"y_prob contains values less than 0.\")","2318","","2319","    return label_binarize(y_true, labels)[:, 0]","2320","","2321","","2355","        Label of the positive class. If None, the maximum label is used as","2356","        positive class","2392","        pos_label = y_true.max()","2394","    y_true = _check_binary_probabilistic_predictions(y_true, y_prob)"]}],"sklearn\/metrics\/tests\/test_classification.py":[{"add":["1999","","2000","    # ensure to raise an error for multiclass y_true","2001","    y_true = np.array([0, 1, 2, 0])","2002","    y_pred = np.array([0.8, 0.6, 0.4, 0.2])","2003","    error_message = (\"Only binary classification is supported. Labels \"","2004","                     \"in y_true: {}\".format(np.array([0, 1, 2])))","2005","    assert_raise_message(ValueError, error_message, brier_score_loss,","2006","                         y_true, y_pred)","2007","","2008","    # calculate correctly when there's only one class in y_true","2009","    assert_almost_equal(brier_score_loss([-1], [0.4]), 0.16)","2010","    assert_almost_equal(brier_score_loss([0], [0.4]), 0.16)","2011","    assert_almost_equal(brier_score_loss([1], [0.4]), 0.36)","2012","    assert_almost_equal(","2013","        brier_score_loss(['foo'], [0.4], pos_label='bar'), 0.16)","2014","    assert_almost_equal(","2015","        brier_score_loss(['foo'], [0.4], pos_label='foo'), 0.36)"],"delete":["1999","    # calculate even if only single class in y_true (#6980)","2000","    assert_almost_equal(brier_score_loss([0], [0.5]), 0.25)","2001","    assert_almost_equal(brier_score_loss([1], [0.5]), 0.25)"]}],"sklearn\/metrics\/ranking.py":[{"add":["471","        True binary labels. If labels are not either {-1, 1} or {0, 1}, then","472","        pos_label should be explicitly given.","478","        The label of the positive class.","479","        When ``pos_label=None``, if y_true is in {-1, 1} or {0, 1},","480","        ``pos_label`` is set to 1, otherwise an error will be raised.","557","        The label of the positive class.","558","        When ``pos_label=None``, if y_true is in {-1, 1} or {0, 1},","559","        ``pos_label`` is set to 1, otherwise an error will be raised."],"delete":["471","        True targets of binary classification in range {-1, 1} or {0, 1}.","477","        The label of the positive class","554","        Label considered as positive and others are considered negative."]}]}},"ce9dedb1a37eedc64d9294beafaf4abc6b9970d6":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/compose\/tests\/test_column_transformer.py":"MODIFY","sklearn\/compose\/_column_transformer.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["24",":mod:`sklearn.compose`","25","......................","26","","27","- |Fix| Fixed a bug in :class:`compose.ColumnTransformer` to handle","28","  negative indexes in the columns list of the transformers.","29","  :issue:`12946` by :user:`Pierre Tallotte <pierretallotte>`.","30","","32","............................"],"delete":["25","..........................."]}],"sklearn\/compose\/tests\/test_column_transformer.py":[{"add":["1021","","1022","","1023","def test_column_transformer_negative_column_indexes():","1024","    X = np.random.randn(2, 2)","1025","    X_categories = np.array([[1], [2]])","1026","    X = np.concatenate([X, X_categories], axis=1)","1027","","1028","    ohe = OneHotEncoder(categories='auto')","1029","","1030","    tf_1 = ColumnTransformer([('ohe', ohe, [-1])], remainder='passthrough')","1031","    tf_2 = ColumnTransformer([('ohe', ohe,  [2])], remainder='passthrough')","1032","    assert_array_equal(tf_1.fit_transform(X), tf_2.fit_transform(X))"],"delete":[]}],"sklearn\/compose\/_column_transformer.py":[{"add":["630","    if (_check_key_type(key, int)","631","            or hasattr(key, 'dtype') and np.issubdtype(key.dtype, np.bool_)):","632","        # Convert key into positive indexes","633","        idx = np.arange(n_columns)[key]","634","        return np.atleast_1d(idx).tolist()"],"delete":["630","    if _check_key_type(key, int):","631","        if isinstance(key, int):","632","            return [key]","633","        elif isinstance(key, slice):","634","            return list(range(n_columns)[key])","635","        else:","636","            return list(key)","637","","660","","661","    elif hasattr(key, 'dtype') and np.issubdtype(key.dtype, np.bool_):","662","        # boolean mask","663","        return list(np.arange(n_columns)[key])"]}]}},"9ac5793af1d00d2bf18c78c0f9c3aae5b0b705b8":{"changes":{"sklearn\/preprocessing\/_discretization.py":"MODIFY","doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/preprocessing\/tests\/test_discretization.py":"MODIFY"},"diff":{"sklearn\/preprocessing\/_discretization.py":[{"add":["174","                # Must sort, centers may be unsorted even with sorted init","175","                centers.sort()"],"delete":[]}],"doc\/whats_new\/v0.20.rst":[{"add":["57","- |Fix| Fixed a bug in :class:`preprocessing.KBinsDiscretizer` where","58","  ``strategy='kmeans'`` fails with an error during transformation due to unsorted","59","  bin edges. :issue:`13134` by :user:`Sandro Casagrande <SandroCasagrande>`.","60",""],"delete":[]}],"sklearn\/preprocessing\/tests\/test_discretization.py":[{"add":["187","    'strategy, expected_2bins, expected_3bins, expected_5bins',","188","    [('uniform', [0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 2, 2], [0, 0, 1, 1, 4, 4]),","189","     ('kmeans', [0, 0, 0, 0, 1, 1], [0, 0, 1, 1, 2, 2], [0, 0, 1, 2, 3, 4]),","190","     ('quantile', [0, 0, 0, 1, 1, 1], [0, 0, 1, 1, 2, 2], [0, 1, 2, 3, 4, 4])])","191","def test_nonuniform_strategies(","192","        strategy, expected_2bins, expected_3bins, expected_5bins):","205","    # with 5 bins","206","    est = KBinsDiscretizer(n_bins=5, strategy=strategy, encode='ordinal')","207","    Xt = est.fit_transform(X)","208","    assert_array_equal(expected_5bins, Xt.ravel())","209",""],"delete":["187","    'strategy, expected_2bins, expected_3bins',","188","    [('uniform', [0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 2, 2]),","189","     ('kmeans', [0, 0, 0, 0, 1, 1], [0, 0, 1, 1, 2, 2]),","190","     ('quantile', [0, 0, 0, 1, 1, 1], [0, 0, 1, 1, 2, 2])])","191","def test_nonuniform_strategies(strategy, expected_2bins, expected_3bins):"]}]}},"dc5e4d8896cb61e802da9dc845fbc873ddf5de3e":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/utils\/sparsefuncs.py":"MODIFY","sklearn\/utils\/tests\/test_sparsefuncs.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["35",":mod:`sklearn.utils.sparsefuncs`","36","................................","37","","38","- |Fix| Fixed a bug where :func:`min_max_axis` would fail on 32-bit systems","39","  for certain large inputs. This affects :class:`preprocessing.MaxAbsScaler`, ","40","  :func:`preprocessing.normalize` and :class:`preprocessing.LabelBinarizer`.","41","  :pr:`13741` by :user:`Roddy MacSween <rlms>`.","42",""],"delete":[]}],"sklearn\/utils\/sparsefuncs.py":[{"add":["343","","344","    # reduceat tries casts X.indptr to intp, which errors","345","    # if it is int64 on a 32 bit system.","346","    # Reinitializing prevents this where possible, see #13737","347","    X = type(X)((X.data, X.indices, X.indptr), shape=X.shape)"],"delete":[]}],"sklearn\/utils\/tests\/test_sparsefuncs.py":[{"add":["395","@pytest.mark.parametrize(\"large_indices\", [True, False])","397","                 max_func, ignore_nan, large_indices):","404","    if large_indices:","405","        X_sparse.indices = X_sparse.indices.astype('int64')","406","        X_sparse.indptr = X_sparse.indptr.astype('int64')"],"delete":["396","                 max_func, ignore_nan):"]}]}},"d0747ea2f27332c78b0009914c9bae42db719891":{"changes":{"sklearn\/ensemble\/_gradient_boosting.pyx":"MODIFY","doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/ensemble\/tests\/test_partial_dependence.py":"MODIFY"},"diff":{"sklearn\/ensemble\/_gradient_boosting.pyx":[{"add":["344","                    left_sample_frac = root_node[current_node.left_child].weighted_n_node_samples \/ \\","345","                                       current_node.weighted_n_node_samples","347","                        raise ValueError(\"left_sample_frac:%d, \"","348","                                         \"weighted_n_node_samples current: %d, \"","349","                                         \"weighted_n_node_samples left: %d\"","351","                                            current_node.weighted_n_node_samples,","352","                                            root_node[current_node.left_child].weighted_n_node_samples))"],"delete":["344","                    left_sample_frac = root_node[current_node.left_child].n_node_samples \/ \\","345","                                       <double>current_node.n_node_samples","347","                        raise ValueError(\"left_sample_frac:%f, \"","348","                                         \"n_samples current: %d, \"","349","                                         \"n_samples left: %d\"","351","                                            current_node.n_node_samples,","352","                                            root_node[current_node.left_child].n_node_samples))"]}],"doc\/whats_new\/v0.21.rst":[{"add":["227","- |Fix| :func:`ensemble.partial_dependence` now takes sample weights into","228","  account for the partial dependence computation when the","229","  gradient boosting model has been trained with sample weights.","230","  :issue:`13193` by :user:`Samuel O. Ronsin <samronsin>`.","231",""],"delete":[]}],"sklearn\/ensemble\/tests\/test_partial_dependence.py":[{"add":["6","from numpy.testing import assert_array_equal, assert_allclose","20","sample_weight = [1, 1, 1, 2, 2, 2]","48","    # with trivial (no-op) sample weights","49","    clf.fit(X, y, sample_weight=np.ones(len(y)))","50","","51","    pdp_w, axes_w = partial_dependence(clf, [0], X=X, grid_resolution=5)","52","","53","    assert pdp_w.shape == (1, 4)","54","    assert axes_w[0].shape[0] == 4","55","    assert_allclose(pdp_w, pdp)","56","","57","    # with non-trivial sample weights","58","    clf.fit(X, y, sample_weight=sample_weight)","59","","60","    pdp_w2, axes_w2 = partial_dependence(clf, [0], X=X, grid_resolution=5)","61","","62","    assert pdp_w2.shape == (1, 4)","63","    assert axes_w2[0].shape[0] == 4","64","    assert np.all(np.abs(pdp_w2 - pdp_w) \/ np.abs(pdp_w) > 0.1)","65","","95","def test_partial_dependence_sample_weight():","96","    # Test near perfect correlation between partial dependence and diagonal","97","    # when sample weights emphasize y = x predictions","98","    N = 1000","99","    rng = np.random.RandomState(123456)","100","    mask = rng.randint(2, size=N, dtype=bool)","101","","102","    x = rng.rand(N)","103","    # set y = x on mask and y = -x outside","104","    y = x.copy()","105","    y[~mask] = -y[~mask]","106","    X = np.c_[mask, x]","107","    # sample weights to emphasize data points where y = x","108","    sample_weight = np.ones(N)","109","    sample_weight[mask] = 1000.","110","","111","    clf = GradientBoostingRegressor(n_estimators=10, random_state=1)","112","    clf.fit(X, y, sample_weight=sample_weight)","113","","114","    grid = np.arange(0, 1, 0.01)","115","    pdp = partial_dependence(clf, [1], grid=grid)","116","","117","    assert np.corrcoef(np.ravel(pdp[0]), grid)[0, 1] > 0.99","118","","119",""],"delete":["6","from numpy.testing import assert_array_equal","20","T = [[-1, -1], [2, 2], [3, 2]]","21","true_result = [-1, 1, 1]"]}]}},"5307b2df7fd378269a2b2d5076dbefde46367083":{"changes":{"sklearn\/externals\/_arff.py":"MODIFY"},"diff":{"sklearn\/externals\/_arff.py":[{"add":["30","(LIAC), which takes place at the Federal University of Rio Grande do Sul","35","softwares. This file format was created to be used in Weka, the best","38","An ARFF file can be divided into two sections: header and data. The Header","39","describes the metadata of the dataset, including a general description of the","40","dataset, its name and its attributes. The source below is an example of a","43","    %","45","    %","49","    %","50","    %","57","The Data section of an ARFF file describes the observations of the dataset, in","65","    %","66","    %","67","    %","69","Notice that several lines are starting with an ``%`` symbol, denoting a","71","description part at the beginning of the file. The declarations ``@RELATION``,","75","http:\/\/www.cs.waikato.ac.nz\/~ml\/weka\/arff.html","81","This module uses built-ins python objects to represent a deserialized ARFF","87","- **attributes**: (OBLIGATORY) a list of attributes with the following","94","- **data**: (OBLIGATORY) a list of data instances. Each data instance must be","97","The above keys must follow the case which were described, i.e., the keys are","98","case sensitive. The attribute type ``attribute_type`` must be one of these","99","strings (they are not case sensitive): ``NUMERIC``, ``INTEGER``, ``REAL`` or","100","``STRING``. For nominal attributes, the ``atribute_type`` must be a list of","103","In this format, the XOR dataset presented above can be represented as a python","130","- Supports `scipy.sparse.coo <http:\/\/docs.scipy","135","- Has an interface similar to other built-in modules such as ``json``, or","141","- Under `MIT License <http:\/\/opensource.org\/licenses\/MIT>`_","148","__version__ = '2.4.0'","166","_RE_QUOTE_CHARS = re.compile(r'[\"\\'\\\\\\s%,\\000-\\031]', re.UNICODE)","167","_RE_ESCAPE_CHARS = re.compile(r'(?=[\"\\'\\\\%])|[\\n\\r\\t\\000-\\031]')","168","_RE_SPARSE_LINE = re.compile(r'^\\s*\\{.*\\}\\s*$', re.UNICODE)","169","_RE_NONTRIVIAL_DATA = re.compile('[\"\\'{}\\\\s]', re.UNICODE)","225","_ESCAPE_SUB_MAP = {","226","    '\\\\\\\\': '\\\\',","227","    '\\\\\"': '\"',","228","    \"\\\\'\": \"'\",","229","    '\\\\t': '\\t',","230","    '\\\\n': '\\n',","231","    '\\\\r': '\\r',","232","    '\\\\b': '\\b',","233","    '\\\\f': '\\f',","234","    '\\\\%': '%',","235","}","236","_UNESCAPE_SUB_MAP = {chr(i): '\\\\%03o' % i for i in range(32)}","237","_UNESCAPE_SUB_MAP.update({v: k for k, v in _ESCAPE_SUB_MAP.items()})","238","_UNESCAPE_SUB_MAP[''] = '\\\\'","239","_ESCAPE_SUB_MAP.update({'\\\\%d' % i: chr(i) for i in range(10)})","240","","241","","242","def _escape_sub_callback(match):","243","    s = match.group()","244","    if len(s) == 2:","245","        try:","246","            return _ESCAPE_SUB_MAP[s]","247","        except KeyError:","248","            raise ValueError('Unsupported escape sequence: %s' % s)","249","    if s[1] == 'u':","250","        return unichr(int(s[2:], 16))","251","    else:","252","        return chr(int(s[1:], 8))","253","","254","","257","        return re.sub(r'\\\\([0-9]{1,3}|u[0-9a-f]{4}|.)', _escape_sub_callback,","258","                      v[1:-1])","295","DENSE = 0     # Constant value representing a dense matrix","296","COO = 1       # Constant value representing a sparse matrix in coordinate format","297","LOD = 2       # Constant value representing a sparse matrix in list of","298","              # dictionaries format","299","DENSE_GEN = 3 # Generator of dictionaries","300","LOD_GEN = 4   # Generator of dictionaries","301","_SUPPORTED_DATA_STRUCTURES = [DENSE, COO, LOD, DENSE_GEN, LOD_GEN]","311","    unichr = chr","346","    '''Error raised when some invalid type is provided into the attribute","363","    '''Error raised when a value in used in some data instance but is not","383","    '''Error raised when and invalid numerical value is used in some data","402","    '''Error raised when the object representing the ARFF file has something","404","    def __init__(self, msg='Invalid object.'):","408","        return '%s' % self.msg","409","","413","def _unescape_sub_callback(match):","414","    return _UNESCAPE_SUB_MAP[match.group()]","415","","416","","419","        return u\"'%s'\" % _RE_ESCAPE_CHARS.sub(_unescape_sub_callback, s)","452","class DenseGeneratorData(object):","456","    def decode_rows(self, stream, conversors):","457","        for row in stream:","458","            values = _parse_values(row)","460","            if isinstance(values, dict):","461","                if values and max(values) >= len(conversors):","462","                    raise BadDataFormat(row)","463","                # XXX: int 0 is used for implicit values, not '0'","464","                values = [values[i] if i in values else 0 for i in","465","                          xrange(len(conversors))]","466","            else:","467","                if len(values) != len(conversors):","468","                    raise BadDataFormat(row)","470","            yield self._decode_values(values, conversors)","514","class _DataListMixin(object):","515","    \"\"\"Mixin to return a list from decode_rows instead of a generator\"\"\"","516","    def decode_rows(self, stream, conversors):","517","        return list(super(_DataListMixin, self).decode_rows(stream, conversors))","520","class Data(_DataListMixin, DenseGeneratorData):","521","    pass","522","","523","","524","class COOData(object):","525","    def decode_rows(self, stream, conversors):","526","        data, rows, cols = [], [], []","527","        for i, row in enumerate(stream):","528","            values = _parse_values(row)","529","            if not isinstance(values, dict):","530","                raise BadLayout()","531","            if not values:","532","                continue","533","            row_cols, values = zip(*sorted(values.items()))","534","            try:","535","                values = [value if value is None else conversors[key](value)","536","                          for key, value in zip(row_cols, values)]","537","            except ValueError as exc:","538","                if 'float: ' in str(exc):","539","                    raise BadNumericalValue()","540","                raise","541","            except IndexError:","542","                # conversor out of range","543","                raise BadDataFormat(row)","544","","545","            data.extend(values)","546","            rows.extend([i] * len(values))","547","            cols.extend(row_cols)","548","","549","        return data, rows, cols","587","class LODGeneratorData(object):","588","    def decode_rows(self, stream, conversors):","589","        for row in stream:","590","            values = _parse_values(row)","592","            if not isinstance(values, dict):","593","                raise BadLayout()","594","            try:","595","                yield {key: None if value is None else conversors[key](value)","596","                       for key, value in values.items()}","597","            except ValueError as exc:","598","                if 'float: ' in str(exc):","599","                    raise BadNumericalValue()","600","                raise","601","            except IndexError:","602","                # conversor out of range","603","                raise BadDataFormat(row)","629","class LODData(_DataListMixin, LODGeneratorData):","630","    pass","631","","632","","640","    elif matrix_type == DENSE_GEN:","641","        return DenseGeneratorData()","642","    elif matrix_type == LOD_GEN:","643","        return LODGeneratorData()","678","        padding, including the \"\\r\\n\" characters.","689","        The relation declaration is a line with the format ``@RELATION","695","        padding, including the \"\\r\\n\" characters.","712","        The attribute is the most complex declaration in an arff file. All","717","        where ``attribute-name`` is a string, quoted if the name contains any","725","            {<nominal-name1>, <nominal-name2>, <nominal-name3>, ...}","731","        padding, including the \"\\r\\n\" characters.","791","        s = iter(s)","848","                break","855","        else:","856","            # Never found @DATA","857","            raise BadLayout()","859","        def stream():","860","            for row in s:","861","                self._current_line += 1","862","                row = row.strip()","863","                # Ignore empty lines and comment lines.","864","                if row and not row.startswith(_TK_COMMENT):","865","                    yield row","868","        obj['data'] = data.decode_rows(stream(), self._conversors)","884","            dataset. Can be one of `arff.DENSE`, `arff.COO`, `arff.LOD`,","885","            `arff.DENSE_GEN` or `arff.LOD_GEN`.","886","            Consult the sections on `working with sparse data`_ and `loading","887","            progressively`_.","920","        The relation declaration is a line with the format ``@RELATION","921","        <relation-name>``, where ``relation-name`` is a string.","947","            {<nominal-name1>, <nominal-name2>, <nominal-name3>, ...}","962","            type_tmp = [u'%s' % encode_string(type_k) for type_k in type_]","980","        This encodes iteratively a given object and return, one-by-one, the","1044","    a Python object.","1050","        dataset. Can be one of `arff.DENSE`, `arff.COO`, `arff.LOD`,","1051","        `arff.DENSE_GEN` or `arff.LOD_GEN`.","1052","        Consult the sections on `working with sparse data`_ and `loading","1053","        progressively`_.","1068","        dataset. Can be one of `arff.DENSE`, `arff.COO`, `arff.LOD`,","1069","        `arff.DENSE_GEN` or `arff.LOD_GEN`.","1070","        Consult the sections on `working with sparse data`_ and `loading","1071","        progressively`_.","1079","    '''Serialize an object representing the ARFF document to a given file-like"],"delete":["30","(LIAC), which takes place at the Federal University of Rio Grande do Sul ","35","softwares. This file format was created to be used in Weka, the best ","38","An ARFF file can be divided into two sections: header and data. The Header ","39","describes the metadata of the dataset, including a general description of the ","40","dataset, its name and its attributes. The source below is an example of a ","43","    % ","45","    % ","49","    % ","50","    % ","57","The Data section of an ARFF file describes the observations of the dataset, in ","65","    % ","66","    % ","67","    % ","69","Notice that several lines are starting with an ``%`` symbol, denoting a ","71","description part at the beginning of the file. The declarations ``@RELATION``, ","75","https:\/\/www.cs.waikato.ac.nz\/~ml\/weka\/arff.html","81","This module uses built-ins python objects to represent a deserialized ARFF ","87","- **attributes**: (OBLIGATORY) a list of attributes with the following ","94","- **data**: (OBLIGATORY) a list of data instances. Each data instance must be ","97","The above keys must follow the case which were described, i.e., the keys are ","98","case sensitive. The attribute type ``attribute_type`` must be one of these ","99","strings (they are not case sensitive): ``NUMERIC``, ``INTEGER``, ``REAL`` or ","100","``STRING``. For nominal attributes, the ``atribute_type`` must be a list of ","103","In this format, the XOR dataset presented above can be represented as a python ","130","- Supports `scipy.sparse.coo <https:\/\/docs.scipy","135","- Has an interface similar to other built-in modules such as ``json``, or ","141","- Under `MIT License <https:\/\/opensource.org\/licenses\/MIT>`_","148","__version__ = '2.3.1'","166","_RE_QUOTE_CHARS = re.compile(r'[\"\\'\\\\ \\t%,]')","167","_RE_ESCAPE_CHARS = re.compile(r'(?=[\"\\'\\\\%])')  # don't need to capture anything","168","_RE_SPARSE_LINE = re.compile(r'^\\{.*\\}$')","169","_RE_NONTRIVIAL_DATA = re.compile('[\"\\'{}\\\\s]')","227","        return re.sub(r'\\\\(.)', r'\\1', v[1:-1])","264","DENSE = 0   # Constant value representing a dense matrix","265","COO = 1     # Constant value representing a sparse matrix in coordinate format","266","LOD = 2     # Constant value representing a sparse matrix in list of","267","            # dictionaries format","268","_SUPPORTED_DATA_STRUCTURES = [DENSE, COO, LOD]","312","    '''Error raised when some invalid type is provided into the attribute ","329","    '''Error raised when a value in used in some data instance but is not ","349","    '''Error raised when and invalid numerical value is used in some data ","366","class BadObject(ArffException):","367","    '''Error raised when the object representing the ARFF file has something ","368","    wrong.'''","369","","370","    def __str__(self):","371","        return 'Invalid object.'","374","    '''Error raised when the object representing the ARFF file has something ","376","    def __init__(self, msg=''):","380","        return '%s'%self.msg","386","        return u\"'%s'\" % _RE_ESCAPE_CHARS.sub(r'\\\\', s)","419","class Data(object):","422","    def __init__(self):","423","        self.data = []","425","    def decode_data(self, s, conversors):","426","        values = _parse_values(s)","428","        if isinstance(values, dict):","429","            if max(values) >= len(conversors):","430","                raise BadDataFormat(s)","431","            # XXX: int 0 is used for implicit values, not '0'","432","            values = [values[i] if i in values else 0 for i in","433","                      xrange(len(conversors))]","434","        else:","435","            if len(values) != len(conversors):","436","                raise BadDataFormat(s)","438","        self.data.append(self._decode_values(values, conversors))","451","    def _tuplify_sparse_data(self, x):","452","        if len(x) != 2:","453","            raise BadDataFormat(x)","454","        return (int(x[0].strip('\"').strip(\"'\")), x[1])","455","","486","class COOData(Data):","487","    def __init__(self):","488","        self.data = ([], [], [])","489","        self._current_num_data_points = 0","491","    def decode_data(self, s, conversors):","492","        values = _parse_values(s)","494","        if not isinstance(values, dict):","495","            raise BadLayout()","496","        if not values:","497","            self._current_num_data_points += 1","498","            return","499","        col, values = zip(*sorted(values.items()))","500","        try:","501","            values = [value if value is None else conversors[key](value)","502","                      for key, value in zip(col, values)]","503","        except ValueError as exc:","504","            if 'float: ' in str(exc):","505","                raise BadNumericalValue()","506","            raise","507","        except IndexError:","508","            # conversor out of range","509","            raise BadDataFormat(s)","510","        self.data[0].extend(values)","511","        self.data[1].extend([self._current_num_data_points] * len(values))","512","        self.data[2].extend(col)","514","        self._current_num_data_points += 1","552","class LODData(Data):","553","    def __init__(self):","554","        self.data = []","556","    def decode_data(self, s, conversors):","557","        values = _parse_values(s)","558","        n_conversors = len(conversors)","559","","560","        if not isinstance(values, dict):","561","            raise BadLayout()","562","        try:","563","            self.data.append({key: None if value is None else conversors[key](value)","564","                              for key, value in values.items()})","565","        except ValueError as exc:","566","            if 'float: ' in str(exc):","567","                raise BadNumericalValue()","568","            raise","569","        except IndexError:","570","            # conversor out of range","571","            raise BadDataFormat(s)","638","        padding, including the \"\\r\\n\" characters. ","649","        The relation declaration is a line with the format ``@RELATION ","655","        padding, including the \"\\r\\n\" characters. ","672","        The attribute is the most complex declaration in an arff file. All ","677","        where ``attribute-name`` is a string, quoted if the name contains any ","685","            {<nominal-name1>, <nominal-name2>, <nominal-name3>, ...} ","691","        padding, including the \"\\r\\n\" characters. ","807","                STATE = _TK_DATA","815","            # DATA INSTANCES --------------------------------------------------","816","            elif STATE == _TK_DATA:","817","                data.decode_data(row, self._conversors)","818","            # -----------------------------------------------------------------","819","","820","            # UNKNOWN INFORMATION ---------------------------------------------","821","            else:","822","                raise BadLayout()","823","            # -----------------------------------------------------------------","826","        obj['data'] = data.data","842","            dataset. Can be one of `arff.DENSE`, `arff.COO` and `arff.LOD`.","843","            Consult the section on `working with sparse data`_","876","        The relation declaration is a line with the format ``@RELATION ","877","        <relation-name>``, where ``relation-name`` is a string. ","903","            {<nominal-name1>, <nominal-name2>, <nominal-name3>, ...} ","918","            type_tmp = []","919","            for i in range(len(type_)):","920","                type_tmp.append(u'%s' % encode_string(type_[i]))","938","        This encodes iteratively a given object and return, one-by-one, the ","1002","    a Python object. ","1008","        dataset. Can be one of `arff.DENSE`, `arff.COO` and `arff.LOD`.","1009","        Consult the section on `working with sparse data`_","1024","        dataset. Can be one of `arff.DENSE`, `arff.COO` and `arff.LOD`.","1025","        Consult the section on `working with sparse data`_","1033","    '''Serialize an object representing the ARFF document to a given file-like "]}]}},"2a7194de7ad9ff90e93a65fe7972baa11e04b7e4":{"changes":{"sklearn\/ensemble\/_hist_gradient_boosting\/gradient_boosting.py":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/binning.py":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_gradient_boosting.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY"},"diff":{"sklearn\/ensemble\/_hist_gradient_boosting\/gradient_boosting.py":[{"add":["126","            X_train, X_val, y_train, y_val = train_test_split(","127","                X, y, test_size=self.validation_fraction, stratify=stratify,","128","                random_state=rng)","130","            X_train, y_train = X, y","131","            X_val, y_val = None, None","132","","133","        # Bin the data","134","        self.bin_mapper_ = _BinMapper(max_bins=self.max_bins, random_state=rng)","135","        X_binned_train = self._bin_data(X_train, rng, is_training_data=True)","136","        if X_val is not None:","137","            X_binned_val = self._bin_data(X_val, rng, is_training_data=False)","138","        else:","139","            X_binned_val = None","381","    def _bin_data(self, X, rng, is_training_data):","382","        \"\"\"Bin data X.","383","","384","        If is_training_data, then set the bin_mapper_ attribute.","385","        Else, the binned data is converted to a C-contiguous array.","386","        \"\"\"","387","","388","        description = 'training' if is_training_data else 'validation'","389","        if self.verbose:","390","            print(\"Binning {:.3f} GB of {} data: \".format(","391","                X.nbytes \/ 1e9, description), end=\"\", flush=True)","392","        tic = time()","393","        if is_training_data:","394","            X_binned = self.bin_mapper_.fit_transform(X)  # F-aligned array","395","        else:","396","            X_binned = self.bin_mapper_.transform(X)  # F-aligned array","397","            # We convert the array to C-contiguous since predicting is faster","398","            # with this layout (training is faster on F-arrays though)","399","            X_binned = np.ascontiguousarray(X_binned)","400","        toc = time()","401","        if self.verbose:","402","            duration = toc - tic","403","            print(\"{:.3f} s\".format(duration))","404","","405","        return X_binned","406",""],"delete":["114","        # bin the data","115","        if self.verbose:","116","            print(\"Binning {:.3f} GB of data: \".format(X.nbytes \/ 1e9), end=\"\",","117","                  flush=True)","118","        tic = time()","119","        self.bin_mapper_ = _BinMapper(max_bins=self.max_bins, random_state=rng)","120","        X_binned = self.bin_mapper_.fit_transform(X)","121","        toc = time()","122","        if self.verbose:","123","            duration = toc - tic","124","            print(\"{:.3f} s\".format(duration))","137","            X_binned_train, X_binned_val, y_train, y_val = train_test_split(","138","                X_binned, y, test_size=self.validation_fraction,","139","                stratify=stratify, random_state=rng)","140","","141","            # Predicting is faster of C-contiguous arrays, training is faster","142","            # on Fortran arrays.","143","            X_binned_val = np.ascontiguousarray(X_binned_val)","144","            X_binned_train = np.asfortranarray(X_binned_train)","146","            X_binned_train, y_train = X_binned, y","147","            X_binned_val, y_val = None, None"]}],"sklearn\/ensemble\/_hist_gradient_boosting\/binning.py":[{"add":["142","            The binned data (fortran-aligned)."],"delete":["142","            The binned data."]}],"sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_gradient_boosting.py":[{"add":["8","from sklearn.ensemble._hist_gradient_boosting.binning import _BinMapper","148","","149","","150","def test_binning_train_validation_are_separated():","151","    # Make sure training and validation data are binned separately.","152","    # See issue 13926","153","","154","    rng = np.random.RandomState(0)","155","    validation_fraction = .2","156","    gb = HistGradientBoostingClassifier(","157","        n_iter_no_change=5,","158","        validation_fraction=validation_fraction,","159","        random_state=rng","160","    )","161","    gb.fit(X_classification, y_classification)","162","    mapper_training_data = gb.bin_mapper_","163","","164","    # Note that since the data is small there is no subsampling and the","165","    # random_state doesn't matter","166","    mapper_whole_data = _BinMapper(random_state=0)","167","    mapper_whole_data.fit(X_classification)","168","","169","    n_samples = X_classification.shape[0]","170","    assert np.all(mapper_training_data.actual_n_bins_ ==","171","                  int((1 - validation_fraction) * n_samples))","172","    assert np.all(mapper_training_data.actual_n_bins_ !=","173","                  mapper_whole_data.actual_n_bins_)"],"delete":[]}],"doc\/whats_new\/v0.22.rst":[{"add":["41",":mod:`sklearn.ensemble`","42",".......................","43","","44","- |Fix| :class:`ensemble.HistGradientBoostingClassifier` and","45","  :class:`ensemble.HistGradientBoostingRegressor` now bin the training and","46","  validation data separately to avoid any data leak. :pr:`13933` by","47","  `NicolasHug`_.","48",""],"delete":[]}]}},"e43574e61b4f0cab94683d9c94fd31e2ef03f387":{"changes":{"sklearn\/neighbors\/binary_tree.pxi":"MODIFY","sklearn\/covariance\/shrunk_covariance_.py":"MODIFY","sklearn\/decomposition\/tests\/test_fastica.py":"MODIFY","sklearn\/linear_model\/huber.py":"MODIFY","sklearn\/linear_model\/sag.py":"MODIFY","sklearn\/random_projection.py":"MODIFY","sklearn\/covariance\/empirical_covariance_.py":"MODIFY","sklearn\/covariance\/robust_covariance.py":"MODIFY","sklearn\/linear_model\/stochastic_gradient.py":"MODIFY","sklearn\/svm\/classes.py":"MODIFY","sklearn\/neighbors\/tests\/test_ball_tree.py":"MODIFY","sklearn\/linear_model\/ridge.py":"MODIFY","sklearn\/model_selection\/_search.py":"MODIFY"},"diff":{"sklearn\/neighbors\/binary_tree.pxi":[{"add":["304","    >>> rng = np.random.RandomState(0)","305","    >>> X = rng.random_sample((10, 3))  # 10 points in 3 dimensions","318","    >>> rng = np.random.RandomState(0)","319","    >>> X = rng.random_sample((10, 3))  # 10 points in 3 dimensions","332","    >>> rng = np.random.RandomState(0)","333","    >>> X = rng.random_sample((10, 3))  # 10 points in 3 dimensions","345","    >>> rng = np.random.RandomState(42)","346","    >>> X = rng.random_sample((100, 3))","354","    >>> rng = np.random.RandomState(0)","355","    >>> X = rng.random_sample((30, 3))"],"delete":["304","    >>> np.random.seed(0)","305","    >>> X = np.random.random((10, 3))  # 10 points in 3 dimensions","318","    >>> np.random.seed(0)","319","    >>> X = np.random.random((10, 3))  # 10 points in 3 dimensions","332","    >>> np.random.seed(0)","333","    >>> X = np.random.random((10, 3))  # 10 points in 3 dimensions","345","    >>> np.random.seed(1)","346","    >>> X = np.random.random((100, 3))","354","    >>> np.random.seed(0)","355","    >>> X = np.random.random((30, 3))"]}],"sklearn\/covariance\/shrunk_covariance_.py":[{"add":["105","    >>> rng = np.random.RandomState(0)","106","    >>> X = rng.multivariate_normal(mean=[0, 0],"],"delete":["105","    >>> np.random.seed(0)","106","    >>> X = np.random.multivariate_normal(mean=[0, 0],"]}],"sklearn\/decomposition\/tests\/test_fastica.py":[{"add":[],"delete":["56","    np.random.seed(0)"]}],"sklearn\/linear_model\/huber.py":[{"add":["198","    >>> rng = np.random.RandomState(0)","201","    >>> X[:4] = rng.uniform(10, 20, (4, 2))","202","    >>> y[:4] = rng.uniform(10, 20, 4)"],"delete":["198","    >>> np.random.seed(0)","201","    >>> X[:4] = np.random.uniform(10, 20, (4, 2))","202","    >>> y[:4] = np.random.uniform(10, 20, 4)"]}],"sklearn\/linear_model\/sag.py":[{"add":["203","    >>> rng = np.random.RandomState(0)","204","    >>> X = rng.randn(n_samples, n_features)","205","    >>> y = rng.randn(n_samples)"],"delete":["203","    >>> np.random.seed(0)","204","    >>> X = np.random.randn(n_samples, n_features)","205","    >>> y = np.random.randn(n_samples)"]}],"sklearn\/random_projection.py":[{"add":["468","    >>> rng = np.random.RandomState(42)","469","    >>> X = rng.rand(100, 10000)","470","    >>> transformer = GaussianRandomProjection(random_state=rng)","591","    >>> rng = np.random.RandomState(42)","592","    >>> X = rng.rand(100, 10000)","593","    >>> transformer = SparseRandomProjection(random_state=rng)"],"delete":["468","    >>> X = np.random.rand(100, 10000)","469","    >>> transformer = GaussianRandomProjection()","590","    >>> np.random.seed(42)","591","    >>> X = np.random.rand(100, 10000)","592","    >>> transformer = SparseRandomProjection()"]}],"sklearn\/covariance\/empirical_covariance_.py":[{"add":["122","    >>> rng = np.random.RandomState(0)","123","    >>> X = rng.multivariate_normal(mean=[0, 0],"],"delete":["122","    >>> np.random.seed(0)","123","    >>> X = np.random.multivariate_normal(mean=[0, 0],"]}],"sklearn\/covariance\/robust_covariance.py":[{"add":["588","    >>> rng = np.random.RandomState(0)","589","    >>> X = rng.multivariate_normal(mean=[0, 0],"],"delete":["588","    >>> np.random.seed(0)","589","    >>> X = np.random.multivariate_normal(mean=[0, 0],"]}],"sklearn\/linear_model\/stochastic_gradient.py":[{"add":["1561","    >>> rng = np.random.RandomState(0)","1562","    >>> y = rng.randn(n_samples)","1563","    >>> X = rng.randn(n_samples, n_features)"],"delete":["1561","    >>> np.random.seed(0)","1562","    >>> y = np.random.randn(n_samples)","1563","    >>> X = np.random.randn(n_samples, n_features)"]}],"sklearn\/svm\/classes.py":[{"add":["876","    >>> rng = np.random.RandomState(0)","877","    >>> y = rng.randn(n_samples)","878","    >>> X = rng.randn(n_samples, n_features)"],"delete":["876","    >>> np.random.seed(0)","877","    >>> y = np.random.randn(n_samples)","878","    >>> X = np.random.randn(n_samples, n_features)"]}],"sklearn\/neighbors\/tests\/test_ball_tree.py":[{"add":["154","    rng = np.random.RandomState(0)","155","    X = rng.random_sample((n_samples, n_features))","156","    Y = rng.random_sample((n_samples, n_features))"],"delete":["154","    np.random.seed(0)","155","    X = np.random.random((n_samples, n_features))","156","    Y = np.random.random((n_samples, n_features))"]}],"sklearn\/linear_model\/ridge.py":[{"add":["699","    >>> rng = np.random.RandomState(0)","700","    >>> y = rng.randn(n_samples)","701","    >>> X = rng.randn(n_samples, n_features)"],"delete":["699","    >>> np.random.seed(0)","700","    >>> y = np.random.randn(n_samples)","701","    >>> X = np.random.randn(n_samples, n_features)"]}],"sklearn\/model_selection\/_search.py":[{"add":["230","    >>> rng = np.random.RandomState(0)","232","    >>> param_list = list(ParameterSampler(param_grid, n_iter=4,","233","    ...                                    random_state=rng))"],"delete":["230","    >>> np.random.seed(0)","232","    >>> param_list = list(ParameterSampler(param_grid, n_iter=4))"]}]}},"be17713d85efc496ffe0bd7c86e4efad67f548b1":{"changes":{"sklearn\/exceptions.py":"MODIFY",".circleci\/config.yml":"MODIFY","build_tools\/circle\/build_test_pypy.sh":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_gradient_boosting.py":"MODIFY","sklearn\/feature_extraction\/tests\/test_text.py":"MODIFY","sklearn\/datasets\/tests\/test_base.py":"MODIFY"},"diff":{"sklearn\/exceptions.py":[{"add":["32","    appropriate arguments before using this method.\"...)"],"delete":["32","    appropriate arguments before using this method.\")"]}],".circleci\/config.yml":[{"add":["79","      - image: pypy:3.6-7.1.1"],"delete":["79","      - image: pypy:3-7.0.0"]}],"build_tools\/circle\/build_test_pypy.sh":[{"add":["23","pip install --extra-index https:\/\/antocuni.github.io\/pypy-wheels\/ubuntu numpy Cython pytest","24","pip install scipy sphinx numpydoc docutils joblib pillow","30","export OMP_NUM_THREADS=\"1\"","32","pip install -e .","33","","34","# Check that Python implementation is PyPy","35","python - << EOL","36","import platform","37","from sklearn.utils import IS_PYPY","38","assert IS_PYPY is True, \"platform={}!=PyPy\".format(platform.python_implementation())","39","EOL"],"delete":["23","pip install --extra-index https:\/\/antocuni.github.io\/pypy-wheels\/ubuntu \"numpy==1.15.*\" Cython pytest","24","pip install \"scipy>=1.1.0\" sphinx numpydoc docutils joblib pillow","31","pip install -vv -e . "]}],"sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_gradient_boosting.py":[{"add":["68","    X, y = make_regression(n_samples=50, random_state=0)","89","    make_classification(n_samples=30, random_state=0),","90","    make_classification(n_samples=30, n_classes=3, n_clusters_per_class=1,","91","                        random_state=0)"],"delete":["68","    X, y = make_regression(random_state=0)","89","    make_classification(random_state=0),","90","    make_classification(n_classes=3, n_clusters_per_class=1, random_state=0)"]}],"sklearn\/feature_extraction\/tests\/test_text.py":[{"add":["1094","        pytest.xfail(reason='HashingVectorizer is not supported on PyPy')","1201","     HashingVectorizer]","1209","    if issubclass(Estimator, HashingVectorizer):","1210","        pytest.xfail('HashingVectorizer is not supported on PyPy')","1241","     HashingVectorizer]","1248","    if issubclass(Estimator, HashingVectorizer):","1249","        pytest.xfail('HashingVectorizer is not supported on PyPy')","1250",""],"delete":["1094","        pytest.xfail(reason='HashingVectorizer not supported on PyPy')","1201","     pytest.param(HashingVectorizer, marks=fails_if_pypy)]","1239","     pytest.param(HashingVectorizer, marks=fails_if_pypy)]"]}],"sklearn\/datasets\/tests\/test_base.py":[{"add":["31","from sklearn.utils import IS_PYPY","95","    if IS_PYPY:","96","        pytest.xfail('[PyPy] fails due to string containing NUL characters')","106","    if IS_PYPY:","107","        pytest.xfail('[PyPy] fails due to string containing NUL characters')"],"delete":[]}]}},"c41876126582577461cead2c9ce8b0335f401d81":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/cross_decomposition\/pls_.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["14",":mod:`sklearn.decomposition`","15","............................","16","","17","- |Fix| Fixed a bug in :class:`cross_decomposition.CCA` improving numerical ","18","  stability when `Y` is close to zero. :pr:`13903` by `Thomas Fan`_.","19",""],"delete":[]}],"sklearn\/cross_decomposition\/pls_.py":[{"add":["287","        Y_eps = np.finfo(Yk.dtype).eps","296","                # Replace columns that are all close to zero with zeros","297","                Yk_mask = np.all(np.abs(Yk) < 10 * Y_eps, axis=0)","298","                Yk[:, Yk_mask] = 0.0","299",""],"delete":[]}]}},"5bc3edccab42921332d54b88c2c595d333f16410":{"changes":{"sklearn\/tree\/_splitter.pyx":"MODIFY","sklearn\/tree\/tests\/test_tree.py":"MODIFY"},"diff":{"sklearn\/tree\/_splitter.pyx":[{"add":["118","                   const DOUBLE_t[:, ::1] y,","238","    cdef const DTYPE_t[:, :] X","262","                  const DOUBLE_t[:, ::1] y,","878","                  const DOUBLE_t[:, ::1] y,"],"delete":["118","                   DOUBLE_t[:, ::1] y,","238","    cdef DTYPE_t[:, :] X","262","                  DOUBLE_t[:, ::1] y,","878","                  DOUBLE_t[:, ::1] y,"]}],"sklearn\/tree\/tests\/test_tree.py":[{"add":["35","from sklearn.utils.testing import TempMemmap","1851","","1852","","1853","def test_decision_tree_memmap():","1854","    # check that decision trees supports read-only buffer (#13626)","1855","    X = np.random.RandomState(0).random_sample((10, 2)).astype(np.float32)","1856","    y = np.zeros(10)","1857","","1858","    with TempMemmap((X, y)) as (X_read_only, y_read_only):","1859","        DecisionTreeClassifier().fit(X_read_only, y_read_only)"],"delete":[]}]}},"61de4021dae4a2edfa42b59357a7b628e634ac14":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/metrics\/pairwise.py":"MODIFY","sklearn\/metrics\/tests\/test_pairwise.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["25","- |Fix| Fixed two bugs in :class:`metrics.pairwise_distances` when","26","  ``n_jobs > 1``. First it used to return a distance matrix with same dtype as","27","  input, even for integer dtype. Then the diagonal was not zeros for euclidean","28","  metric when ``Y`` is ``X``. :issue:`13877` by","29","  :user:`J¨¦r¨¦mie du Boisberranger <jeremiedbb>`.","30",""],"delete":[]}],"sklearn\/metrics\/pairwise.py":[{"add":["1184","    X, Y, dtype = _return_float_dtype(X, Y)","1191","    ret = np.empty((X.shape[0], Y.shape[0]), dtype=dtype, order='F')","1196","    if (X is Y or Y is None) and func is euclidean_distances:","1197","        # zeroing diagonal for euclidean norm.","1198","        # TODO: do it also for other norms.","1199","        np.fill_diagonal(ret, 0)","1200",""],"delete":["1190","    ret = np.empty((X.shape[0], Y.shape[0]), dtype=X.dtype, order='F')"]}],"sklearn\/metrics\/tests\/test_pairwise.py":[{"add":["249","@pytest.mark.parametrize('array_constr', [np.array, csr_matrix])","250","@pytest.mark.parametrize('dtype', [np.float64, int])","251","def test_pairwise_parallel(func, metric, kwds, array_constr, dtype):","252","    rng = np.random.RandomState(0)","253","    X = array_constr(5 * rng.random_sample((5, 4)), dtype=dtype)","254","    Y = array_constr(5 * rng.random_sample((3, 4)), dtype=dtype)","255","","256","    try:","257","        S = func(X, metric=metric, n_jobs=1, **kwds)","258","    except (TypeError, ValueError) as exc:","259","        # Not all metrics support sparse input","260","        # ValueError may be triggered by bad callable","261","        if array_constr is csr_matrix:","262","            with pytest.raises(type(exc)):","263","                func(X, metric=metric, n_jobs=2, **kwds)","264","            return","265","        else:","266","            raise","267","    S2 = func(X, metric=metric, n_jobs=2, **kwds)","268","    assert_allclose(S, S2)","269","","270","    S = func(X, Y, metric=metric, n_jobs=1, **kwds)","271","    S2 = func(X, Y, metric=metric, n_jobs=2, **kwds)","272","    assert_allclose(S, S2)","545","@pytest.mark.parametrize(","546","        'metric',","547","        ('euclidean', 'l2', 'sqeuclidean'))","548","def test_parallel_pairwise_distances_diagonal(metric):","549","    rng = np.random.RandomState(0)","550","    X = rng.normal(size=(1000, 10), scale=1e10)","551","    distances = pairwise_distances(X, metric=metric, n_jobs=2)","552","    assert_allclose(np.diag(distances), 0, atol=1e-10)","553","","554",""],"delete":["233","def check_pairwise_parallel(func, metric, kwds):","234","    rng = np.random.RandomState(0)","235","    for make_data in (np.array, csr_matrix):","236","        X = make_data(rng.random_sample((5, 4)))","237","        Y = make_data(rng.random_sample((3, 4)))","238","","239","        try:","240","            S = func(X, metric=metric, n_jobs=1, **kwds)","241","        except (TypeError, ValueError) as exc:","242","            # Not all metrics support sparse input","243","            # ValueError may be triggered by bad callable","244","            if make_data is csr_matrix:","245","                assert_raises(type(exc), func, X, metric=metric,","246","                              n_jobs=2, **kwds)","247","                continue","248","            else:","249","                raise","250","        S2 = func(X, metric=metric, n_jobs=2, **kwds)","251","        assert_array_almost_equal(S, S2)","252","","253","        S = func(X, Y, metric=metric, n_jobs=1, **kwds)","254","        S2 = func(X, Y, metric=metric, n_jobs=2, **kwds)","255","        assert_array_almost_equal(S, S2)","256","","257","","274","def test_pairwise_parallel(func, metric, kwds):","275","    check_pairwise_parallel(func, metric, kwds)"]}]}},"07b0a420a9dff7de9d97f93f1b6de6ef66d4edbf":{"changes":{"sklearn\/utils\/_pprint.py":"MODIFY","sklearn\/utils\/tests\/test_pprint.py":"MODIFY"},"diff":{"sklearn\/utils\/_pprint.py":[{"add":["323","    # Note: need to copy _dispatch to prevent instances of the builtin","324","    # PrettyPrinter class to call methods of _EstimatorPrettyPrinter (see issue","325","    # 12906)","326","    _dispatch = pprint.PrettyPrinter._dispatch.copy()"],"delete":["323","    _dispatch = pprint.PrettyPrinter._dispatch"]}],"sklearn\/utils\/tests\/test_pprint.py":[{"add":["1","from pprint import PrettyPrinter","314","","315","","316","def test_builtin_prettyprinter():","317","    # non regression test than ensures we can still use the builtin","318","    # PrettyPrinter class for estimators (as done e.g. by joblib).","319","    # Used to be a bug","320","","321","    PrettyPrinter().pprint(LogisticRegression())"],"delete":[]}]}},"0a5af0d2a11c64d59381110f3967acbe7d88a031":{"changes":{"sklearn\/experimental\/enable_successive_halving.py":"ADD","sklearn\/tests\/test_docstring_parameters.py":"MODIFY","doc\/modules\/classes.rst":"MODIFY","sklearn\/model_selection\/_search_successive_halving.py":"ADD","sklearn\/model_selection\/tests\/test_split.py":"MODIFY","sklearn\/model_selection\/tests\/test_successive_halving.py":"ADD","doc\/conftest.py":"MODIFY","sklearn\/experimental\/tests\/test_enable_successive_halving.py":"ADD","doc\/conf.py":"MODIFY","sklearn\/model_selection\/__init__.py":"MODIFY","sklearn\/model_selection\/_split.py":"MODIFY","doc\/modules\/grid_search.rst":"MODIFY","examples\/model_selection\/plot_successive_halving_iterations.py":"ADD","examples\/model_selection\/plot_successive_halving_heatmap.py":"ADD","doc\/whats_new\/v0.24.rst":"MODIFY","sklearn\/model_selection\/_search.py":"MODIFY"},"diff":{"sklearn\/experimental\/enable_successive_halving.py":[{"add":[],"delete":[]}],"sklearn\/tests\/test_docstring_parameters.py":[{"add":["191","               'VotingRegressor', 'SequentialFeatureSelector',","192","               'HalvingGridSearchCV', 'HalvingRandomSearchCV'}"],"delete":["191","               'VotingRegressor', 'SequentialFeatureSelector'}"]}],"doc\/modules\/classes.rst":[{"add":["1196","   model_selection.HalvingGridSearchCV","1200","   model_selection.HalvingRandomSearchCV"],"delete":[]}],"sklearn\/model_selection\/_search_successive_halving.py":[{"add":[],"delete":[]}],"sklearn\/model_selection\/tests\/test_split.py":[{"add":["45","from sklearn.model_selection._split import _yields_constant_splits","1622","","1623","","1624","@pytest.mark.parametrize('cv, expected', [","1625","    (KFold(), True),","1626","    (KFold(shuffle=True, random_state=123), True),","1627","    (StratifiedKFold(), True),","1628","    (StratifiedKFold(shuffle=True, random_state=123), True),","1629","    (RepeatedKFold(random_state=123), True),","1630","    (RepeatedStratifiedKFold(random_state=123), True),","1631","    (ShuffleSplit(random_state=123), True),","1632","    (GroupShuffleSplit(random_state=123), True),","1633","    (StratifiedShuffleSplit(random_state=123), True),","1634","    (GroupKFold(), True),","1635","    (TimeSeriesSplit(), True),","1636","    (LeaveOneOut(), True),","1637","    (LeaveOneGroupOut(), True),","1638","    (LeavePGroupsOut(n_groups=2), True),","1639","    (LeavePOut(p=2), True),","1640","","1641","    (KFold(shuffle=True, random_state=None), False),","1642","    (KFold(shuffle=True, random_state=None), False),","1643","    (StratifiedKFold(shuffle=True, random_state=np.random.RandomState(0)),","1644","     False),","1645","    (StratifiedKFold(shuffle=True, random_state=np.random.RandomState(0)),","1646","     False),","1647","    (RepeatedKFold(random_state=None), False),","1648","    (RepeatedKFold(random_state=np.random.RandomState(0)), False),","1649","    (RepeatedStratifiedKFold(random_state=None), False),","1650","    (RepeatedStratifiedKFold(random_state=np.random.RandomState(0)), False),","1651","    (ShuffleSplit(random_state=None), False),","1652","    (ShuffleSplit(random_state=np.random.RandomState(0)), False),","1653","    (GroupShuffleSplit(random_state=None), False),","1654","    (GroupShuffleSplit(random_state=np.random.RandomState(0)), False),","1655","    (StratifiedShuffleSplit(random_state=None), False),","1656","    (StratifiedShuffleSplit(random_state=np.random.RandomState(0)), False),","1657","])","1658","def test_yields_constant_splits(cv, expected):","1659","    assert _yields_constant_splits(cv) == expected"],"delete":[]}],"sklearn\/model_selection\/tests\/test_successive_halving.py":[{"add":[],"delete":[]}],"doc\/conftest.py":[{"add":["59","def setup_grid_search():","60","    try:","61","        import pandas  # noqa","62","    except ImportError:","63","        raise SkipTest(\"Skipping grid_search.rst, pandas not installed\")","64","","65","","95","    elif fname.endswith('modules\/grid_search.rst'):","96","        setup_grid_search()"],"delete":[]}],"sklearn\/experimental\/tests\/test_enable_successive_halving.py":[{"add":[],"delete":[]}],"doc\/conf.py":[{"add":["358","from sklearn.experimental import enable_successive_halving  # noqa"],"delete":[]}],"sklearn\/model_selection\/__init__.py":[{"add":["0","import typing","1","","33","if typing.TYPE_CHECKING:","34","    # Avoid errors in type checkers (e.g. mypy) for experimental estimators.","35","    # TODO: remove this check once the estimator is no longer experimental.","36","    from ._search_successive_halving import (  # noqa","37","        HalvingGridSearchCV, HalvingRandomSearchCV","38","    )","39","","40","","41","__all__ = ['BaseCrossValidator',","68","           'validation_curve']"],"delete":["31","__all__ = ('BaseCrossValidator',","58","           'validation_curve')"]}],"sklearn\/model_selection\/_split.py":[{"add":["2237","","2238","","2239","def _yields_constant_splits(cv):","2240","    # Return True if calling cv.split() always returns the same splits","2241","    # We assume that if a cv doesn't have a shuffle parameter, it shuffles by","2242","    # default (e.g. ShuffleSplit). If it actually doesn't shuffle (e.g.","2243","    # LeaveOneOut), then it won't have a random_state parameter anyway, in","2244","    # which case it will default to 0, leading to output=True","2245","    shuffle = getattr(cv, 'shuffle', True)","2246","    random_state = getattr(cv, 'random_state', 0)","2247","    return isinstance(random_state, numbers.Integral) or not shuffle"],"delete":[]}],"doc\/modules\/grid_search.rst":[{"add":["32","Two generic approaches to parameter search are provided in","36","distribution. Both these tools have successive halving counterparts","37",":class:`HalvingGridSearchCV` and :class:`HalvingRandomSearchCV`, which can be","38","much faster at finding a good parameter combination.","39","","40","After describing these tools we detail :ref:`best practices","41","<grid_search_tips>` applicable to these approaches. Some models allow for","42","specialized, efficient parameter search strategies, outlined in","43",":ref:`alternative_cv`.","173",".. _successive_halving_user_guide:","174","","175","Searching for optimal parameters with successive halving","176","========================================================","177","","178","Scikit-learn also provides the :class:`HalvingGridSearchCV` and","179",":class:`HalvingRandomSearchCV` estimators that can be used to","180","search a parameter space using successive halving [1]_ [2]_. Successive","181","halving (SH) is like a tournament among candidate parameter combinations.","182","SH is an iterative selection process where all candidates (the","183","parameter combinations) are evaluated with a small amount of resources at","184","the first iteration. Only some of these candidates are selected for the next","185","iteration, which will be allocated more resources. For parameter tuning, the","186","resource is typically the number of training samples, but it can also be an","187","arbitrary numeric parameter such as `n_estimators` in a random forest.","188","","189","As illustrated in the figure below, only a subset of candidates","190","'survive' until the last iteration. These are the candidates that have","191","consistently ranked among the top-scoring candidates across all iterations.","192","Each iteration is allocated an increasing amount of resources per candidate,","193","here the number of samples.","194","","195",".. figure:: ..\/auto_examples\/model_selection\/images\/sphx_glr_plot_successive_halving_iterations_001.png","196","   :target: ..\/auto_examples\/model_selection\/plot_successive_halving_iterations.html","197","   :align: center","198","","199","We here briefly describe the main parameters, but each parameter and their","200","interactions are described in more details in the sections below. The","201","``factor`` (> 1) parameter controls the rate at which the resources grow, and","202","the rate at which the number of candidates decreases. In each iteration, the","203","number of resources per candidate is multiplied by ``factor`` and the number","204","of candidates is divided by the same factor. Along with ``resource`` and","205","``min_resources``, ``factor`` is the most important parameter to control the","206","search in our implementation, though a value of 3 usually works well.","207","``factor`` effectively controls the number of iterations in","208",":class:`HalvingGridSearchCV` and the number of candidates (by default) and","209","iterations in :class:`HalvingRandomSearchCV`. ``aggressive_elimination=True``","210","can also be used if the number of available resources is small. More control","211","is available through tuning the ``min_resources`` parameter.","212","","213","These estimators are still **experimental**: their predictions","214","and their API might change without any deprecation cycle. To use them, you","215","need to explicitly import ``enable_successive_halving``::","216","","217","  >>> # explicitly require this experimental feature","218","  >>> from sklearn.experimental import enable_successive_halving  # noqa","219","  >>> # now you can import normally from model_selection","220","  >>> from sklearn.model_selection import HalvingGridSearchCV","221","  >>> from sklearn.model_selection import HalvingRandomSearchCV","222","","223",".. topic:: Examples:","224","","225","    * :ref:`sphx_glr_auto_examples_model_selection_plot_successive_halving_heatmap.py`","226","    * :ref:`sphx_glr_auto_examples_model_selection_plot_successive_halving_iterations.py`","227","","228","Choosing ``min_resources`` and the number of candidates","229","-------------------------------------------------------","230","","231","Beside ``factor``, the two main parameters that influence the behaviour of a","232","successive halving search are the ``min_resources`` parameter, and the","233","number of candidates (or parameter combinations) that are evaluated.","234","``min_resources`` is the amount of resources allocated at the first","235","iteration for each candidate. The number of candidates is specified directly","236","in :class:`HalvingRandomSearchCV`, and is determined from the ``param_grid``","237","parameter of :class:`HalvingGridSearchCV`.","238","","239","Consider a case where the resource is the number of samples, and where we","240","have 1000 samples. In theory, with ``min_resources=10`` and ``factor=2``, we","241","are able to run **at most** 7 iterations with the following number of","242","samples: ``[10, 20, 40, 80, 160, 320, 640]``.","243","","244","But depending on the number of candidates, we might run less than 7","245","iterations: if we start with a **small** number of candidates, the last","246","iteration might use less than 640 samples, which means not using all the","247","available resources (samples). For example if we start with 5 candidates, we","248","only need 2 iterations: 5 candidates for the first iteration, then","249","`5 \/\/ 2 = 2` candidates at the second iteration, after which we know which","250","candidate performs the best (so we don't need a third one). We would only be","251","using at most 20 samples which is a waste since we have 1000 samples at our","252","disposal. On the other hand, if we start with a **high** number of","253","candidates, we might end up with a lot of candidates at the last iteration,","254","which may not always be ideal: it means that many candidates will run with","255","the full resources, basically reducing the procedure to standard search.","256","","257","In the case of :class:`HalvingRandomSearchCV`, the number of candidates is set","258","by default such that the last iteration uses as much of the available","259","resources as possible. For :class:`HalvingGridSearchCV`, the number of","260","candidates is determined by the `param_grid` parameter. Changing the value of","261","``min_resources`` will impact the number of possible iterations, and as a","262","result will also have an effect on the ideal number of candidates.","263","","264","Another consideration when choosing ``min_resources`` is whether or not it","265","is easy to discriminate between good and bad candidates with a small amount","266","of resources. For example, if you need a lot of samples to distinguish","267","between good and bad parameters, a high ``min_resources`` is recommended. On","268","the other hand if the distinction is clear even with a small amount of","269","samples, then a small ``min_resources`` may be preferable since it would","270","speed up the computation.","271","","272","Notice in the example above that the last iteration does not use the maximum","273","amount of resources available: 1000 samples are available, yet only 640 are","274","used, at most. By default, both :class:`HalvingRandomSearchCV` and","275",":class:`HalvingGridSearchCV` try to use as many resources as possible in the","276","last iteration, with the constraint that this amount of resources must be a","277","multiple of both `min_resources` and `factor` (this constraint will be clear","278","in the next section). :class:`HalvingRandomSearchCV` achieves this by","279","sampling the right amount of candidates, while :class:`HalvingGridSearchCV`","280","achieves this by properly setting `min_resources`. Please see","281",":ref:`exhausting_the_resources` for details.","282","","283",".. _amount_of_resource_and_number_of_candidates:","284","","285","Amount of resource and number of candidates at each iteration","286","-------------------------------------------------------------","287","","288","At any iteration `i`, each candidate is allocated a given amount of resources","289","which we denote `n_resources_i`. This quantity is controlled by the","290","parameters ``factor`` and ``min_resources`` as follows (`factor` is strictly","291","greater than 1)::","292","","293","    n_resources_i = factor**i * min_resources,","294","","295","or equivalently::","296","","297","    n_resources_{i+1} = n_resources_i * factor","298","","299","where ``min_resources == n_resources_0`` is the amount of resources used at","300","the first iteration. ``factor`` also defines the proportions of candidates","301","that will be selected for the next iteration::","302","","303","    n_candidates_i = n_candidates \/\/ (factor ** i)","304","","305","or equivalently::","306","","307","    n_candidates_0 = n_candidates","308","    n_candidates_{i+1} = n_candidates_i \/\/ factor","309","","310","So in the first iteration, we use ``min_resources`` resources","311","``n_candidates`` times. In the second iteration, we use ``min_resources *","312","factor`` resources ``n_candidates \/\/ factor`` times. The third again","313","multiplies the resources per candidate and divides the number of candidates.","314","This process stops when the maximum amount of resource per candidate is","315","reached, or when we have identified the best candidate. The best candidate","316","is identified at the iteration that is evaluating `factor` or less candidates","317","(see just below for an explanation).","318","","319","Here is an example with ``min_resources=3`` and ``factor=2``, starting with","320","70 candidates:","321","","322","+-----------------------+-----------------------+","323","| ``n_resources_i``     | ``n_candidates_i``    |","324","+=======================+=======================+","325","| 3 (=min_resources)    | 70 (=n_candidates)    |","326","+-----------------------+-----------------------+","327","| 3 * 2 = 6             | 70 \/\/ 2 = 35          |","328","+-----------------------+-----------------------+","329","| 6 * 2 = 12            | 35 \/\/ 2 = 17          |","330","+-----------------------+-----------------------+","331","| 12 * 2 = 24           | 17 \/\/ 2 = 8           |","332","+-----------------------+-----------------------+","333","| 24 * 2 = 48           | 8 \/\/ 2 = 4            |","334","+-----------------------+-----------------------+","335","| 48 * 2 = 96           | 4 \/\/ 2 = 2            |","336","+-----------------------+-----------------------+","337","","338","We can note that:","339","","340","- the process stops at the first iteration which evaluates `factor=2`","341","  candidates: the best candidate is the best out of these 2 candidates. It","342","  is not necessary to run an additional iteration, since it would only","343","  evaluate one candidate (namely the best one, which we have already","344","  identified). For this reason, in general, we want the last iteration to","345","  run at most ``factor`` candidates. If the last iteration evaluates more","346","  than `factor` candidates, then this last iteration reduces to a regular","347","  search (as in :class:`RandomizedSearchCV` or :class:`GridSearchCV`).","348","- each ``n_resources_i`` is a multiple of both ``factor`` and","349","  ``min_resources`` (which is confirmed by its definition above).","350","","351","The amount of resources that is used at each iteration can be found in the","352","`n_resources_` attribute.","353","","354","Choosing a resource","355","-------------------","356","","357","By default, the resource is defined in terms of number of samples. That is,","358","each iteration will use an increasing amount of samples to train on. You can","359","however manually specify a parameter to use as the resource with the","360","``resource`` parameter. Here is an example where the resource is defined in","361","terms of the number of estimators of a random forest::","362","","363","    >>> from sklearn.datasets import make_classification","364","    >>> from sklearn.ensemble import RandomForestClassifier","365","    >>> from sklearn.experimental import enable_successive_halving  # noqa","366","    >>> from sklearn.model_selection import HalvingGridSearchCV","367","    >>> import pandas as pd","368","    >>>","369","    >>> param_grid = {'max_depth': [3, 5, 10],","370","    ...               'min_samples_split': [2, 5, 10]}","371","    >>> base_estimator = RandomForestClassifier(random_state=0)","372","    >>> X, y = make_classification(n_samples=1000, random_state=0)","373","    >>> sh = HalvingGridSearchCV(base_estimator, param_grid, cv=5,","374","    ...                          factor=2, resource='n_estimators',","375","    ...                          max_resources=30).fit(X, y)","376","    >>> sh.best_estimator_","377","    RandomForestClassifier(max_depth=5, n_estimators=24, random_state=0)","378","","379","Note that it is not possible to budget on a parameter that is part of the","380","parameter grid.","381","","382",".. _exhausting_the_resources:","383","","384","Exhausting the available resources","385","----------------------------------","386","","387","As mentioned above, the number of resources that is used at each iteration","388","depends on the `min_resources` parameter.","389","If you have a lot of resources available but start with a low number of","390","resources, some of them might be wasted (i.e. not used)::","391","","392","    >>> from sklearn.datasets import make_classification","393","    >>> from sklearn.svm import SVC","394","    >>> from sklearn.experimental import enable_successive_halving  # noqa","395","    >>> from sklearn.model_selection import HalvingGridSearchCV","396","    >>> import pandas as pd","397","    >>> param_grid= {'kernel': ('linear', 'rbf'),","398","    ...              'C': [1, 10, 100]}","399","    >>> base_estimator = SVC(gamma='scale')","400","    >>> X, y = make_classification(n_samples=1000)","401","    >>> sh = HalvingGridSearchCV(base_estimator, param_grid, cv=5,","402","    ...                          factor=2, min_resources=20).fit(X, y)","403","    >>> sh.n_resources_","404","    [20, 40, 80]","405","","406","The search process will only use 80 resources at most, while our maximum","407","amount of available resources is ``n_samples=1000``. Here, we have","408","``min_resources = r_0 = 20``.","409","","410","For :class:`HalvingGridSearchCV`, by default, the `min_resources` parameter","411","is set to 'exhaust'. This means that `min_resources` is automatically set","412","such that the last iteration can use as many resources as possible, within","413","the `max_resources` limit::","414","","415","    >>> sh = HalvingGridSearchCV(base_estimator, param_grid, cv=5,","416","    ...                          factor=2, min_resources='exhaust').fit(X, y)","417","    >>> sh.n_resources_","418","    [250, 500, 1000]","419","","420","`min_resources` was here automatically set to 250, which results in the last","421","iteration using all the resources. The exact value that is used depends on","422","the number of candidate parameter, on `max_resources` and on `factor`.","423","","424","For :class:`HalvingRandomSearchCV`, exhausting the resources can be done in 2","425","ways:","426","","427","- by setting `min_resources='exhaust'`, just like for","428","  :class:`HalvingGridSearchCV`;","429","- by setting `n_candidates='exhaust'`.","430","","431","Both options are mutally exclusive: using `min_resources='exhaust'` requires","432","knowing the number of candidates, and symmetrically `n_candidates='exhaust'`","433","requires knowing `min_resources`.","434","","435","In general, exhausting the total number of resources leads to a better final","436","candidate parameter, and is slightly more time-intensive.","437","","438",".. _aggressive_elimination:","439","","440","Aggressive elimination of candidates","441","------------------------------------","442","","443","Ideally, we want the last iteration to evaluate ``factor`` candidates (see","444",":ref:`amount_of_resource_and_number_of_candidates`). We then just have to","445","pick the best one. When the number of available resources is small with","446","respect to the number of candidates, the last iteration may have to evaluate","447","more than ``factor`` candidates::","448","","449","    >>> from sklearn.datasets import make_classification","450","    >>> from sklearn.svm import SVC","451","    >>> from sklearn.experimental import enable_successive_halving  # noqa","452","    >>> from sklearn.model_selection import HalvingGridSearchCV","453","    >>> import pandas as pd","454","    >>>","455","    >>>","456","    >>> param_grid = {'kernel': ('linear', 'rbf'),","457","    ...               'C': [1, 10, 100]}","458","    >>> base_estimator = SVC(gamma='scale')","459","    >>> X, y = make_classification(n_samples=1000)","460","    >>> sh = HalvingGridSearchCV(base_estimator, param_grid, cv=5,","461","    ...                          factor=2, max_resources=40,","462","    ...                          aggressive_elimination=False).fit(X, y)","463","    >>> sh.n_resources_","464","    [20, 40]","465","    >>> sh.n_candidates_","466","    [6, 3]","467","","468","Since we cannot use more than ``max_resources=40`` resources, the process","469","has to stop at the second iteration which evaluates more than ``factor=2``","470","candidates.","471","","472","Using the ``aggressive_elimination`` parameter, you can force the search","473","process to end up with less than ``factor`` candidates at the last","474","iteration. To do this, the process will eliminate as many candidates as","475","necessary using ``min_resources`` resources::","476","","477","    >>> sh = HalvingGridSearchCV(base_estimator, param_grid, cv=5,","478","    ...                            factor=2,","479","    ...                            max_resources=40,","480","    ...                            aggressive_elimination=True,","481","    ...                            ).fit(X, y)","482","    >>> sh.n_resources_","483","    [20, 20,  40]","484","    >>> sh.n_candidates_","485","    [6, 3, 2]","486","","487","Notice that we end with 2 candidates at the last iteration since we have","488","eliminated enough candidates during the first iterations, using ``n_resources =","489","min_resources = 20``.","490","","491",".. _successive_halving_cv_results:","492","","493","Analysing results with the `cv_results_` attribute","494","--------------------------------------------------","495","","496","The ``cv_results_`` attribute contains useful information for analysing the","497","results of a search. It can be converted to a pandas dataframe with ``df =","498","pd.DataFrame(est.cv_results_)``. The ``cv_results_`` attribute of","499",":class:`HalvingGridSearchCV` and :class:`HalvingRandomSearchCV` is similar","500","to that of :class:`GridSearchCV` and :class:`RandomizedSearchCV`, with","501","additional information related to the successive halving process.","502","","503","Here is an example with some of the columns of a (truncated) dataframe:","504","","505","====  ======  ===============  =================  =======================================================================================","506","  ..    iter      n_resources    mean_test_score  params","507","====  ======  ===============  =================  =======================================================================================","508","   0       0              125           0.983667  {'criterion': 'entropy', 'max_depth': None, 'max_features': 9, 'min_samples_split': 5}","509","   1       0              125           0.983667  {'criterion': 'gini', 'max_depth': None, 'max_features': 8, 'min_samples_split': 7}","510","   2       0              125           0.983667  {'criterion': 'gini', 'max_depth': None, 'max_features': 10, 'min_samples_split': 10}","511","   3       0              125           0.983667  {'criterion': 'entropy', 'max_depth': None, 'max_features': 6, 'min_samples_split': 6}","512"," ...     ...              ...                ...  ...","513","  15       2              500           0.951958  {'criterion': 'entropy', 'max_depth': None, 'max_features': 9, 'min_samples_split': 10}","514","  16       2              500           0.947958  {'criterion': 'gini', 'max_depth': None, 'max_features': 10, 'min_samples_split': 10}","515","  17       2              500           0.951958  {'criterion': 'gini', 'max_depth': None, 'max_features': 10, 'min_samples_split': 4}","516","  18       3             1000           0.961009  {'criterion': 'entropy', 'max_depth': None, 'max_features': 9, 'min_samples_split': 10}","517","  19       3             1000           0.955989  {'criterion': 'gini', 'max_depth': None, 'max_features': 10, 'min_samples_split': 4}","518","====  ======  ===============  =================  =======================================================================================","519","","520","Each row corresponds to a given parameter combination (a candidate) and a given","521","iteration. The iteration is given by the ``iter`` column. The ``n_resources``","522","column tells you how many resources were used.","523","","524","In the example above, the best parameter combination is ``{'criterion':","525","'entropy', 'max_depth': None, 'max_features': 9, 'min_samples_split': 10}``","526","since it has reached the last iteration (3) with the highest score:","527","0.96.","528","","529",".. topic:: References:","530","","531","    .. [1] K. Jamieson, A. Talwalkar,","532","       `Non-stochastic Best Arm Identification and Hyperparameter","533","       Optimization <http:\/\/proceedings.mlr.press\/v51\/jamieson16.html>`_, in","534","       proc. of Machine Learning Research, 2016.","535","    .. [2] L. Li, K. Jamieson, G. DeSalvo, A. Rostamizadeh, A. Talwalkar,","536","       `Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization","537","       <https:\/\/arxiv.org\/abs\/1603.06560>`_, in Machine Learning Research","538","       18, 2018.","539","","556","scoring function can be specified via the ``scoring`` parameter of most","557","parameter search tools. See :ref:`scoring_parameter` for more details.","564",":class:`GridSearchCV` and :class:`RandomizedSearchCV` allow specifying","565","multiple metrics for the ``scoring`` parameter.","580",":class:`HalvingRandomSearchCV` and :class:`HalvingGridSearchCV` do not support","581","multimetric scoring.","582","","627","Please refer to :ref:`pipeline` for performing parameter searches over","628","pipelines.","639","be fed to the :class:`GridSearchCV` instance) and an **evaluation set**","648","The parameter search tools evaluate each parameter combination on each data","649","fold independently. Computations can be run in parallel by using the keyword","650","``n_jobs=-1``. See function signature for more details, and also the Glossary","651","entry for :term:`n_jobs`."],"delete":["32","Some models allow for specialized, efficient parameter search strategies,","33",":ref:`outlined below <alternative_cv>`.","34","Two generic approaches to sampling search candidates are provided in","38","distribution. After describing these tools we detail","39",":ref:`best practice <grid_search_tips>` applicable to both approaches.","185","scoring function can be specified via the ``scoring`` parameter to","186",":class:`GridSearchCV`, :class:`RandomizedSearchCV` and many of the","187","specialized cross-validation tools described below.","188","See :ref:`scoring_parameter` for more details.","195","``GridSearchCV`` and ``RandomizedSearchCV`` allow specifying multiple metrics","196","for the ``scoring`` parameter.","265","be fed to the ``GridSearchCV`` instance) and an **evaluation set**","274",":class:`GridSearchCV` and :class:`RandomizedSearchCV` evaluate each parameter","275","setting independently.  Computations can be run in parallel if your OS","276","supports it, by using the keyword ``n_jobs=-1``. See function signature for","277","more details."]}],"examples\/model_selection\/plot_successive_halving_iterations.py":[{"add":[],"delete":[]}],"examples\/model_selection\/plot_successive_halving_heatmap.py":[{"add":[],"delete":[]}],"doc\/whats_new\/v0.24.rst":[{"add":["414","- |Feature| Added (experimental) parameter search estimators","415","  :class:`model_selection.HalvingRandomSearchCV` and","416","  :class:`model_selection.HalvingGridSearchCV` which implement Successive","417","  Halving, and can be used as a drop-in replacements for","418","  :class:`model_selection.RandomizedSearchCV` and","419","  :class:`model_selection.GridSearchCV`. :pr:`13900` by `Nicolas Hug`_, `Joel","420","  Nothman`_ and `Andreas M¨¹ller`_.","421",""],"delete":[]}],"sklearn\/model_selection\/_search.py":[{"add":["643","        For example, Successive Halving is implemented by calling","644","        `evaluate_candidates` multiples times (once per iteration of the SH","645","        process), each time passing a different set of candidates with `X`","646","        and `y` of increasing sizes.","651","            This callback accepts:","652","                - a list of candidates, where each candidate is a dict of","653","                  parameter settings.","654","                - an optional `cv` parameter which can be used to e.g.","655","                  evaluate candidates on different dataset splits, or","656","                  evaluate candidates on subsampled data (as done in the","657","                  SucessiveHaling estimators). By default, the original `cv`","658","                  parameter is used, and it is available as a private","659","                  `_checked_cv_orig` attribute.","660","                - an optional `more_results` dict. Each key will be added to","661","                  the `cv_results_` attribute. Values should be lists of","662","                  length `n_candidates`","663","","664","            It returns a dict of all results so far, formatted like","665","            ``cv_results_``.","666","","667","            Important note (relevant whether the default cv is used or not):","668","            in randomized splitters, and unless the random_state parameter of","669","            cv was set to an int, calling cv.split() multiple times will","670","            yield different splits. Since cv.split() is called in","671","            evaluate_candidates, this means that candidates will be evaluated","672","            on different splits each time evaluate_candidates is called. This","673","            might be a methodological issue depending on the search strategy","674","            that you're implementing. To prevent randomized splitters from","675","            being used, you may use _split._yields_constant_splits()","747","        cv_orig = check_cv(self.cv, y, classifier=is_classifier(estimator))","748","        n_splits = cv_orig.get_n_splits(X, y, groups)","767","            all_more_results = defaultdict(list)","769","            def evaluate_candidates(candidate_params, cv=None,","770","                                    more_results=None):","771","                cv = cv or cv_orig","815","                if more_results is not None:","816","                    for key, value in more_results.items():","817","                        all_more_results[key].extend(value)","821","                    all_candidate_params, n_splits, all_out,","822","                    all_more_results)","823","","879","    def _format_results(self, candidate_params, n_splits, out,","880","                        more_results=None):","884","        results = dict(more_results or {})"],"delete":["647","            This callback accepts a list of candidates, where each candidate is","648","            a dict of parameter settings. It returns a dict of all results so","649","            far, formatted like ``cv_results_``.","707","        cv = check_cv(self.cv, y, classifier=is_classifier(estimator))","708","","723","        n_splits = cv.get_n_splits(X, y, groups)","743","            def evaluate_candidates(candidate_params):","790","                    all_candidate_params, n_splits, all_out)","846","    def _format_results(self, candidate_params, n_splits, out):","850","        results = {}"]}]}},"e7bd8a33e580c2ea42dec82bb9aad9e353ec412d":{"changes":{"sklearn\/cluster\/tests\/test_optics.py":"MODIFY","sklearn\/cluster\/optics_.py":"MODIFY"},"diff":{"sklearn\/cluster\/tests\/test_optics.py":[{"add":["100","                   max_eps=20, cluster_method='xi',","110","                   max_eps=20, cluster_method='xi',","129","    rng = np.random.RandomState(0)","132","    C2 = [0, 0] + 50 * rng.randn(n_points_per_cluster, 2)"],"delete":["100","                   max_eps=np.inf, cluster_method='xi',","110","                   max_eps=np.inf, cluster_method='xi',","131","    C2 = [0, 0] + 10 * rng.randn(n_points_per_cluster, 2)"]}],"sklearn\/cluster\/optics_.py":[{"add":["846","                    # Our implementation corrects a mistake in the original","847","                    # paper, i.e., in Definition 11 4c, r(x) < r(sD) should be","848","                    # r(x) > r(sD).","849","                    while (reachability_plot[c_end - 1] > D_max"],"delete":["846","                    while (reachability_plot[c_end - 1] < D_max"]}]}},"703991f3f9bdf5bc95755b84a387b63394a825ee":{"changes":{"doc\/datasets\/index.rst":"MODIFY","doc\/about.rst":"MODIFY","sklearn\/decomposition\/pca.py":"MODIFY"},"diff":{"doc\/datasets\/index.rst":[{"add":[],"delete":["83",".. toctree::","84","    :maxdepth: 2","85","    :hidden:","86","","87","    boston_house_prices","88","    iris","89","    diabetes","90","    digits","91","    linnerud","92","    wine_data","93","    breast_cancer","94","","134",".. toctree::","135","    :maxdepth: 2","136","    :hidden:","137","","138","    olivetti_faces","139","    twenty_newsgroups","140","    labeled_faces","141","    covtype","142","    rcv1","143","    kddcup99","144","    california_housing","145",""]}],"doc\/about.rst":[{"add":["178",".. _Vlad Niculae: https:\/\/vene.ro\/","179",""],"delete":[]}],"sklearn\/decomposition\/pca.py":[{"add":["253","    Tipping, M. E., and Bishop, C. M. (1999). \"Probabilistic principal"],"delete":["253","    `Tipping, M. E., and Bishop, C. M. (1999). \"Probabilistic principal"]}]}},"e7ca6236c50f94777343d8a7a9e8af73b6c24276":{"changes":{"sklearn\/kernel_ridge.py":"MODIFY","sklearn\/exceptions.py":"MODIFY","sklearn\/feature_extraction\/text.py":"MODIFY","sklearn\/covariance\/graph_lasso_.py":"MODIFY","sklearn\/ensemble\/_gb_losses.py":"MODIFY","sklearn\/covariance\/empirical_covariance_.py":"MODIFY","sklearn\/neural_network\/_stochastic_optimizers.py":"MODIFY","sklearn\/neighbors\/lof.py":"MODIFY","sklearn\/decomposition\/base.py":"MODIFY","sklearn\/datasets\/base.py":"MODIFY","sklearn\/tree\/__init__.py":"MODIFY","sklearn\/datasets\/california_housing.py":"MODIFY","sklearn\/cluster\/hierarchical.py":"MODIFY","sklearn\/discriminant_analysis.py":"MODIFY","sklearn\/preprocessing\/data.py":"MODIFY","sklearn\/preprocessing\/label.py":"MODIFY","sklearn\/model_selection\/_validation.py":"MODIFY","sklearn\/decomposition\/dict_learning.py":"MODIFY","sklearn\/metrics\/pairwise.py":"MODIFY","sklearn\/ensemble\/gradient_boosting.py":"MODIFY","sklearn\/pipeline.py":"MODIFY","sklearn\/datasets\/svmlight_format.py":"MODIFY","sklearn\/dummy.py":"MODIFY","sklearn\/svm\/base.py":"MODIFY","sklearn\/ensemble\/weight_boosting.py":"MODIFY","sklearn\/decomposition\/kernel_pca.py":"MODIFY","sklearn\/cluster\/k_means_.py":"MODIFY"},"diff":{"sklearn\/kernel_ridge.py":[{"add":["29","    medium-sized datasets. On the other hand, the learned model is non-sparse"],"delete":["29","    medium-sized datasets. On the other  hand, the learned model is non-sparse"]}],"sklearn\/exceptions.py":[{"add":["31","    ... # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS","116","    ...         gs.fit(X, y)  # This will raise a ValueError since C is < 0"],"delete":["31","    ...                        # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS","116","    ...         gs.fit(X, y)   # This will raise a ValueError since C is < 0"]}],"sklearn\/feature_extraction\/text.py":[{"add":["112","    else:  # assume it's a collection","1498","        if ``use_idf`` is True."],"delete":["112","    else:               # assume it's a collection","1498","        if  ``use_idf`` is True."]}],"sklearn\/covariance\/graph_lasso_.py":[{"add":["339","    >>> true_cov = np.array([[0.8, 0.0, 0.2, 0.0],","340","    ...                      [0.0, 0.4, 0.0, 0.0],","341","    ...                      [0.2, 0.0, 0.3, 0.1],","342","    ...                      [0.0, 0.0, 0.1, 0.7]])","594","    >>> true_cov = np.array([[0.8, 0.0, 0.2, 0.0],","595","    ...                      [0.0, 0.4, 0.0, 0.0],","596","    ...                      [0.2, 0.0, 0.3, 0.1],","597","    ...                      [0.0, 0.0, 0.1, 0.7]])"],"delete":["339","    >>> true_cov = np.array([[.8, 0., .2, 0.],","340","    ...                      [0., .4, 0., 0.],","341","    ...                      [.2, 0., .3, .1],","342","    ...                      [0., 0., .1, .7]])","594","    >>> true_cov = np.array([[.8, 0., .2, 0.],","595","    ...                      [0., .4, 0., 0.],","596","    ...                      [.2, 0., .3, .1],","597","    ...                      [0., 0., .1, .7]])"]}],"sklearn\/ensemble\/_gb_losses.py":[{"add":["881","    'deviance': None,  # for both, multinomial and binomial"],"delete":["881","    'deviance': None,    # for both, multinomial and binomial"]}],"sklearn\/covariance\/empirical_covariance_.py":[{"add":["124","    ...                             cov=real_cov,","125","    ...                             size=500)"],"delete":["124","    ...                                   cov=real_cov,","125","    ...                                   size=500)"]}],"sklearn\/neural_network\/_stochastic_optimizers.py":[{"add":["3","# Authors: Jiyuan Qian <jq401@nyu.edu>"],"delete":["3","# Authors:  Jiyuan Qian <jq401@nyu.edu>"]}],"sklearn\/neighbors\/lof.py":[{"add":["502","        # 1e-10 to avoid `nan' when nb of duplicates > n_neighbors_:"],"delete":["502","        #  1e-10 to avoid `nan' when nb of duplicates > n_neighbors_:"]}],"sklearn\/decomposition\/base.py":[{"add":["29","        where S**2 contains the explained variances, and sigma2 contains the"],"delete":["29","        where  S**2 contains the explained variances, and sigma2 contains the"]}],"sklearn\/datasets\/base.py":[{"add":["570","    ==============   ==================","571","    Samples total    442","572","    Dimensionality   10","573","    Features         real, -.2 < x < .2","574","    Targets          integer 25 - 346","575","    ==============   ==================","623","    ==============   ============================","624","    Samples total    20","625","    Dimensionality   3 (for both data and target)","626","    Features         integer","627","    Targets          integer","628","    ==============   ============================","687","    ==============   ==============","688","    Samples total               506","689","    Dimensionality               13","690","    Features         real, positive","691","    Targets           real 5. - 50.","692","    ==============   =============="],"delete":["570","    ==============      ==================","571","    Samples total       442","572","    Dimensionality      10","573","    Features            real, -.2 < x < .2","574","    Targets             integer 25 - 346","575","    ==============      ==================","623","    ==============    ============================","624","    Samples total     20","625","    Dimensionality    3 (for both data and target)","626","    Features          integer","627","    Targets           integer","628","    ==============    ============================","687","    ==============     ==============","688","    Samples total                 506","689","    Dimensionality                 13","690","    Features           real, positive","691","    Targets             real 5. - 50.","692","    ==============     =============="]}],"sklearn\/tree\/__init__.py":[{"add":["13","           \"plot_tree\", \"export_text\"]"],"delete":["13","           \"plot_tree\",  \"export_text\"]"]}],"sklearn\/datasets\/california_housing.py":[{"add":["52","    ==============   ==============","53","    Samples total             20640","54","    Dimensionality                8","55","    Features                   real","56","    Target           real 0.15 - 5.","57","    ==============   =============="],"delete":["52","    ==============     ==============","53","    Samples total               20640","54","    Dimensionality                  8","55","    Features                     real","56","    Target             real 0.15 - 5.","57","    ==============     =============="]}],"sklearn\/cluster\/hierarchical.py":[{"add":["150","        feature matrix representing n_samples samples to be clustered","221","        from scipy.cluster import hierarchy  # imports PIL","435","        from scipy.cluster import hierarchy  # imports PIL","599","# Functions for cutting hierarchical clustering tree"],"delete":["150","        feature matrix  representing n_samples samples to be clustered","221","        from scipy.cluster import hierarchy     # imports PIL","435","        from scipy.cluster import hierarchy     # imports PIL","599","# Functions for cutting  hierarchical clustering tree"]}],"sklearn\/discriminant_analysis.py":[{"add":["718","        norm2 = np.array(norm2).T  # shape = [len(X), n_classes]"],"delete":["718","        norm2 = np.array(norm2).T   # shape = [len(X), n_classes]"]}],"sklearn\/preprocessing\/data.py":[{"add":["2264","            # for inverse transform, match a uniform distribution"],"delete":["2264","            #  for inverse transform, match a uniform distribution"]}],"sklearn\/preprocessing\/label.py":[{"add":["422","        The output of transform is sometimes referred to as"],"delete":["422","        The output of transform is sometimes referred to    as"]}],"sklearn\/model_selection\/_validation.py":[{"add":["1120","                   random_state=None, error_score='raise-deprecating'):"],"delete":["1120","                   random_state=None,  error_score='raise-deprecating'):"]}],"sklearn\/decomposition\/dict_learning.py":[{"add":["173","                         '\"lasso_cd\", \"lasso\", \"threshold\" or \"omp\", got %s.'"],"delete":["173","                         '\"lasso_cd\",  \"lasso\", \"threshold\" or \"omp\", got %s.'"]}],"sklearn\/metrics\/pairwise.py":[{"add":["1171","    ============   ====================================","1172","    metric         Function","1173","    ============   ====================================","1174","    'cityblock'    metrics.pairwise.manhattan_distances","1175","    'cosine'       metrics.pairwise.cosine_distances","1176","    'euclidean'    metrics.pairwise.euclidean_distances","1177","    'haversine'    metrics.pairwise.haversine_distances","1178","    'l1'           metrics.pairwise.manhattan_distances","1179","    'l2'           metrics.pairwise.euclidean_distances","1180","    'manhattan'    metrics.pairwise.manhattan_distances","1181","    ============   ===================================="],"delete":["1171","    ============     ====================================","1172","    metric           Function","1173","    ============     ====================================","1174","    'cityblock'      metrics.pairwise.manhattan_distances","1175","    'cosine'         metrics.pairwise.cosine_distances","1176","    'euclidean'      metrics.pairwise.euclidean_distances","1177","    'haversine'      metrics.pairwise.haversine_distances","1178","    'l1'             metrics.pairwise.manhattan_distances","1179","    'l2'             metrics.pairwise.euclidean_distances","1180","    'manhattan'      metrics.pairwise.manhattan_distances","1181","    ============     ===================================="]}],"sklearn\/ensemble\/gradient_boosting.py":[{"add":["1696","        X = check_array(X, dtype=DTYPE, order=\"C\", accept_sparse='csr')","2087","        X = check_array(X, dtype=DTYPE, order=\"C\", accept_sparse='csr')","2529","        X = check_array(X, dtype=DTYPE, order=\"C\", accept_sparse='csr')"],"delete":["1696","        X = check_array(X, dtype=DTYPE, order=\"C\",  accept_sparse='csr')","2087","        X = check_array(X, dtype=DTYPE, order=\"C\",  accept_sparse='csr')","2529","        X = check_array(X, dtype=DTYPE, order=\"C\",  accept_sparse='csr')"]}],"sklearn\/pipeline.py":[{"add":["97","    ... # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE","102","    >>> anova_svm.score(X, y)  # doctest: +ELLIPSIS","673","    ... # doctest: +NORMALIZE_WHITESPACE","784","    >>> union.fit_transform(X)  # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS","1010","    >>> make_union(PCA(), TruncatedSVD())  # doctest: +NORMALIZE_WHITESPACE"],"delete":["97","    ...                      # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE","102","    >>> anova_svm.score(X, y)                        # doctest: +ELLIPSIS","673","    ...     # doctest: +NORMALIZE_WHITESPACE","784","    >>> union.fit_transform(X)    # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS","1010","    >>> make_union(PCA(), TruncatedSVD())    # doctest: +NORMALIZE_WHITESPACE"]}],"sklearn\/datasets\/svmlight_format.py":[{"add":["437","            comment.decode(\"ascii\")  # just for the exception"],"delete":["437","            comment.decode(\"ascii\")     # just for the exception"]}],"sklearn\/dummy.py":[{"add":["494","        y : array, shape = [n_samples] or [n_samples, n_outputs]","497","        y_std : array, shape = [n_samples] or [n_samples, n_outputs]"],"delete":["494","        y : array, shape = [n_samples]  or [n_samples, n_outputs]","497","        y_std : array, shape = [n_samples]  or [n_samples, n_outputs]"]}],"sklearn\/svm\/base.py":[{"add":["295","        else:  # regression"],"delete":["295","        else:   # regression"]}],"sklearn\/ensemble\/weight_boosting.py":[{"add":["687","        else:  # self.algorithm == \"SAMME\"","782","        else:  # self.algorithm == \"SAMME\""],"delete":["687","        else:   # self.algorithm == \"SAMME\"","782","        else:   # self.algorithm == \"SAMME\""]}],"sklearn\/decomposition\/kernel_pca.py":[{"add":["232","        #     then Phi(X)v  is an eigenvector of Phi(X)Phi(X)'","234","        #     then Phi(X)'u is an eigenvector of Phi(X)Phi(X)'"],"delete":["232","        #                      then Phi(X)v  is an eigenvector of Phi(X)Phi(X)'","234","        #                      then Phi(X)'u is an eigenvector of Phi(X)Phi(X)'"]}],"sklearn\/cluster\/k_means_.py":[{"add":["708","    x_squared_norms : array, shape (n_samples,), optional","1421","    ...                          random_state=0,","1422","    ...                          batch_size=6)","1432","    ...                          random_state=0,","1433","    ...                          batch_size=6,","1434","    ...                          max_iter=10).fit(X)"],"delete":["708","    x_squared_norms :  array, shape (n_samples,), optional","1421","    ...         random_state=0,","1422","    ...         batch_size=6)","1432","    ...         random_state=0,","1433","    ...         batch_size=6,","1434","    ...         max_iter=10).fit(X)"]}]}},"5c8167a159d8d0a24204a4757822dc0860f35606":{"changes":{"sklearn\/svm\/tests\/test_sparse.py":"MODIFY","doc\/tutorial\/basic\/tutorial.rst":"MODIFY","doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/model_selection\/tests\/test_search.py":"MODIFY","sklearn\/svm\/classes.py":"MODIFY","sklearn\/svm\/base.py":"MODIFY","doc\/modules\/model_evaluation.rst":"MODIFY","sklearn\/svm\/tests\/test_svm.py":"MODIFY"},"diff":{"sklearn\/svm\/tests\/test_sparse.py":[{"add":["89","            clf = svm.SVC(gamma=1, kernel=kernel, probability=True,","91","            sp_clf = svm.SVC(gamma=1, kernel=kernel, probability=True,","295","    clf = svm.OneClassSVM(gamma=1, kernel=kernel)","296","    sp_clf = svm.OneClassSVM(gamma=1, kernel=kernel)"],"delete":["89","            clf = svm.SVC(gamma='scale', kernel=kernel, probability=True,","91","            sp_clf = svm.SVC(gamma='scale', kernel=kernel, probability=True,","295","    clf = svm.OneClassSVM(gamma='scale', kernel=kernel)","296","    sp_clf = svm.OneClassSVM(gamma='scale', kernel=kernel)"]}],"doc\/tutorial\/basic\/tutorial.rst":[{"add":["346","  array([0, 0, 0, 1, 0])"],"delete":["346","  array([1, 0, 1, 1, 0])"]}],"doc\/whats_new\/v0.20.rst":[{"add":["38",":mod:`sklearn.feature_extraction`","39",".................................","40","","41","- |Fix| Fixed a bug in :class:`feature_extraction.text.CountVectorizer` which ","42","  would result in the sparse feature matrix having conflicting `indptr` and","43","  `indices` precisions under very large vocabularies. :issue:`11295` by","44","  :user:`Gabriel Vacaliuc <gvacaliuc>`.","45","","78",":mod:`sklearn.svm`","79","..................","81","- |FIX| Fixed a bug in :class:`svm.SVC`, :class:`svm.NuSVC`, :class:`svm.SVR`,","82","  :class:`svm.NuSVR` and :class:`svm.OneClassSVM` where the ``scale`` option","83","  of parameter ``gamma`` is erroneously defined as","84","  ``1 \/ (n_features * X.std())``. It's now defined as","85","  ``1 \/ (n_features * X.var())``.","86","  :issue:`13221` by :user:`Hanmin Qin <qinhanmin2014>`."],"delete":["70",":mod:`sklearn.feature_extraction.text`","71","......................................","73","- |Fix| Fixed a bug in :class:`feature_extraction.text.CountVectorizer` which ","74","  would result in the sparse feature matrix having conflicting `indptr` and","75","  `indices` precisions under very large vocabularies. :issue:`11295` by","76","  :user:`Gabriel Vacaliuc <gvacaliuc>`."]}],"sklearn\/model_selection\/tests\/test_search.py":[{"add":["1712","    grid = GridSearchCV(SVC(gamma='scale', random_state=0),","1713","                        param_grid={'C': [10]}, cv=3)","1717","    grid = GridSearchCV(SVC(gamma='scale', random_state=0),","1718","                        param_grid={'C': [10]}, cv=5)","1722","    grid = GridSearchCV(SVC(gamma='scale', random_state=0),","1723","                        param_grid={'C': [10]}, cv=2)","1727","    grid = GridSearchCV(SVC(gamma='scale', random_state=0),","1728","                        param_grid={'C': [10]}, cv=KFold(2))"],"delete":["1712","    grid = GridSearchCV(SVC(gamma='scale'), param_grid={'C': [1]}, cv=3)","1716","    grid = GridSearchCV(SVC(gamma='scale'), param_grid={'C': [1]}, cv=5)","1720","    grid = GridSearchCV(SVC(gamma='scale'), param_grid={'C': [1]}, cv=2)","1724","    grid = GridSearchCV(SVC(gamma='scale'), param_grid={'C': [1]}, cv=KFold(2))"]}],"sklearn\/svm\/classes.py":[{"add":["465","        if ``gamma='scale'`` is passed then it uses 1 \/ (n_features * X.var())","653","        if ``gamma='scale'`` is passed then it uses 1 \/ (n_features * X.var())","814","        if ``gamma='scale'`` is passed then it uses 1 \/ (n_features * X.var())","950","        if ``gamma='scale'`` is passed then it uses 1 \/ (n_features * X.var())","1067","        if ``gamma='scale'`` is passed then it uses 1 \/ (n_features * X.var())"],"delete":["465","        if ``gamma='scale'`` is passed then it uses 1 \/ (n_features * X.std())","653","        if ``gamma='scale'`` is passed then it uses 1 \/ (n_features * X.std())","814","        if ``gamma='scale'`` is passed then it uses 1 \/ (n_features * X.std())","950","        if ``gamma='scale'`` is passed then it uses 1 \/ (n_features * X.std())","1067","        if ``gamma='scale'`` is passed then it uses 1 \/ (n_features * X.std())"]}],"sklearn\/svm\/base.py":[{"add":["171","                # var = E[X^2] - E[X]^2","172","                X_var = (X.multiply(X)).mean() - (X.mean()) ** 2","174","                X_var = X.var()","176","                if X_var != 0:","177","                    self._gamma = 1.0 \/ (X.shape[1] * X_var)","183","                if kernel_uses_gamma and not np.isclose(X_var, 1.0):"],"delete":["171","                # std = sqrt(E[X^2] - E[X]^2)","172","                X_std = np.sqrt((X.multiply(X)).mean() - (X.mean())**2)","174","                X_std = X.std()","176","                if X_std != 0:","177","                    self._gamma = 1.0 \/ (X.shape[1] * X_std)","183","                if kernel_uses_gamma and not np.isclose(X_std, 1.0):"]}],"doc\/modules\/model_evaluation.rst":[{"add":["104","    array([0.96..., 0.96..., 0.96..., 0.93..., 1.        ])","1949","  0.94..."],"delete":["104","    array([0.96..., 1.  ..., 0.96..., 0.96..., 1.        ])","1949","  0.97..."]}],"sklearn\/svm\/tests\/test_svm.py":[{"add":["245","    assert_array_equal(pred, [1, -1, -1])","247","    assert_array_almost_equal(clf.intercept_, [-1.218], decimal=3)","249","                              [[0.750, 0.750, 0.750, 0.750]],","1005","    assert_almost_equal(clf._gamma, 4)","1007","    # X_var ~= 1 shouldn't raise warning, for when"],"delete":["245","    assert_array_equal(pred, [-1, -1, -1])","247","    assert_array_almost_equal(clf.intercept_, [-1.117], decimal=3)","249","                              [[0.681, 0.139, 0.68, 0.14, 0.68, 0.68]],","1005","    assert_equal(clf._gamma, 2.)","1007","    # X_std ~= 1 shouldn't raise warning, for when"]}]}},"98aefc1fdcfc53f5c8572b02c7f0c5bcba23e351":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/metrics\/pairwise.py":"MODIFY","sklearn\/metrics\/tests\/test_pairwise.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["4",".. _changes_0_21_2:","5","","6","Version 0.21.2","7","==============","8","","9","**June 2019**","10","","11","Changelog","12","---------","13","","14",":mod:`sklearn.metrics`","15","......................","16","","17","- |Fix| Fixed a bug in :func:`euclidean_distances` where a part of the distance","18","  matrix was left un-instanciated for suffiently large float32 datasets","19","  (regression introduced in 0.21) :issue:`13910`","20","  by :user:`J¨¦r¨¦mie du Boisberranger <jeremiedbb>`.","21",""],"delete":[]}],"sklearn\/metrics\/pairwise.py":[{"add":["285","def _euclidean_distances_upcast(X, XX=None, Y=None, YY=None, batch_size=None):","300","    if batch_size is None:","301","        x_density = X.nnz \/ np.prod(X.shape) if issparse(X) else 1","302","        y_density = Y.nnz \/ np.prod(Y.shape) if issparse(Y) else 1","304","        # Allow 10% more memory than X, Y and the distance matrix take (at","305","        # least 10MiB)","306","        maxmem = max(","307","            ((x_density * n_samples_X + y_density * n_samples_Y) * n_features","308","             + (x_density * n_samples_X * y_density * n_samples_Y)) \/ 10,","309","            10 * 2 ** 17)","311","        # The increase amount of memory in 8-byte blocks is:","312","        # - x_density * batch_size * n_features (copy of chunk of X)","313","        # - y_density * batch_size * n_features (copy of chunk of Y)","314","        # - batch_size * batch_size (chunk of distance matrix)","315","        # Hence x? + (xd+yd)kx = M, where x=batch_size, k=n_features, M=maxmem","316","        #                                 xd=x_density and yd=y_density","317","        tmp = (x_density + y_density) * n_features","318","        batch_size = (-tmp + np.sqrt(tmp ** 2 + 4 * maxmem)) \/ 2","319","        batch_size = max(int(batch_size), 1)","321","    x_batches = gen_batches(n_samples_X, batch_size)","330","        y_batches = gen_batches(n_samples_Y, batch_size)","331",""],"delete":["285","def _euclidean_distances_upcast(X, XX=None, Y=None, YY=None):","300","    x_density = X.nnz \/ np.prod(X.shape) if issparse(X) else 1","301","    y_density = Y.nnz \/ np.prod(Y.shape) if issparse(Y) else 1","303","    # Allow 10% more memory than X, Y and the distance matrix take (at least","304","    # 10MiB)","305","    maxmem = max(","306","        ((x_density * n_samples_X + y_density * n_samples_Y) * n_features","307","         + (x_density * n_samples_X * y_density * n_samples_Y)) \/ 10,","308","        10 * 2 ** 17)","310","    # The increase amount of memory in 8-byte blocks is:","311","    # - x_density * batch_size * n_features (copy of chunk of X)","312","    # - y_density * batch_size * n_features (copy of chunk of Y)","313","    # - batch_size * batch_size (chunk of distance matrix)","314","    # Hence x? + (xd+yd)kx = M, where x=batch_size, k=n_features, M=maxmem","315","    #                                 xd=x_density and yd=y_density","316","    tmp = (x_density + y_density) * n_features","317","    batch_size = (-tmp + np.sqrt(tmp ** 2 + 4 * maxmem)) \/ 2","318","    batch_size = max(int(batch_size), 1)","320","    x_batches = gen_batches(X.shape[0], batch_size)","321","    y_batches = gen_batches(Y.shape[0], batch_size)"]}],"sklearn\/metrics\/tests\/test_pairwise.py":[{"add":["50","from sklearn.metrics.pairwise import _euclidean_distances_upcast","690","@pytest.mark.parametrize(\"batch_size\", [None, 5, 7, 101])","691","@pytest.mark.parametrize(\"x_array_constr\", [np.array, csr_matrix],","692","                         ids=[\"dense\", \"sparse\"])","693","@pytest.mark.parametrize(\"y_array_constr\", [np.array, csr_matrix],","694","                         ids=[\"dense\", \"sparse\"])","695","def test_euclidean_distances_upcast(batch_size, x_array_constr,","696","                                    y_array_constr):","697","    # check batches handling when Y != X (#13910)","698","    rng = np.random.RandomState(0)","699","    X = rng.random_sample((100, 10)).astype(np.float32)","700","    X[X < 0.8] = 0","701","    Y = rng.random_sample((10, 10)).astype(np.float32)","702","    Y[Y < 0.8] = 0","703","","704","    expected = cdist(X, Y)","705","","706","    X = x_array_constr(X)","707","    Y = y_array_constr(Y)","708","    distances = _euclidean_distances_upcast(X, Y=Y, batch_size=batch_size)","709","    distances = np.sqrt(np.maximum(distances, 0))","710","","711","    # the default rtol=1e-7 is too close to the float32 precision","712","    # and fails due too rounding errors.","713","    assert_allclose(distances, expected, rtol=1e-6)","714","","715","","716","@pytest.mark.parametrize(\"batch_size\", [None, 5, 7, 101])","717","@pytest.mark.parametrize(\"x_array_constr\", [np.array, csr_matrix],","718","                         ids=[\"dense\", \"sparse\"])","719","def test_euclidean_distances_upcast_sym(batch_size, x_array_constr):","720","    # check batches handling when X is Y (#13910)","721","    rng = np.random.RandomState(0)","722","    X = rng.random_sample((100, 10)).astype(np.float32)","723","    X[X < 0.8] = 0","724","","725","    expected = squareform(pdist(X))","726","","727","    X = x_array_constr(X)","728","    distances = _euclidean_distances_upcast(X, Y=X, batch_size=batch_size)","729","    distances = np.sqrt(np.maximum(distances, 0))","730","","731","    # the default rtol=1e-7 is too close to the float32 precision","732","    # and fails due too rounding errors.","733","    assert_allclose(distances, expected, rtol=1e-6)","734","","735",""],"delete":[]}]}}}