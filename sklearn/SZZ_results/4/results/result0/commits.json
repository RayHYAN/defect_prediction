{"70fd42eec369e6bdb1f1a3f3578eec13c1d69abe":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/feature_extraction\/text.py":"MODIFY","sklearn\/feature_extraction\/tests\/test_text.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["29","- :class:`sklearn.feature_extraction.text.HashingVectorizer`,","30","  :class:`sklearn.feature_extraction.text.TfidfVectorizer`, and","31","  :class:`sklearn.feature_extraction.text.CountVectorizer` |API|","270",":mod:`sklearn.feature_extraction`","271",".................................","272","","273","- |API| If ``input='file'`` or ``input='filename'``, and a callable is given","274","  as the ``analyzer``, :class:`sklearn.feature_extraction.text.HashingVectorizer`,","275","  :class:`sklearn.feature_extraction.text.TfidfVectorizer`, and","276","  :class:`sklearn.feature_extraction.text.CountVectorizer` now read the data","277","  from the file(s) and then pass it to the given ``analyzer``, instead of","278","  passing the file name(s) or the file object(s) to the analyzer.","279","  :issue:`13641` by `Adrin Jalali`_.","280",""],"delete":[]}],"sklearn\/feature_extraction\/text.py":[{"add":["33","from ..exceptions import ChangedBehaviorWarning","307","    def _validate_custom_analyzer(self):","308","        # This is to check if the given custom analyzer expects file or a","309","        # filename instead of data.","310","        # Behavior changed in v0.21, function could be removed in v0.23","311","        import tempfile","312","        with tempfile.NamedTemporaryFile() as f:","313","            fname = f.name","314","        # now we're sure fname doesn't exist","315","","316","        msg = (\"Since v0.21, vectorizers pass the data to the custom analyzer \"","317","               \"and not the file names or the file objects. This warning \"","318","               \"will be removed in v0.23.\")","319","        try:","320","            self.analyzer(fname)","321","        except FileNotFoundError:","322","            warnings.warn(msg, ChangedBehaviorWarning)","323","        except AttributeError as e:","324","            if str(e) == \"'str' object has no attribute 'read'\":","325","                warnings.warn(msg, ChangedBehaviorWarning)","326","        except Exception:","327","            pass","328","","332","            if self.input in ['file', 'filename']:","333","                self._validate_custom_analyzer()","334","            return lambda doc: self.analyzer(self.decode(doc))","517","        .. versionchanged:: 0.21","518","        Since v0.21, if ``input`` is ``filename`` or ``file``, the data is","519","        first read from the file and then passed to the given callable","520","        analyzer.","521","","777","        .. versionchanged:: 0.21","778","        Since v0.21, if ``input`` is ``filename`` or ``file``, the data is","779","        first read from the file and then passed to the given callable","780","        analyzer.","781","","1406","        .. versionchanged:: 0.21","1407","        Since v0.21, if ``input`` is ``filename`` or ``file``, the data is","1408","        first read from the file and then passed to the given callable","1409","        analyzer.","1410",""],"delete":["309","            return self.analyzer"]}],"sklearn\/feature_extraction\/tests\/test_text.py":[{"add":["31","from sklearn.exceptions import ChangedBehaviorWarning","1199","","1200","","1201","@pytest.mark.parametrize('Estimator',","1202","                         [CountVectorizer, TfidfVectorizer, HashingVectorizer])","1203","@pytest.mark.parametrize(","1204","    'input_type, err_type, err_msg',","1205","    [('filename', FileNotFoundError, ''),","1206","     ('file', AttributeError, \"'str' object has no attribute 'read'\")]","1207",")","1208","def test_callable_analyzer_error(Estimator, input_type, err_type, err_msg):","1209","    data = ['this is text, not file or filename']","1210","    with pytest.raises(err_type, match=err_msg):","1211","        Estimator(analyzer=lambda x: x.split(),","1212","                  input=input_type).fit_transform(data)","1213","","1214","","1215","@pytest.mark.parametrize('Estimator',","1216","                         [CountVectorizer, TfidfVectorizer, HashingVectorizer])","1217","@pytest.mark.parametrize(","1218","    'analyzer', [lambda doc: open(doc, 'r'), lambda doc: doc.read()]","1219",")","1220","@pytest.mark.parametrize('input_type', ['file', 'filename'])","1221","def test_callable_analyzer_change_behavior(Estimator, analyzer, input_type):","1222","    data = ['this is text, not file or filename']","1223","    warn_msg = 'Since v0.21, vectorizer'","1224","    with pytest.raises((FileNotFoundError, AttributeError)):","1225","        with pytest.warns(ChangedBehaviorWarning, match=warn_msg) as records:","1226","            Estimator(analyzer=analyzer, input=input_type).fit_transform(data)","1227","    assert len(records) == 1","1228","    assert warn_msg in str(records[0])","1229","","1230","","1231","@pytest.mark.parametrize('Estimator',","1232","                         [CountVectorizer, TfidfVectorizer, HashingVectorizer])","1233","def test_callable_analyzer_reraise_error(tmpdir, Estimator):","1234","    # check if a custom exception from the analyzer is shown to the user","1235","    def analyzer(doc):","1236","        raise Exception(\"testing\")","1237","","1238","    f = tmpdir.join(\"file.txt\")","1239","    f.write(\"sample content\\n\")","1240","","1241","    with pytest.raises(Exception, match=\"testing\"):","1242","        Estimator(analyzer=analyzer, input='file').fit_transform([f])"],"delete":[]}]}},"8a604f7f87ce8a9b8dca83902dd090b296a37a37":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/cluster\/k_means_.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["17",":mod:`sklearn.cluster`","18","......................","19","","20","- |Fix| Fixed a bug in :class:`cluster.KMeans` where computation was single","21","  threaded when `n_jobs > 1` or `n_jobs = -1`.","22","  :issue:`12949` by :user:`Prabakaran Kumaresshan <nixphix>`.","23",""],"delete":[]}],"sklearn\/cluster\/k_means_.py":[{"add":["369","    if effective_n_jobs(n_jobs) == 1:","870","    ...               [10, 2], [10, 4], [10, 0]])","873","    array([1, 1, 1, 0, 0, 0], dtype=int32)","874","    >>> kmeans.predict([[0, 0], [12, 3]])","875","    array([1, 0], dtype=int32)","877","    array([[10.,  2.],","878","           [ 1.,  2.]])"],"delete":["369","    if effective_n_jobs(n_jobs):","870","    ...               [4, 2], [4, 4], [4, 0]])","873","    array([0, 0, 0, 1, 1, 1], dtype=int32)","874","    >>> kmeans.predict([[0, 0], [4, 4]])","875","    array([0, 1], dtype=int32)","877","    array([[1., 2.],","878","           [4., 2.]])"]}]}},"db2290ce38aea1d1d561e0966f952e9e7be05368":{"changes":{"doc\/conf.py":"MODIFY","doc\/sphinxext\/custom_references_resolver.py":"ADD","examples\/cluster\/plot_dict_face_patches.py":"MODIFY"},"diff":{"doc\/conf.py":[{"add":["39","    'custom_references_resolver'","105","# sklearn uses a custom extension: `custom_references_resolver` to modify","106","# the order of link resolution for the 'any' role. It resolves python class","107","# links first before resolving 'std' domain links. Unresolved roles are","108","# considered to be <code> blocks."],"delete":[]}],"doc\/sphinxext\/custom_references_resolver.py":[{"add":[],"delete":[]}],"examples\/cluster\/plot_dict_face_patches.py":[{"add":["11","500 of these patches (using 10 images), we run the","12",":func:`~sklearn.cluster.MiniBatchKMeans.partial_fit` method"],"delete":["11","500 of these patches (using 10 images), we run the `partial_fit` method"]}]}},"16100944b9f5a4d611a48eb372a39144b790d6a0":{"changes":{"sklearn\/utils\/fixes.py":"MODIFY","sklearn\/ensemble\/iforest.py":"MODIFY","sklearn\/ensemble\/tests\/test_iforest.py":"MODIFY"},"diff":{"sklearn\/utils\/fixes.py":[{"add":[],"delete":["31","# < numpy 1.8.0","32","euler_gamma = getattr(np, 'euler_gamma',","33","                      0.577215664901532860606512090082402431)","34",""]}],"sklearn\/ensemble\/iforest.py":[{"add":["6","import numpy as np","7","from scipy.sparse import issparse","8","from warnings import warn","9","","443","            return 2. * (np.log(n_samples_leaf - 1.) + np.euler_gamma) - 2. * (","457","            np.log(n_samples_leaf[not_mask] - 1.) + np.euler_gamma) - 2. * ("],"delete":["5","import numpy as np","6","from warnings import warn","7","from sklearn.utils.fixes import euler_gamma","8","","9","from scipy.sparse import issparse","10","","445","            return 2. * (np.log(n_samples_leaf - 1.) + euler_gamma) - 2. * (","459","            np.log(n_samples_leaf[not_mask] - 1.) + euler_gamma) - 2. * ("]}],"sklearn\/ensemble\/tests\/test_iforest.py":[{"add":["265","    result_one = 2. * (np.log(4.) + np.euler_gamma) - 2. * 4. \/ 5.","266","    result_two = 2. * (np.log(998.) + np.euler_gamma) - 2. * 998. \/ 999."],"delete":["12","from sklearn.utils.fixes import euler_gamma","266","    result_one = 2. * (np.log(4.) + euler_gamma) - 2. * 4. \/ 5.","267","    result_two = 2. * (np.log(998.) + euler_gamma) - 2. * 998. \/ 999."]}]}},"3aefc834dce72e850bff48689bea3c7dff5f3fad":{"changes":{"examples\/gaussian_process\/plot_gpr_prior_posterior.py":"MODIFY","examples\/exercises\/plot_iris_exercise.py":"MODIFY","examples\/gaussian_process\/plot_gpc.py":"MODIFY","build_tools\/circle\/build_doc.sh":"MODIFY",".circleci\/config.yml":"MODIFY","doc\/modules\/gaussian_process.rst":"MODIFY","doc\/modules\/calibration.rst":"MODIFY","examples\/calibration\/plot_calibration_multiclass.py":"MODIFY","examples\/gaussian_process\/plot_gpr_noisy.py":"MODIFY"},"diff":{"examples\/gaussian_process\/plot_gpr_prior_posterior.py":[{"add":["35","for kernel in kernels:","40","    plt.figure(figsize=(8, 8))"],"delete":["35","for fig_index, kernel in enumerate(kernels):","40","    plt.figure(fig_index, figsize=(8, 8))"]}],"examples\/exercises\/plot_iris_exercise.py":[{"add":["37","for kernel in ('linear', 'rbf', 'poly'):","41","    plt.figure()"],"delete":["37","for fig_num, kernel in enumerate(('linear', 'rbf', 'poly')):","41","    plt.figure(fig_num)"]}],"examples\/gaussian_process\/plot_gpc.py":[{"add":["65","plt.figure()","82","plt.figure()"],"delete":["65","plt.figure(0)","82","plt.figure(1)"]}],"build_tools\/circle\/build_doc.sh":[{"add":["103","    latexmk gsfonts"],"delete":["103","    latexmk"]}],".circleci\/config.yml":[{"add":["5","      - image: circleci\/python:3.6","34","      - image: circleci\/python:3.6","63","      - image: circleci\/python:3.6","92","      - image: circleci\/python:3.6"],"delete":["5","      - image: circleci\/python:3.6.1","34","      - image: circleci\/python:3.6.1","63","      - image: circleci\/python:3.6.1","92","      - image: circleci\/python:3.6.1"]}],"doc\/modules\/gaussian_process.rst":[{"add":["90",".. figure:: ..\/auto_examples\/gaussian_process\/images\/sphx_glr_plot_gpr_noisy_001.png","97",".. figure:: ..\/auto_examples\/gaussian_process\/images\/sphx_glr_plot_gpr_noisy_002.png","108",".. figure:: ..\/auto_examples\/gaussian_process\/images\/sphx_glr_plot_gpr_noisy_003.png","308",".. figure:: ..\/auto_examples\/gaussian_process\/images\/sphx_glr_plot_gpc_001.png","312",".. figure:: ..\/auto_examples\/gaussian_process\/images\/sphx_glr_plot_gpc_002.png","495",".. figure:: ..\/auto_examples\/gaussian_process\/images\/sphx_glr_plot_gpr_prior_posterior_001.png","536",".. figure:: ..\/auto_examples\/gaussian_process\/images\/sphx_glr_plot_gpr_prior_posterior_005.png","558",".. figure:: ..\/auto_examples\/gaussian_process\/images\/sphx_glr_plot_gpr_prior_posterior_002.png","576",".. figure:: ..\/auto_examples\/gaussian_process\/images\/sphx_glr_plot_gpr_prior_posterior_003.png","596",".. figure:: ..\/auto_examples\/gaussian_process\/images\/sphx_glr_plot_gpr_prior_posterior_004.png"],"delete":["90",".. figure:: ..\/auto_examples\/gaussian_process\/images\/sphx_glr_plot_gpr_noisy_000.png","97",".. figure:: ..\/auto_examples\/gaussian_process\/images\/sphx_glr_plot_gpr_noisy_001.png","108",".. figure:: ..\/auto_examples\/gaussian_process\/images\/sphx_glr_plot_gpr_noisy_002.png","308",".. figure:: ..\/auto_examples\/gaussian_process\/images\/sphx_glr_plot_gpc_000.png","312",".. figure:: ..\/auto_examples\/gaussian_process\/images\/sphx_glr_plot_gpc_001.png","495",".. figure:: ..\/auto_examples\/gaussian_process\/images\/sphx_glr_plot_gpr_prior_posterior_000.png","536",".. figure:: ..\/auto_examples\/gaussian_process\/images\/sphx_glr_plot_gpr_prior_posterior_004.png","558",".. figure:: ..\/auto_examples\/gaussian_process\/images\/sphx_glr_plot_gpr_prior_posterior_001.png","576",".. figure:: ..\/auto_examples\/gaussian_process\/images\/sphx_glr_plot_gpr_prior_posterior_002.png","596",".. figure:: ..\/auto_examples\/gaussian_process\/images\/sphx_glr_plot_gpr_prior_posterior_003.png"]}],"doc\/modules\/calibration.rst":[{"add":["173",".. figure:: ..\/auto_examples\/calibration\/images\/sphx_glr_plot_calibration_multiclass_001.png","185",".. figure:: ..\/auto_examples\/calibration\/images\/sphx_glr_plot_calibration_multiclass_002.png"],"delete":["173",".. figure:: ..\/auto_examples\/calibration\/images\/sphx_glr_plot_calibration_multiclass_000.png","185",".. figure:: ..\/auto_examples\/calibration\/images\/sphx_glr_plot_calibration_multiclass_001.png"]}],"examples\/calibration\/plot_calibration_multiclass.py":[{"add":["66","plt.figure()","133","plt.figure()"],"delete":["66","plt.figure(0)","133","plt.figure(1)"]}],"examples\/gaussian_process\/plot_gpr_noisy.py":[{"add":["37","plt.figure()","56","plt.figure()","75","plt.figure()"],"delete":["37","plt.figure(0)","56","plt.figure(1)","75","plt.figure(2)"]}]}},"77b73d63d05bc198ba89193582aee93cae1f69a4":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/linear_model\/tests\/test_ridge.py":"MODIFY","sklearn\/linear_model\/ridge.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["186","","188","  parameter, allowing iterative addition of trees to an isolation","371","- |Fix| Fixed bug in :func:`linear_model.ridge.ridge_regression`,","372","  :class:`linear_model.ridge.Ridge` and","373","  :class:`linear_model.ridge.ridge.RidgeClassifier` that","374","  caused unhandled exception for arguments ``return_intercept=True`` and","375","  ``solver=auto`` (default) or any other solver different from ``sag``.","376","  :issue:`13363` by :user:`Bartosz Telenczuk <btel>`","377","","378","- |Fix| :func:`linear_model.ridge.ridge_regression` will now raise an exception","379","  if ``return_intercept=True`` and solver is different from ``sag``. Previously,","380","  only warning was issued. :issue:`13363` by :user:`Bartosz Telenczuk <btel>`","381","","382","- |API| :func:`linear_model.ridge.ridge_regression` will choose ``sparse_cg``","383","  solver for sparse inputs when ``solver=auto`` and ``sample_weight``","384","  is provided (previously `cholesky` solver was selected). :issue:`13363`","385","  by :user:`Bartosz Telenczuk <btel>`","386",""],"delete":["186","  ","188","  parameter, allowing iterative addition of trees to an isolation "]}],"sklearn\/linear_model\/tests\/test_ridge.py":[{"add":["9","from sklearn.utils.testing import assert_allclose","781","    message = (\"Known solvers are 'sparse_cg', 'cholesky', 'svd'\"","782","               \" 'lsqr', 'sag' or 'saga'. Got %s.\" % wrong_solver)","836","        assert_raises_regex(ValueError, \"In Ridge,\", sparse.fit, X_csr, y)","837","","838","","839","@pytest.mark.parametrize('return_intercept', [False, True])","840","@pytest.mark.parametrize('sample_weight', [None, np.ones(1000)])","841","@pytest.mark.parametrize('arr_type', [np.array, sp.csr_matrix])","842","@pytest.mark.parametrize('solver', ['auto', 'sparse_cg', 'cholesky', 'lsqr',","843","                                    'sag', 'saga'])","844","def test_ridge_regression_check_arguments_validity(return_intercept,","845","                                                   sample_weight, arr_type,","846","                                                   solver):","847","    \"\"\"check if all combinations of arguments give valid estimations\"\"\"","848","","849","    # test excludes 'svd' solver because it raises exception for sparse inputs","850","","851","    rng = check_random_state(42)","852","    X = rng.rand(1000, 3)","853","    true_coefs = [1, 2, 0.1]","854","    y = np.dot(X, true_coefs)","855","    true_intercept = 0.","856","    if return_intercept:","857","        true_intercept = 10000.","858","    y += true_intercept","859","    X_testing = arr_type(X)","860","","861","    alpha, atol, tol = 1e-3, 1e-4, 1e-6","862","","863","    if solver not in ['sag', 'auto'] and return_intercept:","864","        assert_raises_regex(ValueError,","865","                            \"In Ridge, only 'sag' solver\",","866","                            ridge_regression, X_testing, y,","867","                            alpha=alpha,","868","                            solver=solver,","869","                            sample_weight=sample_weight,","870","                            return_intercept=return_intercept,","871","                            tol=tol)","872","        return","873","","874","    out = ridge_regression(X_testing, y, alpha=alpha,","875","                           solver=solver,","876","                           sample_weight=sample_weight,","877","                           return_intercept=return_intercept,","878","                           tol=tol,","879","                           )","880","","881","    if return_intercept:","882","        coef, intercept = out","883","        assert_allclose(coef, true_coefs, rtol=0, atol=atol)","884","        assert_allclose(intercept, true_intercept, rtol=0, atol=atol)","885","    else:","886","        assert_allclose(out, true_coefs, rtol=0, atol=atol)"],"delete":["780","    message = \"Solver %s not understood\" % wrong_solver","834","        assert_warns(UserWarning, sparse.fit, X_csr, y)","835","        assert_almost_equal(dense.intercept_, sparse.intercept_)","836","        assert_array_almost_equal(dense.coef_, sparse.coef_)"]}],"sklearn\/linear_model\/ridge.py":[{"add":["370","    has_sw = sample_weight is not None","371","","372","    if solver == 'auto':","373","        if return_intercept:","374","            # only sag supports fitting intercept directly","375","            solver = \"sag\"","376","        elif not sparse.issparse(X):","377","            solver = \"cholesky\"","378","        else:","379","            solver = \"sparse_cg\"","380","","381","    if solver not in ('sparse_cg', 'cholesky', 'svd', 'lsqr', 'sag', 'saga'):","382","        raise ValueError(\"Known solvers are 'sparse_cg', 'cholesky', 'svd'\"","383","                         \" 'lsqr', 'sag' or 'saga'. Got %s.\" % solver)","384","","385","    if return_intercept and solver != 'sag':","386","        raise ValueError(\"In Ridge, only 'sag' solver can directly fit the \"","387","                         \"intercept. Please change solver to 'sag' or set \"","388","                         \"return_intercept=False.\")","561","            if sparse.issparse(X) and self.solver == 'sparse_cg':"],"delete":["370","    if return_intercept and sparse.issparse(X) and solver != 'sag':","371","        if solver != 'auto':","372","            warnings.warn(\"In Ridge, only 'sag' solver can currently fit the \"","373","                          \"intercept when X is sparse. Solver has been \"","374","                          \"automatically changed into 'sag'.\")","375","        solver = 'sag'","406","    has_sw = sample_weight is not None","408","    if solver == 'auto':","409","        # cholesky if it's a dense array and cg in any other case","410","        if not sparse.issparse(X) or has_sw:","411","            solver = 'cholesky'","412","        else:","413","            solver = 'sparse_cg'","434","    if solver not in ('sparse_cg', 'cholesky', 'svd', 'lsqr', 'sag', 'saga'):","435","        raise ValueError('Solver %s not understood' % solver)","557","            if sparse.issparse(X):"]}]}},"5890cbc995cde1991f1fc52cae495f5c50194df4":{"changes":{"sklearn\/tests\/test_impute.py":"MODIFY","doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/impute.py":"MODIFY"},"diff":{"sklearn\/tests\/test_impute.py":[{"add":["937","    \"missing_values, dtype, arr_type\",","938","    [(np.nan, np.float64, np.array),","939","     (0,      np.int32,   np.array),","940","     (-1,     np.int32,   np.array),","941","     (np.nan, np.float64, sparse.csc_matrix),","942","     (-1,     np.int32,   sparse.csc_matrix),","943","     (np.nan, np.float64, sparse.csr_matrix),","944","     (-1,     np.int32,   sparse.csr_matrix),","945","     (np.nan, np.float64, sparse.coo_matrix),","946","     (-1,     np.int32,   sparse.coo_matrix),","947","     (np.nan, np.float64, sparse.lil_matrix),","948","     (-1,     np.int32,   sparse.lil_matrix),","949","     (np.nan, np.float64, sparse.bsr_matrix),","950","     (-1,     np.int32,   sparse.bsr_matrix)","951","     ])","1003","    [sparse.csc_matrix, sparse.csr_matrix, sparse.coo_matrix,","1004","     sparse.lil_matrix, sparse.bsr_matrix])","1005","def test_missing_indicator_raise_on_sparse_with_missing_0(arr_type):","1006","    # test for sparse input and missing_value == 0","1007","","1008","    missing_values = 0","1009","    X_fit = np.array([[missing_values, missing_values, 1],","1010","                      [4, missing_values, 2]])","1011","    X_trans = np.array([[missing_values, missing_values, 1],","1012","                        [4, 12, 10]])","1013","","1014","    # convert the input to the right array format","1015","    X_fit_sparse = arr_type(X_fit)","1016","    X_trans_sparse = arr_type(X_trans)","1017","","1018","    indicator = MissingIndicator(missing_values=missing_values)","1019","","1020","    with pytest.raises(ValueError, match=\"Sparse input with missing_values=0\"):","1021","        indicator.fit_transform(X_fit_sparse)","1022","","1023","    indicator.fit_transform(X_fit)","1024","    with pytest.raises(ValueError, match=\"Sparse input with missing_values=0\"):","1025","        indicator.transform(X_trans_sparse)","1026","","1027","","1028","@pytest.mark.parametrize(\"param_sparse\", [True, False, 'auto'])","1029","@pytest.mark.parametrize(\"missing_values, arr_type\",","1030","                         [(np.nan, np.array),","1031","                          (0,      np.array),","1032","                          (np.nan, sparse.csc_matrix),","1033","                          (np.nan, sparse.csr_matrix),","1034","                          (np.nan, sparse.coo_matrix),","1035","                          (np.nan, sparse.lil_matrix)","1036","                          ])"],"delete":["937","    \"missing_values, dtype\",","938","    [(np.nan, np.float64),","939","     (0, np.int32),","940","     (-1, np.int32)])","941","@pytest.mark.parametrize(","942","    \"arr_type\",","943","    [np.array, sparse.csc_matrix, sparse.csr_matrix, sparse.coo_matrix,","944","     sparse.lil_matrix, sparse.bsr_matrix])","994","@pytest.mark.parametrize(\"param_sparse\", [True, False, 'auto'])","995","@pytest.mark.parametrize(\"missing_values\", [np.nan, 0])","998","    [np.array, sparse.csc_matrix, sparse.csr_matrix, sparse.coo_matrix])"]}],"doc\/whats_new\/v0.21.rst":[{"add":["84","  `n_components <= min(n_samples, n_features)`.","140","- |Fix| In :class:`impute.MissingIndicator` avoid implicit densification by","141","  raising an exception if input is sparse add `missing_values` property","142","  is set to 0. :issue:`13240` by :user:`Bartosz Telenczuk <btel>`.","143","","177","   parameter value ``copy_X=True`` in ``fit``."],"delete":["84","  `n_components <= min(n_samples, n_features)`. ","173","   parameter value ``copy_X=True`` in ``fit``. "]}],"sklearn\/impute.py":[{"add":["1136","        if sparse.issparse(X):","1182","","1183","        if sparse.issparse(X) and self.missing_values == 0:","1184","            # missing_values = 0 not allowed with sparse data as it would","1185","            # force densification","1186","            raise ValueError(\"Sparse input with missing_values=0 is \"","1187","                             \"not supported. Provide a dense \"","1188","                             \"array instead.\")","1189",""],"delete":["1136","        if sparse.issparse(X) and self.missing_values != 0:","1159","            if sparse.issparse(X):","1160","                # case of sparse matrix with 0 as missing values. Implicit and","1161","                # explicit zeros are considered as missing values.","1162","                X = X.toarray()"]}]}},"314686a65d543bd3b36d2af4b34ed23711991a57":{"changes":{"sklearn\/linear_model\/coordinate_descent.py":"MODIFY"},"diff":{"sklearn\/linear_model\/coordinate_descent.py":[{"add":["1556","    0.199..."],"delete":["1556","    0.1994727942696716"]}]}},"251d6e643fdc0c40059274fffdd808ad2461f0c7":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/cluster\/birch.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["87","- |Fix| Fixed a bug where :class:`cluster.Birch` could occasionally raise an","88","  AttributeError. :issue:`13651` by `Joel Nothman`_.","89",""],"delete":[]}],"sklearn\/cluster\/birch.py":[{"add":["279","            self.centroid_ = self.linear_sum_ = 0"],"delete":["279","            self.linear_sum_ = 0"]}]}},"3373e9c12502b62916c53f2651620fafd3342c3c":{"changes":{"sklearn\/mixture\/bayesian_mixture.py":"MODIFY"},"diff":{"sklearn\/mixture\/bayesian_mixture.py":[{"add":["142","        Controls the extend to where means can be placed. Larger","262","        Larger values concentrate the means of each clusters around"],"delete":["142","        Controls the extend to where means can be placed. Smaller","262","        Smaller values concentrate the means of each clusters around"]}]}},"afc6cc58da7b8b45b845443ed54e75de5017087c":{"changes":{"sklearn\/model_selection\/_split.py":"MODIFY","doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/model_selection\/tests\/test_split.py":"MODIFY"},"diff":{"sklearn\/model_selection\/_split.py":[{"add":["578","        Whether to shuffle each class's samples before splitting into batches.","621","        rng = check_random_state(self.random_state)"],"delete":["578","        Whether to shuffle each stratification of the data before splitting","579","        into batches.","622","        rng = self.random_state"]}],"doc\/whats_new\/v0.21.rst":[{"add":["269","- |Fix| Fixed a bug where :class:`model_selection.StratifiedKFold`","270","  shuffles each class's samples with the same ``random_state``,","271","  making ``shuffle=True`` ineffective.","272","  :issue:`13124` by :user:`Hanmin Qin <qinhanmin2014>`.","273",""],"delete":[]}],"sklearn\/model_selection\/tests\/test_split.py":[{"add":["495","    # Ensure that we shuffle each class's samples with different","496","    # random_state in StratifiedKFold","497","    # See https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/13124","498","    X = np.arange(10)","499","    y = [0] * 5 + [1] * 5","500","    kf1 = StratifiedKFold(5, shuffle=True, random_state=0)","501","    kf2 = StratifiedKFold(5, shuffle=True, random_state=1)","502","    test_set1 = sorted([tuple(s[1]) for s in kf1.split(X, y)])","503","    test_set2 = sorted([tuple(s[1]) for s in kf2.split(X, y)])","504","    assert test_set1 != test_set2","505",""],"delete":[]}]}},"37b0e66c871e8fb032a9c7086b2a1d5419838154":{"changes":{"sklearn\/ensemble\/iforest.py":"MODIFY","doc\/modules\/outlier_detection.rst":"MODIFY"},"diff":{"sklearn\/ensemble\/iforest.py":[{"add":["144","    Notes","145","    -----","146","    The implementation is based on an ensemble of ExtraTreeRegressor. The","147","    maximum depth of each tree is set to ``ceil(log_2(n))`` where","148","    :math:`n` is the number of samples used to build the tree","149","    (see (Liu et al., 2008) for more details).","150",""],"delete":[]}],"doc\/modules\/outlier_detection.rst":[{"add":["241","The implementation of :class:`ensemble.IsolationForest` is based on an ensemble","242","of :class:`tree.ExtraTreeRegressor`. Following Isolation Forest original paper,","243","the maximum depth of each tree is set to :math:`\\lceil \\log_2(n) \\rceil` where","244",":math:`n` is the number of samples used to build the tree (see (Liu et al.,","245","2008) for more details).","246","","247","This algorithm is illustrated below."],"delete":["241","This strategy is illustrated below."]}]}},"6216b247681ecdab3fdac873e467e0efa07553d3":{"changes":{"sklearn\/cluster\/dbscan_.py":"MODIFY"},"diff":{"sklearn\/cluster\/dbscan_.py":[{"add":["12","import warnings","147","        with warnings.catch_warnings():","148","            warnings.simplefilter('ignore', sparse.SparseEfficiencyWarning)"],"delete":["16","from ..utils.testing import ignore_warnings","147","        with ignore_warnings():"]}]}},"e35f040200d8aa9270e6a6f0c35abea386964a08":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/preprocessing\/_encoders.py":"MODIFY","sklearn\/preprocessing\/tests\/test_encoders.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["23","- |Fix| Fixed a bug in :func:`metrics.euclidean_distances` where a part of the","24","  distance matrix was left un-instanciated for suffiently large float32","25","  datasets (regression introduced in 0.21). :pr:`13910` by :user:`J¨¦r¨¦mie du","26","  Boisberranger <jeremiedbb>`.","27","","28",":mod:`sklearn.preprocessing`","29","............................","30","","31","- |Fix| Fixed a bug in :class:`preprocessing.OneHotEncoder` where the new","32","  `drop` parameter was not reflected in `get_feature_names`. :pr:`13894`","33","  by :user:`James Myatt <jamesmyatt>`."],"delete":["23","- |Fix| Fixed a bug in :func:`euclidean_distances` where a part of the distance","24","  matrix was left un-instanciated for suffiently large float32 datasets","25","  (regression introduced in 0.21) :issue:`13910`","26","  by :user:`J¨¦r¨¦mie du Boisberranger <jeremiedbb>`."]}],"sklearn\/preprocessing\/_encoders.py":[{"add":["853","            if self.drop is not None:","854","                names.pop(self.drop_idx_[i])"],"delete":[]}],"sklearn\/preprocessing\/tests\/test_encoders.py":[{"add":["592","@pytest.mark.parametrize(\"drop, expected_names\",","593","                         [('first', ['x0_c', 'x2_b']),","594","                          (['c', 2, 'b'], ['x0_b', 'x2_a'])],","595","                         ids=['first', 'manual'])","596","def test_one_hot_encoder_feature_names_drop(drop, expected_names):","597","    X = [['c', 2, 'a'],","598","         ['b', 2, 'b']]","599","","600","    ohe = OneHotEncoder(drop=drop)","601","    ohe.fit(X)","602","    feature_names = ohe.get_feature_names()","603","    assert isinstance(feature_names, np.ndarray)","604","    assert_array_equal(expected_names, feature_names)","605","","606",""],"delete":[]}]}},"851a4b8ed1b2bcd4212680b20722326a18e640ab":{"changes":{"sklearn\/model_selection\/tests\/test_search.py":"MODIFY","sklearn\/model_selection\/_search.py":"MODIFY"},"diff":{"sklearn\/model_selection\/tests\/test_search.py":[{"add":["1727","","1728","","1729","def test_empty_cv_iterator_error():","1730","    # Use global X, y","1731","","1732","    # create cv","1733","    cv = KFold(n_splits=3).split(X)","1734","","1735","    # pop all of it, this should cause the expected ValueError","1736","    [u for u in cv]","1737","    # cv is empty now","1738","","1739","    train_size = 100","1740","    ridge = RandomizedSearchCV(Ridge(), {'alpha': [1e-3, 1e-2, 1e-1]},","1741","                               cv=cv, n_jobs=-1)","1742","","1743","    # assert that this raises an error","1744","    with pytest.raises(ValueError,","1745","                       match='No fits were performed. '","1746","                             'Was the CV iterator empty\\\\? '","1747","                             'Were there no candidates\\\\?'):","1748","        ridge.fit(X[:train_size], y[:train_size])","1749","","1750","","1751","def test_random_search_bad_cv():","1752","    # Use global X, y","1753","","1754","    class BrokenKFold(KFold):","1755","        def get_n_splits(self, *args, **kw):","1756","            return 1","1757","","1758","    # create bad cv","1759","    cv = BrokenKFold(n_splits=3)","1760","","1761","    train_size = 100","1762","    ridge = RandomizedSearchCV(Ridge(), {'alpha': [1e-3, 1e-2, 1e-1]},","1763","                               cv=cv, n_jobs=-1)","1764","","1765","    # assert that this raises an error","1766","    with pytest.raises(ValueError,","1767","                       match='cv.split and cv.get_n_splits returned '","1768","                             'inconsistent results. Expected \\\\d+ '","1769","                             'splits, got \\\\d+'):","1770","        ridge.fit(X[:train_size], y[:train_size])"],"delete":[]}],"sklearn\/model_selection\/_search.py":[{"add":["668","                if len(out) < 1:","669","                    raise ValueError('No fits were performed. '","670","                                     'Was the CV iterator empty? '","671","                                     'Were there no candidates?')","672","                elif len(out) != n_candidates * n_splits:","673","                    raise ValueError('cv.split and cv.get_n_splits returned '","674","                                     'inconsistent results. Expected {} '","675","                                     'splits, got {}'","676","                                     .format(n_splits,","677","                                             len(out) \/\/ n_candidates))","678",""],"delete":[]}]}},"db17f3e2221fb6cec256d2d3501e259c5d5db934":{"changes":{"sklearn\/decomposition\/dict_learning.py":"MODIFY","doc\/whats_new\/v0.20.rst":"MODIFY"},"diff":{"sklearn\/decomposition\/dict_learning.py":[{"add":["302","    if effective_n_jobs(n_jobs) == 1 or algorithm == 'threshold':"],"delete":["302","    if effective_n_jobs(n_jobs) or algorithm == 'threshold':"]}],"doc\/whats_new\/v0.20.rst":[{"add":["24",":mod:`sklearn.decomposition`","25","...........................","26","","27","- |Fix| Fixed a bug in :func:`decomposition.sparse_encode` where computation was single","28","  threaded when `n_jobs > 1` or `n_jobs = -1`.","29","  :issue:`13005` by :user:`Prabakaran Kumaresshan <nixphix>`.","30",""],"delete":[]}]}},"bcdeadd7b8d17e0144e8d9ae10d778796ae26f5d":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/ensemble\/iforest.py":"MODIFY","sklearn\/neighbors\/tests\/test_lof.py":"MODIFY","sklearn\/utils\/estimator_checks.py":"MODIFY","sklearn\/ensemble\/tests\/test_iforest.py":"MODIFY","sklearn\/utils\/tests\/test_estimator_checks.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["111","- |Fix| Fixed the output of the average path length computed in","112","  :class:`ensemble.IsolationForest` when the input is either 0, 1 or 2.","113","  :issue:`13251` by :user:`Albert Thomas <albertcthomas>`","114","  and :user:`joshuakennethjones <joshuakennethjones>`.","115",""],"delete":[]}],"sklearn\/ensemble\/iforest.py":[{"add":["441","            return 0.","442","        elif n_samples_leaf <= 2:","454","        mask_1 = n_samples_leaf <= 1","455","        mask_2 = n_samples_leaf == 2","456","        not_mask = ~np.logical_or(mask_1, mask_2)","458","        average_path_length[mask_1] = 0.","459","        average_path_length[mask_2] = 1."],"delete":["452","        mask = (n_samples_leaf <= 1)","453","        not_mask = np.logical_not(mask)","455","        average_path_length[mask] = 1."]}],"sklearn\/neighbors\/tests\/test_lof.py":[{"add":["23","from sklearn.utils.estimator_checks import check_outlier_corruption","255","","256","","257","def test_predicted_outlier_number():","258","    # the number of predicted outliers should be equal to the number of","259","    # expected outliers unless there are ties in the abnormality scores.","260","    X = iris.data","261","    n_samples = X.shape[0]","262","    expected_outliers = 30","263","    contamination = float(expected_outliers)\/n_samples","264","","265","    clf = neighbors.LocalOutlierFactor(contamination=contamination)","266","    y_pred = clf.fit_predict(X)","267","","268","    num_outliers = np.sum(y_pred != 1)","269","    if num_outliers != expected_outliers:","270","        y_dec = clf.negative_outlier_factor_","271","        check_outlier_corruption(num_outliers, expected_outliers, y_dec)"],"delete":[]}],"sklearn\/utils\/estimator_checks.py":[{"add":["1526","def check_outlier_corruption(num_outliers, expected_outliers, decision):","1527","    # Check for deviation from the precise given contamination level that may","1528","    # be due to ties in the anomaly scores.","1529","    if num_outliers < expected_outliers:","1530","        start = num_outliers","1531","        end = expected_outliers + 1","1532","    else:","1533","        start = expected_outliers","1534","        end = num_outliers + 1","1535","","1536","    # ensure that all values in the 'critical area' are tied,","1537","    # leading to the observed discrepancy between provided","1538","    # and actual contamination levels.","1539","    sorted_decision = np.sort(decision)","1540","    msg = ('The number of predicted outliers is not equal to the expected '","1541","           'number of outliers and this difference is not explained by the '","1542","           'number of ties in the decision_function values')","1543","    assert len(np.unique(sorted_decision[start:end])) == 1, msg","1544","","1545","","1547","    n_samples = 300","1548","    X, _ = make_blobs(n_samples=n_samples, random_state=0)","1569","    scores = estimator.score_samples(X)","1570","    for output in [decision, scores]:","1571","        assert output.dtype == np.dtype('float')","1572","        assert output.shape == (n_samples,)","1586","    y_dec = scores - estimator.offset_","1599","        expected_outliers = 30","1600","        contamination = expected_outliers \/ n_samples","1604","","1605","        num_outliers = np.sum(y_pred != 1)","1606","        # num_outliers should be equal to expected_outliers unless","1607","        # there are ties in the decision_function values. this can","1608","        # only be tested for estimators with a decision_function","1609","        # method, i.e. all estimators except LOF which is already","1610","        # excluded from this if branch.","1611","        if num_outliers != expected_outliers:","1612","            decision = estimator.decision_function(X)","1613","            check_outlier_corruption(num_outliers, expected_outliers, decision)","2384","    n_samples = 300","2385","    X, _ = make_blobs(n_samples=n_samples, random_state=0)","2407","        expected_outliers = 30","2408","        contamination = float(expected_outliers)\/n_samples","2411","","2412","        num_outliers = np.sum(y_pred != 1)","2413","        # num_outliers should be equal to expected_outliers unless","2414","        # there are ties in the decision_function values. this can","2415","        # only be tested for estimators with a decision_function","2416","        # method","2417","        if (num_outliers != expected_outliers and","2418","                hasattr(estimator, 'decision_function')):","2419","            decision = estimator.decision_function(X)","2420","            check_outlier_corruption(num_outliers, expected_outliers, decision)"],"delete":["20","from sklearn.utils.testing import assert_almost_equal","1528","    X, _ = make_blobs(n_samples=300, random_state=0)","1549","    assert decision.dtype == np.dtype('float')","1550","","1551","    score = estimator.score_samples(X)","1552","    assert score.dtype == np.dtype('float')","1558","    decision = estimator.decision_function(X)","1559","    assert decision.shape == (n_samples,)","1568","    y_scores = estimator.score_samples(X)","1569","    assert y_scores.shape == (n_samples,)","1570","    y_dec = y_scores - estimator.offset_","1583","        contamination = 0.1","1587","        assert_almost_equal(np.mean(y_pred != 1), contamination)","2358","    X, _ = make_blobs(n_samples=300, random_state=0)","2380","        contamination = 0.1","2383","        assert_almost_equal(np.mean(y_pred != 1), contamination)"]}],"sklearn\/ensemble\/tests\/test_iforest.py":[{"add":["21","from sklearn.utils.testing import assert_allclose","265","    # Updated to check average path length when input is <= 2 (issue #11839)","269","    assert _average_path_length(0) == pytest.approx(0)","270","    assert _average_path_length(1) == pytest.approx(0)","271","    assert _average_path_length(2) == pytest.approx(1)","272","    assert_allclose(_average_path_length(5), result_one)","273","    assert_allclose(_average_path_length(999), result_two)","274","    assert_allclose(_average_path_length(np.array([1, 2, 5, 999])),","275","                    [0., 1., result_one, result_two])","276","","277","    # _average_path_length is increasing","278","    avg_path_length = _average_path_length(np.arange(5))","279","    assert_array_equal(avg_path_length, np.sort(avg_path_length))","280",""],"delete":["267","    assert_almost_equal(_average_path_length(1), 1., decimal=10)","268","    assert_almost_equal(_average_path_length(5), result_one, decimal=10)","269","    assert_almost_equal(_average_path_length(999), result_two, decimal=10)","270","    assert_array_almost_equal(_average_path_length(np.array([1, 5, 999])),","271","                              [1., result_one, result_two], decimal=10)"]}],"sklearn\/utils\/tests\/test_estimator_checks.py":[{"add":["11","from sklearn.utils.testing import (assert_raises_regex,","12","                                   assert_equal, ignore_warnings,","13","                                   assert_warns, assert_raises)","20","from sklearn.utils.estimator_checks import check_outlier_corruption","364","def test_check_outlier_corruption():","365","    # should raise AssertionError","366","    decision = np.array([0., 1., 1.5, 2.])","367","    assert_raises(AssertionError, check_outlier_corruption, 1, 2, decision)","368","    # should pass","369","    decision = np.array([0., 1., 1., 2.])","370","    check_outlier_corruption(1, 2, decision)","371","","372",""],"delete":["11","from sklearn.utils.testing import (assert_raises_regex, assert_equal,","12","                                   ignore_warnings, assert_warns)"]}]}},"e747376eef58ab671243fbc463e6ef8bf342636c":{"changes":{"build_tools\/azure\/install.sh":"MODIFY"},"diff":{"build_tools\/azure\/install.sh":[{"add":["56","    sudo add-apt-repository --remove ppa:ubuntu-toolchain-r\/test"],"delete":[]}]}},"18920e312122a90d3f9e0f19900103a613550f79":{"changes":{"build_tools\/circle\/build_doc.sh":"MODIFY"},"diff":{"build_tools\/circle\/build_doc.sh":[{"add":["122","  pandas=\"${PANDAS_VERSION:-*}\" \\","126","if [[ -n \"$SCIKIT_IMAGE_VERSION\" ]]; then","127","    pip install scikit-image==\"$SCIKIT_IMAGE_VERSION\"","128","else","129","    pip install scikit-image","130","fi"],"delete":["122","  scikit-image=\"${SCIKIT_IMAGE_VERSION:-*}\" pandas=\"${PANDAS_VERSION:-*}\" \\"]}]}},"1f5bcaeb39698c9a150ef92cdeaeec75c355a274":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/utils\/validation.py":"MODIFY","sklearn\/utils\/extmath.py":"MODIFY","sklearn\/preprocessing\/tests\/test_data.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["224","- |Fix| Fixed the calculation overflow when using a float16 dtype with","225","  :class:`preprocessing.StandardScaler`. :issue:`13007` by","226","  :user:`Raffaello Baluyot <baluyotraf>`","227",""],"delete":[]}],"sklearn\/utils\/validation.py":[{"add":["36","    # validation is also imported in extmath","37","    from .extmath import _safe_accumulator_op","38","","44","    # false positives from overflow in sum method. The sum is also calculated","45","    # safely to reduce dtype induced overflows.","47","    if is_float and (np.isfinite(_safe_accumulator_op(np.sum, X))):"],"delete":["41","    # false positives from overflow in sum method.","43","    if is_float and np.isfinite(X.sum()):"]}],"sklearn\/utils\/extmath.py":[{"add":["660","# Use at least float64 for the accumulating functions to avoid precision issue","661","# see https:\/\/github.com\/numpy\/numpy\/issues\/9393. The float64 is also retained","662","# as it is in case the float overflows","663","def _safe_accumulator_op(op, x, *args, **kwargs):","664","    \"\"\"","665","    This function provides numpy accumulator functions with a float64 dtype","666","    when used on a floating point input. This prevents accumulator overflow on","667","    smaller floating point dtypes.","668","","669","    Parameters","670","    ----------","671","    op : function","672","        A numpy accumulator function such as np.mean or np.sum","673","    x : numpy array","674","        A numpy array to apply the accumulator function","675","    *args : positional arguments","676","        Positional arguments passed to the accumulator function after the","677","        input x","678","    **kwargs : keyword arguments","679","        Keyword arguments passed to the accumulator function","680","","681","    Returns","682","    -------","683","    result : The output of the accumulator function passed to this function","684","    \"\"\"","685","    if np.issubdtype(x.dtype, np.floating) and x.dtype.itemsize < 8:","686","        result = op(x, *args, **kwargs, dtype=np.float64)","687","    else:","688","        result = op(x, *args, **kwargs)","689","    return result","690","","691","","742","    new_sum = _safe_accumulator_op(np.nansum, X, axis=0)","752","        new_unnormalized_variance = (","753","            _safe_accumulator_op(np.nanvar, X, axis=0) * new_sample_count)"],"delete":["710","    if np.issubdtype(X.dtype, np.floating) and X.dtype.itemsize < 8:","711","        # Use at least float64 for the accumulator to avoid precision issues;","712","        # see https:\/\/github.com\/numpy\/numpy\/issues\/9393","713","        new_sum = np.nansum(X, axis=0, dtype=np.float64).astype(X.dtype)","714","    else:","715","        new_sum = np.nansum(X, axis=0)","725","        new_unnormalized_variance = np.nanvar(X, axis=0) * new_sample_count"]}],"sklearn\/preprocessing\/tests\/test_data.py":[{"add":["452","def test_scaler_float16_overflow():","453","    # Test if the scaler will not overflow on float16 numpy arrays","454","    rng = np.random.RandomState(0)","455","    # float16 has a maximum of 65500.0. On the worst case 5 * 200000 is 100000","456","    # which is enough to overflow the data type","457","    X = rng.uniform(5, 10, [200000, 1]).astype(np.float16)","458","","459","    with np.errstate(over='raise'):","460","        scaler = StandardScaler().fit(X)","461","        X_scaled = scaler.transform(X)","462","","463","    # Calculate the float64 equivalent to verify result","464","    X_scaled_f64 = StandardScaler().fit_transform(X.astype(np.float64))","465","","466","    # Overflow calculations may cause -inf, inf, or nan. Since there is no nan","467","    # input, all of the outputs should be finite. This may be redundant since a","468","    # FloatingPointError exception will be thrown on overflow above.","469","    assert np.all(np.isfinite(X_scaled))","470","","471","    # The normal distribution is very unlikely to go above 4. At 4.0-8.0 the","472","    # float16 precision is 2^-8 which is around 0.004. Thus only 2 decimals are","473","    # checked to account for precision differences.","474","    assert_array_almost_equal(X_scaled, X_scaled_f64, decimal=2)","475","","476",""],"delete":[]}]}},"0a1ee74a14ed8fe94bb0c7c10c9e3d99db9cd2b8":{"changes":{"examples\/model_selection\/plot_confusion_matrix.py":"MODIFY"},"diff":{"examples\/model_selection\/plot_confusion_matrix.py":[{"add":["34","from sklearn.utils.multiclass import unique_labels","51","def plot_confusion_matrix(y_true, y_pred, classes,","53","                          title=None,","59","    if not title:","60","        if normalize:","61","            title = 'Normalized confusion matrix'","62","        else:","63","            title = 'Confusion matrix, without normalization'","64","","65","    # Compute confusion matrix","66","    cm = confusion_matrix(y_true, y_pred)","67","    # Only use the labels that appear in the data","68","    classes = classes[unique_labels(y_true, y_pred)]","77","    fig, ax = plt.subplots()","78","    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)","79","    ax.figure.colorbar(im, ax=ax)","80","    # We want to show all ticks...","81","    ax.set(xticks=np.arange(cm.shape[1]),","82","           yticks=np.arange(cm.shape[0]),","83","           # ... and label them with the respective list entries","84","           xticklabels=classes, yticklabels=classes,","85","           title=title,","86","           ylabel='True label',","87","           xlabel='Predicted label')","89","    # Rotate the tick labels and set their alignment.","90","    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",","91","             rotation_mode=\"anchor\")","92","","93","    # Loop over data dimensions and create text annotations.","96","    for i in range(cm.shape[0]):","97","        for j in range(cm.shape[1]):","98","            ax.text(j, i, format(cm[i, j], fmt),","99","                    ha=\"center\", va=\"center\",","100","                    color=\"white\" if cm[i, j] > thresh else \"black\")","101","    fig.tight_layout()","102","    return ax","108","plot_confusion_matrix(y_test, y_pred, classes=class_names,","112","plot_confusion_matrix(y_test, y_pred, classes=class_names, normalize=True,"],"delete":["28","import itertools","51","def plot_confusion_matrix(cm, classes,","53","                          title='Confusion matrix',","67","    plt.imshow(cm, interpolation='nearest', cmap=cmap)","68","    plt.title(title)","69","    plt.colorbar()","70","    tick_marks = np.arange(len(classes))","71","    plt.xticks(tick_marks, classes, rotation=45)","72","    plt.yticks(tick_marks, classes)","76","    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):","77","        plt.text(j, i, format(cm[i, j], fmt),","78","                 horizontalalignment=\"center\",","79","                 color=\"white\" if cm[i, j] > thresh else \"black\")","80","","81","    plt.ylabel('True label')","82","    plt.xlabel('Predicted label')","83","    plt.tight_layout()","86","# Compute confusion matrix","87","cnf_matrix = confusion_matrix(y_test, y_pred)","91","plt.figure()","92","plot_confusion_matrix(cnf_matrix, classes=class_names,","96","plt.figure()","97","plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,"]}]}},"50946b3ef23641a3a38f9a758d34d5b556742e07":{"changes":{"sklearn\/model_selection\/_validation.py":"MODIFY","doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/model_selection\/tests\/test_validation.py":"MODIFY"},"diff":{"sklearn\/model_selection\/_validation.py":[{"add":["878","                              'predict_proba': 0.0}","881","                                                  default_values[method],","882","                                                  predictions.dtype)"],"delete":["878","                              'predict_proba': 0}","881","                                                  default_values[method])"]}],"doc\/whats_new\/v0.21.rst":[{"add":["321","- |Fix| Fixed an issue in :func:`~model_selection.cross_val_predict` where","322","  `method=\"predict_proba\"` returned always `0.0` when one of the classes was","323","  excluded in a cross-validation fold.","324","  :issue:`13366` by :user:`Guillaume Fournier <gfournier>`","325",""],"delete":[]}],"sklearn\/model_selection\/tests\/test_validation.py":[{"add":["977","@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22","978","@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22","979","def test_cross_val_predict_unbalanced():","980","    X, y = make_classification(n_samples=100, n_features=2, n_redundant=0,","981","                               n_informative=2, n_clusters_per_class=1,","982","                               random_state=1)","983","    # Change the first sample to a new class","984","    y[0] = 2","985","    clf = LogisticRegression(random_state=1)","986","    cv = StratifiedKFold(n_splits=2, random_state=1)","987","    train, test = list(cv.split(X, y))","988","    yhat_proba = cross_val_predict(clf, X, y, cv=cv, method=\"predict_proba\")","989","    assert y[test[0]][0] == 2  # sanity check for further assertions","990","    assert np.all(yhat_proba[test[0]][:, 2] == 0)","991","    assert np.all(yhat_proba[test[0]][:, 0:1] > 0)","992","    assert np.all(yhat_proba[test[1]] > 0)","993","    assert_array_almost_equal(yhat_proba.sum(axis=1), np.ones(y.shape),","994","                              decimal=12)","995","","996",""],"delete":[]}]}},"5d240c6a0bca0d78795874d438e7fd4bc163c209":{"changes":{"build_tools\/azure\/upload_codecov.sh":"MODIFY"},"diff":{"build_tools\/azure\/upload_codecov.sh":[{"add":["11","coverage combine --append"],"delete":["11","coverage combine"]}]}},"f02ef9f52f81c2d212f428092ad7c3f2f3fbd0f5":{"changes":{"sklearn\/linear_model\/setup.py":"MODIFY","sklearn\/utils\/seq_dataset.pxd.tp":"ADD",".gitignore":"MODIFY","sklearn\/linear_model\/tests\/test_logistic.py":"MODIFY","sklearn\/linear_model\/logistic.py":"MODIFY","sklearn\/linear_model\/sag_fast.pyx.tp":"ADD","sklearn\/linear_model\/tests\/test_base.py":"MODIFY","sklearn\/linear_model\/sag.py":"MODIFY","sklearn\/utils\/seq_dataset.pyx.tp":"ADD","sklearn\/utils\/setup.py":"MODIFY","sklearn\/linear_model\/base.py":"MODIFY","sklearn\/utils\/tests\/test_seq_dataset.py":"MODIFY","\/dev\/null":"DELETE","doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/linear_model\/sgd_fast_helpers.h":"MODIFY","benchmarks\/bench_saga.py":"MODIFY","sklearn\/linear_model\/sgd_fast.pyx":"MODIFY"},"diff":{"sklearn\/linear_model\/setup.py":[{"add":["4","from Cython import Tempita","25","    # generate sag_fast from template","26","    sag_cython_file = 'sklearn\/linear_model\/sag_fast.pyx.tp'","27","    sag_file = sag_cython_file.replace('.tp', '')","28","","29","    if not (os.path.exists(sag_file) and","30","            os.stat(sag_cython_file).st_mtime < os.stat(sag_file).st_mtime):","31","","32","        with open(sag_cython_file, \"r\") as f:","33","            tmpl = f.read()","34","        tmpl_ = Tempita.sub(tmpl)","35","","36","        with open(sag_file, \"w\") as f:","37","            f.write(tmpl_)","38",""],"delete":[]}],"sklearn\/utils\/seq_dataset.pxd.tp":[{"add":[],"delete":[]}],".gitignore":[{"add":["73","","74","# files generated from a template","75","sklearn\/utils\/seq_dataset.pyx","76","sklearn\/utils\/seq_dataset.pxd","77","sklearn\/linear_model\/sag_fast.pyx"],"delete":[]}],"sklearn\/linear_model\/tests\/test_logistic.py":[{"add":["1361","@pytest.mark.parametrize('solver', ['newton-cg', 'saga'])","1362","def test_dtype_match(solver, multi_class):"],"delete":["1361","def test_dtype_match(multi_class):","1370","    solver = 'newton-cg'","1371",""]}],"sklearn\/linear_model\/logistic.py":[{"add":["966","                target = target.astype(X.dtype, copy=False)","1488","","1489","        Notes","1490","        -----","1491","        The SAGA solver supports both float64 and float32 bit arrays.","1526","        if solver in ['lbfgs', 'liblinear']:","1528","        else:","1529","            _dtype = [np.float64, np.float32]"],"delete":["966","                target = target.astype(np.float64)","1522","        if solver in ['newton-cg']:","1523","            _dtype = [np.float64, np.float32]","1524","        else:"]}],"sklearn\/linear_model\/sag_fast.pyx.tp":[{"add":[],"delete":[]}],"sklearn\/linear_model\/tests\/test_base.py":[{"add":["12","from sklearn.utils.testing import assert_array_equal","15","from sklearn.utils.testing import assert_allclose","20","from sklearn.linear_model.base import make_dataset","25","from sklearn.datasets import load_iris","28","rtol = 1e-6","429","","430","","431","def test_fused_types_make_dataset():","432","    iris = load_iris()","433","","434","    X_32 = iris.data.astype(np.float32)","435","    y_32 = iris.target.astype(np.float32)","436","    X_csr_32 = sparse.csr_matrix(X_32)","437","    sample_weight_32 = np.arange(y_32.size, dtype=np.float32)","438","","439","    X_64 = iris.data.astype(np.float64)","440","    y_64 = iris.target.astype(np.float64)","441","    X_csr_64 = sparse.csr_matrix(X_64)","442","    sample_weight_64 = np.arange(y_64.size, dtype=np.float64)","443","","444","    # array","445","    dataset_32, _ = make_dataset(X_32, y_32, sample_weight_32)","446","    dataset_64, _ = make_dataset(X_64, y_64, sample_weight_64)","447","    xi_32, yi_32, _, _ = dataset_32._next_py()","448","    xi_64, yi_64, _, _ = dataset_64._next_py()","449","    xi_data_32, _, _ = xi_32","450","    xi_data_64, _, _ = xi_64","451","","452","    assert xi_data_32.dtype == np.float32","453","    assert xi_data_64.dtype == np.float64","454","    assert_allclose(yi_64, yi_32, rtol=rtol)","455","","456","    # csr","457","    datasetcsr_32, _ = make_dataset(X_csr_32, y_32, sample_weight_32)","458","    datasetcsr_64, _ = make_dataset(X_csr_64, y_64, sample_weight_64)","459","    xicsr_32, yicsr_32, _, _ = datasetcsr_32._next_py()","460","    xicsr_64, yicsr_64, _, _ = datasetcsr_64._next_py()","461","    xicsr_data_32, _, _ = xicsr_32","462","    xicsr_data_64, _, _ = xicsr_64","463","","464","    assert xicsr_data_32.dtype == np.float32","465","    assert xicsr_data_64.dtype == np.float64","466","","467","    assert_allclose(xicsr_data_64, xicsr_data_32, rtol=rtol)","468","    assert_allclose(yicsr_64, yicsr_32, rtol=rtol)","469","","470","    assert_array_equal(xi_data_32, xicsr_data_32)","471","    assert_array_equal(xi_data_64, xicsr_data_64)","472","    assert_array_equal(yi_32, yicsr_32)","473","    assert_array_equal(yi_64, yicsr_64)"],"delete":["11",""]}],"sklearn\/linear_model\/sag.py":[{"add":["11","from .sag_fast import sag32, sag64","247","        _dtype = [np.float64, np.float32]","248","        X = check_array(X, dtype=_dtype, accept_sparse='csr', order='C')","249","        y = check_array(y, dtype=_dtype, ensure_2d=False, order='C')","261","        sample_weight = np.ones(n_samples, dtype=X.dtype, order='C')","267","        coef_init = np.zeros((n_features, n_classes), dtype=X.dtype,","277","        intercept_init = np.zeros(n_classes, dtype=X.dtype)","282","        intercept_sum_gradient = np.zeros(n_classes, dtype=X.dtype)","288","                                        dtype=X.dtype, order='C')","293","                                     dtype=X.dtype, order='C')","316","    sag = sag64 if X.dtype == np.float64 else sag32","333",""],"delete":["11","from .sag_fast import sag","247","        X = check_array(X, dtype=np.float64, accept_sparse='csr', order='C')","248","        y = check_array(y, dtype=np.float64, ensure_2d=False, order='C')","260","        sample_weight = np.ones(n_samples, dtype=np.float64, order='C')","266","        coef_init = np.zeros((n_features, n_classes), dtype=np.float64,","276","        intercept_init = np.zeros(n_classes, dtype=np.float64)","281","        intercept_sum_gradient = np.zeros(n_classes, dtype=np.float64)","287","                                        dtype=np.float64, order='C')","292","                                     dtype=np.float64, order='C')"]}],"sklearn\/utils\/seq_dataset.pyx.tp":[{"add":[],"delete":[]}],"sklearn\/utils\/setup.py":[{"add":["7","    from Cython import Tempita","48","    # generate files from a template","49","    pyx_templates = ['sklearn\/utils\/seq_dataset.pyx.tp',","50","                     'sklearn\/utils\/seq_dataset.pxd.tp']","51","","52","    for pyxfiles in pyx_templates:","53","        outfile = pyxfiles.replace('.tp', '')","54","        # if .pyx.tp is not updated, no need to output .pyx","55","        if (os.path.exists(outfile) and","56","                os.stat(pyxfiles).st_mtime < os.stat(outfile).st_mtime):","57","            continue","58","","59","        with open(pyxfiles, \"r\") as f:","60","            tmpl = f.read()","61","        pyxcontent = Tempita.sub(tmpl)","62","","63","        with open(outfile, \"w\") as f:","64","            f.write(pyxcontent)","65",""],"delete":[]}],"sklearn\/linear_model\/base.py":[{"add":["34","from ..utils.seq_dataset import ArrayDataset32, CSRDataset32","35","from ..utils.seq_dataset import ArrayDataset64, CSRDataset64","79","    # seed should never be 0 in SequentialDataset64","82","    if X.dtype == np.float32:","83","        CSRData = CSRDataset32","84","        ArrayData = ArrayDataset32","85","    else:","86","        CSRData = CSRDataset64","87","        ArrayData = ArrayDataset64","88","","90","        dataset = CSRData(X.data, X.indptr, X.indices, y, sample_weight,","91","                          seed=seed)","94","        dataset = ArrayData(X, y, sample_weight, seed=seed)"],"delete":["34","from ..utils.seq_dataset import ArrayDataset, CSRDataset","78","    # seed should never be 0 in SequentialDataset","82","        dataset = CSRDataset(X.data, X.indptr, X.indices, y, sample_weight,","83","                             seed=seed)","86","        dataset = ArrayDataset(X, y, sample_weight, seed=seed)"]}],"sklearn\/utils\/tests\/test_seq_dataset.py":[{"add":["1","#         Joan Massich <mailsik@gmail.com>","5","import pytest","8","from sklearn.utils.testing import assert_allclose","11","from sklearn.utils.seq_dataset import ArrayDataset64","12","from sklearn.utils.seq_dataset import ArrayDataset32","13","from sklearn.utils.seq_dataset import CSRDataset64","14","from sklearn.utils.seq_dataset import CSRDataset32","15","","19","X64 = iris.data.astype(np.float64)","20","y64 = iris.target.astype(np.float64)","21","X_csr64 = sp.csr_matrix(X64)","22","sample_weight64 = np.arange(y64.size, dtype=np.float64)","23","","24","X32 = iris.data.astype(np.float32)","25","y32 = iris.target.astype(np.float32)","26","X_csr32 = sp.csr_matrix(X32)","27","sample_weight32 = np.arange(y32.size, dtype=np.float32)","30","def assert_csr_equal_values(current, expected):","31","    current.eliminate_zeros()","32","    expected.eliminate_zeros()","33","    expected = expected.astype(current.dtype)","34","    assert current.shape[0] == expected.shape[0]","35","    assert current.shape[1] == expected.shape[1]","36","    assert_array_equal(current.data, expected.data)","37","    assert_array_equal(current.indices, expected.indices)","38","    assert_array_equal(current.indptr, expected.indptr)","41","def make_dense_dataset_32():","42","    return ArrayDataset32(X32, y32, sample_weight32, seed=42)","45","def make_dense_dataset_64():","46","    return ArrayDataset64(X64, y64, sample_weight64, seed=42)","48","","49","def make_sparse_dataset_32():","50","    return CSRDataset32(X_csr32.data, X_csr32.indptr, X_csr32.indices, y32,","51","                        sample_weight32, seed=42)","52","","53","","54","def make_sparse_dataset_64():","55","    return CSRDataset64(X_csr64.data, X_csr64.indptr, X_csr64.indices, y64,","56","                        sample_weight64, seed=42)","57","","58","","59","@pytest.mark.parametrize('dataset_constructor', [","60","    make_dense_dataset_32,","61","    make_dense_dataset_64,","62","    make_sparse_dataset_32,","63","    make_sparse_dataset_64,","64","])","65","def test_seq_dataset_basic_iteration(dataset_constructor):","66","    NUMBER_OF_RUNS = 5","67","    dataset = dataset_constructor()","68","    for _ in range(NUMBER_OF_RUNS):","69","        # next sample","70","        xi_, yi, swi, idx = dataset._next_py()","71","        xi = sp.csr_matrix((xi_), shape=(1, X64.shape[1]))","72","","73","        assert_csr_equal_values(xi, X_csr64[idx])","74","        assert yi == y64[idx]","75","        assert swi == sample_weight64[idx]","76","","77","        # random sample","78","        xi_, yi, swi, idx = dataset._random_py()","79","        xi = sp.csr_matrix((xi_), shape=(1, X64.shape[1]))","80","","81","        assert_csr_equal_values(xi, X_csr64[idx])","82","        assert yi == y64[idx]","83","        assert swi == sample_weight64[idx]","84","","85","","86","@pytest.mark.parametrize('make_dense_dataset,make_sparse_dataset', [","87","    (make_dense_dataset_32, make_sparse_dataset_32),","88","    (make_dense_dataset_64, make_sparse_dataset_64),","89","])","90","def test_seq_dataset_shuffle(make_dense_dataset, make_sparse_dataset):","91","    dense_dataset, sparse_dataset = make_dense_dataset(), make_sparse_dataset()","94","        _, _, _, idx1 = dense_dataset._next_py()","95","        _, _, _, idx2 = sparse_dataset._next_py()","96","        assert idx1 == i","97","        assert idx2 == i","100","        _, _, _, idx1 = dense_dataset._random_py()","101","        _, _, _, idx2 = sparse_dataset._random_py()","102","        assert idx1 == idx2","105","    dense_dataset._shuffle_py(seed)","106","    sparse_dataset._shuffle_py(seed)","109","        _, _, _, idx1 = dense_dataset._next_py()","110","        _, _, _, idx2 = sparse_dataset._next_py()","111","        assert idx1 == idx2","113","        _, _, _, idx1 = dense_dataset._random_py()","114","        _, _, _, idx2 = sparse_dataset._random_py()","115","        assert idx1 == idx2","116","","117","","118","@pytest.mark.parametrize('make_dataset_32,make_dataset_64', [","119","    (make_dense_dataset_32, make_dense_dataset_64),","120","    (make_sparse_dataset_32, make_sparse_dataset_64),","121","])","122","def test_fused_types_consistency(make_dataset_32, make_dataset_64):","123","    dataset_32, dataset_64 = make_dataset_32(), make_dataset_64()","124","    NUMBER_OF_RUNS = 5","125","    for _ in range(NUMBER_OF_RUNS):","126","        # next sample","127","        (xi_data32, _, _), yi32, _, _ = dataset_32._next_py()","128","        (xi_data64, _, _), yi64, _, _ = dataset_64._next_py()","129","","130","        assert xi_data32.dtype == np.float32","131","        assert xi_data64.dtype == np.float64","132","","133","        assert_allclose(xi_data64, xi_data32, rtol=1e-5)","134","        assert_allclose(yi64, yi32, rtol=1e-5)","135","","136","","137","def test_buffer_dtype_mismatch_error():","138","    with pytest.raises(ValueError, match='Buffer dtype mismatch'):","139","        ArrayDataset64(X32, y32, sample_weight32, seed=42),","140","","141","    with pytest.raises(ValueError, match='Buffer dtype mismatch'):","142","        ArrayDataset32(X64, y64, sample_weight64, seed=42),","143","","144","    with pytest.raises(ValueError, match='Buffer dtype mismatch'):","145","        CSRDataset64(X_csr32.data, X_csr32.indptr, X_csr32.indices, y32,","146","                     sample_weight32, seed=42),","147","","148","    with pytest.raises(ValueError, match='Buffer dtype mismatch'):","149","        CSRDataset32(X_csr64.data, X_csr64.indptr, X_csr64.indices, y64,","150","                     sample_weight64, seed=42),"],"delete":["8","from sklearn.utils.seq_dataset import ArrayDataset, CSRDataset","11","from sklearn.utils.testing import assert_equal","12","","14","X = iris.data.astype(np.float64)","15","y = iris.target.astype(np.float64)","16","X_csr = sp.csr_matrix(X)","17","sample_weight = np.arange(y.size, dtype=np.float64)","20","def assert_csr_equal(X, Y):","21","    X.eliminate_zeros()","22","    Y.eliminate_zeros()","23","    assert_equal(X.shape[0], Y.shape[0])","24","    assert_equal(X.shape[1], Y.shape[1])","25","    assert_array_equal(X.data, Y.data)","26","    assert_array_equal(X.indices, Y.indices)","27","    assert_array_equal(X.indptr, Y.indptr)","30","def test_seq_dataset():","31","    dataset1 = ArrayDataset(X, y, sample_weight, seed=42)","32","    dataset2 = CSRDataset(X_csr.data, X_csr.indptr, X_csr.indices,","33","                          y, sample_weight, seed=42)","34","","35","    for dataset in (dataset1, dataset2):","36","        for i in range(5):","37","            # next sample","38","            xi_, yi, swi, idx = dataset._next_py()","39","            xi = sp.csr_matrix((xi_), shape=(1, X.shape[1]))","40","","41","            assert_csr_equal(xi, X_csr[idx])","42","            assert_equal(yi, y[idx])","43","            assert_equal(swi, sample_weight[idx])","44","","45","            # random sample","46","            xi_, yi, swi, idx = dataset._random_py()","47","            xi = sp.csr_matrix((xi_), shape=(1, X.shape[1]))","48","","49","            assert_csr_equal(xi, X_csr[idx])","50","            assert_equal(yi, y[idx])","51","            assert_equal(swi, sample_weight[idx])","54","def test_seq_dataset_shuffle():","55","    dataset1 = ArrayDataset(X, y, sample_weight, seed=42)","56","    dataset2 = CSRDataset(X_csr.data, X_csr.indptr, X_csr.indices,","57","                          y, sample_weight, seed=42)","61","        _, _, _, idx1 = dataset1._next_py()","62","        _, _, _, idx2 = dataset2._next_py()","63","        assert_equal(idx1, i)","64","        assert_equal(idx2, i)","67","        _, _, _, idx1 = dataset1._random_py()","68","        _, _, _, idx2 = dataset2._random_py()","69","        assert_equal(idx1, idx2)","72","    dataset1._shuffle_py(seed)","73","    dataset2._shuffle_py(seed)","76","        _, _, _, idx1 = dataset1._next_py()","77","        _, _, _, idx2 = dataset2._next_py()","78","        assert_equal(idx1, idx2)","80","        _, _, _, idx1 = dataset1._random_py()","81","        _, _, _, idx2 = dataset2._random_py()","82","        assert_equal(idx1, idx2)"]}],"\/dev\/null":[{"add":[],"delete":[]}],"doc\/whats_new\/v0.21.rst":[{"add":["164","- |Enhancement| :class:`linear_model.make_dataset` now preserves","165","  ``float32`` and ``float64`` dtypes. :issues:`8769` and :issues:`11000` by","166","  :user:`Nelle Varoquaux`_, :user:`Arthur Imbert <Henley13>`,","167","  :user:`Guillaume Lemaitre <glemaitre>`, and :user:`Joan Massich <massich>`","168",""],"delete":[]}],"sklearn\/linear_model\/sgd_fast_helpers.h":[{"add":["2","\/\/ When re-declaring the functions in the template for cython","3","\/\/ specific for each parameter input type, it needs to be 2 different functions","4","\/\/ as cython doesn't support function overloading.","8","# define skl_isfinite32 _finite","9","# define skl_isfinite64 _finite","13","# define skl_isfinite32 npy_isfinite","14","# define skl_isfinite64 npy_isfinite"],"delete":[]}],"benchmarks\/bench_saga.py":[{"add":["0","\"\"\"Author: Arthur Mensch, Nelle Varoquaux","7","import os","23","               max_iter=10, skip_slow=False, dtype=np.float64):","39","    X = X.astype(dtype)","40","    y = y.astype(dtype)","72","                                    fit_intercept=False, tol=0,","76","","77","        # Makes cpu cache even for all fit calls","78","        X_train.max()","80","","113","def exp(solvers, penalty, single_target,","114","        n_samples=30000, max_iter=20,","116","    dtypes_mapping = {","117","        \"float64\": np.float64,","118","        \"float32\": np.float32,","119","    }","163","        delayed(fit_single)(solver, X, y,","165","                            dtype=dtype,","168","        for dtype in dtypes_mapping.values())","172","    for dtype_name in dtypes_mapping.keys():","173","        for solver in solvers:","174","            if not (skip_slow and","175","                    solver == 'lightning' and","176","                    penalty == 'l1'):","179","                                dtype=dtype_name,","191","def plot(outname=None):","196","    res.set_index(['single_target'], inplace=True)","198","    grouped = res.groupby(level=['single_target'])","200","    colors = {'saga': 'C0', 'liblinear': 'C1', 'lightning': 'C2'}","201","    linestyles = {\"float32\": \"--\", \"float64\": \"-\"}","202","    alpha = {\"float64\": 0.5, \"float32\": 1}","205","        single_target = idx","206","        fig, axes = plt.subplots(figsize=(12, 4), ncols=4)","207","        ax = axes[0]","209","        for scores, times, solver, dtype in zip(group['train_scores'],","210","                                                group['times'],","211","                                                group['solver'],","212","                                                group[\"dtype\"]):","213","            ax.plot(times, scores, label=\"%s - %s\" % (solver, dtype),","214","                    color=colors[solver],","215","                    alpha=alpha[dtype],","216","                    marker=\".\",","217","                    linestyle=linestyles[dtype])","218","            ax.axvline(times[-1], color=colors[solver],","219","                       alpha=alpha[dtype],","220","                       linestyle=linestyles[dtype])","225","        ax = axes[1]","227","        for scores, times, solver, dtype in zip(group['test_scores'],","228","                                                group['times'],","229","                                                group['solver'],","230","                                                group[\"dtype\"]):","231","            ax.plot(times, scores, label=solver, color=colors[solver],","232","                    linestyle=linestyles[dtype],","233","                    marker=\".\",","234","                    alpha=alpha[dtype])","235","            ax.axvline(times[-1], color=colors[solver],","236","                       alpha=alpha[dtype],","237","                       linestyle=linestyles[dtype])","243","        ax = axes[2]","244","        for accuracy, times, solver, dtype in zip(group['accuracies'],","245","                                                  group['times'],","246","                                                  group['solver'],","247","                                                  group[\"dtype\"]):","248","            ax.plot(times, accuracy, label=\"%s - %s\" % (solver, dtype),","249","                    alpha=alpha[dtype],","250","                    marker=\".\",","251","                    color=colors[solver], linestyle=linestyles[dtype])","252","            ax.axvline(times[-1], color=colors[solver],","253","                       alpha=alpha[dtype],","254","                       linestyle=linestyles[dtype])","262","        if outname is None:","263","            outname = name + '.png'","266","","267","        ax = axes[3]","268","        for scores, times, solver, dtype in zip(group['train_scores'],","269","                                                group['times'],","270","                                                group['solver'],","271","                                                group[\"dtype\"]):","272","            ax.plot(np.arange(len(scores)),","273","                    scores, label=\"%s - %s\" % (solver, dtype),","274","                    marker=\".\",","275","                    alpha=alpha[dtype],","276","                    color=colors[solver], linestyle=linestyles[dtype])","277","","278","        ax.set_yscale(\"log\")","279","        ax.set_xlabel('# iterations')","280","        ax.set_ylabel('Objective function')","281","        ax.legend()","282","","283","        plt.savefig(outname)","289","    n_samples = [100000, 300000, 500000, 800000, None]","291","    for penalty in penalties:","292","        for n_sample in n_samples:","293","            exp(solvers, penalty, single_target,","294","                n_samples=n_sample, n_jobs=1,","295","                dataset='rcv1', max_iter=10)","296","            if n_sample is not None:","297","                outname = \"figures\/saga_%s_%d.png\" % (penalty, n_sample)","298","            else:","299","                outname = \"figures\/saga_%s_all.png\" % (penalty,)","300","            try:","301","                os.makedirs(\"figures\")","302","            except OSError:","303","                pass","304","            plot(outname)"],"delete":["0","\"\"\"Author: Arthur Mensch","7","from os.path import expanduser","23","               max_iter=10, skip_slow=False):","39","","71","                                    fit_intercept=False, tol=1e-24,","108","def exp(solvers, penalties, single_target, n_samples=30000, max_iter=20,","110","    mem = Memory(cachedir=expanduser('~\/cache'), verbose=0)","153","    cached_fit = mem.cache(fit_single)","155","        delayed(cached_fit)(solver, X, y,","159","        for penalty in penalties)","163","    for solver in solvers:","164","        for penalty in penalties:","165","            if not (skip_slow and solver == 'lightning' and penalty == 'l1'):","179","def plot():","184","    res.set_index(['single_target', 'penalty'], inplace=True)","186","    grouped = res.groupby(level=['single_target', 'penalty'])","188","    colors = {'saga': 'blue', 'liblinear': 'orange', 'lightning': 'green'}","191","        single_target, penalty = idx","192","        fig = plt.figure(figsize=(12, 4))","193","        ax = fig.add_subplot(131)","195","        train_scores = group['train_scores'].values","196","        ref = np.min(np.concatenate(train_scores)) * 0.999","197","","198","        for scores, times, solver in zip(group['train_scores'], group['times'],","199","                                         group['solver']):","200","            scores = scores \/ ref - 1","201","            ax.plot(times, scores, label=solver, color=colors[solver])","206","        ax = fig.add_subplot(132)","208","        test_scores = group['test_scores'].values","209","        ref = np.min(np.concatenate(test_scores)) * 0.999","211","        for scores, times, solver in zip(group['test_scores'], group['times'],","212","                                         group['solver']):","213","            scores = scores \/ ref - 1","214","            ax.plot(times, scores, label=solver, color=colors[solver])","219","        ax = fig.add_subplot(133)","221","        for accuracy, times, solver in zip(group['accuracies'], group['times'],","222","                                           group['solver']):","223","            ax.plot(times, accuracy, label=solver, color=colors[solver])","230","        name += '.png'","233","        plt.savefig(name)","234","        plt.close(fig)","241","    exp(solvers, penalties, single_target, n_samples=None, n_jobs=1,","242","        dataset='20newspaper', max_iter=20)","243","    plot()"]}],"sklearn\/linear_model\/sgd_fast.pyx":[{"add":["24","from sklearn.utils.seq_dataset cimport SequentialDataset64 as SequentialDataset","510","    dataset : SequentialDataset","511","        A concrete ``SequentialDataset`` object."],"delete":["24","from sklearn.utils.seq_dataset cimport SequentialDataset"]}]}},"9faa41429c708b5cbd5801cf9cdfe7349481e1dd":{"changes":{"sklearn\/ensemble\/voting_classifier.py":"MODIFY"},"diff":{"sklearn\/ensemble\/voting_classifier.py":[{"add":["196","            delayed(_parallel_fit_estimator)(clone(clf), X, transformed_y,","197","                                             sample_weight=sample_weight)","198","            for clf in clfs if clf is not None)","219","            The input samples.","263","            The input samples."],"delete":["196","                delayed(_parallel_fit_estimator)(clone(clf), X, transformed_y,","197","                                                 sample_weight=sample_weight)","198","                for clf in clfs if clf is not None)","219","            Training vectors, where n_samples is the number of samples and","220","            n_features is the number of features.","264","            Training vectors, where n_samples is the number of samples and","265","            n_features is the number of features."]}]}}}