{"4eb85b471ac862dc82ef86cfc600f99ed0d8cbd5":{"changes":{"sklearn\/linear_model\/stochastic_gradient.py":"MODIFY","doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/ensemble\/gradient_boosting.py":"MODIFY","sklearn\/linear_model\/perceptron.py":"MODIFY","sklearn\/neural_network\/multilayer_perceptron.py":"MODIFY","sklearn\/neural_network\/tests\/test_mlp.py":"MODIFY","sklearn\/ensemble\/tests\/test_gradient_boosting.py":"MODIFY","sklearn\/linear_model\/passive_aggressive.py":"MODIFY"},"diff":{"sklearn\/linear_model\/stochastic_gradient.py":[{"add":["830","        a stratified fraction of training data as validation and terminate","831","        training when validation score is not improving by at least tol for","1435","        a fraction of training data as validation and terminate","1436","        training when validation score is not improving by at least tol for"],"delete":["830","        a fraction of training data as validation and terminate training when","831","        validation score is not improving by at least tol for","1435","        a fraction of training data as validation and terminate training when","1436","        validation score is not improving by at least tol for"]}],"doc\/whats_new\/v0.21.rst":[{"add":["28","- :class:`ensemble.GradientBoostingClassifier` |Fix|","29","- :class:`neural_network.MLPClassifier` |Fix|","186","  validation sets for early stopping were not sampled with stratification.","187","  :issue:`13164` by :user:`Nicolas Hug<NicolasHug>`.","188","","189","- |Fix| Fixed a bug in :class:`ensemble.GradientBoostingClassifier` where","432","- |Fix| Fixed a bug in :class:`neural_network.MLPClassifier` where","433","  validation sets for early stopping were not sampled with stratification. In","434","  multilabel case however, splits are still not stratified.","435","  :issue:`13164` by :user:`Nicolas Hug<NicolasHug>`.","436",""],"delete":["28","- :class:`ensemble.GradientBoostingClassifier` for multiclass","29","  classification. |Fix|"]}],"sklearn\/ensemble\/gradient_boosting.py":[{"add":["1449","            stratify = y if is_classifier(self) else None","1453","                                 test_size=self.validation_fraction,","1454","                                 stratify=stratify))","1937","        iterations. The split is stratified."],"delete":["1452","                                 test_size=self.validation_fraction))","1935","        iterations."]}],"sklearn\/linear_model\/perceptron.py":[{"add":["64","        a stratified fraction of training data as validation and terminate","65","        training when validation score is not improving by at least tol for"],"delete":["64","        a fraction of training data as validation and terminate training when","65","        validation score is not improving by at least tol for"]}],"sklearn\/neural_network\/multilayer_perceptron.py":[{"add":["486","            # don't stratify in multilabel classification","487","            should_stratify = is_classifier(self) and self.n_outputs_ == 1","488","            stratify = y if should_stratify else None","491","                test_size=self.validation_fraction,","492","                stratify=stratify)","809","        ``n_iter_no_change`` consecutive epochs. The split is stratified,","810","        except in a multilabel setting."],"delete":["488","                test_size=self.validation_fraction)","805","        ``n_iter_no_change`` consecutive epochs."]}],"sklearn\/neural_network\/tests\/test_mlp.py":[{"add":["310","    # Make sure early stopping still work now that spliting is stratified by","311","    # default (it is disabled for multilabel classification)","312","    mlp = MLPClassifier(early_stopping=True)","313","    mlp.fit(X, y).predict(X)","314","","670","","671","","672","def test_early_stopping_stratified():","673","    # Make sure data splitting for early stopping is stratified","674","    X = [[1, 2], [2, 3], [3, 4], [4, 5]]","675","    y = [0, 0, 0, 1]","676","","677","    mlp = MLPClassifier(early_stopping=True)","678","    with pytest.raises(","679","            ValueError,","680","            match='The least populated class in y has only 1 member'):","681","        mlp.fit(X, y)"],"delete":[]}],"sklearn\/ensemble\/tests\/test_gradient_boosting.py":[{"add":["1267","    for est, tol, early_stop_n_estimators in ((gbc, 1e-1, 28), (gbr, 1e-1, 13),","1268","                                              (gbc, 1e-3, 70),","1323","def test_early_stopping_stratified():","1324","    # Make sure data splitting for early stopping is stratified","1325","    X = [[1, 2], [2, 3], [3, 4], [4, 5]]","1326","    y = [0, 0, 0, 1]","1327","","1328","    gbc = GradientBoostingClassifier(n_iter_no_change=5)","1329","    with pytest.raises(","1330","            ValueError,","1331","            match='The least populated class in y has only 1 member'):","1332","        gbc.fit(X, y)","1333","","1334","","1395","    # when doing early stopping (_, , y_train, _ = train_test_split(X, y))","1400","    X = [[1]] * 10","1401","    y = [0, 0] + [1] * 8  # only 2 negative class over 10 samples","1402","    gb = GradientBoostingClassifier(n_iter_no_change=5, random_state=0,","1403","                                    validation_fraction=8)","1409","    # No error if we let training data be big enough","1410","    gb = GradientBoostingClassifier(n_iter_no_change=5, random_state=0,","1411","                                    validation_fraction=4)"],"delete":["1267","    for est, tol, early_stop_n_estimators in ((gbc, 1e-1, 24), (gbr, 1e-1, 13),","1268","                                              (gbc, 1e-3, 36),","1383","    # when doing early stopping (_, y_train, _, _ = train_test_split(X, y))","1388","    X = [[1, 2], [2, 3], [3, 4], [4, 5]]","1389","    y = [0, 1, 1, 1]","1390","    gb = GradientBoostingClassifier(n_iter_no_change=5, random_state=4)","1396","    # No error with another random seed","1397","    gb = GradientBoostingClassifier(n_iter_no_change=5, random_state=0)","1398","    gb.fit(X, y)"]}],"sklearn\/linear_model\/passive_aggressive.py":[{"add":["39","        a stratified fraction of training data as validation and terminate","40","        training when validation score is not improving by at least tol for","284","        a fraction of training data as validation and terminate","285","        training when validation score is not improving by at least tol for"],"delete":["39","        a fraction of training data as validation and terminate training when","40","        validation score is not improving by at least tol for","284","        a fraction of training data as validation and terminate training when","285","        validation score is not improving by at least tol for"]}]}},"701144559f0448e7b140fead0384b9cba99af096":{"changes":{"sklearn\/utils\/fixes.py":"MODIFY","sklearn\/feature_extraction\/text.py":"MODIFY","sklearn\/model_selection\/tests\/test_search.py":"MODIFY","sklearn\/utils\/tests\/test_fixes.py":"MODIFY","sklearn\/covariance\/graph_lasso_.py":"MODIFY","sklearn\/utils\/multiclass.py":"MODIFY","sklearn\/datasets\/samples_generator.py":"MODIFY","sklearn\/utils\/__init__.py":"MODIFY","sklearn\/feature_extraction\/tests\/test_text.py":"MODIFY","sklearn\/preprocessing\/data.py":"MODIFY","sklearn\/metrics\/scorer.py":"MODIFY","sklearn\/model_selection\/_split.py":"MODIFY","sklearn\/feature_extraction\/dict_vectorizer.py":"MODIFY","sklearn\/model_selection\/_search.py":"MODIFY"},"diff":{"sklearn\/utils\/fixes.py":[{"add":[],"delete":["14","from collections.abc import Sequence as _Sequence  # noqa","15","from collections.abc import Iterable as _Iterable  # noqa","16","from collections.abc import Mapping as _Mapping  # noqa","17","from collections.abc import Sized as _Sized  # noqa","18","","22","from scipy.special import boxcox  # noqa","24","from numpy import nanpercentile  # noqa","25","from numpy import nanmedian  # noqa"]}],"sklearn\/feature_extraction\/text.py":[{"add":["17","from collections.abc import Mapping"],"delete":["32","from ..utils.fixes import _Mapping as Mapping  # noqa"]}],"sklearn\/model_selection\/tests\/test_search.py":[{"add":["2","from collections.abc import Iterable, Sized"],"delete":["14","from sklearn.utils.fixes import _Iterable as Iterable, _Sized as Sized"]}],"sklearn\/utils\/tests\/test_fixes.py":[{"add":[],"delete":["14","from sklearn.utils.fixes import nanmedian","15","from sklearn.utils.fixes import nanpercentile","30","@pytest.mark.parametrize(","31","    \"axis, expected_median\",","32","    [(None, 4.0),","33","     (0, np.array([1., 3.5, 3.5, 4., 7., np.nan])),","34","     (1, np.array([1., 6.]))]","35",")","36","def test_nanmedian(axis, expected_median):","37","    X = np.array([[1, 1, 1, 2, np.nan, np.nan],","38","                  [np.nan, 6, 6, 6, 7, np.nan]])","39","    median = nanmedian(X, axis=axis)","40","    if axis is None:","41","        assert median == pytest.approx(expected_median)","42","    else:","43","        assert_allclose(median, expected_median)","44","","45","","46","@pytest.mark.parametrize(","47","    \"a, q, expected_percentile\",","48","    [(np.array([1, 2, 3, np.nan]), [0, 50, 100], np.array([1., 2., 3.])),","49","     (np.array([1, 2, 3, np.nan]), 50, 2.),","50","     (np.array([np.nan, np.nan]), [0, 50], np.array([np.nan, np.nan]))]","51",")","52","def test_nanpercentile(a, q, expected_percentile):","53","    percentile = nanpercentile(a, q)","54","    assert_allclose(percentile, expected_percentile)","55","","56",""]}],"sklearn\/covariance\/graph_lasso_.py":[{"add":["7","from collections.abc import Sequence"],"delete":["21","from ..utils.fixes import _Sequence as Sequence"]}],"sklearn\/utils\/multiclass.py":[{"add":["8","from collections.abc import Sequence"],"delete":["8","from __future__ import division","18","from ..utils.fixes import _Sequence as Sequence"]}],"sklearn\/datasets\/samples_generator.py":[{"add":["10","from collections.abc import Iterable","11",""],"delete":["17","from ..utils.fixes import _Iterable as Iterable"]}],"sklearn\/utils\/__init__.py":[{"add":["3","from collections.abc import Sequence"],"delete":["15","from .fixes import _Sequence as Sequence"]}],"sklearn\/feature_extraction\/tests\/test_text.py":[{"add":["2","from collections.abc import Mapping"],"delete":["38","from sklearn.utils.fixes import _Mapping as Mapping"]}],"sklearn\/preprocessing\/data.py":[{"add":["19","from scipy.special import boxcox","1140","            self.center_ = np.nanmedian(X, axis=0)","1155","                quantiles.append(np.nanpercentile(column_data,","1156","                                                  self.quantile_range))","2103","            self.quantiles_.append(np.nanpercentile(col, references))","2145","                self.quantiles_.append(","2146","                        np.nanpercentile(column_data, references))"],"delete":["24","from ..utils.fixes import boxcox, nanpercentile, nanmedian","1140","            self.center_ = nanmedian(X, axis=0)","1155","                quantiles.append(nanpercentile(column_data,","1156","                                               self.quantile_range))","2103","            self.quantiles_.append(nanpercentile(col, references))","2145","                self.quantiles_.append(nanpercentile(column_data, references))"]}],"sklearn\/metrics\/scorer.py":[{"add":["21","from collections.abc import Iterable"],"delete":["41","from ..utils.fixes import _Iterable as Iterable"]}],"sklearn\/model_selection\/_split.py":[{"add":["15","from collections.abc import Iterable"],"delete":["29","from ..utils.fixes import _Iterable as Iterable"]}],"sklearn\/feature_extraction\/dict_vectorizer.py":[{"add":["5","from collections.abc import Mapping"],"delete":["12","from ..utils.fixes import _Mapping as Mapping"]}],"sklearn\/model_selection\/_search.py":[{"add":["16","from collections.abc import Mapping, Sequence, Iterable"],"delete":["34","from ..utils.fixes import _Mapping as Mapping, _Sequence as Sequence","35","from ..utils.fixes import _Iterable as Iterable"]}]}},"9597da9b0270ee1cbc783fd7f6633e7af624e34f":{"changes":{"doc\/modules\/linear_model.rst":"MODIFY"},"diff":{"doc\/modules\/linear_model.rst":[{"add":["694","with :math:`\\text{diag}(A) = \\lambda = \\{\\lambda_{1},...,\\lambda_{p}\\}`."],"delete":["694","with :math:`\\diag \\; (A) = \\lambda = \\{\\lambda_{1},...,\\lambda_{p}\\}`."]}]}},"11e4da51beb88a49d40af6ba0edf8c862369af8e":{"changes":{"sklearn\/ensemble\/tests\/test_iforest.py":"MODIFY"},"diff":{"sklearn\/ensemble\/tests\/test_iforest.py":[{"add":[],"delete":["12","from sklearn.utils.testing import assert_almost_equal","282",""]}]}},"5f0263fe7f1ce66a91a3af01a54caad7ac546443":{"changes":{"sklearn\/tree\/_utils.pyx":"MODIFY","sklearn\/linear_model\/cd_fast.pyx":"MODIFY","sklearn\/utils\/seq_dataset.pyx.tp":"MODIFY"},"diff":{"sklearn\/tree\/_utils.pyx":[{"add":["22","from ..utils._random cimport our_rand_r","66","    return low + our_rand_r(random_state) % (high - low)","72","    return ((high - low) * <double> our_rand_r(random_state) \/"],"delete":["22","from ..utils cimport _random","66","    return low + _random.our_rand_r(random_state) % (high - low)","72","    return ((high - low) * <double> _random.our_rand_r(random_state) \/"]}],"sklearn\/linear_model\/cd_fast.pyx":[{"add":["26","from ..utils._random cimport our_rand_r","44","    return our_rand_r(random_state) % end"],"delete":["26","from ..utils cimport _random","44","    return _random.our_rand_r(random_state) % end"]}],"sklearn\/utils\/seq_dataset.pyx.tp":[{"add":["47","from ._random cimport our_rand_r","157","            j = i + our_rand_r(&seed) % (n - i)","171","        cdef int current_index = our_rand_r(&self.seed) % n"],"delete":["47","from . cimport _random","157","            j = i + _random.our_rand_r(&seed) % (n - i)","171","        cdef int current_index = _random.our_rand_r(&self.seed) % n"]}]}},"02a0101c7c9e7c883fc1b916f0d2777e8581e1f8":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/ensemble\/gradient_boosting.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["80","  error for multiclass multioutput forests models if any targets were strings.","81","  :issue:`12834` by :user:`Elizabeth Sander <elsander>`.","82","","83","- |Fix| Fixed a bug in :class:`ensemble.gradient_boosting.LossFunction` and","84","  :class:`ensemble.gradient_boosting.LeastSquaresError` where the default","85","  value of ``learning_rate`` in ``update_terminal_regions`` is not consistent","86","  with the document and the caller functions.","87","  :issue:`6463` by :user:`movelikeriver <movelikeriver>`.","147","  :class:`~model_selection.RandomizedSearchCV`, and methods"],"delete":["80","   error for multiclass multioutput forests models if any targets were strings.","81","   :issue:`12834` by :user:`Elizabeth Sander <elsander>`.","141","  :class:`~model_selection.RandomSearchCV`, and methods"]}],"sklearn\/ensemble\/gradient_boosting.py":[{"add":["357","                                learning_rate=0.1, k=0):","471","                                learning_rate=0.1, k=0):","1203","                                         learning_rate=self.learning_rate, k=k)"],"delete":["357","                                learning_rate=1.0, k=0):","471","                                learning_rate=1.0, k=0):","1203","                                         self.learning_rate, k=k)"]}]}},"c8e757df96280fe1d592eb2bc249a9b230fc9eda":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/linear_model\/tests\/test_ridge.py":"MODIFY","sklearn\/linear_model\/ridge.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["232","- |Enhancement| `sparse_cg` solver in :class:`linear_model.ridge.Ridge`","233","  now supports fitting the intercept (i.e. ``fit_intercept=True``) when","234","  inputs are sparse . :issue:`13336` by :user:`Bartosz Telenczuk <btel>`","235",""],"delete":[]}],"sklearn\/linear_model\/tests\/test_ridge.py":[{"add":["817","","820","    for solver in ['sag', 'sparse_cg']:","824","        with pytest.warns(None) as record:","825","            sparse.fit(X_csr, y)","826","        assert len(record) == 0","831","    for solver in ['saga', 'lsqr']:","832","        sparse = Ridge(alpha=1., tol=1.e-15, solver=solver, fit_intercept=True)","833","        assert_warns(UserWarning, sparse.fit, X_csr, y)","834","        assert_almost_equal(dense.intercept_, sparse.intercept_)","835","        assert_array_almost_equal(dense.coef_, sparse.coef_)"],"delete":["819","    for solver in ['saga', 'sag']:","823","        sparse.fit(X_csr, y)","828","    sparse = Ridge(alpha=1., tol=1.e-15, solver='lsqr', fit_intercept=True)","829","    assert_warns(UserWarning, sparse.fit, X_csr, y)","830","    assert_almost_equal(dense.intercept_, sparse.intercept_)","831","    assert_array_almost_equal(dense.coef_, sparse.coef_)"]}],"sklearn\/linear_model\/ridge.py":[{"add":["35","def _solve_sparse_cg(X, y, alpha, max_iter=None, tol=1e-3, verbose=0,","36","                     X_offset=None, X_scale=None):","37","","38","    def _get_rescaled_operator(X):","39","","40","        X_offset_scale = X_offset \/ X_scale","41","","42","        def matvec(b):","43","            return X.dot(b) - b.dot(X_offset_scale)","44","","45","        def rmatvec(b):","46","            return X.T.dot(b) - X_offset_scale * np.sum(b)","47","","48","        X1 = sparse.linalg.LinearOperator(shape=X.shape,","49","                                          matvec=matvec,","50","                                          rmatvec=rmatvec)","51","        return X1","52","","54","","55","    if X_offset is None or X_scale is None:","56","        X1 = sp_linalg.aslinearoperator(X)","57","    else:","58","        X1 = _get_rescaled_operator(X)","59","","350","","351","    return _ridge_regression(X, y, alpha,","352","                             sample_weight=sample_weight,","353","                             solver=solver,","354","                             max_iter=max_iter,","355","                             tol=tol,","356","                             verbose=verbose,","357","                             random_state=random_state,","358","                             return_n_iter=return_n_iter,","359","                             return_intercept=return_intercept,","360","                             X_scale=None,","361","                             X_offset=None)","362","","363","","364","def _ridge_regression(X, y, alpha, sample_weight=None, solver='auto',","365","                      max_iter=None, tol=1e-3, verbose=0, random_state=None,","366","                      return_n_iter=False, return_intercept=False,","367","                      X_scale=None, X_offset=None):","368","","438","        coef = _solve_sparse_cg(X, y, alpha,","439","                                max_iter=max_iter,","440","                                tol=tol,","441","                                verbose=verbose,","442","                                X_offset=X_offset,","443","                                X_scale=X_scale)","540","        # when X is sparse we only remove offset from y","543","            sample_weight=sample_weight, return_mean=True)","546","        if (sparse.issparse(X) and self.fit_intercept and","547","           self.solver != 'sparse_cg'):","548","            self.coef_, self.n_iter_, self.intercept_ = _ridge_regression(","553","            # add the offset which was subtracted by _preprocess_data","556","            if sparse.issparse(X):","557","                # required to fit intercept with sparse_cg solver","558","                params = {'X_offset': X_offset, 'X_scale': X_scale}","559","            else:","560","                # for dense matrices or when intercept is set to 0","561","                params = {}","562","","563","            self.coef_, self.n_iter_ = _ridge_regression(","567","                return_intercept=False, **params)","568",""],"delete":["35","def _solve_sparse_cg(X, y, alpha, max_iter=None, tol=1e-3, verbose=0):","37","    X1 = sp_linalg.aslinearoperator(X)","397","        coef = _solve_sparse_cg(X, y, alpha, max_iter, tol, verbose)","496","            sample_weight=sample_weight)","499","        if sparse.issparse(X) and self.fit_intercept:","500","            self.coef_, self.n_iter_, self.intercept_ = ridge_regression(","507","            self.coef_, self.n_iter_ = ridge_regression(","511","                return_intercept=False)"]}]}},"979c76129c912727c701c8706236a6db5590997c":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/calibration.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["228","- |Fix| Fixed a bug that made `calibration.CalibratedClassifierCV` fail when","229","  given a `sample_weight` parameter of type `list` (in the case where","230","  `sample_weights` are not supported by the wrapped estimator). :issue:`1375`","231","  by :user:`William de Vazelhes <wdevazelhes>`.","232",""],"delete":[]}],"sklearn\/calibration.py":[{"add":["20","from .base import (BaseEstimator, ClassifierMixin, RegressorMixin, clone,","21","                   MetaEstimatorMixin)","30","class CalibratedClassifierCV(BaseEstimator, ClassifierMixin,","31","                             MetaEstimatorMixin):","174","                sample_weight = check_array(sample_weight, ensure_2d=False)"],"delete":["20","from .base import BaseEstimator, ClassifierMixin, RegressorMixin, clone","29","class CalibratedClassifierCV(BaseEstimator, ClassifierMixin):"]}]}},"2a2dc8c876e75f9bbbbda1d75f569060a2627403":{"changes":{"azure-pipelines.yml":"ADD","build_tools\/azure\/posix.yml":"ADD","build_tools\/azure\/test_script.cmd":"ADD","build_tools\/azure\/test_pytest_soft_dependency.sh":"ADD","build_tools\/azure\/test_docs.sh":"ADD","build_tools\/azure\/upload_codecov.sh":"ADD","build_tools\/azure\/test_script.sh":"ADD","build_tools\/azure\/install.cmd":"ADD","build_tools\/azure\/install.sh":"ADD","build_tools\/azure\/windows.yml":"ADD"},"diff":{"azure-pipelines.yml":[{"add":[],"delete":[]}],"build_tools\/azure\/posix.yml":[{"add":[],"delete":[]}],"build_tools\/azure\/test_script.cmd":[{"add":[],"delete":[]}],"build_tools\/azure\/test_pytest_soft_dependency.sh":[{"add":[],"delete":[]}],"build_tools\/azure\/test_docs.sh":[{"add":[],"delete":[]}],"build_tools\/azure\/upload_codecov.sh":[{"add":[],"delete":[]}],"build_tools\/azure\/test_script.sh":[{"add":[],"delete":[]}],"build_tools\/azure\/install.cmd":[{"add":[],"delete":[]}],"build_tools\/azure\/install.sh":[{"add":[],"delete":[]}],"build_tools\/azure\/windows.yml":[{"add":[],"delete":[]}]}},"ffd27c8594fa58b8b0559ae0cf461116bf09e381":{"changes":{"sklearn\/__init__.py":"MODIFY","build_tools\/travis\/install.sh":"MODIFY","build_tools\/azure\/install.sh":"MODIFY"},"diff":{"sklearn\/__init__.py":[{"add":["18","import os","50","# On OSX, we can get a runtime error due to multiple OpenMP libraries loaded","51","# simultaneously. This can happen for instance when calling BLAS inside a","52","# prange. Setting the following environment variable allows multiple OpenMP","53","# libraries to be loaded. It should not degrade performances since we manually","54","# take care of potential over-subcription performance issues, in sections of","55","# the code where nested OpenMP loops can happen, by dynamically reconfiguring","56","# the inner OpenMP runtime to temporarily disable it while under the scope of","57","# the outer OpenMP parallel section.","58","os.environ.setdefault(\"KMP_DUPLICATE_LIB_OK\", \"True\")","59","","60",""],"delete":[]}],"build_tools\/travis\/install.sh":[{"add":[],"delete":["40","","41","    # avoid error due to multiple OpenMP libraries loaded simultaneously","42","    export KMP_DUPLICATE_LIB_OK=TRUE"]}],"build_tools\/azure\/install.sh":[{"add":[],"delete":["18","","19","    # avoid error due to multiple OpenMP libraries loaded simultaneously","20","    export KMP_DUPLICATE_LIB_OK=TRUE"]}]}},"c22b8712d88133ddb9f3572944c111c93b9af300":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/ensemble\/iforest.py":"MODIFY","sklearn\/ensemble\/tests\/test_iforest.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["170","- |Efficiency| :class:`ensemble.IsolationForest` now uses chunks of data at","171","  prediction step, thus capping the memory usage. :issue:`13283` by","172","  `Nicolas Goix`_.","173",""],"delete":[]}],"sklearn\/ensemble\/iforest.py":[{"add":["11","from ..utils import (","12","    check_random_state,","13","    check_array,","14","    gen_batches,","15","    get_chunk_n_rows,","16",")","18","from ..utils.validation import check_is_fitted, _num_samples","396","        # Take the opposite of the scores as bigger is better (here less","397","        # abnormal)","398","        return -self._compute_chunked_score_samples(X)","399","","400","    @property","401","    def threshold_(self):","402","        if self.behaviour != 'old':","403","            raise AttributeError(\"threshold_ attribute does not exist when \"","404","                                 \"behaviour != 'old'\")","405","        warn(\"threshold_ attribute is deprecated in 0.20 and will\"","406","             \" be removed in 0.22.\", DeprecationWarning)","407","        return self._threshold_","408","","409","    def _compute_chunked_score_samples(self, X):","410","","411","        n_samples = _num_samples(X)","418","        # We get as many rows as possible within our working_memory budget","419","        # (defined by sklearn.get_config()['working_memory']) to store","420","        # self._max_features in each row during computation.","421","        #","422","        # Note:","423","        #  - this will get at least 1 row, even if 1 row of score will","424","        #    exceed working_memory.","425","        #  - this does only account for temporary memory usage while loading","426","        #    the data needed to compute the scores -- the returned scores","427","        #    themselves are 1D.","428","","429","        chunk_n_rows = get_chunk_n_rows(row_bytes=16 * self._max_features,","430","                                        max_n_rows=n_samples)","431","        slices = gen_batches(n_samples, chunk_n_rows)","432","","433","        scores = np.zeros(n_samples, order=\"f\")","434","","435","        for sl in slices:","436","            # compute score on the slices of test samples:","437","            scores[sl] = self._compute_score_samples(X[sl], subsample_features)","438","","439","        return scores","440","","441","    def _compute_score_samples(self, X, subsample_features):","442","        \"\"\"Compute the score of each samples in X going through the extra trees.","443","","444","        Parameters","445","        ----------","446","        X : array-like or sparse matrix","447","","448","        subsample_features : bool,","449","            whether features should be subsampled","450","        \"\"\"","451","        n_samples = X.shape[0]","452","","453","        depths = np.zeros(n_samples, order=\"f\")","454","","456","            X_subset = X[:, features] if subsample_features else X","457","","473","        return scores"],"delete":["11","from ..utils import check_random_state, check_array","13","from ..utils.validation import check_is_fitted","390","        n_samples = X.shape[0]","392","        n_samples_leaf = np.zeros(n_samples, order=\"f\")","393","        depths = np.zeros(n_samples, order=\"f\")","401","            if subsample_features:","402","                X_subset = X[:, features]","403","            else:","404","                X_subset = X","420","","421","        # Take the opposite of the scores as bigger is better (here less","422","        # abnormal)","423","        return -scores","424","","425","    @property","426","    def threshold_(self):","427","        if self.behaviour != 'old':","428","            raise AttributeError(\"threshold_ attribute does not exist when \"","429","                                 \"behaviour != 'old'\")","430","        warn(\"threshold_ attribute is deprecated in 0.20 and will\"","431","             \" be removed in 0.22.\", DeprecationWarning)","432","        return self._threshold_"]}],"sklearn\/ensemble\/tests\/test_iforest.py":[{"add":["31","from unittest.mock import Mock, patch","328","","329","","330","# mock get_chunk_n_rows to actually test more than one chunk (here one","331","# chunk = 3 rows:","332","@patch(","333","    \"sklearn.ensemble.iforest.get_chunk_n_rows\",","334","    side_effect=Mock(**{\"return_value\": 3}),","335",")","336","@pytest.mark.parametrize(","337","    \"contamination, n_predict_calls\", [(0.25, 3), (\"auto\", 2)]","338",")","339","@pytest.mark.filterwarnings(\"ignore:threshold_ attribute\")","340","def test_iforest_chunks_works1(","341","    mocked_get_chunk, contamination, n_predict_calls","342","):","343","    test_iforest_works(contamination)","344","    assert mocked_get_chunk.call_count == n_predict_calls","345","","346","","347","# idem with chunk_size = 5 rows","348","@patch(","349","    \"sklearn.ensemble.iforest.get_chunk_n_rows\",","350","    side_effect=Mock(**{\"return_value\": 10}),","351",")","352","@pytest.mark.parametrize(","353","    \"contamination, n_predict_calls\", [(0.25, 3), (\"auto\", 2)]","354",")","355","@pytest.mark.filterwarnings(\"ignore:threshold_ attribute\")","356","def test_iforest_chunks_works2(","357","    mocked_get_chunk, contamination, n_predict_calls","358","):","359","    test_iforest_works(contamination)","360","    assert mocked_get_chunk.call_count == n_predict_calls"],"delete":[]}]}},"cdfca8cba33be63ef50ba9e14d8823cc551baf92":{"changes":{"sklearn\/base.py":"MODIFY","sklearn\/manifold\/locally_linear.py":"MODIFY","sklearn\/cross_decomposition\/cca_.py":"MODIFY"},"diff":{"sklearn\/base.py":[{"add":["8","import platform","557","class _UnstableArchMixin(object):","558","    \"\"\"Mark estimators that are non-determinstic on 32bit or PowerPC\"\"\"","560","        return {'non_deterministic': (","561","            _IS_32BIT or platform.machine().startswith(('ppc', 'powerpc')))}"],"delete":["8","import struct","557","class _UnstableOn32BitMixin(object):","558","    \"\"\"Mark estimators that are non-determinstic on 32bit.\"\"\"","560","        return {'non_deterministic': _IS_32BIT}"]}],"sklearn\/manifold\/locally_linear.py":[{"add":["11","from ..base import BaseEstimator, TransformerMixin, _UnstableArchMixin","521","                             _UnstableArchMixin):"],"delete":["11","from ..base import BaseEstimator, TransformerMixin, _UnstableOn32BitMixin","521","                             _UnstableOn32BitMixin):"]}],"sklearn\/cross_decomposition\/cca_.py":[{"add":["1","from ..base import _UnstableArchMixin","6","class CCA(_PLS, _UnstableArchMixin):"],"delete":["1","from ..base import _UnstableOn32BitMixin","6","class CCA(_PLS, _UnstableOn32BitMixin):"]}]}},"b40868d52b479eedf31e49e43b6b8bec1a7dfa44":{"changes":{"sklearn\/preprocessing\/_discretization.py":"MODIFY","doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/preprocessing\/tests\/test_discretization.py":"MODIFY"},"diff":{"sklearn\/preprocessing\/_discretization.py":[{"add":["58","        Number of bins per feature. Bins whose width are too small","59","        (i.e., <= 1e-8) are removed with a warning.","105","    ``KBinsDiscretizer`` might produce constant features (e.g., when","106","    ``encode = 'onehot'`` and certain bins do not contain any data).","107","    These features can be removed with feature selection algorithms","108","    (e.g., :class:`sklearn.feature_selection.VarianceThreshold`).","109","","185","            # Remove bins whose width are too small (i.e., <= 1e-8)","186","            if self.strategy in ('quantile', 'kmeans'):","187","                mask = np.ediff1d(bin_edges[jj], to_begin=np.inf) > 1e-8","188","                bin_edges[jj] = bin_edges[jj][mask]","189","                if len(bin_edges[jj]) - 1 != n_bins[jj]:","190","                    warnings.warn('Bins whose width are too small (i.e., <= '","191","                                  '1e-8) in feature %d are removed. Consider '","192","                                  'decreasing the number of bins.' % jj)","193","                    n_bins[jj] = len(bin_edges[jj]) - 1","194",""],"delete":["58","        Number of bins per feature."]}],"doc\/whats_new\/v0.20.rst":[{"add":["66","- |Fix| Bins whose width are too small (i.e., <= 1e-8) are removed","67","  with a warning in :class:`preprocessing.KBinsDiscretizer`.","68","  :issue:`13165` by :user:`Hanmin Qin <qinhanmin2014>`.","69",""],"delete":[]}],"sklearn\/preprocessing\/tests\/test_discretization.py":[{"add":["9","    assert_array_almost_equal,","212","@pytest.mark.parametrize(","213","    'strategy, expected_inv',","214","    [('uniform', [[-1.5, 2., -3.5, -0.5], [-0.5, 3., -2.5, -0.5],","215","                  [0.5, 4., -1.5, 0.5], [0.5, 4., -1.5, 1.5]]),","216","     ('kmeans', [[-1.375, 2.125, -3.375, -0.5625],","217","                 [-1.375, 2.125, -3.375, -0.5625],","218","                 [-0.125, 3.375, -2.125, 0.5625],","219","                 [0.75, 4.25, -1.25, 1.625]]),","220","     ('quantile', [[-1.5, 2., -3.5, -0.75], [-0.5, 3., -2.5, 0.],","221","                   [0.5, 4., -1.5, 1.25], [0.5, 4., -1.5, 1.25]])])","223","def test_inverse_transform(strategy, encode, expected_inv):","226","    Xinv = kbd.inverse_transform(Xt)","227","    assert_array_almost_equal(expected_inv, Xinv)","254","","255","","256","@pytest.mark.parametrize(","257","    'strategy, expected_bin_edges',","258","    [('quantile', [0, 1, 3]), ('kmeans', [0, 1.5, 3])])","259","def test_redundant_bins(strategy, expected_bin_edges):","260","    X = [[0], [0], [0], [0], [3], [3]]","261","    kbd = KBinsDiscretizer(n_bins=3, strategy=strategy)","262","    msg = (\"Bins whose width are too small (i.e., <= 1e-8) in feature 0 \"","263","           \"are removed. Consider decreasing the number of bins.\")","264","    assert_warns_message(UserWarning, msg, kbd.fit, X)","265","    assert_array_almost_equal(kbd.bin_edges_[0], expected_bin_edges)","266","","267","","268","def test_percentile_numeric_stability():","269","    X = np.array([0.05, 0.05, 0.95]).reshape(-1, 1)","270","    bin_edges = np.array([0.05, 0.23, 0.41, 0.59, 0.77, 0.95])","271","    Xt = np.array([0, 0, 4]).reshape(-1, 1)","272","    kbd = KBinsDiscretizer(n_bins=10, encode='ordinal',","273","                           strategy='quantile')","274","    msg = (\"Bins whose width are too small (i.e., <= 1e-8) in feature 0 \"","275","           \"are removed. Consider decreasing the number of bins.\")","276","    assert_warns_message(UserWarning, msg, kbd.fit, X)","277","    assert_array_almost_equal(kbd.bin_edges_[0], bin_edges)","278","    assert_array_almost_equal(kbd.transform(X), Xt)"],"delete":["211","@pytest.mark.parametrize('strategy', ['uniform', 'kmeans', 'quantile'])","213","def test_inverse_transform(strategy, encode):","214","    X = np.random.RandomState(0).randn(100, 3)","217","    X2 = kbd.inverse_transform(Xt)","218","    X2t = kbd.fit_transform(X2)","219","    if encode == 'onehot':","220","        assert_array_equal(Xt.todense(), X2t.todense())","221","    else:","222","        assert_array_equal(Xt, X2t)","223","    if 'onehot' in encode:","224","        Xt = kbd._encoder.inverse_transform(Xt)","225","        X2t = kbd._encoder.inverse_transform(X2t)","226","","227","    assert_array_equal(Xt.max(axis=0) + 1, kbd.n_bins_)","228","    assert_array_equal(X2t.max(axis=0) + 1, kbd.n_bins_)"]}]}},"ce869d8ddb723ef5e870903cbebd8e256835e234":{"changes":{"sklearn\/preprocessing\/data.py":"MODIFY"},"diff":{"sklearn\/preprocessing\/data.py":[{"add":["1534","                combinations = self._combinations(n_features, self.degree,","1535","                                                  self.interaction_only,","1536","                                                  self.include_bias)","1551","","1552","                # What follows is a faster implementation of:","1553","                # for i, comb in enumerate(combinations):","1554","                #     XP[:, i] = X[:, comb].prod(1)","1555","                # This implementation uses two optimisations.","1556","                # First one is broadcasting,","1557","                # multiply ([X1, ..., Xn], X1) -> [X1 X1, ..., Xn X1]","1558","                # multiply ([X2, ..., Xn], X2) -> [X2 X2, ..., Xn X2]","1559","                # ...","1560","                # multiply ([X[:, start:end], X[:, start]) -> ...","1561","                # Second optimisation happens for degrees >= 3.","1562","                # Xi^3 is computed reusing previous computation:","1563","                # Xi^3 = Xi^2 * Xi.","1564","","1565","                if self.include_bias:","1566","                    XP[:, 0] = 1","1567","                    current_col = 1","1568","                else:","1569","                    current_col = 0","1570","","1571","                # d = 0","1572","                XP[:, current_col:current_col + n_features] = X","1573","                index = list(range(current_col,","1574","                                   current_col + n_features))","1575","                current_col += n_features","1576","                index.append(current_col)","1577","","1578","                # d >= 1","1579","                for _ in range(1, self.degree):","1580","                    new_index = []","1581","                    end = index[-1]","1582","                    for feature_idx in range(n_features):","1583","                        start = index[feature_idx]","1584","                        new_index.append(current_col)","1585","                        if self.interaction_only:","1586","                            start += (index[feature_idx + 1] -","1587","                                      index[feature_idx])","1588","                        next_col = current_col + end - start","1589","                        if next_col <= current_col:","1590","                            break","1591","                        # XP[:, start:end] are terms of degree d - 1","1592","                        # that exclude feature #feature_idx.","1593","                        np.multiply(XP[:, start:end],","1594","                                    X[:, feature_idx:feature_idx + 1],","1595","                                    out=XP[:, current_col:next_col],","1596","                                    casting='no')","1597","                        current_col = next_col","1598","","1599","                    new_index.append(current_col)","1600","                    index = new_index"],"delete":["1533","            combinations = self._combinations(n_features, self.degree,","1534","                                              self.interaction_only,","1535","                                              self.include_bias)","1551","                for i, comb in enumerate(combinations):","1552","                    XP[:, i] = X[:, comb].prod(1)"]}]}},"2ad8735c500343d76339c1b06a247193bce06df8":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","doc\/modules\/preprocessing.rst":"MODIFY","sklearn\/preprocessing\/tests\/test_data.py":"MODIFY","sklearn\/preprocessing\/data.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["378","- |Fix| Fixed a bug in :class:`preprocessing.QuantileTransformer` and","379","  :func:`preprocessing.quantile_transform` to force n_quantiles to be at most","380","  equal to n_samples. Values of n_quantiles larger than n_samples were either","381","  useless or resulting in a wrong approximation of the cumulative distribution","382","  function estimator. :issue:`13333` by :user:`Albert Thomas <albertcthomas>`.","383",""],"delete":[]}],"doc\/modules\/preprocessing.rst":[{"add":["389","  array([[4.3, 2. , 1. , 0.1],","390","         [4.4, 2.2, 1.1, 0.1],","391","         [4.4, 2.2, 1.2, 0.1],","393","         [7.7, 4.1, 6.7, 2.5],","394","         [7.7, 4.2, 6.7, 2.5],","395","         [7.9, 4.4, 6.9, 2.5]])"],"delete":["389","  array([[4.3...,   2...,     1...,     0.1...],","390","         [4.31...,  2.02...,  1.01...,  0.1...],","391","         [4.32...,  2.05...,  1.02...,  0.1...],","393","         [7.84...,  4.34...,  6.84...,  2.5...],","394","         [7.87...,  4.37...,  6.87...,  2.5...],","395","         [7.9...,   4.4...,   6.9...,   2.5...]])"]}],"sklearn\/preprocessing\/tests\/test_data.py":[{"add":["1262","    # check that a warning is raised is n_quantiles > n_samples","1263","    transformer = QuantileTransformer(n_quantiles=100)","1264","    warn_msg = \"n_quantiles is set to n_samples\"","1265","    with pytest.warns(UserWarning, match=warn_msg) as record:","1266","        transformer.fit(X)","1267","    assert len(record) == 1","1268","    assert transformer.n_quantiles_ == X.shape[0]"],"delete":[]}],"sklearn\/preprocessing\/data.py":[{"add":["426","","594","","2043","    n_quantiles : int, optional (default=1000 or n_samples)","2046","        If n_quantiles is larger than the number of samples, n_quantiles is set","2047","        to the number of samples as a larger number of quantiles does not give","2048","        a better approximation of the cumulative distribution function","2049","        estimator.","2078","    n_quantiles_ : integer","2079","        The actual number of quantiles used to discretize the cumulative","2080","        distribution function.","2081","","2228","        n_samples = X.shape[0]","2229","","2230","        if self.n_quantiles > n_samples:","2231","            warnings.warn(\"n_quantiles (%s) is greater than the total number \"","2232","                          \"of samples (%s). n_quantiles is set to \"","2233","                          \"n_samples.\"","2234","                          % (self.n_quantiles, n_samples))","2235","        self.n_quantiles_ = max(1, min(self.n_quantiles, n_samples))","2236","","2240","        self.references_ = np.linspace(0, 1, self.n_quantiles_,","2462","    n_quantiles : int, optional (default=1000 or n_samples)","2465","        If n_quantiles is larger than the number of samples, n_quantiles is set","2466","        to the number of samples as a larger number of quantiles does not give","2467","        a better approximation of the cumulative distribution function","2468","        estimator."],"delete":["426"," ","594","    ","2043","    n_quantiles : int, optional (default=1000)","2223","        self.references_ = np.linspace(0, 1, self.n_quantiles,","2445","    n_quantiles : int, optional (default=1000)"]}]}},"0e9520b235522146b7395b6d848319d3810aba97":{"changes":{"sklearn\/linear_model\/tests\/test_huber.py":"MODIFY","doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/linear_model\/huber.py":"MODIFY"},"diff":{"sklearn\/linear_model\/tests\/test_huber.py":[{"add":["55","","56","    def loss_func(x, *args):","57","        return _huber_loss_and_gradient(x, *args)[0]","58","","59","    def grad_func(x, *args):","60","        return _huber_loss_and_gradient(x, *args)[1]","82","    # Rescale coefs before comparing with assert_array_almost_equal to make","83","    # sure that the number of decimal places used is somewhat insensitive to","84","    # the amplitude of the coefficients and therefore to the scale of the","85","    # data and the regularization parameter","173","        fit_intercept=True, alpha=1.0, max_iter=10000, warm_start=True,","174","        tol=1e-1)","197","    # give a worse score on the non-outliers as compared to the huber","198","    # regressor.","207","","208","","209","def test_huber_bool():","210","    # Test that it does not crash with bool data","211","    X, y = make_regression(n_samples=200, n_features=2, noise=4.0,","212","                           random_state=0)","213","    X_bool = X > 0","214","    HuberRegressor().fit(X_bool, y)"],"delete":["55","    loss_func = lambda x, *args: _huber_loss_and_gradient(x, *args)[0]","56","    grad_func = lambda x, *args: _huber_loss_and_gradient(x, *args)[1]","78","    # Rescale coefs before comparing with assert_array_almost_equal to make sure","79","    # that the number of decimal places used is somewhat insensitive to the","80","    # amplitude of the coefficients and therefore to the scale of the data","81","    # and the regularization parameter","169","        fit_intercept=True, alpha=1.0, max_iter=10000, warm_start=True, tol=1e-1)","192","    # give a worse score on the non-outliers as compared to the huber regressor."]}],"doc\/whats_new\/v0.21.rst":[{"add":["240","- |Fix| Fixed a bug in :class:`linear_model.HuberRegressor` that was","241","  broken when X was of dtype bool.","242","  :issue:`13328` by `Alexandre Gramfort`_.","243","","244",""],"delete":[]}],"sklearn\/linear_model\/huber.py":[{"add":["253","            X, y, copy=False, accept_sparse=['csr'], y_numeric=True,","254","            dtype=[np.float64, np.float32])"],"delete":["253","            X, y, copy=False, accept_sparse=['csr'], y_numeric=True)"]}]}},"4b6273b87442a4437d8b3873ea3022ae163f4fdf":{"changes":{"sklearn\/ensemble\/_hist_gradient_boosting\/_binning.pyx":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/_predictor.pyx":"MODIFY","sklearn\/ensemble\/setup.py":"MODIFY","doc\/modules\/ensemble.rst":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_gradient_boosting.py":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/common.pyx":"ADD","sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_compare_lightgbm.py":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/splitting.pyx":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_grower.py":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_loss.py":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/common.pxd":"ADD","sklearn\/ensemble\/_hist_gradient_boosting\/predictor.py":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/binning.py":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/grower.py":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/histogram.pyx":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_histogram.py":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/gradient_boosting.py":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/loss.py":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_predictor.py":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/_gradient_boosting.pyx":"MODIFY","\/dev\/null":"DELETE","sklearn\/ensemble\/_hist_gradient_boosting\/_loss.pyx":"MODIFY","benchmarks\/bench_hist_gradient_boosting.py":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/utils.pyx":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_binning.py":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_splitting.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY"},"diff":{"sklearn\/ensemble\/_hist_gradient_boosting\/_binning.pyx":[{"add":["14","from libc.math cimport isnan","16","from .common cimport X_DTYPE_C, X_BINNED_DTYPE_C","18","def _map_to_bins(const X_DTYPE_C [:, :] data,","19","                 list binning_thresholds,","20","                 const unsigned char missing_values_bin_idx,","21","                 X_BINNED_DTYPE_C [::1, :] binned):","40","                             missing_values_bin_idx,","46","                               const unsigned char missing_values_bin_idx,","56","","57","        if isnan(data[i]):","58","            binned[i] = missing_values_bin_idx","60","            # for known values, use binary search"],"delete":["15","from .types cimport X_DTYPE_C, X_BINNED_DTYPE_C","17","cpdef _map_to_bins(const X_DTYPE_C [:, :] data, list binning_thresholds,","18","                   X_BINNED_DTYPE_C [::1, :] binned):","51","        if data[i] == INFINITY:","52","            # Special case for +inf.","53","            # -inf is handled properly by binary search.","54","            binned[i] = binning_thresholds.shape[0]"]}],"sklearn\/ensemble\/_hist_gradient_boosting\/_predictor.pyx":[{"add":["9","from libc.math cimport isnan","14","from .common cimport X_DTYPE_C","15","from .common cimport Y_DTYPE_C","16","from .common import Y_DTYPE","17","from .common cimport X_BINNED_DTYPE_C","18","from .common cimport node_struct","46","","47","        if isnan(numeric_data[row, node.feature_idx]):","48","            if node.missing_go_to_left:","49","                node = nodes[node.left]","50","            else:","51","                node = nodes[node.right]","62","        const unsigned char missing_values_bin_idx,","69","        out[i] = _predict_one_from_binned_data(nodes, binned_data, i,","70","                                               missing_values_bin_idx)","76","        const int row,","77","        const unsigned char missing_values_bin_idx) nogil:","87","        if binned_data[row, node.feature_idx] ==  missing_values_bin_idx:","88","            if node.missing_go_to_left:","89","                node = nodes[node.left]","90","            else:","91","                node = nodes[node.right]","93","            if binned_data[row, node.feature_idx] <= node.bin_threshold:","94","                node = nodes[node.left]","95","            else:","96","                node = nodes[node.right]"],"delete":["13","from .types cimport X_DTYPE_C","14","from .types cimport Y_DTYPE_C","15","from .types import Y_DTYPE","16","from .types cimport X_BINNED_DTYPE_C","17","from .types cimport node_struct","45","        if numeric_data[row, node.feature_idx] == INFINITY:","46","            # if data is +inf we always go to the right child, even when the","47","            # threhsold is +inf","48","            node = nodes[node.right]","65","        out[i] = _predict_one_from_binned_data(nodes, binned_data, i)","71","        const int row) nogil:","81","        if binned_data[row, node.feature_idx] <= node.bin_threshold:","82","            node = nodes[node.left]","84","            node = nodes[node.right]"]}],"sklearn\/ensemble\/setup.py":[{"add":["39","    config.add_extension(\"_hist_gradient_boosting.common\",","40","                         sources=[\"_hist_gradient_boosting\/common.pyx\"],"],"delete":["39","    config.add_extension(\"_hist_gradient_boosting.types\",","40","                         sources=[\"_hist_gradient_boosting\/types.pyx\"],"]}],"doc\/modules\/ensemble.rst":[{"add":["866","controls the number of iterations of the boosting process::","876","  >>> clf = HistGradientBoostingClassifier(max_iter=100).fit(X_train, y_train)","878","  0.8965","897","Missing values support","898","----------------------","899","","900",":class:`HistGradientBoostingClassifier` and","901",":class:`HistGradientBoostingRegressor` have built-in support for missing","902","values (NaNs).","903","","904","During training, the tree grower learns at each split point whether samples","905","with missing values should go to the left or right child, based on the","906","potential gain. When predicting, samples with missing values are assigned to","907","the left or right child consequently::","908","","909","  >>> from sklearn.experimental import enable_hist_gradient_boosting  # noqa","910","  >>> from sklearn.ensemble import HistGradientBoostingClassifier","911","  >>> import numpy as np","912","","913","  >>> X = np.array([0, 1, 2, np.nan]).reshape(-1, 1)","914","  >>> y = [0, 0, 1, 1]","915","","916","  >>> gbdt = HistGradientBoostingClassifier(min_samples_leaf=1).fit(X, y)","917","  >>> gbdt.predict(X)","918","  array([0, 0, 1, 1])","919","","920","When the missingness pattern is predictive, the splits can be done on","921","whether the feature value is missing or not::","922","","923","  >>> X = np.array([0, np.nan, 1, 2, np.nan]).reshape(-1, 1)","924","  >>> y = [0, 1, 0, 0, 1]","925","  >>> gbdt = HistGradientBoostingClassifier(min_samples_leaf=1,","926","  ...                                       max_depth=2,","927","  ...                                       learning_rate=1,","928","  ...                                       max_iter=1).fit(X, y)","929","  >>> gbdt.predict(X)","930","  array([0, 1, 0, 0, 1])","931","","932","If no missing values were encountered for a given feature during training,","933","then samples with missing values are mapped to whichever child has the most","934","samples.","935",""],"delete":["866","controls the number of iterations of the boosting process:","875","  >>> clf = HistGradientBoostingClassifier(max_iter=100).fit(X_train, y_train)","878","  0.8998"]}],"sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_gradient_boosting.py":[{"add":["2","from numpy.testing import assert_allclose","4","from sklearn.preprocessing import KBinsDiscretizer, MinMaxScaler","5","from sklearn.model_selection import train_test_split","6","from sklearn.base import clone, BaseEstimator, TransformerMixin","7","from sklearn.pipeline import make_pipeline","38","     ({'max_bins': 256}, 'max_bins=256 should be no smaller than 2 and no'),","177","    assert np.all(mapper_training_data.n_bins_non_missing_ ==","179","    assert np.all(mapper_training_data.n_bins_non_missing_ !=","180","                  mapper_whole_data.n_bins_non_missing_)","181","","182","","183","def test_missing_values_trivial():","184","    # sanity check for missing values support. With only one feature and","185","    # y == isnan(X), the gbdt is supposed to reach perfect accuracy on the","186","    # training set.","187","","188","    n_samples = 100","189","    n_features = 1","190","    rng = np.random.RandomState(0)","191","","192","    X = rng.normal(size=(n_samples, n_features))","193","    mask = rng.binomial(1, .5, size=X.shape).astype(np.bool)","194","    X[mask] = np.nan","195","    y = mask.ravel()","196","    gb = HistGradientBoostingClassifier()","197","    gb.fit(X, y)","198","","199","    assert gb.score(X, y) == pytest.approx(1)","200","","201","","202","@pytest.mark.parametrize('problem', ('classification', 'regression'))","203","@pytest.mark.parametrize(","204","    'missing_proportion, expected_min_score_classification, '","205","    'expected_min_score_regression', [","206","        (.1, .97, .89),","207","        (.2, .93, .81),","208","        (.5, .79, .52)])","209","def test_missing_values_resilience(problem, missing_proportion,","210","                                   expected_min_score_classification,","211","                                   expected_min_score_regression):","212","    # Make sure the estimators can deal with missing values and still yield","213","    # decent predictions","214","","215","    rng = np.random.RandomState(0)","216","    n_samples = 1000","217","    n_features = 2","218","    if problem == 'regression':","219","        X, y = make_regression(n_samples=n_samples, n_features=n_features,","220","                               n_informative=n_features, random_state=rng)","221","        gb = HistGradientBoostingRegressor()","222","        expected_min_score = expected_min_score_regression","223","    else:","224","        X, y = make_classification(n_samples=n_samples, n_features=n_features,","225","                                   n_informative=n_features, n_redundant=0,","226","                                   n_repeated=0, random_state=rng)","227","        gb = HistGradientBoostingClassifier()","228","        expected_min_score = expected_min_score_classification","229","","230","    mask = rng.binomial(1, missing_proportion, size=X.shape).astype(np.bool)","231","    X[mask] = np.nan","232","","233","    gb.fit(X, y)","234","","235","    assert gb.score(X, y) > expected_min_score","284","def test_missing_values_minmax_imputation():","285","    # Compare the buit-in missing value handling of Histogram GBC with an","286","    # a-priori missing value imputation strategy that should yield the same","287","    # results in terms of decision function.","288","    #","289","    # Each feature (containing NaNs) is replaced by 2 features:","290","    # - one where the nans are replaced by min(feature) - 1","291","    # - one where the nans are replaced by max(feature) + 1","292","    # A split where nans go to the left has an equivalent split in the","293","    # first (min) feature, and a split where nans go to the right has an","294","    # equivalent split in the second (max) feature.","295","    #","296","    # Assuming the data is such that there is never a tie to select the best","297","    # feature to split on during training, the learned decision trees should be","298","    # strictly equivalent (learn a sequence of splits that encode the same","299","    # decision function).","300","    #","301","    # The MinMaxImputer transformer is meant to be a toy implementation of the","302","    # \"Missing In Attributes\" (MIA) missing value handling for decision trees","303","    # https:\/\/www.sciencedirect.com\/science\/article\/abs\/pii\/S0167865508000305","304","    # The implementation of MIA as an imputation transformer was suggested by","305","    # \"Remark 3\" in https:\/\/arxiv.org\/abs\/1902.06931","306","","307","    class MinMaxImputer(BaseEstimator, TransformerMixin):","308","","309","        def fit(self, X, y=None):","310","            mm = MinMaxScaler().fit(X)","311","            self.data_min_ = mm.data_min_","312","            self.data_max_ = mm.data_max_","313","            return self","314","","315","        def transform(self, X):","316","            X_min, X_max = X.copy(), X.copy()","317","","318","            for feature_idx in range(X.shape[1]):","319","                nan_mask = np.isnan(X[:, feature_idx])","320","                X_min[nan_mask, feature_idx] = self.data_min_[feature_idx] - 1","321","                X_max[nan_mask, feature_idx] = self.data_max_[feature_idx] + 1","322","","323","            return np.concatenate([X_min, X_max], axis=1)","324","","325","    def make_missing_value_data(n_samples=int(1e4), seed=0):","326","        rng = np.random.RandomState(seed)","327","        X, y = make_regression(n_samples=n_samples, n_features=4,","328","                               random_state=rng)","329","","330","        # Pre-bin the data to ensure a deterministic handling by the 2","331","        # strategies and also make it easier to insert np.nan in a structured","332","        # way:","333","        X = KBinsDiscretizer(n_bins=42, encode=\"ordinal\").fit_transform(X)","334","","335","        # First feature has missing values completely at random:","336","        rnd_mask = rng.rand(X.shape[0]) > 0.9","337","        X[rnd_mask, 0] = np.nan","338","","339","        # Second and third features have missing values for extreme values","340","        # (censoring missingness):","341","        low_mask = X[:, 1] == 0","342","        X[low_mask, 1] = np.nan","343","","344","        high_mask = X[:, 2] == X[:, 2].max()","345","        X[high_mask, 2] = np.nan","346","","347","        # Make the last feature nan pattern very informative:","348","        y_max = np.percentile(y, 70)","349","        y_max_mask = y >= y_max","350","        y[y_max_mask] = y_max","351","        X[y_max_mask, 3] = np.nan","352","","353","        # Check that there is at least one missing value in each feature:","354","        for feature_idx in range(X.shape[1]):","355","            assert any(np.isnan(X[:, feature_idx]))","356","","357","        # Let's use a test set to check that the learned decision function is","358","        # the same as evaluated on unseen data. Otherwise it could just be the","359","        # case that we find two independent ways to overfit the training set.","360","        return train_test_split(X, y, random_state=rng)","361","","362","    # n_samples need to be large enough to minimize the likelihood of having","363","    # several candidate splits with the same gain value in a given tree.","364","    X_train, X_test, y_train, y_test = make_missing_value_data(","365","        n_samples=int(1e4), seed=0)","366","","367","    # Use a small number of leaf nodes and iterations so as to keep","368","    # under-fitting models to minimize the likelihood of ties when training the","369","    # model.","370","    gbm1 = HistGradientBoostingRegressor(max_iter=100,","371","                                         max_leaf_nodes=5,","372","                                         random_state=0)","373","    gbm1.fit(X_train, y_train)","374","","375","    gbm2 = make_pipeline(MinMaxImputer(), clone(gbm1))","376","    gbm2.fit(X_train, y_train)","377","","378","    # Check that the model reach the same score:","379","    assert gbm1.score(X_train, y_train) == \\","380","        pytest.approx(gbm2.score(X_train, y_train))","381","","382","    assert gbm1.score(X_test, y_test) == \\","383","        pytest.approx(gbm2.score(X_test, y_test))","384","","385","    # Check the individual prediction match as a finer grained","386","    # decision function check.","387","    assert_allclose(gbm1.predict(X_train), gbm2.predict(X_train))","388","    assert_allclose(gbm1.predict(X_test), gbm2.predict(X_test))","389","","390","","392","    # Basic test for infinite values","400","","401","","402","def test_infinite_values_missing_values():","403","    # High level test making sure that inf and nan values are properly handled","404","    # when both are present. This is similar to","405","    # test_split_on_nan_with_infinite_values() in test_grower.py, though we","406","    # cannot check the predicitons for binned values here.","407","","408","    X = np.asarray([-np.inf, 0, 1, np.inf, np.nan]).reshape(-1, 1)","409","    y_isnan = np.isnan(X.ravel())","410","    y_isinf = X.ravel() == np.inf","411","","412","    stump_clf = HistGradientBoostingClassifier(min_samples_leaf=1, max_iter=1,","413","                                               learning_rate=1, max_depth=2)","414","","415","    assert stump_clf.fit(X, y_isinf).score(X, y_isinf) == 1","416","    assert stump_clf.fit(X, y_isnan).score(X, y_isnan) == 1"],"delete":["33","     ({'max_bins': 257}, 'max_bins=257 should be no smaller than 2 and no'),","172","    assert np.all(mapper_training_data.actual_n_bins_ ==","174","    assert np.all(mapper_training_data.actual_n_bins_ !=","175","                  mapper_whole_data.actual_n_bins_)"]}],"sklearn\/ensemble\/_hist_gradient_boosting\/common.pyx":[{"add":[],"delete":[]}],"sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_compare_lightgbm.py":[{"add":["45","    max_bins = 255","53","        X = _BinMapper(n_bins=max_bins + 1).fit_transform(X).astype(np.float32)","97","    max_bins = 255","105","        X = _BinMapper(n_bins=max_bins + 1).fit_transform(X).astype(np.float32)","157","    max_bins = 255","167","        X = _BinMapper(n_bins=max_bins + 1).fit_transform(X).astype(np.float32)"],"delete":["45","    max_bins = 256","53","        X = _BinMapper(max_bins=max_bins).fit_transform(X).astype(np.float32)","97","    max_bins = 256","105","        X = _BinMapper(max_bins=max_bins).fit_transform(X).astype(np.float32)","157","    max_bins = 256","167","        X = _BinMapper(max_bins=max_bins).fit_transform(X).astype(np.float32)"]}],"sklearn\/ensemble\/_hist_gradient_boosting\/splitting.pyx":[{"add":["22","from .common cimport X_BINNED_DTYPE_C","23","from .common cimport Y_DTYPE_C","24","from .common cimport hist_struct","25","from .common import HISTOGRAM_DTYPE","34","    unsigned char missing_go_to_left","54","    missing_go_to_left : bool","55","        Whether missing values should go to the left child.","69","    def __init__(self, gain, feature_idx, bin_idx,","70","                 missing_go_to_left, sum_gradient_left, sum_hessian_left,","71","                 sum_gradient_right, sum_hessian_right, n_samples_left,","72","                 n_samples_right):","76","        self.missing_go_to_left = missing_go_to_left","98","    n_bins_non_missing : ndarray, shape (n_features,)","99","        For each feature, gives the number of bins actually used for","100","        non-missing values.","101","    missing_values_bin_idx : uint8","102","        Index of the bin that is used for missing values. This is the index of","103","        the last bin and is always equal to max_bins (as passed to the GBDT","104","        classes), or equivalently to n_bins - 1.","105","    has_missing_values : ndarray, shape (n_features,)","106","        Whether missing values were observed in the training data, for each","107","        feature.","125","        const unsigned int [::1] n_bins_non_missing","126","        unsigned char missing_values_bin_idx","127","        const unsigned char [::1] has_missing_values","138","    def __init__(self,","139","                 const X_BINNED_DTYPE_C [::1, :] X_binned,","140","                 const unsigned int [::1] n_bins_non_missing,","141","                 const unsigned char missing_values_bin_idx,","142","                 const unsigned char [::1] has_missing_values,","143","                 Y_DTYPE_C l2_regularization,","144","                 Y_DTYPE_C min_hessian_to_split=1e-3,","145","                 unsigned int min_samples_leaf=20,","146","                 Y_DTYPE_C min_gain_to_split=0.,","151","        self.n_bins_non_missing = n_bins_non_missing","152","        self.missing_values_bin_idx = missing_values_bin_idx","153","        self.has_missing_values = has_missing_values","250","            unsigned char missing_go_to_left = split_info.missing_go_to_left","251","            unsigned char missing_values_bin_idx = self.missing_values_bin_idx","276","            unsigned char turn_left","298","                    turn_left = sample_goes_left(","299","                        missing_go_to_left,","300","                        missing_values_bin_idx, bin_idx,","301","                        X_binned[sample_idx])","302","","303","                    if turn_left:","380","            const unsigned char [:] has_missing_values = self.has_missing_values","389","                # Start with a gain of -1 (if no better split is found, that","390","                # means one of the constraints isn't respected","391","                # (min_samples_leaf, etc) and the grower will later turn the","392","                # node into a leaf.","393","                split_infos[feature_idx].gain = -1","394","","395","                # We will scan bins from left to right (in all cases), and if","396","                # there are any missing values, we will also scan bins from","397","                # right to left. This way, we can consider whichever case","398","                # yields the best gain: either missing values go to the right","399","                # (left to right scan) or to the left (right to left case).","400","                # See algo 3 from the XGBoost paper","401","                # https:\/\/arxiv.org\/abs\/1603.02754","402","","403","                self._find_best_bin_to_split_left_to_right(","404","                    feature_idx, has_missing_values[feature_idx],","405","                    histograms, n_samples, sum_gradients, sum_hessians,","406","                    &split_infos[feature_idx])","407","","408","                if has_missing_values[feature_idx]:","409","                    # We need to explore both directions to check whether","410","                    # sending the nans to the left child would lead to a higher","411","                    # gain","412","                    self._find_best_bin_to_split_right_to_left(","413","                        feature_idx, histograms, n_samples,","414","                        sum_gradients, sum_hessians, &split_infos[feature_idx])","425","            split_info.missing_go_to_left,","436","    cdef unsigned int _find_best_feature_to_split_helper(","441","            unsigned int feature_idx","442","            unsigned int best_feature_idx = 0","450","    cdef void _find_best_bin_to_split_left_to_right(","453","            unsigned char has_missing_values,","457","            Y_DTYPE_C sum_hessians,","458","            split_info_struct * split_info) nogil:  # OUT","462","        (min_gain_to_split, etc.) are discarded here.","463","","464","        We scan node from left to right. This version is called whether there","465","        are missing values or not. If any, missing values are assigned to the","466","        right node.","473","            # We set the 'end' variable such that the last non-missing-values","474","            # bin never goes to the left child (which would result in and","475","            # empty right child), unless there are missing values, since these","476","            # would go to the right child.","477","            unsigned int end = \\","478","                self.n_bins_non_missing[feature_idx] - 1 + has_missing_values","489","                                                   sum_hessians,","490","                                                   self.l2_regularization)","492","","493","        for bin_idx in range(end):","524","            if gain > split_info.gain and gain > self.min_gain_to_split:","525","                split_info.gain = gain","526","                split_info.feature_idx = feature_idx","527","                split_info.bin_idx = bin_idx","528","                # we scan from left to right so missing values go to the right","529","                split_info.missing_go_to_left = False","530","                split_info.sum_gradient_left = sum_gradient_left","531","                split_info.sum_gradient_right = sum_gradient_right","532","                split_info.sum_hessian_left = sum_hessian_left","533","                split_info.sum_hessian_right = sum_hessian_right","534","                split_info.n_samples_left = n_samples_left","535","                split_info.n_samples_right = n_samples_right","537","    cdef void _find_best_bin_to_split_right_to_left(","538","            self,","539","            unsigned int feature_idx,","540","            const hist_struct [:, ::1] histograms,  # IN","541","            unsigned int n_samples,","542","            Y_DTYPE_C sum_gradients,","543","            Y_DTYPE_C sum_hessians,","544","            split_info_struct * split_info) nogil:  # OUT","545","        \"\"\"Find best bin to split on for a given feature.","547","        Splits that do not satisfy the splitting constraints","548","        (min_gain_to_split, etc.) are discarded here.","549","","550","        We scan node from right to left. This version is only called when","551","        there are missing values. Missing values are assigned to the left","552","        child.","553","","554","        If no missing value are present in the data this method isn't called","555","        since only calling _find_best_bin_to_split_left_to_right is enough.","556","        \"\"\"","557","","558","        cdef:","559","            unsigned int bin_idx","560","            unsigned int n_samples_left","561","            unsigned int n_samples_right","562","            unsigned int n_samples_ = n_samples","563","            Y_DTYPE_C sum_hessian_left","564","            Y_DTYPE_C sum_hessian_right","565","            Y_DTYPE_C sum_gradient_left","566","            Y_DTYPE_C sum_gradient_right","567","            Y_DTYPE_C negative_loss_current_node","568","            Y_DTYPE_C gain","569","            unsigned int start = self.n_bins_non_missing[feature_idx] - 2","570","","571","        sum_gradient_right, sum_hessian_right = 0., 0.","572","        n_samples_right = 0","573","        negative_loss_current_node = negative_loss(sum_gradients,","574","                                                   sum_hessians,","575","                                                   self.l2_regularization)","576","","577","        for bin_idx in range(start, -1, -1):","578","            n_samples_right += histograms[feature_idx, bin_idx + 1].count","579","            n_samples_left = n_samples_ - n_samples_right","580","","581","            if self.hessians_are_constant:","582","                sum_hessian_right += histograms[feature_idx, bin_idx + 1].count","583","            else:","584","                sum_hessian_right += \\","585","                    histograms[feature_idx, bin_idx + 1].sum_hessians","586","            sum_hessian_left = sum_hessians - sum_hessian_right","587","","588","            sum_gradient_right += \\","589","                histograms[feature_idx, bin_idx + 1].sum_gradients","590","            sum_gradient_left = sum_gradients - sum_gradient_right","591","","592","            if n_samples_right < self.min_samples_leaf:","593","                continue","594","            if n_samples_left < self.min_samples_leaf:","595","                # won't get any better","596","                break","597","","598","            if sum_hessian_right < self.min_hessian_to_split:","599","                continue","600","            if sum_hessian_left < self.min_hessian_to_split:","601","                # won't get any better (hessians are > 0 since loss is convex)","602","                break","603","","604","            gain = _split_gain(sum_gradient_left, sum_hessian_left,","605","                               sum_gradient_right, sum_hessian_right,","606","                               negative_loss_current_node,","607","                               self.l2_regularization)","608","","609","            if gain > split_info.gain and gain > self.min_gain_to_split:","610","                split_info.gain = gain","611","                split_info.feature_idx = feature_idx","612","                split_info.bin_idx = bin_idx","613","                # we scan from right to left so missing values go to the left","614","                split_info.missing_go_to_left = True","615","                split_info.sum_gradient_left = sum_gradient_left","616","                split_info.sum_gradient_right = sum_gradient_right","617","                split_info.sum_hessian_left = sum_hessian_left","618","                split_info.sum_hessian_right = sum_hessian_right","619","                split_info.n_samples_left = n_samples_left","620","                split_info.n_samples_right = n_samples_right","652","","653","cdef inline unsigned char sample_goes_left(","654","        unsigned char missing_go_to_left,","655","        unsigned char missing_values_bin_idx,","656","        X_BINNED_DTYPE_C split_bin_idx,","657","        X_BINNED_DTYPE_C bin_value) nogil:","658","    \"\"\"Helper to decide whether sample should go to left or right child.\"\"\"","659","","660","    return (","661","        (","662","            missing_go_to_left and","663","            bin_value == missing_values_bin_idx","664","        )","665","        or (","666","            bin_value <= split_bin_idx","667","        ))"],"delete":["22","from .types cimport X_BINNED_DTYPE_C","23","from .types cimport Y_DTYPE_C","24","from .types cimport hist_struct","25","from .types import HISTOGRAM_DTYPE","66","    def __init__(self, gain, feature_idx, bin_idx, sum_gradient_left,","67","                 sum_hessian_left, sum_gradient_right, sum_hessian_right,","68","                 n_samples_left, n_samples_right):","93","    actual_n_bins : ndarray, shape (n_features,)","94","        The actual number of bins needed for each feature, which is lower or","95","        equal to max_bins.","113","        unsigned int [::1] actual_n_bins","124","    def __init__(self, const X_BINNED_DTYPE_C [::1, :] X_binned,","125","                 np.ndarray[np.uint32_t] actual_n_bins,","126","                 Y_DTYPE_C l2_regularization, Y_DTYPE_C","127","                 min_hessian_to_split=1e-3, unsigned int","128","                 min_samples_leaf=20, Y_DTYPE_C min_gain_to_split=0.,","133","        self.actual_n_bins = actual_n_bins","275","                    if X_binned[sample_idx] <= bin_idx:","360","                split_info = self._find_best_bin_to_split_helper(","361","                    feature_idx, histograms, n_samples,","362","                    sum_gradients, sum_hessians)","363","                split_infos[feature_idx] = split_info","384","    cdef int _find_best_feature_to_split_helper(","389","            int feature_idx","390","            int best_feature_idx = 0","398","    cdef split_info_struct _find_best_bin_to_split_helper(","404","            Y_DTYPE_C sum_hessians) nogil:","408","        (min_gain_to_split, etc.) are discarded here. If no split can","409","        satisfy the constraints, a SplitInfo with a gain of -1 is returned.","410","        If for a given node the best SplitInfo has a gain of -1, it is","411","        finalized into a leaf in the grower.","424","            split_info_struct best_split","426","        best_split.gain = -1.","430","            sum_hessians, self.l2_regularization)","432","        for bin_idx in range(self.actual_n_bins[feature_idx] - 1):","433","            # Note that considering splitting on the last bin is useless since","434","            # it would result in having 0 samples in the right node (forbidden)","465","            if gain > best_split.gain and gain > self.min_gain_to_split:","466","                best_split.gain = gain","467","                best_split.feature_idx = feature_idx","468","                best_split.bin_idx = bin_idx","469","                best_split.sum_gradient_left = sum_gradient_left","470","                best_split.sum_gradient_right = sum_gradient_right","471","                best_split.sum_hessian_left = sum_hessian_left","472","                best_split.sum_hessian_right = sum_hessian_right","473","                best_split.n_samples_left = n_samples_left","474","                best_split.n_samples_right = n_samples_right","476","        return best_split"]}],"sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_grower.py":[{"add":["6","from sklearn.ensemble._hist_gradient_boosting.common import X_BINNED_DTYPE","7","from sklearn.ensemble._hist_gradient_boosting.common import Y_DTYPE","8","from sklearn.ensemble._hist_gradient_boosting.common import G_H_DTYPE","87","                        n_bins=n_bins, shrinkage=shrinkage,","149","                        n_bins=n_bins, shrinkage=1.,","165","        [128, 254],","169","        [254, 85],","172","        [129, 254],","175","    missing_values_bin_idx = n_bins - 1","176","    predictions = predictor.predict_binned(input_data, missing_values_bin_idx)","181","    predictions = predictor.predict_binned(X_binned, missing_values_bin_idx)","206","    mapper = _BinMapper(n_bins=n_bins)","213","                        n_bins=n_bins, shrinkage=1.,","238","    n_bins = 256","243","    mapper = _BinMapper(n_bins=n_bins)","249","                        n_bins=n_bins, shrinkage=1.,","264","    n_bins = 256","270","    mapper = _BinMapper(n_bins=n_bins)","310","","311","","312","def test_missing_value_predict_only():","313","    # Make sure that missing values are supported at predict time even if they","314","    # were not encountered in the training data: the missing values are","315","    # assigned to whichever child has the most samples.","316","","317","    rng = np.random.RandomState(0)","318","    n_samples = 100","319","    X_binned = rng.randint(0, 256, size=(n_samples, 1), dtype=np.uint8)","320","    X_binned = np.asfortranarray(X_binned)","321","","322","    gradients = rng.normal(size=n_samples).astype(G_H_DTYPE)","323","    hessians = np.ones(shape=1, dtype=G_H_DTYPE)","324","","325","    grower = TreeGrower(X_binned, gradients, hessians, min_samples_leaf=5,","326","                        has_missing_values=False)","327","    grower.grow()","328","","329","    predictor = grower.make_predictor()","330","","331","    # go from root to a leaf, always following node with the most samples.","332","    # That's the path nans are supposed to take","333","    node = predictor.nodes[0]","334","    while not node['is_leaf']:","335","        left = predictor.nodes[node['left']]","336","        right = predictor.nodes[node['right']]","337","        node = left if left['count'] > right['count'] else right","338","","339","    prediction_main_path = node['value']","340","","341","    # now build X_test with only nans, and make sure all predictions are equal","342","    # to prediction_main_path","343","    all_nans = np.full(shape=(n_samples, 1), fill_value=np.nan)","344","    assert np.all(predictor.predict(all_nans) == prediction_main_path)","345","","346","","347","def test_split_on_nan_with_infinite_values():","348","    # Make sure the split on nan situations are respected even when there are","349","    # samples with +inf values (we set the threshold to +inf when we have a","350","    # split on nan so this test makes sure this does not introduce edge-case","351","    # bugs). We need to use the private API so that we can also test","352","    # predict_binned().","353","","354","    X = np.array([0, 1, np.inf, np.nan, np.nan]).reshape(-1, 1)","355","    # the gradient values will force a split on nan situation","356","    gradients = np.array([0, 0, 0, 100, 100], dtype=G_H_DTYPE)","357","    hessians = np.ones(shape=1, dtype=G_H_DTYPE)","358","","359","    bin_mapper = _BinMapper()","360","    X_binned = bin_mapper.fit_transform(X)","361","","362","    n_bins_non_missing = 3","363","    has_missing_values = True","364","    grower = TreeGrower(X_binned, gradients, hessians,","365","                        n_bins_non_missing=n_bins_non_missing,","366","                        has_missing_values=has_missing_values,","367","                        min_samples_leaf=1)","368","","369","    grower.grow()","370","","371","    predictor = grower.make_predictor(","372","        bin_thresholds=bin_mapper.bin_thresholds_","373","    )","374","","375","    # sanity check: this was a split on nan","376","    assert predictor.nodes[0]['threshold'] == np.inf","377","    assert predictor.nodes[0]['bin_threshold'] == n_bins_non_missing - 1","378","","379","    # Make sure in particular that the +inf sample is mapped to the left child","380","    # Note that lightgbm \"fails\" here and will assign the inf sample to the","381","    # right child, even though it's a \"split on nan\" situation.","382","    predictions = predictor.predict(X)","383","    predictions_binned = predictor.predict_binned(","384","        X_binned, missing_values_bin_idx=bin_mapper.missing_values_bin_idx_)","385","    assert np.all(predictions == -gradients)","386","    assert np.all(predictions_binned == -gradients)"],"delete":["6","from sklearn.ensemble._hist_gradient_boosting.types import X_BINNED_DTYPE","7","from sklearn.ensemble._hist_gradient_boosting.types import Y_DTYPE","8","from sklearn.ensemble._hist_gradient_boosting.types import G_H_DTYPE","87","                        max_bins=n_bins, shrinkage=shrinkage,","149","                        max_bins=n_bins, shrinkage=1.,","165","        [128, 255],","169","        [255, 85],","172","        [129, 255],","175","    predictions = predictor.predict_binned(input_data)","180","    predictions = predictor.predict_binned(X_binned)","205","    mapper = _BinMapper(max_bins=n_bins)","212","                        max_bins=n_bins, shrinkage=1.,","237","    max_bins = 255","242","    mapper = _BinMapper(max_bins=max_bins)","248","                        max_bins=max_bins, shrinkage=1.,","263","    max_bins = 255","269","    mapper = _BinMapper(max_bins=max_bins)"]}],"sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_loss.py":[{"add":["9","from sklearn.ensemble._hist_gradient_boosting.common import Y_DTYPE","10","from sklearn.ensemble._hist_gradient_boosting.common import G_H_DTYPE"],"delete":["9","from sklearn.ensemble._hist_gradient_boosting.types import Y_DTYPE","10","from sklearn.ensemble._hist_gradient_boosting.types import G_H_DTYPE"]}],"sklearn\/ensemble\/_hist_gradient_boosting\/common.pxd":[{"add":[],"delete":[]}],"sklearn\/ensemble\/_hist_gradient_boosting\/predictor.py":[{"add":["7","from .common import Y_DTYPE","49","    def predict_binned(self, X, missing_values_bin_idx):","56","        missing_values_bin_idx : uint8","57","            Index of the bin that is used for missing values. This is the","58","            index of the last bin and is always equal to max_bins (as passed","59","            to the GBDT classes), or equivalently to n_bins - 1.","67","        _predict_from_binned_data(self.nodes, X, missing_values_bin_idx, out)"],"delete":["7","from .types import Y_DTYPE","49","    def predict_binned(self, X):","63","        _predict_from_binned_data(self.nodes, X, out)"]}],"sklearn\/ensemble\/_hist_gradient_boosting\/binning.py":[{"add":["15","from .common import X_DTYPE, X_BINNED_DTYPE, ALMOST_INF","21","    Missing values are ignored for finding the thresholds.","22","","27","    max_bins: int","28","        The maximum number of bins to use for non-missing values. If for a","29","        given feature the number of unique values is less than ``max_bins``,","30","        then those unique values will be used to compute the bin thresholds,","31","        instead of the quantiles.","54","        col_data = data[:, f_idx]","55","        # ignore missing values when computing bin thresholds","56","        missing_mask = np.isnan(col_data)","57","        if missing_mask.any():","58","            col_data = col_data[~missing_mask]","59","        col_data = np.ascontiguousarray(col_data, dtype=X_DTYPE)","70","            percentiles = np.linspace(0, 100, num=max_bins + 1)","71","            percentiles = percentiles[1:-1]","74","            assert midpoints.shape[0] == max_bins - 1","75","","76","        # We avoid having +inf thresholds: +inf thresholds are only allowed in","77","        # a \"split on nan\" situation.","78","        np.clip(midpoints, a_min=None, a_max=ALMOST_INF, out=midpoints)","79","","81","","94","    Features with a small number of values may be binned into less than","95","    ``n_bins`` bins. The last bin (at index ``n_bins - 1``) is always reserved","96","    for missing values.","100","    n_bins : int, optional (default=256)","101","        The maximum number of bins to use (including the bin for missing","102","        values). Non-missing values are binned on ``max_bins = n_bins - 1``","103","        bins. The last bin is always reserved for missing values. If for a","104","        given feature the number of unique values is less than ``max_bins``,","105","        then those unique values will be used to compute the bin thresholds,","106","        instead of the quantiles.","115","","116","    Attributes","117","    ----------","118","    bin_thresholds_ : list of arrays","119","        For each feature, gives the real-valued bin threhsolds. There are","120","        ``max_bins - 1`` thresholds, where ``max_bins = n_bins - 1`` is the","121","        number of bins used for non-missing values.","122","    n_bins_non_missing_ : array of uint32","123","        For each feature, gives the number of bins actually used for","124","        non-missing values. For features with a lot of unique values, this is","125","        equal to ``n_bins - 1``.","126","    missing_values_bin_idx_ : uint8","127","        The index of the bin where missing values are mapped. This is a","128","        constant accross all features. This corresponds to the last bin, and","129","        it is always equal to ``n_bins - 1``. Note that if ``n_bins_missing_``","130","        is less than ``n_bins - 1`` for a given feature, then there are","131","        empty (and unused) bins.","133","    def __init__(self, n_bins=256, subsample=int(2e5), random_state=None):","134","        self.n_bins = n_bins","141","        The last bin is reserved for missing values, whether missing values","142","        are present in the data or not.","143","","155","        if not (3 <= self.n_bins <= 256):","156","            # min is 3: at least 2 distinct bins and a missing values bin","157","            raise ValueError('n_bins={} should be no smaller than 3 '","158","                             'and no larger than 256.'.format(self.n_bins))","159","","161","        max_bins = self.n_bins - 1","163","            X, max_bins, subsample=self.subsample,","166","        self.n_bins_non_missing_ = np.array(","170","        self.missing_values_bin_idx_ = self.n_bins - 1","171","","177","        Missing values will be mapped to the last bin.","178","","191","        if X.shape[1] != self.n_bins_non_missing_.shape[0]:","194","                'to transform()'.format(self.n_bins_non_missing_.shape[0],","198","        _map_to_bins(X, self.bin_thresholds_, self.missing_values_bin_idx_,","199","                     binned)"],"delete":["15","from .types import X_DTYPE, X_BINNED_DTYPE","25","    max_bins : int","26","        The maximum number of bins to use. If for a given feature the number of","27","        unique values is less than ``max_bins``, then those unique values","28","        will be used to compute the bin thresholds, instead of the quantiles.","44","    if not (2 <= max_bins <= 256):","45","        raise ValueError('max_bins={} should be no smaller than 2 '","46","                         'and no larger than 256.'.format(max_bins))","52","    percentiles = np.linspace(0, 100, num=max_bins + 1)","53","    percentiles = percentiles[1:-1]","56","        col_data = np.ascontiguousarray(data[:, f_idx], dtype=X_DTYPE)","82","    If the number of unique values for a given feature is less than","83","    ``max_bins``, then the unique values of this feature are used instead of","84","    the quantiles.","88","    max_bins : int, optional (default=256)","89","        The maximum number of bins to use. If for a given feature the number of","90","        unique values is less than ``max_bins``, then those unique values","91","        will be used to compute the bin thresholds, instead of the quantiles.","101","    def __init__(self, max_bins=256, subsample=int(2e5), random_state=None):","102","        self.max_bins = max_bins","122","            X, self.max_bins, subsample=self.subsample,","125","        self.actual_n_bins_ = np.array(","146","        if X.shape[1] != self.actual_n_bins_.shape[0]:","149","                'to transform()'.format(self.actual_n_bins_.shape[0],","153","        _map_to_bins(X, self.bin_thresholds_, binned)"]}],"sklearn\/ensemble\/_hist_gradient_boosting\/grower.py":[{"add":["17","from .common import PREDICTOR_RECORD_DTYPE","18","from .common import Y_DTYPE","143","    n_bins : int, optional (default=256)","144","        The total number of bins, including the bin for missing values. Used","145","        to define the shape of the histograms.","146","    n_bins_non_missing_ : array of uint32","147","        For each feature, gives the number of bins actually used for","148","        non-missing values. For features with a lot of unique values, this","149","        is equal to ``n_bins - 1``. If it's an int, all features are","150","        considered to have the same number of bins. If None, all features","151","        are considered to have ``n_bins - 1`` bins.","152","    has_missing_values : ndarray of bool or bool, optional (default=False)","153","        Whether each feature contains missing values (in the training data).","154","        If it's a bool, the same value is used for all features.","167","                 n_bins=256, n_bins_non_missing=None, has_missing_values=False,","168","                 l2_regularization=0., min_hessian_to_split=1e-3,","169","                 shrinkage=1.):","175","        if n_bins_non_missing is None:","176","            n_bins_non_missing = n_bins - 1","178","        if isinstance(n_bins_non_missing, numbers.Integral):","179","            n_bins_non_missing = np.array(","180","                [n_bins_non_missing] * X_binned.shape[1],","183","            n_bins_non_missing = np.asarray(n_bins_non_missing,","184","                                            dtype=np.uint32)","185","","186","        if isinstance(has_missing_values, bool):","187","            has_missing_values = [has_missing_values] * X_binned.shape[1]","188","        has_missing_values = np.asarray(has_missing_values, dtype=np.uint8)","192","            X_binned, n_bins, gradients, hessians, hessians_are_constant)","193","        missing_values_bin_idx = n_bins - 1","195","            X_binned, n_bins_non_missing, missing_values_bin_idx,","196","            has_missing_values, l2_regularization, min_hessian_to_split,","197","            min_samples_leaf, min_gain_to_split, hessians_are_constant)","198","        self.n_bins_non_missing = n_bins_non_missing","200","        self.has_missing_values = has_missing_values","347","        if not self.has_missing_values[node.split_info.feature_idx]:","348","            # If no missing values are encountered at fit time, then samples","349","            # with missing values during predict() will go to whichever child","350","            # has the most samples.","351","            node.split_info.missing_go_to_left = (","352","                left_child_node.n_samples > right_child_node.n_samples)","353","","449","                                   bin_thresholds, self.n_bins_non_missing)","454","                               bin_thresholds, n_bins_non_missing,","455","                               next_free_idx=0):","476","        node['missing_go_to_left'] = split_info.missing_go_to_left","477","","478","        if split_info.bin_idx == n_bins_non_missing[feature_idx] - 1:","479","            # Split is on the last non-missing bin: it's a \"split on nans\". All","480","            # nans go to the right, the rest go to the left.","481","            node['threshold'] = np.inf","482","        elif bin_thresholds is not None:","483","            node['threshold'] = bin_thresholds[feature_idx][bin_idx]","484","","490","            bin_thresholds=bin_thresholds,","491","            n_bins_non_missing=n_bins_non_missing,","492","            next_free_idx=next_free_idx)","497","            bin_thresholds=bin_thresholds,","498","            n_bins_non_missing=n_bins_non_missing,","499","            next_free_idx=next_free_idx)"],"delete":["17","from .types import PREDICTOR_RECORD_DTYPE","18","from .types import Y_DTYPE","143","    max_bins : int, optional (default=256)","144","        The maximum number of bins. Used to define the shape of the","145","        histograms.","146","    actual_n_bins : ndarray of int or int, optional (default=None)","147","        The actual number of bins needed for each feature, which is lower or","148","        equal to ``max_bins``. If it's an int, all features are considered to","149","        have the same number of bins. If None, all features are considered to","150","        have ``max_bins`` bins.","163","                 max_bins=256, actual_n_bins=None, l2_regularization=0.,","164","                 min_hessian_to_split=1e-3, shrinkage=1.):","170","        if actual_n_bins is None:","171","            actual_n_bins = max_bins","173","        if isinstance(actual_n_bins, numbers.Integral):","174","            actual_n_bins = np.array(","175","                [actual_n_bins] * X_binned.shape[1],","178","            actual_n_bins = np.asarray(actual_n_bins, dtype=np.uint32)","182","            X_binned, max_bins, gradients, hessians, hessians_are_constant)","184","            X_binned, actual_n_bins, l2_regularization,","185","            min_hessian_to_split, min_samples_leaf, min_gain_to_split,","186","            hessians_are_constant)","188","        self.max_bins = max_bins","430","                                   bin_thresholds=bin_thresholds)","435","                               bin_thresholds, next_free_idx=0):","456","        if bin_thresholds is not None:","457","            threshold = bin_thresholds[feature_idx][bin_idx]","458","            node['threshold'] = threshold","464","            bin_thresholds=bin_thresholds, next_free_idx=next_free_idx)","469","            bin_thresholds=bin_thresholds, next_free_idx=next_free_idx)"]}],"sklearn\/ensemble\/_hist_gradient_boosting\/histogram.pyx":[{"add":["14","from .common import HISTOGRAM_DTYPE","15","from .common cimport hist_struct","16","from .common cimport X_BINNED_DTYPE_C","17","from .common cimport G_H_DTYPE_C","64","    n_bins : int","65","        The total number of bins, including the bin for missing values. Used","66","        to define the shape of the histograms.","79","        unsigned int n_bins","87","                 unsigned int n_bins, G_H_DTYPE_C [::1] gradients,","93","        # Note: all histograms will have <n_bins> bins, but some of the","94","        # bins may be unused if a feature has a small number of unique values.","95","        self.n_bins = n_bins","117","        histograms : ndarray of HISTOGRAM_DTYPE, shape (n_features, n_bins)","133","                shape=(self.n_features, self.n_bins),","212","                shape (n_features, n_bins)","215","                shape (n_features, n_bins)","220","        histograms : ndarray of HISTOGRAM_DTYPE, shape(n_features, n_bins)","228","                shape=(self.n_features, self.n_bins),","235","                                 self.n_bins,"],"delete":["14","from .types import HISTOGRAM_DTYPE","15","from .types cimport hist_struct","16","from .types cimport X_BINNED_DTYPE_C","17","from .types cimport G_H_DTYPE_C","64","    max_bins : int","65","        The maximum number of bins. Used to define the shape of the","66","        histograms.","79","        unsigned int max_bins","87","                 unsigned int max_bins, G_H_DTYPE_C [::1] gradients,","93","        # Note: all histograms will have <max_bins> bins, but some of the","94","        # last bins may be unused if actual_n_bins[f] < max_bins","95","        self.max_bins = max_bins","117","        histograms : ndarray of HISTOGRAM_DTYPE, shape (n_features, max_bins)","133","                shape=(self.n_features, self.max_bins),","212","                shape (n_features, max_bins)","215","                shape (n_features, max_bins)","220","        histograms : ndarray of HISTOGRAM_DTYPE, shape(n_features, max_bins)","228","                shape=(self.n_features, self.max_bins),","235","                                 self.max_bins,"]}],"sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_histogram.py":[{"add":["14","from sklearn.ensemble._hist_gradient_boosting.common import HISTOGRAM_DTYPE","15","from sklearn.ensemble._hist_gradient_boosting.common import G_H_DTYPE","16","from sklearn.ensemble._hist_gradient_boosting.common import X_BINNED_DTYPE"],"delete":["14","from sklearn.ensemble._hist_gradient_boosting.types import HISTOGRAM_DTYPE","15","from sklearn.ensemble._hist_gradient_boosting.types import G_H_DTYPE","16","from sklearn.ensemble._hist_gradient_boosting.types import X_BINNED_DTYPE"]}],"sklearn\/ensemble\/_hist_gradient_boosting\/gradient_boosting.py":[{"add":["4","from functools import partial","17","from .common import Y_DTYPE, X_DTYPE, X_BINNED_DTYPE","78","        if not (2 <= self.max_bins <= 255):","79","            raise ValueError('max_bins={} should be no smaller than 2 '","80","                             'and no larger than 255.'.format(self.max_bins))","81","","150","        has_missing_values = np.isnan(X_train).any(axis=0).astype(np.uint8)","151","","153","        # For ease of use of the API, the user-facing GBDT classes accept the","154","        # parameter max_bins, which doesn't take into account the bin for","155","        # missing values (which is always allocated). However, since max_bins","156","        # isn't the true maximal number of bins, all other private classes","157","        # (binmapper, histbuilder...) accept n_bins instead, which is the","158","        # actual total number of bins. Everywhere in the code, the","159","        # convention is that n_bins == max_bins + 1","160","        n_bins = self.max_bins + 1  # + 1 for missing values","161","        self.bin_mapper_ = _BinMapper(n_bins=n_bins, random_state=rng)","310","                    n_bins=n_bins,","311","                    n_bins_non_missing=self.bin_mapper_.n_bins_non_missing_,","312","                    has_missing_values=has_missing_values,","343","                                pred.predict_binned(","344","                                    X_binned_val,","345","                                    self.bin_mapper_.missing_values_bin_idx_","346","                                )","347","                            )","578","                if is_binned:","579","                    predict = partial(","580","                        predictor.predict_binned,","581","                        missing_values_bin_idx=self.bin_mapper_.missing_values_bin_idx_  # noqa","582","                    )","583","                else:","584","                    predict = predictor.predict","620","    def _more_tags(self):","621","        return {'allow_nan': True}","622","","644","    This estimator has native support for missing values (NaNs). During","645","    training, the tree grower learns at each split point whether samples","646","    with missing values should go to the left or right child, based on the","647","    potential gain. When predicting, samples with missing values are","648","    assigned to the left or right child consequently. If no missing values","649","    were encountered for a given feature during training, then samples with","650","    missing values are mapped to whichever child has the most samples.","651","","695","    max_bins : int, optional (default=255)","696","        The maximum number of bins to use for non-missing values. Before","697","        training, each feature of the input array `X` is binned into","698","        integer-valued bins, which allows for a much faster training stage.","699","        Features with a small number of unique values may use less than","700","        ``max_bins`` bins. In addition to the ``max_bins`` bins, one more bin","701","        is always reserved for missing values. Must be no larger than 255.","772","                 min_samples_leaf=20, l2_regularization=0., max_bins=255,","821","    This estimator has native support for missing values (NaNs). During","822","    training, the tree grower learns at each split point whether samples","823","    with missing values should go to the left or right child, based on the","824","    potential gain. When predicting, samples with missing values are","825","    assigned to the left or right child consequently. If no missing values","826","    were encountered for a given feature during training, then samples with","827","    missing values are mapped to whichever child has the most samples.","828","","875","    max_bins : int, optional (default=255)","876","        The maximum number of bins to use for non-missing values. Before","877","        training, each feature of the input array `X` is binned into","878","        integer-valued bins, which allows for a much faster training stage.","879","        Features with a small number of unique values may use less than","880","        ``max_bins`` bins. In addition to the ``max_bins`` bins, one more bin","881","        is always reserved for missing values. Must be no larger than 255.","954","                 l2_regularization=0., max_bins=255, warm_start=False,"],"delete":["16","from .types import Y_DTYPE, X_DTYPE, X_BINNED_DTYPE","146","        self.bin_mapper_ = _BinMapper(max_bins=self.max_bins, random_state=rng)","295","                    max_bins=self.max_bins,","296","                    actual_n_bins=self.bin_mapper_.actual_n_bins_,","327","                                pred.predict_binned(X_binned_val))","558","                predict = (predictor.predict_binned if is_binned","559","                           else predictor.predict)","608","    def _more_tags(self):","609","        # This is not strictly True, but it's needed since","610","        # force_all_finite=False means accept both nans and infinite values.","611","        # Without the tag, common checks would fail.","612","        # This comment must be removed once we merge PR 13911","613","        return {'allow_nan': True}","614","","666","    max_bins : int, optional (default=256)","667","        The maximum number of bins to use. Before training, each feature of","668","        the input array ``X`` is binned into at most ``max_bins`` bins, which","669","        allows for a much faster training stage. Features with a small","670","        number of unique values may use less than ``max_bins`` bins. Must be no","671","        larger than 256.","742","                 min_samples_leaf=20, l2_regularization=0., max_bins=256,","837","    max_bins : int, optional (default=256)","838","        The maximum number of bins to use. Before training, each feature of","839","        the input array ``X`` is binned into at most ``max_bins`` bins, which","840","        allows for a much faster training stage. Features with a small","841","        number of unique values may use less than ``max_bins`` bins. Must be no","842","        larger than 256.","915","                 l2_regularization=0., max_bins=256, warm_start=False,"]}],"sklearn\/ensemble\/_hist_gradient_boosting\/loss.py":[{"add":["17","from .common import Y_DTYPE","18","from .common import G_H_DTYPE"],"delete":["17","from .types import Y_DTYPE","18","from .types import G_H_DTYPE"]}],"sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_predictor.py":[{"add":["9","from sklearn.ensemble._hist_gradient_boosting.common import (","10","    G_H_DTYPE, PREDICTOR_RECORD_DTYPE, ALMOST_INF)","13","@pytest.mark.parametrize('n_bins', [200, 256])","14","def test_boston_dataset(n_bins):","19","    mapper = _BinMapper(n_bins=n_bins, random_state=42)","30","                        max_leaf_nodes=max_leaf_nodes, n_bins=n_bins,","31","                        n_bins_non_missing=mapper.n_bins_non_missing_)","44","    (ALMOST_INF, [0, 0, 0, 1]),","45","    (np.inf, [0, 0, 0, 0]),","49","    # In particular, if a value is +inf and the threshold is ALMOST_INF the","50","    # sample should go to the right child. If the threshold is inf (split on","51","    # nan), the +inf sample will go to the left child."],"delete":["9","from sklearn.ensemble._hist_gradient_boosting.types import (","10","    G_H_DTYPE, PREDICTOR_RECORD_DTYPE)","13","@pytest.mark.parametrize('max_bins', [200, 256])","14","def test_boston_dataset(max_bins):","19","    mapper = _BinMapper(max_bins=max_bins, random_state=42)","30","                        max_leaf_nodes=max_leaf_nodes, max_bins=max_bins,","31","                        actual_n_bins=mapper.actual_n_bins_)","44","    (np.inf, [0, 0, 0, 1]),","48","    # In paticular, if a value is +inf and the threhsold is +inf, the sample","49","    # should go to the right child."]}],"sklearn\/ensemble\/_hist_gradient_boosting\/_gradient_boosting.pyx":[{"add":["12","from .common import Y_DTYPE","13","from .common cimport Y_DTYPE_C"],"delete":["12","from .types import Y_DTYPE","13","from .types cimport Y_DTYPE_C"]}],"\/dev\/null":[{"add":[],"delete":[]}],"sklearn\/ensemble\/_hist_gradient_boosting\/_loss.pyx":[{"add":["14","from .common cimport Y_DTYPE_C","15","from .common cimport G_H_DTYPE_C"],"delete":["14","from .types cimport Y_DTYPE_C","15","from .types cimport G_H_DTYPE_C"]}],"benchmarks\/bench_hist_gradient_boosting.py":[{"add":["4","import numpy as np","28","parser.add_argument('--missing-fraction', type=float, default=0)","56","if args.missing_fraction:","57","    mask = np.random.binomial(1, args.missing_fraction, size=X.shape).astype(","58","        np.bool)","59","    X[mask] = np.nan","60",""],"delete":[]}],"sklearn\/ensemble\/_hist_gradient_boosting\/utils.pyx":[{"add":["11","from .common cimport G_H_DTYPE_C","12","from .common cimport Y_DTYPE_C"],"delete":["11","from .types cimport G_H_DTYPE_C","12","from .types cimport Y_DTYPE_C"]}],"sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_binning.py":[{"add":["9","from sklearn.ensemble._hist_gradient_boosting.common import X_DTYPE","10","from sklearn.ensemble._hist_gradient_boosting.common import X_BINNED_DTYPE","11","from sklearn.ensemble._hist_gradient_boosting.common import ALMOST_INF","19","def _find_binning_thresholds(data, max_bins=255, subsample=int(2e5),","55","    bin_thresholds = _find_binning_thresholds(DATA, max_bins=255,","56","                                              random_state=0)","59","        assert bin_thresholds[i].shape == (254,)  # 255 - 1","78","@pytest.mark.parametrize('n_bins', (2, 257))","79","def test_invalid_n_bins(n_bins):","80","    err_msg = (","81","        'n_bins={} should be no smaller than 3 and no larger than 256'","82","        .format(n_bins))","84","        _BinMapper(n_bins=n_bins).fit(DATA)","88","    mapper = _BinMapper(n_bins=42, random_state=42).fit(DATA)","94","@pytest.mark.parametrize('max_bins', [16, 128, 255])","95","def test_map_to_bins(max_bins):","96","    bin_thresholds = _find_binning_thresholds(DATA, max_bins=max_bins,","99","    last_bin_idx = max_bins","100","    _map_to_bins(DATA, bin_thresholds, last_bin_idx, binned)","111","        assert binned[max_idx, feature_idx] == max_bins - 1","114","@pytest.mark.parametrize(\"max_bins\", [5, 10, 42])","115","def test_bin_mapper_random_data(max_bins):","118","    expected_count_per_bin = n_samples \/\/ max_bins","121","    # max_bins is the number of bins for non-missing values","122","    n_bins = max_bins + 1","123","    mapper = _BinMapper(n_bins=n_bins, random_state=42).fit(DATA)","129","    assert_array_equal(binned.max(axis=0),","130","                       np.array([max_bins - 1, max_bins - 1]))","133","        assert bin_thresholds_feature.shape == (max_bins - 1,)","135","    assert np.all(mapper.n_bins_non_missing_ == max_bins)","139","        for bin_idx in range(max_bins):","144","@pytest.mark.parametrize(\"n_samples, max_bins\", [","150","def test_bin_mapper_small_random_data(n_samples, max_bins):","154","    # max_bins is the number of bins for non-missing values","155","    n_bins = max_bins + 1","156","    mapper = _BinMapper(n_bins=n_bins, random_state=42)","165","@pytest.mark.parametrize(\"max_bins, n_distinct, multiplier\", [","170","def test_bin_mapper_identity_repeated_values(max_bins, n_distinct, multiplier):","172","    # max_bins is the number of bins for non-missing values","173","    n_bins = max_bins + 1","174","    binned = _BinMapper(n_bins=n_bins).fit_transform(data)","191","    mapper_1 = _BinMapper(n_bins=n_distinct + 1)","196","    mapper_2 = _BinMapper(n_bins=min(256, n_distinct * 3) + 1)","203","@pytest.mark.parametrize(\"max_bins, scale, offset\", [","206","    (255, 0.3, 42),","208","def test_bin_mapper_identity_small(max_bins, scale, offset):","209","    data = np.arange(max_bins).reshape(-1, 1) * scale + offset","210","    # max_bins is the number of bins for non-missing values","211","    n_bins = max_bins + 1","212","    binned = _BinMapper(n_bins=n_bins).fit_transform(data)","213","    assert_array_equal(binned, np.arange(max_bins).reshape(-1, 1))","216","@pytest.mark.parametrize('max_bins_small, max_bins_large', [","221","    (255, 255),","223","    (42, 255),","225","def test_bin_mapper_idempotence(max_bins_small, max_bins_large):","226","    assert max_bins_large >= max_bins_small","228","    mapper_small = _BinMapper(n_bins=max_bins_small + 1)","229","    mapper_large = _BinMapper(n_bins=max_bins_small + 1)","235","@pytest.mark.parametrize('n_bins', [10, 100, 256])","237","def test_n_bins_non_missing(n_bins, diff):","238","    # Check that n_bins_non_missing is n_unique_values when","239","    # there are not a lot of unique values, else n_bins - 1.","241","    n_unique_values = n_bins + diff","244","    mapper = _BinMapper(n_bins=n_bins).fit(X)","245","    assert np.all(mapper.n_bins_non_missing_ == min(","246","        n_bins - 1, n_unique_values))","260","@pytest.mark.parametrize(","261","    'n_bins, n_bins_non_missing, X_trans_expected', [","262","        (256, [4, 2, 2], [[0,   0,   0],  # 255 <=> missing value","263","                          [255, 255, 0],","264","                          [1,   0,   0],","265","                          [255, 1,   1],","266","                          [2,   1,   1],","267","                          [3,   0,   0]]),","268","        (3, [2, 2, 2], [[0, 0, 0],  # 2 <=> missing value","269","                        [2, 2, 0],","270","                        [0, 0, 0],","271","                        [2, 1, 1],","272","                        [1, 1, 1],","273","                        [1, 0, 0]])])","274","def test_missing_values_support(n_bins, n_bins_non_missing, X_trans_expected):","275","    # check for missing values: make sure nans are mapped to the last bin","276","    # and that the _BinMapper attributes are correct","277","","278","    X = [[1,      1,      0],","279","         [np.NaN, np.NaN, 0],","280","         [2,      1,      0],","281","         [np.NaN, 2,      1],","282","         [3,      2,      1],","283","         [4,      1,      0]]","284","","285","    X = np.array(X)","286","","287","    mapper = _BinMapper(n_bins=n_bins)","288","    mapper.fit(X)","289","","290","    assert_array_equal(mapper.n_bins_non_missing_, n_bins_non_missing)","291","","292","    for feature_idx in range(X.shape[1]):","293","        assert len(mapper.bin_thresholds_[feature_idx]) == \\","294","            n_bins_non_missing[feature_idx] - 1","295","","296","    assert mapper.missing_values_bin_idx_ == n_bins - 1","297","","298","    X_trans = mapper.transform(X)","299","    assert_array_equal(X_trans, X_trans_expected)","300","","301","","309","    assert_allclose(bin_mapper.bin_thresholds_[0], [-np.inf, .5, ALMOST_INF])","310","    assert bin_mapper.n_bins_non_missing_ == [4]"],"delete":["9","from sklearn.ensemble._hist_gradient_boosting.types import X_DTYPE","10","from sklearn.ensemble._hist_gradient_boosting.types import X_BINNED_DTYPE","18","def _find_binning_thresholds(data, max_bins=256, subsample=int(2e5),","54","    bin_thresholds = _find_binning_thresholds(DATA, random_state=0)","57","        assert bin_thresholds[i].shape == (255,)  # 256 - 1","76","def test_find_binning_thresholds_invalid_n_bins():","77","    err_msg = 'no smaller than 2 and no larger than 256'","79","        _find_binning_thresholds(DATA, max_bins=1024)","83","    mapper = _BinMapper(max_bins=42, random_state=42).fit(DATA)","89","@pytest.mark.parametrize('n_bins', [16, 128, 256])","90","def test_map_to_bins(n_bins):","91","    bin_thresholds = _find_binning_thresholds(DATA, max_bins=n_bins,","94","    _map_to_bins(DATA, bin_thresholds, binned)","105","        assert binned[max_idx, feature_idx] == n_bins - 1","108","@pytest.mark.parametrize(\"n_bins\", [5, 10, 42])","109","def test_bin_mapper_random_data(n_bins):","112","    expected_count_per_bin = n_samples \/\/ n_bins","115","    mapper = _BinMapper(max_bins=n_bins, random_state=42).fit(DATA)","121","    assert_array_equal(binned.max(axis=0), np.array([n_bins - 1, n_bins - 1]))","124","        assert bin_thresholds_feature.shape == (n_bins - 1,)","126","    assert np.all(mapper.actual_n_bins_ == n_bins)","130","        for bin_idx in range(n_bins):","135","@pytest.mark.parametrize(\"n_samples, n_bins\", [","141","def test_bin_mapper_small_random_data(n_samples, n_bins):","145","    mapper = _BinMapper(max_bins=n_bins, random_state=42)","154","@pytest.mark.parametrize(\"n_bins, n_distinct, multiplier\", [","159","def test_bin_mapper_identity_repeated_values(n_bins, n_distinct, multiplier):","161","    binned = _BinMapper(max_bins=n_bins).fit_transform(data)","178","    mapper_1 = _BinMapper(max_bins=n_distinct)","183","    mapper_2 = _BinMapper(max_bins=min(256, n_distinct * 3))","190","@pytest.mark.parametrize(\"n_bins, scale, offset\", [","193","    (256, 0.3, 42),","195","def test_bin_mapper_identity_small(n_bins, scale, offset):","196","    data = np.arange(n_bins).reshape(-1, 1) * scale + offset","197","    binned = _BinMapper(max_bins=n_bins).fit_transform(data)","198","    assert_array_equal(binned, np.arange(n_bins).reshape(-1, 1))","201","@pytest.mark.parametrize('n_bins_small, n_bins_large', [","206","    (256, 256),","208","    (42, 256),","210","def test_bin_mapper_idempotence(n_bins_small, n_bins_large):","211","    assert n_bins_large >= n_bins_small","213","    mapper_small = _BinMapper(max_bins=n_bins_small)","214","    mapper_large = _BinMapper(max_bins=n_bins_large)","220","@pytest.mark.parametrize('max_bins', [10, 100, 256])","222","def test_actual_n_bins(max_bins, diff):","223","    # Check that actual_n_bins is n_unique_values when","224","    # n_unique_values <= max_bins, else max_bins.","226","    n_unique_values = max_bins + diff","229","    mapper = _BinMapper(max_bins=max_bins).fit(X)","230","    assert np.all(mapper.actual_n_bins_ == min(max_bins, n_unique_values))","251","    assert_allclose(bin_mapper.bin_thresholds_[0], [-np.inf, .5, np.inf])","252","    assert bin_mapper.actual_n_bins_ == [4]"]}],"sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_splitting.py":[{"add":["3","from sklearn.ensemble._hist_gradient_boosting.common import HISTOGRAM_DTYPE","4","from sklearn.ensemble._hist_gradient_boosting.common import G_H_DTYPE","5","from sklearn.ensemble._hist_gradient_boosting.common import X_BINNED_DTYPE","20","        rng.randint(0, n_bins - 1, size=(int(1e4), 1)), dtype=X_BINNED_DTYPE)","28","    for true_bin in range(1, n_bins - 2):","41","            n_bins_non_missing = np.array([n_bins - 1] * X_binned.shape[1],","42","                                          dtype=np.uint32)","43","            has_missing_values = np.array([False] * X_binned.shape[1],","44","                                          dtype=np.uint8)","45","            missing_values_bin_idx = n_bins - 1","47","                                n_bins_non_missing,","48","                                missing_values_bin_idx,","49","                                has_missing_values,","105","    n_bins_non_missing = np.array([n_bins - 1] * X_binned.shape[1],","106","                                  dtype=np.uint32)","107","    has_missing_values = np.array([False] * X_binned.shape[1], dtype=np.uint8)","108","    missing_values_bin_idx = n_bins - 1","109","    splitter = Splitter(X_binned, n_bins_non_missing, missing_values_bin_idx,","110","                        has_missing_values, l2_regularization,","111","                        min_hessian_to_split, min_samples_leaf,","112","                        min_gain_to_split, constant_hessian)","205","    n_bins_non_missing = np.array([n_bins] * X_binned.shape[1],","206","                                  dtype=np.uint32)","207","    has_missing_values = np.array([False] * X_binned.shape[1], dtype=np.uint8)","208","    missing_values_bin_idx = n_bins - 1","209","    splitter = Splitter(X_binned, n_bins_non_missing, missing_values_bin_idx,","210","                        has_missing_values, l2_regularization,","211","                        min_hessian_to_split, min_samples_leaf,","212","                        min_gain_to_split, hessians_are_constant)","262","    n_bins_non_missing = np.array([n_bins - 1] * X_binned.shape[1],","263","                                  dtype=np.uint32)","264","    has_missing_values = np.array([False] * X_binned.shape[1], dtype=np.uint8)","265","    missing_values_bin_idx = n_bins - 1","266","    splitter = Splitter(X_binned, n_bins_non_missing, missing_values_bin_idx,","267","                        has_missing_values, l2_regularization,","268","                        min_hessian_to_split, min_samples_leaf,","269","                        min_gain_to_split, hessians_are_constant)","270","","271","    histograms = builder.compute_histograms_brute(sample_indices)","272","    split_info = splitter.find_node_split(n_samples, histograms,","273","                                          sum_gradients, sum_hessians)","274","    assert split_info.gain == -1","275","","276","","277","@pytest.mark.parametrize(","278","    'X_binned, all_gradients, has_missing_values, n_bins_non_missing, '","279","    ' expected_split_on_nan, expected_bin_idx, expected_go_to_left', [","280","","281","        # basic sanity check with no missing values: given the gradient","282","        # values, the split must occur on bin_idx=3","283","        ([0, 1, 2, 3, 4, 5, 6, 7, 8, 9],  # X_binned","284","         [1, 1, 1, 1, 5, 5, 5, 5, 5, 5],  # gradients","285","         False,  # no missing values","286","         10,  # n_bins_non_missing","287","         False,  # don't split on nans","288","         3,  # expected_bin_idx","289","         'not_applicable'),","290","","291","        # We replace 2 samples by NaNs (bin_idx=8)","292","        # These 2 samples were mapped to the left node before, so they should","293","        # be mapped to left node again","294","        # Notice how the bin_idx threshold changes from 3 to 1.","295","        ([8, 0, 1, 8, 2, 3, 4, 5, 6, 7],  # 8 <=> missing","296","         [1, 1, 1, 1, 5, 5, 5, 5, 5, 5],","297","         True,  # missing values","298","         8,  # n_bins_non_missing","299","         False,  # don't split on nans","300","         1,  # cut on bin_idx=1","301","         True),  # missing values go to left","302","","303","        # same as above, but with non-consecutive missing_values_bin","304","        ([9, 0, 1, 9, 2, 3, 4, 5, 6, 7],  # 9 <=> missing","305","         [1, 1, 1, 1, 5, 5, 5, 5, 5, 5],","306","         True,  # missing values","307","         8,  # n_bins_non_missing","308","         False,  # don't split on nans","309","         1,  # cut on bin_idx=1","310","         True),  # missing values go to left","311","","312","        # this time replacing 2 samples that were on the right.","313","        ([0, 1, 2, 3, 8, 4, 8, 5, 6, 7],  # 8 <=> missing","314","         [1, 1, 1, 1, 5, 5, 5, 5, 5, 5],","315","         True,  # missing values","316","         8,  # n_bins_non_missing","317","         False,  # don't split on nans","318","         3,  # cut on bin_idx=3 (like in first case)","319","         False),  # missing values go to right","320","","321","        # same as above, but with non-consecutive missing_values_bin","322","        ([0, 1, 2, 3, 9, 4, 9, 5, 6, 7],  # 9 <=> missing","323","         [1, 1, 1, 1, 5, 5, 5, 5, 5, 5],","324","         True,  # missing values","325","         8,  # n_bins_non_missing","326","         False,  # don't split on nans","327","         3,  # cut on bin_idx=3 (like in first case)","328","         False),  # missing values go to right","329","","330","        # For the following cases, split_on_nans is True (we replace all of","331","        # the samples with nans, instead of just 2).","332","        ([0, 1, 2, 3, 4, 4, 4, 4, 4, 4],  # 4 <=> missing","333","         [1, 1, 1, 1, 5, 5, 5, 5, 5, 5],","334","         True,  # missing values","335","         4,  # n_bins_non_missing","336","         True,  # split on nans","337","         3,  # cut on bin_idx=3","338","         False),  # missing values go to right","339","","340","        # same as above, but with non-consecutive missing_values_bin","341","        ([0, 1, 2, 3, 9, 9, 9, 9, 9, 9],  # 9 <=> missing","342","         [1, 1, 1, 1, 1, 1, 5, 5, 5, 5],","343","         True,  # missing values","344","         4,  # n_bins_non_missing","345","         True,  # split on nans","346","         3,  # cut on bin_idx=3","347","         False),  # missing values go to right","348","","349","        ([6, 6, 6, 6, 0, 1, 2, 3, 4, 5],  # 4 <=> missing","350","         [1, 1, 1, 1, 5, 5, 5, 5, 5, 5],","351","         True,  # missing values","352","         6,  # n_bins_non_missing","353","         True,  # split on nans","354","         5,  # cut on bin_idx=5","355","         False),  # missing values go to right","356","","357","        # same as above, but with non-consecutive missing_values_bin","358","        ([9, 9, 9, 9, 0, 1, 2, 3, 4, 5],  # 9 <=> missing","359","         [1, 1, 1, 1, 5, 5, 5, 5, 5, 5],","360","         True,  # missing values","361","         6,  # n_bins_non_missing","362","         True,  # split on nans","363","         5,  # cut on bin_idx=5","364","         False),  # missing values go to right","365","    ]","366",")","367","def test_splitting_missing_values(X_binned, all_gradients,","368","                                  has_missing_values, n_bins_non_missing,","369","                                  expected_split_on_nan, expected_bin_idx,","370","                                  expected_go_to_left):","371","    # Make sure missing values are properly supported.","372","    # we build an artificial example with gradients such that the best split","373","    # is on bin_idx=3, when there are no missing values.","374","    # Then we introduce missing values and:","375","    #   - make sure the chosen bin is correct (find_best_bin()): it's","376","    #     still the same split, even though the index of the bin may change","377","    #   - make sure the missing values are mapped to the correct child","378","    #     (split_indices())","379","","380","    n_bins = max(X_binned) + 1","381","    n_samples = len(X_binned)","382","    l2_regularization = 0.","383","    min_hessian_to_split = 1e-3","384","    min_samples_leaf = 1","385","    min_gain_to_split = 0.","386","","387","    sample_indices = np.arange(n_samples, dtype=np.uint32)","388","    X_binned = np.array(X_binned, dtype=X_BINNED_DTYPE).reshape(-1, 1)","389","    X_binned = np.asfortranarray(X_binned)","390","    all_gradients = np.array(all_gradients, dtype=G_H_DTYPE)","391","    has_missing_values = np.array([has_missing_values], dtype=np.uint8)","392","    all_hessians = np.ones(1, dtype=G_H_DTYPE)","393","    sum_gradients = all_gradients.sum()","394","    sum_hessians = 1 * n_samples","395","    hessians_are_constant = True","396","","397","    builder = HistogramBuilder(X_binned, n_bins,","398","                               all_gradients, all_hessians,","399","                               hessians_are_constant)","400","","401","    n_bins_non_missing = np.array([n_bins_non_missing], dtype=np.uint32)","402","    missing_values_bin_idx = n_bins - 1","403","    splitter = Splitter(X_binned, n_bins_non_missing,","404","                        missing_values_bin_idx, has_missing_values,","412","","413","    assert split_info.bin_idx == expected_bin_idx","414","    if has_missing_values:","415","        assert split_info.missing_go_to_left == expected_go_to_left","416","","417","    split_on_nan = split_info.bin_idx == n_bins_non_missing[0] - 1","418","    assert split_on_nan == expected_split_on_nan","419","","420","    # Make sure the split is properly computed.","421","    # This also make sure missing values are properly assigned to the correct","422","    # child in split_indices()","423","    samples_left, samples_right, _ = splitter.split_indices(","424","        split_info, splitter.partition)","425","","426","    if not expected_split_on_nan:","427","        # When we don't split on nans, the split should always be the same.","428","        assert set(samples_left) == set([0, 1, 2, 3])","429","        assert set(samples_right) == set([4, 5, 6, 7, 8, 9])","430","    else:","431","        # When we split on nans, samples with missing values are always mapped","432","        # to the right child.","433","        missing_samples_indices = np.flatnonzero(","434","            np.array(X_binned) == missing_values_bin_idx)","435","        non_missing_samples_indices = np.flatnonzero(","436","            np.array(X_binned) != missing_values_bin_idx)","437","","438","        assert set(samples_right) == set(missing_samples_indices)","439","        assert set(samples_left) == set(non_missing_samples_indices)"],"delete":["3","from sklearn.ensemble._hist_gradient_boosting.types import HISTOGRAM_DTYPE","4","from sklearn.ensemble._hist_gradient_boosting.types import G_H_DTYPE","5","from sklearn.ensemble._hist_gradient_boosting.types import X_BINNED_DTYPE","20","        rng.randint(0, n_bins, size=(int(1e4), 1)), dtype=X_BINNED_DTYPE)","28","    for true_bin in range(1, n_bins - 1):","36","            actual_n_bins = np.array([n_bins] * X_binned.shape[1],","37","                                     dtype=np.uint32)","44","                                actual_n_bins,","98","    actual_n_bins = np.array([n_bins] * X_binned.shape[1],","99","                             dtype=np.uint32)","102","    splitter = Splitter(X_binned, actual_n_bins,","103","                        l2_regularization, min_hessian_to_split,","104","                        min_samples_leaf, min_gain_to_split, constant_hessian)","194","    actual_n_bins = np.array([n_bins] * X_binned.shape[1],","195","                             dtype=np.uint32)","199","    splitter = Splitter(X_binned, actual_n_bins,","200","                        l2_regularization, min_hessian_to_split,","201","                        min_samples_leaf, min_gain_to_split,","202","                        hessians_are_constant)","250","    actual_n_bins = np.array([n_bins] * X_binned.shape[1],","251","                             dtype=np.uint32)","254","    splitter = Splitter(X_binned, actual_n_bins,","262","    assert split_info.gain == -1"]}],"doc\/whats_new\/v0.22.rst":[{"add":["27","- :class:`ensemble.HistGradientBoostingClassifier` and","28","  :class:`ensemble.HistGradientBoostingRegressor` |Fix|, |Feature|,","29","  |Enhancement|.","115","- Many improvements were made to","116","  :class:`ensemble.HistGradientBoostingClassifier` and","117","  :class:`ensemble.HistGradientBoostingRegressor`:","119","  - |MajorFeature| Estimators now natively support dense data with missing","120","    values both for training and predicting. They also support infinite","121","    values. :pr:`13911` and :pr:`14406` by `Nicolas Hug`_, `Adrin Jalali`_","122","    and `Olivier Grisel`_.","123","  - |Feature| Estimators now have an additional `warm_start` parameter that","124","    enables warm starting. :pr:`14012` by :user:`Johann Faouzi <johannfaouzi>`.","125","  - |Enhancement| for :class:`ensemble.HistGradientBoostingClassifier` the","126","    training loss or score is now monitored on a class-wise stratified","127","    subsample to preserve the class balance of the original training set.","128","    :pr:`14194` by :user:`Johann Faouzi <johannfaouzi>`.","129","  - |Feature| :func:`inspection.partial_dependence` and","130","    :func:`inspection.plot_partial_dependence` now support the fast 'recursion'","131","    method for both estimators. :pr:`13769` by `Nicolas Hug`_.","132","  - |Fix| Estimators now bin the training and validation data separately to","133","    avoid any data leak. :pr:`13933` by `Nicolas Hug`_.","134","","135","  Note that pickles from 0.21 will not work in 0.22.","192","- |Feature| :func:`inspection.partial_dependence` and","193","  :func:`inspection.plot_partial_dependence` now support the fast 'recursion'","194","  method for :class:`ensemble.HistGradientBoostingClassifier` and","195","  :class:`ensemble.HistGradientBoostingRegressor`. :pr:`13769` by","196","  `Nicolas Hug`_.","197",""],"delete":["25","","27","","114","- |Feature| :class:`ensemble.HistGradientBoostingClassifier` and","115","  :class:`ensemble.HistGradientBoostingRegressor` have an additional","116","  parameter called `warm_start` that enables warm starting. :pr:`14012` by","117","  :user:`Johann Faouzi <johannfaouzi>`.","119","- |Fix| :class:`ensemble.HistGradientBoostingClassifier` and","120","  :class:`ensemble.HistGradientBoostingRegressor` now bin the training and","121","  validation data separately to avoid any data leak. :pr:`13933` by","122","  `Nicolas Hug`_.","127","- |Enhancement| :class:`ensemble.HistGradientBoostingClassifier` the training","128","  loss or score is now monitored on a class-wise stratified subsample to","129","  preserve the class balance of the original training set. :pr:`14194`","130","  by :user:`Johann Faouzi <johannfaouzi>`.","131",""]}]}},"f9af18b4e5b9d4b379867d32381296062782dc15":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","doc\/whats_new\/_contributors.rst":"MODIFY","sklearn\/ensemble\/tests\/test_forest.py":"MODIFY","sklearn\/ensemble\/forest.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["196","- |Fix| The values of ``feature_importances_`` in all random forest based","197","  models (i.e.","198","  :class:`ensemble.RandomForestClassifier`,","199","  :class:`ensemble.RandomForestRegressor`,","200","  :class:`ensemble.ExtraTreesClassifier`,","201","  :class:`ensemble.ExtraTreesRegressor`, and","202","  :class:`ensemble.RandomTreesEmbedding`) now:","203","","204","  - sum up to ``1``","205","  - all the single node trees in feature importance calculation are ignored","206","  - in case all trees have only one single node (i.e. a root node),","207","    feature importances will be an array of all zeros.","208","  :issue:`13636` by `Adrin Jalali`_.","209",""],"delete":[]}],"doc\/whats_new\/_contributors.rst":[{"add":["176",".. _Nicolas Hug: https:\/\/github.com\/NicolasHug"],"delete":["176",".. _Nicolas Hug: https:\/\/github.com\/NicolasHug"]}],"sklearn\/ensemble\/tests\/test_forest.py":[{"add":["11","import math","45","from sklearn.datasets import make_classification","1363","","1364","","1365","def test_forest_feature_importances_sum():","1366","    X, y = make_classification(n_samples=15, n_informative=3, random_state=1,","1367","                               n_classes=3)","1368","    clf = RandomForestClassifier(min_samples_leaf=5, random_state=42,","1369","                                 n_estimators=200).fit(X, y)","1370","    assert math.isclose(1, clf.feature_importances_.sum(), abs_tol=1e-7)","1371","","1372","","1373","def test_forest_degenerate_feature_importances():","1374","    # build a forest of single node trees. See #13636","1375","    X = np.zeros((10, 10))","1376","    y = np.ones((10,))","1377","    gbr = RandomForestRegressor(n_estimators=10).fit(X, y)","1378","    assert_array_equal(gbr.feature_importances_,","1379","                       np.zeros(10, dtype=np.float64))"],"delete":[]}],"sklearn\/ensemble\/forest.py":[{"add":["368","            The values of this array sum to 1, unless all trees are single node","369","            trees consisting of only the root node, in which case it will be an","370","            array of zeros.","377","            for tree in self.estimators_ if tree.tree_.node_count > 1)","379","        if not all_importances:","380","            return np.zeros(self.n_features_, dtype=np.float64)","381","","382","        all_importances = np.mean(all_importances,","383","                                  axis=0, dtype=np.float64)","384","        return all_importances \/ np.sum(all_importances)"],"delete":["374","            for tree in self.estimators_)","376","        return sum(all_importances) \/ len(self.estimators_)"]}]}},"ac79dff3f50b03e2dfa58ecb59e5a5e2887ce9b2":{"changes":{"sklearn\/cluster\/tests\/test_affinity_propagation.py":"MODIFY","sklearn\/cluster\/affinity_propagation_.py":"MODIFY"},"diff":{"sklearn\/cluster\/tests\/test_affinity_propagation.py":[{"add":["6","import pytest","7","from scipy.sparse import csr_matrix","166","","167","","168","@pytest.mark.parametrize('centers', [csr_matrix(np.zeros((1, 10))),","169","                                     np.zeros((1, 10))])","170","def test_affinity_propagation_convergence_warning_dense_sparse(centers):","171","    \"\"\"Non-regression, see #13334\"\"\"","172","    rng = np.random.RandomState(42)","173","    X = rng.rand(40, 10)","174","    y = (4 * rng.rand(40)).astype(np.int)","175","    ap = AffinityPropagation()","176","    ap.fit(X, y)","177","    ap.cluster_centers_ = centers","178","    with pytest.warns(None) as record:","179","        assert_array_equal(ap.predict(X),","180","                           np.zeros(X.shape[0], dtype=int))","181","    assert len(record) == 0"],"delete":[]}],"sklearn\/cluster\/affinity_propagation_.py":[{"add":["405","        if self.cluster_centers_.shape[0] > 0:"],"delete":["405","        if self.cluster_centers_.size > 0:"]}]}},"bcf4f803e4e826cbdb9f8e0f2108651568664fe7":{"changes":{"sklearn\/tests\/test_impute.py":"MODIFY","doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/impute.py":"MODIFY"},"diff":{"sklearn\/tests\/test_impute.py":[{"add":["1121","","1122","","1123","def test_missing_indicator_no_missing():","1124","    # check that all features are dropped if there are no missing values when","1125","    # features='missing-only' (#13491)","1126","    X = np.array([[1, 1],","1127","                  [1, 1]])","1128","","1129","    mi = MissingIndicator(features='missing-only', missing_values=-1)","1130","    Xt = mi.fit_transform(X)","1131","","1132","    assert Xt.shape[1] == 0","1133","","1134","","1135","def test_missing_indicator_sparse_no_explicit_zeros():","1136","    # Check that non missing values don't become explicit zeros in the mask","1137","    # generated by missing indicator when X is sparse. (#13491)","1138","    X = sparse.csr_matrix([[0, 1, 2],","1139","                           [1, 2, 0],","1140","                           [2, 0, 1]])","1141","","1142","    mi = MissingIndicator(features='all', missing_values=1)","1143","    Xt = mi.fit_transform(X)","1144","","1145","    assert Xt.getnnz() == Xt.sum()"],"delete":[]}],"doc\/whats_new\/v0.21.rst":[{"add":["251","- |Fix| Fixed two bugs in :class:`MissingIndicator`. First, when ``X`` is","252","  sparse, all the non-zero non missing values used to become explicit False in","253","  the transformed data. Then, when ``features='missing-only'``, all features","254","  used to be kept if there were no missing values at all. :issue:`13562` by","255","  :user:`Jrmie du Boisberranger <jeremiedbb>`.","256",""],"delete":[]}],"sklearn\/impute.py":[{"add":["1146","            imputer_mask.eliminate_zeros()","1148","            if self.features == 'missing-only':","1149","                n_missing = imputer_mask.getnnz(axis=0)","1157","","1158","            if self.features == 'missing-only':","1159","                n_missing = imputer_mask.sum(axis=0)","1164","        if self.features == 'all':","1165","            features_indices = np.arange(X.shape[1])","1166","        else:","1167","            features_indices = np.flatnonzero(n_missing)","1168","","1169","        return imputer_mask, features_indices","1222","        self.features_ = self._get_missing_features_info(X)[1]","1257","            if self.features_.size < self._n_features:"],"delete":["1147","            missing_values_mask = imputer_mask.copy()","1148","            missing_values_mask.eliminate_zeros()","1149","            features_with_missing = (","1150","                np.flatnonzero(np.diff(missing_values_mask.indptr))","1151","                if missing_values_mask.format == 'csc'","1152","                else np.unique(missing_values_mask.indices))","1160","            features_with_missing = np.flatnonzero(imputer_mask.sum(axis=0))","1165","        return imputer_mask, features_with_missing","1218","        self.features_ = (self._get_missing_features_info(X)[1]","1219","                          if self.features == 'missing-only'","1220","                          else np.arange(self._n_features))","1255","            if (self.features_.size > 0 and","1256","                    self.features_.size < self._n_features):"]}]}},"98c1f07079387efa6efb635af514938adf50902e":{"changes":{"sklearn\/utils\/testing.py":"MODIFY"},"diff":{"sklearn\/utils\/testing.py":[{"add":["649","        if \".tests.\" in modname or \"externals\" in modname:"],"delete":["649","        if \".tests.\" in modname:"]}]}},"fce73db57a602eec89583dd1582e69781244fec5":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/preprocessing\/_encoders.py":"MODIFY","sklearn\/preprocessing\/tests\/test_encoders.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["4"," .. _changes_0_20_3:","5","","6","Version 0.20.3","7","==============","8","","9","**??, 2019**","10","","11","This is a bug-fix release with some minor documentation improvements and","12","enhancements to features released in 0.20.0.","13","","14","Changelog","15","---------","16","","17",":mod:`sklearn.preprocessing`","18","............................","19","","20","- |Fix| Fixed a bug in :class:`preprocessing.OneHotEncoder` where the","21","  deprecation of ``categorical_features`` was handled incorrectly in","22","  combination with ``handle_unknown='ignore'``.","23","  :issue:`12881` by `Joris Van den Bossche`_.","24",""],"delete":[]}],"sklearn\/preprocessing\/_encoders.py":[{"add":["333","            # n_values can also be None (default to catch usage), so set","334","            # _n_values to 'auto' explicitly","335","            self._n_values = 'auto'","456","                                % type(self._n_values))"],"delete":["368","                    self._n_values = 'auto'","454","                                % type(X))"]}],"sklearn\/preprocessing\/tests\/test_encoders.py":[{"add":["228","def test_one_hot_encoder_categorical_features_ignore_unknown():","229","    # GH12881 bug in combination of categorical_features with ignore","230","    X = np.array([[1, 2, 3], [4, 5, 6], [2, 3, 2]]).T","231","    oh = OneHotEncoder(categorical_features=[2], handle_unknown='ignore')","232","","233","    with ignore_warnings(category=DeprecationWarning):","234","        res = oh.fit_transform(X)","235","","236","    expected = np.array([[1, 0, 1], [0, 1, 0], [1, 2, 3], [4, 5, 6]]).T","237","    assert_array_equal(res.toarray(), expected)","238","","239",""],"delete":[]}]}},"5d2e8979c133a240c7bbd2020a1557ffa11109f2":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["31","  :class:`sklearn.feature_extraction.text.CountVectorizer` |Fix|","273","- |Fix| |API| If ``input='file'`` or ``input='filename'``, and a callable is given","402",".......................","528","..........................","664","...................."],"delete":["31","  :class:`sklearn.feature_extraction.text.CountVectorizer` |API|","273","- |API| If ``input='file'`` or ``input='filename'``, and a callable is given","402","............................","528","........................","664","..................."]}]}},"7f50e8266360b8ab6021db7253a0ae5e0acf1b18":{"changes":{"sklearn\/impute\/tests\/test_impute.py":"MODIFY","doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/impute\/_iterative.py":"MODIFY","sklearn\/impute\/_base.py":"MODIFY"},"diff":{"sklearn\/impute\/tests\/test_impute.py":[{"add":["447","@pytest.mark.parametrize('Imputer', (SimpleImputer, IterativeImputer))","448","def test_imputation_missing_value_in_test_array(Imputer):","449","    # [Non Regression Test for issue #13968] Missing value in test set should","450","    # not throw an error and return a finite dataset","451","    train = [[1], [2]]","452","    test = [[3], [np.nan]]","453","    imputer = Imputer(add_indicator=True)","454","    imputer.fit(train).transform(test)","455","","456",""],"delete":[]}],"doc\/whats_new\/v0.21.rst":[{"add":["14",":mod:`sklearn.impute`","15",".....................","16","","17","- |Fix| Fixed a bug in :class:`SimpleImputer` and :class:`IterativeImputer`","18","  so that no errors are thrown when there are missing values in training data.","19","  :pr:`13974` by `Frank Hoang <fhoang7>`.","20",""],"delete":[]}],"sklearn\/impute\/_iterative.py":[{"add":["522","                missing_values=self.missing_values, error_on_new=False)"],"delete":["522","                missing_values=self.missing_values)"]}],"sklearn\/impute\/_base.py":[{"add":["271","                missing_values=self.missing_values, error_on_new=False)"],"delete":["271","                missing_values=self.missing_values)"]}]}},"1ae1f1db718e03b958c169c386bd1f7e91ffb6d9":{"changes":{"sklearn\/tests\/test_impute.py":"MODIFY","doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/impute.py":"MODIFY","sklearn\/utils\/estimator_checks.py":"MODIFY"},"diff":{"sklearn\/tests\/test_impute.py":[{"add":["15","from sklearn.pipeline import make_union","512","      \"'sparse' has to be a boolean or 'auto'\"),","513","     (np.array([['a', 'b'], ['c', 'a']], dtype=str),","514","      np.array([['a', 'b'], ['c', 'a']], dtype=str),","515","      {}, \"MissingIndicator does not support data with dtype\")]","620","def test_missing_indicator_string():","621","    X = np.array([['a', 'b', 'c'], ['b', 'c', 'a']], dtype=object)","622","    indicator = MissingIndicator(missing_values='a', features='all')","623","    X_trans = indicator.fit_transform(X)","624","    assert_array_equal(X_trans, np.array([[True, False, False],","625","                                          [False, False, True]]))","626","","627","","628","@pytest.mark.parametrize(","629","    \"X, missing_values, X_trans_exp\",","630","    [(np.array([['a', 'b'], ['b', 'a']], dtype=object), 'a',","631","      np.array([['b', 'b', True, False], ['b', 'b', False, True]],","632","               dtype=object)),","633","     (np.array([[np.nan, 1.], [1., np.nan]]), np.nan,","634","      np.array([[1., 1., True, False], [1., 1., False, True]])),","635","     (np.array([[np.nan, 'b'], ['b', np.nan]], dtype=object), np.nan,","636","      np.array([['b', 'b', True, False], ['b', 'b', False, True]],","637","               dtype=object)),","638","     (np.array([[None, 'b'], ['b', None]], dtype=object), None,","639","      np.array([['b', 'b', True, False], ['b', 'b', False, True]],","640","               dtype=object))]","641",")","642","def test_missing_indicator_with_imputer(X, missing_values, X_trans_exp):","643","    trans = make_union(","644","        SimpleImputer(missing_values=missing_values, strategy='most_frequent'),","645","        MissingIndicator(missing_values=missing_values)","646","    )","647","    X_trans = trans.fit_transform(X)","648","    assert_array_equal(X_trans, X_trans_exp)","649","","650",""],"delete":["511","      \"'sparse' has to be a boolean or 'auto'\")]"]}],"doc\/whats_new\/v0.20.rst":[{"add":["38",":mod:`sklearn.impute`","39",".....................","40","","41","- |Fix| add support for non-numeric data in","42","  :class:`sklearn.impute.MissingIndicator` which was not supported while","43","  :class:`sklearn.impute.SimpleImputer` was supporting this for some","44","  imputation strategies.","45","  :issue:`13046` by :user:`Guillaume Lemaitre <glemaitre>`.","46",""],"delete":[]}],"sklearn\/impute.py":[{"add":["535","    def _validate_input(self, X):","536","        if not is_scalar_nan(self.missing_values):","537","            force_all_finite = True","538","        else:","539","            force_all_finite = \"allow-nan\"","540","        X = check_array(X, accept_sparse=('csc', 'csr'), dtype=None,","541","                        force_all_finite=force_all_finite)","542","        _check_inputs_dtype(X, self.missing_values)","543","        if X.dtype.kind not in (\"i\", \"u\", \"f\", \"O\"):","544","            raise ValueError(\"MissingIndicator does not support data with \"","545","                             \"dtype {0}. Please provide either a numeric array\"","546","                             \" (with a floating point or integer dtype) or \"","547","                             \"categorical data represented either as an array \"","548","                             \"with integer dtype or an array of string values \"","549","                             \"with an object dtype.\".format(X.dtype))","550","        return X","551","","566","        X = self._validate_input(X)","600","        X = self._validate_input(X)"],"delete":["549","        if not is_scalar_nan(self.missing_values):","550","            force_all_finite = True","551","        else:","552","            force_all_finite = \"allow-nan\"","553","        X = check_array(X, accept_sparse=('csc', 'csr'),","554","                        force_all_finite=force_all_finite)","555","        _check_inputs_dtype(X, self.missing_values)","556","","590","","591","        if not is_scalar_nan(self.missing_values):","592","            force_all_finite = True","593","        else:","594","            force_all_finite = \"allow-nan\"","595","        X = check_array(X, accept_sparse=('csc', 'csr'),","596","                        force_all_finite=force_all_finite)","597","        _check_inputs_dtype(X, self.missing_values)"]}],"sklearn\/utils\/estimator_checks.py":[{"add":["77","SUPPORT_STRING = ['SimpleImputer', 'MissingIndicator']","625","    if name not in SUPPORT_STRING:","626","        X[0, 0] = {'foo': 'bar'}","627","        msg = \"argument must be a string or a number\"","628","        assert_raises_regex(TypeError, msg, estimator.fit, X, y)","629","    else:","630","        # Estimators supporting string will not call np.asarray to convert the","631","        # data to numeric and therefore, the error will not be raised.","632","        # Checking for each element dtype in the input array will be costly.","633","        # Refer to #11401 for full discussion.","634","        estimator.fit(X, y)"],"delete":["74","","625","    X[0, 0] = {'foo': 'bar'}","626","    msg = \"argument must be a string or a number\"","627","    assert_raises_regex(TypeError, msg, estimator.fit, X, y)"]}]}}}