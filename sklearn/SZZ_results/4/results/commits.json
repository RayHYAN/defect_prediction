{"7500693ea22b05b06d9ab8789d156ed00df35b11":{"changes":{"sklearn\/preprocessing\/tests\/test_imputation.py":"MODIFY","sklearn\/model_selection\/_search.py":"MODIFY"},"diff":{"sklearn\/preprocessing\/tests\/test_imputation.py":[{"add":["280","    X = sparse_random_matrix(l, l, density=0.10, random_state=0)","281","    Y = sparse_random_matrix(l, 1, density=0.10, random_state=0).toarray()"],"delete":["280","    X = sparse_random_matrix(l, l, density=0.10)","281","    Y = sparse_random_matrix(l, 1, density=0.10).toarray()"]}],"sklearn\/model_selection\/_search.py":[{"add":["382","    def __init__(self, estimator, scoring=None, n_jobs=None, iid='warn',"],"delete":["382","    def __init__(self, estimator, scoring=None,","383","                 fit_params=None, n_jobs=None, iid='warn',","390","        self.fit_params = fit_params"]}]}},"9597da9b0270ee1cbc783fd7f6633e7af624e34f":{"changes":{"doc\/modules\/linear_model.rst":"MODIFY"},"diff":{"doc\/modules\/linear_model.rst":[{"add":["694","with :math:`\\text{diag}(A) = \\lambda = \\{\\lambda_{1},...,\\lambda_{p}\\}`."],"delete":["694","with :math:`\\diag \\; (A) = \\lambda = \\{\\lambda_{1},...,\\lambda_{p}\\}`."]}]}},"8a604f7f87ce8a9b8dca83902dd090b296a37a37":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/cluster\/k_means_.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["17",":mod:`sklearn.cluster`","18","......................","19","","20","- |Fix| Fixed a bug in :class:`cluster.KMeans` where computation was single","21","  threaded when `n_jobs > 1` or `n_jobs = -1`.","22","  :issue:`12949` by :user:`Prabakaran Kumaresshan <nixphix>`.","23",""],"delete":[]}],"sklearn\/cluster\/k_means_.py":[{"add":["369","    if effective_n_jobs(n_jobs) == 1:","870","    ...               [10, 2], [10, 4], [10, 0]])","873","    array([1, 1, 1, 0, 0, 0], dtype=int32)","874","    >>> kmeans.predict([[0, 0], [12, 3]])","875","    array([1, 0], dtype=int32)","877","    array([[10.,  2.],","878","           [ 1.,  2.]])"],"delete":["369","    if effective_n_jobs(n_jobs):","870","    ...               [4, 2], [4, 4], [4, 0]])","873","    array([0, 0, 0, 1, 1, 1], dtype=int32)","874","    >>> kmeans.predict([[0, 0], [4, 4]])","875","    array([0, 1], dtype=int32)","877","    array([[1., 2.],","878","           [4., 2.]])"]}]}},"16100944b9f5a4d611a48eb372a39144b790d6a0":{"changes":{"sklearn\/utils\/fixes.py":"MODIFY","sklearn\/ensemble\/iforest.py":"MODIFY","sklearn\/ensemble\/tests\/test_iforest.py":"MODIFY"},"diff":{"sklearn\/utils\/fixes.py":[{"add":[],"delete":["31","# < numpy 1.8.0","32","euler_gamma = getattr(np, 'euler_gamma',","33","                      0.577215664901532860606512090082402431)","34",""]}],"sklearn\/ensemble\/iforest.py":[{"add":["6","import numpy as np","7","from scipy.sparse import issparse","8","from warnings import warn","9","","443","            return 2. * (np.log(n_samples_leaf - 1.) + np.euler_gamma) - 2. * (","457","            np.log(n_samples_leaf[not_mask] - 1.) + np.euler_gamma) - 2. * ("],"delete":["5","import numpy as np","6","from warnings import warn","7","from sklearn.utils.fixes import euler_gamma","8","","9","from scipy.sparse import issparse","10","","445","            return 2. * (np.log(n_samples_leaf - 1.) + euler_gamma) - 2. * (","459","            np.log(n_samples_leaf[not_mask] - 1.) + euler_gamma) - 2. * ("]}],"sklearn\/ensemble\/tests\/test_iforest.py":[{"add":["265","    result_one = 2. * (np.log(4.) + np.euler_gamma) - 2. * 4. \/ 5.","266","    result_two = 2. * (np.log(998.) + np.euler_gamma) - 2. * 998. \/ 999."],"delete":["12","from sklearn.utils.fixes import euler_gamma","266","    result_one = 2. * (np.log(4.) + euler_gamma) - 2. * 4. \/ 5.","267","    result_two = 2. * (np.log(998.) + euler_gamma) - 2. * 998. \/ 999."]}]}},"2f35e9e79fdf6f8179c326722aa72f81871781fd":{"changes":{"sklearn\/datasets\/tests\/test_svmlight_format.py":"MODIFY"},"diff":{"sklearn\/datasets\/tests\/test_svmlight_format.py":[{"add":["228","                for dtype in [np.float32, np.float64, np.int32, np.int64]:","239","                    # Note: with dtype=np.int32 we are performing unsafe casts,","240","                    # where X.astype(dtype) overflows. The result is","241","                    # then platform dependent and X_dense.astype(dtype) may be","242","                    # different from X_sparse.astype(dtype).asarray().","243","                    X_input = X.astype(dtype)","244","","245","                    dump_svmlight_file(X_input, y, f, comment=\"test\",","265","                    if sp.issparse(X_input):","266","                        X_input_dense = X_input.toarray()","267","                    else:","268","                        X_input_dense = X_input","273","                            X_input_dense, X2_dense, 4)","279","                            X_input_dense, X2_dense, 15)"],"delete":["228","                for dtype in [np.float32, np.float64, np.int32]:","239","                    dump_svmlight_file(X.astype(dtype), y, f, comment=\"test\",","263","                            X_dense.astype(dtype), X2_dense, 4)","269","                            X_dense.astype(dtype), X2_dense, 15)"]}]}},"5f0263fe7f1ce66a91a3af01a54caad7ac546443":{"changes":{"sklearn\/tree\/_utils.pyx":"MODIFY","sklearn\/linear_model\/cd_fast.pyx":"MODIFY","sklearn\/utils\/seq_dataset.pyx.tp":"MODIFY"},"diff":{"sklearn\/tree\/_utils.pyx":[{"add":["22","from ..utils._random cimport our_rand_r","66","    return low + our_rand_r(random_state) % (high - low)","72","    return ((high - low) * <double> our_rand_r(random_state) \/"],"delete":["22","from ..utils cimport _random","66","    return low + _random.our_rand_r(random_state) % (high - low)","72","    return ((high - low) * <double> _random.our_rand_r(random_state) \/"]}],"sklearn\/linear_model\/cd_fast.pyx":[{"add":["26","from ..utils._random cimport our_rand_r","44","    return our_rand_r(random_state) % end"],"delete":["26","from ..utils cimport _random","44","    return _random.our_rand_r(random_state) % end"]}],"sklearn\/utils\/seq_dataset.pyx.tp":[{"add":["47","from ._random cimport our_rand_r","157","            j = i + our_rand_r(&seed) % (n - i)","171","        cdef int current_index = our_rand_r(&self.seed) % n"],"delete":["47","from . cimport _random","157","            j = i + _random.our_rand_r(&seed) % (n - i)","171","        cdef int current_index = _random.our_rand_r(&self.seed) % n"]}]}},"3aefc834dce72e850bff48689bea3c7dff5f3fad":{"changes":{"examples\/gaussian_process\/plot_gpr_prior_posterior.py":"MODIFY","examples\/exercises\/plot_iris_exercise.py":"MODIFY","examples\/gaussian_process\/plot_gpc.py":"MODIFY","build_tools\/circle\/build_doc.sh":"MODIFY",".circleci\/config.yml":"MODIFY","doc\/modules\/gaussian_process.rst":"MODIFY","doc\/modules\/calibration.rst":"MODIFY","examples\/calibration\/plot_calibration_multiclass.py":"MODIFY","examples\/gaussian_process\/plot_gpr_noisy.py":"MODIFY"},"diff":{"examples\/gaussian_process\/plot_gpr_prior_posterior.py":[{"add":["35","for kernel in kernels:","40","    plt.figure(figsize=(8, 8))"],"delete":["35","for fig_index, kernel in enumerate(kernels):","40","    plt.figure(fig_index, figsize=(8, 8))"]}],"examples\/exercises\/plot_iris_exercise.py":[{"add":["37","for kernel in ('linear', 'rbf', 'poly'):","41","    plt.figure()"],"delete":["37","for fig_num, kernel in enumerate(('linear', 'rbf', 'poly')):","41","    plt.figure(fig_num)"]}],"examples\/gaussian_process\/plot_gpc.py":[{"add":["65","plt.figure()","82","plt.figure()"],"delete":["65","plt.figure(0)","82","plt.figure(1)"]}],"build_tools\/circle\/build_doc.sh":[{"add":["103","    latexmk gsfonts"],"delete":["103","    latexmk"]}],".circleci\/config.yml":[{"add":["5","      - image: circleci\/python:3.6","34","      - image: circleci\/python:3.6","63","      - image: circleci\/python:3.6","92","      - image: circleci\/python:3.6"],"delete":["5","      - image: circleci\/python:3.6.1","34","      - image: circleci\/python:3.6.1","63","      - image: circleci\/python:3.6.1","92","      - image: circleci\/python:3.6.1"]}],"doc\/modules\/gaussian_process.rst":[{"add":["90",".. figure:: ..\/auto_examples\/gaussian_process\/images\/sphx_glr_plot_gpr_noisy_001.png","97",".. figure:: ..\/auto_examples\/gaussian_process\/images\/sphx_glr_plot_gpr_noisy_002.png","108",".. figure:: ..\/auto_examples\/gaussian_process\/images\/sphx_glr_plot_gpr_noisy_003.png","308",".. figure:: ..\/auto_examples\/gaussian_process\/images\/sphx_glr_plot_gpc_001.png","312",".. figure:: ..\/auto_examples\/gaussian_process\/images\/sphx_glr_plot_gpc_002.png","495",".. figure:: ..\/auto_examples\/gaussian_process\/images\/sphx_glr_plot_gpr_prior_posterior_001.png","536",".. figure:: ..\/auto_examples\/gaussian_process\/images\/sphx_glr_plot_gpr_prior_posterior_005.png","558",".. figure:: ..\/auto_examples\/gaussian_process\/images\/sphx_glr_plot_gpr_prior_posterior_002.png","576",".. figure:: ..\/auto_examples\/gaussian_process\/images\/sphx_glr_plot_gpr_prior_posterior_003.png","596",".. figure:: ..\/auto_examples\/gaussian_process\/images\/sphx_glr_plot_gpr_prior_posterior_004.png"],"delete":["90",".. figure:: ..\/auto_examples\/gaussian_process\/images\/sphx_glr_plot_gpr_noisy_000.png","97",".. figure:: ..\/auto_examples\/gaussian_process\/images\/sphx_glr_plot_gpr_noisy_001.png","108",".. figure:: ..\/auto_examples\/gaussian_process\/images\/sphx_glr_plot_gpr_noisy_002.png","308",".. figure:: ..\/auto_examples\/gaussian_process\/images\/sphx_glr_plot_gpc_000.png","312",".. figure:: ..\/auto_examples\/gaussian_process\/images\/sphx_glr_plot_gpc_001.png","495",".. figure:: ..\/auto_examples\/gaussian_process\/images\/sphx_glr_plot_gpr_prior_posterior_000.png","536",".. figure:: ..\/auto_examples\/gaussian_process\/images\/sphx_glr_plot_gpr_prior_posterior_004.png","558",".. figure:: ..\/auto_examples\/gaussian_process\/images\/sphx_glr_plot_gpr_prior_posterior_001.png","576",".. figure:: ..\/auto_examples\/gaussian_process\/images\/sphx_glr_plot_gpr_prior_posterior_002.png","596",".. figure:: ..\/auto_examples\/gaussian_process\/images\/sphx_glr_plot_gpr_prior_posterior_003.png"]}],"doc\/modules\/calibration.rst":[{"add":["173",".. figure:: ..\/auto_examples\/calibration\/images\/sphx_glr_plot_calibration_multiclass_001.png","185",".. figure:: ..\/auto_examples\/calibration\/images\/sphx_glr_plot_calibration_multiclass_002.png"],"delete":["173",".. figure:: ..\/auto_examples\/calibration\/images\/sphx_glr_plot_calibration_multiclass_000.png","185",".. figure:: ..\/auto_examples\/calibration\/images\/sphx_glr_plot_calibration_multiclass_001.png"]}],"examples\/calibration\/plot_calibration_multiclass.py":[{"add":["66","plt.figure()","133","plt.figure()"],"delete":["66","plt.figure(0)","133","plt.figure(1)"]}],"examples\/gaussian_process\/plot_gpr_noisy.py":[{"add":["37","plt.figure()","56","plt.figure()","75","plt.figure()"],"delete":["37","plt.figure(0)","56","plt.figure(1)","75","plt.figure(2)"]}]}},"c8e757df96280fe1d592eb2bc249a9b230fc9eda":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/linear_model\/tests\/test_ridge.py":"MODIFY","sklearn\/linear_model\/ridge.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["232","- |Enhancement| `sparse_cg` solver in :class:`linear_model.ridge.Ridge`","233","  now supports fitting the intercept (i.e. ``fit_intercept=True``) when","234","  inputs are sparse . :issue:`13336` by :user:`Bartosz Telenczuk <btel>`","235",""],"delete":[]}],"sklearn\/linear_model\/tests\/test_ridge.py":[{"add":["817","","820","    for solver in ['sag', 'sparse_cg']:","824","        with pytest.warns(None) as record:","825","            sparse.fit(X_csr, y)","826","        assert len(record) == 0","831","    for solver in ['saga', 'lsqr']:","832","        sparse = Ridge(alpha=1., tol=1.e-15, solver=solver, fit_intercept=True)","833","        assert_warns(UserWarning, sparse.fit, X_csr, y)","834","        assert_almost_equal(dense.intercept_, sparse.intercept_)","835","        assert_array_almost_equal(dense.coef_, sparse.coef_)"],"delete":["819","    for solver in ['saga', 'sag']:","823","        sparse.fit(X_csr, y)","828","    sparse = Ridge(alpha=1., tol=1.e-15, solver='lsqr', fit_intercept=True)","829","    assert_warns(UserWarning, sparse.fit, X_csr, y)","830","    assert_almost_equal(dense.intercept_, sparse.intercept_)","831","    assert_array_almost_equal(dense.coef_, sparse.coef_)"]}],"sklearn\/linear_model\/ridge.py":[{"add":["35","def _solve_sparse_cg(X, y, alpha, max_iter=None, tol=1e-3, verbose=0,","36","                     X_offset=None, X_scale=None):","37","","38","    def _get_rescaled_operator(X):","39","","40","        X_offset_scale = X_offset \/ X_scale","41","","42","        def matvec(b):","43","            return X.dot(b) - b.dot(X_offset_scale)","44","","45","        def rmatvec(b):","46","            return X.T.dot(b) - X_offset_scale * np.sum(b)","47","","48","        X1 = sparse.linalg.LinearOperator(shape=X.shape,","49","                                          matvec=matvec,","50","                                          rmatvec=rmatvec)","51","        return X1","52","","54","","55","    if X_offset is None or X_scale is None:","56","        X1 = sp_linalg.aslinearoperator(X)","57","    else:","58","        X1 = _get_rescaled_operator(X)","59","","350","","351","    return _ridge_regression(X, y, alpha,","352","                             sample_weight=sample_weight,","353","                             solver=solver,","354","                             max_iter=max_iter,","355","                             tol=tol,","356","                             verbose=verbose,","357","                             random_state=random_state,","358","                             return_n_iter=return_n_iter,","359","                             return_intercept=return_intercept,","360","                             X_scale=None,","361","                             X_offset=None)","362","","363","","364","def _ridge_regression(X, y, alpha, sample_weight=None, solver='auto',","365","                      max_iter=None, tol=1e-3, verbose=0, random_state=None,","366","                      return_n_iter=False, return_intercept=False,","367","                      X_scale=None, X_offset=None):","368","","438","        coef = _solve_sparse_cg(X, y, alpha,","439","                                max_iter=max_iter,","440","                                tol=tol,","441","                                verbose=verbose,","442","                                X_offset=X_offset,","443","                                X_scale=X_scale)","540","        # when X is sparse we only remove offset from y","543","            sample_weight=sample_weight, return_mean=True)","546","        if (sparse.issparse(X) and self.fit_intercept and","547","           self.solver != 'sparse_cg'):","548","            self.coef_, self.n_iter_, self.intercept_ = _ridge_regression(","553","            # add the offset which was subtracted by _preprocess_data","556","            if sparse.issparse(X):","557","                # required to fit intercept with sparse_cg solver","558","                params = {'X_offset': X_offset, 'X_scale': X_scale}","559","            else:","560","                # for dense matrices or when intercept is set to 0","561","                params = {}","562","","563","            self.coef_, self.n_iter_ = _ridge_regression(","567","                return_intercept=False, **params)","568",""],"delete":["35","def _solve_sparse_cg(X, y, alpha, max_iter=None, tol=1e-3, verbose=0):","37","    X1 = sp_linalg.aslinearoperator(X)","397","        coef = _solve_sparse_cg(X, y, alpha, max_iter, tol, verbose)","496","            sample_weight=sample_weight)","499","        if sparse.issparse(X) and self.fit_intercept:","500","            self.coef_, self.n_iter_, self.intercept_ = ridge_regression(","507","            self.coef_, self.n_iter_ = ridge_regression(","511","                return_intercept=False)"]}]}},"979c76129c912727c701c8706236a6db5590997c":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/calibration.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["228","- |Fix| Fixed a bug that made `calibration.CalibratedClassifierCV` fail when","229","  given a `sample_weight` parameter of type `list` (in the case where","230","  `sample_weights` are not supported by the wrapped estimator). :issue:`1375`","231","  by :user:`William de Vazelhes <wdevazelhes>`.","232",""],"delete":[]}],"sklearn\/calibration.py":[{"add":["20","from .base import (BaseEstimator, ClassifierMixin, RegressorMixin, clone,","21","                   MetaEstimatorMixin)","30","class CalibratedClassifierCV(BaseEstimator, ClassifierMixin,","31","                             MetaEstimatorMixin):","174","                sample_weight = check_array(sample_weight, ensure_2d=False)"],"delete":["20","from .base import BaseEstimator, ClassifierMixin, RegressorMixin, clone","29","class CalibratedClassifierCV(BaseEstimator, ClassifierMixin):"]}]}},"fd93ea03b1e53af4ca544405c2a5b3abfa9389e8":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/ensemble\/gradient_boosting.py":"MODIFY","doc\/whats_new\/_contributors.rst":"MODIFY","sklearn\/ensemble\/tests\/test_gradient_boosting.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["201","  :class:`ensemble.ExtraTreesRegressor`,","202","  :class:`ensemble.RandomTreesEmbedding`,","203","  :class:`ensemble.GradientBoostingClassifier`, and","204","  :class:`ensemble.GradientBoostingRegressor`) now:","210","  :issue:`13636` and :issue:`13620` by `Adrin Jalali`_."],"delete":["201","  :class:`ensemble.ExtraTreesRegressor`, and","202","  :class:`ensemble.RandomTreesEmbedding`) now:","208","  :issue:`13636` by `Adrin Jalali`_."]}],"sklearn\/ensemble\/gradient_boosting.py":[{"add":["1711","            The values of this array sum to 1, unless all trees are single node","1712","            trees consisting of only the root node, in which case it will be an","1713","            array of zeros.","1717","        relevant_trees = [tree","1718","                          for stage in self.estimators_ for tree in stage","1719","                          if tree.tree_.node_count > 1]","1720","        if not relevant_trees:","1721","            # degenerate case where all trees have only one node","1722","            return np.zeros(shape=self.n_features_, dtype=np.float64)","1724","        relevant_feature_importances = [","1725","            tree.tree_.compute_feature_importances(normalize=False)","1726","            for tree in relevant_trees","1727","        ]","1728","        avg_feature_importances = np.mean(relevant_feature_importances,","1729","                                          axis=0, dtype=np.float64)","1730","        return avg_feature_importances \/ np.sum(avg_feature_importances)"],"delete":["1714","        total_sum = np.zeros((self.n_features_, ), dtype=np.float64)","1715","        for stage in self.estimators_:","1716","            stage_sum = sum(tree.tree_.compute_feature_importances(","1717","                normalize=False) for tree in stage) \/ len(stage)","1718","            total_sum += stage_sum","1720","        importances = total_sum \/ total_sum.sum()","1721","        return importances"]}],"doc\/whats_new\/_contributors.rst":[{"add":["176",".. _Nicolas Hug: https:\/\/github.com\/NicolasHug"],"delete":["176",".. _Nicolas Hug: https:\/\/github.com\/NicolasHug"]}],"sklearn\/ensemble\/tests\/test_gradient_boosting.py":[{"add":["1442","","1443","","1444","def test_gbr_degenerate_feature_importances():","1445","    # growing an ensemble of single node trees. See #13620","1446","    X = np.zeros((10, 10))","1447","    y = np.ones((10,))","1448","    gbr = GradientBoostingRegressor().fit(X, y)","1449","    assert_array_equal(gbr.feature_importances_,","1450","                       np.zeros(10, dtype=np.float64))"],"delete":[]}]}},"79b549cfd9f1c62769da6c7d75f561cc6093323c":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/linear_model\/tests\/test_least_angle.py":"MODIFY","sklearn\/linear_model\/least_angle.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["167","- |Fix| Fixed a bug in :class:`linear_model.LassoLarsIC`, where user input","168","   ``copy_X=False`` at instance creation would be overridden by default","169","   parameter value ``copy_X=True`` in ``fit``. ","170","   :issue:`12972` by :user:`Lucio Fernandez-Arjona <luk-f-a>`","171",""],"delete":[]}],"sklearn\/linear_model\/tests\/test_least_angle.py":[{"add":["20","from sklearn.linear_model.least_angle import _lars_path_residues, LassoLarsIC","688","","689","","690","@pytest.mark.parametrize('copy_X', [True, False])","691","def test_lasso_lars_copyX_behaviour(copy_X):","692","    \"\"\"","693","    Test that user input regarding copy_X is not being overridden (it was until","694","    at least version 0.21)","695","","696","    \"\"\"","697","    lasso_lars = LassoLarsIC(copy_X=copy_X, precompute=False)","698","    rng = np.random.RandomState(0)","699","    X = rng.normal(0, 1, (100, 5))","700","    X_copy = X.copy()","701","    y = X[:, 2]","702","    lasso_lars.fit(X, y)","703","    assert copy_X == np.array_equal(X, X_copy)","704","","705","","706","@pytest.mark.parametrize('copy_X', [True, False])","707","def test_lasso_lars_fit_copyX_behaviour(copy_X):","708","    \"\"\"","709","    Test that user input to .fit for copy_X overrides default __init__ value","710","","711","    \"\"\"","712","    lasso_lars = LassoLarsIC(precompute=False)","713","    rng = np.random.RandomState(0)","714","    X = rng.normal(0, 1, (100, 5))","715","    X_copy = X.copy()","716","    y = X[:, 2]","717","    lasso_lars.fit(X, y, copy_X=copy_X)","718","    assert copy_X == np.array_equal(X, X_copy)"],"delete":["20","from sklearn.linear_model.least_angle import _lars_path_residues"]}],"sklearn\/linear_model\/least_angle.py":[{"add":["1479","    def fit(self, X, y, copy_X=None):","1490","        copy_X : boolean, optional, default None","1491","            If provided, this parameter will override the choice","1492","            of copy_X made at instance creation.","1500","        if copy_X is None:","1501","            copy_X = self.copy_X","1505","            X, y, self.fit_intercept, self.normalize, copy_X)"],"delete":["1479","    def fit(self, X, y, copy_X=True):","1490","        copy_X : boolean, optional, default True","1501","            X, y, self.fit_intercept, self.normalize, self.copy_X)"]}]}},"314686a65d543bd3b36d2af4b34ed23711991a57":{"changes":{"sklearn\/linear_model\/coordinate_descent.py":"MODIFY"},"diff":{"sklearn\/linear_model\/coordinate_descent.py":[{"add":["1556","    0.199..."],"delete":["1556","    0.1994727942696716"]}]}},"afc6cc58da7b8b45b845443ed54e75de5017087c":{"changes":{"sklearn\/model_selection\/_split.py":"MODIFY","doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/model_selection\/tests\/test_split.py":"MODIFY"},"diff":{"sklearn\/model_selection\/_split.py":[{"add":["578","        Whether to shuffle each class's samples before splitting into batches.","621","        rng = check_random_state(self.random_state)"],"delete":["578","        Whether to shuffle each stratification of the data before splitting","579","        into batches.","622","        rng = self.random_state"]}],"doc\/whats_new\/v0.21.rst":[{"add":["269","- |Fix| Fixed a bug where :class:`model_selection.StratifiedKFold`","270","  shuffles each class's samples with the same ``random_state``,","271","  making ``shuffle=True`` ineffective.","272","  :issue:`13124` by :user:`Hanmin Qin <qinhanmin2014>`.","273",""],"delete":[]}],"sklearn\/model_selection\/tests\/test_split.py":[{"add":["495","    # Ensure that we shuffle each class's samples with different","496","    # random_state in StratifiedKFold","497","    # See https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/13124","498","    X = np.arange(10)","499","    y = [0] * 5 + [1] * 5","500","    kf1 = StratifiedKFold(5, shuffle=True, random_state=0)","501","    kf2 = StratifiedKFold(5, shuffle=True, random_state=1)","502","    test_set1 = sorted([tuple(s[1]) for s in kf1.split(X, y)])","503","    test_set2 = sorted([tuple(s[1]) for s in kf2.split(X, y)])","504","    assert test_set1 != test_set2","505",""],"delete":[]}]}},"ce9dedb1a37eedc64d9294beafaf4abc6b9970d6":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/compose\/tests\/test_column_transformer.py":"MODIFY","sklearn\/compose\/_column_transformer.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["24",":mod:`sklearn.compose`","25","......................","26","","27","- |Fix| Fixed a bug in :class:`compose.ColumnTransformer` to handle","28","  negative indexes in the columns list of the transformers.","29","  :issue:`12946` by :user:`Pierre Tallotte <pierretallotte>`.","30","","32","............................"],"delete":["25","..........................."]}],"sklearn\/compose\/tests\/test_column_transformer.py":[{"add":["1021","","1022","","1023","def test_column_transformer_negative_column_indexes():","1024","    X = np.random.randn(2, 2)","1025","    X_categories = np.array([[1], [2]])","1026","    X = np.concatenate([X, X_categories], axis=1)","1027","","1028","    ohe = OneHotEncoder(categories='auto')","1029","","1030","    tf_1 = ColumnTransformer([('ohe', ohe, [-1])], remainder='passthrough')","1031","    tf_2 = ColumnTransformer([('ohe', ohe,  [2])], remainder='passthrough')","1032","    assert_array_equal(tf_1.fit_transform(X), tf_2.fit_transform(X))"],"delete":[]}],"sklearn\/compose\/_column_transformer.py":[{"add":["630","    if (_check_key_type(key, int)","631","            or hasattr(key, 'dtype') and np.issubdtype(key.dtype, np.bool_)):","632","        # Convert key into positive indexes","633","        idx = np.arange(n_columns)[key]","634","        return np.atleast_1d(idx).tolist()"],"delete":["630","    if _check_key_type(key, int):","631","        if isinstance(key, int):","632","            return [key]","633","        elif isinstance(key, slice):","634","            return list(range(n_columns)[key])","635","        else:","636","            return list(key)","637","","660","","661","    elif hasattr(key, 'dtype') and np.issubdtype(key.dtype, np.bool_):","662","        # boolean mask","663","        return list(np.arange(n_columns)[key])"]}]}},"dc5e4d8896cb61e802da9dc845fbc873ddf5de3e":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/utils\/sparsefuncs.py":"MODIFY","sklearn\/utils\/tests\/test_sparsefuncs.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["35",":mod:`sklearn.utils.sparsefuncs`","36","................................","37","","38","- |Fix| Fixed a bug where :func:`min_max_axis` would fail on 32-bit systems","39","  for certain large inputs. This affects :class:`preprocessing.MaxAbsScaler`, ","40","  :func:`preprocessing.normalize` and :class:`preprocessing.LabelBinarizer`.","41","  :pr:`13741` by :user:`Roddy MacSween <rlms>`.","42",""],"delete":[]}],"sklearn\/utils\/sparsefuncs.py":[{"add":["343","","344","    # reduceat tries casts X.indptr to intp, which errors","345","    # if it is int64 on a 32 bit system.","346","    # Reinitializing prevents this where possible, see #13737","347","    X = type(X)((X.data, X.indices, X.indptr), shape=X.shape)"],"delete":[]}],"sklearn\/utils\/tests\/test_sparsefuncs.py":[{"add":["395","@pytest.mark.parametrize(\"large_indices\", [True, False])","397","                 max_func, ignore_nan, large_indices):","404","    if large_indices:","405","        X_sparse.indices = X_sparse.indices.astype('int64')","406","        X_sparse.indptr = X_sparse.indptr.astype('int64')"],"delete":["396","                 max_func, ignore_nan):"]}]}},"5307b2df7fd378269a2b2d5076dbefde46367083":{"changes":{"sklearn\/externals\/_arff.py":"MODIFY"},"diff":{"sklearn\/externals\/_arff.py":[{"add":["30","(LIAC), which takes place at the Federal University of Rio Grande do Sul","35","softwares. This file format was created to be used in Weka, the best","38","An ARFF file can be divided into two sections: header and data. The Header","39","describes the metadata of the dataset, including a general description of the","40","dataset, its name and its attributes. The source below is an example of a","43","    %","45","    %","49","    %","50","    %","57","The Data section of an ARFF file describes the observations of the dataset, in","65","    %","66","    %","67","    %","69","Notice that several lines are starting with an ``%`` symbol, denoting a","71","description part at the beginning of the file. The declarations ``@RELATION``,","75","http:\/\/www.cs.waikato.ac.nz\/~ml\/weka\/arff.html","81","This module uses built-ins python objects to represent a deserialized ARFF","87","- **attributes**: (OBLIGATORY) a list of attributes with the following","94","- **data**: (OBLIGATORY) a list of data instances. Each data instance must be","97","The above keys must follow the case which were described, i.e., the keys are","98","case sensitive. The attribute type ``attribute_type`` must be one of these","99","strings (they are not case sensitive): ``NUMERIC``, ``INTEGER``, ``REAL`` or","100","``STRING``. For nominal attributes, the ``atribute_type`` must be a list of","103","In this format, the XOR dataset presented above can be represented as a python","130","- Supports `scipy.sparse.coo <http:\/\/docs.scipy","135","- Has an interface similar to other built-in modules such as ``json``, or","141","- Under `MIT License <http:\/\/opensource.org\/licenses\/MIT>`_","148","__version__ = '2.4.0'","166","_RE_QUOTE_CHARS = re.compile(r'[\"\\'\\\\\\s%,\\000-\\031]', re.UNICODE)","167","_RE_ESCAPE_CHARS = re.compile(r'(?=[\"\\'\\\\%])|[\\n\\r\\t\\000-\\031]')","168","_RE_SPARSE_LINE = re.compile(r'^\\s*\\{.*\\}\\s*$', re.UNICODE)","169","_RE_NONTRIVIAL_DATA = re.compile('[\"\\'{}\\\\s]', re.UNICODE)","225","_ESCAPE_SUB_MAP = {","226","    '\\\\\\\\': '\\\\',","227","    '\\\\\"': '\"',","228","    \"\\\\'\": \"'\",","229","    '\\\\t': '\\t',","230","    '\\\\n': '\\n',","231","    '\\\\r': '\\r',","232","    '\\\\b': '\\b',","233","    '\\\\f': '\\f',","234","    '\\\\%': '%',","235","}","236","_UNESCAPE_SUB_MAP = {chr(i): '\\\\%03o' % i for i in range(32)}","237","_UNESCAPE_SUB_MAP.update({v: k for k, v in _ESCAPE_SUB_MAP.items()})","238","_UNESCAPE_SUB_MAP[''] = '\\\\'","239","_ESCAPE_SUB_MAP.update({'\\\\%d' % i: chr(i) for i in range(10)})","240","","241","","242","def _escape_sub_callback(match):","243","    s = match.group()","244","    if len(s) == 2:","245","        try:","246","            return _ESCAPE_SUB_MAP[s]","247","        except KeyError:","248","            raise ValueError('Unsupported escape sequence: %s' % s)","249","    if s[1] == 'u':","250","        return unichr(int(s[2:], 16))","251","    else:","252","        return chr(int(s[1:], 8))","253","","254","","257","        return re.sub(r'\\\\([0-9]{1,3}|u[0-9a-f]{4}|.)', _escape_sub_callback,","258","                      v[1:-1])","295","DENSE = 0     # Constant value representing a dense matrix","296","COO = 1       # Constant value representing a sparse matrix in coordinate format","297","LOD = 2       # Constant value representing a sparse matrix in list of","298","              # dictionaries format","299","DENSE_GEN = 3 # Generator of dictionaries","300","LOD_GEN = 4   # Generator of dictionaries","301","_SUPPORTED_DATA_STRUCTURES = [DENSE, COO, LOD, DENSE_GEN, LOD_GEN]","311","    unichr = chr","346","    '''Error raised when some invalid type is provided into the attribute","363","    '''Error raised when a value in used in some data instance but is not","383","    '''Error raised when and invalid numerical value is used in some data","402","    '''Error raised when the object representing the ARFF file has something","404","    def __init__(self, msg='Invalid object.'):","408","        return '%s' % self.msg","409","","413","def _unescape_sub_callback(match):","414","    return _UNESCAPE_SUB_MAP[match.group()]","415","","416","","419","        return u\"'%s'\" % _RE_ESCAPE_CHARS.sub(_unescape_sub_callback, s)","452","class DenseGeneratorData(object):","456","    def decode_rows(self, stream, conversors):","457","        for row in stream:","458","            values = _parse_values(row)","460","            if isinstance(values, dict):","461","                if values and max(values) >= len(conversors):","462","                    raise BadDataFormat(row)","463","                # XXX: int 0 is used for implicit values, not '0'","464","                values = [values[i] if i in values else 0 for i in","465","                          xrange(len(conversors))]","466","            else:","467","                if len(values) != len(conversors):","468","                    raise BadDataFormat(row)","470","            yield self._decode_values(values, conversors)","514","class _DataListMixin(object):","515","    \"\"\"Mixin to return a list from decode_rows instead of a generator\"\"\"","516","    def decode_rows(self, stream, conversors):","517","        return list(super(_DataListMixin, self).decode_rows(stream, conversors))","520","class Data(_DataListMixin, DenseGeneratorData):","521","    pass","522","","523","","524","class COOData(object):","525","    def decode_rows(self, stream, conversors):","526","        data, rows, cols = [], [], []","527","        for i, row in enumerate(stream):","528","            values = _parse_values(row)","529","            if not isinstance(values, dict):","530","                raise BadLayout()","531","            if not values:","532","                continue","533","            row_cols, values = zip(*sorted(values.items()))","534","            try:","535","                values = [value if value is None else conversors[key](value)","536","                          for key, value in zip(row_cols, values)]","537","            except ValueError as exc:","538","                if 'float: ' in str(exc):","539","                    raise BadNumericalValue()","540","                raise","541","            except IndexError:","542","                # conversor out of range","543","                raise BadDataFormat(row)","544","","545","            data.extend(values)","546","            rows.extend([i] * len(values))","547","            cols.extend(row_cols)","548","","549","        return data, rows, cols","587","class LODGeneratorData(object):","588","    def decode_rows(self, stream, conversors):","589","        for row in stream:","590","            values = _parse_values(row)","592","            if not isinstance(values, dict):","593","                raise BadLayout()","594","            try:","595","                yield {key: None if value is None else conversors[key](value)","596","                       for key, value in values.items()}","597","            except ValueError as exc:","598","                if 'float: ' in str(exc):","599","                    raise BadNumericalValue()","600","                raise","601","            except IndexError:","602","                # conversor out of range","603","                raise BadDataFormat(row)","629","class LODData(_DataListMixin, LODGeneratorData):","630","    pass","631","","632","","640","    elif matrix_type == DENSE_GEN:","641","        return DenseGeneratorData()","642","    elif matrix_type == LOD_GEN:","643","        return LODGeneratorData()","678","        padding, including the \"\\r\\n\" characters.","689","        The relation declaration is a line with the format ``@RELATION","695","        padding, including the \"\\r\\n\" characters.","712","        The attribute is the most complex declaration in an arff file. All","717","        where ``attribute-name`` is a string, quoted if the name contains any","725","            {<nominal-name1>, <nominal-name2>, <nominal-name3>, ...}","731","        padding, including the \"\\r\\n\" characters.","791","        s = iter(s)","848","                break","855","        else:","856","            # Never found @DATA","857","            raise BadLayout()","859","        def stream():","860","            for row in s:","861","                self._current_line += 1","862","                row = row.strip()","863","                # Ignore empty lines and comment lines.","864","                if row and not row.startswith(_TK_COMMENT):","865","                    yield row","868","        obj['data'] = data.decode_rows(stream(), self._conversors)","884","            dataset. Can be one of `arff.DENSE`, `arff.COO`, `arff.LOD`,","885","            `arff.DENSE_GEN` or `arff.LOD_GEN`.","886","            Consult the sections on `working with sparse data`_ and `loading","887","            progressively`_.","920","        The relation declaration is a line with the format ``@RELATION","921","        <relation-name>``, where ``relation-name`` is a string.","947","            {<nominal-name1>, <nominal-name2>, <nominal-name3>, ...}","962","            type_tmp = [u'%s' % encode_string(type_k) for type_k in type_]","980","        This encodes iteratively a given object and return, one-by-one, the","1044","    a Python object.","1050","        dataset. Can be one of `arff.DENSE`, `arff.COO`, `arff.LOD`,","1051","        `arff.DENSE_GEN` or `arff.LOD_GEN`.","1052","        Consult the sections on `working with sparse data`_ and `loading","1053","        progressively`_.","1068","        dataset. Can be one of `arff.DENSE`, `arff.COO`, `arff.LOD`,","1069","        `arff.DENSE_GEN` or `arff.LOD_GEN`.","1070","        Consult the sections on `working with sparse data`_ and `loading","1071","        progressively`_.","1079","    '''Serialize an object representing the ARFF document to a given file-like"],"delete":["30","(LIAC), which takes place at the Federal University of Rio Grande do Sul ","35","softwares. This file format was created to be used in Weka, the best ","38","An ARFF file can be divided into two sections: header and data. The Header ","39","describes the metadata of the dataset, including a general description of the ","40","dataset, its name and its attributes. The source below is an example of a ","43","    % ","45","    % ","49","    % ","50","    % ","57","The Data section of an ARFF file describes the observations of the dataset, in ","65","    % ","66","    % ","67","    % ","69","Notice that several lines are starting with an ``%`` symbol, denoting a ","71","description part at the beginning of the file. The declarations ``@RELATION``, ","75","https:\/\/www.cs.waikato.ac.nz\/~ml\/weka\/arff.html","81","This module uses built-ins python objects to represent a deserialized ARFF ","87","- **attributes**: (OBLIGATORY) a list of attributes with the following ","94","- **data**: (OBLIGATORY) a list of data instances. Each data instance must be ","97","The above keys must follow the case which were described, i.e., the keys are ","98","case sensitive. The attribute type ``attribute_type`` must be one of these ","99","strings (they are not case sensitive): ``NUMERIC``, ``INTEGER``, ``REAL`` or ","100","``STRING``. For nominal attributes, the ``atribute_type`` must be a list of ","103","In this format, the XOR dataset presented above can be represented as a python ","130","- Supports `scipy.sparse.coo <https:\/\/docs.scipy","135","- Has an interface similar to other built-in modules such as ``json``, or ","141","- Under `MIT License <https:\/\/opensource.org\/licenses\/MIT>`_","148","__version__ = '2.3.1'","166","_RE_QUOTE_CHARS = re.compile(r'[\"\\'\\\\ \\t%,]')","167","_RE_ESCAPE_CHARS = re.compile(r'(?=[\"\\'\\\\%])')  # don't need to capture anything","168","_RE_SPARSE_LINE = re.compile(r'^\\{.*\\}$')","169","_RE_NONTRIVIAL_DATA = re.compile('[\"\\'{}\\\\s]')","227","        return re.sub(r'\\\\(.)', r'\\1', v[1:-1])","264","DENSE = 0   # Constant value representing a dense matrix","265","COO = 1     # Constant value representing a sparse matrix in coordinate format","266","LOD = 2     # Constant value representing a sparse matrix in list of","267","            # dictionaries format","268","_SUPPORTED_DATA_STRUCTURES = [DENSE, COO, LOD]","312","    '''Error raised when some invalid type is provided into the attribute ","329","    '''Error raised when a value in used in some data instance but is not ","349","    '''Error raised when and invalid numerical value is used in some data ","366","class BadObject(ArffException):","367","    '''Error raised when the object representing the ARFF file has something ","368","    wrong.'''","369","","370","    def __str__(self):","371","        return 'Invalid object.'","374","    '''Error raised when the object representing the ARFF file has something ","376","    def __init__(self, msg=''):","380","        return '%s'%self.msg","386","        return u\"'%s'\" % _RE_ESCAPE_CHARS.sub(r'\\\\', s)","419","class Data(object):","422","    def __init__(self):","423","        self.data = []","425","    def decode_data(self, s, conversors):","426","        values = _parse_values(s)","428","        if isinstance(values, dict):","429","            if max(values) >= len(conversors):","430","                raise BadDataFormat(s)","431","            # XXX: int 0 is used for implicit values, not '0'","432","            values = [values[i] if i in values else 0 for i in","433","                      xrange(len(conversors))]","434","        else:","435","            if len(values) != len(conversors):","436","                raise BadDataFormat(s)","438","        self.data.append(self._decode_values(values, conversors))","451","    def _tuplify_sparse_data(self, x):","452","        if len(x) != 2:","453","            raise BadDataFormat(x)","454","        return (int(x[0].strip('\"').strip(\"'\")), x[1])","455","","486","class COOData(Data):","487","    def __init__(self):","488","        self.data = ([], [], [])","489","        self._current_num_data_points = 0","491","    def decode_data(self, s, conversors):","492","        values = _parse_values(s)","494","        if not isinstance(values, dict):","495","            raise BadLayout()","496","        if not values:","497","            self._current_num_data_points += 1","498","            return","499","        col, values = zip(*sorted(values.items()))","500","        try:","501","            values = [value if value is None else conversors[key](value)","502","                      for key, value in zip(col, values)]","503","        except ValueError as exc:","504","            if 'float: ' in str(exc):","505","                raise BadNumericalValue()","506","            raise","507","        except IndexError:","508","            # conversor out of range","509","            raise BadDataFormat(s)","510","        self.data[0].extend(values)","511","        self.data[1].extend([self._current_num_data_points] * len(values))","512","        self.data[2].extend(col)","514","        self._current_num_data_points += 1","552","class LODData(Data):","553","    def __init__(self):","554","        self.data = []","556","    def decode_data(self, s, conversors):","557","        values = _parse_values(s)","558","        n_conversors = len(conversors)","559","","560","        if not isinstance(values, dict):","561","            raise BadLayout()","562","        try:","563","            self.data.append({key: None if value is None else conversors[key](value)","564","                              for key, value in values.items()})","565","        except ValueError as exc:","566","            if 'float: ' in str(exc):","567","                raise BadNumericalValue()","568","            raise","569","        except IndexError:","570","            # conversor out of range","571","            raise BadDataFormat(s)","638","        padding, including the \"\\r\\n\" characters. ","649","        The relation declaration is a line with the format ``@RELATION ","655","        padding, including the \"\\r\\n\" characters. ","672","        The attribute is the most complex declaration in an arff file. All ","677","        where ``attribute-name`` is a string, quoted if the name contains any ","685","            {<nominal-name1>, <nominal-name2>, <nominal-name3>, ...} ","691","        padding, including the \"\\r\\n\" characters. ","807","                STATE = _TK_DATA","815","            # DATA INSTANCES --------------------------------------------------","816","            elif STATE == _TK_DATA:","817","                data.decode_data(row, self._conversors)","818","            # -----------------------------------------------------------------","819","","820","            # UNKNOWN INFORMATION ---------------------------------------------","821","            else:","822","                raise BadLayout()","823","            # -----------------------------------------------------------------","826","        obj['data'] = data.data","842","            dataset. Can be one of `arff.DENSE`, `arff.COO` and `arff.LOD`.","843","            Consult the section on `working with sparse data`_","876","        The relation declaration is a line with the format ``@RELATION ","877","        <relation-name>``, where ``relation-name`` is a string. ","903","            {<nominal-name1>, <nominal-name2>, <nominal-name3>, ...} ","918","            type_tmp = []","919","            for i in range(len(type_)):","920","                type_tmp.append(u'%s' % encode_string(type_[i]))","938","        This encodes iteratively a given object and return, one-by-one, the ","1002","    a Python object. ","1008","        dataset. Can be one of `arff.DENSE`, `arff.COO` and `arff.LOD`.","1009","        Consult the section on `working with sparse data`_","1024","        dataset. Can be one of `arff.DENSE`, `arff.COO` and `arff.LOD`.","1025","        Consult the section on `working with sparse data`_","1033","    '''Serialize an object representing the ARFF document to a given file-like "]}]}},"ea63c56638aa6e0b987afe742c77d1e18007b768":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/ensemble\/gradient_boosting.py":"MODIFY","sklearn\/ensemble\/_gb_losses.py":"ADD","sklearn\/ensemble\/tests\/test_gradient_boosting.py":"MODIFY","sklearn\/ensemble\/tests\/test_gradient_boosting_loss_functions.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["130","- |Fix| Fixed a bug in :class:`ensemble.GradientBoostingClassifier` and","131","  :class:`ensemble.GradientBoostingRegressor`, which didn't support","132","  scikit-learn estimators as the initial estimator. Also added support of","133","  initial estimator which does not support sample weights. :issue:`12436` by","134","  :user:`J¨¦r¨¦mie du Boisberranger <jeremiedbb>` and :issue:`12983` by","135","  :user:`Nicolas Hug<NicolasHug>`.","136","","146","- |Fix| Fixed a bug in :class:`ensemble.GradientBoostingClassifier` where","147","  the default initial prediction of a multiclass classifier would predict the","148","  classes priors instead of the log of the priors. :issue:`12983` by","149","  :user:`Nicolas Hug<NicolasHug>`.","150",""],"delete":[]}],"sklearn\/ensemble\/gradient_boosting.py":[{"add":["28","from ..base import BaseEstimator","29","from ..base import is_classifier","48","from . import _gb_losses","63","# FIXME: 0.23","64","# All the losses and corresponding init estimators have been moved to the","65","# _losses module in 0.21. We deprecate them and keep them here for now in case","66","# someone has imported them. None of these losses can be used as a parameter","67","# to a GBDT estimator anyway (loss param only accepts strings).","68","","69","@deprecated(\"QuantileEstimator is deprecated in version \"","70","            \"0.21 and will be removed in version 0.23.\")","124","@deprecated(\"MeanEstimator is deprecated in version \"","125","            \"0.21 and will be removed in version 0.23.\")","167","@deprecated(\"LogOddsEstimator is deprecated in version \"","168","            \"0.21 and will be removed in version 0.23.\")","219","@deprecated(\"ScaledLogOddsEstimator is deprecated in version \"","220","            \"0.21 and will be removed in version 0.23.\")","226","@deprecated(\"PriorProbablityEstimator is deprecated in version \"","227","            \"0.21 and will be removed in version 0.23.\")","271","@deprecated(\"Using ZeroEstimator is deprecated in version \"","272","            \"0.21 and will be removed in version 0.23.\")","274","    \"\"\"An estimator that simply predicts zero.","275","","276","    .. deprecated:: 0.21","277","        Using ``ZeroEstimator`` or ``init='zero'`` is deprecated in version","278","        0.21 and will be removed in version 0.23.","279","","280","    \"\"\"","324","    def predict_proba(self, X):","325","        return self.predict(X)","327","","328","@deprecated(\"All Losses in sklearn.ensemble.gradient_boosting are \"","329","            \"deprecated in version \"","330","            \"0.21 and will be removed in version 0.23.\")","438","@deprecated(\"All Losses in sklearn.ensemble.gradient_boosting are \"","439","            \"deprecated in version \"","440","            \"0.21 and will be removed in version 0.23.\")","456","@deprecated(\"All Losses in sklearn.ensemble.gradient_boosting are \"","457","            \"deprecated in version \"","458","            \"0.21 and will be removed in version 0.23.\")","542","@deprecated(\"All Losses in sklearn.ensemble.gradient_boosting are \"","543","            \"deprecated in version \"","544","            \"0.21 and will be removed in version 0.23.\")","601","@deprecated(\"All Losses in sklearn.ensemble.gradient_boosting are \"","602","            \"deprecated in version \"","603","            \"0.21 and will be removed in version 0.23.\")","707","@deprecated(\"All Losses in sklearn.ensemble.gradient_boosting are \"","708","            \"deprecated in version \"","709","            \"0.21 and will be removed in version 0.23.\")","787","@deprecated(\"All Losses in sklearn.ensemble.gradient_boosting are \"","788","            \"deprecated in version \"","789","            \"0.21 and will be removed in version 0.23.\")","808","@deprecated(\"All Losses in sklearn.ensemble.gradient_boosting are \"","809","            \"deprecated in version \"","810","            \"0.21 and will be removed in version 0.23.\")","902","@deprecated(\"All Losses in sklearn.ensemble.gradient_boosting are \"","903","            \"deprecated in version \"","904","            \"0.21 and will be removed in version 0.23.\")","1000","@deprecated(\"All Losses in sklearn.ensemble.gradient_boosting are \"","1001","            \"deprecated in version \"","1002","            \"0.21 and will be removed in version 0.23.\")","1090","class VerboseReporter(object):","1201","    def _fit_stage(self, i, X, y, raw_predictions, sample_weight, sample_mask,","1209","        # Need to pass a copy of raw_predictions to negative_gradient()","1210","        # because raw_predictions is partially updated at the end of the loop","1211","        # in update_terminal_regions(), and gradients need to be evaluated at","1213","        raw_predictions_copy = raw_predictions.copy()","1219","            residual = loss.negative_gradient(y, raw_predictions_copy, k=k,","1246","            loss.update_terminal_regions(","1247","                tree.tree_, X, y, residual, raw_predictions, sample_weight,","1248","                sample_mask, learning_rate=self.learning_rate, k=k)","1253","        return raw_predictions","1266","                or self.loss not in _gb_losses.LOSS_FUNCTIONS):","1270","            loss_class = (_gb_losses.MultinomialDeviance","1272","                          else _gb_losses.BinomialDeviance)","1274","            loss_class = _gb_losses.LOSS_FUNCTIONS[self.loss]","1286","            # init must be an estimator or 'zero'","1287","            if isinstance(self.init, BaseEstimator):","1288","                self.loss_.check_init_estimator(self.init)","1289","            elif not (isinstance(self.init, str) and self.init == 'zero'):","1290","                raise ValueError(","1291","                    \"The init parameter must be an estimator or 'zero'. \"","1292","                    \"Got init={}\".format(self.init)","1293","                )","1342","        self.init_ = self.init","1343","        if self.init_ is None:","1442","","1443","        sample_weight_is_none = sample_weight is None","1444","        if sample_weight_is_none:","1448","            sample_weight_is_none = False","1459","            if is_classifier(self):","1460","                if self.n_classes_ != np.unique(y).shape[0]:","1461","                    # We choose to error here. The problem is that the init","1462","                    # estimator would be trained on y, which has some missing","1463","                    # classes now, so its predictions would not have the","1464","                    # correct shape.","1465","                    raise ValueError(","1466","                        'The training data after the early stopping split '","1467","                        'is missing some classes. Try using another random '","1468","                        'seed.'","1469","                    )","1479","            # fit initial model and initialize raw predictions","1480","            if self.init_ == 'zero':","1481","                raw_predictions = np.zeros(shape=(X.shape[0], self.loss_.K),","1482","                                           dtype=np.float64)","1483","            else:","1484","                try:","1485","                    self.init_.fit(X, y, sample_weight=sample_weight)","1486","                except TypeError:","1487","                    if sample_weight_is_none:","1488","                        self.init_.fit(X, y)","1489","                    else:","1490","                        raise ValueError(","1491","                            \"The initial estimator {} does not support sample \"","1492","                            \"weights.\".format(self.init_.__class__.__name__))","1494","                raw_predictions = \\","1495","                    self.loss_.get_init_raw_predictions(X, self.init_)","1496","","1497","","1517","            raw_predictions = self._raw_predict(X)","1536","        n_stages = self._fit_stages(","1537","            X, y, raw_predictions, sample_weight, self._rng, X_val, y_val,","1538","            sample_weight_val, begin_at_stage, monitor, X_idx_sorted)","1550","    def _fit_stages(self, X, y, raw_predictions, sample_weight, random_state,","1584","            y_val_pred_iter = self._staged_raw_predict(X_val)","1596","                                      raw_predictions[~sample_mask],","1600","            raw_predictions = self._fit_stage(","1601","                i, X, y, raw_predictions, sample_weight, sample_mask,","1602","                random_state, X_idx_sorted, X_csc, X_csr)","1607","                                             raw_predictions[sample_mask],","1611","                                          raw_predictions[~sample_mask],","1615","                self.train_score_[i] = loss_(y, raw_predictions, sample_weight)","1646","    def _raw_predict_init(self, X):","1647","        \"\"\"Check input and compute raw predictions of the init estimtor.\"\"\"","1653","        if self.init_ == 'zero':","1654","            raw_predictions = np.zeros(shape=(X.shape[0], self.loss_.K),","1655","                                       dtype=np.float64)","1656","        else:","1657","            raw_predictions = self.loss_.get_init_raw_predictions(","1658","                X, self.init_).astype(np.float64)","1659","        return raw_predictions","1661","    def _raw_predict(self, X):","1662","        \"\"\"Return the sum of the trees raw predictions (+ init estimator).\"\"\"","1663","        raw_predictions = self._raw_predict_init(X)","1664","        predict_stages(self.estimators_, X, self.learning_rate,","1665","                       raw_predictions)","1666","        return raw_predictions","1668","    def _staged_raw_predict(self, X):","1669","        \"\"\"Compute raw predictions of ``X`` for each iteration.","1683","        raw_predictions : generator of array, shape (n_samples, k)","1684","            The raw predictions of the input samples. The order of the","1690","        raw_predictions = self._raw_predict_init(X)","1692","            predict_stage(self.estimators_, i, X, self.learning_rate,","1693","                          raw_predictions)","1694","            yield raw_predictions.copy()","1872","    init : estimator or 'zero', optional (default=None)","1873","        An estimator object that is used to compute the initial predictions.","1874","        ``init`` has to provide `fit` and `predict_proba`. If 'zero', the","1875","        initial raw predictions are set to zero. By default, a","1876","        ``DummyEstimator`` predicting the classes priors is used.","2064","            The decision function of the input samples, which corresponds to","2065","            the raw values predicted from the trees of the ensemble . The","2066","            order of the classes corresponds to that in the attribute","2067","            `classes_`. Regression and binary classification produce an","2068","            array of shape [n_samples].","2071","        raw_predictions = self._raw_predict(X)","2072","        if raw_predictions.shape[1] == 1:","2073","            return raw_predictions.ravel()","2074","        return raw_predictions","2092","            The decision function of the input samples, which corresponds to","2093","            the raw values predicted from the trees of the ensemble . The","2098","        yield from self._staged_raw_predict(X)","2115","        raw_predictions = self.decision_function(X)","2116","        encoded_labels = \\","2117","            self.loss_._raw_prediction_to_decision(raw_predictions)","2118","        return self.classes_.take(encoded_labels, axis=0)","2138","        for raw_predictions in self._staged_raw_predict(X):","2139","            encoded_labels = \\","2140","                self.loss_._raw_prediction_to_decision(raw_predictions)","2141","            yield self.classes_.take(encoded_labels, axis=0)","2164","        raw_predictions = self.decision_function(X)","2166","            return self.loss_._raw_prediction_to_proba(raw_predictions)","2216","            for raw_predictions in self._staged_raw_predict(X):","2217","                yield self.loss_._raw_prediction_to_proba(raw_predictions)","2335","    init : estimator or 'zero', optional (default=None)","2336","        An estimator object that is used to compute the initial predictions.","2337","        ``init`` has to provide `fit` and `predict`. If 'zero', the initial","2338","        raw predictions are set to zero. By default a ``DummyEstimator`` is","2339","        used, predicting either the average target value (for loss='ls'), or","2340","        a quantile for the other losses.","2512","        # In regression we can directly return the raw value from the trees.","2513","        return self._raw_predict(X).ravel()","2533","        for raw_predictions in self._staged_raw_predict(X):","2534","            yield raw_predictions.ravel()"],"delete":["253","    \"\"\"An estimator that simply predicts zero. \"\"\"","1030","LOSS_FUNCTIONS = {'ls': LeastSquaresError,","1031","                  'lad': LeastAbsoluteError,","1032","                  'huber': HuberLossFunction,","1033","                  'quantile': QuantileLossFunction,","1034","                  'deviance': None,    # for both, multinomial and binomial","1035","                  'exponential': ExponentialLoss,","1036","                  }","1037","","1038","","1039","INIT_ESTIMATORS = {'zero': ZeroEstimator}","1040","","1041","","1042","class VerboseReporter:","1153","    def _fit_stage(self, i, X, y, y_pred, sample_weight, sample_mask,","1161","        # Need to pass a copy of y_pred to negative_gradient() because y_pred","1162","        # is partially updated at the end of the loop in","1163","        # update_terminal_regions(), and gradients need to be evaluated at","1165","        y_pred_copy = y_pred.copy()","1171","            residual = loss.negative_gradient(y, y_pred_copy, k=k,","1198","            loss.update_terminal_regions(tree.tree_, X, y, residual, y_pred,","1199","                                         sample_weight, sample_mask,","1200","                                         learning_rate=self.learning_rate, k=k)","1205","        return y_pred","1218","                or self.loss not in LOSS_FUNCTIONS):","1222","            loss_class = (MultinomialDeviance","1224","                          else BinomialDeviance)","1226","            loss_class = LOSS_FUNCTIONS[self.loss]","1238","            if isinstance(self.init, str):","1239","                if self.init not in INIT_ESTIMATORS:","1240","                    raise ValueError('init=\"%s\" is not supported' % self.init)","1241","            else:","1242","                if (not hasattr(self.init, 'fit')","1243","                        or not hasattr(self.init, 'predict')):","1244","                    raise ValueError(\"init=%r must be valid BaseEstimator \"","1245","                                     \"and support both fit and \"","1246","                                     \"predict\" % self.init)","1295","        if self.init is None:","1297","        elif isinstance(self.init, str):","1298","            self.init_ = INIT_ESTIMATORS[self.init]()","1299","        else:","1300","            self.init_ = self.init","1398","        if sample_weight is None:","1421","            # fit initial model - FIXME make sample_weight optional","1422","            self.init_.fit(X, y, sample_weight)","1424","            # init predictions","1425","            y_pred = self.init_.predict(X)","1445","            y_pred = self._decision_function(X)","1464","        n_stages = self._fit_stages(X, y, y_pred, sample_weight, self._rng,","1465","                                    X_val, y_val, sample_weight_val,","1466","                                    begin_at_stage, monitor, X_idx_sorted)","1478","    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,","1512","            y_val_pred_iter = self._staged_decision_function(X_val)","1524","                                      y_pred[~sample_mask],","1528","            y_pred = self._fit_stage(i, X, y, y_pred, sample_weight,","1529","                                     sample_mask, random_state, X_idx_sorted,","1530","                                     X_csc, X_csr)","1535","                                             y_pred[sample_mask],","1539","                                          y_pred[~sample_mask],","1543","                self.train_score_[i] = loss_(y, y_pred, sample_weight)","1574","    def _init_decision_function(self, X):","1575","        \"\"\"Check input and compute prediction of ``init``. \"\"\"","1581","        score = self.init_.predict(X).astype(np.float64)","1582","        return score","1584","    def _decision_function(self, X):","1585","        # for use in inner loop, not raveling the output in single-class case,","1586","        # not doing input validation.","1587","        score = self._init_decision_function(X)","1588","        predict_stages(self.estimators_, X, self.learning_rate, score)","1589","        return score","1591","","1592","    def _staged_decision_function(self, X):","1593","        \"\"\"Compute decision function of ``X`` for each iteration.","1607","        score : generator of array, shape (n_samples, k)","1608","            The decision function of the input samples. The order of the","1614","        score = self._init_decision_function(X)","1616","            predict_stage(self.estimators_, i, X, self.learning_rate, score)","1617","            yield score.copy()","1795","    init : estimator, optional","1796","        An estimator object that is used to compute the initial","1797","        predictions. ``init`` has to provide ``fit`` and ``predict``.","1798","        If None it uses ``loss.init_estimator``.","1986","            The decision function of the input samples. The order of the","1987","            classes corresponds to that in the attribute `classes_`.","1988","            Regression and binary classification produce an array of shape","1989","            [n_samples].","1992","        score = self._decision_function(X)","1993","        if score.shape[1] == 1:","1994","            return score.ravel()","1995","        return score","2013","            The decision function of the input samples. The order of the","2018","        yield from self._staged_decision_function(X)","2035","        score = self.decision_function(X)","2036","        decisions = self.loss_._score_to_decision(score)","2037","        return self.classes_.take(decisions, axis=0)","2057","        for score in self._staged_decision_function(X):","2058","            decisions = self.loss_._score_to_decision(score)","2059","            yield self.classes_.take(decisions, axis=0)","2082","        score = self.decision_function(X)","2084","            return self.loss_._score_to_proba(score)","2134","            for score in self._staged_decision_function(X):","2135","                yield self.loss_._score_to_proba(score)","2253","    init : estimator, optional (default=None)","2254","        An estimator object that is used to compute the initial","2255","        predictions. ``init`` has to provide ``fit`` and ``predict``.","2256","        If None it uses ``loss.init_estimator``.","2428","        return self._decision_function(X).ravel()","2448","        for y in self._staged_decision_function(X):","2449","            yield y.ravel()"]}],"sklearn\/ensemble\/_gb_losses.py":[{"add":[],"delete":[]}],"sklearn\/ensemble\/tests\/test_gradient_boosting.py":[{"add":["15","from sklearn.base import BaseEstimator","16","from sklearn.datasets import (make_classification, fetch_california_housing,","17","                              make_regression)","22","from sklearn.preprocessing import OneHotEncoder","23","from sklearn.svm import LinearSVC","40","from sklearn.dummy import DummyClassifier, DummyRegressor","41","","1054","    # Test if init='zero' works for regression.","1069","    # Test if init='zero' works for classification.","1321","","1322","","1323","class _NoSampleWeightWrapper(BaseEstimator):","1324","    def __init__(self, est):","1325","        self.est = est","1326","","1327","    def fit(self, X, y):","1328","        self.est.fit(X, y)","1329","","1330","    def predict(self, X):","1331","        return self.est.predict(X)","1332","","1333","    def predict_proba(self, X):","1334","        return self.est.predict_proba(X)","1335","","1336","","1337","def _make_multiclass():","1338","    return make_classification(n_classes=3, n_clusters_per_class=1)","1339","","1340","","1341","@pytest.mark.parametrize(","1342","    \"gb, dataset_maker, init_estimator\",","1343","    [(GradientBoostingClassifier, make_classification, DummyClassifier),","1344","     (GradientBoostingClassifier, _make_multiclass, DummyClassifier),","1345","     (GradientBoostingRegressor, make_regression, DummyRegressor)],","1346","    ids=[\"binary classification\", \"multiclass classification\", \"regression\"])","1347","def test_gradient_boosting_with_init(gb, dataset_maker, init_estimator):","1348","    # Check that GradientBoostingRegressor works when init is a sklearn","1349","    # estimator.","1350","    # Check that an error is raised if trying to fit with sample weight but","1351","    # inital estimator does not support sample weight","1352","","1353","    X, y = dataset_maker()","1354","    sample_weight = np.random.RandomState(42).rand(100)","1355","","1356","    # init supports sample weights","1357","    init_est = init_estimator()","1358","    gb(init=init_est).fit(X, y, sample_weight=sample_weight)","1359","","1360","    # init does not support sample weights","1361","    init_est = _NoSampleWeightWrapper(init_estimator())","1362","    gb(init=init_est).fit(X, y)  # ok no sample weights","1363","    with pytest.raises(ValueError,","1364","                       match=\"estimator.*does not support sample weights\"):","1365","        gb(init=init_est).fit(X, y, sample_weight=sample_weight)","1366","","1367","","1368","@pytest.mark.parametrize('estimator, missing_method', [","1369","    (GradientBoostingClassifier(init=LinearSVC()), 'predict_proba'),","1370","    (GradientBoostingRegressor(init=OneHotEncoder()), 'predict')","1371","])","1372","def test_gradient_boosting_init_wrong_methods(estimator, missing_method):","1373","    # Make sure error is raised if init estimators don't have the required","1374","    # methods (fit, predict, predict_proba)","1375","","1376","    message = (\"The init parameter must be a valid estimator and support \"","1377","               \"both fit and \" + missing_method)","1378","    with pytest.raises(ValueError, match=message):","1379","        estimator.fit(X, y)","1380","","1381","","1382","def test_early_stopping_n_classes():","1383","    # when doing early stopping (_, y_train, _, _ = train_test_split(X, y))","1384","    # there might be classes in y that are missing in y_train. As the init","1385","    # estimator will be trained on y_train, we need to raise an error if this","1386","    # happens.","1387","","1388","    X = [[1, 2], [2, 3], [3, 4], [4, 5]]","1389","    y = [0, 1, 1, 1]","1390","    gb = GradientBoostingClassifier(n_iter_no_change=5, random_state=4)","1391","    with pytest.raises(","1392","                ValueError,","1393","                match='The training data after the early stopping split'):","1394","        gb.fit(X, y)","1395","","1396","    # No error with another random seed","1397","    gb = GradientBoostingClassifier(n_iter_no_change=5, random_state=0)","1398","    gb.fit(X, y)"],"delete":["15","from sklearn.datasets import make_classification, fetch_california_housing","1048","    # Test if ZeroEstimator works for regression.","1049","    est = GradientBoostingRegressor(n_estimators=20, max_depth=1,","1050","                                    random_state=1, init=ZeroEstimator())","1051","    est.fit(boston.data, boston.target)","1052","    y_pred = est.predict(boston.data)","1053","    mse = mean_squared_error(boston.target, y_pred)","1054","    assert_almost_equal(mse, 33.0, decimal=0)","1069","    # Test if ZeroEstimator works for classification.","1072","    est = GradientBoostingClassifier(n_estimators=20, max_depth=1,","1073","                                     random_state=1, init=ZeroEstimator())","1074","    est.fit(X, y)","1075","","1076","    assert_greater(est.score(X, y), 0.96)"]}],"sklearn\/ensemble\/tests\/test_gradient_boosting_loss_functions.py":[{"add":["6","from numpy.testing import assert_allclose","10","from sklearn.utils.stats import _weighted_percentile","11","from sklearn.ensemble._gb_losses import RegressionLossFunction","12","from sklearn.ensemble._gb_losses import LeastSquaresError","13","from sklearn.ensemble._gb_losses import LeastAbsoluteError","14","from sklearn.ensemble._gb_losses import HuberLossFunction","15","from sklearn.ensemble._gb_losses import QuantileLossFunction","16","from sklearn.ensemble._gb_losses import BinomialDeviance","17","from sklearn.ensemble._gb_losses import MultinomialDeviance","18","from sklearn.ensemble._gb_losses import ExponentialLoss","19","from sklearn.ensemble._gb_losses import LOSS_FUNCTIONS","93","        out = loss.get_init_raw_predictions(X, init_est)","98","        sw_out = loss.get_init_raw_predictions(X, sw_init_est)","102","        assert_allclose(out, sw_out, rtol=1e-2)","176","","177","","178","def test_init_raw_predictions_shapes():","179","    # Make sure get_init_raw_predictions returns float64 arrays with shape","180","    # (n_samples, K) where K is 1 for binary classification and regression, and","181","    # K = n_classes for multiclass classification","182","    rng = np.random.RandomState(0)","183","","184","    n_samples = 100","185","    X = rng.normal(size=(n_samples, 5))","186","    y = rng.normal(size=n_samples)","187","    for loss in (LeastSquaresError(n_classes=1),","188","                 LeastAbsoluteError(n_classes=1),","189","                 QuantileLossFunction(n_classes=1),","190","                 HuberLossFunction(n_classes=1)):","191","        init_estimator = loss.init_estimator().fit(X, y)","192","        raw_predictions = loss.get_init_raw_predictions(y, init_estimator)","193","        assert raw_predictions.shape == (n_samples, 1)","194","        assert raw_predictions.dtype == np.float64","195","","196","    y = rng.randint(0, 2, size=n_samples)","197","    for loss in (BinomialDeviance(n_classes=2),","198","                 ExponentialLoss(n_classes=2)):","199","        init_estimator = loss.init_estimator().fit(X, y)","200","        raw_predictions = loss.get_init_raw_predictions(y, init_estimator)","201","        assert raw_predictions.shape == (n_samples, 1)","202","        assert raw_predictions.dtype == np.float64","203","","204","    for n_classes in range(3, 5):","205","        y = rng.randint(0, n_classes, size=n_samples)","206","        loss = MultinomialDeviance(n_classes=n_classes)","207","        init_estimator = loss.init_estimator().fit(X, y)","208","        raw_predictions = loss.get_init_raw_predictions(y, init_estimator)","209","        assert raw_predictions.shape == (n_samples, n_classes)","210","        assert raw_predictions.dtype == np.float64","211","","212","","213","def test_init_raw_predictions_values():","214","    # Make sure the get_init_raw_predictions() returns the expected values for","215","    # each loss.","216","    rng = np.random.RandomState(0)","217","","218","    n_samples = 100","219","    X = rng.normal(size=(n_samples, 5))","220","    y = rng.normal(size=n_samples)","221","","222","    # Least squares loss","223","    loss = LeastSquaresError(n_classes=1)","224","    init_estimator = loss.init_estimator().fit(X, y)","225","    raw_predictions = loss.get_init_raw_predictions(y, init_estimator)","226","    # Make sure baseline prediction is the mean of all targets","227","    assert_almost_equal(raw_predictions, y.mean())","228","","229","    # Least absolute and huber loss","230","    for Loss in (LeastAbsoluteError, HuberLossFunction):","231","        loss = Loss(n_classes=1)","232","        init_estimator = loss.init_estimator().fit(X, y)","233","        raw_predictions = loss.get_init_raw_predictions(y, init_estimator)","234","        # Make sure baseline prediction is the median of all targets","235","        assert_almost_equal(raw_predictions, np.median(y))","236","","237","    # Quantile loss","238","    for alpha in (.1, .5, .9):","239","        loss = QuantileLossFunction(n_classes=1, alpha=alpha)","240","        init_estimator = loss.init_estimator().fit(X, y)","241","        raw_predictions = loss.get_init_raw_predictions(y, init_estimator)","242","        # Make sure baseline prediction is the alpha-quantile of all targets","243","        assert_almost_equal(raw_predictions, np.percentile(y, alpha * 100))","244","","245","    y = rng.randint(0, 2, size=n_samples)","246","","247","    # Binomial deviance","248","    loss = BinomialDeviance(n_classes=2)","249","    init_estimator = loss.init_estimator().fit(X, y)","250","    # Make sure baseline prediction is equal to link_function(p), where p","251","    # is the proba of the positive class. We want predict_proba() to return p,","252","    # and by definition","253","    # p = inverse_link_function(raw_prediction) = sigmoid(raw_prediction)","254","    # So we want raw_prediction = link_function(p) = log(p \/ (1 - p))","255","    raw_predictions = loss.get_init_raw_predictions(y, init_estimator)","256","    p = y.mean()","257","    assert_almost_equal(raw_predictions, np.log(p \/ (1 - p)))","258","","259","    # Exponential loss","260","    loss = ExponentialLoss(n_classes=2)","261","    init_estimator = loss.init_estimator().fit(X, y)","262","    raw_predictions = loss.get_init_raw_predictions(y, init_estimator)","263","    p = y.mean()","264","    assert_almost_equal(raw_predictions, .5 * np.log(p \/ (1 - p)))","265","","266","    # Multinomial deviance loss","267","    for n_classes in range(3, 5):","268","        y = rng.randint(0, n_classes, size=n_samples)","269","        loss = MultinomialDeviance(n_classes=n_classes)","270","        init_estimator = loss.init_estimator().fit(X, y)","271","        raw_predictions = loss.get_init_raw_predictions(y, init_estimator)","272","        for k in range(n_classes):","273","            p = (y == k).mean()","274","        assert_almost_equal(raw_predictions[:, k], np.log(p))"],"delete":["5","from numpy.testing import assert_array_equal","10","from sklearn.utils.testing import assert_raises","11","from sklearn.ensemble.gradient_boosting import BinomialDeviance","12","from sklearn.ensemble.gradient_boosting import LogOddsEstimator","13","from sklearn.ensemble.gradient_boosting import LeastSquaresError","14","from sklearn.ensemble.gradient_boosting import RegressionLossFunction","15","from sklearn.ensemble.gradient_boosting import LOSS_FUNCTIONS","16","from sklearn.ensemble.gradient_boosting import _weighted_percentile","17","from sklearn.ensemble.gradient_boosting import QuantileLossFunction","54","def test_log_odds_estimator():","55","    # Check log odds estimator.","56","    est = LogOddsEstimator()","57","    assert_raises(ValueError, est.fit, None, np.array([1]))","58","","59","    est.fit(None, np.array([1.0, 0.0]))","60","    assert_equal(est.prior, 0.0)","61","    assert_array_equal(est.predict(np.array([[1.0], [1.0]])),","62","                       np.array([[0.0], [0.0]]))","63","","64","","102","        out = init_est.predict(X)","107","        sw_out = init_est.predict(X)","111","        assert_array_equal(out, sw_out)","157","    X = rng.rand(100, 2)"]}]}},"db17f3e2221fb6cec256d2d3501e259c5d5db934":{"changes":{"sklearn\/decomposition\/dict_learning.py":"MODIFY","doc\/whats_new\/v0.20.rst":"MODIFY"},"diff":{"sklearn\/decomposition\/dict_learning.py":[{"add":["302","    if effective_n_jobs(n_jobs) == 1 or algorithm == 'threshold':"],"delete":["302","    if effective_n_jobs(n_jobs) or algorithm == 'threshold':"]}],"doc\/whats_new\/v0.20.rst":[{"add":["24",":mod:`sklearn.decomposition`","25","...........................","26","","27","- |Fix| Fixed a bug in :func:`decomposition.sparse_encode` where computation was single","28","  threaded when `n_jobs > 1` or `n_jobs = -1`.","29","  :issue:`13005` by :user:`Prabakaran Kumaresshan <nixphix>`.","30",""],"delete":[]}]}},"66899eddd01ef7c694565bf71a8bc8a8b0adeae5":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/linear_model\/tests\/test_base.py":"MODIFY","sklearn\/linear_model\/base.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["202","- |Fix| Fixed a bug in :class:`linear_model.LinearRegression` that","203","  was not returning the same coeffecients and intercepts with","204","  ``fit_intercept=True`` in sparse and dense case.","205","  :issue:`13279` by `Alexandre Gramfort`_","206",""],"delete":[]}],"sklearn\/linear_model\/tests\/test_base.py":[{"add":["156","@pytest.mark.parametrize('normalize', [True, False])","157","@pytest.mark.parametrize('fit_intercept', [True, False])","158","def test_linear_regression_sparse_equal_dense(normalize, fit_intercept):","159","    # Test that linear regression agrees between sparse and dense","160","    rng = check_random_state(0)","161","    n_samples = 200","162","    n_features = 2","163","    X = rng.randn(n_samples, n_features)","164","    X[X < 0.1] = 0.","165","    Xcsr = sparse.csr_matrix(X)","166","    y = rng.rand(n_samples)","167","    params = dict(normalize=normalize, fit_intercept=fit_intercept)","168","    clf_dense = LinearRegression(**params)","169","    clf_sparse = LinearRegression(**params)","170","    clf_dense.fit(X, y)","171","    clf_sparse.fit(Xcsr, y)","172","    assert clf_dense.intercept_ == pytest.approx(clf_sparse.intercept_)","173","    assert_allclose(clf_dense.coef_, clf_sparse.coef_)","174","","175",""],"delete":[]}],"sklearn\/linear_model\/base.py":[{"add":["469","            copy=self.copy_X, sample_weight=sample_weight,","470","            return_mean=True)","477","            X_offset_scale = X_offset \/ X_scale","478","","479","            def matvec(b):","480","                return X.dot(b) - b.dot(X_offset_scale)","481","","482","            def rmatvec(b):","483","                return X.T.dot(b) - X_offset_scale * np.sum(b)","484","","485","            X_centered = sparse.linalg.LinearOperator(shape=X.shape,","486","                                                      matvec=matvec,","487","                                                      rmatvec=rmatvec)","488","","490","                out = sparse_lsqr(X_centered, y)","496","                    delayed(sparse_lsqr)(X_centered, y[:, j].ravel())"],"delete":["469","            copy=self.copy_X, sample_weight=sample_weight)","477","                out = sparse_lsqr(X, y)","483","                    delayed(sparse_lsqr)(X, y[:, j].ravel())"]}]}},"0e9520b235522146b7395b6d848319d3810aba97":{"changes":{"sklearn\/linear_model\/tests\/test_huber.py":"MODIFY","doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/linear_model\/huber.py":"MODIFY"},"diff":{"sklearn\/linear_model\/tests\/test_huber.py":[{"add":["55","","56","    def loss_func(x, *args):","57","        return _huber_loss_and_gradient(x, *args)[0]","58","","59","    def grad_func(x, *args):","60","        return _huber_loss_and_gradient(x, *args)[1]","82","    # Rescale coefs before comparing with assert_array_almost_equal to make","83","    # sure that the number of decimal places used is somewhat insensitive to","84","    # the amplitude of the coefficients and therefore to the scale of the","85","    # data and the regularization parameter","173","        fit_intercept=True, alpha=1.0, max_iter=10000, warm_start=True,","174","        tol=1e-1)","197","    # give a worse score on the non-outliers as compared to the huber","198","    # regressor.","207","","208","","209","def test_huber_bool():","210","    # Test that it does not crash with bool data","211","    X, y = make_regression(n_samples=200, n_features=2, noise=4.0,","212","                           random_state=0)","213","    X_bool = X > 0","214","    HuberRegressor().fit(X_bool, y)"],"delete":["55","    loss_func = lambda x, *args: _huber_loss_and_gradient(x, *args)[0]","56","    grad_func = lambda x, *args: _huber_loss_and_gradient(x, *args)[1]","78","    # Rescale coefs before comparing with assert_array_almost_equal to make sure","79","    # that the number of decimal places used is somewhat insensitive to the","80","    # amplitude of the coefficients and therefore to the scale of the data","81","    # and the regularization parameter","169","        fit_intercept=True, alpha=1.0, max_iter=10000, warm_start=True, tol=1e-1)","192","    # give a worse score on the non-outliers as compared to the huber regressor."]}],"doc\/whats_new\/v0.21.rst":[{"add":["240","- |Fix| Fixed a bug in :class:`linear_model.HuberRegressor` that was","241","  broken when X was of dtype bool.","242","  :issue:`13328` by `Alexandre Gramfort`_.","243","","244",""],"delete":[]}],"sklearn\/linear_model\/huber.py":[{"add":["253","            X, y, copy=False, accept_sparse=['csr'], y_numeric=True,","254","            dtype=[np.float64, np.float32])"],"delete":["253","            X, y, copy=False, accept_sparse=['csr'], y_numeric=True)"]}]}},"bf6949ba9d5fc019fd48eda247249d8fd8c420fa":{"changes":{"sklearn\/multioutput.py":"MODIFY"},"diff":{"sklearn\/multioutput.py":[{"add":["462","        check_is_fitted(self, 'estimators_')","639","        return {'_skip_test': True,","640","                'multioutput_only': True}","726","        return {'multioutput_only': True}"],"delete":["638","        return {'_skip_test': True}","724","        # FIXME","725","        return {'_skip_test': True}"]}]}},"5bc3edccab42921332d54b88c2c595d333f16410":{"changes":{"sklearn\/tree\/_splitter.pyx":"MODIFY","sklearn\/tree\/tests\/test_tree.py":"MODIFY"},"diff":{"sklearn\/tree\/_splitter.pyx":[{"add":["118","                   const DOUBLE_t[:, ::1] y,","238","    cdef const DTYPE_t[:, :] X","262","                  const DOUBLE_t[:, ::1] y,","878","                  const DOUBLE_t[:, ::1] y,"],"delete":["118","                   DOUBLE_t[:, ::1] y,","238","    cdef DTYPE_t[:, :] X","262","                  DOUBLE_t[:, ::1] y,","878","                  DOUBLE_t[:, ::1] y,"]}],"sklearn\/tree\/tests\/test_tree.py":[{"add":["35","from sklearn.utils.testing import TempMemmap","1851","","1852","","1853","def test_decision_tree_memmap():","1854","    # check that decision trees supports read-only buffer (#13626)","1855","    X = np.random.RandomState(0).random_sample((10, 2)).astype(np.float32)","1856","    y = np.zeros(10)","1857","","1858","    with TempMemmap((X, y)) as (X_read_only, y_read_only):","1859","        DecisionTreeClassifier().fit(X_read_only, y_read_only)"],"delete":[]}]}},"bcf4f803e4e826cbdb9f8e0f2108651568664fe7":{"changes":{"sklearn\/tests\/test_impute.py":"MODIFY","doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/impute.py":"MODIFY"},"diff":{"sklearn\/tests\/test_impute.py":[{"add":["1121","","1122","","1123","def test_missing_indicator_no_missing():","1124","    # check that all features are dropped if there are no missing values when","1125","    # features='missing-only' (#13491)","1126","    X = np.array([[1, 1],","1127","                  [1, 1]])","1128","","1129","    mi = MissingIndicator(features='missing-only', missing_values=-1)","1130","    Xt = mi.fit_transform(X)","1131","","1132","    assert Xt.shape[1] == 0","1133","","1134","","1135","def test_missing_indicator_sparse_no_explicit_zeros():","1136","    # Check that non missing values don't become explicit zeros in the mask","1137","    # generated by missing indicator when X is sparse. (#13491)","1138","    X = sparse.csr_matrix([[0, 1, 2],","1139","                           [1, 2, 0],","1140","                           [2, 0, 1]])","1141","","1142","    mi = MissingIndicator(features='all', missing_values=1)","1143","    Xt = mi.fit_transform(X)","1144","","1145","    assert Xt.getnnz() == Xt.sum()"],"delete":[]}],"doc\/whats_new\/v0.21.rst":[{"add":["251","- |Fix| Fixed two bugs in :class:`MissingIndicator`. First, when ``X`` is","252","  sparse, all the non-zero non missing values used to become explicit False in","253","  the transformed data. Then, when ``features='missing-only'``, all features","254","  used to be kept if there were no missing values at all. :issue:`13562` by","255","  :user:`J¨¦r¨¦mie du Boisberranger <jeremiedbb>`.","256",""],"delete":[]}],"sklearn\/impute.py":[{"add":["1146","            imputer_mask.eliminate_zeros()","1148","            if self.features == 'missing-only':","1149","                n_missing = imputer_mask.getnnz(axis=0)","1157","","1158","            if self.features == 'missing-only':","1159","                n_missing = imputer_mask.sum(axis=0)","1164","        if self.features == 'all':","1165","            features_indices = np.arange(X.shape[1])","1166","        else:","1167","            features_indices = np.flatnonzero(n_missing)","1168","","1169","        return imputer_mask, features_indices","1222","        self.features_ = self._get_missing_features_info(X)[1]","1257","            if self.features_.size < self._n_features:"],"delete":["1147","            missing_values_mask = imputer_mask.copy()","1148","            missing_values_mask.eliminate_zeros()","1149","            features_with_missing = (","1150","                np.flatnonzero(np.diff(missing_values_mask.indptr))","1151","                if missing_values_mask.format == 'csc'","1152","                else np.unique(missing_values_mask.indices))","1160","            features_with_missing = np.flatnonzero(imputer_mask.sum(axis=0))","1165","        return imputer_mask, features_with_missing","1218","        self.features_ = (self._get_missing_features_info(X)[1]","1219","                          if self.features == 'missing-only'","1220","                          else np.arange(self._n_features))","1255","            if (self.features_.size > 0 and","1256","                    self.features_.size < self._n_features):"]}]}},"19c8af6d6a9528162fd41d8a3363b807e337d63d":{"changes":{"sklearn\/metrics\/scorer.py":"MODIFY","sklearn\/metrics\/tests\/test_score_objects.py":"MODIFY","doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/metrics\/tests\/test_common.py":"MODIFY","doc\/modules\/classes.rst":"MODIFY","sklearn\/metrics\/classification.py":"MODIFY","sklearn\/metrics\/tests\/test_classification.py":"MODIFY","examples\/multioutput\/plot_classifier_chain_yeast.py":"MODIFY","doc\/modules\/model_evaluation.rst":"MODIFY","sklearn\/metrics\/__init__.py":"MODIFY","sklearn\/tests\/test_multioutput.py":"MODIFY"},"diff":{"sklearn\/metrics\/scorer.py":[{"add":["30","               brier_score_loss, jaccard_score)","533","                     ('recall', recall_score), ('f1', f1_score),","534","                     ('jaccard', jaccard_score)]:","535","    SCORERS[name] = make_scorer(metric, average='binary')"],"delete":["30","               brier_score_loss)","533","                     ('recall', recall_score), ('f1', f1_score)]:","534","    SCORERS[name] = make_scorer(metric)"]}],"sklearn\/metrics\/tests\/test_score_objects.py":[{"add":["19","                             log_loss, precision_score, recall_score,","20","                             jaccard_score)","54","               'neg_log_loss', 'log_loss', 'brier_score_loss',","55","               'jaccard', 'jaccard_weighted', 'jaccard_macro',","56","               'jaccard_micro']","68","MULTILABEL_ONLY_SCORERS = ['precision_samples', 'recall_samples', 'f1_samples',","69","                           'jaccard_samples']","290","                           ('recall', recall_score),","291","                           ('jaccard', jaccard_score)]:"],"delete":["7","","20","                             log_loss, precision_score, recall_score)","54","               'neg_log_loss', 'log_loss', 'brier_score_loss']","66","MULTILABEL_ONLY_SCORERS = ['precision_samples', 'recall_samples', 'f1_samples']","287","                           ('recall', recall_score)]:"]}],"doc\/whats_new\/v0.21.rst":[{"add":["295","- |Feature| :func:`metrics.jaccard_score` has been added to calculate the","296","  Jaccard coefficient as an evaluation metric for binary, multilabel and","297","  multiclass tasks, with an interface analogous to :func:`metrics.f1_score`.","298","  :issue:`13151` by :user:`Gaurav Dhingra <gxyd>` and `Joel Nothman`_.","299","","325","- |API| :func:`metrics.jaccard_similarity_score` is deprecated in favour of","326","  the more consistent :func:`metrics.jaccard_score`. The former behavior for","327","  binary and multiclass targets is broken.","328","  :issue:`13151` by `Joel Nothman`_.","329",""],"delete":[]}],"sklearn\/metrics\/tests\/test_common.py":[{"add":["39","from sklearn.metrics import jaccard_score","128","    \"jaccard_score\": jaccard_score,","141","    \"weighted_jaccard_score\": partial(jaccard_score, average=\"weighted\"),","148","    \"micro_jaccard_score\": partial(jaccard_score, average=\"micro\"),","155","    \"macro_jaccard_score\": partial(jaccard_score, average=\"macro\"),","162","    \"samples_jaccard_score\": partial(jaccard_score, average=\"samples\"),","250","    \"samples_jaccard_score\",","272","    \"jaccard_score\",","273","","292","    \"precision_score\", \"recall_score\", \"f1_score\", \"f2_score\", \"f0.5_score\",","293","    \"jaccard_score\"","309","    \"jaccard_score\",","339","    \"jaccard_score\",","343","    \"weighted_jaccard_score\",","347","    \"micro_jaccard_score\",","351","    \"macro_jaccard_score\",","352","","388","    \"weighted_jaccard_score\",","392","    \"macro_jaccard_score\",","396","    \"micro_jaccard_score\",","397","","402","    \"samples_jaccard_score\",","418","    \"micro_jaccard_score\", \"macro_jaccard_score\",","419","    \"jaccard_score\",","420","    \"samples_jaccard_score\",","421","","449","    \"weighted_precision_score\", \"weighted_jaccard_score\",","450","    \"unnormalized_multilabel_confusion_matrix\",","471","    y_true_bin = random_state.randint(0, 2, size=(20, 25))","472","    y_pred_bin = random_state.randint(0, 2, size=(20, 25))","473","","487","        if name in METRIC_UNDEFINED_BINARY:","488","            if name in MULTILABELS_METRICS:","489","                assert_allclose(metric(y_true_bin, y_pred_bin),","490","                                metric(y_pred_bin, y_true_bin),","491","                                err_msg=\"%s is not symmetric\" % name)","492","            else:","493","                assert False, \"This case is currently unhandled\"","494","        else:","495","            assert_allclose(metric(y_true, y_pred),","496","                            metric(y_pred, y_true),","497","                            err_msg=\"%s is not symmetric\" % name)"],"delete":["39","from sklearn.metrics import jaccard_similarity_score","124","    \"jaccard_similarity_score\": jaccard_similarity_score,","125","    \"unnormalized_jaccard_similarity_score\":","126","    partial(jaccard_similarity_score, normalize=False),","127","","288","    \"precision_score\", \"recall_score\", \"f1_score\", \"f2_score\", \"f0.5_score\"","351","    \"jaccard_similarity_score\",","374","    \"jaccard_similarity_score\", \"unnormalized_jaccard_similarity_score\",","402","    \"jaccard_similarity_score\", \"unnormalized_jaccard_similarity_score\",","432","    \"weighted_precision_score\", \"unnormalized_multilabel_confusion_matrix\",","466","        assert_allclose(metric(y_true, y_pred), metric(y_pred, y_true),","467","                        err_msg=\"%s is not symmetric\" % name)"]}],"doc\/modules\/classes.rst":[{"add":["846","   metrics.jaccard_score","1507","   metrics.jaccard_similarity_score"],"delete":["846","   metrics.jaccard_similarity_score"]}],"sklearn\/metrics\/classification.py":[{"add":["150","    jaccard_score, hamming_loss, zero_one_loss","155","    to the ``jaccard_score`` function.","583","    .. deprecated:: 0.21","584","        This is deprecated to be removed in 0.23, since its handling of","585","        binary and multiclass inputs was broken. `jaccard_score` has an API","586","        that is consistent with precision_score, f_score, etc.","631","    warnings.warn('jaccard_similarity_score has been deprecated and replaced '","632","                  'with jaccard_score. It will be removed in version 0.23. '","633","                  'This implementation has surprising behavior for binary '","634","                  'and multiclass classification tasks.', DeprecationWarning)","652","def jaccard_score(y_true, y_pred, labels=None, pos_label=1,","653","                  average='binary', sample_weight=None):","654","    \"\"\"Jaccard similarity coefficient score","655","","656","    The Jaccard index [1], or Jaccard similarity coefficient, defined as","657","    the size of the intersection divided by the size of the union of two label","658","    sets, is used to compare set of predicted labels for a sample to the","659","    corresponding set of labels in ``y_true``.","660","","661","    Read more in the :ref:`User Guide <jaccard_score>`.","662","","663","    Parameters","664","    ----------","665","    y_true : 1d array-like, or label indicator array \/ sparse matrix","666","        Ground truth (correct) labels.","667","","668","    y_pred : 1d array-like, or label indicator array \/ sparse matrix","669","        Predicted labels, as returned by a classifier.","670","","671","    labels : list, optional","672","        The set of labels to include when ``average != 'binary'``, and their","673","        order if ``average is None``. Labels present in the data can be","674","        excluded, for example to calculate a multiclass average ignoring a","675","        majority negative class, while labels not present in the data will","676","        result in 0 components in a macro average. For multilabel targets,","677","        labels are column indices. By default, all labels in ``y_true`` and","678","        ``y_pred`` are used in sorted order.","679","","680","    pos_label : str or int, 1 by default","681","        The class to report if ``average='binary'`` and the data is binary.","682","        If the data are multiclass or multilabel, this will be ignored;","683","        setting ``labels=[pos_label]`` and ``average != 'binary'`` will report","684","        scores for that label only.","685","","686","    average : string, [None, 'binary' (default), 'micro', 'macro', 'samples', \\","687","                       'weighted']","688","        If ``None``, the scores for each class are returned. Otherwise, this","689","        determines the type of averaging performed on the data:","690","","691","        ``'binary'``:","692","            Only report results for the class specified by ``pos_label``.","693","            This is applicable only if targets (``y_{true,pred}``) are binary.","694","        ``'micro'``:","695","            Calculate metrics globally by counting the total true positives,","696","            false negatives and false positives.","697","        ``'macro'``:","698","            Calculate metrics for each label, and find their unweighted","699","            mean.  This does not take label imbalance into account.","700","        ``'weighted'``:","701","            Calculate metrics for each label, and find their average, weighted","702","            by support (the number of true instances for each label). This","703","            alters 'macro' to account for label imbalance.","704","        ``'samples'``:","705","            Calculate metrics for each instance, and find their average (only","706","            meaningful for multilabel classification).","707","","708","    sample_weight : array-like of shape = [n_samples], optional","709","        Sample weights.","710","","711","    Returns","712","    -------","713","    score : float (if average is not None) or array of floats, shape =\\","714","            [n_unique_labels]","715","","716","    See also","717","    --------","718","    accuracy_score, f_score, multilabel_confusion_matrix","719","","720","    Notes","721","    -----","722","    :func:`jaccard_score` may be a poor metric if there are no","723","    positives for some samples or classes. Jaccard is undefined if there are","724","    no true or predicted labels, and our implementation will return a score","725","    of 0 with a warning.","726","","727","    References","728","    ----------","729","    .. [1] `Wikipedia entry for the Jaccard index","730","           <https:\/\/en.wikipedia.org\/wiki\/Jaccard_index>`_","731","","732","    Examples","733","    --------","734","    >>> import numpy as np","735","    >>> from sklearn.metrics import jaccard_score","736","    >>> y_true = np.array([[0, 1, 1],","737","    ...                    [1, 1, 0]])","738","    >>> y_pred = np.array([[1, 1, 1],","739","    ...                    [1, 0, 0]])","740","","741","    In the binary case:","742","","743","    >>> jaccard_score(y_true[0], y_pred[0])  # doctest: +ELLIPSIS","744","    0.6666...","745","","746","    In the multilabel case:","747","","748","    >>> jaccard_score(y_true, y_pred, average='samples')","749","    0.5833...","750","    >>> jaccard_score(y_true, y_pred, average='macro')","751","    0.6666...","752","    >>> jaccard_score(y_true, y_pred, average=None)","753","    array([0.5, 0.5, 1. ])","754","","755","    In the multiclass case:","756","","757","    >>> y_pred = [0, 2, 1, 2]","758","    >>> y_true = [0, 1, 2, 2]","759","    >>> jaccard_score(y_true, y_pred, average=None)","760","    ... # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS","761","    array([1. , 0. , 0.33...])","762","    \"\"\"","763","    labels = _check_set_wise_labels(y_true, y_pred, average, labels,","764","                                    pos_label)","765","    samplewise = average == 'samples'","766","    MCM = multilabel_confusion_matrix(y_true, y_pred,","767","                                      sample_weight=sample_weight,","768","                                      labels=labels, samplewise=samplewise)","769","    numerator = MCM[:, 1, 1]","770","    denominator = MCM[:, 1, 1] + MCM[:, 0, 1] + MCM[:, 1, 0]","771","","772","    if average == 'micro':","773","        numerator = np.array([numerator.sum()])","774","        denominator = np.array([denominator.sum()])","775","","776","    jaccard = _prf_divide(numerator, denominator, 'jaccard',","777","                          'true or predicted', average, ('jaccard',))","778","    if average is None:","779","        return jaccard","780","    if average == 'weighted':","781","        weights = MCM[:, 1, 0] + MCM[:, 1, 1]","782","        if not np.any(weights):","783","            # numerator is 0, and warning should have already been issued","784","            weights = None","785","    elif average == 'samples' and sample_weight is not None:","786","        weights = sample_weight","787","    else:","788","        weights = None","789","    return np.average(jaccard, weights=weights)","790","","791","","917","    accuracy_score, hamming_loss, jaccard_score","1028","    fbeta_score, precision_recall_fscore_support, jaccard_score,","1195","    denominator = denominator.copy()","1196","    denominator[mask] = 1  # avoid infs\/nans","1197","    result = numerator \/ denominator","1228","def _check_set_wise_labels(y_true, y_pred, average, labels, pos_label):","1229","    \"\"\"Validation associated with set-wise metrics","1230","","1231","    Returns identified labels","1232","    \"\"\"","1233","    average_options = (None, 'micro', 'macro', 'weighted', 'samples')","1234","    if average not in average_options and average != 'binary':","1235","        raise ValueError('average has to be one of ' +","1236","                         str(average_options))","1237","","1238","    y_type, y_true, y_pred = _check_targets(y_true, y_pred)","1239","    present_labels = unique_labels(y_true, y_pred)","1240","    if average == 'binary':","1241","        if y_type == 'binary':","1242","            if pos_label not in present_labels:","1243","                if len(present_labels) >= 2:","1244","                    raise ValueError(\"pos_label=%r is not a valid label: \"","1245","                                     \"%r\" % (pos_label, present_labels))","1246","            labels = [pos_label]","1247","        else:","1248","            average_options = list(average_options)","1249","            if y_type == 'multiclass':","1250","                average_options.remove('samples')","1251","            raise ValueError(\"Target is %s but average='binary'. Please \"","1252","                             \"choose another average setting, one of %r.\"","1253","                             % (y_type, average_options))","1254","    elif pos_label not in (None, 1):","1255","        warnings.warn(\"Note that pos_label (set to %r) is ignored when \"","1256","                      \"average != 'binary' (got %r). You may use \"","1257","                      \"labels=[pos_label] to specify a single positive class.\"","1258","                      % (pos_label, average), UserWarning)","1259","    return labels","1260","","1261","","1413","    labels = _check_set_wise_labels(y_true, y_pred, average, labels,","1414","                                    pos_label)","1433","    # Divide, and on zero-division, set scores to 0 and warn:","1434","","1435","    precision = _prf_divide(tp_sum, pred_sum,","1436","                            'precision', 'predicted', average, warn_for)","1437","    recall = _prf_divide(tp_sum, true_sum,","1438","                         'recall', 'true', average, warn_for)","1439","    # Don't need to warn for F: either P or R warned, or tp == 0 where pos","1440","    # and true are nonzero, in which case, F is well-defined and zero","1441","    denom = beta2 * precision + recall","1442","    denom[denom == 0.] = 1  # avoid division by 0","1443","    f_score = (1 + beta2) * precision * recall \/ denom","1985","    accuracy_score, jaccard_score, zero_one_loss"],"delete":["150","    jaccard_similarity_score, hamming_loss, zero_one_loss","155","    to the ``jaccard_similarity_score`` function.","583","    The Jaccard index [1], or Jaccard similarity coefficient, defined as","584","    the size of the intersection divided by the size of the union of two label","585","    sets, is used to compare set of predicted labels for a sample to the","586","    corresponding set of labels in ``y_true``.","630","","631","","632","    Examples","633","    --------","634","    >>> from sklearn.metrics import jaccard_similarity_score","635","    >>> y_pred = [0, 2, 1, 3]","636","    >>> y_true = [0, 1, 2, 3]","637","    >>> jaccard_similarity_score(y_true, y_pred)","638","    0.5","639","    >>> jaccard_similarity_score(y_true, y_pred, normalize=False)","640","    2","641","","642","    In the multilabel case with binary label indicators:","643","","644","    >>> import numpy as np","645","    >>> jaccard_similarity_score(np.array([[0, 1], [1, 1]]),\\","646","        np.ones((2, 2)))","647","    0.75","791","    accuracy_score, hamming_loss, jaccard_similarity_score","902","    fbeta_score, precision_recall_fscore_support, jaccard_similarity_score,","1068","    result = numerator \/ denominator","1073","    # remove infs","1074","    result[mask] = 0.0","1075","","1252","    average_options = (None, 'micro', 'macro', 'weighted', 'samples')","1253","    if average not in average_options and average != 'binary':","1254","        raise ValueError('average has to be one of ' +","1255","                         str(average_options))","1258","","1259","    y_type, y_true, y_pred = _check_targets(y_true, y_pred)","1260","    check_consistent_length(y_true, y_pred, sample_weight)","1261","    present_labels = unique_labels(y_true, y_pred)","1262","","1263","    if average == 'binary':","1264","        if y_type == 'binary':","1265","            if pos_label not in present_labels and len(present_labels) >= 2:","1266","                raise ValueError(\"pos_label=%r is not a valid label: %r\" %","1267","                                 (pos_label, present_labels))","1268","            labels = [pos_label]","1269","        else:","1270","            raise ValueError(\"Target is %s but average='binary'. Please \"","1271","                             \"choose another average setting.\" % y_type)","1272","    elif pos_label not in (None, 1):","1273","        warnings.warn(\"Note that pos_label (set to %r) is ignored when \"","1274","                      \"average != 'binary' (got %r). You may use \"","1275","                      \"labels=[pos_label] to specify a single positive class.\"","1276","                      % (pos_label, average), UserWarning)","1294","    with np.errstate(divide='ignore', invalid='ignore'):","1295","        # Divide, and on zero-division, set scores to 0 and warn:","1297","        # Oddly, we may get an \"invalid\" rather than a \"divide\" error","1298","        # here.","1299","        precision = _prf_divide(tp_sum, pred_sum,","1300","                                'precision', 'predicted', average, warn_for)","1301","        recall = _prf_divide(tp_sum, true_sum,","1302","                             'recall', 'true', average, warn_for)","1303","        # Don't need to warn for F: either P or R warned, or tp == 0 where pos","1304","        # and true are nonzero, in which case, F is well-defined and zero","1305","        f_score = ((1 + beta2) * precision * recall \/","1306","                   (beta2 * precision + recall))","1307","        f_score[tp_sum == 0] = 0.0","1849","    accuracy_score, jaccard_similarity_score, zero_one_loss"]}],"sklearn\/metrics\/tests\/test_classification.py":[{"add":["13","from sklearn.preprocessing import label_binarize, LabelBinarizer","39","from sklearn.metrics import jaccard_score","1143","def test_jaccard_score_validation():","1144","    y_true = np.array([0, 1, 0, 1, 1])","1145","    y_pred = np.array([0, 1, 0, 1, 1])","1146","    assert_raise_message(ValueError, \"pos_label=2 is not a valid label: \"","1147","                         \"array([0, 1])\", jaccard_score, y_true,","1148","                         y_pred, average='binary', pos_label=2)","1149","","1150","    y_true = np.array([[0, 1, 1], [1, 0, 0]])","1151","    y_pred = np.array([[1, 1, 1], [1, 0, 1]])","1152","    msg1 = (\"Target is multilabel-indicator but average='binary'. \"","1153","            \"Please choose another average setting, one of [None, \"","1154","            \"'micro', 'macro', 'weighted', 'samples'].\")","1155","    assert_raise_message(ValueError, msg1, jaccard_score, y_true,","1156","                         y_pred, average='binary', pos_label=-1)","1157","","1158","    y_true = np.array([0, 1, 1, 0, 2])","1159","    y_pred = np.array([1, 1, 1, 1, 0])","1160","    msg2 = (\"Target is multiclass but average='binary'. Please choose \"","1161","            \"another average setting, one of [None, 'micro', 'macro', \"","1162","            \"'weighted'].\")","1163","    assert_raise_message(ValueError, msg2, jaccard_score, y_true,","1164","                         y_pred, average='binary')","1165","    msg3 = (\"Samplewise metrics are not available outside of multilabel \"","1166","            \"classification.\")","1167","    assert_raise_message(ValueError, msg3, jaccard_score, y_true,","1168","                         y_pred, average='samples')","1169","","1170","    assert_warns_message(UserWarning,","1171","                         \"Note that pos_label (set to 3) is ignored when \"","1172","                         \"average != 'binary' (got 'micro'). You may use \"","1173","                         \"labels=[pos_label] to specify a single positive \"","1174","                         \"class.\", jaccard_score, y_true, y_pred,","1175","                         average='micro', pos_label=3)","1176","","1177","","1178","def test_multilabel_jaccard_score(recwarn):","1186","    assert jaccard_score(y1, y2, average='samples') == 0.75","1187","    assert jaccard_score(y1, y1, average='samples') == 1","1188","    assert jaccard_score(y2, y2, average='samples') == 1","1189","    assert jaccard_score(y2, np.logical_not(y2), average='samples') == 0","1190","    assert jaccard_score(y1, np.logical_not(y1), average='samples') == 0","1191","    assert jaccard_score(y1, np.zeros(y1.shape), average='samples') == 0","1192","    assert jaccard_score(y2, np.zeros(y1.shape), average='samples') == 0","1193","","1194","    y_true = np.array([[0, 1, 1], [1, 0, 0]])","1195","    y_pred = np.array([[1, 1, 1], [1, 0, 1]])","1196","    # average='macro'","1197","    assert_almost_equal(jaccard_score(y_true, y_pred,","1198","                                      average='macro'), 2. \/ 3)","1199","    # average='micro'","1200","    assert_almost_equal(jaccard_score(y_true, y_pred,","1201","                                      average='micro'), 3. \/ 5)","1202","    # average='samples'","1203","    assert_almost_equal(jaccard_score(y_true, y_pred, average='samples'),","1204","                        7. \/ 12)","1205","    assert_almost_equal(jaccard_score(y_true, y_pred,","1206","                                      average='samples',","1207","                                      labels=[0, 2]), 1. \/ 2)","1208","    assert_almost_equal(jaccard_score(y_true, y_pred,","1209","                                      average='samples',","1210","                                      labels=[1, 2]), 1. \/ 2)","1211","    # average=None","1212","    assert_array_equal(jaccard_score(y_true, y_pred, average=None),","1213","                       np.array([1. \/ 2, 1., 1. \/ 2]))","1214","","1215","    y_true = np.array([[0, 1, 1], [1, 0, 1]])","1216","    y_pred = np.array([[1, 1, 1], [1, 0, 1]])","1217","    assert_almost_equal(jaccard_score(y_true, y_pred,","1218","                                      average='macro'), 5. \/ 6)","1219","    # average='weighted'","1220","    assert_almost_equal(jaccard_score(y_true, y_pred,","1221","                                      average='weighted'), 7. \/ 8)","1222","","1223","    msg2 = 'Got 4 > 2'","1224","    assert_raise_message(ValueError, msg2, jaccard_score, y_true,","1225","                         y_pred, labels=[4], average='macro')","1226","    msg3 = 'Got -1 < 0'","1227","    assert_raise_message(ValueError, msg3, jaccard_score, y_true,","1228","                         y_pred, labels=[-1], average='macro')","1229","","1230","    msg = ('Jaccard is ill-defined and being set to 0.0 in labels '","1231","           'with no true or predicted samples.')","1232","    assert assert_warns_message(UndefinedMetricWarning, msg,","1233","                                jaccard_score,","1234","                                np.array([[0, 1]]),","1235","                                np.array([[0, 1]]),","1236","                                average='macro') == 0.5","1237","","1238","    msg = ('Jaccard is ill-defined and being set to 0.0 in samples '","1239","           'with no true or predicted labels.')","1240","    assert assert_warns_message(UndefinedMetricWarning, msg,","1241","                                jaccard_score,","1242","                                np.array([[0, 0], [1, 1]]),","1243","                                np.array([[0, 0], [1, 1]]),","1244","                                average='samples') == 0.5","1245","","1246","    assert not list(recwarn)","1247","","1248","","1249","def test_multiclass_jaccard_score(recwarn):","1250","    y_true = ['ant', 'ant', 'cat', 'cat', 'ant', 'cat', 'bird', 'bird']","1251","    y_pred = ['cat', 'ant', 'cat', 'cat', 'ant', 'bird', 'bird', 'cat']","1252","    labels = ['ant', 'bird', 'cat']","1253","    lb = LabelBinarizer()","1254","    lb.fit(labels)","1255","    y_true_bin = lb.transform(y_true)","1256","    y_pred_bin = lb.transform(y_pred)","1257","    multi_jaccard_score = partial(jaccard_score, y_true,","1258","                                  y_pred)","1259","    bin_jaccard_score = partial(jaccard_score,","1260","                                y_true_bin, y_pred_bin)","1261","    multi_labels_list = [['ant', 'bird'], ['ant', 'cat'], ['cat', 'bird'],","1262","                         ['ant'], ['bird'], ['cat'], None]","1263","    bin_labels_list = [[0, 1], [0, 2], [2, 1], [0], [1], [2], None]","1264","","1265","    # other than average='samples'\/'none-samples', test everything else here","1266","    for average in ('macro', 'weighted', 'micro', None):","1267","        for m_label, b_label in zip(multi_labels_list, bin_labels_list):","1268","            assert_almost_equal(multi_jaccard_score(average=average,","1269","                                                    labels=m_label),","1270","                                bin_jaccard_score(average=average,","1271","                                                  labels=b_label))","1272","","1273","    y_true = np.array([[0, 0], [0, 0], [0, 0]])","1274","    y_pred = np.array([[0, 0], [0, 0], [0, 0]])","1275","    with ignore_warnings():","1276","        assert (jaccard_score(y_true, y_pred, average='weighted')","1277","                == 0)","1278","","1279","    assert not list(recwarn)","1280","","1281","","1282","def test_average_binary_jaccard_score(recwarn):","1283","    # tp=0, fp=0, fn=1, tn=0","1284","    assert jaccard_score([1], [0], average='binary') == 0.","1285","    # tp=0, fp=0, fn=0, tn=1","1286","    msg = ('Jaccard is ill-defined and being set to 0.0 due to '","1287","           'no true or predicted samples')","1288","    assert assert_warns_message(UndefinedMetricWarning,","1289","                                msg,","1290","                                jaccard_score,","1291","                                [0, 0], [0, 0],","1292","                                average='binary') == 0.","1293","    # tp=1, fp=0, fn=0, tn=0 (pos_label=0)","1294","    assert jaccard_score([0], [0], pos_label=0,","1295","                         average='binary') == 1.","1296","    y_true = np.array([1, 0, 1, 1, 0])","1297","    y_pred = np.array([1, 0, 1, 1, 1])","1298","    assert_almost_equal(jaccard_score(y_true, y_pred,","1299","                                      average='binary'), 3. \/ 4)","1300","    assert_almost_equal(jaccard_score(y_true, y_pred,","1301","                                      average='binary',","1302","                                      pos_label=0), 1. \/ 2)","1303","","1304","    assert not list(recwarn)","1684","    msg_mc = (\"Target is multiclass but average='binary'. Please \"","1685","              \"choose another average setting, one of [\"","1686","              \"None, 'micro', 'macro', 'weighted'].\")","1689","    msg_ind = (\"Target is multilabel-indicator but average='binary'. Please \"","1690","               \"choose another average setting, one of [\"","1691","               \"None, 'micro', 'macro', 'weighted', 'samples'].\")","1693","    for y_true, y_pred, msg in [","1694","        (y_true_mc, y_pred_mc, msg_mc),","1695","        (y_true_ind, y_pred_ind, msg_ind),","1699","            assert_raise_message(ValueError, msg,","2025","","2026","","2027","def test_multilabel_jaccard_similarity_score_deprecation():","2028","    # Dense label indicator matrix format","2029","    y1 = np.array([[0, 1, 1], [1, 0, 1]])","2030","    y2 = np.array([[0, 0, 1], [1, 0, 1]])","2031","","2032","    # size(y1 \\inter y2) = [1, 2]","2033","    # size(y1 \\union y2) = [2, 2]","2034","","2035","    jss = partial(assert_warns, DeprecationWarning, jaccard_similarity_score)","2036","    assert_equal(jss(y1, y2), 0.75)","2037","    assert_equal(jss(y1, y1), 1)","2038","    assert_equal(jss(y2, y2), 1)","2039","    assert_equal(jss(y2, np.logical_not(y2)), 0)","2040","    assert_equal(jss(y1, np.logical_not(y1)), 0)","2041","    assert_equal(jss(y1, np.zeros(y1.shape)), 0)","2042","    assert_equal(jss(y2, np.zeros(y1.shape)), 0)"],"delete":["13","from sklearn.preprocessing import label_binarize","1142","def test_multilabel_jaccard_similarity_score():","1150","    assert_equal(jaccard_similarity_score(y1, y2), 0.75)","1151","    assert_equal(jaccard_similarity_score(y1, y1), 1)","1152","    assert_equal(jaccard_similarity_score(y2, y2), 1)","1153","    assert_equal(jaccard_similarity_score(y2, np.logical_not(y2)), 0)","1154","    assert_equal(jaccard_similarity_score(y1, np.logical_not(y1)), 0)","1155","    assert_equal(jaccard_similarity_score(y1, np.zeros(y1.shape)), 0)","1156","    assert_equal(jaccard_similarity_score(y2, np.zeros(y1.shape)), 0)","1539","    for y_true, y_pred, y_type in [","1540","        (y_true_mc, y_pred_mc, 'multiclass'),","1541","        (y_true_ind, y_pred_ind, 'multilabel-indicator'),","1545","            assert_raise_message(ValueError,","1546","                                 \"Target is %s but average='binary'. Please \"","1547","                                 \"choose another average setting.\" % y_type,"]}],"examples\/multioutput\/plot_classifier_chain_yeast.py":[{"add":["12",":ref:`jaccard score <jaccard_score>` for each sample.","43","from sklearn.metrics import jaccard_score","60","ovr_jaccard_score = jaccard_score(Y_test, Y_pred_ovr, average='samples')","71","chain_jaccard_scores = [jaccard_score(Y_test, Y_pred_chain >= .5,","72","                                      average='samples')","76","ensemble_jaccard_score = jaccard_score(Y_test,","77","                                       Y_pred_ensemble >= .5,","78","                                       average='samples')"],"delete":["12",":ref:`jaccard similarity score <jaccard_similarity_score>`.","43","from sklearn.metrics import jaccard_similarity_score","60","ovr_jaccard_score = jaccard_similarity_score(Y_test, Y_pred_ovr)","71","chain_jaccard_scores = [jaccard_similarity_score(Y_test, Y_pred_chain >= .5)","75","ensemble_jaccard_score = jaccard_similarity_score(Y_test,","76","                                                  Y_pred_ensemble >= .5)"]}],"doc\/modules\/model_evaluation.rst":[{"add":["72","'jaccard' etc.                    :func:`metrics.jaccard_score`                     suffixes apply as with 'f1'","329","   jaccard_score","349",".. _average:","350","","923",".. _jaccard_similarity_score:","924","","925","Jaccard similarity coefficient score","926","-------------------------------------","927","","928","The :func:`jaccard_score` function computes the average of `Jaccard similarity","929","coefficients <https:\/\/en.wikipedia.org\/wiki\/Jaccard_index>`_, also called the","930","Jaccard index, between pairs of label sets.","931","","932","The Jaccard similarity coefficient of the :math:`i`-th samples,","933","with a ground truth label set :math:`y_i` and predicted label set","934",":math:`\\hat{y}_i`, is defined as","935","","936",".. math::","937","","938","    J(y_i, \\hat{y}_i) = \\frac{|y_i \\cap \\hat{y}_i|}{|y_i \\cup \\hat{y}_i|}.","939","","940",":func:`jaccard_score` works like :func:`precision_recall_fscore_support` as a","941","naively set-wise measure applying natively to binary targets, and extended to","942","apply to multilabel and multiclass through the use of `average` (see","943",":ref:`above <average>`).","944","","945","In the binary case: ::","946","","947","  >>> import numpy as np","948","  >>> from sklearn.metrics import jaccard_score","949","  >>> y_true = np.array([[0, 1, 1],","950","  ...                    [1, 1, 0]])","951","  >>> y_pred = np.array([[1, 1, 1],","952","  ...                    [1, 0, 0]])","953","  >>> jaccard_score(y_true[0], y_pred[0])  # doctest: +ELLIPSIS","954","  0.6666...","955","","956","In the multilabel case with binary label indicators: ::","957","","958","  >>> jaccard_score(y_true, y_pred, average='samples')  # doctest: +ELLIPSIS","959","  0.5833...","960","  >>> jaccard_score(y_true, y_pred, average='macro')  # doctest: +ELLIPSIS","961","  0.6666...","962","  >>> jaccard_score(y_true, y_pred, average=None)","963","  array([0.5, 0.5, 1. ])","964","","965","Multiclass problems are binarized and treated like the corresponding","966","multilabel problem: ::","967","","968","  >>> y_pred = [0, 2, 1, 2]","969","  >>> y_true = [0, 1, 2, 2]","970","  >>> jaccard_score(y_true, y_pred, average=None)","971","  ... # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS","972","  array([1. , 0. , 0.33...])","973","  >>> jaccard_score(y_true, y_pred, average='macro')","974","  0.44...","975","  >>> jaccard_score(y_true, y_pred, average='micro')","976","  0.33...","977",""],"delete":["328","   jaccard_similarity_score","357",".. _average:","358","","682",".. _jaccard_similarity_score:","683","","684","Jaccard similarity coefficient score","685","-------------------------------------","686","","687","The :func:`jaccard_similarity_score` function computes the average (default)","688","or sum of `Jaccard similarity coefficients","689","<https:\/\/en.wikipedia.org\/wiki\/Jaccard_index>`_, also called the Jaccard index,","690","between pairs of label sets.","691","","692","The Jaccard similarity coefficient of the :math:`i`-th samples,","693","with a ground truth label set :math:`y_i` and predicted label set","694",":math:`\\hat{y}_i`, is defined as","695","","696",".. math::","697","","698","    J(y_i, \\hat{y}_i) = \\frac{|y_i \\cap \\hat{y}_i|}{|y_i \\cup \\hat{y}_i|}.","699","","700","In binary and multiclass classification, the Jaccard similarity coefficient","701","score is equal to the classification accuracy.","702","","703","::","704","","705","  >>> import numpy as np","706","  >>> from sklearn.metrics import jaccard_similarity_score","707","  >>> y_pred = [0, 2, 1, 3]","708","  >>> y_true = [0, 1, 2, 3]","709","  >>> jaccard_similarity_score(y_true, y_pred)","710","  0.5","711","  >>> jaccard_similarity_score(y_true, y_pred, normalize=False)","712","  2","713","","714","In the multilabel case with binary label indicators: ::","715","","716","  >>> jaccard_similarity_score(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))","717","  0.75","718",""]}],"sklearn\/metrics\/__init__.py":[{"add":["25","from .classification import jaccard_score","101","    'jaccard_score',"],"delete":[]}],"sklearn\/tests\/test_multioutput.py":[{"add":["25","from sklearn.metrics import jaccard_score, mean_squared_error","433","    assert_greater(jaccard_score(Y_test, Y_pred_chain, average='samples'),","434","                   jaccard_score(Y_test, Y_pred_ovr, average='samples'))","512","            assert jaccard_score(Y, Y_pred_cv, average='samples') > .4"],"delete":["25","from sklearn.metrics import jaccard_similarity_score, mean_squared_error","433","    assert_greater(jaccard_similarity_score(Y_test, Y_pred_chain),","434","                   jaccard_similarity_score(Y_test, Y_pred_ovr))","512","            assert jaccard_similarity_score(Y, Y_pred_cv) > .4"]}]}},"98c1f07079387efa6efb635af514938adf50902e":{"changes":{"sklearn\/utils\/testing.py":"MODIFY"},"diff":{"sklearn\/utils\/testing.py":[{"add":["649","        if \".tests.\" in modname or \"externals\" in modname:"],"delete":["649","        if \".tests.\" in modname:"]}]}},"0a5af0d2a11c64d59381110f3967acbe7d88a031":{"changes":{"sklearn\/experimental\/enable_successive_halving.py":"ADD","sklearn\/tests\/test_docstring_parameters.py":"MODIFY","doc\/modules\/classes.rst":"MODIFY","sklearn\/model_selection\/_search_successive_halving.py":"ADD","sklearn\/model_selection\/tests\/test_split.py":"MODIFY","sklearn\/model_selection\/tests\/test_successive_halving.py":"ADD","doc\/conftest.py":"MODIFY","sklearn\/experimental\/tests\/test_enable_successive_halving.py":"ADD","doc\/conf.py":"MODIFY","sklearn\/model_selection\/__init__.py":"MODIFY","sklearn\/model_selection\/_split.py":"MODIFY","doc\/modules\/grid_search.rst":"MODIFY","examples\/model_selection\/plot_successive_halving_iterations.py":"ADD","examples\/model_selection\/plot_successive_halving_heatmap.py":"ADD","doc\/whats_new\/v0.24.rst":"MODIFY","sklearn\/model_selection\/_search.py":"MODIFY"},"diff":{"sklearn\/experimental\/enable_successive_halving.py":[{"add":[],"delete":[]}],"sklearn\/tests\/test_docstring_parameters.py":[{"add":["191","               'VotingRegressor', 'SequentialFeatureSelector',","192","               'HalvingGridSearchCV', 'HalvingRandomSearchCV'}"],"delete":["191","               'VotingRegressor', 'SequentialFeatureSelector'}"]}],"doc\/modules\/classes.rst":[{"add":["1196","   model_selection.HalvingGridSearchCV","1200","   model_selection.HalvingRandomSearchCV"],"delete":[]}],"sklearn\/model_selection\/_search_successive_halving.py":[{"add":[],"delete":[]}],"sklearn\/model_selection\/tests\/test_split.py":[{"add":["45","from sklearn.model_selection._split import _yields_constant_splits","1622","","1623","","1624","@pytest.mark.parametrize('cv, expected', [","1625","    (KFold(), True),","1626","    (KFold(shuffle=True, random_state=123), True),","1627","    (StratifiedKFold(), True),","1628","    (StratifiedKFold(shuffle=True, random_state=123), True),","1629","    (RepeatedKFold(random_state=123), True),","1630","    (RepeatedStratifiedKFold(random_state=123), True),","1631","    (ShuffleSplit(random_state=123), True),","1632","    (GroupShuffleSplit(random_state=123), True),","1633","    (StratifiedShuffleSplit(random_state=123), True),","1634","    (GroupKFold(), True),","1635","    (TimeSeriesSplit(), True),","1636","    (LeaveOneOut(), True),","1637","    (LeaveOneGroupOut(), True),","1638","    (LeavePGroupsOut(n_groups=2), True),","1639","    (LeavePOut(p=2), True),","1640","","1641","    (KFold(shuffle=True, random_state=None), False),","1642","    (KFold(shuffle=True, random_state=None), False),","1643","    (StratifiedKFold(shuffle=True, random_state=np.random.RandomState(0)),","1644","     False),","1645","    (StratifiedKFold(shuffle=True, random_state=np.random.RandomState(0)),","1646","     False),","1647","    (RepeatedKFold(random_state=None), False),","1648","    (RepeatedKFold(random_state=np.random.RandomState(0)), False),","1649","    (RepeatedStratifiedKFold(random_state=None), False),","1650","    (RepeatedStratifiedKFold(random_state=np.random.RandomState(0)), False),","1651","    (ShuffleSplit(random_state=None), False),","1652","    (ShuffleSplit(random_state=np.random.RandomState(0)), False),","1653","    (GroupShuffleSplit(random_state=None), False),","1654","    (GroupShuffleSplit(random_state=np.random.RandomState(0)), False),","1655","    (StratifiedShuffleSplit(random_state=None), False),","1656","    (StratifiedShuffleSplit(random_state=np.random.RandomState(0)), False),","1657","])","1658","def test_yields_constant_splits(cv, expected):","1659","    assert _yields_constant_splits(cv) == expected"],"delete":[]}],"sklearn\/model_selection\/tests\/test_successive_halving.py":[{"add":[],"delete":[]}],"doc\/conftest.py":[{"add":["59","def setup_grid_search():","60","    try:","61","        import pandas  # noqa","62","    except ImportError:","63","        raise SkipTest(\"Skipping grid_search.rst, pandas not installed\")","64","","65","","95","    elif fname.endswith('modules\/grid_search.rst'):","96","        setup_grid_search()"],"delete":[]}],"sklearn\/experimental\/tests\/test_enable_successive_halving.py":[{"add":[],"delete":[]}],"doc\/conf.py":[{"add":["358","from sklearn.experimental import enable_successive_halving  # noqa"],"delete":[]}],"sklearn\/model_selection\/__init__.py":[{"add":["0","import typing","1","","33","if typing.TYPE_CHECKING:","34","    # Avoid errors in type checkers (e.g. mypy) for experimental estimators.","35","    # TODO: remove this check once the estimator is no longer experimental.","36","    from ._search_successive_halving import (  # noqa","37","        HalvingGridSearchCV, HalvingRandomSearchCV","38","    )","39","","40","","41","__all__ = ['BaseCrossValidator',","68","           'validation_curve']"],"delete":["31","__all__ = ('BaseCrossValidator',","58","           'validation_curve')"]}],"sklearn\/model_selection\/_split.py":[{"add":["2237","","2238","","2239","def _yields_constant_splits(cv):","2240","    # Return True if calling cv.split() always returns the same splits","2241","    # We assume that if a cv doesn't have a shuffle parameter, it shuffles by","2242","    # default (e.g. ShuffleSplit). If it actually doesn't shuffle (e.g.","2243","    # LeaveOneOut), then it won't have a random_state parameter anyway, in","2244","    # which case it will default to 0, leading to output=True","2245","    shuffle = getattr(cv, 'shuffle', True)","2246","    random_state = getattr(cv, 'random_state', 0)","2247","    return isinstance(random_state, numbers.Integral) or not shuffle"],"delete":[]}],"doc\/modules\/grid_search.rst":[{"add":["32","Two generic approaches to parameter search are provided in","36","distribution. Both these tools have successive halving counterparts","37",":class:`HalvingGridSearchCV` and :class:`HalvingRandomSearchCV`, which can be","38","much faster at finding a good parameter combination.","39","","40","After describing these tools we detail :ref:`best practices","41","<grid_search_tips>` applicable to these approaches. Some models allow for","42","specialized, efficient parameter search strategies, outlined in","43",":ref:`alternative_cv`.","173",".. _successive_halving_user_guide:","174","","175","Searching for optimal parameters with successive halving","176","========================================================","177","","178","Scikit-learn also provides the :class:`HalvingGridSearchCV` and","179",":class:`HalvingRandomSearchCV` estimators that can be used to","180","search a parameter space using successive halving [1]_ [2]_. Successive","181","halving (SH) is like a tournament among candidate parameter combinations.","182","SH is an iterative selection process where all candidates (the","183","parameter combinations) are evaluated with a small amount of resources at","184","the first iteration. Only some of these candidates are selected for the next","185","iteration, which will be allocated more resources. For parameter tuning, the","186","resource is typically the number of training samples, but it can also be an","187","arbitrary numeric parameter such as `n_estimators` in a random forest.","188","","189","As illustrated in the figure below, only a subset of candidates","190","'survive' until the last iteration. These are the candidates that have","191","consistently ranked among the top-scoring candidates across all iterations.","192","Each iteration is allocated an increasing amount of resources per candidate,","193","here the number of samples.","194","","195",".. figure:: ..\/auto_examples\/model_selection\/images\/sphx_glr_plot_successive_halving_iterations_001.png","196","   :target: ..\/auto_examples\/model_selection\/plot_successive_halving_iterations.html","197","   :align: center","198","","199","We here briefly describe the main parameters, but each parameter and their","200","interactions are described in more details in the sections below. The","201","``factor`` (> 1) parameter controls the rate at which the resources grow, and","202","the rate at which the number of candidates decreases. In each iteration, the","203","number of resources per candidate is multiplied by ``factor`` and the number","204","of candidates is divided by the same factor. Along with ``resource`` and","205","``min_resources``, ``factor`` is the most important parameter to control the","206","search in our implementation, though a value of 3 usually works well.","207","``factor`` effectively controls the number of iterations in","208",":class:`HalvingGridSearchCV` and the number of candidates (by default) and","209","iterations in :class:`HalvingRandomSearchCV`. ``aggressive_elimination=True``","210","can also be used if the number of available resources is small. More control","211","is available through tuning the ``min_resources`` parameter.","212","","213","These estimators are still **experimental**: their predictions","214","and their API might change without any deprecation cycle. To use them, you","215","need to explicitly import ``enable_successive_halving``::","216","","217","  >>> # explicitly require this experimental feature","218","  >>> from sklearn.experimental import enable_successive_halving  # noqa","219","  >>> # now you can import normally from model_selection","220","  >>> from sklearn.model_selection import HalvingGridSearchCV","221","  >>> from sklearn.model_selection import HalvingRandomSearchCV","222","","223",".. topic:: Examples:","224","","225","    * :ref:`sphx_glr_auto_examples_model_selection_plot_successive_halving_heatmap.py`","226","    * :ref:`sphx_glr_auto_examples_model_selection_plot_successive_halving_iterations.py`","227","","228","Choosing ``min_resources`` and the number of candidates","229","-------------------------------------------------------","230","","231","Beside ``factor``, the two main parameters that influence the behaviour of a","232","successive halving search are the ``min_resources`` parameter, and the","233","number of candidates (or parameter combinations) that are evaluated.","234","``min_resources`` is the amount of resources allocated at the first","235","iteration for each candidate. The number of candidates is specified directly","236","in :class:`HalvingRandomSearchCV`, and is determined from the ``param_grid``","237","parameter of :class:`HalvingGridSearchCV`.","238","","239","Consider a case where the resource is the number of samples, and where we","240","have 1000 samples. In theory, with ``min_resources=10`` and ``factor=2``, we","241","are able to run **at most** 7 iterations with the following number of","242","samples: ``[10, 20, 40, 80, 160, 320, 640]``.","243","","244","But depending on the number of candidates, we might run less than 7","245","iterations: if we start with a **small** number of candidates, the last","246","iteration might use less than 640 samples, which means not using all the","247","available resources (samples). For example if we start with 5 candidates, we","248","only need 2 iterations: 5 candidates for the first iteration, then","249","`5 \/\/ 2 = 2` candidates at the second iteration, after which we know which","250","candidate performs the best (so we don't need a third one). We would only be","251","using at most 20 samples which is a waste since we have 1000 samples at our","252","disposal. On the other hand, if we start with a **high** number of","253","candidates, we might end up with a lot of candidates at the last iteration,","254","which may not always be ideal: it means that many candidates will run with","255","the full resources, basically reducing the procedure to standard search.","256","","257","In the case of :class:`HalvingRandomSearchCV`, the number of candidates is set","258","by default such that the last iteration uses as much of the available","259","resources as possible. For :class:`HalvingGridSearchCV`, the number of","260","candidates is determined by the `param_grid` parameter. Changing the value of","261","``min_resources`` will impact the number of possible iterations, and as a","262","result will also have an effect on the ideal number of candidates.","263","","264","Another consideration when choosing ``min_resources`` is whether or not it","265","is easy to discriminate between good and bad candidates with a small amount","266","of resources. For example, if you need a lot of samples to distinguish","267","between good and bad parameters, a high ``min_resources`` is recommended. On","268","the other hand if the distinction is clear even with a small amount of","269","samples, then a small ``min_resources`` may be preferable since it would","270","speed up the computation.","271","","272","Notice in the example above that the last iteration does not use the maximum","273","amount of resources available: 1000 samples are available, yet only 640 are","274","used, at most. By default, both :class:`HalvingRandomSearchCV` and","275",":class:`HalvingGridSearchCV` try to use as many resources as possible in the","276","last iteration, with the constraint that this amount of resources must be a","277","multiple of both `min_resources` and `factor` (this constraint will be clear","278","in the next section). :class:`HalvingRandomSearchCV` achieves this by","279","sampling the right amount of candidates, while :class:`HalvingGridSearchCV`","280","achieves this by properly setting `min_resources`. Please see","281",":ref:`exhausting_the_resources` for details.","282","","283",".. _amount_of_resource_and_number_of_candidates:","284","","285","Amount of resource and number of candidates at each iteration","286","-------------------------------------------------------------","287","","288","At any iteration `i`, each candidate is allocated a given amount of resources","289","which we denote `n_resources_i`. This quantity is controlled by the","290","parameters ``factor`` and ``min_resources`` as follows (`factor` is strictly","291","greater than 1)::","292","","293","    n_resources_i = factor**i * min_resources,","294","","295","or equivalently::","296","","297","    n_resources_{i+1} = n_resources_i * factor","298","","299","where ``min_resources == n_resources_0`` is the amount of resources used at","300","the first iteration. ``factor`` also defines the proportions of candidates","301","that will be selected for the next iteration::","302","","303","    n_candidates_i = n_candidates \/\/ (factor ** i)","304","","305","or equivalently::","306","","307","    n_candidates_0 = n_candidates","308","    n_candidates_{i+1} = n_candidates_i \/\/ factor","309","","310","So in the first iteration, we use ``min_resources`` resources","311","``n_candidates`` times. In the second iteration, we use ``min_resources *","312","factor`` resources ``n_candidates \/\/ factor`` times. The third again","313","multiplies the resources per candidate and divides the number of candidates.","314","This process stops when the maximum amount of resource per candidate is","315","reached, or when we have identified the best candidate. The best candidate","316","is identified at the iteration that is evaluating `factor` or less candidates","317","(see just below for an explanation).","318","","319","Here is an example with ``min_resources=3`` and ``factor=2``, starting with","320","70 candidates:","321","","322","+-----------------------+-----------------------+","323","| ``n_resources_i``     | ``n_candidates_i``    |","324","+=======================+=======================+","325","| 3 (=min_resources)    | 70 (=n_candidates)    |","326","+-----------------------+-----------------------+","327","| 3 * 2 = 6             | 70 \/\/ 2 = 35          |","328","+-----------------------+-----------------------+","329","| 6 * 2 = 12            | 35 \/\/ 2 = 17          |","330","+-----------------------+-----------------------+","331","| 12 * 2 = 24           | 17 \/\/ 2 = 8           |","332","+-----------------------+-----------------------+","333","| 24 * 2 = 48           | 8 \/\/ 2 = 4            |","334","+-----------------------+-----------------------+","335","| 48 * 2 = 96           | 4 \/\/ 2 = 2            |","336","+-----------------------+-----------------------+","337","","338","We can note that:","339","","340","- the process stops at the first iteration which evaluates `factor=2`","341","  candidates: the best candidate is the best out of these 2 candidates. It","342","  is not necessary to run an additional iteration, since it would only","343","  evaluate one candidate (namely the best one, which we have already","344","  identified). For this reason, in general, we want the last iteration to","345","  run at most ``factor`` candidates. If the last iteration evaluates more","346","  than `factor` candidates, then this last iteration reduces to a regular","347","  search (as in :class:`RandomizedSearchCV` or :class:`GridSearchCV`).","348","- each ``n_resources_i`` is a multiple of both ``factor`` and","349","  ``min_resources`` (which is confirmed by its definition above).","350","","351","The amount of resources that is used at each iteration can be found in the","352","`n_resources_` attribute.","353","","354","Choosing a resource","355","-------------------","356","","357","By default, the resource is defined in terms of number of samples. That is,","358","each iteration will use an increasing amount of samples to train on. You can","359","however manually specify a parameter to use as the resource with the","360","``resource`` parameter. Here is an example where the resource is defined in","361","terms of the number of estimators of a random forest::","362","","363","    >>> from sklearn.datasets import make_classification","364","    >>> from sklearn.ensemble import RandomForestClassifier","365","    >>> from sklearn.experimental import enable_successive_halving  # noqa","366","    >>> from sklearn.model_selection import HalvingGridSearchCV","367","    >>> import pandas as pd","368","    >>>","369","    >>> param_grid = {'max_depth': [3, 5, 10],","370","    ...               'min_samples_split': [2, 5, 10]}","371","    >>> base_estimator = RandomForestClassifier(random_state=0)","372","    >>> X, y = make_classification(n_samples=1000, random_state=0)","373","    >>> sh = HalvingGridSearchCV(base_estimator, param_grid, cv=5,","374","    ...                          factor=2, resource='n_estimators',","375","    ...                          max_resources=30).fit(X, y)","376","    >>> sh.best_estimator_","377","    RandomForestClassifier(max_depth=5, n_estimators=24, random_state=0)","378","","379","Note that it is not possible to budget on a parameter that is part of the","380","parameter grid.","381","","382",".. _exhausting_the_resources:","383","","384","Exhausting the available resources","385","----------------------------------","386","","387","As mentioned above, the number of resources that is used at each iteration","388","depends on the `min_resources` parameter.","389","If you have a lot of resources available but start with a low number of","390","resources, some of them might be wasted (i.e. not used)::","391","","392","    >>> from sklearn.datasets import make_classification","393","    >>> from sklearn.svm import SVC","394","    >>> from sklearn.experimental import enable_successive_halving  # noqa","395","    >>> from sklearn.model_selection import HalvingGridSearchCV","396","    >>> import pandas as pd","397","    >>> param_grid= {'kernel': ('linear', 'rbf'),","398","    ...              'C': [1, 10, 100]}","399","    >>> base_estimator = SVC(gamma='scale')","400","    >>> X, y = make_classification(n_samples=1000)","401","    >>> sh = HalvingGridSearchCV(base_estimator, param_grid, cv=5,","402","    ...                          factor=2, min_resources=20).fit(X, y)","403","    >>> sh.n_resources_","404","    [20, 40, 80]","405","","406","The search process will only use 80 resources at most, while our maximum","407","amount of available resources is ``n_samples=1000``. Here, we have","408","``min_resources = r_0 = 20``.","409","","410","For :class:`HalvingGridSearchCV`, by default, the `min_resources` parameter","411","is set to 'exhaust'. This means that `min_resources` is automatically set","412","such that the last iteration can use as many resources as possible, within","413","the `max_resources` limit::","414","","415","    >>> sh = HalvingGridSearchCV(base_estimator, param_grid, cv=5,","416","    ...                          factor=2, min_resources='exhaust').fit(X, y)","417","    >>> sh.n_resources_","418","    [250, 500, 1000]","419","","420","`min_resources` was here automatically set to 250, which results in the last","421","iteration using all the resources. The exact value that is used depends on","422","the number of candidate parameter, on `max_resources` and on `factor`.","423","","424","For :class:`HalvingRandomSearchCV`, exhausting the resources can be done in 2","425","ways:","426","","427","- by setting `min_resources='exhaust'`, just like for","428","  :class:`HalvingGridSearchCV`;","429","- by setting `n_candidates='exhaust'`.","430","","431","Both options are mutally exclusive: using `min_resources='exhaust'` requires","432","knowing the number of candidates, and symmetrically `n_candidates='exhaust'`","433","requires knowing `min_resources`.","434","","435","In general, exhausting the total number of resources leads to a better final","436","candidate parameter, and is slightly more time-intensive.","437","","438",".. _aggressive_elimination:","439","","440","Aggressive elimination of candidates","441","------------------------------------","442","","443","Ideally, we want the last iteration to evaluate ``factor`` candidates (see","444",":ref:`amount_of_resource_and_number_of_candidates`). We then just have to","445","pick the best one. When the number of available resources is small with","446","respect to the number of candidates, the last iteration may have to evaluate","447","more than ``factor`` candidates::","448","","449","    >>> from sklearn.datasets import make_classification","450","    >>> from sklearn.svm import SVC","451","    >>> from sklearn.experimental import enable_successive_halving  # noqa","452","    >>> from sklearn.model_selection import HalvingGridSearchCV","453","    >>> import pandas as pd","454","    >>>","455","    >>>","456","    >>> param_grid = {'kernel': ('linear', 'rbf'),","457","    ...               'C': [1, 10, 100]}","458","    >>> base_estimator = SVC(gamma='scale')","459","    >>> X, y = make_classification(n_samples=1000)","460","    >>> sh = HalvingGridSearchCV(base_estimator, param_grid, cv=5,","461","    ...                          factor=2, max_resources=40,","462","    ...                          aggressive_elimination=False).fit(X, y)","463","    >>> sh.n_resources_","464","    [20, 40]","465","    >>> sh.n_candidates_","466","    [6, 3]","467","","468","Since we cannot use more than ``max_resources=40`` resources, the process","469","has to stop at the second iteration which evaluates more than ``factor=2``","470","candidates.","471","","472","Using the ``aggressive_elimination`` parameter, you can force the search","473","process to end up with less than ``factor`` candidates at the last","474","iteration. To do this, the process will eliminate as many candidates as","475","necessary using ``min_resources`` resources::","476","","477","    >>> sh = HalvingGridSearchCV(base_estimator, param_grid, cv=5,","478","    ...                            factor=2,","479","    ...                            max_resources=40,","480","    ...                            aggressive_elimination=True,","481","    ...                            ).fit(X, y)","482","    >>> sh.n_resources_","483","    [20, 20,  40]","484","    >>> sh.n_candidates_","485","    [6, 3, 2]","486","","487","Notice that we end with 2 candidates at the last iteration since we have","488","eliminated enough candidates during the first iterations, using ``n_resources =","489","min_resources = 20``.","490","","491",".. _successive_halving_cv_results:","492","","493","Analysing results with the `cv_results_` attribute","494","--------------------------------------------------","495","","496","The ``cv_results_`` attribute contains useful information for analysing the","497","results of a search. It can be converted to a pandas dataframe with ``df =","498","pd.DataFrame(est.cv_results_)``. The ``cv_results_`` attribute of","499",":class:`HalvingGridSearchCV` and :class:`HalvingRandomSearchCV` is similar","500","to that of :class:`GridSearchCV` and :class:`RandomizedSearchCV`, with","501","additional information related to the successive halving process.","502","","503","Here is an example with some of the columns of a (truncated) dataframe:","504","","505","====  ======  ===============  =================  =======================================================================================","506","  ..    iter      n_resources    mean_test_score  params","507","====  ======  ===============  =================  =======================================================================================","508","   0       0              125           0.983667  {'criterion': 'entropy', 'max_depth': None, 'max_features': 9, 'min_samples_split': 5}","509","   1       0              125           0.983667  {'criterion': 'gini', 'max_depth': None, 'max_features': 8, 'min_samples_split': 7}","510","   2       0              125           0.983667  {'criterion': 'gini', 'max_depth': None, 'max_features': 10, 'min_samples_split': 10}","511","   3       0              125           0.983667  {'criterion': 'entropy', 'max_depth': None, 'max_features': 6, 'min_samples_split': 6}","512"," ...     ...              ...                ...  ...","513","  15       2              500           0.951958  {'criterion': 'entropy', 'max_depth': None, 'max_features': 9, 'min_samples_split': 10}","514","  16       2              500           0.947958  {'criterion': 'gini', 'max_depth': None, 'max_features': 10, 'min_samples_split': 10}","515","  17       2              500           0.951958  {'criterion': 'gini', 'max_depth': None, 'max_features': 10, 'min_samples_split': 4}","516","  18       3             1000           0.961009  {'criterion': 'entropy', 'max_depth': None, 'max_features': 9, 'min_samples_split': 10}","517","  19       3             1000           0.955989  {'criterion': 'gini', 'max_depth': None, 'max_features': 10, 'min_samples_split': 4}","518","====  ======  ===============  =================  =======================================================================================","519","","520","Each row corresponds to a given parameter combination (a candidate) and a given","521","iteration. The iteration is given by the ``iter`` column. The ``n_resources``","522","column tells you how many resources were used.","523","","524","In the example above, the best parameter combination is ``{'criterion':","525","'entropy', 'max_depth': None, 'max_features': 9, 'min_samples_split': 10}``","526","since it has reached the last iteration (3) with the highest score:","527","0.96.","528","","529",".. topic:: References:","530","","531","    .. [1] K. Jamieson, A. Talwalkar,","532","       `Non-stochastic Best Arm Identification and Hyperparameter","533","       Optimization <http:\/\/proceedings.mlr.press\/v51\/jamieson16.html>`_, in","534","       proc. of Machine Learning Research, 2016.","535","    .. [2] L. Li, K. Jamieson, G. DeSalvo, A. Rostamizadeh, A. Talwalkar,","536","       `Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization","537","       <https:\/\/arxiv.org\/abs\/1603.06560>`_, in Machine Learning Research","538","       18, 2018.","539","","556","scoring function can be specified via the ``scoring`` parameter of most","557","parameter search tools. See :ref:`scoring_parameter` for more details.","564",":class:`GridSearchCV` and :class:`RandomizedSearchCV` allow specifying","565","multiple metrics for the ``scoring`` parameter.","580",":class:`HalvingRandomSearchCV` and :class:`HalvingGridSearchCV` do not support","581","multimetric scoring.","582","","627","Please refer to :ref:`pipeline` for performing parameter searches over","628","pipelines.","639","be fed to the :class:`GridSearchCV` instance) and an **evaluation set**","648","The parameter search tools evaluate each parameter combination on each data","649","fold independently. Computations can be run in parallel by using the keyword","650","``n_jobs=-1``. See function signature for more details, and also the Glossary","651","entry for :term:`n_jobs`."],"delete":["32","Some models allow for specialized, efficient parameter search strategies,","33",":ref:`outlined below <alternative_cv>`.","34","Two generic approaches to sampling search candidates are provided in","38","distribution. After describing these tools we detail","39",":ref:`best practice <grid_search_tips>` applicable to both approaches.","185","scoring function can be specified via the ``scoring`` parameter to","186",":class:`GridSearchCV`, :class:`RandomizedSearchCV` and many of the","187","specialized cross-validation tools described below.","188","See :ref:`scoring_parameter` for more details.","195","``GridSearchCV`` and ``RandomizedSearchCV`` allow specifying multiple metrics","196","for the ``scoring`` parameter.","265","be fed to the ``GridSearchCV`` instance) and an **evaluation set**","274",":class:`GridSearchCV` and :class:`RandomizedSearchCV` evaluate each parameter","275","setting independently.  Computations can be run in parallel if your OS","276","supports it, by using the keyword ``n_jobs=-1``. See function signature for","277","more details."]}],"examples\/model_selection\/plot_successive_halving_iterations.py":[{"add":[],"delete":[]}],"examples\/model_selection\/plot_successive_halving_heatmap.py":[{"add":[],"delete":[]}],"doc\/whats_new\/v0.24.rst":[{"add":["414","- |Feature| Added (experimental) parameter search estimators","415","  :class:`model_selection.HalvingRandomSearchCV` and","416","  :class:`model_selection.HalvingGridSearchCV` which implement Successive","417","  Halving, and can be used as a drop-in replacements for","418","  :class:`model_selection.RandomizedSearchCV` and","419","  :class:`model_selection.GridSearchCV`. :pr:`13900` by `Nicolas Hug`_, `Joel","420","  Nothman`_ and `Andreas M¨¹ller`_.","421",""],"delete":[]}],"sklearn\/model_selection\/_search.py":[{"add":["643","        For example, Successive Halving is implemented by calling","644","        `evaluate_candidates` multiples times (once per iteration of the SH","645","        process), each time passing a different set of candidates with `X`","646","        and `y` of increasing sizes.","651","            This callback accepts:","652","                - a list of candidates, where each candidate is a dict of","653","                  parameter settings.","654","                - an optional `cv` parameter which can be used to e.g.","655","                  evaluate candidates on different dataset splits, or","656","                  evaluate candidates on subsampled data (as done in the","657","                  SucessiveHaling estimators). By default, the original `cv`","658","                  parameter is used, and it is available as a private","659","                  `_checked_cv_orig` attribute.","660","                - an optional `more_results` dict. Each key will be added to","661","                  the `cv_results_` attribute. Values should be lists of","662","                  length `n_candidates`","663","","664","            It returns a dict of all results so far, formatted like","665","            ``cv_results_``.","666","","667","            Important note (relevant whether the default cv is used or not):","668","            in randomized splitters, and unless the random_state parameter of","669","            cv was set to an int, calling cv.split() multiple times will","670","            yield different splits. Since cv.split() is called in","671","            evaluate_candidates, this means that candidates will be evaluated","672","            on different splits each time evaluate_candidates is called. This","673","            might be a methodological issue depending on the search strategy","674","            that you're implementing. To prevent randomized splitters from","675","            being used, you may use _split._yields_constant_splits()","747","        cv_orig = check_cv(self.cv, y, classifier=is_classifier(estimator))","748","        n_splits = cv_orig.get_n_splits(X, y, groups)","767","            all_more_results = defaultdict(list)","769","            def evaluate_candidates(candidate_params, cv=None,","770","                                    more_results=None):","771","                cv = cv or cv_orig","815","                if more_results is not None:","816","                    for key, value in more_results.items():","817","                        all_more_results[key].extend(value)","821","                    all_candidate_params, n_splits, all_out,","822","                    all_more_results)","823","","879","    def _format_results(self, candidate_params, n_splits, out,","880","                        more_results=None):","884","        results = dict(more_results or {})"],"delete":["647","            This callback accepts a list of candidates, where each candidate is","648","            a dict of parameter settings. It returns a dict of all results so","649","            far, formatted like ``cv_results_``.","707","        cv = check_cv(self.cv, y, classifier=is_classifier(estimator))","708","","723","        n_splits = cv.get_n_splits(X, y, groups)","743","            def evaluate_candidates(candidate_params):","790","                    all_candidate_params, n_splits, all_out)","846","    def _format_results(self, candidate_params, n_splits, out):","850","        results = {}"]}]}},"f483a70dba3d094c8b612d0cb820338745351e18":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/tests\/test_dummy.py":"MODIFY","sklearn\/dummy.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["162","- |Fix| Fixed a bug in :class:`dummy.DummyClassifier` where it was throwing a","163","  dimension mismatch error in prediction time if a column vector ``y`` with","164","  ``shape=(n, 1)`` was given at ``fit`` time. :issue:`13545` by :user:`Nick","165","  Sorros <nsorros>` and `Adrin Jalali`_.","166",""],"delete":[]}],"sklearn\/tests\/test_dummy.py":[{"add":["104","def test_most_frequent_and_prior_strategy_with_2d_column_y():","105","    # non-regression test added in","106","    # https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/13545","107","    X = [[0], [0], [0], [0]]","108","    y_1d = [1, 2, 1, 1]","109","    y_2d = [[1], [2], [1], [1]]","110","","111","    for strategy in (\"most_frequent\", \"prior\"):","112","        clf_1d = DummyClassifier(strategy=strategy, random_state=0)","113","        clf_2d = DummyClassifier(strategy=strategy, random_state=0)","114","","115","        clf_1d.fit(X, y_1d)","116","        clf_2d.fit(X, y_2d)","117","        assert_array_equal(clf_1d.predict(X), clf_2d.predict(X))","118","","119",""],"delete":[]}],"sklearn\/dummy.py":[{"add":["122","        self.output_2d_ = y.ndim == 2 and y.shape[1] > 1","123","","428","        self.output_2d_ = y.ndim == 2 and y.shape[1] > 1","429",""],"delete":["122","        self.output_2d_ = y.ndim == 2","427","        self.output_2d_ = y.ndim == 2"]}]}},"adc1e590d4dc1e230b49a4c10b4cd7b672bb3d69":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/covariance\/graph_lasso_.py":"MODIFY","sklearn\/covariance\/tests\/test_graphical_lasso.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["31",":mod:`sklearn.covariance`","32","......................","33","","34","- |Fix| Fixed a regression in :func:`covariance.graphical_lasso` so that","35","  the case `n_features=2` is handled correctly. :issue:`13276` by","36","  :user:`Aur¨¦lien Bellet <bellet>`.","37",""],"delete":[]}],"sklearn\/covariance\/graph_lasso_.py":[{"add":["206","        sub_covariance = np.copy(covariance_[1:, 1:], order='C')"],"delete":["206","        sub_covariance = np.ascontiguousarray(covariance_[1:, 1:])"]}],"sklearn\/covariance\/tests\/test_graphical_lasso.py":[{"add":["87","def test_graph_lasso_2D():","88","    # Hard-coded solution from Python skggm package","89","    # obtained by calling `quic(emp_cov, lam=.1, tol=1e-8)`","90","    cov_skggm = np.array([[3.09550269, 1.186972],","91","                         [1.186972, 0.57713289]])","92","","93","    icov_skggm = np.array([[1.52836773, -3.14334831],","94","                          [-3.14334831,  8.19753385]])","95","    X = datasets.load_iris().data[:, 2:]","96","    emp_cov = empirical_covariance(X)","97","    for method in ('cd', 'lars'):","98","        cov, icov = graphical_lasso(emp_cov, alpha=.1, return_costs=False,","99","                                    mode=method)","100","        assert_array_almost_equal(cov, cov_skggm)","101","        assert_array_almost_equal(icov, icov_skggm)","102","","103",""],"delete":[]}]}},"fa383a4aca0dc503f798c9576e934b11bfc1a6a4":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/tree\/tests\/test_export.py":"MODIFY","sklearn\/tree\/export.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["46",":mod:`sklearn.tree`","47","................................","48","","49","- |Fix| Fixed an issue with :func:`plot_tree` where it display","50","  entropy calculations even for `gini` criterion in DecisionTreeClassifiers.","51","  :pr:`13947` by :user:`Frank Hoang <fhoang7>`.","52","","974","Frank Hoang, Fibinse Xavier`, Finn O'Shea, Gabriel Marzinotto, Gabriel Vacaliuc, ","975","Gabriele Calvo, Gael Varoquaux, GauravAhlawat, Giuseppe Vettigli, Greg Gandenberger,"],"delete":["967","Fibinse Xavier`, Finn O'Shea, Gabriel Marzinotto, Gabriel Vacaliuc, Gabriele","968","Calvo, Gael Varoquaux, GauravAhlawat, Giuseppe Vettigli, Greg Gandenberger,"]}],"sklearn\/tree\/tests\/test_export.py":[{"add":["401","def test_plot_tree_entropy(pyplot):","403","    # Check correctness of export_graphviz for criterion = entropy","404","    clf = DecisionTreeClassifier(max_depth=3,","405","                                 min_samples_split=2,","406","                                 criterion=\"entropy\",","407","                                 random_state=2)","408","    clf.fit(X, y)","409","","410","    # Test export code","411","    feature_names = ['first feat', 'sepal_width']","412","    nodes = plot_tree(clf, feature_names=feature_names)","413","    assert len(nodes) == 3","414","    assert nodes[0].get_text() == (\"first feat <= 0.0\\nentropy = 1.0\\n\"","415","                                   \"samples = 6\\nvalue = [3, 3]\")","416","    assert nodes[1].get_text() == \"entropy = 0.0\\nsamples = 3\\nvalue = [3, 0]\"","417","    assert nodes[2].get_text() == \"entropy = 0.0\\nsamples = 3\\nvalue = [0, 3]\"","418","","419","","420","def test_plot_tree_gini(pyplot):","421","    # mostly smoke tests","422","    # Check correctness of export_graphviz for criterion = gini","433","    assert nodes[0].get_text() == (\"first feat <= 0.0\\ngini = 0.5\\n\"","435","    assert nodes[1].get_text() == \"gini = 0.0\\nsamples = 3\\nvalue = [3, 0]\"","436","    assert nodes[2].get_text() == \"gini = 0.0\\nsamples = 3\\nvalue = [0, 3]\""],"delete":["401","def test_plot_tree(pyplot):","403","    # Check correctness of export_graphviz","414","    assert nodes[0].get_text() == (\"first feat <= 0.0\\nentropy = 0.5\\n\"","416","    assert nodes[1].get_text() == \"entropy = 0.0\\nsamples = 3\\nvalue = [3, 0]\"","417","    assert nodes[2].get_text() == \"entropy = 0.0\\nsamples = 3\\nvalue = [0, 3]\""]}],"sklearn\/tree\/export.py":[{"add":["549","    def _make_tree(self, node_id, et, criterion, depth=0):","552","        name = self.node_to_str(et, node_id, criterion=criterion)","556","                                        criterion, depth=depth + 1),","558","                                        criterion, depth=depth + 1)]","570","        my_tree = self._make_tree(0, decision_tree.tree_,","571","                                  decision_tree.criterion)"],"delete":["549","    def _make_tree(self, node_id, et, depth=0):","552","        name = self.node_to_str(et, node_id, criterion='entropy')","556","                                        depth=depth + 1),","558","                                        depth=depth + 1)]","570","        my_tree = self._make_tree(0, decision_tree.tree_)"]}]}},"5c8167a159d8d0a24204a4757822dc0860f35606":{"changes":{"sklearn\/svm\/tests\/test_sparse.py":"MODIFY","doc\/tutorial\/basic\/tutorial.rst":"MODIFY","doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/model_selection\/tests\/test_search.py":"MODIFY","sklearn\/svm\/classes.py":"MODIFY","sklearn\/svm\/base.py":"MODIFY","doc\/modules\/model_evaluation.rst":"MODIFY","sklearn\/svm\/tests\/test_svm.py":"MODIFY"},"diff":{"sklearn\/svm\/tests\/test_sparse.py":[{"add":["89","            clf = svm.SVC(gamma=1, kernel=kernel, probability=True,","91","            sp_clf = svm.SVC(gamma=1, kernel=kernel, probability=True,","295","    clf = svm.OneClassSVM(gamma=1, kernel=kernel)","296","    sp_clf = svm.OneClassSVM(gamma=1, kernel=kernel)"],"delete":["89","            clf = svm.SVC(gamma='scale', kernel=kernel, probability=True,","91","            sp_clf = svm.SVC(gamma='scale', kernel=kernel, probability=True,","295","    clf = svm.OneClassSVM(gamma='scale', kernel=kernel)","296","    sp_clf = svm.OneClassSVM(gamma='scale', kernel=kernel)"]}],"doc\/tutorial\/basic\/tutorial.rst":[{"add":["346","  array([0, 0, 0, 1, 0])"],"delete":["346","  array([1, 0, 1, 1, 0])"]}],"doc\/whats_new\/v0.20.rst":[{"add":["38",":mod:`sklearn.feature_extraction`","39",".................................","40","","41","- |Fix| Fixed a bug in :class:`feature_extraction.text.CountVectorizer` which ","42","  would result in the sparse feature matrix having conflicting `indptr` and","43","  `indices` precisions under very large vocabularies. :issue:`11295` by","44","  :user:`Gabriel Vacaliuc <gvacaliuc>`.","45","","78",":mod:`sklearn.svm`","79","..................","81","- |FIX| Fixed a bug in :class:`svm.SVC`, :class:`svm.NuSVC`, :class:`svm.SVR`,","82","  :class:`svm.NuSVR` and :class:`svm.OneClassSVM` where the ``scale`` option","83","  of parameter ``gamma`` is erroneously defined as","84","  ``1 \/ (n_features * X.std())``. It's now defined as","85","  ``1 \/ (n_features * X.var())``.","86","  :issue:`13221` by :user:`Hanmin Qin <qinhanmin2014>`."],"delete":["70",":mod:`sklearn.feature_extraction.text`","71","......................................","73","- |Fix| Fixed a bug in :class:`feature_extraction.text.CountVectorizer` which ","74","  would result in the sparse feature matrix having conflicting `indptr` and","75","  `indices` precisions under very large vocabularies. :issue:`11295` by","76","  :user:`Gabriel Vacaliuc <gvacaliuc>`."]}],"sklearn\/model_selection\/tests\/test_search.py":[{"add":["1712","    grid = GridSearchCV(SVC(gamma='scale', random_state=0),","1713","                        param_grid={'C': [10]}, cv=3)","1717","    grid = GridSearchCV(SVC(gamma='scale', random_state=0),","1718","                        param_grid={'C': [10]}, cv=5)","1722","    grid = GridSearchCV(SVC(gamma='scale', random_state=0),","1723","                        param_grid={'C': [10]}, cv=2)","1727","    grid = GridSearchCV(SVC(gamma='scale', random_state=0),","1728","                        param_grid={'C': [10]}, cv=KFold(2))"],"delete":["1712","    grid = GridSearchCV(SVC(gamma='scale'), param_grid={'C': [1]}, cv=3)","1716","    grid = GridSearchCV(SVC(gamma='scale'), param_grid={'C': [1]}, cv=5)","1720","    grid = GridSearchCV(SVC(gamma='scale'), param_grid={'C': [1]}, cv=2)","1724","    grid = GridSearchCV(SVC(gamma='scale'), param_grid={'C': [1]}, cv=KFold(2))"]}],"sklearn\/svm\/classes.py":[{"add":["465","        if ``gamma='scale'`` is passed then it uses 1 \/ (n_features * X.var())","653","        if ``gamma='scale'`` is passed then it uses 1 \/ (n_features * X.var())","814","        if ``gamma='scale'`` is passed then it uses 1 \/ (n_features * X.var())","950","        if ``gamma='scale'`` is passed then it uses 1 \/ (n_features * X.var())","1067","        if ``gamma='scale'`` is passed then it uses 1 \/ (n_features * X.var())"],"delete":["465","        if ``gamma='scale'`` is passed then it uses 1 \/ (n_features * X.std())","653","        if ``gamma='scale'`` is passed then it uses 1 \/ (n_features * X.std())","814","        if ``gamma='scale'`` is passed then it uses 1 \/ (n_features * X.std())","950","        if ``gamma='scale'`` is passed then it uses 1 \/ (n_features * X.std())","1067","        if ``gamma='scale'`` is passed then it uses 1 \/ (n_features * X.std())"]}],"sklearn\/svm\/base.py":[{"add":["171","                # var = E[X^2] - E[X]^2","172","                X_var = (X.multiply(X)).mean() - (X.mean()) ** 2","174","                X_var = X.var()","176","                if X_var != 0:","177","                    self._gamma = 1.0 \/ (X.shape[1] * X_var)","183","                if kernel_uses_gamma and not np.isclose(X_var, 1.0):"],"delete":["171","                # std = sqrt(E[X^2] - E[X]^2)","172","                X_std = np.sqrt((X.multiply(X)).mean() - (X.mean())**2)","174","                X_std = X.std()","176","                if X_std != 0:","177","                    self._gamma = 1.0 \/ (X.shape[1] * X_std)","183","                if kernel_uses_gamma and not np.isclose(X_std, 1.0):"]}],"doc\/modules\/model_evaluation.rst":[{"add":["104","    array([0.96..., 0.96..., 0.96..., 0.93..., 1.        ])","1949","  0.94..."],"delete":["104","    array([0.96..., 1.  ..., 0.96..., 0.96..., 1.        ])","1949","  0.97..."]}],"sklearn\/svm\/tests\/test_svm.py":[{"add":["245","    assert_array_equal(pred, [1, -1, -1])","247","    assert_array_almost_equal(clf.intercept_, [-1.218], decimal=3)","249","                              [[0.750, 0.750, 0.750, 0.750]],","1005","    assert_almost_equal(clf._gamma, 4)","1007","    # X_var ~= 1 shouldn't raise warning, for when"],"delete":["245","    assert_array_equal(pred, [-1, -1, -1])","247","    assert_array_almost_equal(clf.intercept_, [-1.117], decimal=3)","249","                              [[0.681, 0.139, 0.68, 0.14, 0.68, 0.68]],","1005","    assert_equal(clf._gamma, 2.)","1007","    # X_std ~= 1 shouldn't raise warning, for when"]}]}},"701144559f0448e7b140fead0384b9cba99af096":{"changes":{"sklearn\/utils\/fixes.py":"MODIFY","sklearn\/feature_extraction\/text.py":"MODIFY","sklearn\/model_selection\/tests\/test_search.py":"MODIFY","sklearn\/utils\/tests\/test_fixes.py":"MODIFY","sklearn\/covariance\/graph_lasso_.py":"MODIFY","sklearn\/utils\/multiclass.py":"MODIFY","sklearn\/datasets\/samples_generator.py":"MODIFY","sklearn\/utils\/__init__.py":"MODIFY","sklearn\/feature_extraction\/tests\/test_text.py":"MODIFY","sklearn\/preprocessing\/data.py":"MODIFY","sklearn\/metrics\/scorer.py":"MODIFY","sklearn\/model_selection\/_split.py":"MODIFY","sklearn\/feature_extraction\/dict_vectorizer.py":"MODIFY","sklearn\/model_selection\/_search.py":"MODIFY"},"diff":{"sklearn\/utils\/fixes.py":[{"add":[],"delete":["14","from collections.abc import Sequence as _Sequence  # noqa","15","from collections.abc import Iterable as _Iterable  # noqa","16","from collections.abc import Mapping as _Mapping  # noqa","17","from collections.abc import Sized as _Sized  # noqa","18","","22","from scipy.special import boxcox  # noqa","24","from numpy import nanpercentile  # noqa","25","from numpy import nanmedian  # noqa"]}],"sklearn\/feature_extraction\/text.py":[{"add":["17","from collections.abc import Mapping"],"delete":["32","from ..utils.fixes import _Mapping as Mapping  # noqa"]}],"sklearn\/model_selection\/tests\/test_search.py":[{"add":["2","from collections.abc import Iterable, Sized"],"delete":["14","from sklearn.utils.fixes import _Iterable as Iterable, _Sized as Sized"]}],"sklearn\/utils\/tests\/test_fixes.py":[{"add":[],"delete":["14","from sklearn.utils.fixes import nanmedian","15","from sklearn.utils.fixes import nanpercentile","30","@pytest.mark.parametrize(","31","    \"axis, expected_median\",","32","    [(None, 4.0),","33","     (0, np.array([1., 3.5, 3.5, 4., 7., np.nan])),","34","     (1, np.array([1., 6.]))]","35",")","36","def test_nanmedian(axis, expected_median):","37","    X = np.array([[1, 1, 1, 2, np.nan, np.nan],","38","                  [np.nan, 6, 6, 6, 7, np.nan]])","39","    median = nanmedian(X, axis=axis)","40","    if axis is None:","41","        assert median == pytest.approx(expected_median)","42","    else:","43","        assert_allclose(median, expected_median)","44","","45","","46","@pytest.mark.parametrize(","47","    \"a, q, expected_percentile\",","48","    [(np.array([1, 2, 3, np.nan]), [0, 50, 100], np.array([1., 2., 3.])),","49","     (np.array([1, 2, 3, np.nan]), 50, 2.),","50","     (np.array([np.nan, np.nan]), [0, 50], np.array([np.nan, np.nan]))]","51",")","52","def test_nanpercentile(a, q, expected_percentile):","53","    percentile = nanpercentile(a, q)","54","    assert_allclose(percentile, expected_percentile)","55","","56",""]}],"sklearn\/covariance\/graph_lasso_.py":[{"add":["7","from collections.abc import Sequence"],"delete":["21","from ..utils.fixes import _Sequence as Sequence"]}],"sklearn\/utils\/multiclass.py":[{"add":["8","from collections.abc import Sequence"],"delete":["8","from __future__ import division","18","from ..utils.fixes import _Sequence as Sequence"]}],"sklearn\/datasets\/samples_generator.py":[{"add":["10","from collections.abc import Iterable","11",""],"delete":["17","from ..utils.fixes import _Iterable as Iterable"]}],"sklearn\/utils\/__init__.py":[{"add":["3","from collections.abc import Sequence"],"delete":["15","from .fixes import _Sequence as Sequence"]}],"sklearn\/feature_extraction\/tests\/test_text.py":[{"add":["2","from collections.abc import Mapping"],"delete":["38","from sklearn.utils.fixes import _Mapping as Mapping"]}],"sklearn\/preprocessing\/data.py":[{"add":["19","from scipy.special import boxcox","1140","            self.center_ = np.nanmedian(X, axis=0)","1155","                quantiles.append(np.nanpercentile(column_data,","1156","                                                  self.quantile_range))","2103","            self.quantiles_.append(np.nanpercentile(col, references))","2145","                self.quantiles_.append(","2146","                        np.nanpercentile(column_data, references))"],"delete":["24","from ..utils.fixes import boxcox, nanpercentile, nanmedian","1140","            self.center_ = nanmedian(X, axis=0)","1155","                quantiles.append(nanpercentile(column_data,","1156","                                               self.quantile_range))","2103","            self.quantiles_.append(nanpercentile(col, references))","2145","                self.quantiles_.append(nanpercentile(column_data, references))"]}],"sklearn\/metrics\/scorer.py":[{"add":["21","from collections.abc import Iterable"],"delete":["41","from ..utils.fixes import _Iterable as Iterable"]}],"sklearn\/model_selection\/_split.py":[{"add":["15","from collections.abc import Iterable"],"delete":["29","from ..utils.fixes import _Iterable as Iterable"]}],"sklearn\/feature_extraction\/dict_vectorizer.py":[{"add":["5","from collections.abc import Mapping"],"delete":["12","from ..utils.fixes import _Mapping as Mapping"]}],"sklearn\/model_selection\/_search.py":[{"add":["16","from collections.abc import Mapping, Sequence, Iterable"],"delete":["34","from ..utils.fixes import _Mapping as Mapping, _Sequence as Sequence","35","from ..utils.fixes import _Iterable as Iterable"]}]}},"db2290ce38aea1d1d561e0966f952e9e7be05368":{"changes":{"doc\/conf.py":"MODIFY","doc\/sphinxext\/custom_references_resolver.py":"ADD","examples\/cluster\/plot_dict_face_patches.py":"MODIFY"},"diff":{"doc\/conf.py":[{"add":["39","    'custom_references_resolver'","105","# sklearn uses a custom extension: `custom_references_resolver` to modify","106","# the order of link resolution for the 'any' role. It resolves python class","107","# links first before resolving 'std' domain links. Unresolved roles are","108","# considered to be <code> blocks."],"delete":[]}],"doc\/sphinxext\/custom_references_resolver.py":[{"add":[],"delete":[]}],"examples\/cluster\/plot_dict_face_patches.py":[{"add":["11","500 of these patches (using 10 images), we run the","12",":func:`~sklearn.cluster.MiniBatchKMeans.partial_fit` method"],"delete":["11","500 of these patches (using 10 images), we run the `partial_fit` method"]}]}},"be03467b9cdcdcd6940350195d23310f992cb6d7":{"changes":{"sklearn\/feature_selection\/variance_threshold.py":"MODIFY","sklearn\/feature_selection\/tests\/test_variance_threshold.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY"},"diff":{"sklearn\/feature_selection\/variance_threshold.py":[{"add":["7","from ..utils.sparsefuncs import mean_variance_axis, min_max_axis","67","            if self.threshold == 0:","68","                mins, maxes = min_max_axis(X, axis=0)","69","                peak_to_peaks = maxes - mins","72","            if self.threshold == 0:","73","                peak_to_peaks = np.ptp(X, axis=0)","74","","75","        if self.threshold == 0:","76","            # Use peak-to-peak to avoid numeric precision issues","77","            # for constant features","78","            self.variances_ = np.minimum(self.variances_, peak_to_peaks)"],"delete":["7","from ..utils.sparsefuncs import mean_variance_axis"]}],"sklearn\/feature_selection\/tests\/test_variance_threshold.py":[{"add":["0","import numpy as np","1","import pytest","2","","31","","32","","33","def test_zero_variance_floating_point_error():","34","    # Test that VarianceThreshold(0.0).fit eliminates features that have","35","    # the same value in every sample, even when floating point errors","36","    # cause np.var not to be 0 for the feature.","37","    # See #13691","38","","39","    data = [[-0.13725701]] * 10","40","    assert np.var(data) != 0","41","    for X in [data, csr_matrix(data), csc_matrix(data), bsr_matrix(data)]:","42","        msg = \"No feature in X meets the variance threshold 0.00000\"","43","        with pytest.raises(ValueError, match=msg):","44","            VarianceThreshold().fit(X)"],"delete":[]}],"doc\/whats_new\/v0.22.rst":[{"add":["74","","75",":mod:`sklearn.feature_selection`","76","................................","77","- |Fix| Fixed a bug where :class:`VarianceThreshold` with `threshold=0` did not","78","  remove constant features due to numerical instability, by using range","79","  rather than variance in this case.","80","  :pr:`13704` by `Roddy MacSween <rlms>`."],"delete":["67","","76","\t"]}]}},"251d6e643fdc0c40059274fffdd808ad2461f0c7":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/cluster\/birch.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["87","- |Fix| Fixed a bug where :class:`cluster.Birch` could occasionally raise an","88","  AttributeError. :issue:`13651` by `Joel Nothman`_.","89",""],"delete":[]}],"sklearn\/cluster\/birch.py":[{"add":["279","            self.centroid_ = self.linear_sum_ = 0"],"delete":["279","            self.linear_sum_ = 0"]}]}},"3373e9c12502b62916c53f2651620fafd3342c3c":{"changes":{"sklearn\/mixture\/bayesian_mixture.py":"MODIFY"},"diff":{"sklearn\/mixture\/bayesian_mixture.py":[{"add":["142","        Controls the extend to where means can be placed. Larger","262","        Larger values concentrate the means of each clusters around"],"delete":["142","        Controls the extend to where means can be placed. Smaller","262","        Smaller values concentrate the means of each clusters around"]}]}},"9ac5793af1d00d2bf18c78c0f9c3aae5b0b705b8":{"changes":{"sklearn\/preprocessing\/_discretization.py":"MODIFY","doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/preprocessing\/tests\/test_discretization.py":"MODIFY"},"diff":{"sklearn\/preprocessing\/_discretization.py":[{"add":["174","                # Must sort, centers may be unsorted even with sorted init","175","                centers.sort()"],"delete":[]}],"doc\/whats_new\/v0.20.rst":[{"add":["57","- |Fix| Fixed a bug in :class:`preprocessing.KBinsDiscretizer` where","58","  ``strategy='kmeans'`` fails with an error during transformation due to unsorted","59","  bin edges. :issue:`13134` by :user:`Sandro Casagrande <SandroCasagrande>`.","60",""],"delete":[]}],"sklearn\/preprocessing\/tests\/test_discretization.py":[{"add":["187","    'strategy, expected_2bins, expected_3bins, expected_5bins',","188","    [('uniform', [0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 2, 2], [0, 0, 1, 1, 4, 4]),","189","     ('kmeans', [0, 0, 0, 0, 1, 1], [0, 0, 1, 1, 2, 2], [0, 0, 1, 2, 3, 4]),","190","     ('quantile', [0, 0, 0, 1, 1, 1], [0, 0, 1, 1, 2, 2], [0, 1, 2, 3, 4, 4])])","191","def test_nonuniform_strategies(","192","        strategy, expected_2bins, expected_3bins, expected_5bins):","205","    # with 5 bins","206","    est = KBinsDiscretizer(n_bins=5, strategy=strategy, encode='ordinal')","207","    Xt = est.fit_transform(X)","208","    assert_array_equal(expected_5bins, Xt.ravel())","209",""],"delete":["187","    'strategy, expected_2bins, expected_3bins',","188","    [('uniform', [0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 2, 2]),","189","     ('kmeans', [0, 0, 0, 0, 1, 1], [0, 0, 1, 1, 2, 2]),","190","     ('quantile', [0, 0, 0, 1, 1, 1], [0, 0, 1, 1, 2, 2])])","191","def test_nonuniform_strategies(strategy, expected_2bins, expected_3bins):"]}]}},"37b0e66c871e8fb032a9c7086b2a1d5419838154":{"changes":{"sklearn\/ensemble\/iforest.py":"MODIFY","doc\/modules\/outlier_detection.rst":"MODIFY"},"diff":{"sklearn\/ensemble\/iforest.py":[{"add":["144","    Notes","145","    -----","146","    The implementation is based on an ensemble of ExtraTreeRegressor. The","147","    maximum depth of each tree is set to ``ceil(log_2(n))`` where","148","    :math:`n` is the number of samples used to build the tree","149","    (see (Liu et al., 2008) for more details).","150",""],"delete":[]}],"doc\/modules\/outlier_detection.rst":[{"add":["241","The implementation of :class:`ensemble.IsolationForest` is based on an ensemble","242","of :class:`tree.ExtraTreeRegressor`. Following Isolation Forest original paper,","243","the maximum depth of each tree is set to :math:`\\lceil \\log_2(n) \\rceil` where","244",":math:`n` is the number of samples used to build the tree (see (Liu et al.,","245","2008) for more details).","246","","247","This algorithm is illustrated below."],"delete":["241","This strategy is illustrated below."]}]}},"ce869d8ddb723ef5e870903cbebd8e256835e234":{"changes":{"sklearn\/preprocessing\/data.py":"MODIFY"},"diff":{"sklearn\/preprocessing\/data.py":[{"add":["1534","                combinations = self._combinations(n_features, self.degree,","1535","                                                  self.interaction_only,","1536","                                                  self.include_bias)","1551","","1552","                # What follows is a faster implementation of:","1553","                # for i, comb in enumerate(combinations):","1554","                #     XP[:, i] = X[:, comb].prod(1)","1555","                # This implementation uses two optimisations.","1556","                # First one is broadcasting,","1557","                # multiply ([X1, ..., Xn], X1) -> [X1 X1, ..., Xn X1]","1558","                # multiply ([X2, ..., Xn], X2) -> [X2 X2, ..., Xn X2]","1559","                # ...","1560","                # multiply ([X[:, start:end], X[:, start]) -> ...","1561","                # Second optimisation happens for degrees >= 3.","1562","                # Xi^3 is computed reusing previous computation:","1563","                # Xi^3 = Xi^2 * Xi.","1564","","1565","                if self.include_bias:","1566","                    XP[:, 0] = 1","1567","                    current_col = 1","1568","                else:","1569","                    current_col = 0","1570","","1571","                # d = 0","1572","                XP[:, current_col:current_col + n_features] = X","1573","                index = list(range(current_col,","1574","                                   current_col + n_features))","1575","                current_col += n_features","1576","                index.append(current_col)","1577","","1578","                # d >= 1","1579","                for _ in range(1, self.degree):","1580","                    new_index = []","1581","                    end = index[-1]","1582","                    for feature_idx in range(n_features):","1583","                        start = index[feature_idx]","1584","                        new_index.append(current_col)","1585","                        if self.interaction_only:","1586","                            start += (index[feature_idx + 1] -","1587","                                      index[feature_idx])","1588","                        next_col = current_col + end - start","1589","                        if next_col <= current_col:","1590","                            break","1591","                        # XP[:, start:end] are terms of degree d - 1","1592","                        # that exclude feature #feature_idx.","1593","                        np.multiply(XP[:, start:end],","1594","                                    X[:, feature_idx:feature_idx + 1],","1595","                                    out=XP[:, current_col:next_col],","1596","                                    casting='no')","1597","                        current_col = next_col","1598","","1599","                    new_index.append(current_col)","1600","                    index = new_index"],"delete":["1533","            combinations = self._combinations(n_features, self.degree,","1534","                                              self.interaction_only,","1535","                                              self.include_bias)","1551","                for i, comb in enumerate(combinations):","1552","                    XP[:, i] = X[:, comb].prod(1)"]}]}},"6216b247681ecdab3fdac873e467e0efa07553d3":{"changes":{"sklearn\/cluster\/dbscan_.py":"MODIFY"},"diff":{"sklearn\/cluster\/dbscan_.py":[{"add":["12","import warnings","147","        with warnings.catch_warnings():","148","            warnings.simplefilter('ignore', sparse.SparseEfficiencyWarning)"],"delete":["16","from ..utils.testing import ignore_warnings","147","        with ignore_warnings():"]}]}},"e43574e61b4f0cab94683d9c94fd31e2ef03f387":{"changes":{"sklearn\/neighbors\/binary_tree.pxi":"MODIFY","sklearn\/covariance\/shrunk_covariance_.py":"MODIFY","sklearn\/decomposition\/tests\/test_fastica.py":"MODIFY","sklearn\/linear_model\/huber.py":"MODIFY","sklearn\/linear_model\/sag.py":"MODIFY","sklearn\/random_projection.py":"MODIFY","sklearn\/covariance\/empirical_covariance_.py":"MODIFY","sklearn\/covariance\/robust_covariance.py":"MODIFY","sklearn\/linear_model\/stochastic_gradient.py":"MODIFY","sklearn\/svm\/classes.py":"MODIFY","sklearn\/neighbors\/tests\/test_ball_tree.py":"MODIFY","sklearn\/linear_model\/ridge.py":"MODIFY","sklearn\/model_selection\/_search.py":"MODIFY"},"diff":{"sklearn\/neighbors\/binary_tree.pxi":[{"add":["304","    >>> rng = np.random.RandomState(0)","305","    >>> X = rng.random_sample((10, 3))  # 10 points in 3 dimensions","318","    >>> rng = np.random.RandomState(0)","319","    >>> X = rng.random_sample((10, 3))  # 10 points in 3 dimensions","332","    >>> rng = np.random.RandomState(0)","333","    >>> X = rng.random_sample((10, 3))  # 10 points in 3 dimensions","345","    >>> rng = np.random.RandomState(42)","346","    >>> X = rng.random_sample((100, 3))","354","    >>> rng = np.random.RandomState(0)","355","    >>> X = rng.random_sample((30, 3))"],"delete":["304","    >>> np.random.seed(0)","305","    >>> X = np.random.random((10, 3))  # 10 points in 3 dimensions","318","    >>> np.random.seed(0)","319","    >>> X = np.random.random((10, 3))  # 10 points in 3 dimensions","332","    >>> np.random.seed(0)","333","    >>> X = np.random.random((10, 3))  # 10 points in 3 dimensions","345","    >>> np.random.seed(1)","346","    >>> X = np.random.random((100, 3))","354","    >>> np.random.seed(0)","355","    >>> X = np.random.random((30, 3))"]}],"sklearn\/covariance\/shrunk_covariance_.py":[{"add":["105","    >>> rng = np.random.RandomState(0)","106","    >>> X = rng.multivariate_normal(mean=[0, 0],"],"delete":["105","    >>> np.random.seed(0)","106","    >>> X = np.random.multivariate_normal(mean=[0, 0],"]}],"sklearn\/decomposition\/tests\/test_fastica.py":[{"add":[],"delete":["56","    np.random.seed(0)"]}],"sklearn\/linear_model\/huber.py":[{"add":["198","    >>> rng = np.random.RandomState(0)","201","    >>> X[:4] = rng.uniform(10, 20, (4, 2))","202","    >>> y[:4] = rng.uniform(10, 20, 4)"],"delete":["198","    >>> np.random.seed(0)","201","    >>> X[:4] = np.random.uniform(10, 20, (4, 2))","202","    >>> y[:4] = np.random.uniform(10, 20, 4)"]}],"sklearn\/linear_model\/sag.py":[{"add":["203","    >>> rng = np.random.RandomState(0)","204","    >>> X = rng.randn(n_samples, n_features)","205","    >>> y = rng.randn(n_samples)"],"delete":["203","    >>> np.random.seed(0)","204","    >>> X = np.random.randn(n_samples, n_features)","205","    >>> y = np.random.randn(n_samples)"]}],"sklearn\/random_projection.py":[{"add":["468","    >>> rng = np.random.RandomState(42)","469","    >>> X = rng.rand(100, 10000)","470","    >>> transformer = GaussianRandomProjection(random_state=rng)","591","    >>> rng = np.random.RandomState(42)","592","    >>> X = rng.rand(100, 10000)","593","    >>> transformer = SparseRandomProjection(random_state=rng)"],"delete":["468","    >>> X = np.random.rand(100, 10000)","469","    >>> transformer = GaussianRandomProjection()","590","    >>> np.random.seed(42)","591","    >>> X = np.random.rand(100, 10000)","592","    >>> transformer = SparseRandomProjection()"]}],"sklearn\/covariance\/empirical_covariance_.py":[{"add":["122","    >>> rng = np.random.RandomState(0)","123","    >>> X = rng.multivariate_normal(mean=[0, 0],"],"delete":["122","    >>> np.random.seed(0)","123","    >>> X = np.random.multivariate_normal(mean=[0, 0],"]}],"sklearn\/covariance\/robust_covariance.py":[{"add":["588","    >>> rng = np.random.RandomState(0)","589","    >>> X = rng.multivariate_normal(mean=[0, 0],"],"delete":["588","    >>> np.random.seed(0)","589","    >>> X = np.random.multivariate_normal(mean=[0, 0],"]}],"sklearn\/linear_model\/stochastic_gradient.py":[{"add":["1561","    >>> rng = np.random.RandomState(0)","1562","    >>> y = rng.randn(n_samples)","1563","    >>> X = rng.randn(n_samples, n_features)"],"delete":["1561","    >>> np.random.seed(0)","1562","    >>> y = np.random.randn(n_samples)","1563","    >>> X = np.random.randn(n_samples, n_features)"]}],"sklearn\/svm\/classes.py":[{"add":["876","    >>> rng = np.random.RandomState(0)","877","    >>> y = rng.randn(n_samples)","878","    >>> X = rng.randn(n_samples, n_features)"],"delete":["876","    >>> np.random.seed(0)","877","    >>> y = np.random.randn(n_samples)","878","    >>> X = np.random.randn(n_samples, n_features)"]}],"sklearn\/neighbors\/tests\/test_ball_tree.py":[{"add":["154","    rng = np.random.RandomState(0)","155","    X = rng.random_sample((n_samples, n_features))","156","    Y = rng.random_sample((n_samples, n_features))"],"delete":["154","    np.random.seed(0)","155","    X = np.random.random((n_samples, n_features))","156","    Y = np.random.random((n_samples, n_features))"]}],"sklearn\/linear_model\/ridge.py":[{"add":["699","    >>> rng = np.random.RandomState(0)","700","    >>> y = rng.randn(n_samples)","701","    >>> X = rng.randn(n_samples, n_features)"],"delete":["699","    >>> np.random.seed(0)","700","    >>> y = np.random.randn(n_samples)","701","    >>> X = np.random.randn(n_samples, n_features)"]}],"sklearn\/model_selection\/_search.py":[{"add":["230","    >>> rng = np.random.RandomState(0)","232","    >>> param_list = list(ParameterSampler(param_grid, n_iter=4,","233","    ...                                    random_state=rng))"],"delete":["230","    >>> np.random.seed(0)","232","    >>> param_list = list(ParameterSampler(param_grid, n_iter=4))"]}]}},"b34751b7ed02b2cfcc36037fb729d4360480a299":{"changes":{"azure-pipelines.yml":"MODIFY","sklearn\/tree\/tests\/test_export.py":"MODIFY","sklearn\/utils\/testing.py":"MODIFY","sklearn\/conftest.py":"ADD","sklearn\/ensemble\/tests\/test_partial_dependence.py":"MODIFY","sklearn\/tests\/test_common.py":"MODIFY","doc\/developers\/contributing.rst":"MODIFY","build_tools\/azure\/install.cmd":"MODIFY","sklearn\/inspection\/tests\/test_partial_dependence.py":"MODIFY"},"diff":{"azure-pipelines.yml":[{"add":["24","        MATPLOTLIB_VERSION: '1.5.1'"],"delete":[]}],"sklearn\/tree\/tests\/test_export.py":[{"add":["401","def test_plot_tree(pyplot):"],"delete":["401","def test_plot_tree():","403","    pytest.importorskip(\"matplotlib.pyplot\")"]}],"sklearn\/utils\/testing.py":[{"add":[],"delete":["716","def if_matplotlib(func):","717","    \"\"\"Test decorator that skips test if matplotlib not installed.","718","","719","    Parameters","720","    ----------","721","    func","722","    \"\"\"","723","    @wraps(func)","724","    def run_test(*args, **kwargs):","725","        try:","726","            import matplotlib","727","            matplotlib.use('Agg', warn=False)","728","            # this fails if no $DISPLAY specified","729","            import matplotlib.pyplot as plt","730","            plt.figure()","731","        except ImportError:","732","            raise SkipTest('Matplotlib not available.')","733","        else:","734","            return func(*args, **kwargs)","735","    return run_test","736","","737","","1026","","1027","","1028","def close_figure(fig=None):","1029","    \"\"\"Close a matplotlibt figure.","1030","","1031","    Parameters","1032","    ----------","1033","    fig : int or str or Figure, optional (default=None)","1034","        The figure, figure number or figure name to close. If ``None``, all","1035","        current figures are closed.","1036","    \"\"\"","1037","    from matplotlib.pyplot import get_fignums, close as _close  # noqa","1038","","1039","    if fig is None:","1040","        for fig in get_fignums():","1041","            _close(fig)","1042","    else:","1043","        _close(fig)"]}],"sklearn\/conftest.py":[{"add":[],"delete":[]}],"sklearn\/ensemble\/tests\/test_partial_dependence.py":[{"add":["156","def test_plot_partial_dependence(pyplot):","190","def test_plot_partial_dependence_input(pyplot):","227","def test_plot_partial_dependence_multiclass(pyplot):","262","@pytest.mark.parametrize(","263","    \"func, params\",","264","    [(partial_dependence, {'target_variables': [0], 'X': boston.data}),","265","     (plot_partial_dependence, {'X': boston.data, 'features': [0, 1, (0, 1)]})]","266",")","267","def test_raise_deprecation_warning(pyplot, func, params):","272","    warn_msg = \"The function ensemble.{} has been deprecated\".format(","273","        func.__name__","274","    )","275","    with pytest.warns(DeprecationWarning, match=warn_msg):","276","        func(clf, **params, grid_resolution=grid_resolution)"],"delete":["9","from sklearn.utils.testing import if_matplotlib","16","from sklearn.utils.testing import assert_warns_message","158","@if_matplotlib","159","def test_plot_partial_dependence():","192","@if_matplotlib","194","def test_plot_partial_dependence_input():","230","@if_matplotlib","232","def test_plot_partial_dependence_multiclass():","267","def test_warning_raised_partial_dependence():","268","    # Test that deprecation warning is raised","269","","274","    assert_warns_message(DeprecationWarning, \"The function \"","275","                         \"ensemble.partial_dependence has been deprecated \",","276","                         partial_dependence, clf, [0], X=boston.data,","277","                         grid_resolution=grid_resolution)","278","","279","","280","@if_matplotlib","281","def test_warning_raised_partial_dependence_plot():","282","    # Test that deprecation warning is raised","283","","284","    clf = GradientBoostingRegressor(n_estimators=10, random_state=1)","285","    clf.fit(boston.data, boston.target)","286","    grid_resolution = 25","287","","288","    assert_warns_message(DeprecationWarning, \"The function \"","289","                         \"ensemble.plot_partial_dependence has been \"","290","                         \"deprecated\",","291","                         plot_partial_dependence, clf, boston.data,","292","                         [0, 1, (0, 1)], grid_resolution=grid_resolution,","293","                         feature_names=boston.feature_names)"]}],"sklearn\/tests\/test_common.py":[{"add":["217","    EXCEPTIONS = ('utils', 'tests', 'base', 'setup', 'conftest')"],"delete":["217","    EXCEPTIONS = ('utils', 'tests', 'base', 'setup')"]}],"doc\/developers\/contributing.rst":[{"add":["628","-----------------------------------","646","Writing matplotlib related tests","647","................................","648","","649","Test fixtures ensure that a set of tests will be executing with the appropriate","650","initialization and cleanup. The scikit-learn test suite implements a fixture","651","which can be used with ``matplotlib``.","652","","653","``pyplot``","654","    The ``pyplot`` fixture should be used when a test function is dealing with","655","    ``matplotlib``. ``matplotlib`` is a soft dependency and is not required.","656","    This fixture is in charge of skipping the tests if ``matplotlib`` is not","657","    installed. In addition, figures created during the tests will be","658","    automatically closed once the test function has been executed.","659","","660","To use this fixture in a test function, one needs to pass it as an","661","argument::","662","","663","    def test_requiring_mpl_fixture(pyplot):","664","        # you can now safely use matplotlib","665","","666","Workflow to improve test coverage","667",".................................","668","","669","To test code coverage, you need to install the `coverage","670","<https:\/\/pypi.org\/project\/coverage\/>`_ package in addition to pytest.","671","","672","1. Run 'make test-coverage'. The output lists for each file the line","673","    numbers that are not tested.","674","","675","2. Find a low hanging fruit, looking at which lines are not tested,","676","    write or adapt a test specifically for these lines.","677","","678","3. Loop."],"delete":["628","------------------------------------","643",".. note:: **Workflow to improve test coverage**","644","","645","   To test code coverage, you need to install the `coverage","646","   <https:\/\/pypi.org\/project\/coverage\/>`_ package in addition to pytest.","647","","648","   1. Run 'make test-coverage'. The output lists for each file the line","649","      numbers that are not tested.","650","","651","   2. Find a low hanging fruit, looking at which lines are not tested,","652","      write or adapt a test specifically for these lines.","653","","654","   3. Loop.","655",""]}],"build_tools\/azure\/install.cmd":[{"add":["13","    conda create -n %VIRTUALENV% -q -y python=%PYTHON_VERSION% numpy scipy cython matplotlib pytest wheel pillow joblib"],"delete":["13","    conda create -n %VIRTUALENV% -q -y python=%PYTHON_VERSION% numpy scipy cython pytest wheel pillow joblib"]}],"sklearn\/inspection\/tests\/test_partial_dependence.py":[{"add":["397","def test_plot_partial_dependence(pyplot):","407","    fig = pyplot.gcf()","418","    fig = pyplot.gcf()","429","    fig = pyplot.gcf()","435","def test_plot_partial_dependence_multiclass(pyplot):","445","    fig = pyplot.gcf()","459","    fig = pyplot.gcf()","465","def test_plot_partial_dependence_multioutput(pyplot):","475","    fig = pyplot.gcf()","483","    fig = pyplot.gcf()","518","def test_plot_partial_dependence_error(pyplot, data, params, err_msg):","526","def test_plot_partial_dependence_fig(pyplot):","532","    fig = pyplot.figure()","537","    assert pyplot.gcf() is fig"],"delete":["29","from sklearn.utils.testing import if_matplotlib","398","@if_matplotlib","399","def test_plot_partial_dependence():","401","    import matplotlib.pyplot as plt  # noqa","402","","411","    fig = plt.gcf()","422","    fig = plt.gcf()","433","    fig = plt.gcf()","438","    plt.close('all')","440","","441","@if_matplotlib","442","def test_plot_partial_dependence_multiclass():","444","    import matplotlib.pyplot as plt  # noqa","453","    fig = plt.gcf()","467","    fig = plt.gcf()","472","    plt.close('all')","474","","475","@if_matplotlib","476","def test_plot_partial_dependence_multioutput():","478","    import matplotlib.pyplot as plt  # noqa","487","    fig = plt.gcf()","495","    fig = plt.gcf()","500","    plt.close('all')","502","","503","@if_matplotlib","533","def test_plot_partial_dependence_error(data, params, err_msg):","534","    import matplotlib.pyplot as plt  # noqa","541","    plt.close()","543","","544","@if_matplotlib","545","def test_plot_partial_dependence_fig():","547","","548","    import matplotlib.pyplot as plt","549","","554","    fig = plt.figure()","559","    assert plt.gcf() is fig","560","","561","    plt.close()"]}]}},"37a9d187f9eb4864fb52556e9f293f8ff4f698b7":{"changes":{"sklearn\/metrics\/pairwise.py":"MODIFY","sklearn\/neighbors\/dist_metrics.pyx":"MODIFY"},"diff":{"sklearn\/metrics\/pairwise.py":[{"add":["452","       D(x, y) = 2\\\\arcsin[\\\\sqrt{\\\\sin^2((x1 - y1) \/ 2)","453","                                + \\\\cos(x1)\\\\cos(y1)\\\\sin^2((x2 - y2) \/ 2)}]"],"delete":["452","       D(x, y) = 2\\arcsin[\\\\sqrt{\\\\sin^2((x1 - y1) \/ 2)","453","                                + cos(x1)cos(y1)sin^2((x2 - y2) \/ 2)}]"]}],"sklearn\/neighbors\/dist_metrics.pyx":[{"add":["982","       D(x, y) = 2\\\\arcsin[\\\\sqrt{\\\\sin^2((x1 - y1) \/ 2)","983","                                + \\\\cos(x1)\\\\cos(y1)\\\\sin^2((x2 - y2) \/ 2)}]"],"delete":["982","       D(x, y) = 2\\arcsin[\\sqrt{\\sin^2((x1 - y1) \/ 2)","983","                                + cos(x1)cos(y1)sin^2((x2 - y2) \/ 2)}]"]}]}},"18920e312122a90d3f9e0f19900103a613550f79":{"changes":{"build_tools\/circle\/build_doc.sh":"MODIFY"},"diff":{"build_tools\/circle\/build_doc.sh":[{"add":["122","  pandas=\"${PANDAS_VERSION:-*}\" \\","126","if [[ -n \"$SCIKIT_IMAGE_VERSION\" ]]; then","127","    pip install scikit-image==\"$SCIKIT_IMAGE_VERSION\"","128","else","129","    pip install scikit-image","130","fi"],"delete":["122","  scikit-image=\"${SCIKIT_IMAGE_VERSION:-*}\" pandas=\"${PANDAS_VERSION:-*}\" \\"]}]}},"4b6273b87442a4437d8b3873ea3022ae163f4fdf":{"changes":{"sklearn\/ensemble\/_hist_gradient_boosting\/_binning.pyx":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/_predictor.pyx":"MODIFY","sklearn\/ensemble\/setup.py":"MODIFY","doc\/modules\/ensemble.rst":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_gradient_boosting.py":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/common.pyx":"ADD","sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_compare_lightgbm.py":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/splitting.pyx":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_grower.py":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_loss.py":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/common.pxd":"ADD","sklearn\/ensemble\/_hist_gradient_boosting\/predictor.py":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/binning.py":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/grower.py":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/histogram.pyx":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_histogram.py":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/gradient_boosting.py":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/loss.py":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_predictor.py":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/_gradient_boosting.pyx":"MODIFY","\/dev\/null":"DELETE","sklearn\/ensemble\/_hist_gradient_boosting\/_loss.pyx":"MODIFY","benchmarks\/bench_hist_gradient_boosting.py":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/utils.pyx":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_binning.py":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_splitting.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY"},"diff":{"sklearn\/ensemble\/_hist_gradient_boosting\/_binning.pyx":[{"add":["14","from libc.math cimport isnan","16","from .common cimport X_DTYPE_C, X_BINNED_DTYPE_C","18","def _map_to_bins(const X_DTYPE_C [:, :] data,","19","                 list binning_thresholds,","20","                 const unsigned char missing_values_bin_idx,","21","                 X_BINNED_DTYPE_C [::1, :] binned):","40","                             missing_values_bin_idx,","46","                               const unsigned char missing_values_bin_idx,","56","","57","        if isnan(data[i]):","58","            binned[i] = missing_values_bin_idx","60","            # for known values, use binary search"],"delete":["15","from .types cimport X_DTYPE_C, X_BINNED_DTYPE_C","17","cpdef _map_to_bins(const X_DTYPE_C [:, :] data, list binning_thresholds,","18","                   X_BINNED_DTYPE_C [::1, :] binned):","51","        if data[i] == INFINITY:","52","            # Special case for +inf.","53","            # -inf is handled properly by binary search.","54","            binned[i] = binning_thresholds.shape[0]"]}],"sklearn\/ensemble\/_hist_gradient_boosting\/_predictor.pyx":[{"add":["9","from libc.math cimport isnan","14","from .common cimport X_DTYPE_C","15","from .common cimport Y_DTYPE_C","16","from .common import Y_DTYPE","17","from .common cimport X_BINNED_DTYPE_C","18","from .common cimport node_struct","46","","47","        if isnan(numeric_data[row, node.feature_idx]):","48","            if node.missing_go_to_left:","49","                node = nodes[node.left]","50","            else:","51","                node = nodes[node.right]","62","        const unsigned char missing_values_bin_idx,","69","        out[i] = _predict_one_from_binned_data(nodes, binned_data, i,","70","                                               missing_values_bin_idx)","76","        const int row,","77","        const unsigned char missing_values_bin_idx) nogil:","87","        if binned_data[row, node.feature_idx] ==  missing_values_bin_idx:","88","            if node.missing_go_to_left:","89","                node = nodes[node.left]","90","            else:","91","                node = nodes[node.right]","93","            if binned_data[row, node.feature_idx] <= node.bin_threshold:","94","                node = nodes[node.left]","95","            else:","96","                node = nodes[node.right]"],"delete":["13","from .types cimport X_DTYPE_C","14","from .types cimport Y_DTYPE_C","15","from .types import Y_DTYPE","16","from .types cimport X_BINNED_DTYPE_C","17","from .types cimport node_struct","45","        if numeric_data[row, node.feature_idx] == INFINITY:","46","            # if data is +inf we always go to the right child, even when the","47","            # threhsold is +inf","48","            node = nodes[node.right]","65","        out[i] = _predict_one_from_binned_data(nodes, binned_data, i)","71","        const int row) nogil:","81","        if binned_data[row, node.feature_idx] <= node.bin_threshold:","82","            node = nodes[node.left]","84","            node = nodes[node.right]"]}],"sklearn\/ensemble\/setup.py":[{"add":["39","    config.add_extension(\"_hist_gradient_boosting.common\",","40","                         sources=[\"_hist_gradient_boosting\/common.pyx\"],"],"delete":["39","    config.add_extension(\"_hist_gradient_boosting.types\",","40","                         sources=[\"_hist_gradient_boosting\/types.pyx\"],"]}],"doc\/modules\/ensemble.rst":[{"add":["866","controls the number of iterations of the boosting process::","876","  >>> clf = HistGradientBoostingClassifier(max_iter=100).fit(X_train, y_train)","878","  0.8965","897","Missing values support","898","----------------------","899","","900",":class:`HistGradientBoostingClassifier` and","901",":class:`HistGradientBoostingRegressor` have built-in support for missing","902","values (NaNs).","903","","904","During training, the tree grower learns at each split point whether samples","905","with missing values should go to the left or right child, based on the","906","potential gain. When predicting, samples with missing values are assigned to","907","the left or right child consequently::","908","","909","  >>> from sklearn.experimental import enable_hist_gradient_boosting  # noqa","910","  >>> from sklearn.ensemble import HistGradientBoostingClassifier","911","  >>> import numpy as np","912","","913","  >>> X = np.array([0, 1, 2, np.nan]).reshape(-1, 1)","914","  >>> y = [0, 0, 1, 1]","915","","916","  >>> gbdt = HistGradientBoostingClassifier(min_samples_leaf=1).fit(X, y)","917","  >>> gbdt.predict(X)","918","  array([0, 0, 1, 1])","919","","920","When the missingness pattern is predictive, the splits can be done on","921","whether the feature value is missing or not::","922","","923","  >>> X = np.array([0, np.nan, 1, 2, np.nan]).reshape(-1, 1)","924","  >>> y = [0, 1, 0, 0, 1]","925","  >>> gbdt = HistGradientBoostingClassifier(min_samples_leaf=1,","926","  ...                                       max_depth=2,","927","  ...                                       learning_rate=1,","928","  ...                                       max_iter=1).fit(X, y)","929","  >>> gbdt.predict(X)","930","  array([0, 1, 0, 0, 1])","931","","932","If no missing values were encountered for a given feature during training,","933","then samples with missing values are mapped to whichever child has the most","934","samples.","935",""],"delete":["866","controls the number of iterations of the boosting process:","875","  >>> clf = HistGradientBoostingClassifier(max_iter=100).fit(X_train, y_train)","878","  0.8998"]}],"sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_gradient_boosting.py":[{"add":["2","from numpy.testing import assert_allclose","4","from sklearn.preprocessing import KBinsDiscretizer, MinMaxScaler","5","from sklearn.model_selection import train_test_split","6","from sklearn.base import clone, BaseEstimator, TransformerMixin","7","from sklearn.pipeline import make_pipeline","38","     ({'max_bins': 256}, 'max_bins=256 should be no smaller than 2 and no'),","177","    assert np.all(mapper_training_data.n_bins_non_missing_ ==","179","    assert np.all(mapper_training_data.n_bins_non_missing_ !=","180","                  mapper_whole_data.n_bins_non_missing_)","181","","182","","183","def test_missing_values_trivial():","184","    # sanity check for missing values support. With only one feature and","185","    # y == isnan(X), the gbdt is supposed to reach perfect accuracy on the","186","    # training set.","187","","188","    n_samples = 100","189","    n_features = 1","190","    rng = np.random.RandomState(0)","191","","192","    X = rng.normal(size=(n_samples, n_features))","193","    mask = rng.binomial(1, .5, size=X.shape).astype(np.bool)","194","    X[mask] = np.nan","195","    y = mask.ravel()","196","    gb = HistGradientBoostingClassifier()","197","    gb.fit(X, y)","198","","199","    assert gb.score(X, y) == pytest.approx(1)","200","","201","","202","@pytest.mark.parametrize('problem', ('classification', 'regression'))","203","@pytest.mark.parametrize(","204","    'missing_proportion, expected_min_score_classification, '","205","    'expected_min_score_regression', [","206","        (.1, .97, .89),","207","        (.2, .93, .81),","208","        (.5, .79, .52)])","209","def test_missing_values_resilience(problem, missing_proportion,","210","                                   expected_min_score_classification,","211","                                   expected_min_score_regression):","212","    # Make sure the estimators can deal with missing values and still yield","213","    # decent predictions","214","","215","    rng = np.random.RandomState(0)","216","    n_samples = 1000","217","    n_features = 2","218","    if problem == 'regression':","219","        X, y = make_regression(n_samples=n_samples, n_features=n_features,","220","                               n_informative=n_features, random_state=rng)","221","        gb = HistGradientBoostingRegressor()","222","        expected_min_score = expected_min_score_regression","223","    else:","224","        X, y = make_classification(n_samples=n_samples, n_features=n_features,","225","                                   n_informative=n_features, n_redundant=0,","226","                                   n_repeated=0, random_state=rng)","227","        gb = HistGradientBoostingClassifier()","228","        expected_min_score = expected_min_score_classification","229","","230","    mask = rng.binomial(1, missing_proportion, size=X.shape).astype(np.bool)","231","    X[mask] = np.nan","232","","233","    gb.fit(X, y)","234","","235","    assert gb.score(X, y) > expected_min_score","284","def test_missing_values_minmax_imputation():","285","    # Compare the buit-in missing value handling of Histogram GBC with an","286","    # a-priori missing value imputation strategy that should yield the same","287","    # results in terms of decision function.","288","    #","289","    # Each feature (containing NaNs) is replaced by 2 features:","290","    # - one where the nans are replaced by min(feature) - 1","291","    # - one where the nans are replaced by max(feature) + 1","292","    # A split where nans go to the left has an equivalent split in the","293","    # first (min) feature, and a split where nans go to the right has an","294","    # equivalent split in the second (max) feature.","295","    #","296","    # Assuming the data is such that there is never a tie to select the best","297","    # feature to split on during training, the learned decision trees should be","298","    # strictly equivalent (learn a sequence of splits that encode the same","299","    # decision function).","300","    #","301","    # The MinMaxImputer transformer is meant to be a toy implementation of the","302","    # \"Missing In Attributes\" (MIA) missing value handling for decision trees","303","    # https:\/\/www.sciencedirect.com\/science\/article\/abs\/pii\/S0167865508000305","304","    # The implementation of MIA as an imputation transformer was suggested by","305","    # \"Remark 3\" in https:\/\/arxiv.org\/abs\/1902.06931","306","","307","    class MinMaxImputer(BaseEstimator, TransformerMixin):","308","","309","        def fit(self, X, y=None):","310","            mm = MinMaxScaler().fit(X)","311","            self.data_min_ = mm.data_min_","312","            self.data_max_ = mm.data_max_","313","            return self","314","","315","        def transform(self, X):","316","            X_min, X_max = X.copy(), X.copy()","317","","318","            for feature_idx in range(X.shape[1]):","319","                nan_mask = np.isnan(X[:, feature_idx])","320","                X_min[nan_mask, feature_idx] = self.data_min_[feature_idx] - 1","321","                X_max[nan_mask, feature_idx] = self.data_max_[feature_idx] + 1","322","","323","            return np.concatenate([X_min, X_max], axis=1)","324","","325","    def make_missing_value_data(n_samples=int(1e4), seed=0):","326","        rng = np.random.RandomState(seed)","327","        X, y = make_regression(n_samples=n_samples, n_features=4,","328","                               random_state=rng)","329","","330","        # Pre-bin the data to ensure a deterministic handling by the 2","331","        # strategies and also make it easier to insert np.nan in a structured","332","        # way:","333","        X = KBinsDiscretizer(n_bins=42, encode=\"ordinal\").fit_transform(X)","334","","335","        # First feature has missing values completely at random:","336","        rnd_mask = rng.rand(X.shape[0]) > 0.9","337","        X[rnd_mask, 0] = np.nan","338","","339","        # Second and third features have missing values for extreme values","340","        # (censoring missingness):","341","        low_mask = X[:, 1] == 0","342","        X[low_mask, 1] = np.nan","343","","344","        high_mask = X[:, 2] == X[:, 2].max()","345","        X[high_mask, 2] = np.nan","346","","347","        # Make the last feature nan pattern very informative:","348","        y_max = np.percentile(y, 70)","349","        y_max_mask = y >= y_max","350","        y[y_max_mask] = y_max","351","        X[y_max_mask, 3] = np.nan","352","","353","        # Check that there is at least one missing value in each feature:","354","        for feature_idx in range(X.shape[1]):","355","            assert any(np.isnan(X[:, feature_idx]))","356","","357","        # Let's use a test set to check that the learned decision function is","358","        # the same as evaluated on unseen data. Otherwise it could just be the","359","        # case that we find two independent ways to overfit the training set.","360","        return train_test_split(X, y, random_state=rng)","361","","362","    # n_samples need to be large enough to minimize the likelihood of having","363","    # several candidate splits with the same gain value in a given tree.","364","    X_train, X_test, y_train, y_test = make_missing_value_data(","365","        n_samples=int(1e4), seed=0)","366","","367","    # Use a small number of leaf nodes and iterations so as to keep","368","    # under-fitting models to minimize the likelihood of ties when training the","369","    # model.","370","    gbm1 = HistGradientBoostingRegressor(max_iter=100,","371","                                         max_leaf_nodes=5,","372","                                         random_state=0)","373","    gbm1.fit(X_train, y_train)","374","","375","    gbm2 = make_pipeline(MinMaxImputer(), clone(gbm1))","376","    gbm2.fit(X_train, y_train)","377","","378","    # Check that the model reach the same score:","379","    assert gbm1.score(X_train, y_train) == \\","380","        pytest.approx(gbm2.score(X_train, y_train))","381","","382","    assert gbm1.score(X_test, y_test) == \\","383","        pytest.approx(gbm2.score(X_test, y_test))","384","","385","    # Check the individual prediction match as a finer grained","386","    # decision function check.","387","    assert_allclose(gbm1.predict(X_train), gbm2.predict(X_train))","388","    assert_allclose(gbm1.predict(X_test), gbm2.predict(X_test))","389","","390","","392","    # Basic test for infinite values","400","","401","","402","def test_infinite_values_missing_values():","403","    # High level test making sure that inf and nan values are properly handled","404","    # when both are present. This is similar to","405","    # test_split_on_nan_with_infinite_values() in test_grower.py, though we","406","    # cannot check the predicitons for binned values here.","407","","408","    X = np.asarray([-np.inf, 0, 1, np.inf, np.nan]).reshape(-1, 1)","409","    y_isnan = np.isnan(X.ravel())","410","    y_isinf = X.ravel() == np.inf","411","","412","    stump_clf = HistGradientBoostingClassifier(min_samples_leaf=1, max_iter=1,","413","                                               learning_rate=1, max_depth=2)","414","","415","    assert stump_clf.fit(X, y_isinf).score(X, y_isinf) == 1","416","    assert stump_clf.fit(X, y_isnan).score(X, y_isnan) == 1"],"delete":["33","     ({'max_bins': 257}, 'max_bins=257 should be no smaller than 2 and no'),","172","    assert np.all(mapper_training_data.actual_n_bins_ ==","174","    assert np.all(mapper_training_data.actual_n_bins_ !=","175","                  mapper_whole_data.actual_n_bins_)"]}],"sklearn\/ensemble\/_hist_gradient_boosting\/common.pyx":[{"add":[],"delete":[]}],"sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_compare_lightgbm.py":[{"add":["45","    max_bins = 255","53","        X = _BinMapper(n_bins=max_bins + 1).fit_transform(X).astype(np.float32)","97","    max_bins = 255","105","        X = _BinMapper(n_bins=max_bins + 1).fit_transform(X).astype(np.float32)","157","    max_bins = 255","167","        X = _BinMapper(n_bins=max_bins + 1).fit_transform(X).astype(np.float32)"],"delete":["45","    max_bins = 256","53","        X = _BinMapper(max_bins=max_bins).fit_transform(X).astype(np.float32)","97","    max_bins = 256","105","        X = _BinMapper(max_bins=max_bins).fit_transform(X).astype(np.float32)","157","    max_bins = 256","167","        X = _BinMapper(max_bins=max_bins).fit_transform(X).astype(np.float32)"]}],"sklearn\/ensemble\/_hist_gradient_boosting\/splitting.pyx":[{"add":["22","from .common cimport X_BINNED_DTYPE_C","23","from .common cimport Y_DTYPE_C","24","from .common cimport hist_struct","25","from .common import HISTOGRAM_DTYPE","34","    unsigned char missing_go_to_left","54","    missing_go_to_left : bool","55","        Whether missing values should go to the left child.","69","    def __init__(self, gain, feature_idx, bin_idx,","70","                 missing_go_to_left, sum_gradient_left, sum_hessian_left,","71","                 sum_gradient_right, sum_hessian_right, n_samples_left,","72","                 n_samples_right):","76","        self.missing_go_to_left = missing_go_to_left","98","    n_bins_non_missing : ndarray, shape (n_features,)","99","        For each feature, gives the number of bins actually used for","100","        non-missing values.","101","    missing_values_bin_idx : uint8","102","        Index of the bin that is used for missing values. This is the index of","103","        the last bin and is always equal to max_bins (as passed to the GBDT","104","        classes), or equivalently to n_bins - 1.","105","    has_missing_values : ndarray, shape (n_features,)","106","        Whether missing values were observed in the training data, for each","107","        feature.","125","        const unsigned int [::1] n_bins_non_missing","126","        unsigned char missing_values_bin_idx","127","        const unsigned char [::1] has_missing_values","138","    def __init__(self,","139","                 const X_BINNED_DTYPE_C [::1, :] X_binned,","140","                 const unsigned int [::1] n_bins_non_missing,","141","                 const unsigned char missing_values_bin_idx,","142","                 const unsigned char [::1] has_missing_values,","143","                 Y_DTYPE_C l2_regularization,","144","                 Y_DTYPE_C min_hessian_to_split=1e-3,","145","                 unsigned int min_samples_leaf=20,","146","                 Y_DTYPE_C min_gain_to_split=0.,","151","        self.n_bins_non_missing = n_bins_non_missing","152","        self.missing_values_bin_idx = missing_values_bin_idx","153","        self.has_missing_values = has_missing_values","250","            unsigned char missing_go_to_left = split_info.missing_go_to_left","251","            unsigned char missing_values_bin_idx = self.missing_values_bin_idx","276","            unsigned char turn_left","298","                    turn_left = sample_goes_left(","299","                        missing_go_to_left,","300","                        missing_values_bin_idx, bin_idx,","301","                        X_binned[sample_idx])","302","","303","                    if turn_left:","380","            const unsigned char [:] has_missing_values = self.has_missing_values","389","                # Start with a gain of -1 (if no better split is found, that","390","                # means one of the constraints isn't respected","391","                # (min_samples_leaf, etc) and the grower will later turn the","392","                # node into a leaf.","393","                split_infos[feature_idx].gain = -1","394","","395","                # We will scan bins from left to right (in all cases), and if","396","                # there are any missing values, we will also scan bins from","397","                # right to left. This way, we can consider whichever case","398","                # yields the best gain: either missing values go to the right","399","                # (left to right scan) or to the left (right to left case).","400","                # See algo 3 from the XGBoost paper","401","                # https:\/\/arxiv.org\/abs\/1603.02754","402","","403","                self._find_best_bin_to_split_left_to_right(","404","                    feature_idx, has_missing_values[feature_idx],","405","                    histograms, n_samples, sum_gradients, sum_hessians,","406","                    &split_infos[feature_idx])","407","","408","                if has_missing_values[feature_idx]:","409","                    # We need to explore both directions to check whether","410","                    # sending the nans to the left child would lead to a higher","411","                    # gain","412","                    self._find_best_bin_to_split_right_to_left(","413","                        feature_idx, histograms, n_samples,","414","                        sum_gradients, sum_hessians, &split_infos[feature_idx])","425","            split_info.missing_go_to_left,","436","    cdef unsigned int _find_best_feature_to_split_helper(","441","            unsigned int feature_idx","442","            unsigned int best_feature_idx = 0","450","    cdef void _find_best_bin_to_split_left_to_right(","453","            unsigned char has_missing_values,","457","            Y_DTYPE_C sum_hessians,","458","            split_info_struct * split_info) nogil:  # OUT","462","        (min_gain_to_split, etc.) are discarded here.","463","","464","        We scan node from left to right. This version is called whether there","465","        are missing values or not. If any, missing values are assigned to the","466","        right node.","473","            # We set the 'end' variable such that the last non-missing-values","474","            # bin never goes to the left child (which would result in and","475","            # empty right child), unless there are missing values, since these","476","            # would go to the right child.","477","            unsigned int end = \\","478","                self.n_bins_non_missing[feature_idx] - 1 + has_missing_values","489","                                                   sum_hessians,","490","                                                   self.l2_regularization)","492","","493","        for bin_idx in range(end):","524","            if gain > split_info.gain and gain > self.min_gain_to_split:","525","                split_info.gain = gain","526","                split_info.feature_idx = feature_idx","527","                split_info.bin_idx = bin_idx","528","                # we scan from left to right so missing values go to the right","529","                split_info.missing_go_to_left = False","530","                split_info.sum_gradient_left = sum_gradient_left","531","                split_info.sum_gradient_right = sum_gradient_right","532","                split_info.sum_hessian_left = sum_hessian_left","533","                split_info.sum_hessian_right = sum_hessian_right","534","                split_info.n_samples_left = n_samples_left","535","                split_info.n_samples_right = n_samples_right","537","    cdef void _find_best_bin_to_split_right_to_left(","538","            self,","539","            unsigned int feature_idx,","540","            const hist_struct [:, ::1] histograms,  # IN","541","            unsigned int n_samples,","542","            Y_DTYPE_C sum_gradients,","543","            Y_DTYPE_C sum_hessians,","544","            split_info_struct * split_info) nogil:  # OUT","545","        \"\"\"Find best bin to split on for a given feature.","547","        Splits that do not satisfy the splitting constraints","548","        (min_gain_to_split, etc.) are discarded here.","549","","550","        We scan node from right to left. This version is only called when","551","        there are missing values. Missing values are assigned to the left","552","        child.","553","","554","        If no missing value are present in the data this method isn't called","555","        since only calling _find_best_bin_to_split_left_to_right is enough.","556","        \"\"\"","557","","558","        cdef:","559","            unsigned int bin_idx","560","            unsigned int n_samples_left","561","            unsigned int n_samples_right","562","            unsigned int n_samples_ = n_samples","563","            Y_DTYPE_C sum_hessian_left","564","            Y_DTYPE_C sum_hessian_right","565","            Y_DTYPE_C sum_gradient_left","566","            Y_DTYPE_C sum_gradient_right","567","            Y_DTYPE_C negative_loss_current_node","568","            Y_DTYPE_C gain","569","            unsigned int start = self.n_bins_non_missing[feature_idx] - 2","570","","571","        sum_gradient_right, sum_hessian_right = 0., 0.","572","        n_samples_right = 0","573","        negative_loss_current_node = negative_loss(sum_gradients,","574","                                                   sum_hessians,","575","                                                   self.l2_regularization)","576","","577","        for bin_idx in range(start, -1, -1):","578","            n_samples_right += histograms[feature_idx, bin_idx + 1].count","579","            n_samples_left = n_samples_ - n_samples_right","580","","581","            if self.hessians_are_constant:","582","                sum_hessian_right += histograms[feature_idx, bin_idx + 1].count","583","            else:","584","                sum_hessian_right += \\","585","                    histograms[feature_idx, bin_idx + 1].sum_hessians","586","            sum_hessian_left = sum_hessians - sum_hessian_right","587","","588","            sum_gradient_right += \\","589","                histograms[feature_idx, bin_idx + 1].sum_gradients","590","            sum_gradient_left = sum_gradients - sum_gradient_right","591","","592","            if n_samples_right < self.min_samples_leaf:","593","                continue","594","            if n_samples_left < self.min_samples_leaf:","595","                # won't get any better","596","                break","597","","598","            if sum_hessian_right < self.min_hessian_to_split:","599","                continue","600","            if sum_hessian_left < self.min_hessian_to_split:","601","                # won't get any better (hessians are > 0 since loss is convex)","602","                break","603","","604","            gain = _split_gain(sum_gradient_left, sum_hessian_left,","605","                               sum_gradient_right, sum_hessian_right,","606","                               negative_loss_current_node,","607","                               self.l2_regularization)","608","","609","            if gain > split_info.gain and gain > self.min_gain_to_split:","610","                split_info.gain = gain","611","                split_info.feature_idx = feature_idx","612","                split_info.bin_idx = bin_idx","613","                # we scan from right to left so missing values go to the left","614","                split_info.missing_go_to_left = True","615","                split_info.sum_gradient_left = sum_gradient_left","616","                split_info.sum_gradient_right = sum_gradient_right","617","                split_info.sum_hessian_left = sum_hessian_left","618","                split_info.sum_hessian_right = sum_hessian_right","619","                split_info.n_samples_left = n_samples_left","620","                split_info.n_samples_right = n_samples_right","652","","653","cdef inline unsigned char sample_goes_left(","654","        unsigned char missing_go_to_left,","655","        unsigned char missing_values_bin_idx,","656","        X_BINNED_DTYPE_C split_bin_idx,","657","        X_BINNED_DTYPE_C bin_value) nogil:","658","    \"\"\"Helper to decide whether sample should go to left or right child.\"\"\"","659","","660","    return (","661","        (","662","            missing_go_to_left and","663","            bin_value == missing_values_bin_idx","664","        )","665","        or (","666","            bin_value <= split_bin_idx","667","        ))"],"delete":["22","from .types cimport X_BINNED_DTYPE_C","23","from .types cimport Y_DTYPE_C","24","from .types cimport hist_struct","25","from .types import HISTOGRAM_DTYPE","66","    def __init__(self, gain, feature_idx, bin_idx, sum_gradient_left,","67","                 sum_hessian_left, sum_gradient_right, sum_hessian_right,","68","                 n_samples_left, n_samples_right):","93","    actual_n_bins : ndarray, shape (n_features,)","94","        The actual number of bins needed for each feature, which is lower or","95","        equal to max_bins.","113","        unsigned int [::1] actual_n_bins","124","    def __init__(self, const X_BINNED_DTYPE_C [::1, :] X_binned,","125","                 np.ndarray[np.uint32_t] actual_n_bins,","126","                 Y_DTYPE_C l2_regularization, Y_DTYPE_C","127","                 min_hessian_to_split=1e-3, unsigned int","128","                 min_samples_leaf=20, Y_DTYPE_C min_gain_to_split=0.,","133","        self.actual_n_bins = actual_n_bins","275","                    if X_binned[sample_idx] <= bin_idx:","360","                split_info = self._find_best_bin_to_split_helper(","361","                    feature_idx, histograms, n_samples,","362","                    sum_gradients, sum_hessians)","363","                split_infos[feature_idx] = split_info","384","    cdef int _find_best_feature_to_split_helper(","389","            int feature_idx","390","            int best_feature_idx = 0","398","    cdef split_info_struct _find_best_bin_to_split_helper(","404","            Y_DTYPE_C sum_hessians) nogil:","408","        (min_gain_to_split, etc.) are discarded here. If no split can","409","        satisfy the constraints, a SplitInfo with a gain of -1 is returned.","410","        If for a given node the best SplitInfo has a gain of -1, it is","411","        finalized into a leaf in the grower.","424","            split_info_struct best_split","426","        best_split.gain = -1.","430","            sum_hessians, self.l2_regularization)","432","        for bin_idx in range(self.actual_n_bins[feature_idx] - 1):","433","            # Note that considering splitting on the last bin is useless since","434","            # it would result in having 0 samples in the right node (forbidden)","465","            if gain > best_split.gain and gain > self.min_gain_to_split:","466","                best_split.gain = gain","467","                best_split.feature_idx = feature_idx","468","                best_split.bin_idx = bin_idx","469","                best_split.sum_gradient_left = sum_gradient_left","470","                best_split.sum_gradient_right = sum_gradient_right","471","                best_split.sum_hessian_left = sum_hessian_left","472","                best_split.sum_hessian_right = sum_hessian_right","473","                best_split.n_samples_left = n_samples_left","474","                best_split.n_samples_right = n_samples_right","476","        return best_split"]}],"sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_grower.py":[{"add":["6","from sklearn.ensemble._hist_gradient_boosting.common import X_BINNED_DTYPE","7","from sklearn.ensemble._hist_gradient_boosting.common import Y_DTYPE","8","from sklearn.ensemble._hist_gradient_boosting.common import G_H_DTYPE","87","                        n_bins=n_bins, shrinkage=shrinkage,","149","                        n_bins=n_bins, shrinkage=1.,","165","        [128, 254],","169","        [254, 85],","172","        [129, 254],","175","    missing_values_bin_idx = n_bins - 1","176","    predictions = predictor.predict_binned(input_data, missing_values_bin_idx)","181","    predictions = predictor.predict_binned(X_binned, missing_values_bin_idx)","206","    mapper = _BinMapper(n_bins=n_bins)","213","                        n_bins=n_bins, shrinkage=1.,","238","    n_bins = 256","243","    mapper = _BinMapper(n_bins=n_bins)","249","                        n_bins=n_bins, shrinkage=1.,","264","    n_bins = 256","270","    mapper = _BinMapper(n_bins=n_bins)","310","","311","","312","def test_missing_value_predict_only():","313","    # Make sure that missing values are supported at predict time even if they","314","    # were not encountered in the training data: the missing values are","315","    # assigned to whichever child has the most samples.","316","","317","    rng = np.random.RandomState(0)","318","    n_samples = 100","319","    X_binned = rng.randint(0, 256, size=(n_samples, 1), dtype=np.uint8)","320","    X_binned = np.asfortranarray(X_binned)","321","","322","    gradients = rng.normal(size=n_samples).astype(G_H_DTYPE)","323","    hessians = np.ones(shape=1, dtype=G_H_DTYPE)","324","","325","    grower = TreeGrower(X_binned, gradients, hessians, min_samples_leaf=5,","326","                        has_missing_values=False)","327","    grower.grow()","328","","329","    predictor = grower.make_predictor()","330","","331","    # go from root to a leaf, always following node with the most samples.","332","    # That's the path nans are supposed to take","333","    node = predictor.nodes[0]","334","    while not node['is_leaf']:","335","        left = predictor.nodes[node['left']]","336","        right = predictor.nodes[node['right']]","337","        node = left if left['count'] > right['count'] else right","338","","339","    prediction_main_path = node['value']","340","","341","    # now build X_test with only nans, and make sure all predictions are equal","342","    # to prediction_main_path","343","    all_nans = np.full(shape=(n_samples, 1), fill_value=np.nan)","344","    assert np.all(predictor.predict(all_nans) == prediction_main_path)","345","","346","","347","def test_split_on_nan_with_infinite_values():","348","    # Make sure the split on nan situations are respected even when there are","349","    # samples with +inf values (we set the threshold to +inf when we have a","350","    # split on nan so this test makes sure this does not introduce edge-case","351","    # bugs). We need to use the private API so that we can also test","352","    # predict_binned().","353","","354","    X = np.array([0, 1, np.inf, np.nan, np.nan]).reshape(-1, 1)","355","    # the gradient values will force a split on nan situation","356","    gradients = np.array([0, 0, 0, 100, 100], dtype=G_H_DTYPE)","357","    hessians = np.ones(shape=1, dtype=G_H_DTYPE)","358","","359","    bin_mapper = _BinMapper()","360","    X_binned = bin_mapper.fit_transform(X)","361","","362","    n_bins_non_missing = 3","363","    has_missing_values = True","364","    grower = TreeGrower(X_binned, gradients, hessians,","365","                        n_bins_non_missing=n_bins_non_missing,","366","                        has_missing_values=has_missing_values,","367","                        min_samples_leaf=1)","368","","369","    grower.grow()","370","","371","    predictor = grower.make_predictor(","372","        bin_thresholds=bin_mapper.bin_thresholds_","373","    )","374","","375","    # sanity check: this was a split on nan","376","    assert predictor.nodes[0]['threshold'] == np.inf","377","    assert predictor.nodes[0]['bin_threshold'] == n_bins_non_missing - 1","378","","379","    # Make sure in particular that the +inf sample is mapped to the left child","380","    # Note that lightgbm \"fails\" here and will assign the inf sample to the","381","    # right child, even though it's a \"split on nan\" situation.","382","    predictions = predictor.predict(X)","383","    predictions_binned = predictor.predict_binned(","384","        X_binned, missing_values_bin_idx=bin_mapper.missing_values_bin_idx_)","385","    assert np.all(predictions == -gradients)","386","    assert np.all(predictions_binned == -gradients)"],"delete":["6","from sklearn.ensemble._hist_gradient_boosting.types import X_BINNED_DTYPE","7","from sklearn.ensemble._hist_gradient_boosting.types import Y_DTYPE","8","from sklearn.ensemble._hist_gradient_boosting.types import G_H_DTYPE","87","                        max_bins=n_bins, shrinkage=shrinkage,","149","                        max_bins=n_bins, shrinkage=1.,","165","        [128, 255],","169","        [255, 85],","172","        [129, 255],","175","    predictions = predictor.predict_binned(input_data)","180","    predictions = predictor.predict_binned(X_binned)","205","    mapper = _BinMapper(max_bins=n_bins)","212","                        max_bins=n_bins, shrinkage=1.,","237","    max_bins = 255","242","    mapper = _BinMapper(max_bins=max_bins)","248","                        max_bins=max_bins, shrinkage=1.,","263","    max_bins = 255","269","    mapper = _BinMapper(max_bins=max_bins)"]}],"sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_loss.py":[{"add":["9","from sklearn.ensemble._hist_gradient_boosting.common import Y_DTYPE","10","from sklearn.ensemble._hist_gradient_boosting.common import G_H_DTYPE"],"delete":["9","from sklearn.ensemble._hist_gradient_boosting.types import Y_DTYPE","10","from sklearn.ensemble._hist_gradient_boosting.types import G_H_DTYPE"]}],"sklearn\/ensemble\/_hist_gradient_boosting\/common.pxd":[{"add":[],"delete":[]}],"sklearn\/ensemble\/_hist_gradient_boosting\/predictor.py":[{"add":["7","from .common import Y_DTYPE","49","    def predict_binned(self, X, missing_values_bin_idx):","56","        missing_values_bin_idx : uint8","57","            Index of the bin that is used for missing values. This is the","58","            index of the last bin and is always equal to max_bins (as passed","59","            to the GBDT classes), or equivalently to n_bins - 1.","67","        _predict_from_binned_data(self.nodes, X, missing_values_bin_idx, out)"],"delete":["7","from .types import Y_DTYPE","49","    def predict_binned(self, X):","63","        _predict_from_binned_data(self.nodes, X, out)"]}],"sklearn\/ensemble\/_hist_gradient_boosting\/binning.py":[{"add":["15","from .common import X_DTYPE, X_BINNED_DTYPE, ALMOST_INF","21","    Missing values are ignored for finding the thresholds.","22","","27","    max_bins: int","28","        The maximum number of bins to use for non-missing values. If for a","29","        given feature the number of unique values is less than ``max_bins``,","30","        then those unique values will be used to compute the bin thresholds,","31","        instead of the quantiles.","54","        col_data = data[:, f_idx]","55","        # ignore missing values when computing bin thresholds","56","        missing_mask = np.isnan(col_data)","57","        if missing_mask.any():","58","            col_data = col_data[~missing_mask]","59","        col_data = np.ascontiguousarray(col_data, dtype=X_DTYPE)","70","            percentiles = np.linspace(0, 100, num=max_bins + 1)","71","            percentiles = percentiles[1:-1]","74","            assert midpoints.shape[0] == max_bins - 1","75","","76","        # We avoid having +inf thresholds: +inf thresholds are only allowed in","77","        # a \"split on nan\" situation.","78","        np.clip(midpoints, a_min=None, a_max=ALMOST_INF, out=midpoints)","79","","81","","94","    Features with a small number of values may be binned into less than","95","    ``n_bins`` bins. The last bin (at index ``n_bins - 1``) is always reserved","96","    for missing values.","100","    n_bins : int, optional (default=256)","101","        The maximum number of bins to use (including the bin for missing","102","        values). Non-missing values are binned on ``max_bins = n_bins - 1``","103","        bins. The last bin is always reserved for missing values. If for a","104","        given feature the number of unique values is less than ``max_bins``,","105","        then those unique values will be used to compute the bin thresholds,","106","        instead of the quantiles.","115","","116","    Attributes","117","    ----------","118","    bin_thresholds_ : list of arrays","119","        For each feature, gives the real-valued bin threhsolds. There are","120","        ``max_bins - 1`` thresholds, where ``max_bins = n_bins - 1`` is the","121","        number of bins used for non-missing values.","122","    n_bins_non_missing_ : array of uint32","123","        For each feature, gives the number of bins actually used for","124","        non-missing values. For features with a lot of unique values, this is","125","        equal to ``n_bins - 1``.","126","    missing_values_bin_idx_ : uint8","127","        The index of the bin where missing values are mapped. This is a","128","        constant accross all features. This corresponds to the last bin, and","129","        it is always equal to ``n_bins - 1``. Note that if ``n_bins_missing_``","130","        is less than ``n_bins - 1`` for a given feature, then there are","131","        empty (and unused) bins.","133","    def __init__(self, n_bins=256, subsample=int(2e5), random_state=None):","134","        self.n_bins = n_bins","141","        The last bin is reserved for missing values, whether missing values","142","        are present in the data or not.","143","","155","        if not (3 <= self.n_bins <= 256):","156","            # min is 3: at least 2 distinct bins and a missing values bin","157","            raise ValueError('n_bins={} should be no smaller than 3 '","158","                             'and no larger than 256.'.format(self.n_bins))","159","","161","        max_bins = self.n_bins - 1","163","            X, max_bins, subsample=self.subsample,","166","        self.n_bins_non_missing_ = np.array(","170","        self.missing_values_bin_idx_ = self.n_bins - 1","171","","177","        Missing values will be mapped to the last bin.","178","","191","        if X.shape[1] != self.n_bins_non_missing_.shape[0]:","194","                'to transform()'.format(self.n_bins_non_missing_.shape[0],","198","        _map_to_bins(X, self.bin_thresholds_, self.missing_values_bin_idx_,","199","                     binned)"],"delete":["15","from .types import X_DTYPE, X_BINNED_DTYPE","25","    max_bins : int","26","        The maximum number of bins to use. If for a given feature the number of","27","        unique values is less than ``max_bins``, then those unique values","28","        will be used to compute the bin thresholds, instead of the quantiles.","44","    if not (2 <= max_bins <= 256):","45","        raise ValueError('max_bins={} should be no smaller than 2 '","46","                         'and no larger than 256.'.format(max_bins))","52","    percentiles = np.linspace(0, 100, num=max_bins + 1)","53","    percentiles = percentiles[1:-1]","56","        col_data = np.ascontiguousarray(data[:, f_idx], dtype=X_DTYPE)","82","    If the number of unique values for a given feature is less than","83","    ``max_bins``, then the unique values of this feature are used instead of","84","    the quantiles.","88","    max_bins : int, optional (default=256)","89","        The maximum number of bins to use. If for a given feature the number of","90","        unique values is less than ``max_bins``, then those unique values","91","        will be used to compute the bin thresholds, instead of the quantiles.","101","    def __init__(self, max_bins=256, subsample=int(2e5), random_state=None):","102","        self.max_bins = max_bins","122","            X, self.max_bins, subsample=self.subsample,","125","        self.actual_n_bins_ = np.array(","146","        if X.shape[1] != self.actual_n_bins_.shape[0]:","149","                'to transform()'.format(self.actual_n_bins_.shape[0],","153","        _map_to_bins(X, self.bin_thresholds_, binned)"]}],"sklearn\/ensemble\/_hist_gradient_boosting\/grower.py":[{"add":["17","from .common import PREDICTOR_RECORD_DTYPE","18","from .common import Y_DTYPE","143","    n_bins : int, optional (default=256)","144","        The total number of bins, including the bin for missing values. Used","145","        to define the shape of the histograms.","146","    n_bins_non_missing_ : array of uint32","147","        For each feature, gives the number of bins actually used for","148","        non-missing values. For features with a lot of unique values, this","149","        is equal to ``n_bins - 1``. If it's an int, all features are","150","        considered to have the same number of bins. If None, all features","151","        are considered to have ``n_bins - 1`` bins.","152","    has_missing_values : ndarray of bool or bool, optional (default=False)","153","        Whether each feature contains missing values (in the training data).","154","        If it's a bool, the same value is used for all features.","167","                 n_bins=256, n_bins_non_missing=None, has_missing_values=False,","168","                 l2_regularization=0., min_hessian_to_split=1e-3,","169","                 shrinkage=1.):","175","        if n_bins_non_missing is None:","176","            n_bins_non_missing = n_bins - 1","178","        if isinstance(n_bins_non_missing, numbers.Integral):","179","            n_bins_non_missing = np.array(","180","                [n_bins_non_missing] * X_binned.shape[1],","183","            n_bins_non_missing = np.asarray(n_bins_non_missing,","184","                                            dtype=np.uint32)","185","","186","        if isinstance(has_missing_values, bool):","187","            has_missing_values = [has_missing_values] * X_binned.shape[1]","188","        has_missing_values = np.asarray(has_missing_values, dtype=np.uint8)","192","            X_binned, n_bins, gradients, hessians, hessians_are_constant)","193","        missing_values_bin_idx = n_bins - 1","195","            X_binned, n_bins_non_missing, missing_values_bin_idx,","196","            has_missing_values, l2_regularization, min_hessian_to_split,","197","            min_samples_leaf, min_gain_to_split, hessians_are_constant)","198","        self.n_bins_non_missing = n_bins_non_missing","200","        self.has_missing_values = has_missing_values","347","        if not self.has_missing_values[node.split_info.feature_idx]:","348","            # If no missing values are encountered at fit time, then samples","349","            # with missing values during predict() will go to whichever child","350","            # has the most samples.","351","            node.split_info.missing_go_to_left = (","352","                left_child_node.n_samples > right_child_node.n_samples)","353","","449","                                   bin_thresholds, self.n_bins_non_missing)","454","                               bin_thresholds, n_bins_non_missing,","455","                               next_free_idx=0):","476","        node['missing_go_to_left'] = split_info.missing_go_to_left","477","","478","        if split_info.bin_idx == n_bins_non_missing[feature_idx] - 1:","479","            # Split is on the last non-missing bin: it's a \"split on nans\". All","480","            # nans go to the right, the rest go to the left.","481","            node['threshold'] = np.inf","482","        elif bin_thresholds is not None:","483","            node['threshold'] = bin_thresholds[feature_idx][bin_idx]","484","","490","            bin_thresholds=bin_thresholds,","491","            n_bins_non_missing=n_bins_non_missing,","492","            next_free_idx=next_free_idx)","497","            bin_thresholds=bin_thresholds,","498","            n_bins_non_missing=n_bins_non_missing,","499","            next_free_idx=next_free_idx)"],"delete":["17","from .types import PREDICTOR_RECORD_DTYPE","18","from .types import Y_DTYPE","143","    max_bins : int, optional (default=256)","144","        The maximum number of bins. Used to define the shape of the","145","        histograms.","146","    actual_n_bins : ndarray of int or int, optional (default=None)","147","        The actual number of bins needed for each feature, which is lower or","148","        equal to ``max_bins``. If it's an int, all features are considered to","149","        have the same number of bins. If None, all features are considered to","150","        have ``max_bins`` bins.","163","                 max_bins=256, actual_n_bins=None, l2_regularization=0.,","164","                 min_hessian_to_split=1e-3, shrinkage=1.):","170","        if actual_n_bins is None:","171","            actual_n_bins = max_bins","173","        if isinstance(actual_n_bins, numbers.Integral):","174","            actual_n_bins = np.array(","175","                [actual_n_bins] * X_binned.shape[1],","178","            actual_n_bins = np.asarray(actual_n_bins, dtype=np.uint32)","182","            X_binned, max_bins, gradients, hessians, hessians_are_constant)","184","            X_binned, actual_n_bins, l2_regularization,","185","            min_hessian_to_split, min_samples_leaf, min_gain_to_split,","186","            hessians_are_constant)","188","        self.max_bins = max_bins","430","                                   bin_thresholds=bin_thresholds)","435","                               bin_thresholds, next_free_idx=0):","456","        if bin_thresholds is not None:","457","            threshold = bin_thresholds[feature_idx][bin_idx]","458","            node['threshold'] = threshold","464","            bin_thresholds=bin_thresholds, next_free_idx=next_free_idx)","469","            bin_thresholds=bin_thresholds, next_free_idx=next_free_idx)"]}],"sklearn\/ensemble\/_hist_gradient_boosting\/histogram.pyx":[{"add":["14","from .common import HISTOGRAM_DTYPE","15","from .common cimport hist_struct","16","from .common cimport X_BINNED_DTYPE_C","17","from .common cimport G_H_DTYPE_C","64","    n_bins : int","65","        The total number of bins, including the bin for missing values. Used","66","        to define the shape of the histograms.","79","        unsigned int n_bins","87","                 unsigned int n_bins, G_H_DTYPE_C [::1] gradients,","93","        # Note: all histograms will have <n_bins> bins, but some of the","94","        # bins may be unused if a feature has a small number of unique values.","95","        self.n_bins = n_bins","117","        histograms : ndarray of HISTOGRAM_DTYPE, shape (n_features, n_bins)","133","                shape=(self.n_features, self.n_bins),","212","                shape (n_features, n_bins)","215","                shape (n_features, n_bins)","220","        histograms : ndarray of HISTOGRAM_DTYPE, shape(n_features, n_bins)","228","                shape=(self.n_features, self.n_bins),","235","                                 self.n_bins,"],"delete":["14","from .types import HISTOGRAM_DTYPE","15","from .types cimport hist_struct","16","from .types cimport X_BINNED_DTYPE_C","17","from .types cimport G_H_DTYPE_C","64","    max_bins : int","65","        The maximum number of bins. Used to define the shape of the","66","        histograms.","79","        unsigned int max_bins","87","                 unsigned int max_bins, G_H_DTYPE_C [::1] gradients,","93","        # Note: all histograms will have <max_bins> bins, but some of the","94","        # last bins may be unused if actual_n_bins[f] < max_bins","95","        self.max_bins = max_bins","117","        histograms : ndarray of HISTOGRAM_DTYPE, shape (n_features, max_bins)","133","                shape=(self.n_features, self.max_bins),","212","                shape (n_features, max_bins)","215","                shape (n_features, max_bins)","220","        histograms : ndarray of HISTOGRAM_DTYPE, shape(n_features, max_bins)","228","                shape=(self.n_features, self.max_bins),","235","                                 self.max_bins,"]}],"sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_histogram.py":[{"add":["14","from sklearn.ensemble._hist_gradient_boosting.common import HISTOGRAM_DTYPE","15","from sklearn.ensemble._hist_gradient_boosting.common import G_H_DTYPE","16","from sklearn.ensemble._hist_gradient_boosting.common import X_BINNED_DTYPE"],"delete":["14","from sklearn.ensemble._hist_gradient_boosting.types import HISTOGRAM_DTYPE","15","from sklearn.ensemble._hist_gradient_boosting.types import G_H_DTYPE","16","from sklearn.ensemble._hist_gradient_boosting.types import X_BINNED_DTYPE"]}],"sklearn\/ensemble\/_hist_gradient_boosting\/gradient_boosting.py":[{"add":["4","from functools import partial","17","from .common import Y_DTYPE, X_DTYPE, X_BINNED_DTYPE","78","        if not (2 <= self.max_bins <= 255):","79","            raise ValueError('max_bins={} should be no smaller than 2 '","80","                             'and no larger than 255.'.format(self.max_bins))","81","","150","        has_missing_values = np.isnan(X_train).any(axis=0).astype(np.uint8)","151","","153","        # For ease of use of the API, the user-facing GBDT classes accept the","154","        # parameter max_bins, which doesn't take into account the bin for","155","        # missing values (which is always allocated). However, since max_bins","156","        # isn't the true maximal number of bins, all other private classes","157","        # (binmapper, histbuilder...) accept n_bins instead, which is the","158","        # actual total number of bins. Everywhere in the code, the","159","        # convention is that n_bins == max_bins + 1","160","        n_bins = self.max_bins + 1  # + 1 for missing values","161","        self.bin_mapper_ = _BinMapper(n_bins=n_bins, random_state=rng)","310","                    n_bins=n_bins,","311","                    n_bins_non_missing=self.bin_mapper_.n_bins_non_missing_,","312","                    has_missing_values=has_missing_values,","343","                                pred.predict_binned(","344","                                    X_binned_val,","345","                                    self.bin_mapper_.missing_values_bin_idx_","346","                                )","347","                            )","578","                if is_binned:","579","                    predict = partial(","580","                        predictor.predict_binned,","581","                        missing_values_bin_idx=self.bin_mapper_.missing_values_bin_idx_  # noqa","582","                    )","583","                else:","584","                    predict = predictor.predict","620","    def _more_tags(self):","621","        return {'allow_nan': True}","622","","644","    This estimator has native support for missing values (NaNs). During","645","    training, the tree grower learns at each split point whether samples","646","    with missing values should go to the left or right child, based on the","647","    potential gain. When predicting, samples with missing values are","648","    assigned to the left or right child consequently. If no missing values","649","    were encountered for a given feature during training, then samples with","650","    missing values are mapped to whichever child has the most samples.","651","","695","    max_bins : int, optional (default=255)","696","        The maximum number of bins to use for non-missing values. Before","697","        training, each feature of the input array `X` is binned into","698","        integer-valued bins, which allows for a much faster training stage.","699","        Features with a small number of unique values may use less than","700","        ``max_bins`` bins. In addition to the ``max_bins`` bins, one more bin","701","        is always reserved for missing values. Must be no larger than 255.","772","                 min_samples_leaf=20, l2_regularization=0., max_bins=255,","821","    This estimator has native support for missing values (NaNs). During","822","    training, the tree grower learns at each split point whether samples","823","    with missing values should go to the left or right child, based on the","824","    potential gain. When predicting, samples with missing values are","825","    assigned to the left or right child consequently. If no missing values","826","    were encountered for a given feature during training, then samples with","827","    missing values are mapped to whichever child has the most samples.","828","","875","    max_bins : int, optional (default=255)","876","        The maximum number of bins to use for non-missing values. Before","877","        training, each feature of the input array `X` is binned into","878","        integer-valued bins, which allows for a much faster training stage.","879","        Features with a small number of unique values may use less than","880","        ``max_bins`` bins. In addition to the ``max_bins`` bins, one more bin","881","        is always reserved for missing values. Must be no larger than 255.","954","                 l2_regularization=0., max_bins=255, warm_start=False,"],"delete":["16","from .types import Y_DTYPE, X_DTYPE, X_BINNED_DTYPE","146","        self.bin_mapper_ = _BinMapper(max_bins=self.max_bins, random_state=rng)","295","                    max_bins=self.max_bins,","296","                    actual_n_bins=self.bin_mapper_.actual_n_bins_,","327","                                pred.predict_binned(X_binned_val))","558","                predict = (predictor.predict_binned if is_binned","559","                           else predictor.predict)","608","    def _more_tags(self):","609","        # This is not strictly True, but it's needed since","610","        # force_all_finite=False means accept both nans and infinite values.","611","        # Without the tag, common checks would fail.","612","        # This comment must be removed once we merge PR 13911","613","        return {'allow_nan': True}","614","","666","    max_bins : int, optional (default=256)","667","        The maximum number of bins to use. Before training, each feature of","668","        the input array ``X`` is binned into at most ``max_bins`` bins, which","669","        allows for a much faster training stage. Features with a small","670","        number of unique values may use less than ``max_bins`` bins. Must be no","671","        larger than 256.","742","                 min_samples_leaf=20, l2_regularization=0., max_bins=256,","837","    max_bins : int, optional (default=256)","838","        The maximum number of bins to use. Before training, each feature of","839","        the input array ``X`` is binned into at most ``max_bins`` bins, which","840","        allows for a much faster training stage. Features with a small","841","        number of unique values may use less than ``max_bins`` bins. Must be no","842","        larger than 256.","915","                 l2_regularization=0., max_bins=256, warm_start=False,"]}],"sklearn\/ensemble\/_hist_gradient_boosting\/loss.py":[{"add":["17","from .common import Y_DTYPE","18","from .common import G_H_DTYPE"],"delete":["17","from .types import Y_DTYPE","18","from .types import G_H_DTYPE"]}],"sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_predictor.py":[{"add":["9","from sklearn.ensemble._hist_gradient_boosting.common import (","10","    G_H_DTYPE, PREDICTOR_RECORD_DTYPE, ALMOST_INF)","13","@pytest.mark.parametrize('n_bins', [200, 256])","14","def test_boston_dataset(n_bins):","19","    mapper = _BinMapper(n_bins=n_bins, random_state=42)","30","                        max_leaf_nodes=max_leaf_nodes, n_bins=n_bins,","31","                        n_bins_non_missing=mapper.n_bins_non_missing_)","44","    (ALMOST_INF, [0, 0, 0, 1]),","45","    (np.inf, [0, 0, 0, 0]),","49","    # In particular, if a value is +inf and the threshold is ALMOST_INF the","50","    # sample should go to the right child. If the threshold is inf (split on","51","    # nan), the +inf sample will go to the left child."],"delete":["9","from sklearn.ensemble._hist_gradient_boosting.types import (","10","    G_H_DTYPE, PREDICTOR_RECORD_DTYPE)","13","@pytest.mark.parametrize('max_bins', [200, 256])","14","def test_boston_dataset(max_bins):","19","    mapper = _BinMapper(max_bins=max_bins, random_state=42)","30","                        max_leaf_nodes=max_leaf_nodes, max_bins=max_bins,","31","                        actual_n_bins=mapper.actual_n_bins_)","44","    (np.inf, [0, 0, 0, 1]),","48","    # In paticular, if a value is +inf and the threhsold is +inf, the sample","49","    # should go to the right child."]}],"sklearn\/ensemble\/_hist_gradient_boosting\/_gradient_boosting.pyx":[{"add":["12","from .common import Y_DTYPE","13","from .common cimport Y_DTYPE_C"],"delete":["12","from .types import Y_DTYPE","13","from .types cimport Y_DTYPE_C"]}],"\/dev\/null":[{"add":[],"delete":[]}],"sklearn\/ensemble\/_hist_gradient_boosting\/_loss.pyx":[{"add":["14","from .common cimport Y_DTYPE_C","15","from .common cimport G_H_DTYPE_C"],"delete":["14","from .types cimport Y_DTYPE_C","15","from .types cimport G_H_DTYPE_C"]}],"benchmarks\/bench_hist_gradient_boosting.py":[{"add":["4","import numpy as np","28","parser.add_argument('--missing-fraction', type=float, default=0)","56","if args.missing_fraction:","57","    mask = np.random.binomial(1, args.missing_fraction, size=X.shape).astype(","58","        np.bool)","59","    X[mask] = np.nan","60",""],"delete":[]}],"sklearn\/ensemble\/_hist_gradient_boosting\/utils.pyx":[{"add":["11","from .common cimport G_H_DTYPE_C","12","from .common cimport Y_DTYPE_C"],"delete":["11","from .types cimport G_H_DTYPE_C","12","from .types cimport Y_DTYPE_C"]}],"sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_binning.py":[{"add":["9","from sklearn.ensemble._hist_gradient_boosting.common import X_DTYPE","10","from sklearn.ensemble._hist_gradient_boosting.common import X_BINNED_DTYPE","11","from sklearn.ensemble._hist_gradient_boosting.common import ALMOST_INF","19","def _find_binning_thresholds(data, max_bins=255, subsample=int(2e5),","55","    bin_thresholds = _find_binning_thresholds(DATA, max_bins=255,","56","                                              random_state=0)","59","        assert bin_thresholds[i].shape == (254,)  # 255 - 1","78","@pytest.mark.parametrize('n_bins', (2, 257))","79","def test_invalid_n_bins(n_bins):","80","    err_msg = (","81","        'n_bins={} should be no smaller than 3 and no larger than 256'","82","        .format(n_bins))","84","        _BinMapper(n_bins=n_bins).fit(DATA)","88","    mapper = _BinMapper(n_bins=42, random_state=42).fit(DATA)","94","@pytest.mark.parametrize('max_bins', [16, 128, 255])","95","def test_map_to_bins(max_bins):","96","    bin_thresholds = _find_binning_thresholds(DATA, max_bins=max_bins,","99","    last_bin_idx = max_bins","100","    _map_to_bins(DATA, bin_thresholds, last_bin_idx, binned)","111","        assert binned[max_idx, feature_idx] == max_bins - 1","114","@pytest.mark.parametrize(\"max_bins\", [5, 10, 42])","115","def test_bin_mapper_random_data(max_bins):","118","    expected_count_per_bin = n_samples \/\/ max_bins","121","    # max_bins is the number of bins for non-missing values","122","    n_bins = max_bins + 1","123","    mapper = _BinMapper(n_bins=n_bins, random_state=42).fit(DATA)","129","    assert_array_equal(binned.max(axis=0),","130","                       np.array([max_bins - 1, max_bins - 1]))","133","        assert bin_thresholds_feature.shape == (max_bins - 1,)","135","    assert np.all(mapper.n_bins_non_missing_ == max_bins)","139","        for bin_idx in range(max_bins):","144","@pytest.mark.parametrize(\"n_samples, max_bins\", [","150","def test_bin_mapper_small_random_data(n_samples, max_bins):","154","    # max_bins is the number of bins for non-missing values","155","    n_bins = max_bins + 1","156","    mapper = _BinMapper(n_bins=n_bins, random_state=42)","165","@pytest.mark.parametrize(\"max_bins, n_distinct, multiplier\", [","170","def test_bin_mapper_identity_repeated_values(max_bins, n_distinct, multiplier):","172","    # max_bins is the number of bins for non-missing values","173","    n_bins = max_bins + 1","174","    binned = _BinMapper(n_bins=n_bins).fit_transform(data)","191","    mapper_1 = _BinMapper(n_bins=n_distinct + 1)","196","    mapper_2 = _BinMapper(n_bins=min(256, n_distinct * 3) + 1)","203","@pytest.mark.parametrize(\"max_bins, scale, offset\", [","206","    (255, 0.3, 42),","208","def test_bin_mapper_identity_small(max_bins, scale, offset):","209","    data = np.arange(max_bins).reshape(-1, 1) * scale + offset","210","    # max_bins is the number of bins for non-missing values","211","    n_bins = max_bins + 1","212","    binned = _BinMapper(n_bins=n_bins).fit_transform(data)","213","    assert_array_equal(binned, np.arange(max_bins).reshape(-1, 1))","216","@pytest.mark.parametrize('max_bins_small, max_bins_large', [","221","    (255, 255),","223","    (42, 255),","225","def test_bin_mapper_idempotence(max_bins_small, max_bins_large):","226","    assert max_bins_large >= max_bins_small","228","    mapper_small = _BinMapper(n_bins=max_bins_small + 1)","229","    mapper_large = _BinMapper(n_bins=max_bins_small + 1)","235","@pytest.mark.parametrize('n_bins', [10, 100, 256])","237","def test_n_bins_non_missing(n_bins, diff):","238","    # Check that n_bins_non_missing is n_unique_values when","239","    # there are not a lot of unique values, else n_bins - 1.","241","    n_unique_values = n_bins + diff","244","    mapper = _BinMapper(n_bins=n_bins).fit(X)","245","    assert np.all(mapper.n_bins_non_missing_ == min(","246","        n_bins - 1, n_unique_values))","260","@pytest.mark.parametrize(","261","    'n_bins, n_bins_non_missing, X_trans_expected', [","262","        (256, [4, 2, 2], [[0,   0,   0],  # 255 <=> missing value","263","                          [255, 255, 0],","264","                          [1,   0,   0],","265","                          [255, 1,   1],","266","                          [2,   1,   1],","267","                          [3,   0,   0]]),","268","        (3, [2, 2, 2], [[0, 0, 0],  # 2 <=> missing value","269","                        [2, 2, 0],","270","                        [0, 0, 0],","271","                        [2, 1, 1],","272","                        [1, 1, 1],","273","                        [1, 0, 0]])])","274","def test_missing_values_support(n_bins, n_bins_non_missing, X_trans_expected):","275","    # check for missing values: make sure nans are mapped to the last bin","276","    # and that the _BinMapper attributes are correct","277","","278","    X = [[1,      1,      0],","279","         [np.NaN, np.NaN, 0],","280","         [2,      1,      0],","281","         [np.NaN, 2,      1],","282","         [3,      2,      1],","283","         [4,      1,      0]]","284","","285","    X = np.array(X)","286","","287","    mapper = _BinMapper(n_bins=n_bins)","288","    mapper.fit(X)","289","","290","    assert_array_equal(mapper.n_bins_non_missing_, n_bins_non_missing)","291","","292","    for feature_idx in range(X.shape[1]):","293","        assert len(mapper.bin_thresholds_[feature_idx]) == \\","294","            n_bins_non_missing[feature_idx] - 1","295","","296","    assert mapper.missing_values_bin_idx_ == n_bins - 1","297","","298","    X_trans = mapper.transform(X)","299","    assert_array_equal(X_trans, X_trans_expected)","300","","301","","309","    assert_allclose(bin_mapper.bin_thresholds_[0], [-np.inf, .5, ALMOST_INF])","310","    assert bin_mapper.n_bins_non_missing_ == [4]"],"delete":["9","from sklearn.ensemble._hist_gradient_boosting.types import X_DTYPE","10","from sklearn.ensemble._hist_gradient_boosting.types import X_BINNED_DTYPE","18","def _find_binning_thresholds(data, max_bins=256, subsample=int(2e5),","54","    bin_thresholds = _find_binning_thresholds(DATA, random_state=0)","57","        assert bin_thresholds[i].shape == (255,)  # 256 - 1","76","def test_find_binning_thresholds_invalid_n_bins():","77","    err_msg = 'no smaller than 2 and no larger than 256'","79","        _find_binning_thresholds(DATA, max_bins=1024)","83","    mapper = _BinMapper(max_bins=42, random_state=42).fit(DATA)","89","@pytest.mark.parametrize('n_bins', [16, 128, 256])","90","def test_map_to_bins(n_bins):","91","    bin_thresholds = _find_binning_thresholds(DATA, max_bins=n_bins,","94","    _map_to_bins(DATA, bin_thresholds, binned)","105","        assert binned[max_idx, feature_idx] == n_bins - 1","108","@pytest.mark.parametrize(\"n_bins\", [5, 10, 42])","109","def test_bin_mapper_random_data(n_bins):","112","    expected_count_per_bin = n_samples \/\/ n_bins","115","    mapper = _BinMapper(max_bins=n_bins, random_state=42).fit(DATA)","121","    assert_array_equal(binned.max(axis=0), np.array([n_bins - 1, n_bins - 1]))","124","        assert bin_thresholds_feature.shape == (n_bins - 1,)","126","    assert np.all(mapper.actual_n_bins_ == n_bins)","130","        for bin_idx in range(n_bins):","135","@pytest.mark.parametrize(\"n_samples, n_bins\", [","141","def test_bin_mapper_small_random_data(n_samples, n_bins):","145","    mapper = _BinMapper(max_bins=n_bins, random_state=42)","154","@pytest.mark.parametrize(\"n_bins, n_distinct, multiplier\", [","159","def test_bin_mapper_identity_repeated_values(n_bins, n_distinct, multiplier):","161","    binned = _BinMapper(max_bins=n_bins).fit_transform(data)","178","    mapper_1 = _BinMapper(max_bins=n_distinct)","183","    mapper_2 = _BinMapper(max_bins=min(256, n_distinct * 3))","190","@pytest.mark.parametrize(\"n_bins, scale, offset\", [","193","    (256, 0.3, 42),","195","def test_bin_mapper_identity_small(n_bins, scale, offset):","196","    data = np.arange(n_bins).reshape(-1, 1) * scale + offset","197","    binned = _BinMapper(max_bins=n_bins).fit_transform(data)","198","    assert_array_equal(binned, np.arange(n_bins).reshape(-1, 1))","201","@pytest.mark.parametrize('n_bins_small, n_bins_large', [","206","    (256, 256),","208","    (42, 256),","210","def test_bin_mapper_idempotence(n_bins_small, n_bins_large):","211","    assert n_bins_large >= n_bins_small","213","    mapper_small = _BinMapper(max_bins=n_bins_small)","214","    mapper_large = _BinMapper(max_bins=n_bins_large)","220","@pytest.mark.parametrize('max_bins', [10, 100, 256])","222","def test_actual_n_bins(max_bins, diff):","223","    # Check that actual_n_bins is n_unique_values when","224","    # n_unique_values <= max_bins, else max_bins.","226","    n_unique_values = max_bins + diff","229","    mapper = _BinMapper(max_bins=max_bins).fit(X)","230","    assert np.all(mapper.actual_n_bins_ == min(max_bins, n_unique_values))","251","    assert_allclose(bin_mapper.bin_thresholds_[0], [-np.inf, .5, np.inf])","252","    assert bin_mapper.actual_n_bins_ == [4]"]}],"sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_splitting.py":[{"add":["3","from sklearn.ensemble._hist_gradient_boosting.common import HISTOGRAM_DTYPE","4","from sklearn.ensemble._hist_gradient_boosting.common import G_H_DTYPE","5","from sklearn.ensemble._hist_gradient_boosting.common import X_BINNED_DTYPE","20","        rng.randint(0, n_bins - 1, size=(int(1e4), 1)), dtype=X_BINNED_DTYPE)","28","    for true_bin in range(1, n_bins - 2):","41","            n_bins_non_missing = np.array([n_bins - 1] * X_binned.shape[1],","42","                                          dtype=np.uint32)","43","            has_missing_values = np.array([False] * X_binned.shape[1],","44","                                          dtype=np.uint8)","45","            missing_values_bin_idx = n_bins - 1","47","                                n_bins_non_missing,","48","                                missing_values_bin_idx,","49","                                has_missing_values,","105","    n_bins_non_missing = np.array([n_bins - 1] * X_binned.shape[1],","106","                                  dtype=np.uint32)","107","    has_missing_values = np.array([False] * X_binned.shape[1], dtype=np.uint8)","108","    missing_values_bin_idx = n_bins - 1","109","    splitter = Splitter(X_binned, n_bins_non_missing, missing_values_bin_idx,","110","                        has_missing_values, l2_regularization,","111","                        min_hessian_to_split, min_samples_leaf,","112","                        min_gain_to_split, constant_hessian)","205","    n_bins_non_missing = np.array([n_bins] * X_binned.shape[1],","206","                                  dtype=np.uint32)","207","    has_missing_values = np.array([False] * X_binned.shape[1], dtype=np.uint8)","208","    missing_values_bin_idx = n_bins - 1","209","    splitter = Splitter(X_binned, n_bins_non_missing, missing_values_bin_idx,","210","                        has_missing_values, l2_regularization,","211","                        min_hessian_to_split, min_samples_leaf,","212","                        min_gain_to_split, hessians_are_constant)","262","    n_bins_non_missing = np.array([n_bins - 1] * X_binned.shape[1],","263","                                  dtype=np.uint32)","264","    has_missing_values = np.array([False] * X_binned.shape[1], dtype=np.uint8)","265","    missing_values_bin_idx = n_bins - 1","266","    splitter = Splitter(X_binned, n_bins_non_missing, missing_values_bin_idx,","267","                        has_missing_values, l2_regularization,","268","                        min_hessian_to_split, min_samples_leaf,","269","                        min_gain_to_split, hessians_are_constant)","270","","271","    histograms = builder.compute_histograms_brute(sample_indices)","272","    split_info = splitter.find_node_split(n_samples, histograms,","273","                                          sum_gradients, sum_hessians)","274","    assert split_info.gain == -1","275","","276","","277","@pytest.mark.parametrize(","278","    'X_binned, all_gradients, has_missing_values, n_bins_non_missing, '","279","    ' expected_split_on_nan, expected_bin_idx, expected_go_to_left', [","280","","281","        # basic sanity check with no missing values: given the gradient","282","        # values, the split must occur on bin_idx=3","283","        ([0, 1, 2, 3, 4, 5, 6, 7, 8, 9],  # X_binned","284","         [1, 1, 1, 1, 5, 5, 5, 5, 5, 5],  # gradients","285","         False,  # no missing values","286","         10,  # n_bins_non_missing","287","         False,  # don't split on nans","288","         3,  # expected_bin_idx","289","         'not_applicable'),","290","","291","        # We replace 2 samples by NaNs (bin_idx=8)","292","        # These 2 samples were mapped to the left node before, so they should","293","        # be mapped to left node again","294","        # Notice how the bin_idx threshold changes from 3 to 1.","295","        ([8, 0, 1, 8, 2, 3, 4, 5, 6, 7],  # 8 <=> missing","296","         [1, 1, 1, 1, 5, 5, 5, 5, 5, 5],","297","         True,  # missing values","298","         8,  # n_bins_non_missing","299","         False,  # don't split on nans","300","         1,  # cut on bin_idx=1","301","         True),  # missing values go to left","302","","303","        # same as above, but with non-consecutive missing_values_bin","304","        ([9, 0, 1, 9, 2, 3, 4, 5, 6, 7],  # 9 <=> missing","305","         [1, 1, 1, 1, 5, 5, 5, 5, 5, 5],","306","         True,  # missing values","307","         8,  # n_bins_non_missing","308","         False,  # don't split on nans","309","         1,  # cut on bin_idx=1","310","         True),  # missing values go to left","311","","312","        # this time replacing 2 samples that were on the right.","313","        ([0, 1, 2, 3, 8, 4, 8, 5, 6, 7],  # 8 <=> missing","314","         [1, 1, 1, 1, 5, 5, 5, 5, 5, 5],","315","         True,  # missing values","316","         8,  # n_bins_non_missing","317","         False,  # don't split on nans","318","         3,  # cut on bin_idx=3 (like in first case)","319","         False),  # missing values go to right","320","","321","        # same as above, but with non-consecutive missing_values_bin","322","        ([0, 1, 2, 3, 9, 4, 9, 5, 6, 7],  # 9 <=> missing","323","         [1, 1, 1, 1, 5, 5, 5, 5, 5, 5],","324","         True,  # missing values","325","         8,  # n_bins_non_missing","326","         False,  # don't split on nans","327","         3,  # cut on bin_idx=3 (like in first case)","328","         False),  # missing values go to right","329","","330","        # For the following cases, split_on_nans is True (we replace all of","331","        # the samples with nans, instead of just 2).","332","        ([0, 1, 2, 3, 4, 4, 4, 4, 4, 4],  # 4 <=> missing","333","         [1, 1, 1, 1, 5, 5, 5, 5, 5, 5],","334","         True,  # missing values","335","         4,  # n_bins_non_missing","336","         True,  # split on nans","337","         3,  # cut on bin_idx=3","338","         False),  # missing values go to right","339","","340","        # same as above, but with non-consecutive missing_values_bin","341","        ([0, 1, 2, 3, 9, 9, 9, 9, 9, 9],  # 9 <=> missing","342","         [1, 1, 1, 1, 1, 1, 5, 5, 5, 5],","343","         True,  # missing values","344","         4,  # n_bins_non_missing","345","         True,  # split on nans","346","         3,  # cut on bin_idx=3","347","         False),  # missing values go to right","348","","349","        ([6, 6, 6, 6, 0, 1, 2, 3, 4, 5],  # 4 <=> missing","350","         [1, 1, 1, 1, 5, 5, 5, 5, 5, 5],","351","         True,  # missing values","352","         6,  # n_bins_non_missing","353","         True,  # split on nans","354","         5,  # cut on bin_idx=5","355","         False),  # missing values go to right","356","","357","        # same as above, but with non-consecutive missing_values_bin","358","        ([9, 9, 9, 9, 0, 1, 2, 3, 4, 5],  # 9 <=> missing","359","         [1, 1, 1, 1, 5, 5, 5, 5, 5, 5],","360","         True,  # missing values","361","         6,  # n_bins_non_missing","362","         True,  # split on nans","363","         5,  # cut on bin_idx=5","364","         False),  # missing values go to right","365","    ]","366",")","367","def test_splitting_missing_values(X_binned, all_gradients,","368","                                  has_missing_values, n_bins_non_missing,","369","                                  expected_split_on_nan, expected_bin_idx,","370","                                  expected_go_to_left):","371","    # Make sure missing values are properly supported.","372","    # we build an artificial example with gradients such that the best split","373","    # is on bin_idx=3, when there are no missing values.","374","    # Then we introduce missing values and:","375","    #   - make sure the chosen bin is correct (find_best_bin()): it's","376","    #     still the same split, even though the index of the bin may change","377","    #   - make sure the missing values are mapped to the correct child","378","    #     (split_indices())","379","","380","    n_bins = max(X_binned) + 1","381","    n_samples = len(X_binned)","382","    l2_regularization = 0.","383","    min_hessian_to_split = 1e-3","384","    min_samples_leaf = 1","385","    min_gain_to_split = 0.","386","","387","    sample_indices = np.arange(n_samples, dtype=np.uint32)","388","    X_binned = np.array(X_binned, dtype=X_BINNED_DTYPE).reshape(-1, 1)","389","    X_binned = np.asfortranarray(X_binned)","390","    all_gradients = np.array(all_gradients, dtype=G_H_DTYPE)","391","    has_missing_values = np.array([has_missing_values], dtype=np.uint8)","392","    all_hessians = np.ones(1, dtype=G_H_DTYPE)","393","    sum_gradients = all_gradients.sum()","394","    sum_hessians = 1 * n_samples","395","    hessians_are_constant = True","396","","397","    builder = HistogramBuilder(X_binned, n_bins,","398","                               all_gradients, all_hessians,","399","                               hessians_are_constant)","400","","401","    n_bins_non_missing = np.array([n_bins_non_missing], dtype=np.uint32)","402","    missing_values_bin_idx = n_bins - 1","403","    splitter = Splitter(X_binned, n_bins_non_missing,","404","                        missing_values_bin_idx, has_missing_values,","412","","413","    assert split_info.bin_idx == expected_bin_idx","414","    if has_missing_values:","415","        assert split_info.missing_go_to_left == expected_go_to_left","416","","417","    split_on_nan = split_info.bin_idx == n_bins_non_missing[0] - 1","418","    assert split_on_nan == expected_split_on_nan","419","","420","    # Make sure the split is properly computed.","421","    # This also make sure missing values are properly assigned to the correct","422","    # child in split_indices()","423","    samples_left, samples_right, _ = splitter.split_indices(","424","        split_info, splitter.partition)","425","","426","    if not expected_split_on_nan:","427","        # When we don't split on nans, the split should always be the same.","428","        assert set(samples_left) == set([0, 1, 2, 3])","429","        assert set(samples_right) == set([4, 5, 6, 7, 8, 9])","430","    else:","431","        # When we split on nans, samples with missing values are always mapped","432","        # to the right child.","433","        missing_samples_indices = np.flatnonzero(","434","            np.array(X_binned) == missing_values_bin_idx)","435","        non_missing_samples_indices = np.flatnonzero(","436","            np.array(X_binned) != missing_values_bin_idx)","437","","438","        assert set(samples_right) == set(missing_samples_indices)","439","        assert set(samples_left) == set(non_missing_samples_indices)"],"delete":["3","from sklearn.ensemble._hist_gradient_boosting.types import HISTOGRAM_DTYPE","4","from sklearn.ensemble._hist_gradient_boosting.types import G_H_DTYPE","5","from sklearn.ensemble._hist_gradient_boosting.types import X_BINNED_DTYPE","20","        rng.randint(0, n_bins, size=(int(1e4), 1)), dtype=X_BINNED_DTYPE)","28","    for true_bin in range(1, n_bins - 1):","36","            actual_n_bins = np.array([n_bins] * X_binned.shape[1],","37","                                     dtype=np.uint32)","44","                                actual_n_bins,","98","    actual_n_bins = np.array([n_bins] * X_binned.shape[1],","99","                             dtype=np.uint32)","102","    splitter = Splitter(X_binned, actual_n_bins,","103","                        l2_regularization, min_hessian_to_split,","104","                        min_samples_leaf, min_gain_to_split, constant_hessian)","194","    actual_n_bins = np.array([n_bins] * X_binned.shape[1],","195","                             dtype=np.uint32)","199","    splitter = Splitter(X_binned, actual_n_bins,","200","                        l2_regularization, min_hessian_to_split,","201","                        min_samples_leaf, min_gain_to_split,","202","                        hessians_are_constant)","250","    actual_n_bins = np.array([n_bins] * X_binned.shape[1],","251","                             dtype=np.uint32)","254","    splitter = Splitter(X_binned, actual_n_bins,","262","    assert split_info.gain == -1"]}],"doc\/whats_new\/v0.22.rst":[{"add":["27","- :class:`ensemble.HistGradientBoostingClassifier` and","28","  :class:`ensemble.HistGradientBoostingRegressor` |Fix|, |Feature|,","29","  |Enhancement|.","115","- Many improvements were made to","116","  :class:`ensemble.HistGradientBoostingClassifier` and","117","  :class:`ensemble.HistGradientBoostingRegressor`:","119","  - |MajorFeature| Estimators now natively support dense data with missing","120","    values both for training and predicting. They also support infinite","121","    values. :pr:`13911` and :pr:`14406` by `Nicolas Hug`_, `Adrin Jalali`_","122","    and `Olivier Grisel`_.","123","  - |Feature| Estimators now have an additional `warm_start` parameter that","124","    enables warm starting. :pr:`14012` by :user:`Johann Faouzi <johannfaouzi>`.","125","  - |Enhancement| for :class:`ensemble.HistGradientBoostingClassifier` the","126","    training loss or score is now monitored on a class-wise stratified","127","    subsample to preserve the class balance of the original training set.","128","    :pr:`14194` by :user:`Johann Faouzi <johannfaouzi>`.","129","  - |Feature| :func:`inspection.partial_dependence` and","130","    :func:`inspection.plot_partial_dependence` now support the fast 'recursion'","131","    method for both estimators. :pr:`13769` by `Nicolas Hug`_.","132","  - |Fix| Estimators now bin the training and validation data separately to","133","    avoid any data leak. :pr:`13933` by `Nicolas Hug`_.","134","","135","  Note that pickles from 0.21 will not work in 0.22.","192","- |Feature| :func:`inspection.partial_dependence` and","193","  :func:`inspection.plot_partial_dependence` now support the fast 'recursion'","194","  method for :class:`ensemble.HistGradientBoostingClassifier` and","195","  :class:`ensemble.HistGradientBoostingRegressor`. :pr:`13769` by","196","  `Nicolas Hug`_.","197",""],"delete":["25","","27","","114","- |Feature| :class:`ensemble.HistGradientBoostingClassifier` and","115","  :class:`ensemble.HistGradientBoostingRegressor` have an additional","116","  parameter called `warm_start` that enables warm starting. :pr:`14012` by","117","  :user:`Johann Faouzi <johannfaouzi>`.","119","- |Fix| :class:`ensemble.HistGradientBoostingClassifier` and","120","  :class:`ensemble.HistGradientBoostingRegressor` now bin the training and","121","  validation data separately to avoid any data leak. :pr:`13933` by","122","  `Nicolas Hug`_.","127","- |Enhancement| :class:`ensemble.HistGradientBoostingClassifier` the training","128","  loss or score is now monitored on a class-wise stratified subsample to","129","  preserve the class balance of the original training set. :pr:`14194`","130","  by :user:`Johann Faouzi <johannfaouzi>`.","131",""]}]}},"c41876126582577461cead2c9ce8b0335f401d81":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/cross_decomposition\/pls_.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["14",":mod:`sklearn.decomposition`","15","............................","16","","17","- |Fix| Fixed a bug in :class:`cross_decomposition.CCA` improving numerical ","18","  stability when `Y` is close to zero. :pr:`13903` by `Thomas Fan`_.","19",""],"delete":[]}],"sklearn\/cross_decomposition\/pls_.py":[{"add":["287","        Y_eps = np.finfo(Yk.dtype).eps","296","                # Replace columns that are all close to zero with zeros","297","                Yk_mask = np.all(np.abs(Yk) < 10 * Y_eps, axis=0)","298","                Yk[:, Yk_mask] = 0.0","299",""],"delete":[]}]}},"ac79dff3f50b03e2dfa58ecb59e5a5e2887ce9b2":{"changes":{"sklearn\/cluster\/tests\/test_affinity_propagation.py":"MODIFY","sklearn\/cluster\/affinity_propagation_.py":"MODIFY"},"diff":{"sklearn\/cluster\/tests\/test_affinity_propagation.py":[{"add":["6","import pytest","7","from scipy.sparse import csr_matrix","166","","167","","168","@pytest.mark.parametrize('centers', [csr_matrix(np.zeros((1, 10))),","169","                                     np.zeros((1, 10))])","170","def test_affinity_propagation_convergence_warning_dense_sparse(centers):","171","    \"\"\"Non-regression, see #13334\"\"\"","172","    rng = np.random.RandomState(42)","173","    X = rng.rand(40, 10)","174","    y = (4 * rng.rand(40)).astype(np.int)","175","    ap = AffinityPropagation()","176","    ap.fit(X, y)","177","    ap.cluster_centers_ = centers","178","    with pytest.warns(None) as record:","179","        assert_array_equal(ap.predict(X),","180","                           np.zeros(X.shape[0], dtype=int))","181","    assert len(record) == 0"],"delete":[]}],"sklearn\/cluster\/affinity_propagation_.py":[{"add":["405","        if self.cluster_centers_.shape[0] > 0:"],"delete":["405","        if self.cluster_centers_.size > 0:"]}]}},"9328581ec5213e743fca116db06e7beef9a9da00":{"changes":{"examples\/decomposition\/plot_faces_decomposition.py":"MODIFY"},"diff":{"examples\/decomposition\/plot_faces_decomposition.py":[{"add":["156","                                                  fit_algorithm='cd',","163","                                                  fit_algorithm='cd',"],"delete":["38",""]}]}},"5d2e8979c133a240c7bbd2020a1557ffa11109f2":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["31","  :class:`sklearn.feature_extraction.text.CountVectorizer` |Fix|","273","- |Fix| |API| If ``input='file'`` or ``input='filename'``, and a callable is given","402",".......................","528","..........................","664","...................."],"delete":["31","  :class:`sklearn.feature_extraction.text.CountVectorizer` |API|","273","- |API| If ``input='file'`` or ``input='filename'``, and a callable is given","402","............................","528","........................","664","..................."]}]}},"7f50e8266360b8ab6021db7253a0ae5e0acf1b18":{"changes":{"sklearn\/impute\/tests\/test_impute.py":"MODIFY","doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/impute\/_iterative.py":"MODIFY","sklearn\/impute\/_base.py":"MODIFY"},"diff":{"sklearn\/impute\/tests\/test_impute.py":[{"add":["447","@pytest.mark.parametrize('Imputer', (SimpleImputer, IterativeImputer))","448","def test_imputation_missing_value_in_test_array(Imputer):","449","    # [Non Regression Test for issue #13968] Missing value in test set should","450","    # not throw an error and return a finite dataset","451","    train = [[1], [2]]","452","    test = [[3], [np.nan]]","453","    imputer = Imputer(add_indicator=True)","454","    imputer.fit(train).transform(test)","455","","456",""],"delete":[]}],"doc\/whats_new\/v0.21.rst":[{"add":["14",":mod:`sklearn.impute`","15",".....................","16","","17","- |Fix| Fixed a bug in :class:`SimpleImputer` and :class:`IterativeImputer`","18","  so that no errors are thrown when there are missing values in training data.","19","  :pr:`13974` by `Frank Hoang <fhoang7>`.","20",""],"delete":[]}],"sklearn\/impute\/_iterative.py":[{"add":["522","                missing_values=self.missing_values, error_on_new=False)"],"delete":["522","                missing_values=self.missing_values)"]}],"sklearn\/impute\/_base.py":[{"add":["271","                missing_values=self.missing_values, error_on_new=False)"],"delete":["271","                missing_values=self.missing_values)"]}]}},"404387c067934ac46bc0901e9a15ae14974541c5":{"changes":{"doc\/governance.rst":"MODIFY","doc\/modules\/clustering.rst":"MODIFY","doc\/whats_new\/v0.19.rst":"MODIFY","doc\/about.rst":"MODIFY","doc\/glossary.rst":"MODIFY","doc\/modules\/computing.rst":"MODIFY"},"diff":{"doc\/governance.rst":[{"add":["27","\u2013 not only code \u2013 as detailed in the :ref:`contributors guide <contributing>`."],"delete":["27","\u2013 not only code \u2013 as detailed in the `contributors guide <contributing>`_."]}],"doc\/modules\/clustering.rst":[{"add":["120","The :class:`KMeans` algorithm clusters data by trying to separate samples in n","121","groups of equal variance, minimizing a criterion known as the *inertia* or","122","within-cluster sum-of-squares (see below). This algorithm requires the number","123","of clusters to be specified. It scales well to large number of samples and has","124","been used across a large range of application areas in many different fields.","126","The k-means algorithm divides a set of :math:`N` samples :math:`X` into","127",":math:`K` disjoint clusters :math:`C`, each described by the mean :math:`\\mu_j`","128","of the samples in the cluster. The means are commonly called the cluster","129","\"centroids\"; note that they are not, in general, points from :math:`X`,","131","","132","The K-means algorithm aims to choose centroids that minimise the **inertia**,","133","or **within-cluster sum-of-squares criterion**:","137","Inertia can be recognized as a measure of how internally coherent clusters are.","148","  Running a dimensionality reduction algorithm such as :ref:`PCA` prior to","149","  k-means clustering can alleviate this problem and speed up the","150","  computations."],"delete":["120","The :class:`KMeans` algorithm clusters data by trying to separate samples","121","in n groups of equal variance, minimizing a criterion known as the","122","`inertia <inertia>`_ or within-cluster sum-of-squares.","123","This algorithm requires the number of clusters to be specified.","124","It scales well to large number of samples and has been used","125","across a large range of application areas in many different fields.","127","The k-means algorithm divides a set of :math:`N` samples :math:`X`","128","into :math:`K` disjoint clusters :math:`C`,","129","each described by the mean :math:`\\mu_j` of the samples in the cluster.","130","The means are commonly called the cluster \"centroids\";","131","note that they are not, in general, points from :math:`X`,","133","The K-means algorithm aims to choose centroids","134","that minimise the *inertia*, or within-cluster sum of squared criterion:","138","Inertia, or the within-cluster sum of squares criterion,","139","can be recognized as a measure of how internally coherent clusters are.","150","  Running a dimensionality reduction algorithm such as `PCA <PCA>`_","151","  prior to k-means clustering can alleviate this problem","152","  and speed up the computations."]}],"doc\/whats_new\/v0.19.rst":[{"add":["229","  classification. By :user:`Adam Kleczewski <adamklec>`."],"delete":["229","  classification. By `Adam Kleczewski <adamklec>`_."]}],"doc\/about.rst":[{"add":["58","out in the :ref:`governance document <governance>`."],"delete":["58","out in the `governance document <governance>`_."]}],"doc\/glossary.rst":[{"add":["172","        `categorical-encoding","173","        <https:\/\/contrib.scikit-learn.org\/categorical-encoding>`_","174","        package for tools related to encoding categorical features."],"delete":["172","        `https:\/\/contrib.scikit-learn.org\/categorical-encoding\/","173","        <category_encoders>`_ package for tools related to encoding","174","        categorical features."]}],"doc\/modules\/computing.rst":[{"add":["502","  - :ref:`scikit-learn developer performance documentation <performance-howto>`"],"delete":["502","  - `scikit-learn developer performance documentation <..\/developers\/performance.html>`_"]}]}},"9faa41429c708b5cbd5801cf9cdfe7349481e1dd":{"changes":{"sklearn\/ensemble\/voting_classifier.py":"MODIFY"},"diff":{"sklearn\/ensemble\/voting_classifier.py":[{"add":["196","            delayed(_parallel_fit_estimator)(clone(clf), X, transformed_y,","197","                                             sample_weight=sample_weight)","198","            for clf in clfs if clf is not None)","219","            The input samples.","263","            The input samples."],"delete":["196","                delayed(_parallel_fit_estimator)(clone(clf), X, transformed_y,","197","                                                 sample_weight=sample_weight)","198","                for clf in clfs if clf is not None)","219","            Training vectors, where n_samples is the number of samples and","220","            n_features is the number of features.","264","            Training vectors, where n_samples is the number of samples and","265","            n_features is the number of features."]}]}},"372092cfac5186496490772caac626ed222b06c3":{"changes":{"doc\/modules\/clustering.rst":"MODIFY","sklearn\/manifold\/tests\/test_spectral_embedding.py":"MODIFY","sklearn\/manifold\/spectral_embedding_.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY"},"diff":{"doc\/modules\/clustering.rst":[{"add":["436",":class:`SpectralClustering` performs a low-dimension embedding of the","437","affinity matrix between samples, followed by clustering, e.g., by KMeans,","438","of the components of the eigenvectors in the low dimensional space.","439","It is especially computationally efficient if the affinity matrix is sparse","440","and the `amg` solver is used for the eigenvalue problem (Note, the `amg` solver","441","requires that the `pyamg <https:\/\/github.com\/pyamg\/pyamg>`_ module is installed.)","443","The present version of SpectralClustering requires the number of clusters","444","to be specified in advance. It works well for a small number of clusters,","445","but is not advised for many clusters.","446","","447","For two clusters, SpectralClustering solves a convex relaxation of the","448","`normalised cuts <https:\/\/people.eecs.berkeley.edu\/~malik\/papers\/SM-ncut.pdf>`_","449","problem on the similarity graph: cutting the graph in two so that the weight of","450","the edges cut is small compared to the weights of the edges inside each","451","cluster. This criteria is especially interesting when working on images, where","452","graph vertices are pixels, and weights of the edges of the similarity graph are","453","computed using a function of a gradient of the image.","500","``\"kmeans\"`` strategy can match finer details, but can be unstable.","501","In particular, unless you control the ``random_state``, it may not be","502","reproducible from run-to-run, as it depends on random initialization.","503","The alternative ``\"discretize\"`` strategy is 100% reproducible, but tends","504","to create parcels of fairly even and geometrical shape.","515","Spectral Clustering can also be used to partition graphs via their spectral","542"," * `\"Preconditioned Spectral Clustering for Stochastic","543","   Block Partition Streaming Graph Challenge\"","544","   <https:\/\/arxiv.org\/abs\/1708.07481>`_","545","   David Zhuzhunashvili, Andrew Knyazev"],"delete":["436",":class:`SpectralClustering` does a low-dimension embedding of the","437","affinity matrix between samples, followed by a KMeans in the low","438","dimensional space. It is especially efficient if the affinity matrix is","439","sparse and the `pyamg <https:\/\/github.com\/pyamg\/pyamg>`_ module is installed.","440","SpectralClustering requires the number of clusters to be specified. It","441","works well for a small number of clusters but is not advised when using","442","many clusters.","444","For two clusters, it solves a convex relaxation of the `normalised","445","cuts <https:\/\/people.eecs.berkeley.edu\/~malik\/papers\/SM-ncut.pdf>`_ problem on","446","the similarity graph: cutting the graph in two so that the weight of the","447","edges cut is small compared to the weights of the edges inside each","448","cluster. This criteria is especially interesting when working on images:","449","graph vertices are pixels, and edges of the similarity graph are a","450","function of the gradient of the image.","497","The ``\"kmeans\"`` strategy can match finer details of the data, but it can be","498","more unstable. In particular, unless you control the ``random_state``, it","499","may not be reproducible from run-to-run, as it depends on a random","500","initialization. On the other hand, the ``\"discretize\"`` strategy is 100%","501","reproducible, but it tends to create parcels of fairly even and","502","geometrical shape.","513","Spectral Clustering can also be used to cluster graphs by their spectral"]}],"sklearn\/manifold\/tests\/test_spectral_embedding.py":[{"add":["164","    pytest.importorskip('pyamg')","192","def test_spectral_embedding_amg_solver_failure(seed=36):","193","    # Test spectral embedding with amg solver failure, see issue #13393","194","    pytest.importorskip('pyamg')","195","","196","    # The generated graph below is NOT fully connected if n_neighbors=3","197","    n_samples = 200","198","    n_clusters = 3","199","    n_features = 3","200","    centers = np.eye(n_clusters, n_features)","201","    S, true_labels = make_blobs(n_samples=n_samples, centers=centers,","202","                                cluster_std=1., random_state=42)","203","","204","    se_amg0 = SpectralEmbedding(n_components=3, affinity=\"nearest_neighbors\",","205","                                eigen_solver=\"amg\", n_neighbors=3,","206","                                random_state=np.random.RandomState(seed))","207","    embed_amg0 = se_amg0.fit_transform(S)","208","","209","    for i in range(10):","210","        se_amg0.set_params(random_state=np.random.RandomState(seed + 1))","211","        embed_amg1 = se_amg0.fit_transform(S)","212","","213","        assert _check_with_col_sign_flipping(embed_amg0, embed_amg1, 0.05)","214","","215","","216","@pytest.mark.filterwarnings(\"ignore:the behavior of nmi will \"","217","                            \"change in version 0.22\")"],"delete":["164","    try:","165","        from pyamg import smoothed_aggregation_solver  # noqa","166","    except ImportError:","167","        raise SkipTest(\"pyamg not available.\")"]}],"sklearn\/manifold\/spectral_embedding_.py":[{"add":["291","","292","        # The Laplacian matrix is always singular, having at least one zero","293","        # eigenvalue, corresponding to the trivial eigenvector, which is a","294","        # constant. Using a singular matrix for preconditioning may result in","295","        # random failures in LOBPCG and is not supported by the existing","296","        # theory:","297","        #     see https:\/\/doi.org\/10.1007\/s10208-015-9297-1","298","        # Shift the Laplacian so its diagononal is not all ones. The shift","299","        # does change the eigenpairs however, so we'll feed the shifted","300","        # matrix to the solver and afterward set it back to the original.","301","        diag_shift = 1e-5 * sparse.eye(laplacian.shape[0])","302","        laplacian += diag_shift","304","        laplacian -= diag_shift","305","","309","        _, diffusion_map = lobpcg(laplacian, X, M=M, tol=1.e-5,","391","        to be installed. It can be faster on very large, sparse problems."],"delete":["295","        _, diffusion_map = lobpcg(laplacian, X, M=M, tol=1.e-12,","377","        to be installed. It can be faster on very large, sparse problems,","378","        but may also lead to instabilities."]}],"doc\/whats_new\/v0.22.rst":[{"add":["235","","238","  computed wrong eigenvalues with ``eigen_solver='amg'`` when","241","- |Fix| Fixed a bug in :func:`manifold.spectral_embedding`  used in","242","  :class:`manifold.SpectralEmbedding` and :class:`cluster.spectral.SpectralClustering`","243","  where ``eigen_solver=\"amg\"`` would sometimes result in a LinAlgError.","244","  :issue:`13393` by :user:`Andrew Knyazev <lobpcg>`","245","  :pr:`13707` by :user:`Scott White <whitews>`","246",""],"delete":["233","","238","  computed wrong eigenvalues with ``solver='amg'`` when"]}]}},"4eb85b471ac862dc82ef86cfc600f99ed0d8cbd5":{"changes":{"sklearn\/linear_model\/stochastic_gradient.py":"MODIFY","doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/ensemble\/gradient_boosting.py":"MODIFY","sklearn\/linear_model\/perceptron.py":"MODIFY","sklearn\/neural_network\/multilayer_perceptron.py":"MODIFY","sklearn\/neural_network\/tests\/test_mlp.py":"MODIFY","sklearn\/ensemble\/tests\/test_gradient_boosting.py":"MODIFY","sklearn\/linear_model\/passive_aggressive.py":"MODIFY"},"diff":{"sklearn\/linear_model\/stochastic_gradient.py":[{"add":["830","        a stratified fraction of training data as validation and terminate","831","        training when validation score is not improving by at least tol for","1435","        a fraction of training data as validation and terminate","1436","        training when validation score is not improving by at least tol for"],"delete":["830","        a fraction of training data as validation and terminate training when","831","        validation score is not improving by at least tol for","1435","        a fraction of training data as validation and terminate training when","1436","        validation score is not improving by at least tol for"]}],"doc\/whats_new\/v0.21.rst":[{"add":["28","- :class:`ensemble.GradientBoostingClassifier` |Fix|","29","- :class:`neural_network.MLPClassifier` |Fix|","186","  validation sets for early stopping were not sampled with stratification.","187","  :issue:`13164` by :user:`Nicolas Hug<NicolasHug>`.","188","","189","- |Fix| Fixed a bug in :class:`ensemble.GradientBoostingClassifier` where","432","- |Fix| Fixed a bug in :class:`neural_network.MLPClassifier` where","433","  validation sets for early stopping were not sampled with stratification. In","434","  multilabel case however, splits are still not stratified.","435","  :issue:`13164` by :user:`Nicolas Hug<NicolasHug>`.","436",""],"delete":["28","- :class:`ensemble.GradientBoostingClassifier` for multiclass","29","  classification. |Fix|"]}],"sklearn\/ensemble\/gradient_boosting.py":[{"add":["1449","            stratify = y if is_classifier(self) else None","1453","                                 test_size=self.validation_fraction,","1454","                                 stratify=stratify))","1937","        iterations. The split is stratified."],"delete":["1452","                                 test_size=self.validation_fraction))","1935","        iterations."]}],"sklearn\/linear_model\/perceptron.py":[{"add":["64","        a stratified fraction of training data as validation and terminate","65","        training when validation score is not improving by at least tol for"],"delete":["64","        a fraction of training data as validation and terminate training when","65","        validation score is not improving by at least tol for"]}],"sklearn\/neural_network\/multilayer_perceptron.py":[{"add":["486","            # don't stratify in multilabel classification","487","            should_stratify = is_classifier(self) and self.n_outputs_ == 1","488","            stratify = y if should_stratify else None","491","                test_size=self.validation_fraction,","492","                stratify=stratify)","809","        ``n_iter_no_change`` consecutive epochs. The split is stratified,","810","        except in a multilabel setting."],"delete":["488","                test_size=self.validation_fraction)","805","        ``n_iter_no_change`` consecutive epochs."]}],"sklearn\/neural_network\/tests\/test_mlp.py":[{"add":["310","    # Make sure early stopping still work now that spliting is stratified by","311","    # default (it is disabled for multilabel classification)","312","    mlp = MLPClassifier(early_stopping=True)","313","    mlp.fit(X, y).predict(X)","314","","670","","671","","672","def test_early_stopping_stratified():","673","    # Make sure data splitting for early stopping is stratified","674","    X = [[1, 2], [2, 3], [3, 4], [4, 5]]","675","    y = [0, 0, 0, 1]","676","","677","    mlp = MLPClassifier(early_stopping=True)","678","    with pytest.raises(","679","            ValueError,","680","            match='The least populated class in y has only 1 member'):","681","        mlp.fit(X, y)"],"delete":[]}],"sklearn\/ensemble\/tests\/test_gradient_boosting.py":[{"add":["1267","    for est, tol, early_stop_n_estimators in ((gbc, 1e-1, 28), (gbr, 1e-1, 13),","1268","                                              (gbc, 1e-3, 70),","1323","def test_early_stopping_stratified():","1324","    # Make sure data splitting for early stopping is stratified","1325","    X = [[1, 2], [2, 3], [3, 4], [4, 5]]","1326","    y = [0, 0, 0, 1]","1327","","1328","    gbc = GradientBoostingClassifier(n_iter_no_change=5)","1329","    with pytest.raises(","1330","            ValueError,","1331","            match='The least populated class in y has only 1 member'):","1332","        gbc.fit(X, y)","1333","","1334","","1395","    # when doing early stopping (_, , y_train, _ = train_test_split(X, y))","1400","    X = [[1]] * 10","1401","    y = [0, 0] + [1] * 8  # only 2 negative class over 10 samples","1402","    gb = GradientBoostingClassifier(n_iter_no_change=5, random_state=0,","1403","                                    validation_fraction=8)","1409","    # No error if we let training data be big enough","1410","    gb = GradientBoostingClassifier(n_iter_no_change=5, random_state=0,","1411","                                    validation_fraction=4)"],"delete":["1267","    for est, tol, early_stop_n_estimators in ((gbc, 1e-1, 24), (gbr, 1e-1, 13),","1268","                                              (gbc, 1e-3, 36),","1383","    # when doing early stopping (_, y_train, _, _ = train_test_split(X, y))","1388","    X = [[1, 2], [2, 3], [3, 4], [4, 5]]","1389","    y = [0, 1, 1, 1]","1390","    gb = GradientBoostingClassifier(n_iter_no_change=5, random_state=4)","1396","    # No error with another random seed","1397","    gb = GradientBoostingClassifier(n_iter_no_change=5, random_state=0)","1398","    gb.fit(X, y)"]}],"sklearn\/linear_model\/passive_aggressive.py":[{"add":["39","        a stratified fraction of training data as validation and terminate","40","        training when validation score is not improving by at least tol for","284","        a fraction of training data as validation and terminate","285","        training when validation score is not improving by at least tol for"],"delete":["39","        a fraction of training data as validation and terminate training when","40","        validation score is not improving by at least tol for","284","        a fraction of training data as validation and terminate training when","285","        validation score is not improving by at least tol for"]}]}},"70fd42eec369e6bdb1f1a3f3578eec13c1d69abe":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/feature_extraction\/text.py":"MODIFY","sklearn\/feature_extraction\/tests\/test_text.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["29","- :class:`sklearn.feature_extraction.text.HashingVectorizer`,","30","  :class:`sklearn.feature_extraction.text.TfidfVectorizer`, and","31","  :class:`sklearn.feature_extraction.text.CountVectorizer` |API|","270",":mod:`sklearn.feature_extraction`","271",".................................","272","","273","- |API| If ``input='file'`` or ``input='filename'``, and a callable is given","274","  as the ``analyzer``, :class:`sklearn.feature_extraction.text.HashingVectorizer`,","275","  :class:`sklearn.feature_extraction.text.TfidfVectorizer`, and","276","  :class:`sklearn.feature_extraction.text.CountVectorizer` now read the data","277","  from the file(s) and then pass it to the given ``analyzer``, instead of","278","  passing the file name(s) or the file object(s) to the analyzer.","279","  :issue:`13641` by `Adrin Jalali`_.","280",""],"delete":[]}],"sklearn\/feature_extraction\/text.py":[{"add":["33","from ..exceptions import ChangedBehaviorWarning","307","    def _validate_custom_analyzer(self):","308","        # This is to check if the given custom analyzer expects file or a","309","        # filename instead of data.","310","        # Behavior changed in v0.21, function could be removed in v0.23","311","        import tempfile","312","        with tempfile.NamedTemporaryFile() as f:","313","            fname = f.name","314","        # now we're sure fname doesn't exist","315","","316","        msg = (\"Since v0.21, vectorizers pass the data to the custom analyzer \"","317","               \"and not the file names or the file objects. This warning \"","318","               \"will be removed in v0.23.\")","319","        try:","320","            self.analyzer(fname)","321","        except FileNotFoundError:","322","            warnings.warn(msg, ChangedBehaviorWarning)","323","        except AttributeError as e:","324","            if str(e) == \"'str' object has no attribute 'read'\":","325","                warnings.warn(msg, ChangedBehaviorWarning)","326","        except Exception:","327","            pass","328","","332","            if self.input in ['file', 'filename']:","333","                self._validate_custom_analyzer()","334","            return lambda doc: self.analyzer(self.decode(doc))","517","        .. versionchanged:: 0.21","518","        Since v0.21, if ``input`` is ``filename`` or ``file``, the data is","519","        first read from the file and then passed to the given callable","520","        analyzer.","521","","777","        .. versionchanged:: 0.21","778","        Since v0.21, if ``input`` is ``filename`` or ``file``, the data is","779","        first read from the file and then passed to the given callable","780","        analyzer.","781","","1406","        .. versionchanged:: 0.21","1407","        Since v0.21, if ``input`` is ``filename`` or ``file``, the data is","1408","        first read from the file and then passed to the given callable","1409","        analyzer.","1410",""],"delete":["309","            return self.analyzer"]}],"sklearn\/feature_extraction\/tests\/test_text.py":[{"add":["31","from sklearn.exceptions import ChangedBehaviorWarning","1199","","1200","","1201","@pytest.mark.parametrize('Estimator',","1202","                         [CountVectorizer, TfidfVectorizer, HashingVectorizer])","1203","@pytest.mark.parametrize(","1204","    'input_type, err_type, err_msg',","1205","    [('filename', FileNotFoundError, ''),","1206","     ('file', AttributeError, \"'str' object has no attribute 'read'\")]","1207",")","1208","def test_callable_analyzer_error(Estimator, input_type, err_type, err_msg):","1209","    data = ['this is text, not file or filename']","1210","    with pytest.raises(err_type, match=err_msg):","1211","        Estimator(analyzer=lambda x: x.split(),","1212","                  input=input_type).fit_transform(data)","1213","","1214","","1215","@pytest.mark.parametrize('Estimator',","1216","                         [CountVectorizer, TfidfVectorizer, HashingVectorizer])","1217","@pytest.mark.parametrize(","1218","    'analyzer', [lambda doc: open(doc, 'r'), lambda doc: doc.read()]","1219",")","1220","@pytest.mark.parametrize('input_type', ['file', 'filename'])","1221","def test_callable_analyzer_change_behavior(Estimator, analyzer, input_type):","1222","    data = ['this is text, not file or filename']","1223","    warn_msg = 'Since v0.21, vectorizer'","1224","    with pytest.raises((FileNotFoundError, AttributeError)):","1225","        with pytest.warns(ChangedBehaviorWarning, match=warn_msg) as records:","1226","            Estimator(analyzer=analyzer, input=input_type).fit_transform(data)","1227","    assert len(records) == 1","1228","    assert warn_msg in str(records[0])","1229","","1230","","1231","@pytest.mark.parametrize('Estimator',","1232","                         [CountVectorizer, TfidfVectorizer, HashingVectorizer])","1233","def test_callable_analyzer_reraise_error(tmpdir, Estimator):","1234","    # check if a custom exception from the analyzer is shown to the user","1235","    def analyzer(doc):","1236","        raise Exception(\"testing\")","1237","","1238","    f = tmpdir.join(\"file.txt\")","1239","    f.write(\"sample content\\n\")","1240","","1241","    with pytest.raises(Exception, match=\"testing\"):","1242","        Estimator(analyzer=analyzer, input='file').fit_transform([f])"],"delete":[]}]}},"d14276428ba122b8a2e47775599f1ca91553d80d":{"changes":{"sklearn\/utils\/_pprint.py":"MODIFY","sklearn\/utils\/tests\/test_pprint.py":"MODIFY"},"diff":{"sklearn\/utils\/_pprint.py":[{"add":["97","        if (repr(v) != repr(init_params[k]) and"],"delete":["97","        if (v != init_params[k] and"]}],"sklearn\/utils\/tests\/test_pprint.py":[{"add":["6","from sklearn.linear_model import LogisticRegressionCV","215","    # make sure array parameters don't throw error (see #13583)","216","    repr(LogisticRegressionCV(Cs=np.array([0.1, 1])))","217",""],"delete":[]}]}},"3cb0b3bc22b0a6f99f80c025fd378660896a2c68":{"changes":{"build_tools\/circle\/build_test_pypy.sh":"MODIFY"},"diff":{"build_tools\/circle\/build_test_pypy.sh":[{"add":["23","pip install --extra-index https:\/\/antocuni.github.io\/pypy-wheels\/ubuntu \"numpy==1.15.*\" Cython pytest"],"delete":["23","pip install --extra-index https:\/\/antocuni.github.io\/pypy-wheels\/ubuntu \"numpy==1.5.*\" Cython pytest"]}]}},"415fd83dbf089d48a2c5bb15002933f432810421":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/tests\/test_discriminant_analysis.py":"MODIFY","sklearn\/discriminant_analysis.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["95","- |Enhancement| :class:`discriminant_analysis.LinearDiscriminantAnalysis` now","96","  preserves ``float32`` and ``float64`` dtypes. :issues:`8769` and","97","  :issues:`11000` by :user:`Thibault Sejourne <thibsej>`","98",""],"delete":[]}],"sklearn\/tests\/test_discriminant_analysis.py":[{"add":["9","from sklearn.utils.testing import assert_allclose","299","@pytest.mark.parametrize(\"data_type, expected_type\", [","300","    (np.float32, np.float32),","301","    (np.float64, np.float64),","302","    (np.int32, np.float64),","303","    (np.int64, np.float64)","304","])","305","def test_lda_dtype_match(data_type, expected_type):","306","    for (solver, shrinkage) in solver_shrinkage:","307","        clf = LinearDiscriminantAnalysis(solver=solver, shrinkage=shrinkage)","308","        clf.fit(X.astype(data_type), y.astype(data_type))","309","        assert clf.coef_.dtype == expected_type","310","","311","","312","def test_lda_numeric_consistency_float32_float64():","313","    for (solver, shrinkage) in solver_shrinkage:","314","        clf_32 = LinearDiscriminantAnalysis(solver=solver, shrinkage=shrinkage)","315","        clf_32.fit(X.astype(np.float32), y.astype(np.float32))","316","        clf_64 = LinearDiscriminantAnalysis(solver=solver, shrinkage=shrinkage)","317","        clf_64.fit(X.astype(np.float64), y.astype(np.float64))","318","","319","        # Check value consistency between types","320","        rtol = 1e-6","321","        assert_allclose(clf_32.coef_, clf_64.coef_, rtol=rtol)","322","","323",""],"delete":[]}],"sklearn\/discriminant_analysis.py":[{"add":["429","        X, y = check_X_y(X, y, ensure_min_samples=2, estimator=self,","430","                         dtype=[np.float64, np.float32])","488","            self.coef_ = np.array(self.coef_[1, :] - self.coef_[0, :], ndmin=2,","489","                                  dtype=X.dtype)","491","                                       ndmin=1, dtype=X.dtype)"],"delete":["429","        X, y = check_X_y(X, y, ensure_min_samples=2, estimator=self)","487","            self.coef_ = np.array(self.coef_[1, :] - self.coef_[0, :], ndmin=2)","489","                                       ndmin=1)"]}]}},"d1c52f402c89586ff66c2ec6e86f794da7715a18":{"changes":{"examples\/inspection\/plot_permutation_importance.py":"ADD","sklearn\/inspection\/tests\/test_permutation_importance.py":"ADD","examples\/inspection\/plot_permutation_importance_multicollinear.py":"ADD","doc\/modules\/classes.rst":"MODIFY","doc\/modules\/permutation_importance.rst":"ADD","sklearn\/inspection\/permutation_importance.py":"ADD","sklearn\/inspection\/__init__.py":"MODIFY","doc\/inspection.rst":"MODIFY"},"diff":{"examples\/inspection\/plot_permutation_importance.py":[{"add":[],"delete":[]}],"sklearn\/inspection\/tests\/test_permutation_importance.py":[{"add":[],"delete":[]}],"examples\/inspection\/plot_permutation_importance_multicollinear.py":[{"add":[],"delete":[]}],"doc\/modules\/classes.rst":[{"add":["659","   inspection.permutation_importance"],"delete":["1259",""]}],"doc\/modules\/permutation_importance.rst":[{"add":[],"delete":[]}],"sklearn\/inspection\/permutation_importance.py":[{"add":[],"delete":[]}],"sklearn\/inspection\/__init__.py":[{"add":["3","from .permutation_importance import permutation_importance","8","    'permutation_importance'"],"delete":["3",""]}],"doc\/inspection.rst":[{"add":["7","Predictive performance is often the main goal of developing machine learning","8","models. Yet summarising performance with an evaluation metric is often","9","insufficient: it assumes that the evaluation metric and test dataset","10","perfectly reflect the target domain, which is rarely true. In certain domains,","11","a model needs a certain level of interpretability before it can be deployed.","12","A model that is exhibiting performance issues needs to be debugged for one to ","13","understand the model's underlying issue. The ","14",":mod:`sklearn.inspection` module provides tools to help understand the ","15","predictions from a model and what affects them. This can be used to ","16","evaluate assumptions and biases of a model, design a better model, or","17","to diagnose issues with model performance.","18","","22","    modules\/permutation_importance"],"delete":[]}]}},"3ce62371f853492b58f3a4103cf03021971ebedd":{"changes":{"sklearn\/neural_network\/tests\/test_mlp.py":"MODIFY"},"diff":{"sklearn\/neural_network\/tests\/test_mlp.py":[{"add":["179","        random_state = np.random.RandomState(seed=42)","180","        X = random_state.rand(n_samples, n_features)"],"delete":["179","        X = np.random.random((n_samples, n_features))"]}]}},"c22b8712d88133ddb9f3572944c111c93b9af300":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/ensemble\/iforest.py":"MODIFY","sklearn\/ensemble\/tests\/test_iforest.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["170","- |Efficiency| :class:`ensemble.IsolationForest` now uses chunks of data at","171","  prediction step, thus capping the memory usage. :issue:`13283` by","172","  `Nicolas Goix`_.","173",""],"delete":[]}],"sklearn\/ensemble\/iforest.py":[{"add":["11","from ..utils import (","12","    check_random_state,","13","    check_array,","14","    gen_batches,","15","    get_chunk_n_rows,","16",")","18","from ..utils.validation import check_is_fitted, _num_samples","396","        # Take the opposite of the scores as bigger is better (here less","397","        # abnormal)","398","        return -self._compute_chunked_score_samples(X)","399","","400","    @property","401","    def threshold_(self):","402","        if self.behaviour != 'old':","403","            raise AttributeError(\"threshold_ attribute does not exist when \"","404","                                 \"behaviour != 'old'\")","405","        warn(\"threshold_ attribute is deprecated in 0.20 and will\"","406","             \" be removed in 0.22.\", DeprecationWarning)","407","        return self._threshold_","408","","409","    def _compute_chunked_score_samples(self, X):","410","","411","        n_samples = _num_samples(X)","418","        # We get as many rows as possible within our working_memory budget","419","        # (defined by sklearn.get_config()['working_memory']) to store","420","        # self._max_features in each row during computation.","421","        #","422","        # Note:","423","        #  - this will get at least 1 row, even if 1 row of score will","424","        #    exceed working_memory.","425","        #  - this does only account for temporary memory usage while loading","426","        #    the data needed to compute the scores -- the returned scores","427","        #    themselves are 1D.","428","","429","        chunk_n_rows = get_chunk_n_rows(row_bytes=16 * self._max_features,","430","                                        max_n_rows=n_samples)","431","        slices = gen_batches(n_samples, chunk_n_rows)","432","","433","        scores = np.zeros(n_samples, order=\"f\")","434","","435","        for sl in slices:","436","            # compute score on the slices of test samples:","437","            scores[sl] = self._compute_score_samples(X[sl], subsample_features)","438","","439","        return scores","440","","441","    def _compute_score_samples(self, X, subsample_features):","442","        \"\"\"Compute the score of each samples in X going through the extra trees.","443","","444","        Parameters","445","        ----------","446","        X : array-like or sparse matrix","447","","448","        subsample_features : bool,","449","            whether features should be subsampled","450","        \"\"\"","451","        n_samples = X.shape[0]","452","","453","        depths = np.zeros(n_samples, order=\"f\")","454","","456","            X_subset = X[:, features] if subsample_features else X","457","","473","        return scores"],"delete":["11","from ..utils import check_random_state, check_array","13","from ..utils.validation import check_is_fitted","390","        n_samples = X.shape[0]","392","        n_samples_leaf = np.zeros(n_samples, order=\"f\")","393","        depths = np.zeros(n_samples, order=\"f\")","401","            if subsample_features:","402","                X_subset = X[:, features]","403","            else:","404","                X_subset = X","420","","421","        # Take the opposite of the scores as bigger is better (here less","422","        # abnormal)","423","        return -scores","424","","425","    @property","426","    def threshold_(self):","427","        if self.behaviour != 'old':","428","            raise AttributeError(\"threshold_ attribute does not exist when \"","429","                                 \"behaviour != 'old'\")","430","        warn(\"threshold_ attribute is deprecated in 0.20 and will\"","431","             \" be removed in 0.22.\", DeprecationWarning)","432","        return self._threshold_"]}],"sklearn\/ensemble\/tests\/test_iforest.py":[{"add":["31","from unittest.mock import Mock, patch","328","","329","","330","# mock get_chunk_n_rows to actually test more than one chunk (here one","331","# chunk = 3 rows:","332","@patch(","333","    \"sklearn.ensemble.iforest.get_chunk_n_rows\",","334","    side_effect=Mock(**{\"return_value\": 3}),","335",")","336","@pytest.mark.parametrize(","337","    \"contamination, n_predict_calls\", [(0.25, 3), (\"auto\", 2)]","338",")","339","@pytest.mark.filterwarnings(\"ignore:threshold_ attribute\")","340","def test_iforest_chunks_works1(","341","    mocked_get_chunk, contamination, n_predict_calls","342","):","343","    test_iforest_works(contamination)","344","    assert mocked_get_chunk.call_count == n_predict_calls","345","","346","","347","# idem with chunk_size = 5 rows","348","@patch(","349","    \"sklearn.ensemble.iforest.get_chunk_n_rows\",","350","    side_effect=Mock(**{\"return_value\": 10}),","351",")","352","@pytest.mark.parametrize(","353","    \"contamination, n_predict_calls\", [(0.25, 3), (\"auto\", 2)]","354",")","355","@pytest.mark.filterwarnings(\"ignore:threshold_ attribute\")","356","def test_iforest_chunks_works2(","357","    mocked_get_chunk, contamination, n_predict_calls","358","):","359","    test_iforest_works(contamination)","360","    assert mocked_get_chunk.call_count == n_predict_calls"],"delete":[]}]}},"89e6c967d653710b48722dd6b162e692d3c42365":{"changes":{"sklearn\/linear_model\/tests\/test_ridge.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY","sklearn\/linear_model\/ridge.py":"MODIFY"},"diff":{"sklearn\/linear_model\/tests\/test_ridge.py":[{"add":["1010","@pytest.mark.parametrize('solver', ['sparse_cg', 'auto'])","1011","def test_ridge_fit_intercept_sparse(solver):","1012","    X, y = _make_sparse_offset_regression(n_features=20, random_state=0)","1015","    # for now only sparse_cg can correctly fit an intercept with sparse X with","1016","    # default tol and max_iter.","1017","    # sag is tested separately in test_ridge_fit_intercept_sparse_sag","1018","    # because it requires more iterations and should raise a warning if default","1019","    # max_iter is used.","1020","    # other solvers raise an exception, as checked in","1021","    # test_ridge_fit_intercept_sparse_error","1022","    #","1023","    # \"auto\" should switch to \"sparse_cg\" when X is sparse","1024","    # so the reference we use for both (\"auto\" and \"sparse_cg\") is","1025","    # Ridge(solver=\"sparse_cg\"), fitted using the dense representation (note","1026","    # that \"sparse_cg\" can fit sparse or dense data)","1027","    dense_ridge = Ridge(alpha=1., solver='sparse_cg', fit_intercept=True)","1028","    sparse_ridge = Ridge(alpha=1., solver=solver, fit_intercept=True)","1029","    dense_ridge.fit(X, y)","1030","    with pytest.warns(None) as record:","1031","        sparse_ridge.fit(X_csr, y)","1032","    assert len(record) == 0","1033","    assert np.allclose(dense_ridge.intercept_, sparse_ridge.intercept_)","1034","    assert np.allclose(dense_ridge.coef_, sparse_ridge.coef_)","1036","","1037","@pytest.mark.parametrize('solver', ['saga', 'lsqr', 'svd', 'cholesky'])","1038","def test_ridge_fit_intercept_sparse_error(solver):","1039","    X, y = _make_sparse_offset_regression(n_features=20, random_state=0)","1040","    X_csr = sp.csr_matrix(X)","1041","    sparse_ridge = Ridge(alpha=1., solver=solver, fit_intercept=True)","1042","    err_msg = \"solver='{}' does not support\".format(solver)","1043","    with pytest.raises(ValueError, match=err_msg):","1044","        sparse_ridge.fit(X_csr, y)","1045","","1046","","1047","def test_ridge_fit_intercept_sparse_sag():","1048","    X, y = _make_sparse_offset_regression(","1049","        n_features=5, n_samples=20, random_state=0, X_offset=5.)","1050","    X_csr = sp.csr_matrix(X)","1051","","1052","    params = dict(alpha=1., solver='sag', fit_intercept=True,","1053","                  tol=1e-10, max_iter=100000)","1054","    dense_ridge = Ridge(**params)","1055","    sparse_ridge = Ridge(**params)","1056","    dense_ridge.fit(X, y)","1057","    with pytest.warns(None) as record:","1058","        sparse_ridge.fit(X_csr, y)","1059","    assert len(record) == 0","1060","    assert np.allclose(dense_ridge.intercept_, sparse_ridge.intercept_,","1061","                       rtol=1e-4)","1062","    assert np.allclose(dense_ridge.coef_, sparse_ridge.coef_, rtol=1e-4)","1063","    with pytest.warns(UserWarning, match='\"sag\" solver requires.*'):","1064","        Ridge(solver='sag').fit(X_csr, y)"],"delete":["1010","def test_ridge_fit_intercept_sparse():","1011","    X, y = make_regression(n_samples=1000, n_features=2, n_informative=2,","1012","                           bias=10., random_state=42)","1013","","1016","    for solver in ['sag', 'sparse_cg']:","1017","        dense = Ridge(alpha=1., tol=1.e-15, solver=solver, fit_intercept=True)","1018","        sparse = Ridge(alpha=1., tol=1.e-15, solver=solver, fit_intercept=True)","1019","        dense.fit(X, y)","1020","        with pytest.warns(None) as record:","1021","            sparse.fit(X_csr, y)","1022","        assert len(record) == 0","1023","        assert_almost_equal(dense.intercept_, sparse.intercept_)","1024","        assert_array_almost_equal(dense.coef_, sparse.coef_)","1026","    # test the solver switch and the corresponding warning","1027","    for solver in ['saga', 'lsqr']:","1028","        sparse = Ridge(alpha=1., tol=1.e-15, solver=solver, fit_intercept=True)","1029","        assert_raises_regex(ValueError, \"In Ridge,\", sparse.fit, X_csr, y)"]}],"doc\/whats_new\/v0.22.rst":[{"add":["26","- :class:`linear_model.Ridge` when `X` is sparse. |Fix|","27","","118","- |Fix| :class:`linear_model.Ridge` now correctly fits an intercept when `X` is","119","  sparse, `solver=\"auto\"` and `fit_intercept=True`, because the default solver","120","  in this configuration has changed to `sparse_cg`, which can fit an intercept","121","  with sparse data. :pr:`13995` by :user:`J¨¦r?me Dock¨¨s <jeromedockes>`.","122",""],"delete":[]}],"sklearn\/linear_model\/ridge.py":[{"add":["547","        if sparse.issparse(X) and self.fit_intercept:","548","            if self.solver not in ['auto', 'sparse_cg', 'sag']:","549","                raise ValueError(","550","                    \"solver='{}' does not support fitting the intercept \"","551","                    \"on sparse data. Please set the solver to 'auto' or \"","552","                    \"'sparse_cg', 'sag', or set `fit_intercept=False`\"","553","                    .format(self.solver))","554","            if (self.solver == 'sag' and self.max_iter is None and","555","                    self.tol > 1e-4):","556","                warnings.warn(","557","                    '\"sag\" solver requires many iterations to fit '","558","                    'an intercept with sparse inputs. Either set the '","559","                    'solver to \"auto\" or \"sparse_cg\", or set a low '","560","                    '\"tol\" and a high \"max_iter\" (especially if inputs are '","561","                    'not standardized).')","562","                solver = 'sag'","563","            else:","564","                solver = 'sparse_cg'","565","        else:","566","            solver = self.solver","577","        if solver == 'sag' and sparse.issparse(X) and self.fit_intercept:","585","","587","            if sparse.issparse(X) and self.fit_intercept:","596","                max_iter=self.max_iter, tol=self.tol, solver=solver,","684","        'sparse_cg' supports sparse input when `fit_intercept` is True."],"delete":["557","        # temporary fix for fitting the intercept with sparse data using 'sag'","558","        if (sparse.issparse(X) and self.fit_intercept and","559","           self.solver != 'sparse_cg'):","568","            if sparse.issparse(X) and self.solver == 'sparse_cg':","577","                max_iter=self.max_iter, tol=self.tol, solver=self.solver,","665","        'sag' and 'sparse_cg' supports sparse input when `fit_intercept` is","666","        True."]}]}},"cdfca8cba33be63ef50ba9e14d8823cc551baf92":{"changes":{"sklearn\/base.py":"MODIFY","sklearn\/manifold\/locally_linear.py":"MODIFY","sklearn\/cross_decomposition\/cca_.py":"MODIFY"},"diff":{"sklearn\/base.py":[{"add":["8","import platform","557","class _UnstableArchMixin(object):","558","    \"\"\"Mark estimators that are non-determinstic on 32bit or PowerPC\"\"\"","560","        return {'non_deterministic': (","561","            _IS_32BIT or platform.machine().startswith(('ppc', 'powerpc')))}"],"delete":["8","import struct","557","class _UnstableOn32BitMixin(object):","558","    \"\"\"Mark estimators that are non-determinstic on 32bit.\"\"\"","560","        return {'non_deterministic': _IS_32BIT}"]}],"sklearn\/manifold\/locally_linear.py":[{"add":["11","from ..base import BaseEstimator, TransformerMixin, _UnstableArchMixin","521","                             _UnstableArchMixin):"],"delete":["11","from ..base import BaseEstimator, TransformerMixin, _UnstableOn32BitMixin","521","                             _UnstableOn32BitMixin):"]}],"sklearn\/cross_decomposition\/cca_.py":[{"add":["1","from ..base import _UnstableArchMixin","6","class CCA(_PLS, _UnstableArchMixin):"],"delete":["1","from ..base import _UnstableOn32BitMixin","6","class CCA(_PLS, _UnstableOn32BitMixin):"]}]}},"b40868d52b479eedf31e49e43b6b8bec1a7dfa44":{"changes":{"sklearn\/preprocessing\/_discretization.py":"MODIFY","doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/preprocessing\/tests\/test_discretization.py":"MODIFY"},"diff":{"sklearn\/preprocessing\/_discretization.py":[{"add":["58","        Number of bins per feature. Bins whose width are too small","59","        (i.e., <= 1e-8) are removed with a warning.","105","    ``KBinsDiscretizer`` might produce constant features (e.g., when","106","    ``encode = 'onehot'`` and certain bins do not contain any data).","107","    These features can be removed with feature selection algorithms","108","    (e.g., :class:`sklearn.feature_selection.VarianceThreshold`).","109","","185","            # Remove bins whose width are too small (i.e., <= 1e-8)","186","            if self.strategy in ('quantile', 'kmeans'):","187","                mask = np.ediff1d(bin_edges[jj], to_begin=np.inf) > 1e-8","188","                bin_edges[jj] = bin_edges[jj][mask]","189","                if len(bin_edges[jj]) - 1 != n_bins[jj]:","190","                    warnings.warn('Bins whose width are too small (i.e., <= '","191","                                  '1e-8) in feature %d are removed. Consider '","192","                                  'decreasing the number of bins.' % jj)","193","                    n_bins[jj] = len(bin_edges[jj]) - 1","194",""],"delete":["58","        Number of bins per feature."]}],"doc\/whats_new\/v0.20.rst":[{"add":["66","- |Fix| Bins whose width are too small (i.e., <= 1e-8) are removed","67","  with a warning in :class:`preprocessing.KBinsDiscretizer`.","68","  :issue:`13165` by :user:`Hanmin Qin <qinhanmin2014>`.","69",""],"delete":[]}],"sklearn\/preprocessing\/tests\/test_discretization.py":[{"add":["9","    assert_array_almost_equal,","212","@pytest.mark.parametrize(","213","    'strategy, expected_inv',","214","    [('uniform', [[-1.5, 2., -3.5, -0.5], [-0.5, 3., -2.5, -0.5],","215","                  [0.5, 4., -1.5, 0.5], [0.5, 4., -1.5, 1.5]]),","216","     ('kmeans', [[-1.375, 2.125, -3.375, -0.5625],","217","                 [-1.375, 2.125, -3.375, -0.5625],","218","                 [-0.125, 3.375, -2.125, 0.5625],","219","                 [0.75, 4.25, -1.25, 1.625]]),","220","     ('quantile', [[-1.5, 2., -3.5, -0.75], [-0.5, 3., -2.5, 0.],","221","                   [0.5, 4., -1.5, 1.25], [0.5, 4., -1.5, 1.25]])])","223","def test_inverse_transform(strategy, encode, expected_inv):","226","    Xinv = kbd.inverse_transform(Xt)","227","    assert_array_almost_equal(expected_inv, Xinv)","254","","255","","256","@pytest.mark.parametrize(","257","    'strategy, expected_bin_edges',","258","    [('quantile', [0, 1, 3]), ('kmeans', [0, 1.5, 3])])","259","def test_redundant_bins(strategy, expected_bin_edges):","260","    X = [[0], [0], [0], [0], [3], [3]]","261","    kbd = KBinsDiscretizer(n_bins=3, strategy=strategy)","262","    msg = (\"Bins whose width are too small (i.e., <= 1e-8) in feature 0 \"","263","           \"are removed. Consider decreasing the number of bins.\")","264","    assert_warns_message(UserWarning, msg, kbd.fit, X)","265","    assert_array_almost_equal(kbd.bin_edges_[0], expected_bin_edges)","266","","267","","268","def test_percentile_numeric_stability():","269","    X = np.array([0.05, 0.05, 0.95]).reshape(-1, 1)","270","    bin_edges = np.array([0.05, 0.23, 0.41, 0.59, 0.77, 0.95])","271","    Xt = np.array([0, 0, 4]).reshape(-1, 1)","272","    kbd = KBinsDiscretizer(n_bins=10, encode='ordinal',","273","                           strategy='quantile')","274","    msg = (\"Bins whose width are too small (i.e., <= 1e-8) in feature 0 \"","275","           \"are removed. Consider decreasing the number of bins.\")","276","    assert_warns_message(UserWarning, msg, kbd.fit, X)","277","    assert_array_almost_equal(kbd.bin_edges_[0], bin_edges)","278","    assert_array_almost_equal(kbd.transform(X), Xt)"],"delete":["211","@pytest.mark.parametrize('strategy', ['uniform', 'kmeans', 'quantile'])","213","def test_inverse_transform(strategy, encode):","214","    X = np.random.RandomState(0).randn(100, 3)","217","    X2 = kbd.inverse_transform(Xt)","218","    X2t = kbd.fit_transform(X2)","219","    if encode == 'onehot':","220","        assert_array_equal(Xt.todense(), X2t.todense())","221","    else:","222","        assert_array_equal(Xt, X2t)","223","    if 'onehot' in encode:","224","        Xt = kbd._encoder.inverse_transform(Xt)","225","        X2t = kbd._encoder.inverse_transform(X2t)","226","","227","    assert_array_equal(Xt.max(axis=0) + 1, kbd.n_bins_)","228","    assert_array_equal(X2t.max(axis=0) + 1, kbd.n_bins_)"]}]}},"d0747ea2f27332c78b0009914c9bae42db719891":{"changes":{"sklearn\/ensemble\/_gradient_boosting.pyx":"MODIFY","doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/ensemble\/tests\/test_partial_dependence.py":"MODIFY"},"diff":{"sklearn\/ensemble\/_gradient_boosting.pyx":[{"add":["344","                    left_sample_frac = root_node[current_node.left_child].weighted_n_node_samples \/ \\","345","                                       current_node.weighted_n_node_samples","347","                        raise ValueError(\"left_sample_frac:%d, \"","348","                                         \"weighted_n_node_samples current: %d, \"","349","                                         \"weighted_n_node_samples left: %d\"","351","                                            current_node.weighted_n_node_samples,","352","                                            root_node[current_node.left_child].weighted_n_node_samples))"],"delete":["344","                    left_sample_frac = root_node[current_node.left_child].n_node_samples \/ \\","345","                                       <double>current_node.n_node_samples","347","                        raise ValueError(\"left_sample_frac:%f, \"","348","                                         \"n_samples current: %d, \"","349","                                         \"n_samples left: %d\"","351","                                            current_node.n_node_samples,","352","                                            root_node[current_node.left_child].n_node_samples))"]}],"doc\/whats_new\/v0.21.rst":[{"add":["227","- |Fix| :func:`ensemble.partial_dependence` now takes sample weights into","228","  account for the partial dependence computation when the","229","  gradient boosting model has been trained with sample weights.","230","  :issue:`13193` by :user:`Samuel O. Ronsin <samronsin>`.","231",""],"delete":[]}],"sklearn\/ensemble\/tests\/test_partial_dependence.py":[{"add":["6","from numpy.testing import assert_array_equal, assert_allclose","20","sample_weight = [1, 1, 1, 2, 2, 2]","48","    # with trivial (no-op) sample weights","49","    clf.fit(X, y, sample_weight=np.ones(len(y)))","50","","51","    pdp_w, axes_w = partial_dependence(clf, [0], X=X, grid_resolution=5)","52","","53","    assert pdp_w.shape == (1, 4)","54","    assert axes_w[0].shape[0] == 4","55","    assert_allclose(pdp_w, pdp)","56","","57","    # with non-trivial sample weights","58","    clf.fit(X, y, sample_weight=sample_weight)","59","","60","    pdp_w2, axes_w2 = partial_dependence(clf, [0], X=X, grid_resolution=5)","61","","62","    assert pdp_w2.shape == (1, 4)","63","    assert axes_w2[0].shape[0] == 4","64","    assert np.all(np.abs(pdp_w2 - pdp_w) \/ np.abs(pdp_w) > 0.1)","65","","95","def test_partial_dependence_sample_weight():","96","    # Test near perfect correlation between partial dependence and diagonal","97","    # when sample weights emphasize y = x predictions","98","    N = 1000","99","    rng = np.random.RandomState(123456)","100","    mask = rng.randint(2, size=N, dtype=bool)","101","","102","    x = rng.rand(N)","103","    # set y = x on mask and y = -x outside","104","    y = x.copy()","105","    y[~mask] = -y[~mask]","106","    X = np.c_[mask, x]","107","    # sample weights to emphasize data points where y = x","108","    sample_weight = np.ones(N)","109","    sample_weight[mask] = 1000.","110","","111","    clf = GradientBoostingRegressor(n_estimators=10, random_state=1)","112","    clf.fit(X, y, sample_weight=sample_weight)","113","","114","    grid = np.arange(0, 1, 0.01)","115","    pdp = partial_dependence(clf, [1], grid=grid)","116","","117","    assert np.corrcoef(np.ravel(pdp[0]), grid)[0, 1] > 0.99","118","","119",""],"delete":["6","from numpy.testing import assert_array_equal","20","T = [[-1, -1], [2, 2], [3, 2]]","21","true_result = [-1, 1, 1]"]}]}},"896a76eb22fe498494ab1b73594a13a7f266b912":{"changes":{"sklearn\/cluster\/hierarchical.py":"MODIFY","sklearn\/cluster\/affinity_propagation_.py":"MODIFY","sklearn\/cluster\/dbscan_.py":"MODIFY","sklearn\/cluster\/spectral.py":"MODIFY"},"diff":{"sklearn\/cluster\/hierarchical.py":[{"add":["775","        \"\"\"Fit the hierarchical clustering from features, or distance matrix.","779","        X : array-like, shape (n_samples, n_features) or (n_samples, n_samples)","780","            Training instances to cluster, or distances between instances if","781","            ``affinity='precomputed'``.","784","            Not used, present here for API consistency by convention.","878","    def fit_predict(self, X, y=None):","879","        \"\"\"Fit the hierarchical clustering from features or distance matrix,","880","        and return cluster labels.","881","","882","        Parameters","883","        ----------","884","        X : array-like, shape (n_samples, n_features) or (n_samples, n_samples)","885","            Training instances to cluster, or distances between instances if","886","            ``affinity='precomputed'``.","887","","888","        y : Ignored","889","            Not used, present here for API consistency by convention.","890","","891","        Returns","892","        -------","893","        labels : ndarray, shape (n_samples,)","894","            Cluster labels.","895","        \"\"\"","896","        return super().fit_predict(X, y)","897",""],"delete":["775","        \"\"\"Fit the hierarchical clustering on the data","779","        X : array-like, shape = [n_samples, n_features]","780","            Training data. Shape [n_samples, n_features], or [n_samples,","781","            n_samples] if affinity=='precomputed'."]}],"sklearn\/cluster\/affinity_propagation_.py":[{"add":["353","        \"\"\"Fit the clustering from features, or affinity matrix.","357","        X : array-like or sparse matrix, shape (n_samples, n_features), or \\","358","            array-like, shape (n_samples, n_samples)","359","            Training instances to cluster, or similarities \/ affinities between","360","            instances if ``affinity='precomputed'``. If a sparse feature matrix","361","            is provided, it will be converted into a sparse ``csr_matrix``.","364","            Not used, present here for API consistency by convention.","365","","366","        Returns","367","        -------","368","        self","401","        X : array-like or sparse matrix, shape (n_samples, n_features)","402","            New data to predict. If a sparse matrix is provided, it will be","403","            converted into a sparse ``csr_matrix``.","407","        labels : ndarray, shape (n_samples,)","408","            Cluster labels.","422","","423","    def fit_predict(self, X, y=None):","424","        \"\"\"Fit the clustering from features or affinity matrix, and return","425","        cluster labels.","426","","427","        Parameters","428","        ----------","429","        X : array-like or sparse matrix, shape (n_samples, n_features), or \\","430","            array-like, shape (n_samples, n_samples)","431","            Training instances to cluster, or similarities \/ affinities between","432","            instances if ``affinity='precomputed'``. If a sparse feature matrix","433","            is provided, it will be converted into a sparse ``csr_matrix``.","434","","435","        y : Ignored","436","            Not used, present here for API consistency by convention.","437","","438","        Returns","439","        -------","440","        labels : ndarray, shape (n_samples,)","441","            Cluster labels.","442","        \"\"\"","443","        return super().fit_predict(X, y)"],"delete":["353","        \"\"\" Create affinity matrix from negative euclidean distances, then","354","        apply affinity propagation clustering.","358","","359","        X : array-like, shape (n_samples, n_features) or (n_samples, n_samples)","360","            Data matrix or, if affinity is ``precomputed``, matrix of","361","            similarities \/ affinities.","396","        X : {array-like, sparse matrix}, shape (n_samples, n_features)","397","            New data to predict.","401","        labels : array, shape (n_samples,)","402","            Index of the cluster each sample belongs to."]}],"sklearn\/cluster\/dbscan_.py":[{"add":["331","        \"\"\"Perform DBSCAN clustering from features, or distance matrix.","335","        X : array-like or sparse matrix, shape (n_samples, n_features), or \\","336","            (n_samples, n_samples)","337","            Training instances to cluster, or distances between instances if","338","            ``metric='precomputed'``. If a sparse matrix is provided, it will","339","            be converted into a sparse ``csr_matrix``.","340","","343","            ``min_samples`` is by itself a core sample; a sample with a","344","            negative weight may inhibit its eps-neighbor from being core.","348","            Not used, present here for API consistency by convention.","349","","350","        Returns","351","        -------","352","        self","368","        \"\"\"Perform DBSCAN clustering from features or distance matrix,","369","        and return cluster labels.","373","        X : array-like or sparse matrix, shape (n_samples, n_features), or \\","374","            (n_samples, n_samples)","375","            Training instances to cluster, or distances between instances if","376","            ``metric='precomputed'``. If a sparse matrix is provided, it will","377","            be converted into a sparse ``csr_matrix``.","378","","381","            ``min_samples`` is by itself a core sample; a sample with a","382","            negative weight may inhibit its eps-neighbor from being core.","386","            Not used, present here for API consistency by convention.","390","        labels : ndarray, shape (n_samples,)","391","            Cluster labels. Noisy samples are given the label -1."],"delete":["331","        \"\"\"Perform DBSCAN clustering from features or distance matrix.","335","        X : array or sparse (CSR) matrix of shape (n_samples, n_features), or \\","336","                array of shape (n_samples, n_samples)","337","            A feature array, or array of distances between samples if","338","            ``metric='precomputed'``.","341","            ``min_samples`` is by itself a core sample; a sample with negative","342","            weight may inhibit its eps-neighbor from being core.","361","        \"\"\"Performs clustering on X and returns cluster labels.","365","        X : array or sparse (CSR) matrix of shape (n_samples, n_features), or \\","366","                array of shape (n_samples, n_samples)","367","            A feature array, or array of distances between samples if","368","            ``metric='precomputed'``.","371","            ``min_samples`` is by itself a core sample; a sample with negative","372","            weight may inhibit its eps-neighbor from being core.","379","        y : ndarray, shape (n_samples,)","380","            cluster labels"]}],"sklearn\/cluster\/spectral.py":[{"add":["450","        \"\"\"Perform spectral clustering from features, or affinity matrix.","454","        X : array-like or sparse matrix, shape (n_samples, n_features), or \\","455","            array-like, shape (n_samples, n_samples)","456","            Training instances to cluster, or similarities \/ affinities between","457","            instances if ``affinity='precomputed'``. If a sparse matrix is","458","            provided in a format other than ``csr_matrix``, ``csc_matrix``,","459","            or ``coo_matrix``, it will be converted into a sparse","460","            ``csr_matrix``.","463","            Not used, present here for API consistency by convention.","464","","465","        Returns","466","        -------","467","        self","508","    def fit_predict(self, X, y=None):","509","        \"\"\"Perform spectral clustering from features, or affinity matrix,","510","        and return cluster labels.","511","","512","        Parameters","513","        ----------","514","        X : array-like or sparse matrix, shape (n_samples, n_features), or \\","515","            array-like, shape (n_samples, n_samples)","516","            Training instances to cluster, or similarities \/ affinities between","517","            instances if ``affinity='precomputed'``. If a sparse matrix is","518","            provided in a format other than ``csr_matrix``, ``csc_matrix``,","519","            or ``coo_matrix``, it will be converted into a sparse","520","            ``csr_matrix``.","521","","522","        y : Ignored","523","            Not used, present here for API consistency by convention.","524","","525","        Returns","526","        -------","527","        labels : ndarray, shape (n_samples,)","528","            Cluster labels.","529","        \"\"\"","530","        return super().fit_predict(X, y)","531",""],"delete":["450","        \"\"\"Creates an affinity matrix for X using the selected affinity,","451","        then applies spectral clustering to this affinity matrix.","455","        X : array-like or sparse matrix, shape (n_samples, n_features)","456","            OR, if affinity==`precomputed`, a precomputed affinity","457","            matrix of shape (n_samples, n_samples)"]}]}},"e35f040200d8aa9270e6a6f0c35abea386964a08":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/preprocessing\/_encoders.py":"MODIFY","sklearn\/preprocessing\/tests\/test_encoders.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["23","- |Fix| Fixed a bug in :func:`metrics.euclidean_distances` where a part of the","24","  distance matrix was left un-instanciated for suffiently large float32","25","  datasets (regression introduced in 0.21). :pr:`13910` by :user:`J¨¦r¨¦mie du","26","  Boisberranger <jeremiedbb>`.","27","","28",":mod:`sklearn.preprocessing`","29","............................","30","","31","- |Fix| Fixed a bug in :class:`preprocessing.OneHotEncoder` where the new","32","  `drop` parameter was not reflected in `get_feature_names`. :pr:`13894`","33","  by :user:`James Myatt <jamesmyatt>`."],"delete":["23","- |Fix| Fixed a bug in :func:`euclidean_distances` where a part of the distance","24","  matrix was left un-instanciated for suffiently large float32 datasets","25","  (regression introduced in 0.21) :issue:`13910`","26","  by :user:`J¨¦r¨¦mie du Boisberranger <jeremiedbb>`."]}],"sklearn\/preprocessing\/_encoders.py":[{"add":["853","            if self.drop is not None:","854","                names.pop(self.drop_idx_[i])"],"delete":[]}],"sklearn\/preprocessing\/tests\/test_encoders.py":[{"add":["592","@pytest.mark.parametrize(\"drop, expected_names\",","593","                         [('first', ['x0_c', 'x2_b']),","594","                          (['c', 2, 'b'], ['x0_b', 'x2_a'])],","595","                         ids=['first', 'manual'])","596","def test_one_hot_encoder_feature_names_drop(drop, expected_names):","597","    X = [['c', 2, 'a'],","598","         ['b', 2, 'b']]","599","","600","    ohe = OneHotEncoder(drop=drop)","601","    ohe.fit(X)","602","    feature_names = ohe.get_feature_names()","603","    assert isinstance(feature_names, np.ndarray)","604","    assert_array_equal(expected_names, feature_names)","605","","606",""],"delete":[]}]}},"851a4b8ed1b2bcd4212680b20722326a18e640ab":{"changes":{"sklearn\/model_selection\/tests\/test_search.py":"MODIFY","sklearn\/model_selection\/_search.py":"MODIFY"},"diff":{"sklearn\/model_selection\/tests\/test_search.py":[{"add":["1727","","1728","","1729","def test_empty_cv_iterator_error():","1730","    # Use global X, y","1731","","1732","    # create cv","1733","    cv = KFold(n_splits=3).split(X)","1734","","1735","    # pop all of it, this should cause the expected ValueError","1736","    [u for u in cv]","1737","    # cv is empty now","1738","","1739","    train_size = 100","1740","    ridge = RandomizedSearchCV(Ridge(), {'alpha': [1e-3, 1e-2, 1e-1]},","1741","                               cv=cv, n_jobs=-1)","1742","","1743","    # assert that this raises an error","1744","    with pytest.raises(ValueError,","1745","                       match='No fits were performed. '","1746","                             'Was the CV iterator empty\\\\? '","1747","                             'Were there no candidates\\\\?'):","1748","        ridge.fit(X[:train_size], y[:train_size])","1749","","1750","","1751","def test_random_search_bad_cv():","1752","    # Use global X, y","1753","","1754","    class BrokenKFold(KFold):","1755","        def get_n_splits(self, *args, **kw):","1756","            return 1","1757","","1758","    # create bad cv","1759","    cv = BrokenKFold(n_splits=3)","1760","","1761","    train_size = 100","1762","    ridge = RandomizedSearchCV(Ridge(), {'alpha': [1e-3, 1e-2, 1e-1]},","1763","                               cv=cv, n_jobs=-1)","1764","","1765","    # assert that this raises an error","1766","    with pytest.raises(ValueError,","1767","                       match='cv.split and cv.get_n_splits returned '","1768","                             'inconsistent results. Expected \\\\d+ '","1769","                             'splits, got \\\\d+'):","1770","        ridge.fit(X[:train_size], y[:train_size])"],"delete":[]}],"sklearn\/model_selection\/_search.py":[{"add":["668","                if len(out) < 1:","669","                    raise ValueError('No fits were performed. '","670","                                     'Was the CV iterator empty? '","671","                                     'Were there no candidates?')","672","                elif len(out) != n_candidates * n_splits:","673","                    raise ValueError('cv.split and cv.get_n_splits returned '","674","                                     'inconsistent results. Expected {} '","675","                                     'splits, got {}'","676","                                     .format(n_splits,","677","                                             len(out) \/\/ n_candidates))","678",""],"delete":[]}]}},"2ad8735c500343d76339c1b06a247193bce06df8":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","doc\/modules\/preprocessing.rst":"MODIFY","sklearn\/preprocessing\/tests\/test_data.py":"MODIFY","sklearn\/preprocessing\/data.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["378","- |Fix| Fixed a bug in :class:`preprocessing.QuantileTransformer` and","379","  :func:`preprocessing.quantile_transform` to force n_quantiles to be at most","380","  equal to n_samples. Values of n_quantiles larger than n_samples were either","381","  useless or resulting in a wrong approximation of the cumulative distribution","382","  function estimator. :issue:`13333` by :user:`Albert Thomas <albertcthomas>`.","383",""],"delete":[]}],"doc\/modules\/preprocessing.rst":[{"add":["389","  array([[4.3, 2. , 1. , 0.1],","390","         [4.4, 2.2, 1.1, 0.1],","391","         [4.4, 2.2, 1.2, 0.1],","393","         [7.7, 4.1, 6.7, 2.5],","394","         [7.7, 4.2, 6.7, 2.5],","395","         [7.9, 4.4, 6.9, 2.5]])"],"delete":["389","  array([[4.3...,   2...,     1...,     0.1...],","390","         [4.31...,  2.02...,  1.01...,  0.1...],","391","         [4.32...,  2.05...,  1.02...,  0.1...],","393","         [7.84...,  4.34...,  6.84...,  2.5...],","394","         [7.87...,  4.37...,  6.87...,  2.5...],","395","         [7.9...,   4.4...,   6.9...,   2.5...]])"]}],"sklearn\/preprocessing\/tests\/test_data.py":[{"add":["1262","    # check that a warning is raised is n_quantiles > n_samples","1263","    transformer = QuantileTransformer(n_quantiles=100)","1264","    warn_msg = \"n_quantiles is set to n_samples\"","1265","    with pytest.warns(UserWarning, match=warn_msg) as record:","1266","        transformer.fit(X)","1267","    assert len(record) == 1","1268","    assert transformer.n_quantiles_ == X.shape[0]"],"delete":[]}],"sklearn\/preprocessing\/data.py":[{"add":["426","","594","","2043","    n_quantiles : int, optional (default=1000 or n_samples)","2046","        If n_quantiles is larger than the number of samples, n_quantiles is set","2047","        to the number of samples as a larger number of quantiles does not give","2048","        a better approximation of the cumulative distribution function","2049","        estimator.","2078","    n_quantiles_ : integer","2079","        The actual number of quantiles used to discretize the cumulative","2080","        distribution function.","2081","","2228","        n_samples = X.shape[0]","2229","","2230","        if self.n_quantiles > n_samples:","2231","            warnings.warn(\"n_quantiles (%s) is greater than the total number \"","2232","                          \"of samples (%s). n_quantiles is set to \"","2233","                          \"n_samples.\"","2234","                          % (self.n_quantiles, n_samples))","2235","        self.n_quantiles_ = max(1, min(self.n_quantiles, n_samples))","2236","","2240","        self.references_ = np.linspace(0, 1, self.n_quantiles_,","2462","    n_quantiles : int, optional (default=1000 or n_samples)","2465","        If n_quantiles is larger than the number of samples, n_quantiles is set","2466","        to the number of samples as a larger number of quantiles does not give","2467","        a better approximation of the cumulative distribution function","2468","        estimator."],"delete":["426"," ","594","    ","2043","    n_quantiles : int, optional (default=1000)","2223","        self.references_ = np.linspace(0, 1, self.n_quantiles,","2445","    n_quantiles : int, optional (default=1000)"]}]}},"bcdeadd7b8d17e0144e8d9ae10d778796ae26f5d":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/ensemble\/iforest.py":"MODIFY","sklearn\/neighbors\/tests\/test_lof.py":"MODIFY","sklearn\/utils\/estimator_checks.py":"MODIFY","sklearn\/ensemble\/tests\/test_iforest.py":"MODIFY","sklearn\/utils\/tests\/test_estimator_checks.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["111","- |Fix| Fixed the output of the average path length computed in","112","  :class:`ensemble.IsolationForest` when the input is either 0, 1 or 2.","113","  :issue:`13251` by :user:`Albert Thomas <albertcthomas>`","114","  and :user:`joshuakennethjones <joshuakennethjones>`.","115",""],"delete":[]}],"sklearn\/ensemble\/iforest.py":[{"add":["441","            return 0.","442","        elif n_samples_leaf <= 2:","454","        mask_1 = n_samples_leaf <= 1","455","        mask_2 = n_samples_leaf == 2","456","        not_mask = ~np.logical_or(mask_1, mask_2)","458","        average_path_length[mask_1] = 0.","459","        average_path_length[mask_2] = 1."],"delete":["452","        mask = (n_samples_leaf <= 1)","453","        not_mask = np.logical_not(mask)","455","        average_path_length[mask] = 1."]}],"sklearn\/neighbors\/tests\/test_lof.py":[{"add":["23","from sklearn.utils.estimator_checks import check_outlier_corruption","255","","256","","257","def test_predicted_outlier_number():","258","    # the number of predicted outliers should be equal to the number of","259","    # expected outliers unless there are ties in the abnormality scores.","260","    X = iris.data","261","    n_samples = X.shape[0]","262","    expected_outliers = 30","263","    contamination = float(expected_outliers)\/n_samples","264","","265","    clf = neighbors.LocalOutlierFactor(contamination=contamination)","266","    y_pred = clf.fit_predict(X)","267","","268","    num_outliers = np.sum(y_pred != 1)","269","    if num_outliers != expected_outliers:","270","        y_dec = clf.negative_outlier_factor_","271","        check_outlier_corruption(num_outliers, expected_outliers, y_dec)"],"delete":[]}],"sklearn\/utils\/estimator_checks.py":[{"add":["1526","def check_outlier_corruption(num_outliers, expected_outliers, decision):","1527","    # Check for deviation from the precise given contamination level that may","1528","    # be due to ties in the anomaly scores.","1529","    if num_outliers < expected_outliers:","1530","        start = num_outliers","1531","        end = expected_outliers + 1","1532","    else:","1533","        start = expected_outliers","1534","        end = num_outliers + 1","1535","","1536","    # ensure that all values in the 'critical area' are tied,","1537","    # leading to the observed discrepancy between provided","1538","    # and actual contamination levels.","1539","    sorted_decision = np.sort(decision)","1540","    msg = ('The number of predicted outliers is not equal to the expected '","1541","           'number of outliers and this difference is not explained by the '","1542","           'number of ties in the decision_function values')","1543","    assert len(np.unique(sorted_decision[start:end])) == 1, msg","1544","","1545","","1547","    n_samples = 300","1548","    X, _ = make_blobs(n_samples=n_samples, random_state=0)","1569","    scores = estimator.score_samples(X)","1570","    for output in [decision, scores]:","1571","        assert output.dtype == np.dtype('float')","1572","        assert output.shape == (n_samples,)","1586","    y_dec = scores - estimator.offset_","1599","        expected_outliers = 30","1600","        contamination = expected_outliers \/ n_samples","1604","","1605","        num_outliers = np.sum(y_pred != 1)","1606","        # num_outliers should be equal to expected_outliers unless","1607","        # there are ties in the decision_function values. this can","1608","        # only be tested for estimators with a decision_function","1609","        # method, i.e. all estimators except LOF which is already","1610","        # excluded from this if branch.","1611","        if num_outliers != expected_outliers:","1612","            decision = estimator.decision_function(X)","1613","            check_outlier_corruption(num_outliers, expected_outliers, decision)","2384","    n_samples = 300","2385","    X, _ = make_blobs(n_samples=n_samples, random_state=0)","2407","        expected_outliers = 30","2408","        contamination = float(expected_outliers)\/n_samples","2411","","2412","        num_outliers = np.sum(y_pred != 1)","2413","        # num_outliers should be equal to expected_outliers unless","2414","        # there are ties in the decision_function values. this can","2415","        # only be tested for estimators with a decision_function","2416","        # method","2417","        if (num_outliers != expected_outliers and","2418","                hasattr(estimator, 'decision_function')):","2419","            decision = estimator.decision_function(X)","2420","            check_outlier_corruption(num_outliers, expected_outliers, decision)"],"delete":["20","from sklearn.utils.testing import assert_almost_equal","1528","    X, _ = make_blobs(n_samples=300, random_state=0)","1549","    assert decision.dtype == np.dtype('float')","1550","","1551","    score = estimator.score_samples(X)","1552","    assert score.dtype == np.dtype('float')","1558","    decision = estimator.decision_function(X)","1559","    assert decision.shape == (n_samples,)","1568","    y_scores = estimator.score_samples(X)","1569","    assert y_scores.shape == (n_samples,)","1570","    y_dec = y_scores - estimator.offset_","1583","        contamination = 0.1","1587","        assert_almost_equal(np.mean(y_pred != 1), contamination)","2358","    X, _ = make_blobs(n_samples=300, random_state=0)","2380","        contamination = 0.1","2383","        assert_almost_equal(np.mean(y_pred != 1), contamination)"]}],"sklearn\/ensemble\/tests\/test_iforest.py":[{"add":["21","from sklearn.utils.testing import assert_allclose","265","    # Updated to check average path length when input is <= 2 (issue #11839)","269","    assert _average_path_length(0) == pytest.approx(0)","270","    assert _average_path_length(1) == pytest.approx(0)","271","    assert _average_path_length(2) == pytest.approx(1)","272","    assert_allclose(_average_path_length(5), result_one)","273","    assert_allclose(_average_path_length(999), result_two)","274","    assert_allclose(_average_path_length(np.array([1, 2, 5, 999])),","275","                    [0., 1., result_one, result_two])","276","","277","    # _average_path_length is increasing","278","    avg_path_length = _average_path_length(np.arange(5))","279","    assert_array_equal(avg_path_length, np.sort(avg_path_length))","280",""],"delete":["267","    assert_almost_equal(_average_path_length(1), 1., decimal=10)","268","    assert_almost_equal(_average_path_length(5), result_one, decimal=10)","269","    assert_almost_equal(_average_path_length(999), result_two, decimal=10)","270","    assert_array_almost_equal(_average_path_length(np.array([1, 5, 999])),","271","                              [1., result_one, result_two], decimal=10)"]}],"sklearn\/utils\/tests\/test_estimator_checks.py":[{"add":["11","from sklearn.utils.testing import (assert_raises_regex,","12","                                   assert_equal, ignore_warnings,","13","                                   assert_warns, assert_raises)","20","from sklearn.utils.estimator_checks import check_outlier_corruption","364","def test_check_outlier_corruption():","365","    # should raise AssertionError","366","    decision = np.array([0., 1., 1.5, 2.])","367","    assert_raises(AssertionError, check_outlier_corruption, 1, 2, decision)","368","    # should pass","369","    decision = np.array([0., 1., 1., 2.])","370","    check_outlier_corruption(1, 2, decision)","371","","372",""],"delete":["11","from sklearn.utils.testing import (assert_raises_regex, assert_equal,","12","                                   ignore_warnings, assert_warns)"]}]}},"1f75ffa75ff91603bf19c55c9f6718eae47f39b7":{"changes":{"sklearn\/datasets\/tests\/data\/openml\/2\/api-v1-json-data-qualities-2.json.gz":"ADD","sklearn\/datasets\/tests\/data\/openml\/61\/api-v1-json-data-qualities-61.json.gz":"ADD","sklearn\/datasets\/tests\/test_openml.py":"MODIFY","sklearn\/datasets\/tests\/data\/openml\/561\/api-v1-json-data-qualities-561.json.gz":"ADD","sklearn\/datasets\/tests\/data\/openml\/1119\/api-v1-json-data-qualities-1119.json.gz":"ADD","sklearn\/datasets\/tests\/data\/openml\/3\/api-v1-json-data-qualities-3.json.gz":"ADD","sklearn\/datasets\/tests\/data\/openml\/40589\/api-v1-json-data-qualities-40589.json.gz":"ADD","sklearn\/datasets\/tests\/data\/openml\/40675\/api-v1-json-data-qualities-40675.json.gz":"ADD","sklearn\/datasets\/tests\/data\/openml\/1\/api-v1-json-data-qualities-1.json.gz":"ADD","sklearn\/datasets\/tests\/data\/openml\/40966\/api-v1-json-data-qualities-40966.json.gz":"ADD","sklearn\/datasets\/openml.py":"MODIFY"},"diff":{"sklearn\/datasets\/tests\/data\/openml\/2\/api-v1-json-data-qualities-2.json.gz":[{"add":[],"delete":[]}],"sklearn\/datasets\/tests\/data\/openml\/61\/api-v1-json-data-qualities-61.json.gz":[{"add":[],"delete":[]}],"sklearn\/datasets\/tests\/test_openml.py":[{"add":["57","    data_downloaded = np.array(list(data_arff['data']), dtype='O')"],"delete":["57","    data_downloaded = np.array(data_arff['data'], dtype='O')"]}],"sklearn\/datasets\/tests\/data\/openml\/561\/api-v1-json-data-qualities-561.json.gz":[{"add":[],"delete":[]}],"sklearn\/datasets\/tests\/data\/openml\/1119\/api-v1-json-data-qualities-1119.json.gz":[{"add":[],"delete":[]}],"sklearn\/datasets\/tests\/data\/openml\/3\/api-v1-json-data-qualities-3.json.gz":[{"add":[],"delete":[]}],"sklearn\/datasets\/tests\/data\/openml\/40589\/api-v1-json-data-qualities-40589.json.gz":[{"add":[],"delete":[]}],"sklearn\/datasets\/tests\/data\/openml\/40675\/api-v1-json-data-qualities-40675.json.gz":[{"add":[],"delete":[]}],"sklearn\/datasets\/tests\/data\/openml\/1\/api-v1-json-data-qualities-1.json.gz":[{"add":[],"delete":[]}],"sklearn\/datasets\/tests\/data\/openml\/40966\/api-v1-json-data-qualities-40966.json.gz":[{"add":[],"delete":[]}],"sklearn\/datasets\/openml.py":[{"add":["8","import itertools","9","from collections.abc import Generator","27","_DATA_QUALITIES = \"api\/v1\/json\/data\/qualities\/{}\"","215","def _convert_arff_data(arff_data, col_slice_x, col_slice_y, shape=None):","239","    if isinstance(arff_data, Generator):","240","        if shape[0] == -1:","241","            count = -1","242","        else:","243","            count = shape[0] * shape[1]","244","        data = np.fromiter(itertools.chain.from_iterable(arff_data),","245","                           dtype='float64', count=count)","246","        data = data.reshape(*shape)","247","        X = data[:, col_slice_x]","248","        y = data[:, col_slice_y]","344","def _get_data_qualities(data_id, data_home):","345","    # OpenML API function:","346","    # https:\/\/www.openml.org\/api_docs#!\/data\/get_data_qualities_id","347","    url = _DATA_QUALITIES.format(data_id)","348","    error_message = \"Dataset with data_id {} not found.\".format(data_id)","349","    json_data = _get_json_content_from_openml_api(url, error_message, True,","350","                                                  data_home)","351","    try:","352","        return json_data['data_qualities']['quality']","353","    except KeyError:","354","        # the qualities might not be available, but we still try to process","355","        # the data","356","        return None","357","","358","","359","def _get_data_shape(data_qualities):","360","    # Using the data_info dictionary from _get_data_info_by_name to extract","361","    # the number of samples \/ features","362","    if data_qualities is None:","363","        return None","364","    qualities = {d['name']: d['value'] for d in data_qualities}","365","    try:","366","        return (int(float(qualities['NumberOfInstances'])),","367","                int(float(qualities['NumberOfFeatures'])))","368","    except AttributeError:","369","        return None","370","","371","","385","                return_type = _arff.DENSE_GEN","547","        if version != \"active\":","623","    if not return_sparse:","624","        data_qualities = _get_data_qualities(data_id, data_home)","625","        shape = _get_data_shape(data_qualities)","626","        # if the data qualities were not available, we can still get the","627","        # n_features from the feature list, with the n_samples unknown","628","        if shape is None:","629","            shape = (-1, len(features_list))","630","    else:","631","        shape = None","632","","636","","644","    X, y = _convert_arff_data(arff['data'], col_slice_x, col_slice_y, shape)"],"delete":["212","def _convert_arff_data(arff_data, col_slice_x, col_slice_y):","236","    if isinstance(arff_data, list):","237","        data = np.array(arff_data, dtype=np.float64)","238","        X = np.array(data[:, col_slice_x], dtype=np.float64)","239","        y = np.array(data[:, col_slice_y], dtype=np.float64)","348","                return_type = _arff.DENSE","510","        if version is not \"active\":","589","    arff_data = arff['data']","597","    X, y = _convert_arff_data(arff_data, col_slice_x, col_slice_y)"]}]}},"e747376eef58ab671243fbc463e6ef8bf342636c":{"changes":{"build_tools\/azure\/install.sh":"MODIFY"},"diff":{"build_tools\/azure\/install.sh":[{"add":["56","    sudo add-apt-repository --remove ppa:ubuntu-toolchain-r\/test"],"delete":[]}]}},"f9af18b4e5b9d4b379867d32381296062782dc15":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","doc\/whats_new\/_contributors.rst":"MODIFY","sklearn\/ensemble\/tests\/test_forest.py":"MODIFY","sklearn\/ensemble\/forest.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["196","- |Fix| The values of ``feature_importances_`` in all random forest based","197","  models (i.e.","198","  :class:`ensemble.RandomForestClassifier`,","199","  :class:`ensemble.RandomForestRegressor`,","200","  :class:`ensemble.ExtraTreesClassifier`,","201","  :class:`ensemble.ExtraTreesRegressor`, and","202","  :class:`ensemble.RandomTreesEmbedding`) now:","203","","204","  - sum up to ``1``","205","  - all the single node trees in feature importance calculation are ignored","206","  - in case all trees have only one single node (i.e. a root node),","207","    feature importances will be an array of all zeros.","208","  :issue:`13636` by `Adrin Jalali`_.","209",""],"delete":[]}],"doc\/whats_new\/_contributors.rst":[{"add":["176",".. _Nicolas Hug: https:\/\/github.com\/NicolasHug"],"delete":["176",".. _Nicolas Hug: https:\/\/github.com\/NicolasHug"]}],"sklearn\/ensemble\/tests\/test_forest.py":[{"add":["11","import math","45","from sklearn.datasets import make_classification","1363","","1364","","1365","def test_forest_feature_importances_sum():","1366","    X, y = make_classification(n_samples=15, n_informative=3, random_state=1,","1367","                               n_classes=3)","1368","    clf = RandomForestClassifier(min_samples_leaf=5, random_state=42,","1369","                                 n_estimators=200).fit(X, y)","1370","    assert math.isclose(1, clf.feature_importances_.sum(), abs_tol=1e-7)","1371","","1372","","1373","def test_forest_degenerate_feature_importances():","1374","    # build a forest of single node trees. See #13636","1375","    X = np.zeros((10, 10))","1376","    y = np.ones((10,))","1377","    gbr = RandomForestRegressor(n_estimators=10).fit(X, y)","1378","    assert_array_equal(gbr.feature_importances_,","1379","                       np.zeros(10, dtype=np.float64))"],"delete":[]}],"sklearn\/ensemble\/forest.py":[{"add":["368","            The values of this array sum to 1, unless all trees are single node","369","            trees consisting of only the root node, in which case it will be an","370","            array of zeros.","377","            for tree in self.estimators_ if tree.tree_.node_count > 1)","379","        if not all_importances:","380","            return np.zeros(self.n_features_, dtype=np.float64)","381","","382","        all_importances = np.mean(all_importances,","383","                                  axis=0, dtype=np.float64)","384","        return all_importances \/ np.sum(all_importances)"],"delete":["374","            for tree in self.estimators_)","376","        return sum(all_importances) \/ len(self.estimators_)"]}]}},"50946b3ef23641a3a38f9a758d34d5b556742e07":{"changes":{"sklearn\/model_selection\/_validation.py":"MODIFY","doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/model_selection\/tests\/test_validation.py":"MODIFY"},"diff":{"sklearn\/model_selection\/_validation.py":[{"add":["878","                              'predict_proba': 0.0}","881","                                                  default_values[method],","882","                                                  predictions.dtype)"],"delete":["878","                              'predict_proba': 0}","881","                                                  default_values[method])"]}],"doc\/whats_new\/v0.21.rst":[{"add":["321","- |Fix| Fixed an issue in :func:`~model_selection.cross_val_predict` where","322","  `method=\"predict_proba\"` returned always `0.0` when one of the classes was","323","  excluded in a cross-validation fold.","324","  :issue:`13366` by :user:`Guillaume Fournier <gfournier>`","325",""],"delete":[]}],"sklearn\/model_selection\/tests\/test_validation.py":[{"add":["977","@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22","978","@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22","979","def test_cross_val_predict_unbalanced():","980","    X, y = make_classification(n_samples=100, n_features=2, n_redundant=0,","981","                               n_informative=2, n_clusters_per_class=1,","982","                               random_state=1)","983","    # Change the first sample to a new class","984","    y[0] = 2","985","    clf = LogisticRegression(random_state=1)","986","    cv = StratifiedKFold(n_splits=2, random_state=1)","987","    train, test = list(cv.split(X, y))","988","    yhat_proba = cross_val_predict(clf, X, y, cv=cv, method=\"predict_proba\")","989","    assert y[test[0]][0] == 2  # sanity check for further assertions","990","    assert np.all(yhat_proba[test[0]][:, 2] == 0)","991","    assert np.all(yhat_proba[test[0]][:, 0:1] > 0)","992","    assert np.all(yhat_proba[test[1]] > 0)","993","    assert_array_almost_equal(yhat_proba.sum(axis=1), np.ones(y.shape),","994","                              decimal=12)","995","","996",""],"delete":[]}]}},"3a471242059187097f33f1539163dca9fbb0b21c":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/mixture\/tests\/test_gaussian_mixture.py":"MODIFY","sklearn\/mixture\/base.py":"MODIFY","sklearn\/mixture\/tests\/test_bayesian_mixture.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["307",":mod:`sklearn.mixture`","308","......................","309","","310","- |Fix| Fixed a bug in :class:`mixture.BaseMixture` and therefore on estimators","311","  based on it, i.e. :class:`mixture.GaussianMixture` and","312","  :class:`mixture.BayesianGaussianMixture`, where ``fit_predict`` and","313","  ``fit.predict`` were not equivalent. :issue:`13142` by","314","  :user:`J¨¦r¨¦mie du Boisberranger <jeremiedbb>`.","315",""],"delete":[]}],"sklearn\/mixture\/tests\/test_gaussian_mixture.py":[{"add":["600","def test_gaussian_mixture_fit_predict_n_init():","601","    # Check that fit_predict is equivalent to fit.predict, when n_init > 1","602","    X = np.random.RandomState(0).randn(1000, 5)","603","    gm = GaussianMixture(n_components=5, n_init=5, random_state=0)","604","    y_pred1 = gm.fit_predict(X)","605","    y_pred2 = gm.predict(X)","606","    assert_array_equal(y_pred1, y_pred2)","607","","608",""],"delete":[]}],"sklearn\/mixture\/base.py":[{"add":["270","        # Always do a final e-step to guarantee that the labels returned by","271","        # fit_predict(X) are always consistent with fit(X).predict(X)","272","        # for any value of max_iter and tol (and any random_state).","273","        _, log_resp = self._e_step(X)","274",""],"delete":["259","        # Always do a final e-step to guarantee that the labels returned by","260","        # fit_predict(X) are always consistent with fit(X).predict(X)","261","        # for any value of max_iter and tol (and any random_state).","262","        _, log_resp = self._e_step(X)","263",""]}],"sklearn\/mixture\/tests\/test_bayesian_mixture.py":[{"add":["453","def test_bayesian_mixture_fit_predict_n_init():","454","    # Check that fit_predict is equivalent to fit.predict, when n_init > 1","455","    X = np.random.RandomState(0).randn(1000, 5)","456","    gm = BayesianGaussianMixture(n_components=5, n_init=10, random_state=0)","457","    y_pred1 = gm.fit_predict(X)","458","    y_pred2 = gm.predict(X)","459","    assert_array_equal(y_pred1, y_pred2)","460","","461",""],"delete":[]}]}},"f02ef9f52f81c2d212f428092ad7c3f2f3fbd0f5":{"changes":{"sklearn\/linear_model\/setup.py":"MODIFY","sklearn\/utils\/seq_dataset.pxd.tp":"ADD",".gitignore":"MODIFY","sklearn\/linear_model\/tests\/test_logistic.py":"MODIFY","sklearn\/linear_model\/logistic.py":"MODIFY","sklearn\/linear_model\/sag_fast.pyx.tp":"ADD","sklearn\/linear_model\/tests\/test_base.py":"MODIFY","sklearn\/linear_model\/sag.py":"MODIFY","sklearn\/utils\/seq_dataset.pyx.tp":"ADD","sklearn\/utils\/setup.py":"MODIFY","sklearn\/linear_model\/base.py":"MODIFY","sklearn\/utils\/tests\/test_seq_dataset.py":"MODIFY","\/dev\/null":"DELETE","doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/linear_model\/sgd_fast_helpers.h":"MODIFY","benchmarks\/bench_saga.py":"MODIFY","sklearn\/linear_model\/sgd_fast.pyx":"MODIFY"},"diff":{"sklearn\/linear_model\/setup.py":[{"add":["4","from Cython import Tempita","25","    # generate sag_fast from template","26","    sag_cython_file = 'sklearn\/linear_model\/sag_fast.pyx.tp'","27","    sag_file = sag_cython_file.replace('.tp', '')","28","","29","    if not (os.path.exists(sag_file) and","30","            os.stat(sag_cython_file).st_mtime < os.stat(sag_file).st_mtime):","31","","32","        with open(sag_cython_file, \"r\") as f:","33","            tmpl = f.read()","34","        tmpl_ = Tempita.sub(tmpl)","35","","36","        with open(sag_file, \"w\") as f:","37","            f.write(tmpl_)","38",""],"delete":[]}],"sklearn\/utils\/seq_dataset.pxd.tp":[{"add":[],"delete":[]}],".gitignore":[{"add":["73","","74","# files generated from a template","75","sklearn\/utils\/seq_dataset.pyx","76","sklearn\/utils\/seq_dataset.pxd","77","sklearn\/linear_model\/sag_fast.pyx"],"delete":[]}],"sklearn\/linear_model\/tests\/test_logistic.py":[{"add":["1361","@pytest.mark.parametrize('solver', ['newton-cg', 'saga'])","1362","def test_dtype_match(solver, multi_class):"],"delete":["1361","def test_dtype_match(multi_class):","1370","    solver = 'newton-cg'","1371",""]}],"sklearn\/linear_model\/logistic.py":[{"add":["966","                target = target.astype(X.dtype, copy=False)","1488","","1489","        Notes","1490","        -----","1491","        The SAGA solver supports both float64 and float32 bit arrays.","1526","        if solver in ['lbfgs', 'liblinear']:","1528","        else:","1529","            _dtype = [np.float64, np.float32]"],"delete":["966","                target = target.astype(np.float64)","1522","        if solver in ['newton-cg']:","1523","            _dtype = [np.float64, np.float32]","1524","        else:"]}],"sklearn\/linear_model\/sag_fast.pyx.tp":[{"add":[],"delete":[]}],"sklearn\/linear_model\/tests\/test_base.py":[{"add":["12","from sklearn.utils.testing import assert_array_equal","15","from sklearn.utils.testing import assert_allclose","20","from sklearn.linear_model.base import make_dataset","25","from sklearn.datasets import load_iris","28","rtol = 1e-6","429","","430","","431","def test_fused_types_make_dataset():","432","    iris = load_iris()","433","","434","    X_32 = iris.data.astype(np.float32)","435","    y_32 = iris.target.astype(np.float32)","436","    X_csr_32 = sparse.csr_matrix(X_32)","437","    sample_weight_32 = np.arange(y_32.size, dtype=np.float32)","438","","439","    X_64 = iris.data.astype(np.float64)","440","    y_64 = iris.target.astype(np.float64)","441","    X_csr_64 = sparse.csr_matrix(X_64)","442","    sample_weight_64 = np.arange(y_64.size, dtype=np.float64)","443","","444","    # array","445","    dataset_32, _ = make_dataset(X_32, y_32, sample_weight_32)","446","    dataset_64, _ = make_dataset(X_64, y_64, sample_weight_64)","447","    xi_32, yi_32, _, _ = dataset_32._next_py()","448","    xi_64, yi_64, _, _ = dataset_64._next_py()","449","    xi_data_32, _, _ = xi_32","450","    xi_data_64, _, _ = xi_64","451","","452","    assert xi_data_32.dtype == np.float32","453","    assert xi_data_64.dtype == np.float64","454","    assert_allclose(yi_64, yi_32, rtol=rtol)","455","","456","    # csr","457","    datasetcsr_32, _ = make_dataset(X_csr_32, y_32, sample_weight_32)","458","    datasetcsr_64, _ = make_dataset(X_csr_64, y_64, sample_weight_64)","459","    xicsr_32, yicsr_32, _, _ = datasetcsr_32._next_py()","460","    xicsr_64, yicsr_64, _, _ = datasetcsr_64._next_py()","461","    xicsr_data_32, _, _ = xicsr_32","462","    xicsr_data_64, _, _ = xicsr_64","463","","464","    assert xicsr_data_32.dtype == np.float32","465","    assert xicsr_data_64.dtype == np.float64","466","","467","    assert_allclose(xicsr_data_64, xicsr_data_32, rtol=rtol)","468","    assert_allclose(yicsr_64, yicsr_32, rtol=rtol)","469","","470","    assert_array_equal(xi_data_32, xicsr_data_32)","471","    assert_array_equal(xi_data_64, xicsr_data_64)","472","    assert_array_equal(yi_32, yicsr_32)","473","    assert_array_equal(yi_64, yicsr_64)"],"delete":["11",""]}],"sklearn\/linear_model\/sag.py":[{"add":["11","from .sag_fast import sag32, sag64","247","        _dtype = [np.float64, np.float32]","248","        X = check_array(X, dtype=_dtype, accept_sparse='csr', order='C')","249","        y = check_array(y, dtype=_dtype, ensure_2d=False, order='C')","261","        sample_weight = np.ones(n_samples, dtype=X.dtype, order='C')","267","        coef_init = np.zeros((n_features, n_classes), dtype=X.dtype,","277","        intercept_init = np.zeros(n_classes, dtype=X.dtype)","282","        intercept_sum_gradient = np.zeros(n_classes, dtype=X.dtype)","288","                                        dtype=X.dtype, order='C')","293","                                     dtype=X.dtype, order='C')","316","    sag = sag64 if X.dtype == np.float64 else sag32","333",""],"delete":["11","from .sag_fast import sag","247","        X = check_array(X, dtype=np.float64, accept_sparse='csr', order='C')","248","        y = check_array(y, dtype=np.float64, ensure_2d=False, order='C')","260","        sample_weight = np.ones(n_samples, dtype=np.float64, order='C')","266","        coef_init = np.zeros((n_features, n_classes), dtype=np.float64,","276","        intercept_init = np.zeros(n_classes, dtype=np.float64)","281","        intercept_sum_gradient = np.zeros(n_classes, dtype=np.float64)","287","                                        dtype=np.float64, order='C')","292","                                     dtype=np.float64, order='C')"]}],"sklearn\/utils\/seq_dataset.pyx.tp":[{"add":[],"delete":[]}],"sklearn\/utils\/setup.py":[{"add":["7","    from Cython import Tempita","48","    # generate files from a template","49","    pyx_templates = ['sklearn\/utils\/seq_dataset.pyx.tp',","50","                     'sklearn\/utils\/seq_dataset.pxd.tp']","51","","52","    for pyxfiles in pyx_templates:","53","        outfile = pyxfiles.replace('.tp', '')","54","        # if .pyx.tp is not updated, no need to output .pyx","55","        if (os.path.exists(outfile) and","56","                os.stat(pyxfiles).st_mtime < os.stat(outfile).st_mtime):","57","            continue","58","","59","        with open(pyxfiles, \"r\") as f:","60","            tmpl = f.read()","61","        pyxcontent = Tempita.sub(tmpl)","62","","63","        with open(outfile, \"w\") as f:","64","            f.write(pyxcontent)","65",""],"delete":[]}],"sklearn\/linear_model\/base.py":[{"add":["34","from ..utils.seq_dataset import ArrayDataset32, CSRDataset32","35","from ..utils.seq_dataset import ArrayDataset64, CSRDataset64","79","    # seed should never be 0 in SequentialDataset64","82","    if X.dtype == np.float32:","83","        CSRData = CSRDataset32","84","        ArrayData = ArrayDataset32","85","    else:","86","        CSRData = CSRDataset64","87","        ArrayData = ArrayDataset64","88","","90","        dataset = CSRData(X.data, X.indptr, X.indices, y, sample_weight,","91","                          seed=seed)","94","        dataset = ArrayData(X, y, sample_weight, seed=seed)"],"delete":["34","from ..utils.seq_dataset import ArrayDataset, CSRDataset","78","    # seed should never be 0 in SequentialDataset","82","        dataset = CSRDataset(X.data, X.indptr, X.indices, y, sample_weight,","83","                             seed=seed)","86","        dataset = ArrayDataset(X, y, sample_weight, seed=seed)"]}],"sklearn\/utils\/tests\/test_seq_dataset.py":[{"add":["1","#         Joan Massich <mailsik@gmail.com>","5","import pytest","8","from sklearn.utils.testing import assert_allclose","11","from sklearn.utils.seq_dataset import ArrayDataset64","12","from sklearn.utils.seq_dataset import ArrayDataset32","13","from sklearn.utils.seq_dataset import CSRDataset64","14","from sklearn.utils.seq_dataset import CSRDataset32","15","","19","X64 = iris.data.astype(np.float64)","20","y64 = iris.target.astype(np.float64)","21","X_csr64 = sp.csr_matrix(X64)","22","sample_weight64 = np.arange(y64.size, dtype=np.float64)","23","","24","X32 = iris.data.astype(np.float32)","25","y32 = iris.target.astype(np.float32)","26","X_csr32 = sp.csr_matrix(X32)","27","sample_weight32 = np.arange(y32.size, dtype=np.float32)","30","def assert_csr_equal_values(current, expected):","31","    current.eliminate_zeros()","32","    expected.eliminate_zeros()","33","    expected = expected.astype(current.dtype)","34","    assert current.shape[0] == expected.shape[0]","35","    assert current.shape[1] == expected.shape[1]","36","    assert_array_equal(current.data, expected.data)","37","    assert_array_equal(current.indices, expected.indices)","38","    assert_array_equal(current.indptr, expected.indptr)","41","def make_dense_dataset_32():","42","    return ArrayDataset32(X32, y32, sample_weight32, seed=42)","45","def make_dense_dataset_64():","46","    return ArrayDataset64(X64, y64, sample_weight64, seed=42)","48","","49","def make_sparse_dataset_32():","50","    return CSRDataset32(X_csr32.data, X_csr32.indptr, X_csr32.indices, y32,","51","                        sample_weight32, seed=42)","52","","53","","54","def make_sparse_dataset_64():","55","    return CSRDataset64(X_csr64.data, X_csr64.indptr, X_csr64.indices, y64,","56","                        sample_weight64, seed=42)","57","","58","","59","@pytest.mark.parametrize('dataset_constructor', [","60","    make_dense_dataset_32,","61","    make_dense_dataset_64,","62","    make_sparse_dataset_32,","63","    make_sparse_dataset_64,","64","])","65","def test_seq_dataset_basic_iteration(dataset_constructor):","66","    NUMBER_OF_RUNS = 5","67","    dataset = dataset_constructor()","68","    for _ in range(NUMBER_OF_RUNS):","69","        # next sample","70","        xi_, yi, swi, idx = dataset._next_py()","71","        xi = sp.csr_matrix((xi_), shape=(1, X64.shape[1]))","72","","73","        assert_csr_equal_values(xi, X_csr64[idx])","74","        assert yi == y64[idx]","75","        assert swi == sample_weight64[idx]","76","","77","        # random sample","78","        xi_, yi, swi, idx = dataset._random_py()","79","        xi = sp.csr_matrix((xi_), shape=(1, X64.shape[1]))","80","","81","        assert_csr_equal_values(xi, X_csr64[idx])","82","        assert yi == y64[idx]","83","        assert swi == sample_weight64[idx]","84","","85","","86","@pytest.mark.parametrize('make_dense_dataset,make_sparse_dataset', [","87","    (make_dense_dataset_32, make_sparse_dataset_32),","88","    (make_dense_dataset_64, make_sparse_dataset_64),","89","])","90","def test_seq_dataset_shuffle(make_dense_dataset, make_sparse_dataset):","91","    dense_dataset, sparse_dataset = make_dense_dataset(), make_sparse_dataset()","94","        _, _, _, idx1 = dense_dataset._next_py()","95","        _, _, _, idx2 = sparse_dataset._next_py()","96","        assert idx1 == i","97","        assert idx2 == i","100","        _, _, _, idx1 = dense_dataset._random_py()","101","        _, _, _, idx2 = sparse_dataset._random_py()","102","        assert idx1 == idx2","105","    dense_dataset._shuffle_py(seed)","106","    sparse_dataset._shuffle_py(seed)","109","        _, _, _, idx1 = dense_dataset._next_py()","110","        _, _, _, idx2 = sparse_dataset._next_py()","111","        assert idx1 == idx2","113","        _, _, _, idx1 = dense_dataset._random_py()","114","        _, _, _, idx2 = sparse_dataset._random_py()","115","        assert idx1 == idx2","116","","117","","118","@pytest.mark.parametrize('make_dataset_32,make_dataset_64', [","119","    (make_dense_dataset_32, make_dense_dataset_64),","120","    (make_sparse_dataset_32, make_sparse_dataset_64),","121","])","122","def test_fused_types_consistency(make_dataset_32, make_dataset_64):","123","    dataset_32, dataset_64 = make_dataset_32(), make_dataset_64()","124","    NUMBER_OF_RUNS = 5","125","    for _ in range(NUMBER_OF_RUNS):","126","        # next sample","127","        (xi_data32, _, _), yi32, _, _ = dataset_32._next_py()","128","        (xi_data64, _, _), yi64, _, _ = dataset_64._next_py()","129","","130","        assert xi_data32.dtype == np.float32","131","        assert xi_data64.dtype == np.float64","132","","133","        assert_allclose(xi_data64, xi_data32, rtol=1e-5)","134","        assert_allclose(yi64, yi32, rtol=1e-5)","135","","136","","137","def test_buffer_dtype_mismatch_error():","138","    with pytest.raises(ValueError, match='Buffer dtype mismatch'):","139","        ArrayDataset64(X32, y32, sample_weight32, seed=42),","140","","141","    with pytest.raises(ValueError, match='Buffer dtype mismatch'):","142","        ArrayDataset32(X64, y64, sample_weight64, seed=42),","143","","144","    with pytest.raises(ValueError, match='Buffer dtype mismatch'):","145","        CSRDataset64(X_csr32.data, X_csr32.indptr, X_csr32.indices, y32,","146","                     sample_weight32, seed=42),","147","","148","    with pytest.raises(ValueError, match='Buffer dtype mismatch'):","149","        CSRDataset32(X_csr64.data, X_csr64.indptr, X_csr64.indices, y64,","150","                     sample_weight64, seed=42),"],"delete":["8","from sklearn.utils.seq_dataset import ArrayDataset, CSRDataset","11","from sklearn.utils.testing import assert_equal","12","","14","X = iris.data.astype(np.float64)","15","y = iris.target.astype(np.float64)","16","X_csr = sp.csr_matrix(X)","17","sample_weight = np.arange(y.size, dtype=np.float64)","20","def assert_csr_equal(X, Y):","21","    X.eliminate_zeros()","22","    Y.eliminate_zeros()","23","    assert_equal(X.shape[0], Y.shape[0])","24","    assert_equal(X.shape[1], Y.shape[1])","25","    assert_array_equal(X.data, Y.data)","26","    assert_array_equal(X.indices, Y.indices)","27","    assert_array_equal(X.indptr, Y.indptr)","30","def test_seq_dataset():","31","    dataset1 = ArrayDataset(X, y, sample_weight, seed=42)","32","    dataset2 = CSRDataset(X_csr.data, X_csr.indptr, X_csr.indices,","33","                          y, sample_weight, seed=42)","34","","35","    for dataset in (dataset1, dataset2):","36","        for i in range(5):","37","            # next sample","38","            xi_, yi, swi, idx = dataset._next_py()","39","            xi = sp.csr_matrix((xi_), shape=(1, X.shape[1]))","40","","41","            assert_csr_equal(xi, X_csr[idx])","42","            assert_equal(yi, y[idx])","43","            assert_equal(swi, sample_weight[idx])","44","","45","            # random sample","46","            xi_, yi, swi, idx = dataset._random_py()","47","            xi = sp.csr_matrix((xi_), shape=(1, X.shape[1]))","48","","49","            assert_csr_equal(xi, X_csr[idx])","50","            assert_equal(yi, y[idx])","51","            assert_equal(swi, sample_weight[idx])","54","def test_seq_dataset_shuffle():","55","    dataset1 = ArrayDataset(X, y, sample_weight, seed=42)","56","    dataset2 = CSRDataset(X_csr.data, X_csr.indptr, X_csr.indices,","57","                          y, sample_weight, seed=42)","61","        _, _, _, idx1 = dataset1._next_py()","62","        _, _, _, idx2 = dataset2._next_py()","63","        assert_equal(idx1, i)","64","        assert_equal(idx2, i)","67","        _, _, _, idx1 = dataset1._random_py()","68","        _, _, _, idx2 = dataset2._random_py()","69","        assert_equal(idx1, idx2)","72","    dataset1._shuffle_py(seed)","73","    dataset2._shuffle_py(seed)","76","        _, _, _, idx1 = dataset1._next_py()","77","        _, _, _, idx2 = dataset2._next_py()","78","        assert_equal(idx1, idx2)","80","        _, _, _, idx1 = dataset1._random_py()","81","        _, _, _, idx2 = dataset2._random_py()","82","        assert_equal(idx1, idx2)"]}],"\/dev\/null":[{"add":[],"delete":[]}],"doc\/whats_new\/v0.21.rst":[{"add":["164","- |Enhancement| :class:`linear_model.make_dataset` now preserves","165","  ``float32`` and ``float64`` dtypes. :issues:`8769` and :issues:`11000` by","166","  :user:`Nelle Varoquaux`_, :user:`Arthur Imbert <Henley13>`,","167","  :user:`Guillaume Lemaitre <glemaitre>`, and :user:`Joan Massich <massich>`","168",""],"delete":[]}],"sklearn\/linear_model\/sgd_fast_helpers.h":[{"add":["2","\/\/ When re-declaring the functions in the template for cython","3","\/\/ specific for each parameter input type, it needs to be 2 different functions","4","\/\/ as cython doesn't support function overloading.","8","# define skl_isfinite32 _finite","9","# define skl_isfinite64 _finite","13","# define skl_isfinite32 npy_isfinite","14","# define skl_isfinite64 npy_isfinite"],"delete":[]}],"benchmarks\/bench_saga.py":[{"add":["0","\"\"\"Author: Arthur Mensch, Nelle Varoquaux","7","import os","23","               max_iter=10, skip_slow=False, dtype=np.float64):","39","    X = X.astype(dtype)","40","    y = y.astype(dtype)","72","                                    fit_intercept=False, tol=0,","76","","77","        # Makes cpu cache even for all fit calls","78","        X_train.max()","80","","113","def exp(solvers, penalty, single_target,","114","        n_samples=30000, max_iter=20,","116","    dtypes_mapping = {","117","        \"float64\": np.float64,","118","        \"float32\": np.float32,","119","    }","163","        delayed(fit_single)(solver, X, y,","165","                            dtype=dtype,","168","        for dtype in dtypes_mapping.values())","172","    for dtype_name in dtypes_mapping.keys():","173","        for solver in solvers:","174","            if not (skip_slow and","175","                    solver == 'lightning' and","176","                    penalty == 'l1'):","179","                                dtype=dtype_name,","191","def plot(outname=None):","196","    res.set_index(['single_target'], inplace=True)","198","    grouped = res.groupby(level=['single_target'])","200","    colors = {'saga': 'C0', 'liblinear': 'C1', 'lightning': 'C2'}","201","    linestyles = {\"float32\": \"--\", \"float64\": \"-\"}","202","    alpha = {\"float64\": 0.5, \"float32\": 1}","205","        single_target = idx","206","        fig, axes = plt.subplots(figsize=(12, 4), ncols=4)","207","        ax = axes[0]","209","        for scores, times, solver, dtype in zip(group['train_scores'],","210","                                                group['times'],","211","                                                group['solver'],","212","                                                group[\"dtype\"]):","213","            ax.plot(times, scores, label=\"%s - %s\" % (solver, dtype),","214","                    color=colors[solver],","215","                    alpha=alpha[dtype],","216","                    marker=\".\",","217","                    linestyle=linestyles[dtype])","218","            ax.axvline(times[-1], color=colors[solver],","219","                       alpha=alpha[dtype],","220","                       linestyle=linestyles[dtype])","225","        ax = axes[1]","227","        for scores, times, solver, dtype in zip(group['test_scores'],","228","                                                group['times'],","229","                                                group['solver'],","230","                                                group[\"dtype\"]):","231","            ax.plot(times, scores, label=solver, color=colors[solver],","232","                    linestyle=linestyles[dtype],","233","                    marker=\".\",","234","                    alpha=alpha[dtype])","235","            ax.axvline(times[-1], color=colors[solver],","236","                       alpha=alpha[dtype],","237","                       linestyle=linestyles[dtype])","243","        ax = axes[2]","244","        for accuracy, times, solver, dtype in zip(group['accuracies'],","245","                                                  group['times'],","246","                                                  group['solver'],","247","                                                  group[\"dtype\"]):","248","            ax.plot(times, accuracy, label=\"%s - %s\" % (solver, dtype),","249","                    alpha=alpha[dtype],","250","                    marker=\".\",","251","                    color=colors[solver], linestyle=linestyles[dtype])","252","            ax.axvline(times[-1], color=colors[solver],","253","                       alpha=alpha[dtype],","254","                       linestyle=linestyles[dtype])","262","        if outname is None:","263","            outname = name + '.png'","266","","267","        ax = axes[3]","268","        for scores, times, solver, dtype in zip(group['train_scores'],","269","                                                group['times'],","270","                                                group['solver'],","271","                                                group[\"dtype\"]):","272","            ax.plot(np.arange(len(scores)),","273","                    scores, label=\"%s - %s\" % (solver, dtype),","274","                    marker=\".\",","275","                    alpha=alpha[dtype],","276","                    color=colors[solver], linestyle=linestyles[dtype])","277","","278","        ax.set_yscale(\"log\")","279","        ax.set_xlabel('# iterations')","280","        ax.set_ylabel('Objective function')","281","        ax.legend()","282","","283","        plt.savefig(outname)","289","    n_samples = [100000, 300000, 500000, 800000, None]","291","    for penalty in penalties:","292","        for n_sample in n_samples:","293","            exp(solvers, penalty, single_target,","294","                n_samples=n_sample, n_jobs=1,","295","                dataset='rcv1', max_iter=10)","296","            if n_sample is not None:","297","                outname = \"figures\/saga_%s_%d.png\" % (penalty, n_sample)","298","            else:","299","                outname = \"figures\/saga_%s_all.png\" % (penalty,)","300","            try:","301","                os.makedirs(\"figures\")","302","            except OSError:","303","                pass","304","            plot(outname)"],"delete":["0","\"\"\"Author: Arthur Mensch","7","from os.path import expanduser","23","               max_iter=10, skip_slow=False):","39","","71","                                    fit_intercept=False, tol=1e-24,","108","def exp(solvers, penalties, single_target, n_samples=30000, max_iter=20,","110","    mem = Memory(cachedir=expanduser('~\/cache'), verbose=0)","153","    cached_fit = mem.cache(fit_single)","155","        delayed(cached_fit)(solver, X, y,","159","        for penalty in penalties)","163","    for solver in solvers:","164","        for penalty in penalties:","165","            if not (skip_slow and solver == 'lightning' and penalty == 'l1'):","179","def plot():","184","    res.set_index(['single_target', 'penalty'], inplace=True)","186","    grouped = res.groupby(level=['single_target', 'penalty'])","188","    colors = {'saga': 'blue', 'liblinear': 'orange', 'lightning': 'green'}","191","        single_target, penalty = idx","192","        fig = plt.figure(figsize=(12, 4))","193","        ax = fig.add_subplot(131)","195","        train_scores = group['train_scores'].values","196","        ref = np.min(np.concatenate(train_scores)) * 0.999","197","","198","        for scores, times, solver in zip(group['train_scores'], group['times'],","199","                                         group['solver']):","200","            scores = scores \/ ref - 1","201","            ax.plot(times, scores, label=solver, color=colors[solver])","206","        ax = fig.add_subplot(132)","208","        test_scores = group['test_scores'].values","209","        ref = np.min(np.concatenate(test_scores)) * 0.999","211","        for scores, times, solver in zip(group['test_scores'], group['times'],","212","                                         group['solver']):","213","            scores = scores \/ ref - 1","214","            ax.plot(times, scores, label=solver, color=colors[solver])","219","        ax = fig.add_subplot(133)","221","        for accuracy, times, solver in zip(group['accuracies'], group['times'],","222","                                           group['solver']):","223","            ax.plot(times, accuracy, label=solver, color=colors[solver])","230","        name += '.png'","233","        plt.savefig(name)","234","        plt.close(fig)","241","    exp(solvers, penalties, single_target, n_samples=None, n_jobs=1,","242","        dataset='20newspaper', max_iter=20)","243","    plot()"]}],"sklearn\/linear_model\/sgd_fast.pyx":[{"add":["24","from sklearn.utils.seq_dataset cimport SequentialDataset64 as SequentialDataset","510","    dataset : SequentialDataset","511","        A concrete ``SequentialDataset`` object."],"delete":["24","from sklearn.utils.seq_dataset cimport SequentialDataset"]}]}},"1015caf54df347c00152c67adaf1851c5771f0a0":{"changes":{"sklearn\/neighbors\/tests\/test_neighbors.py":"MODIFY","sklearn\/tests\/test_site_joblib.py":"MODIFY","sklearn\/utils\/testing.py":"MODIFY","sklearn\/covariance\/graph_lasso_.py":"MODIFY","sklearn\/datasets\/rcv1.py":"MODIFY","sklearn\/multioutput.py":"MODIFY","sklearn\/ensemble\/base.py":"MODIFY","sklearn\/model_selection\/_search.py":"MODIFY","sklearn\/datasets\/species_distributions.py":"MODIFY","sklearn\/datasets\/california_housing.py":"MODIFY","sklearn\/datasets\/olivetti_faces.py":"MODIFY","sklearn\/linear_model\/logistic.py":"MODIFY","sklearn\/ensemble\/tests\/test_forest.py":"MODIFY","sklearn\/linear_model\/omp.py":"MODIFY","sklearn\/cluster\/mean_shift_.py":"MODIFY","sklearn\/ensemble\/forest.py":"MODIFY","sklearn\/metrics\/tests\/test_score_objects.py":"MODIFY","sklearn\/tests\/test_pipeline.py":"MODIFY","sklearn\/datasets\/covtype.py":"MODIFY","sklearn\/metrics\/pairwise.py":"MODIFY","sklearn\/utils\/validation.py":"MODIFY","sklearn\/neighbors\/base.py":"MODIFY","sklearn\/pipeline.py":"MODIFY","sklearn\/linear_model\/tests\/test_sgd.py":"MODIFY","sklearn\/multiclass.py":"MODIFY","sklearn\/compose\/_column_transformer.py":"MODIFY","sklearn\/datasets\/lfw.py":"MODIFY","sklearn\/utils\/fixes.py":"MODIFY","sklearn\/utils\/tests\/test_fixes.py":"MODIFY","sklearn\/linear_model\/coordinate_descent.py":"MODIFY","sklearn\/decomposition\/online_lda.py":"MODIFY","sklearn\/manifold\/mds.py":"MODIFY","sklearn\/datasets\/kddcup99.py":"MODIFY","sklearn\/neighbors\/tests\/test_kde.py":"MODIFY","sklearn\/utils\/tests\/test_estimator_checks.py":"MODIFY","sklearn\/ensemble\/partial_dependence.py":"MODIFY","sklearn\/ensemble\/voting.py":"MODIFY","sklearn\/feature_selection\/rfe.py":"MODIFY","sklearn\/linear_model\/theil_sen.py":"MODIFY","sklearn\/datasets\/twenty_newsgroups.py":"MODIFY","sklearn\/utils\/estimator_checks.py":"MODIFY","sklearn\/decomposition\/tests\/test_sparse_pca.py":"MODIFY","sklearn\/tests\/test_multioutput.py":"MODIFY","sklearn\/linear_model\/base.py":"MODIFY","sklearn\/ensemble\/bagging.py":"MODIFY","sklearn\/ensemble\/tests\/test_bagging.py":"MODIFY","sklearn\/linear_model\/stochastic_gradient.py":"MODIFY","sklearn\/model_selection\/_validation.py":"MODIFY","sklearn\/decomposition\/dict_learning.py":"MODIFY","sklearn\/cluster\/k_means_.py":"MODIFY","sklearn\/linear_model\/least_angle.py":"MODIFY"},"diff":{"sklearn\/neighbors\/tests\/test_neighbors.py":[{"add":["29","import joblib","1352","    with joblib.parallel_backend(backend):"],"delete":["29","from sklearn.utils._joblib import joblib","30","from sklearn.utils._joblib import parallel_backend","1353","    with parallel_backend(backend):"]}],"sklearn\/tests\/test_site_joblib.py":[{"add":[],"delete":["0","from sklearn.utils._joblib import Parallel, delayed  # noqa","1","from sklearn.utils._joblib import Memory, parallel_backend # noqa"]}],"sklearn\/utils\/testing.py":[{"add":["45","import joblib"],"delete":["50","from sklearn.utils._joblib import joblib"]}],"sklearn\/covariance\/graph_lasso_.py":[{"add":["15","from joblib import Parallel, delayed"],"delete":["24","from ..utils._joblib import Parallel, delayed"]}],"sklearn\/datasets\/rcv1.py":[{"add":["18","import joblib","183","        joblib.dump(X, samples_path, compress=9)","184","        joblib.dump(sample_id, sample_id_path, compress=9)","191","        X = joblib.load(samples_path)","192","        sample_id = joblib.load(sample_id_path)","242","        joblib.dump(y, sample_topics_path, compress=9)","243","        joblib.dump(categories, topics_path, compress=9)","245","        y = joblib.load(sample_topics_path)","246","        categories = joblib.load(topics_path)"],"delete":["23","from ..utils import _joblib","183","        _joblib.dump(X, samples_path, compress=9)","184","        _joblib.dump(sample_id, sample_id_path, compress=9)","191","        X = _joblib.load(samples_path)","192","        sample_id = _joblib.load(sample_id_path)","242","        _joblib.dump(y, sample_topics_path, compress=9)","243","        _joblib.dump(categories, topics_path, compress=9)","245","        y = _joblib.load(sample_topics_path)","246","        categories = _joblib.load(topics_path)"]}],"sklearn\/multioutput.py":[{"add":["18","from joblib import Parallel, delayed","19",""],"delete":["27","from .utils._joblib import Parallel, delayed"]}],"sklearn\/ensemble\/base.py":[{"add":["10","from joblib import effective_n_jobs","11",""],"delete":["14","from ..utils._joblib import effective_n_jobs"]}],"sklearn\/model_selection\/_search.py":[{"add":["31","from joblib import Parallel, delayed"],"delete":["31","from ..utils._joblib import Parallel, delayed"]}],"sklearn\/datasets\/species_distributions.py":[{"add":["46","import joblib","47","","259","        joblib.dump(bunch, archive_path, compress=9)","261","        bunch = joblib.load(archive_path)"],"delete":["43","","52","from ..utils import _joblib","259","        _joblib.dump(bunch, archive_path, compress=9)","261","        bunch = _joblib.load(archive_path)"]}],"sklearn\/datasets\/california_housing.py":[{"add":["30","import joblib","31","","127","            joblib.dump(cal_housing, filepath, compress=6)","131","        cal_housing = joblib.load(filepath)"],"delete":["35","from ..utils import _joblib","126","            _joblib.dump(cal_housing, filepath, compress=6)","130","        cal_housing = _joblib.load(filepath)"]}],"sklearn\/datasets\/olivetti_faces.py":[{"add":["20","import joblib","106","        joblib.dump(faces, filepath, compress=6)","109","        faces = joblib.load(filepath)"],"delete":["25","from ..utils import _joblib","106","        _joblib.dump(faces, filepath, compress=6)","109","        faces = _joblib.load(filepath)"]}],"sklearn\/linear_model\/logistic.py":[{"add":["18","from joblib import Parallel, delayed, effective_n_jobs"],"delete":["35","from ..utils._joblib import Parallel, delayed, effective_n_jobs"]}],"sklearn\/ensemble\/tests\/test_forest.py":[{"add":["25","import joblib","1281","joblib.register_parallel_backend('testing', MyBackend)","1284","@pytest.mark.skipif(joblib.__version__ < LooseVersion('0.12'),","1290","    with joblib.parallel_backend(\"testing\") as (ba, n_jobs):","1296","    with joblib.parallel_backend(\"testing\") as (ba, _):"],"delete":["25","from sklearn.utils._joblib import joblib","26","from sklearn.utils._joblib import parallel_backend","27","from sklearn.utils._joblib import register_parallel_backend","28","from sklearn.utils._joblib import __version__ as __joblib_version__","1284","register_parallel_backend('testing', MyBackend)","1287","@pytest.mark.skipif(__joblib_version__ < LooseVersion('0.12'),","1293","    with parallel_backend(\"testing\") as (ba, n_jobs):","1299","    with parallel_backend(\"testing\") as (ba, _):"]}],"sklearn\/linear_model\/omp.py":[{"add":["13","from joblib import Parallel, delayed"],"delete":["18","from ..utils._joblib import Parallel, delayed"]}],"sklearn\/cluster\/mean_shift_.py":[{"add":["18","from joblib import Parallel, delayed"],"delete":["25","from ..utils._joblib import Parallel","26","from ..utils._joblib import delayed"]}],"sklearn\/ensemble\/forest.py":[{"add":["49","from joblib import Parallel, delayed"],"delete":["51","from ..utils._joblib import Parallel, delayed"]}],"sklearn\/metrics\/tests\/test_score_objects.py":[{"add":["8","import joblib","101","    joblib.dump((X, y, y_ml), filename)","102","    X_mm, y_mm, y_ml_mm = joblib.load(filename, mmap_mode='r')"],"delete":["39","from sklearn.utils import _joblib","101","    _joblib.dump((X, y, y_ml), filename)","102","    X_mm, y_mm, y_ml_mm = _joblib.load(filename, mmap_mode='r')"]}],"sklearn\/tests\/test_pipeline.py":[{"add":["13","import joblib","1031","        if LooseVersion(joblib.__version__) < LooseVersion('0.12'):","1033","            memory = joblib.Memory(cachedir=cachedir, verbose=10)","1035","            memory = joblib.Memory(location=cachedir, verbose=10)","1093","    if LooseVersion(joblib.__version__) < LooseVersion('0.12'):","1095","        memory = joblib.Memory(cachedir=cachedir, verbose=10)","1097","        memory = joblib.Memory(location=cachedir, verbose=10)"],"delete":["37","from sklearn.utils._joblib import Memory","38","from sklearn.utils._joblib import __version__ as joblib_version","1032","        if LooseVersion(joblib_version) < LooseVersion('0.12'):","1034","            memory = Memory(cachedir=cachedir, verbose=10)","1036","            memory = Memory(location=cachedir, verbose=10)","1094","    if LooseVersion(joblib_version) < LooseVersion('0.12'):","1096","        memory = Memory(cachedir=cachedir, verbose=10)","1098","        memory = Memory(location=cachedir, verbose=10)"]}],"sklearn\/datasets\/covtype.py":[{"add":["22","import joblib","119","        joblib.dump(X, samples_path, compress=9)","120","        joblib.dump(y, targets_path, compress=9)","127","        X = joblib.load(samples_path)","128","        y = joblib.load(targets_path)"],"delete":["28","from ..utils import _joblib","119","        _joblib.dump(X, samples_path, compress=9)","120","        _joblib.dump(y, targets_path, compress=9)","127","        X = _joblib.load(samples_path)","128","        y = _joblib.load(targets_path)"]}],"sklearn\/metrics\/pairwise.py":[{"add":["19","from joblib import Parallel, delayed, effective_n_jobs"],"delete":["27","from ..utils._joblib import Parallel","28","from ..utils._joblib import delayed","29","from ..utils._joblib import effective_n_jobs"]}],"sklearn\/utils\/validation.py":[{"add":["19","import joblib","177","        if LooseVersion(joblib.__version__) < '0.12':","178","            memory = joblib.Memory(cachedir=memory, verbose=0)","180","            memory = joblib.Memory(location=memory, verbose=0)"],"delete":["25","from ._joblib import Memory","26","from ._joblib import __version__ as joblib_version","178","        if LooseVersion(joblib_version) < '0.12':","179","            memory = Memory(cachedir=memory, verbose=0)","181","            memory = Memory(location=memory, verbose=0)"]}],"sklearn\/neighbors\/base.py":[{"add":["17","import joblib","18","from joblib import Parallel, delayed, effective_n_jobs","441","            old_joblib = (","442","                    LooseVersion(joblib.__version__) < LooseVersion('0.12'))","738","            if LooseVersion(joblib.__version__) < LooseVersion('0.12'):"],"delete":["27","from ..utils._joblib import Parallel, delayed, effective_n_jobs","28","from ..utils._joblib import __version__ as joblib_version","441","            old_joblib = LooseVersion(joblib_version) < LooseVersion('0.12')","737","            if LooseVersion(joblib_version) < LooseVersion('0.12'):"]}],"sklearn\/pipeline.py":[{"add":["16","from joblib import Parallel, delayed"],"delete":["18","from .utils._joblib import Parallel, delayed"]}],"sklearn\/linear_model\/tests\/test_sgd.py":[{"add":["6","import joblib","1594","    if joblib.__version__ < LooseVersion('0.12') and backend == 'loky':","1615","    with joblib.parallel_backend(backend=backend):"],"delete":["27","from sklearn.utils import _joblib","28","from sklearn.utils._joblib import parallel_backend","29","","1596","    if _joblib.__version__ < LooseVersion('0.12') and backend == 'loky':","1617","    with parallel_backend(backend=backend):"]}],"sklearn\/multiclass.py":[{"add":["55","from joblib import Parallel, delayed"],"delete":["55","from .utils._joblib import Parallel","56","from .utils._joblib import delayed"]}],"sklearn\/compose\/_column_transformer.py":[{"add":["13","from joblib import Parallel, delayed"],"delete":["15","from ..utils._joblib import Parallel, delayed"]}],"sklearn\/datasets\/lfw.py":[{"add":["17","import joblib","18","from joblib import Memory","305","    if LooseVersion(joblib.__version__) < LooseVersion('0.12'):","476","    if LooseVersion(joblib.__version__) < LooseVersion('0.12'):"],"delete":["20","from ..utils._joblib import Memory","21","from ..utils import _joblib","305","    if LooseVersion(_joblib.__version__) < LooseVersion('0.12'):","476","    if LooseVersion(_joblib.__version__) < LooseVersion('0.12'):"]}],"sklearn\/utils\/fixes.py":[{"add":["220","    import joblib","222","    if joblib.__version__ >= LooseVersion('0.12'):","228","                                  % (list(extra_args), joblib.__version__))"],"delete":["220","    from . import _joblib","222","    if _joblib.__version__ >= LooseVersion('0.12'):","228","                                  % (list(extra_args), _joblib.__version__))"]}],"sklearn\/utils\/tests\/test_fixes.py":[{"add":["29","    import joblib","30","    monkeypatch.setattr(joblib, '__version__', joblib_version)"],"delete":["29","    import sklearn.utils._joblib","30","    monkeypatch.setattr(sklearn.utils._joblib, '__version__', joblib_version)"]}],"sklearn\/linear_model\/coordinate_descent.py":[{"add":["13","from joblib import Parallel, delayed, effective_n_jobs"],"delete":["20","from ..utils._joblib import Parallel, delayed, effective_n_jobs"]}],"sklearn\/decomposition\/online_lda.py":[{"add":["16","from joblib import Parallel, delayed, effective_n_jobs"],"delete":["23","from ..utils._joblib import Parallel, delayed, effective_n_jobs"]}],"sklearn\/manifold\/mds.py":[{"add":["8","from joblib import Parallel, delayed, effective_n_jobs"],"delete":["14","from ..utils._joblib import Parallel","15","from ..utils._joblib import delayed","16","from ..utils._joblib import effective_n_jobs"]}],"sklearn\/datasets\/kddcup99.py":[{"add":["17","import joblib","285","        joblib.dump(X, samples_path, compress=0)","286","        joblib.dump(y, targets_path, compress=0)","294","        X = joblib.load(samples_path)","295","        y = joblib.load(targets_path)"],"delete":["17","","23","from ..utils import _joblib","286","        _joblib.dump(X, samples_path, compress=0)","287","        _joblib.dump(y, targets_path, compress=0)","295","        X = _joblib.load(samples_path)","296","        y = _joblib.load(targets_path)"]}],"sklearn\/neighbors\/tests\/test_kde.py":[{"add":["12","import joblib","221","    joblib.dump(kde, file_path)","222","    kde = joblib.load(file_path)"],"delete":["12","from sklearn.utils import _joblib","221","    _joblib.dump(kde, file_path)","222","    kde = _joblib.load(file_path)"]}],"sklearn\/utils\/tests\/test_estimator_checks.py":[{"add":["5","import joblib","441","            old_hash = joblib.hash(est)","443","        assert_equal(old_hash, joblib.hash(est))","452","            old_hash = joblib.hash(est)","454","        assert_equal(old_hash, joblib.hash(est))"],"delete":["10","from sklearn.utils import _joblib","441","            old_hash = _joblib.hash(est)","443","        assert_equal(old_hash, _joblib.hash(est))","452","            old_hash = _joblib.hash(est)","454","        assert_equal(old_hash, _joblib.hash(est))"]}],"sklearn\/ensemble\/partial_dependence.py":[{"add":["14","from joblib import Parallel, delayed"],"delete":["16","from ..utils._joblib import Parallel, delayed"]}],"sklearn\/ensemble\/voting.py":[{"add":["18","from joblib import Parallel, delayed","19",""],"delete":["23","from ..utils._joblib import Parallel, delayed"]}],"sklearn\/feature_selection\/rfe.py":[{"add":["9","from joblib import Parallel, delayed, effective_n_jobs","10",""],"delete":["17","from ..utils._joblib import Parallel, delayed, effective_n_jobs"]}],"sklearn\/linear_model\/theil_sen.py":[{"add":["17","from joblib import Parallel, delayed, effective_n_jobs"],"delete":["22","from ..utils._joblib import Parallel, delayed, effective_n_jobs"]}],"sklearn\/datasets\/twenty_newsgroups.py":[{"add":["37","import joblib","400","        X_train, X_test = joblib.load(target_file)","405","        joblib.dump((X_train, X_test), target_file, compress=9)"],"delete":["45","from ..utils import _joblib","400","        X_train, X_test = _joblib.load(target_file)","405","        _joblib.dump((X_train, X_test), target_file, compress=9)"]}],"sklearn\/utils\/estimator_checks.py":[{"add":["12","import joblib","2058","        assert_equal(joblib.hash(new_value), joblib.hash(original_value),","2222","                           np.float64, types.FunctionType, joblib.Memory])"],"delete":["14","from . import _joblib","2058","        assert_equal(_joblib.hash(new_value), _joblib.hash(original_value),","2222","                           np.float64, types.FunctionType, _joblib.Memory])"]}],"sklearn\/decomposition\/tests\/test_sparse_pca.py":[{"add":["144","        import joblib","145","        _mp = joblib.parallel.multiprocessing","146","        joblib.parallel.multiprocessing = None","152","            joblib.parallel.multiprocessing = _mp"],"delete":["144","        import sklearn.utils._joblib.parallel as joblib_par","145","        _mp = joblib_par.multiprocessing","146","        joblib_par.multiprocessing = None","152","            joblib_par.multiprocessing = _mp"]}],"sklearn\/tests\/test_multioutput.py":[{"add":["4","from joblib import cpu_count"],"delete":["19","from sklearn.utils._joblib import cpu_count"]}],"sklearn\/linear_model\/base.py":[{"add":["24","from joblib import Parallel, delayed"],"delete":["25","from ..utils._joblib import Parallel, delayed"]}],"sklearn\/ensemble\/bagging.py":[{"add":["12","from joblib import Parallel, delayed","13",""],"delete":["14","from ..utils._joblib import Parallel, delayed"]}],"sklearn\/ensemble\/tests\/test_bagging.py":[{"add":["8","import joblib","225","        self.training_hash_ = joblib.hash(X)"],"delete":["34","from sklearn.utils import _joblib","225","        self.training_hash_ = _joblib.hash(X)"]}],"sklearn\/linear_model\/stochastic_gradient.py":[{"add":["11","from joblib import Parallel, delayed"],"delete":["11","from ..utils._joblib import Parallel, delayed"]}],"sklearn\/model_selection\/_validation.py":[{"add":["19","from joblib import Parallel, delayed"],"delete":["25","from ..utils._joblib import Parallel, delayed"]}],"sklearn\/decomposition\/dict_learning.py":[{"add":["13","from joblib import Parallel, delayed, effective_n_jobs"],"delete":["15","from ..utils._joblib import Parallel, delayed, effective_n_jobs"]}],"sklearn\/cluster\/k_means_.py":[{"add":["17","from joblib import Parallel, delayed, effective_n_jobs"],"delete":["30","from ..utils._joblib import Parallel","31","from ..utils._joblib import delayed","32","from ..utils._joblib import effective_n_jobs"]}],"sklearn\/linear_model\/least_angle.py":[{"add":["17","from joblib import Parallel, delayed"],"delete":["23","from ..utils._joblib import Parallel, delayed"]}]}},"98aefc1fdcfc53f5c8572b02c7f0c5bcba23e351":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/metrics\/pairwise.py":"MODIFY","sklearn\/metrics\/tests\/test_pairwise.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["4",".. _changes_0_21_2:","5","","6","Version 0.21.2","7","==============","8","","9","**June 2019**","10","","11","Changelog","12","---------","13","","14",":mod:`sklearn.metrics`","15","......................","16","","17","- |Fix| Fixed a bug in :func:`euclidean_distances` where a part of the distance","18","  matrix was left un-instanciated for suffiently large float32 datasets","19","  (regression introduced in 0.21) :issue:`13910`","20","  by :user:`J¨¦r¨¦mie du Boisberranger <jeremiedbb>`.","21",""],"delete":[]}],"sklearn\/metrics\/pairwise.py":[{"add":["285","def _euclidean_distances_upcast(X, XX=None, Y=None, YY=None, batch_size=None):","300","    if batch_size is None:","301","        x_density = X.nnz \/ np.prod(X.shape) if issparse(X) else 1","302","        y_density = Y.nnz \/ np.prod(Y.shape) if issparse(Y) else 1","304","        # Allow 10% more memory than X, Y and the distance matrix take (at","305","        # least 10MiB)","306","        maxmem = max(","307","            ((x_density * n_samples_X + y_density * n_samples_Y) * n_features","308","             + (x_density * n_samples_X * y_density * n_samples_Y)) \/ 10,","309","            10 * 2 ** 17)","311","        # The increase amount of memory in 8-byte blocks is:","312","        # - x_density * batch_size * n_features (copy of chunk of X)","313","        # - y_density * batch_size * n_features (copy of chunk of Y)","314","        # - batch_size * batch_size (chunk of distance matrix)","315","        # Hence x? + (xd+yd)kx = M, where x=batch_size, k=n_features, M=maxmem","316","        #                                 xd=x_density and yd=y_density","317","        tmp = (x_density + y_density) * n_features","318","        batch_size = (-tmp + np.sqrt(tmp ** 2 + 4 * maxmem)) \/ 2","319","        batch_size = max(int(batch_size), 1)","321","    x_batches = gen_batches(n_samples_X, batch_size)","330","        y_batches = gen_batches(n_samples_Y, batch_size)","331",""],"delete":["285","def _euclidean_distances_upcast(X, XX=None, Y=None, YY=None):","300","    x_density = X.nnz \/ np.prod(X.shape) if issparse(X) else 1","301","    y_density = Y.nnz \/ np.prod(Y.shape) if issparse(Y) else 1","303","    # Allow 10% more memory than X, Y and the distance matrix take (at least","304","    # 10MiB)","305","    maxmem = max(","306","        ((x_density * n_samples_X + y_density * n_samples_Y) * n_features","307","         + (x_density * n_samples_X * y_density * n_samples_Y)) \/ 10,","308","        10 * 2 ** 17)","310","    # The increase amount of memory in 8-byte blocks is:","311","    # - x_density * batch_size * n_features (copy of chunk of X)","312","    # - y_density * batch_size * n_features (copy of chunk of Y)","313","    # - batch_size * batch_size (chunk of distance matrix)","314","    # Hence x? + (xd+yd)kx = M, where x=batch_size, k=n_features, M=maxmem","315","    #                                 xd=x_density and yd=y_density","316","    tmp = (x_density + y_density) * n_features","317","    batch_size = (-tmp + np.sqrt(tmp ** 2 + 4 * maxmem)) \/ 2","318","    batch_size = max(int(batch_size), 1)","320","    x_batches = gen_batches(X.shape[0], batch_size)","321","    y_batches = gen_batches(Y.shape[0], batch_size)"]}],"sklearn\/metrics\/tests\/test_pairwise.py":[{"add":["50","from sklearn.metrics.pairwise import _euclidean_distances_upcast","690","@pytest.mark.parametrize(\"batch_size\", [None, 5, 7, 101])","691","@pytest.mark.parametrize(\"x_array_constr\", [np.array, csr_matrix],","692","                         ids=[\"dense\", \"sparse\"])","693","@pytest.mark.parametrize(\"y_array_constr\", [np.array, csr_matrix],","694","                         ids=[\"dense\", \"sparse\"])","695","def test_euclidean_distances_upcast(batch_size, x_array_constr,","696","                                    y_array_constr):","697","    # check batches handling when Y != X (#13910)","698","    rng = np.random.RandomState(0)","699","    X = rng.random_sample((100, 10)).astype(np.float32)","700","    X[X < 0.8] = 0","701","    Y = rng.random_sample((10, 10)).astype(np.float32)","702","    Y[Y < 0.8] = 0","703","","704","    expected = cdist(X, Y)","705","","706","    X = x_array_constr(X)","707","    Y = y_array_constr(Y)","708","    distances = _euclidean_distances_upcast(X, Y=Y, batch_size=batch_size)","709","    distances = np.sqrt(np.maximum(distances, 0))","710","","711","    # the default rtol=1e-7 is too close to the float32 precision","712","    # and fails due too rounding errors.","713","    assert_allclose(distances, expected, rtol=1e-6)","714","","715","","716","@pytest.mark.parametrize(\"batch_size\", [None, 5, 7, 101])","717","@pytest.mark.parametrize(\"x_array_constr\", [np.array, csr_matrix],","718","                         ids=[\"dense\", \"sparse\"])","719","def test_euclidean_distances_upcast_sym(batch_size, x_array_constr):","720","    # check batches handling when X is Y (#13910)","721","    rng = np.random.RandomState(0)","722","    X = rng.random_sample((100, 10)).astype(np.float32)","723","    X[X < 0.8] = 0","724","","725","    expected = squareform(pdist(X))","726","","727","    X = x_array_constr(X)","728","    distances = _euclidean_distances_upcast(X, Y=X, batch_size=batch_size)","729","    distances = np.sqrt(np.maximum(distances, 0))","730","","731","    # the default rtol=1e-7 is too close to the float32 precision","732","    # and fails due too rounding errors.","733","    assert_allclose(distances, expected, rtol=1e-6)","734","","735",""],"delete":[]}]}},"11e4da51beb88a49d40af6ba0edf8c862369af8e":{"changes":{"sklearn\/ensemble\/tests\/test_iforest.py":"MODIFY"},"diff":{"sklearn\/ensemble\/tests\/test_iforest.py":[{"add":[],"delete":["12","from sklearn.utils.testing import assert_almost_equal","282",""]}]}},"02a0101c7c9e7c883fc1b916f0d2777e8581e1f8":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/ensemble\/gradient_boosting.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["80","  error for multiclass multioutput forests models if any targets were strings.","81","  :issue:`12834` by :user:`Elizabeth Sander <elsander>`.","82","","83","- |Fix| Fixed a bug in :class:`ensemble.gradient_boosting.LossFunction` and","84","  :class:`ensemble.gradient_boosting.LeastSquaresError` where the default","85","  value of ``learning_rate`` in ``update_terminal_regions`` is not consistent","86","  with the document and the caller functions.","87","  :issue:`6463` by :user:`movelikeriver <movelikeriver>`.","147","  :class:`~model_selection.RandomizedSearchCV`, and methods"],"delete":["80","   error for multiclass multioutput forests models if any targets were strings.","81","   :issue:`12834` by :user:`Elizabeth Sander <elsander>`.","141","  :class:`~model_selection.RandomSearchCV`, and methods"]}],"sklearn\/ensemble\/gradient_boosting.py":[{"add":["357","                                learning_rate=0.1, k=0):","471","                                learning_rate=0.1, k=0):","1203","                                         learning_rate=self.learning_rate, k=k)"],"delete":["357","                                learning_rate=1.0, k=0):","471","                                learning_rate=1.0, k=0):","1203","                                         self.learning_rate, k=k)"]}]}},"77b73d63d05bc198ba89193582aee93cae1f69a4":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/linear_model\/tests\/test_ridge.py":"MODIFY","sklearn\/linear_model\/ridge.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["186","","188","  parameter, allowing iterative addition of trees to an isolation","371","- |Fix| Fixed bug in :func:`linear_model.ridge.ridge_regression`,","372","  :class:`linear_model.ridge.Ridge` and","373","  :class:`linear_model.ridge.ridge.RidgeClassifier` that","374","  caused unhandled exception for arguments ``return_intercept=True`` and","375","  ``solver=auto`` (default) or any other solver different from ``sag``.","376","  :issue:`13363` by :user:`Bartosz Telenczuk <btel>`","377","","378","- |Fix| :func:`linear_model.ridge.ridge_regression` will now raise an exception","379","  if ``return_intercept=True`` and solver is different from ``sag``. Previously,","380","  only warning was issued. :issue:`13363` by :user:`Bartosz Telenczuk <btel>`","381","","382","- |API| :func:`linear_model.ridge.ridge_regression` will choose ``sparse_cg``","383","  solver for sparse inputs when ``solver=auto`` and ``sample_weight``","384","  is provided (previously `cholesky` solver was selected). :issue:`13363`","385","  by :user:`Bartosz Telenczuk <btel>`","386",""],"delete":["186","  ","188","  parameter, allowing iterative addition of trees to an isolation "]}],"sklearn\/linear_model\/tests\/test_ridge.py":[{"add":["9","from sklearn.utils.testing import assert_allclose","781","    message = (\"Known solvers are 'sparse_cg', 'cholesky', 'svd'\"","782","               \" 'lsqr', 'sag' or 'saga'. Got %s.\" % wrong_solver)","836","        assert_raises_regex(ValueError, \"In Ridge,\", sparse.fit, X_csr, y)","837","","838","","839","@pytest.mark.parametrize('return_intercept', [False, True])","840","@pytest.mark.parametrize('sample_weight', [None, np.ones(1000)])","841","@pytest.mark.parametrize('arr_type', [np.array, sp.csr_matrix])","842","@pytest.mark.parametrize('solver', ['auto', 'sparse_cg', 'cholesky', 'lsqr',","843","                                    'sag', 'saga'])","844","def test_ridge_regression_check_arguments_validity(return_intercept,","845","                                                   sample_weight, arr_type,","846","                                                   solver):","847","    \"\"\"check if all combinations of arguments give valid estimations\"\"\"","848","","849","    # test excludes 'svd' solver because it raises exception for sparse inputs","850","","851","    rng = check_random_state(42)","852","    X = rng.rand(1000, 3)","853","    true_coefs = [1, 2, 0.1]","854","    y = np.dot(X, true_coefs)","855","    true_intercept = 0.","856","    if return_intercept:","857","        true_intercept = 10000.","858","    y += true_intercept","859","    X_testing = arr_type(X)","860","","861","    alpha, atol, tol = 1e-3, 1e-4, 1e-6","862","","863","    if solver not in ['sag', 'auto'] and return_intercept:","864","        assert_raises_regex(ValueError,","865","                            \"In Ridge, only 'sag' solver\",","866","                            ridge_regression, X_testing, y,","867","                            alpha=alpha,","868","                            solver=solver,","869","                            sample_weight=sample_weight,","870","                            return_intercept=return_intercept,","871","                            tol=tol)","872","        return","873","","874","    out = ridge_regression(X_testing, y, alpha=alpha,","875","                           solver=solver,","876","                           sample_weight=sample_weight,","877","                           return_intercept=return_intercept,","878","                           tol=tol,","879","                           )","880","","881","    if return_intercept:","882","        coef, intercept = out","883","        assert_allclose(coef, true_coefs, rtol=0, atol=atol)","884","        assert_allclose(intercept, true_intercept, rtol=0, atol=atol)","885","    else:","886","        assert_allclose(out, true_coefs, rtol=0, atol=atol)"],"delete":["780","    message = \"Solver %s not understood\" % wrong_solver","834","        assert_warns(UserWarning, sparse.fit, X_csr, y)","835","        assert_almost_equal(dense.intercept_, sparse.intercept_)","836","        assert_array_almost_equal(dense.coef_, sparse.coef_)"]}],"sklearn\/linear_model\/ridge.py":[{"add":["370","    has_sw = sample_weight is not None","371","","372","    if solver == 'auto':","373","        if return_intercept:","374","            # only sag supports fitting intercept directly","375","            solver = \"sag\"","376","        elif not sparse.issparse(X):","377","            solver = \"cholesky\"","378","        else:","379","            solver = \"sparse_cg\"","380","","381","    if solver not in ('sparse_cg', 'cholesky', 'svd', 'lsqr', 'sag', 'saga'):","382","        raise ValueError(\"Known solvers are 'sparse_cg', 'cholesky', 'svd'\"","383","                         \" 'lsqr', 'sag' or 'saga'. Got %s.\" % solver)","384","","385","    if return_intercept and solver != 'sag':","386","        raise ValueError(\"In Ridge, only 'sag' solver can directly fit the \"","387","                         \"intercept. Please change solver to 'sag' or set \"","388","                         \"return_intercept=False.\")","561","            if sparse.issparse(X) and self.solver == 'sparse_cg':"],"delete":["370","    if return_intercept and sparse.issparse(X) and solver != 'sag':","371","        if solver != 'auto':","372","            warnings.warn(\"In Ridge, only 'sag' solver can currently fit the \"","373","                          \"intercept when X is sparse. Solver has been \"","374","                          \"automatically changed into 'sag'.\")","375","        solver = 'sag'","406","    has_sw = sample_weight is not None","408","    if solver == 'auto':","409","        # cholesky if it's a dense array and cg in any other case","410","        if not sparse.issparse(X) or has_sw:","411","            solver = 'cholesky'","412","        else:","413","            solver = 'sparse_cg'","434","    if solver not in ('sparse_cg', 'cholesky', 'svd', 'lsqr', 'sag', 'saga'):","435","        raise ValueError('Solver %s not understood' % solver)","557","            if sparse.issparse(X):"]}]}},"2a2dc8c876e75f9bbbbda1d75f569060a2627403":{"changes":{"azure-pipelines.yml":"ADD","build_tools\/azure\/posix.yml":"ADD","build_tools\/azure\/test_script.cmd":"ADD","build_tools\/azure\/test_pytest_soft_dependency.sh":"ADD","build_tools\/azure\/test_docs.sh":"ADD","build_tools\/azure\/upload_codecov.sh":"ADD","build_tools\/azure\/test_script.sh":"ADD","build_tools\/azure\/install.cmd":"ADD","build_tools\/azure\/install.sh":"ADD","build_tools\/azure\/windows.yml":"ADD"},"diff":{"azure-pipelines.yml":[{"add":[],"delete":[]}],"build_tools\/azure\/posix.yml":[{"add":[],"delete":[]}],"build_tools\/azure\/test_script.cmd":[{"add":[],"delete":[]}],"build_tools\/azure\/test_pytest_soft_dependency.sh":[{"add":[],"delete":[]}],"build_tools\/azure\/test_docs.sh":[{"add":[],"delete":[]}],"build_tools\/azure\/upload_codecov.sh":[{"add":[],"delete":[]}],"build_tools\/azure\/test_script.sh":[{"add":[],"delete":[]}],"build_tools\/azure\/install.cmd":[{"add":[],"delete":[]}],"build_tools\/azure\/install.sh":[{"add":[],"delete":[]}],"build_tools\/azure\/windows.yml":[{"add":[],"delete":[]}]}},"5890cbc995cde1991f1fc52cae495f5c50194df4":{"changes":{"sklearn\/tests\/test_impute.py":"MODIFY","doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/impute.py":"MODIFY"},"diff":{"sklearn\/tests\/test_impute.py":[{"add":["937","    \"missing_values, dtype, arr_type\",","938","    [(np.nan, np.float64, np.array),","939","     (0,      np.int32,   np.array),","940","     (-1,     np.int32,   np.array),","941","     (np.nan, np.float64, sparse.csc_matrix),","942","     (-1,     np.int32,   sparse.csc_matrix),","943","     (np.nan, np.float64, sparse.csr_matrix),","944","     (-1,     np.int32,   sparse.csr_matrix),","945","     (np.nan, np.float64, sparse.coo_matrix),","946","     (-1,     np.int32,   sparse.coo_matrix),","947","     (np.nan, np.float64, sparse.lil_matrix),","948","     (-1,     np.int32,   sparse.lil_matrix),","949","     (np.nan, np.float64, sparse.bsr_matrix),","950","     (-1,     np.int32,   sparse.bsr_matrix)","951","     ])","1003","    [sparse.csc_matrix, sparse.csr_matrix, sparse.coo_matrix,","1004","     sparse.lil_matrix, sparse.bsr_matrix])","1005","def test_missing_indicator_raise_on_sparse_with_missing_0(arr_type):","1006","    # test for sparse input and missing_value == 0","1007","","1008","    missing_values = 0","1009","    X_fit = np.array([[missing_values, missing_values, 1],","1010","                      [4, missing_values, 2]])","1011","    X_trans = np.array([[missing_values, missing_values, 1],","1012","                        [4, 12, 10]])","1013","","1014","    # convert the input to the right array format","1015","    X_fit_sparse = arr_type(X_fit)","1016","    X_trans_sparse = arr_type(X_trans)","1017","","1018","    indicator = MissingIndicator(missing_values=missing_values)","1019","","1020","    with pytest.raises(ValueError, match=\"Sparse input with missing_values=0\"):","1021","        indicator.fit_transform(X_fit_sparse)","1022","","1023","    indicator.fit_transform(X_fit)","1024","    with pytest.raises(ValueError, match=\"Sparse input with missing_values=0\"):","1025","        indicator.transform(X_trans_sparse)","1026","","1027","","1028","@pytest.mark.parametrize(\"param_sparse\", [True, False, 'auto'])","1029","@pytest.mark.parametrize(\"missing_values, arr_type\",","1030","                         [(np.nan, np.array),","1031","                          (0,      np.array),","1032","                          (np.nan, sparse.csc_matrix),","1033","                          (np.nan, sparse.csr_matrix),","1034","                          (np.nan, sparse.coo_matrix),","1035","                          (np.nan, sparse.lil_matrix)","1036","                          ])"],"delete":["937","    \"missing_values, dtype\",","938","    [(np.nan, np.float64),","939","     (0, np.int32),","940","     (-1, np.int32)])","941","@pytest.mark.parametrize(","942","    \"arr_type\",","943","    [np.array, sparse.csc_matrix, sparse.csr_matrix, sparse.coo_matrix,","944","     sparse.lil_matrix, sparse.bsr_matrix])","994","@pytest.mark.parametrize(\"param_sparse\", [True, False, 'auto'])","995","@pytest.mark.parametrize(\"missing_values\", [np.nan, 0])","998","    [np.array, sparse.csc_matrix, sparse.csr_matrix, sparse.coo_matrix])"]}],"doc\/whats_new\/v0.21.rst":[{"add":["84","  `n_components <= min(n_samples, n_features)`.","140","- |Fix| In :class:`impute.MissingIndicator` avoid implicit densification by","141","  raising an exception if input is sparse add `missing_values` property","142","  is set to 0. :issue:`13240` by :user:`Bartosz Telenczuk <btel>`.","143","","177","   parameter value ``copy_X=True`` in ``fit``."],"delete":["84","  `n_components <= min(n_samples, n_features)`. ","173","   parameter value ``copy_X=True`` in ``fit``. "]}],"sklearn\/impute.py":[{"add":["1136","        if sparse.issparse(X):","1182","","1183","        if sparse.issparse(X) and self.missing_values == 0:","1184","            # missing_values = 0 not allowed with sparse data as it would","1185","            # force densification","1186","            raise ValueError(\"Sparse input with missing_values=0 is \"","1187","                             \"not supported. Provide a dense \"","1188","                             \"array instead.\")","1189",""],"delete":["1136","        if sparse.issparse(X) and self.missing_values != 0:","1159","            if sparse.issparse(X):","1160","                # case of sparse matrix with 0 as missing values. Implicit and","1161","                # explicit zeros are considered as missing values.","1162","                X = X.toarray()"]}]}},"ffd27c8594fa58b8b0559ae0cf461116bf09e381":{"changes":{"sklearn\/__init__.py":"MODIFY","build_tools\/travis\/install.sh":"MODIFY","build_tools\/azure\/install.sh":"MODIFY"},"diff":{"sklearn\/__init__.py":[{"add":["18","import os","50","# On OSX, we can get a runtime error due to multiple OpenMP libraries loaded","51","# simultaneously. This can happen for instance when calling BLAS inside a","52","# prange. Setting the following environment variable allows multiple OpenMP","53","# libraries to be loaded. It should not degrade performances since we manually","54","# take care of potential over-subcription performance issues, in sections of","55","# the code where nested OpenMP loops can happen, by dynamically reconfiguring","56","# the inner OpenMP runtime to temporarily disable it while under the scope of","57","# the outer OpenMP parallel section.","58","os.environ.setdefault(\"KMP_DUPLICATE_LIB_OK\", \"True\")","59","","60",""],"delete":[]}],"build_tools\/travis\/install.sh":[{"add":[],"delete":["40","","41","    # avoid error due to multiple OpenMP libraries loaded simultaneously","42","    export KMP_DUPLICATE_LIB_OK=TRUE"]}],"build_tools\/azure\/install.sh":[{"add":[],"delete":["18","","19","    # avoid error due to multiple OpenMP libraries loaded simultaneously","20","    export KMP_DUPLICATE_LIB_OK=TRUE"]}]}},"296ee0ca45319a95f8a836c4855a613eeb6c9041":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/calibration.py":"MODIFY","sklearn\/metrics\/classification.py":"MODIFY","sklearn\/metrics\/tests\/test_classification.py":"MODIFY","sklearn\/metrics\/ranking.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["482","- |Fix| Fixed a bug where :func:`metrics.brier_score_loss` will sometimes","483","  return incorrect result when there's only one class in ``y_true``.","484","  :pr:`13628` by :user:`Hanmin Qin <qinhanmin2014>`.","485","","486","- |Fix| Fixed a bug in :func:`metrics.label_ranking_average_precision_score`"],"delete":["482","- |Fix| Fixed a bug in :func:`metrics.label_ranking_average_precision_score` "]}],"sklearn\/calibration.py":[{"add":["573","    check_consistent_length(y_true, y_prob)","581","    labels = np.unique(y_true)","582","    if len(labels) > 2:","583","        raise ValueError(\"Only binary classification is supported. \"","584","                         \"Provided labels %s.\" % labels)","585","    y_true = label_binarize(y_true, labels)[:, 0]"],"delete":["27","from .metrics.classification import _check_binary_probabilistic_predictions","581","    y_true = _check_binary_probabilistic_predictions(y_true, y_prob)"]}],"sklearn\/metrics\/classification.py":[{"add":["30","from ..preprocessing import LabelBinarizer","2336","        Label of the positive class.","2337","        Defaults to the greater label unless y_true is all 0 or all -1","2338","        in which case pos_label defaults to 1.","2373","    labels = np.unique(y_true)","2374","    if len(labels) > 2:","2375","        raise ValueError(\"Only binary classification is supported. \"","2376","                         \"Labels in y_true: %s.\" % labels)","2377","    if y_prob.max() > 1:","2378","        raise ValueError(\"y_prob contains values greater than 1.\")","2379","    if y_prob.min() < 0:","2380","        raise ValueError(\"y_prob contains values less than 0.\")","2381","","2382","    # if pos_label=None, when y_true is in {-1, 1} or {0, 1},","2383","    # pos_labe is set to 1 (consistent with precision_recall_curve\/roc_curve),","2384","    # otherwise pos_label is set to the greater label","2385","    # (different from precision_recall_curve\/roc_curve,","2386","    # the purpose is to keep backward compatibility).","2388","        if (np.array_equal(labels, [0]) or","2389","                np.array_equal(labels, [-1])):","2390","            pos_label = 1","2391","        else:","2392","            pos_label = y_true.max()"],"delete":["30","from ..preprocessing import LabelBinarizer, label_binarize","2303","def _check_binary_probabilistic_predictions(y_true, y_prob):","2304","    \"\"\"Check that y_true is binary and y_prob contains valid probabilities\"\"\"","2305","    check_consistent_length(y_true, y_prob)","2306","","2307","    labels = np.unique(y_true)","2308","","2309","    if len(labels) > 2:","2310","        raise ValueError(\"Only binary classification is supported. \"","2311","                         \"Provided labels %s.\" % labels)","2312","","2313","    if y_prob.max() > 1:","2314","        raise ValueError(\"y_prob contains values greater than 1.\")","2315","","2316","    if y_prob.min() < 0:","2317","        raise ValueError(\"y_prob contains values less than 0.\")","2318","","2319","    return label_binarize(y_true, labels)[:, 0]","2320","","2321","","2355","        Label of the positive class. If None, the maximum label is used as","2356","        positive class","2392","        pos_label = y_true.max()","2394","    y_true = _check_binary_probabilistic_predictions(y_true, y_prob)"]}],"sklearn\/metrics\/tests\/test_classification.py":[{"add":["1999","","2000","    # ensure to raise an error for multiclass y_true","2001","    y_true = np.array([0, 1, 2, 0])","2002","    y_pred = np.array([0.8, 0.6, 0.4, 0.2])","2003","    error_message = (\"Only binary classification is supported. Labels \"","2004","                     \"in y_true: {}\".format(np.array([0, 1, 2])))","2005","    assert_raise_message(ValueError, error_message, brier_score_loss,","2006","                         y_true, y_pred)","2007","","2008","    # calculate correctly when there's only one class in y_true","2009","    assert_almost_equal(brier_score_loss([-1], [0.4]), 0.16)","2010","    assert_almost_equal(brier_score_loss([0], [0.4]), 0.16)","2011","    assert_almost_equal(brier_score_loss([1], [0.4]), 0.36)","2012","    assert_almost_equal(","2013","        brier_score_loss(['foo'], [0.4], pos_label='bar'), 0.16)","2014","    assert_almost_equal(","2015","        brier_score_loss(['foo'], [0.4], pos_label='foo'), 0.36)"],"delete":["1999","    # calculate even if only single class in y_true (#6980)","2000","    assert_almost_equal(brier_score_loss([0], [0.5]), 0.25)","2001","    assert_almost_equal(brier_score_loss([1], [0.5]), 0.25)"]}],"sklearn\/metrics\/ranking.py":[{"add":["471","        True binary labels. If labels are not either {-1, 1} or {0, 1}, then","472","        pos_label should be explicitly given.","478","        The label of the positive class.","479","        When ``pos_label=None``, if y_true is in {-1, 1} or {0, 1},","480","        ``pos_label`` is set to 1, otherwise an error will be raised.","557","        The label of the positive class.","558","        When ``pos_label=None``, if y_true is in {-1, 1} or {0, 1},","559","        ``pos_label`` is set to 1, otherwise an error will be raised."],"delete":["471","        True targets of binary classification in range {-1, 1} or {0, 1}.","477","        The label of the positive class","554","        Label considered as positive and others are considered negative."]}]}},"2a7194de7ad9ff90e93a65fe7972baa11e04b7e4":{"changes":{"sklearn\/ensemble\/_hist_gradient_boosting\/gradient_boosting.py":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/binning.py":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_gradient_boosting.py":"MODIFY","doc\/whats_new\/v0.22.rst":"MODIFY"},"diff":{"sklearn\/ensemble\/_hist_gradient_boosting\/gradient_boosting.py":[{"add":["126","            X_train, X_val, y_train, y_val = train_test_split(","127","                X, y, test_size=self.validation_fraction, stratify=stratify,","128","                random_state=rng)","130","            X_train, y_train = X, y","131","            X_val, y_val = None, None","132","","133","        # Bin the data","134","        self.bin_mapper_ = _BinMapper(max_bins=self.max_bins, random_state=rng)","135","        X_binned_train = self._bin_data(X_train, rng, is_training_data=True)","136","        if X_val is not None:","137","            X_binned_val = self._bin_data(X_val, rng, is_training_data=False)","138","        else:","139","            X_binned_val = None","381","    def _bin_data(self, X, rng, is_training_data):","382","        \"\"\"Bin data X.","383","","384","        If is_training_data, then set the bin_mapper_ attribute.","385","        Else, the binned data is converted to a C-contiguous array.","386","        \"\"\"","387","","388","        description = 'training' if is_training_data else 'validation'","389","        if self.verbose:","390","            print(\"Binning {:.3f} GB of {} data: \".format(","391","                X.nbytes \/ 1e9, description), end=\"\", flush=True)","392","        tic = time()","393","        if is_training_data:","394","            X_binned = self.bin_mapper_.fit_transform(X)  # F-aligned array","395","        else:","396","            X_binned = self.bin_mapper_.transform(X)  # F-aligned array","397","            # We convert the array to C-contiguous since predicting is faster","398","            # with this layout (training is faster on F-arrays though)","399","            X_binned = np.ascontiguousarray(X_binned)","400","        toc = time()","401","        if self.verbose:","402","            duration = toc - tic","403","            print(\"{:.3f} s\".format(duration))","404","","405","        return X_binned","406",""],"delete":["114","        # bin the data","115","        if self.verbose:","116","            print(\"Binning {:.3f} GB of data: \".format(X.nbytes \/ 1e9), end=\"\",","117","                  flush=True)","118","        tic = time()","119","        self.bin_mapper_ = _BinMapper(max_bins=self.max_bins, random_state=rng)","120","        X_binned = self.bin_mapper_.fit_transform(X)","121","        toc = time()","122","        if self.verbose:","123","            duration = toc - tic","124","            print(\"{:.3f} s\".format(duration))","137","            X_binned_train, X_binned_val, y_train, y_val = train_test_split(","138","                X_binned, y, test_size=self.validation_fraction,","139","                stratify=stratify, random_state=rng)","140","","141","            # Predicting is faster of C-contiguous arrays, training is faster","142","            # on Fortran arrays.","143","            X_binned_val = np.ascontiguousarray(X_binned_val)","144","            X_binned_train = np.asfortranarray(X_binned_train)","146","            X_binned_train, y_train = X_binned, y","147","            X_binned_val, y_val = None, None"]}],"sklearn\/ensemble\/_hist_gradient_boosting\/binning.py":[{"add":["142","            The binned data (fortran-aligned)."],"delete":["142","            The binned data."]}],"sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_gradient_boosting.py":[{"add":["8","from sklearn.ensemble._hist_gradient_boosting.binning import _BinMapper","148","","149","","150","def test_binning_train_validation_are_separated():","151","    # Make sure training and validation data are binned separately.","152","    # See issue 13926","153","","154","    rng = np.random.RandomState(0)","155","    validation_fraction = .2","156","    gb = HistGradientBoostingClassifier(","157","        n_iter_no_change=5,","158","        validation_fraction=validation_fraction,","159","        random_state=rng","160","    )","161","    gb.fit(X_classification, y_classification)","162","    mapper_training_data = gb.bin_mapper_","163","","164","    # Note that since the data is small there is no subsampling and the","165","    # random_state doesn't matter","166","    mapper_whole_data = _BinMapper(random_state=0)","167","    mapper_whole_data.fit(X_classification)","168","","169","    n_samples = X_classification.shape[0]","170","    assert np.all(mapper_training_data.actual_n_bins_ ==","171","                  int((1 - validation_fraction) * n_samples))","172","    assert np.all(mapper_training_data.actual_n_bins_ !=","173","                  mapper_whole_data.actual_n_bins_)"],"delete":[]}],"doc\/whats_new\/v0.22.rst":[{"add":["41",":mod:`sklearn.ensemble`","42",".......................","43","","44","- |Fix| :class:`ensemble.HistGradientBoostingClassifier` and","45","  :class:`ensemble.HistGradientBoostingRegressor` now bin the training and","46","  validation data separately to avoid any data leak. :pr:`13933` by","47","  `NicolasHug`_.","48",""],"delete":[]}]}},"be17713d85efc496ffe0bd7c86e4efad67f548b1":{"changes":{"sklearn\/exceptions.py":"MODIFY",".circleci\/config.yml":"MODIFY","build_tools\/circle\/build_test_pypy.sh":"MODIFY","sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_gradient_boosting.py":"MODIFY","sklearn\/feature_extraction\/tests\/test_text.py":"MODIFY","sklearn\/datasets\/tests\/test_base.py":"MODIFY"},"diff":{"sklearn\/exceptions.py":[{"add":["32","    appropriate arguments before using this method.\"...)"],"delete":["32","    appropriate arguments before using this method.\")"]}],".circleci\/config.yml":[{"add":["79","      - image: pypy:3.6-7.1.1"],"delete":["79","      - image: pypy:3-7.0.0"]}],"build_tools\/circle\/build_test_pypy.sh":[{"add":["23","pip install --extra-index https:\/\/antocuni.github.io\/pypy-wheels\/ubuntu numpy Cython pytest","24","pip install scipy sphinx numpydoc docutils joblib pillow","30","export OMP_NUM_THREADS=\"1\"","32","pip install -e .","33","","34","# Check that Python implementation is PyPy","35","python - << EOL","36","import platform","37","from sklearn.utils import IS_PYPY","38","assert IS_PYPY is True, \"platform={}!=PyPy\".format(platform.python_implementation())","39","EOL"],"delete":["23","pip install --extra-index https:\/\/antocuni.github.io\/pypy-wheels\/ubuntu \"numpy==1.15.*\" Cython pytest","24","pip install \"scipy>=1.1.0\" sphinx numpydoc docutils joblib pillow","31","pip install -vv -e . "]}],"sklearn\/ensemble\/_hist_gradient_boosting\/tests\/test_gradient_boosting.py":[{"add":["68","    X, y = make_regression(n_samples=50, random_state=0)","89","    make_classification(n_samples=30, random_state=0),","90","    make_classification(n_samples=30, n_classes=3, n_clusters_per_class=1,","91","                        random_state=0)"],"delete":["68","    X, y = make_regression(random_state=0)","89","    make_classification(random_state=0),","90","    make_classification(n_classes=3, n_clusters_per_class=1, random_state=0)"]}],"sklearn\/feature_extraction\/tests\/test_text.py":[{"add":["1094","        pytest.xfail(reason='HashingVectorizer is not supported on PyPy')","1201","     HashingVectorizer]","1209","    if issubclass(Estimator, HashingVectorizer):","1210","        pytest.xfail('HashingVectorizer is not supported on PyPy')","1241","     HashingVectorizer]","1248","    if issubclass(Estimator, HashingVectorizer):","1249","        pytest.xfail('HashingVectorizer is not supported on PyPy')","1250",""],"delete":["1094","        pytest.xfail(reason='HashingVectorizer not supported on PyPy')","1201","     pytest.param(HashingVectorizer, marks=fails_if_pypy)]","1239","     pytest.param(HashingVectorizer, marks=fails_if_pypy)]"]}],"sklearn\/datasets\/tests\/test_base.py":[{"add":["31","from sklearn.utils import IS_PYPY","95","    if IS_PYPY:","96","        pytest.xfail('[PyPy] fails due to string containing NUL characters')","106","    if IS_PYPY:","107","        pytest.xfail('[PyPy] fails due to string containing NUL characters')"],"delete":[]}]}},"8155d0e492b4fe5913eb1dc3d76fbf7d46365318":{"changes":{"sklearn\/isotonic.py":"MODIFY","sklearn\/tests\/test_isotonic.py":"MODIFY"},"diff":{"sklearn\/isotonic.py":[{"add":["9","from .utils import check_array, check_consistent_length","121","    y = check_array(y, ensure_2d=False, dtype=[np.float64, np.float32])","317","        check_params = dict(accept_sparse=False, ensure_2d=False,","318","                            dtype=[np.float64, np.float32])","319","        X = check_array(X, **check_params)","320","        y = check_array(y, **check_params)","321","        check_consistent_length(X, y, sample_weight)","322",""],"delete":["9","from .utils import as_float_array, check_array, check_consistent_length","121","    y = as_float_array(y)","242","        X = as_float_array(X)","243","        y = check_array(y, dtype=X.dtype, ensure_2d=False)","244","        check_consistent_length(X, y, sample_weight)"]}],"sklearn\/tests\/test_isotonic.py":[{"add":["8","from sklearn.utils.validation import check_array","17","","27","    y_transformed_s = \\","28","        ir.fit(x_s, y_s, sample_weight=sample_weight_s).transform(x)","433","    y_train = np.less(rng.rand(n_samples),","434","                      expit(X_train)).astype('int64').astype('float64')","475","            expected_dtype = \\","476","                check_array(y_np, dtype=[np.float64, np.float32],","477","                            ensure_2d=False).dtype"],"delete":["8","from sklearn.utils.validation import as_float_array","26","    y_transformed_s = ir.fit(x_s, y_s, sample_weight=sample_weight_s).transform(x)","431","    y_train = np.less(rng.rand(n_samples), expit(X_train)).astype('int64')","472","            expected_dtype = as_float_array(y_np).dtype"]}]}},"fc6d906c01848a24b6f247b8598ba5a37c0098fb":{"changes":{"sklearn\/linear_model\/tests\/test_coordinate_descent.py":"MODIFY","sklearn\/linear_model\/cd_fast.pyx":"MODIFY"},"diff":{"sklearn\/linear_model\/tests\/test_coordinate_descent.py":[{"add":["847","","848","","849","def test_convergence_warnings():","850","    random_state = np.random.RandomState(0)","851","    X = random_state.standard_normal((1000, 500))","852","    y = random_state.standard_normal((1000, 3))","853","","854","    # check that the model fails to converge","855","    with pytest.warns(ConvergenceWarning):","856","        MultiTaskElasticNet(max_iter=1, tol=0).fit(X, y)","857","","858","    # check that the model converges w\/o warnings","859","    with pytest.warns(None) as record:","860","        MultiTaskElasticNet(max_iter=1000).fit(X, y)","861","","862","    assert not record.list","863","","864","","865","def test_sparse_input_convergence_warning():","866","    X, y, _, _ = build_dataset(n_samples=1000, n_features=500)","867","","868","    with pytest.warns(ConvergenceWarning):","869","        ElasticNet(max_iter=1, tol=0).fit(","870","            sparse.csr_matrix(X, dtype=np.float32), y)","871","","872","    # check that the model converges w\/o warnings","873","    with pytest.warns(None) as record:","874","        Lasso(max_iter=1000).fit(sparse.csr_matrix(X, dtype=np.float32), y)","875","","876","    assert not record.list"],"delete":[]}],"sklearn\/linear_model\/cd_fast.pyx":[{"add":["156","        warnings.warn(\"Coordinate descent with no regularization may lead to \"","157","                      \"unexpected results and is discouraged.\")","250","            # for\/else, runs if for doesn't end with a `break`","252","                warnings.warn(\"Objective did not converge. You might want to \"","253","                              \"increase the number of iterations. Duality \"","254","                              \"gap: {}, tolerance: {}\".format(gap, tol),","255","                              ConvergenceWarning)","465","            # for\/else, runs if for doesn't end with a `break`","467","                warnings.warn(\"Objective did not converge. You might want to \"","468","                              \"increase the number of iterations. Duality \"","469","                              \"gap: {}, tolerance: {}\".format(gap, tol),","470","                              ConvergenceWarning)","617","        else:","618","            # for\/else, runs if for doesn't end with a `break`","619","            with gil:","620","                warnings.warn(\"Objective did not converge. You might want to \"","621","                              \"increase the number of iterations. Duality \"","622","                              \"gap: {}, tolerance: {}\".format(gap, tol),","623","                              ConvergenceWarning)","812","        else:","813","            # for\/else, runs if for doesn't end with a `break`","814","            with gil:","815","                warnings.warn(\"Objective did not converge. You might want to \"","816","                              \"increase the number of iterations. Duality \"","817","                              \"gap: {}, tolerance: {}\".format(gap, tol),","818","                              ConvergenceWarning)"],"delete":["156","        warnings.warn(\"Coordinate descent with no regularization may lead to unexpected\"","157","            \" results and is discouraged.\")","251","                warnings.warn(\"Objective did not converge.\"","252","                \" You might want to increase the number of iterations.\"","253","                \" Duality gap: {}, tolerance: {}\".format(gap, tol),","254","                ConvergenceWarning)","465","                warnings.warn(\"Objective did not converge.\"","466","                \" You might want to increase the number of iterations.\"","467","                \" Duality gap: {}, tolerance: {}\".format(gap, tol),","468","                ConvergenceWarning)","615","","616","        with gil:","617","            warnings.warn(\"Objective did not converge.\"","618","            \" You might want to increase the number of iterations.\"","619","            \" Duality gap: {}, tolerance: {}\".format(gap, tol),","620","            ConvergenceWarning)","809","                else:","810","                    with gil:","811","                        warnings.warn(\"Objective did not converge.\"","812","                        \" You might want to increase the number of iterations.\"","813","                        \" Duality gap: {}, tolerance: {}\".format(gap, tol),","814","                        ConvergenceWarning)"]}]}},"1f5bcaeb39698c9a150ef92cdeaeec75c355a274":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/utils\/validation.py":"MODIFY","sklearn\/utils\/extmath.py":"MODIFY","sklearn\/preprocessing\/tests\/test_data.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["224","- |Fix| Fixed the calculation overflow when using a float16 dtype with","225","  :class:`preprocessing.StandardScaler`. :issue:`13007` by","226","  :user:`Raffaello Baluyot <baluyotraf>`","227",""],"delete":[]}],"sklearn\/utils\/validation.py":[{"add":["36","    # validation is also imported in extmath","37","    from .extmath import _safe_accumulator_op","38","","44","    # false positives from overflow in sum method. The sum is also calculated","45","    # safely to reduce dtype induced overflows.","47","    if is_float and (np.isfinite(_safe_accumulator_op(np.sum, X))):"],"delete":["41","    # false positives from overflow in sum method.","43","    if is_float and np.isfinite(X.sum()):"]}],"sklearn\/utils\/extmath.py":[{"add":["660","# Use at least float64 for the accumulating functions to avoid precision issue","661","# see https:\/\/github.com\/numpy\/numpy\/issues\/9393. The float64 is also retained","662","# as it is in case the float overflows","663","def _safe_accumulator_op(op, x, *args, **kwargs):","664","    \"\"\"","665","    This function provides numpy accumulator functions with a float64 dtype","666","    when used on a floating point input. This prevents accumulator overflow on","667","    smaller floating point dtypes.","668","","669","    Parameters","670","    ----------","671","    op : function","672","        A numpy accumulator function such as np.mean or np.sum","673","    x : numpy array","674","        A numpy array to apply the accumulator function","675","    *args : positional arguments","676","        Positional arguments passed to the accumulator function after the","677","        input x","678","    **kwargs : keyword arguments","679","        Keyword arguments passed to the accumulator function","680","","681","    Returns","682","    -------","683","    result : The output of the accumulator function passed to this function","684","    \"\"\"","685","    if np.issubdtype(x.dtype, np.floating) and x.dtype.itemsize < 8:","686","        result = op(x, *args, **kwargs, dtype=np.float64)","687","    else:","688","        result = op(x, *args, **kwargs)","689","    return result","690","","691","","742","    new_sum = _safe_accumulator_op(np.nansum, X, axis=0)","752","        new_unnormalized_variance = (","753","            _safe_accumulator_op(np.nanvar, X, axis=0) * new_sample_count)"],"delete":["710","    if np.issubdtype(X.dtype, np.floating) and X.dtype.itemsize < 8:","711","        # Use at least float64 for the accumulator to avoid precision issues;","712","        # see https:\/\/github.com\/numpy\/numpy\/issues\/9393","713","        new_sum = np.nansum(X, axis=0, dtype=np.float64).astype(X.dtype)","714","    else:","715","        new_sum = np.nansum(X, axis=0)","725","        new_unnormalized_variance = np.nanvar(X, axis=0) * new_sample_count"]}],"sklearn\/preprocessing\/tests\/test_data.py":[{"add":["452","def test_scaler_float16_overflow():","453","    # Test if the scaler will not overflow on float16 numpy arrays","454","    rng = np.random.RandomState(0)","455","    # float16 has a maximum of 65500.0. On the worst case 5 * 200000 is 100000","456","    # which is enough to overflow the data type","457","    X = rng.uniform(5, 10, [200000, 1]).astype(np.float16)","458","","459","    with np.errstate(over='raise'):","460","        scaler = StandardScaler().fit(X)","461","        X_scaled = scaler.transform(X)","462","","463","    # Calculate the float64 equivalent to verify result","464","    X_scaled_f64 = StandardScaler().fit_transform(X.astype(np.float64))","465","","466","    # Overflow calculations may cause -inf, inf, or nan. Since there is no nan","467","    # input, all of the outputs should be finite. This may be redundant since a","468","    # FloatingPointError exception will be thrown on overflow above.","469","    assert np.all(np.isfinite(X_scaled))","470","","471","    # The normal distribution is very unlikely to go above 4. At 4.0-8.0 the","472","    # float16 precision is 2^-8 which is around 0.004. Thus only 2 decimals are","473","    # checked to account for precision differences.","474","    assert_array_almost_equal(X_scaled, X_scaled_f64, decimal=2)","475","","476",""],"delete":[]}]}},"61de4021dae4a2edfa42b59357a7b628e634ac14":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/metrics\/pairwise.py":"MODIFY","sklearn\/metrics\/tests\/test_pairwise.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["25","- |Fix| Fixed two bugs in :class:`metrics.pairwise_distances` when","26","  ``n_jobs > 1``. First it used to return a distance matrix with same dtype as","27","  input, even for integer dtype. Then the diagonal was not zeros for euclidean","28","  metric when ``Y`` is ``X``. :issue:`13877` by","29","  :user:`J¨¦r¨¦mie du Boisberranger <jeremiedbb>`.","30",""],"delete":[]}],"sklearn\/metrics\/pairwise.py":[{"add":["1184","    X, Y, dtype = _return_float_dtype(X, Y)","1191","    ret = np.empty((X.shape[0], Y.shape[0]), dtype=dtype, order='F')","1196","    if (X is Y or Y is None) and func is euclidean_distances:","1197","        # zeroing diagonal for euclidean norm.","1198","        # TODO: do it also for other norms.","1199","        np.fill_diagonal(ret, 0)","1200",""],"delete":["1190","    ret = np.empty((X.shape[0], Y.shape[0]), dtype=X.dtype, order='F')"]}],"sklearn\/metrics\/tests\/test_pairwise.py":[{"add":["249","@pytest.mark.parametrize('array_constr', [np.array, csr_matrix])","250","@pytest.mark.parametrize('dtype', [np.float64, int])","251","def test_pairwise_parallel(func, metric, kwds, array_constr, dtype):","252","    rng = np.random.RandomState(0)","253","    X = array_constr(5 * rng.random_sample((5, 4)), dtype=dtype)","254","    Y = array_constr(5 * rng.random_sample((3, 4)), dtype=dtype)","255","","256","    try:","257","        S = func(X, metric=metric, n_jobs=1, **kwds)","258","    except (TypeError, ValueError) as exc:","259","        # Not all metrics support sparse input","260","        # ValueError may be triggered by bad callable","261","        if array_constr is csr_matrix:","262","            with pytest.raises(type(exc)):","263","                func(X, metric=metric, n_jobs=2, **kwds)","264","            return","265","        else:","266","            raise","267","    S2 = func(X, metric=metric, n_jobs=2, **kwds)","268","    assert_allclose(S, S2)","269","","270","    S = func(X, Y, metric=metric, n_jobs=1, **kwds)","271","    S2 = func(X, Y, metric=metric, n_jobs=2, **kwds)","272","    assert_allclose(S, S2)","545","@pytest.mark.parametrize(","546","        'metric',","547","        ('euclidean', 'l2', 'sqeuclidean'))","548","def test_parallel_pairwise_distances_diagonal(metric):","549","    rng = np.random.RandomState(0)","550","    X = rng.normal(size=(1000, 10), scale=1e10)","551","    distances = pairwise_distances(X, metric=metric, n_jobs=2)","552","    assert_allclose(np.diag(distances), 0, atol=1e-10)","553","","554",""],"delete":["233","def check_pairwise_parallel(func, metric, kwds):","234","    rng = np.random.RandomState(0)","235","    for make_data in (np.array, csr_matrix):","236","        X = make_data(rng.random_sample((5, 4)))","237","        Y = make_data(rng.random_sample((3, 4)))","238","","239","        try:","240","            S = func(X, metric=metric, n_jobs=1, **kwds)","241","        except (TypeError, ValueError) as exc:","242","            # Not all metrics support sparse input","243","            # ValueError may be triggered by bad callable","244","            if make_data is csr_matrix:","245","                assert_raises(type(exc), func, X, metric=metric,","246","                              n_jobs=2, **kwds)","247","                continue","248","            else:","249","                raise","250","        S2 = func(X, metric=metric, n_jobs=2, **kwds)","251","        assert_array_almost_equal(S, S2)","252","","253","        S = func(X, Y, metric=metric, n_jobs=1, **kwds)","254","        S2 = func(X, Y, metric=metric, n_jobs=2, **kwds)","255","        assert_array_almost_equal(S, S2)","256","","257","","274","def test_pairwise_parallel(func, metric, kwds):","275","    check_pairwise_parallel(func, metric, kwds)"]}]}},"eb011b8c2ee2d335df101d648753cb1dc94631a9":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/tests\/test_dummy.py":"MODIFY","sklearn\/dummy.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["101","  ","102",":mod:`sklearn.dummy`","103","....................","104","","105","- |Fix| Fixed a bug in :class:`dummy.DummyClassifier` where the","106","  ``predict_proba`` method was returning int32 array instead of","107","  float64 for the ``stratified`` strategy. :issue:`13266` by","108","  :user:`Christos Aridas<chkoar>`."],"delete":[]}],"sklearn\/tests\/test_dummy.py":[{"add":["711","","712","","713","@pytest.mark.parametrize(","714","    \"strategy\", [\"stratified\", \"most_frequent\", \"prior\", \"uniform\", \"constant\"]","715",")","716","def test_dtype_of_classifier_probas(strategy):","717","    y = [0, 2, 1, 1]","718","    X = np.zeros(4)","719","    model = DummyClassifier(strategy=strategy, random_state=0, constant=0)","720","    probas = model.fit(X, y).predict_proba(X)","721","","722","    assert probas.dtype == np.float64"],"delete":[]}],"sklearn\/dummy.py":[{"add":["279","                out = out.astype(np.float64)"],"delete":[]}]}},"0a1ee74a14ed8fe94bb0c7c10c9e3d99db9cd2b8":{"changes":{"examples\/model_selection\/plot_confusion_matrix.py":"MODIFY"},"diff":{"examples\/model_selection\/plot_confusion_matrix.py":[{"add":["34","from sklearn.utils.multiclass import unique_labels","51","def plot_confusion_matrix(y_true, y_pred, classes,","53","                          title=None,","59","    if not title:","60","        if normalize:","61","            title = 'Normalized confusion matrix'","62","        else:","63","            title = 'Confusion matrix, without normalization'","64","","65","    # Compute confusion matrix","66","    cm = confusion_matrix(y_true, y_pred)","67","    # Only use the labels that appear in the data","68","    classes = classes[unique_labels(y_true, y_pred)]","77","    fig, ax = plt.subplots()","78","    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)","79","    ax.figure.colorbar(im, ax=ax)","80","    # We want to show all ticks...","81","    ax.set(xticks=np.arange(cm.shape[1]),","82","           yticks=np.arange(cm.shape[0]),","83","           # ... and label them with the respective list entries","84","           xticklabels=classes, yticklabels=classes,","85","           title=title,","86","           ylabel='True label',","87","           xlabel='Predicted label')","89","    # Rotate the tick labels and set their alignment.","90","    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",","91","             rotation_mode=\"anchor\")","92","","93","    # Loop over data dimensions and create text annotations.","96","    for i in range(cm.shape[0]):","97","        for j in range(cm.shape[1]):","98","            ax.text(j, i, format(cm[i, j], fmt),","99","                    ha=\"center\", va=\"center\",","100","                    color=\"white\" if cm[i, j] > thresh else \"black\")","101","    fig.tight_layout()","102","    return ax","108","plot_confusion_matrix(y_test, y_pred, classes=class_names,","112","plot_confusion_matrix(y_test, y_pred, classes=class_names, normalize=True,"],"delete":["28","import itertools","51","def plot_confusion_matrix(cm, classes,","53","                          title='Confusion matrix',","67","    plt.imshow(cm, interpolation='nearest', cmap=cmap)","68","    plt.title(title)","69","    plt.colorbar()","70","    tick_marks = np.arange(len(classes))","71","    plt.xticks(tick_marks, classes, rotation=45)","72","    plt.yticks(tick_marks, classes)","76","    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):","77","        plt.text(j, i, format(cm[i, j], fmt),","78","                 horizontalalignment=\"center\",","79","                 color=\"white\" if cm[i, j] > thresh else \"black\")","80","","81","    plt.ylabel('True label')","82","    plt.xlabel('Predicted label')","83","    plt.tight_layout()","86","# Compute confusion matrix","87","cnf_matrix = confusion_matrix(y_test, y_pred)","91","plt.figure()","92","plot_confusion_matrix(cnf_matrix, classes=class_names,","96","plt.figure()","97","plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,"]}]}},"07b0a420a9dff7de9d97f93f1b6de6ef66d4edbf":{"changes":{"sklearn\/utils\/_pprint.py":"MODIFY","sklearn\/utils\/tests\/test_pprint.py":"MODIFY"},"diff":{"sklearn\/utils\/_pprint.py":[{"add":["323","    # Note: need to copy _dispatch to prevent instances of the builtin","324","    # PrettyPrinter class to call methods of _EstimatorPrettyPrinter (see issue","325","    # 12906)","326","    _dispatch = pprint.PrettyPrinter._dispatch.copy()"],"delete":["323","    _dispatch = pprint.PrettyPrinter._dispatch"]}],"sklearn\/utils\/tests\/test_pprint.py":[{"add":["1","from pprint import PrettyPrinter","314","","315","","316","def test_builtin_prettyprinter():","317","    # non regression test than ensures we can still use the builtin","318","    # PrettyPrinter class for estimators (as done e.g. by joblib).","319","    # Used to be a bug","320","","321","    PrettyPrinter().pprint(LogisticRegression())"],"delete":[]}]}},"e7bd8a33e580c2ea42dec82bb9aad9e353ec412d":{"changes":{"sklearn\/cluster\/tests\/test_optics.py":"MODIFY","sklearn\/cluster\/optics_.py":"MODIFY"},"diff":{"sklearn\/cluster\/tests\/test_optics.py":[{"add":["100","                   max_eps=20, cluster_method='xi',","110","                   max_eps=20, cluster_method='xi',","129","    rng = np.random.RandomState(0)","132","    C2 = [0, 0] + 50 * rng.randn(n_points_per_cluster, 2)"],"delete":["100","                   max_eps=np.inf, cluster_method='xi',","110","                   max_eps=np.inf, cluster_method='xi',","131","    C2 = [0, 0] + 10 * rng.randn(n_points_per_cluster, 2)"]}],"sklearn\/cluster\/optics_.py":[{"add":["846","                    # Our implementation corrects a mistake in the original","847","                    # paper, i.e., in Definition 11 4c, r(x) < r(sD) should be","848","                    # r(x) > r(sD).","849","                    while (reachability_plot[c_end - 1] > D_max"],"delete":["846","                    while (reachability_plot[c_end - 1] < D_max"]}]}},"fce73db57a602eec89583dd1582e69781244fec5":{"changes":{"doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/preprocessing\/_encoders.py":"MODIFY","sklearn\/preprocessing\/tests\/test_encoders.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.20.rst":[{"add":["4"," .. _changes_0_20_3:","5","","6","Version 0.20.3","7","==============","8","","9","**??, 2019**","10","","11","This is a bug-fix release with some minor documentation improvements and","12","enhancements to features released in 0.20.0.","13","","14","Changelog","15","---------","16","","17",":mod:`sklearn.preprocessing`","18","............................","19","","20","- |Fix| Fixed a bug in :class:`preprocessing.OneHotEncoder` where the","21","  deprecation of ``categorical_features`` was handled incorrectly in","22","  combination with ``handle_unknown='ignore'``.","23","  :issue:`12881` by `Joris Van den Bossche`_.","24",""],"delete":[]}],"sklearn\/preprocessing\/_encoders.py":[{"add":["333","            # n_values can also be None (default to catch usage), so set","334","            # _n_values to 'auto' explicitly","335","            self._n_values = 'auto'","456","                                % type(self._n_values))"],"delete":["368","                    self._n_values = 'auto'","454","                                % type(X))"]}],"sklearn\/preprocessing\/tests\/test_encoders.py":[{"add":["228","def test_one_hot_encoder_categorical_features_ignore_unknown():","229","    # GH12881 bug in combination of categorical_features with ignore","230","    X = np.array([[1, 2, 3], [4, 5, 6], [2, 3, 2]]).T","231","    oh = OneHotEncoder(categorical_features=[2], handle_unknown='ignore')","232","","233","    with ignore_warnings(category=DeprecationWarning):","234","        res = oh.fit_transform(X)","235","","236","    expected = np.array([[1, 0, 1], [0, 1, 0], [1, 2, 3], [4, 5, 6]]).T","237","    assert_array_equal(res.toarray(), expected)","238","","239",""],"delete":[]}]}},"5d240c6a0bca0d78795874d438e7fd4bc163c209":{"changes":{"build_tools\/azure\/upload_codecov.sh":"MODIFY"},"diff":{"build_tools\/azure\/upload_codecov.sh":[{"add":["11","coverage combine --append"],"delete":["11","coverage combine"]}]}},"1ae1f1db718e03b958c169c386bd1f7e91ffb6d9":{"changes":{"sklearn\/tests\/test_impute.py":"MODIFY","doc\/whats_new\/v0.20.rst":"MODIFY","sklearn\/impute.py":"MODIFY","sklearn\/utils\/estimator_checks.py":"MODIFY"},"diff":{"sklearn\/tests\/test_impute.py":[{"add":["15","from sklearn.pipeline import make_union","512","      \"'sparse' has to be a boolean or 'auto'\"),","513","     (np.array([['a', 'b'], ['c', 'a']], dtype=str),","514","      np.array([['a', 'b'], ['c', 'a']], dtype=str),","515","      {}, \"MissingIndicator does not support data with dtype\")]","620","def test_missing_indicator_string():","621","    X = np.array([['a', 'b', 'c'], ['b', 'c', 'a']], dtype=object)","622","    indicator = MissingIndicator(missing_values='a', features='all')","623","    X_trans = indicator.fit_transform(X)","624","    assert_array_equal(X_trans, np.array([[True, False, False],","625","                                          [False, False, True]]))","626","","627","","628","@pytest.mark.parametrize(","629","    \"X, missing_values, X_trans_exp\",","630","    [(np.array([['a', 'b'], ['b', 'a']], dtype=object), 'a',","631","      np.array([['b', 'b', True, False], ['b', 'b', False, True]],","632","               dtype=object)),","633","     (np.array([[np.nan, 1.], [1., np.nan]]), np.nan,","634","      np.array([[1., 1., True, False], [1., 1., False, True]])),","635","     (np.array([[np.nan, 'b'], ['b', np.nan]], dtype=object), np.nan,","636","      np.array([['b', 'b', True, False], ['b', 'b', False, True]],","637","               dtype=object)),","638","     (np.array([[None, 'b'], ['b', None]], dtype=object), None,","639","      np.array([['b', 'b', True, False], ['b', 'b', False, True]],","640","               dtype=object))]","641",")","642","def test_missing_indicator_with_imputer(X, missing_values, X_trans_exp):","643","    trans = make_union(","644","        SimpleImputer(missing_values=missing_values, strategy='most_frequent'),","645","        MissingIndicator(missing_values=missing_values)","646","    )","647","    X_trans = trans.fit_transform(X)","648","    assert_array_equal(X_trans, X_trans_exp)","649","","650",""],"delete":["511","      \"'sparse' has to be a boolean or 'auto'\")]"]}],"doc\/whats_new\/v0.20.rst":[{"add":["38",":mod:`sklearn.impute`","39",".....................","40","","41","- |Fix| add support for non-numeric data in","42","  :class:`sklearn.impute.MissingIndicator` which was not supported while","43","  :class:`sklearn.impute.SimpleImputer` was supporting this for some","44","  imputation strategies.","45","  :issue:`13046` by :user:`Guillaume Lemaitre <glemaitre>`.","46",""],"delete":[]}],"sklearn\/impute.py":[{"add":["535","    def _validate_input(self, X):","536","        if not is_scalar_nan(self.missing_values):","537","            force_all_finite = True","538","        else:","539","            force_all_finite = \"allow-nan\"","540","        X = check_array(X, accept_sparse=('csc', 'csr'), dtype=None,","541","                        force_all_finite=force_all_finite)","542","        _check_inputs_dtype(X, self.missing_values)","543","        if X.dtype.kind not in (\"i\", \"u\", \"f\", \"O\"):","544","            raise ValueError(\"MissingIndicator does not support data with \"","545","                             \"dtype {0}. Please provide either a numeric array\"","546","                             \" (with a floating point or integer dtype) or \"","547","                             \"categorical data represented either as an array \"","548","                             \"with integer dtype or an array of string values \"","549","                             \"with an object dtype.\".format(X.dtype))","550","        return X","551","","566","        X = self._validate_input(X)","600","        X = self._validate_input(X)"],"delete":["549","        if not is_scalar_nan(self.missing_values):","550","            force_all_finite = True","551","        else:","552","            force_all_finite = \"allow-nan\"","553","        X = check_array(X, accept_sparse=('csc', 'csr'),","554","                        force_all_finite=force_all_finite)","555","        _check_inputs_dtype(X, self.missing_values)","556","","590","","591","        if not is_scalar_nan(self.missing_values):","592","            force_all_finite = True","593","        else:","594","            force_all_finite = \"allow-nan\"","595","        X = check_array(X, accept_sparse=('csc', 'csr'),","596","                        force_all_finite=force_all_finite)","597","        _check_inputs_dtype(X, self.missing_values)"]}],"sklearn\/utils\/estimator_checks.py":[{"add":["77","SUPPORT_STRING = ['SimpleImputer', 'MissingIndicator']","625","    if name not in SUPPORT_STRING:","626","        X[0, 0] = {'foo': 'bar'}","627","        msg = \"argument must be a string or a number\"","628","        assert_raises_regex(TypeError, msg, estimator.fit, X, y)","629","    else:","630","        # Estimators supporting string will not call np.asarray to convert the","631","        # data to numeric and therefore, the error will not be raised.","632","        # Checking for each element dtype in the input array will be costly.","633","        # Refer to #11401 for full discussion.","634","        estimator.fit(X, y)"],"delete":["74","","625","    X[0, 0] = {'foo': 'bar'}","626","    msg = \"argument must be a string or a number\"","627","    assert_raises_regex(TypeError, msg, estimator.fit, X, y)"]}]}},"703991f3f9bdf5bc95755b84a387b63394a825ee":{"changes":{"doc\/datasets\/index.rst":"MODIFY","doc\/about.rst":"MODIFY","sklearn\/decomposition\/pca.py":"MODIFY"},"diff":{"doc\/datasets\/index.rst":[{"add":[],"delete":["83",".. toctree::","84","    :maxdepth: 2","85","    :hidden:","86","","87","    boston_house_prices","88","    iris","89","    diabetes","90","    digits","91","    linnerud","92","    wine_data","93","    breast_cancer","94","","134",".. toctree::","135","    :maxdepth: 2","136","    :hidden:","137","","138","    olivetti_faces","139","    twenty_newsgroups","140","    labeled_faces","141","    covtype","142","    rcv1","143","    kddcup99","144","    california_housing","145",""]}],"doc\/about.rst":[{"add":["178",".. _Vlad Niculae: https:\/\/vene.ro\/","179",""],"delete":[]}],"sklearn\/decomposition\/pca.py":[{"add":["253","    Tipping, M. E., and Bishop, C. M. (1999). \"Probabilistic principal"],"delete":["253","    `Tipping, M. E., and Bishop, C. M. (1999). \"Probabilistic principal"]}]}},"e7ca6236c50f94777343d8a7a9e8af73b6c24276":{"changes":{"sklearn\/kernel_ridge.py":"MODIFY","sklearn\/exceptions.py":"MODIFY","sklearn\/feature_extraction\/text.py":"MODIFY","sklearn\/covariance\/graph_lasso_.py":"MODIFY","sklearn\/ensemble\/_gb_losses.py":"MODIFY","sklearn\/covariance\/empirical_covariance_.py":"MODIFY","sklearn\/neural_network\/_stochastic_optimizers.py":"MODIFY","sklearn\/neighbors\/lof.py":"MODIFY","sklearn\/decomposition\/base.py":"MODIFY","sklearn\/datasets\/base.py":"MODIFY","sklearn\/tree\/__init__.py":"MODIFY","sklearn\/datasets\/california_housing.py":"MODIFY","sklearn\/cluster\/hierarchical.py":"MODIFY","sklearn\/discriminant_analysis.py":"MODIFY","sklearn\/preprocessing\/data.py":"MODIFY","sklearn\/preprocessing\/label.py":"MODIFY","sklearn\/model_selection\/_validation.py":"MODIFY","sklearn\/decomposition\/dict_learning.py":"MODIFY","sklearn\/metrics\/pairwise.py":"MODIFY","sklearn\/ensemble\/gradient_boosting.py":"MODIFY","sklearn\/pipeline.py":"MODIFY","sklearn\/datasets\/svmlight_format.py":"MODIFY","sklearn\/dummy.py":"MODIFY","sklearn\/svm\/base.py":"MODIFY","sklearn\/ensemble\/weight_boosting.py":"MODIFY","sklearn\/decomposition\/kernel_pca.py":"MODIFY","sklearn\/cluster\/k_means_.py":"MODIFY"},"diff":{"sklearn\/kernel_ridge.py":[{"add":["29","    medium-sized datasets. On the other hand, the learned model is non-sparse"],"delete":["29","    medium-sized datasets. On the other  hand, the learned model is non-sparse"]}],"sklearn\/exceptions.py":[{"add":["31","    ... # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS","116","    ...         gs.fit(X, y)  # This will raise a ValueError since C is < 0"],"delete":["31","    ...                        # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS","116","    ...         gs.fit(X, y)   # This will raise a ValueError since C is < 0"]}],"sklearn\/feature_extraction\/text.py":[{"add":["112","    else:  # assume it's a collection","1498","        if ``use_idf`` is True."],"delete":["112","    else:               # assume it's a collection","1498","        if  ``use_idf`` is True."]}],"sklearn\/covariance\/graph_lasso_.py":[{"add":["339","    >>> true_cov = np.array([[0.8, 0.0, 0.2, 0.0],","340","    ...                      [0.0, 0.4, 0.0, 0.0],","341","    ...                      [0.2, 0.0, 0.3, 0.1],","342","    ...                      [0.0, 0.0, 0.1, 0.7]])","594","    >>> true_cov = np.array([[0.8, 0.0, 0.2, 0.0],","595","    ...                      [0.0, 0.4, 0.0, 0.0],","596","    ...                      [0.2, 0.0, 0.3, 0.1],","597","    ...                      [0.0, 0.0, 0.1, 0.7]])"],"delete":["339","    >>> true_cov = np.array([[.8, 0., .2, 0.],","340","    ...                      [0., .4, 0., 0.],","341","    ...                      [.2, 0., .3, .1],","342","    ...                      [0., 0., .1, .7]])","594","    >>> true_cov = np.array([[.8, 0., .2, 0.],","595","    ...                      [0., .4, 0., 0.],","596","    ...                      [.2, 0., .3, .1],","597","    ...                      [0., 0., .1, .7]])"]}],"sklearn\/ensemble\/_gb_losses.py":[{"add":["881","    'deviance': None,  # for both, multinomial and binomial"],"delete":["881","    'deviance': None,    # for both, multinomial and binomial"]}],"sklearn\/covariance\/empirical_covariance_.py":[{"add":["124","    ...                             cov=real_cov,","125","    ...                             size=500)"],"delete":["124","    ...                                   cov=real_cov,","125","    ...                                   size=500)"]}],"sklearn\/neural_network\/_stochastic_optimizers.py":[{"add":["3","# Authors: Jiyuan Qian <jq401@nyu.edu>"],"delete":["3","# Authors:  Jiyuan Qian <jq401@nyu.edu>"]}],"sklearn\/neighbors\/lof.py":[{"add":["502","        # 1e-10 to avoid `nan' when nb of duplicates > n_neighbors_:"],"delete":["502","        #  1e-10 to avoid `nan' when nb of duplicates > n_neighbors_:"]}],"sklearn\/decomposition\/base.py":[{"add":["29","        where S**2 contains the explained variances, and sigma2 contains the"],"delete":["29","        where  S**2 contains the explained variances, and sigma2 contains the"]}],"sklearn\/datasets\/base.py":[{"add":["570","    ==============   ==================","571","    Samples total    442","572","    Dimensionality   10","573","    Features         real, -.2 < x < .2","574","    Targets          integer 25 - 346","575","    ==============   ==================","623","    ==============   ============================","624","    Samples total    20","625","    Dimensionality   3 (for both data and target)","626","    Features         integer","627","    Targets          integer","628","    ==============   ============================","687","    ==============   ==============","688","    Samples total               506","689","    Dimensionality               13","690","    Features         real, positive","691","    Targets           real 5. - 50.","692","    ==============   =============="],"delete":["570","    ==============      ==================","571","    Samples total       442","572","    Dimensionality      10","573","    Features            real, -.2 < x < .2","574","    Targets             integer 25 - 346","575","    ==============      ==================","623","    ==============    ============================","624","    Samples total     20","625","    Dimensionality    3 (for both data and target)","626","    Features          integer","627","    Targets           integer","628","    ==============    ============================","687","    ==============     ==============","688","    Samples total                 506","689","    Dimensionality                 13","690","    Features           real, positive","691","    Targets             real 5. - 50.","692","    ==============     =============="]}],"sklearn\/tree\/__init__.py":[{"add":["13","           \"plot_tree\", \"export_text\"]"],"delete":["13","           \"plot_tree\",  \"export_text\"]"]}],"sklearn\/datasets\/california_housing.py":[{"add":["52","    ==============   ==============","53","    Samples total             20640","54","    Dimensionality                8","55","    Features                   real","56","    Target           real 0.15 - 5.","57","    ==============   =============="],"delete":["52","    ==============     ==============","53","    Samples total               20640","54","    Dimensionality                  8","55","    Features                     real","56","    Target             real 0.15 - 5.","57","    ==============     =============="]}],"sklearn\/cluster\/hierarchical.py":[{"add":["150","        feature matrix representing n_samples samples to be clustered","221","        from scipy.cluster import hierarchy  # imports PIL","435","        from scipy.cluster import hierarchy  # imports PIL","599","# Functions for cutting hierarchical clustering tree"],"delete":["150","        feature matrix  representing n_samples samples to be clustered","221","        from scipy.cluster import hierarchy     # imports PIL","435","        from scipy.cluster import hierarchy     # imports PIL","599","# Functions for cutting  hierarchical clustering tree"]}],"sklearn\/discriminant_analysis.py":[{"add":["718","        norm2 = np.array(norm2).T  # shape = [len(X), n_classes]"],"delete":["718","        norm2 = np.array(norm2).T   # shape = [len(X), n_classes]"]}],"sklearn\/preprocessing\/data.py":[{"add":["2264","            # for inverse transform, match a uniform distribution"],"delete":["2264","            #  for inverse transform, match a uniform distribution"]}],"sklearn\/preprocessing\/label.py":[{"add":["422","        The output of transform is sometimes referred to as"],"delete":["422","        The output of transform is sometimes referred to    as"]}],"sklearn\/model_selection\/_validation.py":[{"add":["1120","                   random_state=None, error_score='raise-deprecating'):"],"delete":["1120","                   random_state=None,  error_score='raise-deprecating'):"]}],"sklearn\/decomposition\/dict_learning.py":[{"add":["173","                         '\"lasso_cd\", \"lasso\", \"threshold\" or \"omp\", got %s.'"],"delete":["173","                         '\"lasso_cd\",  \"lasso\", \"threshold\" or \"omp\", got %s.'"]}],"sklearn\/metrics\/pairwise.py":[{"add":["1171","    ============   ====================================","1172","    metric         Function","1173","    ============   ====================================","1174","    'cityblock'    metrics.pairwise.manhattan_distances","1175","    'cosine'       metrics.pairwise.cosine_distances","1176","    'euclidean'    metrics.pairwise.euclidean_distances","1177","    'haversine'    metrics.pairwise.haversine_distances","1178","    'l1'           metrics.pairwise.manhattan_distances","1179","    'l2'           metrics.pairwise.euclidean_distances","1180","    'manhattan'    metrics.pairwise.manhattan_distances","1181","    ============   ===================================="],"delete":["1171","    ============     ====================================","1172","    metric           Function","1173","    ============     ====================================","1174","    'cityblock'      metrics.pairwise.manhattan_distances","1175","    'cosine'         metrics.pairwise.cosine_distances","1176","    'euclidean'      metrics.pairwise.euclidean_distances","1177","    'haversine'      metrics.pairwise.haversine_distances","1178","    'l1'             metrics.pairwise.manhattan_distances","1179","    'l2'             metrics.pairwise.euclidean_distances","1180","    'manhattan'      metrics.pairwise.manhattan_distances","1181","    ============     ===================================="]}],"sklearn\/ensemble\/gradient_boosting.py":[{"add":["1696","        X = check_array(X, dtype=DTYPE, order=\"C\", accept_sparse='csr')","2087","        X = check_array(X, dtype=DTYPE, order=\"C\", accept_sparse='csr')","2529","        X = check_array(X, dtype=DTYPE, order=\"C\", accept_sparse='csr')"],"delete":["1696","        X = check_array(X, dtype=DTYPE, order=\"C\",  accept_sparse='csr')","2087","        X = check_array(X, dtype=DTYPE, order=\"C\",  accept_sparse='csr')","2529","        X = check_array(X, dtype=DTYPE, order=\"C\",  accept_sparse='csr')"]}],"sklearn\/pipeline.py":[{"add":["97","    ... # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE","102","    >>> anova_svm.score(X, y)  # doctest: +ELLIPSIS","673","    ... # doctest: +NORMALIZE_WHITESPACE","784","    >>> union.fit_transform(X)  # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS","1010","    >>> make_union(PCA(), TruncatedSVD())  # doctest: +NORMALIZE_WHITESPACE"],"delete":["97","    ...                      # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE","102","    >>> anova_svm.score(X, y)                        # doctest: +ELLIPSIS","673","    ...     # doctest: +NORMALIZE_WHITESPACE","784","    >>> union.fit_transform(X)    # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS","1010","    >>> make_union(PCA(), TruncatedSVD())    # doctest: +NORMALIZE_WHITESPACE"]}],"sklearn\/datasets\/svmlight_format.py":[{"add":["437","            comment.decode(\"ascii\")  # just for the exception"],"delete":["437","            comment.decode(\"ascii\")     # just for the exception"]}],"sklearn\/dummy.py":[{"add":["494","        y : array, shape = [n_samples] or [n_samples, n_outputs]","497","        y_std : array, shape = [n_samples] or [n_samples, n_outputs]"],"delete":["494","        y : array, shape = [n_samples]  or [n_samples, n_outputs]","497","        y_std : array, shape = [n_samples]  or [n_samples, n_outputs]"]}],"sklearn\/svm\/base.py":[{"add":["295","        else:  # regression"],"delete":["295","        else:   # regression"]}],"sklearn\/ensemble\/weight_boosting.py":[{"add":["687","        else:  # self.algorithm == \"SAMME\"","782","        else:  # self.algorithm == \"SAMME\""],"delete":["687","        else:   # self.algorithm == \"SAMME\"","782","        else:   # self.algorithm == \"SAMME\""]}],"sklearn\/decomposition\/kernel_pca.py":[{"add":["232","        #     then Phi(X)v  is an eigenvector of Phi(X)Phi(X)'","234","        #     then Phi(X)'u is an eigenvector of Phi(X)Phi(X)'"],"delete":["232","        #                      then Phi(X)v  is an eigenvector of Phi(X)Phi(X)'","234","        #                      then Phi(X)'u is an eigenvector of Phi(X)Phi(X)'"]}],"sklearn\/cluster\/k_means_.py":[{"add":["708","    x_squared_norms : array, shape (n_samples,), optional","1421","    ...                          random_state=0,","1422","    ...                          batch_size=6)","1432","    ...                          random_state=0,","1433","    ...                          batch_size=6,","1434","    ...                          max_iter=10).fit(X)"],"delete":["708","    x_squared_norms :  array, shape (n_samples,), optional","1421","    ...         random_state=0,","1422","    ...         batch_size=6)","1432","    ...         random_state=0,","1433","    ...         batch_size=6,","1434","    ...         max_iter=10).fit(X)"]}]}},"f16227ba44ea5c2c95228b09474d3f7e8bc22210":{"changes":{"doc\/whats_new\/v0.21.rst":"MODIFY","sklearn\/metrics\/pairwise.py":"MODIFY","sklearn\/metrics\/pairwise_fast.pyx":"MODIFY","sklearn\/metrics\/tests\/test_pairwise.py":"MODIFY"},"diff":{"doc\/whats_new\/v0.21.rst":[{"add":["545","  in version 0.21 and will be removed in version 0.23. :pr:`10580` by","546","  :user:`Reshama Shaikh <reshamas>` and :user:`Sandra Mitrovic <SandraMNE>`.","547","","548","- |Fix| The function :func:`euclidean_distances`, and therefore","549","  several estimators with ``metric='euclidean'``, suffered from numerical","550","  precision issues with ``float32`` features. Precision has been increased at the","551","  cost of a small drop of performance. :pr:`13554` by :user:`Celelibi` and","552","  :user:`J¨¦r¨¦mie du Boisberranger <jeremiedbb>`."],"delete":["545","  in version 0.21 and will be removed in version 0.23.","546","  :pr:`10580` by :user:`Reshama Shaikh <reshamas>` and :user:`Sandra","547","  Mitrovic <SandraMNE>`."]}],"sklearn\/metrics\/pairwise.py":[{"add":["195","        May be ignored in some cases, see the note below.","203","        May be ignored in some cases, see the note below.","204","","205","    Notes","206","    -----","207","    To achieve better accuracy, `X_norm_squared`?and `Y_norm_squared` may be","208","    unused if they are passed as ``float32``.","212","    distances : array, shape (n_samples_1, n_samples_2)","233","    # If norms are passed as float32, they are unused. If arrays are passed as","234","    # float32, norms needs to be recomputed on upcast chunks.","235","    # TODO: use a float64 accumulator in row_norms to avoid the latter.","243","        if XX.dtype == np.float32:","244","            XX = None","245","    elif X.dtype == np.float32:","246","        XX = None","250","    if X is Y and XX is not None:","251","        # shortcut in the common case euclidean_distances(X, X)","259","        if YY.dtype == np.float32:","260","            YY = None","261","    elif Y.dtype == np.float32:","262","        YY = None","266","    if X.dtype == np.float32:","267","        # To minimize precision issues with float32, we compute the distance","268","        # matrix on chunks of X and Y upcast to float64","269","        distances = _euclidean_distances_upcast(X, XX, Y, YY)","270","    else:","271","        # if dtype is already float64, no need to chunk and upcast","272","        distances = - 2 * safe_sparse_dot(X, Y.T, dense_output=True)","273","        distances += XX","274","        distances += YY","277","    # Ensure that distances between vectors and themselves are set to 0.0.","278","    # This may not be the case due to floating point rounding errors.","280","        np.fill_diagonal(distances, 0)","285","def _euclidean_distances_upcast(X, XX=None, Y=None, YY=None):","286","    \"\"\"Euclidean distances between X and Y","287","","288","    Assumes X and Y have float32 dtype.","289","    Assumes XX and YY have float64 dtype or are None.","290","","291","    X and Y are upcast to float64 by chunks, which size is chosen to limit","292","    memory increase by approximately 10% (at least 10MiB).","293","    \"\"\"","294","    n_samples_X = X.shape[0]","295","    n_samples_Y = Y.shape[0]","296","    n_features = X.shape[1]","297","","298","    distances = np.empty((n_samples_X, n_samples_Y), dtype=np.float32)","299","","300","    x_density = X.nnz \/ np.prod(X.shape) if issparse(X) else 1","301","    y_density = Y.nnz \/ np.prod(Y.shape) if issparse(Y) else 1","302","","303","    # Allow 10% more memory than X, Y and the distance matrix take (at least","304","    # 10MiB)","305","    maxmem = max(","306","        ((x_density * n_samples_X + y_density * n_samples_Y) * n_features","307","         + (x_density * n_samples_X * y_density * n_samples_Y)) \/ 10,","308","        10 * 2**17)","309","","310","    # The increase amount of memory in 8-byte blocks is:","311","    # - x_density * batch_size * n_features (copy of chunk of X)","312","    # - y_density * batch_size * n_features (copy of chunk of Y)","313","    # - batch_size * batch_size (chunk of distance matrix)","314","    # Hence x? + (xd+yd)kx = M, where x=batch_size, k=n_features, M=maxmem","315","    #                                 xd=x_density and yd=y_density","316","    tmp = (x_density + y_density) * n_features","317","    batch_size = (-tmp + np.sqrt(tmp**2 + 4 * maxmem)) \/ 2","318","    batch_size = max(int(batch_size), 1)","319","","320","    x_batches = gen_batches(X.shape[0], batch_size)","321","    y_batches = gen_batches(Y.shape[0], batch_size)","322","","323","    for i, x_slice in enumerate(x_batches):","324","        X_chunk = X[x_slice].astype(np.float64)","325","        if XX is None:","326","            XX_chunk = row_norms(X_chunk, squared=True)[:, np.newaxis]","327","        else:","328","            XX_chunk = XX[x_slice]","329","","330","        for j, y_slice in enumerate(y_batches):","331","            if X is Y and j < i:","332","                # when X is Y the distance matrix is symmetric so we only need","333","                # to compute half of it.","334","                d = distances[y_slice, x_slice].T","335","","336","            else:","337","                Y_chunk = Y[y_slice].astype(np.float64)","338","                if YY is None:","339","                    YY_chunk = row_norms(Y_chunk, squared=True)[np.newaxis, :]","340","                else:","341","                    YY_chunk = YY[:, y_slice]","342","","343","                d = -2 * safe_sparse_dot(X_chunk, Y_chunk.T, dense_output=True)","344","                d += XX_chunk","345","                d += YY_chunk","346","","347","            distances[x_slice, y_slice] = d.astype(np.float32, copy=False)","348","","349","    return distances","350","","351",""],"delete":["205","    distances : {array, sparse matrix}, shape (n_samples_1, n_samples_2)","236","    if X is Y:  # shortcut in the common case euclidean_distances(X, X)","247","    distances = safe_sparse_dot(X, Y.T, dense_output=True)","248","    distances *= -2","249","    distances += XX","250","    distances += YY","254","        # Ensure that distances between vectors and themselves are set to 0.0.","255","        # This may not be the case due to floating point rounding errors.","256","        distances.flat[::distances.shape[0] + 1] = 0.0"]}],"sklearn\/metrics\/pairwise_fast.pyx":[{"add":["12","from libc.string cimport memset"],"delete":["9","from libc.string cimport memset"]}],"sklearn\/metrics\/tests\/test_pairwise.py":[{"add":["586","@pytest.mark.parametrize(\"x_array_constr\", [np.array, csr_matrix],","587","                         ids=[\"dense\", \"sparse\"])","588","@pytest.mark.parametrize(\"y_array_constr\", [np.array, csr_matrix],","589","                         ids=[\"dense\", \"sparse\"])","590","def test_euclidean_distances_known_result(x_array_constr, y_array_constr):","591","    # Check the pairwise Euclidean distances computation on known result","592","    X = x_array_constr([[0]])","593","    Y = y_array_constr([[1], [2]])","595","    assert_allclose(D, [[1., 2.]])","598","@pytest.mark.parametrize(\"dtype\", [np.float32, np.float64])","599","@pytest.mark.parametrize(\"y_array_constr\", [np.array, csr_matrix],","600","                         ids=[\"dense\", \"sparse\"])","601","def test_euclidean_distances_with_norms(dtype, y_array_constr):","603","    # and that we get a wrong answer with wrong {X,Y}_norm_squared","604","    rng = np.random.RandomState(0)","605","    X = rng.random_sample((10, 10)).astype(dtype, copy=False)","606","    Y = rng.random_sample((20, 10)).astype(dtype, copy=False)","607","","608","    # norms will only be used if their dtype is float64","609","    X_norm_sq = (X.astype(np.float64) ** 2).sum(axis=1).reshape(1, -1)","610","    Y_norm_sq = (Y.astype(np.float64) ** 2).sum(axis=1).reshape(1, -1)","611","","612","    Y = y_array_constr(Y)","613","","619","    assert_allclose(D2, D1)","620","    assert_allclose(D3, D1)","621","    assert_allclose(D4, D1)","627","    with pytest.raises(AssertionError):","628","        assert_allclose(wrong_D, D1)","629","","630","","631","@pytest.mark.parametrize(\"dtype\", [np.float32, np.float64])","632","@pytest.mark.parametrize(\"x_array_constr\", [np.array, csr_matrix],","633","                         ids=[\"dense\", \"sparse\"])","634","@pytest.mark.parametrize(\"y_array_constr\", [np.array, csr_matrix],","635","                         ids=[\"dense\", \"sparse\"])","636","def test_euclidean_distances(dtype, x_array_constr, y_array_constr):","637","    # check that euclidean distances gives same result as scipy cdist","638","    # when X and Y != X are provided","639","    rng = np.random.RandomState(0)","640","    X = rng.random_sample((100, 10)).astype(dtype, copy=False)","641","    X[X < 0.8] = 0","642","    Y = rng.random_sample((10, 10)).astype(dtype, copy=False)","643","    Y[Y < 0.8] = 0","644","","645","    expected = cdist(X, Y)","646","","647","    X = x_array_constr(X)","648","    Y = y_array_constr(Y)","649","    distances = euclidean_distances(X, Y)","650","","651","    # the default rtol=1e-7 is too close to the float32 precision","652","    # and fails due too rounding errors.","653","    assert_allclose(distances, expected, rtol=1e-6)","654","    assert distances.dtype == dtype","655","","656","","657","@pytest.mark.parametrize(\"dtype\", [np.float32, np.float64])","658","@pytest.mark.parametrize(\"x_array_constr\", [np.array, csr_matrix],","659","                         ids=[\"dense\", \"sparse\"])","660","def test_euclidean_distances_sym(dtype, x_array_constr):","661","    # check that euclidean distances gives same result as scipy pdist","662","    # when only X is provided","663","    rng = np.random.RandomState(0)","664","    X = rng.random_sample((100, 10)).astype(dtype, copy=False)","665","    X[X < 0.8] = 0","666","","667","    expected = squareform(pdist(X))","668","","669","    X = x_array_constr(X)","670","    distances = euclidean_distances(X)","671","","672","    # the default rtol=1e-7 is too close to the float32 precision","673","    # and fails due too rounding errors.","674","    assert_allclose(distances, expected, rtol=1e-6)","675","    assert distances.dtype == dtype","676","","677","","678","@pytest.mark.parametrize(","679","    \"dtype, eps, rtol\",","680","    [(np.float32, 1e-4, 1e-5),","681","     pytest.param(","682","         np.float64, 1e-8, 0.99,","683","         marks=pytest.mark.xfail(reason='failing due to lack of precision'))])","684","@pytest.mark.parametrize(\"dim\", [1, 1000000])","685","def test_euclidean_distances_extreme_values(dtype, eps, rtol, dim):","686","    # check that euclidean distances is correct with float32 input thanks to","687","    # upcasting. On float64 there are still precision issues.","688","    X = np.array([[1.] * dim], dtype=dtype)","689","    Y = np.array([[1. + eps] * dim], dtype=dtype)","690","","691","    distances = euclidean_distances(X, Y)","692","    expected = cdist(X, Y)","693","","694","    assert_allclose(distances, expected, rtol=1e-5)"],"delete":["586","def test_euclidean_distances():","587","    # Check the pairwise Euclidean distances computation","588","    X = [[0]]","589","    Y = [[1], [2]]","591","    assert_array_almost_equal(D, [[1., 2.]])","593","    X = csr_matrix(X)","594","    Y = csr_matrix(Y)","595","    D = euclidean_distances(X, Y)","596","    assert_array_almost_equal(D, [[1., 2.]])","598","    rng = np.random.RandomState(0)","599","    X = rng.random_sample((10, 4))","600","    Y = rng.random_sample((20, 4))","601","    X_norm_sq = (X ** 2).sum(axis=1).reshape(1, -1)","602","    Y_norm_sq = (Y ** 2).sum(axis=1).reshape(1, -1)","603","","610","    assert_array_almost_equal(D2, D1)","611","    assert_array_almost_equal(D3, D1)","612","    assert_array_almost_equal(D4, D1)","615","    X_norm_sq *= 0.5","616","    Y_norm_sq *= 0.5","620","    assert_greater(np.max(np.abs(wrong_D - D1)), .01)"]}]}}}